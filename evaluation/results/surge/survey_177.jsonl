{"id": "c624e9cc-f268-41e0-b42c-8b2236aeabd5", "title": "Introduction", "level": "section", "subsections": ["f08f4b5e-3fba-496a-8520-a6d98805f31d"], "parent_id": "cee5380c-e5e4-4247-be7e-57acffba6561", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "Introduction"]], "content": "\\begin{figure*}[h]\n\\begin{center}\n\\includegraphics[width=14cm, height=12cm]{ammu-timeline.png}\n\\caption{\\label{ammu-timeline} Key milestones in T-BPLMs } \n\\end{center}\n\\end{figure*}\n\\IEEEPARstart{T}{ransformer} based PLMs like BERT , RoBERTa , T5  have started a new era in modern NLP. These models combine the power of transformers, transfer learning, and self-supervised learning. Transformers use self-attention which can be run in parallel and can model long-range relationships with ease. In transfer learning , knowledge gained by the model in the source task is transferred to the target task. For example, computer vision models are trained over large labeled datasets, and then these pretrained models are used in similar tasks where the labeled datasets are small . The main advantages of pretrained models are a) they learn language representations that are useful across tasks and b) no need to train the downstream models from scratch. However, in NLP, it is quite expensive and difficult to obtain such large, annotated datasets. So, transformer-based PLMs are pretrained over large unlabeled text data using self-supervised learning. Self-supervised learning is in between supervised and unsupervised learning. Supervised learning requires human-annotated instances while unsupervised learning does not require any labeled instances. Self-supervised learning relies on labels like supervised and semi-supervised learning. However, these labels are not human assigned but created automatically by using the relationships between various sections of the input data. Once the model is pre-trained over large volumes of text, it can be used in various downstream tasks by fine-tuning after adding task-specific layers . \n\\begin{figure*}[h]\n\\begin{center}\n\\includegraphics[width=18cm, height=10cm]{ammu-summary.png}\n\\caption{\\label{ammu-summary} Summmary of AMMU survey paper } \n\\end{center}\n\\end{figure*}\nIn the initial days, NLP systems are mostly rule-based. The development of rule-based systems is quite difficult as it requires significant human intervention in the form of domain expertise to frame the rules. It is required to reframe the rules with even with a small change in the input data which makes it expensive and laborious. Machine learning systems to some extent brought flexibility in developing NLP systems. Machine learning systems learn the rules during training and thereby avoids the laborious process of manual rule framing. However, the main drawback in machine learning models is the requirement of feature engineering which again requires domain expertise. With the development of various deep learning models like convolutional neural networks (CNNs) and recurrent neural networks (RNNs) which can learn features automatically and better hardware like GPUs, NLP researchers shifted to deep learning models with dense word vectors as input . Traditional text representation methods like tf-idf and one-hot vectors are high-dimensional which demand more computational resources. Moreover, these representations are unable to encode syntactic and sematic information. This requirement of low-dimensional text vectors which can also encode language information leads to the development of embedding models like Word2Vec , Glove . As these models cannot encode sub-word information and suffer from the out of vocabulary (OOV) problem, FastText  is proposed. Some of the drawbacks of using CNN or RNN with dense word vectors as input are a) Embeddings models like Word2Vec, Glove, and FastText are based on shallow neural networks. Shallow neural networks with only two or three layers are unable to capture more language information into word vectors. Being context insensitive further limits the quality of these word vectors. b) Even though word embeddings are pre-trained on text corpus, the parameters of models like CNN and RNN are randomly initialized and learned during model training. Learning model parameters from scratch requires a large number of training instances.\nSelf-attention computes the representation of every token in the input based on its interaction with every token in the input. As a result, the self-attention mechanism can better handle long distance word relationships compared to CNN and RNN . Moreover, transformers can learn complex language information by applying self-attention layers iteratively i.e., by using a stack of self-attention layers. Transformers with self-attention as the core component have become the primary choice of architecture for pretrained language models in NLP. Transformer-based PLMs like BERT , RoBERTa , ALBERT , T5  achieved tremendous success in many of the NLP tasks. These models eliminate the requirement of training a downstream model from scratch. With the success of these models, pretraining the model on large volumes of text and then fine-tuning it  on task-specific datasets has become a standard approach in modern NLP. Following the success of transformer-based PLMs in the general domain, biomedical NLP researchers have developed models like BioBERT , ClinicalBERT , and BlueBERT . All these models are obtained by further pretraining general BERT on biomedical texts except ClinicalBERT which is initialized from BioBERT. \nLee et al.  proposed BioBERT in January 2019 and it is the first transformer-based BPLM. After that, number of models are proposed like ClinicalBERT , ClinicalXLNet , BlueBERT , PubMedBERT , ouBioBERT . Since BioBERT, around 40+ BPLMs are proposed to push the state-of-the-art in various biomedical NLP tasks. Figure \\ref{ammu-timeline} summarizes key milestones in transformer-based BPLMs. Transformer-based BPLMs have become the first choice for any task in biomedical NLP. However, there is no survey paper that presents the recent trends in the transformer-based PLMs in biomedical NLP.\nCurrently, there are three survey papers that provide a comprehensive review of embeddings in the biomedical domain and three survey papers that provide a comprehensive review of transformer-based PLMs in the general domain. The survey paper written by Kalyan and Sangeetha  is the first comprehensive survey on embeddings in biomedical NLP. This paper a) classify and compare various biomedical corpora b) present a brief overview of various context insensitive embedding models and compare them c) classify and explain various biomedical embeddings d) present solutions to various challenges in biomedical embeddings. The survey papers written by Chiu and Baker , Khattak et al.  also present the same contents differently. All these three survey papers provide information mostly on context insensitive biomedical embeddings with very little emphasis on transformer-based BPLMs. The paper by Wang et al.  provides empirical evaluation of word embeddings trained from various corpora.  The survey papers written by Qiu et al. , Liu et al.  and Kalyan et al.  present a review of various transformer-based PLMs in the general domain only. So, we strongly believe there is a need for a survey paper that presents the recent trends related to transformer-based BPLMs (T-BPLMs). Figure \\ref{ammu-summary} summarizes the contents of this survey paper.", "cites": [1150, 4030, 4034, 7, 305, 8369, 7165, 2607, 4035, 826, 38, 9, 8719, 4033, 4031, 1445, 1156, 4032, 4029], "cite_extract_rate": 0.7307692307692307, "origin_cites_number": 26, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes ideas from multiple cited papers, integrating concepts like self-supervised learning, transfer learning, and the evolution of NLP architectures to create a coherent narrative on the development of biomedical PLMs. It also identifies gaps in existing surveys, showing some critical evaluation. While it provides useful context and abstraction by discussing broader trends, it primarily serves as an analytical foundation for the survey rather than offering novel conceptual frameworks."}}
{"id": "ac354f89-0a46-4758-afe1-2ad8abf1bc49", "title": "Embedding Layer", "level": "subsection", "subsections": [], "parent_id": "f52c4b59-c1c2-4ce9-a7af-b154289522ac", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "Foundations "], ["subsection", "Embedding Layer"]], "content": "\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=8cm, height=4cm]{ammu-embedding-matrix1.png}\n\\caption{\\label{ammu-embedding-matrix} Final input vectors obtained by summing all the three vectors.} \n\\end{center}\n\\end{figure}\nUsually, the embedding layer consists of three sub-layers with each sub-layer representing a particular embedding type. In some models, there are more than three also. For example, embedding layer of BERT-EHR  contains code, position, segment, age and gender embeddings. A detailed description of various embedding types is presented in Section \\ref{embeddings-ssec}. The first sub-layer converts input tokens to a sequence of vectors while the other two sub-layers provide auxiliary information like position and segmentation. The first sub-layer can be char, sub-word, or code embedding based. For example, BioCharBERT  uses CharCNN  on the top of character embeddings, BERT uses WordPiece  embeddings while BEHRT , MedBERT  and BERT-EHR  models use code embeddings. Unlike BioCharBERT and BERT models, the input for BEHRT, MedBERT, BERT-EHR models is patient visits where each patient visit is expressed as a sequence of codes. The final input representation $X$ for the given input tokens $\\{x_1, x_2, … x_n\\}$is obtained by adding the embeddings from the three sub-layers (for simplicity, we have included only three embedding types – refer Figure \\ref{ammu-embedding-matrix}). \n\\begin{equation}\n    X=I+P+S\n\\end{equation}\nWhere $X \\in R^{n \\: \\times \\: e}$  represents final input embeddings matrix and $I \\in R^{n \\: \\times \\: e}$, $P \\in R^{n \\: \\times \\: e}$  and $S \\in R^{n \\: \\times \\: e}$ represents the three embedding type matrices. Here $n$ represents length of input sequence and $e$ represents embedding size.", "cites": [207, 4036], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of embedding layers in biomedical PLMs, citing specific examples and methods from different models. It synthesizes the information to some extent by grouping embedding types and contrasting approaches (e.g., char vs. code embeddings), but lacks deeper integration or novel framing. There is minimal critical evaluation or identification of broader patterns or principles."}}
{"id": "cce03a9d-6b26-483a-a765-ef7249d310c7", "title": "Position-wise Feed Forward Network (PFN)", "level": "subsubsection", "subsections": [], "parent_id": "7fc532a9-d874-4948-b30c-4bcf378dc802", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "Foundations "], ["subsection", "Transformer Encoder"], ["subsubsection", "Position-wise Feed Forward Network (PFN)"]], "content": "Two linear layers with a non-linear activation constitutes the PFN. PFN is applied to every input token vector. Models like BERT uses Gelu  activation function. Here the parameters of PFNs applied on each of the token vectors are the same. Formally, \n\\begin{equation}\n    PFN(y)=Gelu(yW_1+ b_1 ) W_2+ b_2\n\\end{equation}", "cites": [1512], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal description of the Position-wise Feed Forward Network (PFN) in transformers and cites only one paper (GELU) as an example of an activation function. There is no synthesis of ideas from multiple sources, no critical evaluation of the cited paper or alternative activation functions, and no abstraction to broader patterns or principles in BPLMs."}}
{"id": "f1907c7a-f285-4f7a-b884-e78b83612e8f", "title": "Self-Supervised Learning", "level": "subsection", "subsections": [], "parent_id": "f52c4b59-c1c2-4ce9-a7af-b154289522ac", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "Foundations "], ["subsection", "Self-Supervised Learning"]], "content": "Deep learning algorithms dominated rule-based and machine learning algorithms in the last decade. This is because deep learning models can learn features automatically which eliminates the requirement of expensive feature engineering and process the inputs in an end-to-end manner i.e., take raw inputs and give the decisions.  The success of the deep learning algorithms comes from the knowledge gained during training from human-labeled instances. However, supervised learning has a lot of limitations a) with less data, the model may get overfitted and prone to bias b) some of the domains like biomedical are supervision starved i.e., difficult to get labeled data. In general, we expect models to be close to human intelligence i.e., more general and make decisions with just a few samples. This desire of developing models with more generalization ability and learning from fewer samples has made the researchers focus on other learning paradigms like Self-Supervised Learning . \n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=7cm, height=2cm]{CPT.png}\n\\caption{\\label{ammu-cpt} Continual Pretraining (CPT) } \n\\end{center}\n\\end{figure}\nRobotics is the first AI field to use self-supervised learning methods . Over the last five years, self-supervised learning has become popular in other AI fields like natural language processing , computer vision , and speech processing . SSL is a new learning paradigm that draws inspiration from both supervised and unsupervised learning methods. SSL is similar to unsupervised learning as it does not depend on human-labeled instances. It is also similar to supervised learning as it learns using supervision. However, in SSL the supervision is provided by the pseudo labels which are generated automatically from the pretraining data. SSL involves pretraining the model over a large unlabelled corpus using one or more pretraining tasks. The pseudo labels are generated depending on the definitions of pre-training tasks. SSL methods fall into three categories namely Generative, Contrastive, and Generate-Contrastive . In Generative SSL, encoder maps input vector x to vector y, and decoder recovers x from y (e.g., masked language modeling). In Contrastive SSL, the encoder maps input vector x to vector y to measure similarity (e.g., mutual information maximization). In Generate-Contrastive SSL, fake samples are generated using encoder-decoder while the discriminator identifies the fake samples (e.g., replaced token detection).  For more details about different SSL methods, please refer to the survey paper written by Liu et al. .", "cites": [1458, 4037, 7040, 4035, 9141, 4030, 864, 1445], "cite_extract_rate": 1.0, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic descriptive overview of self-supervised learning (SSL), categorizing it into three types and mentioning examples like masked language modeling. It references multiple papers but primarily uses them to support a general explanation rather than synthesize or critically evaluate their contributions. Some abstraction is attempted by highlighting SSL's role in reducing dependency on labeled data, but the analysis remains superficial without deeper insights or evaluation of the cited works."}}
{"id": "5f2027de-cc00-441d-a1a3-525d45f31760", "title": "Mixed-Domain Pretraining (MDPT)", "level": "subsubsection", "subsections": [], "parent_id": "6ddc8b70-02c3-4ef9-88a6-cbdbf8a0abe7", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "T-BPLMs Core Concepts"], ["subsection", "Pretraining Methods"], ["subsubsection", "Mixed-Domain Pretraining (MDPT)"]], "content": "Mixed domain pretraining involves training the model using both general and in-domain text. Depending on whether the pretraining is done simultaneously or not, mixed domain pretraining can be classified into a) Continual pretraining – initially the model is pre-trained over general domain text and then adapted to the biomedical domain  and b) Simultaneous pretraining – the model is pre-trained over the combined corpora having both general and in-domain text where the in-domain text is up sampled to ensure balanced pretraining .\n\\textbf{Continual Pretraining (CPT)} : It is the standard approach followed by the biomedical NLP research community to develop transformer-based BPLMs. It is also referred to as further pretraining. In this approach, the model is initialized with general PLM weights and then the model is adapted to in-domain by further pretraining on large volumes of in-domain text (refer Figure \\ref{ammu-cpt}). For example, BioBERT is initialized with general BERT weights and then further pretrained on PubMed abstracts and PMC full-text articles . In the case of BlueBERT, the authors used both PubMed abstracts and MIMIC-III clinical notes for continual pretraining . \n\\begin{figure}[t!]\n\\begin{center}\n\\includegraphics[width=7cm, height=1cm]{PTS.png}\n\\caption{\\label{ammu-dspt} Domain-Specific Pretraining (DSPT) } \n\\end{center}\n\\end{figure}\n\\textbf{Simultaneous Pretraining (SPT)} : Continual pretraining achieved good results by adapting general models to the biomedical domain . However, it requires large volumes of in-domain text. Otherwise, CPT may result in suboptimal performance. Simultaneous pretraining comes to the rescue when only a small amount of in-domain text is available. Here, the pretraining corpora consist of both in-domain and general domain text where the in-domain text is up sampled to ensure a balanced pretraining (refer Figure \\ref{ammu-spt}). For example, BERT (jpCR+jpW)  is developed by simultaneous pretraining over a small amount of Japanese clinical text and a large amount of Japanese Wikipedia text. This model outperformed UTH-BERT in clinical text classification. UTH-BERT  is trained from scratch over Japanese clinical text.", "cites": [4038, 4031, 4039, 4029], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic description of mixed-domain pretraining with a distinction between continual and simultaneous pretraining. It integrates a few examples from cited papers but lacks deeper synthesis across them. There is minimal critical evaluation or identification of broader trends, focusing mainly on the methods and results described in the individual studies."}}
{"id": "db4459f3-98dd-477e-b2d8-f2c7c0af0c3f", "title": "Domain-Specific Pretraining (DSPT)", "level": "subsubsection", "subsections": [], "parent_id": "6ddc8b70-02c3-4ef9-88a6-cbdbf8a0abe7", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "T-BPLMs Core Concepts"], ["subsection", "Pretraining Methods"], ["subsubsection", "Domain-Specific Pretraining (DSPT)"]], "content": "The main drawback in continual pretraining is the general domain vocabulary. For example, the WordPiece vocabulary in BERT is learned over English Wikipedia and Books Corpus . As a result, the vocabulary does not represent the biomedical domain and hence many of the biomedical words are split into several subwords which hinders the model learning during pretraining and fine-tuning. Moreover, the length of the input sequence also increases as many of the in-domain words are split into several subwords. DSPT over in-domain text allows the model to have the in-domain vocabulary (refer Figure \\ref{ammu-dspt}). For example, PubMedBERT is trained from scratch using PubMed abstracts and PMC full-text articles . PubMed achieved state-of-the-art results in the BLURB benchmark. Similarly, RoBERTa-base-PM-M3-Voc is trained from scratch over PubMed and PMC and MIMIC-III clinical notes .", "cites": [8719, 7], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of domain-specific pretraining and mentions a couple of examples like PubMedBERT and RoBERTa-base-PM-M3-Voc. It briefly cites two papers but does not integrate or synthesize their ideas in a meaningful way. There is minimal critical analysis or abstraction beyond the specific models discussed."}}
{"id": "d07015f1-b4f8-4b17-b310-89ce0f6597c9", "title": "Task Adaptive Pretraining (TAPT)", "level": "subsubsection", "subsections": [], "parent_id": "6ddc8b70-02c3-4ef9-88a6-cbdbf8a0abe7", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "T-BPLMs Core Concepts"], ["subsection", "Pretraining Methods"], ["subsubsection", "Task Adaptive Pretraining (TAPT)"]], "content": "\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=7cm, height=1.6cm]{TAPT.png}\n\\caption{\\label{ammu-tapt} Task Adaptive Pretraining (TAPT) } \n\\end{center}\n\\end{figure}\nBoth DSPT and MDPT require training the model over large volumes of text to allow the model to learn domain-specific knowledge which helps to perform it to perform better in downstream tasks. Pretraining over in-domain text allows the model to learn universal in-domain representations which are useful to all the in-domain tasks. However, pretraining over large volumes of text is expensive in terms of both computational resources and time. Task Adaptive Pretraining (TAPT) is based on the hypothesis that pretraining over task-related unlabelled text allows the model to learn both domain and task-specific knowledge  (refer Figure \\ref{ammu-tapt}). In TAPT, task-related unlabelled sentences are gathered, and then the model is further pretrained. TAPT is less expensive compared to other pretraining methods as it involves pretraining the model over a relatively small corpus of task-related unlabelled sentences.", "cites": [2978], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section introduces TAPT and briefly connects it to prior pretraining methods (DSPT and MDPT), but it does so in a primarily descriptive manner. It lacks in-depth synthesis of multiple sources, critical evaluation of the cited work, or abstraction of broader principles. The figure is referenced but not discussed in a way that adds analytical depth."}}
{"id": "c3628c21-7fe8-44c6-b73a-c33db6589f32", "title": "Pretraining Tasks", "level": "subsection", "subsections": ["30f8a3c1-5544-4213-a53b-6619617b93c5", "7a75c282-e052-416c-8c5a-d3e56b5ad84e"], "parent_id": "b6f77230-f5a2-4e39-8a06-90a6eee6cf02", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "T-BPLMs Core Concepts"], ["subsection", "Pretraining Tasks"]], "content": "During pretraining, the language models learn language representations based on the supervision provided by one or more pretraining tasks. A pretraining task is a pseudo-supervised task whose labels are generated automatically. A pretraining task can be main or auxiliary. The main pretraining tasks allow the model to learn language representations while auxiliary pretraining tasks allow the model to gain knowledge from human-curated sources like Ontology . The classification of pretraining tasks is given in Figure \\ref{ammu-pretraining-tasks} and a brief summary of various pretraining tasks is presented in Table \\ref{table-pretraining-tasks}.   \n\\begin{figure*}[h]\n\\begin{center}\n\\includegraphics[width=12cm, height=5cm]{ammu-Pretraining-Tasks.png}\n\\caption{\\label{ammu-pretraining-tasks} Pretraining Tasks } \n\\end{center}\n\\end{figure*}\n\\begin{table*}[t!]\n\\begin{center}\n{\\renewcommand{\\arraystretch}{1.5}\n\\begin{tabular}{|p{2cm}|p{2cm}|p{9.5cm}|p{2.5cm}|}\n\\hline\nPretraining Task  & Type         & Key points    & Models    \\\\ \\hline\nMLM                        & word-level     & Model learns by predicting the masked tokens. Less   training signal per instance as the model predicts only 15\\% of the tokens.  & BERT                              \\\\ \\hline\nMLM + Dynamic Masking      & word-level     & Dynamic Masking allows masking different tokens in   the sentences for different epochs due to which the model learns more by   predicting different tokens every time.  & RoBERTa    \\\\ \\hline\nMLM + Whole Word Masking   & word-level     & Whole word masking is more challenging as it is   difficult to predict the entire word compared to a subword.   & PubMedBERT  \\\\ \\hline\nMLM + Whole Entity Masking & word-level     & Whole entity masking allows the model to learn   entity-centric knowledge.    & MC-BERT    \\\\ \\hline\nMLM + Whole Span Masking   & word-level     & Whole span masking allows the model to learn more   linguistic knowledge.     & MC-BERT    \\\\ \\hline\nNSP    & sentence-level & Allows the model to learn sentence-level reasoning   skills which are useful in tasks like NLI. Less challenging as it involves   topic prediction which is a relatively easy task. & BERT     \\\\ \\hline\nSOP    & sentence-level & Allows the model to learn sentence-level reasoning   skills by modeling inter-sentence coherence. More challenging compared to NSP   as SOP involves only sentence coherence.       & ALBERT      \\\\ \\hline\nSBO   & phrase-level   & Model predicts the masked tokens in a span based on   boundary token representations and position embeddings.    & SpanBERT   \\\\ \\hline\nRTD   & word-level     & Model checks every token whether it is replaced or   not. More efficient compared to MLM as it involves all the tokens in the   input.  & ELECTRA    \\\\ \\hline\n\\end{tabular}}\n\\end{center}\n\\caption{\\label{table-pretraining-tasks} Summary of pretraining tasks.} \n\\end{table*}", "cites": [8719, 2473, 1557, 4040, 826, 9141, 4041, 1150, 7], "cite_extract_rate": 0.8181818181818182, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of pretraining tasks used in various transformer-based biomedical PLMs, but lacks deeper synthesis, critical evaluation, or abstraction. It lists tasks and their associated models without analyzing their relative strengths, limitations, or broader implications in the biomedical domain."}}
{"id": "30f8a3c1-5544-4213-a53b-6619617b93c5", "title": "Main Pretraining Tasks", "level": "subsubsection", "subsections": [], "parent_id": "c3628c21-7fe8-44c6-b73a-c33db6589f32", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "T-BPLMs Core Concepts"], ["subsection", "Pretraining Tasks"], ["subsubsection", "Main Pretraining Tasks"]], "content": "The main pretraining tasks allow the model to learn language representations. Some of the commonly used main pretraining tasks are masked language modelling (MLM) , replaced token detection (RTD) , sentence boundary objective (SBO) , next sentence prediction (NSP)   and sentence order prediction (SOP) .\n\\textbf{Masked Language Modeling (MLM)}.  It is an improved version of Language Modeling which utilizes both left and right contexts to predict the missing tokens . The main drawback in Unidirectional LM (Forward LM or Backward LM) is the inability to utilize both left and right contexts at the same time to predict the tokens. However, the meaning of a word depends on both the left and right contexts. Devlin et al.  utilized MLM as a pretraining task for learning the parameters of the BERT model. Formally, for a given sequence $x$ with tokens $\\{x_1, x_2, …, x_m\\}$, a subset of tokens is randomly chosen and these tokens are replaced.  The authors replaced tokens, 80\\% of the time with a special token ‘[MASK]’, 10\\% of the time with a random token, and 10\\% of the time with the same token. This is done to handle the mismatch between pretraining and fine-tuning phases. Formally,\n\\begin{equation}\n    L_{MLM} = - \\frac{1}{|m(x)|}\\sum_{i \\in m(x)} logP(x_i/\\hat{x})\n\\end{equation}\nwhere $\\hat{x}$ is the masked version of $x$ and $m(x)$ represents the set of masked token positions.\nSome of the improvements like dynamic masking , whole word masking , whole entity masking , and whole span masking  are introduced in MLM to further improve its efficiency as a pretraining task. Delvin et al.  used static masking to replace the tokens i.e., the input sentences are masked once during pre-processing and the model predicts the same masked tokens in the input sentences for every epoch during pretraining.  In the case of dynamic masking , different tokens are masked in the input sentence for different epochs which prevents the model from predicting the same masked tokens in every epoch and hence it learns more.  Whole word masking is much more challenging as the model has to predict the entire word rather than part of a word. In the case of the whole entity and span maskings, in-domain entities and phrases in the input sentences are identified and then masked rather than masking the randomly chosen tokens. As a result, the model learns entity-centric and in-domain linguistic knowledge during pretraining which enhances the performance of the model in downstream tasks . For example, Zhang et al.  trained MC-BERT using NSP and MLM with whole entity and span maskings. Michalopoulos et al.  used novel multi-label loss-based MLM along with NSP to further pretrain ClinicalBERT on MIMIC-III clinical notes to get UmlsBERT. Novel multilabel loss-based MLM allows the model to connect all the words under the same concept. Sequence-to-Sequence MLM (Seq2SeqLM) is an extension of MLM to models based on encoder-decoder architecture. Models like T5  are pretrained using  Seq2SeqLM pretraining task.\n\\textbf{Replaced Token Detection (RTD) }.  It is a novel pretraining task that involves verifying whether each token in the input is replaced or not.  Initially, some of the tokens in the input sentences are replaced with words predicted by a small generator network, and then the model (discriminator) is asked to predict the status of each word as replaced or not.  The two advantages of RTD over MLM are a) RTD provides more training signal compared to MLM as RTD involves checking the status of every token in the input rather than a subset of randomly chosen tokens like MLM. and b) Unlike MLM, RTD does not use any special tokens like ‘[MASK]’ to corrupt the input. So, it avoids the mismatch problem that the special token ‘[MASK]’ is seen only during pretraining but not during fine-tuning. Formally, \n\\begin{equation}\n    L_{RTD} = -\\frac{1}{|\\hat{x}|} \\sum_{i=1}^{|\\hat{x}|} logP(t/\\hat{x_i})\n\\end{equation}\nwhere $\\hat{x}$ is the corrupted version of $x$ and $t=1$ when the token is not a replaced one.\n\\textbf{Span Boundary Objective (SBO) }.  It is a novel pretraining task that involves predicting the entire masked span based on the context. Initially, a contiguous span of tokens is randomly chosen and masked and then the model is asked to predict the masked tokens in the span based on the token representations at the boundary. In the case of MLM, the model predicts the masked token based on the final hidden vector of the masked token. However, in the case of SBO, the model predicts the masked token in the span based on the final hidden vectors of the boundary tokens and the position embedding of the masked token. SBO is more challenging as it is difficult to predict the entire span \\textit{“frequent bathroom runs”} than predicting \\textit{“frequent”} when the model already sees \\textit{“bathroom runs”}. SBO helps the model to achieve better results in span extraction-based tasks like entity extraction and question answering . Let $s$ and $e$ represent the start and end indices of the span in the input sequence. Then, each token $x_i$ in the span is predicted based on the final hidden vectors of the boundary tokens $x_{s-1}$, $x_{e+1}$ and its position embedding $p_{i-s+1}$. Then\n\\begin{equation}\n    L_{SBO} = -\\frac{1}{|S|} \\sum_{i\\in S} logP(x_i/y_i)\n\\end{equation}\nwhere $y_i=g(x_{s-1},x_{e+1},p_{i-s+1}) $, $g()$ represents feed-forward network of two layers and $S$  represents the positions of tokens in contiguous span.\n\\textbf{Next Sentence Prediction (NSP) }.  NSP is a sentence-level pretraining task that involves predicting whether given two sentences appear consecutively or not . It is basically a two-way sentence pair classification task. Formally, for a given sentence pair $(x, y)$, the model has to predict one of the two labels $\\{IsNext, IsNotNext\\}$ depending on whether the two sentences are consecutive or not. NSP helps the model to learn sentence-level reasoning skills which are useful in downstream tasks involving sentence pairs like natural language inference, text similarity, and question answering . For a balanced pretraining, the training examples are chosen in a 1:1 ratio i.e., 50\\% are positive and the rest negative.  Let $z$ represents aggregate vector representation of the sentence pair (x, y). Then,\n\\begin{equation}\n    L_{NSP} = -logP(t/z)\n\\end{equation}\nwhere $t=1$ when the two sentences $x$ and $y$ are consecutive.\n\\textbf{Sentence Order Prediction (SOP) }. SOP is a novel sentence-level pretraining task which models inter-sentence coherence. Like NSP, SOP is a two-way sentence pair classification. Formally, for a given sentence pair $(x, y)$, the model has to predict one of the two labels $\\{IsSwapped, IsNotSwapped\\}$ depending on whether the sentences are swapped or not. For a balanced pretraining, the training examples are chosen in a 1:1 ratio i.e., 50\\% are swapped and the rest are not swapped.  Unlike NSP which involves the prediction of both topic and coherence, SOP involves only sentence coherence prediction . Topic prediction is comparatively easier which questions the effectiveness of NSP as a pretraining task . Let $z$ represent aggregate vector representation of the sentence pair $(x, y)$. Then,\n\\begin{equation}\n    L_{SOP} = -logP(t/z)\n\\end{equation}\nwhere $t=1$ when the two sentences $x$ and $y$ are not swapped.", "cites": [4042, 9, 2473, 1557, 8719, 826, 1150, 4041, 7], "cite_extract_rate": 0.75, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple pretraining tasks and connects them across cited works (e.g., MLM from BERT, RTD from ELECTRA, SBO from SpanBERT, SOP as a refinement of NSP). It includes critical analysis by comparing the strengths and weaknesses of these tasks, such as the mismatch problem in MLM or the limitations of NSP. It also abstracts the tasks into broader categories (e.g., token-level vs. sentence-level objectives) and explains their impact on downstream biomedical NLP tasks."}}
{"id": "7a75c282-e052-416c-8c5a-d3e56b5ad84e", "title": "Auxiliary Pretraining Tasks", "level": "subsubsection", "subsections": [], "parent_id": "c3628c21-7fe8-44c6-b73a-c33db6589f32", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "T-BPLMs Core Concepts"], ["subsection", "Pretraining Tasks"], ["subsubsection", "Auxiliary Pretraining Tasks"]], "content": "Auxiliary pretraining tasks help to inject knowledge from human-curated sources like UMLS  into in-domain models to further enhance them. For example, the triple classification pretraining task involves identifying whether two concepts are connected by the relation or not . This auxiliary task is used by Hao et al.  to inject UMLS relation knowledge into in-domain models. Yuan et al.  used two auxiliary pretraining tasks based on multi-similarity Loss and Knowledge embedding loss to further pretrain BioBERT on UMLS. Similarly, Liu et al.  used multi-similarity loss-based pretraining task to inject UMLS synonym knowledge into PubMedBERT.", "cites": [9141, 4040], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of auxiliary pretraining tasks, mentioning their purpose and examples from different papers. However, it lacks synthesis by not drawing connections between the different approaches or explaining how they contribute to a unified understanding. There is no critical evaluation of the methods, nor are there broader abstractions or principles identified."}}
{"id": "204b6534-1fbb-43a2-abbd-7224099f38cf", "title": "Intermediate Fine-Tuning (IFT)", "level": "subsubsection", "subsections": [], "parent_id": "b802501e-db28-4586-903b-9bfe4cc3c247", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "T-BPLMs Core Concepts"], ["subsection", "Fine-Tuning Methods"], ["subsubsection", "Intermediate Fine-Tuning (IFT)"]], "content": "IFT on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets. IFT can be done in following four ways\n\\textbf{Same Task Different Domain} – Here, the source and target datasets are from the same task but different domains. Model can be fine-tuned on general domain datasets before fine-tuning on small in-domain datasets  . For example, Cengiz et al.  fine-tuned in-domain model on general NLI datasets like SNLI  and MNLI  before fine-tuning on MedNLI .\n\\textbf{Same Task Same Domain} – Here, the source and target datasets are from the same task and domain. But the source dataset is a more generic one while the target dataset is more specific . For example, Gao et al.  fine-tuned BlueBERT on a large general biomedical NER corpus like MedMentions  or Semantic Medline before fine-tuning on the small target NER corpus. \n\\textbf{Different Task Same Domain} – Here, the source and target datasets are from different tasks but the same domain. Fine-tuning on source dataset which is from the same domain allows the model to gain more domain-specific knowledge which improves the performance of the model on the same domain target task .  McCreery et al.  fine-tuned the model on the medical question-answer pairs dataset to enhance its performance on the medical question similarity dataset. \n\\textbf{Different Task Different Domain} – Here, the source and target datasets are from different tasks and different domains. For example, Jeong et al.  fine-tuned BioBERT on a general MultiNLI dataset to improve the performance of the model in biomedical QA. Here the model learns sentence level reasoning skills which are useful in biomedical QA.", "cites": [4043, 4046, 1174, 4048, 4047, 4044, 4045, 1173], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key examples from multiple papers to create a structured categorization of IFT approaches. It connects the concept of domain adaptation with practical strategies for improving performance on biomedical tasks. However, while it identifies patterns across approaches, it lacks deeper critical evaluation of their effectiveness or limitations, and the abstraction remains focused on task and domain alignment rather than broader principles."}}
{"id": "c0c97c2d-552f-486e-b776-511508251be9", "title": "Multi-Task Fine-Tuning", "level": "subsubsection", "subsections": [], "parent_id": "b802501e-db28-4586-903b-9bfe4cc3c247", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "T-BPLMs Core Concepts"], ["subsection", "Fine-Tuning Methods"], ["subsubsection", "Multi-Task Fine-Tuning"]], "content": "Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously . Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks. At the same time, due to the increase in training set size, the model is less prone to over-fitting. Multi-task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain . Moreover, having a single model for multi-tasks eliminates the need of deploying separate models for each task which saves computational resources, time, and deployment costs . Multi-task fine-tuning may not provide the best results all the time . In such cases, multi-task fine tuning can be applied iteratively to identify the best possible subset of tasks . For example, Mahanjan et al.  applied multi-task fine-tuning iteratively to choose the best subset of related datasets. Finally, the authors fine-tuned the model on best subset of related datasets and achieved the best results on the Clinical STS  dataset.   After multi-task fine-tuning, the model can be further fine-tuned on the target specific dataset separately to further enhance the performance of the model .", "cites": [4050, 326, 4049, 9120], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes the concept of multi-task fine-tuning by drawing from multiple papers, connecting their findings into a coherent explanation of how it benefits biomedical NLP. It provides a critical discussion by noting that multi-task fine-tuning may not always yield the best results and suggesting iterative application for task subset selection. The abstraction is strong, as it generalizes the use of shared encoder layers and task-specific heads, and highlights broader benefits like reduced overfitting and computational efficiency in low-resource settings."}}
{"id": "3b0e45bf-21e4-46af-9317-c459e5c8a96c", "title": "Main Embeddings", "level": "subsubsection", "subsections": [], "parent_id": "c507bff4-e027-405f-af8a-23cf17a17804", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "T-BPLMs Core Concepts"], ["subsection", "Embeddings"], ["subsubsection", "Main Embeddings"]], "content": "Text embeddings map the given sequence of words into a sequence of vectors. Text embeddings can be char, subword or code-based. \n\\textbf{Character Embeddings} -  In character embeddings, the vocabulary consists of letters, punctuation symbols, special characters and numbers only. Each character is represented using an embedding. These embeddings are initialized randomly and learned during model pretraining. ELMo embedding model uses CharCNN to generate word representations from character embeddings . Inspired by ELMo, BioCharBERT also uses CharCNN on the top of character embeddings to generate word representations . AlphaBERT  also uses character embeddings. Unlike CharacterBERT, AlphaBERT directly combines character embeddings with position embeddings and then applies a stack of transformer encoders. The main advantage with character embeddings is the small size of vocabulary as it includes only characters. The disadvantage is longer pretraining times . As the sequence length increases with character level embeddings, models are slow to pre-train. \n\\textbf{Subword Embeddings}- In subword embeddings, the vocabulary consists of characters and the most frequent subwords and words. The main principle driving the subword embedding vocabulary construction is that frequent words should be represented as a single word and rare words should be represented in terms of meaningful subwords. Subword embedding vocabularies are always moderate in size as they use sub-words to represent rare and misspelled words. Some of the popular algorithms to generate vocabulary for sub-word embeddings are Byte-Pair Encoding (BPE) , Byte-Level BPE , Word-Piece , Unigram , and Sentencepiece . \n\\textit{Byte-Pair Encoding (BPE) } - It starts with a base vocabulary having all the unique characters in the training corpus. It augments the base vocabulary with the most frequent pairs until the desired vocabulary size is achieved. Byte-Pair Encoding algorithm can be summarized as\n\\begin{enumerate}\n    \\item Prepare a large training corpus and fix the vocabulary size.\n    \\item Generate a base vocabulary having all the unique characters in the training corpus.\n    \\item Calculate the frequency of all the words in the corpus.\n    \\item Augment the vocabulary with the most frequently occurring pair.\n    \\item Until the desired vocabulary size is achieved, repeat step 4.  \n\\end{enumerate}\n\\textit{Byte-Level BPE } - In Byte-Level BPE, each character is represented as a byte, and the rest of the procedure is the same as in BPE. Text is converted into a sequence of bytes and the most frequent byte pair is added into the base vocabulary until the desired size is achieved. Byte-Level BPE is extremely beneficial in the multilingual scenario. GPT-2  and RoBERTa  use Byte-Level BPE embeddings.\n \\textit{WordPiece } - The working of WordPiece is almost the same as BPE. Word-Piece and BPE differ in the strategy used in selecting the symbol pair to augment the base vocabulary. BPE chooses the most frequent symbol pair while Word-Piece uses a language model to choose the symbol pair. BERT , DistilBERT , and ELECTRA model use WordPiece embeddings.\n \\textit{SentencePiece } -  A common problem in BPE and WordPiece is that they assume that words in the input sentences are separated by space. However, this assumption is not applicable in all languages. To overcome this, SentencePiece treats space as a character and includes it in the base vocabulary. The final vocabulary is generated iteratively using BPE or Unigram.  XLNet , ALBERT , and T5  models use SentencePiece embeddings.\n \\textit{Unigram } - Unigram is similar to BPE and WordPiece in fixing the vocabulary size at the beginning itself. BPE and Word-piece start with a small base vocabulary and then augments the base vocabulary for a certain number of iterations. Unlike BPE and Word-Piece, Unigram starts with a large base vocabulary and then iteratively trims the symbols to arrive at a small final vocabulary. It is not used directly in any of the models. SentencePiece uses the Unigram algorithm to generate the final vocabulary. \n \\begin{figure*}[h]\n\\begin{center}\n\\includegraphics[width=18cm, height=12cm]{ammu-taxonomy.png}\n\\caption{\\label{ammu-taxonomy} T-BPLMs taxonomy } \n\\end{center}\n\\end{figure*}\n \\textbf{Code embeddings} - Code embeddings map the given sequence of codes into a sequence of vectors. For example, in the case of models like BERT-EHR , MedBERT , and BEHRT , the input is not a sequence of words. Instead, input is patient visits. Each patient visit is represented as a sequence of codes. The number of code embeddings varies from model to model. For example, MedBERT and BEHRT include embeddings only for disease codes while BERT-EHR includes embeddings for disease, medication, procedure, and clinical notes.", "cites": [207, 4051, 9, 11, 826, 8385, 856, 1150, 8559, 7], "cite_extract_rate": 0.5882352941176471, "origin_cites_number": 17, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual summary of different embedding types used in biomedical PLMs, with some integration of cited papers (e.g., ELMo, BERT, RoBERTa, SentencePiece). However, it lacks deep synthesis by not explicitly linking how these embedding strategies influence model performance or suitability for biomedical tasks. Critical analysis is limited, with only surface-level mentions of advantages and disadvantages. There is some abstraction in grouping embedding methods (character, subword, code), but broader patterns or principles are not explored in depth."}}
{"id": "bda2fea8-90ba-46cf-8ff8-44b9f819f725", "title": "Auxiliary Embeddings", "level": "subsubsection", "subsections": [], "parent_id": "c507bff4-e027-405f-af8a-23cf17a17804", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "T-BPLMs Core Concepts"], ["subsection", "Embeddings"], ["subsubsection", "Auxiliary Embeddings"]], "content": "Main embeddings represent the given input sequence in low dimensional space. The purpose of auxiliary embeddings is to provide additional information to the model so that the model can learn better. For each input token, a representation vector is obtained by summing the main and two or more auxiliary embeddings. The various auxiliary embeddings are \n\\textbf{Position Embeddings} - Position embeddings enhance the final input representation of a token by providing its position information in the input sequence. As there is no convolution or recurrence layers which can learn the order of input tokens automatically, we need to explicitly provide the location of each token in the input sequence through position embeddings. Position embeddings can be pre-determined  or learned  during model pretraining .  \n\\textbf{Segment Embeddings} - Segment embeddings help to distinguish tokens of different input sequences. Segment embedding is the same for all the tokens in the same input sequence. \n\\textbf{Age Embeddings} - In models like BEHRT  and BERT-EHR , age embeddings are used in addition to other embeddings. Age embeddings provide the age of the patient and help the model to leverage temporal information. Age embedding is the same for all the codes in a single patient visit. \n\\textbf{Gender Embeddings} - In models like BEHRT  and BERT-EHR , gender embeddings are used in addition to other embeddings. Gender embeddings provide the gender information of the patient to the model. Gender embedding is the same for all the codes in all the patient visits.\n\\textbf{Semantic Group Embeddings} - Semantic group embeddings are used in UmlsBERT  to explicitly inform the model to learn similar representations for words from the same semantic group i.e., semantic group embedding is same for all the words which fall into the same semantic group. Besides, it also helps to provide better representations for rare words. \n\\begin{table*}[t!]\n\\begin{center}\n{\\renewcommand{\\arraystretch}{1.5}\n\\begin{tabular}{|p{2.5cm}|p{2cm}|p{1.5cm}|p{3.5cm}|p{1.5cm}|p{4cm}|}\n\\hline\n    \\textbf{Model}                              & \\textbf{Type} & \\textbf{Pretrained from}    & \\textbf{Corpus}                        & \\begin{tabular}[c]{@{}l@{}}\\textbf{Publicly}\\\\    \\\\ \\textbf{Available}\\end{tabular} & \\textbf{Evaluation}                                            \\\\ \\hline\nClinicalBERT              & EHR  & BioBERT            & MIMIC-III Clinical Notes      & Yes               & MedNLI and  Clinical Concept Extraction               \\\\ \\hline\nClinicalBERT (discharge)   & EHR  & BioBERT            & MIMIC-III Discharge summaries & Yes              & MedNLI and  Clinical Concept Extraction               \\\\ \\hline\nMIMIC-BERT                 & EHR  & General BERT       & MIMIC-III Clinical Notes      & Yes                & Clinical Concept Extraction                           \\\\ \\hline\nClinicalXLNet (nursing)    & EHR  & General XLNet      & MIMIC-III Nursing notes       & Yes              & Prolonged Mechanical Ventilation   Prediction problem \\\\ \\hline\nClinicalXLNet (discharge)   & EHR  & General XLNet      & MIMIC-III Discharge notes     & Yes               & Prolonged Mechanical Ventilation   Prediction problem \\\\ \\hline\nBERT-MIMIC                 & EHR  & General BERT       & MIMIC-III Clinical Notes      & Yes              & Clinical Concept Extraction                           \\\\ \\hline\nELECTRA-MIMIC               & EHR  & General ELECTRA    & MIMIC-III Clinical Notes      & Yes               & Clinical Concept Extraction                           \\\\ \\hline\nXLNet-MIMIC               & EHR  & General XLNet      & MIMIC-III Clinical Notes      & Yes             & Clinical Concept Extraction                           \\\\ \\hline\nRoBERTa-MIMIC              & EHR  & General RoBERTa    & MIMIC-III Clinical Notes      & Yes             & Clinical Concept Extraction                           \\\\ \\hline\nALBERT-MIMIC                & EHR  & General ALBERTA    & MIMIC-III Clinical Notes      & Yes             & Clinical Concept Extraction                           \\\\ \\hline\nDeBERTa-MIMIC               & EHR  & General DeBERTa    & MIMIC-III Clinical Notes      & Yes              & Clinical Concept Extraction                           \\\\ \\hline\nLongformer-MIMIC            & EHR  & General Longformer & MIMIC-III Clinical Notes      & Yes               & Clinical Concept Extraction                           \\\\ \\hline\nMedBERT                    & EHR  & Scratch            & Private EHR                   & No                & Disease Prediction                                    \\\\ \\hline\nBEHRT                      & EHR  & Scratch            & Private EHR                   & No               & Disease Prediction                                    \\\\ \\hline\nBERT-EHR                   & EHR  & Scratch            & Private EHR                   & No                & Disease Prediction                                    \\\\ \\hline\nAlphaBERT                 & EHR  & Scratch            & Private EHR                   & No                & Text Summarization      \\\\ \\hline                               \n\\end{tabular}}\n\\end{center}\n\\caption{\\label{table-ehr-based} Summary of EHR-based T-BPLMs.} \n\\end{table*}", "cites": [4038, 4031, 4032, 7], "cite_extract_rate": 0.4, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of auxiliary embeddings used in biomedical PLMs, citing relevant papers but without substantial synthesis or critical analysis. While it mentions specific models and their use of age, gender, or semantic group embeddings, it does not compare or contrast these approaches in depth. The abstraction level is limited, focusing mainly on concrete examples rather than broader patterns or principles."}}
{"id": "a79c464f-8904-4eb6-8e32-5111b571b485", "title": "Electronic Health Records", "level": "subsubsection", "subsections": [], "parent_id": "76f76d7f-fc64-443c-b8dc-cb43b506ab4c", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "T-BPLMs TAXONOMY"], ["subsection", " Pretraining Corpus"], ["subsubsection", "Electronic Health Records"]], "content": "In the last decade, most hospitals have been using Electronic Health Records (EHRs) to record patient as well as treatment details right from admission to discharge . EHRs contain a vast amount of medical data which can be used to provide better patient care by knowledge discovery and the development of better algorithms. As EHR contains sensitive information related to patients, medical data must be de-identified before sharing. EHRs include both structured and unstructured data . Structured data includes laboratory test results, various medical codes, etc. Unstructured data  include clinical notes like medication instructions, progress notes, discharge summaries, etc. Clinical notes include the most valuable patient information which is difficult and expensive to extract manually. So, there is a need for automatic information extraction methods to utilize the abundant medical data from EHRs in research as well as applications . Following the success of transformer-based PLMs in the general domain, researchers in biomedical NLP also developed EHR-based T-BPLMs by pretraining over clinical notes or medical codes or both. MIMIC  is the largest publicly available dataset of medical records. MIT Lab researchers gathered medical records from Beth Israel Deaconess Medical Center, de-identified the sensitive patient information, and then released four versions of the MIMIC dataset.\n\\begin{table*}[t!]\n\\begin{center}\n{\\renewcommand{\\arraystretch}{1.5}\n\\begin{tabular}{|p{2.5cm}|p{2cm}|p{1.5cm}|p{3.5cm}|p{1.5cm}|p{4cm}|}\n\\hline\n    \\textbf{Model}             & \\textbf{Type}      & \\textbf{Pretrained from}     & \\textbf{Corpus}                    & \\textbf{Publicly Available} & \\textbf{Evaluation}                       \\\\ \\hline\nRadBERT   & Radiology & General BERT        & RadCore           & No                 & Radiology Reports Classification \\\\ \\hline\nFS-BERT  & Radiology & Scratch             & Private Radiology Reports & No                 & Radiology Reports Classification \\\\ \\hline\nRAD-BERT  & Radiology & German general BERT & Private Radiology Reports & No                 & Radiology Reports Classification \\\\ \\hline\n\\end{tabular}}\n\\end{center}\n\\caption{\\label{table-radiology-reports} Summary of radiology reports-based T-BPLMs.} \n\\end{table*}\n\\begin{table*}[h]\n\\begin{center}\n{\\renewcommand{\\arraystretch}{1.5}\n\\begin{tabular}{|p{2.5cm}|p{2cm}|p{1.5cm}|p{3.5cm}|p{1.5cm}|p{4cm}|}\n\\hline\n\\textbf{Model} & \\textbf{Type}         & \\textbf{Pretrained from}   & \\textbf{Corpus}                             & \\textbf{Publicly Available} & \\textbf{Evaluation}                                               \\\\ \\hline\nCT-BERT           & Social Media & General BERT      & Covid Tweets                       & Yes                & Text Classification                                      \\\\ \\hline\nBERTweetCovid19  & Social Media & BERTweet          & Covid Tweets                       & Yes                & Text Classification                                      \\\\ \\hline\nBioRedditBERT    & Social Media & BioBERT           & Health related Reddit posts        & Yes                & Unsupervised Medical Concept   Normalization             \\\\ \\hline\nRuDR-BERT         & Social Media & Multilingual BERT & Russian health reviews             & Yes                & Sentence classification and   Clinical Entity Extraction \\\\ \\hline\nEnRuDR-BERT      & Social Media & Multilingual BERT & Russian and English health reviews & Yes                & ADR (Adverse Drug Reaction) Tweets Classification                                \\\\ \\hline\nEnDR-BERT     & Social Media & Multilingual BERT & English health reviews             & Yes                & ADR Tweets Classification and ADR   Normalization       \\\\ \\hline\n\\end{tabular}}\n\\end{center}\n\\caption{\\label{table-social-media}  Summary of social media-based T-BPLMs.} \n\\end{table*}\nAlsentzer et al.  further pretrained BioBERT (BioBERT-Base v1.0 + PubMed 200K + PMC 270K) on MIMIC III clinical notes to get ClinicalBERT. It took around 17- 18 days to pretrain the model using one GTX TITAN X GPU. It is the first publicly available model pretrained on clinical notes. Si et al.  further pre-trained general BERT (base and large) on MIMIC-III clinical notes to get MIMIC-BERT. The authors evaluated the models on various clinical entity extraction datasets. Huang et al.  further pre-trained XLNet-base on MIMIC-III Clinical Notes.  The authors released two models namely a) pretrained model on nursing notes and b) pretrained model on discharge summaries. Yang et al.  further pre-trained general models like BERT, ELECTRA, RoBERTa, XLNet, and ALBERT on MIMIC-III and released in-domain PLMs. It is the first work to release in-domain models based on all the popular transformer-based PLMs. Unlike the above pre-trained models which are pretrained on clinical text, recent works  released models which are pre-trained on disease codes or multi-modal EHR data. BEHRT  is trained from scratch using 1.6 million patient EHR data with MLM as pretraining task. The authors used code, position, age, and segment embeddings. Med-BERT  is trained from scratch using 28,490,650 patient EHR data with MLM and LOS (Length of Stay) as pretraining tasks. The authors used code, serialization and visit embeddings. BERT-EHR  is trained from scratch using multi-modal data from 43967 patient records with MLM as a pretraining task. Table \\ref{table-ehr-based} contains summary of various EHR based BPLMs.\n\\begin{table*}[t!]\n\\begin{center}\n{\\renewcommand{\\arraystretch}{1.5}\n\\begin{tabular}{|p{2cm}|p{1.5cm}|p{1.5cm}|p{4cm}|p{1.5cm}|p{4.5cm}|}\n\\hline\n\\textbf{Model}    & \\textbf{Type}  & \\textbf{Pretrained from}   & \\textbf{Corpus}   & \\textbf{Publicly Available} & \\textbf{Evaluation}      \\\\ \\hline\nBioBERT            & Scientific Literature & General BERT      & PubMed and PMC  & Yes               & Biomedical NER, RE, and QA.   \\\\ \\hline\nRoBERTa-base-PM      & Scientific Literature & General RoBERTa   & PubMed and PMC              & Yes     & Sequence Labelling and Text   Classification \\\\ \\hline\nRoBERTa-base-PM-Voc  & Scientific Literature & Scratch           & PubMed and PMC              & Yes    & Sequence Labelling and Text   Classification \\\\ \\hline\nBioALBERT            & Scientific Literature & General ALBERT    & PubMed and PMC              & Yes     & Biomedical Concept Extraction     \\\\ \\hline\nBioBERTpt-bio       & Scientific Literature & Multilingual BERT & Brazilian Biomedical corpus & No    & Clinical Concept Extraction                  \\\\ \\hline\nPubMedBERT           & Scientific Literature & Scratch   & PubMed and PMC     & Yes    & BLURB     \\\\ \\hline\nBioELECTRA         & Scientific Literature & Scratch           & PubMed   & Yes                & Biomedical NER, QA and RE.         \\\\ \\hline\nBioELECTRA ++         & Scientific Literature & BioELECTRA        & PMC                         & Yes     & Biomedical NER, QA and RE.     \\\\ \\hline\nBioMegatron          & Scientific Literature & Scratch           & PubMed and PMC              & No     & Biomedical NER, RE and QA.    \\\\ \\hline\nOuBioBERT            & Scientific Literature & Scratch      & PubMed      & Yes    & BLUE    \\\\ \\hline\nBlueBERT-PM     & Scientific Literature & General BERT      & PubMed    & Yes    & BLUE     \\\\ \\hline\nBioMedBERT           & Scientific Literature & General BERT      & BREATHE 1.0                 & No      & Biomedical NER, IR and QA.     \\\\ \\hline\nELECTRAMed           & Scientific Literature & Scratch           & PubMed                      & Yes     & Biomedical NER, RE and QA                   \\\\ \\hline\nBioELECTRA-P   & Scientific Literature & Scratch           & PubMed                      & Yes         & BLURB, BLUE       \\\\ \\hline\nBioELECTRA-PM        & Scientific Literature & Scratch           & PubMed, PMC                 & Yes    & BLURB, BLUE    \\\\ \\hline\nBioALBERT-P    & Scientific Literature & ALBERT            & PubMed     & Yes      & BLURB    \\\\ \\hline\nBioALBERT-PM   & Scientific Literature & ALBERT            & PubMed, PMC   & Yes   & BLURB   \\\\ \\hline\n\\end{tabular}}\n\\end{center}\n\\caption{\\label{table-scientific-literature}  Summary of scientific literature-based BPLMs. NER - Named Entity Recognition, RE - Relation Extraction, IR - Information Retrieval, QA - Question Answering.} \n\\end{table*}", "cites": [4053, 4029, 4055, 8720, 8721, 4052, 4056, 8719, 4031, 4038, 4032, 4054], "cite_extract_rate": 0.35294117647058826, "origin_cites_number": 34, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of EHR-based transformer models, listing several models with details about their training data and evaluation tasks. While it mentions some variations in pretraining approaches, it lacks deeper synthesis of ideas or critical evaluation of the models' strengths and weaknesses. There is minimal abstraction or identification of broader trends in the use of EHR data for BPLM pretraining."}}
{"id": "9608f93a-58bb-49f4-b7a6-e767ea31caf9", "title": "Radiology Reports", "level": "subsubsection", "subsections": [], "parent_id": "76f76d7f-fc64-443c-b8dc-cb43b506ab4c", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "T-BPLMs TAXONOMY"], ["subsection", " Pretraining Corpus"], ["subsubsection", "Radiology Reports"]], "content": "Following the success of EHR-based T-BPLMs, recently researchers focused on developing PLMs specifically for radiology reports. RadCore  dataset consists of around 2 million radiology reports. These reports were gathered from three major healthcare organizations: Mayo Clinic, MD Anderson Cancer Center, and Medical College of Wisconsin in 2007. Meng et al.  further pre-trained general BERT on radiology reports with impression section headings from RadCore dataset to get RadBERT. The authors used RadBERT to classify radiology reports. Bressem et al.  released two T-BPLMs for radiology reports namely FS-BERT and RAD-BERT. FS-BERT is obtained by training from scratch using around 3.8 M radiology reports having 415M words (3.6GB) and custom WordPiece vocabulary. RAD-BERT is obtained by further pretraining German general BERT with custom WordPiece vocabulary over 3.8 M radiology reports having 415M words (3.6GB). Table \\ref{table-radiology-reports} contains a summary of  radiology reports-based T-BPLMs.\n\\begin{table*}[t!]\n\\begin{center}\n{\\renewcommand{\\arraystretch}{1.5}\n\\begin{tabular}{|p{2cm}|p{1.5cm}|p{1.5cm}|p{4cm}|p{1.5cm}|p{4.5cm}|}\n\\hline\n\\textbf{Model}   & \\textbf{Type}   & \\textbf{Pretrained from}       & \\textbf{Corpora}   & \\textbf{Publicly Available} & {Evaluation}   \\\\ \\hline                                \nBlueBERT-P-M3      & Hybrid & General BERT       & PubMed + MIMIC-III    & Yes   & BLUE    \\\\ \\hline                                     \nRoBERTa-base-P-M3     & Hybrid & General RoBERTa     & PubMed + MIMIC-III  & Yes                                                                & Sequence Labelling and Text   Classification \\\\ \\hline\nRoBERTa-base-P-M3-Voc  & Hybrid & Scratch   & PubMed + MIMIC-III   & Yes                                                                & Sequence Labelling and Text   Classification \\\\ \\hline\nBioBERTpt-All   & Hybrid & Multilingual BERT     & Brazilian Clinical Text +   Biomedical Text   & Yes                                      & Clinical Concept Extraction                  \\\\ \\hline\nBioCharBERT   & Hybrid & General CharacterBERT & PMC Abstracts + MIMIC-III  & Yes                                                                & Clinical NER, MedNLI, RE and STS.        \\\\ \\hline\nBERT  (sP +W+BC)     & Hybrid & Scratch   & PubMed + English Wikipedia +   BooksCorpus    & Yes                                                           & BLUE        \\\\ \\hline\nBERT (jpCR+jpW)  & Hybrid & Scratch     & Japanese Clinical Text + Japanese   Wikipedia Text & No                                          & Text Classification        \\\\ \\hline\nAraBioBERT   & Hybrid & AraBERT  & General Arabic Text, Arabic Biomedical   Text      & No   & NER                                         \\\\ \\hline\nSciBERT    & Hybrid & Scratch    & Biomedical + Computer Science   Literature Text    & Yes                & NER and RE                                 \\\\ \\hline\nSciFive-P   & Hybrid & T5  & C4, PubMed      & Yes      & NER, RE, MedNLI, QA              \\\\ \\hline\nSciFive-PM      & Hybrid & T5      & C4, PubMed, PMC       & Yes       & NER, RE, MedNLI, QA                             \\\\ \\hline\nBioALBERT-P-M3  & Hybrid & ALBERT  & PubMed, MIMIC-III    & Yes  & BLURB   \\\\ \\hline                                      \nBioALBERT-PM-M3   & Hybrid & ALBERT   & PubMed, PMC, MIMIC-III   & Yes  & BLURB    \\\\ \\hline                                     \n\\end{tabular}}\n\\end{center}\n\\caption{\\label{table-hybrid-corpora}  Summary of hybrid corpora-based T-BPLMs.} \n\\end{table*}", "cites": [4053, 4057, 2483, 4039], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a factual description of radiology report-based T-BPLMs and includes a table summarizing various models, their sources, corpora, and evaluation tasks. However, it lacks synthesis of ideas across the cited works, does not engage in meaningful comparison or critique, and offers no abstraction or meta-level insights about trends or principles in the use of radiology reports for pretraining. The narrative remains primarily descriptive without deeper analysis."}}
{"id": "abe8b63d-463c-4a6d-9b54-cb91b1ed02b0", "title": "Social Media", "level": "subsubsection", "subsections": [], "parent_id": "76f76d7f-fc64-443c-b8dc-cb43b506ab4c", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "T-BPLMs TAXONOMY"], ["subsection", " Pretraining Corpus"], ["subsubsection", "Social Media"]], "content": "In the last decade, social media has become the first choice for internet users to express their thoughts. Apart from views about general topics, various social media platforms like Twitter, Reddit, AskAPatient, WebMD are used to share health-related experiences in the form of tweets, reviews, questions, and answers . Recent works have shown that health-related social media data is useful in many applications to provide better health-related services . The performance of general T-PLMs on Wikipedia and Books Corpus is limited on health-related social media datasets . This is because social media text is highly informal with a lot of nonstandard abbreviations, irregular grammar, and typos. Researchers working at the intersection of social media and health, trained social media text-based T-BPLMs to handle social media texts. CT-BERT  is initialized from BERT-large and further pretrained on a corpus of 22.5M covid related tweets and this model showed up to 30\\% improvement compared to BERT-large, on five different classification datasets.  BioRedditBERT  is initialized from BioBERT and further pretrained on health-related Reddit posts. The authors showed that BioRedditBERT outperforms in-domains models like BioBERT, BlueBERT, PubMedBERT, ClinicalBERT by up to 1.8\\% in normalizing health-related entity mentions. RuDR-BERT  is initialized from Multilingual BERT and pretrained on the raw part of the RuDReC corpus (1.4M reviews). The authors showed that RuDR-BERT outperforms multilingual BERT and Russian BERT on Russian sentence classification and clinical entity extraction datasets by large margins. EnRuDR-BERT  and EnDR-BERT  are obtained by further pretraining multilingual BERT on Russian and English health reviews and English health reviews respectively. Table \\ref{table-social-media} contains summary of social media text-based BPLMs.", "cites": [4059, 4058, 4056, 8721], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces the use of social media in health-related NLP and describes several T-BPLMs (e.g., CT-BERT, BioRedditBERT) and their pretraining data and performance. However, it lacks deeper synthesis across the cited works, critical evaluation of methods or limitations, and abstraction to broader principles or trends in the domain."}}
{"id": "0004762f-3ef5-4b5a-bc3a-1340a1025842", "title": "Scientific Literature", "level": "subsubsection", "subsections": [], "parent_id": "76f76d7f-fc64-443c-b8dc-cb43b506ab4c", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "T-BPLMs TAXONOMY"], ["subsection", " Pretraining Corpus"], ["subsubsection", "Scientific Literature"]], "content": "In the last few decades, the amount of biomedical literature is growing at a rapid scale. As knowledge discovery from biomedical literature is useful in many applications, biomedical text mining is gaining popularity in the research community . However, biomedical text significantly differs from the general text with a lot of domain-specific words. As a result, the performance of general T-PLMs is limited in many of the tasks. So, biomedical researchers focused on developing in-domain T-PLMs to handle biomedical text. PubMed and PMC are the two popular sources of biomedical text. PubMed contains only biomedical literature citations and abstracts only while PMC contains full-text biomedical articles. As of March 2020, PubMed includes 30M citations and abstracts while PMC contains 7.5M full-text articles. Due to the large collection and broad coverage, these two are the first choice to pretrain T-BPLMs . \nAs DSPT is expensive, most of the works developed in-domain T-PLMs by initializing from general BERT models and then further pretraining on biomedical text. BioBERT  is the first biomedical pre-trained language model which is obtained by further pretraining general BERT on biomedical literature. BioBERTpt-bio   is obtained by further pretraining BERT Multilingual (base) on Brazilian biomedical corpus - scientific papers from PubMed (0.8M- only literature titles) + Scielo (health – 12.4M + biological- 3.2M: both titles and abstracts). BioMedBERT  is obtained by further pretraining BERT-large on BREATHE 1.0 corpus. BioMedBERT outperformed BioBERT on biomedical question answering. The key reason for the better performance of BioMedBERT is the diversity of biomedical text in the BREATHE corpus. The main drawback in developing biomedical models by further pretraining general models is the general vocabulary. To overcome this, researchers started to develop biomedical models by using DSPT. Microsoft researchers developed PubMed  model by DSPT with in-domain vocabulary and whole word masking strategy. OuBioBERT  is trained from scratch using focused PubMed abstracts (280M words) as core corpora and PubMed abstracts (2800M words) as satellite corpora. It outperforms BioBERT and BlueBERT in many of the tasks in the BLUE benchmark. Table \\ref{table-scientific-literature} contains summary of scientific literature-based T-BPLMs.", "cites": [8719, 4029, 4055], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of how scientific literature is used for pretraining biomedical PLMs and mentions specific models like BioBERT and BioMedBERT. While it connects some ideas (e.g., DSPT vs. further pretraining on general BERT), the synthesis is limited to listing models and their training data. There is minimal critical evaluation or abstraction into broader trends or principles."}}
{"id": "d8f6bdd8-5a5f-4e21-aeeb-57346d185136", "title": "Hybrid Corpora", "level": "subsubsection", "subsections": [], "parent_id": "76f76d7f-fc64-443c-b8dc-cb43b506ab4c", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "T-BPLMs TAXONOMY"], ["subsection", " Pretraining Corpus"], ["subsubsection", "Hybrid Corpora"]], "content": "It is difficult to obtain a large amount of in-domain text in some cases. For example, MIMIC  is the largest publicly available dataset of medical records. The MIMIC dataset is small compared to the general Wikipedia corpus or biomedical scientific literature (from PubMed + PMC). However, to pretrain a transformer-based PLM from scratch, we require large volumes of text. To overcome this, some of the models are pretrained on general + in-domain text  or, in-domain + related domain text . For example, BERT (jpCR+jpW)  - Japanese medical BERT is pretrained from scratch using Japanese Digital Clinical references (jpCR) and Japanese general Wikipedia (jpW). This model outperformed UTH-BERT  on Japanese clinical document classification. BERT  (sP +W+BC) - trained from scratch using small PubMed abstracts as core corpora and English Wikipedia (W) + BooksCorpus(BC) as satellite corpora. This model achieved performance comparable to OuBioBERT  in the BLUE benchmark. AraBioBERT  is obtained by further pretraining AraBERT  on general Arabic corpus + biomedical Arabic text. The author showed pretraining a monolingual BERT model like AraBERT on a small-scale domain-specific corpus can still improve the performance of the model. Table \\ref{table-hybrid-corpora} contains hybrid corpora-based T-BPLMs.", "cites": [4039], "cite_extract_rate": 0.1111111111111111, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of hybrid corpora in T-BPLMs, mentioning a few models and their pretraining strategies. While it attempts to connect the need for large corpora with the use of hybrid data, it lacks deeper synthesis across multiple works and does not offer a comparative or evaluative perspective. There is minimal abstraction beyond the specific examples given."}}
{"id": "25a3c6f5-71dd-45cc-afca-e25b33023e76", "title": "Language-Specific", "level": "subsubsection", "subsections": [], "parent_id": "53f83f54-a99c-4dc8-be35-f2014de10a77", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "T-BPLMs TAXONOMY"], ["subsection", "Extensions"], ["subsubsection", "Language-Specific"]], "content": "Following the success of BioBERT, ClinicalBERT, PubMedBERT in English biomedical tasks, researchers focused on developing T-BPLMs for other languages also by pretraining from scratch  or pretraining from Multilingual BERT  or pretraining from monolingual BERT   models. For example, CHMBERT  is the first Chinese medical BERT model which is initialized from the general Chinese BERT model and further pretrained on a huge (185GB) corpus of Chinese medical text gathered from more than 100 hospitals. MC-BERT  is also initialized from general Chinese BERT and further pre-trained on hybrid corpora which includes general, biomedical, and medical texts. Table \\ref{table-language-specific} contains a summary of language-specific T-BPLMs.\n\\begin{table*}[t!]\n\\begin{center}\n{\\renewcommand{\\arraystretch}{1.5}\n\\begin{tabular}{|p{2cm}|p{1.5cm}|p{1.5cm}|p{4cm}|p{1.5cm}|p{4.5cm}|}\n\\hline\n\\textbf{Model}    & \\textbf{Language}   & \\textbf{Pretrained from}      & \\textbf{Corpora}  &  \\textbf{Publicly Available} & \\textbf{Evaluation}   \\\\ \\hline                                                                                \nBERT (jpCR+jpW)  & Japanese   & Scratch  & Japanese Clinical Text + Japanese   Wikipedia           & No                                                                 & Text Classification      \\\\ \\hline                                                                    \nBioBERTpt-bio   & Portuguese & Multilingual BERT    & Brazilian Biomedical Text    & Yes                                                                & Clinical Concept Extraction      \\\\ \\hline                                                            \nBioBERTpt-clin  & Portuguese & Multilingual BERT    & Brazilian Clinical Text                                 & Yes                & Clinical Concept Extraction      \\\\ \\hline                                                            \nBioBERTpt-all  & Portuguese & Multilingual BERT    & Brazilian Clinical Text +   Biomedical Text             & Yes         & Clinical Concept Extraction     \\\\ \\hline                                                              \nRuDR-BERT        & Russian    & Multilingual BERT    & Russian Health Reviews    & Yes                           & Text classification and Clinical   NER      \\\\ \\hline\nEnRuDR-BERT      & Russian    & Multilingual BERT    & Russian and English Health Reviews & Yes                                                                & ADR Tweets Classification     \\\\ \\hline                                                               \nFS-BERT          & German     & Scratch              & Private Radiology Reports                               & No  & Radiology Reports Classification           \\\\ \\hline \nRAD-BERT         & German     & General German BERT  & Private Radiology Reports & No                                                                 & Radiology Reports Classification   \\\\ \\hline                                                           \nCHMBERT       & Chinese    & General Chinese BERT & Private EHRs   & No                                                                 & Disease Prediction and Department Recommendation     \\\\ \\hline                                        \nSpanishBERT    & Spanish    & Scratch              & Spanish Biomedical Text   & No                                                                 & Biomedical NER     \\\\ \\hline                                                                 \nAraBioBERT      & Arabic     & AraBERT              & General Arabic Text+ Arabic Biomedical Text             & No           & Biomedical NER      \\\\ \\hline   \nCamemBioBERT    & French     & CamemBERT   & French Biomedical Corpus & No                                                                 & Biomedical NER      \\\\ \\hline                                                                          \nMC-BERT        & Chinese    & General Chinese BERT & Chinese Biomedical Text, Encyclopedia , Medical records & Yes & ChineseBLUE        \\\\ \\hline      \nUTH-BERT        & Japanese   & Scratch  & Japanese Clinical Text                & Yes                                                                & Text Classification     \\\\ \\hline                                                                    \nSINA-BERT       & Persian    & ParsBERT    & Persian Medical Corpus  & No                      & Medical Question Classification, Medical Question   Retrieval and Medical Sentiment Analysis \\\\ \\hline \nmBERT-Galen     & Spanish    & Multilingual BERT    & Spanish Clinical Text corpus  & Yes  & Medical Coding                                                                              \\\\ \\hline\nBETO-Galen      & Spanish    & BETO        & Spanish Clinical Text corpus  & Yes              & Medical Coding                                                                    \\\\ \\hline\nXLM-R-Galen     & Spanish    & XLM-R       & Spanish Clinical Text corpus  & Yes                                                                & Medical Coding          \\\\ \\hline                                                                    \n\\end{tabular}}\n\\end{center}\n\\caption{\\label{table-language-specific}  Summary of language-specific T-BPLMs.} \n\\end{table*}", "cites": [4061, 2484, 8722, 4060, 4041, 4039, 7097], "cite_extract_rate": 0.4375, "origin_cites_number": 16, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of language-specific T-BPLMs and includes a table summarizing key features of each model. It briefly connects the cited papers by mentioning their language focus and pretraining strategies but lacks deeper synthesis or analysis of trends. There is minimal critical evaluation or abstraction into broader principles or frameworks."}}
{"id": "43771964-d5ce-4a4d-b083-ea3a0ddd703a", "title": "Ontology Enriched", "level": "subsubsection", "subsections": [], "parent_id": "53f83f54-a99c-4dc8-be35-f2014de10a77", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "T-BPLMs TAXONOMY"], ["subsection", "Extensions"], ["subsubsection", "Ontology Enriched"]], "content": "T-BPLMs like BioBERT, BlueBERT and PubMedBERT have achieved good results in many biomedical NLP tasks. These models acquire domain-specific knowledge by pretraining on large volumes of biomedical text. Recent works   showed that these models acquire only the knowledge available in pretraining corpora and the performance of these models can be further enhanced by integrating knowledge from various human-curated knowledge sources like UMLS. UMLS is a human-curated knowledge source connecting medical terms from various clinical vocabularies. In UMLS, each medical concept has a Concept Unique Identifier (CUI), preferred term, and synonym terms. UMLS concepts are linked by various semantic relationships. Domain knowledge of T-BPLMs can be further enhanced by further pretraining them on UMLS synonyms and relations  .\n\\begin{table*}[t!]\n\\begin{center}\n{\\renewcommand{\\arraystretch}{1.5}\n\\begin{tabular}{|p{2cm}|p{1.5cm}|p{3.5cm}|p{1.5cm}|p{7cm}|}\n\\hline\n\\textbf{Model}    & \\textbf{Pretrained from}   & \\textbf{UMLS data}     & \\textbf{Publicly Available} & \\textbf{Evaluation}  \\\\ \\hline\nClinical Kb-BERT    & BioBERT           & UMLS Relations   & Yes & Clinical NER and NLI     \\\\ \\hline                                                                        \nClinical Kb-ALBERT  & General ALBERT    & UMLS Relations   & Yes       & Clinical NER and NLI   \\\\ \\hline                                                                         \nUmlsBERT   & ClinicalBERT  & UMLS Synonyms      & Yes  & Clinical NER and NLI      \\\\ \\hline                                                                      \nCoderBERT   & BioBERT   & UMLS Synonyms and Relations & Yes  & Unsupervised Medical Concept Normalization, Semantic Similarity, and Relation Classification. \\\\ \\hline\nCoderBERT-ALL      & Multilingual BERT & UMLS Synonyms and Relations & Yes & Unsupervised Medical Concept Normalization, Semantic Similarity, and Relation Classification. \\\\ \\hline\nSapBERT   & PubMedBERT        & UMLS Synonyms      & Yes   & Medical Concept Normalization     \\\\ \\hline\nSapBERT-XLMR     & XLM-RoBERTa       & UMLS Synonyms     & Yes  & Medical Concept Normalization  \\\\ \\hline                                                                 \nKeBioLM    & PubMedBERT    & UMLS Relations    & Yes   & Named Entity Recognition and Relation   Extraction   \\\\ \\hline                                                                                                           \n\\end{tabular}}\n\\end{center}\n\\caption{\\label{table-ontology-enriched}  Summary of ontology enriched T-BPLMs.} \n\\end{table*}\nClinical Kb-BERT and Clinical Kb-ALBERT  are obtained by further pretraining BioBERT and ALBERT models on MIMIC-III clinical notes and UMLS relation triplets. Here, pretraining involves three loss functions namely MLM, NSP, and triple classification. Triple classification involves identifying whether two concepts are connected by the relation or not and helps to inject UMLS relationship knowledge into the model. UmlsBERT  is initialized from ClinicalBERT and further pretrained on MIMIC-III clinical notes using novel multi-label loss-based MLM and NSP. The novel multi-label loss function allows the model to connect all the words under the same CUI. CoderBERT  is initialized from BioBERT and further pre-trained on UMLS concepts and relations using multi-similarity loss and knowledge embedding loss. Multi-similarity loss helps to learn close embeddings for entities under the same CUI and Knowledge embedding loss helps to inject relationship knowledge. SapBERT  is initialized from PubMedBERT and further pre-trained on UMLS synonyms using multi-similarity loss. Table \\ref{table-ontology-enriched} contains a summary of ontology enriched T-BPLMs.", "cites": [4062, 4040, 9141, 8723], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes several ontology-enriched T-BPLMs, integrating their methods and objectives into a structured narrative. While it provides a clear explanation of how each model leverages UMLS data, it lacks deeper critical analysis of their strengths, weaknesses, or trade-offs. Some abstraction is achieved through general descriptions of training techniques like multi-similarity loss and triple classification, but broader theoretical insights or trends are not fully emphasized."}}
{"id": "d8623dfa-cbba-4118-99e3-0d48d53fee62", "title": "Debiased Models", "level": "subsubsection", "subsections": [], "parent_id": "53f83f54-a99c-4dc8-be35-f2014de10a77", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "T-BPLMs TAXONOMY"], ["subsection", "Extensions"], ["subsubsection", "Debiased Models"]], "content": "T-PLMs are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained . It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group. Zhang et al.  further pretrained SciBERT  on MIMIC-III clinical notes and showed that the performance of the model is different for different groups. The authors applied adversarial pretraining debiasing to reduce the gender bias in the model. The authors released both the models publicly to encourage further research in debiasing T-BPLMs.", "cites": [4064, 2483, 4063], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of debiased models, mentioning Zhang et al.'s adversarial pretraining approach on SciBERT with MIMIC-III data. However, it lacks deeper synthesis of the cited works, does not compare or contrast different debiasing strategies, and offers minimal critical or abstract insight into the broader implications or limitations of debiasing in biomedical PLMs."}}
{"id": "215dd079-ed3a-4b12-b804-e4b167360989", "title": "Multi-Modal Models", "level": "subsubsection", "subsections": [], "parent_id": "53f83f54-a99c-4dc8-be35-f2014de10a77", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "T-BPLMs TAXONOMY"], ["subsection", "Extensions"], ["subsubsection", "Multi-Modal Models"]], "content": "T-PLMs have achieved success in many of the NLP tasks including in specific domains like Biomedical. Recent research works have focused on developing pretrained models that can handle multi-modal data i.e., video + text , image + text   etc. In Biomedical domain, models like BERTHop  and Medical-VLBERT   have been proposed recently to handle image + text data. BERTHop   is a multi-modal T-BPLM  developed for Chest X-ray disease diagnosis. Like  ViLBERT  and LXMERT , BERTHop uses separate encoder to encoder image and text inputs. BERTHop uses PixelHop++  to encode image data and BlueBERT as text encoder. Medical-VLBERT is developed for automatic report generation from COVID-19 scans. Unlike BERTHop, Medical-VLBERT  uses shared encoder based on VL-BERT  to encode image and text data.", "cites": [4066, 4065, 768, 1278, 2023, 2012], "cite_extract_rate": 0.75, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic description of multi-modal T-BPLMs in the biomedical domain, specifically mentioning BERTHop and Medical-VLBERT. It makes minimal connections between the cited works, such as noting that BERTHop uses separate encoders while Medical-VLBERT uses a shared one. However, it lacks deeper synthesis of ideas, critical evaluation of model strengths or weaknesses, and abstraction to broader trends or principles in multi-modal biomedical modeling."}}
{"id": "1dc628b2-539c-403f-92b0-e473cbdc73d1", "title": "Natural Language Inference", "level": "subsection", "subsections": [], "parent_id": "7cf448b7-0883-424b-a8a6-0bbeae881763", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "BIOMEDICAL NLP TASKS"], ["subsection", "Natural Language Inference"]], "content": "Natural Language Inference (NLI) is an important NLP task that requires sentence-level semantics. It involves identifying the relationship between a pair of sentences i.e., whether the second sentence entails or contradicts or neural with the first sentence. Training the model on NLI datasets helps the models to learn sentence-level semantics, which is useful in many tasks like paraphrase mining, information retrieval  in the general domain and medical concept normalization , semantic relatedness , question answering  in the biomedical domain. NLI is framed as a three-way sentence pair classification problem. Here models like BERT learn the representation of given two sentences jointly and the three-way task-specific softmax classifier predicts the relationship between the given sentence pair. MedNLI  is the in-domain NLI dataset with around 14k instances generated from MIMIC-III  clinical notes. As MedNLI contains sentence pairs taken from EHRs, Kanakarajan et al.  further pretrained BioBERT  on MIMIC-III and then fine-tuned the model on MedNLI to achieve an accuracy of 83.45\\%. Cengiz et al.  applied an ensemble of two BioBERT models and achieved an accuracy of 84.7\\%. The authors fine-tuned each of the BioBERT models on general NLI datasets like SNLI  and MultiNLI  and then fine-tuned them on MedNLI.", "cites": [4068, 4046, 1174, 4048, 4067, 1173], "cite_extract_rate": 0.5, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of NLI and its applications in biomedical NLP but lacks deeper synthesis of the cited works. It lists results from a few studies (e.g., Kanakarajan et al. and Cengiz et al.) without connecting their approaches or outcomes to broader themes or trends in the field. There is minimal critical evaluation or abstraction beyond the specific tasks and models discussed."}}
{"id": "be668f34-1a87-498e-8f74-4ed700d99261", "title": "Entity Extraction", "level": "subsection", "subsections": [], "parent_id": "7cf448b7-0883-424b-a8a6-0bbeae881763", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "BIOMEDICAL NLP TASKS"], ["subsection", "Entity Extraction"]], "content": "Entity Extraction is the first step in unlocking valuable information in unstructured text data. Entity Extraction is useful in many tasks like entity linking, relation extraction, knowledge graph construction, etc. Extracting clinical entities like drug and adverse drug reactions is useful in pharmacovigilance and biomedical entities like proteins, chemicals and drugs is useful to discover knowledge in scientific literature. Some of the popular entity extraction datasets are I2B2 2010 , VAERS , CADEC , N2C2 2018  , BC4CHEMD , B5CDR-Chem , JNLPBA  and NCBI-Disease . \nMost of the existing work approaches entity extraction as sequence labeling or machine reading comprehension. In case of sequence labelling approach, BERT based models generate contextual representations for each token and then softmax layer , BiLSTM+Softmax , BiLSTM+CRF  or CRF  is applied. Recent works showed adding BiLSTM on the top of the BERT model does not show much difference in performance . This is because transformer encoder layers in BERT based models do the same job of encoding contextual in token representations like BiLSTM. Some of the works experimented with general BERT for extracting clinical and biomedical entities. For example, Portelli et al.  showed that SpanBERT+CRF outperformed in-domain BERT models also in extracting clinical entities in social media text. Boudjellal et al.   developed ABioNER by further pretraining AraBERT  on general Arabic corpus + biomedical Arabic text and showed that ABioNER outperformed both multilingual BERT  and AraBERT on Arabic biomedical entity extraction. This shows that further pretraining general AraBERT on small in-domain text corpus improves the performance of the model. As in-domain datasets are comparatively small, some of the recent works   initially fine-tuned the models on similar datasets before fine-tuning on small target datasets. This intermediate fine-tuning allows the model to learn more task-specific knowledge which improves the performance of the model on small target datasets. For example, Gao et al.  proposed a novel approach entity extraction approach based on intermediate fine-tuning and semi-supervised learning. Here intermediate fine-tuning allows the model to learn more task-specific knowledge while semi-supervised learning allows to leverage task-related unlabelled data by assigning pseudo labels. Recently Sun et al.  formulated biomedical entity extraction as question answering and showed that BioBERT+QA outperformed BioBERT+ (Softmax / CRF / BiLSTM-CRF) on six datasets.", "cites": [4039, 7, 4047, 4069], "cite_extract_rate": 0.2, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several papers to present a coherent narrative about entity extraction methods in biomedical NLP, connecting the use of BERT-based models with alternative approaches like CRF, BiLSTM, and QA. It critically evaluates the effectiveness of additional layers (e.g., BiLSTM) and highlights limitations due to small in-domain datasets. While it identifies broader patterns (e.g., the value of intermediate fine-tuning and semi-supervised learning), the abstraction remains somewhat task-focused rather than reaching meta-level insights."}}
{"id": "c1e8248c-6c31-4cd4-be3e-256e05ec2078", "title": "Semantic Textual Similarity", "level": "subsection", "subsections": [], "parent_id": "7cf448b7-0883-424b-a8a6-0bbeae881763", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "BIOMEDICAL NLP TASKS"], ["subsection", "Semantic Textual Similarity"]], "content": "Semantic Textual Similarity quantifies the degree of semantic similarity between two phrases or sentences. Unlike NLI which classifies the given sentence pair into one of three classes, semantic textual similarity returns a value that indicates the degree of similarity. Both NLI and STS require sentence-level semantics. STS is useful in tasks like concept relatedness , medical concept normalization  , duplicate text detection , question answering  and text summarization . Moreover, Reimers et al.  showed that training transformer-based PLMs on STS datasets allow the model to learn sentence-level semantics and hence better represent variable-length texts like phrases or sentences. Models like BERT learn the joint representation of given sentence pair and a task-specific sigmoid layer gives the similarity value. BIOSSES  and Clinical STS  are the commonly used datasets to train and evaluate in-domain STS models. \nRecent works exploited general models for clinical STS . For example, Yang et al.  achieved the best results on the 2019 N2C2 STS dataset using Roberta-large model. As clinical STS datasets are small in size, recent works initially fine-tuned the models on general STS datasets and then fine-tuned them on clinical datasets . Xiong et al.  enhanced in-domain BERT-based text similarity using CNN-based character level representations and TransE  based entity representations. Mutinda et al.  achieved a Pearson correlation score of 0.8320 by fine-tuning Clinical BERT on a combined training set having instances from both general and clinical STS datasets. Mahajan et al.  proposed a novel approach based on ClinicalBERT fine-tuned using iterative multi-task learning and achieved the best results on the Clinical STS dataset. Iterative multi-task learning a) helps the model to learn task-specific knowledge from related datasets and b) choose the best-related datasets for intermediate multi-task fine-tuning. The main drawback in the above existing works is giving `[CLS]` vector as sentence pair representation to the sigmoid layer. This is because the `[CLS]` vector contains only partial information. Unlike existing works, Wang et al.  applied hierarchical convolution on the final hidden state vectors and then applied max and min pooling to get the final sentence pair representation and achieved better results.", "cites": [4068, 4067, 4043], "cite_extract_rate": 0.1875, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple approaches to biomedical semantic textual similarity, integrating insights from BERT-based models and domain-specific extensions. It identifies a common limitation (reliance on [CLS] vector) and highlights a more effective alternative (hierarchical convolution with pooling). While it offers some abstraction by pointing to general trends in model representation strategies, the critique and analysis could be deeper to reach a high insight level."}}
{"id": "370bd848-1d61-4586-a78f-ce485af87097", "title": "Relation Extraction", "level": "subsection", "subsections": [], "parent_id": "7cf448b7-0883-424b-a8a6-0bbeae881763", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "BIOMEDICAL NLP TASKS"], ["subsection", "Relation Extraction"]], "content": "Relation extraction is a crucial task in information extraction which identifies the semantic relations between entities in text. Entity extraction followed by relation extraction helps to convert unstructured text into structured data. Extracting relations between entities is useful in many tasks like knowledge graph construction, text summarization, and question answering. Some of relation extraction datasets are I2B2 2012 , AIMED , ChemProt , DDI , I2B2 2010  and EU-ADR . Wei et al.  achieved the best results on two datasets using MIMIC-BERT  +Softmax. Thillaisundaram and Togia  applied SciBERT +Softmax to extract relations from biomedical abstracts as part of AGAC track of BioNLP-OST 2019 shared tasks . Liu et al.  proposed SciBERT+Softmax for relation extraction in biomedical text. They showed SciBERT+Softmax outperforms BERT+Softmax on three biomedical relation extraction datasets. The main drawback in the above existing works is that they utilize only the partial knowledge from the last layer in the form of `[CLS]` vector. Su et al.  added attention on the top of BioBERT to fully utilize the information in the last layer and achieved the best results on three biomedical extraction datasets. The authors generated the final representation by concatenating `[CLS]` vector and weighted sum vector of final hidden state vectors. When compared to applying LSTM on the final hidden state vectors, the attention layer gives better results.", "cites": [4038, 4070], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.3, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes several papers by discussing common trends in using transformer-based models for relation extraction in biomedical NLP, particularly the use of `[CLS]` vectors and the shift toward utilizing full layer information. It provides a critical analysis by identifying a major drawback in prior approaches (partial knowledge usage from the last layer) and highlighting how attention mechanisms address this issue. The abstraction is moderate, as it identifies the pattern of improving performance through better representation techniques but does not elevate this to a broader meta-level understanding of BPLM design principles."}}
{"id": "7aa54472-6eab-4540-9bd0-c775ae6e87dc", "title": "Text Classification", "level": "subsection", "subsections": [], "parent_id": "7cf448b7-0883-424b-a8a6-0bbeae881763", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "BIOMEDICAL NLP TASKS"], ["subsection", "Text Classification"]], "content": "Text classification involves assigning one of the predefined labels to variable-length texts like phrases, sentences, paragraphs, or documents. Text classification involves an encoder which is usually a transformer-based PLM and a task-specific softmax classifier. `[CLS]` vector or weighted sum of final hidden state vectors is treated as an aggregate representation of given text. The fully connected dense layer in the classifier projects text representation vector into n-dimensional vector space where ‘n’ represents the number of predefined labels. Finally, the softmax function is applied to get the probabilities of all the labels. Garadi et al.  formulated prescription medication (PM) identification from tweets as four-way text classification. They achieved good results using models like BERT, RoBERTa, XLNet, ALBERT, and DistillBERT. They trained different ML-based meta classifiers with predictions from pre-trained models as inputs and further improved the results.\nShen et al.  applied various in-domain BERT for Alzheimer disease clinical notes classification and achieved the best results using PubMedBERT and BioBERT. They generated labels for the training instances using a rule-based NLP algorithm.  Chen et al.  further pretrained the general BERT model on 1.5 drug-related tweets and showed that further pretraining improves the performance of the model on ADR tweets classification. Recent works  showed that adding attention on the top of the BERT model improves the performance of the model in clinical text classification. They introduced a custom attention model to aggregate the encoder output and showed that this leads to improved performance and interpretability.\n\\begin{table*}[t!]\n\\begin{center}\n{\\renewcommand{\\arraystretch}{1.5}\n\\begin{tabular}{|p{3.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|}\n\\hline\n\\textbf{Model}   & \\textbf{NER}  & \\textbf{PICO}   & \\textbf{RE} & \\textbf{SS} & \\textbf{DC} & \\textbf{QA} & \\textbf{BLURB Score} \\\\ \\hline\nBioELECTRA  & 86.67\t& 74.13\t& 81.44\t& 92.76\t& 84.20 & \t76.38 & 82.60 \\\\ \\hline\nPubMedBERT  & 86.13\t& 73.72\t& 80.59 &\t92.31 &\t82.62\t& 73.61 & 81.50 \\\\ \\hline\nBioBERT  & 85.81\t& 73.18 & \t79.79 &\t89.52 &\t81.54 &\t72.19 & 80.34 \\\\ \\hline\nScibert  & 85.43 &\t73.12 &\t79.56 &\t86.25\t& 80.66 &\t68.12 &  78.86 \\\\ \\hline\nClinicalBERT  & 83.99 &\t72.06 &\t76.91 &\t91.23 &\t80.74 &\t58.79 & 77.29 \\\\ \\hline\nBlueBERT  & 84.50 &\t72.54 &\t76.13 &\t85.38 &\t80.48 &\t58.57 & 76.27 \\\\ \\hline\n\\end{tabular}}\n\\end{center}\n\\caption{\\label{table-evaluation}  Performance comparison of various T-BPLMs (scores from BLURB benchmark). NER- Named Entity Recognition, PICO - Patient Population Interventions Comparator and Outcomes, RE-Relation Extraction, SS - Sentence Similarity, DC - Document Classification and QA - Question Answering. } \n\\end{table*}", "cites": [8719, 2483, 4071, 4032], "cite_extract_rate": 0.36363636363636365, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual summary of how transformer-based models are used for text classification in biomedical NLP, citing several studies with specific results. While it connects different papers by describing common architectures (e.g., using [CLS] vector, attention layers), it lacks deeper synthesis or abstraction into broader trends. Critical analysis is minimal, focusing mostly on performance outcomes without evaluating limitations or trade-offs."}}
{"id": "396dc9f5-0ec1-4f12-bf10-e76a2bdb1517", "title": "Question Answering", "level": "subsection", "subsections": [], "parent_id": "7cf448b7-0883-424b-a8a6-0bbeae881763", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "BIOMEDICAL NLP TASKS"], ["subsection", "Question Answering"]], "content": "Question Answering (QA) aims to extract answers for the given queries. QA helps to quickly find information in clinical notes or biomedical literature and thus saves a lot of time. The main challenge in developing automated QA systems for the clinical or biomedical domain is the small size of datasets. Developing large QA datasets in the clinical or biomedical domain is quite expensive and requires a lot of time also. Some of the popular in-domain QA datasets are emrQA , CliCR , PubMedQA  COVID-QA , MASH-QA  and Health-QA . Chakraborty et al.  showed BioMedBERT obtained by further pretraining BERT-Large on BREATHE 1.0 corpus outperformed BioBERT on biomedical question answering. The main reason for this is the diversity of text in BREATHE 1.0 corpus. \nPergola et al.  introduced Biomedical Entity Masking (BEM) which allows the model to learn entity-centric knowledge during further pretraining. They showed that BEM improved the performance of both general and in-domain models on two in-domain QA datasets. Recent works used intermediate fine-tuning on general QA  or NLI  datasets or multi-tasking  to improve the performance of in-domain QA models. For example, Soni et al.  achieved the best results on a) CliCR by intermediate fine-tuning on SQuaD using BioBERT and b) emrQA by intermediate fine-tuning on SQuaD and CliCR using Clinical BERT. Yoon et al.  showed that intermediate fine-tuning on general domain Squad datasets improves the performance on biomedical question answering datasets. Akdemir et al.  proposed a novel  multi-task model based on BioBERT for biomedical question answering. They used biomedical NER as an auxiliary task and showed that transfer learning from the bioNER task improves performance on question answering tasks.", "cites": [4042, 4046, 4073, 4045, 4072, 4074], "cite_extract_rate": 0.5454545454545454, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key approaches from multiple papers to explain trends in biomedical QA, such as the use of domain-specific pretraining, entity masking, and intermediate fine-tuning. It provides some critical evaluation by highlighting the challenges of data scarcity and how different strategies address these, but does not deeply critique the limitations or assumptions of the methods. The abstraction is moderate, as it identifies patterns like the use of auxiliary tasks and cross-domain transfer, but stops short of developing a comprehensive meta-framework."}}
{"id": "769150c8-9e42-4dff-8e40-bf37a8bb893f", "title": "Low Cost Domain Adaptation", "level": "subsection", "subsections": [], "parent_id": "90f7ee70-2ebc-453c-a00f-9290d22da83e", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "CHALLENGES AND SOLUTIONS"], ["subsection", "Low Cost Domain Adaptation"]], "content": "The two popular approaches for developing T-BPLMs are MDPT and DSPT. These approaches involve pretraining on large volumes of in-domain text using high-end GPUs or TPUs for days. These two approaches are quite successful in developing BPLMs. However, these approaches are quite expensive requiring high computing resources with long pretraining durations . For example, BioBERT - it took around ten days to adapt general BERT to the biomedical domain using eight GPUs . Moreover, DSPT is more expensive compared to continual pretraining as it involves learning model weights from scratch .  So, there is a need for lost cost domain adaptation methods to adapt general BERT models to the biomedical domain. Two such low-cost domain adaptation methods are \\textbf{a)} \\textbf{Task Adaptative Pretraining (TAPT)} - It involves further pretraining on task-related unlabelled instances . TAPT allows the models to learn domain as well as task-specific knowledge by pretraining on a relatively small number of task-related unlabelled instances. The number of unlabelled instances can be increased by retrieving task-related unlabelled sentences using lightweight approaches like VAMPIRE .  \\textbf{b)} \\textit{Extending embedding layer} - General T-PLMs can be adapted to the biomedical domain by refining the embedding layer with the addition of new in-domain vocabulary . The new in-domain vocabulary can be i) generated over in-domain text using Word2Vec and then aligned with general word-piece vocabulary  or ii) generated over in-domain text using word-piece .", "cites": [4075, 2978], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from two cited papers to present two low-cost domain adaptation methods (TAPT and embedding layer extension) as alternatives to expensive approaches like MDPT and DSPT. It provides a critical perspective by highlighting the resource intensity of traditional methods and offering alternatives. While it identifies some general patterns (e.g., the need for cost-effective adaptation), it stops short of deeper abstraction or a novel framework."}}
{"id": "ff854654-b489-4a38-91d9-39a7884d2b03", "title": " Ontology Knowledge Injection", "level": "subsection", "subsections": [], "parent_id": "90f7ee70-2ebc-453c-a00f-9290d22da83e", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "CHALLENGES AND SOLUTIONS"], ["subsection", " Ontology Knowledge Injection"]], "content": "Models like BioBERT, PubMedBERT have achieved good results in many of the tasks. However, these models lack knowledge from human-curated knowledge sources. These models can be further enhanced by ontology knowledge injection. Ontology knowledge injection can be done in many ways namely a) continual pretraining the models on UMLS synonyms  or relations  or both  b) continual pretraining the models on UMLS concept definitions  and c) feature vector constructed using ontology is added to the  sequence vector learned by the models .", "cites": [4040, 9141], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes two papers to introduce different methods of ontology knowledge injection, providing a basic categorization of approaches. While it offers a critical note about the knowledge limitations of models like BioBERT and PubMedBERT, it does not deeply evaluate or compare the effectiveness of the proposed solutions. The abstraction is limited to general techniques without identifying broader principles or trends."}}
{"id": "e788e6e9-167d-40b8-9756-97064092cb47", "title": " Small Datasets", "level": "subsection", "subsections": [], "parent_id": "90f7ee70-2ebc-453c-a00f-9290d22da83e", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "CHALLENGES AND SOLUTIONS"], ["subsection", " Small Datasets"]], "content": "Pretraining on large volumes of in-domain text allows the model language representations while fine-tuning allows the model to learn task-specific knowledge. During fine-tuning the model must learn sufficient task-specific knowledge to achieve good results on the task where the input distribution, as well as label space, is different from pretraining . With small target datasets, the models are not able to learn enough task-specific which limits the performance. To over the small size of target datasets, the possible solutions are \n\\textbf{Intermediate Fine-Tuning} – Intermediate fine-tuning on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets  and \n\\textbf{Multi-Task Fine-Tuning} -  Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously . Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks. Multi-Task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain .\n\\textbf{Data Augmentation} - Data augmentation helps us to create new training instances from existing instances. These newly creating training instances are close to original training data and helpful in low resource scenarios. Back translation and EDA  are the top popular techniques for data augmentation. For example, Wang et al.  used back translation to augment the training instances to train the clinical text similarity model. The domain-specific ontologies like UMLS can also be used to augment the training instances . \n\\textbf{Semi-Supervised Learning} - Semi-supervised learning augments the training set with pseudo-labeled instances. The model which is fine-tuned on the original training set is used to label the task-related unlabelled instances. The model is again fine-tuned on the augmented training set and this process is repeated until the model converges  . Table \\ref{table-small-datasets} contains a brief summary of these approaches.", "cites": [4043, 9120, 4046, 4049, 7211, 8724, 4047, 4045, 4076, 4050, 326], "cite_extract_rate": 0.55, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes several strategies for addressing small datasets in biomedical NLP, drawing from a mix of general and domain-specific methods. It integrates key approaches like intermediate fine-tuning, multi-task fine-tuning, data augmentation, and semi-supervised learning, and references relevant papers (e.g., Wang et al. on back-translation, MT-DNN on multi-task learning). While it provides a coherent narrative and some generalization (e.g., noting the utility of multi-task learning in low-resource settings), it lacks deeper critical evaluation of the strengths and limitations of these methods and does not compare them systematically."}}
{"id": "aa5a4fdc-df5d-4505-8fcf-5448e1038fa2", "title": "Robustness to Noise", "level": "subsection", "subsections": [], "parent_id": "90f7ee70-2ebc-453c-a00f-9290d22da83e", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "CHALLENGES AND SOLUTIONS"], ["subsection", "Robustness to Noise"]], "content": "Transformed based PLMs have achieved the best results in many of the tasks. However, the performance of these models on noisy test instances is limited  . This is because the model is mostly trained on less noisy instances. As these models mostly never encounter noisy instances during fine-tuning, the performance is significantly reduced on noisy instances. Apart from this, the noisy words in the instances are split into several subtokens which affect the model learning. The robustness of models is crucial particularly insensitive domains like biomedical . Two possible solutions are \\textbf{a)} CharBERT  – replaced the WordPiece based embedding layer with CharCNN based embedding layer. Here word representation is generated from character embeddings using CharCNN. \\textbf{b)} Adversarial Training  – Here, the training set is augmented with the noisy instances. Training the model on an augmented training set exposes the model to noisy instances and hence the model performs better on noisy instances.", "cites": [4078, 4077], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.5, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic analytical discussion on the robustness of BPLMs to noise, citing two relevant papers to support the challenges and proposed solutions. It integrates the ideas of character-based embeddings and adversarial training to address the issue, but lacks deeper synthesis or comparison of the two approaches. The analysis is somewhat superficial, with limited generalization to broader patterns or principles in biomedical NLP."}}
{"id": "4af4c343-1a60-4c10-8d22-f8434efd8c07", "title": "Quality In-Domain Word Representations", "level": "subsection", "subsections": [], "parent_id": "90f7ee70-2ebc-453c-a00f-9290d22da83e", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "CHALLENGES AND SOLUTIONS"], ["subsection", "Quality In-Domain Word Representations"]], "content": "Continual pretraining allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of in-domain text. Though the models are adapted to in-domain, they still contain general vocabulary. As the vocabulary is learned over general text, it mostly includes subwords and words which are specific to the general domain. As a result, many of the in-domain words are not represented in a meaningful way. The two possible options to represent in-domain words in a meaningful way are \\textbf{a)} in-domain vocabulary through DSPT  \\textbf{b)} extending the general vocabulary with in-domain vocabulary .", "cites": [8719], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical discussion of the challenges in obtaining quality in-domain word representations, referencing one cited paper to support its argument. It integrates the concept of continual pretraining and contrasts it with domain-specific pretraining (DSPT), suggesting a coherent narrative. However, it lacks deeper critical analysis and a broader synthesis of multiple works, limiting its overall insight quality."}}
{"id": "ada5bc98-8bc0-4c5a-b9c3-bb991b3662c1", "title": "Low Resource (In-Domain Corpus) Pretraining", "level": "subsection", "subsections": [], "parent_id": "90f7ee70-2ebc-453c-a00f-9290d22da83e", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "CHALLENGES AND SOLUTIONS"], ["subsection", "Low Resource (In-Domain Corpus) Pretraining"]], "content": "CPT or DSPT involves pretraining the language model on large volumes of in-domain text. During pretraining, the model learns language representations that are useful across many tasks. The size of the pretraining corpus influences how well the model learns the language representations. It is not possible to get a large volume of in-domain text all the time. In such scenarios with less in-domain corpus, the model may not learn well when trained using any of the above two methods. The possible solution for this is simultaneous pretraining. In simultaneous pretraining , the model is trained on combined corpora having both general and in-domain text. As the in-domain text is comparatively less, up-sampling can be used to have a balanced pretraining.", "cites": [4029], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly describes the issue of low-resource in-domain pretraining and mentions a solution (simultaneous pretraining) without delving into the nuances or differences between the approaches. It integrates only one cited paper and lacks deeper analysis or abstraction to broader trends in the literature."}}
{"id": "3018ebd1-5f7c-4951-96d3-921e113a626d", "title": "Quality Sequence Representation", "level": "subsection", "subsections": [], "parent_id": "90f7ee70-2ebc-453c-a00f-9290d22da83e", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "CHALLENGES AND SOLUTIONS"], ["subsection", "Quality Sequence Representation"]], "content": "For text classification or sentence pair classification tasks like NLI and STS, Devlin et al.  suggested to use the final hidden vector of the special token added at the beginning of the input sequence as the final input sequence representation. According to Devlin et al. , the final hidden vector of the special token aggregates the entire sequence information. The final hidden vector is given to a linear layer which projects into n-dimensional vector space whether n represents the size of label space. Finally, a softmax is applied to convert it into a vector of probabilities. However, some of the recent works showed that involving all the final hidden vectors using max-pooling , attention , or hierarchical convolution layers  gives a much better final sequence representation compared to using only special token vector.", "cites": [4068, 4071, 7, 4070], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the approach of using the special token's hidden vector for sequence representation (from BERT) and contrasts it with newer methods using max-pooling, attention, or hierarchical convolution. It critically notes the limitations of the traditional approach and identifies a trend toward more holistic sequence representation in recent works. While it offers some abstraction by highlighting a shift in methodology, it does not fully generalize or propose a novel framework."}}
{"id": "98e4f6b4-b6e8-4fa5-90f4-c075344a40f8", "title": "Mitigating Bias", "level": "subsection", "subsections": [], "parent_id": "d0a2858b-b8a9-4e7e-9fb2-ae327dd51f36", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "FUTURE DIRECTIONS"], ["subsection", "Mitigating Bias"]], "content": "With the success of deep learning models in various tasks, deep learning-based systems are used to automate the decisions like department recommendation , disease prediction  etc. However, these models are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained . Real-world datasets have a bias in many forms. It can be based on various attributes like gender, age, ethnicity, and marital status. These attributes are considered as protected or sensitive .  For example, in the MIMIC-III dataset  a) heart disease is more common in males compared to females– an example of gender bias b) there are fewer clinical studies involving black patients compared to other groups –an example of ethnicity bias.  It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group. There are few works that identified and addressed bias in transformer-based biomedical language models. Zhang et al.  using a simple word competition task showed that SciBERT  exhibits ethnicity bias. Moreover, the authors showed SciBERT model when further-pre-trained on clinical notes exhibits performance differences for different protected attributes. They further showed that adversarial pretraining debiasing has little impact in reducing bias. Minot et al.  proposed an approach based on data augmentation to identify and reduce gender bias in patient notes. This is an area that needs to be explored further to improve reduce bias and improve the fairness in model decisions.", "cites": [4064, 4079, 2483, 4063], "cite_extract_rate": 0.36363636363636365, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple cited papers to present a coherent discussion on bias in biomedical PLMs. It connects findings from Zhang et al. and Minot et al., highlighting different approaches to identifying and mitigating bias. However, the critical analysis is limited, as it does not deeply evaluate the strengths or weaknesses of the methods. The abstraction is also moderate, as it identifies the importance of bias mitigation but does not generalize to broader NLP or healthcare AI principles."}}
{"id": "fc42e17b-e1cb-45a5-8a8d-687de92cf004", "title": "Novel Pretraining Tasks", "level": "subsection", "subsections": [], "parent_id": "d0a2858b-b8a9-4e7e-9fb2-ae327dd51f36", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "FUTURE DIRECTIONS"], ["subsection", "Novel Pretraining Tasks"]], "content": "Most of the biomedical language models (except ELECTRA-based models) are pre-trained using MLM. In MLM, only 15\\% of tokens are randomly masked and the model learns by predicting that 15\\% of masked tokens only. Here the main drawbacks are a) as tokens are randomly chosen for masking, the model may not learn much by predicting random tokens b) as only 15\\% of tokens are predicted, the training signal per example is less. So, the model has to see more examples to learn enough language information which results in the requirement of large pretraining corpora and more computational resources. There is a need for novel pretraining tasks like Replaced Token Detection (RTD) which can provide more training signal per example. Moreover, when the model is pretrained using multiple pretraining tasks, the model receives more training signals per example and hence can learn enough language information using less pretraining corpora and computational resources .", "cites": [4080], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of pretraining tasks in biomedical language models, particularly focusing on the limitations of MLM and the potential of RTD and multi-task learning. It synthesizes the core idea from the cited paper regarding the drawbacks of NSP and the exploration of novel pretraining tasks, connecting this to broader trends in BPLM development. While it identifies key issues and suggests improvements, it does not offer a novel framework or deep critique of the cited works."}}
{"id": "8c9b4ff9-c93a-48cf-8b22-726dbaf87853", "title": "Benchmarks", "level": "subsection", "subsections": [], "parent_id": "d0a2858b-b8a9-4e7e-9fb2-ae327dd51f36", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "FUTURE DIRECTIONS"], ["subsection", "Benchmarks"]], "content": "In general, a benchmark is a tool to evaluate the performance of models across different NLP tasks. A benchmark is required because we expect the pre-trained language models to be general and robust i.e., the models perform well across tasks rather than on one or two specific tasks. A benchmark with one or more datasets for multiple NLP tasks helps to assess the general ability and robustness of models. In general domain, we have a number of benchmarks like GLUE  and SuperGLUE  (general language understanding), XGLUE  (cross lingual language understanding) and LinCE  (code switching).  In biomedical domain there are three benchmarks namely BLUE , BLURB  and ChineseBLUE . BLUE introduced by Peng et al.  contains ten datasets for five biomedical NLP tasks, while BLURB contains thirteen datasets for six tasks and ChineseBLUE contains eight tasks with nine datasets. BLUE and ChineseBLUE include both EHR and scientific literature-based datasets, while BLURB includes only biomedical scientific literature-based datasets. The semantics of EHR and medical social media texts are different from biomedical scientific literature. So, there is a need of exclusive benchmarks for EHR and medical social media-based datasets.", "cites": [1569, 8719, 1568, 4041, 7766, 4081], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of existing biomedical benchmarks and their task coverage but does not synthesize the cited works into a deeper narrative or framework. There is minimal critical analysis of the benchmarks, and while it mentions the need for EHR and medical social media benchmarks, it does not generalize this into broader patterns or principles in biomedical NLP."}}
{"id": "0267bad4-c91b-4873-88b7-437c4b104266", "title": "Intrinsic Probes", "level": "subsection", "subsections": [], "parent_id": "d0a2858b-b8a9-4e7e-9fb2-ae327dd51f36", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "FUTURE DIRECTIONS"], ["subsection", "Intrinsic Probes"]], "content": "During pretraining, PLMs learn syntactic, semantic knowledge along with factual and common-sense knowledge available in the pretraining corpus . Intrinsic probes through light on the knowledge learned by PLMs during pretraining. In general NLP, researchers proposed several intrinsic probes like LAMA, Negated and Misprimed LAMA , XLAMA , X-FACTR , MickeyProbe  to understand the knowledge encoded in pretrained models. For example, LAMA  probes the factual and common-sense knowledge of English pretrained models, while X-FACTR  probes the factual knowledge of multi-lingual pretrained models. However, there is no such intrinsic probes in Biomedical domain to through light on the knowledge learned by BPLMs during pretraining. This is an area which requires much attention from Biomedical NLP community.", "cites": [4082, 4083, 3171, 4030], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes several intrinsic probe methods from general NLP and contrasts their absence in the biomedical domain, showing a basic level of integration. It identifies a research gap but does not critically evaluate the effectiveness or limitations of the general NLP probes. The section abstracts the concept of intrinsic probing to highlight its relevance in biomedical NLP, though the insight remains at a moderate level without deeper theoretical or methodological exploration."}}
{"id": "6c9ba37b-5c6e-4a4f-bdc1-917d0176d665", "title": "Efficient Models", "level": "subsection", "subsections": [], "parent_id": "d0a2858b-b8a9-4e7e-9fb2-ae327dd51f36", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "FUTURE DIRECTIONS"], ["subsection", "Efficient Models"]], "content": "Pretraining provides BPLMs with necessary background knowledge which can be transferred to downstream tasks. However, pretraining is computationally very expensive and also requires large volumes of pretraining data. So, there is need of novel model architecture which reduces the pretraining time as well as the amount of pretraining corpus. In general NLP, recently efficient models like ConvBERT  and DeBERTa  are proposed which reduces the pretraining time and amount of pretraining corpus required respectively. DeBERTa with two novel improvements (Disentangled attention mechanism and enhanced masked decoder) achieves better compared to RoBERTa. DeBERTa is pretrained on just 78GB of data while RoBERTa is pretrained on 160GB of data. ConvBERT with mixed attention block outperforms ELECTRA while using just $1/4^{th}$ of its pretraining cost. Biomedical NLP research community must focus on developing pretrained models based on these novel model architectures.", "cites": [7581, 1481], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from two cited papers, ConvBERT and DeBERTa, and connects their architectural innovations to the broader context of efficient model design in NLP. It provides a critical evaluation by highlighting how these models reduce pretraining costs and corpus requirements while maintaining or improving performance over established models. The section also abstracts to a degree by suggesting that the biomedical NLP community should adopt similar approaches, indicating a broader trend and potential direction."}}
{"id": "a87742ce-725e-4c7f-a29b-1a58e54b0f6d", "title": "Limitations", "level": "section", "subsections": [], "parent_id": "cee5380c-e5e4-4247-be7e-57acffba6561", "prefix_titles": [["title", "AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models"], ["section", "Limitations"]], "content": "We have comprehensively covered the research works related to T-BPLMs. As our focus is on T-BPLMs, we have not included any papers related to context insensitive biomedical embeddings. For detailed information regarding context insensitive biomedical embeddings, please refer the survey paper written by Kalyan and Sangeetha .  As it is a survey focused on T-BPLMs, we have covered the foundation concepts like transformers and self-supervised learning in a very brief way only.", "cites": [4033], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly states the survey's scope and acknowledges the exclusion of context-insensitive biomedical embeddings, but it lacks deeper synthesis or integration of ideas from the cited paper. It does not critically analyze the excluded work or provide broader patterns or principles. The narrative remains at a surface level description of limitations."}}
