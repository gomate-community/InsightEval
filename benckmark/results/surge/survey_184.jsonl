{"id": "a9cd6aa5-9969-4661-9411-dbd90ff12330", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "d9782043-9373-462c-b592-7d9a42b0beee", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Introduction"]], "content": "Deep learning has become the dominant approach in coping with various tasks in Natural Language Processing (NLP) today, especially when operated on large-scale text corpora. Conventionally, text sequences are considered as a bag of tokens such as BoW and TF-IDF in NLP tasks. With recent success of Word Embeddings techniques , sentences are typically represented as a sequence of tokens in NLP tasks. Hence, popular deep learning techniques such as recurrent neural networks  and convolutional neural networks  have been widely applied for modeling text sequence.\nHowever, there is a rich variety of NLP problems that can be best expressed with a graph structure. For instance, the sentence structural information in text sequence (i.e. syntactic parsing trees like dependency and constituency parsing trees) can be exploited to augment original sequence data by incorporating the task-specific knowledge. Similarly, the semantic information in sequence data (i.e. semantic parsing graphs like Abstract Meaning Representation graphs and Information Extraction graphs) can be leveraged to enhance original sequence data as well. Therefore, these graph-structured data can encode complicated pairwise relationships between entity tokens for learning more informative representations. \nUnfortunately, deep learning techniques that were disruptive for Euclidean data (e.g, images) or sequence data (e.g, text) are not immediately applicable to graph-structured data, due to the complexity of graph data such as irregular structure and varying size of node neighbors. As a result, this gap has driven a tide in research for deep learning on graphs, especially in development of graph neural networks (GNNs) .\n\\begin{figure}[h]\n\\centering\n\\includegraphics[width=\\textwidth]{framework.png}\n\\caption{The taxonomy, which systematically organizes GNNs for NLP along four axes: graph construction, graph representation learning, encoder-decoder models, and the applications.}\n\\label{fig:taxonomy}\n\\end{figure}\nThis wave of research at the intersection of deep learning on graphs and NLP has influenced a variety of NLP tasks~. There has seen a surge of interests in applying and developing different GNNs variants and achieved considerable success in many NLP tasks, ranging from classification tasks like sentence classification , semantic role labeling , and relation extraction , to generation tasks like machine translation , question generation , and summarization . Despite the successes these existing research has achieved, deep learning on graphs for NLP still encounters many challenges, namely,\n\\begin{itemize}\n\\item Automatically transforming original text sequence data into highly graph-structured data. Such challenge is profound in NLP since most of the NLP tasks involving using the text sequences as the original inputs. Automatic graph construction from the text sequence to utilize the underlying structural information is a crucial step in utilizing graph neural networks for NLP problems.\\vspace{-0.1cm}\n\\item Properly determining graph representation learning techniques. It is critical to come up with specially-designed GNNs to learn the unique characteristics of different graph-structures data such as undirected, directed, multi-relational and heterogeneous graphs. \\vspace{-0.1cm}\n\\item Effectively modeling complex data. Such challenge is important since many NLP tasks involve learning the mapping between the graph-based inputs and other highly structured output data such as sequences, trees, as well as graph data with multi-types in both nodes and edges. \\vspace{-0.1cm}\n\\end{itemize}\nIn this survey, we will present for the first time a comprehensive overview of \\textit{Graph Neural Networks for Natural Language Processing}. Our survey is timely for both Machine Learning and NLP communities, which covers relevant and interesting topics, including automatic graph construction for NLP, graph representation learning for NLP, various advanced GNNs-based encoder-decoder models (i.e. graph2seq, graph2tree, and graph2graph) for NLP, and the applications of GNNs in various NLP tasks. \nWe highlight our main contributions as follows:\n\\begin{itemize}\n    \\item We propose a new taxonomy of GNNs for NLP, which systematically organizes existing research of GNNs for NLP along four axes: graph construction, graph representation learning, and graph based encoder-decoder models. \\vspace{-0.1cm}\n    \\item We present the most comprehensive overview of the state-of-the-art GNNs-based approaches for various NLP tasks. We provide detailed descriptions and necessary comparisons on various graph construction approaches based on the domain knowledge and semantic space, graph representation learning approaches for various categories of graph-structures data, GNNs-based encoder-decoder models given different combinations of inputs and output data types. \\vspace{-0.1cm}\n    \\item We introduce a large number of NLP applications that are exploiting the power of GNNs, including how they handle these NLP tasks along three key components (i.e., graph construction, graph representation learning, and embedding initialization), as well as providing corresponding benchmark datasets, evaluation metrics, and open-source codes. \\vspace{-0.1cm}\n    \\item We outline various outstanding challenges for making the full use of GNNs for NLP and provides discussions and suggestions for fruitful and unexplored research directions. \n\\end{itemize}\nThe rest of the survey is structured as follows. \nSection \\ref{sec:Graph Based Algorithms for Natural Language Processing} reviews the NLP problems from a graph perspective, and then briefly introduces some representative traditional graph-based methods for solving NLP problems. \nSection \\ref{sec:Graph Neural Networks} elaborates basic foundations and methodologies for graph neural networks, which are a class of modern neural networks that directly operate on graph-structured data. We also provide a list of notations used throughout this survey. \nSection \\ref{sec:Graph Construction Methods for Natural Language Processing} focuses on introducing two major graph construction approaches, namely static graph construction and dynamic graph construction for constructing graph structured inputs in various NLP tasks. \nSection \\ref{sec:Graph Representation Learning for NLP} discusses various graph representation learning techniques that are directly operated on the constructed graphs for various NLP tasks.\nSection \\ref{sec:GNN Based Encoder-Decoder Models} first introduces the typical Seq2Seq models, and then discusses two typical graph-based encoder-decoder models for NLP tasks (i.e., graph-to-tree and graph-to-graph models).\nSection \\ref{sec:Applications} discusses 12 typical NLP applications using GNNs bu providing the summary of all the applications with their sub-tasks, evaluation metrics and open-source codes.\nSection \\ref{sec:General Challenges and Future Directions} discusses various general challenges of GNNs for NLP and pinpoints the future research directions.\nFinally, Section \\ref{sec:Conclusions} summarizes the paper. The taxonomy, which systematically organizes GNN for NLP approaches along four axes: graph construction, graph representation learning, encoder-decoder models, and the applications is illustrated in Fig.\\ref{fig:taxonomy}.", "cites": [242, 8313, 1684, 1053, 7213, 1054, 7325, 1056, 1055, 1052], "cite_extract_rate": 0.6111111111111112, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.8, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple cited papers to present a coherent narrative on the use of GNNs in NLP, particularly through its proposed taxonomy. While some critical analysis is present, such as identifying challenges in graph construction and modeling, it is not deeply evaluative of the cited works themselves. The abstraction is strong, with the introduction of a novel framework and broader categorization of GNN usage in NLP."}}
{"id": "e0ba128a-bcc7-48d5-96cc-055ee95b5f2a", "title": "Natural Language Processing: A Graph Perspective", "level": "subsection", "subsections": [], "parent_id": "e3a7331b-0273-425f-a212-329b1c5b8a12", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Based Algorithms for NLP"], ["subsection", "Natural Language Processing: A Graph Perspective"]], "content": "The way we represent natural language reflects our particular perspective on it, and has a fundamental influence on the way we process and understand it.\nIn general, there are three different ways of representing natural language.\nThe most simplified way is to represent natural language as a bag of tokens. This view of natural language completely ignores the specific positions of tokens appearing in text, and only considers how many times a unique token appears in text. If one randomly shuffles a given text, the meaning of the text does not change at all from this perspective.\nThe most representative NLP technique which takes this view is topic modeling~ which aims to model each input text as a mixture of topics where each topic can be further modeled as a mixture of words. \nA more natural way is to represent natural language as a sequence of tokens. This is how human beings normally speak and write natural language. Compared to the above bag perspective, this view of natural language is able to capture richer information of text, e.g., which two tokens are consecutive and how many times a word pair co-occurs in local context.\nThe most representative NLP techniques which take this view include the linear-chain CRF~ which implements sequential dependencies in the predictions, and the word2vec~ which learns word embeddings by predicting the context words of a target word.\nThe third way is to represent natural language as a graph. Graphs are ubiquitous in NLP. While it is probably most apparent to regard text as sequential data, in the NLP community, there is a long history of representing text as various kinds of graphs. Common graph representations of text or world knowledge include dependency graphs, constituency graphs, AMR graphs, IE graphs, lexical networks, and knowledge graphs. Besides, one can also construct a text graph containing multiple hierarchies of elements such as document, passage, sentence and word. In comparison with the above two perspectives, this view of natural language is able to capture richer relationships among text elements. As we will introduce in next section, many traditional graph-based methods (e.g., random walk, label propagation) have been successfully applied to challenging NLP problems including word-sense disambiguation, name disambiguation, co-reference resolution, sentiment analysis, and text clustering.", "cites": [1684], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a general overview of three ways to represent natural language, mentioning a few representative techniques and briefly referencing one cited paper (word2vec). It lacks deeper synthesis of multiple sources, critical evaluation of the methods, or broader abstraction. The narrative is primarily descriptive with minimal insight into the implications or comparative advantages of graph-based approaches."}}
{"id": "53b51ac6-2dbb-47ff-9293-6a0ab9c00011", "title": "Graph Filtering", "level": "subsubsection", "subsections": ["9115c35b-820c-4a97-9add-e9039d618115", "2e49e19a-2f07-44e5-8c78-3e8264e2d614", "5e6f2661-a5cc-4b5b-9713-7d5dab71269d", "42a3a59f-e668-4833-b2dd-45ad77ff15f7"], "parent_id": "49d3f10b-102d-4f33-ae39-537e05bd85bd", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Neural Networks"], ["subsection", "Methodologies"], ["subsubsection", "Graph Filtering"]], "content": "There exists a variety of implementations of graph filter $f$ in Eq.\\eqref{eq-filter}, which could be roughly categorized into spectral-based graph filters, spatial-based graph filters, attention-based graph filters and recurrent-based graph filters. Conceptually, the spectral-based graph filters are based on spectral graph theory while the spatial-based methods compute a node embedding using its spatially close neighbor nodes on the graph. Some spectral-based graph filters can be converted to spatial-based graph filters. The attention-based graph filters are inspired by the self-attention mechanism  to assign different attention weights to different neighbor nodes. Recurrent-based graph filters introduce gating mechanism, and the model parameters are shared across different GNN layers. Next, we will explain these four types of graph filters in detail by introducing some of their representative GNN models.", "cites": [38], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of different graph filtering approaches, including their conceptual differences and some representative models. While it integrates a few ideas (e.g., the connection between spectral and spatial-based filters), it lacks in-depth comparative analysis or critique of the cited works. The abstraction is limited to general categories without deeper insights or overarching principles."}}
{"id": "9115c35b-820c-4a97-9add-e9039d618115", "title": "Spectral-based Graph Filters", "level": "paragraph", "subsections": [], "parent_id": "53b51ac6-2dbb-47ff-9293-6a0ab9c00011", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Neural Networks"], ["subsection", "Methodologies"], ["subsubsection", "Graph Filtering"], ["paragraph", "Spectral-based Graph Filters"]], "content": "Inspired by graph signal processing, \\citeauthor{defferrard2016convolutional} proposed a spectral graph theoretical formulation of CNNs, which generalizes CNNs to graphs and provides the same linear computational complexity and constant learning complexity as classical CNNs. A more typical example of spectral-based graph filters is Graph Convolutional Networks (GCN)~. Spectral convolution on graphs is defined as the multiplication of a signal $\\mathbf{x}_i \\in \\mathbb{R}^n$ (a scalar for node $v_i$) with the filter $f_\\mathbf{filter} = \\text{diag}(\\theta)$ parameterized by $\\theta \\in \\mathbb{R}^n$ in the Fourier domain:\n\\begin{equation}\n\\label{gcn-0}\n    f_\\mathbf{filter} \\ast \\mathbf{x}_i = \\mathbf{U} f(\\mathbf{\\Lambda})\\mathbf{U}^T\\mathbf{x}_i\n\\end{equation}\nwhere $\\mathbf{U}$ is the matrix of eigenvectors of the normalized graph Laplacian ${L}={I}_n-{D}^{-\\frac{1}{2}}{A}{D}^{-\\frac{1}{2}}$. ${I}_n$ is the identity matrix, ${D}$ is the degree matrix and $\\mathbf{\\Lambda}$ is the eigenvalues of ${L}$.\nHowever, the computation of the full eigen-decomposition is prohibitively expensive. To solve this problem,    uses a truncated expansion in terms of Chebyshev polynomials $\\mathbf{T}_p(x)$ up to $P^{th}$-order to approximate $\\mathbf{g}_\\theta(\\mathbf{\\Lambda})$. Eq. \\eqref{gcn-0} can be represented as follows:\n\\begin{equation}\n\\label{gcn-1}\n    f'_\\mathbf{filter} \\ast \\mathbf{x}_i \\approx \\sum_{p=0}^P\\theta_p' \\mathbf{T}_p(\\Tilde{\\mathbf{L}})\\mathbf{x}_i\n\\end{equation}\nwhere $\\Tilde{{L}}=\\frac{2}{\\lambda_{max}}{L}-{I}_n$. $\\lambda_{max}$ is the largest eigenvalue of ${L}$. $\\theta_k' \\in \\mathbb{R}^P$ is a vector of Chebyshev coefficients. The Chebyshev polynomials can be defined recursively: $\\mathbf{T}_k(\\mathbf{x}_i)=2\\mathbf{x}_i\\mathbf{T}_{k-1}(\\mathbf{x}_i)-\\mathbf{T}_{k-2}(\\mathbf{x}_i)$, with $\\mathbf{T}_0(\\mathbf{x}_i)=1$ and $\\mathbf{T}_1(\\mathbf{x}_i)=\\mathbf{x}_i$. Eq.\\eqref{gcn-1} is a $K$th-order polynomial in the Laplacian, which shows that every central node depends only on nodes in the $P$-hop range.\nTherefore, a neural network model based on graph convolution can stack multiple convolutional layers using Eq. \\eqref{gcn-1}. By limiting the layer-wise convolution operation to $P = 1$ and stacking multiple convolutional layers,  proposed a multi-layer Graph Convolutional Network (GCN). It further approximates $\\lambda_{max} \\approx 2$ and Eq. \\eqref{gcn-1} is simplified to:\n\\begin{equation}\n   f'_\\mathbf{filter} \\ast \\mathbf{h}_i^{(l)} \\approx \\theta_0' \\mathbf{h}_i^{(l)} + \\theta_1' ({L}-{I}_n)\\mathbf{h}_i^{(l)} = \\theta_0' \\mathbf{h}_i^{(l)} - \\theta_1' {D}^{-\\frac{1}{2}}{A}{D}^{-\\frac{1}{2}} \\mathbf{h}_i^{(l)}\n\\end{equation}\nwith two free parameters $\\theta_0'$ and $\\theta_1'$. To alleviate the problem of overfitting and minimize the number of operations (such as matrix multiplications), it is beneficial to constrain the number of parameters by setting a single parameter $\\theta = \\theta_0' = -\\theta_1'$:\n\\begin{equation}\n    f_\\mathbf{filter} \\ast\\mathbf{h}_i^{(l)} \\approx \\theta({I}_n+{D}^{-\\frac{1}{2}}{A}{D}^{-\\frac{1}{2}})\\mathbf{h}_i^{(l)}\n\\end{equation}\nRepeat application of this operator may cause numerical instability and explosion/vanishing gradients,  proposed to use a \\textit{renormalization trick}: ${I}_n+{D}^{-\\frac{1}{2}}{A}{D}^{-\\frac{1}{2}} \\rightarrow {\\Tilde{D}}^{-\\frac{1}{2}}{\\Tilde{A}}{\\Tilde{D}}^{-\\frac{1}{2}}$, with ${\\Tilde{A}}={A}+{I}_n$ and ${\\Tilde{D}}_{ii} = \\sum_j {\\Tilde{A}}_{ij}$. Finally, the definition can be generalized with a signal $\\mathbf{H} \\in \\mathbb{R}^{n \\times d}$ with $d$ input channels (i.e. a $d$-dimensional feature vector for each node) and $F$ filters or feature maps as follows:\n\\begin{equation}\n    \\mathbf{H}^{(l)}=\\sigma( {\\Tilde{D}}^{-\\frac{1}{2}}{\\Tilde{A}}{\\Tilde{D}}^{-\\frac{1}{2}} \\mathbf{H}^{(l-1)} \\mathbf{W}^{(l-1)})\n\\end{equation}\nHere, $\\mathbf{W}^{(l-1)}$ is a layer-specific trainable weight matrix and $\\sigma(\\cdot)$ denotes an activation function. $\\mathbf{H}^{(l)} \\in \\mathbb{R}^{n \\times d}$ is the activated node embeddings at $(l-1)$-th layer.", "cites": [8313], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the core idea from the cited paper by Defferrard et al., effectively integrating spectral graph theory into the framework of CNNs for graphs. It also explains successive simplifications and optimizations (e.g., Chebyshev approximation, renormalization trick), showing a clear analytical progression. However, it primarily focuses on methodological explanation rather than comparing with other GNN approaches or critically evaluating limitations of the spectral formulation."}}
{"id": "2e49e19a-2f07-44e5-8c78-3e8264e2d614", "title": "Spatial-based Graph Filters", "level": "paragraph", "subsections": [], "parent_id": "53b51ac6-2dbb-47ff-9293-6a0ab9c00011", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Neural Networks"], ["subsection", "Methodologies"], ["subsubsection", "Graph Filtering"], ["paragraph", "Spatial-based Graph Filters"]], "content": "Analogous to the convolutional operation of a conventional\nCNN, spatial-based graph filters operate the graph convolutions based on a node’s spatial relations. The spatial-based graph filters derive the updated representation for the target node via convolving its representation with its neighbors’ representations. On the other hand, spatial-based graph filters hold the idea of information propagation, namely, message passing. The spatial-based graph convolutional operation essentially propagates node information as messages along the edges. Here we introduce two typical GNNs based on spatial-based graph filters are Message Passing Neural Network (MPNN)~ and GraphSage~.\nMPNN~ proposes a general framework of spatial-based graph filters $f_\\mathbf{filter}$ which is a composite function consisting of $f_U$ and $f_M$. It treats graph convolutions as a message passing process in which\ninformation can be passed from one node to another along\nthe edges directly. MPNN runs $K$-step message passing iterations\nto let information propagate further to K-hop neighboring nodes. The message passing\nfunction, namely the spatial-based graph filter, on the target node $v_i$ is defined as\n\\begin{equation}\n    \\mathbf{h}^{(l)}_{i}=f_\\mathbf{filter}(A, \\mathbf{H}^{(l-1)})=f_U(\\mathbf{h}^{(l-1)}_i,\\sum_{v_j\\in N(v_i)} f_M(\\mathbf{h}_i^{(l-1)},\\mathbf{h}_j^{(l-1)},\\mathbf{e}_{i,j})),\n\\end{equation}\nwhere $\\mathbf{h}^{(0)}_{i}=\\mathbf{x}_i$, $f_U(\\cdot)$ and $f_M(\\cdot)$ are the update and message aggregate functions with learnable parameters, respectively. After deriving the hidden representations of each node, $\\mathbf{h}^{(L)}_i$ ($L$ is the number of graph convolution layers) can be passed to an output layer to perform\nnode-level prediction tasks or to a readout function to perform graph-level prediction tasks. MPNN is very general to include many existing GNNs by applying different functions of $f_U(\\cdot)$ and $f_M(\\cdot)$.\nConsidering that the number of neighbors of a node can vary from one to a thousand or even more, it is inefficient to take the full size of a node’s neighborhood in a giant graph with thousands of millions of nodes. GraphSage~ adopts sampling to obtain a fixed number of neighbors for each node as\n\\begin{equation}\n    f_\\mathbf{filter}(A, \\mathbf{H}^{(l-1)})=\\sigma(\\mathbf{W}^{(l)}\\cdot f_M(\\mathbf{h}^{(l-1)}_i,\\{\\mathbf{h}^{(l-1)}_j, \\forall v_j\\in N(v_i)\\})),\n\\end{equation}\nwhere $N(v_i)$ is a random sample of the neighboring nodes of node $v_i$. The aggregation function can be any functions that are invariant to the permutations of node orderings such as mean, sum or max operations.", "cites": [216, 242], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key methodologies from both MPNN and GraphSage, framing them within the broader concept of spatial-based graph filters and message passing. It provides a clear abstraction by defining general forms of the graph filtering operations and highlights the inductive advantage of GraphSAGE. While it describes the models well, it offers some critical insight regarding the scalability issue of handling large neighborhoods, but could have more explicitly compared the trade-offs between the two approaches."}}
{"id": "5e6f2661-a5cc-4b5b-9713-7d5dab71269d", "title": "Attention-based Graph Filters", "level": "paragraph", "subsections": [], "parent_id": "53b51ac6-2dbb-47ff-9293-6a0ab9c00011", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Neural Networks"], ["subsection", "Methodologies"], ["subsubsection", "Graph Filtering"], ["paragraph", "Attention-based Graph Filters"]], "content": "The original versions of GNNs take edge connections of the input graph as fixed, and do not dynamically adjust the connectivity information during the graph learning process.\nMotivated by the above observation, and inspired by the successful applications of multi-head attention mechanism in the Transformer model~ proposed the Graph Attention Network (GAT) by introducing the multi-head attention mechanism to the GNN architecture which is able to dynamically learn the weights (i.e., attention scores) on the edges when performing message passing. \nMore specifically, when aggregating embeddings from neighboring nodes for each target node in the graph, the semantic similarity between the target node and each neighboring node will be considered by the multi-head attention mechanism, and important neighboring nodes will be assigned higher attention scores when performing the neighborhood aggregation. For the $l$-th layer, GAT thus uses the following formulation of the attention mechanism,\n\\begin{align}\n\\alpha_{ij} = \\frac{\\text{exp}(\\text{LeakyReLU}({\\vec{u}^{(l)T}} [\\vec{W}^{(l)} \\mathbf{h}_i^{(l-1)} || \\vec{W}^{(l)} \\mathbf{h}_j^{(l-1)}]))}{\\sum_{v_k \\in N(v_i)} \\text{exp}(\\text{LeakyReLU}({\\vec{u}^{(l)T}} [\\vec{W}^{(l)} \\mathbf{h}_i^{(l-1)} || \\vec{W}^{(l)} \\mathbf{h}_k^{(l-1)}]))}\n\\end{align}\nwhere $\\vec{u}^{(l)}$ and $\\vec{W}^{(l)}$ are the weight vector and weight matrix at $l$-th layer, respectively, and $||$ is the vector concatenation operation. Note that $N(v_i)$ is the 1-hop neighborhood of $v_i$ including itself.\nAfter obtaining the attention scores $\\alpha_{ij}$ for each pair of nodes $v_i$ and $v_j$, the updated node embeddings can be computed as a linear combination of the input node features followed by some nonlinearity $\\sigma$, formulated as,\n\\begin{align}\n\\mathbf{h}_i^{(l)} =f_\\mathbf{filter}(A, \\mathbf{H}^{(l-1)})=\\sigma(\\sum_{v_j \\in N(v_i)}\\alpha_{ij}\\vec{W}^{(l)}\\mathbf{h}^{(l-1)}_j)\n\\end{align}\nIn order to stabilize the learning process of the above self-attention, inspired by , multiple independent self-attention mechanisms are employed and their outputs are concatenated to produce the following node embedding:\n\\begin{align}\nf_\\mathbf{filter}(A, \\mathbf{H}^{(l-1)})= ||_{k=1}^{K} \\sigma(\\sum_{v_j \\in N(v_i)}\\alpha_{ij}^k\\vec{W}^{(l)}_k\\mathbf{h}^{(l-1)}_j),\n\\end{align}\nwhile the final GAT layer (i.e., the $L$-th layer for a GNNs with $L$ layers) employs averaging instead of concatenation to combine multi-head attention outputs.\n\\begin{align}\nf_\\mathbf{filter}(A, \\mathbf{H}^{(L-1)})= \\sigma(\\frac{1}{K} \\sum_{k=1}^K \\sum_{v_j \\in N(v_i)}\\alpha_{ij}^k\\vec{W}^{(L)}_k\\mathbf{h}_j^{(L-1)})\n\\end{align}", "cites": [180, 38], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the key idea of Graph Attention Networks (GAT) by integrating it with the broader context of attention mechanisms from the Transformer model. It provides a clear analytical explanation of how attention introduces dynamic weighting in GNNs. While it does describe the method well, it lacks deeper critical evaluation of the limitations or alternatives to attention-based filtering and does not abstract attention mechanisms into broader design principles of GNNs."}}
{"id": "42a3a59f-e668-4833-b2dd-45ad77ff15f7", "title": "Recurrent-based Graph Filters", "level": "paragraph", "subsections": [], "parent_id": "53b51ac6-2dbb-47ff-9293-6a0ab9c00011", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Neural Networks"], ["subsection", "Methodologies"], ["subsubsection", "Graph Filtering"], ["paragraph", "Recurrent-based Graph Filters"]], "content": "A typical example of recurrent-based graph filters is the Gated Graph Neural Networks (GGNN)-filter. The biggest modification from typical GNNs to GGNNs is the use of Gated Recurrent Units (GRU) . Analogous to RNN, GGNN unfolds the recurrence in a fixed T time steps and uses back propagation through time to calculate the gradients. The GGNN-filter also takes the edge type and edge direction into consideration. To this end, $e_{i,j}$ denotes the directed edge from node $v_i$ to node $v_j$ and the edge type of $e_{i,j}$ is $t_{i,j}$. The propagation process of recurrent-based filter  $f_\\mathbf{filter}$ in GGNN can be summarized as follows:\n\\begin{align}\n    \\mathbf{h}_i^{(0)} &= [\\mathbf{x}_i^T, \\mathbf{0}]^T \n    \\label{ggnn-0} \\\\\n    \\mathbf{a}_i^{(l)} &= A_{i:}^T[\\mathbf{h}_1^{(l-1)}...\\mathbf{h}_n^{(l-1)}]^T\n    \\label{eq:ggnn-aggregation}\n    \\\\\n    \\mathbf{h}_i^{(l)} &= \\text{GRU}(\\mathbf{a}_i^{(l)}, \\mathbf{h}_i^{(l-1)})\n\\end{align}\nwhere $A \\in \\mathbb{R}^{{dn} \\times 2dn}$ is a matrix determining how nodes in the graph communicating with each other. $n$ is the number of nodes in the graph. $A_{i:} \\in \\mathbb{R}^{d \\times 2d}$ are the two columns of blocks in $A$ corresponding to node $v_i$. In Eq. \\eqref{ggnn-0}, the initial node feature $\\mathbf{x}_i$ are padded with extra zeros to make the input size equal to the hidden size. Eq. \\eqref{eq:ggnn-aggregation} computes $\\mathbf{a}_i^{(l)} \\in \\mathbb{R}^{2d}$ by aggregating information from different nodes via incoming and outgoing edges with parameters dependent on the edge type and direction. The following step uses a GRU unit to update the hidden state of  node $v$ by incorporating $\\mathbf{a}_i^{(l)}$ and the previous timestep hidden state $\\mathbf{h}_i^{(l-1)}$.", "cites": [243], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the GGNN-filter and its use of GRUs, drawing from the concept of recurrence in RNNs as outlined in the cited paper. However, it lacks synthesis of broader ideas, critical evaluation of the method's strengths or weaknesses, and abstraction to overarching principles. The explanation is factual and focused on a single approach without contextualizing it within the larger GNN landscape."}}
{"id": "fc6c8783-fa14-4e5d-8232-e5203dcb5d1c", "title": "Flat Graph Pooling", "level": "paragraph", "subsections": [], "parent_id": "c880a365-c45e-4472-8f21-48330234f109", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Neural Networks"], ["subsection", "Methodologies"], ["subsubsection", "Graph Pooling"], ["paragraph", "Flat Graph Pooling"]], "content": "Ideally, an aggregator function would be invariant to permutations of its input while maintaining a large expressive capacity. The graph pooling operation $f_\\mathbf{pool}$ is commonly implemented as Max-pooling and Average-pooling. Another popular choices are the variants of the Max-pooling and Average pooling operations by following a fully-connected layer (FC) transformation. The resulting max pooling and FCmax can be expressed as:\n\\begin{equation}\n    \\mathbf{r}_i = \\text{max}(\\mathbf{H}_{:,i}) \\ \\ \\textit{or } \\ \\ \\mathbf{r}_i = \\text{max}(\\mathbf{W} \\mathbf{H}_{:,i})\n\\end{equation}\nwhere $i$ denotes the $i$-th channel of the node embedding and $\\mathbf{H}_{:,i} \\in \\mathbf{R}^{n \\times 1}$ is a vector. $\\mathbf{W}$ is a matrix that denotes to the trainable parameters of the FCmax pooling layer. $\\mathbf{r}_i$ is a scalar and the final graph embedding $\\mathbf{R} = [r_1, r_2,...,r_n]^T$. \nFinally, a powerful but less common pooling operation is the BiLSTM aggregation\nfunction which is not permutation invariant on the set of\nnode embeddings. However, it has been often demonstrated to have better expressive power than other flat pooling operations .", "cites": [242], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of graph pooling methods such as max-pooling, average-pooling, and FCmax, but does not effectively synthesize information from the cited paper or others. It lacks critical evaluation of the methods' strengths and weaknesses or comparative insights. The abstraction level is minimal, focusing on definitions rather than broader patterns or implications in the field."}}
{"id": "a1fe506d-8749-4a69-8f24-e18f325087b3", "title": "Hierarchical Graph Pooling", "level": "paragraph", "subsections": [], "parent_id": "c880a365-c45e-4472-8f21-48330234f109", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Neural Networks"], ["subsection", "Methodologies"], ["subsubsection", "Graph Pooling"], ["paragraph", "Hierarchical Graph Pooling"]], "content": "Hierarchical graph pooling coarsens the graph step by step to learn the graph-level embeddings. Hierarchical pooling layers can be divided into two categories according to the ways to coarsen the graph. One type of hierarchical pooling layer coarsens the graph by sub-sampling the most important nodes as the nodes of the coarsened graph . Another type of hierarchical pooling layer combines nodes in the input graph to form supernodes, which serve as the nodes of the coarsened graph . After sub-sampling nodes or generating supernodes, the hierarchical graph pooling $f_\\mathbf{pool}$ can be summarized as: (1) generating graph structure for the coarsened graph; (2) generating node features for the coarsened graph. The graph structure for the coarsened graph is generated from the input graph:\n\\begin{equation}\n    {A}' = COARSEN({A})\n\\end{equation}\nwhere ${A} \\in \\mathbb{R}^{n \\times n}$ is the adjacent matrix of the input graph, and ${A}' \\in \\mathbb{R}^{n' \\times n'}$ is the adjacent matrix of the coarsened graph. $f(.)$ is the graph sub-sampling or supernodes generating function. \n\\begin{comment}\n\\begin{enumerate}\n    \\item Foundations (Hanning) --background and definitions.\n    Graph neural networks can be viewed as a process of representation learning on graphs.\n    \\item Graph Neural network-Methodologies--basic operations\n        \\begin{enumerate}\n        \\item Graph Filtering: takes node features and graph structure as input and outputs a new set of node features\n        \\begin{enumerate}\n            \\item Spectral-based Graph Filters. e.g. GCN\n            \\item Spatial-based Graph Filters,e.g, GraphSage.\n            \\item Attention-based Graph Filters, e.g.,GAT\n            \\item Recurrent-based Graph Filters, e.g., GGNN.\n        \\end{enumerate}\n        \\item Graph Pooling: take a graph as\ninput and then produce a coarsened graph with fewer nodes\n    \\end{enumerate} \n\\end{enumerate}  \n\\end{comment}\n\\begin{comment}", "cites": [8407, 1057, 224], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic description of hierarchical graph pooling by categorizing it into sub-sampling and supernode generation. It integrates the general idea from the cited papers but lacks deeper synthesis or comparison between the different methods. There is minimal critical evaluation or abstraction to overarching principles in the field."}}
{"id": "58441071-326c-4d8d-9f6f-affbd1498d95", "title": "Graph Construction Methods for NLP", "level": "section", "subsections": ["08dbe9ba-d362-478e-808b-dc898c7c876a", "db9a17b8-fb06-450f-b43b-a86de3caa5fd"], "parent_id": "d9782043-9373-462c-b592-7d9a42b0beee", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Construction Methods for NLP"]], "content": "\\label{sec:Graph Construction Methods for Natural Language Processing}\nIn the previous section, we have discussed the basic foundations and methods of GNNs once given a graph input. Unfortunately, for most of the NLP tasks, the typical inputs are sequence of text rather than graphs. Therefore, how to construct a graph input from sequences of text becomes a demanding step in order to leverage the power of GNNs. \nIn this chapter, we will focus on introducing two major graph construction approaches, namely static graph construction and dynamic graph construction for constructing graph structured inputs in various NLP tasks. \n\\begin{table}[tb]\n\\caption{Two major graph construction approaches: static  and dynamic graph constructions}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{|c|c|c|l|l|}\n\\hline\n\\textbf{Approaches} & \\multicolumn{3}{c|}{\\textbf{Techniques}} & \\textbf{References} \\\\ \\hline\n\\multirow{18}{*}{Static Graph} & \\multicolumn{3}{c|}{\\multirow{8}{*}{Dependency Graph}}  &  \\\\\n& \\multicolumn{3}{c|}{} &  \\\\\n& \\multicolumn{3}{c|}{} &  \\\\\n& \\multicolumn{3}{c|}{} &  \\\\\n& \\multicolumn{3}{c|}{} &  \\\\ \n& \\multicolumn{3}{c|}{} &  \\\\ \n& \\multicolumn{3}{c|}{} &  \\\\ \n& \\multicolumn{3}{c|}{} &  \\\\ \\cline{2-5}\n& \\multicolumn{3}{c|}{Constituency Graph} &   \\\\ \\cline{2-5} \n& \\multicolumn{3}{c|}{\\multirow{5}{*}{AMR Graph}}  &  \\\\\n& \\multicolumn{3}{c|}{} &  \\\\\n& \\multicolumn{3}{c|}{} &  \\\\\n& \\multicolumn{3}{c|}{} &  \\\\\n& \\multicolumn{3}{c|}{} &  \\\\ \\cline{2-5} \n& \\multicolumn{3}{c|}{Information Extraction}  &  \\\\ \n& \\multicolumn{3}{c|}{Graph}  &  \\\\ \\cline{2-5} \n & \\multicolumn{3}{c|}{Discourse Graph} &    \\\\ \\cline{2-5} \n& \\multicolumn{3}{c|}{\\multirow{10}{*}{Knowledge Graph}} &  \\\\\n& \\multicolumn{3}{c|}{} &  \\\\\n& \\multicolumn{3}{c|}{} &  \\\\\n& \\multicolumn{3}{c|}{} &  \\\\\n& \\multicolumn{3}{c|}{} &  \\\\\n& \\multicolumn{3}{c|}{} &  \\\\\n& \\multicolumn{3}{c|}{} &  \\\\\n& \\multicolumn{3}{c|}{} &  \\\\\n& \\multicolumn{3}{c|}{} &  \\\\\n& \\multicolumn{3}{c|}{} &  \\\\ \\cline{2-5}\n& \\multicolumn{3}{c|}{\\multirow{2}{*}{Coreference Graph}} &  \\\\\n& \\multicolumn{3}{c|}{} &  \\\\ \\cline{2-5} \n& \\multicolumn{3}{c|}{Topic Graph} &  \\\\ \\cline{2-5} \n& \\multicolumn{3}{c|}{\\multirow{3}{*}{Similarity Graph Construction}}  &  \\\\ \n& \\multicolumn{3}{c|}{}  &  \\\\\n& \\multicolumn{3}{c|}{}  &  \\\\ \\cline{2-5} \n& \\multicolumn{3}{c|}{\\multirow{3}{*}{Co-occurrence Graph}}  &  \\\\\n& \\multicolumn{3}{c|}{}  &  \\\\\n& \\multicolumn{3}{c|}{}  &  \\\\ \\cline{2-5} \n& \\multicolumn{3}{c|}{\\multirow{8}{*}{App-driven Graph}} &   \\\\\n& \\multicolumn{3}{c|}{} &  \\\\\n& \\multicolumn{3}{c|}{} &  \\\\\n& \\multicolumn{3}{c|}{} &  \\\\\n& \\multicolumn{3}{c|}{} &  \\\\\n& \\multicolumn{3}{c|}{} &  \\\\ \n& \\multicolumn{3}{c|}{} &  \\\\\n& \\multicolumn{3}{c|}{} &  \\\\ \\hline\n\\multirow{6}{*}{Dynamic graph} & {Graph Similarity} & \\multicolumn{2}{c|}{Node Embedding Based } &   \\\\ \\cline{3-5} \n& Metric Learning  & \\multicolumn{2}{c|}{Structure-aware} &   \\\\ \\cline{2-5} \n& \\multicolumn{3}{c|}{Graph Sparsification Techniques}  &  \\\\ \\cline{2-5} \n& \\multicolumn{3}{c|}{ Combining Intrinsic and}  & \\multirow{2}{*}{  }\\\\\n& \\multicolumn{3}{c|}{ Implicit Graph Structures}  & \\\\\\hline\n\\end{tabular}\n}\n\\label{tab:graph-construction}\n\\end{table}", "cites": [1086, 1091, 1094, 7328, 1069, 57, 1067, 1090, 1054, 1079, 1073, 1078, 7326, 8410, 7327, 1088, 1087, 1095, 1081, 1053, 1083, 1074, 259, 1068, 1066, 1085, 1071, 1092, 1065, 1076, 1093, 1082, 1077, 1060, 1072, 7325, 7216, 1062, 1064, 8409, 1058, 1089, 1075, 1080, 1063, 7044, 1061, 8408, 1084, 1056, 9148, 1055, 1070, 1059], "cite_extract_rate": 0.5728155339805825, "origin_cites_number": 103, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section introduces a taxonomy of graph construction methods but does not effectively synthesize or integrate the cited papers into a coherent narrative. It primarily lists different graph types and mentions associated papers without elaborating on their relationships or contributions. There is minimal critical analysis or abstraction beyond the individual papers, resulting in a largely descriptive overview."}}
{"id": "1478106f-b6df-459a-bae1-d1d29a79c460", "title": "Dependency Graph Construction", "level": "paragraph", "subsections": [], "parent_id": "9b13822c-0845-4444-b7bf-5e9d6126646e", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Construction Methods for NLP"], ["subsection", "Static Graph Construction"], ["subsubsection", "Static Graph Construction Approaches"], ["paragraph", "Dependency Graph Construction"]], "content": "\\label{subsubsec:dep-graph}\nThe dependency graph is widely used to capture the dependency relations between different objects in the given sentences. Formally, given a paragraph, one can obtain the dependency parsing tree (e.g., syntactic dependency tree or semantic dependency parsing tree) by using various NLP parsing tools (e.g., Stanford CoreNLP~). Then one may extract the dependency relations from the dependency parsing tree and convert them into a dependency graph~.\nMoreover, since the given paragraph has sequential information while the graph nodes are unordered, one may introduce the sequential links to reserve such vital information in the graph structure~. Next, we will discuss a representative dependency graph construction method given the inputs $para$ and its extracted parsing tree, including three key steps: 1) constructing dependency relation, 2) constructing sequential relation, and 3) final graph conversion. An example for the dependency graph is shown in Fig. \\ref{fig:dependency-constitency-graph-sample}.\n\\noindent\\emph{Step 1: Dependency Relations.} Given the sentences in a specific paragraph, one first obtains the dependency parsing tree for each sentence. We denote dependency relations in the dependency tree as $(w_i, rel_{i, j}, w_j)$, where $w_i$, $w_j$ are the word nodes linked by an edge type $rel_{i, j}$. Conceptually, an edge denotes a dependency relation \"$w_i$ depends on $w_j$ with relation $rel_{i, j}$\". We define the dependency relation set as $\\mathcal{R}_{dep}$.\n\\noindent\\emph{Step 2: Sequential Relations.} \\label{sec:sequential_relation} The sequential relation encodes the adjacent relation of the elements in the original paragraph. Specifically, for dependency graph constructing, we define the sequential relation set $\\mathcal{R}_{seq} \\subseteq \\mathcal{V} \\times \\mathcal{V}$, where $\\mathcal{V}$ is the basic element (i.e., word) set. For each sequential relation $(w_i, w_{i+1}) \\in \\mathcal{R}_{seq}$, it means $w_i$ is adjacent to $w_{i+1}$ in the given paragraph.\n\\noindent\\emph{Step 3: Dependency Graph.} The dependency graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$ consists of the word nodes and two relations discussed above. Given the paragraph $para$, dependency relation set $\\mathcal{R}_{dep}$, and the sequential relation set $\\mathcal{R}_{seq}$, firstly, for each relation $(w_i, rel_{i, j}, w_j) \\in \\mathcal{R}_{dep}$, one adds the nodes $v_i$ (for the word $w_i$) and $v_j$ (for the word $w_j$) and  a directed edge from node $v_i$ to node $v_j$ with edge type $rel_{i, j}$. Secondly, for each relation $(w_i, w_j) \\in \\mathcal{R}_{seq}$, one adds two nodes $v_i$ (for the word $w_i$) and $v_j$ (for the word $w_j$) and an undirected edge between nodes $v_i$ and $v_j$ with specific sequential type.\n\\begin{figure}[ht!]\n\\centering\n\\vspace{-2mm}\n\\includegraphics[width=12.0cm]{graph-construction-v1.pdf}\n\\vspace{-4mm}\n\\caption{An example is shown for the dependency graph (left) and the constituency graph (right), respectively. The text input is from \\textbf{JOBS640}~ dataset.}\n\\label{fig:dependency-constitency-graph-sample}\n\\vspace{-0mm}\n\\end{figure}", "cites": [9148, 7329], "cite_extract_rate": 0.25, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a clear, step-by-step description of how dependency graphs are constructed for NLP, referencing concepts from the cited papers (GraphIE and Graph2Seq). However, it largely summarizes methods without critically analyzing their strengths, weaknesses, or comparative effectiveness. There is minimal synthesis of ideas and only basic abstraction to general principles."}}
{"id": "ca079613-148e-4841-96a6-d8b52db890ac", "title": "Constituency Graph Construction", "level": "paragraph", "subsections": [], "parent_id": "9b13822c-0845-4444-b7bf-5e9d6126646e", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Construction Methods for NLP"], ["subsection", "Static Graph Construction"], ["subsubsection", "Static Graph Construction Approaches"], ["paragraph", "Constituency Graph Construction"]], "content": "\\label{subsubsec: Constituency Graph Construction}\nThe constituency graph is another widely used static graph that is able to capture phrase-based syntactic relations in one or more sentences. Unlike dependency parsing, which only focuses on one-to-one correspondences between single words (i.e., word level), constituency parsing models the assembly of one or several corresponded words (i.e., phrase level). Thus it provides a new insight about the grammatical structure of a sentence. In this following subsection, we will discuss the typical approach for constructing a  constituency graph~. We first explain the basic concepts of the constituency relations and then illustrate the constituency graph construction procedure. An example for the Constituency graph is shown in Fig. \\ref{fig:dependency-constitency-graph-sample}.\n\\noindent\\emph{Step 1: Constituency Relations.}\nIn linguistics, constituency relation means the relation following the phrase structure grammars instead of the dependency relation and dependency grammars. Generally, the constituency relation derives from the subject(noun phrase NP)-predicate(verb phrase VP) relation. In this part, we only discuss the constituency relation deriving from the constituency parsing tree. Unlike the dependency parsing tree, in which all nodes have the same type, the constituency parsing tree distinguishes between the terminal and non-terminal nodes. Non-terminal categories of the constituency grammar label the parsing tree's interior nodes (e.g., S for sentence, and NP for noun phrase). In contrast, the leaf nodes are labeled by terminal categories (words in sentences). The nodes set can be denoted as: 1) non-terminal nodes set $\\mathcal{V}_{nt}$ (e.g. S and NP) and 2) terminal nodes set $\\mathcal{V}_{words}$. The constituency relation set are associated with the tree's edges, which can be denoted as $\\mathcal{R}_{cons} \\subseteq \\mathcal{V}_{nt} \\times (\\mathcal{V}_{nt} + \\mathcal{V}_{words})$.\n\\noindent\\emph{Step 2: Constituency Graph.}\nA constituency graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$ consists of both the non-terminal nodes $\\mathcal{V}_{nt}$ and the terminal nodes $\\mathcal{V}_{words}$, and the constituency edges as well as the sequential edges. Similar to the dependency graph, given a paragraph $para$ and the constituency relation set $\\mathcal{R}_{cons}$, for each constituency relation $(w_i, rel_{i, j}, w_j) \\in \\mathcal{R}_{cons}$, one adds the nodes $v_i$ (for the word $w_i$) and $v_j$ (for the word $w_j$) and a directed edge from node $v_i$ to node $v_j$. And then for each word nodes pair $(v_i, v_j)$ for the words which are adjacent in the original text, one adds an undirected edge between them with the specific sequential type. These sequential edges are used to reserve the sequential information~. \n\\begin{figure}[ht!]\n\\centering\n\\includegraphics[width=10.0cm]{amr_v1.pdf}\n\\caption{An example of AMR graph, the original sentence is \"Pual's description of himself: a fighter\".}\n\\label{fig:amr-graph-sample}\n\\vspace{-2mm}\n\\end{figure}", "cites": [1079, 1072], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a clear description of constituency graph construction methods, including definitions and a step-by-step procedure. It integrates basic concepts from the cited papers, particularly from Paper 2, but does not explicitly connect or synthesize ideas from both papers in depth. There is limited critical analysis or identification of broader trends, making the section more descriptive than analytical."}}
{"id": "b9430e1f-0c81-426a-9bab-34a264f6776f", "title": "AMR Graph Construction", "level": "paragraph", "subsections": [], "parent_id": "9b13822c-0845-4444-b7bf-5e9d6126646e", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Construction Methods for NLP"], ["subsection", "Static Graph Construction"], ["subsubsection", "Static Graph Construction Approaches"], ["paragraph", "AMR Graph Construction"]], "content": "\\label{subsubsec: AMR Graph Construction}\nThe AMR graphs are rooted, labeled, directed, acyclic graphs, which are widely used to represent the high-level semantic relations between abstract concepts of the unstructured and concrete natural text.\nDifferent from the syntactic idiosyncrasies, the AMR is the high-level semantic abstraction. More concretely, the different sentences that are semantically similar may share the same AMR parsing results, e.g., \"Paul described himself as a fighter\" and \"Paul's description of himself: a fighter\", as shown in Fig.~\\ref{fig:amr-graph-sample}. Despise the fact that the AMR is biased toward English, it is a powerful auxiliary representation for linguistic analysis\n~. \nSimilar to the previously introduced dependency and constituency trees, an AMR graph is derived from an AMR parsing tree. Next, we focus on introducing the general procedure of constructing the AMR graph based on the AMR parsing tree. We will discuss the basic concept of AMR relation and then show how to convert the relations into an AMR graph.\n\\noindent\\emph{Step 1: AMR Relations.}\nConceptually, there are two types of nodes in the AMR parsing tree: 1) the name (e.g. \"Paul\") is the specific value of the node instance and 2) the concepts are either English words (e.g. \"boy\"), PropBank framesets~ (e.g. \"want-01\"), or special keywords. The name nodes are the unique identities, while the concept nodes are shared by different instances. The edges that connect nodes are called relations (e.g. :ARG0 and :name). One may extract these AMR relations from the node pairs with edges, which is denoted as $(n_i, r_{i, j}, n_j) \\in \\mathcal{R}_{amr}$.\n\\noindent\\emph{Step 2: AMR Graph.}\nThe AMR graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$, which is rooted, labeled, directed, acyclic graph (DAG), consists of the AMR nodes and AMR relations discussed above. Similar to the dependency and constituency graphs, given the sentence $sent$ and the AMR relation set $\\mathcal{R}_{amr}$, for each relation $(n_i, r_{i, j}, n_j) \\in \\mathcal{R}_{amr}$, one adds the nodes $v_i$ (for the AMR node $n_i$) and $v_j$ (for the AMR node $n_j$) and add a directed edge from node $v_i$ to node $v_j$ with edge type $r_{i, j}$\n\\begin{figure}[ht!]\n\\centering\n\\vspace{-2mm}\n\\includegraphics[width=12.0cm]{IE_graph_v1.pdf}\n\\vspace{-4mm}\n\\caption{An example for IE graph construction which contains both the Co-reference process and the Open Information Extraction process.}\n\\label{fig:IE-graph-construction-sample}\n\\vspace{-2mm}\n\\end{figure}", "cites": [7216], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the AMR graph construction process and its characteristics, drawing from the cited paper but without synthesizing multiple sources or connecting broader ideas. It lacks critical evaluation of the approach or its limitations and offers minimal abstraction beyond the specific method. The inclusion of a figure and procedural explanation supports a descriptive rather than analytical or comparative role."}}
{"id": "76d5f0a1-9302-43e0-b736-b4b5922e08e5", "title": "Information Extraction Graph Construction", "level": "paragraph", "subsections": [], "parent_id": "9b13822c-0845-4444-b7bf-5e9d6126646e", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Construction Methods for NLP"], ["subsection", "Static Graph Construction"], ["subsubsection", "Static Graph Construction Approaches"], ["paragraph", "Information Extraction Graph Construction"]], "content": "\\label{subsubsec: IE Graph Construction}\nThe information extraction graph (IE Graph) aims to extract the structural information to represent the high-level information among natural sentences, e.g., text-based documents.\nThese extracted relations that capture relations across distant sentences have been demonstrated helpful in many NLP tasks~. In what follows, we will discuss the technical details on how to construct an IE graph for a given paragraph $para$~. We divide this process into two three basic steps: 1) coreference resolution, 2) constructing IE relations, and 3) graph construction.\n\\noindent\\emph{Step 1: Coreference Resolution.}\n\\label{sec: coreference-resolution}\nCoreference resolution is the basic procedure for information extraction task which aims to find expressions that refer to the same entities in the text sequence~. As shown in Figure \\ref{fig:IE-graph-construction-sample},\nthe name \"Pual\", the noun-term \"He\" and \"a renowned computer scientist\" may refer to the same object (person). Many NLP tools such as OpenIE~ provide coreference resolution function to achieve this goal. We denotes the coreference cluster $C$ as a set of phrases referring to the same object. Given a paragraph, one can obtain the coreference sets $\\mathcal{C} = \\{C_1, C_2, ..., C_n\\}$ extracting from unstructured data. \n\\noindent\\emph{Step 2: IE Relations.}\nTo construct an IE graph, the first step is to extract the triples from the paragraphs, which could be completed by leveraging some well-known information extraction systems (i.e. OpenIE~). We call each triple (subject, predicate, object) as a relation, which is denoted $(n_i, r_{i, j}, n_j) \\in \\mathcal{R}_{ie}$. It is worth noting if two triples differ only by one argument, and the other arguments overlap, one only keep the longer triple.\n\\noindent\\emph{Step 3: IE Graph Construction.} \nThe IE graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$ consists of the IE nodes and IE relations discussed above. Given the paragraph $para$ and the IE relation set $\\mathcal{R}_{ie}$, for each relation $(n_i, r_{i, j}, n_j) \\in \\mathcal{R}_{ie}$, one adds the nodes $v_i$ (for the subject $n_i$) and $v_j$ (for the object $n_j$) and add a directed edge from node $v_i$ to node $v_j$ with the corresponding predicate types~. And then, for each coreference cluster $C_i \\in \\mathcal{C}$, one may collapse all coreferential phrases in $C_i$ into one node. This could help greatly reduce the number of nodes and eliminate the ambiguity by keeping only one node.", "cites": [1089], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear, step-by-step description of how to construct an IE graph, and it integrates the concept of coreference resolution and relation extraction, as mentioned in the cited paper. However, it remains largely descriptive without comparing different methods or offering a critical evaluation of their strengths and weaknesses. It begins to abstract by generalizing the graph construction process, but the insight is limited in depth and scope."}}
{"id": "74b6eff5-b1a9-4175-b3d7-ec6118b8f69e", "title": "Discourse Graph Construction", "level": "paragraph", "subsections": [], "parent_id": "9b13822c-0845-4444-b7bf-5e9d6126646e", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Construction Methods for NLP"], ["subsection", "Static Graph Construction"], ["subsubsection", "Static Graph Construction Approaches"], ["paragraph", "Discourse Graph Construction"]], "content": "\\label{subsubsec: Discourse Graph Construction}\nMany NLP tasks suffer from long dependency challenge when the candidate document is too long. The discourse graph, which describes how two sentences are logically connected to one another, are proved effective to tackle such challenge~. In the following subsection, we will briefly discuss the discourse relations between given sentences and then introduce the general procedure to construct the discourse graphs~.\n\\noindent\\emph{Step 1: Discourse Relation.}\nThe discourse relations derive from the discourse analysis, which aims to identify sentence-wise ordering constraints over a set of sentences. Given two sentences $sent_i$ and $sent_j$, one can define the discourse relation as $(sent_i, sent_j)$, which represents the discourse relation \"sentence $sent_j$ can be placed after sentence $sent_i$.\" The discourse analysis has been explored for years, and many theories have been developed for modeling discourse relations such as the Rhetorical Structure Theory (RST)~ and G-Flow~. In many NLP tasks, given a document $doc$, one firstly segments $doc$ into sentences set $\\mathcal{V} = \\{sent_1, sent_2, ..., sent_m\\}$. Then one applies discourse analysis to get the pairwise discourse relation set denoted as $\\mathcal{R}_{sep} \\subseteq \\mathcal{V} \\times \\mathcal{V}$.\n\\noindent\\emph{Step 2: Discourse Graph.} \nThe discourse graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$ consists of the sentences nodes and discourse relations discussed above. Given the document $doc$ and the discourse relation set $\\mathcal{R}_{dis}$, for each relation $(sent_i, sent_j) \\in \\mathcal{R}_{dis}$, one adds the nodes $v_i$ (for the sentence $sent_i$) and $v_j$ (for the sentence $sent_j$) and add a directed edge from node $v_i$ to node $v_j$.\n\\begin{figure}[ht!]\n\\centering\n\\vspace{-2mm}\n\\includegraphics[width=12.0cm]{KG-v1.pdf}\n\\vspace{-4mm}\n\\caption{An example for knowledge graph construction, where the knowledge base (KB) used and the generated concept graph are both from the dataset \\textit{MetaQA}~.}\n\\label{fig:KG-graph-construction-sample}\n\\vspace{-2mm}\n\\end{figure}", "cites": [1073, 1067, 1054, 1053], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of discourse graph construction, following a two-step process. While it mentions related theories and references the cited papers, it lacks meaningful synthesis or integration of the works into a broader framework. There is minimal critical analysis or evaluation of the approaches, and no clear abstraction of overarching principles or patterns is presented."}}
{"id": "d65ac926-8bed-4c26-a660-f20717b94849", "title": "Knowledge Graph Construction", "level": "paragraph", "subsections": [], "parent_id": "9b13822c-0845-4444-b7bf-5e9d6126646e", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Construction Methods for NLP"], ["subsection", "Static Graph Construction"], ["subsubsection", "Static Graph Construction Approaches"], ["paragraph", "Knowledge Graph Construction"]], "content": "\\label{subsubsec: Knowledge Graph Construction}\nKnowledge Graph (KG) that captures entities and relations can greatly facilitate learning and reasoning in many NLP applications. In general, the KGs can be divided into two main categories depending on their graph construction approaches. Many applications treat the KG as the compact and interpretable intermediate representation of the unstructured data (e.g., the document)~. Conceptually, it is almost similar to the IE graph, which we have discussed previously. \nOn the other hand, many other works~ incorporate the existing knowledge bases such as YAGO~) and ConceptNet~ to further enhance the performance of downstream tasks~. \nIn what follows, we will briefly discuss the second category of KG from the view of the graph construction.\nThe KG can be denoted as $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$, which is usually constructed by elements in knowledge base. Formally, one defines the triple $(e_1, rel, e_2)$ as the basic elements in the knowledge base, in which $e_1$ is the source entity, $e_2$ is the target entity, and $rel$ is the relation type. Then one adds two nodes $v_1$ (for the source element $e_1$) and $v_2$ (for the target element $e_2$) in the KG and add a directed edge from node $v_1$ to node $v_2$ with edge type $rel$. An example of such KG is shown in Fig. \\ref{fig:KG-graph-construction-sample}.\nIt is worth noting that the KG plays different roles in various applications. In some applications (e.g. knowledge graph completion and knowledge base question answering), KG is always treated as part of the inputs. In this scenario~, \nresearchers typically use the whole KG $\\mathcal{G}$ as the learning object. But for some other applications (e.g. natural language translation), the KG can be treated as the data augmentation method. In this case, the whole KG such as ConceptNet~ is usually too large and noisy for some domain-specific applications~, and thus it is not suitable to use the whole graph as inputs. In contrast, as shown in Figure \\ref{fig:KG-graph-construction-sample}, \none instead usually constructs subgraphs from the given query (it is often the text-based inputs like the queries in the reading comprehension task) ~.\nThe construction methods could may vary dramatically in the literature. Here, we only present one representative method for illustration purpose~. The first thing for constructing KG is to fetch the term instances in the given query. Then, they could link the term instances to the concepts in the KG by some matching algorithms such as max-substring matching. The concepts are regarded as the initial nodes in the extracted subgraph. Next step is to fetch the 1-hop neighbors of the initial nodes in the KG. Additionally, one may calculate the relevance of the neighbors with the initial nodes by applying some graph node relevance model such as the Personalized PageRank (PPR) algorithm~. Then based on the results, one may further prune out the edges with relevance score that is below the confidence threshold and remove the isolated neighbors. The remaining final subgraph is then used to feed any graph representation learning module later.", "cites": [1061, 7045, 1076, 7046, 1079, 57, 1089, 8408], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a coherent explanation of how knowledge graphs are constructed for NLP tasks and integrates examples from the literature to illustrate common practices. It connects ideas from multiple papers to describe general patterns, such as using existing knowledge bases for augmentation and dynamic subgraph construction. However, it offers limited critical analysis of the methods' strengths or weaknesses and does not fully abstract to reveal deeper principles or trends across the cited works."}}
{"id": "4e39c55f-739d-4d45-b415-a1eea1d88c2a", "title": "Coreference Graph Construction", "level": "paragraph", "subsections": [], "parent_id": "9b13822c-0845-4444-b7bf-5e9d6126646e", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Construction Methods for NLP"], ["subsection", "Static Graph Construction"], ["subsubsection", "Static Graph Construction Approaches"], ["paragraph", "Coreference Graph Construction"]], "content": "\\label{subsubsec: Coreference Graph Construction}\nIn linguistics, coreference (or co-reference) occurs when two or more terms in a given paragraph refer to the same object. Many works have demonstrated that such phenomenon is helpful for better understanding the complex structure and logic of the corpus and resolve the ambiguities~.\nTo effectively leverage the coreference information, the coreference graph is constructed to explicitly model the implicit coreference relations. Given a set of phrases, a coreference graph can link the nodes (phrases) which refer to the same entity in the text corpus. In the following subsection, we focus on the coreference graph construction for a paragraph $para$ consisting of $m$ sentences. We will briefly discuss the coreference relation and then discuss the approaches for building the coreference graph in various NLP tasks~. It is worth noting that although it is similar to the IE graph's first step, the coreference graph will explicitly model the coreference relationship by graph instead of collapse into one node.\n\\noindent\\emph{Step 1: Coreference Relation.}\nThe coreference relations can be obtained easily by the coreference resolution system, as discussed in IE graph construction. Similarly, we can obtain the coreference clusters $\\mathcal{C}$ given a specific paragraph. All phrases in a cluster $C_i \\in \\mathcal{C}$ refer to the same object.\n\\noindent\\emph{Step 2: Coreference Graph.}\nThe coreference graph are built on the coreference relation set $\\mathcal{R}_{coref}$. It can be generally divided into two main category depending on the node type: 1) phrases (or mentions)~, 2) words~. For the first class, the coreference graph $\\mathcal{G}$ consists of all mentions in relation set $\\mathcal{R}_{coref}$. For each phrase pair $p_i, p_j$ in cluster $C_k \\in \\mathcal{C}$, one may add an undirected edge between node $v_i$ (for the phrase $p_i$) and node $v_j$ (for the phrase $p_j$). For the second case, the coreference graph $\\mathcal{G}$ consists of words. One minor difference is that one only links the first word of each phrase for each associated phrases.\n\\begin{figure}[ht!]\n\\centering\n\\vspace{0mm}\n\\includegraphics[width=10.0cm]{similarity_graph_v2.pdf}\n\\vspace{-4mm}\n\\caption{An example for similarity graph construction. We use sentences as nodes and initialize their features with \\textit{TF-IDF} vectors.}\n\\label{fig:sim-graph-construction-sample}\n\\vspace{-2mm}\n\\end{figure}", "cites": [1074, 1067, 9148, 1090, 1085, 57], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of coreference graph construction methods and outlines a two-step process, but it lacks deep synthesis or analysis of the cited papers. While it mentions that coreference graphs differ from IE graphs, it does not explore these differences in detail or connect them to broader trends. The section remains largely descriptive with minimal critique or abstraction."}}
{"id": "9090473e-8067-45ac-9f8b-573ec3b32e80", "title": "Similarity Graph Construction", "level": "paragraph", "subsections": [], "parent_id": "9b13822c-0845-4444-b7bf-5e9d6126646e", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Construction Methods for NLP"], ["subsection", "Static Graph Construction"], ["subsubsection", "Static Graph Construction Approaches"], ["paragraph", "Similarity Graph Construction"]], "content": "\\label{subsubsec: Similarity Graph Construction}\nThe similarity graphs aim to quantify the similarity between nodes, which are widely used in many NLP tasks~.\nSince the similarity graph is typically application-oriented, we focus on the basic procedure of constructing the similarity graph for various types of elements such as entities, sentences and documents, and neglect the application specific details. It is worth noting that the similarity graph construction is conducted during preprocessing and is not jointly trained with the remaining learning system in an end-to-end manner.\nOne example of similarity graph is shown in Fig. \\ref{fig:sim-graph-construction-sample}. \n\\noindent\\emph{Step 1: Similarity Graph.}\nGiven a corpus $C$, in a similarity graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$, the graph nodes can be defined in different granularity levels such as entities, sentences and documents. We denote the basic node set as $\\mathcal{V}$ regardless of specific node types. One can calculate the node features by various mechanisms such as TF-IDF for sentences (or documents)~ and embeddings for entities~. Then, similarity scores between node pairs can be computed by various metrics such as cosine similarity~, and used to indicate edge weights of the node pairs.\n\\noindent\\emph{Step 2: Sparse mechanism.}\nThe initial similarity graph is typically dense even some edge weights are very small or even negative. These values can be treated as noise, which plays little roles in the similarity graph. Thus various sparse techniques are proposed to further improve the quality of graph by sparsifying a graph. One widely used sparse method is k-NN~. Specifically, for node $v_i$ and its' neighbor set $N(v_i)$, one only reserves edges by keeping $k$ largest edge weights and dropping the remaining edges. The other widely used method is $\\epsilon-$sparse~. In particular, one will remove the edges whose weights are smaller than the certain threshold $\\epsilon$.\n\\begin{figure}[ht!]\n\\centering\n\\vspace{-2mm}\n\\includegraphics[width=12.0cm]{co-occurrence-v1.pdf}\n\\vspace{-6mm}\n\\caption{An example for co-occurrence graph construction where edge weights stand for the co-occurrence frequency between words. We set the window size as 3.}\n\\label{fig:co-occurrence-graph-construction-sample}\n\\vspace{-4mm}\n\\end{figure}", "cites": [1053, 1062], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of similarity graph construction, outlining general steps and mechanisms without deeply connecting the cited papers to broader themes or trends. While it does mention application-oriented aspects and preprocessing, it lacks critical evaluation or comparison of the methods used in the cited works. Some abstraction is attempted through general definitions of nodes and edges, but the overall analysis remains at a surface level."}}
{"id": "c5e55b37-a72a-4687-a7e5-52b98d173d8a", "title": "Co-occurrence Graph Construction", "level": "paragraph", "subsections": [], "parent_id": "9b13822c-0845-4444-b7bf-5e9d6126646e", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Construction Methods for NLP"], ["subsection", "Static Graph Construction"], ["subsubsection", "Static Graph Construction Approaches"], ["paragraph", "Co-occurrence Graph Construction"]], "content": "\\label{subsubsec: Co-occurrence Graph Construction}\nThe co-occurrence graph aims to capture the co-occurrence relation between words in the text, which is widely used in many NLP tasks\n~.\nThe co-occurrence relation, which describes the frequency of two words that co-occur within a fix-sized context window, is an important feature capturing the semantic relationship among words in the corpus. In what follows, we will first present the approaches of obtaining the co-occurrence relations and then discuss the basic procedure of building a co-occurrence graph for a corpus $C$. An example of co-occurrence graph can be seen in Fig. \\ref{fig:co-occurrence-graph-construction-sample}.\n\\noindent\\emph{Step 1: Co-occurrence Relation.}\nThe co-occurrence relation is defined by the co-occurrence matrix of the given corpus $C$. For a specific paragraph $para$ consists of $m$ sentences, the co-occurrence matrix describes how words occur together. One may denote the co-occurrence the matrix as $\\mathbf{M} \\in \\mathbb{R}^{|V|\\times |V|}$, where $|V|$ is the vocabulary size of $C$. $\\mathbf{M}_{w_i, w_j}$ describes how many times word $w_i$, $w_j$ occur together within a fix-size sliding windows in the corpus $C$. After obtaining the co-occurrence matrix, there are two typical methods to calculate the weights between words: 1) co-occurrence frequency~ and 2) point-wise mutual information (PMI)~.\n\\noindent\\emph{Step 2: Co-occurrence Graph.}\nThe co-occurrence graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$ consists of the words nodes and co-occurrence relations discussed above. Given the corpus $C$ and the co-occurrence relation set $\\mathcal{R}_{co}$, for each relation $(w_i, w_j) \\in \\mathcal{R}_{co}$, one adds the nodes $v_i$ (for the word $w_i$) and $v_j$ (for the word $w_j$) and add an undirected edge from node $v_i$ to node $v_j$ initialized with the aforementioned calculated edge weights.\n\\begin{figure}[ht!]\n\\centering\n\\vspace{-2mm}\n\\includegraphics[width=12.0cm]{topic_graph_v1.pdf}\n\\vspace{-4mm}\n\\caption{An example for topic graph construction, where the dash line stands for the topic modeling process by leveraging the LDA algorithm on dataset \\textit{AG news}~.}\n\\label{fig:topic-graph-construction-sample}\n\\vspace{-4mm}\n\\end{figure}", "cites": [1081, 8410, 7327, 1096], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic explanation of co-occurrence graph construction, describing the process and the use of co-occurrence frequency and PMI. It cites papers related to graph-based text classification and topic modeling but does not effectively synthesize their contributions or relate them to one another. There is minimal critical evaluation or abstraction into broader principles."}}
{"id": "13cb7d8d-44ef-4f9f-8a75-5f1d16cac2b1", "title": "Topic Graph Construction", "level": "paragraph", "subsections": [], "parent_id": "9b13822c-0845-4444-b7bf-5e9d6126646e", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Construction Methods for NLP"], ["subsection", "Static Graph Construction"], ["subsubsection", "Static Graph Construction Approaches"], ["paragraph", "Topic Graph Construction"]], "content": "\\label{subsubsec: Topic Graph Construction}\nThe topic graph is built on several documents, which aims to model the high-level semantic relations among different topics~. In particular, given a set of documents $\\mathcal{D} = \\{doc_1, doc_2, ..., doc_m\\}$, one first learns the latent topics denoted as $\\mathcal{T}$ using some topic modeling algorithms such as LDA~. Then one could construct the topic graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$ with $\\mathcal{V} = \\mathcal{D} \\cup \\mathcal{T}$. The undirected edge between the node $v_i$ (for a document) and the node $v_j$ (for a topic) is built only if the document has that topic. An example of topic graph is shown in Fig. \\ref{fig:topic-graph-construction-sample}.\n\\begin{figure}[ht!]\n\\centering\n\\includegraphics[width=12.0cm]{app_driven_v1.pdf}\n\\vspace{-2mm}\n\\caption{An example for application-driven graph construction, which is specially designed for SQL query input.}\n\\label{fig:application-driven-graph-construction-sample}\n\\vspace{-2mm}\n\\end{figure}", "cites": [1073], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of how topic graphs are constructed, citing one paper that uses graph representations in summarization. However, it lacks synthesis of multiple sources, critical evaluation of the methods or limitations, and does not generalize to broader patterns or principles in graph construction for NLP."}}
{"id": "02ee7b2b-cf34-4f76-9200-548de38c631c", "title": "App-driven Graph Construction", "level": "paragraph", "subsections": [], "parent_id": "9b13822c-0845-4444-b7bf-5e9d6126646e", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Construction Methods for NLP"], ["subsection", "Static Graph Construction"], ["subsubsection", "Static Graph Construction Approaches"], ["paragraph", "App-driven Graph Construction"]], "content": "\\label{subsubsec: App-driven Graph Construction}\nThe app-driven graphs refer to the graph specially designed for specific NLP tasks\n~, \nwhich cannot be trivially covered by the previously discussed static graph types. In some NLP tasks, it is common to represent unstructured data by structured formation with application-specific approaches. For example, the SQL language can be naturally represented by the SQL parsing tree. Thus it can be converted to the SQL graph~. Since these graphs are too specialized based on the domain knowledge, there are no unified pattern to summarize how to build an app-driven graph. An example of such application-driven graph like SQL graph is illustrated in Fig. \\ref{fig:application-driven-graph-construction-sample}. In Sec. \\ref{sec:Applications}, we will further discuss how these graph construction methods are used in various popular NLP tasks.", "cites": [1054], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of app-driven graph construction, using an example from the cited paper (SQL graph), but does not meaningfully synthesize multiple sources or establish a broader framework. There is minimal critical analysis of the approaches, and the section remains largely concrete without identifying generalizable patterns or principles."}}
{"id": "a2c15e90-9035-4e7d-a2b8-c01a8c92d0f6", "title": "Hybrid Graph Construction and Discussion", "level": "subsubsection", "subsections": [], "parent_id": "08dbe9ba-d362-478e-808b-dc898c7c876a", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Construction Methods for NLP"], ["subsection", "Static Graph Construction"], ["subsubsection", "Hybrid Graph Construction and Discussion"]], "content": "Most previous static graph construction methods only consider one specific relation between nodes. Although the obtained graphs capture the structure information well to some extent, they are also limited in exploiting different types of graph relations. To address this limitation, there is an increasing interest in building a hybrid graph by combing several graphs together in order to enrich the semantic information in graph~. The method of constructing a hybrid graph is mostly application specific, and thus we only present some representative approach for such a graph construction.\nTo capture multiple relations, a common strategy is to construct a heterogeneous graph, which contains multiple types of nodes and edges. Without losing generality, we assume that one may create a hybrid graph $\\mathcal{G}_{hybrid}$ with two different graph sources $\\mathcal{G}_{a}(\\mathcal{V}_{a},\\mathcal{E}_{a})$ and  $\\mathcal{G}_{b}(\\mathcal{V}_{b}, \\mathcal{E}_{b})$. Graphs $a, b$ are two different graph types such as \\textit{dependency graph} and \\textit{constituency graph}. Given these textual inputs, if $\\mathcal{G}_{a}$ and $\\mathcal{G}_{b}$ share the same node sets (i.e., $\\mathcal{V}_{a}$ = $\\mathcal{V}_{b}$), we merge the edge sets by annotating relation-specific edge types~. Otherwise, we merge the $\\mathcal{V}_{a}$ and $\\mathcal{V}_{b}$ to get the hybrid node set, denoted as $\\mathcal{V} = \\mathcal{V}_{a} \\cup \\mathcal{V}_{b}$~. Then we generate $\\mathcal{E}_{a}$ and $\\mathcal{E}_{b}$ to $\\mathcal{E}$ by mapping the source and target nodes from $\\mathcal{V}_{a}$ and $\\mathcal{V}_{b}$ to $\\mathcal{V}$.", "cites": [1074, 8410, 1096, 1067], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of hybrid graph construction, identifying a common limitation in prior methods and proposing a general strategy. It synthesizes concepts from multiple papers to explain how heterogeneous graphs can be formed, but the critical analysis is limited, focusing more on summarizing the approach rather than evaluating its effectiveness or drawbacks. The abstraction level is moderate as it outlines a general framework for merging graphs but does not reach deeper meta-level insights."}}
{"id": "db9a17b8-fb06-450f-b43b-a86de3caa5fd", "title": "Dynamic Graph Construction", "level": "subsection", "subsections": ["bc8f9604-8aae-4fac-8173-b39bc1a51c5f", "7a2f1911-f697-4b6d-a6f0-04f1a4cee37f", "73a7501e-8f75-4b3a-986e-ae73269900ef", "0229e443-2fc8-4153-bab8-d8bba7541aa7"], "parent_id": "58441071-326c-4d8d-9f6f-affbd1498d95", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Construction Methods for NLP"], ["subsection", "Dynamic Graph Construction"]], "content": "\\label{subsec:Dynamic Graph Construction}\nAlthough static graph construction has the advantage of encoding prior knowledge of the data into the graph structure, it has several limitations.\nFirst of all, extensive human efforts and domain expertise are needed in order to construct a reasonably performant graph topology.\nSecondly, the manually constructed graph structure might be error-prone (e.g., noisy or incomplete). Thirdly, since the graph construction stage and graph representation learning stage are disjoint, the errors introduced in the graph construction stage cannot be corrected and might be accumulated to later stages, which can result in degraded performance.\nLastly, the graph construction process is often informed solely by the insights of the NLP practitioners, and might be sub-optimal for the downstream prediction task.\n\\begin{figure}[thb!]\n\\centering\n\\vspace{-2mm}\n\\includegraphics[keepaspectratio=true,scale=0.15]{dynamic_graph_overall.pdf}\n\\vspace{-4mm}\n\\caption{Overall illustration of dynamic graph construction approaches. Dashed lines (in data points on left) indicate the optional intrinsic graph topology.}\n\\label{fig:dynamic_graph_overall}\n\\vspace{-0mm}\n\\end{figure}\nIn order to tackle the above challenges, recent attempts on GNN for NLP~ have explored dynamic graph construction without resorting to human efforts or domain expertise.\nMost dynamic graph construction approaches aim to dynamically learn the graph structure (i.e., a weighted adjacency matrix) on the fly, and the graph construction module can be jointly optimized with the subsequent graph representation learning modules toward the downstream task in an end-to-end manner.\nOne good example of dynamic graph construction is when constructing a graph capturing the semantic relationships among all the words in a text passage in the task of conversational machine reading comprehension~, instead of building a fixed static graph based on domain expertise, one can jointly train a graph structure learning module together with the graph embedding learning module in order to learn an optimal graph structure considering not only the semantic meanings of the words but also the conversation history and current question.\nAs shown in ~\\cref{fig:dynamic_graph_overall}, these dynamic graph construction approaches typically consist of a graph similarity metric learning component for learning an adjacency matrix by considering pair-wise node similarity in the embedding space, and a graph sparsification component for extracting a sparse graph from the learned fully-connected graph. \nIt is reported to be beneficial to combine the intrinsic graph structures and learned implicit graph structures for better learning performance~.\nMoreover, in order to effectively conduct the joint graph structure and representation learning, various learning paradigms have been proposed.\nIn what follows, we will discuss all these effective techniques for dynamic graph construction.\nMore broadly speaking, graph structure learning for GNNs itself is a trending research problem in the machine learning field, and has been actively studied beyond the NLP community~. However, in this survey, we will focus on its recent advances in the NLP field.\nWe hereafter use dynamic graph construction and graph structure learning interchangeably.", "cites": [1077, 1100, 1080, 7047, 1099, 1097, 1058, 1098, 241, 1071], "cite_extract_rate": 1.0, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key ideas from multiple papers to present a coherent narrative on dynamic graph construction, highlighting common components like similarity metric learning and sparsification. It provides critical insights by identifying limitations of static graph methods and how dynamic approaches address them. Furthermore, it abstracts beyond individual papers to describe broader trends in joint graph structure and representation learning within the NLP domain."}}
{"id": "4e33dc81-74b2-4825-8dd1-35c50bed664b", "title": "Node Embedding Based Similarity Metric Learning", "level": "paragraph", "subsections": [], "parent_id": "bc8f9604-8aae-4fac-8173-b39bc1a51c5f", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Construction Methods for NLP"], ["subsection", "Dynamic Graph Construction"], ["subsubsection", "Graph Similarity Metric Learning Techniques"], ["paragraph", "Node Embedding Based Similarity Metric Learning"]], "content": "Node embedding based similarity metric functions are designed to learn a weighted adjacency matrix by computing the pair-wise node similarity in the embedding space. Common metric functions include attention-based metric functions and cosine-based metric functions.\n\\noindent\\textbf{Attention-based Similarity Metric Functions.}\nMost of the similarity metric functions proposed so far are based on the attention mechanism~. \nIn order to increase the learning capacity of dot product based attention mechanism,  proposed a modified dot product by introducing learnable parameters, formulated as follows:\n\\begin{equation}\n\\begin{aligned}\nS_{i,j} = (\\vec{v}_i \\odot \\vec{u})^T \\vec{v}_j\n\\end{aligned}\n\\end{equation}\nwhere $\\vec{u}$ is a non-negative weight vector learning to highlight different dimensions of the node embeddings, and $\\odot$ denotes element-wise multiplication. \nSimilarly,  designed a more expressive version of dot product by introducing a learnable weight matrix, formulated as follows:\n\\begin{equation}\n\\begin{aligned}\nS_{i,j} = \\mathrm{ReLU}(\\vec{W} \\vec{v}_i)^T \\mathrm{ReLU}(\\vec{W} \\vec{v}_j)\n\\end{aligned}\n\\end{equation}\nwhere $\\vec{W}$ is a $d \\times d$ weight matrix, and $\\mathrm{ReLU}(x) = \\max(0, x)$ is a rectified linear unit (ReLU)~ used to enforce the sparsity of the similarity matrix.\n\\noindent\\textbf{Cosine-based Similarity Metric Functions.}\n extended the vanilla cosine similarity to a multi-head weighted cosine similarity to capture pair-wise node similarity from multiple perspectives, formulated as follows:\n\\begin{equation}\n\\begin{aligned}\nS_{i,j}^p &= \\mathrm{cos}(\\vec{w}_p \\odot \\vec{v}_i, \\vec{w}_p \\odot \\vec{v}_j)\\\\\nS_{i,j} &= \\frac{1}{m}\\sum_{p=1}^{m}{S_{ij}^p}\n\\end{aligned}\n\\end{equation}\nwhere $\\vec{w}_p$ is a weight vector associated to the $p$-th perspective, and has the same dimension as the node embeddings.\nIntuitively, $S_{i,j}^p$ computes the pair-wise cosine similarity for the $p$-th perspective where each perspective considers one part of the semantics captured in the embeddings.\nBesides increasing the learning capacity, employing multi-head learners is able to stabilize the learning process, which has also been observed in~.", "cites": [180, 168, 1058, 1071, 38], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of node embedding-based similarity metric learning, distinguishing between attention-based and cosine-based methods. While it references multiple papers, it does not critically analyze their contributions or limitations, nor does it abstract broader principles. The synthesis is minimal, with only surface-level connections between the methods described."}}
{"id": "222e5e1c-ae42-45ad-b630-7b368ae21373", "title": "Structure-aware Similarity Metric Learning", "level": "paragraph", "subsections": [], "parent_id": "bc8f9604-8aae-4fac-8173-b39bc1a51c5f", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Construction Methods for NLP"], ["subsection", "Dynamic Graph Construction"], ["subsubsection", "Graph Similarity Metric Learning Techniques"], ["paragraph", "Structure-aware Similarity Metric Learning"]], "content": "\\label{sec:graph-construction-structure-aware}\nInspired by structure-aware transformers~, recent approaches employ structure-aware similarity metric functions that additionally consider the existing edge information of the intrinsic graph beyond the node information. For instance,\n proposed a structure-aware attention mechanism for learning pair-wise node similarity, formulated as follows:\n\\begin{equation}\n\\begin{aligned}\n     S_{i, j}^l = \\mathrm{softmax}(\\vec{u}^T \\mathrm{tanh}(\\vec{W}[\\vec{h}^l_i, \\vec{h}^l_j, \\vec{v}_i, \\vec{v}_j, \\vec{e}_{i,j}]))\n\\end{aligned}\n\\end{equation}\nwhere $\\vec{v}_i$ represents the embedding of node $i$, $\\vec{e}_{i,j}$ represents the embedding of the edge connecting node $i$ and $j$, $\\vec{h}^l_i$ is the embedding of node $i$ in the $l$-th GNN layer, and $\\vec{u}$ and $\\vec{W}$ are trainable weight vector and weight matrix, respectively.   \nSimilarly,  introduced a structure-aware global attention mechanism, formulated as follows,\n\\begin{equation}\n\\begin{aligned}\nS_{i, j} = \\frac{\\mathrm{ReLU}(\\vec{W}^Q \\vec{v}_i )^T (\\mathrm{ReLU}( \\vec{W}^K \\vec{v}_i)+ \\mathrm{ReLU}(\\vec{W}^R \\vec{e}_{i, j}))}{\\sqrt{d}}\n\\end{aligned}\n\\end{equation}\nwhere $\\vec{e}_{i, j}$ is the embedding of the edge connecting node $i$ and $j$, and $\\vec{W}^Q$, $\\vec{W}^K$, and $\\vec{W}^R$ are linear transformations that map the node and edge embeddings to the latent embeddding space.", "cites": [1080, 1077, 7044, 8411], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of structure-aware similarity metric learning techniques in GNNs for NLP, presenting key formulations from the cited papers. While it attempts to link these approaches under the broader theme of incorporating graph structure, it lacks deeper synthesis across the works or a critical evaluation of their strengths and limitations. The abstraction is limited to the level of the models described without broader conceptual generalization."}}
{"id": "7a2f1911-f697-4b6d-a6f0-04f1a4cee37f", "title": "Graph Sparsification Techniques", "level": "subsubsection", "subsections": [], "parent_id": "db9a17b8-fb06-450f-b43b-a86de3caa5fd", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Construction Methods for NLP"], ["subsection", "Dynamic Graph Construction"], ["subsubsection", "Graph Sparsification Techniques"]], "content": "Most graphs in real-world scenarios are sparse graphs.\nSimilarity metric functions consider relations between any pair of nodes and returns a fully-connected graph, which is not only computationally expensive but also might introduce noise such as unimportant edges.\nTherefore, it can be beneficial to explicitly enforce sparsity to the learned graph structure. \nBesides applying the $\\mathrm{ReLU}$ function in the similarity metric functions~, various graph sparsification techniques have been adopted to enhance the sparsity of the learned graph structure.\n applied a kNN-style sparsification operation to obtain a sparse adjacency matrix from the node similarity matrix computed by the similarity metric learning function, formulated as follows:\n\\begin{equation}\n\\begin{aligned}\n\\vec{A}_{i,:} = \\mathrm{topk}(\\vec{S}_{i,:})\n\\end{aligned}\n\\end{equation}\nwhere for each node, only the $K$ nearest neighbors (including itself) and the associated similarity scores are kept, and the remaining similarity scores are masked off.\n enforced a sparse adjacency matrix by considering only the $\\varepsilon$-neighborhood for each node, formulated as follows:\n\\begin{equation}\n\\begin{aligned}\nA_{i,j} = \n\\left\\{\n        \\begin{array}{ll}\n             S_{i,j} & \\quad  S_{i,j} > \\varepsilon  \\\\\n              0 & \\quad \\text{otherwise}\n        \\end{array}\n    \\right.\n\\end{aligned}\n\\end{equation}\nwhere those elements in $S$ which are smaller than a non-negative threshold $\\varepsilon$ are all masked off (i.e., set to zero).\nBesides explicitly enforcing the sparsity of the learned graph by applying certain form of threshold, sparsity has also been enforced implicitly in a learning-based manner. \n introduced the following regularization term to encourage sparse graphs.\n\\begin{equation}\n\\begin{aligned}\n\\frac{1}{n^2} ||A||_F^2\n\\end{aligned}\n\\end{equation}\nwhere $||\\cdot||_F$ denotes the Frobenius norm of a matrix.", "cites": [1058, 1077, 1071], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of graph sparsification techniques and mentions three papers, but it lacks in-depth synthesis, critical evaluation, or abstraction. It mainly outlines methods (e.g., top-k selection, epsilon thresholding, Frobenius norm regularization) without connecting them to broader trends or discussing their relative strengths and weaknesses."}}
{"id": "73a7501e-8f75-4b3a-986e-ae73269900ef", "title": "Combining Intrinsic Graph Structures and Implicit Graph Structures", "level": "subsubsection", "subsections": [], "parent_id": "db9a17b8-fb06-450f-b43b-a86de3caa5fd", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Construction Methods for NLP"], ["subsection", "Dynamic Graph Construction"], ["subsubsection", "Combining Intrinsic Graph Structures and Implicit Graph Structures"]], "content": "Recent studies~ have shown that it could hurt the downstream task performance if the intrinsic graph structure is totally discard while doing dynamic graph construction. This is probably because the intrinsic graph typically still carries rich and useful information regarding the optimal graph structure for the downstream task.\nThey therefore proposed to combine the learned implicit graph structure with the intrinsic graph structure based on the assumption that the learned implicit graph is potentially a ``shift'' (e.g., substructures) from the intrinsic graph structure which is supplementary to the intrinsic graph structure.\nThe other potential benefit is incorporating the intrinsic graph structure might help accelerate the training process and increase the training stability. Since there is no prior knowledge on the similarity metric and the trainable parameters are randomly initialized, it may usually take long time to converge.\nDifferent ways for combining intrinsic and implicit graph structures have been explored.\nFor instance,  proposed to compute a linear combination of the normalized graph Laplacian of the intrinsic graph structure $L^{(0)}$ and the normalized adjacency matrix of the implicit graph structure $\\mathrm{f}(A)$, formulated as follows:\n\\begin{equation}\n\\begin{aligned}\n\\widetilde{A} = \\lambda L^{(0)} + (1 - \\lambda) \\mathrm{f}(A)\n\\end{aligned}\n\\end{equation}\nwhere $\\mathrm{f}: \\mathbb{R}^{n \\times n} \\to \\mathbb{R}^{n \\times n}$ can be arbitrary normalization operations such as graph Laplacian operation~ and row-normalization operation~.\nInstead of explicitly fusing the two graph adjacency matrices,  proposed a hybrid message passing mechanism for GNNs which fuses the two aggregated node vectors computed from the intrinsic graph and the learned implicit graph, respectively, and then feed the fused vector to a GRU to update node embeddings.", "cites": [241, 1077, 1058], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from the cited papers by discussing the integration of intrinsic and implicit graph structures, and presents a coherent narrative around their combination. It identifies some benefits, such as training stability, but provides only basic critical analysis without in-depth evaluation of limitations or trade-offs. The abstraction is moderate, as it generalizes the concept of graph fusion but does not elevate it to a higher-level meta-framework."}}
{"id": "0229e443-2fc8-4153-bab8-d8bba7541aa7", "title": "Learning Paradigms", "level": "subsubsection", "subsections": [], "parent_id": "db9a17b8-fb06-450f-b43b-a86de3caa5fd", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Construction Methods for NLP"], ["subsection", "Dynamic Graph Construction"], ["subsubsection", "Learning Paradigms"]], "content": "Most existing dynamic graph construction approaches for GNNs consist of two key learning components: graph structure learning (i.e., similarity metric learning) and graph representation learning (i.e., GNN module), and the ultimate goal is to learn the optimized graph structures and representations with respect to certain downstream prediction task. \nHow to optimize the two separate learning components toward the same ultimate goal becomes an important question.\nHere we highlight three representative learning paradigms.\nThe most straightforward strategy~ is to jointly optimize the whole learning system in an end-to-end manner toward the downstream (semi-)supervised prediction task.\nAnother common strategy~ is to adaptively learn the input graph structure to each stacked GNN layer to reflect the changes of the intermediate graph representations.\nThis is similar to how transformer models learn different weighted fully-connected graphs in each layer.\nUnlike the above two paradigms,  proposed an iterative graph learning framework by learning a better graph structure based on better graph representations, and in the meantime, learning better graph representations based on a better graph structure in an iterative manner.\nAs a result, this iterative learning paradigm repeatedly refines the graph structure and the graph representations toward the optimal downstream performance.", "cites": [1077, 1080, 1058, 1071, 1101], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes information by identifying a shared framework across papers and distinguishing three learning paradigms. It abstracts these approaches into a general structure, showing how different strategies address the interplay between graph structure and representation learning. While it offers a clear analytical perspective, the critique of the cited works remains somewhat surface-level, focusing on distinctions rather than deeper evaluation of limitations or trade-offs."}}
{"id": "84ae95bb-0c80-43ad-93fa-77cbd83dd087", "title": "Graph Representation Learning for NLP", "level": "section", "subsections": ["05b37ed2-d1bf-4bc3-bb21-9856bfc938ea", "46a0f611-f8ab-490e-9497-294bad0cd456", "3fffd4e3-dac8-4eb5-86ed-f1cb44bf8217"], "parent_id": "d9782043-9373-462c-b592-7d9a42b0beee", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Representation Learning for NLP"]], "content": "\\label{sec:Graph Representation Learning for NLP}\nIn the previous section, we have presented various graph construction methods, including static graph construction and dynamic graph construction. \nIn this section, we will discuss various graph representation learning techniques that are directly operated on the constructed graphs for various NLP tasks. \nThe goal of graph representation learning is to find a way to incorporate information of graph structures and attributes into a low-dimension embeddings via a machine learning model~. To mathematically formalize this problem, we give the mathematical notations for arbitrary graphs as $\\mathcal{G}(\\mathcal{V}, \\mathcal{E}, \\mathcal{T}, \\mathcal{R})$, where $\\mathcal{V}$ is the node set, $\\mathcal{E}$ is the edge set, $\\mathcal{T} = \\{T_1, T_2, ..., T_{p}\\}$ is the collection of node types, and $\\mathcal{R} = \\{R_1, ..., R_q\\}$ is the collection of edge types. $|\\cdot|$ is the number of elements. $\\tau(\\cdot) \\in \\mathcal{T}$ is the node type indicator function (e.g., $\\tau(v_i) \\in \\mathcal{T}$ is the type of node $v_i$), and $\\phi(\\cdot) \\in \\mathcal{R}$ is the edge type indicator function (e.g., $\\phi(e_{i, j}) \\in \\mathcal{R}$ is the type of edge $e_{i, j}$), respectively.\nGenerally speaking, the constructed graphs from the raw text data are either homogeneous or heterogeneous graphs. Thus, in \\cref{sec:graph_representation_learning_homogeneous}, we will discuss various graph representation learning methods for homogeneous graphs, including scenarios for the original homogeneous graph and some converting from heterogeneous graphs. In \\cref{sec:graph_representation_learning_heterogeneous}, we will discuss the GNN-based methods for multi-relational graphs, and in \\cref{subsec:graph-representation-learning-heterogeneousgnn}, we will discuss the GNNs for dealing with the heterogeneous graphs.", "cites": [7007], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section introduces the concept of graph representation learning and provides mathematical notations for graph structures. However, it does not deeply synthesize or integrate insights from the cited paper; it merely sets up the context without elaborating on the methods or comparing them. There is minimal critical analysis or abstraction to broader principles, making the section largely descriptive."}}
{"id": "0cadd55f-608f-41a0-a92f-842faef03783", "title": "Converting Edge Information to Adjacent Matrix", "level": "paragraph", "subsections": [], "parent_id": "377cbbd2-240b-45da-b504-9b8eac67141e", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Representation Learning for NLP"], ["subsection", "GNNs for Homogeneous Graphs"], ["subsubsection", "Static Graph: Treating Edge Information as Connectivity"], ["paragraph", "Converting Edge Information to Adjacent Matrix"]], "content": "Basically, the edges are viewed as the connection information between nodes. In this case, it is normal to discard the edge type information and retain the connections to convert the heterogeneous graphs~ to homogeneous graphs. After obtaining such a graph, we can represent the topology of the graph as a unified adjacency matrix $A$. Specifically, for an edge $e_{i, j} \\in \\mathcal{E}$ which connect node $v_i$ and $v_j$, $A_{i,j}$ denotes to the edge weight for weighted static graph, or $A_{i,j}=1$ for unweighted connections and $A_{i,j}=0$ otherwise. \nThe static graphs can also be divided into directed and undirected graphs. For the undirected case, the adjacency matrix is symmetric matrix, which means $A_{i, j} = A_{j,i}$. And for the other case, it is always not symmetric. The $A_{i, j}$ is strictly defined by the edge from node $v_i$ to node $v_j$. It is worth noting that the directed graphs can be transformed to undirected graphs~ by averaging the edge weights in both directions. The edge weights are rescaled, whose maximum edge weight is 1 before feeding to the GNN.", "cites": [1064, 7328, 8410, 1053, 8408], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive explanation of how edge information is converted to an adjacency matrix in static graphs but does not deeply integrate or synthesize insights from the cited papers. It lacks critical evaluation or comparative analysis of the approaches taken in these works and offers minimal abstraction beyond the technical definition of adjacency matrices in graph structures."}}
{"id": "54d74677-71ed-4f24-b6d7-5542787007e7", "title": "Node Representation Learning", "level": "paragraph", "subsections": [], "parent_id": "377cbbd2-240b-45da-b504-9b8eac67141e", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Representation Learning for NLP"], ["subsection", "GNNs for Homogeneous Graphs"], ["subsubsection", "Static Graph: Treating Edge Information as Connectivity"], ["paragraph", "Node Representation Learning"]], "content": "Next, given initial node embedding $\\mathbf{X}$ and adjacency matrix $A$, the node representation is extracted base on the classical GNNs techniques. \nFor undirected graphs, most works~ mainly adopt graph representation learning algorithms such as GCN, GGNN, GAT, GraphSage, etc. and stack them to explore deep semantic correlations in the graph. When it comes to the directed graphs, few GNN methods such as GGNN, GAT still work~. While for the other GNNs that can not be directly applied into directed graphs, the simple strategy is to ignore the directions (i.e., converting the directed graphs to undirected graphs)~. However, such methods allow the message to propagate in both directions without constraints. To solve this problem, many efforts have been made to adapt the GNN to directed graphs. For GCN, some spatial-based GCN algorithms are designed for directed graphs such as DCNN~. GraphSage can be easily extended to directed graphs by modifying the aggregation function via specifying the edge directions and aggregating them separately)~.", "cites": [1053, 8410, 1068, 1081, 7329, 1067], "cite_extract_rate": 0.46153846153846156, "origin_cites_number": 13, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a basic descriptive overview of GNN techniques for static graphs in node representation learning, mentioning several GNN variants and how they handle directed graphs. It integrates some ideas (e.g., conversion of directed to undirected graphs) and links to cited works, but lacks deeper synthesis, critical evaluation, or abstraction into broader NLP trends or principles. The analysis remains largely at the methodological level without highlighting trade-offs or overarching patterns."}}
{"id": "dc072dba-3fee-4da7-b987-0831f3f27fe9", "title": "Dynamic Graph", "level": "subsubsection", "subsections": [], "parent_id": "05b37ed2-d1bf-4bc3-bb21-9856bfc938ea", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Representation Learning for NLP"], ["subsection", "GNNs for Homogeneous Graphs"], ["subsubsection", "Dynamic Graph"]], "content": "Dynamic graphs that aim to learn the graph structure together with the downstream task jointly are widely adopted by graph representation learning~.\nEarly works mainly adopt the recurrent network by treating the graph node embeddings as RNN's state encodings~, which can be regarded as the rudiment of GGNN. Then the classic GNNs such as GCN~, GAT~, GGNN~ are utilized to learn the graph embedding effectively.\nRecent researchers adopt attention-based or metric learning-based mechanisms to learn the implicit graph structure (i.e., the graph adjacency matrix $A$) from unstructured texts. The learning process of graph structure is jointly with the downstream tasks via an end-to-end fashion~.", "cites": [1053, 1090, 1058, 1071], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates several works under the theme of dynamic graphs in GNNs for NLP, particularly linking early RNN-based approaches with more recent attention and metric learning methods. It provides a narrative that connects ideas across the cited papers, showing progression in methodology. However, it lacks deeper critical evaluation of the approaches or limitations and offers only moderate abstraction by highlighting the iterative and end-to-end learning trend without exploring broader implications or theoretical underpinnings."}}
{"id": "7017c0d2-6d29-4823-9a3e-3bd59e6505e8", "title": "Graph Neural Networks: Bidirectional Graph Embeddings", "level": "subsubsection", "subsections": [], "parent_id": "05b37ed2-d1bf-4bc3-bb21-9856bfc938ea", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Representation Learning for NLP"], ["subsection", "GNNs for Homogeneous Graphs"], ["subsubsection", "Graph Neural Networks: Bidirectional Graph Embeddings"]], "content": "\\label{sec:bidirectional-graph-embeddings}\nIn the previous sub-sections, we present the typical techniques for constructing and learning node embeddings from the static homogeneous graphs. In this subsection, we provide a detailed discuss on how to handle the edge directions. In reality, many graphs are directed acyclic graphs~(DAG)~, which information is propagated along the specific edge direction. \nHowever, some researchers allow the information propagate equally in both directions~ and others discard the information containing in outgoing edges~, both of which will lose some important structure information for the final representation learning.\nTo deal with this, bidirectional graph neural network (bidirectional GNN) is proposed to learn the node representation from both incoming and outgoing edges in a interleaved fashion. \nTo introduce different variants of bidirectional GNN, we first give some unified mathematical notations. For a specific node $v_i \\in \\mathcal{V}$ in the graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$ and its neighbor nodes $N(v_i)$ (i.e. any node $v_j$ satisfy $e_{i, j} \\in \\mathcal{E}$ or $e_{j, i} \\in \\mathcal{E}$), we define the incoming (backward) nodes set as $N_{\\dashv}(v_i)$ satisfying $e_{j, i} \\in \\mathcal{E}, v_{j} \\in N_{\\dashv}(v_i)$ and outgoing (forward) nodes set as $N_{\\vdash}(v_i)$ holding $e_{i, j} \\in \\mathcal{E}, v_{j} \\in N_{\\vdash}(v_i)$.\n firstly extend the GraphSage to a bi-directional version by calculating the graph embedding separately for each direction and combine them at last. At each computation hop, for each node in the graph, they aggregate the incoming nodes and outgoing nodes separately to get backward and forward immediate-aggregated representation as follows:\n\\begin{equation}\n    \\begin{split}\n    \\mathbf{h}^{(k)}_{i, \\dashv} &= \\sigma(\\mathbf{W}^{(k)}\\cdot f_k^{\\dashv}(\\mathbf{h}^{(k-1)}_{i, \\dashv},\\{\\mathbf{h}^{(k-1)}_{j, \\dashv}, \\forall v_j\\in N_{\\dashv}(v_i)\\})), \\\\\n    \\mathbf{h}^{(k)}_{i, \\vdash} &= \\sigma(\\mathbf{W}^{(k)}\\cdot f_k^{\\vdash}(\\mathbf{h}^{(k-1)}_{i, \\vdash},\\{\\mathbf{h}^{(k-1)}_{j, \\vdash}, \\forall v_j\\in N_{\\vdash}(v_i)\\})),\n    \\end{split}\n\\end{equation}\nwhere $k \\in \\{1, 2, ..., K\\}$ denotes the layer number, and $\\mathbf{h}^{(k)}_{i, \\dashv}, \\mathbf{h}^{(k)}_{i, \\vdash}$ denote the backward and forward aggregated results respectively. At the final step, the final forward and backward representation is concatenated to calculate the final bi-directional representation.\nAlthough works effectively, the bidirectional GraphSage learns both directions separately. To this end,  proposes the bidirectional GGNN to address this issue. Technically, at each iteration, \nafter obtaining aggregated vector representations $\\mathbf{h}^{(k)}_{i, \\vdash}, \\mathbf{h}^{(k)}_{i, \\dashv}$, they opt to fuse them into one vector as follows:\n\\begin{equation}\n    \\mathbf{h}^{(k)}_{N(v_i)} = \\text{Fuse}(\\mathbf{h}^{(k)}_{i, \\vdash}, \\mathbf{h}^{(k)}_{i, \\dashv}),\n\\end{equation}\nwhere the function $\\text{Fuse}(\\cdot, \\cdot)$ is the gated sum of two information sources:\n\\begin{equation}\n    \\text{Fuse}(\\mathbf{a}, \\mathbf{b}) = \\mathbf{z} \\odot \\mathbf{a} + (1 - \\mathbf{z}) \\odot \\mathbf{b}, \\mathbf{z} = \\sigma (\\mathbf{W}_{z} [\\mathbf{a},\\mathbf{b},\\mathbf{a}\\odot\\mathbf{b},\\mathbf{a}-\\mathbf{b}] + \\mathbf{b}_z)\n\\end{equation}\nwhere $\\mathbf{a} \\in \\mathbb{R}^d, \\mathbf{b} \\in \\mathbb{R}^d$ are inputs, $\\mathbf{W}_z \\in \\mathbb{R}^{d \\times 4d}, \\mathbf{b}_z \\in \\mathbb{R}^{d}$ are learnable weights and $\\sigma(\\cdot)$ is the sigmoid function. Then, a Gated Recurrent Unit (GRU) is used to update the node embeddings by\nincorporating the aggregation information as follows:\n\\begin{equation}\n    \\mathbf{h}^{(k)}_{i} = \\text{GRU}(\\mathbf{h}^{(k-1)}_{i}, \\mathbf{h}^{(k)}_{N(v_i)}).\n\\end{equation}\nUnlike previous methods, which are specially designed for the specific GNN methods,  further proposes a general bidirectional GNN framework, which can be easily applied to most existing GNNs. Technically, they first encode the graph in two directions:\n\\begin{equation}\n\\begin{split}\n    \\mathbf{h}^{(k)}_{i, \\dashv} &= \\text{GNN}(\\mathbf{h}^{(k-1)}_i, \\{\\mathbf{h}^{(k-1)}_j: \\forall v_j \\in N_{\\dashv}(v_i)  \\}), \\\\\n    \\mathbf{h}^{(k)}_{i, \\vdash} &= \\text{GNN}(\\mathbf{h}^{(k-1)}_i, \\{\\mathbf{h}^{(k-1)}_j: \\forall v_j \\in N_{\\vdash}(v_i)  \\}),\n\\end{split}\n\\end{equation}\nwhere $\\text{GNN}(\\cdot)$ can denote to any variant of GNNs. Similar to strategy in the bidirectional RNNs~, they learn the forward and backward directions separately and concatenate them together with the original node feature as follows:\n\\begin{equation}\n    \\mathbf{h}^{(k)}_i = [\\mathbf{h}^{(k)}_{i, \\dashv},\\mathbf{h}^{(k)}_{i, \\vdash},\\mathbf{h}^{(k-1)}_{i}].\n\\end{equation}\nThey stack several layers to achieve better performance. At last, the $\\mathbf{h}^{(K)}_i$ is employed in a sequence input of a Bi-LSTM in depth-first order to the final node representation.", "cites": [1068, 1067, 7044, 7329, 1053, 1071], "cite_extract_rate": 0.5, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section analytically presents bidirectional GNNs and their variants, synthesizing mathematical formulations and methodological approaches from multiple papers. It offers a structured comparison between methods like bidirectional GraphSAGE and bidirectional GGNN, identifying how they differ in handling edge directions and information fusion. While the section introduces a general framework, the critical analysis is somewhat limited to pointing out shortcomings rather than deeper evaluation of trade-offs or implications."}}
{"id": "8c95dcd6-b368-4f28-bbdd-eaa55a736ee3", "title": "Multi-relational Graph Formalization", "level": "subsubsection", "subsections": [], "parent_id": "46a0f611-f8ab-490e-9497-294bad0cd456", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Representation Learning for NLP"], ["subsection", "Graph Neural Networks for Multi-relational Graphs"], ["subsubsection", "Multi-relational Graph Formalization"]], "content": "\\label{subsec:graph-representation-learning-asnode}\nSince heterogeneous graphs are commonly observed in NLP domian, such as knowledge graph, AMR graph, etc, most of the researchers~ propose to convert it to a multi-relational graph, which can be learned by relational GNN in Section. \\ref{subsec:graph-representation-learning-rgnn} and Section. \\ref{subsec:graph-representation-learning-advanced-rgnn}. \nAs defined before, the multi-relational graph is denoted as $\\mathcal{G}(\\mathcal{V}, \\mathcal{E}, \\mathcal{T}, \\mathcal{R})$, s.t. $|\\mathcal{T}| = 1$ and $|\\mathcal{R}| >=  1$. \nTo get the multi-relational graph, technically, they ignore node types (i.e., project the nodes to the unified embedding space regardless of original nodes or relational nodes). As for edges, they assign the initial edges with the type \"default\". For each edge $e_{i, j}$, they add a reverse edge $e_{j, i}$ with type \"reverse\". Besides, for each node $v_i$, they add the self-loops with the type \"self\". Thus the converted graph is the multi-relational graph with $|E| = 3$ and $|V|=1$.", "cites": [1055, 57], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of how heterogeneous graphs in NLP are converted to multi-relational graphs. It mentions techniques like ignoring node types and adding reverse and self-loop edges but does not synthesize or connect these ideas across the cited papers. There is minimal critical analysis or abstraction beyond the specific methods described."}}
{"id": "2a37a437-c4ba-4dd9-aa1c-7ded74b23386", "title": "Multi-relational Graph Neural Networks", "level": "subsubsection", "subsections": ["c22155a5-7c60-4467-b0c4-25cfcd760897", "63326596-00d4-4044-84cc-4d96a18bb01e", "8a2e4fa6-71ae-49fd-96af-8dc90c9dfc76", "2fa9ce6b-edbb-4ce9-ad24-ae625b1523e1"], "parent_id": "46a0f611-f8ab-490e-9497-294bad0cd456", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Representation Learning for NLP"], ["subsection", "Graph Neural Networks for Multi-relational Graphs"], ["subsubsection", "Multi-relational Graph Neural Networks"]], "content": "\\label{subsec:graph-representation-learning-rgnn}\nThe multi-relational GNN is the extension of classic GNN for multi-relational graphs, which has the same node type but different edge types. They are originally introduced to encode relation-specific graphs such as knowledge graphs~ and parsing graphs~, which have complicated relationships between nodes with the same type. Generally, most multi-relational GNNs employ type-specific parameters to model the relations individually. In this subsection, we will introduce the classic relational GCN (R-GCN)~, relational GGNN (R-GGNN)~ and relational GAT (R-GAT)~.", "cites": [259, 1066, 7326, 1055, 1087], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes multi-relational GNNs and introduces three specific models (R-GCN, R-GGNN, R-GAT) without providing a deeper synthesis or comparison between them. It lacks critical evaluation of their strengths and weaknesses and does not abstract broader principles or patterns across the cited works."}}
{"id": "c22155a5-7c60-4467-b0c4-25cfcd760897", "title": "R-GCN", "level": "paragraph", "subsections": [], "parent_id": "2a37a437-c4ba-4dd9-aa1c-7ded74b23386", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Representation Learning for NLP"], ["subsection", "Graph Neural Networks for Multi-relational Graphs"], ["subsubsection", "Multi-relational Graph Neural Networks"], ["paragraph", "R-GCN"]], "content": "The R-GCN~ is explicitly developed to handle highly multi-relational graphs, especially knowledge bases. The R-GCN is a natural extension of the message-passing GCN framework~ which operates on local graph neighborhoods. They group the incoming nodes according to the label types and then apply messaging passing separately. \nThus, the aggregation of node $v_i$'s immediate neighbor nodes is defined as\n\\begin{equation}\n    \\mathbf{h}^{(k)}_i = \\sigma(\\sum_{r \\in \\mathcal{E}} \\sum_{v_j \\in N_{r}(v_i)} {\\frac{1}{c_{i, r}} {\\mathbf{W}^{(k)}_{r} \\mathbf{h}^{(k-1)}_{j}} } + \\mathbf{W}^{(k)}_{0}\\mathbf{h}^{(k-1)}_{i}),\n\\end{equation}\nwhere $\\mathbf{W}^{(k)}_{r} \\in \\mathbb{R}^{d \\times d}, \\mathbf{W}^{(k)}_{0} \\in \\mathbb{R}^{d \\times d}$ are trainable parameters, $N_{r}(v_i)$ is the neighborhoods of node $v_i$ with relation $r \\in \\mathcal{E}$, $c_{i, r}$ is the problem-specific normalization scalar such as $|N_{r}(v_i)|$, and $\\sigma(\\cdot)$ is the ReLU activation function. Intuitively, such a step projects neighbor nodes with different relations by relation-specific transformation to unified feature space and then accumulates them through a normalized sum. The self-connection with a special relation type is added to ensure the node itself feature can be held.\nHowever, modeling the multi-relations using separate parameters for each relation can lead to severe over-parameterization, especially on the rare relations. Thus two regularization methods: $\\textit{basis}$ and $\\textit{basis-diagonal-}$decomposition are proposed to address this issue. Firstly, for \\textit{basis} decomposition, each relation weight $\\mathbf{W}_{r}^{(k)}$ is defined as follows:\n\\begin{equation}\n    \\mathbf{W}_{r}^{(k)} = \\sum_{b=1}^{B}a_{rb}^{(k)}\\mathbf{V}_{b}^{(k)},\n    \\label{eq:basis}\n\\end{equation}\nwhere $\\mathbf{V}_{b}^{(k)} \\in \\mathbb{R}^{d \\times d}$ is the basis and $a_{rb}^{(k)}$ is the associated coefficients. This strategy actually regards the relation matrices as the linear combination of shared basis, which can be seen as a form of weight sharing between different relations.\nIn the \\textit{basis-diagonal decomposition}, each $\\mathbf{W}_r^{(k)}$ is defined through the direct sum over a set of low-dimensional matrices as\n\\begin{equation}\n    \\mathbf{W}^{(k)}_{r} = \\bigoplus_{b=1}^{B}\\mathbf{Q}_{br}^{(k)},\n\\end{equation}\nwhere $\\mathbf{Q}_{br}^{(k)} \\in \\mathbb{R}^{d/B \\times d/B}$ is the low-dimensional matrix. Thereby, the $\\mathbf{W}_{r}^{(k)}$ is represented by a set of sub matrices as $\\textit{diag}(\\mathbf{Q}_{1r}^{(k)}, \\mathbf{Q}_{2r}^{(k)}, ..., \\mathbf{Q}_{Br}^{(k)})$. This strategy can be seen as a matrix sparsity constraint. It holds the hypothesis that the latent features can be represented by sets of variables that are more tightly coupled within groups than across groups.\nThere are also some other GCN-based multi-relational graph neural networks for different purposes. For example, Directed-GCN~ is developed to exploit the syntactic graph, which has massive and unbalanced relations. The basic idea of incorporating edge-type information is similar to the R-GCN~, but they solve the over-parameterization issue by sharing projection matrix weights for all edges with the same directions but only keeping the relation-specific biases. The other example is weighted-GCN~, which adopt relation-specific transformation to learn relational information. The weighted-GCN learns the weight score for each relation type end-to-end and inject it into the GCN framework. In this way, the weighted-GCN is capable of controlling how much information each type contributes to the aggregation procedure. As a combination model, the Comp-GCN~ generalizes several of the existing multi-relational GCN methods (i.e., R-GCN~, Weighted-GCN~ and Directed-GCN~) and jointly learn the nodes and relations representation.", "cites": [259, 1102, 274, 216, 1059], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers (R-GCN, Directed-GCN, Weighted-GCN, Comp-GCN) to explain the evolution and design choices in multi-relational GNNs. It critically discusses issues like over-parameterization and compares regularization strategies such as basis and basis-diagonal decomposition. While it provides a clear abstraction by highlighting the hypothesis behind the basis decomposition and relation-specific modeling, it primarily focuses on a specific sub-area of GNNs without broader meta-level insights."}}
{"id": "63326596-00d4-4044-84cc-4d96a18bb01e", "title": "R-GGNN", "level": "paragraph", "subsections": [], "parent_id": "2a37a437-c4ba-4dd9-aa1c-7ded74b23386", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Representation Learning for NLP"], ["subsection", "Graph Neural Networks for Multi-relational Graphs"], ["subsubsection", "Multi-relational Graph Neural Networks"], ["paragraph", "R-GGNN"]], "content": "The relational GGNN~ is originally developed for the graph-to-sequence problem. It is capable of capturing long-distance relations. Similarly to R-GCN, R-GGNN uses relation-specific weights to capture relation-specific correlations between nodes better. Thus, the propagation process of R-GGNN can be summarized as\n\\begin{equation}\n\\begin{split}\n    \\mathbf{r}_i^{(k)} &= \\sigma(\\sum_{v_j \\in N(v_i)} \\frac{1}{c_{v_i, r}} \\mathbf{W}^{r}_{\\phi(e_{i, j})} \\mathbf{h}_{j}^{(k-1)} + \\mathbf{b}^{r}_{\\phi(e_{i, j})}), \\\\\n    \\mathbf{z}_i^{(k)} &= \\sigma(\\sum_{v_j \\in N(v_i)} \\frac{1}{c_{v_i, z}} \\mathbf{W}^{z}_{\\phi(e_{i, j})} \\mathbf{h}_{j}^{(k-1)} + \\mathbf{b}^{z}_{\\phi(e_{i, j})}), \\\\\n    \\tilde{h}_i^{(k)} &= \\rho(\\sum_{v_j \\in N(v_i)} \\frac{1}{c_{v_i}} \\mathbf{W}_{\\phi(e_{i, j})}(\\mathbf{r}_j^{(k)} \\odot \\mathbf{h}_i^{(k-1)}) + \\mathbf{b}_{\\phi(e_{i, j})}),\\\\\n    \\mathbf{h}^{(k)}_i &= (1 - \\mathbf{z}^{(k)}_{i}) \\odot \\mathbf{h}^{(k-1)}_{i} + \\mathbf{z}^{(k)}_{i} \\odot \\tilde{h}^{(k)}_i,\n\\end{split}\n\\end{equation}\nwhere $\\mathbf{W}^{r/z/\\cdot}_{\\phi(e_{i, j})} \\in \\mathbb{R}^{d \\times d}, \\mathbf{b}^{r/z/\\cdot}_{\\phi(e_{i, j})}$ are trainable relation-specific parameters, $\\sigma(\\cdot)$ is the sigmoid function, $c_{v_i, r/z/\\cdot} = |N(v_i)|$, and $\\rho(\\cdot)$ is the non-linear activation function such as tanh and ReLU.", "cites": [1055], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the R-GGNN model and its propagation equations, drawing from a single cited paper. It lacks synthesis of multiple sources and does not compare or contrast R-GGNN with other methods. There is minimal critical analysis or abstraction beyond the technical description of the model."}}
{"id": "8a2e4fa6-71ae-49fd-96af-8dc90c9dfc76", "title": "R-GAT", "level": "paragraph", "subsections": [], "parent_id": "2a37a437-c4ba-4dd9-aa1c-7ded74b23386", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Representation Learning for NLP"], ["subsection", "Graph Neural Networks for Multi-relational Graphs"], ["subsubsection", "Multi-relational Graph Neural Networks"], ["paragraph", "R-GAT"]], "content": " propose to extend the classic GAT to fit the multi-relational graphs. In this section, we will discuss two R-GAT variants. Intuitively, neighbor nodes with different relations should have different influences.\n propose to extend the homogeneous GAT with additional relational heads. Technically, they propose the relational node representation as\n\\begin{equation}\n    \\mathbf{h}^{(k), m}_{i, rel} = \\sum_{v_j \\in N(v_i)} \\beta_{ij}^{(k), m} \\mathbf{W}^{(k), m}\\mathbf{h}^{(k-1)}_{j}\n\\end{equation}\nwhere $m \\in [1, M]$ is the $m-$th head and $\\beta_{ij}^{(k), m}$ is the corresponding attention score for relation head $m$, which is calculated as\n\\begin{equation}\n\\begin{split}\n    s_{ij}^{(k), m} &= f(\\mathbf{e}_{i, j}),\\\\\n    \\beta_{ij}^{(k), m} &= \\text{softmax}_{j}(s_{ij}^{(k), m}),\n\\end{split}\n\\end{equation}\nwhere $s_{ij}^{(k), m}$ is the similarity between node $v_i$ and $v_j$, and $f(\\cdot): \\mathbb{R}^{d^{k}} \\rightarrow \\mathbb{R}$ is the multi-layer transformation (MLP) with non-linearity. The relational representation of node $v_i$ is the concatenation with linear transformation of $M$ heads' results as:\n\\begin{equation}\n    \\mathbf{h}^{(k)}_{i, rel} = g(||_{m=1}^{M}\\mathbf{h}^{(k), m}_{i, rel}),\n\\end{equation}\nwhere $||$ denotes the vector concatenation operation and $g(\\cdot): \\mathbb{R}^{m \\times d^{k}} \\rightarrow \\mathbb{R}^{d^{k}}$ is the liner projection. Thus, the final node representation is the combination of $\\mathbf{h}_{i, rel}^{(k)}$ and $\\mathbf{h}^{(k)}_{i, att}$ as follows:\n\\begin{equation}\n    \\mathbf{h}^{(k)}_{i} = \\sigma(\\mathbf{W}^{(k)}(\\mathbf{h}^{(k)}_{i, rel}||\\mathbf{h}^{(k)}_{i, att}) + \\mathbf{b}^{(k)}),\n\\end{equation}\nwhere $\\mathbf{W}^{(k)} \\in \\mathbb{R}^{d \\times d}, \\mathbf{b}^{(k)} \\in \\mathbb{R}^{d}$ are trainable parameters, $\\sigma(\\cdot)$ is the ReLU activation function.\nUnlike the work by , which learn and fuse the relation-specific node embedding regarding each type of edges,   develops a relation-aware attention mechanism to calculate the attention score $\\alpha_{ij}^{(k), m}$ as\n\\begin{equation}\n    \\begin{split}\n        \\alpha_{ij}^{(k), m} &= \\text{softmax}_j(s_{ij}^{(k), m}), \\\\\n        s_{ij}^{(k), m} &= \\sigma(f^{(k), m}([\\mathbf{W}^{(k), m}\\mathbf{h}^{(k-1)}_{i};\\mathbf{W}^{(k), m}\\mathbf{h}^{(k-1)}_{j};\\mathbf{e}_{i,j}^{(k-1)}])),\n    \\end{split}\n\\end{equation}\n where $\\mathbf{W}^{(k), m} \\in \\mathbb{R}^{d \\times d}$ is the learnable matrix, and $f(\\cdot)^{(k), m}: \\mathbb{R}^{3 \\times d} \\rightarrow \\mathbb{R}$ is the single linear projection layer. \n They learn a global representation for each relation type $r = \\phi(e_{i, j}) \\in \\mathcal{R}$. Technically, for all edges with type $r \\in \\mathcal{R}$, two node sets $S_{r}$ and $T_{r}$. $S_{r}$ are collected regarding the source and target node set of relation $r$, respectively. Thus the edge type embedding $\\mathbf{t}_{r}$ can be calculated by:\n \\begin{equation}\n     \\mathbf{e}_{r} = \\lvert \\frac{\\sum_{o \\in S_r}\\mathbf{W}\\mathbf{h}_o}{|S_r|} - \\frac{\\sum_{o \\in T_r}\\mathbf{W}\\mathbf{h}_o}{|T_r|} \\rvert.\n \\end{equation}\nThus the edge representation is the absolute difference between mean vectors of source and target nodes connected by edges whose type are $r$.", "cites": [7326], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the R-GAT model and its variants, focusing on their technical formulations. It integrates information from at least one cited paper but does so in a largely surface-level manner without connecting ideas across multiple sources or identifying broader patterns. There is minimal critical evaluation or comparison of approaches, and the abstraction level remains low, confined to the specific methods discussed."}}
{"id": "2fa9ce6b-edbb-4ce9-ad24-ae625b1523e1", "title": "Gating Mechanism", "level": "paragraph", "subsections": [], "parent_id": "2a37a437-c4ba-4dd9-aa1c-7ded74b23386", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Representation Learning for NLP"], ["subsection", "Graph Neural Networks for Multi-relational Graphs"], ["subsubsection", "Multi-relational Graph Neural Networks"], ["paragraph", "Gating Mechanism"]], "content": "The multi-relational graph neural networks also face the  over-smoothing problem when stacking several layers to exploit implicit correlations between distant neighbors (i.e., not directly connected)~. To solve this, the gating mechanism, which combines the nodes' input features and aggregated features by gates, is introduced to the multi-relational graph neural network~. Intuitively, the gating mechanism can be regarded as a trade-off between the original signals and the learned information. It regulates how much of the update message that are propagated to the next step, thus preventing the model from thoroughly overwriting the past information.\nHere we introduce the gating mechanism by taking the classic R-GCN~ as an example, which actually can be extended to arbitrary variants. \nWe denote the representation before activation $\\sigma(\\cdot)$ as\n\\begin{equation}\n    \\mathbf{u}^{(k)}_{i} = f^{(k)}(\\mathbf{h}^{(k-1)}_{i}),\n\\end{equation}\nwhere $f$ denotes to the aggregation function. Ultimately, the final representation of node $i$'s representation is a gated combination of the previous embedding $\\mathbf{h}^k_{i}$ and GNN output representation $\\sigma(\\mathbf{u}^k_{i})$ as:\n\\begin{equation}\n    \\mathbf{h}^{(k)}_{i} = \\sigma(\\mathbf{u}^{(k)}_{i}) \\odot \\mathbf{g}^{(k)}_{i} + \\mathbf{h}^{(k-1)}_{i} \\odot (1 - \\mathbf{g}^{(k-1)}_{i})\n\\end{equation}\nwhere $\\mathbf{g}^k_{i}$ is the gating vectors, and $\\sigma(\\cdot)$ is often the $\\textit{tanh}(\\cdot)$ function. The gating vectors are calculated by both the inputs and outputs as follows:\n\\begin{equation}\n    g^{(k)}_{i} = \\sigma (f^{(k)}([\\mathbf{u}^{(k)}_{i},\\mathbf{h}^{(k-1)}_{i}]))\n\\end{equation}\nwhere $\\sigma$ is the sigmoid activation function, and $f^{(k)}(\\cdot): \\mathbb{R}^{2d} \\rightarrow \\mathbb{R}$ is the linear transformation. We repeat calculating $g^{(k)}_{i}$ for $d$ times to get the gating vector $\\mathbf{g}^{(k)}_{i}$.", "cites": [259, 1078, 1085], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear explanation of the gating mechanism in multi-relational GNNs by using R-GCN as an example and generalizing it to other variants, showing moderate synthesis and abstraction. However, it primarily describes the mechanism rather than critically evaluating its strengths, weaknesses, or comparing it with alternatives from the cited papers."}}
{"id": "d83fb86b-2c76-4d0c-8636-77edfac7e111", "title": "Graph Transformer", "level": "subsubsection", "subsections": ["3c55b6c3-97b1-40f1-b423-37507474b7ec", "9bb25709-7469-4d4e-bfdd-febeb4a32477"], "parent_id": "46a0f611-f8ab-490e-9497-294bad0cd456", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Representation Learning for NLP"], ["subsection", "Graph Neural Networks for Multi-relational Graphs"], ["subsubsection", "Graph Transformer"]], "content": "\\label{subsec:graph-representation-learning-advanced-rgnn}\nThe transformer architecture~ has achieved great success in NLP fields. Roughly speaking, the transformer's self-attention mechanism is a special procedure of fully connected implicit graph learning, as we discussed in sec. \\ref{sec:graph-construction-structure-aware}, thus bridging the concept of GNN and transformer. However, the traditional transformer fails to leverage structure information. Inspired by GAT~, which combines the message passing with attention mechanism, much literature incorporates structured information to the transformer (we name it as graph transformer) by developing structure-aware self-attention mechanism~. In this section, we will discuss the techniques of structure-aware self-attention for the multi-relational graph.\nAs a preliminary knowledge, here we give a brief review of self-attention. To make it clear, we omit the multi-head mechanism and only present the self-attention function. Formally, we denote the input of self-attention as $\\mathbf{Q} = \\{\\mathbf{q}_1, \\mathbf{q}_2, ..., \\mathbf{q}_m\\} \\in \\mathbb{R}^{m \\times d^{q}}, \\mathbf{K} = \\{\\mathbf{k}_1, \\mathbf{k}_2, ..., \\mathbf{k}_n\\} \\in \\mathbb{R}^{n \\times d^{k}}, \\mathbf{V} = \\{\\mathbf{v}_1, \\mathbf{v}_2, ..., \\mathbf{v}_n\\} \\in \\mathbb{R}^{n \\times d^{v}}$. Then the output representation $\\mathbf{z}_i$ is calculated as\n\\begin{align}\n    \\mathbf{z}_i &= \\text{Attention}(\\mathbf{q}_{i}, \\mathbf{K}, \\mathbf{V}) = \\sum_{j=1}^{n} \\alpha_{i, j} \\mathbf{W}^{v} \\mathbf{v}_{j} \\label{eq:transformer-aggregation}\\\\\n    \\alpha_{i, j} &= \\text{softmax}_{j}(u_{i, j}) \\\\\n    u_{i, j} &= \\frac{ (\\mathbf{W}^q \\mathbf{q}_{i})^T (\\mathbf{W}^k \\mathbf{k}_{j}) }{\\sqrt{d}} \\label{eq:self-attn}\n\\end{align}\nwhere $\\mathbf{W}^{q} \\in \\mathbb{R}^{d \\times d^{q}}, \\mathbf{W}^{k} \\in \\mathbb{R}^{d \\times d^{k}}, \\mathbf{W}^{v} \\in \\mathbb{R}^{d \\times d^{v}}$ are trainable parameters, and $d$ is the model dimension. Note that for graph transformer, the query, key and value all refer to the nodes' embedding:, namely, $\\mathbf{q}_i = \\mathbf{k}_i = \\mathbf{v}_i = \\mathbf{h}_i$. Thus, we will only use $\\mathbf{h}_i$ to represent query, key and value considering simplification in the following contents.\nThere are various graph transformers for relation graphs that incorporate the structure knowledge, which can be categorized into two classes according to the self-attention function. One class is the R-GAT-based methods which adopt relational GAT-like feature aggregation. Another class reserves the fully connected graph while incorporating the structure-aware relation information to the self-attention function.", "cites": [180, 7044, 1066, 38], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes ideas from multiple papers by connecting the transformer's self-attention mechanism to graph-based attention (GAT), creating a conceptual link between GNNs and transformers. It introduces the idea of a graph transformer and categorizes methods into two classes, which shows some abstraction and organization. However, the critical analysis is limited, as it does not deeply evaluate limitations or trade-offs of the approaches. Overall, it provides analytical insight but lacks a more comprehensive evaluative depth."}}
{"id": "3c55b6c3-97b1-40f1-b423-37507474b7ec", "title": "R-GAT Based Graph Transformer.", "level": "paragraph", "subsections": [], "parent_id": "d83fb86b-2c76-4d0c-8636-77edfac7e111", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Representation Learning for NLP"], ["subsection", "Graph Neural Networks for Multi-relational Graphs"], ["subsubsection", "Graph Transformer"], ["paragraph", "R-GAT Based Graph Transformer."]], "content": "The GAT-based graph transformer~ adopts the GAT-like feature aggregation, which leverages the graph connectivity inductive bias. Technically, they first aggregate neighbors with type-specific aggregation step and then fuse them through feed-forward layer as follows:\n\\begin{equation}\n\\begin{split}\n     \\mathbf{z}_i^{r,(k)} &=  \\sum_{v_j \\in N_{r}(v_i)} \\alpha_{i, j}^k \\mathbf{W}^{v, (k)} \\mathbf{h}_{j}^{(k-1)}, r \\in \\mathcal{E}\\\\\n     \\mathbf{h}^{(k)}_i &= \\text{FFN}^{(k)}(\\mathbf{W}^{O, (k)} [\\mathbf{z}_i^{R_1, (k)},...,\\mathbf{z}_i^{R_q, (k)}]),\n\\end{split}\n\\end{equation}\nwhere $\\text{FFN}^{(k)}(\\cdot)$ denotes the feed-forward layer in transformer~, and $\\alpha_{i, j}$ denotes the dot-product score in eq. \\ref{eq:self-attn}. \nTo incorporate the bi-directional information,  learns forward and backward aggregation representation in graph transformer. Specifically, given the backward and forward features (i.e., $\\mathbf{h}_{i, \\vdash}$ and $\\mathbf{h}_{i, \\dashv}$) for node $v_i$, the backward aggregated feature $\\mathbf{z}_{i, \\dashv}^{(k)}$ for node $v_i$ is formulated by:\n\\begin{equation}\n    \\begin{split}\n        \\mathbf{z}_{i, \\dashv}^{(k)} &= \\sum_{v_j \\in N_{\\dashv}(v_i)} \\alpha_{i, j} \\mathbf{W}^{v, (k)} \\mathbf{a}^{(k)}_{i, j}, \\\\\n        \\mathbf{a}^{(k)}_{i, j} &= f^{(k)}([\\mathbf{h}_{i, \\vdash},\\mathbf{e}_{i, j};\\mathbf{h}_{j, \\dashv}]),\n    \\end{split}\n\\end{equation}\nwhere $f^{(k)}(\\cdot): \\mathbb{R}^{3 \\times d \\rightarrow \\mathbb{R}^{d}}$ is the linear transformation, and $\\alpha_{i, j}$ is the softmax score of incoming neighbors' dot-production results $u_{i, j}$ which can be formulated by:\n\\begin{equation}\n\\begin{split}\n    u_{i, j} = \\frac{ (\\mathbf{W}^{q, (k)} \\mathbf{h}_{i, \\dashv})^T (\\mathbf{W}^{k, (k)} \\mathbf{a}^{(k)}_{i, j}) }{\\sqrt{d}}.\n\\end{split}\n\\end{equation}\nThen they adopt the gating mechanism to fuse bidirectional aggregated features to get the packaged node representation:\n\\begin{equation}\n\\begin{split}\n    \\mathbf{g}^{(k)}_{i} &= \\sigma(f'^{k}([\\mathbf{z}^{(k)}_{i, \\vdash};\\mathbf{z}^{(k)}_{i, \\dashv}])), \\\\\n    \\mathbf{p}^{(k)}_{i} &= \\mathbf{g}^{(k)}_i \\odot \\mathbf{z}^{(k)}_{i, \\vdash} + (1 - \\mathbf{g}^{(k)}_{i}) \\odot \\mathbf{z}^{(k)}_{i, \\dashv}\n\\end{split}\n\\end{equation}\nwhere $f'(\\cdot): \\mathbb{R}^{2 \\times d} \\rightarrow \\mathbb{R}^{d}$, and $\\sigma(\\cdot)$ is the sigmoid activation function. They calculate the the forward and backward node representation based on the packaged representation, respectively:\n\\begin{equation}\n\\begin{split}\n    [\\mathbf{o}^{(k)}_{i, \\vdash}, \\mathbf{o}^{(k)}_{i, \\dashv}] &= \\text{FFN}^{(k)}(\\mathbf{p}^{(k)}_{i}), \\\\\n    \\mathbf{h}^{(k)}_{i, \\vdash} &= \\text{LayerNorm}^{(k)}(\\mathbf{o}^{(k)}_{i, \\vdash} + \\mathbf{h}^{(k-1)}_{i, \\vdash}), \\\\\n    \\mathbf{h}^{(k)}_{i, \\dashv} &= \\text{LayerNorm}^{(k)}(\\mathbf{o}^{(k)}_{i, \\dashv} + \\mathbf{h}^{(k-1)}_{i, \\dashv}), \n\\end{split}\n\\end{equation}\nwhere $\\text{FFN}(\\cdot): \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{2 \\times d}$ is the feed-forward function, and $\\text{LayerNorm}(\\cdot)$ is the layer normalization~. The final node representation is the concatenation of the last layer $K$'s bidirectional representations:\n\\begin{equation}\n    \\mathbf{h}^{(K)}_{i} = f''^{K}([\\mathbf{h}^{(K)}_{i, \\vdash}, \\mathbf{h}^{(K)}_{i, \\dashv}]),\n\\end{equation}\nwhere $f''^{(K)}(\\cdot): \\mathbb{R}^{2 \\times d} \\rightarrow \\mathbb{R}^{d}$ is the linear transformation.", "cites": [38, 57], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the technical components of R-GAT based graph transformers, including aggregation, fusion, and normalization steps. It references the Transformer and Layer Normalization papers but does not synthesize these ideas into a broader context or analyze their implications for NLP tasks. The content is largely procedural and lacks critical evaluation or abstraction."}}
{"id": "9bb25709-7469-4d4e-bfdd-febeb4a32477", "title": "Structure-aware Self-attention Based Graph Transformer.", "level": "paragraph", "subsections": [], "parent_id": "d83fb86b-2c76-4d0c-8636-77edfac7e111", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Representation Learning for NLP"], ["subsection", "Graph Neural Networks for Multi-relational Graphs"], ["subsubsection", "Graph Transformer"], ["paragraph", "Structure-aware Self-attention Based Graph Transformer."]], "content": "Unlike the R-GAT-based graph transformer, which purely relies on the given graph structure as connectivity, the structure-aware self-attention-based graph transformer reserves the original self-attention architecture, allowing non-neighbor nodes' communication. We will firstly discuss the structure-aware self-attention mechanism and then present its unique edge embedding representation.\n firstly attempts to model the relative relations between words (nodes) in the neural machine translation task. Technically, they consider the relation embedding when calculating node-wise similarity in eq. \\ref{eq:self-attn} as follows:\n\\begin{equation}\n    u_{i, j}^{(k)} = \\frac{ (\\mathbf{W}^{q, (k)} \\mathbf{h}^{(k-1)}_{i})^T (\\mathbf{W}^{k, (k)} \\mathbf{h}^{(k-1)}_{j}) + (\\mathbf{W}^{q, (k)} \\mathbf{h}^{(k-1)}_{i})^T \\mathbf{e}_{i, j} }{\\sqrt{d}}.\n\\end{equation}\nMotivated by ,  propose to extend the conventional self-attention architecture to explicitly encode the relational embedding between nodes pairs in the latent space as\n\\begin{equation}\n    \\begin{split}\n        u_{i, j}^{(k)} &= \\frac{ (\\mathbf{W}^{q, (k)} \\mathbf{h}^{(k-1)}_{i})^T (\\mathbf{W}^{k, (k)} \\mathbf{h}^{(k-1)}_{j} + \\mathbf{W}^{r, (k)}\\mathbf{e}_{i, j}) }{\\sqrt{d}}, \\\\\n         \\mathbf{h}_i^{(k)} &= \\sum_{j=1}^{n} \\alpha_{i, j}^k (\\mathbf{W}^{v, (k)} \\mathbf{h}_{j}^{(k-1)} + \\mathbf{W}^{f, (k)}\\mathbf{e}_{i, j}).\n    \\end{split}\n\\end{equation}\nTo adopt the bidirectional relations,  extends the traditional self-attention as follows:\n\\begin{equation}\n    u_{i, j}^{(k)} = \\frac{[\\mathbf{W}^{q, (k)}(\\mathbf{h}^{(k-1)}_{i} + \\mathbf{e}_{i, j})]^T [\\mathbf{W}^{k, (k)} (\\mathbf{h}^{(k-1)}_{j} + \\mathbf{e}_{j, i})]}{\\sqrt{d}}.\n\\end{equation}\nGiven the learnt attention for each relation, edge embedding representation is the next critical step for incorporating the structure-information.\n simply learns the relative position encoding w.r.t the nodes' absolute positions. Technically, they employ $2K+1$ latent labels ($[-K, K]$) and project $j - i$ to one specific label embedding for node pair $(v_i, v_j)$ to fetch the edge embedding $\\mathbf{e}_{i, j}$.  adopts the similar idea as . They define a relative position embedding table and fetch the edge embedding by looking up from it.\n learn the edge representation $\\mathbf{e}_{i, j}$ by the path from node $v_i$ to node $v_j$. For , the natural way is to view the path as a string, which is added to the vocabulary to vectorize it. Other ways are further proposed to learn from labels' embedding along the path, such as 1) taking average, 2) taking sum, 3) encoding them using self-attention, and 4) encoding them using CNN filters.   propose the shortest path based relation encoder. Concretely, they firstly fetch the labels' embedding sequence along the path. Then they employ the bi-directional GRUs for sequence encoding. The last hidden states of the forward and backward GRU networks are finally concatenated to represent the relation embedding $\\mathbf{e}_{i, j}$.", "cites": [7044, 1066], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates multiple papers by discussing variations of structure-aware self-attention mechanisms in graph transformers, showing how each builds upon or modifies the core idea. It provides a clear technical synthesis but lacks deeper critical evaluation of the approaches or their limitations. The abstraction is moderate, as it identifies patterns in how edge embeddings are learned across methods."}}
{"id": "3fffd4e3-dac8-4eb5-86ed-f1cb44bf8217", "title": "Graph Neural Networks for Heterogeneous Graph", "level": "subsection", "subsections": ["1d7c073f-a12b-4d61-af19-f6dbaadac0e9", "2ee65c25-4455-4d81-abc2-25e9e1c6ec53", "7e8fcb1c-dc59-4fd2-98f9-8fd235c38695"], "parent_id": "84ae95bb-0c80-43ad-93fa-77cbd83dd087", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Representation Learning for NLP"], ["subsection", "Graph Neural Networks for Heterogeneous Graph"]], "content": "\\label{subsec:graph-representation-learning-heterogeneousgnn}\nIn practice, many graphs have various node and edge types, such as knowledge graph, AMR graph, etc., which are called heterogeneous graphs. Formally, for a graph $\\mathcal{G}$, s.t. $|\\mathcal{T}| > 1$ or $|\\mathcal{R}| > 1$, it is called heterogeneous graph. Beside transforming the heterogeneous to relation graphs, as introduced in the previous subsection, sometimes it is required to fully leverage the type information for both nodes and edges~. Thus, in Section. \\ref{subsec:levi}, we first introduce a pre-processing technique for heterogeneous graph. Then, in Section. \\ref{subsec:meta-heteronebous gnn} and \\ref{subsec:e-gnn heteronenous gnn}, we will introduce two typical graph representation learning methods specially for heterogeneous graphs.", "cites": [1053, 1103, 1069], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a brief description of heterogeneous graphs and mentions two subsections for specific methods but does not effectively synthesize or connect the cited papers. It lacks critical evaluation and abstraction beyond the concrete examples, offering minimal insight into broader trends or principles."}}
{"id": "1d7c073f-a12b-4d61-af19-f6dbaadac0e9", "title": "Levi Graph Transformation", "level": "subsubsection", "subsections": [], "parent_id": "3fffd4e3-dac8-4eb5-86ed-f1cb44bf8217", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Representation Learning for NLP"], ["subsection", "Graph Neural Networks for Heterogeneous Graph"], ["subsubsection", "Levi Graph Transformation"]], "content": "\\label{subsec:levi}\nSince most existing GNN methods are only designed for homogeneous conditions and there is a massive computation burden when dealing with lots of edge types~ (e.g. an AMR graph may contain more than 100 edge types), it is typical to effectively to treat the edges as nodes in the heterogeneous graphs~. \nOne of the important graph transformation techniques is \\textit{Levi Graph Transformation}.\nTechnically, for each edge $e_{i,j}$ with edge label $\\phi(e_{i,j})$, we will create a new node $v_{e_{i,j}}$. Thus the new graph is denoted as $ \\mathcal{G'}(\\mathcal{V'}, \\mathcal{E'}, \\mathcal{T'}, \\mathcal{R'})$, where the node set is $\\mathcal{V'} = \\mathcal{V} \\cup \\{v_{e_{i,j}}\\}$, the node label set is $\\mathcal{T'} = \\mathcal{T} \\cup \\{\\phi(e)_{i, j}\\}$. We cut off the direct edge between node $v_i, v_j$ and the add two direct edges between: 1) $v_i, v_{e_{i,j}}$, and 2) $v_{e_{i,j}}, v_j$. After converting all edges in $\\mathcal{E}$, the new graph $\\mathcal{G'}$ will be a bipartite graph, s.t. $|\\mathcal{R'}| = 1$. An example of transforming AMR graph to desired levi-graph is illustrated in Fig. \\ref{fig:levi-graph-example}. The obtained graph is a simplified heterogeneous levi graph that has a single edge type but unrestricted node types, which can then be learnt by heterogeneous GNNs described in Section~\\ref{subsec:graph-representation-learning-heterogeneousgnn}.\n\\begin{figure}[ht!]\n\\centering\n\\vspace{-2mm}\n\\includegraphics[width=12.0cm]{levi_graph_transformation.pdf}\n\\vspace{-4mm}\n\\caption{An example for transforming AMR graph to Levi-graph.}\n\\label{fig:levi-graph-example}\n\\vspace{-0mm}\n\\end{figure}", "cites": [1055], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a clear description of the Levi Graph Transformation technique, including its technical implementation and purpose. However, it lacks synthesis with other relevant works beyond the single cited paper and does not offer a comparative or critical analysis of the method. The abstraction is limited to a general explanation of the transformation without identifying broader patterns or principles in heterogeneous graph modeling for NLP."}}
{"id": "2ee65c25-4455-4d81-abc2-25e9e1c6ec53", "title": "Meta-path based Heterogeneous GNN", "level": "subsubsection", "subsections": ["ecd62368-62cc-4c62-b01a-756f37fb9609", "13b86d9d-d0da-4101-b18d-51a49849febd"], "parent_id": "3fffd4e3-dac8-4eb5-86ed-f1cb44bf8217", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Representation Learning for NLP"], ["subsection", "Graph Neural Networks for Heterogeneous Graph"], ["subsubsection", "Meta-path based Heterogeneous GNN"]], "content": "\\label{subsec:meta-heteronebous gnn}\nMeta-path, a composite relation connecting two objects, is a widely used structure to capture the semantics. Take movie data IMDB for example, there are three types of nodes, including movie, actor, and director. The meta-path $Movie \\rightarrow Actor \\rightarrow Movie$, which covers two movie sets and one actor, describes the co-actor relations. Thus different relations between nodes in the heterogeneous graph can be easily revealed by meta-paths.\nFirst, we provide the meta-level (i.e., schema-level) description of a heterogeneous graph for better understanding. We follow the setting of heterogeneous information network (HIN)~ and give the concept of Network Schema. The network schema is a meta template for the heterogeneous graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$ with the node type mapping: $\\mathcal{V} \\rightarrow \\mathcal{T}$ and edge type mapping: $\\mathcal{E} \\rightarrow \\mathcal{R}$. We denote it as $\\mathcal{M}_{\\mathcal{G}}(\\mathcal{T}, \\mathcal{R})$. A meta path is a path on the network schema denoted as $\\Phi = T_1 \\overset{R_1}{\\rightarrow} T_2 \\overset{R_2}{\\rightarrow} ... \\overset{R_l}{\\rightarrow} T_{l+1}$, where $T_i \\in \\mathcal{T}$ is the schema's node and $R_i \\in \\mathcal{R}$ is the corresponding relation node. What's more, we denote the meta-path set as $\\{\\Phi_1, \\Phi_2, ..., \\Phi_p\\}$. For each node $T_i$ on the meta-path $\\Phi_j$, we denote it as $T_{i}^{\\Phi_j}$. Then we combine the network schema with the concrete heterogeneous graph. For each node $v_i$ in the heterogeneous graph and one meta-path $\\Phi_j$, we define the meta-path-based neighbors as $N_{\\Phi_j}(v_i)$, which contains all nodes including itself linked by meta-path $\\Phi_j$. An example of meta-path based heterogeneous graph is shown in Fig. \\ref{fig:metapath-graph-example}. Conceptually, the neighbor set can have multi-hop nodes depending on the length of the meta-path.\n\\begin{figure}[ht!]\n\\centering\n\\vspace{-2mm}\n\\includegraphics[width=12.0cm]{meta_path_in_heterogenous_graph.pdf}\n\\vspace{-4mm}\n\\caption{An example of meta-path based heterogeneous graph.}\n\\label{fig:metapath-graph-example}\n\\vspace{-0mm}\n\\end{figure}\nMost meta-path-based GNN methods adopt the attention-based aggregation strategy~. Technically, they can be generally divided into two stages. Firstly, they aggregate the neighborhoods along each meta-paths, which can be named as ``node-level aggregation\". After this, the nodes receive neighbors' information via different meta-path. Next, they apply meta-path level attention to learn the semantic impact of different meta-path. In this way, they can learn the optimal combination of neighbors connected by multiple meta-paths. In the following, we will discuss two typical heterogeneous GNN models~.", "cites": [1053], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear conceptual framework for meta-path-based heterogeneous GNNs and integrates the idea across multiple works, particularly using the example from the cited paper. It explains the general methodology and structure of such models, showing some synthesis. However, it lacks deeper critical evaluation or comparison of the cited paper with others in the field, and while it abstracts the meta-path concept, it doesn't fully elevate to identifying broader, overarching principles."}}
{"id": "ecd62368-62cc-4c62-b01a-756f37fb9609", "title": "HAN", "level": "paragraph", "subsections": [], "parent_id": "2ee65c25-4455-4d81-abc2-25e9e1c6ec53", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Representation Learning for NLP"], ["subsection", "Graph Neural Networks for Heterogeneous Graph"], ["subsubsection", "Meta-path based Heterogeneous GNN"], ["paragraph", "HAN"]], "content": "~\nDue to the heterogeneity of nodes in graphs, different nodes have different feature spaces, which brings a big challenge for GNN to handle various initial node embedding. To tackle this issue, the type-specific transformation is adopted to project various nodes to a unified feature space as follows:\n\\begin{equation}\n    \\mathbf{h}'_{i} = \\mathbf{W}_{\\tau(v_i)} \\mathbf{h}_{i}. \\label{eq:feature-prj}\n\\end{equation}\nWe overwrite the notation $\\mathbf{h}_{i}$ to denote the transformed node embedding in HAN's discussion.\n\\begin{itemize}\n    \\item \\textbf{Node-level Aggregation.} For the node $v_i$ and its' neighbor node set $N_{\\Phi_k}(v_i)$ on the meta-path $\\Phi_k$, the aggregated feature of node $v_i$ can be represented by:\n    \\begin{equation}\n    \\begin{split}\n         \\mathbf{z}_{i, \\Phi_k} &= \\sigma(\\sum_{v_j \\in N_{\\Phi_k}(v_i)} \\alpha^{\\Phi_k}_{i, j} \\mathbf{h}_{j})\\\\\n         \\alpha^{\\Phi_k}_{i, j} &= \\text{softmax}_j(u^{\\Phi_{k}}_{i, j}) \\\\\n         u^{\\Phi_k}_{i, j} &= \\text{Attention}(\\mathbf{h}_{i}, \\mathbf{h}_j; \\Phi_k) = \\sigma(\\mathbf{W}[\\mathbf{h}_{i},\\mathbf{h}_{j}]),\n    \\end{split}\n    \\end{equation}\n    where $\\mathbf{W} \\in \\mathbb{R}^{1 \\times 2d}$ is the trainable parameter. To make the training process more stable, they further extend the attention by the multi-head mechanism. The final representation of $\\mathbf{z}_{i, \\Phi_j}$ is the concatenation of $L$ heads' results. Given p meta-paths, we can obtain the aggregated embedding via the previous nodel-level aggregation step as $\\{\\mathbf{Z}_{\\Phi_1}, ..., \\mathbf{Z}_{\\Phi_p}\\}$, where $\\mathbf{Z}_{\\Phi_j}$ is the collection of all nodes' representation for meta-path $\\Phi_j$.\n    \\item \\textbf{Meta-path Level Aggregation.} Generally, different meta-path conveys different semantic information. To this end, they aim to learn the importance of each meta-path as:\n    \\begin{equation}\n        (\\beta_{\\Phi_1}, ..., \\beta_{\\Phi_p}) = \\text{Meta\\_Attn}(\\mathbf{Z}_{\\Phi_1}, ..., \\mathbf{Z}_{\\Phi_p}),\n    \\end{equation}\n    where $\\beta_{\\Phi_j}$ denotes the learned importance score for meta-path $\\Phi_j$, and $\\text{Meta\\_Attn}()$ is the attention-based scorer. Technically, for each meta-path, they first learn the semantic-level importance for each node. Then they average them to get the meta-path level importance. It can be formulated by:\n    \\begin{equation}\n        \\begin{split}\n            o_{\\Phi_k} &= \\frac{1}{|\\mathcal{V}|} \\sum_{v_i \\in \\mathcal{V}} \\mathbf{q}^T f(\\mathbf{z}_{i, \\Phi_k}), \\\\\n            \\beta_{\\Phi_k} &= \\text{softmax}_{k} (o_{\\Phi_k}),\n        \\end{split}\n    \\end{equation}\n    where $f(\\cdot): \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^d$ is the MLP with $\\text{tanh}$ non-linearity.\n     Finally, we can obtain the final node representation as:\n    \\begin{equation}\n        \\mathbf{z}_{i} = \\sum_{k=1}^{p} \\beta_{\\Phi_k} \\mathbf{z}_{i, \\Phi_k}.\n    \\end{equation}\n\\end{itemize}", "cites": [248], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the methodology of HAN from the cited paper, integrating the two-level attention mechanism (node- and meta-path-level) into a coherent explanation. It abstracts the model's design to highlight how attention is used to manage heterogeneity and semantic diversity. However, it lacks critical analysis, such as limitations or trade-offs of the approach, and does not compare HAN with other meta-path based methods in detail."}}
{"id": "13b86d9d-d0da-4101-b18d-51a49849febd", "title": "MEIRec", "level": "paragraph", "subsections": [], "parent_id": "2ee65c25-4455-4d81-abc2-25e9e1c6ec53", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Representation Learning for NLP"], ["subsection", "Graph Neural Networks for Heterogeneous Graph"], ["subsubsection", "Meta-path based Heterogeneous GNN"], ["paragraph", "MEIRec"]], "content": "The MEIRec~ is a heterogeneous GNN-based recommend system. In the specific recommendation system condition, they restrict the heterogeneous graph's type amount and meta-path's length and propose a special heterogeneous graph neural network, particularly to fully utilize rich structure information. \nConsidering that the type-specific transformation requires huge parameters when the amount of nodes is large, they propose an efficient unified embedding learning method. For each node, they fetch the terms in the vocabulary and then average them to get the vector representation. \n\\begin{itemize}\n    \\item \\textbf{Node-level Aggregation.} Unlike HAN~, which collect all nodes along the meta-path as neighbors, they treat different hop of neighbors differently. Given the meta-path $\\Phi_j$, they define the neighbors of node $v_i$ as $N_{\\Phi_j}(v_i)^{o}, o \\in [1, 2, ..., O]$ where $o$ denotes the hop. They learn the representation recursively. Take $(o)-$hop neighbors for example, for each nodes in $N_{\\Phi_j}(v_i)^{o}$, they first collect the immediate-neighbors from $(o+1)$-hop neighbors and learn the representation to obtain $(o)$-hop representation. Then they repeat this procedure to obtain $(o-1)$-hop nodes' representation. Finally, $v_i$'s representation for meta-path $\\Phi_j$ is generated. Formally, for $v_k \\in N_{\\Phi_j}(v_i)^{o}$, they define its' immediate neighbor set as $N_{\\Phi_j}(v_k) \\in N_{\\Phi_j}(v_i)^{o+1}$. Node $v_j$'s representation $\\mathbf{z}_{k, \\Phi_j}$ is formulated as:\n    \\begin{equation}\n        \\mathbf{z}_{k, \\Phi_j} = g(\\{\\mathbf{z}_{l, \\Phi_j}, v_l \\in N_{\\Phi_j}(v_k) \\}),\n    \\end{equation}\n    where $g(\\cdot)$ is the aggregation function. In MEIRec, it can be the average function, LSTM function, or the CNN function depend on the nodes' type. Besides, the last hop ($(O)$-hop)'s nodes are represented by initial node embedding.\n    \\item \\textbf{Meta-path Level Aggregation.} Given p meta-path with the starting nodes $v_i$, we can obtain p aggregated embedding by the previous step. Then we adopt a similar procedure as node-level aggregation as follows:\n    \\begin{equation}\n        \\mathbf{z}_{i} = g(\\{\\mathbf{z}_{i, \\Phi_j}, j \\in [1, ..., p]\\}),\n    \\end{equation}\n    where $g(\\cdot)$ is the aggregation function, as we discussed before.\n\\end{itemize}", "cites": [1103], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the MEIRec model and its approach to heterogeneous graph neural networks without connecting it to broader themes or contrasting it with other methods like HAN or HGT in depth. There is minimal synthesis beyond stating its components, and no critical evaluation or abstraction to general principles is evident."}}
{"id": "694663be-cce7-43d1-8e3e-39300af4df29", "title": "HGAT", "level": "paragraph", "subsections": [], "parent_id": "7e8fcb1c-dc59-4fd2-98f9-8fd235c38695", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Representation Learning for NLP"], ["subsection", "Graph Neural Networks for Heterogeneous Graph"], ["subsubsection", "R-GNN based Heterogeneous GNN"], ["paragraph", "HGAT"]], "content": "HGAT~ is proposed for encoding heterogeneous graph which contains various node types but single edge types. In other words, the edge only represents connectivity. Intuitively, for a specific node, different types of neighbors may have different relevance. To fully exploit diverse structure information, HGAT firstly focuses on global types' relevance learning (type-level learning) and then learns the representation for specific nodes (node-level learning).\n\\begin{itemize}\n    \\item \\textbf{Type-level learning}. Technically, for a specific node $v_i$ and its' neighbors $N(v_i)$, HGAT get the type-specific neighbor representation as:\n\\begin{equation}\n    \\mathbf{z}_{t}^{(k)} = \\sum_{v_j \\in N_{t}(v_i)} \\mathbf{h}_{j}^{(k-1)}, t \\in \\mathcal{T}.\n\\end{equation}\nNote that we overwrite $N_{t}(v_i)$ which denotes the neighbors with node type $t$. Then they calculate the relevance of each type by attention mechanism:\n\\begin{equation}\n    \\begin{split}\n        s_t &= \\sigma(\\mathbf{q}^T[\\mathbf{h}_i^{(k-1)}, \\mathbf{z}_{t}^{(k)}]), \\\\\n        \\alpha_{t} &= \\frac{exp(s_t)}{\\sum_{t' \\in \\mathcal{T}} exp(s_{t'}) },\n    \\end{split}\n\\end{equation}\nwhere $\\mathbf{q}$ is the trainable vector.\n\\item \\textbf{Node-level learning}\nSecondly, they apply R-GCN~ like aggregation procedure for a different type of nodes. Formally, for the node $v_i$ and the type relevance scores $\\{\\alpha_t\\}, t \\in \\mathcal{T}$, HGAT calculate each neighbors' attention score as follows:\n\\begin{equation}\n\\begin{split}\n    b_{i, j} &= \\sigma(\\mathbf{q}_{1}^T \\alpha_{\\tau(v_j)} [\\mathbf{h}_i^{(k-1)}, \\mathbf{h}_j^{(k-1)}]), \\\\\n    \\beta_{i, j} &= \\frac{exp(b_{i, j})}{\\sum_{v_m \\in N(v_i)} exp(b_{i, m}) },\n\\end{split}\n\\end{equation}\nwhere $\\mathbf{q}_{1}$ is the trainable vector. Finally, HGAT applies layer-wise heterogeneous GCN to learn the node representation, which is formulated as:\n\\begin{equation}\n    \\mathbf{h}^{(k)}_{i} = \\sigma(\\sum_{t \\in \\mathcal{T}} \\sum_{v_j \\in N_{t}(v_i)} \\mathbf{W}^{(k)}_{t} \\mathbf{h}^{(k-1)}_{j} ).\n\\end{equation}\n\\end{itemize}", "cites": [259], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a clear description of the HGAT model, including its type-level and node-level learning processes with mathematical formulations. While it references the R-GCN paper, it does so primarily to draw a structural analogy rather than synthesize broader ideas. There is minimal critical analysis or abstraction beyond the specific model described, limiting the insight quality to a descriptive level."}}
{"id": "a73f030b-af1b-40bd-9087-09d414589472", "title": "MHGRN", "level": "paragraph", "subsections": [], "parent_id": "7e8fcb1c-dc59-4fd2-98f9-8fd235c38695", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Representation Learning for NLP"], ["subsection", "Graph Neural Networks for Heterogeneous Graph"], ["subsubsection", "R-GNN based Heterogeneous GNN"], ["paragraph", "MHGRN"]], "content": "MHGRN~ is an extension of R-GCN, which can leverage multi-hop information directly on heterogeneous graphs. Generally, it borrows the idea of relation path (e.g., meta-path) to model the relation of two not k-hop connected nodes and extend the existing R-GNN to the path-based heterogeneous graph representation learning paradigm. The $K$-hop relation path between node $v_i, v_j$ is denoted as:\n\\begin{equation}\n    \\Phi_k = \\{(v_i, e_{i, 1}, ..., e_{k-1, j}, v_j) | (v_i, e_{i, 1}, v_1), ..., (v_{k-1}, e_{k-1, j}) \\in \\mathcal{E}\\}.\n\\end{equation}\nNote that the heterogeneous graph may contain more than one k-hop relation path.  \n\\begin{itemize}\n\\item \\textbf{k-hop feature aggregation}. First, to make the GNN aware of the node type, they project the nodes' initial feature to the unified embedding space by type-specific linear transformation (the same as eq. \\ref{eq:feature-prj}). Considering simplification, we overwrite nodes' feature notation $\\mathbf{h}$ to represent the unified features. Then given node $v_i$, they aim to aggregate $k$-hop ($k \\in [1, K]$) neighbors' feature as follows:\n\\begin{equation}\n\\begin{split}\n    \\mathbf{z}^{\\Phi_k}_{i} = \\sum_{(v_j, e_{j, 1}, ..., e_{k-1, i}, v_i) \\in \\Phi_k} \\frac{\\alpha(v_j, e_{j, 1}, ..., e_{k-1, i}, v_i)}{\\sum_{(v_j,..., v_i) \\in \\Phi_k}\\alpha(v_j, ..., v_i)} \\prod_{l=1}^{l=K}\\prod_{o=1}^{o=K}\\mathbf{W}^{l}_{r_o}\\mathbf{h}_j, \\\\\n    (1 \\leq k \\leq K)\n\\end{split}\n\\end{equation}\nwhere $\\mathbf{W}_{r_o}^{l}, 1 \\leq l \\leq K, 1 \\leq o \\leq K$ is the learnable matrix, $r_o$ denotes $o-$th hop's edge, $\\alpha(j, v_1, ..., v_k, v_i)$ is the attention score among all k-hop paths between node $v_j$ and $v_i$. We use $\\mathbf{z}_{i}$ to denote the learned embedding for node $v_i$. \n\\item \\textbf{Fusing different relation paths}. Next, they fuse relation paths with different length via attention mechanism:\n\\begin{equation}\n    \\mathbf{z}'_i = \\sum_{k=1}^{K} \\text{Attention}(\\mathbf{q}, \\mathbf{z}_i^{\\Phi_k}) \\mathbf{z}_i^{\\Phi_k},\n\\end{equation}\nwhere $\\mathbf{q}$ is the task-specific vector (in MHGRN, it is the text-based query vector), $\\text{Attention}(): \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is the normalized attention score. Note that we omit the details of $\\alpha(j, v_1, ..., v_k, v_i)$ and $\\text{Attention}()$ since they are task-specific functions. At last, the final representation of node $v_i$ is the shortcut connection between $\\mathbf{z}_i$ and original feature $\\mathbf{h}_i$ as follows:\n\\begin{equation}\n    \\mathbf{z}_i = \\sigma(\\mathbf{W}_1 \\mathbf{z}'_i + \\mathbf{W}_2 \\mathbf{h}_i),\n\\end{equation}\nwhere $\\mathbf{W}_1, \\mathbf{W}_2$ is the learnable weights.\n\\end{itemize}", "cites": [1069], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the MHGRN model in a factual manner, outlining its components and equations. It integrates the cited paper by explaining the model's architecture but does not connect it to broader themes or other works in the field. There is minimal critical analysis or abstraction, focusing instead on technical details without evaluation or comparison."}}
{"id": "2a9cbaf4-086d-4aa9-8f6a-f167b0131ca3", "title": "HGT", "level": "paragraph", "subsections": [], "parent_id": "7e8fcb1c-dc59-4fd2-98f9-8fd235c38695", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Graph Representation Learning for NLP"], ["subsection", "Graph Neural Networks for Heterogeneous Graph"], ["subsubsection", "R-GNN based Heterogeneous GNN"], ["paragraph", "HGT"]], "content": "The HGT~ is the graph transformer for heterogeneous graphs, which build meta-relations among nodes on top of the network schema, as we discuss in the meta-path-based heterogeneous GNN paragraph. Unlike most previous works that assume the graph structure is static (i.e., time-independent), they propose a relative temporal encoding strategy to learn the time dependency.\nThe meta-relation is a triple based on the network schema. For each edge $e_{i, j}$ which links node $v_i$ and $v_j$, the meta-relation is defined as $\\Phi_{v_i, e_{i, j} v_j} = <\\tau(v_i), \\phi(e_{i, j}), \\tau(v_j)>$. To further represent the time-dependent relations, they add the timestamps to the start nodes when adding directed edges. Generally, the GNN is defined as:\n\\begin{equation}\n    \\mathbf{h}^{(k)}_{i}= \\text{Aggregation}^{(k)}_{v_j \\in N(v_i)}(\\text{Attention}^{(k)}(v_i, e_{i, j}, v_j) \\text{Message}^{(k)}(v_i, e_{i, j}, v_j)),\n\\end{equation}\nwhere $N(v_i)$ denotes the incoming nodes. We will briefly discuss three basic meta-relation based operations: attention, message message passing, and aggregation, as well as the relative temporal encoding strategy.\n\\begin{itemize}\n    \\item \\textbf{Attention operation}. The $\\text{Attention}(\\cdot, \\cdot, \\cdot)$ operation is the mutual attention that calculates the weight of two connected nodes grounded by their meta-relations. Specifically, they employ multi-head attention based on meta-relations, which is formulated as:\n\\begin{equation}\n    \\begin{split}\n        &\\text{Attn\\_head}^{i}(v_i, e_{i, j}, v_j) = \\\\ &\\text{f\\_linear}_{\\tau(v_i)}^{i}(\\mathbf{h}_{i}) \\mathbf{W}^{ATT}_{\\phi(e_{i, j})} \\text{g\\_linear}_{\\tau(v_j)}^{i}(\\mathbf{h}_{j})^T \\frac{E_{\\Phi_{v_i, e_{i, j} v_j}}}{\\sqrt{d}}, \\\\\n         &\\text{Attention}(v_i, e_{i, j}, v_j) = softmax_{v_j \\in N(v_i)}(||_{h=1}^{H} \\text{Attn\\_head}^{i}(v_i, e_{i, j}, v_j) )\n    \\end{split}\n\\end{equation}\nwhere $H$ is the number of heads, $\\text{f\\_linear}^i_{\\tau(\\cdot)}, \\text{g\\_linear}^i_{\\tau(\\cdot)}: \\mathbb{R}^{d/H} \\rightarrow \\mathbb{R}^{d/H}$ are the node-type-specific transformation functions for source nodes and target nodes respectively, $\\mathbf{W}^{ATT}_{\\phi(\\cdot)}$ is the edge-type-specific matrix, and $E_{\\Phi_{v_i, e_{i, j} v_j}}$ is the meta-path-specific scalar weight.\n\\item\\textbf{Message passing operation}. The $\\text{Message}(\\cdot)$ is the heterogeneous message passing function. Similar to the $\\text{Attention}(\\cdot, \\cdot, \\cdot)$ above, they incorporate the meta-relations into the message passing process as follows:\n\\begin{equation}\n    \\begin{split}\n        \\text{msg\\_head}^i(v_i, e_{i, j}), v_j) &= \\text{m\\_linear}^i_{\\tau(v_i)}(\\mathbf{h}_{i}) \\mathbf{W}_{\\phi(e_{i, j})}^{MSG}, \\\\\n        \\text{Message}(v_i, e_{i, j}, v_j) &= ||_{h=1}^{H} \\text{msg\\_head}^h(v_i, e_{i, j}, v_j)\n    \\end{split}\n\\end{equation}\nwhere $\\text{m\\_linear}(\\cdot): \\mathbb{R}^{d/H} \\rightarrow \\mathbb{R}^{d/H}$ is the node-type-specific transformation, and $\\mathbf{W}^{MSG}_{\\phi(\\cdot)}$ is the edge-type-specific matrix. \n\\item\\textbf{Aggregation operation}. For aggregation operation, since the $\\text{Attention}()$ function's results have been normalized by softmax function, they simply use average function as $\\text{Aggregation}(\\cdot)$. Finally, they employ meta-path-specific projection followed by residual connection to learn the final representation of each nodes as follows:\n\\begin{equation}\n    \\begin{split}\n        \\mathbf{z}_i^{(k)} &= \\sum_{v_j \\in N(v_i)}(\\text{Attention}^{(k)}(v_i, e_{i, j}, v_j) \\text{Message}^{(k)}(v_i, e_{i, j}, v_j)), \\\\\n        \\mathbf{h}_{i}^{(k)} &= \\text{A\\_linear}_{\\Phi_{v_i, e_{i, j} v_j}} (\\sigma(\\mathbf{z}_i^{(k)})) + \\mathbf{h}^{(k-1)},\n    \\end{split}\n\\end{equation}\nwhere $\\text{A\\_linear}_{\\Phi_{v_i, e_{i, j} v_j}}(\\cdot): \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ is the meta-relation-specific projection. \n\\item\\textbf{Relative Temporal Encoding}\nTo tackle the graph's time dependency, they propose the Relative Temporal Encoding mechanism to each node's embedding. Technically, they calculate the timestamp difference of target and source nodes as $\\delta_{i, j} = T(v_j) - T(v_i)$, where $T(\\cdot)$ is the timestamp of the node. Then they project the time gap to the specific embedding space. This temporal encoding is added to the source nodes' representation before GNN encoding.\n\\end{itemize}", "cites": [1103], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a detailed description of the HGT model, focusing on its components such as attention, message passing, aggregation, and temporal encoding. It integrates the cited paper but does not connect these ideas to broader trends or other works in the field. There is minimal critical analysis or abstraction beyond the specific model."}}
{"id": "fbbf5ccc-5fc3-4450-bf13-a99c97bab4d3", "title": "GNN Based Encoder-Decoder Models", "level": "section", "subsections": ["9f1362f1-76bd-4c59-b9a2-2c2404b3f155", "28adc005-9257-429d-8587-78f863731e09", "19a10d7d-1915-4f21-a026-7ddf46346d97", "f6f18c0c-ed95-40a7-bceb-d87bbe43439c"], "parent_id": "d9782043-9373-462c-b592-7d9a42b0beee", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "GNN Based Encoder-Decoder Models"]], "content": "\\label{sec:GNN Based Encoder-Decoder Models}\nEncoder-decoder architecture is one of the most widely used machine learning framework in the NLP field, such as the Sequence-to-Sequence (Seq2Seq) models. Given the great power of GNNs for modeling graph-structured data, very recently, many research efforts have been made to develop GNN-based encoder-decoder frameworks including Graph-to-Tree~ and Graph-to-Graph~ models.\nIn this section, we will first introduce the typical Seq2Seq models, and then discuss various graph-based encoder-decoder models for various NLP tasks.\n\\begin{figure}[ht!]\n\\centering\n\\vspace{-2mm}\n\\includegraphics[width=12.0cm]{graph2tree.pdf}\n\\vspace{-8mm}\n\\caption{Overall architecture for graph based encoder-decoder model which contains both the Graph2Seq and Graph2Tree models. Input and output are from \\textbf{JOBS640} dataset~ . Nodes like $\\textit{\\textbf{S}}_1$, $\\textit{\\textbf{S}}_2$ stand for sub-tree nodes, which new branches are generated from.}\n\\label{fig:graph-encoder-decoder-sample}\n\\vspace{-0mm}\n\\end{figure}", "cites": [1079, 2401, 1104, 1105, 243], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of GNN-based encoder-decoder models in NLP, mentioning a few recent works and the general motivation for using graphs in such frameworks. However, it lacks deeper synthesis of the cited papers and does not engage in meaningful comparison or critical evaluation of their approaches. The inclusion of a figure and brief references to graph-to-tree and graph-to-graph models hints at some abstraction, but the section as a whole remains focused on factual presentation rather than insightful analysis."}}
{"id": "9f1362f1-76bd-4c59-b9a2-2c2404b3f155", "title": "Sequence-to-Sequence Models", "level": "subsection", "subsections": ["5723bda4-9efc-4332-a9c1-e600f1d7529e", "b3ecda67-feea-4165-9404-8ad7b34a24e9", "c22d16b8-76ce-4502-afc2-83a7146f2846"], "parent_id": "fbbf5ccc-5fc3-4450-bf13-a99c97bab4d3", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "GNN Based Encoder-Decoder Models"], ["subsection", "Sequence-to-Sequence Models"]], "content": "Sequence-to-Sequence (Seq2Seq) learning~ is one of the most widely used machine learning paradigms in the NLP field.\nIn this section, we first give a brief overview of Seq2Seq learning and introduce some typical Seq2Seq techniques. Then we pinpoint some known limitations of Seq2Seq learning as well as its solutions, namely, incorporating more structured encoder-decoder models as alternatives to Seq2Seq models so as to encode more complex data structures.", "cites": [2401, 243], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic overview of Seq2Seq learning and introduces some limitations, but it lacks deeper synthesis of the cited papers, does not critically evaluate or compare different approaches, and does not abstract broader principles or trends. It primarily describes the general concept and hints at structural alternatives without offering insightful analysis."}}
{"id": "5723bda4-9efc-4332-a9c1-e600f1d7529e", "title": "Overview", "level": "subsubsection", "subsections": [], "parent_id": "9f1362f1-76bd-4c59-b9a2-2c2404b3f155", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "GNN Based Encoder-Decoder Models"], ["subsection", "Sequence-to-Sequence Models"], ["subsubsection", "Overview"]], "content": "Sequence-to-Sequence (Seq2Seq) models were originally developed by  and  for \nsolving general sequence-to-sequence problems (e.g., machine translation).\nThe Seq2Seq model is an end-to-end encoder-decoder framework which learns to map a variable-length input sequence to a variable-length output sequence.\nBasically, the idea is to use an RNN-based encoder to read the input sequence (i.e., one token at a time), to build up a fixed-dimensional vector representation, and then use an RNN-based decoder to generate the output sequence (i.e., one token at a time) conditioned on the encoder output vector. \nThe decoder is essentially a RNN language model except that it is conditioned on the input sequence.\nOne of the most common Seq2Seq variants is to apply a Bidirectional LSTM encoder to encode the input sequence, and apply a LSTM decoder to decode the output sequence~.\nOther Seq2Seq variants replace LSTM with Gated Recurrent Units (GRUs)~, Convolutional Neural Networks (CNNs)~ or Transformer models~.\nDespite the promising achievements in many NLP applications such as machine translation, the original Seq2Seq models suffer a few issues such as the information bottleneck of the fixed-dimensional intermediate vector representation, and the exposure bias of cross-entropy based sequence training.\nIn the original Seq2Seq architecture, the intermediate vector representation becomes an information bottleneck because it summarizes the rich information of the input sequence as a fixed-dimensional embedding, which serves as the only knowledge source for the decoder to generate a high-quality output sequence.\nIn order to increase the learning capacity of the original Seq2Seq models, many effective techniques have been proposed.", "cites": [2401, 790, 38, 243], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic description of Seq2Seq models and their evolution by mentioning different variants (LSTM, GRU, CNN, Transformer) and the original approach from cited works. However, it lacks deeper synthesis of ideas across papers and does not offer a novel framework or critical evaluation of the cited methods. The abstraction is minimal, focusing more on specific components like encoder-decoder structure rather than overarching principles."}}
{"id": "b3ecda67-feea-4165-9404-8ad7b34a24e9", "title": "Approach", "level": "subsubsection", "subsections": [], "parent_id": "9f1362f1-76bd-4c59-b9a2-2c2404b3f155", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "GNN Based Encoder-Decoder Models"], ["subsection", "Sequence-to-Sequence Models"], ["subsubsection", "Approach"]], "content": "The attention mechanism~ was developed to learn the soft alignment between the input sequence and output sequence. Specifically, at each decoding step $t$, an attention vector indicating a probability distribution over the source words is computed as\n\\begin{equation}\\label{eq:attn}\n\\begin{aligned}\ne^t_i &= \\text{f}(\\vec{h}_i, \\vec{s}_t)\\\\\n\\vec{a}^t &= \\text{softmax}(\\vec{e}^t),\n\\end{aligned}\n\\end{equation}\nwhere $\\text{f}$ can be arbitrary neural network computing the relatedness between the decoder state $\\vec{s}_t$ and encoder hidden state state $\\vec{h}_i$. One common option is to apply an additive attention mechanism $\\text{f}(\\vec{h}_i, \\vec{s}_t) = \\vec{v}^T \\text{tanh}(\\vec{W}_h \\vec{h}_i + \\vec{W}_s \\vec{s}_t + b)$ where $\\vec{v}$, $\\vec{W}_h$, $\\vec{W}_s$ and $b$ are learnable weights. Given the attention vector $\\vec{a}^t$ at the $t$-th decoding step, the context vector can be computed as a weighted sum of the encoder hidden states, formulated as\n\\begin{equation}\n\\begin{aligned}\n\\vec{h}_t^* = \\sum_i a^t_i \\vec{h}_i,\n\\end{aligned}\n\\end{equation}\nThe computed context vector will be concatenated with the decoder state, and fed through some neural network for producing a vocabulary distribution.\nThe copying mechanism~ was introduced to directly copy tokens from the input sequence to the output sequence in a learnable manner. This can be very helpful in some scenarios where the output sequence refers to some named entities or out-of-vocabulary tokens in the input sequence. Specifically, at each decoding step $t$, a generation probability will be calculated for deciding whether to generate a token from the vocabulary or copy a token from the input sequence by sampling from the attention distribution $\\vec{a}^t$. The generation probability can be computed as\n\\begin{equation}\n\\begin{aligned}\np_{\\text{gen}} = \\sigma(\\vec{w}_{h^*}^T \\vec{h}_t^* + \\vec{w}_s^T \\vec{s}_t + \\vec{w}_x^T \\vec{x}_t + b_\\text{ptr})),\n\\end{aligned}\n\\end{equation}\nwhere $\\vec{w}_{h^*}$, $\\vec{w}_s$, $\\vec{w}_x$ and $b_\\text{ptr}$ are learnable weights, $\\sigma$ is a sigmoid function, and $p_{\\text{gen}}$ is a scalar between 0 and 1.\nThe coverage mechanism~ was proposed to encourage the full utilization of different tokens in the input sequence. This can be useful in some NLP tasks such as machine translation. Specifically, at each decoding step $t$, a coverage vector $\\vec{c}^t$ which is the aggregated attention vectors over all previous decoding steps will be computed as\n\\begin{equation}\n\\begin{aligned}\n\\vec{c}^t = \\sum_{t'=0}^{t-1} \\vec{a}^{t'}.\n\\end{aligned}\n\\end{equation}\nIn order to encourage better utilization of those source tokens that have not received enough attention scores so far,\nthe above coverage vector will be used as extra input to the aforementioned attention mechanism~\\cref{eq:attn}, that is, \n\\begin{equation}\n\\begin{aligned}\ne^t_i &= \\text{f}(\\vec{h}_i, \\vec{s}_t, c_i^t)\n\\end{aligned}\n\\end{equation}\nTo avoid generating repetitive text, a coverage loss is calculated at each decoding step to penalize repeatedly attending to the same locations, formulated as,\n\\begin{equation}\n\\begin{aligned}\n\\text{covloss}_t = \\sum_i \\text{min}(a_i^t, c_i^t)\n\\end{aligned}\n\\end{equation}\nThe above coverage loss essentially penalizes the overlap between the attention vector and the coverage vector, and is bounded to $\\sum_i a_i^t = 1$. It will be reweighted and added to the overall loss function.\nThe exposure bias occurs when during the training phase, the ground-truth token is used as the input (i.e., for better supervision) to the decoder for predicting the next token, while in the inference phase, the decoder's prediction from the previous time step is used as the input for next step prediction (due to no access to the ground-truth token).\nIn order to reduce this gap between training and inference phases and thus increase the generalization ability of the original Seq2Seq models, scheduled sampling~ was proposed to alleviate this issue by taking as input either the decoder's prediction from the previous time step or the ground truth with some probability for next step prediction, and gradually decreasing the probability of feeding in the ground truth at each iteration of training.\nThe celebrated Seq2Seq models equipped with the above effective techniques have achieved great successes in a wide range of NLP applications such as neural machine translation~, \ntext summarization~,\ntext generation~,\nspeech recognition~,\nand dialog systems~.", "cites": [1114, 1107, 1110, 1113, 1111, 167, 1112, 1109, 790, 1106, 168, 1108, 7186, 7008], "cite_extract_rate": 1.0, "origin_cites_number": 14, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of key components (attention, copying, coverage, scheduled sampling) in sequence-to-sequence models, using mathematical formulations and brief mentions of their application in NLP tasks. It integrates concepts from the cited papers to explain their mechanisms but lacks deeper comparative analysis or critical evaluation of their limitations. Some abstraction is attempted by highlighting general roles of these components, but it remains limited to surface-level patterns."}}
{"id": "c22d16b8-76ce-4502-afc2-83a7146f2846", "title": "Discussions", "level": "subsubsection", "subsections": [], "parent_id": "9f1362f1-76bd-4c59-b9a2-2c2404b3f155", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "GNN Based Encoder-Decoder Models"], ["subsection", "Sequence-to-Sequence Models"], ["subsubsection", "Discussions"]], "content": "Seq2Seq models were originally developed to solve sequence-to-sequence problems, that is, to map a sequential input to a sequential output.\nHowever, many NLP applications naturally admit graph-structured input data such as dependency graphs~, constituency graphs~, AMR graphs~, IE graphs~ and knowledge graphs~.\nIn comparison with sequential data, graph-structured data is able to encode rich syntactic or semantic relationships among objects.\nMoreover, even if the raw input is originally represented in a sequential form, it can still benefit from explicitly incorporating rich structural information (e.g., domain-specific knowledge) to the sequence.\nThe above situations essentially call for an encoder-decoder framework for learning a graph-to-X mapping where X can stand for a sequence, tree or even graph.\nExisting Seq2Seq models face a significant challenge in learning an accurate mapping from graph to the appropriate target due to its incapability of modeling complex graph-structured data.\nVarious attempts have been made in order to extend Seq2Seq models to handle Graph-to-Sequence problems where the input is graph-structured data.\nA simple and straightforward approach is to directly linearize the structured graph data into the sequential data~, and apply the Seq2Seq models to the resulting sequence. However, this kind of approaches suffer significant information loss, which leads to downgraded performance.\nThe root cause of RNN's incapability of modeling complex structured data is because it is a linear chain.\nIn light of this, some research efforts have been devoted to \nextend Seq2Seq models.\nFor instance, Tree2Seq~ extends Seq2Seq models by adopting Tree-LSTM~ which is a generalization of chain-structured LSTM to tree-structured network topologies.\nSet2Seq~ is an extension of Seq2Seq models that goes beyond sequences and handles the input set using the attention mechanism.\nAlthough these Seq2Seq extensions achieve promising results on certain classes of problems, none of them can model arbitrary graph-structured data in a principled way.", "cites": [1072, 1066, 1054, 1079, 7048, 255, 258, 1094, 1055, 8412, 1071, 1115], "cite_extract_rate": 0.75, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the cited works by connecting different extensions of Seq2Seq models to address graph-structured input, forming a coherent narrative around the challenges and evolution of these models. It offers critical insights, such as the limitations of linearizing graphs and the inability of most extensions to handle arbitrary graphs. The abstraction level is strong as it generalizes the discussion to broader graph-to-X mapping problems and the need for principled modeling of structured data."}}
{"id": "a07c4a11-d141-401a-8a0f-b840a69f7282", "title": "Overview", "level": "subsubsection", "subsections": [], "parent_id": "28adc005-9257-429d-8587-78f863731e09", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "GNN Based Encoder-Decoder Models"], ["subsection", "Graph-to-Sequence Models"], ["subsubsection", "Overview"]], "content": "To address the aforementioned limitations of Seq2Seq models on encoding rich and complex data structures, recently, a number of graph-to-sequence encoder-decoder models for NLP tasks have been proposed~.\nThis kind of Graph2Seq models typically adopt a GNN based encoder and a RNN/Transformer based decoder.\nCompared to the Seq2Seq paradigm, the Graph2Seq paradigm is better at capturing the rich structure information of the input text and can be applied to arbitrary graph-structured data.\nGraph2Seq models have shown superior performance in comparison with Seq2Seq models in a wide range of NLP tasks including \nneural machine translation~,\nAMR-to-text~,\ntext summarization~,\nquestion generation~,\nKG-to-text~,\nSQL-to-text~,\ncode summarization~,\nand semantic parsing~.", "cites": [1053, 1066, 7329, 1067, 57, 1077, 1074, 1055, 1071], "cite_extract_rate": 0.4090909090909091, "origin_cites_number": 22, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of Graph2Seq models, listing their components and applications. However, it lacks deeper synthesis across the cited papers, critical evaluation of their strengths or weaknesses, and abstraction to broader principles or trends. It serves more as a descriptive summary than as an insightful analysis."}}
{"id": "57029bee-a401-49a0-8fbe-0bae2c0a56d2", "title": "Graph-based Encoders", "level": "paragraph", "subsections": [], "parent_id": "094b53f5-0e7a-432e-ab02-aed8d3da4405", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "GNN Based Encoder-Decoder Models"], ["subsection", "Graph-to-Sequence Models"], ["subsubsection", "Approach"], ["paragraph", "Graph-based Encoders"]], "content": "Early Graph2Seq methods and their follow-up works~ mainly used some typical GNN variants as the graph encoder inclduing GCN, GGNN, GraphSAGE and GAT.\nSince the edge direction in a NLP graph often encodes critical information about the semantic relations between two vertices, it is often extremely helpful to capture the bidirectional information of text~.\nIn the literature of Graph2Seq paradigm, some efforts have been made to extend the existing GNN models to handle directed graphs.\nThe most common strategy is to introduce separate model parameters for different edge directions (i.e., incoming/outgoing/self-loop edges) when performing neighborhood aggregation~.\nBesides the edge direction information, many graphs in NLP applications are actually multi-relational graphs where the edge type information is very important for the downstream task.\nIn order to encode edge type information, some works~ have extended them by having separate model parameters for different edge types (i.e., similar ideas have been used for encoding edge directions).\nHowever, in many NLP applications (e.g., KG-related tasks), the total number of edge types is large, hence the above strategy can have severe scalability issues.\nTo this end, some works~ proposed to bypass this problem by converting a multi-relational graph to a Levi graph~ and then utilize existing GNNs designed for homogeneous graphs as encoders.\nAnother commonly adopted technique is to explicitly incorporate edge embeddings into the message passing mechanism~.\nBesides the above widely used GNN variants, some Graph2Seq works also explored other GNN variants such as GRN~ and GIN~.\nNotably, GRN is also capable of handling multi-relational graphs by explicitly including edge embeddings in the LSTM-style message passing mechanism.", "cites": [1053, 1116, 1066, 826, 7326, 1076, 1067, 7215, 57, 1077, 1074, 259, 1055], "cite_extract_rate": 0.52, "origin_cites_number": 25, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes various graph encoder techniques by identifying common strategies (e.g., handling edge directions and types) and their motivations, drawing from multiple cited works. It critically evaluates scalability issues of parameter-heavy methods and suggests alternative solutions like Levi graphs and edge embeddings. The abstraction is strong, as it generalizes the approaches into broader design patterns and challenges common in NLP tasks."}}
{"id": "da70e58a-9b81-496d-8767-25ca9cfe3dc3", "title": "Node \\& Edge Embeddings Initialization", "level": "paragraph", "subsections": [], "parent_id": "094b53f5-0e7a-432e-ab02-aed8d3da4405", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "GNN Based Encoder-Decoder Models"], ["subsection", "Graph-to-Sequence Models"], ["subsubsection", "Approach"], ["paragraph", "Node \\& Edge Embeddings Initialization"]], "content": "For GNN based approaches, initialization of nodes and edges is extremely critical.\nWhile both CNNs and RNNs are good at capturing local dependencies among consecutive words in text, GNNs do well in capturing local dependencies among neighboring nodes in a graph.\nMany works on Graph2Seq have shown benefits of initializing node and/or edge embeddings by applying CNNs~ or bidirectional RNNs (BiRNNs)~ to the word embedding sequence before applying the GNN based encoder.\nSome works also explored to initialize node/edge embeddings with BERT embeddings+BiRNNs~ or RoBERTa+BiRNNs~.", "cites": [1077, 1067, 7329, 57, 1071], "cite_extract_rate": 0.45454545454545453, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of node and edge embedding initialization methods used in Graph2Seq models, citing relevant works but without establishing deeper connections or a novel synthesis. It lacks critical evaluation of the approaches and does not abstract broader patterns or principles beyond the cited papers."}}
{"id": "9277353c-31be-4450-946d-a914250086d1", "title": "Sequential Decoding Techniques", "level": "paragraph", "subsections": [], "parent_id": "094b53f5-0e7a-432e-ab02-aed8d3da4405", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "GNN Based Encoder-Decoder Models"], ["subsection", "Graph-to-Sequence Models"], ["subsubsection", "Approach"], ["paragraph", "Sequential Decoding Techniques"]], "content": "Since the main difference between Seq2Seq and Graph2Seq models is on the encoder side, common decoding techniques used in Seq2Seq models such as attention mechanism~, copying mechanism~, coverage mechanism~,\nand scheduled sampling~\ncan also be adopted in Graph2Seq models with potential modifications.\nSome efforts have been made to adapt common decoding techniques to the Graph2Seq paradigm.\nFor example, in order to copy the whole node attribute containing multi-token sequence from the input graph to the output sequence,  extended the token-level copying mechanism to the node-level copying mechanism.\nTo combine the benefits of both sequential encoder and graph encoder,  proposed to fuse their outputs to a single vector before feeding it to a decoder.  designed separate attention modules for sequential encoder and graph encoder, respectively.\n\\begin{equation}\n\\begin{aligned}\na_i^v &= \\text{attn\\_v}(\\vec{s}_t, \\vec{h}_i^v)\\\\\n\\vec{c}^v &= \\sum_i a_i^v \\vec{h}_i^v\\\\\na_j^s &= \\text{attn\\_s}(\\vec{s}_t, \\vec{h}_j^s, \\vec{c}^v)\\\\\n\\vec{c}^s &= \\sum_i a_j^s \\vec{h}_j^s\\\\\n\\vec{c} &= \\vec{c}^v || \\vec{c}^s\n\\end{aligned}\n\\end{equation}\nwhere $\\vec{h}_i^v$ and $\\vec{h}_j^s$ are the graph encoder outputs and sequential encoder outputs, respectively.\n$\\vec{c}$ is the concatenation of the graph context vector $\\vec{c}^v$ and sequential context vector $\\vec{c}^s$.\nIn order to tackle the limitations (e.g., exposure bias and discrepancy between the training and inference phases) of cross-entropy based sequential training,  proposed to train the Graph2Seq system by minimizing a hybrid loss combining both cross-entropy loss and reinforcement learning~ loss.\nWhile LSTM or GRU based decoders are the most commonly used decoder in Graph2Seq models, some works also employed a Transformer based decoder~.", "cites": [1116, 1112, 7008, 1114, 167, 168, 57, 1106, 1055, 1071, 1052], "cite_extract_rate": 0.6111111111111112, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section shows reasonable synthesis by connecting decoding techniques from traditional Seq2Seq to Graph2Seq models and integrating several adaptations from the cited works. It includes critical points such as the limitations of cross-entropy training and the use of reinforcement learning as a solution. The abstraction is moderate, as it identifies broader patterns in decoding strategies and highlights the shift from token-level to node-level copying mechanisms."}}
{"id": "d3f2e918-3796-45fa-a671-0dde1b583f69", "title": "Overview", "level": "subsubsection", "subsections": [], "parent_id": "19a10d7d-1915-4f21-a026-7ddf46346d97", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "GNN Based Encoder-Decoder Models"], ["subsection", "Graph-to-Tree Models"], ["subsubsection", "Overview"]], "content": "Compared to Graph2Seq model, which considers the structural information in the input side, many NLP tasks also contain outputs represented in a complex structured, such as trees, which are also rich in structural information at the output side, e.g., syntactic parsing, semantic parsing, math word problem solving. It is a natural choice for us to consider the structural information of these outputs. \nTo this end, some Graph2Tree models are proposed to incorporate the structural information in both the input and output side, which make the information flow in the encoding-decoding process more complete.", "cites": [1117, 1079], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a brief descriptive explanation of the motivation for Graph2Tree models but lacks significant synthesis of the two cited papers. It does not draw meaningful connections or integrate ideas from the papers to build a broader narrative. Additionally, there is minimal critical analysis or abstraction to higher-level principles or trends in the literature."}}
{"id": "eb5cdbe7-5fca-4b64-86dc-4e85b0147d04", "title": "Graph construction", "level": "paragraph", "subsections": [], "parent_id": "310c1fd5-4afe-4b21-8b6b-77aa77e9674c", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "GNN Based Encoder-Decoder Models"], ["subsection", "Graph-to-Tree Models"], ["subsubsection", "Approach"], ["paragraph", "Graph construction"]], "content": "The graph construction module, which is usually highly related to specific tasks, could be divided into two categories: one with auxiliary information and one without auxiliary information. For the former,  use syntactic graph in both semantic parsing and math word problem solving tasks, which consists of the original sentence and the syntactic pasing tree (dependency and constituency tree). And the input graph in  considers the graph composed of the abstract syntax tree (AST) of a fragment of source code. For the latter, the input can usually form a task graph itself without auxiliary information. For example,  employ the relationship between different numbers in the math word problem in the graph construction module.", "cites": [1079, 8413], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of graph construction approaches in GNN-based encoder-decoder models, citing two papers but without integrating their ideas into a broader framework. It makes minimal connections between the papers and does not offer critical analysis or highlight limitations. The generalization is limited, focusing only on task-specific graph construction examples without deeper abstraction or synthesis."}}
{"id": "e304fba8-7cb1-4554-97cc-faa4fb00b24f", "title": "Graph encoder", "level": "paragraph", "subsections": [], "parent_id": "310c1fd5-4afe-4b21-8b6b-77aa77e9674c", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "GNN Based Encoder-Decoder Models"], ["subsection", "Graph-to-Tree Models"], ["subsubsection", "Approach"], ["paragraph", "Graph encoder"]], "content": "Graph encoder is used for embedding the input graph into the latent representation. To implementing the graph encoder module, several graph2tree models use relatively simple GNN models, such as GCN, GGNN, and GraphSAGE. For , it uses a bidirectional variant of the GraphSage model, and  exploit the GCN model before a transformer layer. And  simply adopt the GGNN model as its neural encoder.", "cites": [1079, 242, 8413, 211], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of how different graph2tree models implement graph encoders using various GNN architectures. It lists the models used (GCN, GGNN, GraphSAGE) and their adoption in specific works but lacks deeper synthesis, critical comparison, or abstraction of broader design principles or trends in graph encoder development for NLP."}}
{"id": "6e1d9841-3353-409d-b9a8-8cc79362b650", "title": "Attention", "level": "paragraph", "subsections": [], "parent_id": "310c1fd5-4afe-4b21-8b6b-77aa77e9674c", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "GNN Based Encoder-Decoder Models"], ["subsection", "Graph-to-Tree Models"], ["subsubsection", "Approach"], ["paragraph", "Attention"]], "content": "The attention module is a key component in an encoder-decoder framework, which carry the important information for bridging the input and output semantics. In the graph2tree model, the input graph often contains different types of nodes, while the traditional attention module can not distinguish between these nodes. In , the author uses the separate attention module to calculate the attention vector for different nodes in the input graph where some nodes is from the original sentence, and others are composed of the nodes in parsing trees generated by the external parser. It has been validated that distinguishing these two types of nodes could facilitate better learning process than the original attention module. This idea is similar to the application of Tree2Seq attention module in machine translation.\nSpecifically in , the decoder generates the tree structure by representing some branching nodes as non-terminal nodes, i.e., node $S_1$ in Figure \\ref{fig:tree-decoder-sample}. Once these nodes generated, the decoder will start a new sequential decoding process. \nThe decoder hidden state $\\mathbf{s}_t$ at time step $t$ is calculated a\n\\vspace{-2mm}\n\\begin{equation}\n  \\textbf{s}_t = f_{decoder}(y_{t-1}, \\textbf{s}_{t-1};\\textbf{s}_{par};\\textbf{s}_{sib}),\n\\end{equation}\nwhere the $\\textbf{s}_{par}, \\textbf{s}_{sib}$ stand for the parent node hidden state and sibling node hidden state as illustrated in Figure \\ref{fig:tree-decoder-sample}. After the current hidden state generated, the output module including attention layer is calculated as follows:\n\\begin{equation}\n\\alpha_{t(v)} = \\frac{\\exp(score( {\\textbf{z}_v},\\textbf{s}_t))}{\\exp(\\sum_{k=1}^{V_1} score({\\textbf{z}_{k}},\\textbf{s}_t))}, \\forall v \\in \\mathcal{V}_1\n\\end{equation}\n\\vspace{-2mm}\n\\begin{equation}\n\\beta_{t(v)} = \\frac{\\exp(score( {\\textbf{z}_v},\\textbf{s}_t))}{\\exp(\\sum_{k=1}^{V_2} score({\\textbf{z}_{k}},\\textbf{s}_t))}, \\forall v \\in \\mathcal{V}_2\n\\end{equation}\n\\vspace{0mm}\n\\vspace{-4mm}\n\\begin{align}\n\\textbf{c}_{v_1} &= \\sum \\alpha_{t(v)} \\textbf{z}_v, \\forall v \\in \\mathcal{V}_1 \\\\\n\\textbf{c}_{v_2} &= \\sum \\beta_{t(v)} \\textbf{z}_v, \\forall v \\in \\mathcal{V}_2\n\\end{align}\nwhere $\\mathbf{z}_v$ denotes to the learned node embedding for node $\\mathbf{v}$, $\\mathcal{V}_1$ denotes to the node set including all words from original sentences, and $\\mathcal{V}_2$ denotes to another node set including all other nodes.\nWe then concatenate the context vector $\\textbf{c}_{v_1}$, context vector $\\textbf{c}_{v_2}$ and decoder hidden state $\\textbf{s}_t$ to compute the final attention hidden state at this time step as:\n\\vspace{-1mm}\n\\begin{equation}\n    \\tilde{\\textbf{s}_t} = \\tanh(W_c \\cdot [\\textbf{c}_{v_1};\\textbf{c}_{v_2};\\textbf{s}_t]+b_c),\n\\end{equation}\nwhere $W_c$ and $b_c$ are learnable parameters. The final context vector $\\bm{\\tilde{s_{t}}}$ is further fed to the output layer which is a softmax function after a feed-forward layer.", "cites": [1079, 7048], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates the attention mechanism from both the graph2tree and Tree2Seq models, highlighting how attention can be modified to handle different node types in graph structures. It provides a mathematical abstraction of the model components and links the idea to the broader context of syntactic modeling in NMT. However, it lacks deeper comparative or critical analysis of the approaches and does not fully synthesize insights into a novel overarching framework."}}
{"id": "a16cfc4d-e006-4311-8c76-aaf457845113", "title": "Tree decoder", "level": "paragraph", "subsections": [], "parent_id": "310c1fd5-4afe-4b21-8b6b-77aa77e9674c", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "GNN Based Encoder-Decoder Models"], ["subsection", "Graph-to-Tree Models"], ["subsubsection", "Approach"], ["paragraph", "Tree decoder"]], "content": "The output of some applications (i.e., semantic parsing, code generation, and math word problem) contain structural information, for example, the output in math word problem is a mathematical equation, which can be expressed naturally by the data structure of the tree. To generate these kinds of outputs, tree decoders are widely used in these tasks. Tree decoders can be divided into two main parts as shown in Figure \\ref{fig:tree-decoder-sample}, namely, dfs (depth first search) based tree decoder, and bfs (breadth first search) based tree decoder.\nFor bfs-based decoders, the main idea is to represent all the sub-trees in the tree as non-terminal nodes, and then use sequence decoding to generate intermediate results. If the results contains non-terminals, then we start branching (begin a new decoding process) with this node as the new root node, until the entire tree is expanded. \nFor dfs-based decoders, they regards the entire tree generation process as a sequence of actions. For example, in the generation of a binary tree (mathematical equation) in , the root node is generated in priority at each step, following by the generation of the left child node. After all the left child nodes are generated, a bottom-up manner is adopted to begin the generation of the right child nodes. \nIn addition, the tree decoder is constantly evolving, and some techniques are proposed to collect more information during the decoding process or leverage the information from the input or output, such as parent feeding, sibling feeding, sub-tree copy, tree based re-ranking and other techniques. At the same time, the wide application of the transformer model also brings about many transformer based tree decoders, which proves the wide application of tree decoder and Graph2tree model.", "cites": [1079, 8413, 1119, 1118], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of tree decoders in GNN-based encoder-decoder models, distinguishing between bfs and dfs approaches and mentioning additional techniques like parent feeding and subtree copy. It integrates a few cited papers but does so in a limited way, mainly to illustrate specific applications (e.g., code generation, UI layout). There is minimal critical evaluation or deeper abstraction, with no clear synthesis of broader principles or trends."}}
{"id": "d160e831-f1e9-4559-8b12-9d2643fd14c0", "title": "Graph-to-graph transformation", "level": "paragraph", "subsections": [], "parent_id": "5f8fca73-7ec2-462f-86b4-99d4fde07372", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "GNN Based Encoder-Decoder Models"], ["subsection", "Graph-to-Graph Models"], ["subsubsection", "Overview"], ["paragraph", "Graph-to-graph transformation"]], "content": "Graph-to-graph models aims to deal with the problem of deep graph transformation~. The goal of graph transformation is to transform an input graph in the source domain to the corresponding output graphs in the target domain via deep learning. Emerging as a new while important problem, deep graph transformation has multiple applications in many areas, such as molecule optimization~ and malware confinement in cyber security~. Considering the entities that are being transformed during the translation process, there are three categories of sub-problems: node transformation, edge transformation, and node-edge-co-transformation. For node transformation, only the node set or nodes' attributes in the input graph can change during the transformation process. For edge transformation, only the graph topology or edge' attributes in the input graph can change during the transformation process. While for node-edge-co-transformation, both the attributes of nodes and edges can change.", "cites": [1120, 1121, 7327, 1104, 1105], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the concept of graph-to-graph transformation by integrating ideas from multiple papers, particularly distinguishing between node transformation, edge transformation, and node-edge-co-transformation. It abstracts the problem into a general framework applicable across domains, such as chemistry and cybersecurity. While it highlights the limitations of existing approaches (e.g., inability to predict both node and edge attributes simultaneously), the critical analysis is not as deep or nuanced as it could be."}}
{"id": "8a5883ba-0d9a-4c87-885a-74e8f9a0384e", "title": "Graph-to-Graph for NLP", "level": "paragraph", "subsections": [], "parent_id": "5f8fca73-7ec2-462f-86b4-99d4fde07372", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "GNN Based Encoder-Decoder Models"], ["subsection", "Graph-to-Graph Models"], ["subsubsection", "Overview"], ["paragraph", "Graph-to-Graph for NLP"]], "content": "Since the natural language or information knowledge graphs can be naturally formalized as graphs with a set of nodes and their relationships, many generation tasks in the domain of NLP can be formalized as a graph transformation problem, which can further be solved by the graph-to-graph models. In this way, the semantic structural information of both the input and output sentences can be fully utilized and captured. Here, two important NLP tasks (i.e., information extraction and semantic parsing), which can be formalized as the graph-to-graph problems, are introduced as follows.\n\\textbf{Graph Transformation for Information Extraction}.\nInformation extraction is to extract the structured information from a text, which usually consists of name entity recognition, relation extraction and co-reference linking. The problem of information extraction can be formalized as a graph transformation problem, where the input is the dependency or constituency graph of a text and the output is the information graph. In input dependency or constituency graph, each node represents a word token and each edge represent the dependency relationship between two nodes. In output information graph, each node represent a name entity and each edge represent the either the semantic relation the co-reference link between two entities. In this way, the information extraction is about generating the output information graph given the input dependency or constituency graph.\n\\textbf{Graph Transformation for Semantic Parsing}.\nThe task of semantic parsing is about mapping natural language to machine interpretable meaning representations, which in turn can be expressed in many different formalisms, including lambda calculus, dependency-based compositional semantics, frame semantics, abstract meaning representations (AMR), minimal recursion semantics, and discourse representation theory~. Explicitly or implicitly, a representation in any of these formalisms can be expressed as a directed acyclic graph (DAG). Thus, semantic parsing can also be formalized as a graph transformation problem, where the input is the dependency or constituency graph and the output is the directed acyclic graph for semantics. For example, the semantic formalism for AMR can be encoded as a rooted, directed, acyclic\ngraph, where nodes represent concepts, and labeled directed edges represent the relationships between them~.\nSequence-to-graph transformation can be regarded as a special case of the graph-to-graph, where the input sequence is a line-graph. Sequence-to-graph models are popularly utilized for AMR parsing tasks, where the goal is to learning the mapping from a sentence to its AMR graph~. To generate the AMR tree with indexed node, the approach to parsing is formalized as a two-stage process: node prediction and edge prediction. The whole process is implemented by an pointer-network, where the encoder is a multi-layer bi-direction-RNN and the nodes in the target graphs are predicted in sequence. After this, the edges among each pair of nodes are predicted based on the learnt embedding of the ending nodes.", "cites": [8414, 1122], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from the cited papers by connecting the idea of semantic parsing as a graph prediction problem with specific applications like AMR parsing. It abstracts these methods by describing general patterns such as the use of pointer networks and sequence-to-graph transduction. However, the critical analysis is limited—while it notes the use of external resources in some approaches, it does not deeply evaluate the trade-offs or limitations of the graph-to-graph paradigm."}}
{"id": "ed5ea8af-987a-454b-80e2-36f87fbdf134", "title": "Approach", "level": "subsubsection", "subsections": [], "parent_id": "f6f18c0c-ed95-40a7-bceb-d87bbe43439c", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "GNN Based Encoder-Decoder Models"], ["subsection", "Graph-to-Graph Models"], ["subsubsection", "Approach"]], "content": "In this subsection, we introduce an example graph-to-graph model in dealing with the task of information extraction by describing its challenges and methodologies. \n\\textbf{Challenges for Graph-to-Graph IE}.\nThere are three main challenges in solving the graph-to-graph IE problem: (1) Different resolution between the input and output graphs. The nodes in the input dependency graph represent word tokens, while the nodes in the output information graph represent name entities; (2) Difficult to involve both the sentence-level and word-level. To learn the word embedding in the graph encoder, it is important to consider both the word interactions in a sentence and the sentence interactions in a text; and (3) Difficult to model the dependency between entity relations and co-reference links in the graph decoder. The generation process of entity relation and co-reference links are dependent on each other. For example, if words “Tom” and “He” in two separate sentences have a co-reference link, and “Tom” and “London” has the relation of “born\\_In”, then “He” and “London” should also have the relation of “born\\_In”. \n\\textbf{Methodology.}\nTo solve the above mentioned challenges for the graph-to-graph IE task, here we introduce an end-to-end encoder-decoder based graph-to-graph transformation model, which transforms the input constructed graphs of text into the information graphs which contains name entities as well as co-reference links and relations among entities. The whole model consists of a hierarchy graph encoder for span embedding and a parallel decoder with co-reference link and entity relation generation. \nFirst, to construct the initial graphs, the dependency information are formalized into a heterogeneous graph which consists of nodes representing word tokens (i.e., word-level nodes) and nodes representing sentences (i.e., sentence-level nodes). There are also three types of edges in the graph. One type of edges represent the dependency relationships between word-level nodes (i.e., dependency edges). One type of edges represent the adjacent relationship between sentence-level nodes (i.e., adjacent edges). The last type of edges represent the belongingness between the word-level and sentence-level nodes (i.e., interactive edges).\nSecond, the constructed heterogeneous graph is inputted into the encoder, which is based on a hierarchy graph convolution neural network. Specifically, for each layer, the conventional message passing operations are first conducted along the dependency and adjacent edges to update the embedding of word-level and sentence-level nodes. Then, the conventional message passing operations are conducted along the interactive edges based on the newly updated word-level and sentence-level nodes' embedding. After several layers of propagation, the embedding of word-evel nodes will contains both the dependency and sentence-level information. \nThird, based on the words' embedding from the encoder, the name entities can be first extracted via BIO Tagging~. Thus, the entity-level embedding are then constructed by summing all of the embedding of the words it contains. Given the entity embedding, to model the dependency between the co-reference links and relations between entities, a parallel graph decoder~ that involves both co-reference link and entity relation generation processes is utilized. \nSpecifically, given the entity embedding $h_i$ and $h_j$ of a pair of name entities $v_i$ and $v_j$, the initial generated latent representation of co-reference link $\\textbf{c}^{0}_{i,j}$ is computed as:\n\\begin{equation}\n\\label{eq:n2edeconv_direct}\n \\textbf{c}^{0}_{i,j}=\\sum\\nolimits^{C}_{m=1}(\\sigma (h_i^{m}\\Bar{\\mu}_j)+\\sigma(h_j^{m}\\Bar{\\nu}_i)),\n\\end{equation}\nwhere $\\sigma (h_i^{m}\\Bar{\\mu}_j)$ means the deconvolution contribution of node $v_i$ to its edge representations with node $v_j$, which is made by the $m$-th entry of its node representations, and $\\Bar{\\mu}_j$ represents one entry of the deconvolution filter vector $\\Bar{\\mu}\\in\\mathbb{R}^{N \\times 1}$ that is related to node $v_j$. The initial relation latent representation $e^{0}_{i,j}$ between a pair of name entities $v_i$ and $v_j$ can also be computed in the same way. $C$ refers to the length of name entity embedding. \nGiven the initial latent representation of co-reference links and relations, the co-reference link representation $\\textbf{c}^{l}_{i,j}$ at the $l$-th layer is computed as follows:\n\\begin{equation}\n\\label{eq:e2edeconv_direct}\n \\textbf{c}^{l}_{i,j}=\\sigma (\\Bar{\\phi}^{l-1}_j\\sum\\nolimits_{k_1=1}^{N}[\\textbf{c}^{l-1};\\textbf{e}^{l-1}]^{l-1}_{i,k_1}x_{k_1})+\\sigma(\\Bar{\\psi}^{l}_i\\sum\\nolimits_{k_2=1}^{N}[\\textbf{c}^{l-1};\\textbf{e}^{l-1}]^{l-1}_{k_2,j}x_{k_1}),\n\\end{equation}\\normalsize \nwhere $\\Bar{\\phi}^{l-1}_j\\sum_{k_1=1}^{N}[\\textbf{c}^{l-1};\\textbf{e}^{l-1}]^{l-1}_{i,k_1}x_{k_1}$ can be interpreted as the decoded contribution of node $v_i$ to its edge representations with node $v_j$, and $\\Bar{\\phi}^{l-1}_j$ refers to the element of deconvolution filter vector that is related to node $v_j$. The output of the last ``edge'' deconvolution layer denotes the probability of the existence of an edge in the target graph. All the symbols $\\sigma$ refers to the activation functions. $[\\textbf{c}^{l-1};\\textbf{e}^{l-1}]$ refers to the concatenation of all the co-reference and relation representations at $(l-1)$-th layer.", "cites": [1120], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical description of a graph-to-graph model for information extraction, using the cited paper as a basis for explaining methodologies and challenges. It synthesizes the key components of the model (e.g., hierarchical encoder, parallel decoder) and integrates them into a structured explanation. However, it lacks critical evaluation of the model's limitations or comparison with alternative approaches, and the abstraction is limited to the specific task of information extraction without broader conceptual generalization."}}
{"id": "00a0dbec-0833-4b8c-ae79-f3460d08d96c", "title": "Applications", "level": "section", "subsections": ["ecd633dd-b3d5-497b-96c1-5a75c85a3bfd", "e4099cb4-17dc-4061-ab18-ba6af3fcc56e", "85163c03-25ce-4b07-be05-801273a47002", "f32b1e00-313f-417e-889c-53d846a3907b", "0cd1532e-70ce-44e8-9c9b-6c81331e81b9", "dc63ed83-6981-438a-8d62-ccb530aa78a8", "56808223-a80c-48fe-9320-d8be7eb95ce2", "7ca94819-03c5-4220-af3d-d7a29d394843", "6a807e8a-9cf4-40e4-9702-d7cee5c06d68", "4522da4f-94d5-4839-a78a-5d561e94a5da", "e12bc973-45c4-4e12-87e1-fa0f7605db2f", "a02902dc-e326-4cd7-8837-66a0cefa042c", "7756814f-b5ea-42f2-8adb-261a5edae44d"], "parent_id": "d9782043-9373-462c-b592-7d9a42b0beee", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"]], "content": "\\label{sec:Applications}\nIn this chapter, we will discuss a large variety of typical NLP applications using GNNs, including natural language generation, machine reading comprehension, question answering, dialog system, text classification, text matching, topic modeling, sentiment classification, knowledge graph, information extraction, semantic and syntactic parsing, reasoning, and semantic role labelling. We also provide the summary of all the applications with their sub-tasks and evaluation metrics in Table~\\ref{tab:app-table}.\n\\begin{table}[]\n\\caption{Typical NLP applications and relevant works using GNNs}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{|c|c|c|l|}\n\\hline\n\\textbf{Application} & \\textbf{Task}  & \\textbf{Evaluation} & \\textbf{References} \\\\ \\hline\n\\multirow{11}{*}{NLG} \n& {Neural}  & \\multirow{3}{*}{BLEU} &  \\\\ \n& Machine & &  \\\\ \n& Translation& &  \\\\ \\cline{2-4} \n& \\multirow{4}{*}{Summarization} & \\multirow{4}{*}{ROUGE}  &    \\\\ \n&  &  &  \\\\\n&  &  &  \\\\\n&  &  &  \\\\ \\cline{2-4} \n&  & \\multirow{4}{*}{BLEU, METEOR} &   \\\\\n& Structural-data &  &  \\\\\n& to Text &  &  \\\\\n&  &  &  \\\\ \\cline{2-4}\n& Natural Question  & BLEU, METEOR, &      \\\\ \n& Generation & ROUGE &  \\\\ \\hline\n\\multirow{9}{*}{MRC and QA} &  & \\multirow{4}{*}{F1, Exact Match}  &  \\\\\n& Machine Reading &  &  \\\\\n& Comprehension &  &  \\\\ \n& &  &  \\\\  \\cline{2-4} \n& Knowledge Base & \\multirow{2}{*}{F1, Accuracy}  &   \\\\\n& Question Answering &  &  \\\\  \\cline{2-4} \n& Open-domain & \\multirow{2}{*}{Hits@1, F1}  & \\multirow{2}{*}{} \\\\\n& Question Answering &  &  \\\\  \\cline{2-4} \n& Community & \\multirow{2}{*}{nDCG, Precision} & \\multirow{2}{*}{} \\\\ \n& Question Answering &  &  \\\\  \\hline\n\\multirow{4}{*}{Dialog Systems}  & Dialog State Tracking  & Accuracy &  \\\\ \\cline{2-4}\n& Dialog Response & BLEU, METEOR,   & \\multirow{2}{*}{} \\\\\n& Generation & ROUGE &  \\\\  \\cline{2-4} \n& Next Utterance Selection  & Recall@K & \\\\ \\hline\n\\multicolumn{2}{|c|}{\\multirow{2}{*}{Text Classification}} & \\multirow{2}{*}{Accuracy} &  \\\\\n\\multicolumn{2}{|c|}{} &  &  \\\\ \\hline\n\\multicolumn{2}{|c|}{Text Matching} & Accuracy, F1  &  \\\\ \\hline\n\\multicolumn{2}{|c|}{\\multirow{2}{*}{Topic Modeling}}   & \\multirow{2}{*}{Topic Coherence Score}  &   \\\\\n\\multicolumn{2}{|c|}{}  & &  \\\\ \\hline\n\\multicolumn{2}{|c|}{\\multirow{4}{*}{Sentiment Classification}} & \\multirow{4}{*}{Accuracy, F1} &    \\\\ \n\\multicolumn{2}{|c|}{}  & &  \\\\\n\\multicolumn{2}{|c|}{}  & &  \\\\ \n\\multicolumn{2}{|c|}{}  & &  \\\\ \\hline\n\\multirow{6}{*}{Knowledge Graph} & Knowledge & \\multirow{6}{*}{Hits@N} &  \\\\ \n& Graph &  &  \\\\\n& Completion &  &  \\\\ \\cline{2-2} \\cline{4-4}\n& Knowledge & & \\\\ \n& Graph &  &  \\\\\n& Alignment &  &  \\\\ \\hline\n\\multirow{5}{*}{Information Extraction}  & Named Entity & \\multirow{5}{*}{Precision, Recall, F1}  & \\\\\n& Recognition &  &  \\\\ \\cline{2-2} \\cline{4-4}\n& \\multirow{2}{*}{Relation Extraction} &  &  \\\\ \n&  &  &  \\\\ \\cline{2-2} \\cline{4-4}\n& Joint Learning Models &  &   \\\\ \\hline\n\\multirow{3}{*}{Parsing} & Syntax-related  & \\multirow{3}{*}{Accuracy} &   \\\\ \\cline{2-2} \\cline{4-4} \n& \\multirow{2}{*}{Semantics-related} &  &   \\\\ \n& &  &   \\\\ \\hline\n\\multirow{6}{*}{Reasoning} & Math Word & \\multirow{6}{*}{Accuracy} &  \\\\ \n& Problem Solving &  &  \\\\ \\cline{2-2} \\cline{4-4}\n& Natural Language & & \\multirow{2}{*}{} \\\\ \n& Inference &  &  \\\\ \\cline{2-2} \\cline{4-4}\n& Commonsense &  & \\multirow{2}{*}{} \\\\\n& Reasoning &  &  \\\\ \\hline\n\\multicolumn{2}{|c|}{\\multirow{2}{*}{Semantic Role Labelling}} & {Precision, Recall,}   &   \\\\ \n\\multicolumn{2}{|c|}{} & {F1}   &   \\\\ \\hline\n\\end{tabular}\n}\n\\label{tab:app-table}\n\\end{table}", "cites": [7330, 1103, 1091, 1094, 7328, 248, 1069, 1090, 1054, 1127, 1117, 1079, 1073, 1078, 7326, 1055, 1052, 7011, 7327, 1123, 1095, 1087, 1053, 1067, 1124, 1083, 1074, 259, 1068, 7213, 1066, 1085, 274, 1096, 1092, 8313, 1076, 1093, 1128, 1060, 1125, 1072, 7325, 1062, 1116, 1058, 1075, 1063, 7044, 7049, 1061, 1126, 8411, 1084, 1056, 1059], "cite_extract_rate": 0.5728155339805825, "origin_cites_number": 103, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of various NLP applications using GNNs, with a table listing tasks, evaluations, and references. However, it lacks synthesis of ideas across papers, critical analysis of methods or limitations, and abstraction to broader patterns or principles, making it primarily a factual summary without deeper insight."}}
{"id": "16f3eab5-980a-49fd-8fcc-4d069b5281f0", "title": "Background and Motivation", "level": "paragraph", "subsections": [], "parent_id": "92481755-cd26-47c5-9cbc-07912b166120", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Natural Language Generation"], ["subsubsection", "Neural Machine Translation"], ["paragraph", "Background and Motivation"]], "content": "The classic neural machine translation (NMT) system aims to map the source language's sentences into the target language without changing the semantic meaning. Most prior works~ adopt the attention-based sequence-to-sequence learning diagram, especially the RNN-based language model. Compared with the traditional machine translation models, these methods can produce much better performance without specific linguistic knowledge. However, these methods suffer from the long-dependency problem. With the development of attention mechanism, fully-attention-based models such as Transformer~, which captures the implicit correlations by self-attention, have made a breakthrough and achieved a new state-of-art. Although these works achieve great success, they rarely take the structural information into account, such as the syntactic structure. Recently, with the help of powerful GNNs, many researchers further boost the performance by mining the structural knowledge contained in the unstructured texts.", "cites": [168, 7008, 38], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the key developments in NMT, from early encoder-decoder models to attention-based approaches and finally to GNNs, by linking the evolution of structural modeling in translation. It provides a critical perspective by highlighting limitations such as the long-dependency problem in RNNs and the lack of explicit structural information in attention-based models like the Transformer. While it identifies trends and shifts in modeling paradigms, it does not offer a highly abstracted or novel theoretical framework, limiting its abstraction level to moderate."}}
{"id": "73a0bfd3-15cd-437b-b700-fed3b4a5d17f", "title": "Methodologies", "level": "paragraph", "subsections": [], "parent_id": "92481755-cd26-47c5-9cbc-07912b166120", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Natural Language Generation"], ["subsubsection", "Neural Machine Translation"], ["paragraph", "Methodologies"]], "content": "Most GNN-based NMT methods cast the conventional seq2seq diagram to the Graph2Seq architecture. They firstly convert the input texts to graph-structured data, and then employ the GNN-based encoder to exploit the structural information. In this section, we introduce and summarize some representative GNN-related techniques adopted in recent NMT approaches regarding graph construction and representation learning.\n\\begin{itemize}\n    \\item \\textbf{Graph Construction}. Various static graphs have been introduced to the NMT task to tackle corresponding challenges.  first converted the given texts into syntactic dependency graph. Such structure doesn't take semantic relations of words into account. Intuitively, it is beneficial to represent the redundant sentences by high-level semantic structure abstractions. To this end,  construct the semantic-role-labeling based dependency graph for the given texts. What's more,  construct the AMR graph for the sentences which can cover more semantic correlations. Besides the classic graph types, some specifically designed graphs (app-driven graphs) are proposed to address the unique challenges. Although source sentences in NMT are determined, either word-level or subword-level segmentations have multiple choices to split a source sequence with different word segments or different subword vocabulary sizes. Such a phenomenon is proposed to affect the performance of NMT~. They propose the lattice graph, which incorporates different segmentation of source sentences.  construct the \\textit{relative position graph} to explicitly model the relative position feature.  build the multi-modal graph to introduce visual knowledge to NMT, which presents the input sentences and corresponding images in a unified graph to capture the semantic correlations better. \n    Despite the single-type static graph,  construct the hybrid graph considering multiple relations for document-level NMT to address the severe long-dependency issue. In detail, they construct the graph considering both intra-sentential and inter-sentential relations. For intra-sentential relations, they link the words with sequential and dependency relations. For inter-sentential relations, they link the words in different sentences with lexical (repeated or similar) and coreference correlations.\n\\item\\textbf{Graph Representation Learning}. Most of the constructed graphs in this line are heterogeneous graphs, which contain multiple node or edge types and can't be exploited directly by typical GNNs. Thus, researchers adopt various heterogeneous graph representation techniques.  regard the dependency graphs as multi-relational graphs and apply directed-GCN to learn the graph representation. Similarly,  firstly convert the constructed multi-relational graph to levi-graph and apply relational GGNN, which employs edge-type-specific parameters to exploit the rich structure information.  regard the edge as connectivity and treat the edge direction as edge types, such as \"in\", \"out\", and \"self\". Then they apply relational GCN to encode the document graph.  convert the heterogeneous graph to levi-graph and adopt the densely connected GCN to learn the embedding.  propose a special type-aware heterogeneous GGNN to learn the node embedding and edge representation jointly. Specifically, they first learn the edge representation by fusing both the source node and edge type's embeddings. Then for each node, they aggregate the representation from its incoming and outgoing neighbors and utilize a RNN based module to update the representation. \nBesides the extension of traditional GNNs, Transformer is further explored to learn from the structural inputs in NMT. Unlike traditional Transformer which adopt absolute sinusoidal position embedding to ensure the self-attention learn the position-specific feature,  adopt position-based edge embedding to capture the position correlations and make the transformer learn from the graph-based inputs.  learn the bidirectional path-based relation embedding and add it to the node embedding when calculating self-attention. They then find the shortest path from the given graph for any two nodes and apply bidirectional GRU to further encode the path to get the relation representation.  apply graph-transformer-based encoder to learn the multi-modal graph. Firstly, for text modal's nodes, they get the initial embedding by summing up the word embedding and position embedding. As for visual nodes, they apply a MLP layer to project them to the unified space as text nodes. Secondly for each modal, they apply multi-head self-attention to learn the intra-modal representation. Thirdly, they employ GAT-based cross-modal fusion to learn the cross-modal representation.\n\\item\\textbf{Special Techniques}. In order to allow information flow from both directions, some technqies are designed for incorporating direction information. For example,  add the corresponding reverse edge as an additional edge type \"reverse\". The self-loops edge type are also added as type \"self\". For another example,   first add a global node and the edges from this global node to other nodes are marked with type \"global\". In addition, they further add bidirectional sequential links with type \"forward\" and \"backward\" between nodes existing in the input texts.\n\\end{itemize}", "cites": [7011, 1066, 1074, 7044], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple papers to present a coherent narrative on graph construction, representation learning, and special techniques in GNN-based NMT. It organizes the methods into broader categories and highlights design choices and variations. However, it lacks deeper critical evaluation of the approaches, such as their effectiveness, trade-offs, or shortcomings. Some abstraction is achieved by grouping techniques under common themes like heterogeneity and modality fusion, but the analysis remains largely at the methodological level."}}
{"id": "3835f308-ae1e-4ba2-9c86-cf2e7568ff4c", "title": "Benchmarks and Evaluation", "level": "paragraph", "subsections": [], "parent_id": "92481755-cd26-47c5-9cbc-07912b166120", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Natural Language Generation"], ["subsubsection", "Neural Machine Translation"], ["paragraph", "Benchmarks and Evaluation"]], "content": "Common benchmarks for NMT from text include News Commentary v11, WMT14, WMT16, WMT19 for training, newstest2013, newstest2015, newstest2016, newsdev2019, newstest2019 for evaluation and testing. As for multi-modal NMT task, Multi30K dataset~ is widely used by previous works. As for evaluation metrics, BLEU is the a typical metric to evaluate the similarity between the generated and real output texts.", "cites": [7008], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic list of benchmarks and evaluation metrics used in neural machine translation, with minimal synthesis or integration of ideas from the cited paper. There is no critical evaluation or comparison of the effectiveness of these approaches, and the content remains at a factual, concrete level without abstracting broader trends or principles."}}
{"id": "1c13e7f2-cb84-4b75-a9a2-d61754b6ca1e", "title": "Background and Motivation", "level": "paragraph", "subsections": [], "parent_id": "dd96bbda-5aa5-4040-a408-02106a418dc6", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Natural Language Generation"], ["subsubsection", "Summarization"], ["paragraph", "Background and Motivation"]], "content": "Automatic summarization is the task of producing a concise and fluent summary while preserving key information content and overall meaning~. It is a well-noticed but challenging problem due to the need of searching in overwhelmed textural data in real world. Broadly, there are two main classic settings in this task: 1) extractive summarization and 2) abstractive summarization. Extractive summarization task focus on selecting sub-sentences from the given text to reduce redundancy, which is formulated as a classification problem. In contrast, abstractive summarization follows the neural language generation task. It normally adopts the encoder-decoder architecture to generate the textual summary. Compared to the extractive summarization, the abstractive summarization setting is more challenging but more attractive since it can produce non-existing expressions. Traditional approaches~ simply regard the inputs as sequences and apply the encoder like LSTM, Transformer, etc. to learn the latent representation, which fail to utilize the rich structural information implicitly existing in the natural inputs. Many researchers find that structural knowledge is beneficial to address some troublesome challenges, e.g., long-dependency problem, and thus propose the GNN-based techniques~ to explicitly leverage the structural information to boost the performance.", "cites": [1129, 1053, 8415, 1131, 1130], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of summarization in NLP using GNNs, integrating concepts from multiple papers to highlight the transition from sequence-based to structure-aware models. It synthesizes the distinction between extractive and abstractive approaches and connects them to the role of GNNs in improving summarization quality. While it identifies the limitations of traditional methods, it stops short of offering deep comparisons or critiques of the cited works, limiting its critical depth."}}
{"id": "fa8881df-bc69-4e02-84f3-90223ac01464", "title": "Methodologies", "level": "paragraph", "subsections": [], "parent_id": "dd96bbda-5aa5-4040-a408-02106a418dc6", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Natural Language Generation"], ["subsubsection", "Summarization"], ["paragraph", "Methodologies"]], "content": "Most GNN-based summarization approaches firstly construct the graph to represent the given natural texts. Then they employ GNN-based encoders to learn the graph representation. After that, for extractive summarization models, they adopt the classifier to select candidate subsentences to compose the final summary. As for abstractive summarization, they mostly adopt the language decoder with maximizing the outputs' likelihood to generate the summary. In the following, we introduce some representative GNN-related techniques from the recent summarization methods.\n\\begin{itemize}\n\\item\\textbf{Graph Construction}.\nHere we introduce the different ways to construct suitable and effective graph for different types of inputs, including sign-documents, multi-documents and codes.\n\\textbf{\\textit{Single-document based.}}  construct the hybrid graph, including sequential and coreference relation. To tackle the issue such as semantic irrelevance and deviation,  construct the semantic dependency graph and cast it as the multi-relational graph for the given texts. To capture the typical long-dependency in document-level summarization,  construct the hybrid graph. They first construct the discourse graph by RST parsing and then add co-reference edges between co-reference mentions in the document.\nTo better capture the long-dependency relation in sentence-level and enrich the semantic correlations,  regards both the sentences and the containing words as nodes and construct a similarity graph to model the semantic relations.\nTo model the redundant relation between sentences,  propose to construct the hybrid heterogeneous graph containing three types of nodes: 1) named entity, 2) word, and 3) sentence as well as four types of edges: 1) sequential, 2) containing, 3) same, and 4) similar.\nHowever, the methods above are mostly focused on the cross-sentence relations and overlook the inter-sentence, especially the topic information.\nTo this end,  and  construct the topic graph by introducing additional topic words to discover the latent topic information. On top of that,  mine the sub-graph of non-topic nodes to represent the original texts while preserving the topic information.\n\\textbf{\\textit{Multi-document based.}}\n decompose the given document clusters into sentences and construct the discourse graph by Personalized Discourse Graph algorithm (PDG).  split the documents into paragraphs and constructs three individual graphs: 1) similarity graph, 2) discourse graph, and 3) topic graph to investigate the effectiveness.\n\\textbf{\\textit{Code based.}}\nTo fully represent the code information in the code summarization task, \n construct the specific code graph for the given program clips. They first break up the identifier tokens (i.e., variables, methods, etc.) into sub-tokens by programming language heuristics. Then they construct the graph to organize the sub-tokens according to sequential positions and lexically usage.  propose another way by firstly parsing the given programs into abstract syntax trees (AST) and then converting them to program graphs.\n\\item\\textbf{Graph Representation Learning}\nIn the literature of summarization tasks, both homogeneous GNNs and heterogeneous GNNs have been explored to learn the graph representation. \nFor homogeneous graphs,  apply self-attention-based GAT to learn the representation on the fully-connected graph. Specifically, they introduce Gaussian kernel to mine the edge importance between nodes from the graph's topology.  adopt the GAT-based graph transformer, which regards the similarity learned by self-attention as edge weight.\nFor heterogeneous graphs, some researchers cast the heterogeneous graphs to homogeneous graphs by special techniques. For example, some works ignore both the edges and nodes' types by treating the edge as connectivity.   project the nodes to the unified embedding space to diminish the heterogeneity. After that, some classic GNNs are employed such as GCN~, GAT~. For example,  employ the relational GGNN to learn type-specific relations between nodes.  firstly split the heterogeneous graph into two sub-graphs according to nodes' type (i.e., words graph and sentence graph) and then apply GAT-based cross-attention on two sub-graphs to learn the representation iteratively.\n\\item\\textbf{Embedding Initialization}\nThe quality of the initial node embedding plays an important role in the overall performance of GNN-based methods. For graphs whose nodes are words, most approaches adopt the pre-trained word embeddings such as BERT~, ALBERT~. Besides, since the topic graph~ introduces additional topic nodes, they initialize them by the latent representation of topic modeling.  apply Transformer to learn the contextual-level node embedding.\nFor nodes such as sentence-level nodes, which are composed of words,  adopt the GRU to learn the sentences' embedding (i.e., the node embeddings) from the corresponding word sequences. They adopt the last hidden state as the sentences' representation. Similarly,  adopt CNN to capture the fine-grained n-gram feature and then employ Bi-LSTM to get the sentences' feature vectors.\n apply the average pooling function to the ALBERT's encoder outputs to represent the sentence nodes, while\n initialize the nodes (utterances) by CNN and the topic words by LDA. \n\\end{itemize}", "cites": [1053, 1060, 1073, 1067], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a structured overview of GNN-based summarization methodologies, categorizing them into graph construction, representation learning, and embedding initialization. While it integrates information from the cited papers and connects some concepts (e.g., different graph types), the synthesis remains largely surface-level. It lacks deeper critical evaluation of the methods' strengths or weaknesses and does not generalize to broader trends or theoretical principles."}}
{"id": "68751394-111a-4ad4-a05b-1e568c0a2acc", "title": "Benchmarks and Evaluation", "level": "paragraph", "subsections": [], "parent_id": "dd96bbda-5aa5-4040-a408-02106a418dc6", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Natural Language Generation"], ["subsubsection", "Summarization"], ["paragraph", "Benchmarks and Evaluation"]], "content": "Common benchmarks for automatic summarization from documents include CNN/DailyMail~, NYT~, WikiSum~, MultiNews~. As for code based summarization, Java~ and Python~ are widely used. As for evaluation metrics, BLEU, ROUGE and human evaluation are commonly used.", "cites": [1134, 1133, 7186, 1132], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section merely lists common benchmarks and evaluation metrics without integrating or synthesizing insights from the cited papers. It lacks critical evaluation of the methods or limitations of these benchmarks and does not abstract beyond the specific examples to identify broader trends or principles in summarization evaluation."}}
{"id": "0f8d1a29-6859-4d92-bdde-9c98b07d5aa3", "title": "Methodologies", "level": "paragraph", "subsections": [], "parent_id": "bf75b508-09f8-42e7-ac2d-08006d9aabc3", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Natural Language Generation"], ["subsubsection", "Structural-data to Text"], ["paragraph", "Methodologies"]], "content": "Most GNN-based AMR-to-text and SQL-to-text approaches typically construct domain-specific graphs such as AMR graphs and SQL-parsing-based graphs to organize the inputs. RDF-to-text generation often uses the graph structure inherent in the RDF triples. Following that, they apply Graph2Seq consisting of GNN encoders and sequential decoders to generate neural language outputs. This section summarizes various graph construction methods and the techniques employed to exploit the informative graphs.\n\\begin{itemize}\n\\item\\textbf{Graph Construction}. \nRegarding the AMR-to-text Generation, \nthe input AMRs can be normally represented as  directed heterogeneous graphs according to the relations~. To incorporate the conventional GNNs specializing in homogeneous-graph learning, \n convert the AMR graphs to levi-graph. In addition, for each edge, they~ add the reverse edges and self-loops to allow information flows in both directions. \nBesides the default, reverse, and self-loop edges,  also introduces fully-connected edges to model indirect nodes and connected edges, which treat original edges as connectivity without direction to model connection information. \n split the given AMR graph $\\mathcal{G}_{AMR}$ into two directed sub-graphs: 1) concept graph $\\mathcal{G}_{c}$, and 2) line graph $\\mathcal{G}_{l}$. They firstly treat the edge as connectivity to get the concept graph. Then for each edge in $\\mathcal{G}_{AMR}$, they create a node in $\\mathcal{G}_{l}$. Two nodes in $\\mathcal{G}_{l}$ are connected if they share the same nodes in $\\mathcal{G}_{AMR}$. The two sub-graphs are connected by original connections in $\\mathcal{G}_{AMR}$. To leverage multi-hop connection information, they preserve the $1-K$ order neighbors in the adjacency matrices.\nRegarding the SQL inputs, the SQL queries can be parsed by many SQL tools\\footnotemark[1] into many sub-clauses without loss, which naturally contain rich structure information. \n\\footnotetext[1]{\\href{http://www.sqlparser.com}{http://www.sqlparser.com}.}\n construct the directed and homogeneous SQL-graph based on the sub-clauses by some hand-craft rules.\nRegarding the RDF triple inputs,  treat the relation in a triple as an additional node in the graph connecting to the subject and object entity nodes.\n\\item\\textbf{Graph Representation Learning}.\n treat the obtained levi-graphs as directed homogeneous graphs and learn the representation by bidirectional GNNs.  also proposes a bidirectional embedding learning framework that traverses the directed graphs in the original and the reversal direction. \n apply classic graph2seq architecture~ with bidirectional GraphSage methods to learn the embedding of SQL graph via two ways, including 1) pooling-based mechanism and 2) node-based mechanism, which means add a supernode connecting to other nodes, to investigate the influence of graph embedding. \nSome approaches directly employ multi-relational GNN to encode the obtained multi-relational graphs. For example,  adopt directed-GCN to exploit the AMR graphs considering both heterogeneity and parameter-overhead.  propose relational GGNN to capture diverse semantic correlations.  employ a variance of GGNN to exploit the multi-relational AMR graphs by aggregating the bidirectional node and edge features and then fusing them via a LSTM network.  propose a heterogeneous GAT to exploit the AMR graphs in different grains. Firstly, they apply GAT to each sub-graph to learn the bidirectional representation separately. Then they apply cross-attention to explore the dependencies between the two sub-graphs.  propose the multi-hop GCN, which dynamically fuses the $1-K$ order neighbors' features to control the information propagate in a range of orders.  apply relational GAT with bidirectional graph embedding mechanism by incorporating the edge types into the attention procedure to learn type-specific attention weights.\nTransformer architectures are also utilized to encode the AMR or SQL graphs.  firstly apply GAT-based graph Transformer in each homogeneous sub-graphs and then concatenate sub-graphs representation to feed the feed-forward layer. Some works~ adopt the structure-aware graph transformer~, which injecting the relation embedding learned by shortest path to the self-attention to involve the structure features. Specifically,  explore various shortest path algorithms to learn the relation representation of arbitrary two nodes. Similarly,  employ the graph Transformer, which leverages the structure information by incorporating the edge types into attention-weight learning formulas.\n\\item\\textbf{Special Mechanisms}.\n apply the Bi-LSTM encoder following the GNN encoder to further encode the sequential information. Despite the language generation procedure,  to better preserve the structural information,  introduce the graph reconstruction on top of the latent graph representation generated by graph transformer encoder.\n\\end{itemize}", "cites": [7011, 7329, 7044, 1126], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 14, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a factual summary of methodologies used in structural-data-to-text generation with GNNs, integrating some terminology and approaches from the cited works. However, it lacks deeper synthesis across papers and does not establish a coherent, overarching narrative or framework. There is minimal critical analysis or evaluation of the methods' strengths and limitations, and while it identifies a few recurring patterns (e.g., bidirectional embedding, multi-hop neighbors), it stops short of abstracting these into broader principles."}}
{"id": "3cecc858-27a9-4182-b59b-688e668b6fdd", "title": "Benchmarks and Evaluation", "level": "paragraph", "subsections": [], "parent_id": "bf75b508-09f8-42e7-ac2d-08006d9aabc3", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Natural Language Generation"], ["subsubsection", "Structural-data to Text"], ["paragraph", "Benchmarks and Evaluation"]], "content": "Common benchmarks for AMR-to-text generation task include LDC2015E85, LDC2015E86, LDC2017T10, and LDC2020T02. As for the SQL-to-text generation task, WikiSQL~ and Stackoverflow~ are widely used by previous works. The RDF-to-text generation task often uses WebNLG~ and New York Times (NYT)~. As for evaluation metrics, the AMR-to-text generation task mostly adopts BLEU, Meteor, CHRF++, and human evaluation including meaning similarity and readability. While BLEU-4 are widely used for SQL-to-text task. The RDF-to-text generation task uses BLEU, Meteor and TER.", "cites": [8416], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual listing of benchmarks and evaluation metrics used in different structural-to-text generation tasks but lacks synthesis of ideas from the cited papers. There is minimal critical evaluation or abstraction; it does not compare the effectiveness of these benchmarks or highlight limitations or trends in their usage."}}
{"id": "40eecc99-c63f-4443-8305-cfc59254db4d", "title": "Background and Motivation", "level": "paragraph", "subsections": [], "parent_id": "76d9de42-778c-4e2e-94c0-840d1badad35", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Natural Language Generation"], ["subsubsection", "Natural Question Generation"], ["paragraph", "Background and Motivation"]], "content": "The natural question generation (QG) task aims at generating natural language questions from certain form of data, such as KG~, tables~, text~ or images~, where the generated questions need to be answerable from the input data.\nMost prior work~ adopts a Seq2Seq architecture which regards the input data as sequential data without considering its rich structural information.\nFor instance, when encoding the input text, \nmost previous approaches~ typically ignore the hidden structural information associated with a word sequence such as the dependency parsing tree.\nEven for the setting of QG from KG, most approaches~ typically linearize the KB subgraph to a sequence and apply a sequence encoder.\nFailing to utilize the graph structure of the input data may limit the effectiveness of QG models. \nAs for the multi-hop QG from text setting which requires reasoning over multiple paragraphs or documents, it is beneficial to capture the relationships among different entity mentions across multiple paragraphs or documents.\nIn summary, modeling the rich structures of the input data is important for many QG tasks.\nRecently, GNNs have been successfully applied to the QG tasks~.", "cites": [1116, 1136, 1137, 1068, 1135, 1071], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple papers by highlighting the common issue of ignoring input data structure in QG tasks and how GNNs can address this. It offers a critical perspective by pointing out limitations of prior methods, such as discarding graph structures or failing to model multi-hop reasoning. It abstracts these findings to emphasize the general importance of structural modeling in QG, transcending individual paper analyses."}}
{"id": "c26d1a46-99e1-4484-9239-1ee4dad7a8cd", "title": "Methodologies", "level": "paragraph", "subsections": [], "parent_id": "76d9de42-778c-4e2e-94c0-840d1badad35", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Natural Language Generation"], ["subsubsection", "Natural Question Generation"], ["paragraph", "Methodologies"]], "content": "Most GNN-based QG approaches adopt a Graph2Seq architecture where a GNN-based encoder is employed to model the graph-structured input data, and a sequence decoder is employed to generate a natural language question.\nIn this section, we introduce and summarize some representative GNN-related techniques adopted in recent QG approaches.\n\\begin{itemize}\n\\item\\textbf{Graph Construction}.\nDifferent graph construction strategies have been proposed to suit the various needs of different QG settings by prior GNN-based approaches. \nSome works~ converted the passage text to a graph based on dependency parsing or semantic role labeling for QG from text. \nAs for multi-hop QG from text, in order to model the relationships among entity mentions across multiple paragraphs or documents, an entity graph is often constructed. For instance,  constructed an entity graph with the named entities in context as nodes and edges connecting the entity pairs appearing in the same sentence or paragraph. In addition, an answer-aware dynamic entity graph was created on the fly by masking out entities irrelevant to the answers.\n built a so-called context-entity graph containing three types of nodes (i.e., named-entity mentions, coreferent entities, and sentence-ids) and added edges connecting them.\nUnlike the above approaches that build a static graph based on prior knowledge, \n explored dynamic graph construction for converting the passage text to a graph of word nodes by leveraging the attention mechanism. As for QG from KG, graph construction is not needed since the KG is already provided. A common option is to extract a k-hop subgraph surrounding the topic entity as the input graph when generating a question~.\n\\item\\textbf{Graph Representation Learning}.\nCommon GNN models used by existing QG approaches include GCN~, GAT~, GGNN~, and graph transformer~.\nIn order to model the edge direction information,  and  extended the GGNN model to handle directed edges. \nIn order to model multi-relational graphs,\n explored two graph encoding strategies: i) converting a multi-relational graph to a Levi graph~ and applying a regular GGNN model, or ii) extending the GGNN model by incorporating the edge information in the message passing process.\n also extended the GGNN model by bringing in the attention mechanism from GAT and introducing edge type aware linear transformations for message passing between node pairs.\n proposed a graph-augmented transformer model employing a relation-aware multi-head attention mechanism similar to .\n found it beneficial to additionally model the sequential information in the input text besides the graph-structured information.\n separately applied a sequence encoder to the document text, and a graph encoder to the semantic graph representation of the document constructed from semantic role labeling or dependency parsing. \nThe outputs of the sequence encoder and graph encoder would then be fused and fed to a sequence decoder for question generation. The model was jointly trained on question decoding and content selection sub-tasks. \n ran both the structure-aware attention network on the input graph and the standard attention network on the input sequence, and fused their output embeddings using some non-linear mapping function to learn the final embeddings for the sequence decoder. During the training, a contrastive objective was proposed to predict supporting facts, serving as a regularization term in addition to the main cross-entropy loss for sequence generation.\n\\end{itemize}", "cites": [1052, 1116, 8411, 1068, 7044, 1055, 1071], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes information from multiple papers, organizing them along key axes like graph construction and representation learning. It highlights methodological trends, such as the use of attention mechanisms and handling of multi-relational graphs, while also offering some critical observations about dynamic vs. static graph approaches and the benefits of fusing sequential and graph information. The abstraction level is strong, as it generalizes common strategies and identifies broader design patterns in GNN-based QG."}}
{"id": "6f8068f8-9018-45d0-82e5-4ada59b81252", "title": "Benchmarks and Evaluation", "level": "paragraph", "subsections": [], "parent_id": "76d9de42-778c-4e2e-94c0-840d1badad35", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Natural Language Generation"], ["subsubsection", "Natural Question Generation"], ["paragraph", "Benchmarks and Evaluation"]], "content": "Common benchmarks for QG from text include \nSQuAD~,\nNewsQA~, and\nHotpotQA~.\nAs for QG from KG, WebQuestions~ and PathQuestions~ are widely used by previous works.\nAs for evaluation metrics, BLEU-4, METEOR, ROUGE-L and human evaluation (e.g., syntactically correct, semantically correct, relevant) are common metrics.\nComplexity is also used to evaluate the performance of multi-hop QG systems.", "cites": [1138, 460, 439], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic list of benchmarks and evaluation metrics for natural question generation tasks without substantial synthesis or comparison between the cited works. It lacks critical analysis of the strengths or weaknesses of these datasets and metrics. While it identifies some common practices, it does not abstract these into broader principles or frameworks."}}
{"id": "9bae22fc-0fd7-4ce1-8aaa-d7e2d9c0a9e0", "title": "Background and Motivation", "level": "paragraph", "subsections": [], "parent_id": "94905e10-7d0f-41fa-a64b-00be08a3981e", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Machine Reading Comprehension and Question Answering"], ["subsubsection", "Machine Reading Comprehension"], ["paragraph", "Background and Motivation"]], "content": "The task of Machine Reading Comprehension (MRC) aims to answer a natural language question using the given passage.\nSignificant progress has been made in the MRC\ntask thanks to the development of various (co-)attention mechanisms that capture the interaction between the question and context~.\nConsidering that the traditional MRC setting mainly focuses on one-hop reasoning which is relatively simple, recently, more research efforts have been made to solve more challenging MRC settings.\nFor instance, the multi-hop MRC task is to answer a natural language question using multiple passages or documents, which requires the multi-hop reasoning capacity.\nThe conversational MRC task is to answer the current natural language question in a conversation given a passage and the previous questions and answers, which requires the capacity of modeling conversation history.\nThe numerical MRC task requires the capacity of performing numerical reasoning over the passage.\nThese challenging MRC tasks call for the learning capacity of modeling complex relations among objects.\nFor example, it is beneficial to model relations among multiple documents and the entity mentions within the documents for the multi-hop MRC task.\nRecently, GNNs have been successfully applied to various types of MRC tasks including multi-hop MRC~, conversational MRC~, and numerical MRC~.", "cites": [1075, 1084, 1078, 1140, 8409, 1085, 1139, 1141, 1070, 8417], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers on various MRC tasks (multi-hop, conversational, numerical) to highlight the need for modeling complex relations and the suitability of GNNs. It abstracts by identifying a common trend: the use of graph-based methods to enhance reasoning capabilities. However, it lacks critical evaluation of these methods or limitations, limiting the depth of analysis."}}
{"id": "a82ab1e5-6efe-4afc-af39-c3cd5b07b373", "title": "Methodologies", "level": "paragraph", "subsections": [], "parent_id": "94905e10-7d0f-41fa-a64b-00be08a3981e", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Machine Reading Comprehension and Question Answering"], ["subsubsection", "Machine Reading Comprehension"], ["paragraph", "Methodologies"]], "content": "GNN-based MRC approaches typically operate by first constructing an entity graph or hierarchical graph capturing rich relations among nodes in the graph, and then applying a GNN-based reasoning module for performing complex reasoning over the graph. Assuming the GNN outputs already encode the semantic meanings of the node itself and its neighboring structure, a prediction module will finally be applied for predicting answers.\nThe graph construction techniques and graph representation techniques developed for solving the MRC task vary between different approaches.\nIn this section, we introduce and summarize some representative GNN-related techniques adopted in recent MRC approaches.\n\\begin{itemize}\n\\item\\noindent\\textbf{Graph Construction}.\nIn order to apply GNNs for complex reasoning in the MRC task, one critical step is graph construction. \nBuilding a high-quality graph capturing rich relations among useful objects (e.g., entity mentions, paragraphs) is the foundation for conducting graph-based complex reasoning.\nMost GNN-based MRC approaches conduct static graph construction by utilizing domain-specific prior knowledge. \nAmong all existing GNN-based MRC approaches, the most widely adopted strategy for static graph construction is to construct an entity graph using carefully designed rules.\nThese approaches~ usually extract entity mentions from questions, paragraphs and candidate answers (if given) as nodes, and connect the nodes with edges capturing different types of relations such as exact match, co-occurrence, coreference and semantic role labeling.\nEdge connectivity with different granularity levels in terms of context window (e.g., sentence, paragraph and document) might also be distinguished for better modeling performance~.\nFor instance,  distinguished cross-document edge and within-document edge when building an entity graph.\nAs for the numerical MRC task, the most important relations are probably the arithmetic relations. In order to explicitly model numerical reasoning,  constructed a graph containing numbers in the question and passage as nodes, and added edges to capture various arithmetic relations among the numbers.\nBesides building an entity graph capturing various types of relations among entity mentions, some approaches~ opt to build a hierarchical graph containing various types of nodes including entity mentions, sentences, paragraphs and documents, and connect these nodes using predefined rules.\nFor example,  constructed a hierarchical graph that contains edges connecting token nodes and sentence nodes, sentence nodes and paragraph nodes as well as paragraph nodes and document nodes. \nVery recently, dynamic graph construction techniques without relying on hand-crafted rules have also been explored for the MRC task and achieved promising results. Unlike static graph construction techniques that have been widely explored in the MRC literature, dynamic graph construction techniques are less studied.\nIn comparison to static graph construction, dynamic graph construction aims to build a graph on the fly without relying on domain-specific prior knowledge, and is typically jointly learned with the remaining learning modules of the system. \nRecently,  proposed a GNN-based model for the conversational MRC task, which is able to dynamically build a question and conversation history aware passage graph containing each passage word as a node at each conversation turn by leveraging the attention mechanism. \nA kNN-style graph sparsification operation was conducted so as to further extract a sparse graph from the fully-connected graph learned by the attention mechanism.\nThe learned sparse graph will be consumed by the subsequent GNN-based reasoning module, and the whole system is end-to-end trainable. \n\\item\\textbf{Graph Representation Learning}.\nMost GNN-based MRC approaches rely on a GNN model for performing complex reasoning over the graph. \nIn the literature of the MRC task, both homogeneous GNNs and multi-relational GNNs have been explored for node representation learning.\nEven though most GNN-based MRC approaches construct a multi-relational or heterogeneous graph, some of them still apply a homogeneous GNN model such as GCN~, GAT~ and Graph Recurrent Network (GRN)~.\nUnlike other works that apply a GNN model to a single graph, \n proposed a Recurrent Graph Neural Network (RGNN) for processing a sequence of passage graphs for modeling conversational history.\nThe most widely used multi-relational GNN model in the MRC task is the RGCN model~.\nMany approaches~ adopt a gating RGCN variant which in addition introduces a gating mechanism regulating how much of the update message propagates to the next step.\n further proposed a question-aware gating mechanism for RGCN, that is able to regulate the aggregated message according to the question, and even bring the question information into the update message.\n\\item\\textbf{Node Embedding Initialization}.\nMany studies have shown that the quality of the initial node embeddings play an important role in the overall performance of GNN-based models.\nMost approaches use pre-trained word embeddings such as GloVe~, ELMo~, BERT~ and RoBERTa~ to initialize tokens.\nSome works~ also concatenated linguistic features to word embeddings to enrich the semantic meanings.\nOn top of the initial word embeddings, most approaches choose to further apply some transformation functions such as MLP for introducing nonlinearity~, \nBiLSTM for capturing local dependency of the text~, co-attention layer for fusing questions to passages~.\n\\item\\textbf{Special Techniques}\n In order to increase the richness of the supervision signals, some approaches adopt the multi-tasking learning strategy to predict not only the answer span, but also the supporting paragraph/sentence/fact and answer type~.\n\\end{itemize}", "cites": [1075, 259, 1084, 1078, 826, 8409, 1085, 8385, 1070], "cite_extract_rate": 0.5625, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple cited papers by categorizing methodologies into graph construction, graph representation learning, node embedding initialization, and special techniques, highlighting commonalities and variations. It abstracts beyond individual works by discussing broader trends such as static vs. dynamic graph construction and the role of pre-trained embeddings. While it provides some critical analysis, such as noting the under-exploration of dynamic graph techniques, deeper comparative evaluation or limitations could be more explicitly articulated."}}
{"id": "ab0b074e-86e0-4482-b44e-7c7335074c59", "title": "Benchmarks and Evaluation", "level": "paragraph", "subsections": [], "parent_id": "94905e10-7d0f-41fa-a64b-00be08a3981e", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Machine Reading Comprehension and Question Answering"], ["subsubsection", "Machine Reading Comprehension"], ["paragraph", "Benchmarks and Evaluation"]], "content": "Common multi-hop MRC benchmarks include HotpotQA~, WikiHop~ and ComplexWebQuestions~.\nCommon conversational MRC benchmarks include CoQA~, QuAC~ and DoQA~.\nDROP~ is a benchmark created for the numerical MRC task.\nAs for evaluation metrics, F1 and EM (i.e., exact match) are the two most widely used evaluation metrics for the MRC task.\nBesides, the Human Equivalence Score (HEQ)~ is used to judge whether a system performs as well as an average human. HEQ-Q and HEQ-D are accuracies at the question level and dialog level, respectively.", "cites": [1143, 1144, 1142, 460, 1098, 1145], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual list of benchmarks and evaluation metrics without deeply connecting or contrasting the cited papers. It lacks critical evaluation of the benchmarks or their limitations, and offers minimal abstraction or generalization about the role of these datasets in the field of MRC."}}
{"id": "885b991b-e0f7-4fa8-b815-1b2086519b3a", "title": "Background and Motivation", "level": "paragraph", "subsections": [], "parent_id": "d39166c1-ff20-4742-989b-5c25a20a25bb", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Machine Reading Comprehension and Question Answering"], ["subsubsection", "Knowledge Base Question Answering"], ["paragraph", "Background and Motivation"]], "content": "Knowledge Base Question Answering (KBQA) has emerged as an important research topic in the past few years ~.\nThe goal of KBQA is to automatically find answers from the KG given a natural language question.\nRecently, due to its nature capability of modeling relationships among objects, GNNs have been successfully applied for performing the multi-hop KBQA task which requires reasoning over multiple edges of the KG to arrive at the right answer.\nA relevant task is open domain QA~ which aims to answer open domain questions by leveraging hybrid knowledge sources including corpus and KG. \nHere we only focus on the QA over KG setting, while the GNN applications in the open domain QA task will be introduced in other sections.", "cites": [1146, 1054], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of KBQA and mentions the use of GNNs for multi-hop reasoning but does not effectively synthesize or connect the cited papers. It lacks critical evaluation of the works and does not abstract broader patterns or principles, remaining focused on factual presentation."}}
{"id": "d394dc2c-151f-4364-95dc-56e54dadd475", "title": "Methodologies", "level": "paragraph", "subsections": [], "parent_id": "d39166c1-ff20-4742-989b-5c25a20a25bb", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Machine Reading Comprehension and Question Answering"], ["subsubsection", "Knowledge Base Question Answering"], ["paragraph", "Methodologies"]], "content": "In this section, we introduce and summarize some representative GNN-related techniques adopted in the recent KBQA research.\n\\begin{itemize}\n    \\item\\textbf{Graph Construction}.\nSemantic parsing based KBQA methods~ aims at converting natural language questions to a semantic graph which can be further executed against the KG to find the correct answers.\nIn order to better model the structure of the semantic graph, \n proposed to use GNNs to encode the candidate semantic graphs. Specifically, they used a similar procedure as  to construct multiple candidate semantic graphs given the question, and chose the one which has the highest matching score to the question in the embedding space.\n focused on a different multi-choice QA setting which is to select the correct answer from the provided candidate answer set given the question and the external KG.\nFor each candidate answer,  proposed to extract from the external KG a ``contextualized'' subgraph according to the question and candidate answer.\nThis subgraph serves as the evidence for selecting the corresponding candidate answer as the final answer.\nSpecifically, they first recognized all the entity mentions in the question and candidate answer set, and linked them to entities in the KG. \nBesides these linked entities in the KG, any other \nentities that appeared in any two-hop paths between pairs of mentioned entities in the KG as well as the corresponding edges were also added to the subgraph.\n constructed a joint graph by regarding the QA context as an additional node (QA context node) and connecting it to the topic entities in the KG subgraph. Specifically, they introduced two new relation types $r_{z,q}$ and $r_{z,a}$ for capturing the relationship between the QA context node and the relevant entities in the KG. The specific relation type is determined by whether the KG entity is found in the question portion or the answer portion of the QA context. \n\\item\\textbf{Graph Representation Learning}\nIn order to better model the constructed multi-relational or heterogeneous graphs, basic GNNs need to be extended to handle edge types or node types.\nTo this end,  extended GGNN~ to include edge embeddings in message passing. After learning the vector representations of both the question and every candidate semantic graph, they used a simple reward function to select the best semantic graph for the question.\nThe final node embedding of the question variable node (q-node) in each semantic graph was extracted and non-linearly transformed to obtain the graph-level representation.\n designed a Multi-hop Graph Relation Network (MHGRN) to unify both GNNs and path-based models.\nSpecifically, they considered both node type and edge type information of the graph by introducing node type specific linear transformation, and node type and relation type aware attention in message passing.\nIn addition, instead of performing one hop message passing at each time, inspired by path-based models~, they proposed to pass messages directly over all the paths of lengths up to $K$.\nGraph-level representations were obtained via attentive pooling over the output node embeddings, and would be concatenated with the text representation of question and each candidate answer to compute the plausibility score.\nSimilarly,  extended GAT by introducing node type and edge type aware message passing to handle multi-relational graphs. They in addition employed a pre-trained language model for KG node relevance scoring in the initial stage and final answer selection stage. \n\\end{itemize}", "cites": [1125, 1069, 1081, 1123, 7049, 1083], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of methodologies in KBQA using GNNs, listing techniques from multiple papers without deep integration or comparative analysis. While it identifies general approaches like graph construction and representation learning, it lacks critical evaluation or abstraction into broader trends or principles."}}
{"id": "d7763868-5995-44ab-ac57-2a711c40cf22", "title": "Benchmarks and Evaluation", "level": "paragraph", "subsections": [], "parent_id": "d39166c1-ff20-4742-989b-5c25a20a25bb", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Machine Reading Comprehension and Question Answering"], ["subsubsection", "Knowledge Base Question Answering"], ["paragraph", "Benchmarks and Evaluation"]], "content": "Common benchmarks for KBQA include \nWebQuestionsSP~,\nMetaQA~,\nQALD-7~,\nCommonsenseQA~, and\nOpenbookQA~.\nF1 and accuracy are common metrics for evaluating KBQA methods.", "cites": [1054, 1147, 456], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section lists common benchmarks and evaluation metrics for KBQA but does not synthesize or integrate insights from the cited papers. It lacks critical analysis of these benchmarks or the methods they evaluate, and it does not abstract to broader patterns or principles in the field."}}
{"id": "4aab8cdd-20a1-4a41-90e7-93d883d99051", "title": "Background and Motivation", "level": "paragraph", "subsections": [], "parent_id": "27bf55c8-6ae7-468f-b9e1-2b54b4cd6371", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Machine Reading Comprehension and Question Answering"], ["subsubsection", "Open-domain Question Answering"], ["paragraph", "Background and Motivation"]], "content": "The task of open-domain question answering aims to identify answers to the natural question given a large scale of open-domain knowledge (e.g. documents, knowledge base and etc.). Untill recent times, the open-domain question answering~ has been mostly exploited through knowledge bases such as Personalized PageRank~, which actually closely related to the Knowledge based Question Answering task (KBQA) in techniques. The knowledge based methods benefit from obtaining external knowledge easily through graph structure. However, these methods limit in the missing information of the knowledge base and fixed schema. Other attempts have been made to answer questions from massive and unstructured documents~. Compared to the KB based methods, these methods can fetch more information but suffer from the difficulty of retrieve relevant and key information from redundant external documents.", "cites": [1054, 1148, 8418], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.3, "critical": 2.7, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis by connecting the use of knowledge bases and unstructured documents in open-domain QA, referencing three different papers. It offers a small degree of critical analysis by pointing out the limitations of knowledge-based methods and the challenges in retrieving key information from documents. The section also begins to abstract by contrasting two general approaches (KG-based vs. document-based), though deeper meta-level insights or a novel framework are not fully developed."}}
{"id": "07d6d240-a82b-46bf-89e3-bcd20d698a61", "title": "Methodologies", "level": "paragraph", "subsections": [], "parent_id": "27bf55c8-6ae7-468f-b9e1-2b54b4cd6371", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Machine Reading Comprehension and Question Answering"], ["subsubsection", "Open-domain Question Answering"], ["paragraph", "Methodologies"]], "content": "In this section, we introduce and summarize some representative GNN-related techniques in the recent open-domain question answering research.\n\\begin{itemize}\n    \\item\\textbf{Graph Construction}. Most of the GNN based methods address the mentioned challenges by constructing a heterogeneous graph with both knowledge base and unstructured documents~.  firstly extract the subgraph from external knowledge base named Personalized PageRank~. Then they fetch a relevant text corpus from Wikipedia and fuse them to the knowledge graph. Specifically, they represent the documents by words' encoding and link the nodes (the nodes in the knowledge graph are entities) which appear in the document.  propose a iteratively constructed heterogeneous graph method from both knowledge base and text corpus. Initially, the graph depends only on the question. Then for each iteration, they expand the subgraph by choosing nodes from which to \"pull\" information about, from the KB or corpus as appropriate. \n\\item\\textbf{Graph Representation Learning}  first initialize the nodes' embedding with pre-trained weight for entities and LSTM encoding for documents. They further propose different update rule for both entities and documents. For entities, they apply R-GCN~ on the sub-graph only from the knowledge base and then take average of the linked words' feature in the connected documents. The entities' representation is the combination of: 1) the previous entities themselves' representation, 2) question encoding, 3) knowledge-subgraph's aggregation results, and 4) related documents' aggregation results. For documents' update operation, they aggregate the features from connected entities.  adopt similar idea for heterogeneous graph representation learning. Technically, before encoding entities, they incorporate the connected words' embedding in the documents to the entities. Then for nodes, they propose GCN with attention weight to aggregate neighbor entities. Note that the question is employed in the attention mechanism to guide the learning process. The documents' updating process is in the same pattern.\n\\end{itemize}", "cites": [259], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of methodologies used in open-domain question answering with GNNs, mainly summarizing the technical approaches from cited works. It integrates a few concepts (e.g., heterogeneous graph construction, R-GCN for entities) and makes some basic connections between different techniques, but lacks deeper critical evaluation or comparison of these methods. The abstraction level is limited, focusing on specific components rather than broader principles."}}
{"id": "dcd49b3b-6307-4790-9a17-dfba86475488", "title": "Benchmarks and Evaluation", "level": "paragraph", "subsections": [], "parent_id": "27bf55c8-6ae7-468f-b9e1-2b54b4cd6371", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Machine Reading Comprehension and Question Answering"], ["subsubsection", "Open-domain Question Answering"], ["paragraph", "Benchmarks and Evaluation"]], "content": "Common benchmarks for Open-domain Question answering include WebQuestionsSP~, MetaQA~, Complex WebQuestions 1.1 (Complex WebQ)~, and WikiMovies-10K~. Hits@1 and F1 scores are the common evaluation metrics for this task~.", "cites": [1144, 1054], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly lists common benchmarks and evaluation metrics for open-domain question answering but does not synthesize or connect ideas from the cited papers in a meaningful way. It lacks critical analysis of the methods or limitations discussed in the papers and offers no abstraction or generalization of broader trends or principles in the field."}}
{"id": "22104112-4684-4bb8-9fc8-219d4e52177a", "title": "Methodologies", "level": "paragraph", "subsections": [], "parent_id": "8b514795-b785-415c-99b2-49e53d0f5f97", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Machine Reading Comprehension and Question Answering"], ["subsubsection", "Community Question Answering"], ["paragraph", "Methodologies"]], "content": "In this section, we introduce and summarize some representative GNN-related techniques adopted in the recent CQA research.\n\\begin{itemize}\n    \\item\\textbf{Graph Construction}. Most of the GNN-based methods construct a multi-modal graph for existing question/answer pairs~. For the given q/a pair (q, a), both of them construct the question/answer $\\mathcal{G}^q/\\mathcal{G}^a$ graph separately. Since in real community based forums, the question/answer pairs may contain both visual and text contents, they employ a multi-modal graph to represent them jointly.  firstly employ object detection models such as YOLO3~ to fetch visual objects. The objects are represented by their labels (visual words more accurately). The visual objects are treated as words in the answers which are modeled with textural contents equally. Then they regard each textural words as vertex and link them with undirected occurrence edges.  adopt the same idea as~ for building occurrence graph for both textural contents and visual words. But for extracting visual words from images, they employ unsupervised Meta-path Link Prediction for Visual Labeling. Concretely, they define the meta-path over image and words and build the heterogeneous image-word graph. \n\\item\\textbf{Graph Representation Learning}. Most of the GNN-based community question answering models adapt the GNN models to capture structure information. Given the question/answer pair (q, a),  stacks the graph pooling network to capture the hierarchical semantic-level correlations between nodes. Conceptually, the graph pooling network extract the high-level semantic representation for both question and answer graphs. Formally, it consists of two GCN-variant APPNP~ encoders. Generally, one APPNP is employed to learn the high-level semantic cluster distribution for each vertex. The other APPNP network is used to learn the immediate node representation. The final node representation is the fusion of the two encoders' results.   employ the APPNP to learn the importance of each vertex's neighbors. \n\\end{itemize}", "cites": [836], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive summary of methodologies used in GNN-based CQA research, mentioning graph construction and representation learning techniques. While it references the cited papers to explain specific approaches, it lacks deeper synthesis, critical evaluation, and abstraction. The content is structured as a list of techniques without comparing them or identifying broader trends or limitations."}}
{"id": "70a9d1d5-a75c-4ab3-b54a-beb19b7d0a84", "title": "Background and Motivation", "level": "paragraph", "subsections": ["20671128-0b12-4d33-baea-a15bc60e57c9", "2781d338-0118-43e9-9b7f-f53c5fb99d86"], "parent_id": "85163c03-25ce-4b07-be05-801273a47002", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Dialog Systems"], ["paragraph", "Background and Motivation"]], "content": "Dialog system~ is a computer system that can continuously converse with a human.\nIn order to build a successful dialog system, it is important to model the dependencies among different interlocutors or utterances within a conversation.\nDue to the ability of modeling complex relations among objects, recently, GNNs have been successfully applied to various dialog system related tasks including dialog state tracking~ which aims at estimating the current dialog state given the conversation history, dialog response generation~ which aims at generating the dialog response given the conversation history, and \nnext utterance selection~ which aims at selecting the next utterance from a candidate list given the conversation history.", "cites": [1127, 1149], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of dialog systems and briefly mentions GNN applications in specific tasks like dialog state tracking and response generation, but does not synthesize or connect ideas from the cited papers meaningfully. There is minimal critical analysis or identification of broader patterns or principles, resulting in a low-level descriptive summary."}}
{"id": "20671128-0b12-4d33-baea-a15bc60e57c9", "title": "Methodologies", "level": "paragraph", "subsections": [], "parent_id": "70a9d1d5-a75c-4ab3-b54a-beb19b7d0a84", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Dialog Systems"], ["paragraph", "Background and Motivation"], ["paragraph", "Methodologies"]], "content": "In this section, we introduce and summarize some representative GNN-related techniques adopted in the recent dialog systems research.\n\\begin{itemize}\n\\item\\textbf{Graph Construction}.\nBuilding a high-quality graph representing a structured conversation session is challenging.\nA real-world conversation can have rich interactions among speakers and utterances. \nHere, we introduce both static and dynamic graph construction techniques used in recent GNN-based approaches.\nFor static graphs, most GNN-based dialog systems rely on prior domain knowledge to construct a graph.\nFor instance, in order to apply GNNs to model  multi-party dialogues (i.e., involving multiple interlocutors),  converted utterances in a structured dialogue session to a directed graph capturing response relationships between utterances. Specifically, they created an edge for every pair of utterances from the same speaker following the chronological order of the utterances.\n built a directed heterogeneous graph according to the domain ontology that consists of edges among slot-dependent nodes and slot-independent nodes.\n constructed three types of graphs including a token-level schema graph according to the original ontology scheme, a utterance graph according to the dialogue utterance, and a domain-specific slot-level schema graph connecting two slots from the same domain or share the same candidate values.\n constructed a graph connecting utterance nodes that are adjacent or belong to dependent topics.\nRegarding the dynamic graph construction,\nunlike most GNN-based approaches that rely on prior knowledge for constructing static graph,  jointly optimized the graph structure and the parameters of GNN by approximating posterior probability of the adjacency matrix (i.e., modeled as a latent variable following a factored Bernoulli distribution) via variational inference~.\n\\item\\textbf{Graph Representation Learning}\nVarious GNN models have been applied in dialog systems. For instance,  applied GCN to facilitate reasoning over all utterances.\n proposed a graph attention matching network to learn the representations of ontology schema and dialogue utterance simultaneously, and a recurrent attention graph neural network which employs a GRU-like gated cell for dialog state updating.\nInspired by the hierarchical sequence-based HRED model for dialog response generation,  proposed an utterance-level graph-structured encoder which is a gated GNN variant, and is able to control how much the new information (from the preceding utterance nodes) should be considered when updating the current state of the utterance node. They also designed a bi-directional information flow algorithm to allow both forward and backward message passing over the directed graph.\nIn order to model multi-relational graphs,\n designed a R-GCN like GNN model employing edge type specific weight matrices.\n\\item\\textbf{Node Embedding Initialization}\nIn terms of node embedding initialization for GNN models,  applied a BiLSTM to first encode the local dependency information in the raw text sequence.\n used the state-of-the-art pre-trained ALBERT embeddings~ to initialize the node embeddings.\n included token embeddings, segmentation embeddings as well as position embeddings to capture the rich semantic meanings of the nodes.\n\\end{itemize}", "cites": [1127, 1151, 1150], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of GNN methodologies in dialog systems, categorizing them into graph construction, representation learning, and node embedding initialization. It connects some ideas across papers (e.g., static vs. dynamic graphs) but lacks deeper comparison or critique. While it highlights common patterns, such as the use of pre-trained embeddings, it stops short of offering abstract or meta-level insights into the field."}}
{"id": "2781d338-0118-43e9-9b7f-f53c5fb99d86", "title": "Benchmarks and Evaluation", "level": "paragraph", "subsections": [], "parent_id": "70a9d1d5-a75c-4ab3-b54a-beb19b7d0a84", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Dialog Systems"], ["paragraph", "Background and Motivation"], ["paragraph", "Benchmarks and Evaluation"]], "content": "Common dialog state tracking benchmarks include\nPyDial~ and MultiWOZ~.\nUbuntu Dialogue Corpus~ and MuTual~ are often used for evaluating dialog response generation and next utterance selection systems.\nAs for evaluation metrics, BLEU, METEOR and ROUGE-L are common metrics for evaluating dialog response generation systems. Besides automatic evaluation, human evaluation (e.g., grammaticality, fluency, rationality) is often conducted.\nAccuracy is the most widely used metric for evaluating dialog state tracking systems.\nRecall at k is often used in the next utterance selection task.", "cites": [8419, 1152, 7331, 1153], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of benchmark datasets and evaluation metrics used in dialog systems, citing relevant papers for each. However, it lacks synthesis by not connecting the datasets or metrics to broader trends or principles in dialog system research. There is minimal critical analysis or abstraction, as it merely lists the papers and their associated resources without deeper discussion or comparison."}}
{"id": "71ae8fcf-a849-4c00-a1bf-8abff19ce37a", "title": "Background and Motivation", "level": "paragraph", "subsections": ["4927ccdd-794d-4745-b3ed-2c13b3174a93", "7eb23997-998f-4e37-8b18-11fac5bf4781"], "parent_id": "f32b1e00-313f-417e-889c-53d846a3907b", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Text Classification"], ["paragraph", "Background and Motivation"]], "content": "Traditional text classification methods heavily rely on feature engineering (e.g., BOW, TF-IDF or more advanced graph path based features) for text representation.\nIn order to learn ``good'' representations from text, various unsupervised approaches have been proposed for word or document representation learning, including word2vec~, GloVe~, topic models~, autoencoder~, and\ndoc2vec~.\nThese pre-trained word or document embeddings can further be consumed by a MLP~, CNN~ or LSTM~ module for training a supervised text classifier.\nIn order to better capture the relations among words in text or documents in corpus, various graph-based approaches have been proposed for text classification.\nFor instance,  proposed to first construct a graph of words, and then apply a CNN to the normalized subgraph.\n proposed a network embedding based approach for text representation learning in a semi-supervised manner by converting a partially labeled text corpora to a heterogeneous text network.\nRecently, given the strong expressive power, GNNs have been successfully applied to both semi-supervised~ and supervised~ text classification.", "cites": [1091, 8313, 1159, 1095, 7264, 1684, 1154, 1155, 673, 1158, 7265, 1156, 1157, 8410, 1081], "cite_extract_rate": 0.75, "origin_cites_number": 20, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of traditional and modern approaches to text classification, including the use of GNNs. While it cites multiple papers, it largely lists them without synthesizing their contributions or comparing them in depth. There is minimal critical evaluation or abstraction to broader trends or frameworks."}}
{"id": "4927ccdd-794d-4745-b3ed-2c13b3174a93", "title": "Methodologies", "level": "paragraph", "subsections": [], "parent_id": "71ae8fcf-a849-4c00-a1bf-8abff19ce37a", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Text Classification"], ["paragraph", "Background and Motivation"], ["paragraph", "Methodologies"]], "content": "GNN-based text classification approaches typically operate by first constructing a document graph or corpus graph capturing rich relations among nodes in the graph, and then applying a GNN to learn good document embeddings which will later be fed into a softmax layer for producing a probabilistic distribution over a class of labels. \nThe graph construction techniques and graph representation techniques developed for solving the text classification task vary between different approaches.\nIn this section, we introduce and summarize some representative GNN-related techniques adopted in recent text classification approaches.\n\\begin{itemize}\n    \\item\\textbf{Graph Construction}.\nSemi-supervised text classification leverages a small amount of labeled data with a large amount of unlabeled data during training.\nUtilizing the relations among labeled and unlabeled documents is essential for performing well in this semi-supervised setting.    \nRegarding the static graph construction, recently, many GNN-based semi-supervised approaches~ have been proposed for text classification to better model the relations among words and documents in the corpus.\nThese approaches typically construct a single heterogeneous graph for the whole corpus containing word nodes and document nodes, and connect the nodes with edges based on word co-occurrence and document-word relations~.\n proposed to enrich the semantics of the short text with additional information (i.e., topics and entities), and constructed a Heterogeneous Information Network (HIN) containing document, topic and entity nodes with document-topic, document-entity and entity-entity edges based on several predefined rules.\nOne limitation of semi-supervised text classification is its incapability of handling unseen documents in the testing phase.\nIn order to handle the inductive learning setting, some GNN-based approaches~ proposed to instead build an individual graph of unique words for each document by leveraging word similarity or co-occurrence between words within certain fixed-sized context window.\nIn comparison to static graph construction, dynamic graph construction does not rely on domain-specific prior knowledge, and the graph structure can be jointly learned with the remaining learning modules of the system. \n proposed to jointly learn a graph of unique words for each input text using a Gaussian kernel.\n proposed to regard each word in text as a node in a graph, and dynamically build a graph for each document.\n\\item\\textbf{Graph Representation Learning}\nEarly graph-based text classification approaches~~ were motivated by extending CNNs to graph CNNs which can directly model graph-structured textual data.\nWith the fast growth of the GNN research, recent work started to explore various GNN models for text classification including GCN~, GGNN~ and message passing mechanism (MPM)~.\n introduced a TensorGCN which first performs intra-graph convolution propagation and then performs inter-graph convolution propagation.\n proposed a Heterogeneous GAT (HGAT) based on a dual-level (i.e., node-level and type-level) attention mechanism.\n\\item\\textbf{Node Embedding Initialization}\nNode embedding initialization is critical for the GNN performance.\nInterestingly,  observed in their experiments that by using only one-hot representation, a vanilla GCN~ without any external word embeddings or knowledge already outperformed state-of-the-art methods for text classification.\nNevertheless, most GNN-based approaches~ still use pre-trained word embeddings to initialize node embeddings.\n further applied a BiLSTM to a sequence of word embeddings to capture the contextual information of text for node embedding initialization.\n\\item\\textbf{Special Techniques} \nAs a common trick used in text classification, some GNN-based approaches removed stop words during preprocessing~.\n\\end{itemize}", "cites": [7213, 8410, 1091, 1081, 1058, 8313, 1095], "cite_extract_rate": 0.875, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes various GNN-based methodologies for text classification by organizing them into distinct categories such as graph construction, representation learning, and node embedding initialization. It connects ideas across papers and presents a structured overview. However, while it mentions limitations (e.g., inductive learning challenges), it lacks deeper critical evaluation or comparison of the effectiveness of different approaches. It identifies some patterns but does not fully abstract them into broader principles or theoretical insights."}}
{"id": "7eb23997-998f-4e37-8b18-11fac5bf4781", "title": "Benchmarks and Evaluation", "level": "paragraph", "subsections": [], "parent_id": "71ae8fcf-a849-4c00-a1bf-8abff19ce37a", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Text Classification"], ["paragraph", "Background and Motivation"], ["paragraph", "Benchmarks and Evaluation"]], "content": "Common benchmarks for evaluating text classification methods include\n20NEWS~,\nOhsumed~,\nReuters~,\nMovie Review (MR)~,\nAGNews~,\nSnippets~,\nTagMyNews~, and\nTwitter\\footnote{\\url{https://www.nltk.org/}}.\nAccuracy is the most common evaluation metric.", "cites": [1096, 8420], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a list of common benchmarks and mentions accuracy as the primary evaluation metric, but fails to synthesize or connect the cited papers meaningfully. It does not offer any comparative analysis or critical evaluation of the methods or datasets, nor does it abstract to broader trends or principles in text classification using GNNs."}}
{"id": "ca690d35-1258-46c5-b02e-9b1656b1ddc5", "title": "Background and Motivation", "level": "paragraph", "subsections": ["453887c3-7f26-4a3a-8789-18749fd2adc4", "418dd887-c31d-46cf-b75c-82d5801fe277"], "parent_id": "0cd1532e-70ce-44e8-9c9b-6c81331e81b9", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Text Matching"], ["paragraph", "Background and Motivation"]], "content": "Most existing text matching approaches operate by mapping each text into a latent embedding space via some neural networks such as CNNs~ or RNNs~, and then computing the matching score based on the similarity between the text representations.\nIn order to model the rich interactions between two texts at different granularity levels, sophisticated attention or matching components are often carefully designed~.\nRecently, there are a few works~ successfully exploring GNNs for modeling the complicated interactions between text elements in the text matching literature.", "cites": [1062, 1162, 1161, 2, 1160], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of text matching approaches and introduces GNNs in the context of recent work, but it lacks deeper synthesis of the cited papers. It does not explicitly compare or contrast the different methods, nor does it evaluate their strengths or limitations. The abstraction is limited to a general statement about the role of GNNs in capturing interactions, without identifying broader principles or trends."}}
{"id": "453887c3-7f26-4a3a-8789-18749fd2adc4", "title": "Methodologies", "level": "paragraph", "subsections": [], "parent_id": "ca690d35-1258-46c5-b02e-9b1656b1ddc5", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Text Matching"], ["paragraph", "Background and Motivation"], ["paragraph", "Methodologies"]], "content": "In this section, we introduce and summarize some representative GNN-related techniques adopted in recent text matching approaches.\n\\begin{itemize}\n    \\item\\textbf{Graph Construction}\nChinese short text matching heavily relies on the quality of word segmentation. \nInstead of segmenting each sentence into a word sequence during preprocessing which can be erroneous, ambiguous or inconsistent,  proposed to construct a word lattice graph from all possible segmentation paths. Specifically, the word lattice graph contains all character subsequences that match words in a lexicon as nodes, and adds an edge between two nodes if they are adjacent in the original sentence. As a result, the constructed graph encodes multiple word segmentation hypotheses for text matching.\nIn order to tackle long text matching,  proposed to organize documents into a graph of concepts (i.e., a keyword or a set of highly correlated keywords in a document), and built a concept interaction heterogeneous graph that consists of three types of edges including keyword-keyword, keyword-concept and sentence-concept edges.\nSpecifically, they first constructed a keyword co-occurrence graph, and based on that, they grouped keywords into concepts by applying community detection algorithms on the keyword co-occurrence graph.\nFinally, they assigned each sentence to the most similar concept.\n\\item\\textbf{Graph Representation Learning}\n applied a GCN model to learn meaningful node embeddings in the constructed graph.\n designed a GNN-based graph matching module which allows bidirectional message passing across nodes in both text graphs.\nIn order to obtain graph-level embeddings from the learned node embeddings, max pooling~ or attentive pooling~ techniques were adopted.\n\\item\\textbf{Node Embedding Initialization}\nAs for node embedding initialization,  used the BERT embeddings while  first computed a match vector for each node in the graph.\n\\end{itemize}", "cites": [1062], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of methodologies used in text matching with GNNs, citing one paper and briefly outlining graph construction, representation learning, and node embedding initialization. It lacks synthesis across multiple works, critical evaluation of the approaches, and broader abstraction or identification of principles."}}
{"id": "418dd887-c31d-46cf-b75c-82d5801fe277", "title": "Benchmarks and Evaluation", "level": "paragraph", "subsections": [], "parent_id": "ca690d35-1258-46c5-b02e-9b1656b1ddc5", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Text Matching"], ["paragraph", "Background and Motivation"], ["paragraph", "Benchmarks and Evaluation"]], "content": "Common benchmarks for text matching include \nLCQMC~,\nBQ~,\nCNSE~, and\nCNSS~.\nAccuracy and F1 are the most widely used evaluation metrics.", "cites": [1062], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a minimal factual summary of benchmarks and metrics in text matching without integrating or connecting ideas from the cited paper. It lacks critical evaluation of the methods or datasets and fails to generalize or abstract broader patterns or principles."}}
{"id": "5bf6d1ca-f8f7-466f-ab34-3240f7f628dd", "title": "Background and Motivation", "level": "paragraph", "subsections": ["b98852de-ac81-4bca-b76d-af786eb5e1da", "344e9e6b-761c-4de5-846d-9b8a6f4fe39c"], "parent_id": "dc63ed83-6981-438a-8d62-ccb530aa78a8", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Topic Modeling"], ["paragraph", "Background and Motivation"]], "content": "The task of topic modeling aims to discover the abstract ``topics'' that emerge in a corpus. Typically, a topic model learns to represent a piece of text as a mixture of topics where a topic itself is represented as a mixture of words from a vocabulary.\nClassical topic models include graphical model based methods~,\nautoregressive model based methods~, and autoencoder based methods~.\nRecent works~ have explored GNN-based methods for topic modeling by explicitly modeling the relationships between documents and words.", "cites": [7327, 1158, 1154, 8421], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly introduces topic modeling and mentions different classical approaches and recent GNN-based methods, but it lacks meaningful synthesis of ideas across the cited papers. It does not critically evaluate the strengths or weaknesses of the methods, nor does it abstract broader principles or trends in GNN-based topic modeling. The content primarily serves as a factual description of existing work."}}
{"id": "b98852de-ac81-4bca-b76d-af786eb5e1da", "title": "Methodologies", "level": "paragraph", "subsections": [], "parent_id": "5bf6d1ca-f8f7-466f-ab34-3240f7f628dd", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Topic Modeling"], ["paragraph", "Background and Motivation"], ["paragraph", "Methodologies"]], "content": "In this section, we introduce and summarize some representative GNN-related techniques adopted in recent topic modeling approaches.\n\\begin{itemize}\n    \\item\\textbf{Graph Construction}\nHow to construct a high-quality graph which naturally captures useful relationships between documents and words is the most important for GNN applications in the topic modeling task. Various graph construction strategies have been proposed for GNN-based topic models.\nIn order to explicitly model the word co-occurrence,  extracted the biterms (i.e., word pairs) within a fixed-length text window for every document from a sampled mini-corpus, and built an undirected biterm graph where each node represents a word, and each edge weight indicates the frequency of the corresponding biterm in the mini-corpus.\n built a graph containing documents and words in the corpus as nodes, and added edges to connect document nodes and word nodes based on co-occurrence information where the edge weight matrix is basically the TF-IDF matrix.\n converted a corpus to a bi-partite graph containing document nodes and word nodes, and the edge weight indicates the frequency of the word in the document.\n\\item\\textbf{Graph Representation Learning}\nGiven a graph representation of the corpus,  designed a GCN-based autoencoder model to reconstruct the input biterm graph. In addition, residual connections were introduced to the GCN architecture so as to avoid oversmoothing when stacking many GCN layers. \nSimilarly,  designed a GCN-based autoencoder\nmodel to restore the original document representations. \nNotably,  reused the adjacency matrix as the node feature matrix which captures the word co-occurrence information.\nDuring the inference time, for both of the autoencoder-based methods, the weight matrix of the decoder network can be interpreted as the (unnormalized) word distributions of the learned topics.\nGiven the observations that Probabilistic Latent Semantic Indexing (pLSI)~ can be interpreted as stochastic block model (SBM)~ on a specific bi-partite graph, and GAT can be interpreted as the semi-amortized inference of SBM,  proposed a GAT-based topic modeling network to model the topic structure of non-i.i.d documents.\nAs for node embedding initialization, they used pre-trained word embeddings to initialize word node features, and term frequency vectors to initialize document node features.\n\\end{itemize}", "cites": [7327], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual description of methodologies used in GNN-based topic modeling, including graph construction and representation learning techniques. While it mentions different strategies and some similarities between methods, it lacks deeper synthesis, critical evaluation, and abstraction into broader principles. The content is organized thematically but remains largely a summary of individual approaches."}}
{"id": "344e9e6b-761c-4de5-846d-9b8a6f4fe39c", "title": "Benchmarks and Evaluation", "level": "paragraph", "subsections": [], "parent_id": "5bf6d1ca-f8f7-466f-ab34-3240f7f628dd", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Topic Modeling"], ["paragraph", "Background and Motivation"], ["paragraph", "Benchmarks and Evaluation"]], "content": "Common benchmarks for the topic modeling task include\n20NEWS ~, All News~, Grolier~, NYTimes~, and Reuters~.\nAs for evaluation metrics, since it is challenging to annotate the ground-truth topics for a document, topic coherence score and case study on learned topics are typical means of judging the quality of the learned topics. \nBesides, with the topic representations of text output by a topic model, the performance on downstream tasks such as text classification can also be used to evaluate topic models.", "cites": [1163], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic list of common benchmarks and evaluation metrics for topic modeling but does not synthesize or connect these elements to the cited paper (ATM) in a meaningful way. It lacks critical evaluation of the methods or limitations and only offers a general description of evaluation practices without deeper abstraction or analysis."}}
{"id": "9f2017d3-0dbb-44b4-b63a-6b9be5e0d4b8", "title": "Background and Motivation", "level": "paragraph", "subsections": ["953042a6-03c7-49d5-8d56-8979c6e23211", "83871427-8d18-4910-9404-4dbe2605c26e"], "parent_id": "56808223-a80c-48fe-9320-d8be7eb95ce2", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Sentiment Classification"], ["paragraph", "Background and Motivation"]], "content": "The sentiment classification task aims to detect the sentiment (i.e., positive, negative or neutral) of a piece of text~.\nUnlike general sentiment classification, aspect level sentiment classification aims at identifying the sentiment polarity of text regarding a specific aspect, and has received more attention~.\nWhile most works focus on sentence level and single domain sentiment classification, some attempts have been made on document level~ and cross-domain~ sentiment classification. \nEarly works on sentiment classification heavily relied on feature engineering~.\nRecent attempts~ leveraged the expressive power of various neural network models such as LSTM~, CNN~ or Memory Networks~.\nVery recently, more attempts have been made to leverage GNNs to better model syntactic and semantic meanings of text for the sentiment classification task.", "cites": [7326, 1164, 9109, 8422], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic overview of the sentiment classification task and mentions the use of GNNs, but it primarily describes the evolution of approaches from feature engineering to neural networks without deeply integrating or contrasting the cited papers. It lacks critical evaluation of the methods or limitations and only touches on general trends without identifying broader principles or frameworks."}}
{"id": "953042a6-03c7-49d5-8d56-8979c6e23211", "title": "Methodologies", "level": "paragraph", "subsections": [], "parent_id": "9f2017d3-0dbb-44b4-b63a-6b9be5e0d4b8", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Sentiment Classification"], ["paragraph", "Background and Motivation"], ["paragraph", "Methodologies"]], "content": "GNN-based sentiment classification approaches typically operate by first constructing a graph representation (e.g., dependency tree) of the text, and then applying a GNN to learn good text embeddings which will be used for predicting the sentiment polarity. \nThe graph construction techniques and graph representation techniques developed for solving the sentiment classification task vary between different approaches.\nIn this section, we introduce and summarize some representative GNN-related techniques adopted in recent sentiment classification approaches.\n\\begin{itemize}\n    \\item\\textbf{Graph Construction}\nMost GNN-based approaches~ for sentence level sentiment classification used a dependency tree structure to represent the input text.\nBesides using a dependency graph for capturing syntactic information,  in addition constructed a global lexical graph to encode the corpus level word co-occurrence information, and further built a concept hierarchy on both the syntactic and lexical graphs.\n constructed a subgraph from ConceptNet~ using seed concepts extracted from text.\nTo capture the document-level sentiment preference information,  built a bipartite graph with edges connecting sentence nodes to the corresponding aspect nodes for capturing the intra-aspect consistency, and a graph with edges connecting sentence nodes within the same document for capturing the inter-aspect tendency.\n\\item\\textbf{Graph Representation Learning}\nBoth the design of GNN models and quality of initial node embeddings are critical for the overall performance of GNN-based sentiment classification methods.\nCommon GNN models adopted in the sentiment classification task include GCN~, GAT~ and Graph Transformer~.\nTo handle multi-relational graphs, R-GCN~ and R-GAT~ were also applied to perform relation-aware message passing over graphs.\nMost approaches used GloVe+BiLSTM~ or BERT~ to initialize node embeddings.\n\\item\\textbf{Special Techniques}\nOne common trick used in aspect level sentiment classification is to include position weights or embeddings~ to emphasize more on tokens closer to the aspect phase.\n\\end{itemize}", "cites": [7326, 7327, 7328, 7325, 7046], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of GNN-based methodologies in sentiment classification, listing common graph construction techniques and models used. While it references several papers, it does not synthesize their contributions into a deeper narrative or framework. There is minimal critical evaluation or abstraction to highlight broader patterns or principles."}}
{"id": "2c0d030f-872e-4617-a671-c5e26bbb1502", "title": "Methodologies.", "level": "paragraph", "subsections": [], "parent_id": "7d6edd11-57f7-441d-94ed-895becb5a5ba", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Knowledge Graph"], ["subsubsection", "Knowledge Graph Completion"], ["paragraph", "Methodologies."]], "content": "KGC can be solved with an encoder-decoder framework. To encode the local neighborhood information of an entity, the encoder can be chosen from a variety of GNNs such as GCN, R-GCN  and Attention-based GNNs . \nThen, the encoder maps each entity (subject entity and object entity) $v_i \\in \\mathcal{V}$ to a real-valued vector $e_i \\in \\mathbb{R}^d$. Relation can be represented as an embedding $e_r$ or a matrix $M_r$. Following the framework concluded by , the GNN encoder in a multi-relational graph (such as KG) can be formulated as: \n\\begin{equation}\n\\label{KGC_eq}\n\\begin{aligned}\n    a_v^{(l)} &= AGGREGATE_l(h_{r,u}^{(l-1)}, \\forall u \\in \\mathcal{N}_v^r) \\\\\n    h_v^{(l)} &= COMBINE_l(h_{r0, v}^{(l-1)}, a_v^{(l)})\n\\end{aligned}\n\\end{equation}\nwhere $h_{r,u}^{(l-1)}$ denotes the message passing from the neighbor node $u$ under relation $r$ at the l th layer. For example, RGCN  sets $h_{r,u}^{(l-1)} = W_r^{(l-1)}h_u^{(l-1)}$ and $AGGREGATE(\\cdot)$ be mean pooling. \nSince the knowledge graph is very large, the update of the node representation Eq.\\ref{KGC_eq} is efficiently implemented by using sparse matrix multiplications to avoid explicit summation over neighborhoods in practice.\nThe decoder is a knowledge graph embedding model and can be regarded as a scoring function. The most common decoders of knowledge graph completion includes translation-based models (TransE ), tensor factorization based models (DistMult , ComplEx) and neural network base models (ConvE ). In Table~\\ref{tab:my_kgc} , we summarize these common scoring functions following . $\\text{Re}(\\cdot)$ denotes the real part of  a vector, $*$ denotes convolution operator, $\\omega$ denotes convolution filters and $g(\\cdot)$ is a non-linear function. For example, RGCN uses DistMult as a scoring function, and DistMult can perform well on the standard link prediction benchmarks when used alone. In DistMult, every relation r is represented by a diagonal matrix $M_r \\in \\mathbb{R}^{d \\times d}$ and a triple is scored as $f(s, r, o) = e_s^T M_r e_o$.\nAt last, the model is trained with negative sampling, which randomly corrupts either the subject or the object of each positive example. To optimize KGC models, cross-entropy loss  and margin-based loss  are common loss functions used for optimizing KGC models.\n\\begin{table}[]\n    \\centering\n    \\small\n    \\caption{KGC Scoring Function.}\n    \\resizebox{\\textwidth}{!}{\n    \\begin{tabular}{cccc}\n    \\hline\nModel & Ent. embed. & Rel. embed. & Scoring Function $f(s, r, o)$ \\\\ \\hline\n{DistMult} & \\multirow{4}{*}{$e_s, e_o \\in \\mathbb{R}^d$} & \\multirow{4}{*}{$M_r \\in \\mathbb{R}^{d \\times d}$} & \\multirow{4}{*}{$e_s^T M_r e_o$} \\\\ \n & & & \\\\\n & & & \\\\\n & & & \\\\ \\hline\nComplEx & \\multirow{2}{*}{$e_s, e_o \\in \\mathbb{C}^d$} & \\multirow{2}{*}{$e_r \\in \\mathbb{C}^d$} & \\multirow{2}{*}{$\\text{Re}(e_r, e_s, \\Bar{e_t})=\\text{Re}(\\sum_{k=1}^K e_r e_s\\Bar{e_t})$} \\\\\n & & & \\\\ \\hline\nConvKB & \\multirow{2}{*}{$e_s, e_o \\in \\mathbb{R}^d$} & \\multirow{2}{*}{$e_r \\in \\mathbb{R}^d$} & \\multirow{2}{*}{$\\text{concat}(\\sigma([e_s, e_r, e_o]*\\omega))\\cdot w$}\\\\\n & & & \\\\ \\hline\nConvE & \\multirow{2}{*}{$M_s \\in \\mathbb{R}^{d_w \\times d_h}, e_o \\in \\mathbb{R}^d$} & \\multirow{2}{*}{$M_r \\in \\mathbb{R}^{d_w \\times d_h}$} &  \\multirow{2}{*}{$\\sigma(vec(\\sigma([M_s; M_r]*\\omega))W)e_o$} \\\\ \n & & & \\\\ \\hline\nConv-TransE & \\multirow{3}{*}{$e_s, e_o \\in \\mathbb{R}^d$} & \\multirow{3}{*}{$e_r \\in \\mathbb{R}^d$} & \\multirow{3}{*}{$g(vec(M(e_s, e_r))W)e_o$} \\\\\n & & & \\\\\n & & & \\\\\n\\hline\n    \\end{tabular}}\n    \\label{tab:my_kgc}\n\\end{table}", "cites": [259, 1076, 1054, 1168, 7332, 1166, 1167, 1165, 1087, 1059], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 15, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of methodologies for Knowledge Graph Completion using GNNs, integrating key encoder and decoder approaches from cited works. It synthesizes the general framework of GNN encoders and links them to various decoder models, but lacks deeper critical evaluation or nuanced comparison of their strengths and weaknesses. While it introduces some general patterns (e.g., relation representation as embeddings or matrices), it remains focused on explaining specific models rather than offering meta-level insights."}}
{"id": "b2a75f7a-cb34-42e8-a77f-b15f9ff16987", "title": "Benchmarks and Evaluation", "level": "paragraph", "subsections": [], "parent_id": "7d6edd11-57f7-441d-94ed-895becb5a5ba", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Knowledge Graph"], ["subsubsection", "Knowledge Graph Completion"], ["paragraph", "Benchmarks and Evaluation"]], "content": "Common KGC benchmark datasets include FB15k-237 , WN18RR , NELL-995   and Kinship  . Two commonly used evaluation metrics are \\textit{mean reciprocal rank} (MRR) and \\textit{Hits at $n$} (H@n), where $n$ is usually 1, 3, or 10.", "cites": [1169, 8423, 1167], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section is largely descriptive, listing benchmark datasets and evaluation metrics without contextualizing them within the cited works. It mentions MRR and Hits@n but does not explain their relevance to the specific contributions or limitations of the cited papers. There is minimal synthesis or abstraction, and no critical evaluation of the methods or datasets."}}
{"id": "e6b7839a-b79c-453e-8bbf-75068032a1c0", "title": "Methodologies.", "level": "paragraph", "subsections": [], "parent_id": "e6909d10-3eb4-43c8-bca9-7c6aa201a2e2", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Knowledge Graph"], ["subsubsection", "Knowledge Graph Alignment"], ["paragraph", "Methodologies."]], "content": "GNN-based KGA or entity alignment approaches mostly use GNN models to learn the representations of the entities and relations in different KGs. Then, entity/relation alignment can be performed by computing the distance between two entities/relations. GCN is widely used in . To further capture the relation information existing in multi-relational KGs,  proposed a Relation-aware Dual-Graph Convolutional Network (RDGCN), which also applied a graph attention mechanism. Similarly,  also introduced relation information by proposing a vectorized relational graph convolutional network (VR-GCN).  proposed\na Multi-channel Graph Neural Network\nmodel (MuGNN) containing a KG self-attention module and a cross-KG attention module to encode two KGs via multiple channels. GAT is another common model, which is applied in . Moreover,  also introduced a gating mechanism to control the aggregation of neighboring information.\nEntity/relation alignments are predicted by the distance between the entity/relation embeddings. The distance measuring functions are mainly based on L1 norm , L2 norm , cosine similarity , and feed-forward neural network .", "cites": [1064, 1093, 1054, 1092, 1094], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a factual overview of GNN-based methodologies for knowledge graph alignment, listing the models used and their core components. While it mentions variations such as relation-aware designs and attention mechanisms, it lacks deeper synthesis of how these approaches relate or differ in their objectives and effectiveness. There is minimal critical evaluation of the methods, and the abstraction remains limited to general mentions of common components rather than identifying broader trends or principles."}}
{"id": "9d96174f-348a-477e-a6c0-693bb2c01b27", "title": "Benchmarks and Evaluation.", "level": "paragraph", "subsections": [], "parent_id": "e6909d10-3eb4-43c8-bca9-7c6aa201a2e2", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Knowledge Graph"], ["subsubsection", "Knowledge Graph Alignment"], ["paragraph", "Benchmarks and Evaluation."]], "content": "Common KGA benchmarks datasets include~$DBP15K$  and $DWY100K$ . $DBP15K$ contains three cross-lingual datasets: $DBP_{ZH-EN}$ (Chinese to English), $DBP_{JA-EN}$ (Japanese to English), and $DBP_{FR-EN}$ (French to English). $DWY100K$ is composed of two large-scale cross-resource datasets: $DWY-WD$ (DBpedia to Wikidata) and $DWY-YG$ (DBpedia to YAGO3). Hits@N, which is calculated by measuring the proportion of correctly aligned entities/relations in the top N list, is used as evaluation metric to assess the performance of the models.", "cites": [1170], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual summary of common KGA benchmarks (DBP15K, DWY100K) and the Hits@N evaluation metric. It mentions the purpose of these datasets but does not integrate or synthesize insights from the cited paper or connect them to broader trends. There is minimal critical evaluation or abstraction beyond the specific examples."}}
{"id": "32090b4e-4673-4b32-85db-1ed30e9a7762", "title": "Background and Motivation", "level": "paragraph", "subsections": ["087f54ae-2ede-4f20-91f6-69317a69c326", "e300c54a-3399-457f-97b4-baeb74f5a763"], "parent_id": "6a807e8a-9cf4-40e4-9702-d7cee5c06d68", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Information Extraction"], ["paragraph", "Background and Motivation"]], "content": "Information Extraction (IE) aims to extract entity pairs and their relationships of a given sentence or document. IE is a significant task because it contributes to the automatic knowledge graph construction from unstructured texts. With the success of deep neural networks, NN-based methods have been applied to information extraction. However, these methods often ignore the non-local and non-sequential context information of the input text . Furthermore, the prediction of overlapping relations, namely the relation prediction of pairs of entities sharing the same entities, cannot be solved properly . To these ends, GNNs have been widely used to model the interaction between entities and relations in the text.", "cites": [9148], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview by identifying limitations of existing methods in information extraction, such as the neglect of non-local and non-sequential context, and the inability to handle overlapping relations. It synthesizes these issues and connects them to the adoption of GNNs. While it introduces a general idea of GNNs modeling entity and relation interactions, the critical analysis is limited to stating problems without a deeper evaluation of the cited paper's solutions or alternatives."}}
{"id": "087f54ae-2ede-4f20-91f6-69317a69c326", "title": "Methodologies", "level": "paragraph", "subsections": [], "parent_id": "32090b4e-4673-4b32-85db-1ed30e9a7762", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Information Extraction"], ["paragraph", "Background and Motivation"], ["paragraph", "Methodologies"]], "content": "Information extraction composed of two sub-tasks: named entity recognition (NER) and relation extraction (RE).\nNER predicts a label for each word in a sentence, which is often regarded as a sequence tagging task . RE predicts a relation type for every pair of entities in the text. When the entities are annotated in the input text, the IE task degrades into an RE task .\nGNN-based IE approaches typically operate via a pipeline approach. First, a text graph is constructed. Then, the entities are recognized and  the relationships between entity pairs are predicted. Very recently, researchers starts to jointly learn the NER and RE to take advantage of the interaction between these two sub-tasks . Followings are the introduction of different GNN-based techniques.\n\\begin{itemize}\n    \\item\\textbf{Graph Construction} Most GNN-based information extraction methods design specific rules to construct static graphs. Because the input of IE task is usually a document containing multiple sentences, the nodes in the constructed graph can be words, entity spans and sentences and the corresponding edges are word-level edges, span-level edges and sentence-level edges. These nodes can be connected by syntactic dependency edges , co-reference edges , re-occurrence edges , co-occurrence edges , adjacent word edges  and adjacent sentence edge . Recently, dynamic graph construction has also been successfully applied in IE tasks.  proposed a general IE framework using dynamically constructed span graphs, which selected the most confident entity spans from the input document and linked these span nodes with co-references and confidence-weighted relation types.  first constructs a static entity-relation bipartite graph and then investigates the dynamic graph for pruning redundant edges.\n\\item\\textbf{Graph Representation Learning}\nTo better capture non-local information of the input document, a variety of GNN models are applied in the NER task and the RE task. In addition, joint learning is a critical technique to reduce error propagation along the pipeline.\nFor the name entity recognition task, common GNN models such as GCN  are applied. GCN is the most common GNN models used in the relation extraction task . To learn edge type-specific representations,  introduces a labelled edge GCN to keep separate parameters for each edge type. Inspired by the graph attention mechanism,  proposes attention guided GCN to prune the irrelevant information from the dependency trees.\nRecently, many joint learning models have been proposed to relieve the error propagation in the pipeline IE systems and leverage the interaction between the NER task and the RE task.  proposes a GraphRel model containing 2 phases prediction of the entities and relations.  introduces a general framework to couple multiple information extraction sub-tasks by sharing entity span representations which are refined using contextualized information from relations and co-references.  develops a paradigm that first detected entity spans, and then performed a joint inference on entity types and relation types.\n\\end{itemize}", "cites": [9148, 1096, 1090, 1054], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of GNN-based methodologies in information extraction, mentioning key techniques and some recent approaches. It integrates basic ideas from the cited papers, such as the use of dynamic graphs and joint learning, but does so at a surface level without deep synthesis or critique. The discussion identifies patterns (e.g., graph construction methods) but lacks meta-level insights or a critical evaluation of the approaches."}}
{"id": "4522da4f-94d5-4839-a78a-5d561e94a5da", "title": "Semantic and Syntactic Parsing", "level": "subsection", "subsections": ["674b7749-3fe5-4ff7-a02e-306d593b772c", "7cf8e4e2-1736-42d0-b6b0-40b54661a101"], "parent_id": "00a0dbec-0833-4b8c-ae79-f3460d08d96c", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Semantic and Syntactic Parsing"]], "content": "In this section, we mainly discuss applications of GNN for parsing, including syntax related and semantics related parsing. For syntax related parsing, GNN has been employed in tasks of dependency parsing and constituency parsing. For semantics related parsing, we will briefly introduce semantic parsing and AMR (Abstract Meaning Representation) parsing.", "cites": [1117], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual overview of GNN applications in syntax and semantics parsing, mentioning dependency parsing, constituency parsing, and AMR parsing. However, it lacks synthesis of ideas from the cited papers, offers minimal critical evaluation, and does not abstract to broader trends or principles beyond listing tasks and briefly naming methods."}}
{"id": "3f9ef9ef-bf95-4885-911d-b649ee923cea", "title": "Background and motivation", "level": "paragraph", "subsections": [], "parent_id": "674b7749-3fe5-4ff7-a02e-306d593b772c", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Semantic and Syntactic Parsing"], ["subsubsection", "Syntax Related Parsing"], ["paragraph", "Background and motivation"]], "content": "The tasks related to syntax are mainly dependency parsing and constituency parsing. Both of them aim to generate a tree with syntactic structure from natural language sentences, conforming to predefined formal grammar rules. Dependency parsing focuses on the dependency relationship between words in sentence. Constituency parsing focuses on the compositional relationship between different components in a sentence. Traditional approaches can be divided into two directions: transition-based and graph-based. Transition-based methods usually formalize this problem as a series of decisions on how to combine different words into a syntactic structure. Graph-based methods firstly score all word pairs in a sentence on the possibility of holding valid dependency relationship, and then exploit decoders to generate the parse trees.", "cites": [2401, 1171], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a basic description of syntax-related parsing tasks and traditional methods (transition-based and graph-based), referencing two papers. However, it does not deeply synthesize their content or connect them meaningfully. There is no critical evaluation or comparison of the cited works, and the abstraction is limited to surface-level categorization without deeper insight into overarching trends or principles."}}
{"id": "a61679e4-4c82-4920-863c-c0dde2c1ebd9", "title": "Methodologies", "level": "paragraph", "subsections": [], "parent_id": "674b7749-3fe5-4ff7-a02e-306d593b772c", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Semantic and Syntactic Parsing"], ["subsubsection", "Syntax Related Parsing"], ["paragraph", "Methodologies"]], "content": "Here, we mainly focus on graph-based parsers where graph neural network plays the role of extracting high-order neighbor features.\n\\begin{itemize}\n    \\item \\textbf{Dependency parsing}.  \nIn graph-based parsers, we take each word as a node in a graph and the key task is to learn a low-dimensional node representation with a neural encoder. \nTo incorporate more dependency structure information,  proposes to employ GNN as a encoder to incorporate high-order information. Their encoder contains both GNN and Bi-LSTM, where the GNN accepts all node embeddings from Bi-LSTM and take them as node embeddings in a complete graph. The constructed graphs are dynamic graphs where edge weight can change consistently during training. There are two kinds of loss functions: 1) the first one considers both tree structure and dependency relation labels; 2) the second one are applied after each GNN layer where only tree structure is considered. \nOther than the generation of dependency parsing trees, some other works focus on how to do reranking among different candidates to choose a best parsing tree.  demonstrate that GNN can also work well as a encoder for dependency parsing trees in a neural reranking model.\n\\item\\textbf{Constituency parsing}. Most approaches for constituency parsing are transition-based~ which generate the constituency parsing tree by executing an action sequences.  proposes to use GNN to encode the partial tree in the decoding process which can generate one token per step. Other methods usually generate the final parsing tree by combining different sub-trees in a shift-reduce way. The authors believe that this strongly incremental way is more closer to the way of human thinking.\n\\end{itemize}", "cites": [1117], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes information from cited papers by integrating the role of GNNs in dependency and constituency parsing, connecting their use as encoders and rerankers. It provides a basic analytical perspective by highlighting how different architectures and training objectives are applied, though the critical evaluation is limited. Some abstraction is attempted by mentioning broader parsing strategies, but the analysis remains grounded in specific methods rather than offering meta-level insights."}}
{"id": "d6911db9-788b-44eb-b210-78ee35fe97ab", "title": "Methodologies", "level": "paragraph", "subsections": [], "parent_id": "7cf8e4e2-1736-42d0-b6b0-40b54661a101", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Semantic and Syntactic Parsing"], ["subsubsection", "Semantics Related Parsing"], ["paragraph", "Methodologies"]], "content": "Here, we provide a summary of the techniques for two typical semantic related parsing tasks, namely, SQL parsing and AMR parsing.\n\\begin{itemize}\n\\item\\textbf{SQL parsing}. The main purpose of SQL parsing is to convert natural language into SQL queries that can be successfully executed.\nMost of the traditional methods~ are sequential encoder based, which however, lost some other useful information at the source side, such as syntax information and DB schema information. Thus, many GNN-based models are proposed.\nFor syntactic information,  use external parser to perform syntactic parsing (i.e., constituency parsing and dependency parsing) on the raw sentence. Then they exploit the syntactic parsing tree instead of the source sentence as input, and use GNN to learn the syntactic structure and dependent information in this \"tree\" graph. It has been proved experimentally that the additional syntactic information is helpful for semantic parsing tasks.\nSQL parsing problem becomes more complex if the DB schema of the training and testing set are different~. To this end, some works propose to model these schema information to achieve better results. For example,  takes the DB schema as a graph and use GGNN to learn the node representation. Then they incorporate schema information on both the encoder and decoder to generate the final results.  employs GNN to globally select the overall structure of the output query which could decrease the ambiguity of DB constants choice.\nAfter the SQL queries are generated, reranking can be utilized to further improve the performance. Reranking the candidates predicted by the model is helpful to reduce the likelihood of picking some sub-optimal results. SQL queries are structured and it is a reasonable way to use GNN to encode the SQL queries in the reranking model. For example,  employs graph-based transformer to rearrange the results generated by the neural semantic parser and achieved good results.\n\\item\\textbf{AMR parsing}.\nSimilar to , syntactic information, especially dependency relation information, are also employed in AMR parsing.  considers both the dependency syntactic graph and the latent probabilistic graph. Specifically, by learning a vector representation for the two graph structures and then fusing them together, their model leverages the structure information in the source side and achieve better performance compared to seq-to-graph-like models.\n\\end{itemize}", "cites": [1079, 8416], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of methodologies used in SQL and AMR parsing with GNNs, referencing some works but without substantial synthesis or abstraction. It lacks critical evaluation of the approaches and primarily serves to outline methods rather than analyze trends or offer a deeper comparative insight."}}
{"id": "2323462b-be89-4297-80a8-27a538cca793", "title": "Benchmark and Evaluation", "level": "paragraph", "subsections": [], "parent_id": "7cf8e4e2-1736-42d0-b6b0-40b54661a101", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Semantic and Syntactic Parsing"], ["subsubsection", "Semantics Related Parsing"], ["paragraph", "Benchmark and Evaluation"]], "content": "For SQL parsing, three benchmark datasets are commanly used, including ATIS, GEO, WikiSQL, SPIDER. For AMR parsing, AMR annotation release is a well-recognized dataset. For evaluation metrics, accuracy, including exact match accuracy and execution accuracy, as well as Smatch score.) are commonly used.", "cites": [8416], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual list of benchmark datasets and evaluation metrics used in semantics-related parsing tasks but lacks synthesis of ideas from the cited paper. It does not compare or evaluate different approaches or highlight limitations, and offers no generalization or abstraction of broader patterns in the field."}}
{"id": "e12bc973-45c4-4e12-87e1-fa0f7605db2f", "title": "Reasoning", "level": "subsection", "subsections": ["11443dd1-c08b-4e5e-86c5-632a95465e72", "55a88d78-db19-4a6d-8c1f-ba90cd0d1b22", "274076f0-a300-43b5-959c-8b9b2316ff7d"], "parent_id": "00a0dbec-0833-4b8c-ae79-f3460d08d96c", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Reasoning"]], "content": "Reasoning is a significant research direction for NLP. In recent years, GNN begins to play an important role in NLP reasoning tasks, such as math word problem solving~, natural language inference~, common sense reasoning~ and so on. In this subsection, we will give a brief introduction for the three tasks and how graph neural networks are employed in these methods.", "cites": [1061, 1079], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal synthesis of the cited papers, merely listing the tasks where GNNs are applied without connecting the underlying methodologies or findings. There is no critical evaluation or comparison of the approaches, and the content lacks abstraction or identification of broader patterns in reasoning tasks using GNNs. It remains at a descriptive level with limited insight."}}
{"id": "eaf16e0b-cc0d-4710-8e31-293025ac12e2", "title": "Methodologies", "level": "paragraph", "subsections": [], "parent_id": "11443dd1-c08b-4e5e-86c5-632a95465e72", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Reasoning"], ["subsubsection", "Math word problem solving"], ["paragraph", "Methodologies"]], "content": " is the first to introduce GNN into math word problem solving. Graph2tree considers both the input and output structure information. At the input side, GNN is used to encode the input syntactic tree. After all the input nodes embedding are generated, on the output side, considering the hierarchy of the equation, a BFS-based tree decoder is used to generate the final equation result in a coarse-to-fine way.\n is another MWP automatic solving model that uses graph data structure to model 1) the relationship between the numbers in the problem, and 2) the relationship between different numbers with their corresponding descriptors. \nIn addition, some works introduce the external knowledge information in another way. For example,  first connects the entities in the problem description into graphs based on external global knowledge information, and then uses GAT as encoder. This method can enhance the ability of modeling the relationship between the entities in the problem, and has obtained good results.", "cites": [1079], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of methodologies used in math word problem solving with GNNs but does not synthesize ideas across the cited works in a meaningful way. It lacks critical evaluation or comparison of approaches, and does not abstract or generalize to highlight broader trends or principles in the use of GNNs for reasoning tasks."}}
{"id": "8f8a7a3e-e525-4ecc-9203-80e2cb39876a", "title": "Benchmarks and Evaluation", "level": "paragraph", "subsections": [], "parent_id": "11443dd1-c08b-4e5e-86c5-632a95465e72", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Reasoning"], ["subsubsection", "Math word problem solving"], ["paragraph", "Benchmarks and Evaluation"]], "content": "For math word problem, three benchmark datasets are commonly used, including MAWPS~, MATH23K~, and MATHQA~.", "cites": [1172], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.0, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section merely lists three benchmark datasets without providing any synthesis of their roles, comparing their characteristics, or abstracting broader trends in math word problem solving with GNNs. It lacks critical evaluation and does not integrate the cited paper into a larger analytical framework."}}
{"id": "8b8bf498-fc6b-4329-94f9-38eb4c9ad84f", "title": "Methodologies", "level": "paragraph", "subsections": [], "parent_id": "55a88d78-db19-4a6d-8c1f-ba90cd0d1b22", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Reasoning"], ["subsubsection", "Natural language inference"], ["paragraph", "Methodologies"]], "content": "Traditional methods are mostly based on neural encoder with attention, and most of them are RNN models. Considering the rich information contained in the external knowledge base, some works try to use external information to improve the accuracy of the model. For example,  uses graph-based attention model to incorporate the information from introduced external knowledge source. Their experiments demonstrate that adding the learned knowledge graph representation to the classifier help to obtain good results. Considering the introduced graph can have noisy information,  employs a encoder with a subgraph filtering module using Personalized PageRank before a GCN layer where the filtering module can help to select context relevant sub-graphs from introduced knowledge graph to reduce noisy information.", "cites": [1124, 1061], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes two papers that both address the infusion of external knowledge in NLI using graph-based methods, connecting their shared motivation of improving inference by leveraging knowledge graphs. It provides a basic comparison of the approaches (graph-based attention vs. subgraph filtering with PPR and GCN). While it identifies the issue of noisy information, it does not deeply critique the methods or explore trade-offs. The abstraction is moderate, as it generalizes the idea of using knowledge graphs and filtering mechanisms for NLI, but does not elevate the discussion to a meta-level framework."}}
{"id": "4b5a87b1-6b1b-45d5-832c-6b058e33e8db", "title": "Benchmarks and Evaluation", "level": "paragraph", "subsections": [], "parent_id": "55a88d78-db19-4a6d-8c1f-ba90cd0d1b22", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Reasoning"], ["subsubsection", "Natural language inference"], ["paragraph", "Benchmarks and Evaluation"]], "content": "For NLI task, three benchmark datasets are commonly used, including SNLI, MultiNLI, and SciTail.", "cites": [1174, 1173], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section simply lists three benchmark datasets used for NLI tasks without integrating or connecting them to the broader themes of the survey or to each other. It lacks critical analysis of the datasets' strengths or limitations and offers no abstraction or generalization beyond the specific examples mentioned."}}
{"id": "5a24c031-ad89-42ed-adc0-078794fb0ddf", "title": "Background and Motivation", "level": "paragraph", "subsections": [], "parent_id": "274076f0-a300-43b5-959c-8b9b2316ff7d", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Reasoning"], ["subsubsection", "Commonsense reasoning"], ["paragraph", "Background and Motivation"]], "content": "Commonsense reasoning helps neural models incorporate the \"common sense\" or world knowledge during inference. Take the commonsense QA as example, we aim to obtain a neural model tended to generate the answer which is more consistent with commonsense from multiple answers that all logically fit the requirements. In fact, large-scale pre-trained models such as GPT-2, BERT with simple fine-tuning can achieve very good results. However, some external knowledge sources can help the model to better characterize the question and the concepts in the answer, which will definitely help the overall performance.", "cites": [826], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides minimal synthesis by only mentioning that pre-trained models like BERT and GPT-2 can achieve good results with fine-tuning. It cites one paper (RoBERTa) but does not integrate it meaningfully into the discussion. There is no critical evaluation of the cited work or broader analysis of the field. The content remains at a descriptive level without abstracting general principles or identifying patterns in commonsense reasoning approaches."}}
{"id": "5136980b-7feb-4b3f-b1e2-e6af40359ed2", "title": "Methodologies", "level": "paragraph", "subsections": [], "parent_id": "274076f0-a300-43b5-959c-8b9b2316ff7d", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Reasoning"], ["subsubsection", "Commonsense reasoning"], ["paragraph", "Methodologies"]], "content": " introduces graph neural networks to the common sense reasoning task. The model first retrieves the concepts in the questions and options into an external knowledge base to obtain a schema graph, and then uses GCN to incorporate information from this retrieved graph to learned features. The learnt features would be fed to a simple score module for each QA pair. Experiments on large benchmarks dataset, e.g., CommonsenseQA~, demonstrate the effectiveness of the external knowledge base introduced by GNN.", "cites": [456], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of how graph neural networks are applied in commonsense reasoning, using one cited paper (CommonsenseQA) to explain the methodology. It lacks synthesis with other works, critical evaluation of the approach's strengths or weaknesses, and abstraction to broader trends or principles in the field."}}
{"id": "f7bfac1c-df06-4ba4-9b93-016da9aafa39", "title": "Benchmarks and Evaluation", "level": "paragraph", "subsections": [], "parent_id": "274076f0-a300-43b5-959c-8b9b2316ff7d", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Reasoning"], ["subsubsection", "Commonsense reasoning"], ["paragraph", "Benchmarks and Evaluation"]], "content": "We introduce some benchmark datasets for commonsense reasoning here: CommonsenseQA~; Event2Mind; SWAG; Winograd Schema Challenge; ReCoRD.", "cites": [1177, 1175, 456, 1176], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a simple list of benchmark datasets for commonsense reasoning without synthesizing their purposes, features, or relationships. It lacks critical evaluation of the datasets or the challenges they address, and offers no abstraction or generalization about trends or principles in commonsense reasoning evaluation."}}
{"id": "6daa5ac7-f507-4177-883a-1f6185330403", "title": "Background and Motivation", "level": "paragraph", "subsections": ["df0796a1-2f32-48fa-97a2-0aad44edb9dd", "a69889ea-7ac3-4576-a656-ea7a7edd32dc"], "parent_id": "a02902dc-e326-4cd7-8837-66a0cefa042c", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Semantic Role Labelling"], ["paragraph", "Background and Motivation"]], "content": "The problem of semantic role labeling (SRL) aims to recover the predicate-argument structure of a sentence, namely, to determine essentially “who did what to whom”, “when”, and “where. More formally, for every predicate, the SRL model must identify all argument spans and label them with their semantic roles. Such high-level structures can be used as semantic information for supporting a variety of downstream tasks, including dialog systems, machine reading and translation~. Recent SRL works can mostly be divided into two categories, i.e., syntax-aware~ and syntax-agnostic~ approaches according to whether incorporating syntactic knowledge or not. Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual\ninformation of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles~ or predicate-argument-role tuples~. Motivated by the strong interplay between syntax and semantics, researchers explore various approaches to integrate syntactic knowledge into syntax-agnostic models considering that the semantic representations are closely related\nto syntactic ones. For example, one can observe that many arcs in the syntactic dependency graph are mirrored in the semantic dependency graph. Given these similarities and the availability\nof accurate syntactic parser for many languages,\nit seems natural to exploit syntactic information\nwhen predicting semantics. \nHowever, the last\ngeneration of SRL models powered by deep learning models put syntax aside in favor of neural sequence models, namely LSTMs~ due to the challenges that (1) it is difficult to effectively incorporate syntactic information into neural SRL models, due to the sophisticated tree structure of syntactic relation; and (2) the syntactic parsers are unreliable on account of the risk of erroneous syntactic input, which may lead to error propagation and an unsatisfactory SRL performance. Given this situation, GNNs are emerging as powerful tools to capture and incorporate the syntax patterns into deep neural network-based SRL models. The nature property of GNN in capturing the complex relationship patterns in the structured data makes it a good fit for modeling syntactic dependency and constituency structures of sentences.", "cites": [1072, 1179, 7327, 1178], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple cited works, connecting syntax-aware and syntax-agnostic approaches in SRL and explaining how GNNs address the limitations of each. It provides a critical perspective by highlighting the challenges of integrating syntactic information and the reliability issues of parsers. The abstraction is strong, as it generalizes the role of GNNs in capturing structured syntactic patterns and their relevance to semantic modeling."}}
{"id": "df0796a1-2f32-48fa-97a2-0aad44edb9dd", "title": "Methodologies", "level": "paragraph", "subsections": [], "parent_id": "6daa5ac7-f507-4177-883a-1f6185330403", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Semantic Role Labelling"], ["paragraph", "Background and Motivation"], ["paragraph", "Methodologies"]], "content": "The problem solved by the GNN-based SRL models can be divided into two categories. One is about argument prediction given the predicates in a sentence~. Formally, SRL can be cast as a sequence labeling problem where given an input sentence, and the position of the predicate in the sentence, the goal is to predict a BIO sequence of semantic roles for the words in sentences; Another is about end-to-end semantic role triple extraction which aims to  detect all the possible predicates and their corresponding arguments in one shot~. Technically, given a sentence, the SRL model predicts a set of labeled predicate-argument-role triplets, while each triple contains a possible predicate token and two candidate tokens. Both of the above mentioned problems can be solved based on the GNN-based SRL models, which consists of two parts, namely, graph construction and graph representation learning.\n\\begin{itemize}\n    \\item\\textbf{Graph Construction}.\nThe graphs are constructed based on the syntax information, which can be extracted from two sources, one is syntactic dependency information and another is syntactic constituents information. Most of the existing GNN-SRL models~ have relied on syntactic dependency representations. In these methods, information from dependency trees are injected into word representations using GNN or self-attention mechanisms. Recently, Marcheggiani et al~ incorporated the constituency syntax into SRL models by conducting the message passing on a graph where nodes represent constituents. \nBased on the syntax information, the graphs constructed in the current SRL models are divided into three main categories: (1) directed homogeneous graphs; (2) heterogeneous graphs; and (3) probability weighted graphs. Most of the works~ represent the syntax information as a directed homogeneous graph where all the nodes are input word tokens and directed with dependent edges. Other work~ enhances SRL with heterogeneous syntactic knowledge by combining various syntactic treebanks that follow different annotation guidelines and domains. Liu et al.~ also construct a heterogeneous syntactic graph by incorporating several types of edges, including lexical relationships, syntactic dependency, co-occurrence relationships. Some work~ utilizes the probability matrix of all dependency arcs for constructing an edge-weighted directed graph to eliminate the influences of the error from the parsing results.\n\\item\\textbf{Graph Representation Learning}. \nAs described in Section 6, various GNN models can be utilized for graph representation learning. Here, we introduce the different roles that GNNs play in different SRL models. In most of the works~, GNN is utilized as an encoder to learn the final representations of words which follows a typical word embedding layer, such as BiLSTM. While in some works~,\nGNN is utilized to extract the initial words' embedding, which are regarded as inputs of the encoder. For example, Xia et al.~ combines the syntax embedding extracted from GNN with the word embedding and character embedding as the input. Fei~ utilizes GNN to refine the initial word embedding which consists of word representation and part-of-speech (POS) tags, and then input the refined word embedding into the BiLSTM encoder.\n\\end{itemize}", "cites": [1072, 1080, 1063, 274], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key ideas from multiple papers on GNN-based SRL, integrating them into a coherent framework around graph construction and representation learning. It abstracts these methods into broad categories, such as types of graphs and roles of GNNs, offering a meta-level understanding of the field. However, it lacks deeper critical analysis of the limitations or trade-offs of the different approaches."}}
{"id": "7756814f-b5ea-42f2-8adb-261a5edae44d", "title": "Related Libraries and Codes", "level": "subsection", "subsections": [], "parent_id": "00a0dbec-0833-4b8c-ae79-f3460d08d96c", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "Applications"], ["subsection", "Related Libraries and Codes"]], "content": "Open-source implementations facilitate the research works of baseline experiments in graph neural networks for NLP. Besides various paper codes were released individually, there is a recently released library called \\textit{Graph4NLP}~\\footnote{The codes and details of \\textit{Graph4NLP} library are provided at~\\url{https://github.com/graph4ai/graph4nlp}.}, which is an easy-to-use library for R\\&D at the intersection of Deep Learning on Graphs and Natural Language Processing. It provides both full implementations of state-of-the-art models mentioned above for several NLP applications including text classification, semantic parsing, machine translation, KG completion, and natural language generation. \\textit{Graph4NLP} also provides flexible interfaces to build customized models for researchers and developers with whole-pipeline support. Built upon highly-optimized runtime libraries including \\textit{DGL} and \\textit{Pytorch}, Graph4NLP has both high running efficiency and great extensibility. The architecture of \\textit{Graph4NLP} consists of four different layers: 1) Data Layer, 2) Module Layer, 3) Model Layer, and 4) Application Layer. There are also some other related GNN-based libraries. \nNoticeably,  published a geometric learning library in \\textit{PyTorch} named \\textit{PyTorch Geometric}, which implements many GNNs. The \\textit{Deep Graph Library (DGL)}~ was released which provides a fast implementation of many GNNs on top of popular deep learning platforms such as \\textit{PyTorch} and \\textit{MXNet}. The \\textit{Dive into Graphs}  was released recently as a research-oriented library that integrates unified and extensible implementations of common graph deep learning algorithms for several advanced tasks.", "cites": [1180, 1181], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual description of relevant libraries and their functionalities, including Graph4NLP, PyTorch Geometric, and Dive into Graphs. It integrates minimal information across the cited papers, focusing on features and purposes rather than deeper connections. There is little critical evaluation or abstraction to broader research trends or principles."}}
{"id": "016f04a1-69a9-41de-bf08-be29d204570e", "title": "Dynamic Graph Construction", "level": "subsection", "subsections": [], "parent_id": "a1a4cd74-5cae-4846-95de-2b1e7e9e3278", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "General Challenges and Future Directions"], ["subsection", "Dynamic Graph Construction"]], "content": "As we see in~\\cref{sec:Applications}, \nmany NLP problems can be tackled from a graph perspective, and GNNs are naturally applicable to and good at handling graph-structured data. \nThus, the graph construction process plays an important role in the overall model performance. However, constructing a high quality and task-specific graph requires a good amount of domain expertise and human effort.\nMoreover, graph construction in NLP is often more art than science, informed solely by insight of the practitioner, and involves many trials and errors. Even though a few of existing works already explored dynamic graph construction, most GNN applications in NLP still heavily relied on domain expertise for static graph construction.\nThe exploration of dynamic graph construction for NLP, is still at its early stage and faces several challenges: first of all, most works on dynamic graph construction focused only on homogeneous graph construction~, and dynamic graph construction for heterogeneous graphs~ is much less explored especially for NLP tasks. \nCompared to homogeneous graphs, heterogeneous graphs are capable of carrying on richer information on node types and edge types, and occur frequently in many NLP problems. Dynamic graph construction for heterogeneous graphs is also supposed to be more challenging because more types of information (e.g., node types, edge types) are expected to be learned from data.\nSecond, most existing dynamic graph construction techniques rely on some form of pair-wise node similarity computation whose time complexity is at least $O(n^2)$ where $n$ is the number of graph nodes. This results in scalability issues when scaling to large graphs such as KGs.\nRecently, a scalable graph structure learning approach with linear time and memory complexity (in terms of the number of nodes) was proposed by adopting the anchor-based approximation technique to avoid explicit pair-wise node similarity computation~. \nFinally, various efficient transformers~ were also developed which could inspire the research in scalable dynamic graph construction considering their close connections;\n(3) As observed in some previous works, dynamic graph construction does not clearly outperform static graph construction in some NLP applications. There is still room for improvement for dynamic graph construction techniques in terms of downstream task performance.\nOther interesting future directions in this line include dynamically learning edge directions for graphs, and combining static and dynamic graph construction for better performance.", "cites": [1077, 1183, 8384, 1080, 798, 1058, 1182, 7333, 1071, 7827], "cite_extract_rate": 0.7692307692307693, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers to highlight the current state and limitations of dynamic graph construction in NLP. It critically assesses the reliance on static graphs and scalability issues, while also pointing out performance gaps and suggesting future research directions. The abstraction is strong, identifying broader trends such as the need for handling heterogeneous graphs and drawing parallels with efficient Transformer variants for scalable solutions."}}
{"id": "f548d0d9-cea7-4c32-919b-30e88b370b3e", "title": "Pre-training GNNs for NLP", "level": "paragraph", "subsections": [], "parent_id": "4a6c944b-81e7-4c80-a42e-6edea96586d5", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "General Challenges and Future Directions"], ["subsection", "GNNs vs Transformers for NLP"], ["paragraph", "Combining GNNs with Transformers for NLP"], ["paragraph", "Pre-training GNNs for NLP"]], "content": "One of the most important trends in NLP over the past few years is to develop large-scale pre-trained models  most of which are based on Transformer architectures. Recently, there are also many research efforts on pre-training GNNs on graphs~ using self-supervised learning methods. However, there are very few attempts to pre-train GNNs for NLP~, which may exploit more types of data sources than Transformers since GNNs can take both graph structured data (e.g., from KGs) and unstructured data (e.g., from free-form text).", "cites": [7335, 679, 1184, 1185, 7, 7334, 8424], "cite_extract_rate": 1.0, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a concise analytical overview of pre-training GNNs for NLP, highlighting the trend and potential benefits over traditional Transformers. It synthesizes multiple cited works to position GNN pre-training as a promising direction that can leverage both structured and unstructured data. While it identifies a gap in the literature, it lacks in-depth critical evaluation or comparison of the cited pre-training methods."}}
{"id": "d5112b26-9cf6-436c-8f21-ef42b7e5e2e1", "title": "Knowledge Graph Embedding and Completion", "level": "paragraph", "subsections": [], "parent_id": "640f05fc-40f2-41f5-9a27-46b7cd44167d", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "General Challenges and Future Directions"], ["subsection", "Knowledge Graph in NLP"], ["paragraph", "Knowledge Graph Augmentation"], ["paragraph", "Knowledge Graph Embedding and Completion"]], "content": "GNN-based KGE and KGC approaches consider incorporating the neighborhood of a head entity or a tail entity. There is a trade-off between \\textit{all triples training on the same large KG } and \\textit{each triple training on a separate knowledge subgraph constructed from the original KG }. The former one provides more computational efficiency while the latter one has more powerful model expressiveness.\nFuture research will focus on jointly reasoning text and KG by applying such methods and paying attention to the entities mentioned in the text . Logic rules play an important role to determine whether a triple is valid or not, which may be useful for KGE and KGC .", "cites": [1076, 1059], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a general analytical discussion of GNN-based approaches for Knowledge Graph Embedding (KGE) and Completion (KGC), noting the trade-off between training efficiency and model expressiveness. It integrates information from the cited papers by referencing subgraph reasoning and structure-aware modeling. However, the analysis remains at a high-level without deep comparative critique or extensive abstraction to overarching principles."}}
{"id": "2b682f29-8553-4202-9fd9-aff5aaa2b317", "title": "Knowledge Graph Alignment", "level": "paragraph", "subsections": [], "parent_id": "640f05fc-40f2-41f5-9a27-46b7cd44167d", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "General Challenges and Future Directions"], ["subsection", "Knowledge Graph in NLP"], ["paragraph", "Knowledge Graph Augmentation"], ["paragraph", "Knowledge Graph Alignment"]], "content": "Most of the existing GNN-based KG alignment models also face three critical challenges to be further explored: (1) Different KGs usually have heterogeneous schemas, and may mislead the representation learning , which makes it is difficult to integrate knowledge from different KGs ; (2) The data in KG is usually incomplete  which needs pre-processing; (3) The seed alignments are limited . How to iteratively discover new entity alignments in the GNN-based framework is a future direction .", "cites": [1064, 1093, 1092], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes key challenges from the cited papers, particularly highlighting schema heterogeneity, incomplete data, and limited seed alignments. While it identifies these issues and links them to GNN-based approaches, the critique remains somewhat general without deep evaluation of the relative merits or limitations of each paper. It abstracts the problems at a moderate level, pointing to broader directions like iterative alignment discovery, but does not fully articulate overarching principles or theoretical implications."}}
{"id": "1b204209-02a4-4cc2-963e-75d531aab4eb", "title": "Multi-relational Graph Neural Networks", "level": "subsection", "subsections": [], "parent_id": "a1a4cd74-5cae-4846-95de-2b1e7e9e3278", "prefix_titles": [["title", "Graph Neural Networks for Natural Language Processing: \\\\ A Survey"], ["section", "General Challenges and Future Directions"], ["subsection", "Multi-relational Graph Neural Networks"]], "content": "Multi-relational graphs, which adopt unified nodes but relation-specific edges, are widely observed and explored in many NLP tasks. As we discussed in Section~\\ref{sec:graph_representation_learning_heterogeneous}, most multi-relational GNNs, which are capable of exploiting multi-relational graphs, are extended from conventional homogeneous GNNs. Technically, most of them either apply relation-specific parameters during neighbor aggregation or split the heterogeneous graphs to homogeneous sub-graphs~. Although impressive progresses have been made, there is still a challenge in handling over-parameterization problem due to the diverse relations existing in the graph. Although several tricks such as parameter-sharing (e.g, see Directed-GCN~) and matrix-decomposition (e.g., see R-GCN~) are widely used to enhance the models' generalization ability to address this issue, they still have limitations, such as resulting in the potential loss of the models' expression ability. There is a hard trade-off between the over-parameterization and powerful model expression ability.\nIt is worth noting that various graph transformers have been introduced to exploit the multi-relational graphs~. However, the challenge exists in how to fully take advantage of the strong inductive bias (i.e., the graph topology) of the graphs by the transformers which are naturally free of that. \nCurrently, most of them simply regard the self-attention's map as a fully-directed graph. On top of that, researchers either apply sparsing mechanisms~ or allow remote connection~ according to the given graph. How to develop an effective and general architecture for multi-relational graphs (or heterogeneous graphs) needs further exploration.", "cites": [259, 7044, 1055, 274], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the concept of multi-relational GNNs across the cited papers, integrating ideas on relation-specific parameters and graph decomposition. It critically analyzes limitations of current approaches, such as over-parameterization and loss of expressive power, and highlights the ongoing trade-off. The discussion abstracts beyond individual papers to present broader architectural and modeling challenges in the context of heterogeneous graphs and transformers."}}
