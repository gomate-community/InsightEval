{"id": "8601ef96-9801-4983-92f1-2af4dfd88e00", "title": "Introduction", "level": "section", "subsections": ["428afaa2-5dec-4cec-9bf9-941903e0e928"], "parent_id": "eab36b40-c6ea-4c4a-96e7-a440a76d2534", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Introduction"]], "content": "\\label{sec:intro}\nLearning an adequate similarity measure on a feature space can significantly determine the performance of machine learning methods. Learning such measures automatically from data is the primary aim of similarity learning. \nSimilarity/Metric learning refers to learning a function to measure the distance or similarity between objects, which is a critical step in many machine learning problems, such as classification, clustering, ranking, etc. For example, in k-Nearest Neighbor (kNN) classification , a metric is needed for measuring the distance between data points and identifying the nearest neighbors; in many clustering algorithms, similarity measurements between data points are used to determine the clusters. Although there are some general metrics like Euclidean distance that can be used for getting similarity measure between objects represented as vectors, these metrics often fail to capture the specific characteristics of the data being studied, especially for structured data. Therefore, it is essential to find or learn a metric for measuring the similarity of data points involved in the specific task. \nMetric learning has been widely studied in many fields on various data types. For instance, in computer vision, metric learning has been explored on images or videos for image classification, object recognition, visual tracking, and other learning tasks . In information retrieval, such as in search engines, metric learning has been used to determine the ranking of relevant documents to a given query . In this paper, we survey the existing work in similarity learning for graphs, which encode relational structures and are ubiquitous in various domains. \nSimilarity learning for graphs has been studied for many real applications, such as molecular graph classification in chemoinformatics , protein-protein interaction network analysis for disease prediction , binary function similarity search in computer security , multi-subject brain network similarity learning for neurological disorder analysis , etc. In many of these application scenarios, the number of training samples available is often very limited, making it a difficult problem to directly train a classification or prediction model. With graph similarity learning strategies, these applications benefit from pairwise learning that utilizes every pair of training samples to learn a metric for mapping the input data to the target space, which further facilitates the specific learning task. \nIn the past few decades, many techniques have emerged for studying the similarity of graphs. Early on, multiple graph similarity metrics were defined, such as the Graph Edit Distance , Maximum Common Subgraph , and Graph Isomorphism , to address the problem of graph similarity search and graph matching. However, the computation of these metrics is an NP-complete problem in general . Although some pruning strategies and heuristic methods have been proposed to approximate the values and speed up the computation, it is difficult to analyze the computational complexities of the above heuristic algorithms and the sub-optimal solutions provided by them are also unbounded . Therefore, these approaches are feasible only for graphs of relatively small size and in practical applications where these metrics are of primary interest. Thus it is hard to adapt these methods to new tasks. In addition, for other methods that are relatively more efficient like the Weisfeiler-Lehman method in , since it is developed specifically for isomorphism testing without mapping functions, it cannot be applied for general graph similarity learning. More recently, researchers have formulated similarity estimation as a learning problem where the goal is to learn a model that maps a pair of graphs to a similarity score based on the graph representations. For example, graph kernels, such as path-based kernels  and the subgraph matching kernel , were proposed for graph similarity learning.  Traditional graph embedding techniques, such as geometric embedding, are also leveraged for graph similarity learning . \n\\gm{With the emergence of deep learning techniques, graph neural networks (GNNs) have become a powerful new tool for learning representations on graphs with various structures for various tasks. The main distinction between GNNs and the traditional graph embedding is that GNNs address graph-related tasks in an end-to-end manner, where the representation learning and the target learning task are conducted jointly , while the graph embedding generally learns graph representations in an isolated stage and the learned representations are then used for the target task. Therefore, the GNN deep models can better leverage the graph features for the specific learning task compared to the graph embedding methods. Moreover, GNNs are easily adapted and extended for various graph related tasks, including deep graph similarity learning tasks in different domains. For instance, in brain connectivity network analysis in neuroscience, community structure among the nodes (i.e. brain regions) within the brain network is an important factor that should be considered when learning node representations for cross-subject similarity analysis. However, none of the traditional graph embedding methods are able to capture such special structure and jointly leverage the learned node representations for similarity learning on brain networks. In , a higher-order GNN model is developed to encode the community-structure of brain networks during the representation learning and leverage it for the similarity learning task on these brain networks. Some more examples from other domains include the GNN-based graph similarity predictive models introduced for chemical compound queries in computational chemistry , and the deep graph matching networks proposed for binary function similarity search and malware detection in computer security .}\nIn this survey paper, we provide a systematic review of the existing work in deep graph similarity learning. Based on the different graph representation learning strategies and how they are leveraged for the deep graph similarity learning task, we propose to categorize deep graph similarity learning models into three groups: Graph Embedding based-methods, GNN-based methods, and Deep Graph Kernel-based methods. Additionally, we sub-categorize the models based on their properties. Table ~\\ref{tab:taxonomy} shows our proposed taxonomy, with some example models for each category as well as the relevant applications. In this survey, we will illustrate how these different categories of models approach the graph similarity learning problem. We will also discuss the loss functions used for the graph similarity learning task.", "cites": [8334, 553, 1655], "cite_extract_rate": 0.11538461538461539, "origin_cites_number": 26, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple cited papers to provide a coherent narrative on the evolution of graph similarity learning, from traditional methods to modern deep learning approaches. It critically evaluates limitations of early methods like Graph Edit Distance and Weisfeiler-Lehman, noting their computational intractability and lack of adaptability. It abstracts by contrasting GNNs with traditional graph embedding and kernel-based approaches, highlighting how deep learning enables end-to-end and domain-adaptive solutions. However, it could offer deeper comparative analysis and more nuanced critiques of individual methods for higher insight."}}
{"id": "428afaa2-5dec-4cec-9bf9-941903e0e928", "title": "Scope and Contributions.", "level": "paragraph", "subsections": ["fb6c195d-53f3-42b7-b605-adcf2d7b16d0"], "parent_id": "8601ef96-9801-4983-92f1-2af4dfd88e00", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Introduction"], ["paragraph", "Scope and Contributions."]], "content": "This paper is focused on surveying the recently emerged deep models for graph similarity learning, where the goal is to use deep strategies on graphs for learning the similarity of given pairs of graphs, instead of computing similarity scores based on predefined measures. We emphasize that this paper does not attempt to survey the extensive literature on graph representation learning, graph neural networks, and graph embedding. Prior work has focused on these topics (see~ for examples). Here instead, we focus on deep graph representation learning methods that explicitly focus on modeling graph similarity. To the best of our knowledge, this is the first survey paper on this problem. We summarize the main contributions of this paper as follows:\n\\begin{itemize}\n    \\item[$\\circ$] Two comprehensive taxonomies to categorize the literature of the emerging field of deep graph similarity learning, based on the type of models and the type of features adopted by the existing methods, respectively. \n    \\item[$\\circ$] Summary and discussion of the key techniques and building blocks of the models in each category.\n    \\item[$\\circ$] Summary and comparison of the different deep graph similarity learning models across the taxonomy.\n    \\item[$\\circ$] Summary and discussion of the real-world applications that can benefit from deep graph similarity learning in a variety of domains. \n    \\item[$\\circ$] Summary and discussion of the major challenges for deep graph similarity learning, the future directions, and the open problems.  \n\\end{itemize}", "cites": [553, 217, 215, 212, 219, 7006], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the scope and contributions of the survey without synthesizing or connecting ideas from the cited papers. It mentions the existence of prior surveys on related topics but does not integrate or contrast their content in depth. There is minimal critical analysis or abstraction to broader principles, with the focus remaining on outlining what this paper will cover."}}
{"id": "5d691490-7dcb-48e4-a01b-973641b02423", "title": "Notation and Preliminaries", "level": "section", "subsections": [], "parent_id": "eab36b40-c6ea-4c4a-96e7-a440a76d2534", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Notation and Preliminaries"]], "content": "\\label{sec:prelims}\nIn this section, we provide the necessary notation and definitions of the fundamental concepts pertaining to the graph similarity problem that will be used throughout this survey.  The notation is summarized in Table~\\ref{tab:notation}.\n\\begin{table*}\n\\begin{center}\n\\caption{Summary of Notation}\n\\label{tab:notation}\n\\begin{tabular}{lllll}\n\\toprule\n$G$ & & & & Input graph\\\\\n$V$& & & & The set of nodes in a graph $G$\\\\\n$E$& & & & The set of edges in a graph $G$\\\\\n$a,\\mathbf{a},\\mathbf{A}$& & & & Scalar, vector, matrix\\\\\n$\\mathcal{G}$ & & & & Graph set $\\mathcal{G} = \\{G_1,G_2,\\cdots ,G_n\\}$\\\\\n$\\mathcal{M}$ & & & & Similarity function\\\\\n$s_{ij}$ & & & & Similarity score between two graphs $G_i , G_j \\in \\mathcal{G}$\\\\\n$\\mathbb{R}^{m\\times m}$& & & & $m-$dimensional Euclidean space\\\\\n$\\mathbf{I}_m$& & & & Identity matrix of dimension $m$ \\\\\n$\\mathbf{A}^T$& & & & Matrix transpose\\\\\n$\\mathbf{L}$& & & & Laplacian matrix\\\\\n$g_\\theta*\\mathbf{x}$& & & & Convolution of $g_\\theta$ and $\\mathbf{x}$ \\\\\n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\vspace{-3mm}\n\\end{table*}\nLet $G = (V,E,\\mathbf{A})$ denote a graph, where $V$ is the set of nodes, $E \\subseteq V \\times V$ is the set of edges, and $\\mathbf{A} \\in \\mathbb{R}^{|V| \\times |V|}$ is the adjacency matrix of the graph. This is a general notation for graphs that covers different types of graphs, including unweighted/weighted graphs, undirected/directed graphs, and attributed/non-attributed graphs.\nWe are also assuming a set of graphs as input, $\\mathcal{G} = \\{G_1, G_2, \\dots, G_n\\}$, and the goal is measure/model their pairwise similarity. This relates to the classical problem of graph isomorphism and its variants. In graph isomorphism~, two graphs $G = (V_G,E_G)$ and $H = (V_H,E_H)$ are isomorphic (i.e., $G \\cong H$), if there is a \\gm{mapping} function $\\pi: V_G \\rightarrow V_H$, such that $(u,v) \\in E_G$ iff $(\\pi(u),\\pi(v)) \\in E_H$. The graph isomorphism is an NP problem, and no efficient algorithms are known for it. Subgraph isomorphism is a generalization of the graph isomorphism problem. In subgraph isomorphism, the goal is to answer for two input graphs $G$ and $H$, if there is a subgraph of $G$ ($G' \\subset G$) such that $G'$ is isomorphic to $H$ (i.e., $G' \\cong H$). This is suitable in a setting in which the two graphs have different sizes. The subgraph isomorphism problem has been proven to be NP-complete (unlike the graph isomorphism problem)~. The maximum common subgraph problem is another less-restrictive measure of graph similarity, in which the similarity between two graphs is defined based on the size of the largest common subgraph in the two input graphs. However, this problem is also NP-complete~.\n\\begin{mydef}[\\textbf{Graph Similarity Learning}]\\label{def:gsim}\nLet $\\mathcal{G}$ be an input set of graphs, $\\mathcal{G} = \\{G_1,G_2,\\cdots ,G_n\\}$ where $G_i=(V_i, E_i, \\mathbf{A}_i)$. Let $\\mathcal{M}$ denote a learnable similarity function, such that $\\mathcal{M}: (G_i, G_j) \\rightarrow \\mathbb{R}$, for any pair of graphs $G_i, G_j \\in \\mathcal{G}$. Assume $s_{ij} \\in \\mathbb{R}$ denote the similarity score computed using $\\mathcal{M}$ between pairs $G_i$ and $G_j$. Then $\\mathcal{M}$ is symmetric if and only if $s_{ij} = s_{ji}$ for any pair of graphs $G_i, G_j \\in \\mathcal{G}$. $\\mathcal{M}$ should satisfy the property that: $s_{ii} >= s_{ij}$ for any pair of graphs $G_i, G_j \\in \\mathcal{G}$. And, $s_{ij}$ is minimum if $G_i$ is the complement of $G_j$, i.e, $G_i = \\Bar{G_j}$, for any graph $G_j \\in \\mathcal{G}$.\n\\end{mydef}\nClearly, graph isomorphism and its related variants (e.g., subgraph isomorphism, maximum common subgraphs, etc.) are focused on measuring the topological equivalence of graphs, which gives rise to a binary similarity measure that outputs $1$ if two graphs are isomorphic and $0$ otherwise. While these methods may sound intuitive, they are actually more restrictive and difficult to compute for large graphs. Here instead, we focus on a relaxed notion of graph similarity that can be measured using machine learning models, where the goal is to learn a model that quantifies the degree of structural similarity and relatedness between two graphs. This is slightly similar to the work done on modeling the structural similarity between nodes in the same graph~. We formally state the definition of graph similarity learning (GSL) in Definition~\\ref{def:gsim}. Note that in the case of deep graph similarity learning, the similarity function $\\mathcal{M}$ is a neural network model that can be trained in an end-to-end fashion.", "cites": [3946, 7762], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides basic notation and definitions for graph similarity learning and briefly mentions related concepts like graph isomorphism and node-based structural similarity. It includes one definition and two cited papers, but does not effectively synthesize or integrate their contributions. There is minimal critical analysis and only rudimentary abstraction, focusing more on foundational concepts than on broader patterns or frameworks."}}
{"id": "cb42bc90-86fa-496b-a3b1-01595eb9c979", "title": "Taxonomy of Models", "level": "section", "subsections": ["ba3e247a-0cb8-4f3f-943b-1ad98253c5f9", "e0dde34c-15b4-4513-8a47-b5a6028c9985", "11604b65-df2b-41c5-b0f9-622d1e43f96d"], "parent_id": "eab36b40-c6ea-4c4a-96e7-a440a76d2534", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Taxonomy of Models"]], "content": "\\begin{figure}[t]\n\\centering\n    \\begin{subfigure}[ ]{\t\t\\includegraphics[width=.47\\linewidth] {Model_taxonomy_new.png}\n\t\t    \\label{fig:model_taxonomy}\n\t\t}\n\t\t\\end{subfigure}\n\t\t\\begin{subfigure}[ ]{\n\t\t\\includegraphics[width=.43\\linewidth]{Feature_taxonomy.png}\n\t\t\\label{fig:feature_taxonomy}\n\t\t}\n\t\t\\end{subfigure}  \n  \\caption{Proposed taxonomy for categorizing the literature of deep graph similarity learning based on (a) Model architecture, (b) Type of features.}\n  \\label{fig:taxonomy}\n\\end{figure}\n\\setlength{\\tabcolsep}{5.2pt} \n\\renewcommand{\\arraystretch}{1.3}\n{\n\\begin{table*}[]\n\\begin{center}\n\\caption{A Taxonomy of Deep Graph Similarity Learning Methods}\n\\label{tab:taxonomy}\n\\scalebox{0.8}{\n\\small\n\\begin{tabular}{c|l|l|c|c|c|c|c|l}\n\\toprule\n\\multicolumn{2}{c|}{\\textbf{Category}}                                                                                                                                                           & \\begin{tabular}[c]{@{}c@{}c@{}} \\textbf{Methods}\\end{tabular}               & \\begin{tabular}[c]{@{}c@{}c@{}}\\rotatebox{90}{Weighted graphs} \\end{tabular} & \\begin{tabular}[c]{@{}c@{}c@{}}\\rotatebox{90}{Heterogeneous graphs\\;\\;}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}c@{}}\\rotatebox{90}{Attributed graphs}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}c@{}}\\rotatebox{90}{Feature propagation} \\end{tabular} & \\begin{tabular}[c]{@{}c@{}c@{}} \\rotatebox{90}{Cross-graph interaction}\\end{tabular} & \\multicolumn{1}{c}{\\textbf{Applications}}                                                                        \\\\ \\toprule\n\\multirow{5}{*}{\\begin{tabular}[c]{@{}c@{}}Graph Embedding \\\\ based GSL\\end{tabular}}   & \\multirow{3}{*}{\\begin{tabular}[c]{@{}l@{}}Node-level Embedding\\end{tabular}} &          & \\xmark                                                         & \\xmark                                                             & \\cmark                                                         & \\xmark                                                             & \\xmark                                                              & \\begin{tabular}[c]{@{}l@{}}Social Network Analysis\\\\ Bioinformatics \\end{tabular}                        \\\\ \\cline{3-9} \n                                                                                        &                                                                                                &  & \\xmark                                                         & \\cmark                                                            & \\xmark                                                          & \\xmark                                                             & \\xmark                                                              & Chemoinformatics                                                                                      \\\\ \\cline{2-9} \n                                                                                        & \\begin{tabular}[c]{@{}l@{}}Graph-level Embedding\\end{tabular}               &   & \\xmark                                                         & \\cmark                                                            & \\xmark                                                          & \\xmark                                                             & \\xmark                                                              & \\begin{tabular}[c]{@{}l@{}}Chemoinformatics\\\\ Bioinformatics \\end{tabular}                        \\\\ \\cline{3-9} \n                                                                                        &                                                                                                &   & \\xmark                                                         & \\cmark                                                            & \\xmark                                                          & \\xmark                                                             & \\xmark                                                              & \\begin{tabular}[c]{@{}l@{}}Social Network Analysis\\\\ Chemoinformatics \\end{tabular}                        \\\\ \\cline{3-9} \n                                                                                        &                                                                                                &  & \\xmark                                                         & \\xmark                                                            & \\cmark                                                          & \\xmark                                                             & \\xmark                                                              & Binary Code Similarity                                                                                      \\\\ \\cline{3-9} \n                                                                                        &                                                                                                &  & \\xmark                                                         & \\xmark                                                            & \\cmark                                                          & \\xmark                                                             & \\xmark                                                              & Chemoinformatics                                                                                      \\\\ \\hline\n\\multirow{7}{*}{GNN-based GSL}                                                          & \\multirow{2}{*}{GNN-CNN Models}                                                          &     & \\xmark                                                         & \\cmark                                                            & \\cmark                                                         & \\cmark                                                            & \\xmark                                                              & Chemoinformatics                                                                                      \\\\ \\cline{3-9} \n                                                                                        &                                                                                                &            & \\xmark                                                         & \\cmark                                                            & \\cmark                                                         & \\cmark                                                            & \\xmark                                                              & Chemoinformatics                                                                                      \\\\ \\cline{2-9} \n                                                                                        & \\multirow{2}{*}{Siamese GNNs}                                                                  &          & \\cmark                                                        & \\xmark                                                             & \\cmark                                                         & \\cmark                                                            & \\xmark                                                              & Brain Network Analysis                                                                                    \\\\ \\cline{3-9} \n                                                                                        &                                                                                                &                             &    \\xmark                                                        & \\cmark                                                               & \\cmark                                                            & \\cmark                                                               & \\xmark                                                                & Malware Detection                                                                                    \\\\ \\cline{3-9} \n                                                                                        &                                                                                                &                             &    \\cmark                                                        & \\xmark                                                               & \\cmark                                                            & \\cmark                                                               & \\xmark                                                                & Image Retrieval                                                                                                                                                                 \\\\ \\cline{2-9} \n                                                                                        & \\multirow{2}{*}{\\begin{tabular}[c]{@{}l@{}}GNN-based Graph\\\\ Matching Networks\\end{tabular}}   &              & \\xmark                                                         & \\xmark                                                             & \\cmark                                                         & \\cmark                                                            & \\cmark                                                             & Binary Code Similarity                                                                                        \\\\ \\cline{3-9} \n                                                                                        &                                                                                                &              & \\xmark                                                         & \\cmark                                                             & \\cmark                                                         & \\cmark                                                            & \\cmark                                                             & Chemoinformatics                                                                                        \\\\ \\cline{3-9} \n                                                                                        &                                                                                                &              & \\xmark                                                         & \\xmark                                                             & \\cmark                                                         & \\cmark                                                            & \\cmark                                                             & Image Matching                                                                                        \\\\ \\cline{3-9} \n                                                                                        &                                                                                                &            & \\cmark                                                        & \\cmark                                                            & \\cmark                                                         & \\cmark                                                            & \\xmark                                                              & 3D Action Recognition                                                                                     \\\\ \\hline\n\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Deep Graph Kernel \\\\ based GSL\\end{tabular}} & \\begin{tabular}[c]{@{}l@{}}Sub-structure based \\\\ Deep Kernels\\end{tabular}                    &         & \\xmark                                                         & \\cmark                                                            & \\xmark                                                          & \\xmark                                                             & \\xmark                                                              & \\begin{tabular}[c]{@{}l@{}}Chemoinformatics\\\\ Bioinformatics \\\\ Social Network Analysis\\end{tabular} \\\\ \\cline{2-9} \n                                                                                        & \\begin{tabular}[c]{@{}l@{}}Deep Neural Network \\\\ based Kernels\\end{tabular}                   &               & \\xmark                                                         & \\cmark                                                            & \\cmark                                                         & \\xmark                                                             & \\cmark                                                             & \\begin{tabular}[c]{@{}l@{}}Chemoinformatics\\\\ Bioinformatics \\end{tabular}                           \\\\ \\cline{3-9} \n                                                                                        &                                                                                                &   & \\xmark                                                         & \\cmark                                                            & \\cmark                                                          & \\cmark                                                             & \\xmark                                                              & \\begin{tabular}[c]{@{}l@{}}Social Network Analysis\\\\ Chemoinformatics \\end{tabular}                        \\\\ \n\\bottomrule\n\\end{tabular}\n}\n\\end{center}\n\\vspace{-4mm}\n\\end{table*}\n}\n\\gm{In this section, we describe the taxonomy for the literature of deep graph similarity learning. As shown in Fig. ~\\ref{fig:taxonomy}, we propose two intuitive taxonomies for categorizing the various deep graph similarity learning methods based on the model architecture and the type of features used in these methods.} \n\\gm{First, we start by discussing the categorization based on which model architecture has been used. There are three main categories of deep graph similarity learning methods (see Fig.~\\ref{fig:model_taxonomy}): (1) graph embedding based methods, which apply graph embedding techniques to obtain node-level or graph-level representations and further use the representations for similarity learning ; (2) graph neural network (GNN) based models, which are based on using GNNs for similarity learning, including GNN-CNNs , Siamese GNNs  and GNN-based graph matching networks ; and (3) deep graph kernels that first map graphs into a new feature space, where kernel functions are defined for similarity learning on graph pairs, including sub-structure based deep kernels  and deep neural network based kernels . In the meantime, different methods may use different types of features in the learning process.}\n\\gm{Second, we discuss the categorization of methods based on the type of features used in them. Existing GSL approaches can be generally grouped into two categories (see Fig.~\\ref{fig:feature_taxonomy}): (1) methods that uses single-graph features\n~;\n(2) methods that uses cross-graph features for similarity learning~. The main difference between these two categories of methods is that for methods using single-graph features, the representation of each graph is learned individually,  while those methods that use cross-graph features allow graphs to learn and propagate features from each other and the cross-graph interaction is leveraged for pairs of graphs. The single-graph features mainly includes graph embeddings at different granularity (i.e.,node-level, graph-level, and subgraph-level), while the cross-graph features includes the cross-graph node-level features and cross-graph graph-level features, which are usually obtained by node-level attention and graph-level attention across the two graphs in each pair.}\nNext, we detail the description of the methods based on the taxonomy in Figures~\\ref{fig:model_taxonomy} and~\\ref{fig:feature_taxonomy}. We summarize the general characteristics and applications of all the methods in Table~\\ref{tab:taxonomy}, including the type of graphs they are developed for, the type of features, and the domains/applications where they could be applied. We describe these methods in the following order:\n\\begin{enumerate}\n    \\item Graph embedding based GSL\n    \\item Graph Neural Network based GSL\n    \\item Deep graph kernel based GSL\n\\end{enumerate}\n\\noindent", "cites": [3952, 3954, 3950, 3949, 3953, 3951, 1657, 3947, 3955, 1655, 3948], "cite_extract_rate": 0.4583333333333333, "origin_cites_number": 24, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a taxonomy of deep graph similarity learning methods, organizing them by model architecture and feature types, and references multiple papers. However, it lacks synthesis of the cited works into a coherent narrative, as the content appears to be primarily structured around categories without connecting underlying themes or innovations across papers. There is minimal critical analysis or abstraction to higher-level insights or principles."}}
{"id": "ba3e247a-0cb8-4f3f-943b-1ad98253c5f9", "title": "Graph Embedding based Graph Similarity Learning", "level": "subsection", "subsections": ["c280caa5-1368-4418-ae59-b2d6c2610207", "0afdb599-02d8-40c1-8ef2-44d0d29b591c"], "parent_id": "cb42bc90-86fa-496b-a3b1-01595eb9c979", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Taxonomy of Models"], ["subsection", "Graph Embedding based Graph Similarity Learning"]], "content": "Graph embedding has received considerable attention in the past decade , and a variety of deep graph embedding models have been proposed in recent years , \\gm{for example the popular \\textit{DeepWalk} model proposed in  and the \\textit{node2vec} model from }. Similarity learning methods based on graph embedding seek to utilize node-level or graph-level representations learned by these graph embedding techniques for defining similarity functions or predicting similarity scores . Given a collection of graphs, these works first aim to convert each graph $G$ into a $d-$dimensional space $(d\\ll\\|V\\|)$, where the graph is represented as either a set of $d-$dimensional vectors with each vector representing the embedding of one node (i.e.,node-level embedding) or a $d-$dimensional vector for the whole graph as the graph-level embedding . The graph embeddings are usually learned in an unsupervised manner in a separate stage prior to the similarity learning stage, where the graph embeddings obtained are used for estimating or predicting the similarity score between each pair of graphs.", "cites": [7496, 217, 3949, 3956, 218, 215, 212, 1010, 3953], "cite_extract_rate": 0.9, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of graph embedding-based similarity learning, mentioning several key papers and concepts like DeepWalk, node2vec, and graph2vec. However, it does not meaningfully synthesize ideas across the cited works, lacks critical evaluation of their strengths or limitations, and offers minimal abstraction or identification of broader patterns in the field."}}
{"id": "a699903e-3d7e-4310-8565-f2e6aaf1b1ca", "title": "node2vec-PCA", "level": "paragraph", "subsections": [], "parent_id": "c280caa5-1368-4418-ae59-b2d6c2610207", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Taxonomy of Models"], ["subsection", "Graph Embedding based Graph Similarity Learning"], ["subsubsection", "Node-level Embedding based Methods"], ["paragraph", "node2vec-PCA"]], "content": ".} In , the node2vec approach  is employed for obtaining the node-level embeddings of graphs. To make the embeddings of all the graphs in the given collection comparable, they apply the principal component analysis (PCA) on the embeddings to retain the first $d \\ll D$ principal components (where $D$ is the dimensionality of the original node embedding space). Afterwards, \\gm{the embedding matrix of each graph is split into $d/2$ 2D slices. Suppose there are $n$ nodes in each graph $G$ and the embedding matrix for graph $G$ is $F \\in \\mathbb{R}^{n\\times d}$, then $d/2$ 2D slices each with $\\mathbb{R}^{n\\times 2}$ will be obtained, which are viewed as $d/2$ \\textit{channels}. Then each 2D slice from the embedding space is turned into regular grids by discretizing them into a fixed number of equallly-sized bins, where the value associate with each bin is the count of the number of nodes falling into that bin. These bins can be viewed as \\textit{pixels}.} Then, the graph is represented as a stack of 2D histograms of its node embeddings. The graphs are then compared in the grid space and input into a 2D CNN as multi-channel image-like structures for a graph classification task.", "cites": [3949, 1010], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes the node2vec-PCA method by outlining its process, from embedding generation to transformation into 2D histograms for CNN-based graph classification. It integrates the two cited papers but in a surface-level manner, without deeper synthesis of their contributions or broader context. There is no critical evaluation of the methodâ€™s strengths or weaknesses, and it lacks abstraction or generalization to wider trends in deep graph similarity learning."}}
{"id": "b3deec59-6b5c-4c72-832a-6eed4d6a34c4", "title": "(1) graph2vec.", "level": "paragraph", "subsections": [], "parent_id": "0afdb599-02d8-40c1-8ef2-44d0d29b591c", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Taxonomy of Models"], ["subsection", "Graph Embedding based Graph Similarity Learning"], ["subsubsection", "Graph-level Embedding based Methods"], ["paragraph", "(1) graph2vec."]], "content": "} In , a graph2vec was proposed to learn distributed representations of graphs, similar to Doc2vec~ in natural language processing. In graph2vec each graph is viewed as a document and the rooted subgraphs around every node in the graph are viewed as words that compose the document. \\gm{There are two main components in this method: first, a procedure to extract rooted subgraphs around every node in a given graph following the Weisfeiler-Lehman relabeling process and second, the procedure to learn embeddings of the given graphs by skip-gram with negative sampling. The Weisfeiler-Lehman relabeling algorithm takes the root node of the given graph and degree of the intended subgraph $d$ as inputs, and returns the intended subgraph.}\n\\gm{In the negative sampling phase, given a graph and a set of rooted subgraphs in its context, a set of randomly chosen subgraphs are selected as negative samples and only the embeddings of the negative samples are updated in the training.} After the graph embedding is obtained for each graph, the similarity or distance between graphs are computed in the embedding space for downstream prediction tasks (e.g., graph classification, clustering, etc.).", "cites": [7265, 3953], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the graph2vec method in a factual manner, drawing from its own abstract and the cited paper. It makes a basic connection to the concept of Doc2vec but does not synthesize broader ideas or place graph2vec in a larger context of graph representation learning. There is little critical evaluation or abstraction to identify general principles or trends."}}
{"id": "12a5d978-9094-4de6-a31d-32e2784d72a1", "title": "(2) Neural Networks with Structure2vec.", "level": "paragraph", "subsections": [], "parent_id": "0afdb599-02d8-40c1-8ef2-44d0d29b591c", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Taxonomy of Models"], ["subsection", "Graph Embedding based Graph Similarity Learning"], ["subsubsection", "Graph-level Embedding based Methods"], ["paragraph", "(2) Neural Networks with Structure2vec."]], "content": "} In , a deep graph embedding approach is proposed for cross-platform binary code similarity detection. A Siamese architecture is applied to enable the pair-wise similarity learning, and the graph embedding network based on Structure2vec  is used for learning graph representations in the twin networks, which share weights with each other. \\gm{The Structure2vec is a neural network approach inspired by graphical model inference algorithms where node-specific features are aggregated recursively according to graph topology. After a few steps of recursion, the network will produce a new feature representation for each node which considers both graph characteristics and long-range interaction between node features.} Given is a set of $K$ pairs of graphs $<G_i, {G_i}^\\prime>$, with ground truth pair label $y_i \\in \\{+1,-1\\}$, where $y_i = +1$ indicates that $G_i$ and ${G_i}^\\prime$ are similar, and $y_i = -1$ indicates they are dissimilar. With the Structure2vec embedding output for $G_i$ and ${G_i}^\\prime$, represented as $\\mathbf{f}_i$ and ${\\mathbf{f}_i}^\\prime$ respectively, they define the Siamese network output for each pair as \n\\begin{align}\n    Sim(G_i,{G_i}^\\prime) = \\cos(\\mathbf{f}_i,{\\mathbf{f}_i}^\\prime) = \\frac{\\langle \\mathbf{f}_i,{\\mathbf{f}_i}^\\prime \\rangle}{\\|\\mathbf{f}_i\\| \\cdot \\| {\\mathbf{f}_i}^\\prime \\|}\n\\end{align}\nand the following loss function is used for training the model. \n\\begin{align}\n    L = \\sum_{i=1}^{K} (Sim(G_i,{G_i}^\\prime) - y_i)^2\n    \\label{eq:loss1}\n\\end{align}", "cites": [3947, 566], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section describes a method using Structure2vec within a Siamese network for graph similarity learning, primarily summarizing the architecture and loss function from the cited paper. While it connects the use of Structure2vec with the Siamese framework, it lacks critical analysis of the method's strengths or weaknesses and does not abstract broader patterns or principles from the literature."}}
{"id": "7c4cd430-4e6c-4426-ac9f-b5240a8e28ab", "title": "(5) DGCNN: Disordered Graph CNN", "level": "paragraph", "subsections": [], "parent_id": "0afdb599-02d8-40c1-8ef2-44d0d29b591c", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Taxonomy of Models"], ["subsection", "Graph Embedding based Graph Similarity Learning"], ["subsubsection", "Graph-level Embedding based Methods"], ["paragraph", "(5) DGCNN: Disordered Graph CNN"]], "content": ".} In , another graph-level representation learning approach called DGCNN is introduced based on graph CNN and mixed Gaussian model, where a set of key nodes are selected from each graph. Specifically, to ensure the number of neighborhoods of the nodes in each graph is consistent, the same number of key nodes are sampled for each graph in a key node selection stage. Then a convolution operation is performed over the kernel parameter matrix and the nodes in the neighborhood of the selected key nodes, after which the graph CNN takes the output of the convolutional layer as the input data of the overall connection layer. Finally, the output of the dense hidden layer is used as the feature vector for each graph in the graph similarity retrieval task.", "cites": [3948], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of the DGCNN method and its general workflow, but it lacks synthesis with other methods or deeper analysis. It does not critically evaluate the approach, highlight its limitations, or connect it to broader trends in graph similarity learning. The content remains largely confined to summarizing the method from the cited paper."}}
{"id": "803c5b75-864f-463b-9de9-b153341db878", "title": "(6) N-Gram Graph Embedding.", "level": "paragraph", "subsections": [], "parent_id": "0afdb599-02d8-40c1-8ef2-44d0d29b591c", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Taxonomy of Models"], ["subsection", "Graph Embedding based Graph Similarity Learning"], ["subsubsection", "Graph-level Embedding based Methods"], ["paragraph", "(6) N-Gram Graph Embedding."]], "content": "} In , an unsupervised graph representation based method called $N$-gram is proposed for similarity learning on molecule graphs. It first views each node in the graph as one token and applies an analog of the CBOW (continuous bag of words)  strategy and trains a neural network to learn the node embeddings for each graph. Then it enumerates the walks of length $n$ in each graph, where each walk is called an $n$-gram, and obtains the embedding for each $n$-gram by assembling the embeddings of the nodes in the $n$-gram using element-wise product. \\gm{The embedding for the n-gram walk set is defined as the sum of the embeddings for all n-grams. The final n-gram graph-level representation up to lenght $T$ is then constructed by concatenating the embeddings of all the $n$-gram sets for $n\\in \\{1,2,\\cdots,T\\}$ in the graph.} Finally, the graph-level embeddings are used for the similarity prediction or graph classification task for molecule analysis.     \n\\gm{By summarizing the embedding based methods, we find the main advantage of these methods is their speed and scalability, due to the fact that the graph representations learned through these factorized models are developed on each single graph where there is no feature interactions across graphs. This property makes these methods a great option for graph similarity learning applications such as graph retrieval, where similarity search becomes a nearest neighbor search in a database of the precomputed graph representations by these factorized methods.} Moreover, these embedding based methods provide a variety of perspectives and strategies for learning representations from graphs and demonstrate that these representations can be used for graph similarity learning. However, there are also shortcomings in these solutions, a common one being that the embeddings are learned independently on the individual graphs in a separate stage from the similarity learning, therefore the graph-graph proximity is not considered or utilized in the graph representation learning process, and the representations learned by these models may not be suitable for graph-graph similarity prediction compared to the methods that integrate the similarity learning with the graph representation learning in an end-to-end framework.", "cites": [3950, 1684], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes information from the N-Gram Graph paper and connects it to the broader context of word embedding strategies from the Skip-gram paper. It critically evaluates the method's advantages, such as speed and scalability, as well as its limitations, particularly the lack of graph-graph interaction during embedding learning. The discussion abstracts to highlight general patterns in embedding-based methods for graph similarity learning, offering a nuanced comparison with end-to-end approaches."}}
{"id": "a477d6ac-ba5b-414f-b6a8-2279f8ee5a9c", "title": "GNN Preliminaries", "level": "paragraph", "subsections": [], "parent_id": "e0dde34c-15b4-4513-8a47-b5a6028c9985", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Taxonomy of Models"], ["subsection", "GNN-based Graph Similarity Learning"], ["paragraph", "GNN Preliminaries"]], "content": ".} Graph neural networks (GNNs) were first formulated in , which proposed to use a propagation process to learn node representations for graphs. It has then been further extended by . Later, graph convolutional networks were proposed which compute node updates by aggregating information in local neighborhoods , and they have become the most popular graph neural networks, which are widely used and extended for graph representation learning in various domains . \nWith the development of graph neural networks, researchers began to build graph similarity learning models based on GNNs. In this section, we will first introduce the workflow of GCNs with the spectral GCN  as an example, and then describe the GNN-based graph similarity learning methods covering three main categories.\nGiven a graph $G=(V, E, \\mathbf{A})$, where $V$ is the set of vertices, $E \\subset V \\times V $ is the set of edges, and  $\\mathbf{A} \\in \\mathbb{R}^{m \\times m}$ is the adjacency matrix, the diagonal degree matrix $\\mathbf{D}$ will have elements $\\mathbf{D}_{ii} = \\sum_j \\mathbf{A}_{ij}$. The graph Laplacian matrix is $\\mathbf{L} = \\mathbf{D} - \\mathbf{A}$, which can be normalized as $\\mathbf{L} = \\mathbf{I}_m - \\mathbf{D}^{-\\frac{1}{2}}\\mathbf{A} \\mathbf{D}^{-\\frac{1}{2}}$, where $\\mathbf{I}_m$ is the identity matrix. Assume the orthonormal eigenvectors of $\\mathbf{L}$ are represented as $\\{u_l\\}_{l=0}^{m-1}\\in \\mathbb{R}^{m \\times m}$, and their associated eigenvalues are $\\{\\lambda_l\\}_{l=0}^{m-1}$, the Laplacian is diagonalized by the Fourier basis $[u_0, \\cdots,u_{m-1}](=\\mathbf{U})\\in \\mathbb{R}^{m \\times m}$ and $\\mathbf{L} = \\mathbf{U\\Lambda U^T}$ where $\\mathbf{\\Lambda} = diag([\\lambda_0,\\cdots,\\lambda_{m-1}])\\in \\mathbb{R}^{m\\times m}$. The graph Fourier transform of a signal $x\\in \\mathbb{R}^m$ can then be defined as $\\hat{x} = \\mathbf{U^T}x \\in \\mathbb{R}^m$. Suppose a signal vector $\\mathbf{x} : V \\rightarrow \\mathbb{R}$ is defined on the nodes of graph $G$, where $\\mathbf{x}_i$ is the value of $\\mathbf{x}$ at the $i^{th}$ node. Then the signal $\\mathbf{x}$ can be filtered by $g_\\theta$ as\n\\vspace{-2mm}\n\\begin{align}\n    y = g_\\theta*\\mathbf{x} = g_\\theta(\\mathbf{L})\\mathbf{x} = g_\\theta(\\mathbf{U{\\Lambda}U^T})\\mathbf{x} = \\mathbf{U}g_\\theta(\\Lambda)\\mathbf{U^T}\\mathbf{x}\n\\label{eq:filter_signal}\n\\end{align}\nwhere the filter $g_\\theta(\\Lambda)$ can be defined as $g_{\\theta}(\\Lambda) = \\sum_{k=0}^{K-1}{\\theta_k}{\\Lambda^k}$, and the parameter $\\theta\\in {\\mathbb{R}}^K$ is a vector of polynomial coefficients . GCNs can be constructed by stacking multiple convolutional layers in the form of Equation (\\ref{eq:filter_signal}), with a non-linearity activation (ReLU) following each layer.\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width = 0.95 \\columnwidth]{Figure1_updated.png}\n    \\caption{Illustration of GNN-based Graph Similarity Learning.}\n    \\label{fig:gsl}\n\\end{figure}\nBased on how graph-graph similarity/proximity is leveraged in the learning, we summarize the existing GNN-based graph similarity learning work into three main categories: 1) GNN-CNN mixed models for graph similarity prediction, 2) Siamese GNNs for graph similarity prediction, and 3) GNN-based graph matching networks.", "cites": [228, 169, 3956, 3957, 8313, 213], "cite_extract_rate": 0.5, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic descriptive overview of GNNs and their application to graph similarity learning. It synthesizes foundational concepts from several papers but lacks deeper integration or comparison of their contributions. Critical analysis and abstraction are minimal, as the section primarily focuses on defining GNNs and categorizing existing methods without evaluating limitations or broader trends."}}
{"id": "ba55f936-a16b-4815-8fdd-4616f3674bcf", "title": "(1) GSimCNN.", "level": "paragraph", "subsections": [], "parent_id": "010c38e9-0287-4ce6-90a6-86435322d428", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Taxonomy of Models"], ["subsection", "GNN-based Graph Similarity Learning"], ["subsubsection", "GNN-CNN Models for Graph Similarity Prediction"], ["paragraph", "(1) GSimCNN."]], "content": "} In , a method called GSimCNN is proposed for pairwise graph similarity prediction, which consists of three stages. In Stage 1, node representations are first generated by multi-layer GCNs, where each layer is defined as \n\\begin{align}\n    conv(\\mathbf{x}_i) = ReLU(\\sum_{j \\in N(i)}\\frac{1}{\\sqrt{d_id_j}}\\mathbf{x}_j\\mathbf{W}^{(l)} + \\mathbf{b}^{(l)})\n\\end{align}\n\\noindent where $N(i)$ is the set of first-order neighbors of node $i$ plus node $i$ itself, $d_i$ is the degree of node $i$ plus $1$, $\\mathbf{W}^{(l)}$ is the weight matrix for the $l-$th GCN layer, $\\mathbf{b}^{(l)}$ is the bias, and $ReLU(x) = max(0,x)$ is the activation function. In Stage 2, the inner products between all possible pairs of node embeddings between two graphs from different GCN layers are calculated, which results in multiple similarity matrices. Finally, the similarity matrices from different layers are processed by multiple independent CNNs, where the output of the CNNs are concatenated and fed into fully connected layers for predicting the final similarity score $s_{ij}$ for each pair of graphs $G_i$ and $G_j$.", "cites": [3955], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of the GSimCNN method, outlining its three stages and equations, but does not integrate or connect this approach with other methods or broader themes in deep graph similarity learning. It lacks critical evaluation or discussion of the model's strengths, weaknesses, or limitations. Additionally, there is no abstraction or generalization of principles across works, keeping the narrative concrete and limited to a single paper."}}
{"id": "72b4bf7d-2ac3-4b19-9802-34aeff325e8d", "title": "(2) SimGNN.", "level": "paragraph", "subsections": [], "parent_id": "010c38e9-0287-4ce6-90a6-86435322d428", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Taxonomy of Models"], ["subsection", "GNN-based Graph Similarity Learning"], ["subsubsection", "GNN-CNN Models for Graph Similarity Prediction"], ["paragraph", "(2) SimGNN."]], "content": "}In , a SimGNN model is introduced based on the GSimCNN from . In addition to pairwise node comparison with node-level embeddings from the GCN output, neural tensor networks (NTN)  are utilized to model the relation between the graph-level embeddings of two input graphs, whereas the graph embedding for each graph is generated via a weighted sum of node embeddings, and a global context-aware attention is applied on each node, such that nodes similar to the global context receive higher attention weights. Finally, both the comparison between node-level embeddings and graph-level embeddings are considered for the similarity score prediction in the CNN fully connected layers.", "cites": [3955], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of the SimGNN model, referencing its relationship to GSimCNN and outlining its components such as NTN, attention mechanisms, and CNN layers for similarity prediction. However, it lacks critical evaluation of the modelâ€™s strengths or weaknesses and does not synthesize insights across multiple papers. The abstraction is limited to a surface-level description of the architecture."}}
{"id": "9a97aa09-7933-47fb-aa54-c2d30b05041f", "title": "(2) Higher-order Siamese GCN.", "level": "paragraph", "subsections": [], "parent_id": "fc53597b-c01f-49a5-ae40-002003ed60c9", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Taxonomy of Models"], ["subsection", "GNN-based Graph Similarity Learning"], ["subsubsection", "Siamese GNN models for Graph Similarity Learning"], ["paragraph", "(2) Higher-order Siamese GCN."]], "content": "} Higher-order Siamese GCN (HS-GCN) is proposed in , which incorporates higher-order node-level proximity into graph convolutional networks so as to perform higher-order convolutions on each of the input graphs for the graph similarity learning task. A Siamese framework is employed with the proposed higher-order GCN in each of the twin networks. Specifically, random walk is used for capturing higher-order proximity from graphs and refining the graph representations used in graph convolutions. Both this work and the S-GCN  introduced above use the Hinge loss for training the Siamese similarity learning models: \n\\begin{align}\n    L_{Hinge} = \\frac{1}{K}\\sum_{i=1}^{N}\\sum_{j=i+1}^{N} max(0,1-{y_{ij}}{s_{ij}}),\n    \\label{hinge_loss}\n\\end{align}\nwhere $N$ is the total number of graphs in the training set, $K = N(N-1)/2$ is the total number of pairs from the training set, $y_{ij}$ is the ground-truth label for the pair of graphs $G_i$ and $G_j$ where $y_{ij} = 1$ for similar pairs and $y_{ij} = -1$ for dissimilar pairs, and $s_{ij}$ is the similarity score estimated by the model. More general forms of higher-order information (e.g., motifs~) have been used for learning graph representations~ and would likely benefit the learning.", "cites": [3958], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual description of the Higher-order Siamese GCN method, explaining its use of random walks and Hinge loss. It references a related paper on graphlets but does not deeply integrate or connect the ideas from the cited work to the HS-GCN approach. There is limited critical analysis or abstraction to broader principles, making the section primarily descriptive with some basic synthesis."}}
{"id": "e6a59a34-baff-4c69-a9ab-28bc865b8f2b", "title": "(5) Siamese GCN for Image Retrieval.", "level": "paragraph", "subsections": [], "parent_id": "fc53597b-c01f-49a5-ae40-002003ed60c9", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Taxonomy of Models"], ["subsection", "GNN-based Graph Similarity Learning"], ["subsubsection", "Siamese GNN models for Graph Similarity Learning"], ["paragraph", "(5) Siamese GCN for Image Retrieval."]], "content": "} In , Siamese GCNs are used for content based remote sensing image retrieval, where each image is converted to a region adjacency graph in which each node represents a region segmented from the image. The goal is to learn an embedding space that pulls semantically coherent images closer while pushing dissimilar samples far apart. Contrastive loss is used in the model training.   \nSince the twin GNNs in the Siamese network share the same weights, an advantage of the Siamese GNN models is that the two input graphs are guaranteed to be processed in the same manner by the networks. As such, similar input graphs would be embedded similarly in the latent space. Therefore, the Siamese GNNs are good for differentiating the two input graphs in the latent space or measuring the similarity between them. \nIn addition to choosing the appropriate GNN models in the twin networks, one needs to choose a proper loss function. Another widely used loss function for Siamese network is the triplet loss . For a triplet $(G_i, G_p, G_n)$, $G_p$ is from the same class as $G_i$, while $G_n$ is from a different class from $G_i$. The triplet loss is defined as follows. \n\\begin{equation}\n    L_{Triplet} = \\frac{1}{K}\\sum_K max(d_{ip} - d_{in} + m, 0)\n    \\label{eq:triplet_loss}\n\\end{equation}\n\\noindent where $K$ is the number of triplets used in the training, $d_{ip}$ represents the distance between $G_i$ and $G_p$, $d_{in}$ represents the distance between $G_i$ and $G_n$, and $m$ is a margin value which is greater than 0. By minimizing the triplet loss, the distance between graphs from same class (i.e., $d_{ip}$) will be pushed to $0$, and the distance between graphs from different classes (i.e.,$d_{in}$ will be pushed to be greater than $d_{ip} + m$. \nIt is important to consider which loss function would be suitable for the targeted problem when applying these Siamese GNN models for the graph similarity learning task in practice.", "cites": [1244], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic description of Siamese GCNs for image retrieval, referencing a specific application where images are converted into region adjacency graphs. It explains the use of contrastive loss and introduces triplet loss, drawing from the FaceNet paper, but does not go beyond to connect multiple papers or ideas in a deeper, synthesized way. There is minimal critical analysis or abstraction, as it focuses mainly on defining the approach and loss functions without evaluating trade-offs or general principles."}}
{"id": "4cea576b-dcb6-4199-be95-d0314a6d2f00", "title": "(1) GMN: Graph Matching Network.", "level": "paragraph", "subsections": [], "parent_id": "8f46301c-d6ef-4ca5-8ae0-72f4f11b3e3f", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Taxonomy of Models"], ["subsection", "GNN-based Graph Similarity Learning"], ["subsubsection", "GNN-based Graph Matching Networks"], ["paragraph", "(1) GMN: Graph Matching Network."]], "content": "} In , \\gm{a GNN based architecture called Graph Matching Network (GMN) is proposed}, where the node update module in each propagation layer takes into account both the aggregated messages on the edges for each graph and a cross-graph matching vector which measures how well a node in one graph can be matched to the nodes in the other graph. Given a pair of graphs as input, the GMN jointly learns graph representations for the pair through the cross-graph attention-based matching mechanism, which propagates node representations by using both the neighborhood information within the same graph and cross-graph node information. A similarity score between the two input graphs is computed in the latent vector space.", "cites": [1655], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the GMN model, summarizing its components and mechanism as introduced in the cited paper. However, it lacks synthesis by not connecting GMN to broader themes or other methods within GNN-based graph similarity learning. There is minimal critical analysis or abstraction, with the section focusing primarily on factual presentation without deeper evaluation or generalization."}}
{"id": "5ccd3981-b575-4a02-9f80-d5d4fcff6100", "title": "(2) NeuralMCS: Neural Maximum Common Subgraph GMN", "level": "paragraph", "subsections": [], "parent_id": "8f46301c-d6ef-4ca5-8ae0-72f4f11b3e3f", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Taxonomy of Models"], ["subsection", "GNN-based Graph Similarity Learning"], ["subsubsection", "GNN-based Graph Matching Networks"], ["paragraph", "(2) NeuralMCS: Neural Maximum Common Subgraph GMN"]], "content": ".}Based on the graph matching network in ,  proposes a neural maximum common subgraph (MCS) detection approach for learning graph similarity. The graph matching network is adapted to learn node representations for two input graphs $G_1$ and $G_2$, after which a likelihood of matching each node in $G_1$ to each node in $G_2$ is computed by a normalized dot product between the node embeddings. The likelihood indicates which node pair is most likely to be in the MCS, and the likelihood for all pairs of nodes constitutes the matching matrix $\\mathbf{Y}$ for $G_1$ and $G_2$. Then a guided subgraph extraction process is applied, which starts by finding the most likely pair and iteratively expands the extracted subgraphs by selecting one more pair at a time until adding more pairs would lead to non-isomorphic subgraphs. To check the subgraph isomorphism, subgraph-level embeddings are computed by aggregating the node embeddings of the neighboring nodes that are included in the MCS, and Euclidean distance between the subgraph embeddings are computed. Finally, a similarity/match score is obtained based on the subgraphs extracted from $G_1$ and $G_2$.", "cites": [1655], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a straightforward description of the NeuralMCS method without connecting it to other GNN-based graph similarity approaches. It lacks critical evaluation or comparison with alternative methods and does not abstract the core idea into broader trends or principles within the field. The explanation is mostly factual and confined to a single approach."}}
{"id": "ce9cf272-1afd-4204-a74e-c8d91bdb27c8", "title": "(4) NCMN: Neural Graph Matching Network.", "level": "paragraph", "subsections": [], "parent_id": "8f46301c-d6ef-4ca5-8ae0-72f4f11b3e3f", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Taxonomy of Models"], ["subsection", "GNN-based Graph Similarity Learning"], ["subsubsection", "GNN-based Graph Matching Networks"], ["paragraph", "(4) NCMN: Neural Graph Matching Network."]], "content": "} In , a Neural Graph Matching Network (NGMN) is proposed for few-shot 3D action recognition, where 3D data are represented as interaction graphs. A GCN is applied for updating node features in the graphs and an MLP is employed for updating the edge strength. A graph matching metric is then defined based on both node matching features and edge matching features. In the proposed NGMN, edge generation and graph matching metric are learned jointly for the few-shot learning task. \nRecently, deep graph matching networks were introduced for the graph matching problem for image matching . Graph matching aims to find node correspondence between graphs, such that the corresponding node and edge's affinity is maximized. Although the problem of graph matching is different from the graph similarity learning problem we focus on in this survey and is beyond the scope of this survey, some work on deep graph matching networks involves graph similarity learning and thus we review some of this work below to provide some insights into how deep similarity learning may be leveraged for graph matching applications, such as image matching.", "cites": [3959, 3951, 1657], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily provides a factual description of the Neural Graph Matching Network (NGMN) and briefly mentions related deep graph matching networks. However, it lacks synthesis of the cited papers' contributions into a cohesive framework and does not engage in meaningful comparison or critical evaluation. The abstraction level is low, as it does not generalize patterns or principles across the works."}}
{"id": "777a3b98-f9fe-49a2-abf3-f9f99539da3f", "title": "(5) GMNs for Image Matching.", "level": "paragraph", "subsections": [], "parent_id": "8f46301c-d6ef-4ca5-8ae0-72f4f11b3e3f", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Taxonomy of Models"], ["subsection", "GNN-based Graph Similarity Learning"], ["subsubsection", "GNN-based Graph Matching Networks"], ["paragraph", "(5) GMNs for Image Matching."]], "content": "}In , a Graph Learning-Matching Network is proposed for image matching. A CNN is first utilized to extract feature descriptors of all feature points for the input images, and graphs are then constructed based on the features. Then the GCNs are used for learning node embeddings from the graphs, in which both intra-graph convolutions and cross-graph convolutions are conducted. The final matching prediction is formulated as node-to-node affinity metric learning in the embedding space, and the constraint regularized loss along with cross-entropy loss is used for the metric learning and the matching prediction. In , another GNN-based graph matching network is proposed for the image matching problem, which consists of a CNN image feature extractor, a GNN-based graph embedding component, an affinity metric function and a permutation prediction component, as an end-to-end learnable framework. Specifically, GCNs are used to learn node-wise embeddings for intra-graph affinity, where a cross-graph aggregation step is introduced to aggregate features of nodes in the other graph for incorporating cross-graph affinity into the node embeddings. The node embeddings are then used for building an affinity matrix which contains the similarity scores at the node level between two graphs, and the affinity matrix is further used for the matching prediction. The cross-entropy loss is used to train the model end-to-end.", "cites": [1657, 3951], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the architecture and components of two GNN-based graph matching networks for image matching without offering a deeper synthesis or comparative analysis. While it connects the two papers by highlighting similar components like CNN feature extractors and GCNs for node embeddings, it lacks critical evaluation or abstraction into broader principles or trends."}}
{"id": "11604b65-df2b-41c5-b0f9-622d1e43f96d", "title": "Deep Graph Kernels", "level": "subsection", "subsections": ["fc4843eb-f2f8-4625-b39b-91c72a2db7d4", "2307fe4f-ffb2-43bf-82ba-59b9b91dacfa", "9d538e0f-e124-4cbb-b508-69e2cfddb00e"], "parent_id": "cb42bc90-86fa-496b-a3b1-01595eb9c979", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Taxonomy of Models"], ["subsection", "Deep Graph Kernels"]], "content": "Graph kernels have become a standard tool for capturing the similarity between graphs for tasks such as graph classification . Given a collection of graphs, possibly with node or edge attributes, the work in graph kernel aim to learn a kernel function that can capture the similarity between any two graphs. Traditional graph kernels, such as random walk kernels, subtree kernels, and shortest-path kernels have been widely used in the graph classification task . Recently, deep graph kernel models have also emerged, which build kernels based on the graph representations learned via deep neural networks.", "cites": [3960, 8330], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic overview of graph kernels and their role in capturing graph similarity, citing two relevant papers. However, it merely describes the concepts and does not offer substantial synthesis, critical evaluation, or abstraction. The narrative remains at the level of summarizing existing methods without deeper integration or analysis of the cited works."}}
{"id": "fc4843eb-f2f8-4625-b39b-91c72a2db7d4", "title": "Deep Graph Kernels", "level": "subsubsection", "subsections": [], "parent_id": "11604b65-df2b-41c5-b0f9-622d1e43f96d", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Taxonomy of Models"], ["subsection", "Deep Graph Kernels"], ["subsubsection", "Deep Graph Kernels"]], "content": "In , a Deep Graph Kernel approach is proposed. For a given set of graphs, each graph is decomposed into its sub-structures. Then the sub-structures are viewed as words and neural language models in the form of CBOW (continuous bag-of-words) and Skip-gram are used to learn latent representations of sub-structures from the graphs, where corpora are generated for the Shortest-path graph and Weisfeiler-Lehman kernels in order to measure the co-occurrence relationship between substructures. Finally, the kernel between two graphs is defined based on the similarity of the sub-structure space. \n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width = 0.8 \\columnwidth]{Survey_Figure2.png}\n    \\caption{The Graph Representation Learning in the Deep Divergence Graph Kernels .}\n    \\label{fig:ddgk}\n\\end{figure}", "cites": [3952], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal description of the Deep Graph Kernels approach, focusing primarily on summarizing the method from the cited paper (DDGK). There is little synthesis of ideas, no comparison with other methods, and no critical evaluation or abstraction to broader principles. The narrative remains at a factual level without offering deeper insights or context."}}
{"id": "2307fe4f-ffb2-43bf-82ba-59b9b91dacfa", "title": "Deep Divergence Graph Kernels", "level": "subsubsection", "subsections": [], "parent_id": "11604b65-df2b-41c5-b0f9-622d1e43f96d", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Taxonomy of Models"], ["subsection", "Deep Graph Kernels"], ["subsubsection", "Deep Divergence Graph Kernels"]], "content": "In , a model called Deep Divergence Graph Kernels (DDGK) is introduced to learn kernel functions for graph pairs. Given two graphs $G_1$ and $G_2$, they aim to learn an embedding based kernel function $k( )$ as a similarity metric for graph pairs, defined as:\n\\begin{align}\n    k(G_1,G_2) = \\|\\Psi(G_1) - \\Psi(G_2)\\|^2\n\\end{align}\n\\noindent where $\\Psi(G_i)$ is a representation learned for $G_i$. This work proposes to learn graph representation by measuring the divergence of the target graph across a population of source graph encoders. Given a source graph collection $\\{G_1, G_2,$ $\\cdots, G_n\\}$, a graph encoder is first trained to learn the structure of each graph in the source collection. Then, for a target graph $G_T$, the divergence of $G_T$ from each source graph is measured, after which the divergence scores are used to compose the vector representation of the target graph $G_T$. Fig.~\\ref{fig:ddgk} illustrates the above graph representation learning process. Specifically, the divergence score between a target graph $G_T=(V_T,E_T)$ and a source graph $G_S=(V_S,E_S)$ is computed as follows:\n\\begin{align}\n    \\mathcal{D}^\\prime (G_T \\| G_S) = \\sum_{v_i \\in V_T} \\sum_{\\substack{j\\\\{e_{ij}\\in E_T}}} -log \\text{Pr}(v_j|v_i, H_S)\n\\end{align}\n\\noindent where $H_S$ is the encoder trained on graph $S$.", "cites": [3952], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a clear description of the Deep Divergence Graph Kernels (DDGK) method, including its formulation and approach to graph representation learning. However, it lacks synthesis with other methods, does not critically evaluate the approach or its limitations, and does not abstract broader principles or patterns in deep graph similarity learning. The narrative remains focused on summarizing the specific paper without contextual or comparative analysis."}}
{"id": "9d538e0f-e124-4cbb-b508-69e2cfddb00e", "title": "Graph Neural Tangent Kernel", "level": "subsubsection", "subsections": [], "parent_id": "11604b65-df2b-41c5-b0f9-622d1e43f96d", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Taxonomy of Models"], ["subsection", "Deep Graph Kernels"], ["subsubsection", "Graph Neural Tangent Kernel"]], "content": "In , a Graph Neural Tangent Kernel (GNTK) is proposed for fusing GNNs with the neural tangent kernel, which is originally formulated for fully-connected neural networks in  and later introduced to CNNs in . Given a pair of graphs $<G,G^\\prime>$, they first apply GNNs on the graphs. Let $f(\\theta, G) \\in \\mathbb{R}$ be the output of the GNN under parameters $\\theta \\in \\mathbb{R}^m$ on input Graph $G$, \\gm{where $m$ is the dimension of the parameters}. To get the corresponding GNTK value, they calculate the expected value of\n\\begin{align}\n    \\Bigg \\langle \\frac{\\partial f(\\theta, G)}{\\partial \\theta}, \\frac{\\partial f(\\theta, G^\\prime )}{\\partial \\theta} \\bigg \\rangle\n\\end{align}\nin the limit that $m \\rightarrow \\infty$ and $\\theta$ are all Gaussian random variables.\nMeanwhile, there are also some deep graph kernels proposed for the node representation learning on graphs for node classification and node similarity learning. For instance, in , a learnable kernel-based framework is proposed for node classification, where the kernel function is decoupled into a feature mapping function and a base kernel. An encoder-decoder function is introduced to project each node into the embedding space and reconstructs pairwise similarity measurements from the node embeddings. Since we focus on the similarity learning between graphs in this survey, we will not discuss this work further.", "cites": [3962, 3954, 3961, 3963], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to explain the concept and methodology of GNTK, connecting the neural tangent kernel from fully-connected and CNN literature to its adaptation for graphs. It critically notes the limitations of graph kernels (e.g., hand-crafted features) and highlights the strengths of GNNs in capturing high-order information, while also specifying its focus on graph-level similarity and excluding node-level approaches. The abstraction is moderate, identifying a theoretical framework and broader patterns in kernel-based graph learning, but not offering a high-level meta-insight across all deep graph similarity learning paradigms."}}
{"id": "612742b9-0b92-48f9-be23-5b3c99365af5", "title": "Datasets and Evaluation", "level": "section", "subsections": ["afeca160-71aa-4aa2-a514-5b7b03c76535", "7e724787-91ac-4b91-a9af-26a7e2a5076b"], "parent_id": "eab36b40-c6ea-4c4a-96e7-a440a76d2534", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Datasets and Evaluation"]], "content": "\\begin{table*}[]\n\\begin{center}\n\\caption{Summary of Benchmark Datasets that are Frequently Used in Deep Graph Similarity Learning.}\n\\label{tab:datasets}\n\\scalebox{0.78}{\n\\begin{tabular}{lclccccl}\n\\hline\n\\multicolumn{1}{|c|}{\\textbf{Graph Type}}                          & \\multicolumn{1}{c|}{\\textbf{Datasets}}         & \\multicolumn{1}{c|}{\\textbf{Source}}                                           & \\multicolumn{1}{c|}{\\rotatebox{90}{Number of Graphs}} & \\multicolumn{1}{c|}{\\rotatebox{90}{\\;\\;Number of Classes\\;\\;}} & \\multicolumn{1}{c|}{\\rotatebox{90}{\\;\\;Avg. Number of Nodes\\;\\;}} & \\multicolumn{1}{c|}{\\rotatebox{90}{\\;\\;Avg. Number of Edges\\;\\;}} & \\multicolumn{1}{c|}{\\textbf{References}} \\\\ \\hline\n\\multicolumn{1}{|c|}{\\multirow{6}{*}{Social Networks}}  & \\multicolumn{1}{c|}{COLLAB}           & \\multicolumn{1}{c|}{}         & \\multicolumn{1}{c|}{5000}           & \\multicolumn{1}{c|}{3}               & \\multicolumn{1}{c|}{74.49}                & \\multicolumn{1}{c|}{2457.78}              & \\multicolumn{1}{c|}{}                            \\\\ \\cline{2-8} \n\\multicolumn{1}{|l|}{}                                  & \\multicolumn{1}{c|}{IMDB-BINARY}      & \\multicolumn{1}{c|}{}         & \\multicolumn{1}{c|}{1000}           & \\multicolumn{1}{c|}{2}               & \\multicolumn{1}{c|}{19.77}                & \\multicolumn{1}{c|}{96.53}                & \\multicolumn{1}{c|}{}                            \\\\ \\cline{2-8} \n\\multicolumn{1}{|l|}{}                                  & \\multicolumn{1}{c|}{IMDB-MULTI}       & \\multicolumn{1}{c|}{}         & \\multicolumn{1}{c|}{1500}           & \\multicolumn{1}{c|}{3}               & \\multicolumn{1}{c|}{13.00}                & \\multicolumn{1}{c|}{65.94}                & \\multicolumn{1}{c|}{}                            \\\\ \\cline{2-8} \n\\multicolumn{1}{|l|}{}                                  & \\multicolumn{1}{c|}{REDDIT-BINARY}    & \\multicolumn{1}{c|}{}         & \\multicolumn{1}{c|}{2000}           & \\multicolumn{1}{c|}{2}               & \\multicolumn{1}{c|}{429.63}               & \\multicolumn{1}{c|}{497.75}               & \\multicolumn{1}{c|}{}                            \\\\ \\cline{2-8} \n\\multicolumn{1}{|l|}{}                                  & \\multicolumn{1}{c|}{REDDIT-MULTI-5K}  & \\multicolumn{1}{c|}{}         & \\multicolumn{1}{c|}{4999}           & \\multicolumn{1}{c|}{5}               & \\multicolumn{1}{c|}{508.52}               & \\multicolumn{1}{c|}{594.87}               & \\multicolumn{1}{c|}{}                            \\\\ \\cline{2-8} \n\\multicolumn{1}{|l|}{}                                  & \\multicolumn{1}{c|}{REDDIT-MULTI-12K} & \\multicolumn{1}{c|}{}         & \\multicolumn{1}{c|}{11929}          & \\multicolumn{1}{c|}{11}              & \\multicolumn{1}{c|}{391.41}               & \\multicolumn{1}{c|}{456.89}               & \\multicolumn{1}{c|}{}                            \\\\ \\hline\n\\multicolumn{1}{|c|}{\\multirow{3}{*}{Bioinformatics}}   & \\multicolumn{1}{c|}{D\\&D}             & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{1178}           & \\multicolumn{1}{c|}{2}               & \\multicolumn{1}{c|}{284.32}               & \\multicolumn{1}{c|}{715.66}               & \\multicolumn{1}{c|}{}                            \\\\ \\cline{2-8} \n\\multicolumn{1}{|l|}{}                                  & \\multicolumn{1}{c|}{ENZYMES}          & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{600}            & \\multicolumn{1}{c|}{6}               & \\multicolumn{1}{c|}{32.63}                & \\multicolumn{1}{c|}{62.14}                & \\multicolumn{1}{c|}{}                            \\\\ \\cline{2-8} \n\\multicolumn{1}{|l|}{}                                  & \\multicolumn{1}{c|}{PROTEINS}         & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{1113}           & \\multicolumn{1}{c|}{2}               & \\multicolumn{1}{c|}{39.06}                & \\multicolumn{1}{c|}{72.82}                & \\multicolumn{1}{c|}{}                            \\\\ \\hline\n\\multicolumn{1}{|c|}{\\multirow{5}{*}{Chemoinformatics}} & \\multicolumn{1}{c|}{AIDS}             & \\multicolumn{1}{c|}{}            & \\multicolumn{1}{c|}{2000}           & \\multicolumn{1}{c|}{2}               & \\multicolumn{1}{c|}{15.69}                & \\multicolumn{1}{c|}{16.20}                & \\multicolumn{1}{c|}{}                            \\\\ \\cline{2-8} \n\\multicolumn{1}{|c|}{}                                  & \\multicolumn{1}{c|}{MUTAG}            & \\multicolumn{1}{c|}{}     & \\multicolumn{1}{c|}{188}            & \\multicolumn{1}{c|}{2}               & \\multicolumn{1}{c|}{17.93}                & \\multicolumn{1}{c|}{19.79}                & \\multicolumn{1}{c|}{}                            \\\\ \\cline{2-8} \n\\multicolumn{1}{|c|}{}                                  & \\multicolumn{1}{c|}{NCI1}             & \\multicolumn{1}{c|}{}       & \\multicolumn{1}{c|}{4110}           & \\multicolumn{1}{c|}{2}               & \\multicolumn{1}{c|}{29.87}                & \\multicolumn{1}{c|}{32.30}                & \\multicolumn{1}{c|}{}                            \\\\ \\cline{2-8} \n\\multicolumn{1}{|c|}{}                                  & \\multicolumn{1}{c|}{NCI109}           & \\multicolumn{1}{c|}{}       & \\multicolumn{1}{c|}{4127}           & \\multicolumn{1}{c|}{2}               & \\multicolumn{1}{c|}{29.68}                & \\multicolumn{1}{c|}{32.13}                & \\multicolumn{1}{c|}{}                            \\\\ \\cline{2-8} \n\\multicolumn{1}{|c|}{}                                  & \\multicolumn{1}{c|}{PTC\\_MR}          & \\multicolumn{1}{c|}{}      & \\multicolumn{1}{c|}{344}            & \\multicolumn{1}{c|}{2}               & \\multicolumn{1}{c|}{14.29}                & \\multicolumn{1}{c|}{14.69}                & \\multicolumn{1}{c|}{}                            \\\\ \\hline\n\\multicolumn{1}{|c|}{\\multirow{3}{*}{Brain Networks}}   & \\multicolumn{1}{c|}{ABIDE}            & \\multicolumn{1}{c|}{}                                                 & \\multicolumn{1}{c|}{871}            & \\multicolumn{1}{c|}{2}               & \\multicolumn{1}{c|}{110}                  & \\multicolumn{1}{c|}{-}                    & \\multicolumn{1}{c|}{}                            \\\\ \\cline{2-8} \n\\multicolumn{1}{|l|}{}                                  & \\multicolumn{1}{c|}{UK Biobank}       & \\multicolumn{1}{c|}{}                                                 & \\multicolumn{1}{c|}{2500}           & \\multicolumn{1}{c|}{2}               & \\multicolumn{1}{c|}{55}                   & \\multicolumn{1}{c|}{-}                    & \\multicolumn{1}{c|}{}                            \\\\ \\cline{2-8} \n\\multicolumn{1}{|l|}{}                                  & \\multicolumn{1}{c|}{HCP}              & \\multicolumn{1}{c|}{}                                                 & \\multicolumn{1}{c|}{1200}            & \\multicolumn{1}{c|}{2}               & \\multicolumn{1}{c|}{360}                  & \\multicolumn{1}{c|}{-}                    & \\multicolumn{1}{c|}{}                            \\\\ \\hline\n\\multicolumn{1}{|c|}{Image Graphs}                      & \\multicolumn{1}{c|}{COIL-DEL}         & \\multicolumn{1}{c|}{}                                                 & \\multicolumn{1}{c|}{3900}           & \\multicolumn{1}{c|}{100}             & \\multicolumn{1}{c|}{21.54}                & \\multicolumn{1}{c|}{54.24}                & \\multicolumn{1}{c|}{}                            \\\\ \\hline\n                                                        & \\multicolumn{1}{l}{}                  &                                                                       & \\multicolumn{1}{l}{}                & \\multicolumn{1}{l}{}                 & \\multicolumn{1}{l}{}                      & \\multicolumn{1}{l}{}                      &                                                 \n\\end{tabular}\n}\n\\end{center}\n\\end{table*}\n\\gm{In this section, we summarize the characteristics of the datasets that are frequently used in deep graph similarity learning methods and the experimental evaluation adopted by these methods. }", "cites": [3952, 3954, 3959, 3949, 3953, 3955, 1655, 3948], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 24, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.2, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section primarily presents a table summarizing benchmark datasets used in deep graph similarity learning, but it lacks synthesis of ideas from the cited papers. It does not connect or integrate these works into a broader narrative. There is minimal critical analysis or abstraction to identify trends or overarching principles, and the only reference is a brief phrase ('In this section...') that does not engage deeply with the content of the cited works."}}
{"id": "afeca160-71aa-4aa2-a514-5b7b03c76535", "title": "Datasets", "level": "subsection", "subsections": [], "parent_id": "612742b9-0b92-48f9-be23-5b3c99365af5", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Datasets and Evaluation"], ["subsection", "Datasets"]], "content": "\\gm{Graph data from various domains have been used to evaluate graph similarity learning methods~, for example, protein-protein graphs from bioinformatics, chemical compound graphs from chemoinformatics, and brain networks from neuroscience, etc. We summarize the benchmark datasets that are frequently used in deep graph similarity learning methods in Table~\\ref{tab:datasets}. }\n\\gm{In addition to these datasets, synthetic graph datasets or other domain-specific datasets are also widely used in some graph similarity learning works. For example, in  and , control flow graphs of binary functions are generated and used to evaluate graph matching networks for binary code similarity search. In , attacks are conducted on testing machines to generate malware data, which are then merged with normal data to evaluate the Siamese GNN model for malware detection. In , images are collected from multiple categories and keypoints are annotated in the images to evaluate the proposed model for graph matching.}", "cites": [3959, 3951, 1655], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic overview of the types of datasets used in deep graph similarity learning, including both real-world and synthetic examples. However, it does not synthesize or integrate the cited papers meaningfully, nor does it compare or critically evaluate the methods described in them. The content remains largely descriptive, offering limited abstraction or insight into broader patterns or principles in the field."}}
{"id": "7e724787-91ac-4b91-a9af-26a7e2a5076b", "title": "Evaluation", "level": "subsection", "subsections": [], "parent_id": "612742b9-0b92-48f9-be23-5b3c99365af5", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Datasets and Evaluation"], ["subsection", "Evaluation"]], "content": "\\gm{During evaluation, most GSL methods take pairs or triplets of graphs as input during training with various objective functions used for various graph similarity tasks. The existing evaluation tasks mainly include pair classification ~, graph classification~, graph clustering~, graph distance prediction~, and graph similarity search~. Classification AUC (i.e., Area Under the ROC Curve) or accuracy are used as the most popular metric for the evaluation of graph-pair classification or graph classification task . Mean squared error (MSE) is used as evaluation metric for the regression task in graph distance prediction~.}\n\\gm{According to the evaluation results reported in the above works, the deep graph similarity learning methods tend to outperform the traditional methods. For example,  shows that the deep divergence graph kernel approach achieves higher classification accuracy scores compared to traditional graph kernels such as the shortest-path kernel  and Weisfeiler-Lehman kernel  in most cases for the graph classification task. Meanwhile, among the deep methods, methods that allow for cross-graph feature interaction tend to achieve a better performance compared to the factorized methods that relies only on single graph features. For instance, the experimental evaluations in  and  have demonstrated that the GNN-based graph matching networks have a superior performance than the Siamese GNNs in pair classification and graph edit distance prediction tasks. }\n\\gm{The efficiency of different methods are also analyzed and evaluated in some of these works.  In , some evaluations have been done for comparing the efficiency of the GNN based graph similarity learning approach SimGNN with traditional GED approximation methods including A*-Beamsearch , Hungarian  and VJ , where the core operation for GED approximation may take polynomial or sub-exponential to the number of nodes in the graphs. For the GNN based model like SimGNN, to compute similarity score for pairs of graphs, the time complexity mainly involves two parts: (1) the node-level and graph-level embedding computation stages, where the time complexity is $O(|E|)$, and $|E|$ is the number of edges of the graph ; and (2) the similarity score computation stage, where the time complexity is $O(D^2K)$ ($D$ is the dimension of the graph-level embedding, and $K$ is the feature map dimension used in the graph-graph interaction stage) for the strategy of using graph-level embedding interaction, and the time complexity is $O(DN^2)$ ($N$ is the number of nodes in the larger graph). The experimental evaluations in  show that the GNN based models consistently achieve the best results in efficiency and effectiveness for the pairwise GED computation  on multiple graph datasets, demonstrating the benefit of using these deep models for the similarity learning tasks.}", "cites": [3952, 3954, 3959, 3950, 3949, 3953, 3947, 1655, 3955, 3964, 3948], "cite_extract_rate": 0.4583333333333333, "origin_cites_number": 24, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes information from multiple cited works to provide a coherent narrative on evaluation practices in deep graph similarity learning. It identifies broader trends, such as the superiority of deep methods over traditional graph kernels and the benefits of cross-graph feature interaction. While it includes some critical analysis by comparing method efficiency and effectiveness, it does not deeply critique individual limitations or propose a novel framework, keeping it at a medium insight level."}}
{"id": "a708f27a-f60a-446c-924e-a19327f0e932", "title": "Computational Chemistry and Biology", "level": "subsection", "subsections": [], "parent_id": "3cf188bf-98ba-4cee-abfe-1f0fb8fd7d53", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Applications"], ["subsection", "Computational Chemistry and Biology"]], "content": "An important application of graph similarity learning in the chemistry and biology domain is to learn the chemical similarity, which aims to learn the similarity of chemical elements, molecules or chemical compounds with respect to their effect on reaction partners in inorganic or biological settings . An example is the compounds query for in-silico drug screening, where searching for similar compounds in a database is the key process. \nIn the literature of graph similarity learning, quite a number of models have been proposed and applied to similarity learning for chemical compounds or molecules. Among these work, the traditional models mainly employ sub-graph based search strategies or graph kernels to solve the problem . However, these methods tend to have high computational complexity and strongly rely on the sub-graph or kernels defined, making it difficult to use them in real applications. Recently, a deep graph similarity learning model SimGNN is proposed in  which also aims to learn similarity for chemical compounds as one of the tasks. Instead of using sub-graphs or other explicit features, the model adopts GCNs to learn node-level embeddings, which are fed into an attention module after multiple layers of GCNs to generate the graph-level embeddings. Then a neural tensor network (NTN)  is used to model the relation between two graph-level embeddings, and the output of the NTN is used together with the pairwise node embedding comparison output in the fully connected layers for predicting the graph edit distance between the two graphs. This work has shown that the proposed deep learning model outperforms the traditional methods for graph edit distance computation in prediction accuracy and with much less running time, which indicates the promising application of the deep graph similarity learning models in the chemo-informatics and bio-informatics.", "cites": [8707], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual summary of the application of graph similarity learning in computational chemistry and biology, mentioning traditional methods and a specific deep learning model (SimGNN). However, it lacks deeper synthesis of ideas across the cited works and offers minimal critical analysis or abstraction beyond the individual methods. The narrative is coherent but primarily descriptive rather than insightful."}}
{"id": "0ae76e08-88bc-4746-8ffd-66cf8e45398d", "title": "Neuroscience", "level": "subsection", "subsections": [], "parent_id": "3cf188bf-98ba-4cee-abfe-1f0fb8fd7d53", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Applications"], ["subsection", "Neuroscience"]], "content": "\\label{sec:apps_neuro}\nMany neuroscience studies have shown that structural and functional connectivity of the human brain reflects the brain activity patterns that could be indicators of the brain health status or cognitive ability level . For example, the functional brain connectivity networks derived from fMRI neuroimaging data can reflect the functional activity across different brain regions, and people with brain disorder like Alzheimer's disease or bipolar disorder tend to have functional activity patterns that differ from those of healthy people . To investigate the difference in brain connectivity patterns for these neuroscience problems, researchers have started to study the similarity of brain networks among multiple subjects with graph similarity learning methods .\nThe organization of functional brain networks is complicated and usually constrained by various factors, such as the underlying brain anatomical network, which plays an important role in shaping the activity across the brain. These constraints make it a challenging task to characterize the structure and organization of brain networks while performing similarity learning on them. Recent work in ,  and  have shown that the deep graph models based on graph convolutional networks have a superior ability to capture brain connectivity features for the similarity analysis compared to the traditional graph embedding based approaches. In particular,  proposes a higher-order Siamese GCN framework that leverages higher-order connectivity structure of functional brain networks for the similarity learning of brain networks. \n\\gm{In view of the work introduced above and the trending research problems in the field of neuroscience, we believe that deep graph similarity learning will benefit the clinical investigation of many brain diseases and other neuroscience applications. Promising research directions include, but are not limited to, deep similarity learning on resting-state or task-related fMRI brain networks for multi-subject analysis with respect to brain health status or cognitive abilities, deep similarity learning on the temporal or multi-task fMRI brain networks of individual subjects for within-subject contrastive analysis over time or across tasks for neurological disorder detection. Some example fMRI brain network datasets that can be used for such analysis have been introduced in Table 3.}", "cites": [3965], "cite_extract_rate": 0.1111111111111111, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the role of deep graph similarity learning in neuroscience, particularly in analyzing fMRI brain networks, and connects it to broader challenges such as capturing complex connectivity patterns. It provides some abstraction by discussing general research directions and applications. However, it lacks deeper critical analysis of the cited papers, such as their limitations or trade-offs, and the synthesis is somewhat constrained by the limited number of references provided."}}
{"id": "0081ac9f-10dd-4233-8737-aec2494b463a", "title": "Computer Security", "level": "subsection", "subsections": [], "parent_id": "3cf188bf-98ba-4cee-abfe-1f0fb8fd7d53", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Applications"], ["subsection", "Computer Security"]], "content": "In the field of computer security, graph similarity has also been studied for various application scenarios, such as the hardware security problem , the malware indexing problem based on function-call graphs , and the binary function similarity search for identifying vulnerable functions . \nIn , a graph similarity heuristic is proposed based on spectral analysis of adjacency matrices for the hardware security problem, where evaluations are done for three tasks, including gate-level netlist reverse engineering, Trojan detection, and obfuscation assessment. The proposed method outperforms the graph edit distance approximation algorithm proposed in  and the neighbor matching approach , which matches neighboring vertices based on graph topology.  is the work that introduced GNN-based deep graph similarity learning models to the security field to solve the binary function similarity search problem. Compared to previous models, the proposed deep model computes similarity scores jointly on pairs of graphs rather than first independently mapping each graph to a vector, and the node representation update process uses an attention-based module which considers both within-graph and cross-graph information. Empirical evaluations demonstrate the superior performance of the proposed deep graph matching networks compared to the Google's open source function similarity search tool , the basic GNN models, and the Siamese GNNs.", "cites": [6986, 1655], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a comparative overview of graph similarity learning in computer security, highlighting specific tasks and contrasting different methods (e.g., spectral analysis, graph edit distance, attention-based GNNs). While it connects a few works and shows some integration, it lacks deeper synthesis and broader abstraction. It identifies performance improvements in deep models but does not critically evaluate limitations or propose overarching principles."}}
{"id": "171c8fd2-f475-4c1e-92d9-2f4636b053d6", "title": "Computer Vision", "level": "subsection", "subsections": [], "parent_id": "3cf188bf-98ba-4cee-abfe-1f0fb8fd7d53", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Applications"], ["subsection", "Computer Vision"]], "content": "\\label{sec:apps_cv}\nGraph similarity learning has also been explored for applications in computer vision. In , context-dependent graph kernels are proposed to measure the similarity between graphs for human action recognition in video sequences. Two directed and attributed graphs are constructed to describe the local features with intra-frame relationships and inter-frame relationships, respectively. The graphs are decomposed into a number of primary walk groups with different walk lengths, and a generalized multiple kernel learning algorithm is applied to combine all the context-dependent graph kernels, which further facilitates human action classification. In , a deep model called Neural Graph Matching Network is first introduced for the 3D action recognition problem in the few-shot learning setting. Interaction graphs are constructed from the 3D scenes, where the nodes represent physical entities in the scene and edges represent interactions between the entities. The proposed NGM Networks jointly learn a graph generator and a graph matching metric function in an end-to-end fashion to directly optimize the few-shot learning objective. It has been shown to significantly improve the few-shot 3D action recognition over the holistic baselines. \n\\gm{Another emerging application of graph similarity learning in computer vision is the image matching problem, where the goal is to find consistent correspondences between the sets of features in two images. As introduced at the end of Section 3.2, recently some deep graph matching networks have been developed for the image matching task , where images are first converted to graphs and the image matching problem is then solved as a graph matching problem. In the graph converted from an image, the nodes represent the unary descriptors of annotated feature points in images, and edges encode the pairwise relationships among different feature points in that image. Based on the new graph representation, the feature matching can be reformulated as graph matching problem. However, it is worth noting that, this graph matching is actually the graph node matching, as the goal is to match the nodes between graphs instead of two entire graphs. Therefore, the graph based image matching problem is a special case or a sub-problem of the general graph matching problem. }\n\\gm{The two application problems discussed above are both promising directions of applying deep graph similarity learning models for the practical learning tasks in computer vision. A key advice we provide on applying graph similarity learning methods for these image applications is to first find an appropriate mapping for converting the images to graphs, so that the learning tasks on images can be formulated as the graph similarity learning based tasks.}", "cites": [1657, 3951], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the core ideas of the cited papers by connecting the use of graph similarity learning in human action recognition and image matching, and it abstracts these into broader application themes within computer vision. It also provides some critical insights, such as noting that image matching via graphs is a special case of graph matching (node-level rather than graph-level). However, deeper comparative analysis or more nuanced critique of the approaches is limited, which prevents a higher insight score."}}
{"id": "eca64974-b940-4870-92e2-070264a53b8b", "title": "Directed Graphs.", "level": "paragraph", "subsections": ["696a19bc-035e-47fd-b7fb-f13a23855870", "45aca054-dfb6-4baf-a0cb-fdd6e4cc107d"], "parent_id": "97bb3f9d-f6a8-4c5d-9324-117940fa741f", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Challenges"], ["subsection", "Various Graph Types"], ["paragraph", "Directed Graphs."]], "content": "In some application scenarios, the graphs are directed, which means all the edges in the graph are directed from one vertex to another. For instance, in a knowledge graph, edges go from one entity to another, where the relationship is directed. In such cases, we should treat the information propagation process differently according to the direction of the edge. Recently some GCN based graph models have suggested some strategies for dealing with such directed graphs. In , a dense graph propagation strategy is proposed for the propagation on knowledge graphs, where two kinds of weight matrices are introduced for the propagation based on a node's relationship to its ancestors and descendants respectively. However, to the best of our knowledge, no work has been done on deep similarity learning specifically for directed graphs, which arises as a challenging problem for this community.\n\\noindent", "cites": [3632], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section briefly synthesizes the approach from the cited paper on directed graphs but does not integrate it with other methods or theories. It provides a limited critical analysis by pointing out the lack of deep similarity learning work for directed graphs and mentioning limitations like Laplacian smoothing. The abstraction is minimal, as it does not generalize the findings to broader patterns in graph similarity learning."}}
{"id": "696a19bc-035e-47fd-b7fb-f13a23855870", "title": "Labeled Graphs.", "level": "paragraph", "subsections": [], "parent_id": "eca64974-b940-4870-92e2-070264a53b8b", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Challenges"], ["subsection", "Various Graph Types"], ["paragraph", "Directed Graphs."], ["paragraph", "Labeled Graphs."]], "content": "Labeled graphs are graphs where vertices or edges have labels. For example, in chemical compound graphs where vertices denote the atoms and the edges represent the chemical bonds between the atoms, each node and edge have labels representing the atom type and bond type, respectively. These labels are important for characterizing the node-node relationship in the graphs, therefore it is important to leverage these label information for the similarity learning. In , the node label information are used as the initial node representations encoded by a one-hot vector and used in the node embedding stage. In this case, the nodes with same type share the same one-hot encoding vector. This should guarantee that even if the node ids are permuted, the aggregation results would be the same. However, the label information is only used for the node embedding process within each graph, and the comparison of the node or edge labels across graphs is not considered during the similarity learning stage. \\gm{In , both node labels and edge labels in the chemo- and bio-informatic graphs have been used as attributes for learning better alignment across graphs, which has been shown to lead to a better performance. Therefore, how to leverage the node / edge attributes of the labeled graphs into the similarity learning process is a critical problem.}  \n\\noindent", "cites": [7762, 3952], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.5, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic analytical overview of how labeled graphs are handled in deep graph similarity learning, citing two papers to discuss the use of node and edge labels. It begins by describing the importance of labels and their role in embedding, then contrasts different approaches to label usage. However, the synthesis is limited, as it does not fully connect or integrate insights from both papers into a broader narrative. The critical analysis is present but shallow, pointing out that one approach does not compare labels across graphs while another does, without deeper evaluation of their implications."}}
{"id": "45aca054-dfb6-4baf-a0cb-fdd6e4cc107d", "title": "Dynamic and Streaming Graphs.", "level": "paragraph", "subsections": [], "parent_id": "eca64974-b940-4870-92e2-070264a53b8b", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Challenges"], ["subsection", "Various Graph Types"], ["paragraph", "Directed Graphs."], ["paragraph", "Dynamic and Streaming Graphs."]], "content": "\\label{sec:chall_dynamic}\nAnother type of graphs is the dynamic graph, which has a static graph structure and dynamic input signals/features. For example, the 3D human action or motion data can be represented as graphs where the entities are represented as nodes and the actions as edges connecting the entities. Then similarity learning on these graphs is an important problem for action and motion recognition. Moreover, another type of graph is the streaming graph, where both the structure and/or features are continuously changing~. For example, online social networks~. The similarity learning would be important for change/anomaly detection, link prediction, relationship strength prediction, etc. Although some work has proposed variants of GNN models for spatio-temporal graphs , and other learning methods for dynamic graphs~, the similarity learning problem on dynamic and streaming graphs has not been well studied. \\gm{For example, in the multi-subject analysis of task-related fMRI brain networks as mentioned in Section~\\ref{sec:apps_neuro}, for each subject, a set of brain connectivity networks can be collected for a give time period, which forms a spatio-temporal graph. It would be interesting to conduct similarity learning on the spatio-temporal graphs of different subjects to analyze their similarity in cognitive abilities, which is an important problem in the neuroscience field. However, to the best of our knowledge, none of the existing similarity learning methods is able to deal with such spatio-temporal graphs.} The main challenge in such problems is how to leverage the temporal updates of the node-level representations and the interactions between the nodes on these graphs while modeling their similarity.", "cites": [25, 3966, 3967, 3968, 7231, 3969, 3970], "cite_extract_rate": 0.6363636363636364, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a coherent discussion on dynamic and streaming graphs by integrating concepts from several papers, such as spatio-temporal modeling and online sampling. It identifies gaps in existing similarity learning methods for these graph types. However, while it connects the ideas, it does not offer a novel framework or deep comparative analysis, and the abstraction level remains moderate without fully generalizing overarching principles."}}
{"id": "b866b6e2-3980-4bae-bdbb-d9ba33bc22f1", "title": "Interpretability", "level": "subsection", "subsections": [], "parent_id": "b6ec31cb-95a5-4145-bd45-07204d0a10f7", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Challenges"], ["subsection", "Interpretability"]], "content": "The deep graph models, such as GNNs, combine node feature information with graph structure by recursively passing neural messages along edges of the graph, which is a complex process and makes it challenging to explain the learning results from these models. Recently, some work has started to explore the interpretability of GNNs . In , a GNNEXPLAINER is proposed for providing interpretable explanations for predictions of GNN-based models. It first identifies a subgraph structure and a subset of node features that are crucial in a prediction. Then it formulates an optimization task that maximizes the mutual information between a GNN's prediction and the distribution of possible subgraph structures.  explores the explainability of GNNs using gradient-based and decomposition-based methods, respectively, on a toy dataset and a chemistry task. Although these works have provided some insights into the interpretability of GNNs, they are mainly for node classification or link prediction tasks on a graph. To the best of our knowledge, the explainability of GNN-based graph similarity models remains unexplored.", "cites": [8708], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of one cited paper (GNNEXPLAINER) and mentions another work without a citation, integrating these to highlight approaches to GNN interpretability. It critically notes that existing methods are limited to node-level or link-level tasks rather than graph similarity. While it identifies a gap, the abstraction is limited, and the insights are not deeply meta-level or comparative across a broader set of works."}}
{"id": "a9af0d29-b684-4008-9968-6b82b2d2a323", "title": "Few-shot Learning", "level": "subsection", "subsections": [], "parent_id": "b6ec31cb-95a5-4145-bd45-07204d0a10f7", "prefix_titles": [["title", "Deep Graph Similarity Learning: A Survey \n"], ["section", "Challenges"], ["subsection", "Few-shot Learning"]], "content": "The task of few-shot learning is to learn classifiers for new classes with only a few training examples per class. A big branch of work in this area is based on metric learning . However, most of the existing work proposes few-shot learning problems on images, such as image recognition  and image retrieval. Little work has been done on metric learning for few-shot learning on graphs, which is an important problem for areas in which data are represented as graphs and data gathering is difficult, for example, brain connectivity network analysis in neuroscience. Since graph data usually has complex structure, how to learn a metric so that it can facilitate generalizing from a few graph examples is a big challenge. Some recent work ~ has begun to explore the few-shot 3D action recognition problem with graph-based similarity learning strategies, where a neural graph matching network is proposed to jointly learn a graph generator and a graph matching metric function to optimize the few-shot learning objective of 3D action recognition. However, since the objective is defined specifically based on the 3D action recognition task, the model can not be directly used for other domains. The remaining problem is to design general deep graph similarity learning models for the few-shot learning task for a multitude of applications.", "cites": [2144], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic analytical overview of few-shot learning in the context of deep graph similarity learning, identifying the limited focus on graph data and highlighting the challenges posed by their structural complexity. While it mentions a specific recent approach and its domain-specific limitations, the synthesis is minimal and does not deeply integrate multiple sources or ideas. It offers some critical evaluation by pointing out the lack of generalizability, but the abstraction remains at a surface level without uncovering broader meta-principles."}}
