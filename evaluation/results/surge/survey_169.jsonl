{"id": "3e9f98a5-6655-4b76-8a05-a45a20274d86", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "c8bf0b78-eb89-4a9b-a0d2-2aeb57b86250", "prefix_titles": [["title", "A survey on practical adversarial examples for malware classifiers"], ["section", "Introduction"]], "content": "\\label{introduction}\nThe field of malware detection and classification has grown considerably since the introduction of hash based file signature methods . With malware authors incorporating evasion techniques, such as obfuscation, into their malicious code, detection methods using static and dynamic analysis were and continue to be developed. Increased computational power has also led to many machine learning based solutions being developed alongside these analysis methods and being deployed in commercial products . \nHowever, in 2014 Szegedy et. al. showed that deep neural networks (DNNs) are susceptible to adversarial attacks. Grosse et. al. went on to show that this vulnerability also held true for machine learning based malware detectors and classifiers . Since this work, there have been many attacks developed against popular machine learning based models such as MalConv , however many of these attacks are not practical. Specifically, many attacks do not generate actual malware and instead generate a feature vector that represents what a possible perturbed malicious file should look like to evade detection. It is unpractical to generate an executable program given a feature vector due to the difficulty of the inverse feature-mapping . This is to say that the feature extraction process is not uniquely invertible nor is there a guarantee that a found solution would contain the same program logic as the original malware sample.\nIn this work, we review practical attacks against machine learning based malware classifiers and detectors, or attacks against these ML models that result in executable malware. In Section \\ref{background}, we introduce and define adversarial examples and the threat models in which they are considered. Then we review practical adversarial example research in the malware domain in Section \\ref{meat}. We offer suggestions for future directions in this field as well as discuss any challenges in Section \\ref{discussion}. Lastly, we conclude in Section \\ref{conclusion}.", "cites": [3861, 6172], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The introduction section synthesizes the cited works to establish the context and problem of adversarial examples in the malware detection domain. It highlights a key limitation in many adversarial attacks — the inability to produce executable malware — and connects this to broader challenges like inverse feature-mapping. While it provides a coherent narrative and identifies practical concerns, it lacks deeper comparative analysis or abstraction into overarching principles."}}
{"id": "0e5254ab-558f-41b0-a765-1c1b7555297b", "title": "Machine learning for malware", "level": "subsection", "subsections": ["f0ac662d-3c98-4a29-af33-205bed0a6c47", "0eab5995-f221-4832-9bcd-52b1459129e1"], "parent_id": "89140a3a-e96b-4aa0-885d-2e2b647df2bd", "prefix_titles": [["title", "A survey on practical adversarial examples for malware classifiers"], ["section", "Background"], ["subsection", "Machine learning for malware"]], "content": "With the increasing prevalence of the internet, we have seen an exponential increase in malware and attackers. To exacerbate the problem, malware authors obfuscate their malicious code to impede detection and evade various static and dynamic analysis methods . \nThe classic approach to detecting malware was to extract a file signature for malicious samples that were found on infected systems and add them to a signature database, also known as signature-based detection . For this approach, the whole sample, as well as subsets of the sample in question, must be searched for known signatures because malicious behavior can be embedded and interleaved in otherwise benign software. However, because signature-based detection relies on capturing a malware sample and then analyzing it to generate a new signature, it can only defend against already known attacks and can only attempt to defend against new or obfuscated malware . Machine learning based approaches have been proposed as a solution to this problem because of their ability to predict labels for new inputs. Machine learning models, such as Support Vector Machines (SVM) and $K$-means clustering, are used in malware classification, detection, and analysis approaches. In the classification problem, we attempt to separate malware samples into predefined malware families. Based on the training data, which consists of labeled malware samples, the learning-based model infers the classification of new malware samples. The detection problem can be seen as a sub-problem of classification. For detection, the learning-based model is used to find or \\textit{detect} malware samples when given malicious and benign executables. As detection is a case of binary classification, learning-based detection models can also be called classifiers. Classification and detection are supervised algorithms as the training data is labeled. Machine learning can also be used to augment malware analysis. Non-supervised clustering algorithms can be used to learn new similarities between malware samples . Additionally, we can reason over learning-based models to better understand what makes malware malicious . More recently, with increased research in deep learning approaches, researchers have begun to utilize convolutional neural networks to classify and detect malware .\nIn Tables \\ref{tab:static} and \\ref{tab:dynamic}, we provide a brief overview of static and dynamic features that are popularly used in machine learning based malware classifiers. The rest of the section describes proposed models that utilize one or more of the listed features.\n\\begin{table}\n    \\caption{In this table, we list static features that are popularly used in machine learning based malware classifiers. We loosely categorize based on the data used to generate each feature.}\n    \\label{tab:static}\n    \\begin{tabular}{|c|l|} \\hline\n    Data     &    Static Feature                                      \\\\ \\hline\n    Bytes        &   $\\bullet$  Extract $n$-grams from byte sequences  \\\\\n                 &   $\\bullet$  Convert bytes to black and white pixels \\\\\n                 &   $\\bullet$  Use byte sequence or hex-dump as input \\\\ \\hline\n    Opcodes      &   $\\bullet$  Opcode frequency vector               \\\\\n                 &   $\\bullet$  Extract $n$-grams from disassembly              \\\\\n                 &   $\\bullet$  Build Markov chain from opcode sequences             \\\\\n                 &   $\\bullet$  Generate control flow graph   \\\\ \\hline\n    API Calls    &   $\\bullet$  Indicator vector based on data mining                \\\\\n                 &   $\\bullet$  Frequency of API call based on data mining          \\\\ \\hline\n    System Calls &   $\\bullet$  Indicator vector based on disassembly  \\\\\n                 &   $\\bullet$  Frequency of system call      \\\\ \\hline\n    Environment  &   $\\bullet$  Hard-coded network addresses    \\\\ \\hline\n    \\end{tabular}\n\\end{table}\n\\begin{table}\n    \\caption{In this table, we list dynamic features that are popularly used in machine learning based malware classifiers. Similar to Table \\ref{tab:static}, we loosely categorize by the data used to generate each feature.}\n    \\label{tab:dynamic}\n    \\begin{tabular}{|c|l|} \\hline\n    Data     &   Dynamic Feature                                \\\\ \\hline\n    Opcodes      &   $\\bullet$  $N$-grams extracted from program trace \\\\ \\hline\n    API Calls    &   $\\bullet$  $N$-grams extracted from program trace \\\\\n                 &   $\\bullet$  Frequency of API call during execution \\\\ \\hline\n    System Calls &   $\\bullet$  $N$-grams based on program trace \\\\\n                 &   $\\bullet$  Frequency of system call during execution \\\\\n                 &   $\\bullet$  Taint analysis of system information \\\\ \n                 &   $\\bullet$  Behavioral profiles using system interaction \\\\\\hline\n    Environment  &   $\\bullet$  Data transfers \\\\ \n                 &   $\\bullet$  Network traffic and communication \\\\\n                 &   $\\bullet$  File and registry edits, creation, and deletion \\\\ \n                 &   $\\bullet$  New processes spawned during execution \\\\ \\hline\n    \\end{tabular}\n\\end{table}", "cites": [3861, 7155], "cite_extract_rate": 0.25, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of how machine learning is applied to malware classification and detection, with a brief mention of some cited papers. However, it does not effectively synthesize or integrate the cited papers into a broader narrative, nor does it critically evaluate their contributions or limitations. The generalization to broader patterns or principles is minimal, and the focus remains largely on summarizing methods and features used in the field."}}
{"id": "f0ac662d-3c98-4a29-af33-205bed0a6c47", "title": "Static Features", "level": "subsubsection", "subsections": [], "parent_id": "0e5254ab-558f-41b0-a765-1c1b7555297b", "prefix_titles": [["title", "A survey on practical adversarial examples for malware classifiers"], ["section", "Background"], ["subsection", "Machine learning for malware"], ["subsubsection", "Static Features"]], "content": "$N$-grams are currently a popular feature used for the classification and detection of malware. Kolter and Maloof proposed extracting the most relevant $n$-grams of byte codes from PE malware for classification using various machine learning models, including naive bayes classifiers, decision trees, support vector machines (SVM), and boosted models . They found that in addition to classification, their model could be used for malware detection.\nMcBoost was introduced as a tool to quickly analyze large amounts of binaries when searching for malware and utilized a three step process . The first step is to detect packers using an ensemble of a heuristic-based classifiers and two different n-grams based classifiers. If a packer is detected, the binary is unpacked using QEMU and dynamic analysis. Lastly, a separate n-gram classifier is used to detect malware that should be forwarded for additional analysis.\nSantos et. al. proposed the use of $n$-grams as an alternative to file signatures based methods in 2009. In doing so, they showed that machine learning, specifically a k-nearest neighbors model, and $n$-grams can successfully be used to detect new malware samples . $N$-grams have also been used together with dynamic features to incorporate multiple views of malware simultaneously without actually conducting dynamic analysis . Recent work such as solutions proposed for the Kaggle Microsoft Malware Challenge , demonstrate continued usage of both byte and opcode $n$-grams with classification accuracies of almost $100\\%$. \nSimilar to using sequences of opcodes to generate $n$-grams, Markov chains can be extracted from the opcode trace from a program. Such a Markov chain uses unique opcodes as its states and shows the transition probabilities from one opcode to another. Anderson et. al. used the similarity between programs' Markov chains as one feature for malware detection. Similarly, Runwal et. al.  proposed using graph similarity between Markov chains to detect embedded malware and Shafiq et. al.  proposed measuring the entropy using Markov chains to detect malware. \nJang et. al. presented BitShred, a tool for malware triage and analysis . BitShred hashes the $n$-gram fingerprints extracted from samples using locality sensitive hashing to reduce the dimensionality of the feature space. The hashes are used in a k-nearest neighbors model to cluster the malware samples. Additionally, the authors showed that BitShred can be used to improve previous malware detection and classification models. For example, the authors showed that BitShred can be used to hash dynamic features, such as the behavior profiles generated in Bayer et. al. , to reduce the dimensionality of the feature space.\nDrebin conducts large-scale static analysis of Android software to extract features such as hardware usage and requests, permissions, API calls, and networking information . These features are used to map the sample to a joint vector space by generating an binary indicator vector. These binary indicator vectors are used as input to a SVM, which labels a sample as benign or malicious. Importantly, Drebin takes advantage of the simplicity of their model to attribute the model's decisions to specific features. This makes Drebin more explainable than malware classifiers and detectors based on complex architectures, such as convolutional neural networks.\nNataraj et. al. proposed using malware images (black and white image representations of binaries) to detect malware . Since then, researchers and commercial antivirus have used malware images to detect malware with high accuracy . Nataraj et. al. used the malware images to create a feature vector that would be used as input to a support vector machine. However, recent work has also shown that it is effective to use the raw images as input to a convolutional neural network .\nResearch has also been done in developing static features that give some insight into how the program behaves during run-time using control flow graphs. This research mainly revolves around constructing a control flow graph and using graph matching techniques to detect malware . Ma et. al. take a similar approach in using control flow graphs, but extracts a sequences of API calls in an attempt to mimic dynamic analysis .\nRaff et. al. takes a different approach and propose a convolutional neural network (CNN) model that takes in the whole binary as input . In particular, the proposed model MalConv looks at the raw bytes of the file to determine maliciousness. MalConv borrows from neural network research in learning higher level representations from long sequences and relies on CNN's ability to capture high level local invariances. MalConv works by extracting the $k$ bytes of a file. These $k$ bytes are padded with $0xff$ bytes to create a feature vector of size $d$. If $k > d$, the first $d$ bytes of the file are used without any additional padding. This $d$-length vector is mapped to a fixed length feature vector by an embedding that is jointly learned with the CNN.", "cites": [6173, 3861], "cite_extract_rate": 0.09523809523809523, "origin_cites_number": 21, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of static features in malware detection, with some synthesis of methods like n-grams and Markov chains across different works. However, it lacks critical evaluation or nuanced comparison of the approaches, and while it touches on patterns (e.g., the use of n-grams across multiple systems), it does not abstract these into broader principles or frameworks. Overall, it delivers a factual summary with limited insight."}}
{"id": "c6d260ea-c505-4b5b-b643-7ee8a1e151ad", "title": "Adversarial examples", "level": "subsection", "subsections": ["668d3e49-570c-4add-8d10-ee4d522a70da", "679f6782-3ca2-46c7-9aed-9cadc8c56939"], "parent_id": "89140a3a-e96b-4aa0-885d-2e2b647df2bd", "prefix_titles": [["title", "A survey on practical adversarial examples for malware classifiers"], ["section", "Background"], ["subsection", "Adversarial examples"]], "content": "The notion of adversarial examples was first introduced in  and expanded upon in . Assume $f$ is the target classifier that an adversary plans to attack. This classifier can be represented as a function $f(\\cdot)$ that takes an input and assigns it a label. An adversarial example $x'$ targeting $f$ is generated by perturbing an original input $x$ with $\\delta$ so that $f(x) \\ne f(x')$.\n$$ x' = x + \\delta ,$$\n$$ f(x') \\ne f(x) $$\nAdditionally, the perturbation is generally bounded by some value $\\epsilon$ using an $l_{2}$, $l_{1}$, or $l_{\\infty}$ norm.\n$$ || x' - x ||_{p}  < \\epsilon $$\nIn the natural image or sound domain, this bound is used to ensure that the perturbations are imperceptible to a discriminator, such as the human eye or ear.\nThere are many ways to find $\\delta$, the most popular being the Fast Gradient Sign Method   and the Carlini-Wagner (C\\&W) Attack  for white box models and substitute model attacks  for black box models. Most attacks will use the gradient of the loss function with respect to the input to find the direction in which the input must be perturbed for the wanted change to the output. This direction is then used to find $\\delta$. We briefly discuss these and other attacks in the Appendix.", "cites": [314, 892, 890], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic definition of adversarial examples and introduces key concepts and methods, citing foundational papers. It integrates some ideas across sources, such as the role of perturbations and norms, but lacks deeper synthesis or comparison of the cited works. While it outlines common approaches like FGSM and C&W, it does not critically evaluate their strengths or weaknesses in the context of malware classification, limiting its analytical depth."}}
{"id": "679f6782-3ca2-46c7-9aed-9cadc8c56939", "title": "Adversarial malware examples", "level": "subsubsection", "subsections": [], "parent_id": "c6d260ea-c505-4b5b-b643-7ee8a1e151ad", "prefix_titles": [["title", "A survey on practical adversarial examples for malware classifiers"], ["section", "Background"], ["subsection", "Adversarial examples"], ["subsubsection", "Adversarial malware examples"]], "content": "\\label{ame}\nMost adversarial example research is conducted using natural image datasets, such as MNIST, CIFAR10, and ImageNet. However, it is necessary to consider the set of allowable perturbations that preserve functionality of adversarial malware examples.\nFor natural images, the pixel values are perturbed to generate an adversarial example. Any negative or positive change to a pixel value will result in a slightly altered image as long as the resulting pixel is between 0 and 255. Executable programs can be represented in a similar way. Each byte of the binary, by definition, is between \\textit{0x00} and \\textit{0xff}. Each byte's hex representation can be translated to its decimal equivalent (between 0 and 255). In this state, a byte and pixel can be perturbed using the same methods. However, an arbitrary perturbation to a byte may not result in a valid executable because executable programs exist in a discrete space. Consider the simple case of altering one byte of an executable. If the byte comes from the \\textit{.text} section of an ELF, the new altered byte may break functionality of the program by changing function arguments or resulting in bad instructions. For this reason, applying adversarial example techniques to the malware domain requires special care in the binary's construction. Most importantly, an adversarial malware example must contain the same malicious program logic and functionality as the original. \nAdversarial malware examples are an immediate threat as they are evasive and malicious executables that can take advantage of many commercial antivirus software's persistent vulnerability to obfuscation and mutation . This differentiates practical adversarial malware examples from an adversarial feature vector. While an adversarial feature vector also evades detection or classification, there is no immediate threat. Pierazzi et. al argue that generating an executable given an adversarial feature vector is difficult and call this the \\textit{inverse feature-mapping} problem. There is no unique solution to the inverse feature-mapping problem. In the simple case of an $n$-gram classifier, the addition of an $n$-gram can be done in multiple ways. However, they are not all guaranteed to result in an executable that contains the same program logic or executability as the original malware sample. This problem becomes more difficult when dealing with black box models, where the attacker has no knowledge of the classifier's input and internals. Pierazzi et. al. explain that there are two ways practical adversarial malware examples circumvent this: (1) A gradient-driven approach where the code perturbations' effect on the gradient is approximated and used to follow the direction of the gradient and (2) a problem-driven approach where mutations are first applied randomly before taking on an evolutionary approach.\n\\begin{table*}\n    \\caption{In this table, we summarize practical adversarial malware example algorithms by showing (1) if the work was evaluated with static features, (2) if the work was evaluated with dynamic features, (3) the target learning-based models in the evaluation, (4) available transformations in the attack, and (5) whether the attack is gradient-driven or problem-driven. We note a general trend in the use of obfuscation and further discuss this in Section \\ref{discussion}. }\n    \\label{tab:tax}\n    \\begin{tabular}{|c|c|c|p{.3\\textwidth}|p{.13\\textwidth}|p{.125\\textwidth}|} \\hline\n    Attack & Static & Dynamic & Target model & Transformation & Approach \\\\ \\hline\n     GADGET               & \\checkmark & \\checkmark & Ensemble of custom machine learning and deep learning models & add API calls & gradient-driven \\\\ \\hline\n     Anderson et. al.      & \\checkmark & & Gradient boosted decision trees & Edit byte features and PE metadata  & problem-driven \\\\ \\hline\n     Kolosnjaji et. al.  & \\checkmark & & MalConv  & Edit padding bytes & gradient-driven \\\\ \\hline\n     Kruek et. al.            & \\checkmark & & MalConv  & Edit padding bytes & gradient-driven \\\\ \\hline\n     Demetrio et. al.      & \\checkmark & & MalConv  & Edit PE header bytes & gradient-driven \\\\ \\hline\n     Park et. al.              & \\checkmark & & Custom CNNs and gradient boosted decision trees & semantic $NOP$ insertion & gradient-driven \\\\ \\hline\n     Song et. al.              & \\checkmark & \\checkmark & Commercial antivirus & Edit byte features and PE metadata, and instruction substitution & problem-driven \\\\ \\hline\n     Yang et. al.              & \\checkmark &  &  AppContext  and Drebin  & Mutates and transplants context features  & problem-driven \\\\ \\hline\n     Kucuk et. al.            & \\checkmark & \\checkmark  & Two custom classifiers using static features and one custom classifier using dynamic features & Control flow obfuscation, bogus code blocks, and add API calls & problem-driven   \\\\ \\hline\n     Pierazzi et. al.      & \\checkmark &  & Drebin  & Bogus code blocks and opaque constructs & problem-driven \\\\ \\hline\n     HideNoSeek                & \\checkmark &  & Custom Bayesian classifier & JavaScript AST transforms & problem-driven \\\\ \\hline\n    \\end{tabular}\n\\end{table*}", "cites": [6172, 3861, 6174, 3862, 8107, 7155], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple cited papers to create a coherent narrative about the differences between adversarial feature vectors and practical adversarial malware examples. It abstracts the concept of adversarial examples from images to malware binaries and introduces the inverse feature-mapping problem. While it provides some critical analysis by highlighting the challenges in generating valid executables, it could offer deeper evaluation of the papers' methodologies or limitations."}}
{"id": "0818cdb9-5759-4f30-b53f-b6a1fa5a6830", "title": "Editing bytes and metadata", "level": "subsubsection", "subsections": [], "parent_id": "7b3eb03a-ef8f-48dc-9f7b-5d10f54463d4", "prefix_titles": [["title", "A survey on practical adversarial examples for malware classifiers"], ["section", "Practical attacks"], ["subsection", "Gradient-driven approaches"], ["subsubsection", "Editing bytes and metadata"]], "content": "\\label{bytes1}\nA popular method for creating practical adversarial malware examples is to add or alter bytes in unused space in the binary. Additionally, this can be done in the header to change header metadata without affecting functionality. In this section, we will review proposed attacks that use this type of transformation. Because these attacks focus on unused or \"unimportant\" (for execution) bytes, they do not require source code for generating their evasive malware samples. However, with the exception of GADGET , these attacks are still white box attacks as they require complete access to the target model to compute gradients.\nIn 2018, Rosenberg et. al. proposed GADGET, a software framework to transform PE malware into evasive variants taking advantage of the transferability property of adversarial examples between DNNs . The proposed attack assumes a black box threat model with no access to the malware source code. However, the attack assumes the target model takes a sequence of API calls as input. To generate adversarial examples, GADGET constructs a surrogate or substitute model that is trained with Jacobian-based dataset augmentation, introduced by Papernot et. al. as an attack against natural image classifiers . The dataset augmentation creates synthetic inputs that help the substitute model better approximate the target black box model's decision boundaries. This increases the probability of the transferability of the attack as both the substitute and target model will have learned similar distributions. Once the substitute model is trained, adversarial malware examples are generated by adding dummy API calls to the original malware's API call sequence. The authors call these dummy API calls semantic \\textit{nops} as the chosen API call or their corresponding arguments have no affect on the original program logic. It is important to note that the authors only add API calls, as removing an API call can break the functionality of the program. Let us say that the original API call sequence is an array $w_0$ where each index $j \\in [0, n]$ contains an API call. Each iteration $i$ of this process returns a new array $w_i$. At iteration $i$, an API call $d$ is added to $w_{i-1}$ at some index $j$ that pushes it towards the direction indicated by the gradient as the most impactful for the substitute model's decision. This results in $w_i$ where $w_i[j] = d$ and $w_i[j+1:] = w_{i-1}[j:]$ because all API calls in the previous sequence after index $j$ are essentially \"pushed back\". This method of perturbing the input by adding dummy API calls ensures that functionality is not broken. To generate the actual executable from this adversarial API call sequence, GADGET implements a wrapper that hooks all API calls. The hooks call the original APIs as well as dummy APIs as necessary from the adversarial API call sequence. These hooks ensure that the resulting adversarial malware example maintains the functionality and behavior of the original sample, as the original sample is being executed in a sense. GADGET was evaluated against Custom models including variants of logistic regression, recurrent neural networks (RNN), fully connected deep neural networks (DNN), convolutional neural networks (CNN), SVM, boosted decision trees, and random forest classifiers. The authors also showed that their attack produces malware that is able to evade classifiers that use static features, such as printable strings.\nKolosnjaji et. al. proposed a white box attack against MalConv that generated adversarial PE malware examples by iteratively manipulating padding bytes at the end of the file . Although the authors note that bytes at any location in the PE can be altered, it requires precise knowledge of the file architecture as a simple change can break file integrity. For this reason, the proposed attack focused only on byte appending. A challenge faced by the authors was the non-differentiability of MalConv due to its embedding layer. To circumvent this, the authors proposed computing the gradient of the objective function with respect to the embedding representation $z$ instead of the input. Each padding byte is replaced with an embedded byte $m$ that is closest to the line $g(\\eta) = z + \\eta n$ where $n$ is the normalized gradient direction. However, if $m$'s projection on the line $g(\\eta)$ is not aligned with $n$, the next closest embedded byte is selected. By only altering the padding at the end of the file, the proposed attack does not change the program logic nor the functionality of the original malware sample. However, this also limits the total number of perturbations allowed by the attack. As explained in Section \\ref{background}, MalConv extracts up to $d$ bytes from a binary. If the size of the binary is less than $d$, the extracted $k$ bytes have $(d-k)$ \\textit{0xff} padding bytes appended to it. This means that the proposed attack is limited by the size of the original malware sample. \nKruek et. al.  extended the work of Kolosnjaji et. al. by proposing a method for reconstructing the PE malware sample given the adversarial example's embedding. The authors found that reconstructing bytes from the perturbed embedding $z*$ is often non-trivial as $z*$ can lose resemblance to embeddings $z \\in Z$ used to learn $M$, the function mapping padding bytes to embedding bytes. Thus, they presented a novel loss function to ensure that perturbed embeddings $z*$ will be close to an actual embedding in $M$. This is done by introducing a distance term in the loss function between generated embeddings and $M$. \nDemetrio et. al. proposed \\textit{feature attribution} as a explainable machine learning algorithm to understand decisions made by machine learning models . Feature attribution was based off of a technique called \\textit{integrated gradients} introduced in 2017 by Sundararajan et. al . Given the target model $f$, an input $x$, and a baseline $x'$, integrated gradients compute the attribution for the $i$th feature of $x$ as\n$$ IG_i(x) = (x_i - x_i') \\int_0^1 \\frac{\\partial f(x' + \\alpha (x-x'))}{\\partial x_i} d\\alpha$$\nAs this integral is computed on all points in the line between $x$ and $x'$, each point should contribute to $f$'s classification of $x$ as long as $x'$ is a \\textit{good} baseline. The authors approximate this integral using summations, pulling from the suggestions in . It is important to note that these contributions are calculated with respect to the chosen baseline $x'$. The authors selected an empty file to be the baseline for the proposed feature attribution technique. Another option for the baseline was a file with only zero bytes. However, this option was labeled malicious with a $20\\%$ probability by MalConv, going against baseline constraints laid out in . Using feature attribution, Demetrio et. al. observed the attribution to each byte of input executables and found that MalConv heavily weighs the PE header section of binaries. The authors exploited this and presented a white box attack against MalConv that only alters bytes in the malware sample's header. This attack used the same algorithm presented in  but perturbed unused and editable bytes inside the header instead of padding at the end of the file.", "cites": [1824, 6174, 3862, 6172, 7155], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.3, "critical": 3.7, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers to present a coherent narrative on gradient-driven attacks that modify bytes and metadata, linking their methodologies and limitations. It includes critical evaluations, such as the non-differentiability challenge in MalConv, and explains the rationale behind design choices (e.g., using an empty file as a baseline). The abstraction is evident in the generalization of techniques like semantic nops and perturbation constraints, offering a deeper understanding of how adversarial malware attacks operate within the PE file structure."}}
{"id": "d6856704-1c06-49e6-aa67-475b849f3106", "title": "Code transformations", "level": "subsubsection", "subsections": [], "parent_id": "7b3eb03a-ef8f-48dc-9f7b-5d10f54463d4", "prefix_titles": [["title", "A survey on practical adversarial examples for malware classifiers"], ["section", "Practical attacks"], ["subsection", "Gradient-driven approaches"], ["subsubsection", "Code transformations"]], "content": "\\label{transform1}\nMany of the works above note that the proposed methods can be used to alter the \\textit{.text} section of malicious binaries as long as the program's functionality and malicious behavior is not altered. The following attacks make use of obfuscation techniques to alter the \\textit{.text} section.\nPark et. al. proposed a white box attack that utilized \\textit{semantic nops}, such as \\textit{mov eax, eax} in x86 assembly, to create adversarial PE malware examples . The authors attacked convolutional neural networks that used the image representation of an executable  as input. The image representation of an executable treats each byte as a pixel and uses the byte's decimal value as the pixel value. The proposed attack has two steps. First, an adversarial example is generated using FGSM. This adversarial example is an image and may not have the same functionality or malicious behavior as the original malware sample. In the second step, the original malware sample and the generated adversarial image are used as input to a dynamic programming algorithm that inserts \\textit{semantic nops} using LLVM passes. Similar to how API calls are added to resemble the generated adversarial feature vector in , the dynamic programming algorithm adds \\textit{semantic nops} in a way such that the resulting malware sample's image representation resembles the generated adversarial image from step 1. The authors went on to show that this attack can be used against a black box model because of the transferability property of adversarial examples and perturbations . Using a simple 2-layer CNN as a substitute model, the authors generated adversarial malware examples that also evaded black box models, one of which being a gradient boosted decision tree using byte-level features. The authors also mention that their attack works best given the malware's source code. However, in the absence of source code, binary translation and rewriting techniques can be used to insert the necessary \\textit{semantic nops}. It is important to note that introducing these techniques also introduces artifacts from binary lifting process.", "cites": [975, 8107, 6172], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.3, "critical": 3.0, "abstraction": 2.7}, "insight_level": "medium", "analysis": "The section integrates the work of Park et al. with general concepts of adversarial attacks, such as transferability and obfuscation techniques, showing basic synthesis. It includes critical discussion of the attack's effectiveness when source code is not available and mentions introduced artifacts, but does not deeply compare or evaluate multiple approaches. The abstraction is limited to the context of code transformations and image-based representations, without broader theoretical generalization."}}
{"id": "c8d850ed-7f09-4203-82f8-a7c59946fc41", "title": "Editing bytes and metadata", "level": "subsubsection", "subsections": [], "parent_id": "bea45b15-3848-42e9-9844-6dcef59bd76b", "prefix_titles": [["title", "A survey on practical adversarial examples for malware classifiers"], ["section", "Practical attacks"], ["subsection", "Problem-driven approaches"], ["subsubsection", "Editing bytes and metadata"]], "content": "\\label{bytes2}\nAnderson et. al. proposed a particularly interesting attack in which a reinforcement learning (RL) agent is equipped with a set of PE functionality preserving operations . The RL agent is rewarded for actions that produce malware that evades detection. Through this game, the agent learns a policy for creating evasive malware. The proposed attack makes use of the following actions that do not change the original program logic:\n\\begin{itemize}\n    \\item Adding functions to the import table that are never used.\n    \\item Changing section names.\n    \\item Creating new but unused sections.\n    \\item Adding bytes to unused space in sections.\n    \\item Removing signer information.\n    \\item Alter debugging information.\n    \\item Packing or unpacking the binary.\n    \\item Modifying the header.\n\\end{itemize}\nUsing these actions, the RL agent is able to alter features such as PE metadata, human readable strings, and byte histograms. After up to 50,000 mutations during the training phase, the RL agent is evaluated against a gradient boosted decision tree model, shown to be successful in classifying malware . The authors note that their adversarial examples should be functional by construction. However, they found that their attack breaks functionality in certain Windows PE's that make use of less common uses of the file format or obfuscation tricks that violate the PE standard. The authors claim that this can simply be fixed by ensuring the original malware samples can be correctly parsed by binary instrumentation frameworks.\nSong et. al. took a different approach in generating adversarial malware examples . The proposed attack randomly generates a sequence of \\textit{macro-actions} and applies them to the original PE malware sample. This is repeated until the resulting transformed malware evades detection. Once the malware sample is evasive, unnecessary macro-actions are removed from the sequence of macro-actions applied to it. This is done to minimize the probability of accidentally breaking functionality due to some obfuscation tricks. The remaining macro-actions are then broken down into \\textit{micro-actions} for a finer detailed trace of transformations leading to the adversarial malware sample. We suggest the reader peruse the original paper for greater detail on each macro and micro action, however, we briefly describe them here. Macro-actions consist of the following:\n\\vspace{2mm}\n\\begin{itemize}[-,nosep]\n    \\item Append bytes to the end of the binary.\n    \\item Append bytes to unused space at the end of a section.\n    \\item Add a new section.\n    \\item Rename a section.\n    \\item Zero out signed certificate.\n    \\item Remove debugging information.\n    \\item Zero out the checksum value in the header.\n    \\item Substitute instructions with semantically equivalent instructions.\n\\end{itemize}\n\\vspace{2mm}\nSome of these macro-actions can be broken down to a sequence of smaller actions, named micro-actions. For example, the action of appending bytes can be broken down to a sequence of adding one byte at a time. The authors claim that by breaking down each macro-action, it is possible to gain insights into why a particular action caused evasion. Instead of utilizing adversarial example generation algorithms such as FGSM or the C\\&W attack, the proposed method instead seeks to provide a more explainable attack against machine learning models. This method was evaluated against commercial antivirus and was also found to be effective against classifiers that incorporate both static and dynamic analysis.", "cites": [6172], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes two distinct approaches to generating adversarial malware by integrating methods from Anderson et al. and Song et al., drawing connections between them (e.g., both aim to preserve functionality while evading detection). It offers a critical perspective by highlighting limitations in functionality preservation and the need for fixes. While it identifies some patterns (e.g., the use of PE metadata modifications), it does not abstract beyond the specific techniques discussed to form a higher-level framework or principle."}}
{"id": "478aa0c2-3ba0-4420-8cb7-951265b4c23d", "title": "Code transformations", "level": "subsubsection", "subsections": [], "parent_id": "bea45b15-3848-42e9-9844-6dcef59bd76b", "prefix_titles": [["title", "A survey on practical adversarial examples for malware classifiers"], ["section", "Practical attacks"], ["subsection", "Problem-driven approaches"], ["subsubsection", "Code transformations"]], "content": "\\label{transform2}\nYang et. al. proposed two attacks to construct Android malware samples to evade detection by machine learning models, but did not use machine learning algorithms . Instead of targeting misclassification, the proposed \\textit{evolution attack} focuses on mimicking the natural evolution of Android malware based on mutating contextual features (made up of temporal features, locale features, and dependency features) . This is done by automating these mutation strategies through an obfuscation tool OCTOPUS and employing them on a large scale to identify \"blind spots\" on target classifiers. Malware families are organized into phylogenetic evolutionary trees  to analyze commonly shared features and divergent features within the family. Each feature mutation is then ranked by feasibility and frequency, and sorted. The top $x$ mutations are then used to generate new malware variants. The authors also proposed a \\textit{feature confusion attack} to complement the evolution attack. The goal of the feature confusion attack is to modify the malware sample such that certain features are similar to those of benign samples. The attack begins by collecting a set of \\textit{confusing features}, or a set of features that both malware and benign samples share. For each feature in the \\textit{confusing feature} set, the number of benign and malicious samples containing that feature is recorded. If there are more benign samples, that feature is added to the target features list. The attack then mutates malware samples to include the found target features to increase probability of evasion. The proposed method was evaluated against Android learning-based malware classifiers AppContext  and Drebin . It is important to note that while the attack does not require white box access to the target model, it does assume (1) malware source code and (2) knowledge of features used by the model.\nKucuk et. al. argued that adversarial malware examples must evade both static and dynamic machine learning based classifiers . As such, they proposed an attack for PE malware utilizing bogus control flow obfuscation and API obfuscation to evade detection by models using both static and dynamic features. The applied control flow obfuscation is based off of the LLVM-Obfuscator . LLVM-Obfuscator alters the control flow of a program at the LLVM-IR level by utilizing opaque predicates and never-executed bogus basic blocks with arbitrary instructions. Using differential analysis, the authors find the optimal control flow obfuscation and bogus basic blocks to generate an adversarial malware example. This perturbs static features, such as $n$-grams, opcode frequency, and imported API calls. The attack uses a genetic algorithm minimizing the Kullback-Leibler (KL) divergence between the frequency feature vectors of the desired target class and the adversarial malware sample. To evade a dynamic API call based malware classifier, the authors use the same genetic algorithm to determine which API calls must be obfuscated and then obfuscates them using the techniques laid out in . Additionally, the same genetic algorithm is used again to determine additional API call sequences that should be added to the original malware sample, similar to the approach taken by .\nPierazzi et. al. proposed a black box attack targeting the Android malware classifier Drebin . The authors proposed a problem-space approach that repeatedly inserts benign code blocks using opaque predicates to change features extracted by Drebin. These benign code blocks are initialized before the attack by analyzing samples in the training set for code sequences that contribute to a negative or benign label. The attack is bounded by a feasibility check to avoid excessive transformations, which may lead to increased suspicion. Additionally, the code blocks are inserted using FlowDroid  and Soot  to minimize side-effects or artifacts.\nHideNoSeek differs from other attacks that apply code transformations in that it attempts to hide malicious JavaScript by transforming the abstract syntax tree (AST) to appear benign . The attack begins by building ASTs of malicious and benign files to detect sub-ASTs or sub-graphs that are shared between the two classes. To create the adversarial example, HideNoSeek utilizes randomization, data obfuscation, and opaque constructs to insert benign-appearing sub-ASTs. The attack can also rewrite existing ASTs to appear benign. These attacks were conducted in a black box model against custom classifiers based on Zozzle, a Bayesian classifier that uses features extracted from JavaScript ASTs .", "cites": [6172], "cite_extract_rate": 0.07692307692307693, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple code transformation-based adversarial attacks, connecting them through common strategies such as obfuscation and feature manipulation. It offers some critical analysis by highlighting assumptions (e.g., access to source code or feature knowledge) and the use of specific tools (e.g., genetic algorithms, LLVM-Obfuscator). While it identifies broader patterns in the use of ASTs and obfuscation techniques, it primarily focuses on the mechanics of each approach rather than offering a meta-level theoretical abstraction."}}
{"id": "7412ee1d-6c75-43f3-bfd7-1d90971cd2cf", "title": "Defending against practical adversarial malware examples. ", "level": "subsubsection", "subsections": [], "parent_id": "877ac466-a94e-4ecf-9767-8bd910ff4d1f", "prefix_titles": [["title", "A survey on practical adversarial examples for malware classifiers"], ["section", "Discussion"], ["subsection", "Possible research directions"], ["subsubsection", "Defending against practical adversarial malware examples. "]], "content": "Some research has already been done in evaluating the use of adversarial training in the malware domain . However, robust machine learning research includes many other defense strategies such as smoothing  and  randomization . It is unclear whether these approaches would transfer and defend against adversarial malware examples.", "cites": [6175, 3854, 6151, 9099], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis by mentioning adversarial training, smoothing, and randomization as defense strategies, and links them to the malware domain through references to relevant papers. It identifies a gap in the literature by noting that while some methods have been proposed, it is unclear if they are effective against practical adversarial malware examples. The abstraction is moderate, as it generalizes the idea that defense techniques from other domains may not directly apply to malware detection, but it stops short of developing a broader theoretical or conceptual framework."}}
{"id": "ca66b19e-7ee7-4e5f-9e27-261b02572edd", "title": "Relationships between obfuscation and adversarial examples.", "level": "subsubsection", "subsections": [], "parent_id": "877ac466-a94e-4ecf-9767-8bd910ff4d1f", "prefix_titles": [["title", "A survey on practical adversarial examples for malware classifiers"], ["section", "Discussion"], ["subsection", "Possible research directions"], ["subsubsection", "Relationships between obfuscation and adversarial examples."]], "content": "Obfuscation and adversarial examples share a common goal: evade detection. Additionally, a majority of the practical adversarial malware example algorithms incorporated popular obfuscation strategies into their attack. One possible research problem is evaluating the feasibility of using more advanced obfuscation methods, such as virtualization, for adversarial example generation. It is also currently unclear what the benefit of adversarial malware examples is when compared against more traditional malware evasion techniques, such as  those summarized in Bulazel et. al. . It would also be interesting to extend upon the work of Song et. al.  and Demetrio et. al.  in the explainability of adversarial malware examples and use this to further develop evasive transformations.", "cites": [7155], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates the concept of obfuscation with adversarial examples, pointing to commonalities and suggesting research directions. It references Song et. al. and Demetrio et. al. for explainability and raises a research question about the comparative benefit of adversarial examples versus traditional obfuscation. However, it lacks deeper synthesis and critical evaluation of the cited works, instead proposing general research problems without extensive analysis."}}
{"id": "1bcd7731-b688-413c-a654-966757aeb692", "title": "Integration of static and dynamic analysis techniques.", "level": "subsubsection", "subsections": [], "parent_id": "877ac466-a94e-4ecf-9767-8bd910ff4d1f", "prefix_titles": [["title", "A survey on practical adversarial examples for malware classifiers"], ["section", "Discussion"], ["subsection", "Possible research directions"], ["subsubsection", "Integration of static and dynamic analysis techniques."]], "content": "Many of the reviewed works assume that no advanced analysis is done on the malware samples prior to being tested. However, this does not always have to be the case. For example, a pre-processing step can be used to deobfuscate the evasive malware samples produced by  and  using a deobfuscation framework such as SATURN . It would be interesting to see future work in attack and defense that considers using classification and detection pipelines instead of a sole machine learning model or commercial antivirus product.", "cites": [8107, 8108], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section synthesizes the idea of adversarial malware generation from Paper 1 and introduces the deobfuscation framework from Paper 2, showing some integration of the two. It offers a modest level of critical analysis by pointing out a limitation in the assumptions of prior works, but it stops short of deeper evaluation or comparison. The abstraction is limited, as it does not generalize to broader patterns or principles but instead suggests a future direction based on the combination of specific techniques."}}
{"id": "bb0dfe38-5f4b-4eef-a6d3-2bfd970226ab", "title": "Other Survey and systematization of knowledge papers", "level": "subsection", "subsections": [], "parent_id": "3e59a436-6980-4756-a668-3f59cacd482f", "prefix_titles": [["title", "A survey on practical adversarial examples for malware classifiers"], ["section", "Discussion"], ["subsection", "Other Survey and systematization of knowledge papers"]], "content": "In this section, we provide other survey and systematization of knowledge papers that cover related topics.\nYuan et. al. survey adversarial attacks and defenses for deep learning . They also provide applications and problem domains in which adversarial attacks can be used. Similar to this work, Maiorca et. al. provide a survey on adversarial attacks against machine learning based PDF malware detection systems . Bulazel and Yener survey dynamic malware analysis evasion and mitigation strategies . Ye et. al. survey the application of data mining techniques to malware detection . Ucci et. al. provides a survey on malware analysis using machine learning . Lastly, van der Kuowe et. al. survey popular benchmarking flaws that must be considered to fairly and accurately evaluate security research.", "cites": [893, 3860], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section merely lists other survey papers without synthesizing their contributions or connecting them to the current work. There is no critical evaluation or comparison of the cited papers, nor any abstraction or generalization of broader themes or trends in the field. It serves as a reference to related literature rather than providing insightful analysis."}}
{"id": "96b91491-c2d3-435a-b857-b1bda0dbc520", "title": "L-BFGS", "level": "subsubsection", "subsections": [], "parent_id": "fe991281-9672-4bdf-bb62-7008459e9d37", "prefix_titles": [["title", "A survey on practical adversarial examples for malware classifiers"], ["section", "Adversarial examples"], ["subsection", "White box attacks"], ["subsubsection", "L-BFGS"]], "content": "Szegedy et. al.  introduced the generation of adversarial examples using a box-constrained L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno) algorithm. Given an image $x$ and target label $t$, the proposed method searches for a similar image $x'$ (measured by the $l_2$ distance) that is classified as $t$. The problem is modeled as follows:\n $$ \\text{minimize} ||x - x'||^{2}_{2} $$\n $$ \\text{such that} \\ C(x') = t $$\n $$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ x' \\in [0, 1]^n $$\nBecause finding a solution to this problem can be difficult, they instead solve the following problem:\n$$ \\text{minimize} \\ c \\cdot ||x-x'||^2_2 + \\text{loss}_{F,t}(x') $$\n$$ \\text{such that} \\ x' \\in [0, 1]^n $$\nwhere $\\text{loss}_{F,t}$ is any loss function, such as cross-entropy loss.", "cites": [314], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of the L-BFGS method introduced by Szegedy et al., including the mathematical formulation. However, it lacks synthesis with other works in the field, does not critically evaluate the method or its limitations, and offers no abstraction or broader context within the domain of adversarial examples in malware classification."}}
{"id": "1f3d3ff7-0766-4c67-81ef-2c347390c66f", "title": "FGSM", "level": "subsubsection", "subsections": [], "parent_id": "fe991281-9672-4bdf-bb62-7008459e9d37", "prefix_titles": [["title", "A survey on practical adversarial examples for malware classifiers"], ["section", "Adversarial examples"], ["subsection", "White box attacks"], ["subsubsection", "FGSM"]], "content": "Goodfellow et. al.  introduced the Fast Gradient Sign Method (FGSM) for generating adversarial examples quickly. Given an image $x$, FGSM produces an adversarial $x'$ such that\n $$x' = x + \\epsilon \\cdot \\text{sign}(\\Delta\\text{loss}_{F,t}(x)) $$\n where $\\epsilon$ is chosen to be sufficiently small to avoid detection. \n  \\textit{Iterative extension.}\n  Kurakin et. al.  extended FGSM to be iterative with the goal of produce adversarial examples that are closer to or more indistinguishable from the original image. This was done by taking multiple steps of size $\\alpha$ in the direction of the gradient-sign instead of taking a single step of size $\\epsilon$. This iterative process begins with \n  $$ x'_0 = 0 $$\n  and each step $x'_i$ is as follows:\n  $$x'_i = x'_{i-1} - \\text{clip}_\\epsilon(\\alpha \\cdot \\text{sign}(\\Delta\\text{loss}_{F,t}(x'_{i-1})))$$", "cites": [892], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of FGSM and its iterative extension, citing the original papers. However, it lacks synthesis of ideas beyond what is directly stated in the papers and does not engage in critical analysis or abstraction to broader principles. It primarily functions as a summary of the method without deeper evaluation or integration with other related attacks."}}
{"id": "813f474d-624f-4d43-a66b-36b1faa642e7", "title": "JSMA", "level": "subsubsection", "subsections": [], "parent_id": "fe991281-9672-4bdf-bb62-7008459e9d37", "prefix_titles": [["title", "A survey on practical adversarial examples for malware classifiers"], ["section", "Adversarial examples"], ["subsection", "White box attacks"], ["subsubsection", "JSMA"]], "content": "Papernot et. al.  introduced the Jacobian-based Saliency Map Attack (JSMA) for generating adversarial examples based on understanding the mapping between input features and the computed output label. For a neural network $F$ and a target label $t$, this method uses the gradient $\\Delta Z_F(x)_t$ to compute a saliency map to model each input pixel's impact on the the computed label of $x$, $C(x)$. Using the saliency map, the most important pixel is modified. This is repeated until an adversarial example is generated or until a set threshold for pixel modifications is reached. For the exact formulation of the saliency map, we recommend reading the original paper.", "cites": [894], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal synthesis by only referencing one paper (Papernot et al.) and briefly describing JSMA. There is no critical analysis of the method's strengths, weaknesses, or applicability to malware detection. The content remains concrete and lacks abstraction or identification of broader patterns or principles in adversarial attacks."}}
{"id": "8f136b38-a8fe-4013-97f3-78c4a0403ef5", "title": "DeepFool.", "level": "subsubsection", "subsections": [], "parent_id": "fe991281-9672-4bdf-bb62-7008459e9d37", "prefix_titles": [["title", "A survey on practical adversarial examples for malware classifiers"], ["section", "Adversarial examples"], ["subsection", "White box attacks"], ["subsubsection", "DeepFool."]], "content": "Moosavi-Dezfooli et. al.  acknowledged the difficulty of understanding the decision making process of a DNN and instead approached a simpler version of the problem. The proposed method assumes a simplified model where the target neural network is completely linear with linearly separable classes. The proposed attack, DeepFool, generates an adversarial example for this simplified problem. This is then repeated until an adversarial example is found for the simpler model that transfers to the non-simplified problem. Similar to our suggestion for JSMA, the full formulation is best found in the original paper.", "cites": [906], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the DeepFool method and its approach to generating adversarial examples by using a simplified linear model. It integrates the cited paper to some extent but lacks deeper synthesis, critical evaluation, or abstraction. The section does not compare DeepFool to other white-box attacks or analyze its limitations in the context of malware classification."}}
{"id": "e6e6fba1-8294-4b0d-9ffa-61792457dd3b", "title": "C\\&W", "level": "subsubsection", "subsections": [], "parent_id": "fe991281-9672-4bdf-bb62-7008459e9d37", "prefix_titles": [["title", "A survey on practical adversarial examples for malware classifiers"], ["section", "Adversarial examples"], ["subsection", "White box attacks"], ["subsubsection", "C\\&W"]], "content": " proposed a new approach to generating adversarial examples, popularly called the Carlini-Wagner (C\\&W) method or attack. The C\\&W attack finds a small perturbation $\\delta$ by solving\n    \\begin{equation}\n    \\label{eq:cw}\n        \\min_{\\delta \\in \\Re^n} \\ \\ \\ ||\\delta||_p + c \\cdot f(x + \\delta)\n    \\end{equation}\n    \\[\n        \\mbox{s.t.} \\ \\ \\ \\ \\ \\ x +  \\delta \\in [0, 1]^n\n    \\]\n    where $f$ is an objective function that leads to $C(x) = t$ and $||\\cdot||_p$ is the $l_p$ norm. To solve the box constrain problem, instead of optimizing over $\\delta$ as in Equation \\ref{eq:cw}, the authors proposed omtimizting over $\\omega$ by setting\n    \\begin{equation}\n        \\delta_i = \\frac{1}{2}(\\tanh(\\omega_i) + 1) - x_i\n    \\end{equation}\n    There are three proposed attacks, an $l_2$ attack, $_0$ attack, and an $l_\\infty$ attack, but we only describe the $l_2$ attack as the authors reported it to be the strongest.\n    The $l_2$ attack attempts to minimize the distortion by searching for an $\\omega$ that minimizes\n    \\begin{equation}\n        ||\\frac{1}{2}(\\tanh (\\omega) + 1) - x||^2_2 + c\\cdot f(\\frac{1}{2}(\\tanh (\\omega) + 1)\n    \\end{equation}\n    where $f$ is the function\n    \\begin{equation}\n        f(x') = \\max(\\max\\{Z(x')_i : i \\ne t\\} - Z(x')_t, -\\kappa)\n    \\end{equation}\n    where $t$ is a chosen target class, $\\kappa$ is a parameter to adjust misclassification confidence, $x'$ is the adversarial example, and $Z(\\cdot)$ is the output of the classifier.", "cites": [890], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"error": "Failed to parse LLM response", "raw_response": "{\n    \"type\": \"descriptive\",\n    \"scores\": {\"synthesis\": 2.0, \"critical\": 1.5, \"abstraction\": 1.5},\n    \"insight_level\": \"low\",\n    \"analysis\": \"The section provides a factual description of the C\\&W method, focusing on its mathematical formulation and implementation details. It integrates some key concepts from the cited paper but does not connect them to broader themes or other works. There is minimal critical analysis or abstraction, as the section primarily summarizes the method without eval"}}
{"id": "d675c932-2359-4f83-9c93-f24e84c0bf46", "title": "Deep representation manipulation.", "level": "subsubsection", "subsections": [], "parent_id": "fe991281-9672-4bdf-bb62-7008459e9d37", "prefix_titles": [["title", "A survey on practical adversarial examples for malware classifiers"], ["section", "Adversarial examples"], ["subsection", "White box attacks"], ["subsubsection", "Deep representation manipulation."]], "content": "Sabour et. al.  proposed generating adversarial examples guided by the internal representation of a deep neural network. The proposed attack uses a guide image $x_g$ and its internal representation, $Ir_F(x_g)$, with respect to $F(\\cdot, \\theta)$, and applies perturbations to a different sample image $x$ such that $Ir_F(x) \\sim Ir_F(x_g)$. The actual perturbations to push $Ir_F(x)$ towards $Ir_F(x_g)$ are found using L-BFGS from .", "cites": [314, 5837], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal synthesis of the cited papers, primarily paraphrasing the method proposed by Sabour et al. without connecting it to broader themes or other related works. It lacks critical evaluation or discussion of limitations and does not abstract the concept into a general principle or framework. The content remains descriptive and does not contribute deeper analytical or comparative insights."}}
{"id": "c69a1183-0a8e-4594-989f-dc9ee75089e5", "title": "Finite difference estimation.", "level": "subsubsection", "subsections": [], "parent_id": "45e379f7-420e-40f0-b142-d7d5998c260a", "prefix_titles": [["title", "A survey on practical adversarial examples for malware classifiers"], ["section", "Adversarial examples"], ["subsection", "Black box attacks"], ["subsubsection", "Finite difference estimation."]], "content": "In finite difference estimation black box attacks, the adversary has access to the class probabilities output by the target model $F$. In this section, we describe black box attacks that take advantage of this information.\n  \\textit{ZOO}.\n  The Zeroth Order Optimization based black box (ZOO) attack  is a method for creating adversarial examples that uses finite difference estimates of the gradients. ZOO adopts an iterative optimization approach, similar to the C\\&W attack . The attack begins with a correctly classified input $x$. Then the adversary defines an adversarial loss function that scores perturbations $\\delta$ applied to the input and optimizes over the adversarial loss function using the estimated gradients to find $\\delta^*$ that creates an adversarial example. ZOO uses \"zeroth order stochastic coordinate descent\" to optimize the input with respect to the adversarial loss directly.\n  \\textit{Limited Queries and Information.}\n  A similar finite difference based approach is adopted by Ilyas et. al.  in a query limited (QL) setting. Like ZOO, the QL attack estimates the adversarial loss's gradients using a finite difference estimation. However, the QL attack attempts to minimize the number of queries needed by the adversary to estimate the gradients using a search distribution. The QL attack updates the adversarial example using Projected Gradient Descent (PGD) as outlined in Madry et. al.  based on the gradient estimates, which are evaluated using Natural Evolutionary Strategies (NES) .\n  This method was later extended upon by the original authors by using a bandit optimization-based algorithm (BAND) with \\textit{gradient priors} . The authors defined two types of gradient priors: (1) time-dependent priors or information from successive gradient estimations and (2) data-dependent priors or information from the structure of the inputs. BAND achieves similar success rates to the QL attack using 90\\% less queries.", "cites": [890, 6143, 909, 917, 8982], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple papers by connecting finite difference estimation methods used in ZOO, QL, and BAND attacks, highlighting their shared strategy and differences in query efficiency. It provides a critical perspective by noting the QL attack's emphasis on minimizing queries and the BAND method's improvement using gradient priors. While it identifies some general patterns (e.g., the use of gradient estimation in black-box settings), it stops short of deeper abstraction or proposing a novel conceptual framework."}}
{"id": "426a3e9e-c3a9-477e-be9b-0cd9429b90f4", "title": "Transferability", "level": "subsubsection", "subsections": [], "parent_id": "45e379f7-420e-40f0-b142-d7d5998c260a", "prefix_titles": [["title", "A survey on practical adversarial examples for malware classifiers"], ["section", "Adversarial examples"], ["subsection", "Black box attacks"], ["subsubsection", "Transferability"]], "content": "Transferability of adversarial examples was introduced by Papernot et. al. . This property can be used to mount a black box attack on a target DNN by instead attacking a substitute model and \\textit{transferring} the resulting adversarial examples to the target black box model. This substitute model is generally trained for the same classification problem as the target model. Because this model is trained by the adversary, the adversary can use any of the white box attacks described above to generate an adversarial example. \n  \\textit{Universal perturbations.}\n  Moosavi-Dezfooli et. al.  also demonstrated this transferability property through a universal adversarial attack. In their experimentation, the authors used DeepFool  to generate adversarial examples and found that an attack on model $A$ transfers to a model $B$ trained for the same classification task. The authors also provide a formulation and explanation for the transferability and universality of perturbations.\n  \\textit{Substitution attack with Jacobian-based dataset augmentation.}\n  This transferability property was extended by Papernot et. al.  in a substitution attack. Instead of solely relying on transferability, the authors proposed training a substitute model using data labeled by the target model. Additionally, the authors introduced Jacobian-based Dataset Augmentation, which uses a similar idea of a saliency map in JSMA , to approximate the target model's decision boundaries. The goal of this attack is to increase the probability of transferable adversarial examples by pushing the substitute model to learn the same decision boundaries as the target model. Using Jacobian-based dataset augmentation decreases the number of queries to the target model during the training phase of the substitute model because the training dataset is \\textit{augmented} to heavily weigh exploring decision boundaries of the model. They experimentally showed their attack successfully evading known image classifiers as well as online black box services.\n \\textit{Generative Adversarial Network (GAN) approach.}\n Zhao et. al.  proposed using a generative adversarial network GAN  to generate adversarial examples. The proposed method trained a GAN with a generator $G$ that maps latent space vectors $Z$ to natural image samples $X$. An \\textit{inverter} $I$ is separately trained to map natural image samples $X$ to latent space vectors $Z$, or invert the mapping of $G$. Instead of generating and applying perturbations in the $X$ space, the latent space vector $z_i = I(x_i)$ is perturbed to produce $\\hat{z}_i$. An adversarial example in this model is a $\\hat{z}_i$ such that $F(G(\\hat{z}_i)) \\ne F(x)$. This attack relies on the transferability property as the GAN is trained separately from the target model. However, the GAN can be trained using data labeled using the target model, depending on the adversary's knowledge, to incorporate the target model into the attack. Similar approaches using GANs have been proposed by Wang et. al.  and Xiao et. al. .\n  \\textit{$\\aleph$-Attack.}\n  Motivated by NES , Li et. al.  introduced a powerful black box attack called $\\aleph$-Attack. Unlike other existing attacks that optimize perturbations specifically for an input $x$, the proposed attack estimates a probability density distribution centered around the input $x$ such that a sample drawn from the estimated distribution is an adversarial example. $\\aleph$-Attack essentially learns the distribution of adversarial examples and samples that distribution to generate a black box attack. The authors experimentally showed that $\\aleph$-Attack was more effective than the BPDA attack  against ML models trained on the CIFAR10 and Imagenet datasets.\n\\end{document}\n\\endinput", "cites": [6176, 975, 918, 906, 894, 8982, 912, 9133], "cite_extract_rate": 0.7272727272727273, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple papers to explain the concept of transferability in black-box adversarial attacks, connecting techniques like universal perturbations, Jacobian-based dataset augmentation, GAN approaches, and the $\\aleph$-Attack. However, it lacks deeper critical evaluation of the methods' limitations or trade-offs. Some abstraction is present, such as identifying the common reliance on transferability, but the section remains grounded in method-specific descriptions without reaching a high level of generalization."}}
