{"id": "018f29a5-9c6a-4a49-a16b-a0e91fe46e19", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "ddfc697b-7da7-4ab2-9fff-861b778b8fa7", "prefix_titles": [["title", "Deep Neural Approaches to Relation Triplets Extraction: A~Comprehensive~Survey"], ["section", "Introduction"]], "content": "A relation triplet consists of two entities and a relation between them. We can find such triplets in a structured format in several publicly available knowledge bases (KBs) such as, Freebase~, DBpedia~, Wikidata~, etc. These triplets are very useful for many natural language processing tasks such as machine reading comprehension , machine translation , abstractive summarization , etc. However, building such knowledge bases is a daunting task. The aforementioned KBs are built by crowdsourcing, which may not be scalable. Although these KBs contain a large number of triplets, they remain incomplete. On the other hand, relation triplets can be automatically distilled from the copious amount of free text on the Web. This can be leveraged for identifying missing links in the existing KBs or build a KB from scratch without human intervention. \n\\begin{table*}[ht]\n\\small\n\\centering\n\\begin{tabular}{l|l|l}\n\\hline\n\\multicolumn{1}{c|}{Class} & \\multicolumn{1}{c|}{Sentence}                                                                                                                          & \\multicolumn{1}{c}{Triplets}                                                                                                                                                                                 \\\\ \\hline\nNEO                                  & \\begin{tabular}[c]{@{}l@{}}The original Joy of Cooking was published in 1931 \\\\by Irma Rombauer, a St. Louis housewife.\\end{tabular} & \\textless{}Irma Rombauer, St. Louis, place\\_lived\\textgreater{}                                                                                                                                      \\\\ \\hline\nEPO                                     & \\begin{tabular}[c]{@{}l@{}}Berlin is the capital of Germany.\\end{tabular}                                                                                                                     & \\begin{tabular}[c]{@{}l@{}}\\textless{}Germany, Berlin, capital\\textgreater\\\\ \\textless{}Germany, Berlin, contains\\textgreater\\\\ \\textless{}Berlin, Germany, country\\textgreater{}\\end{tabular} \\\\ \\hline\nSEO                                     & \\begin{tabular}[c]{@{}l@{}}Dr. C. V. Raman who was born in Chennai worked \\\\mostly in Kolkata.\\end{tabular}                                      & \\begin{tabular}[c]{@{}l@{}} \\textless{}Dr. C. V. Raman, Chennai, birth\\_place\\textgreater\\\\  \\textless{}Dr. C. V. Raman, Kolkata, place\\_lived\\textgreater{}\\end{tabular}                         \\\\ \\hline\n\\end{tabular}\n\\caption{Examples of different classes of overlapping relation triplets. This table is taken from \\newcite{nayak2020deep}.}\n\\label{tab:ov_classes}\n\\end{table*}\nThere are two distinct research paradigms of relation extraction: open information extraction (Open IE) and supervised relation extraction. \\newcite{banko2007open}, \\newcite{christensen2011srlie}, \\newcite{etzioni2011reverb}, and \\newcite{schmitz2012ollie} use open information extraction (Open IE) to extract relation triplets from sentences where relations set is open. Open IE systems like KnowItAll , TEXTRUNNER , REVERB , SRLIE , and OLLIE  use rule-based methods to extract entities from the noun phrases and relations from the verb phrases present in sentences. These systems can extract a large number of triplets of diverse relations from text within a reasonable time frame. These models extract any verb phrase in the sentences as a relation thus yielding too many uninformative triplets. Also, a relation can be expressed in sentences with different surface forms (lives\\_in relation can be expressed with `lives in', `stays', `settles', `lodges', `resident of', etc) and Open IE treats them as different relations which leads to duplication of triplets.\nThe problems of the Open IE can be addressed using supervised relation extraction. In supervised relation extraction, we consider a fixed set of relations, thus there is no need to do any normalization of the extracted relations. This approach requires a large parallel corpus of text and relation triplets for training. There are some annotated and some distantly supervised parallel corpus of (text, triplets) available publicly that can be used for training the models. Creating annotated corpus is difficult and time-consuming, so datasets created in this way are relatively smaller in size. On the other hand, the distant supervision approach can be exploited to create a large training corpus automatically, but these datasets contain a significant amount of noisy labels. These noisy labels in the distantly supervised datasets can affect the performance of the models in a negative way. Several feature-based models and deep neural network-based are proposed in the last decade for relation extraction. In this survey, we discuss these datasets and models in detail in the remaining part of the paper.\nPreviously, \\newcite{Cui2017ASO,Pawar2017RelationE,Kumar2017ASO,Shi2019ABS,Han2020MoreDM} presented survey of the research works in the relation extraction, but they mostly focused on pipeline-based relation extraction approaches at the sentence-level. Different from these survey papers, we extend the survey to document-level relation extraction and joint entity and relation extraction approaches. We also survey very recent research directions in this area such as zero-shot or few-shot relation extraction and noise mitigation in distantly supervised datasets. To the best of our knowledge, this is the first survey paper that covers so many different aspects of relation extraction in detail.\n\\begin{table*}[ht]\n\\small\n\\centering\n\\begin{tabular}{cccl}\n\\hline\n\\multicolumn{1}{c}{Relation} & \\textcolor{red}{Entity 1} & \\textcolor{blue}{Entity 2}                                    & \\multicolumn{1}{c}{Text}                                                                                                                                                                \\\\ \\hline\nacted\\_in                        & \\textcolor{red}{Meera Jasmine} & \\textcolor{blue}{Sootradharan} & \\begin{tabular}[c]{@{}l@{}}\\textcolor{red}{Meera Jasmine} made her debut in the \\\\Malayalam film ``\\textcolor{blue}{Soothradharan}\" .\\end{tabular}                                                                         \\\\ \\\\ \nlocated\\_in                    & \\textcolor{red}{Chakkarakadavu} & \\textcolor{blue}{Kerala}     & \\begin{tabular}[c]{@{}l@{}}\\textcolor{red}{Chakkarakadavu} is a small village to \\\\the east of the town of Cherai, on \\\\Vypin Island in Ernakulam district,\\\\  \\textcolor{blue}{Kerala}, India .\\end{tabular}             \\\\ \\\\\nbirth\\_place                      & \\textcolor{red}{Barack Obama} & \\textcolor{blue}{Hawaii}  & \\begin{tabular}[c]{@{}l@{}}\\textcolor{red}{Barack Obama} was born in \\textcolor{blue}{Hawaii} . \\end{tabular} \\\\ \\\\\nplays\\_for                       & \\textcolor{red}{Moussa Sylla} & \\textcolor{blue}{Horoya AC}   & \\begin{tabular}[c]{@{}l@{}}Fod√© \\textcolor{red}{Moussa Sylla} is a Guinean \\\\football player, who currently plays \\\\for \\textcolor{blue}{Horoya AC} .\\end{tabular}                      \\\\ \\\\\nowns                           & \\textcolor{red}{MTV Channel} & \\textcolor{blue}{Shakthi TV}    & \\begin{tabular}[c]{@{}l@{}}\\textcolor{red}{MTV Channel} (Pvt) Ltd is a Sri \\\\Lankan media company which owns \\\\three national television channels - \\\\\\textcolor{blue}{Shakthi TV}, Sirasa TV and TV 1 .\\end{tabular} \\\\ \\hline\n\\end{tabular}\n\\caption{Examples of relation triplets found in free texts. This table is taken from \\newcite{nayak2020deep}.}\n\\label{tab:text_triplet}\n\\end{table*}", "cites": [2369], "cite_extract_rate": 0.18181818181818182, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic overview of relation triplet extraction, introducing concepts like Open IE and supervised relation extraction, and briefly mentions the limitations of each. However, it does not synthesize ideas from the cited papers into a unified narrative, nor does it offer deep critical analysis or identify overarching patterns or frameworks in the research. The use of examples and citations is primarily for descriptive purposes."}}
{"id": "1b026c35-8640-4667-8784-04df1d0b54a8", "title": "Relation Extraction Datasets", "level": "section", "subsections": [], "parent_id": "ddfc697b-7da7-4ab2-9fff-861b778b8fa7", "prefix_titles": [["title", "Deep Neural Approaches to Relation Triplets Extraction: A~Comprehensive~Survey"], ["section", "Relation Extraction Datasets"]], "content": "\\begin{table*}[ht]\n\\small\n\\centering\n\\begin{tabular}{lccccc}\n\\hline\nDataset Name & Level & \\# Valid Relations & \\# Train & \\# Test & Manual Annotation \\\\ \\hline\nSemEval 2010 Task 8        & sentence         &  18           & 8,000         & 2,717        &  Yes                 \\\\ \nNYT10        & sentence         &  52           & 455,412         & 172,415        &  No                 \\\\ \nNYT11        & sentence         &  24           & 335,843         &  1,450       & Test                   \\\\ \nNYT29        & sentence         &  29           & 63,306         & 4,006        &  No                 \\\\ \nNYT24        & sentence         &  24           & 56,196         &  5,000       & No                   \\\\ \nWebNLG        & sentence         &  216           & 5,519         &  703       & Yes                   \\\\ \nACE05        & sentence         &  7           &  9,038        &  1,535       & Yes                   \\\\ \nCoNLL04      & sentence         &  5           &  1,153        &  288       &   Yes                \\\\ \nGDS          & sentence         &  4           &  13,161        & 5,663        &   Yes                \\\\ \nTACRED   & sentence         &  41           &  90,755        &  15,509       &   Yes                \\\\ \nFewRel 2.0       &  sentence        &  100           &  56,000        &  14,000       &  Yes                 \\\\ \nWikiReading   & document         &  884           &  14.85M        &  3.73M       & No                   \\\\ \nDocRED   & document        &  96           &  4,053        & 1,000        &  Yes                 \\\\ \\hline\n\\end{tabular}\n\\caption{The details of relation extraction datasets.}\n\\label{tab:re_datasets}\n\\end{table*}\nSeveral datasets are available for the relation extraction task.  proposed a shared task on relation extraction in SemEval 2010 and released a dataset with 8,000 training sentences and 2,717 test instances across nine relations including {\\em None}. The relations in this dataset are not taken from any knowledge base. They represent the relationship between two nominals in the sentences. Examples of such relations are {\\em Cause-Effect, Component-Whole,} etc.  mapped Freebase  triplets to Wikipedia articles to obtain a dataset.  (NYT10) and  (NYT11) mapped Freebase triplets to the New York Times (NYT) articles to obtain a similar dataset. These two datasets are used extensively by researchers for their experiments. They have $52$ and $24$ valid relations respectively. The training and test data in NYT10 are distantly supervised, whereas in NYT11, the test data is annotated and training data is distantly supervised. Recently, \\newcite{zhu2020nyth} created an annotated test dataset for the NYT10 dataset with a subset of its relations set. This annotated test set contains 22 relations. They used a binary strategy to annotate each instance either the distantly supervised relation is present or not in the sentences. But this test dataset does not include any {\\em None} samples which makes it unsuitable for the relation extraction task. ACE04  and ACE05  are two datasets containing 7 relations. These two datasets focus on both named entity recognition and relation extraction tasks. CoNLL04  and GDS  are two other datasets with $5$ and $4$ valid relations respectively. ACE04, ACE05, CoNLL04, and GDS datasets are manually annotated but they contain few relations in comparison to distantly supervised datasets. TACRED  is another dataset for relation extraction that has manually annotated training and test data. TACRED contains 41 relations similar to that of the distantly supervised datasets. So that makes this dataset very suitable for comparing models in this task. Automatic evaluation of the models can be carried out on this dataset easily. FewRel 2.0  is a few-shot relation extraction dataset. WebNLG  is another dataset that contains $216$ relations. Recently, this dataset has been used for joint entity and relation extraction. It is curated from the original WebNLG dataset of \\newcite{Gardent2017CreatingTC}. NYT24  and NYT29  are two other popular datasets for joint extraction task. These two datasets are curated from the NYT11 and NYT10 datasets respectively after removing the sentences that do not contain any valid relation triplets. These datasets are created at the sentence level. \nWikiReading  and DocRED  are two document-level relation extraction datasets created using Wikipedia articles and Wikidata items. WikiReading is a slot-filling dataset where a document of an entity and the name of a property (same as the relation) is given to the models as input to predict the second entity. This dataset does not have any {\\em None} instances. Each document in the dataset corresponds to one instance of training or testing. DocRED, on the other hand, is a relation extraction dataset. Training data contains 4,053 documents and test data contains 1,000 documents. Each document contains multiple instances and test data is blind. \\newcite{nayak2020deep} proposed an idea of extending the relation extraction task to multi-documents. They created a 2-hop relation extraction dataset from a multi-hop question answering dataset WikiHop  that contains more relations than the previous sentence-level or document-level datasets. Their idea can be extended to create an N-hop dataset to cover more relations. The details of these datasets are included in Table \\ref{tab:re_datasets}.", "cites": [5810, 1143, 5811, 5809, 8004, 3691], "cite_extract_rate": 0.4117647058823529, "origin_cites_number": 17, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of relation extraction datasets and lists their properties, but it lacks deeper synthesis of the cited works. While it mentions differences between manually annotated and distantly supervised datasets, it does not systematically connect these to broader trends or methodological implications. There is minimal critical analysis or abstraction to identify overarching patterns or principles."}}
{"id": "f42c788f-1e58-4df4-997e-10cf640e0e88", "title": "Pipeline Extraction Approaches", "level": "subsection", "subsections": ["e40be6d9-2831-4fb2-8263-4f498d921c5a", "63506046-4dc8-4ec6-8191-fe10ea9a0d20", "d8704e0a-aa3b-4cfa-a1c9-f6bbdac4ca39", "5537696f-6280-40da-a5fc-d444c896968e", "579683f3-17dc-4f9c-896d-6a51bfb889b4", "21a8e300-a2ff-4aaa-a905-d4bb8ea8382d"], "parent_id": "d9e3adfd-363a-404c-acc1-30f4be39560d", "prefix_titles": [["title", "Deep Neural Approaches to Relation Triplets Extraction: A~Comprehensive~Survey"], ["section", "Relation Extraction Models"], ["subsection", "Pipeline Extraction Approaches"]], "content": "At the beginning of relation extraction research, pipeline approaches were quite popular. A pipeline approach has two steps: (i) First, a named entity recognizer is used to identify the named entities in a text. (ii) Next, a classification model is used to find the relation between a pair of entities. The named entities identified in the first step are mapped to the KB entities. There are several state-of-the-art NER models available as proposed by \\newcite{Huang2015BidirectionalLM,Ma2016EndtoendSL,lample2016neural,chiu2016named} can be used for this purpose. Contextualized word embeddings based model such as ELMo , BERT , RoBERTa , and SpanBERT  can also be used for named entity recognition. In the next step, different classification models are proposed to find the relations between entity pairs and we describe them in detail in the following subsections.", "cites": [826, 5810, 2473], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section describes pipeline extraction approaches in a structured manner, mentioning key components and citing relevant papers for NER and relation classification. However, it lacks deeper synthesis of ideas across the cited works and does not offer critical evaluation or comparison. The content remains at a surface level, focusing on summarizing methods rather than identifying broader patterns or principles."}}
{"id": "63506046-4dc8-4ec6-8191-fe10ea9a0d20", "title": "CNN-Based Neural Models", "level": "subsubsection", "subsections": [], "parent_id": "f42c788f-1e58-4df4-997e-10cf640e0e88", "prefix_titles": [["title", "Deep Neural Approaches to Relation Triplets Extraction: A~Comprehensive~Survey"], ["section", "Relation Extraction Models"], ["subsection", "Pipeline Extraction Approaches"], ["subsubsection", "CNN-Based Neural Models"]], "content": "Distributed representations of words as word embeddings have transformed the way that natural language processing tasks like IE can be tackled. Word2Vec  and GloVe  are two sets of large and publicly available word embeddings that are used for many NLP tasks. Most neural network-based models for information extraction have used the distributed representation of words as their core component. The high dimensional distributed representation of words can encode important semantic information about words, which is very helpful for identifying the relations among the entities present in a sentence. Initially, neural models also follow the pipeline approach to solve this task.\n used a convolutional neural network for relation extraction. They used the pre-trained word embeddings of  to represent the tokens in a sentence and used two distance embedding vectors to represent the distance of each word from the two entities. They used a convolutional neural network (CNN) and max-pooling operation to extract a sentence-level feature vector. This sentence representation is passed to a feed-forward neural network with a softmax activation layer to classify the relation. \n\\begin{figure}[ht]\n\\centering\n\\includegraphics[scale=0.2]{img/pcnn.png}\n\\caption{The architecture of the PCNN model .}\n\\label{fig:pcnn}\n\\end{figure}\n introduced a piecewise convolutional neural network (PCNN) to improve relation extraction.  applied the max-pooling operation across the entire sentence to get the single important feature from the entire sentence for a particular convolutional filter. In PCNN, the max-pooling operation is not performed for the entire sentence. Instead, the sentence is divided into three segments: from the beginning to the argument appearing first in the sentence, from the argument appearing first in the sentence to the argument appearing second in the sentence, and from the argument appearing second in the sentence to the end of the sentence. Max-pooling is performed in each of these three segments and for each convolutional filter to obtain three feature values. A sentence-level feature vector is obtained by concatenating all such feature values and is given to a feed-forward neural network with a softmax activation layer to classify the relation.", "cites": [1684], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of CNN-based models for relation extraction, particularly the PCNN model, and briefly explains their architecture and method. It integrates a single cited paper and connects it to the general use of word embeddings and max-pooling strategies, but lacks deeper synthesis across multiple sources. There is minimal critical analysis or abstraction into broader principles or trends."}}
{"id": "d8704e0a-aa3b-4cfa-a1c9-f6bbdac4ca39", "title": "Attention-Based Neural Models", "level": "subsubsection", "subsections": [], "parent_id": "f42c788f-1e58-4df4-997e-10cf640e0e88", "prefix_titles": [["title", "Deep Neural Approaches to Relation Triplets Extraction: A~Comprehensive~Survey"], ["section", "Relation Extraction Models"], ["subsection", "Pipeline Extraction Approaches"], ["subsubsection", "Attention-Based Neural Models"]], "content": "Recently, attention networks have proven very useful for different NLP tasks. , \\newcite{wang2016relation}, , and  used word-level attention model for single-instance sentence-level relation extraction.  proposed a combination of a convolutional neural network model and an attention network. First, a convolution operation with max-pooling is used to extract the global features of the sentence. Next, attention is applied to the words of the sentence based on the two entities separately. The word embedding of the last token of an entity is concatenated with the embedding of every word. This concatenated representation is passed to a feed-forward layer with tanh activation and then another feed-forward layer with softmax to get a scalar attention score for every word of that entity. The word embeddings are averaged based on the attention scores to get the attentive feature vectors. The global feature vector and two attentive feature vectors for the two entities are concatenated and passed to a feed-forward layer with softmax to determine the relation.\n\\newcite{wang2016relation} used multi-level attention CNNs for this task. Their model achived very high F1 score on the SemEval 2010 Task 8 dataset. \\newcite{zhang2017position} proposed a position-aware attention mechanism over the LSTM sequences for this task. Earlier \\newcite{zeng2014relation} and \\newcite{zeng2015distant} use the position information as dense embedding in the network for feature extraction, whereas \\newcite{zhang2017position} used it in attention modeling for the same task.\n\\begin{figure}[ht]\n\\centering\n\\includegraphics[scale=0.2]{img/multilayer_att_cnn.png}\n\\caption{The architecture of the multi-level attention CNN model .}\n\\label{fig:multi_att_cnn}\n\\end{figure}\n used a bidirectional gated recurrent unit (Bi-GRU)  to capture the long-term dependency among the words in the sentence. The tokens vectors $\\mathbf{x}_t$ are passed to a Bi-GRU layer. The hidden vectors of the Bi-GRU layer are passed to a bi-linear operator which is a combination of two feed-forward layers with softmax to compute a scalar attention score for each word. The hidden vectors of the Bi-GRU layer are multiplied by their corresponding attention scores for scaling up the hidden vectors. A piecewise convolution neural network  is applied to the scaled hidden vectors to obtain the feature vector. This feature vector is passed to a feed-forward layer with softmax to determine the relation. \\newcite{nayak2019effective} used dependency distance based multi-focused attention model for this task. Dependency distance helps to identify the important words in the sentences and multi-factor attention helps to focus on multiple pieces of evidence for a relation. \\newcite{Bowen2019BeyondWA} used segment-level attention in their model rather than using traditional token-level attention for this task. \\newcite{Zhang2019MultilabeledRE} proposed an attention-based capsule network for relation extraction.\n have used attention model for multi-instance relation extraction. They applied attention over a bag of independent sentences containing two entities to extract the relation between them. First, CNN-based models are used to encode the sentences in a bag. Then a bi-linear attention layer is used to determine the importance of each sentence in the bag. This attention helps to mitigate the problem of noisy samples obtained by distant supervision to some extent. The idea is that clean sentences get higher attention scores over the noisy ones. The sentence vectors in the bag are merged in a weighted average fashion based on their attention scores. The weighted average vector of the sentences is passed to a feed-forward neural network with softmax to determine the relation. This bag-level attention is used only for positive relations and not used for {\\em None} relation. The reason is that the representations of the bags that express no relations are always diverse and it is difficult to calculate suitable weights for them.\n used intra-bag and inter-bag attention networks in a multi-instance setting for relation extraction. Their intra-bag attention is similar to the attention used by . Additionally, they used inter-bag attention to address the noisy bag problem. They divide the bags belonging to a relation into multiple groups. The attention score for each bag in a group is obtained based on the similarity of the bags to each other within the group. This inter-bag attention is used only during training as we do not know the relations during testing. Similarly \\newcite{Yuan2019CrossrelationCA} proposed a cross-relation and cross-bag attention for multi-instance relation extraction. \\newcite{Li2020SelfAttentionES} proposed an entity-aware embeddings and self-attention  enhanced PCNN model for relation extraction.", "cites": [303, 5809, 38], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of various attention-based neural models for relation extraction, integrating some ideas (e.g., position-aware attention, multi-level attention). However, it lacks a strong critical evaluation of these methods or their limitations. The level of abstraction is limited, as it mostly presents specific models rather than deriving broader principles or trends."}}
{"id": "5537696f-6280-40da-a5fc-d444c896968e", "title": "Dependency-Based Neural Models", "level": "subsubsection", "subsections": [], "parent_id": "f42c788f-1e58-4df4-997e-10cf640e0e88", "prefix_titles": [["title", "Deep Neural Approaches to Relation Triplets Extraction: A~Comprehensive~Survey"], ["section", "Relation Extraction Models"], ["subsection", "Pipeline Extraction Approaches"], ["subsubsection", "Dependency-Based Neural Models"]], "content": "Some previous works have incorporated the dependency structure information of sentences in their neural models for relation extraction.  used a long short-term memory network (LSTM)  along the shortest dependency path (SDP) between two entities to find the relation between them. Each token along the SDP is represented using four embeddings -- pre-trained word vector, POS tag embedding, embedding for the dependency relation between the token and its child in the SDP, and embedding for its WordNet  hypernym. They divide the SDP into two sub-paths: (i) The left SDP which goes from entity 1 to the common ancestor node (ii) The right SDP which goes from entity 2 to the common ancestor node. This common ancestor node is the lowest common ancestor between the two entities in the dependency tree. The token vectors along the left SDP and right SDP are passed to an LSTM layer separately. A pooling layer is applied to the hidden vectors to extract the feature vector from the left SDP and right SDP. These two vectors are concatenated and passed to a classifier to find the relation. \n exploited the shortest dependency path (SDP) between two entities and the sub-trees attached to that path (augmented dependency path) for relation extraction. Each token in the SDP is represented using its pre-trained embedding and its sub-tree representation. The sub-tree representation of a token is obtained from the sub-tree of the dependency tree where the token is the root node. The dependency relations are represented using trainable embeddings. Each node in the sub-tree of a token receives information from its children including the dependency relations. The sub-tree representation of the token is obtained by following the sub-tree rooted at the token from its leaf nodes to the root in a bottom-up fashion. Next, they use CNN with max-pooling on the vectors of the sequence of the tokens and dependency relations across the SDP. The output of the max-pooling operation is passed to a classifier to find the relation.\n\\begin{figure}[ht]\n\\centering\n\\includegraphics[scale=0.2]{img/treelstm.png}\n\\caption{The architecture of the relation extraction model using LSTMs on sequences and tree structures .}\n\\label{fig:treelstm}\n\\end{figure}\n used a tree LSTM network along the shortest dependency path (SDP) between two entities to find the relation between them. They used a bottom-up tree LSTM and top-down tree LSTM in their model. In the bottom-up tree LSTM, each node receives information from all of its children. The hidden representation of the root node of this bottom-up tree LSTM is used as the final output. In the top-down tree LSTM, each node receives the information from its parent node. The hidden representations of the head token of two entities are the final output of this tree LSTM. The representations of the bottom-up tree LSTM and top-down tree LSTM are concatenated and passed to a classifier to find the relation. They showed that using the SDP tree over the full dependency tree is helpful as unimportant tokens for the relation are ignored in the process. \\newcite{Veyseh2020ExploitingTS} proposed a ON-LSTM  based relation extraction model to preserve the syntax consistency in the model.", "cites": [7008, 8005, 5812], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the methods used in three papers for dependency-based neural models, focusing on their architectures and input representations. While it connects these works by highlighting the use of dependency paths and tree structures, it lacks deeper synthesis, critical evaluation, or abstraction to broader patterns or principles. The narrative remains at a methodological summary level without significant insight or comparative analysis."}}
{"id": "579683f3-17dc-4f9c-896d-6a51bfb889b4", "title": "Graph-Based Neural Models", "level": "subsubsection", "subsections": [], "parent_id": "f42c788f-1e58-4df4-997e-10cf640e0e88", "prefix_titles": [["title", "Deep Neural Approaches to Relation Triplets Extraction: A~Comprehensive~Survey"], ["section", "Relation Extraction Models"], ["subsection", "Pipeline Extraction Approaches"], ["subsubsection", "Graph-Based Neural Models"]], "content": "Graph-based models are popular for many NLP tasks as they work on non-linear structures.  proposed a graph-based model for cross-sentence relation extraction. They built a graph from the sentences where every word is considered as a node in the graph. Edges are created based on the adjacency of the words, dependency tree relations, and discourse relations. They extract all the paths from the graph starting from entity 1 to entity 2. Each path is represented by features such as lexical tokens, the lemma of the tokens, POS tags, etc. They use all the path features to find the relation between the two entities. \n and  used a similar graph for N-ary cross-sentence relation extraction. Rather than using explicit paths, they used an LSTM on a graph. A graph LSTM is a general structure for a linear LSTM or tree LSTM. If the graph contains only the word adjacency edges, then the graph LSTM becomes a linear LSTM. If the graph contains the edges from the dependency tree, it becomes a tree LSTM. A general graph structure may contain cycles. So  divides this graph into two directed acyclic graphs (DAG), where the forward DAG contains only the forward edges among the tokens and the backward DAG contains only the backward edges among the tokens. Each node has a separate forget gate for each of its neighbors. It receives information from the neighbors and updates its hidden states using LSTM equations . If we only consider the word adjacency edges, this graph LSTM becomes a bi-directional linear LSTM.  did not divide the graph into two DAGs, but directly used the graph structure to update the states of the nodes. At time step $t$, each node receives information from its neighbor from the previous time step and update its hidden states using LSTM equations. This process is repeated $k$ number of times where $k$ is a hyper-parameter.\n and  proposed a graph convolutional network (GCN) model which used simple linear transformations to update the node states, unlike the graph LSTMs used by  and .  gave equal weights to the edges, whereas  used an attention mechanism to assign different weights to the edges. , , and  used graph convolutional networks for sentence-level relation extraction. They considered each token in a sentence as a node in the graph and used the syntactic dependency tree to create a graph structure among the nodes.  used the GCN in a multi-instance setting. They used a Bi-GRU layer and a GCN layer over the full dependency tree of the sentences to encode them. The sentence representations in a bag were aggregated and passed to a classifier to find the relation. Following ,  used only the shortest dependency path (SDP) tree to build the adjacency matrix for the graph. Along with the SDP tree, they included the edges that are distance $K$ away from the SDP where $K$ is a hyper-parameter.  proposed a soft pruning strategy over the hard pruning strategy of  in their GCN model. They considered the full dependency tree to build the adjacency matrix but using a multi-head self attention-based soft pruning strategy, they can identify the important and unimportant edges in the graph. \\newcite{Mandya2020GraphCO} proposed GCN over multiple sub-graphs for this task. They created such sub-graphs based on the shortest dependency path between two entities and the tokens associated with the two entities.\n\\begin{figure}[ht]\n\\centering\n\\includegraphics[scale=0.18]{img/gcn_re.png}\n\\caption{The architecture of the attention guided graph convolutional network for relation extraction .}\n\\label{fig:gcn}\n\\end{figure}\n\\newcite{sahu2019inter,christopoulou2019connecting,Nan2020ReasoningWL} used GCN for document-level relation extraction. \\newcite{sahu2019inter} considered each token in a document as a node in a graph. They used syntactic dependency tree edges, word adjacency edges, and coreference edges to create the connections among the nodes. \\newcite{christopoulou2019connecting} considered the entity mentions, entities, and sentences in a document as nodes of a graph. They used rule-based heuristics to create the edges among these nodes. In their graph, each node and each edge were represented by vectors. GCN was used to update the vectors of nodes and edges. Finally, the edge vector between the two concerned entities was passed to a classifier to find the relation. \\newcite{Nan2020ReasoningWL} considered the entity mentions, entities, and tokens on the shortest dependency path between entity mentions as nodes in a graph. They used a structure induction module to learn the latent structure of the document-level graph. A multi-hop reasoning module was used to perform inference on the induced latent structure, where representations of the nodes were updated based on an information aggregation scheme. \\newcite{Zeng2020DoubleGB} proposed a graph aggregation and inference network for document-level relation extraction. They construct an entity-mention level graph to capture their interaction in the document and an entity-level graph to aggregate the mention-level information. \\newcite{Wang2020GlobaltoLocalNN} used a global graph similar to \\newcite{christopoulou2019connecting} to model the entities in a document and then used a multi-head attention network  to aggregate the information in the global graph. \\newcite{Zhou2020GlobalCG} proposed multi-head attention guided graph convolution network and \\newcite{Li2020GraphED} proposed GCN-based dual attention network for document level relation extraction.", "cites": [180, 5813, 38, 8008, 8005, 8956, 8007, 8006, 5814], "cite_extract_rate": 0.8181818181818182, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.0, "abstraction": 3.8}, "insight_level": "high", "analysis": "The section effectively synthesizes information from multiple graph-based relation extraction papers, connecting different models (e.g., graph LSTMs vs. GCNs) and their design choices (e.g., pruning strategies, attention mechanisms). It demonstrates abstraction by highlighting broader trends such as the use of syntactic and discourse structures, and the evolution from explicit path-based models to attention-guided graph convolutions. While it provides some critical perspectives (e.g., potential information loss in graph splitting), the critique could be more in-depth for a higher critical score."}}
{"id": "21a8e300-a2ff-4aaa-a905-d4bb8ea8382d", "title": "Contextualized Embedding-Based Neural Models", "level": "subsubsection", "subsections": [], "parent_id": "f42c788f-1e58-4df4-997e-10cf640e0e88", "prefix_titles": [["title", "Deep Neural Approaches to Relation Triplets Extraction: A~Comprehensive~Survey"], ["section", "Relation Extraction Models"], ["subsection", "Pipeline Extraction Approaches"], ["subsubsection", "Contextualized Embedding-Based Neural Models"]], "content": "Contextualized word embeddings such as ELMo , BERT , and SpanBERT  can be useful for relation extraction. These language models are trained on large corpora and can capture the contextual meaning of words in their vector representations. All neural models that are proposed for relation extraction use word representations such as Word2Vec  or GloVe  in their word embedding layer. Contextualized embeddings can be added in the embedding layer of the relation extraction models to improve their performance further. The SpanBERT model shows significant improvement in performance on the TACRED dataset.  replaced the entity 1 token with its type and SUBJ such as PER-SUBJ and entity 2 token with its type and OBJ such as LOC-OBJ in the sentences to train the model. Finally, they used a linear classifier on top of the CLS token vector to find the relation. \\newcite{soares2019matching} also proposed a BERT based model where they used special marker for entity 1 and entity 2 in the sentences. Then they used the vector of the start token of the entity 1 and entity 2 for relation classification. \n proposed two-step fine-tuning of BERT for document-level relation extraction on the DocRED dataset. In the first step, they used BERT to identify whether or not there is a relation between two entities. In the second step, they used BERT to classify the relation.  also used BERT in their model to show that it significantly improved the performance on the DocRED dataset compared to GloVe vectors.  used BERT to identify all possible relations among the entity pairs in documents in a single pass. They used entity types and special tokens to mark all the entity mentions in documents. All entity mentions of an entity received the same special token. Documents were passed to a pre-trained BERT model. An entity mention vector was obtained by averaging the BERT outputs of the entity mention tokens. An entity vector was obtained by averaging all the entity mention vectors of that entity. A bilinear classifier was used to classify the relation between two entities.  proposed a hierarchical inference network for document-level relation extraction. They also showed that using BERT in their model improved performance significantly. \n\\begin{table*}[ht]\n\\small\n\\centering\n\\begin{tabular}{l|ccc}\n\\hline\nModel     & Prec. & Rec.  & F1   \\\\ \\hline\nSDP-LSTM   & 66.3 & 52.7 & 58.7 \\\\ \nTree-LSTM  & 66.0 & 59.2 & 62.4 \\\\ \nGCN        & 69.8 & 59.0 & 64.0 \\\\ \nPA-LSTM    & 65.7 & 64.5 & 65.1 \\\\ \nAGGCN      & 69.9 & 60.9 & 65.1 \\\\ \nC-GCN      & 69.9 & 63.3 & 66.4 \\\\ \nGCN + PA-LSTM        & 71.7 & 63.0 & 67.1 \\\\ \nC-GCN + PA-LSTM      & 71.3 & 65.4 & 68.2 \\\\ \nC-AGGCN   & 73.1 & 64.2 & 69.0 \\\\ \\hline\nBERT    & 69.1 & 63.9 & 66.4 \\\\ \nBERT$_{EM}$  &   &  & 70.1 \\\\ \nSpanBERT  & 70.8 & 70.9 & 70.8 \\\\ \nBERT$_{EM}$ + MTB  &  &  & 71.5 \\\\ \\hline\n\\end{tabular}\n\\caption{Current State-of-the-art on TACRED dataset.}\n\\label{tab:tacred_sota}\n\\end{table*}\n\\begin{table}[ht]\n\\small\n\\centering\n\\begin{tabular}{l|c}\n\\hline\nModel     & F1   \\\\ \\hline\nSVM   & 82.2 \\\\ \nCNN   & 82.7 \\\\ \nPA-LSTM    & 82.7 \\\\ \nSDP-LSTM   & 83.7 \\\\\nSPTree  & 84.4 \\\\ \nC-GCN      & 84.8 \\\\ \nC-AGGCN   & 85.7 \\\\ \nAtt-Input-CNN  & 87.5 \\\\ \nAtt-Pooling-CNN  & 88.0 \\\\ \\hline\nBERT$_{EM}$  & 89.2 \\\\ \nBERT$_{EM}$ + MTB  & 89.5 \\\\ \\hline\n\\end{tabular}\n\\caption{Current State-of-the-art on SemEval 2010 Task 8 dataset.}\n\\label{tab:semeval_sota}\n\\end{table}\n\\begin{table*}[ht]\n\\small\n\\centering\n\\begin{tabular}{l|cccc}\n\\hline\nModel   & Prec.     & Rec.      & F1   & Entity Matching Type     \\\\ \\hline\nTagging  & 0.624     & 0.317     & 0.420  & P     \\\\\nCopyR   & 0.610     & 0.566     & 0.587 & P     \\\\\nGraphR   & 0.639 & 0.600 & 0.619 & P  \\\\\nCopyMTL$_{Mul}$  & 0.757 & 0.687 & 0.720 & E \\\\\nMrMep        & 0.779  & 0.766   & 0.771  & E    \\\\\nHRL        & 0.781    & 0.771    & 0.776  & E   \\\\\nETLSpan  & 0.855    & 0.717    & 0.780  & E   \\\\\nPNDec        & 0.806 & 0.773 & 0.789     & E    \\\\\nWDec        & 0.881 & 0.761 & 0.817     & E     \\\\\nCasRel$_{LSTM}$         & 0.842    & 0.830   &  0.836    & P  \\\\\nTPLinker$_{LSTM} $ & 0.860 & 0.820 & 0.840 & E  \\\\ \nRSAN  & 0.857    & 0.836    & 0.846  & E   \\\\ \nRIN  & 0.839 & 0.855 & 0.847 & E \\\\ \\hline\nCGT$_{BERT}$   & 0.947 & 0.842 & 0.891 & E \\\\\nCasRel$_{BERT}$         & 0.897    & 0.895   &  0.896    & P   \\\\ \nTPLinker$_{BERT} $ & 0.914 & 0.926 & 0.920 & E  \\\\ \nSPN$_{BERT}$  & 0.925 & 0.922 & 0.923 & E  \\\\ \\hline\n\\end{tabular}\n\\caption{Current state-of-the-art performance on NYT24 datasets for the joint extraction task. P=Partial entity matching, E=Exact entity matching.}\n\\label{tab:nyt24_joint_sota}\n\\end{table*}\n\\begin{table*}[ht]\n\\small\n\\centering\n\\begin{tabular}{l|cccc}\n\\hline\nModel    & Prec.      & Rec.      & F1   & Entity Matching Type  \\\\ \\hline\nTagging      & 0.593     & 0.381     & 0.464     & E        \\\\\nCopyR   &  0.569    &  0.452    &  0.504     &   P          \\\\\nSPTree    & 0.492 & 0.557 & 0.522   & E     \\\\\nHRL     & 0.692 & 0.601 & 0.643     & E   \\\\\nMrMep     & 0.717    & 0.635  & 0.673   & E    \\\\\nPNDec  & 0.732 & 0.624 & 0.673  & E \\\\ \nWDec  & 0.777 & 0.608 & 0.682     & E   \\\\ \\hline\n\\end{tabular}\n\\caption{Current state-of-the-art performance on NYT29 datasets for the joint extraction task.}\n\\label{tab:nyt29_joint_sota}\n\\end{table*}\n\\begin{table*}[ht]\n\\small\n\\centering\n\\begin{tabular}{l|cccc}\n\\hline\nModel   & Prec.     & Rec.      & F1     & Entity Matching Type     \\\\ \\hline\nTagging  & 0.525     & 0.193     & 0.283   & P    \\\\\nCopyR   & 0.377     & 0.364     & 0.371    & P  \\\\\nGraphR   & 0.447 & 0.411 & 0.429  & P  \\\\\nCopyMTL$_{One}$  & 0.578 & 0.601 & 0.589 & E \\\\\nHRL    & 0.695   & 0.629    &  0.660   & E   \\\\\nMrMep    & 0.694    & 0.770  &  0.730  & E    \\\\\nRIN  & 0.773 & 0.768 & 0.770 & E \\\\\nRSAN  & 0.805    & 0.838    & 0.821  & E \\\\\nETLSpan   & 0.843    & 0.820    & 0.831  & E \\\\\nCasRel$_{LSTM}$         & 0.869    & 0.806   &  0.837  & P \\\\ \nTPLinker$_{LSTM} $ & 0.919 & 0.816 & 0.864 & E \\\\ \\hline\nCGT$_{BERT}$   & 0.929 & 0.756 & 0.834 & E \\\\\nTPLinker$_{BERT} $ & 0.889 & 0.845 & 0.867 & E \\\\ \nCasRel$_{BERT}$         & 0.934    & 0.901   &  0.918  & P \\\\ \nSPN$_{BERT}$  & 0.931 & 0.936 & 0.934 & P  \\\\\\hline\n\\end{tabular}\n\\caption{Current state-of-the-art performance on WebNLG datasets for the joint extraction task.}\n\\label{tab:webnlg_joint_sota}\n\\end{table*}", "cites": [5821, 8009, 5811, 5819, 5810, 5818, 5816, 1684, 5813, 8005, 826, 5814, 8010, 5815, 5820, 5822, 2473, 5817, 8956], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 33, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of various contextualized embedding-based models for relation extraction, mentioning specific methods and their performance. It lists multiple papers and their contributions but lacks deeper synthesis of ideas, critical evaluation of their strengths and weaknesses, or abstraction to broader principles or trends in the field."}}
{"id": "bed17fdf-9ef5-43e5-9777-2e678069ff72", "title": "Noise Mitigation for Distantly Supervised Data", "level": "subsection", "subsections": [], "parent_id": "d9e3adfd-363a-404c-acc1-30f4be39560d", "prefix_titles": [["title", "Deep Neural Approaches to Relation Triplets Extraction: A~Comprehensive~Survey"], ["section", "Relation Extraction Models"], ["subsection", "Noise Mitigation for Distantly Supervised Data"]], "content": "The presence of noisy samples in distantly supervised data adversely affects the performance of models. Researchers have used different techniques in their models to mitigate the effects of noisy samples to make them more robust. Multi-instance relation extraction is one of the popular methods for noise mitigation. , , , , , , , and  used this multi-instance learning concept in their proposed relation extraction models. For each entity pair, they used all the sentences that contained these two entities to find the relation between them. Their goal was to reduce the effect of noisy samples using this multi-instance setting. They used different types of sentence selection mechanisms to give importance to the sentences that contained relation-specific keywords and ignored the noisy sentences.  and  used the multi-task learning approach for mitigating the influence of noisy samples. They used fine-grained entity typing as an extra task in their model.\n\\begin{figure}[ht]\n\\centering\n\\includegraphics[scale=0.4]{img/bag_att.png}\n\\caption{The architecture of the attention over sentences model for bag-level relation extraction .}\n\\label{fig:bag_att}\n\\end{figure}\n used an adversarial training approach for the same purpose. They added noise to the word embeddings to make the model more robust for distantly supervised training.  used a generative adversarial network (GAN) to address the issue of noisy samples in relation extraction. They used a separate binary classifier as a generator in their model for each positive relation class to identify the true positives for that relation and filter out the noisy ones.  used reinforcement learning to identify the noisy samples for the positive relation classes. \\newcite{jia2019arnor} proposed an attention-based regularization mechanism to address the noisy samples issue in distantly supervised relation extraction. They used the attention to identify the relation patterns in the sentences and sentences which do not contain such patterns are considered as noisy samples.  used reinforcement learning to identify the noisy samples for the positive relations and then used the identified noisy samples as unlabeled data in their model.  used a clustering approach to identify the noisy samples. They assigned the correct relation label to these noisy samples and used them as additional training data in their model.", "cites": [8011, 5824, 5825, 5823, 8007], "cite_extract_rate": 0.38461538461538464, "origin_cites_number": 13, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of various noise mitigation techniques for distantly supervised relation extraction, mentioning methods like multi-instance learning, multi-task learning, adversarial training, GANs, and reinforcement learning. However, it lacks deeper synthesis of these methods into a coherent framework and does not critically analyze or compare their strengths and weaknesses. There is minimal abstraction beyond the specific techniques described."}}
{"id": "e8f060ff-f9f7-4975-955f-c1aadd332d8a", "title": "Zero-Shot and Few-Shot Relation Extraction", "level": "subsection", "subsections": [], "parent_id": "d9e3adfd-363a-404c-acc1-30f4be39560d", "prefix_titles": [["title", "Deep Neural Approaches to Relation Triplets Extraction: A~Comprehensive~Survey"], ["section", "Relation Extraction Models"], ["subsection", "Zero-Shot and Few-Shot Relation Extraction"]], "content": "Distantly supervised datasets cover a small subset of relations from the KBs. Existing KBs such as Freebase, Wikidata, and DBpedia contain thousands of relations. Due to the mismatch of the surface form of entities in KBs and texts, distant supervision cannot find adequate training samples for most relations in KBs. It means that distantly supervised models cannot fill the missing links belonging to these uncovered relations. Zero-shot or few-shot relation extraction can address this problem. These models can be trained on a set of relations and can be used for inferring another set of relations. \n and  converted the relation extraction task to a question-answering task and used the reading comprehension approach for zero-shot relation extraction. In this approach, entity 1 and the relation are used as questions, and entity 2 is the answer to the question. If entity 2 does not exist, the answer is {\\em NIL}.  used the BiDAF model  with an additional {\\em NIL} node in the output layer for this task on the WikiReading  dataset with additional negative samples. They used a set of relations during training and another set of relations during testing.  used templates to create the question using entity 1 and the relation. They modified the machine-reading comprehension models to a sequence tagging model so that they can find multiple answers to a question. Although they did not experiment with the zero-shot scenario, this approach can be used for zero-shot relation extraction too. FewRel 2.0  is a dataset for few-shot relation extraction. In few-shot relation extraction, training and test relations are different just like zero-shot extraction. But during testing, a few examples of the test relations are provided to the model for better prediction.", "cites": [459, 1686, 1139, 5810], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from multiple papers, connecting the theme of zero-shot and few-shot relation extraction to the broader issue of limited training samples in distantly supervised datasets. While it provides an analytical view by showing how these methods convert relation extraction to QA and sequence tagging, the critical evaluation is limited to pointing out that one paper did not experiment in the zero-shot scenario. Abstraction is moderate, as it generalizes the task transformation approach but does not offer deep meta-level insights."}}
{"id": "1512c689-d89a-4ef0-8df5-ee8a30c4aa50", "title": "Joint Extraction Approaches", "level": "subsection", "subsections": [], "parent_id": "d9e3adfd-363a-404c-acc1-30f4be39560d", "prefix_titles": [["title", "Deep Neural Approaches to Relation Triplets Extraction: A~Comprehensive~Survey"], ["section", "Relation Extraction Models"], ["subsection", "Joint Extraction Approaches"]], "content": "All the previously mentioned works on relation extraction assume that entities are already identified by a named entity recognition system. They classify the relation between two given entities at the sentence level or the bag-of-sentences level. These models depend on an external named entity recognition system to identify the entities in a text. Recently, some researchers  tried to remove this dependency. They tried to bring the entity recognition and relation identification tasks closer by sharing their parameters and optimizing them together. They first identify all the entities in a sentence and then find the relation among all the pairs of identified entities. Although they identify the entities and relations in the same network, they still identify the entities first and then determine the relation among all possible pairs in the same network. So these models miss the interaction among the relation triplets present in a sentence. These approaches resemble the pipeline approach to some extent.\n first proposed a truly joint extraction model for this task. They used a sequence tagging scheme to jointly extract the entities and relations. They created a set of tags derived from the Cartesian product of entity tags and relation tags. These new tags can encode the entity information and relation information together. But this strategy does not work when entities are shared among multiple triplets, as only one tag can be assigned to a token.  proposed an encoder-decoder model with a copy mechanism to extract relation triplets with overlapping entities. Their model has a copy network to copy the last token of two entities from the source sentence and a classification network to classify the relation between copied tokens. Their model cannot extract the full entity names of the triplets. Their best performing model uses a separate decoder to extract each triplet. During training, they need to fix the maximum number of decoders and during inference, their model can only extract up to that fixed number of triplets. Also, due to the use of separate decoders for each triplet, their model misses the interaction among the triplets. \n\\begin{figure}[ht]\n\\centering\n\\includegraphics[scale=0.3]{img/ptrnet.jpg}\n\\caption{The architecture of the joint entity and relation extraction model as proposed in \\newcite{nayak2019ptrnetdecoding}.}\n\\label{fig:ptrnet}\n\\end{figure}\n\\begin{figure*}[ht]\n\\centering\n\\includegraphics[scale=0.45]{img/chart.png}\n\\caption{The statistics of the research articles published in year 2019 (CoNLL, ACL, EMNLP, AAAI, IJCAI) and 2020 (COLING, ACL, EMNLP, AAAI, IJCAI). The left one shows the pipeline vs joint extraction models, the middle one shows the sentence-level vs document-level extraction models, and the right one shows the use of distantly supervised datasets vs annotated datasets.}\n\\label{fig:chart}\n\\end{figure*}\n proposed a hierarchical reinforcement learning-based (RL) deep neural model for joint entity and relation extraction. A high-level RL is used to identify the relation based on the relation-specific tokens in the sentences. After a relation is identified, a low-level RL is used to extract the two entities associated with the relation using a sequence labeling approach. This process is repeated multiple times to extract all the relation triplets present in the sentences. A special {\\em None} relation is used to identify no relation situation in the sentences. Entities extracted associated with the {\\em None} relations are ignored.  used a graph convolutional network (GCN) where they treated each token in a sentence as a node in a graph and edges were considered as relations.  used an N-gram attention mechanism with an encoder-decoder model for the completion of knowledge bases using distantly supervised data.  used the encoder-decoder framework for this task where they used a CNN-based multi-label classifier to find all the relations first, then used multi-head attention  to extract the entities corresponding to each relation. \\newcite{nayak2019ptrnetdecoding} used encoder-decoder network for this joint extraction task. They proposed a word-level decoding framework and a pointer network-based decoding framework for the same. \nCopyMTL model  was proposed to address the issues of CopyR  model. CopyR model can only extract the last token of the entities, whereas CopyMTL model used a sequence tagging approach to extract the full entity names.  decomposed the joint extraction task into two sub-tasks: (i) head entity extraction (ii) tail entity and relation extraction. They used a sequence tagging approach to solve these two sub-tasks. Similarly,  proposed a sequence tagging approach for this task. They first identified the head entities and then for each head entity and each relation, they identified the tail entities using a sequence tagging approach. They used pre-trained BERT  in their model to improve the performance. \\newcite{Yuan2020ARA} used a relation-specific attention mechanism with sequence labeling to jointly extract the entities and relations. \\newcite{wang2020tplinker} proposed a single-stage joint extraction model using entity-pair linking. They aligned the sentence tokens using the Cartesian product so that the boundary tokens of the subject and object entities are aligned. Then they used a classifier to tag each token-pair as entity head, entity tail, subject head, subject tail, object head, and object tail for each relation separately. This scheme can identity the multiple triplets with overlapping entities easily. \\newcite{sui2020jointea} proposed a bipartite matching loss in the encoder-decoder network which considers the group of relation triplets as a set, not as a sequence. \\newcite{ye2020contrastive} transformer-based generative model for this task. They used negative triplets to train the transformer model in contrastive settings. \\newcite{Wang2020TwoAB} proposed a table-sequence encoder model where the sequence encoder captures the entity-related information and the table encoder captures the relation-specific information. \\newcite{Sun2020RecurrentIN} proposed a recurrent multi-task learning architecture to explicitly capture the interaction between entity recognition task and relation classification task. \\newcite{Ji2020SpanbasedJE} proposed a span-based multi-head attention network for joint extraction task. Each text span is a candidate entity and each text span pairs is a candidate for relation triplets.", "cites": [5817, 5818, 5820, 5811, 5819, 8005, 167, 5810, 38], "cite_extract_rate": 0.6, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers to present a coherent narrative on joint extraction approaches, highlighting different strategies like sequence tagging, reinforcement learning, and attention mechanisms. It critically evaluates the limitations of various models, such as missing triplet interactions or constraints on the number of triplets extracted. The section abstracts these methods into broader patterns, such as the evolution from pipeline to joint modeling and the challenges of overlapping entities, providing a high-level analytical perspective."}}
{"id": "795c5582-1575-480a-966e-88577c1a06c1", "title": "Current State-of-the-art \\& Trends", "level": "section", "subsections": [], "parent_id": "ddfc697b-7da7-4ab2-9fff-861b778b8fa7", "prefix_titles": [["title", "Deep Neural Approaches to Relation Triplets Extraction: A~Comprehensive~Survey"], ["section", "Current State-of-the-art \\& Trends"]], "content": "NYT10 is the most popular dataset for experiments in pipeline-based relation extraction. Since the test dataset of NYT10 is not manually annotated, researchers mostly report a precision-recall curve to compare the models . TACRED and SemEval 2010 Task 8 datasets are manually annotated and can be used for automatic evaluation. We have included the current state-of-the-art on these two dataset in Table \\ref{tab:tacred_sota} and Table \\ref{tab:semeval_sota}. DocRED\\footnote{https://competitions.codalab.org/competitions/20717} and FewRel\\footnote{https://thunlp.github.io/2/fewrel2\\_nota.html} datasets have manually annotated testset and their they have a leaderboard where current state-of-the-art can be found. For the joint extraction task researchers used NYT24, NYT29, and WebNLG datasets which have a considerably large number of relations. We have included the current state-of-the-art performance of the models on NYT24, NYT29, and WebNLG datasets in Table \\ref{tab:nyt24_joint_sota}, Table \\ref{tab:nyt29_joint_sota}, and Table \\ref{tab:webnlg_joint_sota} respectively.\n\\begin{figure}[ht!]\n\\centering\n\\includegraphics[scale=0.5]{img/yearly_trend.png}\n\\caption{Publication trend of relation extraction research at ACL, EMNLP, AAAI, and IJCAI in 2016--2020.}\n\\label{fig:5year}\n\\end{figure}\nWe analyze the research articles published in 2019 (CoNLL, ACL, EMNLP, AAAI, IJCAI) and 2020 (COLING, ACL, EMNLP, AAAI, IJCAI) and include statistics in Figure \\ref{fig:chart}. We see that majority of the research focuses on pipeline-based approaches on sentence-level relation extraction. We also see that the use of distantly supervised datasets and annotated datasets for experiments is evenly distributed among the published articles. We also show the increasing trends of yearly publications in relation extraction in Figure \\ref{fig:5year} over the last 5 years period (2016-2020).", "cites": [8007], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily provides a descriptive overview of datasets and trends in relation extraction research without deeply synthesizing or connecting insights from the cited paper. It mentions the RESIDE paper briefly but does not integrate its ideas or methodologies into a broader narrative. There is minimal critical evaluation or abstraction beyond specific facts and figures."}}
