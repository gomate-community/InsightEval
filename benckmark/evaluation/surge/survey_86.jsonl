{"id": "48308b43-2cfe-4236-84fb-c7cd67559d21", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "456ca844-322a-4fcf-90ea-531b14db9d9e", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Introduction"]], "content": "\\label{sec:intro}\nTransformer~ is a prominent deep learning model that has been widely adopted in various fields, such as natural language processing (NLP), computer vision (CV) and speech processing. Transformer was originally proposed as a sequence-to-sequence model~ for machine translation. Later works show that Transformer-based pre-trained models (PTMs)~ can achieve \\textit{state-of-the-art} performances on various tasks. As a consequence, Transformer has become the go-to architecture in NLP, especially for PTMs. In addition to language related applications, Transformer has also been adopted in CV~, audio processing~ and even other disciplines, such as chemistry~ and life sciences~.\nDue to the success, a variety of Transformer variants (a.k.a. X-formers) have been proposed over the past few years.\nThese X-formers improve the vanilla Transformer from different perspectives.\n\\begin{enumerate}\n  \\item \\textit{Model Efficiency}. A key challenge of applying Transformer is its inefficiency at processing long sequences mainly due to the computation and memory complexity of the self-attention module.\nThe improvement methods include lightweight attention (e.g. sparse attention variants) and Divide-and-conquer methods (e.g., recurrent and hierarchical mechanism).\n  \\item \\textit{Model Generalization}. Since the transformer is a flexible architecture and makes few assumptions on the structural bias of input data, it is hard to train on small-scale data. The improvement methods include introducing structural bias or regularization, pre-training on large-scale unlabeled data, etc.\n  \\item \\textit{Model Adaptation}. This line of work aims to adapt the Transformer to specific downstream tasks and applications.\n\\end{enumerate}\nIn this survey, we aim to provide a comprehensive review of the Transformer and its variants. Although we can organize X-formers on the basis of the perspectives mentioned above, many existing X-formers may address one or several issues. For example, sparse attention variants not only reduce the computational complexity but also introduce structural prior on input data to alleviate the overfitting problem on small datasets. Therefore, it is more methodical to categorize the various existing X-formers and propose a new taxonomy mainly according to their ways to improve the vanilla Transformer: architecture modification, pre-training, and applications.\nConsidering the audience of this survey may be from different domains, we mainly focus on the general architecture variants and just briefly discuss the specific variants on pre-training and applications.\nThe rest of the survey is organized as follows. Sec.~\\ref{sec:background} introduces the architecture and the key components of Transformer. Sec.~\\ref{sec:taxonomy} clarifies the categorization of Transformer variants. Sec.~\\ref{sec:attention}$\\sim$\\ref{sec:other_module} review the module-level modifications, including attention module, position encoding, layer normalization and feed-forward layer. Sec.~\\ref{sec:beyond} reviews the architecture-level variants. Sec.~\\ref{sec:ptm} introduces some of the representative Transformer-based PTMs. Sec.~\\ref{sec:app} introduces the application of Transformer to various different fields. Sec.~\\ref{sec:discussion} discusses some aspects of Transformer that researchers might find intriguing and summarizes the paper.", "cites": [7360, 1447, 1445, 7052, 2401, 732, 1446, 38], "cite_extract_rate": 0.7272727272727273, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a reasonable synthesis by connecting the cited works under the broader themes of model efficiency, generalization, and adaptation, and uses these to justify the proposed taxonomy. It abstracts some common challenges and improvement strategies across domains. However, the critical analysis is limited, as it does not deeply evaluate or contrast the approaches or point out limitations of the cited papers."}}
{"id": "cbbc7151-fd9c-4d11-a9dd-a0790a09698d", "title": "Vanilla Transformer", "level": "subsection", "subsections": ["625a964c-dc0c-4228-a0bd-e5fba16df1f2", "fdfb7e39-e1e9-4079-9037-b8bc17e8bb70", "0c6acb39-f7c8-4d89-9969-16b6e42547a1", "64d291c3-febe-419e-831a-4ec33ca28ce7"], "parent_id": "07bae4f8-4d29-4fbd-aec3-ca99cfb7bc62", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Background"], ["subsection", "Vanilla Transformer"]], "content": "\\label{sec:vanilla_xformer}\nThe vanilla Transformer~ is a sequence-to-sequence model and consists of an encoder and a decoder, each of which is a stack of $L$ identical blocks. Each \\textit{encoder block} is mainly composed of a multi-head self-attention module and a position-wise feed-forward network (FFN).\nFor building a deeper model, a residual connection~ is employed around each module, followed by Layer Normalization~ module.\nCompared to the encoder blocks, decoder blocks additionally insert cross-attention modules between the multi-head self-attention modules and the position-wise FFNs. Furthermore, the self-attention modules in the decoder are adapted to prevent each position from attending to subsequent positions.\nThe overall architecture of the vanilla Transformer is shown in Fig. \\ref{fig:xformer_arch}.\n\\begin{figure}[htbp]\n    \\centering\n    \\includegraphics[width=0.7\\linewidth]{assets/xformer.pdf}\n    \\caption{Overview of vanilla Transformer architecture}\n    \\label{fig:xformer_arch}\n\\end{figure}\nIn the following subsection, we shall introduce the key modules of the vanilla Transformer.", "cites": [97, 57, 38], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a factual overview of the vanilla Transformer architecture, citing key papers for the attention mechanism, residual connections, and layer normalization. However, it lacks synthesis and critical analysis, merely describing components without connecting their significance or limitations. There is minimal abstraction beyond the specific components."}}
{"id": "0c6acb39-f7c8-4d89-9969-16b6e42547a1", "title": "Residual Connection and Normalization", "level": "subsubsection", "subsections": [], "parent_id": "cbbc7151-fd9c-4d11-a9dd-a0790a09698d", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Background"], ["subsection", "Vanilla Transformer"], ["subsubsection", "Residual Connection and Normalization"]], "content": "In order to build a deep model, Transformer employs a residual connection~ around each module, followed by Layer Normalization~. For instance, each Transformer encoder block may be written as\n\\begin{align}\n    \\bH'&=\\mathrm{LayerNorm}(\\mathrm{SelfAttention}(\\bX)+\\bX)\\\\\n    \\bH&=\\mathrm{LayerNorm}(\\mathrm{FFN}(\\bH')+\\bH'),\n\\end{align}\nwhere $\\mathrm{SelfAttention}(\\cdot)$ denotes self attention module and $\\mathrm{LayerNorm}(\\cdot)$ denotes the layer normalization operation.", "cites": [97, 57], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of residual connections and layer normalization in the Transformer architecture, citing relevant papers but without integrating or connecting their broader implications. There is minimal critical evaluation or abstraction beyond the specific components of the vanilla Transformer."}}
{"id": "3ddd6e3c-d4a1-4bce-b510-16dfc75582ed", "title": "Model Analysis", "level": "subsection", "subsections": [], "parent_id": "07bae4f8-4d29-4fbd-aec3-ca99cfb7bc62", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Background"], ["subsection", "Model Analysis"]], "content": "\\label{sec:model-analysis}\nTo illustrate the computation time and parameter requirements of the Transformer, we analyze the two core components of the Transformer (i.e., the self-attention module and the position-wise FFN) in Table \\ref{tab:complexity_attn_ffn}. We assume that the hidden dimension $D_m$ of the model is $D$, and that the input sequence length is $T$. The intermediate dimension of FFN is set to $4D$ and the dimension of keys and values are set to $D/H$ as in .\n\\begin{table}[htbp]\n    \\caption{Complexity and parameter counts of self-attention and position-wise FFN}\n    \\label{tab:complexity_attn_ffn}\n    \\centering\n    \\begin{tabular}{c|c|c}\n    \\hline\n     Module & Complexity & \\#Parameters\\\\\n    \\hline\n     self-attention & $\\mathcal O(T^2\\cdot D)$ & $4D^2$\\\\\n    \\hline\n     position-wise FFN & $\\mathcal O(T\\cdot D^2)$ & $8D^2$\\\\\n    \\hline\n    \\end{tabular}\n\\end{table}\nWhen the input sequences are short, the hidden dimension $D$ dominates the complexity of self-attention and position-wise FFN. The bottleneck of Transformer thus lies in FFN. However, as the input sequences grow longer, the sequence length $T$ gradually dominates the complexity of these modules, in which case self-attention becomes the bottleneck of Transformer. Furthermore, the computation of self-attention requires that a $T\\times T$ attention distribution matrix is stored, which makes the computation of Transformer infeasible for long-sequence scenarios (e.g., long text documents and pixel-level modeling of high-resolution images). One shall see that the goal of increasing the efficiency of Transformer generally leads to the long-sequence compatibility of self-attention, as well as the computation and parameter efficiency of position-wise FFN for ordinary settings.", "cites": [38], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the computational properties of the Transformer model, integrating key components (self-attention and FFN) and their complexity in a structured manner. It synthesizes foundational information from the original paper to explain how the model scales with input length. While it does not deeply critique the cited work or compare multiple approaches, it does generalize some patterns regarding efficiency bottlenecks and broader implications for long-sequence modeling."}}
{"id": "037a9c67-64a3-4bbe-b197-a41922561a14", "title": "Analysis of Self-Attention", "level": "subsubsection", "subsections": [], "parent_id": "f0fa958a-db02-433c-858f-f7024645f38d", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Background"], ["subsection", "Comparing Transformer to Other Network Types"], ["subsubsection", "Analysis of Self-Attention"]], "content": "As a central piece of Transformer, self-attention comes with a flexible mechanism to deal with variable-length inputs. It can be understood as a fully connected layer where the weights are dynamically generated from pairwise relations from inputs. Table \\ref{tab:op_complexities} compares the complexity, sequential operations, and maximum path length\\footnote{The maximum length of the paths forward and backward signals have to traverse to get from any input position to arbitrary output position. Shorter length implies a better potential for learning long-range dependencies.} of self-attention with three commonly used layer types. We summarize the advantages of self-attention as follows:\n\\begin{enumerate}\n    \\item It has the same maximum path length as fully connected layers, making it suitable for long-range dependencies modeling. Compared to fully connected layers, it is more parameter-efficient and more flexible in handling variable-length inputs.\n    \\item Due to the limited receptive field of convolutional layers, one typically needs to stack a deep network to have a global receptive field. On the other hand, the constant maximum path length enables self-attention to model long-range dependencies with a constant number of layers.\n    \\item The constant sequential operations and maximum path length make self-attention more parallelizable and better at long-range modeling than recurrent layers.\n\\end{enumerate}\n\\begin{table}[ht]\n    \\caption{Per-layer complexity, minimum number of sequential operations and maximum path lengths for different layer types. $T$ is the sequence length, $D$ is the representation dimension and $K$ is the kernel size of convolutions~.}\n\\label{tab:op_complexities}\n\\begin{center}\n\\vspace{-1mm}\n\\begin{tabular}{lccc}\n\\toprule\nLayer Type & Complexity & Sequential & Maximum Path Length  \\\\\n           &per Layer         & Operations &   \\\\\n\\hline\n\\rule{0pt}{2.0ex}Self-Attention & $\\mathcal O(T^2 \\cdot D)$ & $\\mathcal O(1)$ & $\\mathcal O(1)$ \\\\\nFully Connected & $\\mathcal O(T^2 \\cdot D^2)$ & $\\mathcal O(1)$ & $\\mathcal O(1)$ \\\\\nConvolutional & $\\mathcal O(K \\cdot T \\cdot D^2)$ & $\\mathcal O(1)$ & $\\mathcal O(\\log_K(T))$ \\\\\nRecurrent & $\\mathcal O(T \\cdot D^2)$ & $\\mathcal O(T)$ & $\\mathcal O(T)$ \\\\\n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\end{table}", "cites": [38], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear comparative overview of self-attention against other layer types, using a well-structured table and bullet points. It synthesizes the core idea of self-attention from the cited paper and links it to broader network types, but lacks deeper integration of multiple sources. While it highlights strengths of self-attention, it does not critically evaluate its limitations or compare with alternative mechanisms in detail."}}
{"id": "cf87fcdd-fc72-4f50-9754-c59538a65d3f", "title": "In Terms of Inductive Bias", "level": "subsubsection", "subsections": [], "parent_id": "f0fa958a-db02-433c-858f-f7024645f38d", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Background"], ["subsection", "Comparing Transformer to Other Network Types"], ["subsubsection", "In Terms of Inductive Bias"]], "content": "Transformer is often compared against convolutional and recurrent networks. Convolutional networks are known to impose the inductive biases of translation invariance and locality with shared local kernel functions. Similarly, recurrent networks carry the inductive biases of temporal invariance and locality via their Markovian structure~. On the other hand, the Transformer architecture makes few assumptions about structural information of data. This makes Transformer a universal and flexible architecture. As a side effect, the lack of structural bias makes Transformer prone to overfitting for small-scale data.\nAnother closely related network type is Graph Neural Networks (GNNs) with message passing~. Transformer can be viewed as a GNN defined over a complete directed graph (with self-loop) where each input is a node in the graph. The key difference between Transformer and GNNs is that Transformer introduces no prior knowledge over how input data are structured — the message passing process in Transformer solely depends on similarity measures over the content.", "cites": [208, 553], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from the cited papers regarding inductive biases in neural networks and provides a comparative perspective between Transformers, CNNs, RNNs, and GNNs. It abstracts some general principles, such as the lack of structural bias in Transformers, but the critical analysis is limited to stating that this lack of bias leads to overfitting on small data and does not evaluate the cited works in depth."}}
{"id": "aa12aeb0-6a41-4c9b-bfb0-68b4522245cd", "title": "Taxonomy of Transformers", "level": "section", "subsections": [], "parent_id": "456ca844-322a-4fcf-90ea-531b14db9d9e", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Taxonomy of Transformers"]], "content": "\\label{sec:taxonomy}\nA wide variety of models have been proposed so far based on the vanilla Transformer from three perspectives: types of architecture modification, pre-training methods, and applications.\nFig. \\ref{fig:xformer_taxonomy} gives an illustrations of our categorization of Transformer variants.\n\\begin{figure}[htbp]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{assets/taxonomy.pdf}\n    \\caption{Categorization of Transformer variants.}\n    \\label{fig:xformer_taxonomy}\n\\end{figure}\nFig. \\ref{taxonomy_of_xformer} illustrates our taxonomy and some representative models.\n\\tikzstyle{leaf}=[mybox,minimum height=1em,\nfill=hidden-orange!40, text width=20em,  text=black,align=left,font=\\tiny,\ninner xsep=2pt,\ninner ysep=1pt,\n]\n\\begin{figure*}[tp]\n  \\centering\n\\begin{forest}\n  forked edges,\n  for tree={\n  grow=east,\n  reversed=true,\n  anchor=base west,\n  parent anchor=east,\n  child anchor=west,\n  base=left,\n  font=\\small,\n  rectangle,\n  draw=hiddendraw,\n  rounded corners,align=left,\n  minimum width=2.5em,\ns sep=3pt,\ninner xsep=2pt,\ninner ysep=1pt,\nver/.style={rotate=90, child anchor=north, parent anchor=south, anchor=center},\n  },\n  where level=1{text width=2.7em,font=\\scriptsize,}{},\n  where level=2{text width=3em,font=\\tiny}{},\n  where level=3{text width=3em,font=\\tiny}{},\n  [X-formers, ver\n    [Module\\\\Level\n        [Attention\n           [Sparse\n            [Star-Transformer{,} Longformer{,} ETC{,} BigBird{,} Sparse Transformer\\\\\n            BP-Transformer{,} Image Transformer{,} Axial Transformer\n            ,leaf,text width=21.5em]\n            [Routing Transformer{,} Reformer{,} SAC{,} Sparse Sinkhorn Attention\n            ,leaf,text width=21.5em]\n           ]\n           [Linearized\n            [Linear Transformer{,} Performer{,} RFA{,} Delta Net\n            ,leaf,text width=18.5em]\n           ]\n           [Prototype\n            [Clustered Attention{,} Informer\n            ,leaf,text width=12em]\n           ]\n           [Memory\\\\Compress\n            [MCA{,} Set Transformer{,} Linformer\n            ,leaf,text width=12em]\n           ]\n           [Low-rank\n            [Low-rank Attention{,} CSALR{,} Nystr{\\\"{o}}mformer~\n            ,leaf,text width=15em]\n           ]\n           [Prior\\\\Attention\n            [Local Transformer{,} Gaussian Transformer\n            ,leaf,text width=15em]\n            [Predictive Attention Transformer{,} Realformer{,} Lazyformer\n            ,leaf,text width=18.5em]\n            [CAMTL\n            ,leaf,text width=4em]\n            [Average Attention{,} Hard-Coded Gaussian Attention{,} Synthesizer\n            ,leaf]\n           ]\n           [Multi-head\n            [{,} {,} Talking-head Attention\\\\\n            Collaborative MHA\n            ,leaf,text width=20em]\n            [Adaptive Attention Span{,} Multi-Scale Transformer\n            ,leaf,text width=20em]\n            [Dynamic Routing\n            ,leaf,text width=6.5em]\n           ]\n        ]\n        [Position\\\\ Encoding\n            [Absolute\n                [BERT{,} {,} FLOATER\n                ,leaf,text width=11em]\n            ]\n            [Relative\n                 [{,} Music Transformer{,} T5{,} Transformer-XL\\\\ DeBERTa\n                ,leaf,text width=20em]\n            ]\n            [Other Rep.\n                [TUPE{,} Roformer\n                ,leaf,text width=7em]\n            ]\n            [Implicit Rep.\n                [Complex Embedding{,} R-Transformer {,} CPE\n                ,leaf,text width=16em]\n            ]\n        ]\n        [LayerNorm\n            [Placement\n                [post-LN{,} pre-LN\n                ,leaf,text width=16em]\n            ]\n            [Substitutes\n                [AdaNorm{,} scaled $\\ell_2$ normalization{,} PowerNorm\n                ,leaf,text width=16em]\n            ]\n            [Norm-free\n                 [ReZero-Transformer\n                ,leaf,text width=9.5em]\n            ]\n        ]\n        [FFN\n            [Activ. Func.\n                [Swish{,} GELU{,} GLU\n                ,leaf,text width=9.5em]\n            ]\n            [Enlarge\\\\Capacity\n                [Product-key Memory{,} Gshard{,} Switch Transformer{,}\\\\ Expert Prototyping{,} Hash Layer\n                ,leaf,text width=16em]\n            ]\n            [Dropping\n                [All-Attention layer{,} \n                ,leaf,text width=11em]\n            ]\n        ]\n    ]\n    [Arch.\\\\Level\n        [Lighweight\n            [Lite Transformer{,} Funnel Transformer{,} DeLighT\n            ,leaf,text width=16em]\n        ]\n        [Connectivity\n            [Realformer{,} Predictive Attention Transformer{,} Transparent Attention\\\\\n             Feedback Transformer~\n            ,leaf,text width=24.5em]\n        ]\n        [ACT\n            [UT{,} Conditional Computation Transformer{,} DeeBERT{,} PABEE{,} {,} \\\\\n            ,leaf,text width=24.5em]\n        ]\n        [Divide \\& \\\\\n        Conquer\n            [Recurrence\n                [Transformer-XL{,} Compressive Transformer{,} Memformer\\\\ {,} ERNIE-Doc\n            ,leaf,text width=20em]\n            ]\n            [Hierarchy\n                [{,} HIBERT{,} {,} Hi-Transformer\\\\ TENER{,} TNT\n                ,leaf,text width=20em]\n            ]\n        ]\n        [Alt. Arch.\n             [ET{,} Macaron Transformer{,} Sandwich Transformer{,} MAN{,} DARTSformer\n            ,leaf,text width=24.5em]\n        ]\n    ]\n    [Pre-Train\n        [Encoder\n            [BERT{,} RoBERTa{,} BigBird,leaf,text width=11em]\n        ]\n        [Decoder\n            [GPT{,} GPT-2{,} GPT-3,leaf,text width=11em]\n        ]\n        [Enc.Dec.\n            [ BART{,} T5{,} Switch Transformer\n            ,leaf,text width=11em]\n        ]\n    ]\n    [App.\n        [NLP\n            [BERT{,}ET{,} Transformer-XL{,}Compressive Transformer{,} TENER\n            ,leaf,text width=22em]\n        ]\n        [CV\n            [Image Transformer{,} DETR{,} ViT{,} Swin Transformer{,} ViViT\n            ,leaf,text width=22em]\n        ]\n        [Audio\n            [Speech Transformer{,} Streaming Transformer{,} Reformer-TTS{,} Music Transformer\n            ,leaf,text width=25em]\n        ]\n        [Multimodal\n            [VisualBERT{,} VLBERT{,} VideoBERT{,} M6{,} Chimera{,} DALL-E{,} CogView\n            ,leaf,text width=25em]\n        ]\n    ]\n  ]\n\\end{forest}\n\\caption{Taxonomy of Transformers}\n\\label{taxonomy_of_xformer}\n\\end{figure*}\nIn this survey, we focus on reviewing the works on architecture modifications.\nSince the attention module is the key component of Transformer, we solely describe the attention-related variants in Sec.~\\ref{sec:attention} and introduce the other module-level variants in Sec.~\\ref{sec:other_module}. Then Sec.~\\ref{sec:beyond} describes the other architecture-level variants. Finally, we briefly review the works on pre-training in Sec.~\\ref{sec:ptm} and applications in Sec.~\\ref{sec:app}.\nThere are some comprehensive surveys on the latter two categories of work, such as pre-trained models (PTMs)~ and visual Transformers.", "cites": [1474, 1493, 1457, 1482, 1460, 1491, 7360, 7040, 1484, 1494, 1464, 1488, 1470, 1452, 1466, 1490, 793, 1445, 1502, 7054, 1467, 1463, 8384, 1456, 1480, 794, 1475, 1465, 7361, 1500, 1459, 7053, 1458, 1479, 7365, 1496, 1133, 8454, 768, 7367, 7333, 1489, 1451, 1469, 9, 1476, 1453, 1495, 1487, 1462, 1478, 679, 1483, 1492, 7369, 7, 1473, 1446, 7370, 8456, 826, 7362, 7364, 7363, 1468, 38, 1486, 7298, 1498, 1497, 1501, 7371, 8457, 7368, 1455, 707, 8455, 1471, 1481, 1182, 732, 1272, 7339, 1477, 1449, 7273, 1461, 1454, 1448, 1485, 7366, 798, 1499, 1450, 1472], "cite_extract_rate": 0.8083333333333333, "origin_cites_number": 120, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes a taxonomy of Transformer variants without substantial synthesis, critical evaluation, or abstraction. It organizes cited works into categories but lacks discussion of their relationships, limitations, or broader implications. The focus is on categorization rather than analytical insight."}}
{"id": "c3955c2b-4114-49e4-bbb8-30061d5f27e8", "title": "Sparse Attention", "level": "subsection", "subsections": ["766f8fe1-2a61-4664-a9be-83f0177cc298", "6429c3b2-0cb9-415c-9f1d-1f7bd028a380"], "parent_id": "57f59954-45c7-48ac-baf2-dc92c147a85e", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Attention"], ["subsection", "Sparse Attention"]], "content": "\\label{sec:sparseattn}\nIn the standard self-attention mechanism, every token needs to attend to all other tokens.\nHowever, it is observed that for the trained Transformers the learned attention matrix $\\bA$ is often very sparse across most data points~.\nTherefore, it is possible to reduce computation complexity by incorporating structural bias to limit the number of query-key pairs that each query attends to. Under this limitation, we just compute the similarity score of the query-key pairs according to pre-defined patterns\n\\begin{equation}\\label{Sparse attention:position-based}\n    \\mathrm{\\hat{\\bA}}_{ij} = \\begin{cases} \\bq_i\\bk_{j}^\\top  & \\text{if token }i\\text{ attends to token }j,\\\\\n    -\\infty & \\text{if token }i\\text{ does not attend to token }j,\n    \\end{cases}\n\\end{equation}\\label{eq:sparseattn}\nwhere $\\mathrm{\\hat{\\bA}}$ is un-normalized attention matrix. In implementation the $-\\infty$ item is usually not stored in memory so as to decrease memory footprint.\nFrom another perspective, the standard attention can be regarded as a complete bipartite graph where each query receives information from all memory nodes and updates its representation.\nThe sparse attention can be considered as a sparse graph where some of the connections between nodes are removed.\nBased on the metrics of determining the sparse connection, we categorize these approaches into two classes: \\textit{position-based} and \\textit{content-based} sparse attention.", "cites": [793], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a conceptual overview of sparse attention and introduces a basic categorization into position-based and content-based approaches, demonstrating some synthesis and abstraction. However, it does not elaborate on or critically evaluate the cited paper (Sparse Transformers) or other works in the field, limiting its depth of analysis. The section sets a foundation for understanding the concept but lacks comparative or evaluative insights."}}
{"id": "3abb14c2-1133-4417-a3ba-50491a89789a", "title": "4.1.1.1 Atomic Sparse Attention", "level": "paragraph", "subsections": [], "parent_id": "766f8fe1-2a61-4664-a9be-83f0177cc298", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Attention"], ["subsection", "Sparse Attention"], ["subsubsection", "Position-based Sparse Attention"], ["paragraph", "4.1.1.1 Atomic Sparse Attention"]], "content": "There are mainly five types of atomic sparse attention patterns, as shown in Fig. \\ref{fig:atomic_sparse_attentions}.\n\\begin{figure}[htbp]\n\\begin{center}\n\\subfigure[global\\label{fig:global}]{\n\\includegraphics[width=0.18\\linewidth]{assets/sparse_variants/atomic/global.pdf}\n}\n\\subfigure[band\\label{fig:band}]{\n\\includegraphics[width=0.18\\linewidth]{assets/sparse_variants/atomic/band.pdf}\n}\n\\subfigure[dilated\\label{fig:dilated}]{\n\\includegraphics[width=0.18\\linewidth]{assets/sparse_variants/atomic/dilated.pdf}\n}\n\\subfigure[random\\label{fig:random}]{\n\\includegraphics[width=0.18\\linewidth]{assets/sparse_variants/atomic/random.pdf}\n}\n\\subfigure[block local\\label{fig:block_local}]{\n\\includegraphics[width=0.18\\linewidth]{assets/sparse_variants/atomic/block_local.pdf}\n}\n\\end{center}\n\\caption{Some representative atomic sparse attention patterns. The colored squares means corresponding attention scores are calculated and a blank square means the attention score is discarded.}\\label{fig:atomic_sparse_attentions}\n\\end{figure}\n\\begin{enumerate}\n  \\item \\textit{Global Attention}. To alleviate the degradation of the ability to model the long-range dependencies in sparse attention, one can add some global nodes\\footnote{In practice, these global nodes can be selected from the sequence (internal global nodes) or virtual nodes with trainable parameters (external global nodes).} as the hub for information propagation between nodes. These global nodes can attend all nodes in the sequence and the whole sequence attend to these global nodes, as illustrated in Fig. \\ref{fig:global}.\n  \\item \\textit{Band Attention}(a.k.a \\textit{sliding window attention} or \\textit{local attention}). Since most data come with a strong property of locality, it is natural to restrict each query to attend to its neighbor nodes. A widely adopted class of such sparse pattern is band attention, in which the attention matrix is a band matrix as illustrated in Fig. \\ref{fig:band}.\n  \\item \\textit{Dilated Attention}. Analogous to dilated CNNs~, one can potentially increase the receptive field of the band attention without increasing computation complexity by using a dilated window with gaps of dilation $w_d\\ge 1$, as depicted in Fig. \\ref{fig:dilated}. This can be easily extended to \\textit{strided attention}, where the window size is not limited but the dilation $w_d$ is set to a large value.\n  \\item \\textit{Random Attention}. To increase the ability of non-local interactions, a few edges are randomly sampled for each query, as illustrated in Fig. \\ref{fig:random}. This is based on the observation that random graphs (e.g., Erd\\H os–R\\'enyi random graph) can have similar spectral properties with complete graphs that leads to a fast mixing time for random walking on graphs.\n  \\item \\textit{Block Local Attention}. This class of attention segments input sequence into several non-overlapping query blocks, each of which is associated with a local memory block. All the queries in a query block attend to only the keys in the corresponding memory block. Fig. \\ref{fig:block_local} depicts a commonly used case where the memory blocks are identical to their corresponding query blocks.\n\\end{enumerate}", "cites": [4722], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a descriptive overview of five atomic sparse attention types and their basic mechanisms but does not explicitly synthesize insights from multiple papers. It lacks critical evaluation of these methods or their trade-offs and only offers minimal abstraction by drawing general observations like 'locality in data' or 'non-local interactions.'"}}
{"id": "cf0fe507-a0b3-499f-9145-a52ed5faa99e", "title": "4.1.1.2  Compound Sparse Attention", "level": "paragraph", "subsections": [], "parent_id": "766f8fe1-2a61-4664-a9be-83f0177cc298", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Attention"], ["subsection", "Sparse Attention"], ["subsubsection", "Position-based Sparse Attention"], ["paragraph", "4.1.1.2  Compound Sparse Attention"]], "content": "Existing sparse attentions are often composed of more than one of the above atomic patterns. Fig.~\\ref{fig:sparse_attn} illustrates some representative compound sparse attention patterns.\n\\begin{figure}[htbp]\n\\begin{center}\n\\subfigure[Star-Transformer\\label{fig:starxformer}]{\n\\includegraphics[width=0.18\\linewidth]{assets/sparse_variants/compound/star.pdf}\n}\\quad\n\\quad\n\\subfigure[Longformer\\label{fig:longformer}]{\n\\includegraphics[width=0.18\\linewidth]{assets/sparse_variants/compound/longformer.pdf}\n}\\quad\n\\subfigure[ETC\\label{fig:etc}]{\n\\includegraphics[width=0.18\\linewidth]{assets/sparse_variants/compound/etc.pdf}\n}\\quad\n\\subfigure[BigBird\\label{fig:bigbird}]{\n\\includegraphics[width=0.18\\linewidth]{assets/sparse_variants/compound/bigbird.pdf}\n}\\quad\n\\caption{Some representative compound sparse attention patterns. The red boxes indicate sequence boundaries.}\\label{fig:sparse_attn}\n\\end{center}\n\\end{figure}\nStar-Transformer~ uses a combination of band attention and global attention. Specifically, Star-Transformer just includes only a global node and a band attention with the width of 3, in which any pair of non-adjacent nodes are connected through a shared global node and adjacent nodes are connected directly with each other. This kind of sparse pattern forms a star-shaped graph among nodes.\nLongformer~ uses a combination of band attention and internal global-node attention. The global nodes are chosen to be \\texttt{[CLS]} token for classification and all question tokens for Question Answering tasks. They also replace some of the band attention heads in upper layers with dilated window attention to increase the receptive field without increasing computation.\nAs a concurrent work to Longformer~, Extended Transformer Construction (ETC)~ utilizes combination of band attention and external global-node attention. ETC also includes a masking mechanism to handle structured inputs and adapt Contrastive Predictive Coding (CPC)~ for pre-training.\nIn addition to the band and global attention, BigBird~ uses additional random attention to approximate full attention. Their theoretical analysis also reveals that the usage of a sparse encoder and sparse decoder can simulate any Turing Machine, which explains the success of those sparse attention models.\nSparse Transformer~ uses a factorized attention where different sparse patterns are designed for different types of data. For data with a periodic structure (e.g., images), it uses a composition of band attention and strided attention. Whereas for data without a periodic structure (e.g., text), it uses a composition of block local attention combined with global attention, where global nodes are from fixed positions in the input sequence.", "cites": [7298, 793, 7371, 1499, 7363, 134], "cite_extract_rate": 1.0, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of several sparse attention mechanisms by grouping them under the concept of compound patterns and briefly describing how each combines atomic attention types. However, it does not critically evaluate their relative strengths or weaknesses, nor does it identify overarching principles or trends in the design of compound sparse attention. The abstraction is limited to the observation that multiple patterns are combined, without deeper conceptual generalization."}}
{"id": "8a0b3727-3354-4028-870f-983c49224baa", "title": "4.1.1.3  Extended Sparse Attention", "level": "paragraph", "subsections": [], "parent_id": "766f8fe1-2a61-4664-a9be-83f0177cc298", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Attention"], ["subsection", "Sparse Attention"], ["subsubsection", "Position-based Sparse Attention"], ["paragraph", "4.1.1.3  Extended Sparse Attention"]], "content": "Apart from the above patterns, some existing studies have explored extended sparse patterns for specific data types.\nFor text data, BP-Transformer~ constructs a binary tree where all tokens are leaf nodes and the internal nodes are span nodes containing many tokens. The edges in this graph are constructed so that each leaf node is connected to its neighbor leaf nodes and higher-level span nodes containing tokens from a longer distance. This approach can be seen as an extension of global attention, where global nodes are hierarchically organized and any pair of tokens are connected with paths in the binary tree. An abstract view of this method is illustrated in Fig. \\ref{fig:bpt}.\nThere are also some extensions for vision data. Image Transformer~ explores two types of attention: (1) flattening image pixels in raster-scan order and then applying block local sparse attention. (2) 2D block local attention, where query blocks and memory blocks are arranged directly in 2D plate, as depicted in Fig. \\ref{fig:block_local_2d}. As another example of sparse pattern on vision data, Axial Transformer~ applies independent attention modules over each axis of the image. Each attention module mixes information along one axis while keeping information along the other axis independent, as illustrated in Fig. \\ref{fig:axial}. This can be understood as horizontally and vertically flattening image pixels in raster-scan order and then applying strided attention with gaps of image width and height, respectively.\n\\begin{figure}[htbp]\n\\begin{center}\n\\subfigure[BPT\\label{fig:bpt}]{\n\\includegraphics[width=0.40\\linewidth]{assets/sparse_variants/other/bpt.pdf}\n}\\quad\n\\subfigure[block local (2D)\\label{fig:block_local_2d}]{\n\\includegraphics[width=0.18\\linewidth]{assets/sparse_variants/other/block_local.pdf}\n}\\quad\n\\subfigure[axial (2D)\\label{fig:axial}]{\n\\includegraphics[width=0.18\\linewidth]{assets/sparse_variants/other/axial.pdf}\n}\n\\caption{Other types of sparse attentions. The red box indicates the query position, and the orange nodes/squares means corresponding tokens are attended to by the query.}\\label{fig:other_sparse_attn}\n\\end{center}\n\\end{figure}", "cites": [1484, 1459], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of two extended sparse attention approaches (BP-Transformer and Axial Transformer) and briefly describes their mechanisms. While it attempts to relate these methods to the broader concept of sparse attention, the synthesis is minimal and lacks deeper integration of ideas. There is no critical evaluation of these methods' strengths or weaknesses, and no meta-level generalization beyond the specific implementations."}}
{"id": "6429c3b2-0cb9-415c-9f1d-1f7bd028a380", "title": "Content-based Sparse Attention", "level": "subsubsection", "subsections": [], "parent_id": "c3955c2b-4114-49e4-bbb8-30061d5f27e8", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Attention"], ["subsection", "Sparse Attention"], ["subsubsection", "Content-based Sparse Attention"]], "content": "Another line of work creates a sparse graph based on input content, i.e., the sparse connections are conditioned on inputs.\nA straightforward way of constructing a content-based sparse graph is to select those keys that are likely to have large similarity scores with the given query. To efficiently construct the sparse graph, we can recur to Maximum Inner Product Search (MIPS) problem, where one tries to find the keys with maximum dot product with a query without computing all dot product terms. Routing Transformer~ uses k-means clustering to cluster both queries $\\{\\bq_i\\}_{i=1}^T$ and keys $\\{\\bk_i\\}_{i=}^T$  on the same set of centroid vectors $\\{\\mathbf\\mu_i\\}_{i=1}^k$. Each query only attends to the keys that belong to the same cluster.\nDuring training, the cluster centroid vectors are updated using the exponentially moving average of vectors assigned to it, divided by the exponentially moving average of cluster counts:\n\\begin{align}\n\\tilde\\mu&\\leftarrow \\lambda\\tilde\\mu+(1-\\lambda)\\left(\\sum_{i:\\mu(\\bq_i)=\\mu}\\bq_i+\\sum_{j:\\mu(\\bk_j)=\\mu}\\bk_j\\right),\\\\\nc_\\mu&\\leftarrow \\lambda c_\\mu+(1-\\lambda)|\\mu|,\\\\\n\\mu&\\leftarrow \\frac{\\tilde\\mu}{c_\\mu},\n\\end{align}\nwhere $|\\mu|$ denotes the number of vectors currently in cluster $\\mu$ and $\\lambda\\in(0,1)$ is a hyperparameter.\nLet $\\mathcal{P}_i$ denote the set of indices of keys that the $i$-th query attend to. $\\mathcal{P}_i$ in Routing Transformer is defined as\n\\begin{equation}\n    \\mathcal{P}_i=\\{j: \\mu(\\bq_i)=\\mu(\\bk_j) \\}.\n\\end{equation}\nReformer~ uses locality-sensitive hashing (LSH) to select key-value pairs for each query. The proposed LSH attention allows each token to attend only to the tokens within the same hashing bucket. The basic idea is to use an LSH function to hash queries and keys into several buckets, with similar items fall in the same bucket with high probability. Specifically, they use the random matrix method for the LSH function. Let $b$ be the number of buckets, given a random matrix $R$ of size $[D_k,b/2]$, the LSH function is computed by :\n\\begin{equation}\n    h(x)=\\argmax([xR;-xR]).\n\\end{equation}\nThe LSH attention allows the $i$-th query to attend only to key-value pairs with indices\n\\begin{equation}\n    \\mathcal{P}_i=\\{j: h(\\bq_i)=h(\\bk_j) \\}.\n\\end{equation}\nSparse Adaptive Connection (SAC)~ views the input sequence as a graph and learns to construct attention edges to improve task-specific performances using an adaptive sparse connection. SAC uses an LSTM edge predictor to construct edges between tokens. With no ground truth for edges, the edge predictor is trained with reinforcement learning.\nSparse Sinkhorn Attention~ first splits queries and keys into several blocks and assigns a key block to each query block. Each query is only allowed to attend to the keys in the key block that is assigned to its corresponding query block. The assignment of key blocks is controlled by a sorting network, which uses Sinkhorn normalization to produce a doubly stochastic matrix as the permutation matrix representing the assignment. They use this content-based block sparse attention along with block local attention introduced in Sec.~\\ref{sec:pos_based} to enhance the ability of the model to model locality.", "cites": [1453, 1473, 7362, 794], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear description of content-based sparse attention methods from three different papers but does not go beyond summarizing their individual approaches. There is minimal synthesis of the ideas and no direct comparison or evaluation of their strengths and weaknesses. Some level of abstraction is present in identifying the common theme of using input content to guide sparsity, but deeper meta-level insights or critical analysis are missing."}}
{"id": "c219036a-ca77-49bd-b9c6-4ac269a7d3d6", "title": "Feature Maps", "level": "subsubsection", "subsections": [], "parent_id": "a4f89d55-c38a-442a-8744-5625c30e01ff", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Attention"], ["subsection", "Linearized Attention"], ["subsubsection", "Feature Maps"]], "content": "Linear Transformer~ propose to use a simple feature map $\\phi_i(\\bx)=\\mathrm{elu}(x_i)+1$. This feature map does not aim to approximate dot product attention, but is empirically proved to perform on par with the standard Transformer.\nPerformer~ uses random feature maps that approximate the scoring function of Transformer. The random feature maps take functions $f_1,\\cdots,f_l:\\mathbb R\\rightarrow \\mathbb R$ and $h:\\mathbb{R}^D\\rightarrow \\mathbb R$.\n\\begin{equation}\n    \\phi(\\bx) = \\frac{h(\\bx)}{\\sqrt{m}}[f_1(\\omega_1^\\top \\bx),\\cdots,f_m(\\omega_m^\\top \\bx),\\cdots,f_l(\\omega_1^\\top \\bx),\\cdots,f_l(\\omega_m^\\top \\bx)],\n\\end{equation}\nwhere $\\omega_1,\\cdots,\\omega_m\\stackrel{\\text{iid}}{\\sim} \\mathcal D$ are drawn from some distribution $\\mathcal D\\in\\mathcal P(\\mathbb{R}^D)$.\nThe first version of Performer~ is inspired from the random Fourier feature map~ that was originally used to approximate Gaussian kernel. It uses trigonometric functions with $h(\\bx)=\\exp(\\frac{\\|\\bx\\|^2}{2}), l=2, f_1=\\sin, f_2=\\cos$. This approach has also been used in Random Feature Attention (RFA)~, with the difference that $h(\\bx)$ is set to $1$ as the queries and keys are $\\ell_2$-normalized before applying the feature map.\nAlthough the trigonometric random feature map leads to an unbiased approximation, it does not guarantee non-negative attention scores and thus could lead to unstable behaviors and abnormal behaviors. To mitigate this issue, the second version of Performer~ proposes positive random feature maps, which uses $h(\\bx)=\\exp(-\\frac{\\|\\bx\\|^2}{2}),l=1,f_1=\\exp$ and thus guarantees unbiased and non-negative approximation of dot-product attention. This approach is more stable than  and reports better approximation results.\nIn addition to using random feature maps to approximate standard dot product attention,  and  also explore approximating order-1 arc-cosine kernel with $h(\\bx)=1,l=1,f_1=\\mathrm{ReLU}$. This feature map has been show to be effective in various tasks including machine translation and protein sequence modeling.\n design a feature map that aims at facilitating orthogonality in feature space. Specifically, given an input $\\bx\\in\\mathbb{R}^D$, the feature map $\\phi:\\mathbb{R}^D\\rightarrow \\mathbb{R}^{2\\nu D}$ is defined by the partial function\n\\begin{equation}\n    \\phi_{i+2(j-1)D}(\\bx)=\\mathrm{ReLU}([\\bx,-\\bx])_i\\mathrm{ReLU}([\\bx,-\\bx])_{i+j}\\quad\\text{for }i=1,\\cdots,2D,j=1,\\cdots,\\nu.\n\\end{equation}", "cites": [8384, 1494, 1182, 798], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers on feature maps in linearized attention, connecting Performer, RFA, and other approaches through shared goals and design principles. It also critically evaluates the trade-offs, such as the instability of trigonometric maps and the benefits of non-negative approximations. While it identifies some general patterns (e.g., linear time/space complexity), it remains largely focused on specific implementations rather than offering a higher-level abstraction or novel framework."}}
{"id": "22632f15-ca9f-4bd6-a9a4-e425db6125fa", "title": "Aggregation Rule", "level": "subsubsection", "subsections": [], "parent_id": "a4f89d55-c38a-442a-8744-5625c30e01ff", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Attention"], ["subsection", "Linearized Attention"], ["subsubsection", "Aggregation Rule"]], "content": "In Eq. \\eqref{eq:linearized} the associations $\\{\\phi(\\bk)_j\\otimes\\bv_j\\}$ are aggregated into the memory matrix by simple summation. This is adopted by several studies~. However, it could be more beneficial for the network to selectively drop associations as new associations are added to the memory matrix.\nRFA~ introduces a gating mechanism to the summation to model local dependency in sequence data. Specifically, when adding a new association to the memory matrix $\\bS$, at a particular time step, they weigh $\\bS$ by a learnable, input-dependent scalar $g$, and the new association by $(1-g)$ (and a similar mechanism to $\\bu$). With this modification, history associations are exponentially decayed and recent context is favored in each timestep.\n argue that simple summation limits the capacity of the memory matrix and thus propose to enlarge the capacity in a write-and-remove fashion. Specifically, given a new input key-value pair $(\\bk_i, \\bv_i)$, the model first retrieve the value $\\bar{\\bv}_i$ currently associated with $\\bk_i$ using matrix multiplication. It then writes to the memory matrix a convex combination of $\\bar{\\bv}_i$ and $\\bv_i$, using a input-dependent gating scalar $g$, and removes the association $\\bar{\\bv}_i$. They also propose \\textit{sum normalization} (normalizing $\\phi(\\bq_i),\\phi(\\bk_i)$ by the sum of their components before updating the memory matrix) instead of normalizing with the denominator in Eq. \\eqref{eq:linearized} for this aggregation rule.", "cites": [8384, 1494, 1182, 798], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers to explain different aggregation strategies in linearized attention, particularly RFA and another unnamed method, by highlighting how each addresses the limitations of simple summation. It provides a critical perspective by pointing out the potential benefit of selective dropping and capacity limitations of the memory matrix. The abstraction is moderate, as it begins to generalize the concept of aggregation but stops short of articulating a broader theoretical framework."}}
{"id": "ad89ae14-36aa-4aeb-ab1d-65a0637088d3", "title": "Attention with Prototype Queries", "level": "subsubsection", "subsections": [], "parent_id": "99d263f7-48ac-4be9-9f96-b1cd4380de1b", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Attention"], ["subsection", "Query Prototyping and Memory Compression"], ["subsubsection", "Attention with Prototype Queries"]], "content": "In query prototyping, several prototypes of queries serve as the main source to compute attention distributions. The model either copies the distributions to the positions of represented queries or filling those positions with discrete uniform distributions. Fig. \\ref{fig:query_prototype} illustrates the computing flow of query prototyping.\n\\begin{figure}[htbp]\n\\begin{center}\n\\subfigure[Query prototyping]{\\label{fig:query_prototype}\n\\includegraphics[height=1.5in]{assets/prototype/query_prototype.pdf}\n}\\hspace{.2in}\n\\subfigure[Memory compression]{\\label{fig:compressed_mem}\n\\includegraphics[height=1.5in]{assets/prototype/mem_compress.pdf}\n}\n\\caption{Query prototyping and memory compression.}\\label{fig:prototype_mem_compress}\n\\end{center}\n\\end{figure}\nClustered Attention~ groups queries into several clusters and then computes attention distributions for cluster centroids. All queries in a cluster share the attention distribution calculated with the corresponding centroid.\nInformer~ selects prototypes from queries using explicit query sparsity measurement, which is derived from an approximation of the Kullback-Leibler divergence between the query's attention distribution and the discrete uniform distribution. Attention distributions are then only calculated for the top-$u$ queries under query sparsity measurement. The rest of the queries are assigned with discrete uniform distributions.", "cites": [1479, 1472], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes two specific techniques (Clustered Attention and Informer) in the context of query prototyping without substantial synthesis or abstraction. It offers a basic explanation of their methods but does not compare them, highlight their trade-offs, or situate them within a broader conceptual framework. Critical analysis is minimal, focusing only on summarizing the approaches."}}
{"id": "5fdbbc99-3354-4d28-a5a0-c76985e76cdb", "title": "Attention with Compressed Key-Value Memory", "level": "subsubsection", "subsections": [], "parent_id": "99d263f7-48ac-4be9-9f96-b1cd4380de1b", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Attention"], ["subsection", "Query Prototyping and Memory Compression"], ["subsubsection", "Attention with Compressed Key-Value Memory"]], "content": "\\label{sec:compressed_mem}\nApart from decreasing the number of queries with query prototyping, one can also reduce the complexity by reducing the number of the key-value pairs before applying the attention mechanism, as depicted in Fig. \\ref{fig:compressed_mem}.\n propose Memory Compressed Attention (MCA) that reduces the number of keys and values using a strided convolution. This modification is used as a complement to local attention proposed in the same work (as discussed in Sec.~\\ref{sec:sparseattn}), in that it can capture global context. The mechanism reduces the number of keys and values by a factor of kernel size $k$ and thus allowing to process significantly longer sequences than vanilla Transformer given the same computation resources.\nSet Transformer~ and Luna~ use a number of external trainable global nodes to summarize information from inputs and then the summarized representations serve as a compressed memory that the inputs attend to. This reduces the quadratic complexity of self-attention to linear complexity w.r.t. sequence length.\nLinformer~ utilizes linear projections to project keys and values from length $n$ to a smaller length $n_k$. This also reduces the complexity of self-attention to linear. The drawback of this approach is that an input sequence length has to be assumed and hence it cannot be used in autoregressive attention.\nPoolingformer~ adopts two-level attention that combines a sliding window attention and a compressed memory attention. The compressed memory module is used after the sliding window attention to increase the receptive field. They explore a few different pooling operations as the compression operation to compress the number of keys and values, including max pooling and pooling with Dynamic Convolution~.", "cites": [7333, 1503, 1133, 1504, 1482], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview by grouping methods under the theme of key-value memory compression and explaining their general approach to reducing attention complexity. It synthesizes ideas from multiple papers but with limited depth in connecting them. There is some critical analysis, such as noting the limitation of Linformer in autoregressive settings, but the section could benefit from more detailed comparisons or evaluations of trade-offs across methods."}}
{"id": "9a2c3eb1-bf66-4ddc-822d-bcc1121a8120", "title": "Low-rank Self-Attention", "level": "subsection", "subsections": ["8570c9f2-0cf3-47a3-befe-ca2093b97f7a", "fc87c410-c4a6-46de-806f-b6f8720b2eb4"], "parent_id": "57f59954-45c7-48ac-baf2-dc92c147a85e", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Attention"], ["subsection", "Low-rank Self-Attention"]], "content": "Some empirical and theoretical analyses~ report the self-attention matrix $\\bA \\in \\mathbb{R}^{T\\times T}$ is often low-rank\\footnote{The rank of $\\bA$ is far lower than input length $T$.}. The implications of this property are twofold: (1) The low-rank property could be explicitly modeled with parameterization; (2) The self-attention matrix could be replaced by a low-rank approximation.", "cites": [7333], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a basic analytical overview of low-rank self-attention, citing one relevant paper (Linformer) and highlighting two implications of the low-rank property. While it begins to abstract a general idea (low-rank as a property to be modeled or approximated), it lacks deeper synthesis of multiple sources and does not critically evaluate the limitations or compare with alternative approaches."}}
{"id": "fc87c410-c4a6-46de-806f-b6f8720b2eb4", "title": "Low-rank Approximation", "level": "subsubsection", "subsections": [], "parent_id": "9a2c3eb1-bf66-4ddc-822d-bcc1121a8120", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Attention"], ["subsection", "Low-rank Self-Attention"], ["subsubsection", "Low-rank Approximation"]], "content": "Another implication of the low-rank property of the attention matrix is that one can use a low-rank matrix approximation to reduce the complexity of self-attention. A closely related methodology is the low-rank approximation of kernel matrices. We believe some existing works are inspired by kernel approximation.\nSome of the aforementioned linearized attention methods in Sec.~\\ref{sec:kernel} are inspired from kernel approximation with random feature maps. For example, Performer~ follows the Random Fourier feature map originally proposed to approximate Gaussian kernels. The method first decomposes the attention distribution matrix $\\bA$ into $\\bC_Q\\bG\\bC_K$ where $\\bG$ is a Gaussian kernel matrix and the random feature map is used to approximate $\\bG$.\nAnother line of work follow the idea of Nystr\\\"om method. These Nystr\\\"om-based methods~ first select $m$ landmark nodes from the $T$ inputs with down-sampling methods (e.g., strided average pooling). Let $\\tilde\\bQ,\\tilde\\bK$ be the selected landmark queries and keys, then the follow approximation is used in the attention computation\n\\begin{equation}\\label{eq:nystrom}\n    \\tilde{\\bA}=\\mathrm{softmax}\\left(\\bQ\\tilde{\\bK}^\\top\\right)\\left(\\mathrm{softmax}\\left(\\tilde\\bQ\\tilde{\\bK}^\\top\\right)\\right)^{-1}\\mathrm{softmax}\\left(\\tilde\\bQ\\bK^\\top\\right).\n\\end{equation}\nNote that $\\bM^{-1}=\\left(\\mathrm{softmax}\\left(\\tilde\\bQ\\tilde{\\bK}^\\top\\right)\\right)^{-1}$ in Eq. \\eqref{eq:nystrom} does not always exist. To mitigate this issue, CSALR~ adds an identity matrix to $\\bM$ to make sure that the inverse always exists. Nystr{\\\"{o}}mformer~ uses the Moore-Penrose pseudoinverse of $\\bM$ instead of the inverse so that the approximation can be made for cases where $\\bM$ is singular.", "cites": [1494], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides an analytical overview of low-rank approximation techniques, integrating concepts from kernel approximation and the Nyström method. It connects related ideas and explains their application in Transformer variants like Performer, CSALR, and Nyströmformer. While it offers some critical observations (e.g., the invertibility issue in Nyström-based methods), it could more explicitly compare the effectiveness or limitations of these approaches."}}
{"id": "65f37434-480a-4748-b65e-d8966645bec9", "title": "Prior that Models locality", "level": "subsubsection", "subsections": [], "parent_id": "afe04fc5-1717-4f24-b061-7af855828533", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Attention"], ["subsection", "Attention with Prior"], ["subsubsection", "Prior that Models locality"]], "content": "Some types of data (e.g., text) can exhibit a strong preference for the locality. This property can be explicitly encoded as a prior attention. A simple method would be to use a Gaussian distribution over positions. Specifically, one could multiply the generated attention distribution with some Gaussian density and then renormalize, which is equivalent to adding to the generated attention scores $\\bA$ a bias term $\\bG$, where higher $G_{ij}$ indicates a higher prior probability that the $i$-th input attend to the $j$-th input.\n proposes to first predict a central position $p_i$ for each $\\bq_i$ using a simple feed-forward network. The Gaussian bias is then defined to be\n\\begin{equation}\n    G_{ij}=-\\frac{(j-p_i)^2}{2\\sigma^2},\n\\end{equation}\nwhere $\\sigma$ denotes standard deviation for the Gaussian and can be determined as a hyperparameter or predicted from inputs.\nGaussian Transformer~ assumes the central position to be $i$ for each $\\bq_i$ and defines the bias to bes\n\\begin{equation}\n    G_{ij}=-|w(i-j)^2+b|,\n\\end{equation}\nwhere $w\\ge 0, b\\le 0$ are scalar parameters that controls the deviation and reduce the weight for central position, respectively.", "cites": [1489], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of two methods for incorporating locality into attention mechanisms via Gaussian priors but does not integrate or synthesize the ideas in a meaningful way. It lacks critical evaluation or comparison of these approaches and does not abstract to broader principles or trends in attention modeling."}}
{"id": "af364707-c13f-44cb-9e4e-2366287a19b6", "title": "Prior from Lower Modules", "level": "subsubsection", "subsections": [], "parent_id": "afe04fc5-1717-4f24-b061-7af855828533", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Attention"], ["subsection", "Attention with Prior"], ["subsubsection", "Prior from Lower Modules"]], "content": "\\label{sec:prior_prev_module}\nIn Transformer architecture, it is often observed the attention distributions are similar in adjacent layers. It is thus natural to provide attention distribution from previous layer as a prior for attention computation. The final attention scores can be defined as\n\\begin{equation}\\label{crosslayer}\n    \\hat{\\bA}^{(l)} = w_1\\cdot\\bA^{(l)}+w_2\\cdot g(\\bA^{(l-1)}),\n\\end{equation}\nwhere $\\bA^{(l)}$ denotes the attention scores of the $l$-th layer, $w_1,w_2\\in\\mathbb R$ are weight applied to the scores from adjacent layers, and $g:\\mathbb{R}^{n\\times n}\\rightarrow \\mathbb{R}^{n\\times n}$ is a function that translate previous scores to the prior to be applied.\nPredictive Attention Transformer~ proposes to apply a 2D-convolutional layer to previous attention scores and compute the final attention scores as a convex combination of the generated attention scores and the convolved scores. This is equivalent to setting $w_1=\\alpha,w_2=1-\\alpha$ and $g(\\cdot)$ to be a convolutional layer in Eq. \\eqref{crosslayer}. They experiment training such a model from scratch and finetune after adapting the pre-trained BERT model, and both sets of experiments show improvements over baseline models.\nRealformer~ uses adds the previous attention scores directly to the generated attention scores, thus resembles a residual skip connection on attention maps. It's equivalent to setting $w_1=w_2=1$ and $g(\\cdot)$ to be identity map in Eq. \\eqref{crosslayer}. They conduct pre-training experiments on this model. The results show that this model outperforms the baseline BERT model in multiple datasets and surpasses the baseline model even when pre-training budgets are significantly lower.\nAs an extreme case, Lazyformer~ proposes to share attention maps between a number of adjacent layers. This is equivalent to setting $g(\\cdot)$ to identity and switch the settings of $w_1=0,w_2=1$ and $w_1=1,w_2=0$ alternatingly. The benefit of this approach is that the attention maps are computed only once and reused several times in the succeeding layers, thus reducing the computation cost. Their pre-training experiments show that the resulting model remains effective while being much more efficient to compute.", "cites": [1497, 1492], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a clear synthesis by unifying the discussed methods under a general equation and highlighting their shared motivation of using attention priors from lower modules. It abstracts the concept by identifying a recurring design pattern and framing different approaches within a common structure. However, the critical analysis is limited to stating performance improvements without deeper evaluation of trade-offs or limitations."}}
{"id": "a795db35-abb3-4e07-ad32-5c47de43f06f", "title": "Prior as Multi-task Adapters", "level": "subsubsection", "subsections": [], "parent_id": "afe04fc5-1717-4f24-b061-7af855828533", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Attention"], ["subsection", "Attention with Prior"], ["subsubsection", "Prior as Multi-task Adapters"]], "content": "Adapters are task-dependent, trainale modules that are attached in specific locations of a pre-trained network for cross-task efficient parameter sharing~.  propose a Conditionally Adaptive Multi-Task Learning (CAMTL) framework that uses a trainable attention prior $M(\\bz_i)$ that depends on task encoding $\\bz_i\\in\\mathbb{R}^{D_z}$\n\\begin{equation}\n    M(\\bz_i)=\\bigoplus_{j=1}^m A'_j(\\bz_i),\\quad A'_j(\\bz_i)=A_j\\gamma_i(\\bz_i)+\\beta_i(\\bz_i),\n\\end{equation}\nwhere $\\bigoplus$ denotes direct sum, $A_j\\in \\mathbb{R}^{(n/m)\\times(n/m)}$ are trainable parameters, and $\\gamma_j,\\beta_j:\\mathbb{R}^{D_z}\\rightarrow \\mathbb{R}^{(n/m)\\times(n/m)}$ are are Feature Wise\nLinear Modulation functions~. A maximum sequence length $n_{max}$ is specified in implementation. The prior is formulated as a block diagonal matrix and added to the attention scores of upper layers in pre-trained Transformers to serve as an adapter for parameter-efficient multi-task inductive knowledge transfer.", "cites": [725, 1505], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from the two cited papers by integrating the idea of using adapters for cross-task learning with the formulation of attention priors. It presents a mathematical model to illustrate how these adapters function, showing some analytical depth. However, it lacks deeper critical analysis of the approaches or their limitations and offers only minimal abstraction beyond the specific techniques described."}}
{"id": "55c07bfe-1106-456b-a01c-8edd7f25017b", "title": "Attention with Only Prior", "level": "subsubsection", "subsections": [], "parent_id": "afe04fc5-1717-4f24-b061-7af855828533", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Attention"], ["subsection", "Attention with Prior"], ["subsubsection", "Attention with Only Prior"]], "content": "Some works have explored using an attention distribution that is independent of pair-wise interaction between inputs. In other words, their models exploit only a prior attention distribution.\n design an efficient Transformer decoder variant called average attention network that uses a discrete uniform distribution as the sole source of attention distribution. The values are thus aggregated as a cumulative-average of all values. To improve the expressiveness of the network, they further adds a feed-forward\ngating layer on top of the average attention module. The advantage of this approach is that the adapted Transformer decoder can train in a parallel manner as usual Transformers do and decode like an RNN, thus avoiding the $\\mathcal O(T^2)$ complexity in decoding.\n utilize a Gaussian distribution as the hardcoded attention distribution for attention calculation. The intuition is very similar to  and  in that attention distribution should be focused on a certain local window. Distinctively, they drop the generated attention completely and use only the Gaussian distribution for attention computation. In this approach, the mean (central position) and variance are designed to be hyperparameters. The experiments show that the hardcoded attention, when applied only to self-attention, can achieve comparable performance to the baseline model in machine translation tasks.\nSynthesizer~ proposes to replace generated attention scores with: (1) a learnable, randomly initialized attention scores, and (2) attention scores output by a feed-forward network that is only conditioned on the querying input itself. The experiments on machine translation and language modeling show that these variants can achieve competitive performance with vanilla Transformer. It is not explained why these variants work but the empirical results are intriguing.", "cites": [1489, 7364, 7366, 1476], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers that use prior-based attention mechanisms, connecting them through the common theme of replacing learned attention with predefined or simplified distributions. While it provides some analytical discussion of the approaches and their performance, the critical evaluation is somewhat limited, particularly in not fully addressing why these methods work or their broader implications. The abstraction level is moderate, as it identifies a general pattern of using prior distributions but does not elevate the discussion to a deeper theoretical or conceptual level."}}
{"id": "dcd668ac-e997-46ab-8e9e-c34fd7ac742a", "title": "Head Behavior Modeling", "level": "subsubsection", "subsections": [], "parent_id": "4c615d59-27b6-48ea-a7bd-4c6d27bd91b5", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Attention"], ["subsection", "Improved Multi-Head Mechanism"], ["subsubsection", "Head Behavior Modeling"]], "content": "A basic motivation for using multi-head attention is to allow the model to jointly attend to information from different representation subspaces at different positions~. However, in vanilla Transformer there is no explicit mechanism to guarantee different behavior across attention heads, nor is there any mechanism for heads to interact with each other. A line of work is dedicated to improving multi-head mechanism by introducing incorporating more sophisticated mechanisms that guide the behavior of different attention heads or allow interaction across attention heads.\n introduce an auxiliary disagreement regularization term into loss function to encourage diversity among different attention heads. Two regularization terms are respectively to maximize cosine distances of the input subspaces and output representations, while the last one is to disperse\nthe positions attended by multiple heads with element-wise multiplication of the corresponding attention matrices.\nSeveral probing works have revealed that pre-trained Transformer models exhibit certain patterns of self-attention that are of little linguistic backing. As a representative work,  identify several simple attention patterns in BERT. For instance, many of the attention heads simply pay attention to special BERT tokens \\texttt{[CLS]} and \\texttt{[SEP]}. As a result, some constraints can be introduced to boost the training of Transformer models. To this end,  propose to use an auxiliary loss, which is defined to be the Frobenius norm between attention distribution maps and predefined attention patterns.\nTalking-head Attention~ uses a talking head mechanism that linearly projects the generated attention scores from $h_k$ to $h$ heads, applies softmax in that space, and then projects to $h_v$ heads for value aggregation. The motivation is to encourage the model to move information between attention heads in a learnable fashion.\nCollaborative Multi-head Attention~ uses shared query and key projection $\\bW^Q$ and $\\bW^K$ and a mixing vector $\\bm_i$ for the $i$-th head to filter from the projection parameters such that Eq. \\eqref{eq:headi} is adapted to\n\\begin{equation}\n    \\mathrm{head}_i=\\mathrm{Attention}(\\bQ\\bW^Q\\mathrm{diag}(\\bm_i),\\bK\\bW^K, \\bV\\bW_i^V),\n\\end{equation}\nwhere $\\bW^Q$ and $\\bW^K$ are shared by all the attention heads.", "cites": [1474, 7372, 1460, 1498, 7369, 38], "cite_extract_rate": 1.0, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple approaches to improving the multi-head attention mechanism, connecting different regularization strategies and architectural innovations. It provides a critical perspective by highlighting limitations in head diversity and attention behavior in vanilla Transformers. Furthermore, it abstracts the ideas to present a broader understanding of how attention heads can be guided or made to interact, moving beyond individual paper descriptions."}}
{"id": "6b049103-9bb4-4d9c-bd95-dceaba44ed6d", "title": "Multi-head with Restricted Spans", "level": "subsubsection", "subsections": [], "parent_id": "4c615d59-27b6-48ea-a7bd-4c6d27bd91b5", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Attention"], ["subsection", "Improved Multi-Head Mechanism"], ["subsubsection", "Multi-head with Restricted Spans"]], "content": "Vanilla attention adopts full attention spans assume, where a query can attend to all of the key-value pairs. However, it is often observed that some heads focus their attention distribution mainly in a local context while some other heads attend to broader contexts. It could thus be beneficial to restrict the attention spans:\n\\begin{itemize}\n    \\item \\textit{Locality}. Restricting attention spans induce explicit local constraints. This is advantageous in cases where locality is an important prior.\n    \\item \\textit{Efficiency}. If implemented appropriately, such a model can scale to very long sequences without introducing additional memory footprint and computational time.\n\\end{itemize}\nRestricting attention spans can be expressed as multiplying each attention distribution value with a mask value and then re-normalize, where the mask can be expressed as a non-increasing function that maps a distance to a value in $[0,1]$. A vanilla attention assigns a mask value of $1$ for all distances, as depicted in Fig. \\ref{span_mask}(a).\n\\begin{figure}[htbp]\n\\begin{center}\n\\subfigure[mask function for vanilla attention]{\n\\includegraphics[width=0.3\\linewidth]{assets/span_mask/full_mask.pdf}\n}\n\\subfigure[mask function for adaptive span]{\n\\includegraphics[width=0.3\\linewidth]{assets/span_mask/adaptive_mask.pdf}\n}\n\\subfigure[mask function for fixed span]{\n\\includegraphics[width=0.3\\linewidth]{assets/span_mask/scale.pdf}\n}\n\\caption{Three types of span masking function $m(x)$. The horizontal axis represents distance $x$ and vertical axis the mask value.}\\label{span_mask}\n\\end{center}\n\\end{figure}\n propose to use a learnable attention span, as depicted in Fig. \\ref{span_mask}(b) . The mask is parameterized by a learnable scalar $z$ and a hyperparameter $R$. The experiments on character-level language modeling show that the adaptive-span models outperform baseline models while having significantly fewer FLOPS. It is also observed that lower layers generally have smaller learned spans and higher layers otherwise. This indicates that the model can learn a hierarchical composition of features.\nMulti-Scale Transformer~ proposes to use a fixed attention span, with different heads in different layers using a different max span. The fixed attention span is depicted in  Fig. \\ref{span_mask}(c). The attention is restricted within a fixed window which is controlled by a \\textit{scale} value $w$.  They design the scales from an intuitive linguistic perspective and empirical observation from BERT such that higher layers tend to have more large scales (e.g., large span size), and lower layers should be confined with a smaller scale. Their experiments on several tasks show that the model can outperform baseline models while accelerating inference on long sequences.", "cites": [1465], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a coherent synthesis of two approaches to restricted attention spans by integrating their mechanisms and experimental outcomes. It abstracts the concept of attention span restriction and connects it to broader design principles like locality and efficiency. While it includes some analytical elements such as the hierarchical behavior of learned spans, it does not deeply critique the methods or highlight significant limitations beyond summarizing their results."}}
{"id": "c68f2053-1c0a-4bd7-8d8a-92474761c343", "title": "Multi-head with Refined Aggregation", "level": "subsubsection", "subsections": [], "parent_id": "4c615d59-27b6-48ea-a7bd-4c6d27bd91b5", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Attention"], ["subsection", "Improved Multi-Head Mechanism"], ["subsubsection", "Multi-head with Refined Aggregation"]], "content": "After each attention head computes its output representation, the vanilla multi-head attention~ concatenates these representation and then apply a linear transformation to the concatenated representation to obtain the final output representation, as formulated in Eq. \\eqref{eq:multihead}. Combining Eq. \\eqref{attention}\\eqref{eq:multihead} and \\eqref{eq:headi}, one can see that this \\textit{concatenate-and-project} formulation is equivalent to summation over $H$ re-parameterized attention outputs. To this end, we first divide $\\bW^O\\in\\mathbb{R}^{D_m\\times D_m}$ into $H$ blocks\n\\begin{equation}\n    \\bW^O=[\\bW_1^O;\\bW_2^O;\\cdots;\\bW_H^O],\n\\end{equation}\nwhere each $\\bW_i^O$ is of dimension $D_v\\times D_m$. It's thus easy to see that multi-head attention can be reformulated as\n\\begin{equation}\n    \\mathrm{MultiHeadAttn}(Q,K,V)= \\sum_{i=1}^H \\mathrm{Attention}(Q \\bW_i^Q,K \\bW_i^K, V\\textcolor[rgb]{0,0,1} {\\bW_i^V\\bW_i^O}).\n\\end{equation}\nOne might argue that this simple \\textit{aggregate-by-summation} paradigm does not fully exploit the expressiveness of multi-head attention and that it is more desirable to use a more complex aggregation.\n propose to use routing methods, originally proposed for capsule networks~, to further aggregate information produced by different attention heads. The outputs of attention heads are first transformed into input capsules, then output capsules are obtained after the iterative routing process. The output capsules are then concatenated as a final output of multi-head attention. These two works both utilizes two routing mechanisms, namely \\textit{dynamic routing} and \\textit{EM routing}. One would notice that iterative routing introduces additional parameters and computational overhead.  empirically show that applying the routing mechanism only to the lower layers can best balance the translation performance and computational efficiency.", "cites": [306, 1452, 8455, 38], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a conceptual reformulation of the vanilla multi-head attention mechanism and discusses how routing methods from capsule networks can enhance aggregation. It integrates insights from multiple papers, particularly Papers 2 and 3, but the synthesis is somewhat limited to specific techniques. The section offers some critical perspective on the limitations of the standard aggregation approach but could provide more nuanced comparisons or deeper evaluation of trade-offs. It identifies a broader pattern (the need for improved aggregation methods) but does not offer a meta-level abstraction or overarching principle."}}
{"id": "be224101-c6fc-4828-9ad0-f1f79c3097ee", "title": "Other Modifications", "level": "subsubsection", "subsections": [], "parent_id": "4c615d59-27b6-48ea-a7bd-4c6d27bd91b5", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Attention"], ["subsection", "Improved Multi-Head Mechanism"], ["subsubsection", "Other Modifications"]], "content": "Several other modifications to the multi-head mechanism have been proposed to improve multi-head attention.\n propose multi-query attention, where key-value pairs are shared among attention heads (i.e., to use only one key projection and one value projection for all attention heads). The advantage of this method is that it reduces the memory bandwidth requirements for decoding and results in a model that is faster to decode, while incurring only minor quality degradation from the baseline.\n establish that small attention key size can affect its ability to represent arbitrary distribution. They thus propose to disentangle head size from the number of heads $h$, as opposed to the common practice that sets the head size to be $D_m/h$. It is observed empirically that setting attention head size to be input sequence length is beneficial.", "cites": [1507, 1506], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a brief analytical overview by connecting two distinct modifications to the multi-head attention mechanism. It synthesizes the goals of each approach—efficiency in decoding and better representation capacity—though it does not offer deeper comparisons or a unifying framework. The critical evaluation is limited to stating observed trade-offs (e.g., minor quality degradation) without a more thorough assessment of their implications or relative effectiveness."}}
{"id": "ccc63091-167f-4bc8-97e9-c63a8adbf741", "title": "Absolute Position Representations", "level": "subsubsection", "subsections": [], "parent_id": "3f67f9f0-d57d-471a-bcb5-cbf64e5bcabc", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Other Module-level Modifications"], ["subsection", "Position Representations"], ["subsubsection", "Absolute Position Representations"]], "content": "In vanilla Transformer~, positional information is encoded as absolute sinusoidal position encodings.For each position index $t$, the encoding is a vector $\\bp_t = \\mathrm{PE}(t)\\in\\mathbb{R}^{D_m}$, of which every element is a sinusoidal ($\\sin$/$\\cos$) function of the index with pre-defined frequency.\n\\begin{equation}\\label{eq:PE}\n    \\mathrm{PE}(t)_i = \\begin{cases} \\sin(\\omega_i t) & \\text{if }i\\text{ is even},\\\\\n    \\cos(\\omega_i t) & \\text{if }i\\text{ is odd},\n    \\end{cases}\n\\end{equation}\nwhere $\\omega_i$ is the hand-crafted frequency for each dimension. The position encoding of each position in the sequence is then added to the token embeddings and fed to Transformer.\nAnother way of representing absolute positions is to learn a set of positional embeddings for each position~. Compared to hand-crafted position representation, learned embeddings are more flexible in that position representation can adapt to tasks through back-propagation. But the number of embeddings is limited up to a maximum sequence length determined before training, which makes this approach no longer \\textit{inductive}, i.e., not able to handle sequences longer than sequences seen in the training time.\n propose to use sinusoidal position representation, but with each frequency $\\omega_i$ (in Eq. \\eqref{eq:PE}) learned from data. This approach retains inductiveness but is more flexible than hand-crafted sinusoidal encoding. FLOATER~ frames positional representation as a continuous dynamical system and adopts Neural ODE to enable end-to-end training with backpropagation. This method is inductive and flexible while being parameter efficient compared to a fully learnable approach.\nThe Vanilla approach to incorporating absolute position representations is to add position encodings/embeddings to token embeddings. However, as the input signals propagate through\nthe layers, the positional information might get lost in the upper layers. Later works find it beneficial to add position representations to inputs to each Transformer layer~.", "cites": [1493, 790, 7, 1450, 38], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear analytical overview of absolute position representations in Transformers, contrasting hand-crafted and learned approaches, and introducing inductive methods like FLOATER. It synthesizes ideas from multiple papers to form a coherent progression of techniques. However, it lacks deeper critical evaluation of the cited works' assumptions or trade-offs, and while some abstraction is present, broader principles or trends are not fully articulated."}}
{"id": "9cdcf5bd-ef00-4ae9-a8b3-6d8cd2240795", "title": "Relative Position Representations", "level": "subsubsection", "subsections": [], "parent_id": "3f67f9f0-d57d-471a-bcb5-cbf64e5bcabc", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Other Module-level Modifications"], ["subsection", "Position Representations"], ["subsubsection", "Relative Position Representations"]], "content": "Another line of works focuses on representing positional relationships between tokens instead of positions of individual tokens. The intuition is that in self-attention, pairwise positional relationships between input elements (direction and distance) could be more beneficial than positions of elements. Methods following this principles are called relative positional representation.  propose to add a learnable relative position embedding to keys of attention mechanism\n\\begin{align}\n    \\bk_j' &= \\bk_j + \\br_{ij},\\ \\text{for }i=1,\\cdots, n,\\\\\n    \\br_{ij} &= \\bR_{\\mathrm{clip}(i-j)}\\label{rij},\\\\\n    \\mathrm{clip}(x) &= \\max(-K, \\min(x, K))\\label{eq:shaw_clip},\n\\end{align}\nwhere $\\br_{ij}\\in\\mathbb{R}^{D_k}$ is the relative position embedding for relation between position $i$ and $j$ and $K$ is the largest offset that determines the number of embeddingg. Typically $K$ is set to a length that can accommodate most input sequences. As a special case, InDIGO~ sets $K$ to $3$ for their specially designed framework for non-autoregressive generation. As an incremental effort, Music Transformer~ further introduce a mechanism to reduce the intermediate memory requirements for this approach. Similar to this approach, T5~ adopt a simplified form of relative position embeddings where each embedding is only a learnable scalar that is added to the corresponding score used for computing the attention weights.\nTransformer-XL~ use a sinusoidal encoding to represent positional relationships but fuses contents and position information by redesign the computation of attention scores\\footnote{the scaling factor is omitted without loss of generality.}\n\\begin{equation}\n    \\bA_{ij}=\\bq_i \\bk_j^\\top+\\bq_i\\left(\\bR_{i-j}\\bW^{K,R}\\right)^\\top +\\bu^1\\bk_j^\\top+\\bu^2\\left(\\bR_{i-j}\\bW_{K,R}\\right)^\\top,\n\\end{equation}\nwhere $\\bW^{K,R}\\in\\mathbb{R}^{D_m\\times D_k},\\bu^1,\\bu^2\\in\\mathbb{R}^{D_k}$ are learnable parameters and $\\bR$ is a sinusoidal encoding matrix similar to position encoding in vanilla Transformer. Then softmax function is applied to scores $\\bA$ to provide attention weights. Note that the learnable sinusoidal encoding is also a drop-in replacement to hand-crafted $\\bR$.\nDeBERTa~ utilizes position embeddings like  and applies the embeddings to the model in a disentangled style similar to Transformer-XL~\n\\begin{equation}\n    \\bA_{ij}=\\bq_i \\bk_j^\\top+\\bq_i\\left(\\br_{ij}\\bW^{K,R}\\right)^\\top +\\bk_j\\left(\\br_{ij}\\bW^{Q,R}\\right)^\\top,\n\\end{equation}\nwhere $\\bW^{K,R},\\bW^{Q,R}\\in\\mathbb{R}^{D_m\\times D_k}$ are learnable parameters and $\\br_{ij}$ is the learnable relative positional embedding as in Eq. \\eqref{rij}. The first term is interpreted as a content-to-content attention, and the latter two terms are interpreted as (relative) content-to-position and position-to-content attention, respectively.", "cites": [1456, 1508, 1481, 7370, 7054, 9], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes key concepts from multiple papers on relative position representations, connecting the mathematical formulations across works and highlighting variations in implementation. While it provides some abstraction by identifying a common theme of modeling positional relationships, it lacks deeper critical evaluation of the methods' strengths, weaknesses, or empirical performance. Overall, it offers analytical structure but could benefit from more evaluative depth."}}
{"id": "268772ad-4cd8-43e9-9608-b7ac7de2e35c", "title": "Other Representations", "level": "subsubsection", "subsections": [], "parent_id": "3f67f9f0-d57d-471a-bcb5-cbf64e5bcabc", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Other Module-level Modifications"], ["subsection", "Position Representations"], ["subsubsection", "Other Representations"]], "content": "Some research studies have explored using hybrid positional representations that contains both absolute and relative positional information. Transformer with Untied Position Encoding (TUPE)~ re-designs the computation of attention scores as a combination of a content-to-content term, an absolute position-to-position term and a bias term representing relative positional relationships\n\\begin{equation}\n    \\bA_{ij}=\\bq_i \\bk_j^\\top+\\left(\\bp_i\\bW^{Q,P}\\right)\\left(\\bp_{j}\\bW^{K,P}\\right)^\\top +b_{j-i},\n\\end{equation}\nwhere $\\bW^{K,P},\\bW^{Q,P}\\in\\mathbb{R}^{D_m\\times D_k}$ are learnable parameters, $\\bp_i, \\bp_j$ are the position embeddings for positions $i,j$, and $b_{j-i}$ is a learnable scalar relative position embedding.\nOne can also design a single set of positional representations that express both absolute and relative information. Roformer~ uses Rotary Position Embedding (RoPE) to represent the position of a token by multiplying the affine-transformed embedding of the $t$-th input $x_t$ by a rotatory matrix $\\bR_{\\Theta,t}$\n\\begin{align}\n    \\bq_t=\\bx_t\\bW^Q\\bR_{\\Theta,t}\\quad & \\bk_t=\\bx_t\\bW^K\\bR_{\\Theta,t},\\\\\n    \\bR_{\\Theta,t}&=\\bigoplus_{j=1}^{D_k/2} \\bM(t,\\theta_j),\n\\end{align}\nwhere $\\bigoplus$ denotes \\textit{direct sum} of matrices. Each $\\bM(t,\\theta_j)$ is a 2-D clockwise rotatory matrix of angle $t\\cdot \\theta_j$\n\\begin{equation}\n    \\bM(t,\\theta_j)=\\left[\\begin{matrix} \\cos(t\\cdot \\theta_j)&\\sin(t\\cdot \\theta_j)\\\\\n    -\\sin(t\\cdot \\theta_j)&\\cos(t\\cdot \\theta_j)\\end{matrix}\n    \\right].\n\\end{equation}\nThe key advantage of this formulation is that the induced representation is translation invariant, i.e., the attention score of $(\\bq_i,\\bk_j)$ is only related to their relative position offset\n\\begin{equation}\n    \\bq_i\\bk_j^\\top=\\left(\\bx_i\\bW^Q\\right)\\bR_{\\Theta,\\textcolor[rgb]{0,0,1}{j-i}}\\left(\\bx_j\\bW^K\\right)^\\top.\n\\end{equation}\nIn practice, the embedding matrix multiplication can be implemented by two element-wise multiplication for lower memory footprint. The RoPE uses the form of absolute embedding but can capture relative positional relations. This approach is compatible with linearized attention in Sec.~\\ref{sec:kernel}.", "cites": [1454, 8457], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from both TUPE and RoPE, connecting their treatment of positional information in a coherent way. It provides a critical perspective by highlighting advantages, such as translation invariance in RoPE, and acknowledges how the approach is compatible with other techniques like linearized attention. While it identifies a broader trend of hybrid positional encoding, it does not fully abstract these ideas into a meta-level framework, limiting its abstraction score."}}
{"id": "866b7a57-7c1b-41c7-8a8f-c03a0681cbeb", "title": "Position Representations without Explicit Encoding", "level": "subsubsection", "subsections": [], "parent_id": "3f67f9f0-d57d-471a-bcb5-cbf64e5bcabc", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Other Module-level Modifications"], ["subsection", "Position Representations"], ["subsubsection", "Position Representations without Explicit Encoding"]], "content": "Instead of explicitly introducing additional positional encodings,  propose to encode positional information in word embeddings, by generalizing embedding to continuous (complex-valued) functions over positions.\nR-Transformer~ model locality of sequential data with a local RNN. Specifically, inputs to each block of R-Transformer are first fed to a local RNN and then to multi-Head self-attention module. The RNN structure introduces ordering information and captures local dependencies as a complement to self-attention.\nConditional positional encoding (CPE)~ generate conditional position encodings at each layer for ViT with a 2-D convolution with zero-paddings. The intuition behind this approach is that convolution networks can implicitly encode absolute positional information with zero-paddings~.", "cites": [1480, 1493, 1509, 1471], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of each paper's method for position representation without explicit encoding but lacks meaningful synthesis, critical evaluation, or abstraction. It mentions each approach in isolation, without comparing them or highlighting their relative strengths, weaknesses, or implications. As a result, it offers limited insight into broader trends or conceptual frameworks in position encoding for Transformers."}}
{"id": "90d9b719-8d67-48fa-a44c-ce04b5d77df6", "title": "Position Representation on Transformer Decoders", "level": "subsubsection", "subsections": [], "parent_id": "3f67f9f0-d57d-471a-bcb5-cbf64e5bcabc", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Other Module-level Modifications"], ["subsection", "Position Representations"], ["subsubsection", "Position Representation on Transformer Decoders"]], "content": "It is worth noticing that masked self-attention is not permutation equivariant~. Thus a model that exploits only the decoder of Transformer has the potential of sensing positional information without incorporating explicit positional representation. This is confirmed by some empirical results on language modeling tasks~, where the authors find that removing position encodings even improves performance.", "cites": [1510], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical perspective by noting that masked self-attention in Transformer decoders may inherently capture positional information. It integrates a key idea from the cited paper on language modeling to support this claim. While it does not deeply critique or compare multiple works, it begins to generalize beyond the paper by suggesting broader implications for positional encoding design."}}
{"id": "9c98bd5d-1c97-4d09-acae-d8ba70e8c480", "title": "Placement of Layer Normalization", "level": "subsubsection", "subsections": [], "parent_id": "ba9ac74a-b1ed-471e-a9a5-9097079a09db", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Other Module-level Modifications"], ["subsection", "Layer Normalization"], ["subsubsection", "Placement of Layer Normalization"]], "content": "\\ifx \\smv \\undefined\n\\begin{figure}[htbp]\n\\begin{center}\n\\subfigure[post-LN]{\n\\includegraphics[height=2.2in]{assets/layernorm/post_ln.pdf}\n}\\hspace{.6in}\n\\subfigure[pre-LN]{\n\\includegraphics[height=2.2in]{assets/layernorm/pre_ln.pdf}\n}\n\\caption{Comparison of Transformer Encoder with pre-LN and post-LN.}\\label{fig:pre_post_ln}\n\\end{center}\n\\end{figure}\n\\fi\nIn vanilla Transformer, the LN layer lies between the residual blocks, called post-LN~. Later Transformer implementations~ place the LN layer inside the residual connection before the attention or FFN, with an additional LN after the final layer to control the magnitude of final outputs, which is referred to as pre-LN\\footnote{To the best of our knowledge, this approach is adopted since \\texttt{v1.1.7} in the \\texttt{Tensor2Tensor} implementation~.}. The pre-LN has been adopted by numerous following research studies and implementations, e.g., .\nThe difference between pre-LN and post-LN is shown in Fig. \\ref{fig:pre_post_ln}.\n theoretically investigate the gradients of Transformers and find that the gradients near the output layer are large at initialization in post-LN Transformers, which could be the reason why post-LN Transformers without learning rate warm-up~\\footnote{Learning rate warm-up refers to starting optimization with an extremely small learning rate and then gradually increasing it to a pre-defined maximum value in a certain number of iterations.} leads to unstable training, whereas pre-LN Transformers do not suffer from the same problem. They thus deduce and empirically verify that warm-up stage can be safely removed for pre-LN Transformers.\nAlthough Post-LN often results in unstable training and divergence, it usually outperforms pre-LN variants after convergence~. Similar to ,  conduct theoretical and empirical analysis and find that post-LN encoders do not suffer from gradient imbalance. They thus conjecture that the gradient issue is not the direct cause of unstable post-LN Transformer training and further identify the \\textit{amplification effect} in post-LN Transformers — at initialization, the heavier dependency on residual branch leads to a larger output shift in post-LN Transformers, thus resulting in unstable training. In light of this finding, they introduce additional parameters to post-LN Transformers to control residual dependencies of Post-LN. These parameters are initialized according to activation variations of sample data so that the output shift of post-LN Transformers is not amplified. This approach ensures and boosts convergence of post-LN Transformers and reaches better performance than pre-LN Transformers.", "cites": [1496, 1511, 1448, 793, 1464, 38], "cite_extract_rate": 1.0, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers to explain the theoretical and empirical differences between pre-LN and post-LN configurations in Transformers. It provides critical analysis by questioning the root causes of instability in post-LN and how prior assumptions (e.g., gradient imbalance) were challenged. The abstraction is strong as it introduces the concept of the 'amplification effect' and generalizes the issue to deeper learning patterns in Transformer training."}}
{"id": "0329ae93-9d4a-453b-86ad-768aed04ebd3", "title": "Substitutes of Layer Normalization", "level": "subsubsection", "subsections": [], "parent_id": "ba9ac74a-b1ed-471e-a9a5-9097079a09db", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Other Module-level Modifications"], ["subsection", "Layer Normalization"], ["subsubsection", "Substitutes of Layer Normalization"]], "content": " empirically observe that the learnable parameters in the LN module do not work in most experiments, and even increase the risk of overfitting. They further conclude from controlled experiments that the forward normalization is not the reason why LN works for Transformer. From analysis and experiments, it is concluded that the derivatives of the mean and variance re-center and re-scale the gradients and play a significant role in LN. They thus propose \\textit{AdaNorm}, a normalization technique without learnable parameters\n\\begin{align}\n    \\bz &= C(1-k\\by)\\odot\\by,\\\\\n    \\by &=\\frac{\\bx-\\mu}{\\sigma},\n\\end{align}\nwhere $C,k$ are hyperparameters and $\\odot$ denotes element-wise multiplication. $\\mu$ and $\\sigma$ are the mean and standard deviation of input $\\bx$, respectively.\n propose to replace the LN module with \\textit{scaled $\\ell_2$ normalization}. Given any input $\\bx$ of $d$-dimension, their approach project it onto a $d-1$-sphere of learned radius $g$\n\\begin{equation}\n    \\bz=g\\frac{\\bx}{\\|\\bx\\|},\n\\end{equation}\nwhere $g$ is a learnable  scalar. It is more parameter efficient compared to normal LN and is shown to be effective in machine translation datasets, especially in low-resource settings.\n discuss why Batch Normalization (BN)~ performs poorly in Transformer for text data and conclude that BN's significant performance degradation stems from the instabilities associated with its batch statistics. They thus propose PowerNorm (PN) that has three modifications over BN: (1) it relaxes the zero-mean normalization; (2) it uses the quadratic mean\nof the signal, instead of the variance; (3) it uses running statistics for the\nquadratic mean, instead of using per-batch statistics. Specifically, for the $t$-th iteration, the PN computes the outputs as\n\\begin{align}\n    \\bz^{(t)} &= \\gamma\\odot \\by^{(t)}+\\beta,\\\\\n    \\by^{(t)} &=\\frac{\\bx^{(t)}}{\\psi^{(t-1)}},\\\\\n    (\\psi^{(t)})^2&=\\alpha(\\psi^{(t-1)})^2+(1-\\alpha)\\left(\\frac{1}{|B|}\\sum_{i=1}^{|B|}(\\bx_i^{(t)})^2\\right),\n\\end{align}\nwhere $0<\\alpha<1$ is the moving average coefficient and $\\gamma, \\beta$ are the learnable parameters as in BN formulation.", "cites": [1466, 1467, 1491, 71], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of alternative normalization techniques to LayerNorm in Transformers, drawing insights from multiple papers. It synthesizes key findings about the role of gradients in LayerNorm and presents distinct approaches such as AdaNorm, ScaleNorm, and PowerNorm. While it connects these ideas and highlights their motivations and formulations, it lacks deeper comparative evaluation or meta-level abstraction that would elevate the insight level further."}}
{"id": "29a8b774-a21f-46a2-8095-eac026b86ef8", "title": "Normalization-free Transformer", "level": "subsubsection", "subsections": [], "parent_id": "ba9ac74a-b1ed-471e-a9a5-9097079a09db", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Other Module-level Modifications"], ["subsection", "Layer Normalization"], ["subsubsection", "Normalization-free Transformer"]], "content": "Besides LN, there is another mechanism to construct deeper neural network. ReZero~ replace LN module with a learnable residual connection. For each module $F(\\cdot)$, ReZero re-scales $F(\\cdot)$ in the residual formulation:\n\\begin{equation}\n    \\bH'=\\bH+\\alpha\\cdot F(\\bH),\n\\end{equation}\nwhere $\\alpha$ is a learnable parameter with zero-initialization.\nReplacing LN in Transformer with ReZero mechanism is verified to induce better dynamic isometry for input signals and leads to faster convergence.", "cites": [1495], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section introduces the ReZero mechanism as an alternative to Layer Normalization in Transformers, citing the relevant paper and explaining the mathematical formulation and its effect on signal propagation. It provides a basic synthesis by highlighting the role of ReZero in improving isometry and convergence speed. However, it lacks deeper critical analysis or comparison with other normalization approaches, and while it touches on a broader principle (signal propagation in deep networks), it does not elevate this to a meta-level insight."}}
{"id": "d96aa288-77f3-4bf6-931e-abd30aeafccb", "title": "Activation Function in FFN", "level": "subsubsection", "subsections": [], "parent_id": "e915275c-0207-4658-9d41-438a8b5b3e51", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Other Module-level Modifications"], ["subsection", "Position-wise FFN"], ["subsubsection", "Activation Function in FFN"]], "content": "The vanilla Transformer~ adopts the Rectified Linear Units (ReLU) activation for non-linearity in between the two FFN layers. Over time, several studies have explored different activation other than ReLU.\n try to replace ReLU in Transformer with Swish function $f(x)=x\\mathrm{sigmoid}(\\beta x)$ and observe that it consistently improve performance on WMT 2014 English$\\rightarrow$German dataset.\nGPT~ replace ReLU with Gaussian Error Linear Unit (GELU)~ on language pre-training. It becomes the default practice for many pre-trained language models ~.\n explore using Gated Linear Units (GLU)~ and its variants as a drop-in replacement for ReLU in FFN. Their pre-training experiments show that the GLU variants consistently improve vanilla Transformer with ReLU activation. Note that GLU introduces extra parameters and the experiments are conducted with the intermediate dimension of FFN reduced to match the parameter count with baseline.", "cites": [1481, 1512, 7055, 1455, 7, 7365, 38], "cite_extract_rate": 0.875, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic factual summary of the activation functions used in FFNs of Transformer variants, citing several papers. However, it lacks deeper synthesis of ideas, critical evaluation of trade-offs (e.g., increased parameters with GLU), and broader abstraction to highlight overarching trends or design principles in activation function choices."}}
{"id": "e9f737ba-3b14-4121-b536-765cb8c260d1", "title": "Adapting FFN for Larger Capacity", "level": "subsubsection", "subsections": [], "parent_id": "e915275c-0207-4658-9d41-438a8b5b3e51", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Other Module-level Modifications"], ["subsection", "Position-wise FFN"], ["subsubsection", "Adapting FFN for Larger Capacity"]], "content": "Several works have focused on expanding FFNs in order for a larger model capacity. The basic idea is to replace FFNs with similar structures with much more parameters.\n replace some of the FFNs with the product-key memory layers. A product-key memory is composed of three components: a query network, a key selection module containing two\nsets of sub-keys, and a value lookup table. The model first projects an input to a latent space using the query network, and then compares the generated query to keys that are Cartesian product of the two sets of sub-keys from key selection module to get $k$ nearest neighbors, and finally finds the corresponding values in a value lookup table using the $k$ nearest keys and aggregates them to produce the final output. This process resembles the attention mechanism, in that the generated query attends to a large number of global key-value pairs. They thus propose a multi-head mechanism for the key-product memory to further enlarge the capacity of this module. The experiments on large-scale language modeling suggest that this mechanism significantly improves performance with negligible computational overhead.\nSeveral studies exploits the idea of Mixture-of-Experts (MoE) to increase the capacity of FFNs. Gshard uses sparsely-gated MoE layers to replace FFNs in Transformer. Each MoE layer consists of several FFNs (each called an expert) that are the same structure as position-wise FFNs in vanilla Transformer. The output of the layer is a weighted sum of the outputs of the FFNs, using gate values computed by a routing function $g(\\cdot)$. They design a learnable routing function that assigns tokens to experts, with auxiliary loss to satisfy balanced loads between experts and efficiency at the scale of length such that the experts can be distributed across multiple devices. For each forward pass of the MoE layer, only the experts with top-$k$ gate values are activated.\nInstead of using $k$ experts for each forward pass, Switch Transformer~ proposes to route using only a single expert with the largest gate value, leading to a much smaller computational footprint. The authors also design an auxiliary loss to encourage load balance between experts. It is reported to speed up pre-training by a large margin compared to the non-MoE counterpart while having a similar number of FLOPS.\n propose to replace top-$k$ routing with expert prototyping strategy. Specifically, the proposed strategy splits experts into $k$ different groups and applies top-1 routing within each group. The outputs of prototype groups are combined linearly to form the final output of the MoE layer. This strategy is proved to improve the model quality while maintaining constant computational costs.\nAs opposed to using a learnable routing function for expert assignment,   design \\textit{hash layers} where tokens are hashed into a fixed number of buckets, each bucket corresponding to an expert. This approach requires no routing parameters or any auxiliary loss function, while showing competitive results with existing methods such as Switch Transformer~.", "cites": [680, 1490, 1451, 8454, 707], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple approaches for increasing the capacity of position-wise FFNs, including product-key memory, MoE, Switch Transformer, and hash layers. It connects these ideas under the broader goal of enhancing model capacity while maintaining computational efficiency. While it provides a clear comparison of routing strategies, it could offer a deeper critical evaluation of trade-offs and limitations. Nonetheless, it identifies patterns such as sparsity and hashing as generalizable techniques for large-scale model scaling."}}
{"id": "d1496356-82a7-4f73-8a03-62f59057e605", "title": "Dropping FFN Layers", "level": "subsubsection", "subsections": [], "parent_id": "e915275c-0207-4658-9d41-438a8b5b3e51", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Other Module-level Modifications"], ["subsection", "Position-wise FFN"], ["subsubsection", "Dropping FFN Layers"]], "content": "Notably, one might argue that under some circumstances, FFN layers can be dropped completely, resulting in a simplified network.\n demonstrate that replacing the ReLU activation with Softmax and dropping the bias term in FFN effectively turns FFN into an attention module where position-wise inputs attend to a global key-value memory of $D_{\\mathrm{ffn}}$ slots. They thus propose to drop the FFN module and add to the attention module a set of global key-value pairs, which are learnable parameters concatenated with key and values generated by inputs. This approach simplifies the structure of the network with no loss of performance.\n empirically show that FFNs in the decoder of Transformer, despite its large number of parameters, is not efficient and can be removed safely with only slight or no loss of performance. This approach significantly boosts the training and inference speed.", "cites": [1463, 7053], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes two distinct papers to form a coherent narrative around the role and potential removal of FFN layers in Transformers, highlighting functional overlaps and performance implications. It provides a critical view by noting the inefficiency of decoder FFNs and the potential for simplification without performance loss, but lacks deeper comparative or evaluative analysis of their approaches. The abstraction is moderate, as it identifies a general trend (redundancy of FFN layers in certain configurations) but does not elevate it to a meta-level principle."}}
{"id": "f397ed71-a760-4154-8eaa-b1b20d5004b2", "title": "Adapting Transformer to Be Lightweight", "level": "subsection", "subsections": [], "parent_id": "04795308-eff0-4fbb-962f-a8077d36f894", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Architecture-level Variants"], ["subsection", "Adapting Transformer to Be Lightweight"]], "content": "Apart from the efforts made at the module level to alleviate computation overheads, there are several attempts to adapt Transformer to be lightweight by modifications at a higher level.\nSimilar to low-rank self-attention~ that decomposes attention into a locality-constrained attention and a low-rank global attention, Lite Transformer~ proposes to replace each attention module in Transformer with a two-branch structure, where one branch uses attention to capture long-range contexts while the other branch uses depth-wise convolution and linear layers to capture local dependencies. The architecture is lightweight both in terms of model size and computation, and is thus more suitable for mobile devices.\nFunnel Transformer~ utilizes a funnel-like encoder architecture where the length of the hidden sequence is gradually reduced using pooling along the sequence dimension, and then recovered using up-sampling. The architecture effectively reduces the FLOPs and memory compared to the vanilla Transformer encoder. Naturally, one can use this architecture to build a deeper or wider model using the same computation resources.\nDeLighT~ replaces the standard Transformer block with \\texttt{DeLighT} block, which consists of three sub-modules: (1) a ``expand-and-reduce'' \\texttt{DeLighT} transformation module to learn wider representations with low computation requirements; (2) a single-head self-attention to learn pair-wise interaction; (3) a lightweight ``reduce-and-expand'' FFN (as opposed to vanilla Transformer that first expands the dimension of hidden representations and then reduces them back to $D_m$). They also propose a block-wise scaling strategy that allows for shallower and narrower blocks near the input and wider and deeper blocks near the output. The induced network is much deeper than the vanilla Transformer but with fewer parameters and operations.", "cites": [1468, 1488], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of how different Transformer variants reduce computational load through architectural changes, but does not deeply synthesize or compare the approaches. It mentions each method in isolation, focusing on their components without broader generalization or critical evaluation."}}
{"id": "736cefee-90d8-4e53-a49e-f031442dc1ba", "title": "Strengthening Cross-Block Connectivity", "level": "subsection", "subsections": [], "parent_id": "04795308-eff0-4fbb-962f-a8077d36f894", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Architecture-level Variants"], ["subsection", "Strengthening Cross-Block Connectivity"]], "content": "In vanilla Transformer, each block takes outputs from the previous block as inputs and outputs a sequence of hidden representations. One might be interested in creating more paths along which input signals can run through the networks. In Sec.~\\ref{sec:prior_prev_module}, we introduced Realformer~ and Predictive Attention Transformer~ that reuses attention distributions from previous block to guide attention of current block. This can be seen as creating a forward path between adjacent Transformer blocks.\nIn a deep Transformer encoder-decoder model, the cross-attention modules in the decoder only utilize the final outputs of the encoder, therefore the error signal will have to traverse along the depth of the encoder.  This makes Transformer more susceptible to optimization issues (e.g., vanishing gradients). Transparent Attention~ uses a weighted sum of encoder representations at all encoder layers (including the embedding layer) in each cross-attention module. For the $j$-th decoder block, the cross-attention module is modified to attend to\n\\begin{equation}\\label{eq:transparent}\n    \\tilde{\\bH}^{(j)}=\\sum_{i=0}^{N}\\frac{\\exp(w_{ij})}{\\sum_{k=0}^N \\exp(w_{kj})}\\bH^{(i)},\n\\end{equation}\nwhere each $w_{ij}$ is a trainable parameter. This effectively shortens the path from each layer in the encoder to the error signal and thus eases the optimization of deeper Transformer models.\nAnother issue associated with vanilla Transformer is that each position can only attend to history representations from lower layers. Feedback Transformer~ proposes to add a feedback mechanism to Transformer decoder, where each position attends to a weighted sum of history representations from all layers\n\\begin{equation}\n    \\tilde{\\bh}_i=\\sum_{l=0}^{N}\\frac{\\exp(w_{l})}{\\sum_{k=0}^N \\exp(w_{k})}\\bh_i^{(l)}.\n\\end{equation}", "cites": [1457, 1486, 1492], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from three different Transformer variants (Transparent Attention, Feedback Transformer, and RealFormer) by identifying a common theme: enhancing cross-block connectivity. It provides a coherent narrative by linking the architectural modifications to optimization challenges in vanilla Transformers. While it includes some critical analysis by pointing out limitations such as vanishing gradients and restricted access to sequential information, it could offer deeper comparative insights or limitations of each approach for a higher score."}}
{"id": "690a01cd-27c7-4e1a-baef-83bbdeff8220", "title": "Adaptive Computation Time", "level": "subsection", "subsections": [], "parent_id": "04795308-eff0-4fbb-962f-a8077d36f894", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Architecture-level Variants"], ["subsection", "Adaptive Computation Time"]], "content": "Vanilla Transformer, like most neural models, utilizes a fixed (learned) computation procedure to process each input. An intriguing and promising modification is to make computation time conditioned on the inputs, i.e., to introduce Adaptive Computation Time (ACT)~ into Transformer models. Such modifications potentially give rise to the following advantages:\n\\begin{itemize}\n    \\item Feature refinement for hard examples. For data that are hard to process, a shallow representation might not be adequate to fulfill the task at hand. It would be more ideal to apply more computations to acquire a deeper and more refined representation.\n    \\item Efficiency for easy examples. When processing easy examples, a shallow representation might be enough for the task. In this case, it would be beneficial if the network can learn to extract features using reduced computation time.\n\\end{itemize}\n\\begin{figure}[htbp]\n\\begin{center}\n\\subfigure[dynamic halting]{\\label{fig:ut}\\hspace{2em}\n\\includegraphics[height=1.2in]{assets/ACT/ut.pdf}\\hspace{2em}\n}\n\\subfigure[conditional skipping]{\\label{fig:skip}\\hspace{2em}\n\\includegraphics[height=1.2in]{assets/ACT/skip.pdf}\\hspace{2em}\n}\n\\subfigure[early exit]{\\label{fig:exit}\\hspace{2em}\n\\includegraphics[height=1.2in]{assets/ACT/exit.pdf}\n}\n\\caption{Three typical ACT paradigms.}\\label{fig:act}\n\\end{center}\n\\end{figure}\nUniversal Transformer (UT)~ incorporates a recurrence-over-depth mechanism that iteratively refines representations for all symbols using a module that is shared over depth, as illustrated in Fig. \\ref{fig:ut}. It also adds a per-position dynamic halting mechanism that calculates a halting probability for each symbol at every time step. If a symbol's halting probability is greater than a predefined threshold, then the symbol's representation will remain unchanged for subsequent timesteps. The recurrence is stopped when all symbols halt or when a predefined maximum step is reached.\nConditional Computation Transformer (CCT)~ adds a gating module at each self-attention and feed-forward layer to decide whether to skip the current layer, as illustrated in Fig. \\ref{fig:skip}. The authors also introduce an auxiliary loss that encourages the model to adjust the gating modules to match the practical computation cost to the available computation budget.\nSimilar to the dynamic halting mechanism used in UT, there is a line of work dedicated to adapting the number of layers to each input in order to achieve a good speed-accuracy trade-off, which is called \\textit{early exit} mechanism, as illustrated in Fig. \\ref{fig:exit}. A commonly used technique is to add an internal classifier at each layer and jointly train all classifiers. The core of these methods is the criteria used to decide whether to exit at each layer. DeeBERT~ uses the entropy of the output probability distribution of the current layer to determine whether to exit. PABEE~ counts the number of times that the predictions remain unchanged to decide whether to exit.  design a window-based uncertainty criterion to achieve token-level partial exiting for sequence labeling tasks.  introduces a voting-based exiting strategy that considers at each layer predictions of all the past internal classifiers to infer the correct label and to decide whether to exit.", "cites": [683, 7273, 1485, 1449, 1478], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple papers on ACT-related techniques, integrating them into a coherent narrative about dynamic computation strategies in Transformers. It abstracts beyond individual systems by identifying common themes such as dynamic halting, conditional skipping, and early exit. While it provides a clear comparison of approaches, it stops short of deeply evaluating their limitations or trade-offs, resulting in a moderate critical score."}}
{"id": "88924fec-4dc9-4d83-b667-d6862e359751", "title": "Recurrent Transformers", "level": "subsubsection", "subsections": [], "parent_id": "9b2cdbdc-bac6-48ec-96d0-b5576360be13", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Architecture-level Variants"], ["subsection", "Transformers with Divide-and-Conquer Strategies"], ["subsubsection", "Recurrent Transformers"]], "content": "In recurrent Transformers, a cache memory is maintained to incorporate the history information. While processing a segment of text, the network reads from the cache as an additional input. After the processing is done, the network writes to the memory by simply copying hidden states or using more complex mechanisms. The abstract process is illustrated in Fig. \\ref{fig:recurrence}.\nTransformer-XL~ address the limitation of a fixed length context by caching representations from the previous segment and reuse it as an extended context when the model processes the current segment. For the $l$-th layer and the $(\\tau+1)$-th segment, the input representation $\\bH_{\\tau+1}^{(l-1)}$ is concatenated with the representation $\\bH_\\tau^{(l-1)}$ from previous segment to produce the keys and values\n\\begin{align}\n    \\tilde{\\bH}_{\\tau+1}^{(l)}&=[\\mathrm{SG}(\\bH_\\tau^{(l-1)})\\circ \\bH_{\\tau+1}^{(l-1)}]\\label{eq:xformerxl},\\\\\n    \\bK_{\\tau+1}^{(l)}, \\bV_{\\tau+1}^{(l)}&=\\tilde{\\bH}_{\\tau+1}^{(l)}\\bW^K, \\tilde{\\bH}_{\\tau+1}^{(l)}\\bW^V,\n\\end{align}\nwhere $\\bH_\\tau^{(0)}$ is defined as the word embedding sequence, $\\mathrm{SG}(\\cdot)$ denotes stop-gradient operation and $[\\bX\\circ\\bY]$ denotes concatenating the two vector sequences along the time dimension. This approach extends the maximum context length by $L\\times N_{\\text{mem}}$ where $L$ is the number of layers and $N_{\\text{mem}}$ is the length of cached memory sequence.\nCompressive Transformer~ extends this idea further by extending the cache with two levels of memory. In Transformer-XL, the activations from the previous segment are cached as a memory that is used to augment the current segment, and activations from older segments are discarded. Compressive Transformer, on the other hand,  applies a compression operation (e.g., Convolution, Pooling, etc.) on older activations and stores them in the compressed memory. In order to avoid the expensive backpropagating-through-time (BPTT) from training compression sub-network with gradients from the loss, they propose to use local loss functions where original memories are constructed from the compressed memories. This approach further extends the theoretical maximum history context length from $L\\times N_{\\text{mem}}$ of Transformer-XL to $L\\times (N_{\\text{mem}}+c\\times N_{\\text{cm}})$, where $c$ is the compression rate and $N_{\\text{cm}}$ is the length of compressed memory.\n Memformer~ extends the recurrence mechanism from decoder-only architecture to an encoder-decoder architecture. They introduce to the encoder a memory cross attention similar to the cross attention in vanilla Transformer to allow the Transformer encoder to attend to the memory. They also introduce a memory slot attention on top of the encoder output to explicitly write the memory for the next segment. To avoid BPTT over a long range of timesteps, they propose Memory\nReplay Back-Propagation (MRBP) algorithm, which replays\nthe memory at each timestep to accomplish gradient back-propagation over long unrolls.\n propose a simple fine-tuning mechanism to add recurrence to a pre-trained language model (e.g., GPT-2~).\n\\ifx \\smv \\undefined\nThey first compress the representations produced by the $\\tau$-th segment into one single vector representation, using a weighted average of pooled representations from each layer $l\\in\\{1,\\cdots,L\\}$\n\\begin{equation}\n    \\bz_\\tau=\\sum_{l=1}^Lw_l\\sum_{j=1}^{T_\\tau}\\bh_j^{(l)},\n\\end{equation}\nwhere $T_\\tau$ denotes the sequence length of the $\\tau$-th segment, $w_l=\\mathrm{softmax}(\\mathbf\\alpha)_l$ is the weight softmax-normalized from learnable parameters $\\mathbf\\alpha=[\\alpha_1,\\cdots,\\alpha_L]$. This compressed representation is then fed to a  feed-forward network to produce the memory state $\\bh_{\\text{prev},\\tau}$ for the $\\tau$-th segment, which is then prepended to the key-value inputs of a specific attention layer. This approach effectively extends the context length of a pre-trained language model, without significant change of the architecture of the original model.\n\\fi\nERNIE-Doc~ proposes an enhanced recurrence mechanism based on the recurrence mechanism used in Transformer-XL, by replacing the memory with the history representations from the $l$-th layer.\n\\ifx \\smv \\undefined\n\\begin{align}\n    \\tilde{\\bH}_{\\tau+1}^{(l)}&=[\\mathrm{SG}(\\textcolor[rgb]{0,0,1}{\\bH_\\tau^{(l)}})\\circ \\bH_{\\tau+1}^{(l-1)}],\n\\end{align}\nas opposed to using representations from the $(l-1)$-th layer in Eq. \\eqref{eq:xformerxl}. This modification essentially leads to a larger effective context length.\n\\fi", "cites": [1500, 7370, 1487], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key mechanisms from Transformer-XL, Compressive Transformer, and Memformer, effectively connecting their approaches to handling long-range dependencies through recurrence. It provides a coherent narrative by illustrating how each model builds upon or modifies the previous ones. There is some critical analysis in the explanation of training challenges (e.g., BPTT) and proposed solutions, but a deeper evaluation or comparison of effectiveness is missing. The section identifies patterns in memory usage and context extension, but the abstraction remains at a moderate level, focusing primarily on architectural trends."}}
{"id": "10bc35f7-356c-4cac-a874-ee859573c18e", "title": "6.5.2.1 Hierarchical for long sequence inputs", "level": "paragraph", "subsections": [], "parent_id": "6244ba7c-c68b-4615-be13-f855c4dbca70", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Architecture-level Variants"], ["subsection", "Transformers with Divide-and-Conquer Strategies"], ["subsubsection", "Hierarchical Transformers"], ["paragraph", "6.5.2.1 Hierarchical for long sequence inputs"]], "content": "For tasks with inherently long input length, one can use hierarchical Transformers for effective modeling of long-range dependencies. For document-level machine translation tasks,  introduce dependencies on the previous sentences from both the source and target sides when translating a sentence. They use an attention mechanism as the aggregation operation to summarize low-level information. For document summarization, HIBERT~ encodes a document of text by first learn sentence representations for all sentences and then use these sentence representations to encode document-level representations that are then used to generate the summary. The model uses the last hidden representation (corresponding to the \\texttt{EOS} token) as the representation for each sentence.  propose a similar hierarchical Transformer for multi-document summarization where the extracted low-level representations are aggregated using an attention layer with a global trainable query node and low-level representations as the source of key-value pairs. Hi-Transformer~ first utilizes a sentence Transformer and a document Transformer to hierarchically learn document context-aware sentence representations. The document context-aware sentence representations are then fed to another sentence Transformer to further improve the sentence context modeling.", "cites": [1482, 1477], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section describes hierarchical Transformers for long sequence inputs by mentioning several models and their basic operations, but it lacks synthesis of the underlying ideas across the cited works. It provides a factual summary without critical evaluation or comparison of the approaches. There is minimal abstraction or generalization to broader principles or frameworks in the design of hierarchical models."}}
{"id": "153b332f-732b-4172-8553-f1c9626b599f", "title": "6.5.2.2 Hierarchical for richer representations", "level": "paragraph", "subsections": [], "parent_id": "6244ba7c-c68b-4615-be13-f855c4dbca70", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Architecture-level Variants"], ["subsection", "Transformers with Divide-and-Conquer Strategies"], ["subsubsection", "Hierarchical Transformers"], ["paragraph", "6.5.2.2 Hierarchical for richer representations"]], "content": "One might also be interested in using hierarchical models to acquire richer representations that are beneficial to the tasks at hand. For example, TENER~ uses a low-level Transformer encoder to encode character features, which is then concatenated with word embeddings as the inputs to the high-level Transformer encoder. This incorporates more features and alleviates the problems of data sparsity and out-of-vocabulary (OOV). Recently emerging Vision Transformer~ divides an input image into several patches that serve as the basic input elements of Transformer, which potentially loses intrinsic pixel-level information within patches. To address this issue, Transformer in Transformer (TNT)~ uses at each layer an inner Transformer block that transforms pixel representations and an outer Transformer block that takes fused vectors of patch representations and pixel representations as input.", "cites": [732, 7368, 7367], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from three papers by connecting the use of hierarchical structures in both NLP and vision tasks, showing how each builds richer representations through multiple levels of encoding. It also points out limitations, such as the loss of pixel-level information in Vision Transformer, and proposes a hierarchical solution (TNT). However, the analysis remains somewhat surface-level and lacks deeper comparison or evaluation of the effectiveness of these hierarchical strategies."}}
{"id": "be720c58-062b-41bd-b6e9-732058f0f671", "title": "Exploring Alternative Architecture", "level": "subsection", "subsections": [], "parent_id": "04795308-eff0-4fbb-962f-a8077d36f894", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Architecture-level Variants"], ["subsection", "Exploring Alternative Architecture"]], "content": "Despite the success of Transformer architecture, one might question whether the current Transformer architecture is optimal. Interestingly, several studies have explored alternative architectures for Transformer.\n interpret Transformer as a numerical Ordinary Differential Equation (ODE) solver for a convection-diffusion equation in a multi-particle dynamic system and design Macaron Transformer, which replaces each Transformer block with a \\textit{FFN-attention-FFN} variant.\nSandwich Transformer~ explores reorganizing attention modules and FFN modules such that attention modules are mainly located in lower layers and FFN modules in upper layers. The induced model improves perplexity on multiple language modeling benchmarks, without increasing parameters, memory or training time.\nMask Attention Network (MAN)~ prepends a dynamic mask attention module to the self-attention module in each Transformer block. The mask is conditioned on token representations, the relative distance between tokens and head indices. The proposed dynamic mask attention is shown to effectively model locality in text data and the induced model consistently outperforms the baseline model in machine translation and abstractive summarization.\nNotably, there's a line of work that uses Neural Architecture Search (NAS) to search for alternative Transformer architectures. The Evolved Transformer (ET)~ employs evolution-based architecture search with the standard  Transformer architecture seeding the initial population. The searched model demonstrates consistent improvement\nover Transformer on several language tasks. As another representative work, DARTSformer applies differentiable architecture search (DARTS)~, combined with a multi-split reversible network and a backpropagation-with-reconstruction algorithm for memory efficiency. The resulting model consistently outperforms standard Transformer and compares favorably to larger ET models, with a significantly reduced search cost.", "cites": [8456, 1469, 544, 1475, 1483, 1461], "cite_extract_rate": 1.0, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several papers by connecting different architectural modifications of Transformers into a coherent narrative about exploring alternative designs. It also introduces a theme (e.g., using ODEs, reordering sublayers, and NAS) that ties the works together. While it provides some critical evaluation (e.g., noting that certain models outperform baselines), it does not deeply critique limitations or compare trade-offs in detail. The abstraction is moderate, identifying patterns in how different sublayer arrangements and search techniques improve model performance, but without forming a higher-level theoretical framework."}}
{"id": "b7522af4-45ec-452c-b804-11b6882e6225", "title": "Pre-trained Transformers", "level": "section", "subsections": [], "parent_id": "456ca844-322a-4fcf-90ea-531b14db9d9e", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Pre-trained Transformers"]], "content": "\\label{sec:ptm}\nAs a key difference from convolutional networks and recurrent networks that inherently incorporates the inductive bias of locality, Transformer does not make any assumption about how the data is structured. On the one hand, this effectively makes Transformer a very universal architecture that has the potential of capturing dependencies of different ranges. On the other hand, this makes Transformer prone to overfitting when the data is limited. One way to alleviate this issue is to introduce inductive bias into the model.\nRecent studies suggest that Transformer models that are pre-trained on large corpora can learn universal language representations that are beneficial for downstream tasks~. The models are pre-trained using various self-supervised objectives, e.g., predicting a masked word given its context. After pre-training a model, one can simply fine-tune it on downstream datasets, instead of training a model from scratch. To illustrate typical ways of using Transformers in pre-training, we identify some of the pre-trained Transformers and categorize them as follows.\n\\begin{itemize}\n    \\item \\textit{Encoder only}. A line of work uses the Transformer encoder as its backbone architecture. BERT~ is a representative PTM that is typically used for natural language understanding tasks. It utilizes Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) as the self-supervised training objective. RoBERTa~ further adapts the training of BERT and removes the NSP objective as it is found to hurt performance on downstream tasks.\n    \\item \\textit{Decoder only}. Several studies focus on pre-training Transformer decoders on language modeling. For example, the Generative Pre-trained Transformer (GPT) series (i.e., GPT~, GPT-2~, and GPT-3~) is dedicated to scaling pre-trained Transformer decoders and has recently illustrated that a large-scale PTM can achieve impressive few-shot performance with the task and examples fed to the model as constructed prompts~.\n    \\item \\textit{Encoder-Decoder}. There are also PTMs that adopt Transformer encoder-decoder as the overall architecture. BART~ extends the denoising objective of BERT to encoder-decoder architecture. The benefit of using an encoder-decoder architecture is that the inducing model is equipped with the ability to perform both natural language understanding and generation. T5~ adopts similar architecture and was one of the earliest studies that use task-specific text prefix in downstream tasks.\n\\end{itemize}\nSome of the Transformer architecture variants can also be applied to Transformer-based PTMs. For instance, BigBird~ introduced in Sec.~\\ref{sec:sparseattn} is a encoder-based PTM that uses compound position-based sparse attention to enable long sequence inputs. GPT-3~ uses alternating dense and locally banded sparse attention (which was also introduced in Sec.~\\ref{sec:sparseattn}) in self-attention modules. Switch Transformer~ is an encoder-based PTM that replaces FFN layers with mixture-of-experts layers and can increase parameter count while keeping the FLOPs per example constant.", "cites": [826, 1445, 679, 707, 9, 1499, 7], "cite_extract_rate": 0.7, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a factual overview of pre-trained Transformer models and their architectural types but lacks deeper synthesis or comparative analysis. It connects some ideas, such as the use of sparse attention in BigBird and GPT-3, but does not build a novel framework or highlight broader implications. The critique is minimal, focusing mostly on summarizing modifications rather than evaluating their strengths or limitations."}}
{"id": "b3d753a5-00c0-4eb4-b313-01e490dd9d3d", "title": "Applications of Transformer", "level": "section", "subsections": [], "parent_id": "456ca844-322a-4fcf-90ea-531b14db9d9e", "prefix_titles": [["title", "A Survey of Transformers"], ["section", "Applications of Transformer"]], "content": "\\label{sec:app}\nTransformer was originally designed for machine translation but has been widely adopted in various fields besides NLP, including CV and audio processing, due to its flexible architecture.\n(1) \\textit{Natural Language Processing}. Transformer and its variants have been extensively explored and applied in NLP tasks, e.g., machine translation~, language modeling~ and named entity recognition~. Massive effort has been dedicated to pre-training Transformer models on large-scale text corpora, which we believe is one of the major reasons of Transformer's wide application in NLP.\n(2) \\textit{Computer Vision}. Transformer have also been adapted for various vision tasks, e.g., image classification~, object detection~, image generation~ and video processing~.  and  provide reviews on existing work of visual Transformers. We encourage readers to refer to these surveys for further understand the current research progress on Transformers in CV.\n(3) \\textit{Audio Applications}. Transformer can also be extended for audio-related applications, e.g., speech recognition~, speech synthesis~, speech enhancement~ and music generation~.\n(4) \\textit{Multimodal Applications}. Owing to its flexible architecture, Transformer has also been applied in various multimodal scenarios, e.g., visual question answering~, visual commonsense reasoning~, caption generation~, speech-to-text translation~ and text-to-image generation~.", "cites": [7361, 1500, 7368, 7373, 1458, 1446, 7056, 7360, 7040, 7370, 1470, 768, 732, 38, 1272, 1514, 7339, 1447, 8459, 8458, 1502, 1469, 9, 1515, 1501, 1461, 1453, 1456, 1462, 1503, 1516, 1513], "cite_extract_rate": 0.7674418604651163, "origin_cites_number": 43, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a broad categorization of Transformer applications across different modalities but lacks synthesis of deeper connections between these applications. It describes individual domains and cites relevant papers, but does not integrate their findings or innovations into a cohesive narrative. There is minimal critical analysis or abstraction to broader trends or principles in the use of Transformers."}}
