{"id": "739d8dcb-1e21-4d11-ab59-ef21b7d88150", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "5be78ea8-4e37-4219-b1a2-958030400717", "prefix_titles": [["title", "A survey of face recognition techniques \\\\ under occlusion"], ["section", "Introduction"]], "content": "\\IEEEPARstart{F}{ace} recognition is a computer vision task that has been extensively studied for several decades~. Compared with other popular biometrics such as fingerprint, iris, palm, and vein, the face has a significantly better potential to recognize the identity in a non-intrusive manner. Therefore, face recognition is widely used in many application domains such as surveillance, forensics, and border control. With the development of deep learning techniques~ and the publicly available large-scale face datasets~, face recognition performance has improved substantially~. However, face recognition systems still tend to perform far from satisfactory when encountering challenges such as large-pose variation, varying illumination, low resolution, different facial expressions, and occlusion. Generally, face images stored in a gallery are of high quality and free from the above degradations, while probe faces are suffering from what can be seen as \\textit{a missing data problem} due to these challenges. Consequently, fewer facial parts are available for recognition, which induces a mismatch between feature available in probe faces and gallery faces.\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=3.2in]{figures/occlusion_examples.pdf}\n\\caption{Examples of occluded face images from the MAFA dataset.}\n\\label{fig:occlusion_examples}\n\\end{figure}\n\\begin{table}[t]\n\\renewcommand\\arraystretch{1.1}\n\\caption{A categorization of Occlusion Challenges}\\label{tab:occlusion_challenges}\n\\centering\n\\begin{tabularx}{0.5\\textwidth}{|l|X|}\n\\hline\n\\textbf{Occlusion Scenario} & \\textbf{Examples} \\\\\n\\hline\nFacial accessories & eyeglasses, sunglasses, scarves, mask, hat, hair\\\\\nExternal occlusions & occluded by hands and random objects\\\\\nLimited field of view & partial faces\\\\\nSelf-occlusions& non-frontal pose\\\\\nExtreme illumination& part of face highlighted\\\\\n\\hline\n\\multirow{3}{*}{Artificial Occlusions}&random black rectangles\\\\\n&random white rectangles\\\\\n&random salt \\& pepper noise\\\\\n\\hline\n\\end{tabularx}\n\\end{table}\nFacial occlusion~ is considered one of the most intractable problems because we do not have prior knowledge about the occluded part, which can be anywhere and of any size or shape in a face image. From a practical point of view, it is not feasible to collect a large-scale training dataset with all possible occlusions in a realistic scenario to use deep learning techniques. Therefore, the problem of face recognition under occlusions remains a challenge. Facial occlusion occurs when the subject wears accessories such as a scarf, a face mask, glasses, a hat, etc., or when random objects are present in front of the face. The recognition accuracy has been compromised in some way because of the higher inter-class similarity and the more considerable intra-class variations caused by occlusion. Facial appearance changes substantially due to occlusion, as illustrated in Fig.~\\ref{fig:occlusion_examples}. We present a categorization of occlusion challenges in different scenarios with their typical occlusion examples (see Table~\\ref{tab:occlusion_challenges}). Pose variation can partially be seen as a self-occlusion problem caused by a large rotation of the head. The self-occlusion problem due to pose variation is usually dealt with in pose correction and therefore not discussed here.\n{In most cases, occluded face recognition~(OFR) involves querying a gallery consisting of occlusion-free faces using a probe image from an alternative test dataset of occluded faces. Occluded faces rely on either the collection of real occlusions or synthetic occlusions. We first break down OFR research scenarios in the most obvious way by the pairs of images considered. Fig.~\\ref{fig:OFR_problems} offers an illustration of the five categories regarding OFR testing scenarios. More specifically, five widely used testing scenarios for OFR, ranging from most real to least real, are:\n\\begin{itemize}\n  \\item \\textbf{Real occlusions}: gallery images are mugshots free from occlusion while probe images are faces occluded by realistic images such as sunglasses, or a scarf.\n  \\item \\textbf{Partial faces}: gallery images are mugshots free from occlusion while test face images are partial faces; hence the name partial face recognition is given by researchers.\n  \\item \\textbf{Synthetic occlusions}: gallery images are faces in the wild which are captured from uncontrolled scenarios while probe faces are blocked with synthetic occlusions to simulate real occlusions. \n  \\item \\textbf{Occluding rectangle}: gallery images are occlusion-free mugshots while test face images are occluded with a rectangle such as white and black rectangles.\n  \\item \\textbf{Occluding unrelated images}: gallery images are mugshots free from occlusion while test face images are occluded with unrelated images such as a baboon, or a non-square image.\n\\end{itemize}\n\\begin{figure}[h!]\n\\centering\n\\includegraphics[width=0.45\\textwidth]{figures/OFR_problems.pdf}\n\\caption{Different occluded face recognition testing scenarios involved in OFR.}\n\\label{fig:OFR_problems}\n\\end{figure}\n}\nApproaches to recognizing faces under occlusions can be broadly classified into three categories~(shown in Fig.~\\ref{fig:methods_category}), which are 1) occlusion robust feature extraction~(ORFE), 2) occlusion aware face recognition~(OAFR) and 3) occlusion recovery based face recognition~(ORecFR). An OFR system consists of three components, each corresponding to an important design decision: cross-occlusion strategy, feature extraction, and comparison strategy. Of these components, the second and third have analogues in general face recognition, while cross-occlusion strategy is unique to OFR.\n\\begin{itemize}\n\\item \\textbf{ORFE category} searches for a feature space that is less affected by facial occlusions. Generally, patch-based engineered and learning-based features are used as the cross-occlusion strategy.\n\\item \\textbf{OAFR category} is explicitly aware where the occlusion is. Generally, occlusion-discard is applied as the cross-occlusion strategy. As a result, only visible face parts qualify for face recognition~(i.e., feature extraction, feature comparison). Furthermore, we classify partial face recognition approaches as OAFR because they exclude occlusion parts from face recognition, assuming that visible parts are ready in the beginning.\n\\item \\textbf{ORecFR category} intends to recover an occlusion-free face from the occluded face to meet the demands of conventional face recognition systems. In other words, it takes occlusion recovery as the cross-occlusion strategy.\n\\end{itemize}\n\\begin{figure}[h!]\n\\centering\n\\includegraphics[width=0.5\\textwidth]{figures/methods_category.pdf}\n\\caption{The three categories of methods for face recognition under occlusion challenges.}\n\\label{fig:methods_category}\n\\end{figure}\nNumerous methods have been proposed to push the frontier of face recognition research. Several comprehensive surveys~ have been published for face recognition under occlusion. Of the existing works, the survey by Lahasan et al.~ on face recognition under occlusion challenges presents a thorough overview of the approaches before 2017, which is the most relevant to this paper. However, there are at least two reasons why a new survey on occluded face recognition is needed. \\textit{First, the explosive growth of face recognition techniques these years has stimulated many innovative contributions to handle occluded face recognition problems.} The increased number of publications over the last few years calls for a new survey for occluded face recognition, including up-to-date approaches, especially deep learning techniques. \\textit{Second, several large-scale occluded face datasets have become publicly available in recent years.} Without large-scale training data of occluded face images, deep learning models cannot function well~. Recently, the MAFA dataset~ is accessible for occluded face detection, and the IJB-C dataset~ is introduced as a general evaluation benchmark to include meta-information regarding the occlusion~(i.e., occlusion location, occlusion degree). Predictably, these datasets would encourage occluded face recognition to develop faster. The proposed survey provides a systematic categorization of methods for face recognition. Specifically, occluded face detection techniques are briefly reviewed because an OFR system requires the application of occluded face detection as the first step. Moreover, newly published and innovative papers addressing occlusion problems are thoroughly reviewed. Finally, we represent comparative performance evaluations in terms of occluded face detection and face recognition on widely used datasets as well as newly-developed large-scale datasets.\nThe remainder of the paper is organized as follows: occluded face detection techniques are introduced in Section 2. Methods of occlusion robust feature extraction are described and analyzed in Section 3. We review occlusion-aware face recognition approaches in Section 4. Then Section 5 briefly studies occlusion-recovery face recognition methods. A performance evaluation of the reviewed approaches is given in Section 6. In section 7, we discuss future challenges to datasets as well as to research. Finally, we draw an overall conclusion for occluded face recognition.", "cites": [1214, 1222, 514, 1218, 305, 97, 1219, 1220, 1221, 1210, 1216, 8426, 1215, 1217], "cite_extract_rate": 0.5384615384615384, "origin_cites_number": 26, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of occluded face recognition (OFR) challenges and categorizes existing approaches, but it integrates cited papers minimally and primarily for context rather than to build a deeper synthesis. It lacks critical evaluation of the cited works and does not offer a nuanced analysis of trends or limitations. The section identifies some general patterns (e.g., the importance of large datasets for deep learning), but these insights remain surface-level."}}
{"id": "849dd43f-ae0a-4996-be33-2fabdb84e136", "title": "General face detection", "level": "subsection", "subsections": [], "parent_id": "02c9227b-1254-4e45-872c-168a9224ca8e", "prefix_titles": [["title", "A survey of face recognition techniques \\\\ under occlusion"], ["section", "Occluded face detection"], ["subsection", "General face detection"]], "content": "Face detection generally intends to detect the face that is captured in an unconstrained environment. It is challenging due to large-pose variation, varying illumination, low resolution, and occlusion etc.~. Approaches to general face detection can roughly be classified into three categories, which are \n\\begin{itemize}\n\\item Rigid templates based face detection.\n\\item Deformable part models~(DPM) based face detection.\n\\item Deep convolutional neural networks~(DCNNs) based face detection. \n\\end{itemize}\nThe Viola-Jones face detector and its variations~ are typical in the rigid templates based category, which utilizes Haar-like features and AdaBoost to train cascaded classifiers and can achieve good performance with real-time efficiency. However, the performance of these methods can drop dramatically when handling real-world application~. In contrast, DPM based face detection can achieve significantly better performance based on the cost of high computational complexity~. A third, most promising category of research is DCNNs based face detection~. Some methods~ joint face detection with face alignment to exploit their inherent correlation to boost the performance. There are two major branches of object detection framework: (i)~region proposals based CNN~(i.e.,two-stage detectors), such as R-CNN~, fast R-CNN~, faster R-CNN~; (ii)~region proposals free CNN~(i.e.,one-stage detectors), such as the Single-Shot Multibox Detector~(SSD)~, YOLO~. In short, two-stage detectors achieve higher performance but are time-consuming. One-stage detectors have significant computational advantages but compensate by less accurate detection results. Some methods~ treat a face as a natural object and adopt techniques from object detection in face detection. Most recently, finding tiny faces has become popular in face detection and superior performance has been achieved~. \nRecent years have witnessed promising results of exploring DCNNs for face detection with the introduction of Widerface~, which offers a wide pose variation, significant scale difference~(tiny face), expression variation, make-up, severe illumination, and occlusion. Up to now, Feature Agglomeration Networks~(FANet)~, a single-stage face detector, achieves the state-of-art performance on several face detection benchmarks include the FDDB~, the PASCAL Face~, and the Widerface benchmark~. To exploit inherent multi-scale features of a single convolutional neural network, FANet introduced an Agglomeration Connection module to enhance the context-aware features and augment low-level feature maps with a hierarchical structure so that it can cope with scale variance in face detection effectively. Besides, Hierarchical Loss is proposed to train FANet to become stable and better in an end-to-end way. \\textbf{In short, methods that achieve remarkable detection performance, for example on the Widerface dataset, also provide a solid solution for occluded face detection.}", "cites": [1223, 8428, 1226, 8429, 206, 1229, 1227, 802, 1224, 8427, 1231, 209, 1228, 1230, 1225], "cite_extract_rate": 0.6, "origin_cites_number": 25, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key developments in face detection under occlusion by organizing methods into three coherent categories and drawing connections between them. It provides critical insights by comparing the trade-offs between two-stage and one-stage detectors and highlighting limitations (e.g., performance drops under small scale/occlusion). The abstraction is strong, identifying overarching trends such as multi-scale handling and the shift to deep learning. The section also links specific techniques, like FANetâ€™s Agglomeration Connection module and Hierarchical Loss, to broader challenges in occluded face detection."}}
{"id": "58332e8c-aea0-4d5e-96a0-5382fc491812", "title": "Detecting occluded face", "level": "subsection", "subsections": ["953118eb-8381-49cf-9c57-98f6fdbcc4d0", "ffc99e50-7659-4c74-829a-f7eec57fe6aa", "fd593a64-cb47-4681-bad4-618e0fd1044f"], "parent_id": "02c9227b-1254-4e45-872c-168a9224ca8e", "prefix_titles": [["title", "A survey of face recognition techniques \\\\ under occlusion"], ["section", "Occluded face detection"], ["subsection", "Detecting occluded face"]], "content": "Detecting partially occluded faces aims to locate the face region in a given image where occlusion is present. Handling occlusion in  face detection is challenging due to the unknown location and the type of occlusions~. Recently, occluded face detection is beginning to attract the attention of researchers; therefore a few publications are reviewed. At the same time, detecting occluded pedestrians is a long-standing research topic that has been intensively studied during the past few decades. Therefore, many researchers borrow techniques from pedestrian detection~ to push the frontier of occluded face detection by treating occlusion as the dominating challenge during the detection. \\textbf{Most occluded face detection methods report their performance on the MAFA dataset~ while general face detection methods do not, which means it is not a level playing field for general face detection and occluded face detection.} Approaches to detect partially occluded faces are roughly clustered as 1)~locating visible facial segments to estimate a full face; 2)~fusing the detection results obtained from face sub-regions to mitigate the negative impact of occlusion; 3)~using the occlusion information to help face detection in an adversarial way.", "cites": [7338], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section shows moderate synthesis by connecting occluded face detection with techniques from pedestrian detection, referencing one relevant paper. While it identifies a performance benchmarking issue (reliance on the MAFA dataset), the critical evaluation is limited and lacks in-depth discussion of methodological strengths or weaknesses. It abstracts the approaches into three general categories, which provides a structured overview and suggests broader patterns, though the analysis remains grounded in the cited work without deeper meta-level generalization."}}
{"id": "953118eb-8381-49cf-9c57-98f6fdbcc4d0", "title": "Locating visible facial segments to estimate face", "level": "subsubsection", "subsections": [], "parent_id": "58332e8c-aea0-4d5e-96a0-5382fc491812", "prefix_titles": [["title", "A survey of face recognition techniques \\\\ under occlusion"], ["section", "Occluded face detection"], ["subsection", "Detecting occluded face"], ["subsubsection", "Locating visible facial segments to estimate face"]], "content": "If visible parts of a face are known, then difficulties in face detection due to occlusions are largely relieved. Observing that facial attributes are closely related to facial parts, the attribute-aware CNNs method~ intends to exploit the inherent correlation between a facial attribute and visible facial parts. Specifically, it discovers facial part responses and scores these facial parts for face detection by the spatial structure and arrangement. A set of attribute-aware CNNs are trained with specific part-level facial attributes (e.g., mouth attributes such as big lips, open mouth, smiling, wearing lipstick) to generate facial response maps. Next, a scoring mechanism is proposed to compute the degree of face likeliness by analyzing their spatial arrangement. Finally, face classification and bounding box regression are jointly trained with the face proposals, resulting in precise face locations. The results on FDDB~, PASCAL Face~ and AFW~ demonstrate that the proposed method is capable of yielding good performance. In particular, they can achieve a high recall rate of $90.99\\%$ on FDDB. In short, Ref.~ is the first systematic study to attempt face detection with severe occlusion without using realistic occluded faces for training.\nMore recently, the extension faceness-net~ improves the robustness of feature representations by involving a more effective design of CNN. As a result, it has achieved compelling results on the Widerface dataset~, which is challenging in terms of severe occlusion and unconstrained pose variations. However, it requires the use of labeled attributes of facial data to train attribute-aware CNN, which impairs its practical use in some way.", "cites": [1232, 1233, 1231], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes two related papers (doc_id: 1232 and 1233) on attribute-aware CNNs for occluded face detection, highlighting the use of facial part responses and spatial arrangement for face estimation. It also critically notes the limitation of requiring labeled attributes for training, which affects practical deployment. While it identifies a general approach (scoring facial parts for detection), it does not abstract beyond the specific methods to provide deeper, overarching principles."}}
{"id": "ffc99e50-7659-4c74-829a-f7eec57fe6aa", "title": "Fusing detection results obtained from face sub-regions", "level": "subsubsection", "subsections": [], "parent_id": "58332e8c-aea0-4d5e-96a0-5382fc491812", "prefix_titles": [["title", "A survey of face recognition techniques \\\\ under occlusion"], ["section", "Occluded face detection"], ["subsection", "Detecting occluded face"], ["subsubsection", "Fusing detection results obtained from face sub-regions"]], "content": "In paper~, a facial segment based face detection technique is proposed for mobile phone authentication with faces captured from the front-facing camera. The detectors are AdaBoost cascade classifiers trained with a local binary pattern (LBP) representation of face images. They train fourteen segments-based face detectors to help cluster segments in order to estimate a full face or partially visible face. As a result, this method could achieve excellent performance on the Active Authentication Dataset (AA-01)~. However, the use of simple architecture increases speed by compromising detection accuracy.\nThe introduction of MAFA~ offers plenty of faces wearing various masks, which  contributes significantly to the occluded face detection, especially of masked faces. Based on this dataset, an LLE-CNNs~ is proposed to benchmark the performance of masked face detection on the MAFA dataset. They extract candidate face regions with high-dimensional descriptors by pre-trained CNNs and employ locally linear embedding (LLE) to turn them into similarity-based descriptors. Finally, they jointly train the classification and regression tasks with CNNs to identify candidate facial regions and refine their position.\nTo avoid high false positives due to masks and sunglasses, a face attention network (FAN) detector~ is proposed to highlight the features from the face region. More specifically, the FAN detector integrates an anchor-level attention mechanism into a single-stage object detector like Feature Pyramid Networks~. The attention supervision information is obtained by filling the ground-truth box and is associated with the ground-truth faces which match the anchors at the current layer. The attention maps are first fed into an exponential operation and then combined with feature maps. As a result, the method is capable of achieving impressive results on the Widerface~ with an $88.5\\%$ average precision on the hard subset as well as on an $88.3\\%$ average precision on MAFA~ dataset.", "cites": [1234, 1235, 799, 1231], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates three distinct methods for fusing detection results from face sub-regions, showing some synthesis by connecting their use of sub-regions and attention mechanisms. It provides limited critical analysis, such as noting the trade-off between speed and accuracy in the first paper, but does not deeply evaluate the effectiveness or limitations of the approaches. There is moderate abstraction by grouping the methods under the theme of sub-region-based detection and highlighting their relevance to occluded face detection, but no overarching framework is proposed."}}
{"id": "fd593a64-cb47-4681-bad4-618e0fd1044f", "title": "Occlusion information adversarially used for detection", "level": "subsubsection", "subsections": [], "parent_id": "58332e8c-aea0-4d5e-96a0-5382fc491812", "prefix_titles": [["title", "A survey of face recognition techniques \\\\ under occlusion"], ["section", "Occluded face detection"], ["subsection", "Detecting occluded face"], ["subsubsection", "Occlusion information adversarially used for detection"]], "content": "Apart from selecting the visible facial parts and fusing results obtained from face sub-regions, it is a third way to minimize the adverse effects of face detection due to occlusions. One promising approach is to use a novel grid loss~, which has been incorporated into the convolutional neural network to handle partial occlusion in face detection. It is based on the observation that partial occlusions would confuse a subset of detectors, whereas the remaining ones can still make correct predictions. To this end, this work regards occluded face detection as a particular single-class object detection problem, inspired by other works on object detection~. Furthermore, the proposed grid loss minimizes the error rate on face sub-blocks independently rather than over the whole face to mitigate the adverse effect of partial occlusions and to observe improved face detection accuracy. \nUsing the occluded area as an auxiliary rather than a hindrance is a feasible solution to help face detection adversely. Adversarial occlusion-aware face detection (AOFD)~ is proposed to detect occluded faces and segment the occlusion area simultaneously. They integrate a masking strategy into AOFD to mimic different occlusion situations. More specifically, a mask generator is designed to mask the distinctive part of a face in a training set, forcing the detector to learn what is possibly a face in an adversarial way. Besides, an occlusion segmentation branch is introduced to help detect incomplete faces. The proposed multitask training method showed superior performance on general as well as masked face detection benchmarks. To cope with different poses, scales, illumination, and occlusions, Wu et al.~ introduce a hierarchical attention mechanism, applying long short-term memory~(LSTM) to predict face-specific attention. In this way, it can further model relations between the local parts and adjust their contribution to face detection. The proposed method achieves promising performance compared with Faster R-CNN~.", "cites": [1239, 209, 1236, 1238, 1237], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers, particularly connecting the concept of grid loss and adversarial occlusion-aware detection to tackle occluded face detection. It provides an analytical perspective by discussing how occlusion is not only a challenge but can be used as an auxiliary in training. However, it lacks deeper comparative or critical evaluation of the methods' strengths and limitations, and while it identifies a general approach (adversarial learning), it does not abstract these ideas into broader principles or frameworks beyond face detection."}}
{"id": "d3308e7d-a9e7-4953-a749-c8031b07ae82", "title": "Patch-based matching", "level": "subsubsection", "subsections": [], "parent_id": "b6740507-3d6d-4105-8204-4acaceeca84b", "prefix_titles": [["title", "A survey of face recognition techniques \\\\ under occlusion"], ["section", "Occlusion robust feature extraction"], ["subsection", "Patch-based engineered features"], ["subsubsection", "Patch-based matching"]], "content": "Beside representation, the distance metric also plays an important role. Elastic and partial matching schemes bring in a lot of flexibility when handling challenges in face recognition. Elastic Bunch Graph Matching~(EBGM)~ uses a graph to represent a face, each node of the graph corresponding to Gabor jets extracted from facial landmarks. The matching method is used to calculate the distance between corresponding representations of two faces. To take advantage of elastic and partial matching, Ref.~ proposes a robust matching metric to match Difference of Gaussian~(DoG) filter descriptor of a facial part against its spatial neighborhood in the other faces and select the minimal distance for face recognition. Specifically, they extract $N$ local descriptors from densely overlapped image patches. During the matching, each descriptor in one face is picked up to match its spatial neighborhood descriptors and then the minimal distance is selected, which is effective in reducing the adverse effects of occlusion. A random sampling patch-based method~ has been proposed to use all face patches equally to reduce the effects of occlusion. It trains multiple support vector machine~(SVM) classifiers with selected patches at random. Finally, the results from each classifier are combined to enhance the recognition accuracy. Similarly to elastic bunch graph matching~, dynamic graph representation~ is proposed to build feature graphs based on node representations. Each node corresponds to certain regions of the face image and the edge between nodes represents the spatial relationship between two regions. Dynamic graph representation can enable elastic and partial matching, where each node in one face image is matched with adjacent nodes in the other face image for face recognition, which brings in robustness to occlusion especially when encountering partially occluded biometrics.", "cites": [1240], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of patch-based matching techniques, including EBGM, random sampling with SVM, and dynamic graph representation. It connects these ideas under the theme of using patches and spatial relationships to handle occlusions, showing some synthesis. However, it lacks critical evaluation of their strengths, weaknesses, or trade-offs, and offers only basic abstraction by grouping methods around the concept of spatial patch matching without deeper theoretical insight."}}
{"id": "6f64193b-71e9-4beb-8053-5da4cc76979e", "title": "Learning-based features", "level": "subsection", "subsections": ["11d7267c-4813-4f72-84f7-ae9faa09157e", "f2381fe3-090e-4284-8e0f-f2eea1aa34fe", "cd378342-bfc7-4211-81d3-5a17939b2fa3", "2012a522-72b3-492e-8a83-c85e6b0ec9d5"], "parent_id": "0ab6d3e5-5f21-47d0-96aa-3f9c5795fb03", "prefix_titles": [["title", "A survey of face recognition techniques \\\\ under occlusion"], ["section", "Occlusion robust feature extraction"], ["subsection", "Learning-based features"]], "content": "Compared with engineered features, learned features are more flexible when various occlusion types at different locations are present. Features learned from training data can be effective and have potentially high discriminative power for face recognition. Unlike regular images, face images share common constraints, such as containing a smooth surface and regular texture. Face images are in fact confined to a face subspace. Therefore, \\textit{subspace learning} methods have been successfully applied to learn a subspace that can preserve variations of face manifolds necessary to discriminate among individuals. Taking the occlusion challenge as a major concern, it is natural to apply \\textit{statistical methods} on face patches allowing for the fact that not all types of occlusion have the same probability of occurring.~\\textit{Sparse representation classifier} methods, which fully explore the discriminative power of sparse representation and represent a face with a combination coefficient of training samples, have been the mainstream approach to handle various challenges in face recognition for a long time. The last few years have witnessed a great success of~\\textit{deep learning} techniques~, especially deep convolutional neural networks~(DCNN) for uncontrolled face recognition applications.", "cites": [297], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section introduces learning-based features and briefly mentions subspace learning, statistical methods, sparse representation classifiers, and deep learning. While it connects these concepts to the occlusion problem, it lacks in-depth synthesis or critical evaluation of the cited works. It primarily describes general categories and trends without comparing or analyzing them in a nuanced way."}}
{"id": "cd378342-bfc7-4211-81d3-5a17939b2fa3", "title": "Sparse representation classifier", "level": "subsubsection", "subsections": [], "parent_id": "6f64193b-71e9-4beb-8053-5da4cc76979e", "prefix_titles": [["title", "A survey of face recognition techniques \\\\ under occlusion"], ["section", "Occlusion robust feature extraction"], ["subsection", "Learning-based features"], ["subsubsection", "Sparse representation classifier"]], "content": "Apart from these statistical learning methods, several algorithms use sparse representation classifiers~(SRC) to tackle the occlusion in face recognition. Ever since its introduction~, SRC attracts increasing attention from researchers. This method explores the discriminative power of sparse representation of a test face. It uses a linear combination of training samples plus sparse errors to account for occlusion or corruption as its representation. Yang et al.~ propose a Gabor feature based Robust Representation and Classification (GRRC) scheme. They use Gabor features instead of pixel values to represent face images, which can increase the ability to discriminate identity. Moreover, the use of a compact Gabor occlusion dictionary requires less expensive computation to code the occlusion portions compared with that of the original SRC. To investigate the effectiveness of the proposed method, they conduct extensive experiments to recognize faces with block occlusions as well as real occlusions. A subset of the AR database was used in this experiment. It consists of 799 images (about eight samples per subject) of non-occluded frontal views with various facial expressions for training. The sunglasses test set contains $100$ images with the subject wearing sunglasses~(with a neural expression), and the scarves test set contains $100$ images with the subject wearing a scarf~(with a neural expression). The proposed GRRC achieves $93.0\\%$ recognition accuracy on sunglasses and $79\\%$ accuracy on scarves, which outperforms the original SRC by $5\\%$ and $20\\%$, respectively. In paper~, artificial occlusions are included to construct training data for training sparse and dense hybrid representation framework. The results show that artificially introduced occlusions are important to obtain discriminative features. Structured occlusion coding~(SOC)~ is proposed to employ an occlusion-appended dictionary to simultaneously separate the occlusion and classify the face. In this case, with the use of corresponding parts of the dictionary, face and occlusion can be represented respectively, making it possible to handle large occlusion, like a scarf. In paper~, efficient locality-constrained occlusion coding~(ELOC) is proposed to greatly reduce the running time without sacrificing too much accuracy, inspired by the observation that it is possible to estimate the occlusion using identity-unrelated samples.\nRecently, another work~ attempts face recognition with single sample per person and intends to achieve robustness and effectiveness for complex facial variations such as occlusions. It proposes a joint and collaborative representation with local adaptive convolution feature~(ACF), containing local high-level features from local regular regions. The joint and collaborative representation framework requires ACFs extracted from different local areas to have similar coefficients regarding a representation dictionary. Ref.~ proposes to learn a robust and discriminative low-rank representation~(RDLRR) for optimal classification, which aiming to prepare the learned representations optimally for classification as well as to capture the global structure of data and the holistic structure of each occlusion induced error image. The results demonstrate that the method can yield better performance in case of illumination changes, real occlusion as well as block occlusion.\nRef.~ combines center-symmetric local binary patterns~(CSLBP) with enhanced center-symmetric local binary patterns~(ECSLBP) to build the SRC dictionary for occluded face recognition. In Ref.~, discriminative multi-scale sparse coding~(DMSC) is proposed to address single-sample face recognition with different kinds of occlusion. More specifically, it learns the auxiliary dictionary to model the possible occlusion variations from external data based on PCA and proposes a multi-scale error measurement strategy to detect and disregard outlier pixels due to occlusion. Ref~ proposes a hierarchical sparse and low-rank regression model and uses features based on image gradient direction, leading to a weak low-rankness optimization problem. The model is suited for occluded face recognition and yields better recognition accuracy. In another work, NNAODL~(nuclear norm based adapted occlusion dictionary learning)~ has been proposed to construct corrupted regions and non-corrupted regions for occluded face recognition. The same occlusion parts in training images are used to construct the occlusions while normal training images are used to reconstruct non-occlusion regions, leading to improved computing efficiency. To cope with occluded face recognition with limited training samples, Ref.~ proposes a structural element feature extraction method to capture the local and contextual information inspired by the human optic nerve characteristics for face recognition. Besides, an adaptive fusion method is proposed to use multiple features consisting of a structural element feature, and a connected-granule labeling feature. It applies Reinforced Centrosymmetric Local Binary Pattern~(RCSLBP) to better handle the robustness to the occlusion. Finally, few-shot sparse representation learning is applied for few-shot occluded face recognition.", "cites": [1242, 1241], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of various learning-based feature extraction methods, particularly focusing on the use of sparse representation classifiers for occluded face recognition. While it mentions several approaches and their performance, it lacks deeper synthesis of the underlying ideas and does not clearly connect the cited works into a broader conceptual framework. There is minimal critical analysis or identification of overarching principles or limitations, which limits its analytical depth."}}
{"id": "2012a522-72b3-492e-8a83-c85e6b0ec9d5", "title": "Deep learning", "level": "subsubsection", "subsections": [], "parent_id": "6f64193b-71e9-4beb-8053-5da4cc76979e", "prefix_titles": [["title", "A survey of face recognition techniques \\\\ under occlusion"], ["section", "Occlusion robust feature extraction"], ["subsection", "Learning-based features"], ["subsubsection", "Deep learning"]], "content": "Face representation obtained by DCNN is vastly superior to other learning-based methods in discriminative power, owing to the use of massive training sets~. Face\nverification performance has been boosted due to advanced\ndeep CNN architectures~ and the development\nof loss functions~. From a practical point of view, we can obtain occlusion-robust face representation if a massive training dataset is provided to contain sufficient occluded faces for a deep network to learn to handle occlusions~. However, it is tough to collect such a dataset. Data augmentation seems to be a plausible solution to obtain a large number of occluded faces. Training with augmented occluded faces ensures the features are extracted more locally and equally~. Lv et al.~ synthesize occluded faces with various hairstyles and glasses for data augmentation to enlarge the training dataset, which alleviates the impact of occlusions. Specifically, $87$ common hairstyle templates with various bangs and $100$ glasses templates are collected to synthesize training face images with different hairstyles and face images with different glasses, to enable the CNN model is robust to various hairstyles and glasses. The method indeed relieves the data deficiency problem and results in improved performance. However, its use is limited to handling sunglasses and hair in recognition and does not solve the occlusion problem in general. In paper~, instead of using synthetic occluded faces directly, they first identify the importance of face regions in an occlusion sensitivity experiment and then train a CNN with identified face regions covered to reduce the model's reliance on these regions. Specifically, they propose to augment the training set with face images with occlusions located in high-effect regions~(the central part of the face) more frequently than in low effect regions~(the outer parts of the face). This forces the model to learn more discriminative features from the outer parts of the face, resulting in less degradation when the central part is occluded. However, pre-defined occlusions may cause performance degradation when dealing with face images with an occlusion that is not of the same size. Cen et al.~ propose a DDRC~(deep dictionary representation based classification) scheme to alleviate the occlusion effect in face recognition, where the dictionary is used to linearly code the deep features that are extracted from a convolutional neural network. The dictionary is composed of deep features of the training samples and an auxiliary dictionary associated with the occlusion patterns of the testing face samples. However, the proposed DDRC assumes that the occlusion pattern of the test faces is included in the auxiliary dictionary, which limits its use.", "cites": [514, 309, 305, 1218, 297, 1246, 97, 1219, 1245, 1243, 1244, 1210, 1217], "cite_extract_rate": 0.6842105263157895, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of deep learning approaches for occlusion-robust feature extraction, integrating several relevant papers to discuss methods like data augmentation and loss function design. It connects ideas across works and identifies limitations (e.g., predefined occlusions being suboptimal for variable occlusion sizes). However, it lacks deeper synthesis into a novel framework and broader abstraction to overarching principles, instead focusing on specific techniques."}}
{"id": "da8dc97f-e84d-4b22-b185-797346d85be3", "title": "Visible part selection", "level": "subsubsection", "subsections": [], "parent_id": "efaf8aff-3935-403d-889d-62b585a74f29", "prefix_titles": [["title", "A survey of face recognition techniques \\\\ under occlusion"], ["section", "Occlusion aware face recognition"], ["subsection", "Occlusion detection based face recognition"], ["subsubsection", "Visible part selection"]], "content": "Some works select visible facial parts for recognition and skip occlusion detection by assuming the prior knowledge of occlusion. Ref.~ compares several subspace-based methods including PCA, Non-negative matrix factorization~(NMF)~, Local NMF~ and Spatially Confined NMF~ and uses the partial information available for face recognition. During face recognition, the eye region is selected when people are wearing masks or veils, and the bottom region is used when people are wearing glasses. This method has a deficiency in flexibility use because well-aligned predefined subregions are hard to obtain in the real scenario. A paper~ in this direction extends NMF to include adaptive occlusion estimation based on the reconstruction errors. Low-dimensional representations are learned to ensure that features of the same class are close to that of the mean class center.  This method does not require prior knowledge of occlusions and can handle large continuous occlusions.\nIn paper~, a proposed MaskNet is added to the middle layer of CNN models, aiming to learn image features with high fidelity and to ignore those distorted by occlusions. MaskNet is defined as a shallow convolutional network, which is expected to assign lower weights to hidden units activated by the occluded facial areas. Recently, Song et al.~ propose a pairwise differential siamese network~(PDSN) to build the correspondence between the occluded facial block and the corrupted feature elements with a mask learning strategy. The results show improved performance on synthesized and realistic occluded face datasets.", "cites": [1247], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of visible part selection techniques by discussing their assumptions and limitations. It synthesizes a few key papers, connecting them under the theme of using masks to identify and focus on non-occluded facial regions. While there is some critique (e.g., lack of flexibility in predefined subregions), it is not deeply nuanced, and the abstraction remains at a moderate level, focusing mainly on methodological patterns rather than higher-level principles."}}
{"id": "f3fece0e-05eb-4a36-9d76-6cf62750af94", "title": "Sparse representation classifier", "level": "subsubsection", "subsections": [], "parent_id": "4a9e0415-0203-4c3f-b9a5-d55c4cf62f57", "prefix_titles": [["title", "A survey of face recognition techniques \\\\ under occlusion"], ["section", "Occlusion recovery based face recognition"], ["subsection", "Reconstruction based face recognition"], ["subsubsection", "Sparse representation classifier"]], "content": "The sparse representation classifier~ is considered to be the pioneering work on occlusion robust face recognition. This method explores the discriminative power of sparse representation of a test face. It uses a linear combination of training samples plus sparse errors accounting for occlusions or corruption as its representation. To better tackle the occlusion, the SRC introduces an identity matrix as an occlusion dictionary on the assumption that the occlusion has a sparse representation in this dictionary. Ref.~ improves sparse representation by including prior knowledge of pixel error distribution. In paper~, they design a graph model-based face completion system for partially occluded face reparation. They leverage image-based data mining to find the best-matched patch to guide occluded face synthesis from the images, derived from the selection of sparse representation classification (SRC). The final face completion proceeds in the graph-based domain with the help of graph Laplace. \nSimilarly, Ref.~ proposes a reconstruction approach consisting of occlusion detection and face reconstruction. First, the downsampled SRC is used to locate all possible occlusions at a low computing complexity. Second, all discovered face pixels are imported into an overdetermined equation system to reconstruct an intact face.\nAn innovative solution for the occlusion challenge is presented by structured sparse representation based classification (SSRC)~ to learn an occlusion dictionary. The regularization term of mutual incoherence forces the resulting occlusion dictionary to be independent of the training samples. This method effectively decomposes the occluded face image into a sparse linear combination of the training sample dictionary and the occlusion dictionary. The recognition can be executed on the recovered occlusion-free face images. Nevertheless, this method requires retraining of the model to handle the different occlusions. \nIn paper~, a new criterion to compute modular weight-based SRC is proposed to address the problem of occluded face recognition. They partition a face into small modules and learn the weight function according to the Fisher rate. The modular weight is used to lessen the effect of modules with low discriminant and to detect the occlusion module. More recently, Ref.~ proposes a robust representation to model contiguous block occlusions based on two characteristics. The first characteristic introduces a tailored potential loss function to fit the errors of distribution. Specifically, a Laplacian sparse error distribution or more general distributions based on M-Estimators. The second characteristic models the error image, which is the difference between the occluded test face and the unoccluded training face of the same identity, as low-rank structural. Wang et al.~ proposed a method equipped with two stages: varying occlusion detection stage consisting of occlusion detection and elimination, and iterative recovery stage consisting of occluded parts being recovered and unoccluded parts being reserved. With the use of iteratively recovered strategy, this joint occlusion detecting and recovery method can produce good global features to benefit classification.", "cites": [1248], "cite_extract_rate": 0.125, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.2, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several methods related to sparse representation for occlusion recovery, connecting them through a common theme of using structured sparsity for better reconstruction. It offers some critical evaluation, particularly in pointing out the requirement for retraining in SSRC and the iterative nature of Wang et al.'s method. While it identifies broader patterns in how occlusion is modeled (e.g., sparse errors, low-rank structures), it does not fully abstract to a higher-level framework or provide a deep comparative analysis across all cited works."}}
{"id": "7dca6067-aa37-43fe-9c0d-3c92132a216e", "title": "Deep learning", "level": "subsubsection", "subsections": [], "parent_id": "4a9e0415-0203-4c3f-b9a5-d55c4cf62f57", "prefix_titles": [["title", "A survey of face recognition techniques \\\\ under occlusion"], ["section", "Occlusion recovery based face recognition"], ["subsection", "Reconstruction based face recognition"], ["subsubsection", "Deep learning"]], "content": "A few works use deep learning techniques for occlusion reconstruction. One is work~, which extends a stacked sparse denoising autoencoder to a double channel for facial occlusion removal. It adopts the layerwise algorithm to learn a representation so that the learned encoding parameters of clean data can transfer to noisy data. As a result, the decoding parameters are refined to obtain a noise-free output. The other work~ proposes to combine the LSTM and autoencoder architectures to address the face de-occlusion problem. The proposed robust LSTM-Autoencoders (RLA) model consists of two LSTM components. One spatial LSTM network encodes face patches of different scales sequentially for robust occlusion encoding, and the other dual-channel LSTM network is used to decode the representation to reconstruct the face as well as to detect the occlusion. Additionally, adversarial CNNs are introduced to enhance the discriminative information in the recovered faces.", "cites": [1249], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of two deep learning approaches for occlusion recovery but lacks in-depth synthesis, critical evaluation, or abstraction. It connects the two works only at the surface level and does not analyze their strengths, weaknesses, or broader implications within the field."}}
{"id": "98ab77d1-15f9-4901-a3d8-7d63548773ce", "title": "Deep learning for blind inpainting", "level": "subsubsection", "subsections": [], "parent_id": "3f92ce98-5715-49bb-a7e0-65c6c135859e", "prefix_titles": [["title", "A survey of face recognition techniques \\\\ under occlusion"], ["section", "Occlusion recovery based face recognition"], ["subsection", "Inpainting"], ["subsubsection", "Deep learning for blind inpainting"]], "content": "Generative models are known for the ability to synthesize or generate new samples from the same distribution of the training dataset. The core problem in generative models is to address density estimation by unsupervised learning, which can be carried out by explicit density estimation~, or implicit density estimation~. The most prominent generative models are originated from the variational autoencoder~(VAE)~ and the generative adversarial network~(GAN)~. The traditional autoencoder is used to reconstruct data, and the VAE~ applies probabilistic spin to the traditional autoencoder to allow generating a new image. Having assumed that training data is generated from the underlying latent representation, the VAE derives a lower bound on the data likelihood that is tractable and can be optimized. This method is a principled approach to generative models, and useful features can be extracted by inference of $q(z|x)$. However, generated images are blurrier and of low quality. Instead, the GAN~ learns to generate a new image from training distribution using a minimax game between the generator network and the discriminator network. The discriminator wants to distinguish between real and fake images, while the generator wants to fool the discriminator by generating real-looking images. Ref.~ uses convolutional architectures in the GAN and can generate realistic samples. However, the GAN is notorious for unstable training characteristics. Makhzani et al.~ propose an adversarial autoencoder~(AAE) to use the GAN framework as a variational inference in a probabilistic autoencoder to ensure that generating from any part of prior space results in meaningful samples.\nThere are GAN variants for all kinds of applications. We focus on methods that are relevant to face image editing and image inpainting. One is a blind-inpainting work~ that combines sparse coding~ and deep neural networks to tackle image denoising and inpainting. In particular, a stacked sparse denoising autoencoder is trained to learn the mapping function from generated corrupted noisy overlapping image patches to the original noise-free ones. The network is regularized by a sparsity-inducing term to avoid over-fitting. This method does not need prior information about the missing region and provides solutions to complex pattern removal like the superimposed text from an image. Context Encoders~ combine the encoder-decoder architecture with context information of the missing part by regarding inpainting as a context-based pixel prediction problem. Specifically, the encoder architecture is trained on the input images with missing parts to obtain a latent representation while the decoder architecture learns to recover the lost information by using the latent representation. Pixel-wise reconstruction loss and adversarial loss are jointly used to supervise the context encoders to learn semantic inpainting results. Several variants of context encoders~ are proposed: some extend it by defining global and local discriminators~ and some take the result of context encoders as the input and apply joint optimization of image content and texture constraints to avoid visible artifacts around the border of the hole~. Ref.~ relies on the power of the generative network to complete the corrupted image without requiring an external training dataset. A partial convolution based network~ is proposed to only consider valid pixels and apply a mechanism that can automatically generate an updated mask, resulting in robustness to image inpainting for irregular holes. Information Maximizing Generative Adversarial Networks~(InfoGAN)~ maximize the mutual information between latent variables and the observation in an unsupervised way. It decomposes the input noise vector into the source of incompressible noise $z$ and the latent code $c$ which will target the salient structured semantic features of data distribution. By manipulating latent codes, several visual concepts such as different hairstyles, presence or absence of eyeglasses are discovered. Occlusion-aware GAN~ is proposed to identify a corrupted image region with an associated corrupted region recovered using a GAN pre-trained on occlusion-free faces. Ref.~ employs GAN for eyes-to-face synthesis with only eyes visible. Very recently, AttGAN, a face image editing method, has imposed attribute classification constraints to the generated image so that the desired attributes are incorporated. Hairstyles and eyeglasses that may cause occlusion in a face image are treated as attributes which can be triggered to be present or absent in the generated image. ERGAN~(Eyeglasses removal generative adversarial network)~ is proposed for eyeglasses removal in the wild in an unsupervised manner. It is capable of rendering a competitive removal quality in terms of realism and diversity. In paper~, ID-GAN~(identity-diversity generative adversarial network) combines a CNN-based recognizer and GAN-based recognition to inpaint realism and identity-preserving faces. The recognizer is treated as the third player to compete with the generator.", "cites": [1003, 1252, 1254, 8430, 1250, 1251, 8431, 1253, 524, 5680, 1255, 1256, 1002], "cite_extract_rate": 0.65, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.8}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by integrating multiple deep learning-based inpainting techniques, placing them within a coherent narrative on occlusion recovery. It offers critical analysis by pointing out limitations such as GAN's unstable training and blurry outputs from VAEs. The section also abstracts beyond individual papers by highlighting architectural trends (e.g., encoder-decoder, adversarial learning, partial convolutions) and conceptual themes like attribute manipulation and identity preservation in face inpainting."}}
{"id": "ea588296-1f71-45f6-b93c-6b1b44b18a26", "title": "Evaluation of Occluded Face Detection", "level": "subsection", "subsections": [], "parent_id": "14605930-a574-4766-b31c-9598c1677fbc", "prefix_titles": [["title", "A survey of face recognition techniques \\\\ under occlusion"], ["section", "Performance evaluation"], ["subsection", "Evaluation of Occluded Face Detection"]], "content": "There are several datasets for face detection benchmarking, which are the FDDB~, PASCAL Face~, AFW~, IJB-A~, Widerface~ and MAFA~ datasets. MAFA is created for occluded face detection, involving 60 commonly used masks, such as simple masks, elaborate masks, and masks consisting of parts of the human body, which occur in daily life. Besides, it contains $35,806$ face annotations with a minimum size of $32\\times 32$ pixels. Some examples of occluded face images are shown in Fig.~\\ref{fig:occlusion_examples}. To the best of our knowledge, the MAFA dataset takes the occlusions as the main challenge in face detection, so it is relevant to evaluate the capacity of occluded face detection methods. The performances of representative algorithms on the MAFA dataset are summarized in Table~\\ref{tab:ofdTab}~(derived from paper~). The precision on the MAFA testing set with the IoU threshold value $0.5$ is shown. Only a few methods report the results below.\n\\begin{table}[!htbp]\n\\renewcommand\\arraystretch{1.3}\n\\centering\n\\caption{Evaluation Summary of Different Occluded Face Detection Algorithms on MAFA. The setting `masked' only counts the faces annotated by the MAFA dataset and `w/o Ignored' counts all detected faces, including the ones that are missing annotation. }\\label{tab:ofdTab}\n\\begin{tabularx}{0.49\\textwidth}{p{2cm}Xp{1.5cm}p{1.6cm}}\n\\toprule\n Approach  &Publication  & Precision `masked' & Precision `w/o Ignored' \\\\ \\midrule\nOcclusion unaware&   &    NA & $60.8\\%$ \\\\\n\\midrule\n\\multirow{2}{*}{Occlusion aware}&  &  NA & $76.4\\%$\\\\\n&   & $76.5\\%$ & $88.3\\%$ \\\\\n\\midrule\nOcclusion segmentation &  &$83.5\\%$ & $91.9\\%$ \\\\ \\bottomrule\n\\end{tabularx}\n\\end{table}", "cites": [1225, 1235, 1231], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 9, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section integrates information from the cited papers to present a comparative view of occluded face detection methods on the MAFA dataset. It organizes results into categories, showing synthesis across sources. However, it lacks deeper critical evaluation of the methods or limitations, and only provides basic performance comparisons without abstracting broader principles or trends."}}
{"id": "037a6797-60e9-4040-a1f5-3c5f5f3bfc6c", "title": "Conclusion", "level": "section", "subsections": [], "parent_id": "5be78ea8-4e37-4219-b1a2-958030400717", "prefix_titles": [["title", "A survey of face recognition techniques \\\\ under occlusion"], ["section", "Conclusion"]], "content": "In this paper, we present a thorough survey of face recognition techniques under occlusion and systematically categorize methods up to now into 1) occlusion robust feature extraction, 2) occlusion aware face recognition, and 3) occlusion recovery based face recognition. Newly published and innovative papers, especially recent deep learning techniques for occluded face recognition, have been discussed. Furthermore, we report comparative performance evaluations in terms of occluded face detection and face recognition. In the end, we discuss future challenges in terms of dataset and research~(including potential solutions) that move the field forward.\n\\begin{table*}[htbp]\n\\renewcommand\\arraystretch{0.9}\n\\caption{Evaluation Summary of Representative Algorithms on AR dataset regarding identification rates. Three categories are: 1)~\\textbf{ORFE}=Occlusion Robust Feature Extraction, 2)~\\textbf{OAFR}=Occlusion Aware Face Recognition, and 3)~\\textbf{ORecFR}=Occlusion recovery FR. The detail of `Experiment settings' abbreviations is shown. In column `Occlusions,' the asterisk means specific occlusion under illumination. In column `Identification Rates,' TR means these methods train on session one.}\\label{tab:ARidenrate}\n\\centering\n\\begin{tabularx}{\\textwidth}{lXlXXccc}\n\\toprule\n\\multirow{2}{*}{Category}  & \\multirow{2}{*} {Publication} & \\multirow{2}{*}{Occlusions}&\\multirow{2}{*}{\\begin{tabular}[c]{@{}l@{}}Experiment\\\\Settings\\end{tabular}} &\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}\\# of Train\\\\Subjects\\end{tabular}}&\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}\\# of Test\\\\Subjects\\end{tabular}}& \\multicolumn{2}{c}{Identification Rates (\\%)} \\\\\n\\cline{7-8}\n  &&&&&&Session1& Session2\\\\\n\\midrule\n\\multirow{4}{*}{\\raggedright \\begin{tabular}[c]{@{}l@{}l@{}}\\textbf{ORFE:}\\\\Patch-engineered\\\\Features\\end{tabular} } \n &\\multirow{2}{*}{} & Sunglasses & \\multirow{2}{*}{S-TR-$8$IPS}&\\multirow{2}{*}{50} & \\multirow{2}{*}{50} &84.0&80.0\\\\\n &&Scarf & & &  & 100.0& 96.0\\\\\n&\\multirow{2}{*}{}& Sunglasses &  \\multirow{2}{*}{S-TR-SSPP}&\\multirow{2}{*}{100} & \\multirow{2}{*}{100} & 89.0&98.0\\\\\n&&Scarf&&&&73.0&92.0\\\\\n\\midrule\n\\multirow{30}{*}{\\raggedright \\begin{tabular}[c]{@{}l@{}l@{}}\\textbf{ORFE:}\\\\Learning-based\\\\Features\\end{tabular}} \n&\\multirow{2}{*}{} & Sunglasses &  \\multirow{2}{*}{S-TR-SSPP}&\\multirow{2}{*}{50} &\\multirow{2}{*}{50} & 52.0&NA\\\\\n&&Scarf&&&&48.0&NA\\\\\n&\\multirow{2}{*}{} &Sunglasses& \\multirow{2}{*}{S-TR-$8$IPS}&\\multirow{2}{*}{40} &\\multirow{2}{*}{40} & 80.0&NA\\\\\n&&Scarf&&&&70.8&NA\\\\\n\\cline{2-7}\n&\\multirow{2}{*}{}&Sunglasses& \\multirow{2}{*}{D-TR}&\\multirow{2}{*}{50}&\\multirow{2}{*}{50}&98.0&NA\\\\\n&&Scarf&&&&90.0&NA\\\\\n&\\multirow{2}{*}{}&Sunglasses& \\multirow{2}{*}{S-TR-8IPS}& \\multirow{2}{*}{100}&\\multirow{2}{*}{100}&97.5&NA\\\\\n&&Scarf&&&&93.5&NA\\\\\n&\\multirow{2}{*}{}&Sunglasses\\&Scarf& \\multirow{2}{*}{S-TR-3IPS}&\\multirow{2}{*}{100}&\\multirow{2}{*}{100}&89.0&NA\\\\\n&&Black block&&&& 94.0 &NA\\\\\n&\\multirow{2}{*}{}&Sunglasses& \\multirow{2}{*}{S-TR-7IPS}&\\multirow{2}{*}{100} &\\multirow{2}{*}{100}& 92.3&51.7\\\\\n&&Scarf&&&&95.0&84.3\\\\\n&\\multirow{3}{*}{}&Sunglasses$^\\star$& \\multirow{2}{*}{S-TR-Occ1}&\\multirow{3}{*}{100}&\\multirow{3}{*}{100}&\\multicolumn{2}{c}{93.0}\\\\\n&&Scarf$^\\star$&&&&\\multicolumn{2}{c}{92.7}\\\\\n&&Sunglasses\\&Scarf$^\\star$&&&&\\multicolumn{2}{c}{92.6}\\\\\n&\\multirow{2}{*}{}&Sunglasses$^\\star$& \\multirow{2}{*}{S-TR-3IPS}&\\multirow{2}{*}{100}&\\multirow{2}{*}{100}&95.7& NA\\\\\n&&Scarf$^\\star$&&&&96.3&NA\\\\\n&\\multirow{2}{*}{}&Sunglasses$\\star$& \\multirow{2}{*}{S-TR-Occ3}&\\multirow{2}{*}{100}&\\multirow{2}{*}{100}&TR&98.0\\\\\n&&Scarf$^\\star$&&&&TR&97.0\\\\\n&\\multirow{3}{*}{}&Sunglasses& \\multirow{2}{*}{S-TR-SSPP}&\\multirow{3}{*}{100}&\\multirow{3}{*}{100}&TR& 92.0\\\\\n&&Scarf&&&&TR& 91.0\\\\\n&&Sunglasses\\&Scarf&&&&TR& 82.5\\\\\n&\\multirow{2}{*}{}&Sunglasses$^\\star$& \\multirow{2}{*}{S-TR-SSPP}&\\multirow{2}{*}{80}&\\multirow{2}{*}{80}& 88.0&56.0\\\\\n&&Scarf$^\\star$&&&&69.0&44.0\\\\\n&\\multirow{2}{*}{}&Sunglasses$^\\star$& \\multirow{2}{*}{EX-TR-SSPP}&\\multirow{2}{*}{80} & \\multirow{2}{*}{80}& 95.8& 78.3\\\\\n&&Scarf$^\\star$&&&&90.0&77.9\\\\\n&&Sunglasses\\&Scarf&D-TR-DL&Webface+20 & 80 &100.0&96.3\\\\\n&&Sunglasses\\&Scarf&S-TR-SSPP&100&100&\\multicolumn{2}{c}{75.0}\\\\\n&\\multirow{2}{*}{}&Sunglasses& \\multirow{2}{*}{S-TR-7IPS}&\\multirow{2}{*}{50} & \\multirow{2}{*}{50}& 94.7& 85.3\\\\\n&&Scarf&&&&99.3&98.7\\\\\n&\\multirow{2}{*}{}&Sunglasses&\\multirow{2}{*}{S-TR-3IPS}&\\multirow{2}{*}{80}&\\multirow{2}{*}{80}&99.0&NA\\\\\n&&Scarf&&&&87.6&NA\\\\\n\\midrule\n\\multirow{8}{*}{\\raggedright \\begin{tabular}[c]{@{}l@{}l@{}}\\textbf{OAFR:}\\\\Occlusion Detection\\\\Face Recognition\\end{tabular}}\n&\\multirow{2}{*}{}&Sunglasses& \\multirow{2}{*}{EX-TR-SSPP}&\\multirow{2}{*}{100}&\\multirow{2}{*}{100}&NA&49.0\\\\\n&&Scarf&&&&NA&55.0\\\\\n&\\multirow{2}{*}{}&Sunglasses$^\\star$& \\multirow{2}{*}{S-TR-3IPS$^\\star$}&\\multirow{2}{*}{80}&\\multirow{2}{*}{80}& TR&54.2\\\\\n&&Scarf$^\\star$&&&&TR&81.3\\\\\n&\\multirow{2}{*}{}&Sunglasses$^\\star$& \\multirow{2}{*}{S-TR-8IPS-Occ1}&\\multirow{2}{*}{60}&\\multirow{2}{*}{60}&97.5&NA\\\\\n&&Scarf$^\\star$&&&&99.2&NA\\\\\n&\\multirow{2}{*}{}&Sunglasses& \\multirow{2}{*}{S-TR-RAND}&\\multirow{2}{*}{100}&\\multirow{2}{*}{100}&\\multicolumn{2}{c}{95.2}\\\\\n&&Scarf&&&&\\multicolumn{2}{c}{94.2}\\\\\n&\\multirow{2}{*}{}&Sunglasses&\\multirow{2}{*}{D-TR-DL}&\\multirow{2}{*}{Webface}&\\multirow{2}{*}{100}&99.7&NA\\\\\n&&Scarf&&&&100.0&NA\\\\\n\\midrule\n\\multirow{8}{*}{\\raggedright \\begin{tabular}[c]{@{}l@{}}\\textbf{OAFR:}\\\\Partial Face Recognition\\end{tabular}}\n&\\multirow{3}{*}{}&Sunglasses$^\\star$& \\multirow{2}{*}{S-TR-$7$IPS}&\\multirow{3}{*}{100}&\\multirow{3}{*}{100}&94.3&NA\\\\\n&&Scarf$^\\star$&&&&98.0&NA\\\\\n&&Sunglasses\\&Scarf$^\\star$&&&&96.2&NA\\\\\n&\\multirow{3}{*}{}&Sunglasses$^\\star$& \\multirow{2}{*}{S-TR-$14$IPS}&\\multirow{3}{*}{100}&\\multirow{3}{*}{100}&98.0&NA\\\\\n&&Scarf$^\\star$&&&&97.0&NA\\\\\n&&Sunglasses\\&Scarf$^\\star$&&&&97.5&NA\\\\\n&\\multirow{2}{*}{}&Sunglasses$^\\star$& \\multirow{2}{*}{S-TR-$7$IPS}&\\multirow{2}{*}{100}&\\multirow{2}{*}{100}&100.0&92.0\\\\\n&&Scarf$^\\star$&&&&100.0&95.3\\\\\n\\midrule\n\\midrule\n\\multirow{12}{*}{\\raggedright \\begin{tabular}[c]{@{}l@{}l@{}}\\textbf{ORecFR:}\\\\Occlusion Recovery\\\\Face Recognition\\end{tabular}}\n&~&Sunglasses\\&Scarf$^\\star$&S-TR-13IPS&100&100&TR&90.6\\\\\n&\\multirow{2}{*}{}&Sunglasses& \\multirow{2}{*}{S-TR-$8$IPS}&\\multirow{2}{*}{100}&\\multirow{2}{*}{100}&100.0&NA\\\\\n&&Scarf&&&&97.0&NA\\\\\n&&Sunglasses$^\\star$&\\multirow{2}{*}{S-TR-$7$IPS}&\\multirow{2}{*}{121}&\\multirow{2}{*}{121}&76.6&NA\\\\\n&&Scarf$^\\star$&&&&60.9&NA\\\\\n&&Sunglasses$^\\star$& \\multirow{2}{*}{S-TR-$8$IPS}&100&100&97.5&NA\\\\\n&\\multirow{2}{*}{}&Sunglasses& \\multirow{2}{*}{S-TR-$8$IPS}&\\multirow{2}{*}{100}&\\multirow{2}{*}{100}&94.5&NA\\\\\n&&Scarf&&&&95.0&NA\\\\\n&\\multirow{2}{*}{}&Sunglasses&\\multirow{2}{*}{S-TR-$3$IPS-Occ3}&\\multirow{2}{*}{120}&\\multirow{2}{*}{120}&NA&68.5\\\\\n&&Scarf&&&&NA&70.7\\\\\n&\\multirow{2}{*}{}&Sunglasses& \\multirow{2}{*}{S-TR-$2$IPS}&\\multirow{2}{*}{100}&\\multirow{2}{*}{100}&\\multicolumn{2}{c}{89.8}\\\\\n&&Scarf&&&&\\multicolumn{2}{c}{78.8}\\\\\n&\\multirow{2}{*}{}&Sunglasses&\\multirow{2}{*}{S-TR-$4$IPS}&\\multirow{2}{*}{120}&\\multirow{2}{*}{120}&99.2&99.7\\\\\n&&Scarf&&&&87.5&$83.6$\\\\\n\\bottomrule\n\\end{tabularx}\n\\end{table*}\n\\begin{table*}[!htbp]\n\\renewcommand\\arraystretch{0.95}\n\\caption{Evaluation Summary of Representative Algorithms on the other Benchmarks regarding identification rates. Three categories: 1)~\\textbf{ORFE}=Occlusion Robust Feature Extraction, 2)~\\textbf{OAFR}=Occlusion Aware Face Recognition, and 3)~\\textbf{ORecFR}=Occlusion recovery FR. In the `Occlusion Arguments' column, details including the size of occlusion, the occlusion ratio to the face image and portion of noise are listed. In `\\# of Train Subjects,' NA represents not available from the paper. The Rank 1 identification rates are shown. Some results are roughly estimated from the figure in the original papers. Since not all methods are of the same experimental settings, these methods cannot be compared properly. }\\label{tab:otheridenrate}\n\\centering\n\\begin{tabularx}{\\textwidth}{llXllcccX}\n\\toprule\nCategory & Publication & Occlusions&\\begin{tabular}[c]{@{}l@{}}Occlusion Arguments \\end{tabular}& Benchmark&\\begin{tabular}[c]{@{}c@{}}\\# of Train\\\\Subjects\\end{tabular}& \\begin{tabular}[c]{@{}c@{}}\\# of Test\\\\Subjects\\end{tabular}& \\begin{tabular}[c]{@{}c@{}}Identification \\\\Rates(\\%)\\end{tabular}\\\\\n\\midrule\n\\multirow{20}{*}{\\raggedright\\textbf{ORFE}}&&Black or White Rectangle &occlusion size:$50\\times50$&ORL&40&40&70.0\\\\\n&&Black Rectangle &occlusion size:$50\\times 50$&FERET&240&960&$78.5$\\\\\n&&Unrelated image &occlusion ratio:0.5&Extended Yale B&38&38&$65.3$\\\\\n&&Unrelated image &occlusion ratio:0.5&Extended Yale B&38&38&$87.4$\\\\\n&&Unrelated image &occlusion ratio:0.8&Extended Yale B&38&38&$70.0$\\\\\n&&Unrelated image &occlusion ratio:0.8&Extended Yale B&38&38&$53.0$\\\\\n&\\multirow{2}{*}{}&\\multirow{2}{*}{Wild condition}&NA&\\multirow{2}{*}{LFW}& 108 &50&$86.0$\\\\\n&&&NA&&851&124&$65.3$\\\\\n&\\multirow{3}{*}{}&Glasses\\&Sunglasses&NA&\\multirow{2}{*}{CAS-PEAL}&\\multirow{2}{*}{350}&\\multirow{2}{*}{350}&$96.6$\\\\\n&&Hat&NA&&&&$90.3$\\\\\n&&Unrelated image &occlusion ratio:0.5&Extended Yale B&30&30&78.4\\\\\n&\\multirow{3}{*}{}&Unrelated image &occlusion ratio:0.6&\\multirow{3}{*}{Extended Yale B}&\\multirow{3}{*}{38}&\\multirow{3}{*}{38}&$96.0$\\\\\n&&Random pixels Rectangle &occlusion ratio:0.6&&&&81.0\\\\\n&&Mixture noise&occlusion ratio:0.6&&&&$25.0$\\\\\n&&Unrelated image &occlusion ratio:0.3&Extended Yale B&30&30&$77.6$\\\\\n&\\multirow{4}{*}{}&Pepper noise&portion:40\\%&\\multirow{2}{*}{Extended Yale B}&\\multirow{2}{*}{38}&\\multirow{2}{*}{38}&$81.3$\\\\\n&&White Rectangle&NA&&&&$82.9$\\\\\n&&Salt\\&Pepper noise &portion:40\\%&\\multirow{2}{*}{CMU-PIE}&\\multirow{2}{*}{68}&\\multirow{2}{*}{68}&$98.5$\\\\\n&&White Rectangle&occlusion size:$10\\times10$&&&&$98.8$\\\\\n&\\multirow{3}{*}{}&Black Rectangle &occlusion ratio:0.3&\\multirow{3}{*}{Extended Yale B}&\\multirow{3}{*}{38}&\\multirow{3}{*}{38}&$99.2$\\\\\n&&Black Rectangle&occlusion ratio:0.4&&&&$97.6$\\\\\n&&Black Rectangle&occlusion ratio:0.5&&&&$96.1$\\\\\n&\\multirow{3}{*}{}&Unrelated image &occlusion ratio:0.5&\\multirow{2}{*}{Extended Yale B}&\\multirow{2}{*}{38}&\\multirow{2}{*}{38}&$96.9$\\\\\n&& Random pixels Rectangle&portion:70\\%&&&&$98.9$\\\\\n&&Black Rectangle&occlusion ratio:0.5&CMU-PIE&68&68&$93.9$\\\\\n\\midrule\n\\midrule\n\\multirow{10}{*}{\\raggedright \\textbf{OAFR} } & &Arbitrary Patches&NA&Partial-LFW&158&158&$34.8$\\\\\n&\\multirow{2}{*}{} &Arbitrary Patches&NA&Partial-LFW&158&158&$50.7$\\\\\n&&Unrelated image &occlusion ratio:0.5&Extended Yale B&32&32&$30.2$\\\\\n&\\multirow{2}{*}{}&Unrelated image &occlusion ratio:0.5&Extended Yale B&32&32&$56.7$\\\\\n&&Arbitrary Patches&NA&Partial-PubFig&60&140&$42.9$\\\\\n&\\multirow{3}{*}{}&Arbitrary Patches&NA&Partial-LFW&NA&1000&$27.3$\\\\\n&&Arbitrary Patches&NA&NIR-Distance&NA&276&$94.9$\\\\\n&&Arbitrary Patches&NA&Partial-YTF&NA&200&$61.0$\\\\\n&\\multirow{3}{*}{}&Arbitrary Patches&NA&Partial-LFW&NA&1000&$32.4$\\\\\n&&Arbitrary Patches&NA&NIR-Distance&NA&127&$92.8$\\\\\n&&Arbitrary Patches&NA&NIR-Mobile&NA&178&$93.8$\\\\\n\\midrule\n\\midrule\n\\multirow{8}{*}{\\raggedright \\textbf{ORecFR} } &&Unrelated image &occlusion ratio:0.7&Extended Yale B&38&38&$62.3$\\\\\n&\\multirow{2}{*}{}&Unrelated image &occlusion ratio:0.7&\\multirow{2}{*}{Extended Yale B}&\\multirow{2}{*}{38}&\\multirow{2}{*}{38}&$88.5$\\\\\n&&Multiple patches&occlusion ratio:0.8&&&&$96.0$\\\\\n&\\multirow{2}{*}{}&Black or White Rectangle &occlusion ratio:0.57&\\multirow{2}{*}{Extended Yale B}&\\multirow{2}{*}{38}&\\multirow{2}{*}{38}&$90.4$\\\\\n&&Unrelated image &occlusion ratio:0.5&&&&$87.7$\\\\\n&&Black rectangle on eyes&occlusion ratio:0.3&Extended Yale B&38&38&$98.6$\\\\\n&\\multirow{2}{*}{}&Unrelated image &occlusion ratio:0.6&\\multirow{2}{*}{Extended Yale B}&\\multirow{2}{*}{38}&\\multirow{2}{*}{38}&$95.8$ \\\\\n&&Unrelated image &occlusion ratio:0.9&&&&$71.9$\\\\\n\\bottomrule\n\\end{tabularx}\n\\end{table*}\n\\begin{table*}[!htbp]\n\\caption{Evaluation Summary of Representative Algorithms regarding verification rates. Occlusion Robust Feature Extraction and Occlusion Aware Face Recognition report the verification rates by cut-off rules. In the `Benchmark' column, `Partial-x' means the new partial faces originate from the database named `x.' Some results are roughly estimated from the figure in the original papers. Since not all methods are of the same experimental settings, these methods cannot be compared properly.}\\label{tab:allverirate}\n\\centering\n\\begin{tabularx}{0.68\\textwidth}{lllcl}\n\\toprule\nPublication & Occlusions&Benchmark& \\# of Subjects in Gallery & Verification Rates\\\\\n\\midrule\n&Eyeglasses& FRGC-v2 &466 & 90\\%@FAR=0.1\\%\\\\\n& Wild condition& LFW& 1680& 60\\%@FAR=0.1\\\\\n&Wild condition&LFW&1680&61\\%@FAR=0.1\\\\\n\\midrule\n\\midrule\n\\multirow{2}{*}{}&Arbitrary Patches&Partial-LFW&1680&50\\%@FAR=0.1\\\\\n&Arbitrary Patches&Partial-PubFig&140&63\\%@FAR=0.1\\\\\n&Arbitrary Patches&Partial-LFW&1000&29.8\\%@FAR=0.1\\%\\\\\n&Arbitrary Patches&Partial-LFW&1000&37.9\\%@FAR=0.1\\%\\\\\n\\bottomrule\n\\end{tabularx}\n\\end{table*}\n\\begin{table*}[!htbp]\n\\caption{Summary of Representative Algorithms based on components they work on during OFR. In the `data augmentation/recovery' category, data augmentation component means generating synthesized occluded faces while the data recovery component intends to eliminate the occluded facial part. The fusion strategy component consists of feature-level fusion as well as decision-level fusion.}\\label{tab:components}\n\\renewcommand\\arraystretch{1.3}\n\\centering\n\\begin{tabularx}{\\textwidth}{|l|X|}\n\\hline\nPipeline Category & Publication\\\\\n\\hline\nData Augmentation/ Recovery&\\\\\n\\hline\nFeature Extraction&\\\\\n\\hline\nFeature Comparison&\\\\\n\\hline\nFusion Strategy&\\\\\n\\hline\n\\end{tabularx}\n\\end{table*}\n\\begin{table*}[!htbp]\n\\caption{Summary of Representative Algorithms regarding application-oriented purpose from Fig.~\\ref{fig:methods_category}. There is a category for occluded face detection with the abbreviation `\\textbf{OFD}'. Three classes for occluded face recognition: 1)~\\textbf{ORFE}=Occlusion Robust Feature Extraction, 2)~\\textbf{OAFR}=Occlusion Aware Face Recognition, and 3)~\\textbf{ORFR}=Occlusion recovery FR. To our knowledge, there are no research works on detecting partial face; thus, we mark it as `NA' (not available).}\\label{apporiented}\n\\renewcommand\\arraystretch{1.3}\n\\centering\n\\begin{tabularx}{\\textwidth}{|c|l|X|}\n\\hline\nAbbreviation&Application-oriented Purpose &Publication\\\\\n\\hline\n\\textbf{OFD}&Occluded Face Detection&\\\\\n\\hline\n\\multirow{3}{*}{}\n\\multirow{2}{*}{\\textbf{ORFE}}& Patch based engineered features&\\\\\n\\cline{2-3}\n &Learning based features&\\\\\n\\hline\n\\multirow{3}{*}{\\textbf{OAFR}}\n&Occlusion Detection& \\\\\n\\cline{2-3}\n&Occlusion Discard Face Recognition&\\\\\n\\cline{2-3}\n&Partial Face Detection &NA\\\\\n\\cline{2-3}\n&Partial Face Recognition&\\\\\n\\hline\n\\multirow{2}{*}{\\textbf{ORFR}}\n&Occlusion Recovery&\\\\\n\\cline{2-3}\n&Occlusion Recovery Face Recognition&\\\\\n\\hline\n\\end{tabularx}\n\\end{table*}\n\\appendix\nA glossary of abbreviations and expansions for terminologies is illustrated in Table~\\ref{tab:abbreviation}.\n\\begin{table*}[!ht]\n\\renewcommand\\arraystretch{0.95}\n\\centering\n\\caption{A glossary of abbreviations and expansions for terminologies. }\\label{tab:abbreviation}\n\\begin{tabularx}{\\textwidth}{l l | l l } \n\\toprule\nAbbreviation  &  Expansion & Abbreviation  &  Expansion\\\\ \\midrule\nAAE & Adversarial AutoEncoder &LBP & Local Binary Patterns\\\\\nAFC&Adaptive Feature Convolution&LDA & Linear Discriminant Analysis\\\\\nAOFD & Adversarial Occlusion-aware Face Detection&LLE & Locally Linear Embedding\\\\\nCSLBP&Center-Symmetric Local Binary Patterns&LSTM & Long Short Term Memory\\\\\nDA& Denosing Autoencoder&MDSCNN&Multiscale Double Convolutional Neural Network\\\\\nDCNNs & Deep Convolutional Neural Networks&MKD&Multi-Keypoint Descriptors\\\\\nDDRC&Deep Dictionary Representation based Classification&NNAODL&Nuclear Norm based Adapted Occlusion Dictionary Learning\\\\\nDMSC&Discriminative Multiscale Sparse Coding&NMF&Non-negative Matrix Factorization\\\\\nDoG & Different of Gaussian filters &OAFR & Occlusion Aware Face Recognition\\\\\nDPM & Deformable Part Models&OFR & Occluded Face Recognition\\\\\nDSCNNs&Double Supervision Convolutional Neural Network &ORFE & Occlusion Robust Feature Extraction\\\\\nEBGM & Elastic Bunch Graph Matching&ORecFR & Occlusion Recovery based Face Recognition\\\\\nECSLBP &Enhanced Center-Symmetric Local Binary Patterns&PCA & Principal Component Analysis\\\\\nELOC&Efficient Locality-constrained Occlusion Coding&PDSN&Pairwise Differential Siamese Network\\\\\nERBF&Ensemble of Radial Basis Function&R-CNN &Regions with Convolutional Neural Networks\\\\\nERGAN&Eyeglasses Removal Generative Adversarial Network& RCSLBP&Reinforced Centrosymmetric Local Binary Pattern \\\\\nFAN & Face Attention Network&RDLRR&Robust Discriminative Low-Rank Representation\\\\\nFANet & Feature Agglomeration Networks&RLA & Robust LSTM-Autoencoders\\\\\nFCN&Fully Convolutional Network&RPSM&Robust Point Set Matching \\\\\nFW-PCA&Fast Weighted-Principal Component Analysis&SIFT & Scale Invariant Feature Transform\\\\\nGAN&Generative Adversarial Network&SOC&Structured Occlusion Coding\\\\\nGRRC & Gabor based Robust Representation and Classification&SOM & Self-Organizing Maps\\\\\nHOG & Histogram of Oriented Gradient&SRC & Sparse Representation Classifiers\\\\\nICA & Independent Component Analysis&SSD & Single-Shot multibox Detector\\\\\nID-GAN&Identity-Diversity Generative Adversarial Network &SSRC&Structured Sparse Representation Classification\\\\\nInfoGAN&Information maximizing Generative Adversarial Network&SVM & Support Vector Machine \\\\\nKCFA & Kernel Correlation Feature Analysis &VAE& Variational AutoEncoder\\\\ \nKLD-LGBP &Kullback-Leibler Divergence-Local Gabor Binary Patterns&YOLO & You Only Look Once\\\\\n\\bottomrule\n\\end{tabularx}\n\\end{table*}\n\\clearpage\n\\clearpage\n\\bibliographystyle{plain}\n\\newpage\n\\bibliography{references}\n\\end{document}", "cites": [1249, 1253, 524, 1241, 1231, 1232, 1234, 1235, 1233, 8431, 1251, 1247, 1255, 1242, 1002, 1254, 1237, 1250, 1248], "cite_extract_rate": 0.19, "origin_cites_number": 100, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "low", "analysis": "The conclusion section primarily presents a detailed comparative summary of various face recognition techniques under occlusion using tabulated results. While it categorizes methods and includes performance metrics, it lacks deeper synthesis of ideas or critical evaluation of the cited works. There is some abstraction in grouping methods by category, but no meta-level insights or identification of broader trends in the field."}}
