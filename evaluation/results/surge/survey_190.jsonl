{"id": "11d3c0c0-ce47-40d9-a735-e71e5f125e07", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "e0a4e394-ff2b-4185-9f15-26f30cfaf0c9", "prefix_titles": [["title", "Towards Reasoning in Large Language Models: A Survey"], ["section", "Introduction"]], "content": "Reasoning is a cognitive process that involves using evidence, arguments, and logic to arrive at conclusions or make judgments. It plays a central role in many intellectual activities, such as problem solving, decision making, and critical thinking. The study of reasoning is important in fields like psychology~, philosophy~, and computer science~, as it helps individuals make decisions, solve problems, and think critically.\nRecently, large language models (LLMs)  \\citep[][\\textit{inter alia}]{brown2020language,chowdhery2022palm,chung2022scaling,openai2022chatgpt} such as ChatGPT have made significant advancements in natural language processing and related fields. It has been shown that these models exhibit emergent behaviors, including the ability to ``reason'', when they are large enough . \nFor example, by providing the models with ``\\textit{chain of thoughts}'', i.e., reasoning exemplars, or a simple prompt ``\\textit{Let’s think step by step}'', these models are able to answer questions with explicit reasoning steps , \ne.g., ``all whales are mammals, all mammals have kidneys; therefore, all whales have kidneys.''\nThis has sparked considerable interest in the community since reasoning ability is a hallmark of human intelligence that is frequently considered missed in current artificial intelligence systems . \nHowever, despite the strong performance of LLMs on certain reasoning tasks, \nit remains unclear whether LLMs are actually reasoning and to what extent they are capable of reasoning.\nFor example,  claim that ``LLMs are decent zero-shot reasoners (p.~1)'', while  conclude that ``LLMs are still far from achieving acceptable performance on common planning/reasoning tasks which pose no issues for humans to do (p.~2).''\nThis limitation is also stated by : \n\\begin{itemize}[label={},leftmargin=5mm,topsep=5pt]\n    \\item ``we qualify that although chain of thought emulates the thought processes of human reasoners, this does not answer whether the neural network is actually \\textit{reasoning} (p.~9).''\n\\end{itemize}\nTherefore, in this paper, we aim to provide a comprehensive overview and engage in an insightful discussion on the current state of knowledge on this fast-evolving topic. We initiate our exploration with a clarification of the concept of reasoning (\\S \\ref{sec:definition}). Subsequently, we turn our attention to the techniques for enhancing/eliciting reasoning in LLMs (\\S \\ref{sec:method}), the methods and benchmarks for evaluating reasoning in LLMs (\\S \\ref{sec:evaluation}), and the key findings and implications in this field (\\S \\ref{sec:findings}). Finally, we reflect on and discuss the current state of the field (\\S \\ref{sec:discussion}).\n\\tikzstyle{my-box}=[\n    rectangle,\n    draw=hidden-draw,\n    rounded corners,\n    text opacity=1,\n    minimum height=1.5em,\n    minimum width=5em,\n    inner sep=2pt,\n    align=center,\n    fill opacity=.5,\n]\n\\begin{figure*}[tp]\n    \\centering\n    \\resizebox{\\linewidth}{!}{\n        \\begin{forest}\n            forked edges,\n            for tree={\n                grow=east,\n                reversed=true,\n                anchor=base west,\n                parent anchor=east,\n                child anchor=west,\n                base=left,\n                font=\\small,\n                rectangle,\n                draw=violet,\n                rounded corners,\n                align=left,\n                minimum width=4em,\n                edge+={darkgray, line width=1pt},\n                s sep=3pt,\n                inner xsep=2pt,\n                inner ysep=3pt,\n                ver/.style={rotate=90, child anchor=north, parent anchor=south, anchor=center},\n            },\n            where level=1{text width=7.7em,font=\\scriptsize,}{},\n            where level=2{text width=10.7em,font=\\scriptsize,}{},\n            where level=3{text width=13.8em,font=\\scriptsize,}{},\n            [\n                Reasoning in LLMs, ver\n                [\n                    Techniques (\\S \\ref{sec:method})\n                    [\n                        Fully Supervised Finetuning (\\S \\ref{sec:fully-sup})\n                    ]\n                    [\n                        Prompting \\& In-Context Learning (\\S \\ref{sec:prompting})\n                        [\n                            Chain of Thought and Its Variants (\\S\\ref{sec:cot})\n                        ]\n                        [\n                            Rationale Engineering (\\S\\ref{sec:rationale-eng})\n                        ]  \n                        [\n                            Problem Decomposition  (\\S\\ref{sec:prob-decom})\n                        ]  \n                        [\n                            Others (\\S\\ref{sec:others_prompting})\n                        ]\n                    ]\n                    [\n                        Hybrid Method (\\S \\ref{sec:hybrid})\n                        [\n                            Reasoning-Enhanced Training \\& Prompting (\\S \\ref{sec:trainig_and_prompting})\n                        ]\n                        [\n                            Bootstrapping \\& Self-Improving (\\S \\ref{sec:selfimprove})\n                        ]\n                    ]\n                ]\n                [\n                    Evaluation \\& Analysis (\\S \\ref{sec:evaluation})\n                    [\n                        End Task Performance (\\S \\ref{sec:endtask})\n                    ]\n                    [\n                        Analysis on Reasoning (\\S\\ref{sec:analysis})\n                    ]\n                ]\n                [\n                    Findings \\& Implications (\\S\\ref{sec:findings})\n                ]\n                [\n                    Reflection{,} Discussion \\& Future Directions (\\S\\ref{sec:discussion}), text width=12.7em\n                ]\n            ]\n        \\end{forest}\n    }\n    \\caption{The structure of the paper.}\n    \\label{fig:taxo}\n\\end{figure*}", "cites": [1578, 1550, 5949, 7094, 5948, 8556], "cite_extract_rate": 0.5454545454545454, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from multiple cited papers, such as the emergence of reasoning in LLMs and the role of prompting techniques like chain of thought. It provides a critical perspective by highlighting conflicting viewpoints on the extent of reasoning capabilities in LLMs. The section also introduces a high-level framework for the paper's structure, showing some abstraction beyond individual papers, but it stops short of deeper theoretical synthesis or comprehensive meta-level insights."}}
{"id": "bf709e30-a5ac-4aca-b384-38a22d4e122b", "title": "Towards Reasoning in Large Language Models", "level": "section", "subsections": ["67834244-0eca-466b-a79c-ec86fc61ffe1", "e6815bed-d10b-467d-be19-c8e625845fb1", "c0e116f0-fa6c-4c1a-ac2c-bca76afc4616"], "parent_id": "e0a4e394-ff2b-4185-9f15-26f30cfaf0c9", "prefix_titles": [["title", "Towards Reasoning in Large Language Models: A Survey"], ["section", "Towards Reasoning in Large Language Models"]], "content": "\\label{sec:method}\nReasoning, particularly multi-step reasoning, is often seen as a weakness in language models and other NLP models . \nRecent research has suggested that reasoning ability may emerge in language models at a certain scale, such as models with over 100 billion parameters . In this paper, we follow  in considering reasoning as an ability that is rarely present in small-scale models like GPT-2~ and BERT~, and therefore focus on techniques applicable to improving or eliciting ``reasoning''\\footnote{It is important to note that the term ``reasoning'' in this paper does not necessarily imply that LLMs are truly capable of reasoning or that they are able to reason in the same way that humans do. We will discuss this issue in more detail in~\\S \\ref{sec:discussion}.} in LLMs such as GPT-3~ and PaLM~.", "cites": [1578, 679, 1550, 7461, 458, 1554, 7, 8556], "cite_extract_rate": 0.8, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic descriptive overview of reasoning in LLMs and references several key papers, but it lacks deeper synthesis or integration of their findings into a broader narrative. It briefly mentions the emergence of reasoning at scale and cites examples like GPT-3 and PaLM, but does not analyze how these models differ or what specific factors contribute to reasoning. Critical evaluation and abstraction to general principles are also limited."}}
{"id": "67834244-0eca-466b-a79c-ec86fc61ffe1", "title": "Fully Supervised Finetuning", "level": "subsection", "subsections": [], "parent_id": "bf709e30-a5ac-4aca-b384-38a22d4e122b", "prefix_titles": [["title", "Towards Reasoning in Large Language Models: A Survey"], ["section", "Towards Reasoning in Large Language Models"], ["subsection", "Fully Supervised Finetuning"]], "content": "\\label{sec:fully-sup}\nBefore discussing reasoning in large language models, it is worth mentioning there is research working on eliciting/improving reasoning in small language models through \\textit{fully supervised finetuning} on specific datasets.\nFor example,  finetune a pretrained GPT model~ to generate rationales that explain model predictions with the built CoS-E dataset, and find that models trained with explanations perform better on commonsense question answering tasks~.\n train RoBERTa~ to perform reasoning/inference based on both implicit pre-trained knowledge and explicit free-text statements. \n finetune pretrained language models to solve competition mathematics problems by generating full step-by-step solutions, though the accuracy is relatively low.\n train language models to do multi-step reasoning for program synthesis/execution by generating ``scratchpads'', i.e., intermediate computations, before producing the final answers.  We refer the reader to 's survey for more studies in this line.\nThere are two major limitations of fully supervised finetuning. First, it requires a dataset containing explicit reasoning, which can be difficult and time-consuming to create. Additionally, the model is only trained on a specific dataset, which limits its application to a specific domain and may result in the model relying on artifacts in the training data rather than actual reasoning to make predictions.", "cites": [5950, 8930, 5951, 826, 7801, 456, 3496], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.5, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of several methods involving fully supervised finetuning for reasoning in LLMs, citing relevant papers and mentioning their goals and approaches. It briefly touches on limitations, such as the need for explicit reasoning datasets and domain specificity, but does not offer a deep comparative or critical evaluation of the cited works. The abstraction level is limited, focusing more on individual techniques than on broader principles or frameworks."}}
{"id": "e6815bed-d10b-467d-be19-c8e625845fb1", "title": "Prompting \\& In-Context Learning", "level": "subsection", "subsections": ["4d8ae244-9d20-4362-a447-99645250fd43", "6d1201c3-e877-47ba-abaa-ad51421eba64", "29f6b7b7-7fb9-41d3-b3b8-2967be1b2aba", "90b6f7c8-cb5c-494e-935d-0b2fa322d8e7"], "parent_id": "bf709e30-a5ac-4aca-b384-38a22d4e122b", "prefix_titles": [["title", "Towards Reasoning in Large Language Models: A Survey"], ["section", "Towards Reasoning in Large Language Models"], ["subsection", "Prompting \\& In-Context Learning"]], "content": "\\label{sec:prompting}\n\\begin{figure*}[tp!]\n\\centerline{\\includegraphics[width=0.9\\linewidth]{Rationale_Engineering.pdf}}\n\\caption{An illustration of \\textit{Chain-of-Thought Prompting} and \\textit{Rationale Engineering}, where asterisk (*) denotes the target problem to be solved.}\n\\label{fig:RE}\n\\end{figure*}\nLarge language models such as GPT-3  have demonstrated remarkable few-shot performance across a variety of tasks through in-context learning.\nThese models can be prompted with a question and a few $\\langle$input, output$\\rangle$ exemplars to potentially solve a problem through ``reasoning'', either implicitly or explicitly.\nHowever, research has shown that these models still fall short when it comes to tasks that require multiple steps of reasoning to solve . This may be due to a lack of exploration into the full capabilities of these models, as recent studies have suggested.", "cites": [679, 1550, 7461], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of in-context learning and prompting in LLMs, citing three relevant papers, but it lacks deeper synthesis or integration of their findings. It does not critically assess the methods or limitations, nor does it abstract broader principles or trends. The narrative remains largely descriptive and disconnected from a deeper analytical framework."}}
{"id": "4d8ae244-9d20-4362-a447-99645250fd43", "title": "Chain of Thought and Its Variants", "level": "subsubsection", "subsections": [], "parent_id": "e6815bed-d10b-467d-be19-c8e625845fb1", "prefix_titles": [["title", "Towards Reasoning in Large Language Models: A Survey"], ["section", "Towards Reasoning in Large Language Models"], ["subsection", "Prompting \\& In-Context Learning"], ["subsubsection", "Chain of Thought and Its Variants"]], "content": "\\label{sec:cot}\nTo encourage LLMs to engage in reasoning rather than simply providing answers directly, we may guide LLMs to generate ``reasoning'' explicitly. \nOne approach for doing this is \\textit{chain-of-thought prompting}, proposed by .\nThis approach involves providing a few examples of ``chain of thought'' (CoT), which are intermediate natural language reasoning steps, in the prompt to LLMs (Figure~\\ref{fig:RE}).\nSpecifically, in CoT prompting, $\\langle$input, output$\\rangle$ demonstrations are replaced with $\\langle$\\textq{input}, \\textr{\\textit{chain of thought}}, \\texta{output}$\\rangle$ triples, e.g., ``\\textq{[input] Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?} \\textr{[\\textit{chain of thought}] Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11.} \\texta{[output] The answer is 11.}'' \nIn this way, given a target question, the model learns to generate explicit rationale before producing the final answer. \nExperimental results show that this simple idea can improve LLMs' few-shot performance on arithmetic, symbolic, and commonsense reasoning tasks, sometimes to a striking degree.\nThere are several variants of chain-of-thought prompting that have been proposed in the literature, in a different form or to solve a specific problem.\n\\textit{Different Form:}\n introduce \\textit{Zero-shot-CoT}, in which LLMs are simply prompted with the phrase ``Let's think step by step'' after the input, in order to elicit reasoning without the need for few-shot demonstrations. \n find that LLMs trained with code, e.g., Codex~, can achieve better performance on reasoning tasks by framing reasoning as code generation.\n propose to iteratively prompt chain of thought.\n attempt to retrieve external knowledge in CoT to improve faithfulness of reasoning.\n\\textit{Specific Problem/Setting:}\nBefore chain of thought,  also try to use intermediate computations, named ``scratchpads'', to improve language models' reasoning performance in both finetuning and few-shot regimes, with a particular focus on programs.\n attempt to solve multilingual reasoning tasks with CoT in the native language, CoT in English (regardless of the problem language), and CoT in English (with the problem translated to English).\n apply CoT to table-based reasoning, finding that LLMs can achieve strong performance on table tasks with only one exemplar. \n demonstrate that CoT can improve LLMs' performance on paraphrase selection for metaphors.  apply chain of thought to solve multimodal science questions.", "cites": [1578, 8063, 8931, 8930, 7094, 3384, 2986, 7465, 7957, 5952, 2220], "cite_extract_rate": 0.8461538461538461, "origin_cites_number": 13, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear and organized description of the chain-of-thought (CoT) prompting method and its variants, integrating a number of cited works into a general narrative. However, the synthesis is limited to grouping papers by whether they represent a 'different form' or address a 'specific problem/setting,' without deeper conceptual or theoretical connections. Critical analysis is minimal, as the section largely summarizes findings without evaluating their strengths, weaknesses, or implications. Some patterns (e.g., CoT’s use in multilingual and multimodal settings) are noted, but abstraction to overarching principles or frameworks is not fully developed."}}
{"id": "6d1201c3-e877-47ba-abaa-ad51421eba64", "title": "Rationale Engineering", "level": "subsubsection", "subsections": [], "parent_id": "e6815bed-d10b-467d-be19-c8e625845fb1", "prefix_titles": [["title", "Towards Reasoning in Large Language Models: A Survey"], ["section", "Towards Reasoning in Large Language Models"], ["subsection", "Prompting \\& In-Context Learning"], ["subsubsection", "Rationale Engineering"]], "content": "\\label{sec:rationale-eng}\nThe original version of chain-of-thought prompting, proposed by , relies on manually crafted examples of intermediate reasoning steps and applies greedy decoding in the generation.\n\\textit{Rationale engineering} aims to more effectively elicit or utilize reasoning in LLMs.\nThis can be achieved through \\textit{rationale refinement}, which involves creating more effective examples of reasoning steps, or through \\textit{rationale exploration} and \\textit{rationale verification}, which involve exploring and verifying the rationales produced by LLMs.\nA summary of raltionale engineering is illustrated in Figure~\\ref{fig:RE}.\n\\p{Rationale refinement} \nThe choice of exemplars can significantly affect the few-shot performance of LLMs, as demonstrated in research such as , which also appears in chain-of-thought prompting.\n\\textit{Rationale refinement} aims to create and refine rationale examples that are better able to elicit reasoning in LLMs.\n propose \\textit{complexity-based prompting} to create rationales with more reasoning steps. Their experiments show that the performance of LLMs improves with the increased rationale complexity. \nSimilarly,  propose \\textit{algorithmic prompting}, which suggests that providing more thorough examples of solutions can help improve reasoning performance on some simple math calculations.\n design \\textit{Auto-CoT} to automatically construct exemplars by partitioning questions from a given dataset into clusters and then using Zero-Shot-CoT~ to generate the rationale for a representative question from each cluster. The analysis shows that making exemplars diverse is important in prompting LLMs to produce better rationales.\n\\p{Rationale exploration} In addition to providing better exemplars, we can allow LLMs to fully explore various ways of reasoning to improve their performance on reasoning tasks, named \\textit{rationale exploration}.\nBased on the idea that complex problems often admit multiple ways of thinking that can lead to their unique correct answer,\n present a decoding strategy called \\textit{self-consistency} to improve upon the traditional greedy decoding used in chain-of-thought prompting. This strategy involves sampling a diverse set of rationales, rather than just the greedy one, and selecting the most consistent answer by marginalizing out the sampled rationales. The idea is also used in  to vote over the top complex rationales.\nTo further improve performance,  suggest providing different demonstrations for each question by sampling exemplars from an exemplar base, in order to increase the diversity of the sampled rationales.\n\\p{Rationale verification} \nEnsuring that the rationales produced by LLMs are valid is critical, as incorrect rationales can lead to incorrect final predictions~. To address this issue, the process of \\textit{rationale verification} aims to verify whether the rationales produced by LLMs lead to the correct final answers.  propose augmenting LLMs with a trained verifier that assigns a score to each rationale and solution generated by the LLM, selecting the highest-ranked solution as the final answer when solving math word problems.\n also use this technique to guide rationale selection, in conjunction with the process of rationale exploration.\nDifferent from the above methods that train an external verifier to verify the rationales,  suggest using LLMs themselves as the verifiers.", "cites": [1578, 2467, 7094, 8654, 5954, 2191, 458, 5953, 2189], "cite_extract_rate": 0.8181818181818182, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to present a coherent narrative on rationale engineering, grouping methods into refinement, exploration, and verification. It offers some critical analysis by highlighting the importance of diversity in exemplars and the limitations of greedy decoding, but it does not deeply critique or compare the effectiveness of each approach. The section identifies broader patterns like the role of diversity in prompting and the use of verification, but these insights remain at a mid-level of abstraction."}}
{"id": "29f6b7b7-7fb9-41d3-b3b8-2967be1b2aba", "title": "Problem Decomposition", "level": "subsubsection", "subsections": [], "parent_id": "e6815bed-d10b-467d-be19-c8e625845fb1", "prefix_titles": [["title", "Towards Reasoning in Large Language Models: A Survey"], ["section", "Towards Reasoning in Large Language Models"], ["subsection", "Prompting \\& In-Context Learning"], ["subsubsection", "Problem Decomposition"]], "content": "\\label{sec:prob-decom}\nChain-of-thought prompting, while effective for eliciting reasoning in LLMs, can struggle with complex tasks, e.g., tasks that require compositional generalization .\nTo solve a complex problem, it is helpful to first break it down into smaller, more manageable subproblems. By solving each of these subproblems, we can effectively solve the complex problem. This technique is called \\textit{problem decomposition} or \\textit{divide and conquer}~. \nBased on this idea,  propose \\textit{least-to-most prompting}, which consists of two steps: decomposing the complex problem into subproblems and solving these subproblems in a specific order, with each subproblem being facilitated by the answers obtained from previously solved subproblems.\nAs follow-up work,  introduce \\textit{dynamic least-to-most prompting}, which is designed to solve more realistic semantic parsing problems by decomposing the problems with prompting-based syntactic parsing and dynamically selecting exemplars based on the decomposition.\nIn addition,  design \\textit{decomposed prompting}, which breaks down a complex problem into subproblems that can be handled by a shared library of prompting-based LLMs, each specialized in a particular subproblem.\nFurthermore,  develop \\textit{successive prompting}, which iteratively decomposes a complex problem into a simple problem, with the next subproblem prediction having access to the answers to the previous subproblems. \nWhile the above methods decompose or solve compositional questions with multiple forward passes,  suggest decomposing and solving the input question in one forward pass using CoT prompting.\nOverall, these techniques show promise for helping LLMs to solve complex tasks by decomposing the problem into more manageable subproblems.", "cites": [424, 5565, 4316, 1144, 5567, 5559, 5566, 4286, 5564], "cite_extract_rate": 0.9, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of problem decomposition methods in prompting LLMs for reasoning. It integrates several papers by listing different techniques (least-to-most, dynamic least-to-most, decomposed prompting, successive prompting), but lacks deeper synthesis or a unifying framework. There is minimal critical analysis of the methods or their limitations, and while it identifies a general theme of decomposition, it does not elevate the discussion to broader meta-level principles."}}
{"id": "90b6f7c8-cb5c-494e-935d-0b2fa322d8e7", "title": "Others", "level": "subsubsection", "subsections": [], "parent_id": "e6815bed-d10b-467d-be19-c8e625845fb1", "prefix_titles": [["title", "Towards Reasoning in Large Language Models: A Survey"], ["section", "Towards Reasoning in Large Language Models"], ["subsection", "Prompting \\& In-Context Learning"], ["subsubsection", "Others"]], "content": "\\label{sec:others_prompting}\nThere are other techniques that have been developed to facilitate reasoning in LLMs for specific tasks or settings.\nFor instance,\n introduce a \\textit{selection-inference} framework that uses LLMs as modules to select and infer reasoning steps from a set of facts that culminate in the final answer.\n suggest using backward chaining, i.e., from goal to the set of facts that support it, instead of forward chaining like .\nIn addition,\n propose a method for solving binary questions by prompting LLMs abductively and recursively to rationalize each option.\n design a technique for performing numerical reasoning on complex numbers by replacing the complex numbers with simple numbers to produce simpler expressions, and then using these expressions to perform calculations on the complex numbers. \nThere are also efforts to distill reasoning from LLMs into smaller models, such as the work by .\nFinally, we refer the reader to 's position paper on \\textit{language model cascade}, which presents a unifying framework for understanding chain-of-thought prompting and research in this line.", "cites": [5571, 5958, 5957, 7958, 5956, 5959, 5955], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of various reasoning techniques in LLMs but lacks synthesis by not connecting these ideas into a broader framework or theme. It offers minimal critical analysis or abstraction, merely listing methods without evaluating their strengths, weaknesses, or broader implications."}}
{"id": "f3e0bf35-a5ee-40ba-a75b-4e2765559f8f", "title": "Reasoning-Enhanced Training and Prompting", "level": "subsubsection", "subsections": [], "parent_id": "c0e116f0-fa6c-4c1a-ac2c-bca76afc4616", "prefix_titles": [["title", "Towards Reasoning in Large Language Models: A Survey"], ["section", "Towards Reasoning in Large Language Models"], ["subsection", "Hybrid Method"], ["subsubsection", "Reasoning-Enhanced Training and Prompting"]], "content": "\\label{sec:trainig_and_prompting}\nOne approach to improving the reasoning capabilities of LLMs is to pretrain or finetune the models on datasets that include ``reasoning''.\n find that LLMs trained on datasets containing scientific and mathematical data can achieve better performance on reasoning tasks like quantitative reasoning problems when using CoT prompting\\footnote{This may also be true for models trained with code~.}.\n show that continually pretraining with SQL data can boost the performance of language models, e.g., T5~, on natural language reasoning such as numerical reasoning and logical reasoning.\nFurthermore,  develop Flan models by finetuning PaLM~ and T5~ with 1.8k finetuning tasks, including CoT data, and find that CoT data are critical to keeping reasoning abilities.\nSimilarly,  finetune OPT~ on 10 reasoning datasets and observe that it can improve some reasoning capabilities of LLMs.\n study the length generalization abilities of LLMs, i.e., whether LLMs learned with short problem instances can generalize to long ones. They discover that the combination of few-shot scratchpad (or chain of thought) finetuning and scratchpad prompting results in a significant improvement in LLMs' ability to generalize to longer problems, while this phenomenon is not observed in the standard fully supervised finetuning paradigm.", "cites": [5560, 5960, 9, 7465, 7463, 1554, 5568, 7468, 5563, 5961], "cite_extract_rate": 0.9090909090909091, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic summary of several papers that explore training and prompting methods to improve LLM reasoning. While it lists different approaches and their effects, it lacks a deeper synthesis of ideas or a unifying narrative. There is minimal critical evaluation or identification of broader trends, and the presentation remains largely descriptive with little abstraction."}}
{"id": "68105fbc-d9ad-4299-bab3-6100f17fec97", "title": "Bootstrapping \\& Self-Improving", "level": "subsubsection", "subsections": [], "parent_id": "c0e116f0-fa6c-4c1a-ac2c-bca76afc4616", "prefix_titles": [["title", "Towards Reasoning in Large Language Models: A Survey"], ["section", "Towards Reasoning in Large Language Models"], ["subsection", "Hybrid Method"], ["subsubsection", "Bootstrapping \\& Self-Improving"]], "content": "\\label{sec:selfimprove}\nInstead of finetuning LLMs on pre-built datasets that include reasoning, \nthere are studies that have explored the idea of using LLMs to self-improve their reasoning abilities through a process known as bootstrapping.\nOne example of this is the \\textit{Self-Taught Reasoner (STaR)} introduced by \n, in which a LLM is trained and refined on its own output iteratively. Specifically, with CoT prompting, the model first generates initial rationales. And then, the model is finetuned on rationales that lead to correct answers. This process can be repeated, with each iteration resulting in an improved model that can generate better training data, which in turn leads to further improvements.\nAs a follow-up to this work,  show that LLMs are able to self-improve their reasoning abilities without the need for supervised data by leveraging the self-consistency of reasoning~.", "cites": [2191, 8065, 8064], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of the bootstrapping and self-improving approaches by citing three related papers and outlining their general mechanisms. However, it lacks deeper critical analysis of the strengths and limitations of each method, and does not abstract to broader principles or trends in reasoning improvement for LLMs. The narrative remains descriptive, connecting the papers in a linear fashion without offering a novel framework or evaluative perspective."}}
{"id": "6e77d17d-f942-47b3-b083-ca0a6250395e", "title": "End Task Performance", "level": "subsection", "subsections": [], "parent_id": "576b89ba-14bd-4878-a121-f76e443baa10", "prefix_titles": [["title", "Towards Reasoning in Large Language Models: A Survey"], ["section", "Measuring Reasoning in Large Language Models"], ["subsection", "End Task Performance"]], "content": "\\label{sec:endtask}\nOne way to measure reasoning abilities of LLMs is to report their performance, e.g., accuracy, on end tasks that require reasoning.\nWe list some common benchmarks as follows.\n\\p{Arithmetic Reasoning}\n\\textit{Arithmetic reasoning} is the ability to understand and apply mathematical concepts and principles in order to solve problems involving arithmetic operations. \nThis involves using logical thinking and mathematical principles to determine the correct course of action when solving mathematical problems. \nRepresentative benchmarks for arithmetic reasoning include GSM8K~,\nMath~,\nMathQA~, \nSVAMP~,\nASDiv~,\nAQuA~,\nand MAWPS~. \nIt is worth mentioning that\n generate the \\textit{Parity Datasets} and the \\textit{Boolean Variable Assignment Dataset} for analyzing the length generalization capabilities of LLMs (\\S\\ref{sec:trainig_and_prompting}).\n\\p{Commonsense Reasoning}\n\\textit{Commonsense Reasoning} is the use of everyday knowledge and understanding to make judgments and predictions about new situations. It is a fundamental aspect of human intelligence that enables us to navigate our environment, understand others, and make decisions with incomplete information.\nBenchmarks that can be used for testing commonsense reasoning abilities of LLMs include\nCSQA~, StrategyQA~, and ARC~.\nWe refer the reader to 's survey for more work in this domain.\n\\p{Symbolic Reasoning}\n\\textit{Symbolic reasoning} is a form of reasoning that involves the manipulation of symbols according to formal rules.\nIn symbolic reasoning, we use abstract symbols to represent concepts and relationships, and then manipulate those symbols according to precise rules in order to draw conclusions or solve problems. \nTwo benchmarks of symbolic reasoning are presented in , including \nLast Letter Concatenation and Coin Flip.\n\\p{Others}\nIn practice, there are many benchmarks that can be used to evaluate reasoning abilities of LLMs (indirectly), as long as the downstream task involves reasoning.\nBIG-bench~, for example, includes over 200 tasks that test a range of reasoning skills, including tasks like Date Understanding, Word Sorting, and Causal Judgement.\nOther benchmarks, such as SCAN~ and the one proposed by , focus on evaluating generalization ability. \nLLMs can also be tested on their table reasoning abilities using benchmarks such as WikiTableQA~, FetaQA~, as suggested by .\nIn addition, there are benchmarks for evaluating LLMs' generative relational reasoning abilities, such as \nCommonGen~ and Open Relation Modeling~.", "cites": [8063, 444, 3496, 443, 1172, 1578, 1580, 5964, 5951, 458, 456, 5861, 5963, 5962, 3157, 1579, 8066, 7466, 5961], "cite_extract_rate": 0.8636363636363636, "origin_cites_number": 22, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of benchmarks used to measure reasoning in LLMs, listing tasks and datasets without substantial synthesis or critical evaluation. While it categorizes tasks (e.g., arithmetic, commonsense, symbolic reasoning), it lacks deeper analysis of how these benchmarks relate to each other or evaluate specific reasoning components. Critical insights or limitations of the cited works are not discussed in depth."}}
{"id": "8882379f-ddac-4a0a-915f-e62c66a86e50", "title": "Analysis on Reasoning", "level": "subsection", "subsections": [], "parent_id": "576b89ba-14bd-4878-a121-f76e443baa10", "prefix_titles": [["title", "Towards Reasoning in Large Language Models: A Survey"], ["section", "Measuring Reasoning in Large Language Models"], ["subsection", "Analysis on Reasoning"]], "content": "\\label{sec:analysis}\nAlthough LLMs have demonstrated impressive performance on various reasoning tasks, the extent to which their predictions are based on true reasoning or simple heuristics is not always clear.\nThis is because most existing evaluations focus on their accuracy on end tasks, rather than directly assessing their reasoning steps.\nWhile some error analysis has been conducted on the generated rationales of LLMs \\citep[][\\textit{inter alia}]{wei2022chain,kojima2022large}, this analysis has often been limited in depth.\nThere have been some efforts to develop metrics and benchmarks that enable a more formal/deep analysis of reasoning in LLMs.\n design ROSCOE, a set of interpretable, detailed step-by-step evaluation metrics covering various perspectives including semantic alignment, logical inference, semantic similarity, and language coherence.\n create a synthetic dataset called PrOntoQA that is generated from real or fictional ontologies. Each example in the dataset has a unique proof, which can be converted to simple sentences and back again, allowing for a formal analysis of each reasoning step. \n introduce a dataset called FOLIO to test the first-order logic reasoning capabilities of LLMs. FOLIO contains first-order logic reasoning problems that require models to determine the correctness of conclusions given a set of premises.\nIn addition,  conduct ablation experiments on CoT and find that LLMs may also perform reasoning while prompting with invalid rationals. Their study also suggests that being relevant to the query and correctly ordering the reasoning steps are important for CoT prompting.\nIn summary, most existing studies primarily report the performance of the models on downstream reasoning tasks, without a detailed examination of the quality of the rationales produced. This leaves open the question of whether the models are actually able to reason in a way that is similar to human reasoning, or whether they are simply able to achieve good performance on the tasks through other means. Further research is needed to more formally analyze the reasoning abilities of LLMs.", "cites": [5966, 5965, 8067, 8964], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers on reasoning evaluation in LLMs, connecting concepts like step-by-step reasoning, logical inference, and prompting effectiveness into a cohesive analysis. It critically points out the limitations of current evaluation methods and highlights the need for deeper scrutiny of rationales. The discussion abstracts beyond individual datasets and methods to raise broader questions about the nature of LLM reasoning."}}
{"id": "b8d68930-a1e2-4cef-a5c9-441a260d2e08", "title": "Findings and Implications", "level": "section", "subsections": [], "parent_id": "e0a4e394-ff2b-4185-9f15-26f30cfaf0c9", "prefix_titles": [["title", "Towards Reasoning in Large Language Models: A Survey"], ["section", "Findings and Implications"]], "content": "\\label{sec:findings}\nIn this section, we summarize the important findings and implications of studies on reasoning in large language models.\n\\p{Reasoning seems an emergent ability of LLMs}\n show that \nreasoning ability appears to emerge only in large language models like GPT-3 175B, as evidenced by significant improvements in performance on reasoning tasks at a certain scale (e.g., 100 billion parameters).\nThis suggests that it may be more effective to utilize large models for general reasoning problems rather than training small models for specific tasks. \nHowever, the reason for this emergent ability is not yet fully understood. We refer the reader to  for some potential explanations.\n\\p{Chain of thought elicits ``reasoning'' of LLMs}\nThe use of chain-of-thought (CoT) prompts~ has been shown to improve the performance of LLMs on various reasoning tasks, as demonstrated in the experiments of .\nAdditionally,  (\\S\\ref{sec:analysis}) find that, when using CoT prompts, LLMs are able to produce valid individual proof steps, even when the synthetic ontology is fictional or counterfactual. However, they may sometimes choose the wrong steps when multiple options are available, leading to incomplete or incorrect proofs.\nMoreover, for many reasoning tasks where the performance of standard prompting grows smoothly with model scale, chain-of-thought prompting can lead to dramatic performance improvement.\nIn addition to these benefits, the use of CoT prompts has been shown to improve the out-of-distribution robustness of LLMs \\citep[][\\textit{inter alia}]{wei2022chain,zhou2022least,anil2022exploring}, an advantage that is not typically observed with standard prompting or fully supervised finetuning paradigms.\n\\p{LLMs show human-like content effects on reasoning}\nAccording to , LLMs exhibit reasoning patterns that are similar to those of humans as described in the cognitive literature.\nFor example, the models' predictions are influenced by both prior knowledge and abstract reasoning, and their judgments of logical validity are impacted by the believability of the conclusions. These findings suggest that, although language models may not always perform well on reasoning tasks, their failures often occur in situations that are challenging for humans as well. This provides some evidence that language models may ``reason'' in a way that is similar to human reasoning.\n\\p{LLMs are still unskilled at complex reasoning}\nAlthough LLMs seem to possess impressive reasoning capabilities with the techniques described in \\S\\ref{sec:method}, they still struggle with more complex reasoning tasks or those involving implicature, according to studies such as .\nFor instance,  find that even in relatively simple commonsense planning domains that humans would have no trouble navigating, LLMs such as GPT-3~ and BLOOM~ struggle to perform effectively.\nThese findings suggest that existing benchmarks may be too simple to accurately gauge the true reasoning abilities of LLMs, and that more challenging tasks may be needed to fully evaluate their abilities in this regard.", "cites": [1578, 2221, 679, 5965, 7462, 8067, 8556], "cite_extract_rate": 0.6363636363636364, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by connecting findings across multiple papers to highlight key themes such as emergent abilities, effectiveness of chain-of-thought prompting, human-like reasoning effects, and limitations in complex reasoning. It also provides some critical analysis by noting gaps, such as the unclear reasons for emergent abilities and the inadequacy of current benchmarks. The abstraction level is high, as it generalizes these findings to broader implications for model design and evaluation."}}
{"id": "86fd0a6c-96b7-4c59-8050-a11fd128dc7c", "title": "Reflection, Discussion, and Future Directions", "level": "section", "subsections": [], "parent_id": "e0a4e394-ff2b-4185-9f15-26f30cfaf0c9", "prefix_titles": [["title", "Towards Reasoning in Large Language Models: A Survey"], ["section", "Reflection, Discussion, and Future Directions"]], "content": "\\label{sec:discussion}\n\\ps{Why reasoning}\nReasoning is the process of thinking about something in a logical and systematic way, and it is a key aspect of human intelligence. By incorporating reasoning capabilities into language models, we can enable them to perform tasks that require more complex and nuanced thinking, such as problem solving, decision making, and planning~.\nThis can improve the performance of these models on downstream tasks and increase their out-of-distribution robustness . In addition, reasoning can make language models more explainable and interpretable, as it provides explicit rationales for their predictions.\n\\ps{Right task/application} As  point out, current benchmarks may not adequately reflect the reasoning capabilities of LLMs.\nIn addition, tasks such as solving simple math problems and concatenating letters in strings (\\S\\ref{sec:endtask}) are artificial and do not accurately reflect real-world situations. To truly understand the reasoning ability of LLMs, it is important to consider more realistic and meaningful applications such as decision making~, legal reasoning~, and scientific reasoning~.\nOur ultimate goal should not be to enable LLMs to solve simple math problems, which can be simply done with other programs.\nWhen conducting relevant research, it is essential to ask\n\\textit{whether the specific task being tackled is meaningful} and \\textit{whether the proposed method can be generalized to more realistic tasks and applications}.\n\\ps{Are language models really able to reason}\nThere are several indications that LLMs are able to reason, including 1) high performance on various tasks requiring reasoning ; 2) the ability to reason step-by-step with chain-of-thought prompting ; and 3) the reflection of human-like content effects on reasoning . \nHowever, these findings are not sufficient to conclude that LLMs can truly reason. For 1), it is not clear whether the models are making predictions based on \\textit{reasoning} or \\textit{heuristics} . For many existing benchmarks on reasoning, actually, we can design a program with heuristic rules to achieve very high performance. \nWe usually do not think a program relying on heuristic rules is capable of reasoning.\nFor 2), although the models seem to reason step-by-step, the generated rationales may be incorrect and inconsistent. It is possible that the models are ``generating reasoning-like response'' rather than ``reasoning step-by-step''. For 3), while LLMs display some human-like reasoning patterns, this does not necessarily mean that they behave like humans.\nAdditionally, there are several observations that suggest LLMs may not be capable of reasoning: 1) LLMs still struggle with tasks that require complex reasoning . If LLMs are really decent reasoners, they should handle tasks that can be simply solved by humans through reasoning;\n2) LLMs make mistakes in their reasoning, as explained above;\n3)$^\\#$\\footnote{~$^\\#$indicates the finding has not been carefully examined in language models with more than 100 billion parameters.} The performance of LLMs on downstream tasks has been found to be sensitive to the frequency of certain terms, such as numbers, in the training data  , which would not be expected if the models were solving mathematical problems through reasoning; 4)$^\\#$ Language models have been found to struggle with associating relevant information that they have memorized .\nOverall, it is still too early to draw a conclusion about the proposed question. \nIn fact, there is also an ongoing debate about whether language models can actually \\textit{understand} language or capture \\textit{meaning} . \nFurther in-depth analysis of factors such as training data, model architecture, and optimization objectives is needed, as well as the development of better benchmarks for measuring the reasoning capabilities of LLMs. However, it is clear that the current models are not yet capable of robust reasoning.\n\\p{Improving reasoning capabilities of LLMs}\nWhile techniques like chain-of-thought prompting~ may help to elicit reasoning abilities in large language models, they cannot enable the models to solve tasks beyond their current capabilities.\nTo truly enhance reasoning in LLMs, we need to utilize training data, model architecture, and optimization objectives that are designed to encourage reasoning.\nFor example, finetuning a model with a dataset including CoT data has been shown to improve reasoning , and models can also self-improve through the process of bootstrapping their reasoning .\nThere is still much research that needs to be done in this area, and we look forward to future progress in improving reasoning in large language models.", "cites": [2948, 5955, 1578, 424, 7468, 8556, 2221, 2997, 1579, 5969, 5968, 8965, 5967, 8065, 5587, 8067, 8064, 5961], "cite_extract_rate": 0.6923076923076923, "origin_cites_number": 26, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes key findings from multiple cited papers to form a coherent narrative on the current understanding of reasoning in LLMs. It critically examines the evidence for reasoning, questions whether performance on benchmarks equates to actual reasoning, and highlights limitations such as inconsistencies in rationales and overreliance on heuristics. While it provides strong analysis and identifies broader implications, it stops short of offering a fully novel theoretical framework."}}
{"id": "be9e582b-f35f-4b90-b3c7-b0c2393452dc", "title": "Conclusion", "level": "section", "subsections": [], "parent_id": "e0a4e394-ff2b-4185-9f15-26f30cfaf0c9", "prefix_titles": [["title", "Towards Reasoning in Large Language Models: A Survey"], ["section", "Conclusion"]], "content": "In this paper, we have provided a detailed and up-to-date review of the current state of knowledge on reasoning in large language models. We have discussed techniques for improving and eliciting reasoning in LLMs, methods and benchmarks for evaluating reasoning abilities, and the findings and implications of previous studies in this topic. \nWhile LLMs have made significant progress in natural language processing and related fields, it remains unclear to what extent they are capable of true reasoning or whether they are simply using memorized patterns and heuristics to solve problems. Further research is needed to fully understand the reasoning abilities of LLMs, improve LLMs' reasoning capabilities, and determine their potential for use in a variety of applications. \nWe hope that this paper will serve as a useful overview of the current state of the field and stimulate further discussion and research on this interesting and important topic.\n\\section*{Limitations}\nIn this paper, we provide an overview of the current state of knowledge on reasoning in large language models. Reasoning is a broad concept that encompasses various forms, making it impractical to summarize all related work in a single paper. Therefore, we focus on deductive reasoning, as it is the most commonly studied in the literature. Other forms of reasoning such as inductive reasoning~\\citep[][\\textit{inter alia}]{yang2022language,misra2022property} and abductive reasoning~\\citep[][\\textit{inter alia}]{wiegreffe2021reframing,lampinen2022can,jung2022maieutic} may not be discussed in depth.\nAdditionally, given the rapid evolution and significance of reasoning within large language models, it is crucial to note that new contributions may have emerged in the field concurrent with the writing of this paper. An additional resource to consider is a parallel survey by , which emphasizes reasoning via language model prompting. Our coverage may not extend to papers released during or after 2023 such as evaluation on ChatGPT~. As such, we recommend readers to check the papers that cite this survey for a more comprehensive and updated understanding of this field.\n\\section*{Acknowledgements}\nWe would like to thank Jason Wei (OpenAI) and Denny Zhou (Google DeepMind) for their valuable advice and constructive feedback on this work.\nThis material is based upon work supported by the National Science Foundation IIS 16-19302 and IIS 16-33755, Zhejiang University ZJU Research 083650, IBM-Illinois Center for Cognitive Computing Systems Research (C3SR) and IBM-Illinois Discovery Accelerator Institute (IIDAI), gift grants from eBay and Microsoft Azure, UIUC OVCR CCIL Planning Grant 434S34, UIUC CSBS Small Grant 434C8U, and UIUC New Frontiers Initiative. Any opinions, findings, and conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the funding agencies.\n\\bibliography{custom}\n\\bibliographystyle{acl_natbib}\n\\end{document}", "cites": [5970, 5562], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.5, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section briefly acknowledges the limitations of its coverage and cites two relevant papers, but it does not synthesize or deeply connect their insights to a broader narrative. Critical analysis is limited to noting the incompleteness of the survey and the emergence of new work. Abstraction is minimal, as the section does not generalize findings into overarching principles or frameworks."}}
