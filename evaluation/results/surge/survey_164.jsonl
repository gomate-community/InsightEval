{"id": "197f00d7-b0de-42fb-8e63-8fa5e7f3ef40", "title": "Introduction", "level": "section", "subsections": ["b036b1ae-cc48-4ade-a60f-7714a13b684d"], "parent_id": "c0dc96fc-4540-492b-9493-eb9335bae96c", "prefix_titles": [["title", "Neural Unsupervised Domain Adaptation in NLP---A Survey"], ["section", "Introduction"]], "content": "Deep learning has undoubtedly pushed the frontier in Natural Language Processing (NLP). Particularly large pre-trained language  models have improved results for a wide range of NLP applications. However, the lack of portability of NLP models to new conditions remains a central issue in NLP. For many target applications, labeled data is lacking (Y scarcity), and even for pre-training general models data might be scarce (X scarcity). This makes it even more pressing to revisit a particular type of transfer learning, namely domain adaptation (DA).\nA default assumption in many  machine learning algorithms is that the training and test sets follow the same underlying distribution. When these distributions do not match, we face a \\textit{dataset shift}~ -- in NLP typically referred to as a \\textit{domain shift}. In this setup, the \\emph{target} domain and the \\textit{source} training data differ, they are not sampled from the same underlying distribution. Consequently, performance drops on the target, which  undermines the ability of models to truly generalize \\emph{into the wild}. Domain adaptation is closely tied to a fundamental bigger open issue in machine learning: generalization beyond the training distribution. Ultimately, intelligent systems should be able to adapt and robustly handle any test distribution, without having seen any data from it. This is the broader need for \\textit{out-of-distribution generalization}~, and a more challenging setup targeted at handling \\textit{unknown} domains~.\nWork on domain adaptation focused largely on \\textit{supervised} domain adaptation~. In such a classic supervised DA setup, a small amount of labeled target domain data is available, along with some larger amount of labeled source domain data. The task is to adapt from the source to the specific target domain in light of limited target domain data. \nHowever, annotation is a substantial time-requiring and costly manual effort. While annotation directly mitigates the lack of labeled data, it does not  easily scale to new application targets.  In contrast, DA methods aim to shift the ability of models from the traditional interpolation of similar examples to models that extrapolate to examples outside the original training distribution . \\textit{Unsupervised domain adaptation} (UDA) mitigates the domain shift issue by learning only from \\textit{unlabeled} target data, which is typically available for both source and target domain(s).\nUDA fits the classical real-world scenario better, in which labeled data in the target domain is absent, but unlabeled data might be abundant.\nUDA thus provides an elegant and scalable solution. We believe these advances in UDA will help for out-of-distribution generalization.\n \\begin{figure}[!t]\\centering\n \t\\includegraphics[width=\\columnwidth]{DA-categorization-new}\n \t\\caption{Taxonomy of DA as special case of transductive transfer learning (left). Related problems (e.g., domain and out-of-distribution generalization) and DA setups (1:1 and multi-source adaptation) (right).}\n \t\\label{fig:taxonomy}\n \\end{figure}", "cites": [196, 195], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of domain adaptation, particularly emphasizing the importance of unsupervised approaches for real-world generalization. It integrates key ideas from the cited papers, such as out-of-distribution generalization and adversarial data augmentation, but does not explicitly connect these ideas in a novel framework. While it identifies some limitations of traditional setups and highlights the significance of UDA, the critical analysis remains surface-level without deep evaluation of the cited methods' strengths or weaknesses."}}
{"id": "b036b1ae-cc48-4ade-a60f-7714a13b684d", "title": "A categorization of domain adaptation in NLP", "level": "paragraph", "subsections": ["6ca35633-ded8-4fd4-8c73-ef7be712a0d9"], "parent_id": "197f00d7-b0de-42fb-8e63-8fa5e7f3ef40", "prefix_titles": [["title", "Neural Unsupervised Domain Adaptation in NLP---A Survey"], ["section", "Introduction"], ["paragraph", "A categorization of domain adaptation in NLP"]], "content": "We categorize research into model-centric, data-centric and hybrid approaches, as shown in Figure~\\ref{fig:taxonomy}.  \\textit{Model-centric} methods target approaches to augment the feature space, alter the loss function, the architecture or model parameters~. \\textit{Data-centric} methods focus on the data aspect and either involve pseudo-labeling (or bootstrapping) to bridge the domain gap~, data selection~ and pre-training methods~. As some approaches take elements of both, we include a \\textit{hybrid} category.\\footnote{We take inspiration of the data-centric and model-centric terms from~\\newcite{Chu2018} in MT, and add hybrid.} A comprehensive overview of UDA methods and the tasks each method is applied to is provided in Table~\\ref{tab:overview}.", "cites": [197], "cite_extract_rate": 0.08333333333333333, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a basic categorization of domain adaptation approaches (model-centric, data-centric, hybrid) and mentions some methods and ideas from cited works, but it lacks a deep synthesis of these ideas. It does not analyze the strengths or weaknesses of the approaches, nor does it engage in meaningful comparison or trend identification. The abstraction is limited to the classification of approaches without deeper generalization or meta-level insights."}}
{"id": "6ca35633-ded8-4fd4-8c73-ef7be712a0d9", "title": "Other surveys", "level": "paragraph", "subsections": ["2d0b612e-cd75-4233-ab50-a4e3097623a8"], "parent_id": "b036b1ae-cc48-4ade-a60f-7714a13b684d", "prefix_titles": [["title", "Neural Unsupervised Domain Adaptation in NLP---A Survey"], ["section", "Introduction"], ["paragraph", "A categorization of domain adaptation in NLP"], ["paragraph", "Other surveys"]], "content": "Comprehensive reviews on DA exist, each with a different focus: visual applications , machine translation (MT) , pre-neural DA methods in NLP . Seminal surveys in machine learning on transfer learning include~\\newcite{pan2009survey},~\\newcite{weiss-ea-survey}, and~\\newcite{yang_zhang_dai_pan_2020}.", "cites": [7200], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section simply lists existing surveys on domain adaptation with different focuses, such as visual applications, machine translation, and pre-neural methods. It provides no synthesis of ideas across the cited works, lacks critical evaluation of their strengths or limitations, and offers no abstraction or meta-level insights. The narrative remains minimal and descriptive."}}
{"id": "39fcba5e-b031-4b85-8cfa-0b18ca336cb2", "title": "domain", "level": "section", "subsections": ["9b7e1e2e-438a-4e8d-b8ca-48230986cdad"], "parent_id": "c0dc96fc-4540-492b-9493-eb9335bae96c", "prefix_titles": [["title", "Neural Unsupervised Domain Adaptation in NLP---A Survey"], ["section", "domain"]], "content": "? From the notion of domain to {variety space} and related problems}\\label{sec:varietyspace}\nDespite the formal definition of \\textit{domain} above, the term is quiet loosely used in NLP and there is no common ground on what constitutes a domain~. Typically in NLP, domain is meant to refer to some coherent type of corpus, i.e., predetermined by the given dataset~. This may relate to topic, style, genre, or linguistic register.  The notion of domain and what plays into it has though significantly changed over the last years, leading to relevant research lines.\nFirst, the Penn Treebank WSJ corpus~ and the Brown corpus~ are prototypical examples, with the WSJ being considered widely as the canonical newswire domain. \nIn the recent decade, there has been considerable work on what is considered \\textit{non-canonical} data. The dichotomy between canonical (typically considered well-edited English newswire)  and non-canonical data arose with the increasing interest of working with \\textit{social media} with all its challenges related to the `noisiness' of the domain~.\nModels trained on canonical data failed in light of the challenges on, e.g., Twitter~. \nThe general quest to understand the implications of variations of language on model performance led to  lines of work on how human factors impact data in a covert or overt way, e.g., on how latent socio-demographic factors impact NLP performance~, or how direct data collection strategies like crowdsourcing impact corpus composition~ or frequency effects impact NLP performance~.\nHowever, \\textit{what is a domain?} Is, say, Twitter, its own domain? Or is it a set of subdomains? Similarly, do language samples of social groups (e.g., sociolects) form a domain or a set of subdomains?", "cites": [7201], "cite_extract_rate": 0.09090909090909091, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates the concept of 'domain' with recent discussions on non-canonical data, drawing from the cited paper to form a coherent narrative. It raises critical questions about the granularity of domains, such as whether Twitter constitutes one domain or multiple subdomains, but does not extensively compare or critique specific approaches. The section abstracts beyond individual papers by exploring broader implications of domain definitions in NLP, particularly in relation to language variation and model generalization."}}
{"id": "9b7e1e2e-438a-4e8d-b8ca-48230986cdad", "title": "Variety space", "level": "paragraph", "subsections": ["6db817f6-9f08-4ddc-bb6c-e7b08a7ea9aa"], "parent_id": "39fcba5e-b031-4b85-8cfa-0b18ca336cb2", "prefix_titles": [["title", "Neural Unsupervised Domain Adaptation in NLP---A Survey"], ["section", "domain"], ["paragraph", "Variety space"]], "content": "We believe it is time to reconsider the notion of \\textit{domain}, the use of the term itself, and raise even more awareness of the underlying variation in the data samples NLP works with. NLP is pervasively facing heterogeneity in data along many underlying (often unknown) dimensions.  \nA theoretical notion put forward by~\\newcite{Plank2016} is the \\textit{variety space}.  In the variety space\na \\textit{corpus} is seen as a subspace (subregion), a sample of the variety space. A corpus is a set of instances drawn from the underlying unknown high-dimensional variety space, whose dimensions (or latent factors) are fuzzy language and annotation aspects. These latent factors can be related to the notions discussed above,  such as genre (e.g., scientific, newswire, informal), \nsub-domain (e.g., finance, immunology, politics, environmental law, molecular biology) \nand socio-demographic aspects (e.g., gender), among other unknown factors, as well as stylistic or data sampling impacts (e.g., sentence length, annotator bias). \nIn spirit of the variety space~, \nwe suggest to use the more general term \\textit{variety}, rather than domain, which pinpoints better to the underlying linguistic differences and their implications rather than the technical assumptions. Each corpus is inevitably biased towards a specialized language and some latent aspects. Understanding bias sources and effects, besides effects only~, and documenting the known are the first important steps~, as is building broader, more varied corpora~. What we need more work on is to link the known to the unknown, and studying its impact. Doing so will ultimately help to not only overcome overfitting to overrepresented domains (e.g.,\\ the  newswire bias~), but also work on robustness and ultimately out-of-distribution generalization, as described later on. \nTreating data as `just another input' to machine learning is very problematic. \nFor example, it is less known that the well-known Penn Treebank consists of multiple genres~, including reviews and some prose. It has almost universally been treated as prototypical news domain. Similarly, social media is typically considered only non-canonical data, but an analysis revealed the data to lie on a ``continuum of similarity''~. This has implications on NLP performance.\nAs we have seen, there are a multitude of dimensions to consider in corpus composition and annotations, which are tied to the theoretical notion of a variety space. They challenge the true generalization capabilities of current models. \n\\textit{What remains is to study what variety comprises, how covert and overt factors impact results, and take them into consideration in modeling and evaluation.}", "cites": [7201], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section effectively synthesizes the theoretical framework of the variety space from the cited paper and extends it with broader observations and implications for NLP. It critically evaluates the limitations of the domain-centric view by highlighting biases and oversimplifications in corpus assumptions. The abstraction is strong, as it generalizes the concept into a meta-level discussion of linguistic variation and its impact on model generalization."}}
{"id": "6db817f6-9f08-4ddc-bb6c-e7b08a7ea9aa", "title": "Related problems", "level": "paragraph", "subsections": [], "parent_id": "9b7e1e2e-438a-4e8d-b8ca-48230986cdad", "prefix_titles": [["title", "Neural Unsupervised Domain Adaptation in NLP---A Survey"], ["section", "domain"], ["paragraph", "Variety space"], ["paragraph", "Related problems"]], "content": "Following the idea of the \\textit{variety space}, we discuss three related notions:  \\textit{cross-lingual learning}, \\textit{domain generalization/robustness}, and \\textit{out-of-distribution generalization}.  \nIn \\textit{cross-lingual learning} the feature space drastically changes, as alphabets, vocabularies and word order can be different. It can be seen as extreme adaptation scenario, for which parallel data may exist and can be used to build multilingual representations~. Second, instead of adapting to a particular target, there is some work on \\textit{domain generalization} aimed at building a single system  which is robust on several known target domains. One example is the SANCL shared task~, where participants were asked to build a single system that can robustly parse reviews, weblogs, answers, emails, newsgroups. In this setup, the DA problem boils down to finding a more robust system for given targets. It can be seen as optimizing for both in-domain and out-of-domain(s) accuracy.\nIf domains are unknown a priori, robustness can be taken a step further towards \\textit{out-of-domain generalization}, to unknown targets, the most challenging setup. A recent solution is \\textit{distributionally robust optimization}~, i.e., optimizing for worst-case performance without the knowledge of the test distribution. To do so, it assumes a \\textit{subpopulation shift}, where the test population is a subpopulation mix of the training distribution. A model is then trained to do well over a wide range of potential test distributions. Some early work in dialogue~ and parsing~ adopted a similar idea of \\textit{subdomains}, however, with manually identified subpopulations.  This bears some similarity to early work on leveraging general background knowledge (embeddings trained on general data) for domain adaptation~, and also relates to recent work on pre-training (Section~\\ref{sec:pre-training}). An alternative and complementary interesting line of research is to \\textit{predict test set performance} for new data varieties~.\n\\definecolor{modelc}{rgb}{0.86, 0.86, 0.86} \n\\definecolor{datac}{rgb}{1.0, 0.84, 0.0} \n\\definecolor{hybrid}{rgb}{0.93, 0.86, 0.51}\n\\begin{table}[ht!]\n\\centering\n\\small\n\\resizebox{1\\linewidth}{!}{\n\\begin{tabular}{|l|l|llll|llll|}\n\\toprule\n            & & \\multicolumn{4}{c|}{classif./inference} & \\multicolumn{4}{c|}{struct.\\ prediction}\\\\\nWork              &   Method              & \\rot{SA} & \\rot{LI} & \\rot{TC} &  \\rot{NLI} & \\rot{POS} & \\rot{DEP} & \\rot{NER} & \\rot{RE} \\\\ \n\\midrule\n\\rowcolor{modelc}\n\\textit{Model-centric:} &  &  & & & & & & &   \\\\\n\\rowcolor{modelc}\n(Ziser and Reichart 2017; 2018a; 2018b; 2019) & Neural SCL & \\CK  & & & & & & &   \\\\\n\\rowcolor{modelc}\n & Neural SCL (Joint AE-SCL) & \\CK  & & &  & & &    &\\\\\n\\rowcolor{modelc}\n & SDA  & \\CK & & & & &  & &    \\\\\n\\rowcolor{modelc}\n & MSDA  & \\CK& & & & & &  &  \\\\\n\\rowcolor{modelc}\n & MSDA & &  & &  &  \\CK &  &  &   \\\\ \n\\rowcolor{modelc}\n & MSDA & \\CK  &  & \\CK & & & &  &  \\\\\n\\rowcolor{modelc}\n\\rowcolor{modelc}\n & DANN & \\CK & &  & & &  &  &\\\\\n\\rowcolor{modelc}\n & DANN+SCL MemNet & \\CK &  & & & & & &     \\\\\n\\rowcolor{modelc}\n  & DANN/DSN & & & \\CK &  &   & & \\CK &   \\\\\n\\rowcolor{modelc}\n & DANN & & & & &  & \\CK &  &   \\\\  \n\\rowcolor{modelc}\n & DANN & & & & &  &  &  &  \\CK \\\\  \n\\rowcolor{modelc}\n & DANN & & & & & \\CK &   &  &   \\\\  \n\\rowcolor{modelc}\n & DANN & \\CK& & & &  & &  & \\\\\n\\rowcolor{modelc}\n & DANN & \\CK & \\CK& & &  & &   & \\\\\n\\rowcolor{modelc}\n & DANN & & & \\CK & & &  &   & \\\\\n\\rowcolor{modelc}\n & DANN & & & \\CK & & & & &  \\\\ \n\\rowcolor{modelc}\n & DANN+Wasserstein & & &  \\CK &  & & & &  \\\\ \n\\rowcolor{modelc}\n & DANN & & & & & & & &  \\CK  \\\\ \n\\rowcolor{modelc}\n & DANN & &  & & &  & &  &\\CK  \\\\% relation extraction\\\\\n\\rowcolor{modelc}\n & DANN & & &\\CK &   &  & & &\\\\ \n\\rowcolor{modelc}\n & DSN (GSN) & & &  & & &  &  & \\CK  \\\\ \n\\rowcolor{modelc}\n* & DANN, Shared encoders & \\CK & &  &  \\CK & & &  &  \\\\\n\\rowcolor{modelc}\n & DANN (concept embeddings) & \\CK & &  & & &  & &  \\\\\n\\rowcolor{modelc}\n & DANN (context embeddings) & & &  & & &  & \\CK &  \\\\\n\\rowcolor{datac}\n\\textit{Data-centric:} &  &  & & & & & & &   \\\\\n \\rowcolor{datac}\n & SSL, Multitask tri-training  & \\CK & & & & \\CK & & & \\\\\n \\rowcolor{datac}\n & SSL   &  & & & & \\CK & & & \\\\\n \\rowcolor{datac}\n & Deep self-training & & & & &  & \\CK &  &   \\\\ \n\\rowcolor{datac}\n & AdaptaBERT$\\diamond$ &  & & & & \\CK &  & \\CK &  \\\\\n\\rowcolor{modelc}\n\\rowcolor{datac}\n & Adaptive pre-training & & & & &  & \\CK &  &   \\\\\n\\rowcolor{datac}\n & Adaptive pre-training (incl.\\ multi-phase)& \\CK  & & \\CK & \\CK & & &  &  \\CK \\\\\n\\rowcolor{hybrid}\n\\textit{Hybrid:} &  &  & & & & & & &   \\\\\n\\rowcolor{hybrid}\n & Asymmetric tri-training &  \\CK & & & &  & & & \\\\\n\\rowcolor{hybrid}\n & Adaptive (temporal) ensembling & & & \\CK &   & & &  &  \\\\\n\\rowcolor{hybrid}\n & Cross-domain LM &  & & & & & & \\CK &  \\\\\n\\rowcolor{hybrid}\n & SelfAdapt (pivots+co-training) & \\CK & & & & & &  &   \\\\\n\\rowcolor{hybrid}\n & Multi-task-DA$\\diamond$ &  &  & & & \\CK & & \\CK &  \\\\ \n\\rowcolor{hybrid}\n & DistanceNet-Bandit & \\CK & & & & & &  &   \\\\\n\\rowcolor{hybrid}\n & PERL (pivots+context embeddings) & \\CK & & & & & &  &   \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Overview of neural UDA in NLP: method and task(s).     Methods: SCL = structural correspondence learning; AE = autoencoder; SDA = stacked denoising autoencoder; MSDA = marginalized SDA; DANN = domain-adversarial neural network; DSN = domain separation network; GSN = genre separation network; SSL = semi-supervised learning; LM = language modeling. Tasks: SA = sentiment analysis; LI = language identification; TC = binary text classification (incl.\\ machine reading, duplicate question detection, stance detection, intent classification, political data identification); NLI = natural language inference; POS = part-of-speech (incl.\\ Chinese word segmentation); DEP = dependency parsing; NER = named entity recognition (incl.\\ slot tagging, event trigger identification, named entity segmentation); RE = relation extraction. *with \\emph{cross-lingual} adaptation. $\\diamond$ applicable to UDA but main focus is supervised DA.}\n\\label{tab:overview}\n\\end{table}", "cites": [7208, 7205, 7206, 8328, 198, 200, 7202, 199, 7203, 7207, 197, 201, 7204], "cite_extract_rate": 0.29545454545454547, "origin_cites_number": 44, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple related concepts (cross-lingual learning, domain generalization, and out-of-distribution generalization) and connects them to the broader idea of domain adaptation. It abstracts these into a conceptual framework, highlighting their interrelations and implications for future research. While it includes some critical analysis (e.g., identifying challenges in out-of-distribution scenarios), deeper evaluation of methodological trade-offs is limited."}}
{"id": "e7bfc8a2-bf23-43e6-9205-98bfd6706973", "title": "Pivots-based DA", "level": "paragraph", "subsections": ["f9776413-2fae-418f-a1ca-93bd75a37f6b"], "parent_id": "ca38d72a-5b6c-45e9-87ef-e89625c503ae", "prefix_titles": [["title", "Neural Unsupervised Domain Adaptation in NLP---A Survey"], ["section", "Model-centric approaches"], ["subsection", "Feature-centric methods"], ["paragraph", "Pivots-based DA"]], "content": "Seminal \\textit{pivot-based} methods include: \\textit{structural correspondence learning} (SCL)  and \\textit{spectral feature alignment} (SFA) . They both aim at finding features which are common across domains by using unlabeled data from both domains. The two approaches differ in the specifics of the method to construct the shared space. SCL uses auxiliary functions inspired by~\\newcite{ando2005framework}, while SFA uses a graph-based spectral learning method. Creating domain-specific and domain-general features is the key idea of EasyAdapt~, a seminal supervised DA method.\nA recent line of work~ brings SCL back to neural networks. \nIn particular, \\newcite{Ziser2017} propose to combine the strengths of pivot-based methods with autoencoder neural networks in an \\textit{autoencoder structural correspondence learning} (AE-SCL) model. Autoencoders are used to learn latent representations to map non-pivots to pivots, and these encodings are then used to augment the training data. \nThe main drawback of this approach is that the output vector representations of the text are unique and not context-dependent. To solve this problem, a \\textit{pivot-based language modeling} (PBLM) method has been proposed . PBLM effectively combines SCL with a neural language model based on long short-term memory (LSTM) networks which predicts the presence of pivots and non-pivots, thus making representations structure-aware. A weakness of the PBLM approach relies in the large number of pivots needed. To remedy this issue, Ziser and Reichart~\\shortcite{Ziser2019} adopted a \\textit{task refinement learning} approach using PBLM (called TRL-PBLM), showing gains in both accuracy and stability over different hyperparameters selection choices. The approach is an iterative training process where the network is trained using an increasingly larger amount of pivots. Recent hybrid UDA work extends pivots with contextual embeddings~, as we discuss in Section~\\ref{sec:hybrid}.\nA common issue with the aforementioned methods is that they involve two independent steps: one for representation learning and one for task learning. To tackle this issue, recent studies propose training the two tasks jointly (i.e., pivot prediction and sentiment)~ and learn pivots \\textit{automatically} via attention~, similar to work on automatic non-pivot identification~. \nTo the best of our knowledge, neural pivot-based UDA approaches have been solely applied to sentiment classification, cf.\\ Table~\\ref{tab:overview}. Notably, Ziser and Reichart~\\shortcite{Ziser2018} went a step further, and applied neural SCL cross-lingually; the NLP task is still sentiment classification. The effectiveness of pivot-based methods in neural models remains to be tested. \nEarly non-neural work applied SCL to structure prediction problems with mixed results, i.e., POS~ and parsing~.", "cites": [7205, 198], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key pivot-based DA methods and their neural adaptations, connecting ideas across multiple works (e.g., SCL, SFA, AE-SCL, PBLM, TRL-PBLM) to present a coherent narrative. It provides some critical analysis by highlighting limitations (e.g., non-contextual representations, need for large pivot sets) and how subsequent methods attempt to address them. However, it stops short of offering deep, meta-level insights or a novel framework, focusing more on trends and partial evaluations."}}
{"id": "f9776413-2fae-418f-a1ca-93bd75a37f6b", "title": "Autoencoder-based DA", "level": "paragraph", "subsections": [], "parent_id": "e7bfc8a2-bf23-43e6-9205-98bfd6706973", "prefix_titles": [["title", "Neural Unsupervised Domain Adaptation in NLP---A Survey"], ["section", "Model-centric approaches"], ["subsection", "Feature-centric methods"], ["paragraph", "Pivots-based DA"], ["paragraph", "Autoencoder-based DA"]], "content": "Early neural approaches for UDA have been based on autoencoders. Autoencoders are neural networks that are employed to learn latent representations from raw data in an unsupervised fashion by learning with an input reconstruction loss. Motivated by the \\textit{denoising autoencoders} , the first work in this line is by Glorot et al.~\\shortcite{Glorot2011}, who introduced the \\textit{stacked denoising autoencoder} (SDA) for domain adaptation. Basically, a SDA automatically learns a robust and unified feature representation for all domains by stacking multiple layers, and artificially corrupts the inputs with a Gaussian noise that the decoder needs to reconstruct. However, SDAs showed issues in speed and scalability to high-dimensional data. To mitigate these limitations, a more efficient \\textit{marginalized stacked denoising autoencoder} (MSDA) that marginalizes the noise was proposed . MSDAs have been further extended by Yang and Eisenstein~\\shortcite{yang_fast_2014} with marginalized structured dropout, and by Clinchant et al.~\\shortcite{Clinchant2016}, which improved the regularization of MSDAs following the insights from the domain adversarial training of neural networks  (described in Section \\ref{sec:adversarial}). The main drawback of autoencoder approaches is that the induced representations do not make use of any linguistic information.", "cites": [7207], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the evolution of autoencoder-based domain adaptation methods by connecting early SDA to improved variants like MSDA, and mentions extensions by other researchers. It also critically points out the main drawback of these methods—lack of linguistic information in the induced representations. While there is some level of integration and critique, the analysis remains focused on methodological aspects without broader abstraction or a novel framework."}}
{"id": "daf04785-7400-42dd-9cc0-83125b97bf06", "title": "Domain adversaries", "level": "paragraph", "subsections": ["1f2401b3-fdc0-416c-b856-c1a9baf92cb0"], "parent_id": "ea70b1e9-6236-4e94-93d7-a8565489acda", "prefix_titles": [["title", "Neural Unsupervised Domain Adaptation in NLP---A Survey"], ["section", "Model-centric approaches"], ["subsection", "Loss-centric methods"], ["paragraph", "Domain adversaries"]], "content": "\\label{sec:adversarial}\nThe most widespread methods for neural UDA are based on the use of \\textit{domain adversaries} . Inspired by the way generative adversarial networks (GANs)  minimize the discrepancies between training and synthetic data distributions, domain adversarial training aims at learning latent feature representations that serve at reducing the discrepancy between the source and target distributions. The intuition behind these methods puts its ground on the theory on domain adaptation , which argues that cross-domain generalization can be achieved by means of feature representations for which the origin (domain) of the input example cannot be identified.\nThe seminal approach in this category are DANNs: \\textit{domain-adversarial neural networks}~. The aim is to estimate an accurate predictor for the task  while maximizing the confusion of an auxiliary domain classifier in distinguishing features from the source or the target domain. To learn domain-invariant feature representations, DANNs employ a loss function via a \\textit{gradient reversal layer} which ensures that feature distributions in the source and target domains are made similar. The strength of this approach is in its scalability and generality; \nhowever, DANNs only model feature representations that are shared across both domains, and suffer from a vanishing gradient problem \nwhen the domain classifier accurately discriminates source and target representations . \\textit{Wasserstein} methods~ are more stable training methods than gradient reversal layers. Instead of learning a classifier to distinguish domains, they attempt to reduce the approximated Wasserstein distance (also\nknown as Earth Mover’s Distance). A recent study on question pair classification shows that the two adversarial methods reach similar performance, but Wasserstein enables more stable training~.\nDANNs have been applied in many NLP tasks in the last few years, mainly to sentiment classification (e.g., Ganin et al.~\\shortcite{Ganin2016}, Li et al.~\\shortcite{Li2018}, Shen et al.~\\shortcite{Shen2018}, Rocha and Lopes Cardoso~\\shortcite{rocha-lopes-cardoso-2019-comparative}, Ghoshal et al.~\\shortcite{ghosal-etal-2020-kingdom}, to name a few), \nbut recently to many other tasks as well: language identification , natural language inference~, POS tagging~, parsing~, trigger identification~, relation extraction , and other (binary) text classification tasks like relevancy identification , machine reading comprehension , stance detection~, and duplicate question detection . This makes DANNs the most widely used UDA approach in NLP, as illustrated in Table~\\ref{tab:overview}.\nTo model features that also belong to either the source or target domain, \\textit{domain separation networks} (DSNs)  have been proposed. \nDSNs separate latent representations in i) separate private encoders (i.e., one for each domain) and ii) a shared encoder (in charge to reconstruct the input instance using these representations). This bears similarities to a traditional supervised  method~. \nThe main drawback of DSNs is that domain-specific representations are solely used in the decoder, leaving the classifier to be trained on the domain-invariant representations only.\nDSNs have seen a notable success in Computer Vision (CV) . In NLP, Shi et al.~\\shortcite{Shi2018} propose the \\textit{genre separation networks} (GSNs) as a variant of the DSNs, introducing a novel reconstruction component that leverages both shared and private feature representations in the learning process. As noted also by~\\newcite{Han2019}, a downside of adversarial methods is that they require careful balancing between objectives~ to avoid instability during learning~.", "cites": [7208, 7206, 7202, 7203, 7209, 7204], "cite_extract_rate": 0.3157894736842105, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a coherent synthesis of adversarial methods for domain adaptation in NLP, integrating multiple papers to form a structured overview of DANNs and DSNs. It critically evaluates the strengths and limitations of these methods, such as the vanishing gradient issue with DANNs and the restricted use of domain-specific features in DSNs. The discussion abstracts beyond individual works by identifying broader patterns and challenges in adversarial domain adaptation, including the need for objective balancing and stable training techniques."}}
{"id": "5bce59fd-0c6d-4ecf-9c50-73574dd46157", "title": "Pseudo-labeling", "level": "subsection", "subsections": [], "parent_id": "ff0f92a3-c7da-4d2f-81cf-f0c0d17064a9", "prefix_titles": [["title", "Neural Unsupervised Domain Adaptation in NLP---A Survey"], ["section", "Data-centric methods"], ["subsection", "Pseudo-labeling"]], "content": "The main idea of pseudo-labeling is to apply a trained classifier to predict labels on unlabeled instances, which are then treated as `pseudo' gold labels. Pseudo-labeling applies semi-supervised methods~ such as bootstrapping methods like self-training, co-training and tri-training or  methods such as temporal ensembling~ by using either the same model, a teacher model, or multiple bootstrap models which may include slower but more accurate hand-crafted models~ to guide  pseudo-labeling. Most pseudo-labeling works date back to traditional non-neural learning methods. Bootstrapping methods for domain adaptation are well-studied in parsing~. They include models trained on other grammar formalisms to improve dependency parsing on Twitter~. Recently, this line of classics has been revisited~. For example, classic methods such as tri-training constitute a strong baseline for domain shift in neural times~.  Pseudo-labeling has recently been studied for parsing with contextualized word representations~ and a recent work  proposes \\textit{adaptive ensembling}~ as extension of temporal ensembling (see \\textit{hybrid} methods in Section~\\ref{sec:hybrid}).", "cites": [199, 7203, 200, 201], "cite_extract_rate": 0.2777777777777778, "origin_cites_number": 18, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of pseudo-labeling techniques by connecting traditional and neural approaches, but it primarily functions as a descriptive overview without deep comparative analysis or critique. It identifies patterns such as the use of ensembling and the revival of classic methods, yet lacks a nuanced evaluation of their strengths and limitations in domain adaptation."}}
{"id": "207ed7a7-c79f-4014-9aba-b63c20b1bd7c", "title": "Data selection", "level": "subsection", "subsections": [], "parent_id": "ff0f92a3-c7da-4d2f-81cf-f0c0d17064a9", "prefix_titles": [["title", "Neural Unsupervised Domain Adaptation in NLP---A Survey"], ["section", "Data-centric methods"], ["subsection", "Data selection"]], "content": "\\label{sec:dataselection}\nA relatively unexplored area is data selection for adaptation, which is gaining traction again in light of large pre-trained models (which data should they be trained on?) and the related problem of cross-lingual learning (what is/are the best source language(s) to transfer from?).\nData selection aims to select the best matching data for a new domain, typically by using perplexity~ or using domain similarity measures such as Jensen-Shannon divergence over term or topic distributions~. This has mostly been studied for MT~, but also for parsing~ and sentiment analysis~ though for supervised domain adaptation setups only. For parsing and sentiment analysis, the simple Jensen-Shannon divergence on term distribution constitutes a strong baseline~. Within MT,~\\newcite{van-der-wees-etal-2017-dynamic} propose a dynamic data selection approach which changes the subset of data in each epoch for MT.  Data selection is gaining attention, in light of the abundance of data. Recent work investigates data representation and cosine similarity for MT data selection~. Similarly, distance metrics have been been recently used for multi-source domain adaptation of sentiment classification models using a bandit-based approach~. For morphosyntactic cross-lingual work, simple overlap metrics are indicative~. Another line explores\nwhether tailoring large pre-trained models to the domain of a target task is still beneficial, and use of data selection to overcome costly expert selection. They propose two multi-phase pre-training methods~ (as discussed further below) with promising results on text classification tasks.", "cites": [197, 200], "cite_extract_rate": 0.2, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes a few key papers on data selection, connecting them under the broader theme of domain adaptation in NLP, particularly for MT, parsing, and sentiment analysis. It shows some abstraction by identifying trends like the use of distance metrics and cosine similarity. However, the critical analysis is limited, as it does not deeply evaluate the strengths or weaknesses of the approaches. It remains insightful but could offer more nuanced comparisons or critiques."}}
{"id": "0898eaad-4208-4e7c-87fc-60782cce2831", "title": "varieties", "level": "subsection", "subsections": [], "parent_id": "ff0f92a3-c7da-4d2f-81cf-f0c0d17064a9", "prefix_titles": [["title", "Neural Unsupervised Domain Adaptation in NLP---A Survey"], ["section", "Data-centric methods"], ["subsection", "varieties"]], "content": ") still relevant?}\\label{sec:pre-training}\nLarge pre-trained models have become ubiquitous in NLP~. \\textit{Fine-tuning} a transformer-based model with a small amount of labeled data often reaches high performance across NLP tasks and has become a de-facto standard. It means starting from the pre-trained model weights and training a new task-specific layer on supervised data. A natural question which arises is how universal such large models are.\nIs bigger better? And are domains (or varieties) still relevant? We return to these questions after depicting pre-training strategies. We delineate:\n\\begin{enumerate}\n\\itemsep0em \n    \\item \\textbf{Pre-training}: pre-training alone (e.g., multilingual BERT; language-specfic BERTs from scratch);\n    \\item \\textbf{Adaptive pre-training}: This encompasses pre-training, followed by secondary stages of pre-training on unlabeled data or on labeled data from intermediate higher-resource auxiliary tasks:\n    \\begin{enumerate}\n    \\item \\textbf{Multi-phase pre-training}:  two or more phases of secondary pre-training, from broad-coverage to domain-/task-adaptive pre-training (i.e., BioBERT, AdaptaBERT, DAPT, TAPT). They differ by the source of unlabeled data: broad-domain $\\succ$ domain-specific $\\succ$ task-specific;\n    \\item \\textbf{Auxiliary-task pre-training}: pre-training, followed by (possibly multiple stages of) \\textit{auxiliary-task} pre-training (e.g., supplementary training on intermediate labeled-data tasks, STILTs).\n    \\end{enumerate}\n\\end{enumerate}\n\\textit{Pre-training} (option 1) can be seen as straightforward adaptation, analogous to zero-shot in cross-lingual learning. The key idea is to train encoders  with self-supervised objectives like (masked) language model  and  related unsupervised objectives~. \nIn light of a domain shift, \\textit{adaptive pre-training} is beneficial, in which in one instantiation contextualized embeddings are adapted to text from the target domain by masked language modeling, as introduced by~\\newcite{Han2019}.  \nMore broadly, we distinguish two variants of \\textit{adaptive pre-training}. They differ whether unlabeled data or some form of auxiliary labeled data (or intermediate tasks data) is used. These variants can also be combined, and fine-tuning applies to all setups, if data is available. The key idea of \\textit{multi-phase pre-training} (option 2a) is to use secondary-stage unsupervised pre-training, such as broad-coverage domain-specific BERT variants (e.g., BioBERT). ~\\newcite{gururangan2020dont}  propose  \\textit{domain-adaptive pre-training} (DAPT) from a broader corpus, compared to~, and \\textit{task-specific pre-training} (TAPT) which uses unlabeled data closer-and-closer to the task distribution. As these studies show, domain-relevant data is important for pre-training~ in both high and low resource setups. Similar adaptive pre-training work has been shown to be effective for dependency parsing~. \nThis suggests that there exists a spectrum of domains of varying granularity, confirming ideas around domain similarity~. Domains (\\textit{varieties}) do still matter in today's models.\nAn alternative line of work (option 2b) is \\textit{auxiliary-task pre-training} and use labeled auxiliary tasks either via multi-task learning (MTL)~ or intermediate-task transfer~. The latter proposed  \\textit{supplementary training on intermediate labeled-data tasks for transfer} (STILT)~, and recently adopted this idea to cross-lingual learning, where  English is used as intermediate-task for zero-shot transfer~. \nThe choice of data used for pre-training (or the auxiliary tasks) do matter. Current transformer models are trained on either large general data like BookCorpus and Wikipedia in BERT~ or target-specific samples, like papers from Semantic Scholar in SciBERT~, and PubMed abstracts and PMC full-text articles in BioBERT~. What denotes \\textit{relevant} data is an open question. Today, it is either general background knowledge, domain-specific target data, or a combination thereof, possibly via auxiliary tasks or intermediate training stages. Most of these have been carefully selected manually, raising interesting connections to data selection (Section~\\ref{sec:dataselection}) and finding better curricula~ to learn under domain shift~.\nWhile large pre-trained models have shown to work well, many questions and challenges remain. \nRecent work has shown that these models degrade on out-of-domain data, maximum likelihood training makes them too over-confident~ and particularly calibration is important for out-of-domain generalization .  An acknowledged issue with fine-tuning is the brittleness of the process~. Even with the same hyperparameters, distinct runs can lead to drastically different results and training data order and seed choice have a considerably impact~. Deeper investigations into what such models capture, how they can be  robustly trained in light of known test distributions or out-of-domain conditions are interesting issues.", "cites": [7210, 7211, 8329, 200, 203, 202], "cite_extract_rate": 0.3888888888888889, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple cited papers to present a coherent narrative around pre-training and adaptive pre-training methods in domain adaptation. It critically evaluates the effectiveness and limitations of these techniques, particularly highlighting issues like brittleness and over-confidence. Furthermore, it abstracts key patterns such as the spectrum of domain granularity and the importance of domain-relevant data, offering meta-level insights into the field."}}
{"id": "3233ffc9-940f-41ae-8a10-89c593c75f9a", "title": "Hybrid approaches", "level": "section", "subsections": [], "parent_id": "c0dc96fc-4540-492b-9493-eb9335bae96c", "prefix_titles": [["title", "Neural Unsupervised Domain Adaptation in NLP---A Survey"], ["section", "Hybrid approaches"]], "content": "\\label{sec:hybrid}\nWork on the intersection of data-centric and model-centric methods can be plentiful. It currently includes combining semi-supervised objectives with an adversarial loss~, combining pivot-based approaches with pseudo-labeling~ and very recently with contextualized word embeddings~, and combining multi-task approaches with domain shift~, multi-task learning with pseudo-labeling (multi-task tri-training)~, and  \\textit{adaptive ensembling}~, which uses a student-teacher network with a consistency-based self-ensembling loss and a temporal curriculum. They apply adaptive ensembling to study temporal and topic drift in  political data classification~.", "cites": [199, 198, 7203], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions several hybrid methods and cites relevant papers but lacks a clear synthesis of how these approaches are interconnected or complementary. It does not critically evaluate their strengths or weaknesses, nor does it abstract beyond the specific methods to highlight broader trends or principles. The narrative remains largely descriptive and fragmented."}}
{"id": "83ffc074-23fc-4767-a10b-6f3c634db8d3", "title": "Comprehensive UDA benchmarks", "level": "paragraph", "subsections": ["c5b7a0ec-a43e-47b8-90c1-9cd74c56feee"], "parent_id": "c3dfb536-dc23-4a84-94a2-e0149e6fa571", "prefix_titles": [["title", "Neural Unsupervised Domain Adaptation in NLP---A Survey"], ["section", "Challenges and future directions"], ["paragraph", "Comprehensive UDA benchmarks"]], "content": "Concretely, we recommend a) to create new benchmarks for UDA with multiple tasks and of increasing complexity, setups beyond 1:1 adaptation, and datasets which document known \\textit{variety facets} of the data~. This will help to learn about the known and unknown (Section~\\ref{sec:varietyspace}) as `variety' (domain) matters; \nb) to release unlabeled data from the broader distribution from which annotated data was sampled, in line with~\\newcite{gururangan2020dont}; this allows studying diachronic effects, as labeled evaluation data lacks diversity in terms of topics and time~; and c) to release unaggregated, multiple annotations to study divergences in annotations~.", "cites": [199, 204], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section draws on the cited papers to highlight broader needs in UDA benchmarking, such as data diversity and annotation quality. While it integrates these ideas to some extent, the synthesis is limited to suggesting improvements rather than offering a novel framework. The critical perspective is evident in identifying gaps like the lack of topic and temporal diversity in labeled data, and the analytical nature of the section allows for some abstraction toward principles of better benchmarking."}}
{"id": "c5b7a0ec-a43e-47b8-90c1-9cd74c56feee", "title": "Back to the roots and how knowledge transfers", "level": "paragraph", "subsections": ["0d473dcd-f6aa-4bc5-9b87-33f00540dc19"], "parent_id": "83ffc074-23fc-4767-a10b-6f3c634db8d3", "prefix_titles": [["title", "Neural Unsupervised Domain Adaptation in NLP---A Survey"], ["section", "Challenges and future directions"], ["paragraph", "Comprehensive UDA benchmarks"], ["paragraph", "Back to the roots and how knowledge transfers"]], "content": "Revisiting classics in neural times is beneficial, as shown for example in recent work which brings back SCL and pseudo-labeling methods (see Table~\\ref{tab:overview}), but much is left to see how these methods generalize. This can be linked to the question on what representations capture~ and how knowledge transfers~.", "cites": [205], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions the revival of classical methods like SCL and pseudo-labeling in the context of neural UDA but lacks substantial synthesis of ideas from the cited paper. It hints at the importance of knowledge transfer and representation learning without elaborating on how these concepts are analyzed or connected. The critique and abstraction are minimal, offering only a surface-level analytical attempt without deeper insights or comparisons."}}
