{"id": "55a2be7c-6fcb-4383-8202-8fdfadb20425", "title": "Introduction", "level": "section", "subsections": ["44740c76-c201-4b91-9a7c-ddb0747d5a17"], "parent_id": "c120a577-e918-4f1d-894c-cc2b4adb602b", "prefix_titles": [["title", "A Survey of Evaluation Metrics Used for NLG Systems"], ["section", "Introduction"]], "content": "Natural Language Generation (NLG) refers to the process of automatically generating human-understandable text in one or more natural languages. The ability of a machine to generate such natural language text which is indistinguishable from that generated by humans is considered to be a pre-requisite for Artificial General Intelligence (AGI) - the holy grail of AI. Indeed, the Turing test , widely considered to be the ultimate test of a machine's ability to exhibit human-like intelligent behaviour requires a machine to have natural language conversations with a human evaluator. A machine would pass the test if the evaluator is unable to determine whether the responses are being generated by a human or a machine.\nSeveral attempts have been made, but no machine has been able to convincingly pass the Turing test in the past 70 years since it was proposed. However, steady progress has been made in the field in the past 70 years with remarkable achievements in the past few years since the advent of Deep Learning . \nIndeed, we have come a long way since the early days of AI, when the interest in NLG was limited to developing rule based machine translation systems  and dialog systems .\nThe earliest demonstration of the ability of a machine to translate sentences was the Georgetown-IBM Experiment where an IBM 701 mainframe computer was used to translate 60 Russian sentences into English . The computer used a rule based system with just six grammar rules and a vocabulary of 250 words. Compare this to the modern neural machine translation systems which get trained using millions of parallel sentences on multiple TPUs using a vocabulary of around 100K words . \nThe transition to such mammoth data driven models is the result of two major revolutions that the field of Natural Language Processing (which includes Natural Language Understanding and  Natural Language Generation) has seen in the last five decades. The first being the introduction of machine learning based models in the late 1980s which led to the development of data driven models which derived insights from corpora. This trend continued with the introduction of Decision Trees, Support Vector Machines and statistical models like Hidden Markov Models, the IBM translation model, Maximum Entropy Markov Models, and Conditional Random Fields, which collectively dominated NLP research for at least two decades. The second major revolution was the introduction of deep neural network based models which were able to learn from large amounts of data and establish new state of the art results on a wide variety of tasks . \nThe advent of Deep Learning has not only pushed the state of the art in existing NLG tasks but has created interest in solving newer tasks such as image captioning, video captioning, etc. Indeed, today NLG includes a much wider variety of tasks such as machine translation, automatic summarization, table-to-text generation (more formally, structured data to text generation), dialogue generation, free-form question answering, automatic question generation, image/video captioning, grammar correction, automatic code generation, \\textit{etc}. This wider interest in NLG is aptly demonstrated by the latest GPT-3 model  which can write poems, oped-articles, stories and code (among other things). This success in NLP, in general, and NLG in particular, is largely due to 3 factors: (i) the development of datasets and benchmarks which allow training and evaluating models to track progress in the field (ii) the advancements in Deep Learning which have helped stabilise and accelerate the training of large models and (iii) the availability of powerful and relatively cheaper compute infrastructure on the cloud \\footnote{GCP: https://cloud.google.com/ AWS: https://aws.amazon.com/ Azure: https://azure.microsoft.com/}. Of course, despite these developments, we are still far from developing a machine which can pass the Turing test or a machine which serves as the fictional Babel fish\\footnote{Hitchhiker's Guide to the Galaxy} with the ability to accurately translate from one language to any other language. However, there is no doubt that we have made remarkable progress in the last seven decades. \nThis brings us to the important question of ``tracking progress'' in the field of NLG. How does one convincingly argue that a new NLG system is indeed better than existing state-of-the-art systems? The ideal way of doing this is to show multiple outputs generated by such a system to humans and ask them to assign a score to the outputs. The scores could either be absolute or relative to existing systems. Such scores provided by multiple humans can then be appropriately aggregated to provide a ranking of the systems. However, this requires skilled annotators and elaborate guidelines which makes it a time consuming and expensive task. Such human evaluations can act as a severe bottleneck, preventing rapid progress in the field. For example, after every small change to the model, if researchers were to wait for a few days for the human evaluation results to come back, then this would act as a significant impediment to their work. Given this challenge, the community has settled for automatic evaluation metrics, such as BLEU , which assign a score to the outputs generated by a system and provide a quick and easy means of comparing different systems and tracking progress. \nDespite receiving their fair share of criticism, automatic metrics such as BLEU, METEOR, ROUGE, \\textit{etc.}, continued to remain widely popular simply because there was no other feasible alternative. In particular, despite several studies  showing that BLEU and similar metrics do not correlate well with human judgements, there was no decline in their popularity. This is illustrated in Figure \\ref{fig:init_metrics} plotting the number of citations per year on some of the initial metrics from the time they were proposed up to recent years. The dashed lines indicate the years in which some of the major criticisms were published on these metrics, which, however, did not impact the adoption of these metrics.\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[scale=0.27]{initial_metrics_years.png}\n    \\caption{Number of citations per year on a few popular metrics. Dashed lines represent some of the major criticisms on these metrics at the corresponding year of publication.}\n    \\label{fig:init_metrics}\n\\end{figure}\\\\\nOn the contrary as newer tasks like image captioning, question generation, dialogue generation became popular, these metrics were readily adopted for these tasks too. However, it soon became increasingly clear that such adoption is often not prudent given that these metrics were not designed for the newer tasks for which they are being adopted. For example,  show that for the task of automatic question generation, it is important that the generated question is ``answerable'' and faithful to the entities present in the passage/sentence from which the question is being generated. Clearly, a metric like BLEU is not adequate for this task as it was not designed for checking ``answerability''. Similarly, in a goal oriented dialog system, it is important that the output is not only fluent but also leads to goal fulfillment (something which BLEU was not designed for). \\Akash{Should we give some other example instead of goal oriented dialog system? Although BLEU might be used here, but I think researchers primarily rely on extrinsic evaluation methods to evaluate whether the task is fulfilled or not\\MK{Yes, I agree. Please replace by an apt example.}}\nSummarising the above discussion and looking back at the period from 2014-2016 we make 3 important observations (i) the success of Deep Learning had created an interest in a wider variety of NLG tasks (ii) it was still infeasible to do human evaluations at scale and (iii) existing automatic metrics were proving to be inadequate for capturing the nuances of a diverse set of tasks. This created a fertile ground for research in automatic evaluation metrics for NLG. Indeed, there has been a rapid surge in the number of evaluation metrics proposed since 2014. \nIt is interesting to note that from 2002 (when BLEU was proposed) to 2014 (when Deep Learning became popular) there were only about 10 automatic NLG evaluation metrics in use. Since 2015, a total of atleast 36 new metrics have been proposed. In addition to earlier rule-based or heuristic based metrics such as Word Error Rate (WER), BLEU, METEOR and ROUGE, we now have metrics which exhibit one or more of the following characteristics: (i) use (contextualized) word embeddings  (ii) are pre-trained on large amounts of unlabeled corpus (e.g. monolingual corpus in MT  or Reddit conversations in dialogue) \n(iii) are fine-tuned on task-specific annotated data containing human judgements  and (iv) capture task specific nuances . This rapid surge in a relatively short time has lead to the need for a survey of existing NLG metrics. Such a survey would help existing and new researchers to quickly come up to speed with the developments that have happened in the last few years. \\MK{Somewhere in this paragraph add a sentence about the WMT shared task on evaluation. Also sneak in a sentence about the ``Evaluating evaluation metrics'' which received an honorable mention at ACL 2020 saying that this suggests that this area is really important.}", "cites": [7454, 4222, 2372, 38, 549, 2361, 166, 679, 4223], "cite_extract_rate": 0.391304347826087, "origin_cites_number": 23, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the evolution of NLG and the role of evaluation metrics by integrating historical context, technological advances, and limitations of early metrics. It critically discusses the shortcomings of BLEU and similar metrics for newer tasks like question generation and dialogue systems, referencing relevant papers. While it identifies broader trends in the use and inadequacy of metrics, it does not yet fully generalize into a meta-level framework or propose a novel analytical structure."}}
{"id": "cc9f5c95-032f-491a-9877-1a054c092a2d", "title": "Human Evaluation Setup", "level": "subsection", "subsections": [], "parent_id": "fe64d29d-38db-4ef4-ab98-b1d7d4fbfd57", "prefix_titles": [["title", "A Survey of Evaluation Metrics Used for NLG Systems"], ["section", "Human evaluation of NLG Systems"], ["subsection", "Human Evaluation Setup"]], "content": "Depending on the budget, availability of annotators, speed and required precision, different setups have been tried for evaluating NLG systems. The different factors to consider in such an evaluation setup are as follows:\n\\begin{itemize}\n    \\item \\textbf{Type of evaluators:} The evaluators could be experts , crowdsourced annotators , or even end-users  depending on the requirements of the task and the goal of the evaluation. For example, for evaluating a translation system one could hire bilingual experts (expensive) or even monolingual experts (relatively less expensive). The monolingual experts could just compare the output to an available reference output whereas with bilingual experts such a reference output is not needed. Further, a bilingual expert will be able to better evaluate where the nuances in the source language are accurately captured in the target language. If the speed of evaluation is the primary concern then crowd-sourced workers can also be used. In such a situation, one has to be careful to provide very clear guidelines, vet the workers based on their past records, immediately weed out incompetent workers and have an additional layer of quality check (preferably with the help of 1-2 expert in-house annotators). Clearly such crowdsourced workers are not preferred in situations requiring domain knowledge - e.g., evaluating an NLG system which summarises financial documents. For certain tasks, such as dialogue generation, it is best to allow end-users to evaluate the system by engaging in a conversation with it. They are better suited to judge the real-world effectiveness of the system. \n    \\item \\textbf{Scale of evaluation:} The annotators are typically asked to rate the output on a fixed scale, with each number corresponding to a specific level of quality, called the Likert scale . In a typical Likert scale the numbers 1 to 5 would correspond to Very Poor, Poor, Okay, Good and Very Good. However, some works  have also experimented with a dynamic/movable continuous scale that can allow the evaluator to give more nuanced judgements. \n    An alternate setting asks humans to assign a rating to the output based on the amount of post-editing required, if any, to make the output acceptable . The evaluators could also be asked for binary judgements rather than a rating to indicate whether a particular criteria is satisfied or not. This binary scale is sometimes preferred over a rating scale, which usually contains 5 or 7 rating points, in order to force judges to make a clear decision rather than give an average rating (by choosing a score at the middle of the scale) . By extension, any even-point rating scale could be used to avoid such indecisiveness.\n    \\item \\textbf{Providing a reference and a context:} In many situations, in addition to providing the output generated by the system, it is helpful to also provide the context (input) and a set of reference outputs (if available). However, certain evaluations can be performed even without looking at the context or the reference output. For instance, evaluating fluency (grammatical correctness) of the generated sentence does not require a reference output. References are helpful when the evaluation criteria can be reduced to a problem of comparing the similarity of information contained in the two texts. For example, in most cases, a generated translation can be evaluated for soundness (coherence) and completeness (adequacy) by comparing with the reference (without even looking at the context). However, for most NLG tasks, a single reference is often not enough and the evaluator may benefit from looking at the context. The contexts contains much more information which is difficult to be captured by a small set of references. In particular, referring to the examples provided for ``Abstractive Summarisation'', ``Image Captioning'' and ``Dialogue Generation'' in Table \\ref{tab:examples_nlg}, it is clear that it is difficult for the evaluator to do an accurate assessment by only looking at the generated output and the providing references. Of course, reading the context adds to the cognitive load of the evaluator but is often unavoidable. \n    \\item \\textbf{Absolute v/s relative evaluation :} The candidate output could be evaluated individually or by comparing it with other outputs. In an individual output evaluation, the candidate is provided an absolute rating for each desired criteria. On the other hand, in a comparison setup, an annotator could either be asked to simultaneously rate the multiple outputs (from competing systems)  or be asked to preferentially rank the multiple outputs presented . \n    This could also just be a pairwise comparison  of two systems. In such a setup, \n    the two systems are compared based on the number of times their outputs were preferred (wins), not preferred (losses), and equally preferred (ties).\n    \\item \\textbf{Providing Rationale :} The evaluators might additionally be asked to provide reasons for their decisions, usually by highlighting the corresponding text that influenced the rating . Such fine-grained feedback can often help in further improving the system. \n\\end{itemize}\nIrrespective of the setup being used, typically multiple evaluators are shown the same output and their scores are then aggregated to come up with a final score for each output or the whole system. The aggregate can be computed as a simple average or a weighted average wherein each annotator is weighted based on his/her past performance or agreement with other annotators . In general, it is desired to have a high inter-annotator agreement (IAA), which is usually measured using Cohen's Kappa or Fleiss Kappa co-efficient or Krippendorffâ€™s alpha.  \nAlternatively, although not popularly, IAA could be measured using Jaccard similarity, or an F1-measure (based on precision and recall between annotators) . \nAchieving a high-enough IAA is more difficult on some NLG tasks which have room for subjectivity . A lower IAA can occur due to (i) human-error (ii) inadequacy of the guidelines or setup (iii) ambiguity in the text .  \nTo enhance IAA,  find that asking the evaluators to highlight the portion of the text that lead to their decision or rating helps in getting better agreement. Alternatively,  arrange for a discussion between the annotators after the first round of evaluation, so as to mutually agree upon the criteria for the ratings.\nTo get a better IAA and hence a reliable evaluation, it is important that the human evaluators be provided with clear and sufficient guidelines. These guidelines vary across different NLG tasks as the criteria used for evaluation vary across different tasks, as explained in the next subsection.", "cites": [4225, 3572, 8755, 2364, 4227, 4226, 4223, 4224, 2001, 2372], "cite_extract_rate": 0.4, "origin_cites_number": 25, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section thoughtfully integrates multiple cited papers to discuss key factors in human evaluation setups for NLG systems. It connects ideas across works, such as the use of reference texts and context, and highlights limitations of automatic metrics. While it provides a coherent analytical framework, the critique of the cited works is moderate and focuses more on implications than deep limitations."}}
{"id": "ce4c0442-37d5-49b0-be92-0c1141ceb995", "title": "Criteria used for Evaluating NLG systems ", "level": "subsection", "subsections": [], "parent_id": "fe64d29d-38db-4ef4-ab98-b1d7d4fbfd57", "prefix_titles": [["title", "A Survey of Evaluation Metrics Used for NLG Systems"], ["section", "Human evaluation of NLG Systems"], ["subsection", "Criteria used for Evaluating NLG systems "]], "content": "Most human evaluations are based on checking for task fulfillment, \\textit{i.e.}, humans are asked to rate or compare the generated sentences (and the generating systems) to indicate how satisfactorily they meet the task requirements overall. However, evaluations can also be performed at a more fine-grained level where the various contributing factors are individually evaluated, \\textit{i.e.}, the generated text is assigned a separate rating or ranking based on each of the desired qualities, independent of the other qualities/criteria. \nOne such desired criteria is that the generated texts should have good `fluency'.\n\\textbf{Fluency} refers to correctness of the generated text with respect to grammar and word choice, including spellings. \nTo check for fluency in the generated output, the evaluators might be asked the question, ``How do you judge the fluency of this text?\" followed by a 5-point rating scale :\n1. Incomprehensible 2. Not fluent German 3. Non-native German 4. Good German 5. Flawless German.\nInstead of a 5-point scale, other scales with different quality ratings could be used: ``How natural is the English of the given sentence?'' 1. Very unnatural 2. Mostly unnatural 3. Mostly natural 4. Very natural . \nAnother possibility is to present multiple candidate sentences and ask the evaluator, ``Which of these sentences seems more fluent?\". The evaluator then indicates a preference ordering with ties allowed. \nFluency in the generated output is a desired criteria for all the NLG tasks. However, the comprehensive list of criteria used for evaluation varies across different tasks. Hence, we discuss the set of criteria for each task separately now. Note that we have already defined fluency and mentioned that it is important for all NLG tasks. Hence, we do not discuss it again for each task independently. Further, note that the set of criteria is not standardized and some works use slightly different criteria/ sub-categorizations for the same task. Often the difference is only in the label/term used for the criteria but the spirit of the evaluation remains the same. Thus, for the below discussion, we consider only the most prominently used criteria for each task. In the discussion below and the rest of the paper, we interchangeably refer to the output of an NLG system as the hypothesis. \n\\\\\\\\\n\\textbf{Machine Translation:} Here, bilingual experts are presented with the source sentence and the hypothesis. Alternatively, monolingual experts can be presented with the reference sentence and the hypothesis. For each output, they are usually asked to check two important criteria: fluency and adequacy of the hypothesis  as described below. \n\\begin{itemize}\n    \\item \\textbf{Adequacy:} The generated hypothesis should adequately represent all the information present in the reference. To judge adequacy a human evaluator can be asked the following question : How much of the meaning expressed in the reference translation is also expressed in the hypothesis translation? 1. None 2. Little 3. Much 4. Most 5. All\n\\end{itemize}\n~\\\\\n\\textbf{Abstractive Summarization:} Human evaluators are shown the candidate summary along with the source document and/or a set of references. The evaluators are typically asked to rate informativeness and coherence . \nAlternatively, in a more elaborate evaluation the evaluators are asked to check for fluency, informativeness, non-redundancy, referential clarity, and structure \\& coherence  as described below. \n\\begin{itemize}\n    \\item \\textbf{Informativeness:} The summary should convey the key points of the text. \n    For instance, a summary of a biography should contain the significant events of a person's life. We do not want a summary that only quotes the person's profession, nor do we want a summary that is unnecessarily long/verbose.\n    \\item \\textbf{Non-redundancy:} The summary should not repeat any points, and ideally have maximal information coverage within the limited text length.\n    \\item \\textbf{Referential clarity:} Any \\newA{intra-sentence or cross-sentence} references in the summary should be unambiguous and within the scope of the summary. \\newA{For example, if a pronoun is being used, the corresponding noun it refers to should also be present at some point before it in the summary. Also there should not be any ambiguities regarding the exact entity or information (such as a previous point) that is being referred to.} \n    \\item \\textbf{Focus:} The summary needs to have a focus and all the sentences need to contain information related to this focal point. For example, while summarising a news item about a Presidential debate, the focus of the summary could be the comments made by a candidate during the debate. If so, it should not contain irrelevant sentences about the venue of the debate.\n    \\item \\textbf{Structure and Coherence:} The summary should be a well-organized and coherent body of information, not just a dump of related information. Specifically, the sentences should be connected to one another, maintaining good information flow.\n\\end{itemize}\n~\\\\\n\\textbf{Question Answering:} Here, human evaluators are first presented with the question and the candidate answer to check if the answer is plausible . Subsequently, the context passage/image is provided to check whether the answer is correct and consistent with the context. Alternatively, since question answering datasets are usually provided with gold standard answers for each question, the judges might simply be asked to report how closely the candidate answer captures the same information as the gold standard answer. The important criteria used for QA are fluency and correctness. \n\\begin{itemize}\n    \\item \\textbf{Correctness}: The answer should correctly address the question and be consistent with the contents of the source/context provided.\n\\end{itemize}\n~\\\\\n\\textbf{Question Generation:} Here, the candidate questions are presented to the evaluators along with the context (passage/image, etc.) from which the questions were generated. This may be accompanied with a set of candidate answers , although if they are not provided, even when available in the dataset, it is to avoid creating any bias in the evaluator's mind . The evaluators are then asked to consider the following criteria:\n\\begin{itemize}\n    \\item \\textbf{Answerability}: This is to determine whether the generated question is answerable given the context. A question might be deemed unanswerable due to its lack of completeness or sensibility, or even if the information required to answer the question is not found in the context. The latter could be acceptable in some scenarios where ``insufficient information'' is a legitimate answer (for example, if the questions are used in a quiz to check if the participants are able to recognize a case of insufficient information). However, generating too many such questions is undesirable and the evaluators may be asked to report if that is the case. \n    \\item \\textbf{Relevance}: This is to check if questions are related to the source material they are based upon. Questions that are highly relevant to the context are favoured. For example, a question based on common-sense or universal-facts might be answerable, but if it has no connection to the source material then it is not desired.\n    \\end{itemize}\n~\\\\    \n\\textbf{Data to Text generation:} \nHere, human judges are shown the generated text along with the data (\\textit{i.e.}, table, graph, etc). The criteria considered during human evaluation vary slightly in different works, such as WebNLG challenge , E2E NLG dataset  or WikiBio dataset .  \nHere, we discuss the more fine-grained criteria of ``faithfulness'' and ``coverage'' as used in  as opposed to the single criteria of ``semantic adequacy'' as used in .\n\\begin{itemize}\n    \\item \\textbf{Faithfulness:} It is important for the text to preserve the facts represented in the data. For example, any text that misrepresents the year of birth of a person would be unacceptable and would also be ranked lower than a text that does not mention the year at all.\n    \\item \\textbf{Informativeness or Coverage:} The text needs to adequately verbalize the information present in the data. As per the task requirements, coverage of all the details or the most significant details would be desired.\n\\end{itemize}\n~\\\\\n\\textbf{Automated Dialogue:} For evaluating dialogue systems, humans are typically asked to consider a much broader set of criteria. One such exhaustive set of criteria as adopted by , is presented below along with the corresponding questions provided to the human evaluators:\n\\begin{itemize}\n    \\item \\textbf{Making sense:} Does the bot say things that don't make sense?\n    \\item \\textbf{Engagingness:} Is the dialogue agent enjoyable to talk to?\n    \\item \\textbf{Interestingness:} Did you find the bot interesting to talk to?\n    \\item \\textbf{Inquisitivenes:} Does the bot ask a good amount of questions?\n    \\item \\textbf{Listening:} Does the bot pay attention to what you say?\n    \\item \\textbf{Avoiding Repetition:} Does the bot repeat itself? (either within or across utterances)\n    \\item \\textbf{Humanness:} Is the conversation with a person or a bot?\n\\end{itemize}\nOften for dialogue evaluation, instead of separately evaluating all these factors, the evaluators are asked to simply rate the overall quality of the response , or specifically asked to check for relevance of the response . For task-oriented dialogues, additional constraints are taken into consideration, such as providing the appropriate information or service, guiding the conversation towards a desired end-goal, \\textit{etc}. In open-domain dialogue settings also, additional constraints such as persona adherence , emotion-consistency , \\textit{etc}, are being used to expand the expectations and challenge the state-of-the-art.\n~\\\\\\\\\n\\textbf{Image Captioning:} The captions are presented to the evaluators along with the corresponding images to check for relevance and thoroughness .\n\\begin{itemize}\n    \\item \\textbf{Relevance:} This measures how well the caption is connected to the contents of the image. More relevance corresponds to a less-generic/more-specific caption that accurately describes the image. For example, the caption ``A sunny day'' is a very generic caption and can be applicable for a wide variety of images.\n    \\item \\textbf{Thoroughness:} The caption needs to adequately describe the image. Usually the task does not require a complete description of everything in the image but the caption must cover the main subjects/actions in the image and not miss out any significant details.\n\\end{itemize}\n~\\\\\nIn summary, the main takeaway from the above section is that evaluating NLG systems is a very nuanced task requiring multiple skilled evaluators and accurate guidelines which clearly outline the criteria to be used for evaluation. Further, the evaluation is typically much more than assigning a single score to the system or the generated output. In particular, it requires simultaneous assessment of multiple desired qualities in the output.", "cites": [8755, 4225, 7454, 2001, 2364, 2025, 4228, 4223, 2372], "cite_extract_rate": 0.55, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a structured overview of evaluation criteria for various NLG tasks and references relevant papers to support its discussion. It integrates ideas across sources to highlight how fluency and task-specific metrics (like adequacy or non-redundancy) are applied in different contexts. While it offers some analysis, such as noting that criteria are not standardized and the importance of task-specific considerations, it does not deeply evaluate limitations or compare methods critically. The section identifies task-specific patterns but lacks broader meta-level generalization about evaluation practices in NLG."}}
{"id": "9bd56003-63b1-4b91-98c3-4725c7d18551", "title": "Taxonomy of Automated Evaluation Metrics", "level": "section", "subsections": [], "parent_id": "c120a577-e918-4f1d-894c-cc2b4adb602b", "prefix_titles": [["title", "A Survey of Evaluation Metrics Used for NLG Systems"], ["section", "Taxonomy of Automated Evaluation Metrics"]], "content": "\\label{sec:taxonomy}\nSo far, we have discussed the criteria used by humans for evaluating NLG systems. However, as established earlier, procuring such ratings on a large scale every time a new system is proposed or modified is expensive, tedious and time consuming. Hence, automatic evaluation metrics have become popular. Over the years, many automatic metrics have been proposed, some task-specific and some task-agnostic. Before describing these metrics, we first present a taxonomy of these metrics. To do so, we introduce some notation to refer to the context (or input), reference (or ground-truth) and the hypothesis (or the generated output) which is to be evaluated. \nThe context varies from one task to another and could be a document, passage, image, graph, \\textit{etc}. Additionally, the expected output text is referred to by a specific term in relation to the context. For example, in the case of translation, the context is the source language sentence which is to be translated. The expected output is referred to as the ``translation'' of the source sentence into the target language. We list the various inputs and outputs for each of the NLG tasks in table \\ref{tab:cont_and_ref_forms}. \n\\begin{table}[h]\n\\centering\n    \\begin{tabular}{l|l|l}\n         NLG task & Context & Reference and Hypothesis \\\\\n         \\hline\n         Machine Translation (MT) & Source language sentence & Translation\\\\\n         Abstractive Summarization (AS) & Document & Summary\\\\\n         Question Answering (QA) & Question + Background info (Passage, Image, etc) & Answer\\\\\n         Question Generation (QG) & Passage, Knowledge base, Image & Question\\\\\n         Dialogue Generation (DG) & Conversation history & Response\\\\\n         Image captioning (IC) & Image & Caption\\\\\n         Data to Text (D2T) & Semi-structured data (Tables) & Description\\\\\n         \\hline\n    \\end{tabular}\n    \\caption{Context and reference/hypothesis forms for each NLG task}\n    \\label{tab:cont_and_ref_forms}\n\\end{table}\nIn the following sections discussing the existing automatic metrics, we use the generic terms, context, reference, and hypothesis denoted by $c$, $r$, and $p$ respectively. \nThe reference and hypothesis would be a sequence of words and we denote the lengths of these sequences as $|r|$ and $|p|$ respectively.\nIn case there are multiple reference sentences available for one context, we represent the set of references as $R$. Text-based contexts (sentences, documents, passages) also contain a sequence of words and we refer to the length of this sequence as $|c|$. In the case of `conversation history', there could be additional delimiters to mark the end of an utterance, and distinguish between the speakers involved in the conversation. Images are represented as matrices or multidimensional arrays. Tables are expressed as a set of records or tuples of the form, \\textit{(entity, attribute, value)}. The notations for any such special/additional elements are introduced as and when required.\\\\\nGiven the above definitions, we classify the existing metrics using the taxonomy summarized in Figure \\ref{fig:taxo}. We start with 2 broad categories: (i) Context-free metrics and (ii) Context-dependent metrics. Context-free metrics do not consider the context while judging the appropriateness of the hypothesis. In other words, they only check the similarity between the hypothesis and the given set of references. This makes them task-agnostic and easier to adopt for a wider variety of NLG tasks (as irrespective of the task, the reference and hypothesis would just be a sequence of words that need to be compared). Table \\ref{tab:auto_metric_adoption} depicts the NLG tasks for which each of the automatic metrics were proposed and/or adopted for.  On the other hand, context-dependent metrics also consider the context while judging the appropriateness of the hypothesis. They are typically proposed for a specific task and adopting them for other tasks would require some tweaks. For example, a context-dependent metric proposed for MT would take the source sentence as input and hence it cannot directly be adopted for the task of image captioning or data-to-text generation where the source would be an image or a table. We thus categorize context-dependent metrics based on the original tasks for which they were proposed. \nWe further classify the metrics based on the techniques they use. For example, some metrics are trained using human annotation data whereas some other metrics do not require any training and simply use a fixed set of heuristics. The untrained metrics can be further classified based on whether they operate on words, characters, or word embeddings. Similarly, the trained metrics could use other metrics/heuristics as the input features or be trained in an end-to-end fashion using the representations of the reference, hypothesis, and context. For learning the parameters of a trained metric, various machine learning techniques such as linear regression, SVMs, deep neural networks, \\textit{etc.}, can be used. This trained/untrained categorization is applicable to both the context-free and context-dependent metrics. However, we find that currently most of the context-dependent metrics are trained, with only a handful of untrained metrics. \nWith this taxonomy \nwe discuss the various context-free and context-dependent metrics in the next 2 sections.\n    \\centering\n    \\begin{tabular}{c|c|c|c|c|c|c|c}\n         Metric & MT & AS & DG & IC  & QA & D2T & QG\\\\\n         \\hline\n         \\multicolumn{8}{c}{Context-free metrics}\\\\\n         \\hline\n         BLEU & \\checkmark & * & * & * & * & * & *\\\\\n         NIST & \\checkmark & * & * & * & * & *& *\\\\\n         METEOR & \\checkmark & * & * & * & * & *& *\\\\\n         ROUGE & * & \\checkmark & * & * & * & *& *\\\\\n         GTM & \\checkmark & * &  &  & * & &\\\\\n         CIDEr &  &  &  & \\checkmark & & &\\\\\n         SPICE &  &  &  & \\checkmark &  & &\\\\\n         SPIDer &  &  &  & \\checkmark & & &\\\\\n         WER-family & \\checkmark &  &  &  &  & &\\\\\n         chrF & \\checkmark & * &  & * &  & &\\\\\n         Vector Extrema & * & * & * & * & * & *& \\\\\n         Vector Averaging & * & * & * & * & * & *& \\\\\n         WMD & * & * &  & * &  & &\\\\\n         BERTr & * &  &  &  &  & &\\\\\n         BERTscore & \\checkmark &  & * & \\checkmark & * & &\\\\\n         MoverScore & \\checkmark & \\checkmark &  & \\checkmark &  & \\checkmark &\\\\\n         BEER & \\checkmark &  &  &  &  & & \\\\\n         BLEND & \\checkmark &  &  &  &  &  &\\\\\n         Q-metrics &  &  &  &  &  & & \\checkmark\\\\\n         Composite metrics &  &  &  & \\checkmark & & &\\\\\n         SIMILE & \\checkmark &  &  &  &  &  &\\\\\n         ESIM & \\checkmark &  &  &  &  &  &\\\\\n         RUSE & \\checkmark &  &  &  &  &  &\\\\\n         BERT for MTE & \\checkmark &  &  &  &  &  &\\\\\n         BLEURT & \\checkmark &  &  &  &  & \\checkmark &\\\\\n         NUBIA & \\checkmark &  &  & \\checkmark &  & & \\\\\n         \\hline\n         \\multicolumn{8}{c}{Context-dependent metrics}\\\\\n         \\hline\n         ROUGE-C &  & \\checkmark &  & &  & & \\\\\n         PARENT &  &  &  & &  & \\checkmark & \\\\\n         LEIC &  &  &  & \\checkmark &  & &\\\\\n         ADEM &  &  & \\checkmark &  &  & &\\\\\n         RUBER &  &  & \\checkmark &  &  & &\\\\\n         SSREM &  &  & \\checkmark &  &  & &\\\\\n         RUBER with BERT embeddings &  &  & \\checkmark &  &  & &\\\\\n         MaUde &  &  & \\checkmark &  &  & &\\\\\n         RoBERTa-eval &  &  & \\checkmark &  &  & &\\\\\n         \\hline\n    \\end{tabular}\n    \\caption{Automatic metrics proposed (\\checkmark) and adopted (*) for various NLG tasks }\n    \\label{tab:auto_metric_adoption}\n\\end{table}\n\\definecolor{blue1}{rgb}{0.0, 0.40, 0.95}\n\\tikzstyle{abstract}=[rectangle, draw=black, rounded corners, fill=white, drop shadow,\n        text centered, anchor=north, text=black, text width=3cm]\n\\tikzstyle{comment}=[rectangle, draw=black, rounded corners, fill=white, drop shadow,\n        text centered, anchor=north, text=black, text width=3cm]\n\\tikzstyle{myarrow}=[->, >=open triangle 90, thick]\n\\tikzstyle{line}=[-, thick]\n\\pagebreak\n\\begin{center}\n    \\begin{sideways}\n         \\begin{minipage}{1.3\\linewidth}\n                   \\vspace*{\\fill}\n                \\resizebox{1\\textwidth}{!}{   \n\\begin{tikzpicture}[node distance=1.8cm]\n    \\node (Root) [abstract, text width=7cm]\n        {\n            \\textbf{Automatic Evaluation Metrics}\n        };\n    \\node (AuxNode01) [text width=4cm, below=of Root] {};\n    \\node (Context_Free) [abstract, text width=5cm, rectangle split, rectangle split parts=2, below=of AuxNode01, xshift=-9cm]\n        {\n            \\textbf{Context Free Metrics}\n            \\nodepart{second}(mostly task agnostic)\n        };\n    \\node (Context_Dependent) [abstract, text width=5cm, rectangle split, rectangle split parts=2, below=of AuxNode01, xshift=+9cm]\n        {\n            \\textbf{Context Dependent Metrics}\n            \\nodepart{second}(mostly task specific)\n        };\n    \\node (AuxNode02) [text width=0.5cm, below=of Context_Free] {};\n    \\node (Untrained) [abstract, below=of AuxNode02, xshift=-5.5cm]\n        {\n            \\textbf{Untrained}\n        };\n    \\node (Trained) [abstract, below=of AuxNode02, xshift=+5.5cm]\n        {\n            \\textbf{Trained}\n        };\n    \\node (AuxNode03) [below=of Untrained] {};\n    \\node (Word) [abstract, text width=3.2cm, rectangle split, rectangle split parts=4, below=of Untrained, xshift=-4cm]\n        {\n            \\textbf{Word Based}\n            \\nodepart{second}{\\textcolor{blue1}{\\textbf{N-gram}}\\\\\n            BLEU \\\\NIST  \\\\GTM  \\\\METEOR  \\\\ROUGE  \\\\\\textcolor{red}{CIDEr} \n            \n            }\n            \\nodepart{third}{\n            \\textcolor{blue1}{\\textbf{Edit Distance}}\\\\ \n            WER  \\\\MultiWER \\\\TER  \\\\ITER \\\\CDER  }\n            \\nodepart{fourth}{\n            \\textcolor{blue1}{\\textbf{Others}}\\\\\n            \\textcolor{red}{SPICE} \\\\\n            \\textcolor{red}{SPIDEr} \n            }\n        };\n    \\node (Character) [abstract, text width=3.2cm, rectangle split, rectangle split parts=3, below=of Untrained, xshift=0cm]\n        {\n            \\textbf{Character Based}\n            \\nodepart{second}{\n            \\textcolor{blue1}{\\textbf{N-gram}}\\\\\n            chrF }\n            \\nodepart{third}{\n            \\textcolor{blue1}{\\textbf{Edit Distance}}\\\\ \n            charactTER \\\\\n            EED }\n        };\n    \\node (Embedding) [abstract, text width=3.2cm, rectangle split, rectangle split parts=3, below=of Untrained, xshift=4cm]\n        {   \n            \\textbf{Embedding Based}\n            \\nodepart{second}{\n            \\textcolor{blue1}{\\textbf{Static Embedding}}\n            Greedy Matching \\\\\n            Embedding Average \\\\\n            Vector Extrema \\\\\n             WMD \\\\\n             WEWPI \\\\\n            MEANT }\n            \\nodepart{third}{\\textcolor{blue1}{\\textbf{Contextualised Embedding}}\\\\\n            YiSi \\\\\n            MoverScore \\\\\n            BERTr \\\\\n            BertScore }\n        };\n    \\node (AuxNode04) [below=of Trained] {};\n    \\node (Feature_based) [abstract, rectangle split, rectangle split parts=2, below=of Trained, xshift=-2cm, text width=3.2cm]\n        {\n            \\textbf{Feature Based}\n            \\nodepart{second}{BEER \n            \\\\ BLEND \n            \\\\ Composite \n            \\\\ NNEval  \n            \\\\ \\textcolor{blue}{Q-metrics} }\n        };\n    \\node (Hypothesis_based) [abstract, rectangle split, rectangle split parts=3, below=of Trained, xshift=2cm]\n    {\n        \\textbf{End-to-End}\n        \\nodepart{second}{\n            SIMILE  \\\\\n            ESIM  \\\\\n            RUSE }\n        \\nodepart{third}{\n            \\textcolor{blue1}{\\textbf{Transformer-based}}\n            BERT for MTE  \\\\\n            BLEURT  \\\\\n            NUBIA }\n    };\n    \\node (AuxNode05) [below=of Context_Dependent] {};\n        \\node (Untrained1) [abstract, below=of AuxNode05, xshift=-3.5cm]\n        {\n            \\textbf{Untrained}\n        };\n    \\node (Trained1) [abstract, below=of AuxNode05, xshift=+3.5cm]\n        {\n            \\textbf{Trained}\n        };\n    \\node (AuxNode06) [below=of Untrained1] {};\n    \\node (Word1) [abstract, text width=3.2cm, rectangle split, rectangle split parts=3, below=of Untrained1, xshift=-2.2cm]\n        {\n            \\textbf{Word Based} \\\\\n            \\nodepart{second}{\n                \\textcolor{blue1}{\\textbf{N-gram}}\\\\\n                \\textcolor{cyan}{ROUGE-C} \\\\\n                \\textcolor{olive}{PARENT} \n            }\n            \\nodepart{third}{\\textcolor{blue1}{\\textbf{Other}}\\\\\n            \\textcolor{brown}{XMEANT} }\n        };\n    \\node (Embedding1) [abstract, text width=3.2cm, rectangle split, rectangle split parts=2, below=of Untrained1, xshift=2.2cm]\n        {   \n            \\textbf{Embedding Based}\n            \\nodepart{second}{\n            \\textcolor{blue1}{\\textbf{Contextualised Embedding}}\\\\\n            \\textcolor{brown}{YiSi-2} \n            }\n        };\n    \\node (AuxNode07) [below=of Trained1] {};\n    \\node (Hypothesis_based1) [abstract, rectangle split, rectangle split parts=3, below=of Trained1, xshift=0cm, text width=3.2cm]\n    {\n        \\textbf{End-to-End}\n        \\nodepart{second}{\n              \\textcolor{red}{LEIC} \\\\\n              \\textcolor{orange}{ADEM}  \\\\\n               \\textcolor{orange}{RUBER}  \\\\\n               \\textcolor{orange}{GAN discriminator}  \\\\\n               \\textcolor{orange}{CMADE}  \\\\\n               \\textcolor{orange}{SSREM} \n            }\n        \\nodepart{third}{\n            \\textcolor{blue1}{\\textbf{Transformer-based}}\\\\\n        \\textcolor{orange}{\n        \\textcolor{orange}{RUBER + BERT}  \\\\\n        \\textcolor{orange}{MaUde} \\\\\n        \\textcolor{orange}{ROBERTa-evaluator} \n        }}\n    };\n    \\node (Legend) [abstract, rectangle split, rectangle split parts=1, below=5cm of Embedding1, xshift=0cm, text width=4.5cm]\n    {\n        \\textbf{Legend}\\\\\n        Task Agnostic\\\\\n        \\textcolor{brown}{Machine Translation}\\\\\n        \\textcolor{orange}{Dialogue Generation}\\\\\n        \\textcolor{cyan}{Automatic Summarization}\\\\\n        \\textcolor{red}{Image Captioning}\\\\\n        \\textcolor{blue}{Question Generation}\\\\\n        \\textcolor{magenta}{Question Answering}\\\\\n        \\textcolor{olive}{Data-to-Text Generation}\n    };\n    \\draw[myarrow] (Root.south) -- ++(0,-0.8) -| (Context_Free.north);\n    \\draw[myarrow] (Root.south) -- ++(0,-0.8) -| (Context_Dependent.north);\n    \\draw[myarrow] (Context_Free.south) -- ++(0,-0.8) -| (Untrained.north);\n    \\draw[myarrow] (Context_Free.south) -- ++(0,-0.8) -| (Trained.north);\n    \\draw[myarrow] (Untrained.south) -- ++(0,-0.8) -| (Word.north);\n    \\draw[myarrow] (Untrained.south) -- ++(0,-0.8) -| (Character.north);\n    \\draw[myarrow] (Untrained.south) -- ++(0,-0.8) -| (Embedding.north);\n    \\draw[myarrow] (Trained.south) -- ++(0,-0.8) -| (Feature_based.north);\n    \\draw[myarrow] (Trained.south) -- ++(0,-0.8) -| (Hypothesis_based.north);\n    \\draw[myarrow] (Context_Dependent.south) -- ++(0,-0.8) -| (Untrained1.north);\n    \\draw[myarrow] (Context_Dependent.south) -- ++(0,-0.8) -| (Trained1.north);\n    \\draw[myarrow] (Untrained1.south) -- ++(0,-0.8) -| (Word1.north);\n    \\draw[myarrow] (Untrained1.south) -- ++(0,-0.8) -| (Embedding1.north);\n    \\draw[myarrow] (Trained1.south) -- ++(0,-0.8) -| (Hypothesis_based1.north);\n\\end{tikzpicture}\n}\n \\captionof{figure}{Taxonomy of Automatic Evaluation Metrics}\n         \\label{fig:taxo}\n         \\end{minipage}\n    \\end{sideways}\n    \\end{center}\n\\begin{table}[h]\n    \\centering\n    \\begin{tabular}{c|c|c|c|c|c|c|c}\n         Metric & MT & AS & DG & IC  & QA & D2T & QG\\\\\n         \\hline\n         \\multicolumn{8}{c}{Context-free metrics}\\\\\n         \\hline\n         BLEU & \\checkmark & * & * & * & * & * & *\\\\\n         NIST & \\checkmark & * & * & * & * & *& *\\\\\n         METEOR & \\checkmark & * & * & * & * & *& *\\\\\n         ROUGE & * & \\checkmark & * & * & * & *& *\\\\\n         GTM & \\checkmark & * &  &  & * & &\\\\\n         CIDEr &  &  &  & \\checkmark & & &\\\\\n         SPICE &  &  &  & \\checkmark &  & &\\\\\n         SPIDer &  &  &  & \\checkmark & & &\\\\\n         WER-family & \\checkmark &  &  &  &  & &\\\\\n         chrF & \\checkmark & * &  & * &  & &\\\\\n         Vector Extrema & * & * & * & * & * & *& \\\\\n         Vector Averaging & * & * & * & * & * & *& \\\\\n         WMD & * & * &  & * &  & &\\\\\n         BERTr & * &  &  &  &  & &\\\\\n         BERTscore & \\checkmark &  & * & \\checkmark & * & &\\\\\n         MoverScore & \\checkmark & \\checkmark &  & \\checkmark &  & \\checkmark &\\\\\n         BEER & \\checkmark &  &  &  &  & & \\\\\n         BLEND & \\checkmark &  &  &  &  &  &\\\\\n         Q-metrics &  &  &  &  &  & & \\checkmark\\\\\n         Composite metrics &  &  &  & \\checkmark & & &\\\\\n         SIMILE & \\checkmark &  &  &  &  &  &\\\\\n         ESIM & \\checkmark &  &  &  &  &  &\\\\\n         RUSE & \\checkmark &  &  &  &  &  &\\\\\n         BERT for MTE & \\checkmark &  &  &  &  &  &\\\\\n         BLEURT & \\checkmark &  &  &  &  & \\checkmark &\\\\\n         NUBIA & \\checkmark &  &  & \\checkmark &  & & \\\\\n         \\hline\n         \\multicolumn{8}{c}{Context-dependent metrics}\\\\\n         \\hline\n         ROUGE-C &  & \\checkmark &  & &  & & \\\\\n         PARENT &  &  &  & &  & \\checkmark & \\\\\n         LEIC &  &  &  & \\checkmark &  & &\\\\\n         ADEM &  &  & \\checkmark &  &  & &\\\\\n         RUBER &  &  & \\checkmark &  &  & &\\\\\n         SSREM &  &  & \\checkmark &  &  & &\\\\\n         RUBER with BERT embeddings &  &  & \\checkmark &  &  & &\\\\\n         MaUde &  &  & \\checkmark &  &  & &\\\\\n         RoBERTa-eval &  &  & \\checkmark &  &  & &\\\\\n         \\hline\n    \\end{tabular}\n    \\caption{Automatic metrics proposed (\\checkmark) and adopted (*) for various NLG tasks }\n    \\label{tab:auto_metric_adoption}\n\\end{table}\n\\if 0\n\\mbox{} \n\\begin{center}\n    \\begin{sideways}\n         \\begin{minipage}{1.2\\linewidth}\n                   \\includegraphics[width=1.2\\linewidth,keepaspectratio]{automatic_taxonomy.png}\n                    \\captionof{figure}{Taxonomy of Automatic Evaluation Metrics}\n         \\label{fig:taxo}\n         \\end{minipage}\n    \\end{sideways}\n\\end{center}\n\\fi", "cites": [4229, 7454, 2372, 7793, 2361, 4227, 2025, 4228, 4226, 2027, 2032, 4230, 2030, 7794, 4223], "cite_extract_rate": 0.3469387755102041, "origin_cites_number": 49, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes insights from multiple papers to establish a taxonomy of NLG evaluation metrics, distinguishing between context-free and context-dependent categories. It abstracts broader patterns by introducing a consistent notation and general framework for metric classification. While it provides a clear structure and identifies the general limitations of context-free metrics, it could offer more critical depth in comparing the efficacy of specific approaches."}}
{"id": "deac9b0f-75b0-40c4-b10b-c277e6883498", "title": "Word-based metrics", "level": "subsubsection", "subsections": [], "parent_id": "6555d406-d246-43f7-9325-0b6908aa1ff6", "prefix_titles": [["title", "A Survey of Evaluation Metrics Used for NLG Systems"], ["section", "Context-Free metrics"], ["subsection", "Untrained metrics"], ["subsubsection", "Word-based metrics"]], "content": "} Word-based metrics typically treat the hypothesis and the reference as a bag of words or $n$-grams ($n$ contiguous words). They then assign a score to the hypothesis based on the word or $n$-gram overlap between the hypothesis and the reference. Alternatively, some other metrics assign a score to the hypothesis based on the number of word edits required to make the hypothesis similar to the reference. Most of the early evaluation metrics such as BLEU, NIST, METEOR, etc. are all word-based metrics. Given their simplicity and ease of use, these metrics have been widely adopted for many NLG tasks. \n\\\\\n\\\\\n\\textbf{BLEU} (Bilingual Evaluation Understudy ): This was the among the first and most popular metrics proposed for automatic evaluation of MT systems. \nIt is a precision-based metric that computes the $n$-gram overlap between the reference and the hypothesis. In particular, BLUE is the ratio of the number of overlapping $n$-grams to the total number of $n$-grams in the hypothesis. To be precise, the numerator contains the sum of the overlapping $n$-grams across all the hypotheses (\\textit{i.e.}, all the test instances) and the denominator contains the sum of the total $n$-grams across all the hypotheses (\\textit{i.e.}, all the test instances). This precision is computed separately for different values of $n$ as shown below. \n\\begin{align*}\n    precision_n  = \\frac{\\sum\\limits_{p \\in \\text{hypotheses}} \\sum\\limits_{\\text{$n$-gram} \\in p} Count_{clip}(\\text{$n$-gram})}{\\sum\\limits_{p \\in \\text{hypotheses}} \\sum\\limits_{\\text{$n$-gram} \\in p} Count(\\text{$n$-gram})}\n\\end{align*}\nwhere $Count_{clip}(\\text{$n$-gram})$ is clipped by the maximum number of times the given $n$-gram appears in any one of the corresponding reference sentences. \\newA{For example, if a particular $n$-gram appears thrice in the hypothesis, but twice in one reference and once in another reference in a multireference setting, then we want to consider the matched $n$-gram count as 2 and not as 3.} More precisely,\n    \\begin{equation*}\n    Count_{clip}(\\text{$n$-gram}) = \\min\\Big(\\text{matched $n$-gram count, } \\max_{r \\in R} (\\text{$n$-gram count in r} )\\Big)\n    \\end{equation*}\nNote that we refer to an $n$-gram in the hypothesis which overlaps with an $n$-gram in the reference as a matched $n$-gram.\n\\MK{the above formula is not clear to me. Let's discuss. I think we need to distinguish between what happens when there is a single reference v/s what happens when there are multiple references. \\Ana{Does the above example help?}}    \nOnce the above precision is computed for different values of $n$, a final $BLEU$-$N$ score is computed as a weighted combination of all the $precision_n$ scores, $ n = 1,..,N$. In the original paper, $BLEU$-$N$  was computed as the geometric mean of all the $precision_n$ scores, $ n = 1,..,N$.\nSince precision depends only on the length of the hypothesis and not on the length of the sentence, an NLG system can exploit the metric and acquire high scores by producing only a few matching or common words/$n$-grams as the hypothesis. To discourage such short meaningless hypothesis, a brevity penalty term, BP, is added to the formula:\n\\begin{align*}\n    BP = \n    \\begin{cases}\n     1, & \\text{if } |p|>|r| \\\\\n     e^{\\big(1-\\frac{|r|}{|p|}\\big)} & \\text{otherwise}\n    \\end{cases}\n\\end{align*}\nThe final formula popularly used today is \n\\begin{align*}\n    BLEU\\text{-}N= BP\\cdot exp\\bigg(\\sum_{n=1}^N W_n\\log precision_n \\bigg)\n\\end{align*}\nwhere $W_n$ are the weights of the different $n$-gram precisions, such that $\\sum_{n=1}^N W_n = 1$. (Usually each $W_n$ is set to $\\frac{1}{N}$.)\\\\\nSince each $precision_n$ is summed over all the hypotheses, BLEU is called a corpus-level metric, \\textit{i.e.}, BLEU gives a score over the entire corpus (as opposed to scoring individual sentences and then taking an average). Over the years, several variants of BLEU have been proposed. \\textbf{SentBLEU} is a smoothed version of BLEU that has been shown to correlate better with human judgements at the \\newA{sentence-level}. \\MK{what does segment mean? sentence? \\Ana{Yeah, I've changed it. SentBLEU is sometimes expanded as sentenceBLEU and is used to report sentence-level scores. However, in WMT task, they distinguish `system-level' from `segment-level', rather than sentence-level. I'm not entirely sure why. Update: In some of the older papers, I found this: ` segments  should  be no  less  than  one  sentence  in length'.}} Recently, there was a push for standardizing BLEU  by fixing the tokenization and normalization scheme to the one used by the annual Conference on Machine Translation (WMT). This standardized version is referred to as \\textbf{sacreBLEU}. \nDiscriminative BLEU or \\textbf{$\\Delta$-BLEU}  uses human annotations on a scale [-1,+1] to add weights to multireference BLEU. The aim is to reward the $n$-gram matches between the hypothesis and the good references, and penalize the $n$-grams that only match with the low-rated references. Thus, each $n$-gram is weighted by the highest scoring reference in which it occurs and this weight can sometimes be negative.\\\\\n\\\\\n\\textbf{NIST\\footnote{The name NIST comes from the organization, ``US National Institute of Standards and Technology\".}} : This metric can be thought of as a variant of BLEU which weighs each matched $n$-gram based on its information gain. The information gain for an $n$-gram made up of words $w_1,..,w_n$, is computed \\newA{over the set of reference  translations,} as\n\\begin{align*}\n    Info(\\text{$n$-gram}) = Info(w_1, ..., w_n) = \\log_{2} \\frac{\\text{\\# of occurrences of } w_1,...,w_{n-1}}{\\text{\\# of occurrences of } w_1,...,w_n }\n\\end{align*}\n\\MK{in the denominator do we consider only references sentences or all sentences in the corpus? What about numerator? \\Ana{There are not many details given about this. As I understand, yes both numerator and denominator are computed only over (all the) reference sentences (esp since it came in the context of translation, I don't think they had other options). Anyway, I have added this information in the text rather than the formula now, almost exactly as presented in the paper. I now removed ``reference sentences\" from the denominator as in the original equation. }}\nThe idea is to give more credit if a matched $n$-gram is rare and less credit if a matched $n$-gram is common. This also reduces the chance of gaming the metric by producing trivial $n$-grams. \\newA{The authors further forgo the use of geometric mean to combine the different $precision_n$ scores which makes the contribution of $n$-grams of different length difficult to interpret.}\n\\MK{the portion marked in square brackets is not very clear to me. Please word it better. Use more sentences to explain it, if required. \\Ana{Yeah, this was one part that wasn't clear to me in the paper. I've rephrased it above in a slightly different way. The original point I had written is commented out below. For the exact words by the authors, see section 4, 1st point here: https://dl.acm.org/doi/pdf/10.5555/1289189.1289273}}\nIn addition to these changes, NIST also modifies the brevity penalty term in order to reduce the impact of small variations in hypothesis length $p$ on the score. To easily compare all these changes in NIST (as a variant of BLEU), note that BLEU formula can be written as follows by expanding the penalty term:\n\\begin{align*}\n    BLEU\\text{-}N= \\exp\\bigg(\\sum_{n=1}^N W_n\\log precision_n \\bigg) \\cdot \\exp \\bigg(\\min\\Big(1-\\frac{|r|}{|p|},0\\Big)\\bigg)\n\\end{align*}\n\\begin{align*}\n    NIST = \\sum_{n=1}^N \\bigg\\{\\frac{\\sum_{\\text{all $n$-grams that match}}Info(\\text{$n$-gram})}{\\sum_{n\\text{-}gram \\in hypotheses}(1)} \\bigg\\} \\cdot \\exp \\bigg(\\beta \\log^2 \\Big[\\min\\Big(\\frac{|p|}{|\\bar{r}|},1\\Big) \\Big] \\bigg)\n\\end{align*}\nwhere $\\beta $ is chosen to make brevity penalty factor = $0.5$ when the number of words in the hypothesis is $2/3^{rds}$ of the average number of words in the reference, and $|\\bar{r}|$ is the average number of words in a reference (averaged over all the references).\\\\\n\\MK{Is the above formula taken as it is from the original paper? If not, is there a better way of writing it? \\Ana{The formula was taken from the original paper.} In the numerator, by cooccur do you mean match? \\Ana{Yes, I've replaced it now.} Shouldn't we be dividing by $N$ to take average? \\Ana{This is a good catch. Sorry, I did not write that detail correctly in text last time. While talking about discarding the geometric mean, the authors say ``an alternative would be to use an arithmetic average of N-gram counts rather than a geometric average\" (over the precisions) -> is missing. } Unlike the BLEU-N formula, here it is not clear that we are using precision\\_n for the computation. Can we write this formula in a way that it is easy to compare with the BLEU formula?\\Ana{Does the above presentation work? Otherwise I think we formulate it as info-weighted $precision_n$ terms in the numerator divided by total number of $n$-grams.}}\n\\\\\n\\textbf{GTM} (General Text Matcher) :   observe that systems can game a metric by increasing the precision or recall individually even through bad generations. The authors hence suggest that a good metric should use a combination of precision and recall such, as F-measure (which is the harmonic mean of precision and recall).\n\\newA{Towards this end they propose `GTM', an F-Score based metric, with greater weights for contiguous word sequences matched between the hypothesis and reference. A `\\textit{matching}' is defined as a mapping of words between the hypothesis and the reference, based on their surface-forms, such that no two words of the hypothesis are mapped to the same word in the reference and vice versa. In order to assign higher weights to contiguous matching sequences termed ``runs\", weights are computed for each run as the square of the run length. Note that length of a run could also be $1$ for an isolated word match, more generally it is bound to be between $0$ and $\\min(|p|,|r|)$. The hypothesis and reference could have multiple possible matchings with different number of runs of various lengths. The size of a matching, \\textit{i.e., match size} of $M$ is computed using the weights of its constituent runs as follows:\n\\begin{align*}\n    size(M) = \\mysqrt{-1}{4}{q}{\\sum_{run \\in M}length(run)^q}\n\\end{align*}\nwhere higher values of $q$ more heavily weight longer runs. By comparing the match sizes, a matching with the maximum match size (MMS) is selected. In practice, since finding the MMS is NP-hard for $q>1$, GTM uses a greedy approximation where the largest non-conflicting mapped sequences are added iteratively to form the matching (and use its size as MMS). \nUsing the approximated MMS, the precision and recall are computed as:}\n\\begin{gather*}\n    \\text{(Precision) }P  = \\frac{MMS(p,r)}{|p|}\\text{ , (Recall) } R = \\frac{MMS(p,r)}{|r|} \n    \\\\\n    \\text{GTM = F-score} = \\frac{2PR}{P+R}\n\\end{gather*}\n\\MK{where are we using size(M) in the above formula? What is M (it is not defined anywhere)? It is also not clear how we are using the weights in MMS(p,r) \\Ana{I've rewritten most of the content for this metric so that the idea becomes clear now.}}\nGTM was proposed for evaluating MT systems and showed higher correlations with human judgements compared to BLEU and NIST (with $q =1$). \\\\%, and is extended to summarization and data-to-text generation tasks.\\\\ \n\\\\\n\\textbf{METEOR} (Metric for Evaluation of Translation with Explicit ORdering) :  point out that there are two major drawbacks of BLEU: (i) it does not take recall into account and (ii) it only allows exact $n$-gram matching. To overcome these drawbacks, they proposed METEOR which is based on F-measure and uses a relaxed matching criteria. In particular, even if a unigram in the hypothesis does not have an exact surface level match with a unigram in the reference but is still equivalent to it (say, is a synonym) then METEOR considers this as a matched unigram. More specifically, it first performs exact word (unigram) mapping, followed by stemmed-word matching, and finally synonym and paraphrase matching. It then computes the F-score using this relaxed matching strategy.\\\\\n\\MK{throughout the paper please write all equations between begin\\{align*\\} as opposed to using \\$\\$}\n\\begin{gather*}\n    P(Precision)=\\frac{\\# mapped\\_unigrams}{\\#unigrams\\_in\\_candidate} \\text{ , } R(Recall)=\\frac{\\# mapped\\_unigrams}{\\#unigrams\\_in\\_reference} \n    \\\\\n    F score = \\frac{10PR}{R + 9P}\n\\end{gather*}\nSince METEOR only considers unigram matches (as opposed to $n$-gram matches), it seeks to reward longer contiguous matches using a penalty term \\newA{ known as `fragmentation penalty'. To compute this, `chunks' of matches are identified in the hypothesis, where contiguous hypothesis unigrams that are mapped to contiguous unigrams in a reference can be grouped together into one chunk. Therefore longer $n$-gram matches lead to fewer number of chunks, and the limiting case of one chunk occurs if there is a complete match between the hypothesis and reference. On the other hand, if there are no bigram or longer matches, the number of chunks will be the same as the number of unigrams. The fewest possible number of chunks a hypothesis can have is used to compute the fragmentation penalty used in METEOR as:} \n\\begin{align*}\n    \\text{Penalty} = 0.5*\\Bigg[\\frac{\\#chunks}{\\#unigrams\\_matched}\\Bigg]^3 \\\\\n    \\text{METEOR Score} = F score*(1-Penalty)\n\\end{align*}\n\\MK{what is \\# chunks in the above equation \\Ana{Added above}}    \nSimilar to BLEU, METEOR also has a few variants. \\newA{ For example,  propose \\textbf{METEOR-NEXT} to compute weighted precision and recall by assigning weights to the different matching conditions or the \\textit{`matchers'} used (\\textit{viz.}, exact, stem, synonym and paraphrase matching):\n\\begin{align*}\n     P=\\frac{\\sum_{i \\in |\\{matchers\\}|}w_i.m_i(p)}{|p|} \\text{ , } R=\\frac{\\sum_{i \\in |\\{matchers\\}| }w_i.m_i(r)}{|r|} \n\\end{align*}\nwhere $m_i(p)$ and $m_i(r)$ represent the counts of the mapped words identified by that particular matcher $m_i$ in the hypothesis and reference respectively, and $w_i$ is the corresponding weight. The parameterized F-score is calculated as\n\\begin{align*}\n    F score = \\frac{PR}{\\alpha.P+(1-\\alpha).R}\n\\end{align*}\nFurther building on this variant,  observe that METEOR uses language specific resources (for stemming and matching synonyms) and propose \\textbf{METEOR Universal} that generalizes across languages by automatically building function-word lists and paraphrase lists using parallel text in different languages. With these lists, they define weighted precision and recall similar to METEOR-NEXT that additionally has the flexibility to weigh the content words and function words differently:\n\\begin{align*}\n    P=\\frac{\\sum_{i}w_i.(\\delta . m_i(p_c)+ (1-\\delta).m_i(p_f) }{\\delta.|p_c| + (1-\\delta).|p_f|} \\text{ , } R=\\frac{\\sum_{i}w_i.(\\delta . m_i(r_c)+ (1-\\delta).m_i(r_f)}{\\delta.|r_c| + (1-\\delta).|r_f|} \n\\end{align*}\nwhere $p_c$ and $r_c$ denote the content words in hypothesis and reference, while $p_f$ and $r_f$ represent the function words and $\\delta, w_i^s$ are parameters. In order to have a language-agnostic formula, all the parameters are tuned to encode general human preferences that were empirically observed to be common across languages, such as, preferring recall over precision, word choice over word order, correct translation of content words over function words, etc.}\n\\MK{what are these parameters? the above equations do not contain any parameters which can be tuned. \\Ana{Added above}}   \\textbf{METEOR++ } additionally incorporates ``copy-words\" specially into the metric, to deal with the words that have a high-probability of remaining the same throughout all paraphrases of a sentence. These could be named-entities or words like \\textit{traffic, government, earthquake} which do not have many synonyms. Based on these, METEOR++ aims to capture whether the hypothesis is incomplete (with missing copy words) or inconsistent (with spurious copy-words). \\textbf{METEOR++2.0}  also considers syntactic level paraphrases which are not necessarily contiguous (such as ``not only ... but also ... '') rather than considering only lexical-level paraphrases of consecutive $n$-grams.\\\\\n\\\\\n\\textbf{ROUGE} (Recall-Oriented Understudy for Gisting Evaluation ) : \nROUGE metric includes a set of variants: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S. ROUGE-N is similar to BLEU-N in counting the $n$-gram matches between the hypothesis and reference, however, it a recall-based measure unlike BLEU which is precision-based.\n\\begin{align*}\n    \\text{ROUGE-N}  = \\frac{\\sum\\limits_{s_r \\in \\text{references}} \\sum\\limits_{\\text{$n$-gram} \\in s_r} Count_{match}(\\text{$n$-gram})}{\\sum\\limits_{s_r \\in \\text{references}} \\sum\\limits_{\\text{$n$-gram} \\in s_r} Count(\\text{$n$-gram})}\n\\end{align*}\nROUGE-L measures the longest common subsequence (LCS) between a pair of sentences. Note that a sequence $Z = [z_1, z_2, ...., z_n]$ is called a subsequence of another sequence $X = [x_1, x_2, ..., x_m]$ if there exists a strictly increasing sequence $[i_1, i_2, ... i_n]$ of indices of X such that $x_{i_j} = z_j$ for all $j=1,2,...,n$ . The \\textit{longest common subsequence}, $LCS(p,r)$ is the common subsequence in $p$ and $r$ with maximum length. ROUGE-L is a F-measure where the precision and recall are computed using the the length of the LCS:\n\\begin{gather*}\n    P_{lcs} = \\frac{|LCS(p,r)|}{\\# words\\_in\\_hypothesis} \\text{ , } R_{lcs} = \\frac{|LCS(p,r)|}{\\# words\\_in\\_reference}\n    \\\\\n    \\text{ROUGE-L} = F_{lcs} = \\frac{(1+\\beta^2)R_{lcs}P_{lcs}}{R_{lcs} + \\beta^2 P_{lcs}}\n\\end{gather*}\nNote that ROUGE-L does not check for consecutiveness of the matches as long as the word order is the same. It hence cannot differentiate between hypotheses that could have different semantic implications, as long as they have the same LCS even with different spatial positions of the words w.r.t the reference. ROUGE-W addresses this by using a weighted LCS matching that adds a \\textit{gap penalty} to reduce weight on each non-consecutive match.\\\\\nROUGE-S uses skip-bigram co-occurrence statistics to measure the similarity of the hypothesis and reference. Skip-bigrams are pairs of words in the same sentence order, with arbitrary words in between. ROUGE-S is also computed as an F-score similar to ROUGE-L.\\\\\nROUGE variants were originally proposed for evaluating automatic summarization, but have been adopted for evaluation of other NLG tasks.\\\\  \n\\\\\n\\textbf{CIDEr} (Consensus-based Image Description Evaluation ) : CIDEr weighs each $n$-gram in a sentence based on its frequency in the corpus and in the reference set of the particular instance, using TF-IDF (term-frequency and inverse-document-frequency). It was first proposed in the context of image captioning where each image is accompanied by multiple reference captions. It is based on the premise that $n$-grams that are relevant to an image would occur frequently in its set of reference captions. \nHowever, $n$-grams that appear frequently in the entire dataset (\\textit{i.e.}, in the reference captions of different images) are less likely to be informative/relevant and hence they are assigned a lower weight using inverse-document-frequency (IDF) term. To be more precise, the TF-IDF weight, $g_{n_k}(s)$, for each $n$-gram $k$ in caption $s_i$ are computed as follows: \n\\begin{align*}\n    g_{n_k}(s) = \\frac{t_k(s)}{\\sum_{l \\in V_n}t_l(s)} \\log \\Bigg( \\frac{|I|}{\\sum_{i \\in I}\\min(1,\\sum_{r \\in R_i}t_k(r))} \\Bigg)\n\\end{align*}\nwhere $V_n$ is the vocabulary of all $n$-grams, $g_{n_k}$ refers to the weight assigned to an $n$-gram denoted by $k$, $t_k(s)$ is the number of times $k$ appears in $s$, $I$ is the set of all images, $R_i$ corresponds to the set of references for image $i$.\\\\\n\\Ana{CIDEr paper seems to have conflict between text and formula for the TF term. Shouldn't the denominator of the first term sum over all the references for it to be consistent with what the authors have written in text?}\nCIDEr first stems the words in hypothesis and references and represents each sentence as a set of $n$-grams. It then calculates weights for each $n$-gram using TF-IDF as explained above.\n\\MK{How did we suddenly transition to cosine similarity? \\Ana{Changed content above. The TF-IDF weights are combined.}} \nUsing these TF-IDF weights of all the $n$-grams of length $n$, vectors $g_n(s)$ are formed for each caption $s$. $CIDEr_n$ is calculated as the average cosine similarity between hypothesis and references:\n\\begin{align*}\n    CIDEr_n(p, R) = \\frac{1}{|R|}\\sum_{r \\in R}\\frac{g_n(p).g_n(r)}{||g_n(p)||~||g_n(r)||}\n\\end{align*}\nFinal CIDEr score is the weighted average of $CIDEr_n$ for $n = 1,2,3,4$:\n$$ CIDEr(p,R) = \\sum_{n=1}^N W_n CIDEr_n(p,R) $$\nwhere the weights are uniform $W_n = \\frac{1}{N}$ and N is set to 4.\\\\\n\\\\\n\\textbf{SPICE} (Semantic Propositional Image Caption Evaluation  ): In the context of image captioning,  suggest that instead of focusing on $n$-gram similarity, more importance should be given to the semantic propositions implied by the text. To this end, they propose SPICE which uses `scene-graphs' to represent semantic propositional content. In particular, they parse the sentences into semantic tokens such as object classes $C$, relation types $R$ and attribute types $A$. Formally, a sentence $s$ is parsed into a scene-graph $G(s)$ as: \n$$ G(s) = <O(s), E(s), K(s)>$$\nwhere $O(s) \\subseteq C$ is the set of object mentions in $s$, $E(s) \\subseteq O(s) \\times R \\times O(s)$ is the set of hyperedges representing relations between objects, and $K(s)\\subseteq O(s) \\times A$ is the set of attributes associated with objects. The hypothesis and references are converted into scene graphs and the SPICE score is computed as the F1-score between the scene-graph tuples of the proposed sentence and all reference sentences. For matching the tuples, SPICE also considers synonyms from WordNet similar to METEOR . One issue with SPICE is that it depends heavily on the quality of parsing. Further, the authors note that SPICE neglects fluency assuming that the sentences are well-formed. It is thus possible that SPICE would assign a high score to captions that contain only objects, attributes and relations, but are grammatically incorrect.\\\\\n\\\\\n\\textbf{SPIDEr}  \\footnote{The name is a fusion of `SPICE' and `CIDEr'} : This metric is a linear weighted combination of SPICE and CIDEr. The motivation is to combine the benefits of semantic faithfulness of the SPICE score and syntactic fluency captured by the CIDEr score. Based on initial experiments, the authors use equal weights for SPICE and CIDEr.\\\\\n\\\\\n\\textbf{WER} (Word Error Rate): There is a family of WER-based metrics which measure the edit distance $d(c,r)$, \\textit{i.e.}, the number of insertions, deletions, substitutions  and, possibly, transpositions required to transform the candidate into the reference string. Word Edit Rate (WER) was first adopted for text evaluation from speech evaluation by  in 1992. Since then, several variants and enhancements have been proposed, as discussed below. The original formula is is based on the fraction of word edits as given below: \n$$ WER = \\frac{\\# of substitutions+insertions+deletions}{reference\\ length} $$\nSince WER relies heavily on the reference sentence,   propose enhanced WER that takes into account multiple references. Another issue with WER is that it penalizes different word order heavily since each ``misplaced\" word triggers a deletion operation followed by an insertion operation, when infact the hypothesis could still be valid even with a different word order.\nTo account for this, \\textbf{TER} (Translation Edit Rate ) adds a shifting action/block movement as an editing step. \n\\textbf{ITER}  is a further improved version of TER. In addition to the basic edit operations in TER (insertion, deletion, substitution and shift), ITER also allows stem matching and uses optimizable edit costs and better normalization. \\textbf{PER}  computes `Position-independent Edit Rate' by identifying the alignments/matching words in both sentences. Then depending on whether the proposed sentence is shorter or longer than the reference, the remaining words are counted as insertions or deletions.\n\\textbf{CDER}  models block reordering as a unit edit operation to off-set unnecessary costs in shifting words individually.", "cites": [4227, 7794, 7458, 7795], "cite_extract_rate": 0.18181818181818182, "origin_cites_number": 22, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.0}, "insight_level": "high", "analysis": "The section provides a clear analytical overview of word-based NLG evaluation metrics, particularly BLEU and its variants. It synthesizes information on BLEU's computation, brevity penalty, and extensions like SentBLEU and deltaBLEU, integrating these into a broader understanding of corpus-level vs. sentence-level evaluation. The critical analysis is strong, highlighting BLEU's limitations (e.g., gaming through short outputs) and how NIST and deltaBLEU attempt to address them. While it offers some abstraction (e.g., identifying patterns in metric design), the section primarily focuses on explaining and evaluating specific mechanisms in depth."}}
{"id": "78aa6f22-e24e-48d8-8130-57055344044d", "title": "Embedding based metrics", "level": "subsubsection", "subsections": [], "parent_id": "6555d406-d246-43f7-9325-0b6908aa1ff6", "prefix_titles": [["title", "A Survey of Evaluation Metrics Used for NLG Systems"], ["section", "Context-Free metrics"], ["subsection", "Untrained metrics"], ["subsubsection", "Embedding based metrics"]], "content": "}\nThe word/character based metrics discussed above, rely largely on surface level matches (although a couple of them do consider synonyms). As a result, they often ignore semantic similarities between words. For example, the words `canine' and `dog' are related, and are synonyms in some contexts. Similarly, the words `cat' and `dog', although not synonymous, are closer (by virtue of being pet animals) than say, `dog' and `boat'. Such similarities are better captured by word embeddings such as as Word2Vec , GloVe , \\textit{etc.}, which are trained on large corpora and capture distributional similarity between words. Thus, an alternative to matching words is to compare the similarity between the embeddings of words in the hypothesis and the reference(s). We discuss such word embedding based metrics in this subsection. \nIn all the discussion that follows, we represent the embedding of a word $w$ as $\\overrightarrow{w}$.\\\\\n\\\\\n\\textbf{Greedy Matching} : This metric considers each token in the reference and greedily matches it to the closest token in the hypothesis based on the cosine similarity between the embeddings of the tokens. The aggregate score is obtained by averaging across all the tokens in the reference. However, this greedy approach makes this score direction-dependent, and hence the process is repeated in the reverse direction (\\textit{i.e.}, greedily match each hypothesis token with the reference tokens) to ensure that the metric is symmetric. The final score given by greedy matching metric (GM) is the average of matching in both directions. \n$$ G(p,r) = \\frac{\\sum_{w\\in r}\\max_{\\hat{w} \\in p} cosine(\\overrightarrow{w},\\overrightarrow{\\hat{w}})}{|r|} $$\n$$ GM = \\frac{G(p,r)+G(r,p)}{2} $$\n\\\\\n\\textbf{Embedding Average metric}  : Instead of computing a score for the hypothesis by comparing the embeddings of the words/tokens in the hypothesis and the reference, one could directly compute and compare the embeddings of the sentences involved (\\textit{i.e.}, the hypothesis sentence and the reference sentence). The Vector Averaging or Embedding Average metric does exactly this by first computing a sentence-level embedding by averaging the word embeddings of all the tokens in the sentence.\n$$ \\overrightarrow{s} = \\frac{\\sum_{w\\in s}\\overrightarrow{w}}{|s|} $$\nThe score for a given hypothesis, $EA$, is then computed as the cosine similarity between the embedding of the reference ($\\overrightarrow{r}$) and the embedding of the hypothesis ($\\overrightarrow{p}$).\n$$ EA = cosine(\\overrightarrow{p},\\overrightarrow{r})$$\n\\\\\n\\textbf{Vector Extrema:} The sentence-level embeddings can alternatively be calculated by using Vector Extrema . In this case, a $k$-dimensional sentence embedding is constructed using the $k$-dimensional word embeddings of all the words in the sentence. However, instead of taking an average of the word embeddings, a dimension-wise max/min operation is performed over the word embeddings. In other words, the most extreme value (\\textit{i.e.}, the value farthest from $0$) along each dimension is chosen by considering the embeddings corresponding to all the words in the sentence. \n\\begin{align*}\n    \\overrightarrow{s_d} = \nÂ  Â  \\begin{cases}\nÂ  Â  Â \\max_{w \\in s}\\overrightarrow{w}_d, & \\text{if } \\overrightarrow{w}_d>|\\min_{w'\\in s}\\overrightarrow{w'}_d| \\\\\nÂ  Â  Â \\min_{w\\in s}\\overrightarrow{w}_d & \\text{otherwise}\nÂ  Â  \\end{cases}\n\\end{align*}\n\\MK{the above formula doesn't make sense to me. the if and else parts are the same \\Ana{extrema value can be negative. I verified this with the formula in paper }}\nwhere $d$ indexes the dimensions of a vector. \nThe authors claim that by taking the extreme value along each dimension, we can ignore the common words (which will be pulled towards the origin) and prioritize informative words which will lie further away \\newA{from the origin} in the vector space. \\MK{is this phrasing correct? is it right to say ``further away in the vector space''? \\Ana{should we say ``further away from the origin in the vector space''? If it's between further and farther, both are appropriate in this case.}} The final score assigned to a hypothesis is the cosine similarity between the sentence-level embeddings of the reference and the hypothesis.\\\\\n\\\\\n\\textbf{WMD} (Word Mover-Distance) : This metric was proposed to measure dissimilarity between text documents by computing the minimum cumulative distance between the embeddings of their constituent words. It performs optimal matching rather than greedy matching, based on the Euclidean distance between the word embeddings of the hypothesis and reference words. \\newA{Note that an optimal matching might have each word embedding in the hypothesis to be partially mapped to multiple word embeddings in the reference. To model this effectively, the hypothesis and reference are first represented as $n$-dimensional normalized bag-of-words vectors, $\\overrightarrow{p}$ and $\\overrightarrow{r}$ respectively. \nThe number of dimensions, $n$, of the normalized bag-of-words vector of a sentence is given by the vocabulary size, and the value of each dimension represents the normalized occurrence count of the corresponding word from the vocabulary in the sentence. That is, if the $i^{th}$ vocabulary word appears $t_i$ times in a sentence $s$, then $\\overrightarrow{s}_i= \\frac{t_i}{\\sum^n_{j=1}t_j}$. WMD allows any word in $\\overrightarrow{p}$ to be transformed into any word in $\\overrightarrow{r}$ either in total or in parts, to arrive at the minimum cumulative distance between $\\overrightarrow{p}$ and $\\overrightarrow{r}$ using the embeddings of the constituent words. Specifically, WMD poses a constraint-optimization problem as follows:\n\\begin{gather*}\n     WMD(p,r) = \\min_{T}\\sum_{i,j=1}^{n}T_{ij}.\\Delta(i, j) \n     \\\\\n     \\text{such that } \\sum_{j=1}^{n}T_{ij} = \\overrightarrow{p}_i \\forall i \\in \\{1,..,n\\}, \\text{ and } \\sum_{i=1}^{n}T_{ij} = \\overrightarrow{r}_j \\forall j \\in \\{1,..,n\\}\n\\end{gather*}\nwhere $\\Delta(i,j) = ||\\overrightarrow{w}_i-\\overrightarrow{w}_j||_2$ is the Euclidean distance between the embeddings of the words indexed by $i$ and $j$ in the vocabulary \\footnote{For simplicity, we here onward refer to a word indexed at $i$ in the vocabulary as simply word $i$}, $n$ is the vocabulary size and $T$ is a matrix with $T_{ij}$ representing how much of word $i$ in $\\overrightarrow{p}$ travels to word $j$ in $\\overrightarrow{r}$. The two constraints are to ensure complete transformation of $\\overrightarrow{p}$ into $\\overrightarrow{r}$. That is, the outgoing (partial) amounts of every word $i$ should sum up to the value in the corresponding dimension in $\\overrightarrow{p}$ (\\textit{i.e.}, its total amount/count in the hypothesis). Similarly, the incoming amounts of every word $j$ in the reference should sum up to its corresponding value in $\\overrightarrow{r}$.}\n\\MK{the rest of this para needs to be re-written. What does bag-of-words vectors mean? You are using $n$ for vector size as well as vocabulary size? Are $\\overrightarrow{p}$ and $\\overrightarrow{r}$ sentence embeddings or sets of word embeddings? If its the latter the notation is very confusing given that so far you are referring to $\\overrightarrow{p}$ as sentence embedding. Slow down. USe more sentences to explain what you wnat to say. The intuition behind the formula is not at all clear.\\Ana{I hope the idea is clear now. Should see if we need to word the sentences in a better manner in the next iteration.}} \nAlthough initially proposed for document classification, WMD has been favourably adopted for evaluating the task of image captioning . \nWMD has also been adopted for summarization and MT evaluation. However, since WMD is insensitive to word order,   propose a modified version termed \\textbf{WMD}$_O$ which additionally introduces a penalty term similar to METEOR's fragmentation penalty. \\MK{I don't think we mentioned the term ``fragmentation penalty'' while discussing METEOR \\Ana{Now I have clarified the fragmentation penalty in METEOR.}} \n\\begin{align*}\n    WMD_O = WMD - \\delta (\\frac{1}{2} - penalty)\n\\end{align*}\nwhere $\\delta$ is a weight parameter that controls how much to penalize a different word ordering. In parallel, \\textbf{WE\\_WPI} (Word Embedding-based automatic MT evaluation using Word Position Information)  was proposed which also addresses the word-order issue by using an `align-score' instead of Euclidean distance to match words: \n\\begin{align*}\n    \\Delta(i,j) = align\\_score = \\overrightarrow{w}_i.\\overrightarrow{w}_j \\times \\Bigg(1.0 - \\Big|\\frac{pos(i,h)}{|h|} - \\frac{pos(j,r)}{|r|}\\Big|\\ \\Bigg)\n\\end{align*}\nwhere $pos(i,h)$ and $pos(j,r)$ indicate the positions of word $i$ in the hypothesis and word $j$ in the reference respectively, and $\\Big|\\frac{pos(h_i)}{|h|} - \\frac{pos(r_j)}{|r|}\\Big|$ gives the relative difference between the word positions. WMD$_O$ and WE\\_WPI are currently used only for evaluating MT tasks.\\\\ \n\\\\\n\\textbf{MEANT: }  make use of semantic role labelling in order to focus on both the structure and semantics of the sentences. Semantic role labelling, also called shallow semantic parsing, is the process of assigning labels to words or phrases to indicate their role in the sentence, such as doer, receiver or goal of an action, \\textit{etc}. This annotation would help answer questions like who did what to whom, leading to better semantic analysis of sentences. In this direction, MEANT was proposed as a weighted combination of F-scores computed over the semantic frames as well as their role fillers to evaluate the ``adequacy\" of the hypothesis in representing the meaning of the reference. MEANT first uses a shallow semantic parser on the reference and candidate and aligns the semantic frames using maximum weighted bipartite matching based on lexical similarities (of the predicates). This lexical similarity is computed using word vectors . It then matches the role fillers in a similar manner, and finally computes the weighted F-score over the matching role labels and role fillers.\\\\\nMEANT was originally proposed as a semi-automatic metric  before the above fully automatic form. There have also been several variants  of MEANT metric that followed over the years, with the latest one being \\textbf{MEANT2.0}. MEANT2.0 weighs the importance of each word by IDF (inverse document frequency) to ensure phrases with more matches for content words than for function words are scored higher. It also modifies the phrasal similarity calculation to aggregate on $n$-gram lexical similarities rather than on the bag-of-words in the phrase, so that the word order is taken into account. \\\\\n\\hfill\\\\\n\\textbf{Contextualized Embedding based metrics}:\nThe embedding based metrics discussed above use static word embeddings, \\textit{i.e.}, the embeddings of the words are not dependent on the context in which they are used. However, over the past few years, contextualized word embeddings have become popular. Here, the embedding of a word depends on the context in which it is used. \nSome popular examples of such contextualized embeddings include ElMo  , BERT  and XLNet . In this subsection, we discuss evaluation metrics which use such contextualized word embeddings.\\\\\n\\\\\n\\textbf{YiSi:}  YiSi  is a unified semantic evaluation framework that unifies a suite of metrics, each of which caters to languages with different levels of available resources. YiSi-1 is a metric similar to MEANT2.0, that \nuses contextual word embeddings from BERT rather than word2vec embeddings. \nAdditionally, it makes the time-consuming and resource-dependent step of semantic parsing used in MEANT2.0 optional. In particular, \\MK{The remainder of this sentence is not clear? How is F-score computed using n-gram similarity? What doe we mean by ``weighted'' here? How do we optionally account for semantic structure? Too much crammed in one sentence. Use more sentences to explain this clearly. } YiSi-1 is an F-score that computes $n$-gram similarity as an aggregate of weighted word embeddings cosine similarity, optionally taking the shallow semantic structure into account. \\MK{The next sentence is a very long sentence but not at all informative. Accuracy and cosine similarity are to very different things. How do we jump from one to the other? } YiSi-0 is a degenerate resource-free version which uses the longest common character substring accuracy, instead of word embeddings cosine similarity, to measure the word similarity of the candidate and reference sentences. YiSi-2 is the bilingual version which uses the input sentence and is hence discussed in the next section on context-dependent metrics.\\\\ \n\\\\\n\\textbf{BERTr}:  adopt BERT to obtain the word embeddings and show that using such contextual embeddings with a simple average recall based metric gives competitive results.\nThe BERTr score is the average recall score over all tokens, using a relaxed version of token matching based on BERT embeddings, \\textit{i.e.}, by computing the maximum cosine similarity between the embedding of a reference token $j$ and any token in the hypothesis.\n\\begin{gather*}\n    recall_j = \\max_{i\\in p} cosine(\\overrightarrow{i},\\overrightarrow{j})\n    \\\\\n     \\text{BERTr } = \\sum_{j\\in r}\\frac{recall_j}{|r|}\n\\end{gather*}\n\\\\\n\\textbf{BERTscore}:  compute cosine similarity of each hypothesis token $j$ with each token $i$ in the reference sentence using contextualized embeddings. They use a greedy matching approach instead of a time-consuming best-case matching approach, and then compute the F1 measure as follows:\n\\begin{gather*}\n    R_{BERT} = \\frac{1}{|r|}\\sum_{i \\in r} \\max_{j \\in p} \\overrightarrow{i}^T\\ \\overrightarrow{j} \\text{ , } P_{BERT} = \\frac{1}{|p|}\\sum_{j \\in p} \\max_{i \\in r} \\overrightarrow{i}^T\\ \\overrightarrow{j}\n    \\\\\n    \\text{BERTscore} = F_{BERT} = 2 \\frac{P_{BERT}.R_{BERT}}{P_{BERT}+ R_{BERT}}\n\\end{gather*}\nThe authors show that this metric correlates better with human judgements for the tasks of image captioning and machine translation.\\\\\n\\\\\n\\textbf{MoverScore:}  take inspiration from WMD metric to formulate another optimal matching metric named MoverScore, which uses contextualized embeddings to compute the Euclidean distances between words or $n$-grams. In contrast to BERTscore which allows one-to-one hard matching of words, MoverScore allows many-to-one matching as it uses soft/partial alignments, similar to how WMD allows partial matching with word2vec embeddings. \\MK{I think we need a formula here. \\Ana{The formula is similar to WMD, except that MoverScore uses contextualized embeddings. I've now stated this explicitly. We can add the formula for even more clarity, but I'll keep that pending for a later iteration since the formula will get too elaborate like WMD.}} \nIt has been shown to have competitive correlations with human judgements in 4 NLG tasks: machine translation, image captioning, abstractive summarization and data-to-text generation.  \\MK{What do you mean by generalise? Do you mean it has better correlations for all these tasks? If so, let's just say that.\\Ana{Changed}}", "cites": [8385, 7, 11, 7165, 4224, 2027], "cite_extract_rate": 0.2727272727272727, "origin_cites_number": 22, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear explanation of embedding-based NLG evaluation metrics and links them to the limitations of surface-level metrics, showing analytical depth. It integrates key concepts from relevant papers, particularly explaining WMD's optimization approach. However, the critical evaluation is limited, and some parts lack clarity in connecting the cited works to broader trends or theoretical implications."}}
{"id": "811b8610-b805-42f4-97d5-9cea4fef08a9", "title": "Feature-based trained metrics", "level": "subsubsection", "subsections": [], "parent_id": "8fa43e7c-a90f-473e-8517-2050fe94316f", "prefix_titles": [["title", "A Survey of Evaluation Metrics Used for NLG Systems"], ["section", "Context-Free metrics"], ["subsection", "Trained metrics"], ["subsubsection", "Feature-based trained metrics"]], "content": "}\nFeature-based trained metrics primarily focus on combining various heuristic-based features using a learnable model. These features, obtained from the hypothesis and reference sentences, could be statistical measures such as $n$-gram precision, recall or even untrained metrics such as BLEU or METEOR scores. Further, the learning model can vary from a simple Linear Regressor to a complex Deep Neural Network. We now discuss these different metrics sub-categorized by the learnable model. \\\\\n\\\\\n\\textbf{Linear Regression}\\\\\n\\textbf{BEER} (BEtter Evaluation as Ranking) : \nThe set of input features used by BEER include precision, recall and F1-score on character $n$-grams for various $n$ and on word-level unigrams. Additionally, they use features based on permutation trees  to evaluate word order or fluency. The unigram statistics are computed on function words and content words separately as well as on the entire set of words. \nThe BEER model is a simple linear function of the input features given as:\n\\begin{align*}\n    BEER\\ score(p,r) = \\sum_i W_i x \\phi_i(p,r)\n\\end{align*}\nwhere the different features $\\phi_i(p,r)$ are first computed using the hypothesis $p$ and reference sentence $r$, and the model learns the weights $W_i$ for each feature \\newA{using linear regression with human judgements from WMT13  as gold-standard.} \\Akash{Are model weights learned from human judgements? Pls write a few sentences on the data and task they used \\Ana{Added above, I have tried to verify that other trained metrics also have such details.}}\\\\\n\\\\\n\\textbf{SVM Regression}\\\\\n\\textbf{BLEND} : This metric combines various existing untrained metrics to improve the correlation with human judgements. It uses an SVM regressor with 57 metric scores as features and the DA scores (direct assessment scores on translation quality obtained through human evaluators ) from WMT15 and WMT16 as the gold standard target. The metrics are classified into 3 categories as lexical, syntactic and semantic based metrics. Out of the 57 metrics, 25 are categorized as lexical-based, which correspond only to 9 types of metrics, since some of them are simply different variants of the same metric. \\newA{For instance, eight variants of BLEU are formed by using different combinations of $n$-gram lengths, with or without smoothing, etc.} \\Akash{can u give an example on what the different variants are for a specific metric say Bleu? \\Ana{Added above.}} These 9 metrics are BLEU, NIST, GTM, METEOR, ROUGE, Ol, WER, TER and PER. \\newA{17 syntactic metrics are borrowed from the Asiya toolkit  along with 13 semantic metrics, which in reality correspond to 3 distinct metrics, related to Named entities, Semantic Roles and Discourse Representation.} \\Akash{can u give some examples/insights on these 17 syntactic and 3 semantic metrics? \\Ana{None of these details are in the paper. They mainly say they follow DPMFcomb in identifying these 57 metrics. I've checked that and other papers it refers to to add a few insights for these.}} The authors performed an ablation study to analyse the contribution of each of the categories and found that a combination of all the categories provides the best results. \\\\\n\\\\\n\\textbf{Grid search with bagging}\\\\\n\\textbf{Q-Metrics}:  focus on improving existing $n$-gram metrics such as BLEU, METEOR, ROUGE to obtain a better correlation with human judgements on the answerability criteria for the task of question generation. The authors argue that some words in the hypothesis and reference questions carry more importance that the others and hence propose to assign different weightages to words rather than having equal weights like in standard $n$-gram metrics. \nHence they categorize the words of the hypothesis and reference question into four categories \\textit{viz.} function words, question words (7 Wh-words including `how'), named entities and content words (identified as belonging to none of the previous categories). The $n$-gram precision and recall are computed separately for each of these categories and a weighted average of them to computed to obtain $P_{avg}$ and $R_{avg}$. The Answerability score and Q-metric is defined as:\n$$ Answerability = 2.\\frac{P_{avg}R_{avg}}{P_{avg} + R_{avg}} $$\n$$ Q\\text{-}Metric = \\delta Answerability + (1-\\delta)Metric$$\nwhere $Metric \\in \\{BLEU, NIST, METEOR, ROUGE\\}$. The weights and $\\delta$ are tuned using grid search and bagging to find the optimal values that maximize correlation with human scores.\\\\\n\\\\\n\\textbf{Neural networks/ Deep Learning}\\\\\n\\textbf{Composite metrics:}  propose a set of metrics by training a multi-layer feedforward neural network with various combinations of METEOR, CIDEr, WMD, and SPICE metrics as input features. The neural network classifies the hypothesis image caption as either machine-generated or human-generated. The model is trained on Flicker30k  dataset, by using 3 out of the 5 reference captions available for each image as positive samples, and captions generated by 3 different models (Show and Tell , Show, Attend and Tell  and Adaptive Attention ) as negative training samples. \\textbf{NNEval}  proposed by the same authors additionally considers BLEU(1-4) scores in the feature set input to the neural network.", "cites": [7295, 356, 4223], "cite_extract_rate": 0.1875, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively synthesizes the cited papers by integrating key features and methodologies from multiple evaluation metric works into a structured narrative. It provides some critical analysis by noting the limitations of n-gram metrics and highlighting the need for better correlation with human judgements. However, the abstraction is limited to identifying categories of metrics and their components without offering deeper theoretical or conceptual generalizations."}}
{"id": "6f15443b-8055-409a-a3b1-5c5f932b2c3e", "title": "End-to-end Trained metrics", "level": "subsubsection", "subsections": [], "parent_id": "8fa43e7c-a90f-473e-8517-2050fe94316f", "prefix_titles": [["title", "A Survey of Evaluation Metrics Used for NLG Systems"], ["section", "Context-Free metrics"], ["subsection", "Trained metrics"], ["subsubsection", "End-to-end Trained metrics"]], "content": "}\nEnd-to-end Trained metrics are directly trained using the hypothesis and reference sentences. Note that all the proposed end-to-end trained metrics are based on neural networks. Most of these metrics employ feed-forward neural networks or RNN based models with static/contextualized word embeddings. However, recently pretrained transformer models are also being used in a few metrics.\\\\\n\\\\\n\\textbf{SIMILE:} To facilitate better comparison of hypothesis and reference sentences,  train a sentence encoder, $g$, on a set of paraphrase pairs (from ParaNMT corpus ) using the max margin loss:\n$$ l(s,s') = max\\Big(0,\\delta - cos(g(s),g(s')) + cos(g(s),g(t)) \\Big)$$ where $\\delta$ is the margin, $s$ and $s'$ are paraphrases, and $t$ is a negative example obtained by random sampling the other sentence pairs.\\\\\nThe authors define the metric `SIM' as the cosine similarity of the sentence embeddings of the reference and candidate sentences. To discourage model generations that have repeating words with longer lengths, a length penalty (LP) term is employed in contrast to the Brevity Penalty term in BLEU.\n$$ LP(r,p) = e^{1-\\frac{max(|r|,|p|)}{min(|r|,|p|)}}$$\nFinally SIMILE is defined using SIM and LP as: \n\\begin{align*}\n    SIMILE = LP(r,p)^\\alpha SIM(r,p)\n\\end{align*}\nwhere $\\alpha$ determines the influence of the length penalty term and is tuned over the set {0.25,0.5}.\\\\\n\\\\\n\\textbf{ESIM} (Enhanced Sequential Inference Model): ESIM is a model for natural language inference proposed by , which has been directly adopted for the task of translation evaluation by . It consists of a trained BiLSTM model to first compute sentence representations of the reference and hypothesis. Next, the similarity between the reference and hypothesis is calculated using a cross-sentence attention mechanism. These attention weighted representations are then combined to generate \\textit{enhanced} representations of the hypothesis and the reference. The enhanced representations are passed as input to another BiLSTM. The max-pooled and average-pooled hidden states of the final BiLSTM are used to predict the ESIM score:\n\\begin{gather*}\n    x = [v_{r,avg};v_{r,max};v_{p,avg};v_{p,max}]\n    \\\\\n    ESIM = U^T ReLU(W^Tx+b) +b'\n\\end{gather*}\nwhere $v_{s,avg/max}$ denotes the average or max pooled vector of the final BiLSTM hidden states for sentence $s$, and $U, W, b and b'$ are parameters to be learnt. \nThe metric is trained on the \\textit{Direct Assessment} human evaluation data that is collected for WMT 2016 .\\\\\n\\\\\n\\textbf{RUSE} (Regressor Using Sentence Enbeddings ): RUSE is a MultiLayer Perceptron (MLP) based regression model that combines three pre-trained sentence embeddings. The three types of sentence embeddings used are InferSent , Quick-Thought  and Universal Sentence Encoder. Through these embeddings, RUSE aims to utilize the global sentence information that cannot be captured by any local features that are based on character or word $n$-grams. An MLP regressor predicts the RUSE score by using a combination of the sentence embeddings of the hypothesis and reference.\n\\begin{gather*}\n    \\overrightarrow{s} = \\text{Encoder(s) = [InferSent(s); Quick-Thought(s); UniversalSentenceEncoder(s)]}\n    \\\\\n    \\text{RUSE = MLP-Regressor}\\big( \\overrightarrow{p};\\overrightarrow{r};|\\overrightarrow{p}-\\overrightarrow{r}|;\\overrightarrow{p}*\\overrightarrow{r} ) \\big)\n\\end{gather*}\nThe sentence embeddings are obtained from pre-trained models and only the MLP regressor is trained on human judgements from the WMT shared tasks over the years 2015-2017.\\\\ \n\\\\\n\\textbf{Transformer based trained metrics}\\\\\nTransformer architecture  eschews the well-established route of using Recurrent Neural Networks (RNNs and any of its variants) for tasks in NLP (Natural Language Processing). It instead incorporates multiple levels of feed-forward neural networks with attention components. The transformer-based models such as BERT, RoBERTa, XLNet, etc, have shown a lot of promise in various NLP/NLG tasks and have also forayed into the domain of trained evaluation metrics for NLG. We present the transformer-based metrics here.\\\\\n\\\\\n\\textbf{BERT for MTE}  : This model encodes the reference and hypothesis sentences together by concatenating them and passing them through BERT. A `[SEP]' token is added for separation and a `[CLS]' token is prepended to the pair as per the input-requirements of BERT. An MLP-regressor on top of the final representation of the [CLS] token provides the score. Unlike in RUSE, the pretrained BERT encoder is also jointly finetuned for the evaluation task. The other difference from RUSE is the usage of the \\textit{pair-encoding} of the candidate and reference sentences together instead of using separate sentence embeddings. The authors report an improvement in correlations with this approach over RUSE.\n\\begin{gather*}\n    \\overrightarrow{v} = \\text{BERT pair-encoder([CLS] ; p ; [SEP] ; r ; [SEP])}\n    \\\\\n    \\text{BERT for MTE = MLP-Regressor}\\big(\\overrightarrow{v}_{[CLS]} \\big)\n\\end{gather*}\n\\\\\n\\textbf{BLEURT: }  pretrained BERT with synthetically generated sentence pairs obtained by perturbing Wikipedia sentences via mask-filling with BERT, back-translation or randomly dropping words. A set of pretraining signals are employed including:\\\\\n(i) BLEU, ROUGE and BERTscore, (the latter 2 are split into 3 signals each, using the precision, recall and F-score),\\\\\n(ii) back-translation likelihood indicating the probability that the two sentences are back-translations of the other with either German or French as the intermediate language,\\\\\n(iii) textual entailment signal (indicating Entailment, Contradiction or Neutral) obtained from BERT fine-tuned on entailment task with MNLI (Multi-Genre Natural Language Inference) dataset.\\\\\n(iv) back-translation flag to indicate if the perturbation was actually generated through back-translation or mask-filling.\\\\\nThe various signal/task-level losses are aggregated using weighted sum. The BLEURT rating is obtained using a linear layer on the embedding produced for the prepended [CLS] token:\n$$ BLUERT\\ score = \\hat{y} = f(r,p) = W\\tilde{v}_{[cls]}+b $$\nThe original BERT, further pretrained on synthetic data with the pretraining signals is finetuned on the task-specific supervised data using regression loss.\n$$ loss= \\frac{1}{N}\\sum_{n=1}^N||y_i-\\hat{y}||^2$$\nBLEURT achieves state-of-the-art performance on WMT and WebNLG challenges after finetuning on those datasets.\\\\\n\\\\\n\\textbf{NUBIA:}  propose a 3-stage architecture called NUBIA for NLG evaluation. The first step is neural feature extraction using various transformer based architectures to represent sentence similarity, logical inference and sentence likelihood/legibility.\nThe models used for this are:\n\\begin{itemize}\n    \\item RoBERTa large pretrained model, finetuned on STS-B-benchmark dataset to predict sentence similarity between hypothesis and reference \n    \\item RoBERTa large pretrained model, finetuned on MNLI challenge of GLUE for capturing logical relationship between the hypothesis and reference.\n    \\item GPT-2 model's perplexity score to determine the grammatical correctness of the hypothesis.\n\\end{itemize}\nThe next step/module is termed an aggregator which is either a linear regression model or a feed forward neural network trained to provide a quality score on the hypothesis on its interchangeability with the reference. Finally the calibration step ensures the final value is between 0 and 1, and also that providing the reference sentence as the hypothesis generates a score of 1. The authors show NUBIA outperforms/matches the metrics used to evaluate machine translation, and image captioning in terms of correlations with human judgements.\n    \\centering\n    \\begin{tabular}{c|c|c|c|c|c|c|c}\n         Metric & MT & AS & DG & IC  & QA & D2T & QG\\\\\n         \\hline\n         \\multicolumn{8}{c}{Context-free metrics}\\\\\n         \\hline\n         BLEU & \\checkmark & * & * & * & * & * & *\\\\\n         NIST & \\checkmark & * & * & * & * & *& *\\\\\n         METEOR & \\checkmark & * & * & * & * & *& *\\\\\n         ROUGE & * & \\checkmark & * & * & * & *& *\\\\\n         GTM & \\checkmark & * &  &  & * & &\\\\\n         CIDEr &  &  &  & \\checkmark & & &\\\\\n         SPICE &  &  &  & \\checkmark &  & &\\\\\n         SPIDer &  &  &  & \\checkmark & & &\\\\\n         WER-family & \\checkmark &  &  &  &  & &\\\\\n         chrF & \\checkmark & * &  & * &  & &\\\\\n         Vector Extrema & * & * & * & * & * & *& \\\\\n         Vector Averaging & * & * & * & * & * & *& \\\\\n         WMD & * & * &  & * &  & &\\\\\n         BERTr & * &  &  &  &  & &\\\\\n         BERTscore & \\checkmark &  & * & \\checkmark & * & &\\\\\n         MoverScore & \\checkmark & \\checkmark &  & \\checkmark &  & \\checkmark &\\\\\n         BEER & \\checkmark &  &  &  &  & & \\\\\n         BLEND & \\checkmark &  &  &  &  &  &\\\\\n         Q-metrics &  &  &  &  &  & & \\checkmark\\\\\n         Composite metrics &  &  &  & \\checkmark & & &\\\\\n         SIMILE & \\checkmark &  &  &  &  &  &\\\\\n         ESIM & \\checkmark &  &  &  &  &  &\\\\\n         RUSE & \\checkmark &  &  &  &  &  &\\\\\n         BERT for MTE & \\checkmark &  &  &  &  &  &\\\\\n         BLEURT & \\checkmark &  &  &  &  & \\checkmark &\\\\\n         NUBIA & \\checkmark &  &  & \\checkmark &  & & \\\\\n         \\hline\n         \\multicolumn{8}{c}{Context-dependent metrics}\\\\\n         \\hline\n         ROUGE-C &  & \\checkmark &  & &  & & \\\\\n         PARENT &  &  &  & &  & \\checkmark & \\\\\n         LEIC &  &  &  & \\checkmark &  & &\\\\\n         ADEM &  &  & \\checkmark &  &  & &\\\\\n         RUBER &  &  & \\checkmark &  &  & &\\\\\n         SSREM &  &  & \\checkmark &  &  & &\\\\\n         RUBER with BERT embeddings &  &  & \\checkmark &  &  & &\\\\\n         MaUde &  &  & \\checkmark &  &  & &\\\\\n         RoBERTa-eval &  &  & \\checkmark &  &  & &\\\\\n         \\hline\n    \\end{tabular}\n    \\caption{Automatic metrics proposed (\\checkmark) and adopted (*) for various NLG tasks }\n    \\label{tab:auto_metric_adoption}\n\\end{table}", "cites": [7793, 2361, 826, 7, 4231, 4230, 7490, 38, 11], "cite_extract_rate": 0.625, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 3.8}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers by highlighting the evolution from feed-forward and RNN-based models to transformer-based approaches in end-to-end trained metrics. It offers a critical perspective by pointing out limitations of prior metrics and the advantages of newer methods (e.g., joint fine-tuning in BERT for MTE). While not reaching full abstraction, it identifies trends in embedding-based evaluation and the shift toward more holistic, pre-trained models."}}
{"id": "aba5c5b6-89c9-4286-a883-2e06c6e1241e", "title": "Word based metrics", "level": "subsubsection", "subsections": [], "parent_id": "31c96056-170c-4f72-ab65-eb25e4eff491", "prefix_titles": [["title", "A Survey of Evaluation Metrics Used for NLG Systems"], ["section", "Context-dependent metrics"], ["subsection", "Untrained metrics"], ["subsubsection", "Word based metrics"]], "content": "}\nWord based context-free metrics evaluate a hypothesis by using the word or $n$-gram features of the hypothesis and the context. ROUGE-C  and PARENT  are two metrics under this category. \n\\hfill\\\\\n\\textbf{ROUGE-C: } In the context of abstractive summarization,  proposed a modification to ROUGE, dubbed ROUGE-C, where the candidate summary is compared with the document to be summarized, instead of the reference summary. For instance, ROUGE-C-N is given as:\n\\begin{align*}\n     \\text{ROUGE-C-N}  = \\frac{\\sum\\limits_{s_h \\in \\text{hypothesis}} \\sum\\limits_{\\text{$n$-gram} \\in s_h} Count_{match}(\\text{$n$-gram})}{\\sum\\limits_{s_c \\in \\text{Source Document}} \\sum\\limits_{\\text{$n$-gram} \\in s_c} Count(\\text{$n$-gram})}\n\\end{align*}\nwhere $s_h$ and $s_c$ are sentences belonging to the summary and the document respectively. ROUGE-C is especially beneficial in cases where a reference summary is not available. Additionally for query-focused summarization task, \\textit{i.e.}, the task of creating a summary that answers the given query from the document, the ROUGE-C score is computed as:\n\\begin{align*}\n    \\text{ROUGE-C} = \\lambda\\cdot\\text{ROUGE-C}_{QF} + (1-\\lambda)\\cdot\\text{ROUGE-C}_{D}\n\\end{align*}\nwhere ROUGE-C$_{D}$ is the ROUGE-C score when the document is used as the context and ROUGE-C$_{QF}$ is the ROUGE-C score when the query-focused information, such as the questions, viewpoints, task descriptions, \\textit{etc.}, are used as the context. \\newA{The weighting factor $\\lambda$ is varied from 0 to 1 to check for Pearson/Spearman's correlations with human judgements and original ROUGE scores on DUC (Document Understanding Conference) data\\footnote{https://duc.nist.gov/}. }\\\\\n\\MK{Write a sentence about $\\lambda$. How is it tuned? \\Ana{Added some details. They don't explicitly specify what value of $\\lambda$ they use. They just show a plot varying $\\lambda$ vs correlations with human scores as well as with the original ROUGE-variants scores and make a comment that no one value is the best for all these correlations.}}\\\\\n\\\\\n\\textbf{PARENT} (Precision And Recall of Entailed N-grams from the Table):  proposed the PARENT evaluation metric for the the task of data-to-text generation. PARENT matches the $n$-grams in the hypothesis with both the reference as well as the record/tuple $t$. In order to match the semi-structured data in the table with the unstructured hypothesis, an entailment probability is defined as the probability of an $n$-gram being correct/valid, given the table. The entailment probability for an $n$-gram $g$ is computed either using a word-overlap model, that computes the fraction of words in the $n$-gram that are in the table $t$, \\textit{i.e.}, $Pr(g)=\\sum_{w\\in g}\\mathbbm{1}(w\\in t)/n$, or using a co-occurrence model that first learns the probability of entailment of each word $Pr(w)$ using co-occurrence counts of words from a training set of table-reference pairs. The entailment probability of the $n$-gram $g$ is then given as the geometric mean of the entailment probabilities of the constituent words. $Pr(g) = \\big(\\prod_{w\\in g} Pr(w)\\big)^{1/n}$. \nEntailed precision $P^{Ent}_n$ and entailed recall $R^{Ent}_{n}$ are computed by giving each $n$-gram $g$ a reward of 1 if it overlaps with the reference and a reward proportional to its table entailment probability otherwise. Formally, the entailed precision is given as:\n\\begin{align*}\n    P^{Ent}_n = \\frac{\\sum_{g\\in p} [Pr(g) + (1-Pr(g)).\\mathbbm{1}(g \\in r) ]}{\\sum_{g\\in p}1}\n\\end{align*}\nEntailed recall computes recall against the reference $R^{Ent}_{n}(r)$ and the table $R^{Ent}_{n}(t)$ separately and considers the weighted geometric average of them (with a $\\lambda$ weight parameter) as follows:\n\\begin{align*}\n    R^{Ent}_{n} = R^{Ent}_{n}(r)^{(1-\\lambda)}.R^{Ent}_{n}(t)^\\lambda\n\\end{align*}\nThe $P^{Ent}_n$ and $R^{Ent}_{n}$ for various $n$'s are aggregated using the geometric mean to get the combined precision $P^{Ent}$ and recall $R^{Ent}$. Finally the PARENT score for each instance is the F-score of the combined precision and recall scores. \n\\begin{align*}\n    PARENT = \\frac{2P^{Ent}R^{Ent}}{P^{Ent} + R^{Ent}} \n\\end{align*}\nTo compare with corpus-level metrics \\textit{such as BLEU}, the corpus-level PARENT score is given as the average of instance level PARENT scores.", "cites": [2372], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a detailed explanation of ROUGE-C and PARENT, integrating concepts from the cited paper and placing them within the broader context of context-dependent word-based metrics. It offers some critical analysis by pointing out the limitations of reference-based metrics and the role of entailment probabilities. However, it primarily focuses on describing the mathematical formulations rather than generalizing these ideas into broader patterns or principles in NLG evaluation."}}
{"id": "3fa1f48e-c91d-47c2-8ce5-88964ee7e2be", "title": "End-to-end Trained metrics", "level": "subsubsection", "subsections": [], "parent_id": "fbb9137e-8734-4411-a59b-02324267ff14", "prefix_titles": [["title", "A Survey of Evaluation Metrics Used for NLG Systems"], ["section", "Context-dependent metrics"], ["subsection", "Trained metrics"], ["subsubsection", "End-to-end Trained metrics"]], "content": "}\n\\hfill\\\\\n\\textbf{LEIC} \\footnote{There was no explicit name provided for this metric by the authors. The `LEIC' acronym has been adopted from the paper's title, `Learning to Evaluate Image Captioning' by many later works that refer to this model including }:  observe that the commonly adopted metrics for image captioning evaluation such as CIDEr, METEOR, ROUGE and BLEU mainly focus on the word-overlap between the hypothesis and reference captions and do not correlate well with human judgements. Although SPICE constructs scene graphs from the hypothesis and reference in order to compare semantic similarity, it fails to capture the syntactic structure or fluency of a sentence. For instance, it can be gamed with repetitive sentences as shown in . Moreover, all these rule-based metrics rely solely on similarity between candidate and reference captions, ignoring the image. LEIC is a discriminative evaluation metric that is trained to distinguish between the human and machine-generated captions, by taking the image into account. The image is encoded using ResNet pretrained on ImageNet with fixed weights, and the candidate as well as the reference captions are encoded using an LSTM-based sentence encoder. These encoded feature vectors are combined into a single vector in two different ways which were found to yield comparable results empirically. The first method uses a concatenation of all the vectors followed by a MultiLayer Perceptron (MLP):\n\\begin{align*}\n    v = ReLU(W\\cdot \\text{concat}([i;r;h]) + b)\n\\end{align*}\nwhere ReLU (Rectified Linear Units) is an activation function given as $ReLU(x) = max(x,0)$, and $W,b$ are parameters. \nThe second method is to concatenate the image and reference first, and then combine it with the candidate caption using `Compact Bilinear Pooling' , which has been shown to effectively combine the heterogeneous information of image and text . \nThe final classifier is trained on the combined feature vectors using a softmax classifier with the cross-entropy loss function. COCO dataset  is used for training and machine generated captions are obtained using 3 image captioning models proposed in . Further, in order to enable the model to identify pathological captions, data augmentation is performed by (i) randomly sampling captions, (ii) permuting the caption word order and (iii) randomly replacing words in the captions. These are explicitly added as negative examples during training.\\\\\n\\\\\n\\textbf{ADEM} (Automatic Dialogue Evaluation Model): ADEM  is trained to evaluate responses, given the dialogue context and a reference response, on a scale of 1 to 5. For this, the authors collect human ratings on a variety of responses for contexts from the Twitter corpus . These responses are obtained from dialogue models  as well as humans, and thus contain a mix of good and bad responses. This forms the training data where each instance contains the context, the reference response, the proposed response (which could be human generated or system generated) and the score assigned to the proposed response by human evaluators.\nADEM first encodes the context, proposed response and reference response using a Hierarchical-RNN  encoder consisting of utterance-level and context-level RNNs. These vector representations of the dialogue context $\\overrightarrow{c}$, reference response $\\overrightarrow{r}$, and proposed response $\\overrightarrow{p}$ are used to compute the evaluation score as follows:\n\\begin{equation*}\n    \\mbox{ADEM\\ Score}(c,r,p) = (\\overrightarrow{c}^T .M. \\overrightarrow{p} + \\overrightarrow{r}^T. N. \\overrightarrow{p} - \\alpha)/ \\beta\n\\end{equation*}\nwhere $M$, $N \\in \\mathbb{R}^{n \\times n}$ are learned matrices, and $\\alpha, \\beta$ are scalar constants used to re-scale scores in the closed range $[1, 5]$. \nThe model is trained to minimize the squared error between the model predictions and the human scores with L2-regularization.\n\\begin{align*}\n\t\\mathcal{L}=\\sum_{i=1:K}[score(c_i,r_i,p_i)-human_i]^2+\\gamma||\\theta||_2\n\\end{align*}\n\\\\\n\\textbf{RUBER} (Referenced metric and Unreferenced metric Blended Evaluation Routine ): RUBER is another RNN-based dialogue evaluation model like ADEM. However, while ADEM requires human scores on the responses, which are difficult to obtain at large-scale, RUBER proposes a slightly different approach to use unlabelled data. Specifically, RUBER uses a combination of a referenced metric and an unreferenced metric to score the dialogue response on a scale 0 to 1. Here, the term ``referenced'' indicates the need for a reference response whereas ``unreferenced'' means that only the context and the proposed response are considered.\nThe referenced metric computes the similarity of the reference and proposed response using a modified variant of the vector extrema which is referred to as vector pooling. More specifically, the maximum and minimum values in each dimension are chosen from all the embeddings of the words in a sentence. The closeness of a sentence pair is measured using the cosine similarity of the concatenated max and min-pool vectors.\nFor the unreferenced metric, a model consisting of bidirectional gated recurrent units (GRUs) as encoders is learnt. The context and proposed response are passed through the GRUs and the last states of both directions are concatenated to obtain the context and response embeddings $\\overrightarrow{c}$ and $\\overrightarrow{p}$. These embeddings are concatenated along with a `quadratic feature' defined as $\\overrightarrow{c}^TM\\overrightarrow{p}$, where $M$ is a parameter matrix. Finally a multi-layer perceptron is used to predict a scalar score that measures the relatedness between the dialogue context and a given response. \n\\begin{align*}\n    s_U(c,p) = MLP(\\overrightarrow{c};\\ \\overrightarrow{c}^TM\\overrightarrow{p};\\ \\overrightarrow{p})\n\\end{align*}\nTo train the unreferenced metric, the authors adopt negative sampling strategy (that is, select random responses belonging to other contexts) to get negative examples or invalid responses for a given context. The model is trained on Chinese dialogue data with the Hinge loss function given by: \n\\begin{align*}\n    J = \\max \\{0, \\Delta - s_U(c,p^+) + s_U(c,p^-) \\}\n\\end{align*}\nwhere $\\Delta$ is a threshold parameter that recommends the score of a positive response $p^+$ to be larger than that of a random negative response $p^-$ by at least a margin $\\Delta$.\nFinally to get the RUBER score, the referenced and unreferenced metric scores are first normalized to be in the range [0,1] and then combined using an aggregate function such as min, max, geometric averaging or arithmetic averaging. The authors show that all the different aggregate functions produce similar results.\\\\\n\\\\\n\\textbf{SSREM} (Speaker Sensitive Response Evaluation Model  ): The SSREM metric follows the strategy of RUBER to combine a reference-based score and an unreferenced score. SSREM formulation can be compactly written as:\n\\begin{align*}\n    SSREM(c,r,p) = h(f(c,p),g(r,p))\n\\end{align*}\nwhere $f(c,p) = tanh(\\overrightarrow{c}^TM\\overrightarrow{p})$ is trained to score the relevance of response $p$ to context $c$ with a parameter matrix $M$. $\\overrightarrow{c}$ and $\\overrightarrow{p}$ are obtained using vector averaging on the GloVe embeddings  of the words in the context and response respectively. The function $g(r,p)$ measures the similarity between the proposed response $p$ and reference response $r$ using sentence mover's similarity  with Elmo embeddings . The $h$ function combines these two scores using arithmetic averaging.\nHowever, unlike RUBER, SSREM uses `speaker sensitive samples', instead of one positive sample and one random negative response sample for each context. The speaker sensitive samples are obtained by considering additional speaker-related information while probing for negative samples, in order to obtain responses that have varying degrees of similarity with the reference response. In decreasing order of similarity as hypothesized by the authors, the samples are obtained using utterances from:\\\\ (i) the same speaker in the same conversation\\\\  (ii) the same speaker in a different conversation but with the same partner\\\\\n(iii) the same speaker in a different conversation with a different partner\\\\\n(iv) a random other speaker\\\\\nThese cases are empirically verified to be more challenging to distinguish than the randomly sampled negatives. The authors hence train the $f$ function of SSREM to score the reference response higher than all of these negative cases by formulating the loss function as:\n\\begin{align*}\n    -\\sum_{c}\\log\\frac{\\exp (f(c,r))}{\\sum_{p \\in P} \\exp(f(c,p))}\n\\end{align*}\nwhere $P$ is the set of all proposed responses for context $c$ which contains the reference response $r$ along with the negative samples obtained from the above four scenarios ($P$ is thus a set containing the reference response and the four hard negative samples explained above). SSREM is trained using the Twitter corpus since it contains several conversations from each speaker, which is necessary to obtain the speaker sensitive samples. \\\\\n\\\\\n\\textbf{Transformer based metrics}\\\\\n\\textbf{RUBER with BERT embeddings:}  propose enhancing RUBER with contextualized embeddings. The modification to the referenced part of RUBER is to simply replace the word2vec embeddings with BERT embeddings. The unreferenced model's architectural changes include (i) replacing the Bi-RNN unit which encodes the word2vec embeddings with a pooling unit which aggregates the BERT embeddings of the words and (ii) replacing the MLP network which outputs a single unreferenced score in RUBER with a binary MLP classifier that assigns label 1 to positive responses and 0 to negative responses.\nUnlike RUBER, the authors use the cross entropy loss as the training objective. Note that the BERT embeddings are not finetuned in this case and only the weights of the MLP classifier are learnt by training on the DailyDialog dataset.\\\\ \n\\\\\n\\textbf{MaUde} (Metric for automatic Unreferenced dialogue evaluation) :  propose MaUde to score dialogue responses without any need for reference responses. MaUde obtains the BERT encodings for each utterance $u_i$ in the context, which are then downsampled (to reduce the dimensions) using a learnt matrix $D_g$. These downsampled BERT representations $h_{u_i}$ for every utterance $u_i$ are sequentially passed through a bidirectional-LSTM (BiLSTM). The BiLSTM hidden states are combined using max-pooling to get the context representation which is then transformed to the same vector space as the encoded candidate response vector using weight matrix $W$. This transformed context encoding $\\overrightarrow{c}$ is combined with the response encoding $\\overrightarrow{p}$ and fed to a sigmoid classifier to produce the MaUde score.\n\\begin{align*}\n        \\overrightarrow{u_i} &= D_g f_E^{BERT}(u_i)\\\\\n    h_{u_{i+1}}' &= BiLSTM(\\overrightarrow{u_i},h_{u_i}')\\\\\n    \\overrightarrow{c} &= W.pool_{t \\in \\{u_1,..,u_{n-1}\\}}(h_t')\\\\\n    \\overrightarrow{p} &= D_g f_E^{BERT}(p)\\\\\n    score(c, p) &= \\sigma(concat([\\overrightarrow{p};\\overrightarrow{c};\\overrightarrow{p}*\\overrightarrow{c};\\overrightarrow{p}-\\overrightarrow{c}]))\n\\end{align*}\nwhere $f_E^{BERT}$ is the BERT-encoder that is used to encode each utterance, including the candidate response and $h_{u_i}'$ is the hidden representation of $u_i$ in the BiLSTM. The MaUde score is in the range [0,1]. \nMaUde is trained on PersonaChat, an open domain chit-chat style dataset. MaUde is trained using two types of negative responses: (i) syntactic negative, and (ii) semantic negatives responses.  Syntactic negative responses are obtained using the following strategies: shuffling the word order, dropping out words randomly, and repeating words in the reference response. Similarly, semantic negatives are obtained by (i) random sampling responses from other dialogues, (ii) using a random response generated for another context by a pretrained seq2seq model on the dataset, and (iii) using a paraphrase of a random negative response, obtained via back-translation . \nAdditionally, a back-translation of the correct (reference) response is used as another positive response.\\\\\n\\\\\n\\textbf{RoBERTa-eval:}  utilize the contextualized text embeddings produced by RoBERTa model to encode the dialogue context and proposed response into a single vector $\\overrightarrow{d}$. An MLP classifier with sigmoid activation is used to obtain a score between 1 to 5.\\\\\n\\begin{align*}\n    \\overrightarrow{d} &= RoBERTa([c;p];\\phi)\\\\\n    \\text{RoBERTa-eval}(c,p) &= 4. MLP(\\overrightarrow{d};\\theta) + 1\n\\end{align*}\nwhere RoBERTa's parameters $\\phi$ can be finetuned and the MLP's parameters $\\theta$ are learnt during training. Further, scaling is done to keep the score in the range of 1 to 5. \nROBERTa-evaluator is trained on DailyDialog dataset  using human annotations on response quality. To get a variety of responses of different quality, the authors use random negative sampling, and also obtain responses from generative models proposed in .\n    \\centering\n    \\begin{tabular}{c|c|c|c|c|c|c|c}\n         Metric & MT & AS & DG & IC  & QA & D2T & QG\\\\\n         \\hline\n         \\multicolumn{8}{c}{Context-free metrics}\\\\\n         \\hline\n         BLEU & \\checkmark & * & * & * & * & * & *\\\\\n         NIST & \\checkmark & * & * & * & * & *& *\\\\\n         METEOR & \\checkmark & * & * & * & * & *& *\\\\\n         ROUGE & * & \\checkmark & * & * & * & *& *\\\\\n         GTM & \\checkmark & * &  &  & * & &\\\\\n         CIDEr &  &  &  & \\checkmark & & &\\\\\n         SPICE &  &  &  & \\checkmark &  & &\\\\\n         SPIDer &  &  &  & \\checkmark & & &\\\\\n         WER-family & \\checkmark &  &  &  &  & &\\\\\n         chrF & \\checkmark & * &  & * &  & &\\\\\n         Vector Extrema & * & * & * & * & * & *& \\\\\n         Vector Averaging & * & * & * & * & * & *& \\\\\n         WMD & * & * &  & * &  & &\\\\\n         BERTr & * &  &  &  &  & &\\\\\n         BERTscore & \\checkmark &  & * & \\checkmark & * & &\\\\\n         MoverScore & \\checkmark & \\checkmark &  & \\checkmark &  & \\checkmark &\\\\\n         BEER & \\checkmark &  &  &  &  & & \\\\\n         BLEND & \\checkmark &  &  &  &  &  &\\\\\n         Q-metrics &  &  &  &  &  & & \\checkmark\\\\\n         Composite metrics &  &  &  & \\checkmark & & &\\\\\n         SIMILE & \\checkmark &  &  &  &  &  &\\\\\n         ESIM & \\checkmark &  &  &  &  &  &\\\\\n         RUSE & \\checkmark &  &  &  &  &  &\\\\\n         BERT for MTE & \\checkmark &  &  &  &  &  &\\\\\n         BLEURT & \\checkmark &  &  &  &  & \\checkmark &\\\\\n         NUBIA & \\checkmark &  &  & \\checkmark &  & & \\\\\n         \\hline\n         \\multicolumn{8}{c}{Context-dependent metrics}\\\\\n         \\hline\n         ROUGE-C &  & \\checkmark &  & &  & & \\\\\n         PARENT &  &  &  & &  & \\checkmark & \\\\\n         LEIC &  &  &  & \\checkmark &  & &\\\\\n         ADEM &  &  & \\checkmark &  &  & &\\\\\n         RUBER &  &  & \\checkmark &  &  & &\\\\\n         SSREM &  &  & \\checkmark &  &  & &\\\\\n         RUBER with BERT embeddings &  &  & \\checkmark &  &  & &\\\\\n         MaUde &  &  & \\checkmark &  &  & &\\\\\n         RoBERTa-eval &  &  & \\checkmark &  &  & &\\\\\n         \\hline\n    \\end{tabular}\n    \\caption{Automatic metrics proposed (\\checkmark) and adopted (*) for various NLG tasks }\n    \\label{tab:auto_metric_adoption}\n\\end{table}", "cites": [8385, 4229, 7454, 2001, 1107, 1113, 2025, 4228, 4226, 2027, 4232, 2032, 39, 2401, 7295, 4233, 2047, 7164], "cite_extract_rate": 0.6206896551724138, "origin_cites_number": 29, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section provides a coherent synthesis of multiple end-to-end trained metrics for NLG, linking LEIC, ADEM, RUBER, and SSREM by their goals, architectures, and training approaches. It critically evaluates their limitations and design choices, such as reliance on references or lack of syntactic capture. The section also abstracts to highlight broader trends, such as the shift from rule-based to learned metrics and the use of contextual and speaker-sensitive data."}}
{"id": "0919e658-af03-425a-ad84-bf07aaddd391", "title": "Studies criticising the use of automatic evaluation metrics", "level": "section", "subsections": [], "parent_id": "c120a577-e918-4f1d-894c-cc2b4adb602b", "prefix_titles": [["title", "A Survey of Evaluation Metrics Used for NLG Systems"], ["section", "Studies criticising the use of automatic evaluation metrics"]], "content": "\\label{sec:criticism}\nWhile automatic metrics have been widely used for evaluating a variety of NLG tasks, there has always been skepticism about their ability to replace human judgements. Indeed, there have been several studies which have criticised the use of such automatic metrics. These studies highlight the (i) poor correlation of these metrics with human judgements (ii) lack of interpretability in these metrics (ii) inherent bias in the metrics (iv) poor adaptability to a wider variety of tasks and (v) inability to capture subtle nuances in language. In this section we briefly summarise such studies which critically examine the use of automatic evaluation metrics for different tasks. Note that most of these studies were done in the context of BLEU and some of the early NLG metrics, such as, NIST, METEOR and ROUGE. This is simply because these metrics have been in use for many years and have been widely adopted even for newer NLG tasks (such as image captioning). As a result of their popularity, they also tend to get scrutinised more critically. \\\\\n\\textbf{Poor correlations: }\nOne recurring criticism of automatic metrics is that they correlate poorly with human judgements. One of the earliest studies in this direction computed the correlation between several automatic metrics and human judgements on fluency and adequacy\n. They found that BLEU, NIST, SSA (Simple String Accuracy) , Melamed's F-measure (i.e., GTM ), and LSA (Latent Semantic Analysis)   correlate negatively with human judgements on fluency. Further, these metrics showed moderate to less correlation with human adequacy scores. \nOver the years, similar poor correlations with human judgements have been reported for several metrics for various NLG tasks as summarized in Table \\ref{tab:critic_corrs}.\\\\\n\\begin{table}[h]\n    \\centering\n    \\begin{tabular}{C{0.7cm}|L{6.68cm}|c|L{5.58cm}|}\n         Work & Metrics & Task & Datasets on which poor correlation was observed\\\\ \\hline\n          & BLEU, TER, ROUGE, NIST, LEPOR, METEOR, CIDEr & DG & BAGEL , SF-HOTEL, SF-REST \\\\\n          & Smoothed-BLEU , TER, ROUGE-SU4, Meteor  & IC & Flickr8K, E\\& K\\\\\n          & ROUGE-1,2,L & AS & CNN/DailyMail \\\\\n          & BLEU, NIST, METEOR, ROUGE-L, CIDEr & D2T & E2E NLG dataset \\\\\n          & BLEU, ROUGE-L, NIST, METEOR& QG & SQuAD, WikiMovies, VQA\\\\\n          & ROUGE, CIDEr, METEOR, BLEU, CS & D2T & WikiBio \\\\\n          & BLEU, ROUGE, METEOR, VecSim &QA/AS& MS MARCO, CNN/DailyMail  \\\\\n          & BLEU, ROUGE, SMS, BERTScore & QA& NarrativeQA, SemEval, ROPES\\\\\n          & BLEU, ROUGE, METEOR, Greedy matching, vector extrema, vector avg & DG & Twitter, Ubuntu\\\\\n    \\end{tabular}\n    \\caption{Works showing poor correlation of various metrics with human judgements on various datasets \\MK{should we add one more column on task \\Ana{Done}}}\n    \\label{tab:critic_corrs}\n\\end{table}\\\\\nIn the context of Table \\ref{tab:critic_corrs}, it is worth mentioning that there is a high variance in the correlations reported for the same metric across different studies (as also observed in ). \nThis could be due to differences in the procedures followed to procure human ratings or metric-specific parameter settings used in different studies (which are not always thoroughly reported ). To alleviate the influence of such external factors, the WMT shared metrics task, which annually evaluates the metrics used for MT, standardizes the dataset and human evaluation setup. However, despite such careful standardisation there could still be issues. For example, in the context of the recent WMT-19 task,  study the 24 metrics that were submitted and \nshow that correlations reported are not reliable if all the translation systems to be ranked are good systems. \nIn , the authors further show that this is a generic problem encountered in any scenario which involves ranking a small set of systems that are all of similar capability.\nAlso, at the sentence level,  find that automatic evaluation metrics used for MT evaluation are better at distinguishing between high-quality translations compared to low-quality translations.\nOverall, many studies   re-confirm the findings that automatic metrics are reliable (with better correlations and less variations across studies) \nat the system-level  and less so at the sentence-level . \n\\\\\n\\\\\n\\textbf{Uninterpretability of scores: }\nAs discussed in section \\ref{sec:human_eval}, human evaluators are required to assign multiple scores to a given hypothesis where each score corresponds to a specific criteria (fluency, adequacy, coherence, relevance, thoroughness, \\textit{etc.}). However, automatic evaluation metrics assign a single score to a given hypothesis and it is often not clear which of the relevant criteria this score captures or corresponds to. Hence, these scores assigned by automatic metrics are difficult to interpret. For example, in the context of a summarization system, if a metric assigns a low score to a generated output then it is not clear whether this low score corresponds to poor fluency or poor informativeness or poor coherence.  \nIt is not surprising that within just 2 years from when BLEU and NIST were proposed, these metrics were criticized for their poor interpretability by . \n further demonstrate that an improvement in BLEU score is neither sufficient nor necessary to indicate progress in translation task thereby raising questions about what the score really stands for. In particular, they show BLEU score can be misleading since several permutations of the n-grams of a sentence would get the same score as the original sentence, even though not all of the permutations would be correct or sensible. \nIn other words, BLEU admits many spurious variants. It also penalizes correct translations if they substantially differ from the vocabulary of the references. Even the more recent evaluation metrics are uninterpretable as they just assign one score to the hypothesis as opposed to the different criteria used by humans for evaluating such systems.\n~\\\\\n\\textbf{Bias in the metrics: }\nSome studies have also shown biases in specific metrics. For example, BLEU is found to be favourably biased towards n-gram-based translation systems as opposed to rule-based systems .\nIn the context of more modern evaluation systems, it was found that GAN-based evaluators  have poor generalization. In particular, they are not good at evaluating systems different from the ones that they have been trained upon. \nSimilarly,  found that ADEM, which is used for evaluating dialogue responses, always assigns scores in a narrow range around the median, irrespective of whether the generated response is relevant or not. They further observe that the response encodings generated by ADEM are very close to each in the vector space with a very high conicity (\\textit{i.e.}, there is no clear separation between the encodings of relevant and irrelevant responses). So far such studies which reveal specific biases in a metric are limited to a few metrics as listed above and it would be interesting to check for some biases in the newer metrics proposed in the recent years. \n\\\\\n\\\\\n\\textbf{Poor adaptability across Tasks: }\nAs mentioned multiple times before, the criteria used for evaluating NLG systems vary across different tasks. As a result, the adoption of a metric proposed for one task for another task is not always prudent and has been criticized in many studies . A case in point is the poor choice of using n-gram based metrics such as BLEU, METOER, \\textit{etc.}, for evaluating dialog systems. These metrics check for n-gram based overlap between the reference and the hypothesis, which does not make sense in the context of a dialog system where widely varying responses are possible. In particular, a hypothesis may not have any n-gram overlap with the reference but still be a correct response for the given context.\n\\hfill \\\\\\\\\n\\textbf{Inability to capture all nuances in a task: }\nEven task-specific metrics are unable to account for all the nuances of the task. For example,  criticize the automatic metrics and human evaluations used for abstractive summarization stating that none of them check for factual inconsistencies in the summaries. Similarly,  discuss the lack of a reliable measurement of faithfulness in the context of Data-to-Text Generation. Even dialog specific metrics such as ADEM fail to account for the diversity in valid responses . \nSimilarly,  analyze the Hindi-English translation task and list the various issues and divergence in the language-pair that are not effectively captured by BLEU. These include lexical, categorical, pleonastic, and stylistic divergence.", "cites": [4234, 4225, 3572, 2372, 8755, 2364, 7796, 7797, 446, 439, 4226, 4224, 1966, 1140, 7533, 7795, 441, 4223, 1109], "cite_extract_rate": 0.4418604651162791, "origin_cites_number": 43, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple sources to construct a coherent narrative on the limitations of automatic NLG evaluation metrics. It critically analyzes issues such as poor correlations with human judgments, uninterpretability, bias, and poor adaptability, while also contextualizing these problems through references to standardized evaluations like WMT. The abstraction is strong, as it generalizes findings across different NLG tasks and highlights overarching challenges in metric design and interpretation."}}
{"id": "8bcfdcbd-9c04-4cee-bcc3-81a373149ac5", "title": "Issues and Influence of the Data and Human-evaluation setup", "level": "subsection", "subsections": [], "parent_id": "e7a46dfc-3172-4314-aa51-fad9681a1c7b", "prefix_titles": [["title", "A Survey of Evaluation Metrics Used for NLG Systems"], ["section", "Evaluating Evaluation Metrics"], ["subsection", "Issues and Influence of the Data and Human-evaluation setup"]], "content": "So far, in this section, we have mentioned that the most widely used technique for judging the effectiveness of a metric. We now discuss some studies which talk about how the data \nand/or the human evaluation setup can affect the correlation scores.  were among the earliest to show the shortcomings of correlation based studies in the context of MT, suggesting that human judgements are inconsistent and expensive. Instead they propose to validate how each metric ranks the reference and the n-best machine generated translations. They calculate  the  average  rank  of   the   references in the n-best list, and compute the ratio of  the  average  reference  rank  to  the  length  of the n-best list. This ratio is termed â€œORANGEâ€ (Oracle Ranking for Gisting Evaluation) and the smaller the ratio is, the better the automatic metric is.  This strategy works for MT systems which provide n-best translations.  compare the various automatic metrics and human-based metrics for translation and have demonstrated the effect of the quality of references on automatic metrics, as well as the subjectiveness of the human judges in human-based evaluations.   stress on the influence of corpus-quality on the correlations between human and automatic evaluation schemes, observing that large number of good quality references lead to better correlations.  show that references aren't always the perfect gold-standard that they're assumed to be by most automatic metrics, especially in the datasets collected automatically and heuristically.\n also assess the quality of the data and examine its characteristics such as the length of the references, words,  characters, or syllables per sentence, etc. in the dataset and find that these measures influence the correlations. \n call attention to potential problems in the datasets including layout bias in news datasets and noise in any web-scraped datasets.  observe eliminating outliers (and following other relevant bias and noise eliminating strategies) can lead to more reliable and robust correlations to compare the various metrics. \\\\\n critically examine whether the assessment of the MT evaluation metrics is robust and if monolingual evaluators can be relied upon for human evaluations as opposed to experts or biligual evaluators. \n note the highly varied ranking outcomes while using automatic versus human evaluations. They particularly observe that with most automatic metrics, the fluency of the language can be reliably determined and that seq2seq models perform well on fluency. However in terms of adequacy and correctness, on which the automatic metrics do not agree with human evaluations, seq2seq NLG models perform worse than rule-based models, but they still manage to procure high scores on the automatic metrics. \n also report the effects of having good quality (low variance) human judgements on the correlations of the automatic metrics to optimize evaluation costs.", "cites": [4225, 3572, 8755, 2372], "cite_extract_rate": 0.4, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section shows moderate insight by discussing how data and human evaluation setups influence metric correlation. It integrates several papers to highlight issues such as reference quality, evaluator bias, and noise. While it makes some critical points, such as the limitations of human and automatic metrics, the synthesis remains somewhat fragmented and does not offer a fully unified framework. Abstraction is limited to general observations about the impact of data characteristics on evaluation outcomes."}}
{"id": "a8f52754-d811-44d3-ad49-f6939e246eb2", "title": "Methods to Evaluate Robustness of Metrics", "level": "subsection", "subsections": [], "parent_id": "e7a46dfc-3172-4314-aa51-fad9681a1c7b", "prefix_titles": [["title", "A Survey of Evaluation Metrics Used for NLG Systems"], ["section", "Evaluating Evaluation Metrics"], ["subsection", "Methods to Evaluate Robustness of Metrics"]], "content": "Automatic metrics have been analysed for adversarial scenarios or corner cases in which they fail. This is important to understand the reliability and robustness of the metrics, and to make progress in improving them. In this section we briefly discuss some of the techniques used to construct adversarial examples. Most of these are automated methods based on text manipulation.  \nOne of the earliest works in this direction  demonstrated a permutation attack on BLEU. For instance, by splitting the translation at bigram mismatch points, the resulting units can be reordered without a causing a change to the BLEU2 score. That is, while the permuted sentence need not necessarily make sense, BLUE assigns the same score to it as long as the number of matching n-grams doesn't change. \nMore recently,  provide a framework to create incorrect captions by replacing people (or the scene) in the caption with other people (or scenes), or switching the roles of people in the same caption, or even using other captions from the dataset that share people or scene with a given caption. To check for robustness of the image caption evaluation metrics,  adopt this method to create distracted versions of image captions/descriptions for a sample image. They check whether the metrics correctly identify a changed scene or person. \n design a set of synthetic scenarios to check the sensitivity of trained evaluators on jumbling/reversing the word order, removing punctuation or stopwords, retaining or removing specific combinations of the parts of speech, etc.  compute `Evaluator Reliability Error' (ERE) on trained metrics based on their performance in simplified scenarios including passing the ground-truth as both the hypothesis and reference, similarly passing the candidate sentence as both, and passing random unrelated text as hypothesis. \n\\\\\nAnother approach to create hard or adversarial examples is to check performance on other related tasks or sub-tasks.  \nThe authors of BERTscore  evaluate the robustness of different metrics by considering their performance on a parallel task: adversarial paraphrase detection. Since most metrics rely upon comparison of the hypothesis with the reference, it is useful to determine if the metrics can assign lower scores to these adversarial paraphrases as opposed to the corresponding original paraphrases.", "cites": [7797, 4226, 4224], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates key methods from the cited papers to discuss how evaluation metrics for NLG can be tested for robustness. It synthesizes different adversarial testing strategies, such as permutation attacks and synthetic scenarios, and highlights their relevance in identifying metric weaknesses. While it offers some critical perspective by pointing out limitations (e.g., BLEU's insensitivity to word order), the critique is not deeply nuanced and comparisons between approaches are limited."}}
{"id": "099c5dd3-134e-4ff4-8e8a-e5fb06414de6", "title": "Datasets", "level": "section", "subsections": [], "parent_id": "c120a577-e918-4f1d-894c-cc2b4adb602b", "prefix_titles": [["title", "A Survey of Evaluation Metrics Used for NLG Systems"], ["section", "Datasets"]], "content": "In this section, we briefly describe the commonly used datasets which provide human scores for training/evaluating NLG metrics. \n\\noindent \\textbf{WMT Shared Metric Task:} The WMT shared Metric task is an annual task on automatic evaluation metrics for machine translation since 2008. The task involves automatically evaluating translations produced in the WMT shared translation task using the human reference translations. The training dataset for the task also includes human ratings for the pairs of sentences. For the years 2017, 2018, and 2019, the training sets contain 5,360, 9,492, and 147,691 records. The evaluation metrics are assessed based on correlations with human judgements at both the system and sentence level using either Pearson correlation or the custom Kendall's $\\tau$ as described in the previous section. \\\\\n\\noindent \\textbf{WebNLG:} The WebNLG challenge consist of converting data into natural language text. The data is a set of triples extracted from DBpedia and the text is a verbalisation of these triples. The organizers released the human judgements for 4,677 sentence pairs that span 9 systems. Human judgements are made based on 3 criteria \\textit{viz, } fluency, semantics, and grammar. \\\\\n\\noindent \\textbf{Human Judgements from Persona Chat:}   conducted a large-scale human evaluation study to measure the effect of various controllable parameters in neural text generation models on the PersonaChat task . The study involved human-system conversations from 28 model configurations and also had human-human conversations for comparison. Human annotators interacted with a system/another human and rated the conversational quality on a scale of 1-3 based on 8 criteria \\textit{viz.} avoiding repetition, interestingness, making sense, fluency, listening, inquisitiveness, humanness and engagingness.", "cites": [2001], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of datasets used for evaluating NLG metrics, with minimal synthesis of ideas across the cited papers. It lacks critical analysis or evaluation of the datasets' strengths and limitations. Additionally, it does not abstract beyond individual datasets to highlight broader trends or principles in human evaluation for NLG."}}
{"id": "8550443a-e5d6-4e70-ac75-ecb93c3b4bf6", "title": "Recommendations (possible future research directions)", "level": "section", "subsections": [], "parent_id": "c120a577-e918-4f1d-894c-cc2b4adb602b", "prefix_titles": [["title", "A Survey of Evaluation Metrics Used for NLG Systems"], ["section", "Recommendations (possible future research directions)"]], "content": "Based on the above survey of the field, we would like to make the following recommendations: \\\\\n\\noindent \\textbf{Developing a common code base for evaluation metrics:} As the number of metrics being proposed continues to grow, it is very important that we, as a community, develop a common code base for these metrics. Such a common code base would have multiple benefits: (i) researchers would be able to easily evaluate their NLG systems using a much wider range of metrics as opposed to relying on a very few metrics whose code is easily available (e.g., most MT papers report only BLEU scores even though several studies show that it does not correlate well with human judgements) (ii) results reported across papers will be comparable and not differ due to implementation issues (iii) researchers proposing new evaluation metrics will be able to compare easily against existing metrics while not being bothered by implementation issues (currently, many new metrics tends to compare their results with only those metrics whose code is easily available) and (iv) researchers will be able to critically examine existing metrics if the code is readily available leading to a more careful scrutiny of these metrics (e.g., they could perform white-box attacks or evaluate the metrics using carefully crafted adversarial examples ).\n\\noindent \\textbf{Building datasets containing human judgements:} Development of automatic evaluation metrics relies on the availability of datasets containing tuples of the following form: context, reference response, proposed response and human scores for the proposed response. The proposed response could either be generated by a human, a synthetically created variant of a reference response (e.g., dropping stop words or replacing synonyms, etc) or generated by an NLG system. Each of these proposed responses would then be evaluated by a human and assigned a score(say, on a scale of 0 to 1). Such a dataset would thus contain a mix of good and bad responses with human scores for them. This could act as data for training and evaluating automatic metrics for the given task. Further, given the varied criteria which need to be satisfied across different tasks (as discussed in section \\ref{sec:human_eval}) such datasets should contain multiple scores wherein each score corresponds to a specific criteria (fluency, adequacy, coherence, \\textit{etc}). Despite the evolving interest in this field, there is still a scarcity of such datasets for multiple tasks and languages. The shared task on evaluation metrics at the annual WMT conference  is a good example of creating such standardised datasets. Such shared tasks with standardised datasets for multiple tasks and languages would enable rapid progress in this field.\n\\noindent \\textbf{Developing task-specific context-dependent metrics:} As is evident from the taxonomy diagram, Figure \\ref{fig:taxo}, \nmost of the existing metrics are context-free and thus task independent (although it is possible that the metrics were proposed for a specific task but given that they ignore the context, they can be easily adopted for most NLG tasks). Even the context-dependent metrics have largely focused on the task of dialog evaluation. This is clearly a problem, as context plays a very important role in evaluating whether the generated hypothesis is correct. As illustrated with the help of examples, in section \\ref{sec:nlg_tasks} and \\ref{sec:human_eval}, context is required to ensure that the generated hypothesis is coherent, factually consistent and relevant to the context. In the absence of context, an evaluation metric can only check for the similarity between the given set of references and the generated hypothesis. This is inadequate for tasks such as dialogues generation, summarization, question answering, data2text generation, \\textit{etc.} where a wide variety of responses are possible and word/semantic overlap with the set of references is neither sufficient nor necessary. \n\\noindent \\textbf{Developing more interpretable metrics:} This goes hand in hand with the two recommendation made above (i) collecting criteria specific human judgements (fluency, adequacy, coherence, informativeness, etc) and (ii) developing task specific metrics. Most existing metrics assign a single score to the hypothesis which is clearly in contrast to how human evaluations are done. In particular, humans are asked to assign multiple scores to a hypothesis with each score corresponding to a specific criteria. It makes sense to have a similar expectation from an automated evaluation metric. This is simply because a single score provided by current metrics is often not actionable. For example, if the overall BLEU score of a MT system is 0.3 then should the developer of the system work on improving the fluency of the system (language model) or the adequacy of the system (translation model) or both. Contrast this with an evaluation system which assigns a separate score for fluency and adequacy. The output of such an evaluation system would be more interpretable and hence provide clear future directions to the researchers and/or developers of the NLG system. This would also help end-users or reviewers judge which system is better. For example, while comparing two question generation systems one might prefer a system which scores low on fluency but high on answerability as this would mean that while the questions generated by the system are not  grammatically perfect, they still convey the essence of the question (e.g., ``who director of Titanic''?). However, in the absence of such fine-grained scores it is hard to interpret and compare the relative strengths/shortcomings of multiple systems. Some very recent works in the direction of interpretability of metrics are proposed in .\n\\noindent \\textbf{Creating robust benchmarks for evaluating evaluation metrics:} While some of the early metrics, such as BLEU, NIST, METEOR, ROUGE, \\textit{etc.}, have been critically examined across a wide variety of tasks , many of the recently proposed evaluation metrics have not yet been examined critically. To facilitate such studies, there should be focus on creating adversarial evaluation benchmarks which critically examine the robustness of these metrics. For example, one could create datasets which contain adversarially crafted responses for a given context (say, a summary which has a high word/entity overlap with the passage but is still irrelevant or factually incorrect). Such adversarial evaluation has helped in identifying gaps in other NLP tasks such as QA  and has also shown promise in identifying shortcomings of dialog evaluation metrics . In addition to adversarial evaluations, such studies should also focus on identifying specific biases in the proposed evaluation metrics. For example, GAN based evaluators  are biased towards systems that they have been trained on. Similarly, it is possible that some dialog evaluation metrics have certain biases induced by the data they are pretrained on. For example, if a metric is pretrained on Reddit or social media conversations then it may be biased against more formally written conversations (which have a slightly different word usage and syntactic structure). Lastly, it is also important to closely examine the evaluation measures such as correlation which are used for evaluating such evaluation metrics as recent studies have shown that such measures can be unreliable in certain situations .", "cites": [7797, 4226, 4235, 4223], "cite_extract_rate": 0.3076923076923077, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section demonstrates high insight quality by synthesizing multiple cited works into a coherent argument for future research directions. It critically analyzes the limitations of current evaluation practices, such as the over-reliance on BLEU and the need for interpretability and robustness. The analysis generalizes beyond individual papers to propose overarching principles and frameworks for improving evaluation metrics in NLG."}}
{"id": "0d4446bc-d008-4d07-be1b-a9e354cdba36", "title": "Human Evaluation Setup", "level": "subsection", "subsections": [], "parent_id": "05fef612-65e3-4a62-8ec1-dbe01e860fe7", "prefix_titles": [["title", "A Survey of Evaluation Metrics Used for NLG Systems"], ["section", "Conclusion"], ["subsection", "Human Evaluation Setup"]], "content": "Depending on the budget, availability of annotators, speed and required precision, different setups have been tried for evaluating NLG systems. The different factors to consider in such an evaluation setup are as follows:\n\\begin{itemize}\n    \\item \\textbf{Type of evaluators:} The evaluators could be experts , crowdsourced annotators , or even end-users  depending on the requirements of the task and the goal of the evaluation. For example, for evaluating a translation system one could hire bilingual experts (expensive) or even monolingual experts (relatively less expensive). The monolingual experts could just compare the output to an available reference output whereas with bilingual experts such a reference output is not needed. Further, a bilingual expert will be able to better evaluate where the nuances in the source language are accurately captured in the target language. If the speed of evaluation is the primary concern then crowd-sourced workers can also be used. In such a situation, one has to be careful to provide very clear guidelines, vet the workers based on their past records, immediately weed out incompetent workers and have an additional layer of quality check (preferably with the help of 1-2 expert in-house annotators). Clearly such crowdsourced workers are not preferred in situations requiring domain knowledge - e.g., evaluating an NLG system which summarises financial documents. For certain tasks, such as dialogue generation, it is best to allow end-users to evaluate the system by engaging in a conversation with it. They are better suited to judge the real-world effectiveness of the system. \n    \\item \\textbf{Scale of evaluation:} The annotators are typically asked to rate the output on a fixed scale, with each number corresponding to a specific level of quality, called the Likert scale . In a typical Likert scale the numbers 1 to 5 would correspond to Very Poor, Poor, Okay, Good and Very Good. However, some works  have also experimented with a dynamic/movable continuous scale that can allow the evaluator to give more nuanced judgements. \n    An alternate setting asks humans to assign a rating to the output based on the amount of post-editing required, if any, to make the output acceptable . The evaluators could also be asked for binary judgements rather than a rating to indicate whether a particular criteria is satisfied or not. This binary scale is sometimes preferred over a rating scale, which usually contains 5 or 7 rating points, in order to force judges to make a clear decision rather than give an average rating (by choosing a score at the middle of the scale) . By extension, any even-point rating scale could be used to avoid such indecisiveness.\n    \\item \\textbf{Providing a reference and a context:} In many situations, in addition to providing the output generated by the system, it is helpful to also provide the context (input) and a set of reference outputs (if available). However, certain evaluations can be performed even without looking at the context or the reference output. For instance, evaluating fluency (grammatical correctness) of the generated sentence does not require a reference output. References are helpful when the evaluation criteria can be reduced to a problem of comparing the similarity of information contained in the two texts. For example, in most cases, a generated translation can be evaluated for soundness (coherence) and completeness (adequacy) by comparing with the reference (without even looking at the context). However, for most NLG tasks, a single reference is often not enough and the evaluator may benefit from looking at the context. The contexts contains much more information which is difficult to be captured by a small set of references. In particular, referring to the examples provided for ``Abstractive Summarisation'', ``Image Captioning'' and ``Dialogue Generation'' in Table \\ref{tab:examples_nlg}, it is clear that it is difficult for the evaluator to do an accurate assessment by only looking at the generated output and the providing references. Of course, reading the context adds to the cognitive load of the evaluator but is often unavoidable. \n    \\item \\textbf{Absolute v/s relative evaluation :} The candidate output could be evaluated individually or by comparing it with other outputs. In an individual output evaluation, the candidate is provided an absolute rating for each desired criteria. On the other hand, in a comparison setup, an annotator could either be asked to simultaneously rate the multiple outputs (from competing systems)  or be asked to preferentially rank the multiple outputs presented . \n    This could also just be a pairwise comparison  of two systems. In such a setup, \n    the two systems are compared based on the number of times their outputs were preferred (wins), not preferred (losses), and equally preferred (ties).\n    \\item \\textbf{Providing Rationale :} The evaluators might additionally be asked to provide reasons for their decisions, usually by highlighting the corresponding text that influenced the rating . Such fine-grained feedback can often help in further improving the system. \n\\end{itemize}\nIrrespective of the setup being used, typically multiple evaluators are shown the same output and their scores are then aggregated to come up with a final score for each output or the whole system. The aggregate can be computed as a simple average or a weighted average wherein each annotator is weighted based on his/her past performance or agreement with other annotators . In general, it is desired to have a high inter-annotator agreement (IAA), which is usually measured using Cohen's Kappa or Fleiss Kappa co-efficient or Krippendorffâ€™s alpha.  \nAlternatively, although not popularly, IAA could be measured using Jaccard similarity, or an F1-measure (based on precision and recall between annotators) . \nAchieving a high-enough IAA is more difficult on some NLG tasks which have room for subjectivity . A lower IAA can occur due to (i) human-error (ii) inadequacy of the guidelines or setup (iii) ambiguity in the text .  \nTo enhance IAA,  find that asking the evaluators to highlight the portion of the text that lead to their decision or rating helps in getting better agreement. Alternatively,  arrange for a discussion between the annotators after the first round of evaluation, so as to mutually agree upon the criteria for the ratings.\nTo get a better IAA and hence a reliable evaluation, it is important that the human evaluators be provided with clear and sufficient guidelines. These guidelines vary across different NLG tasks as the criteria used for evaluation vary across different tasks, as explained in the next subsection.", "cites": [4225, 3572, 8755, 2364, 4227, 4226, 4223, 4224, 2001, 2372], "cite_extract_rate": 0.4, "origin_cites_number": 25, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes ideas from multiple papers to discuss key factors in human evaluation setups for NLG systems, such as evaluator type, scale, reference provision, and inter-annotator agreement. It includes critical points about the limitations of automatic metrics and the importance of clear guidelines and context for reliable evaluation. The analysis generalizes beyond individual papers, highlighting broader principles and challenges in human evaluation across various NLG tasks."}}
{"id": "2a342c75-c8f2-4d9b-9de3-1eeea8e74033", "title": "Criteria used for Evaluating NLG systems ", "level": "subsection", "subsections": [], "parent_id": "05fef612-65e3-4a62-8ec1-dbe01e860fe7", "prefix_titles": [["title", "A Survey of Evaluation Metrics Used for NLG Systems"], ["section", "Conclusion"], ["subsection", "Criteria used for Evaluating NLG systems "]], "content": "Most human evaluations are based on checking for task fulfillment, \\textit{i.e.}, humans are asked to rate or compare the generated sentences (and the generating systems) to indicate how satisfactorily they meet the task requirements overall. However, evaluations can also be performed at a more fine-grained level where the various contributing factors are individually evaluated, \\textit{i.e.}, the generated text is assigned a separate rating or ranking based on each of the desired qualities, independent of the other qualities/criteria. \nOne such desired criteria is that the generated texts should have good `fluency'.\n\\textbf{Fluency} refers to correctness of the generated text with respect to grammar and word choice, including spellings. \nTo check for fluency in the generated output, the evaluators might be asked the question, ``How do you judge the fluency of this text?\" followed by a 5-point rating scale :\n1. Incomprehensible 2. Not fluent German 3. Non-native German 4. Good German 5. Flawless German.\nInstead of a 5-point scale, other scales with different quality ratings could be used: ``How natural is the English of the given sentence?'' 1. Very unnatural 2. Mostly unnatural 3. Mostly natural 4. Very natural . \nAnother possibility is to present multiple candidate sentences and ask the evaluator, ``Which of these sentences seems more fluent?\". The evaluator then indicates a preference ordering with ties allowed. \nFluency in the generated output is a desired criteria for all the NLG tasks. However, the comprehensive list of criteria used for evaluation varies across different tasks. Hence, we discuss the set of criteria for each task separately now. Note that we have already defined fluency and mentioned that it is important for all NLG tasks. Hence, we do not discuss it again for each task independently. Further, note that the set of criteria is not standardized and some works use slightly different criteria/ sub-categorizations for the same task. Often the difference is only in the label/term used for the criteria but the spirit of the evaluation remains the same. Thus, for the below discussion, we consider only the most prominently used criteria for each task. In the discussion below and the rest of the paper, we interchangeably refer to the output of an NLG system as the hypothesis. \n\\\\\\\\\n\\textbf{Machine Translation:} Here, bilingual experts are presented with the source sentence and the hypothesis. Alternatively, monolingual experts can be presented with the reference sentence and the hypothesis. For each output, they are usually asked to check two important criteria: fluency and adequacy of the hypothesis  as described below. \n\\begin{itemize}\n    \\item \\textbf{Adequacy:} The generated hypothesis should adequately represent all the information present in the reference. To judge adequacy a human evaluator can be asked the following question : How much of the meaning expressed in the reference translation is also expressed in the hypothesis translation? 1. None 2. Little 3. Much 4. Most 5. All\n\\end{itemize}\n~\\\\\n\\textbf{Abstractive Summarization:} Human evaluators are shown the candidate summary along with the source document and/or a set of references. The evaluators are typically asked to rate informativeness and coherence . \nAlternatively, in a more elaborate evaluation the evaluators are asked to check for fluency, informativeness, non-redundancy, referential clarity, and structure \\& coherence  as described below. \n\\begin{itemize}\n    \\item \\textbf{Informativeness:} The summary should convey the key points of the text. \n    For instance, a summary of a biography should contain the significant events of a person's life. We do not want a summary that only quotes the person's profession, nor do we want a summary that is unnecessarily long/verbose.\n    \\item \\textbf{Non-redundancy:} The summary should not repeat any points, and ideally have maximal information coverage within the limited text length.\n    \\item \\textbf{Referential clarity:} Any \\newA{intra-sentence or cross-sentence} references in the summary should be unambiguous and within the scope of the summary. \\newA{For example, if a pronoun is being used, the corresponding noun it refers to should also be present at some point before it in the summary. Also there should not be any ambiguities regarding the exact entity or information (such as a previous point) that is being referred to.} \n    \\item \\textbf{Focus:} The summary needs to have a focus and all the sentences need to contain information related to this focal point. For example, while summarising a news item about a Presidential debate, the focus of the summary could be the comments made by a candidate during the debate. If so, it should not contain irrelevant sentences about the venue of the debate.\n    \\item \\textbf{Structure and Coherence:} The summary should be a well-organized and coherent body of information, not just a dump of related information. Specifically, the sentences should be connected to one another, maintaining good information flow.\n\\end{itemize}\n~\\\\\n\\textbf{Question Answering:} Here, human evaluators are first presented with the question and the candidate answer to check if the answer is plausible . Subsequently, the context passage/image is provided to check whether the answer is correct and consistent with the context. Alternatively, since question answering datasets are usually provided with gold standard answers for each question, the judges might simply be asked to report how closely the candidate answer captures the same information as the gold standard answer. The important criteria used for QA are fluency and correctness. \n\\begin{itemize}\n    \\item \\textbf{Correctness}: The answer should correctly address the question and be consistent with the contents of the source/context provided.\n\\end{itemize}\n~\\\\\n\\textbf{Question Generation:} Here, the candidate questions are presented to the evaluators along with the context (passage/image, etc.) from which the questions were generated. This may be accompanied with a set of candidate answers , although if they are not provided, even when available in the dataset, it is to avoid creating any bias in the evaluator's mind . The evaluators are then asked to consider the following criteria:\n\\begin{itemize}\n    \\item \\textbf{Answerability}: This is to determine whether the generated question is answerable given the context. A question might be deemed unanswerable due to its lack of completeness or sensibility, or even if the information required to answer the question is not found in the context. The latter could be acceptable in some scenarios where ``insufficient information'' is a legitimate answer (for example, if the questions are used in a quiz to check if the participants are able to recognize a case of insufficient information). However, generating too many such questions is undesirable and the evaluators may be asked to report if that is the case. \n    \\item \\textbf{Relevance}: This is to check if questions are related to the source material they are based upon. Questions that are highly relevant to the context are favoured. For example, a question based on common-sense or universal-facts might be answerable, but if it has no connection to the source material then it is not desired.\n    \\end{itemize}\n~\\\\    \n\\textbf{Data to Text generation:} \nHere, human judges are shown the generated text along with the data (\\textit{i.e.}, table, graph, etc). The criteria considered during human evaluation vary slightly in different works, such as WebNLG challenge , E2E NLG dataset  or WikiBio dataset .  \nHere, we discuss the more fine-grained criteria of ``faithfulness'' and ``coverage'' as used in  as opposed to the single criteria of ``semantic adequacy'' as used in .\n\\begin{itemize}\n    \\item \\textbf{Faithfulness:} It is important for the text to preserve the facts represented in the data. For example, any text that misrepresents the year of birth of a person would be unacceptable and would also be ranked lower than a text that does not mention the year at all.\n    \\item \\textbf{Informativeness or Coverage:} The text needs to adequately verbalize the information present in the data. As per the task requirements, coverage of all the details or the most significant details would be desired.\n\\end{itemize}\n~\\\\\n\\textbf{Automated Dialogue:} For evaluating dialogue systems, humans are typically asked to consider a much broader set of criteria. One such exhaustive set of criteria as adopted by , is presented below along with the corresponding questions provided to the human evaluators:\n\\begin{itemize}\n    \\item \\textbf{Making sense:} Does the bot say things that don't make sense?\n    \\item \\textbf{Engagingness:} Is the dialogue agent enjoyable to talk to?\n    \\item \\textbf{Interestingness:} Did you find the bot interesting to talk to?\n    \\item \\textbf{Inquisitivenes:} Does the bot ask a good amount of questions?\n    \\item \\textbf{Listening:} Does the bot pay attention to what you say?\n    \\item \\textbf{Avoiding Repetition:} Does the bot repeat itself? (either within or across utterances)\n    \\item \\textbf{Humanness:} Is the conversation with a person or a bot?\n\\end{itemize}\nOften for dialogue evaluation, instead of separately evaluating all these factors, the evaluators are asked to simply rate the overall quality of the response , or specifically asked to check for relevance of the response . For task-oriented dialogues, additional constraints are taken into consideration, such as providing the appropriate information or service, guiding the conversation towards a desired end-goal, \\textit{etc}. In open-domain dialogue settings also, additional constraints such as persona adherence , emotion-consistency , \\textit{etc}, are being used to expand the expectations and challenge the state-of-the-art.\n~\\\\\\\\\n\\textbf{Image Captioning:} The captions are presented to the evaluators along with the corresponding images to check for relevance and thoroughness .\n\\begin{itemize}\n    \\item \\textbf{Relevance:} This measures how well the caption is connected to the contents of the image. More relevance corresponds to a less-generic/more-specific caption that accurately describes the image. For example, the caption ``A sunny day'' is a very generic caption and can be applicable for a wide variety of images.\n    \\item \\textbf{Thoroughness:} The caption needs to adequately describe the image. Usually the task does not require a complete description of everything in the image but the caption must cover the main subjects/actions in the image and not miss out any significant details.\n\\end{itemize}\n~\\\\\nIn summary, the main takeaway from the above section is that evaluating NLG systems is a very nuanced task requiring multiple skilled evaluators and accurate guidelines which clearly outline the criteria to be used for evaluation. Further, the evaluation is typically much more than assigning a single score to the system or the generated output. In particular, it requires simultaneous assessment of multiple desired qualities in the output.\n\\begin{acks}\nWe would like to thank Department of Computer Science and Engineering, IIT Madras and Robert Bosch Center for Data Sciences and Artificial Intelligence, IIT Madras (RBC-DSAI) for providing us resources required to carry out this research. We would also like to thank Google for supporting Ananya Sai through their Google India Ph.D. Fellowship Program. We thank Juri Opitz, Nikita Moghe, Pritha Ganguly, and Tarun Kumar for their helpful comments on the paper.\n\\end{acks}\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{main}\n\\if 0\n\\appendix", "cites": [8755, 4225, 7454, 2001, 2364, 2025, 4228, 4223, 2372], "cite_extract_rate": 0.55, "origin_cites_number": 20, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of criteria used in evaluating different NLG tasks, but it lacks strong synthesis of the cited papers into a unified narrative. It briefly mentions some papers but does not deeply analyze or compare them. There is minimal abstraction, as it focuses on listing criteria without identifying broader principles or trends in the evaluation landscape."}}
