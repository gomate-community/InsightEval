{"id": "b2093b4c-a252-444e-8c99-5b104b46bdc0", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "109ef38f-b3f2-41dd-a765-4a141b2d87cf", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "Introduction"]], "content": "Since the beginning of time, language and communication has been central to human interactions. Therefore, translating between different languages has been pivotal in societal and cultural advancements.  \nMachine Translation (MT) was one of the first applications conceived to be solvable by computers; this vision was birthed by the ``translation memorandum'' presented by Warren Weaver, and the word-for-word translation system by IBM in 1954~.  \nConsequently, different techniques were developed to address the problem of Machine Translation, with a prominent being Statistical Machine Translation (SMT).  \nBecause the performance of the SMT system is directly impacted by the number of parallel sentence pairs available for training, a heavy emphasis has been placed on creating parallel datasets (also known as bitext) in addition to research on new MT techniques.\\\\\nIn 2013, the introduction of end-to-end neural encoder-decoder based MT systems saw a breakthrough with promising results, which soon got popularized as Neural Machine Translation (NMT). Currently NMT is the dominant technique in the community. \nHowever, it was quickly realized that these initial NMT systems required huge volumes of parallel data to achieve comparable results to that of SMT~. \nHigh resource language pairs (such as English and French) do not have dataset size concerns because researchers have created ample amounts of parallel corpora over the years\\footnote{The English-French corpus~ used contained 348 Million parallel sentences.}. \nHowever, the requirement of having large amounts of parallel data is not a realistic assumption for many of the 7000+ languages currently in use around the world and therefore is considered a major challenge for low-resource languages (LRLs)~. \nDue to economic and social reasons, it is useful to automatically translate between most of these LRLs, particularly for countries that have multiple official languages. Therefore, in recent years, there has been a noticeable increase in NMT research (both by academia and industry) that specifically focused on LRL pairs.\nDespite this emphasis, we are not aware of any literature review that systematically examines the NMT techniques tailored for LRL pairs.  Although there exists some work that discusses the challenges of using NMT in the context of LRL pairs~ and the application of specific techniques for LRL pairs~, \n none of them gives a comprehensive view of the available NMT techniques for LRL pairs. This makes it difficult for new researchers in the field to identify the best NMT technique for a given dataset specification. In addition, none of these surveys presents a holistic view of the NMT landscape for LRL pairs to derive insights on research efforts and current practices. \nThis survey aims to address the above shortcomings in the NMT research landscape for LRLs. More specifically, it provides researchers working on LRLs a catalogue of methods and approaches for NMT and identifies factors that positively influence NMT research on LRL pairs. To achieve these aims, we answer the following research questions: \n\\begin{enumerate}\n    \\item \\textbf{NMT Techniques:}  What are the major NMT techniques that can be applied to LRL pairs, and what are the current trends?\n    \\item \\textbf{Technique Selection:}  How to select the most suitable NMT technique for a given language?\n    \\item \\textbf{Future Directions:} How to increase research efforts and what are the future directives for NMT on LRL pairs?\n\\end{enumerate}\nTo answer the above questions, we first conducted a systematic analysis of the NMT techniques that have been applied for LRL pairs, and their progress (Section~\\ref{section:label-NMT-Techniques-for-Low-Resource-Languages}). \nSecondly, we critically analysed the applicability of these techniques for LRL pairs in practical terms. Based on our observations, we provide a set of guidelines for those who want to use NMT for LRL pairs to select the most suitable NMT technique by considering the size and type of the datasets, as well as the available computing resources (Section~\\ref{technique_selection}). Lastly, we conducted a comprehensive analysis of the amount of NMT research conducted for LRLs in the world (Section~\\ref{section:trend_analysis}).  Here, we note a strong correlation between the amount of NMT research per language and the amount of publicly available parallel corpora for that language. We also note the recent rise of regional level research communities that contributed to parallel dataset creation, and thus NMT for LRL pairs in turn. \nTherefore, our recommendations to advance the area of NMT for LRL pairs are to 1) create LRL resources (datasets and tools), 2) make computational resources and trained models publicly available, and 3) involve research communities  at a regional-level.  Based on our analysis of the existing NMT techniques, we also recommend possible improvements to existing NMT techniques that would elevate the development of NMT techniques that work well LRL pairs.", "cites": [4965, 303, 2339], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the historical development of MT, from early SMT to modern NMT, while incorporating insights from the cited works to frame the challenges of low-resource language translation. It provides some critical evaluation by pointing out the lack of comprehensive surveys and the practical limitations of existing approaches. The abstraction level is moderate, as it identifies broader patterns such as the correlation between parallel data availability and research efforts, though it does not introduce a novel theoretical framework."}}
{"id": "d5c14c13-e8a2-4183-916a-40ef1afbf3eb", "title": "Low-Resource Languages (LRLs)", "level": "subsection", "subsections": [], "parent_id": "a8eff206-d8a5-42d4-94ac-5f2cf5ed20dc", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "Background"], ["subsection", "Low-Resource Languages (LRLs)"]], "content": "\\label{LR-Definition}\nFor Natural Language Processing (NLP), a low-resource problem can arise mainly due to the considered languages being low-resourced, or the considered domains being low-resourced~. In this paper, we focus on LRLs only. \\\\\nResearchers have attempted to define LRLs by exploring various criteria such as the number of mother-tongue speakers and the number of available datasets. According to , an LRL\\footnote{An LRL is also known as under resourced, low-density, resource-poor, low data, or less-resourced language } is a language that lacks a unique writing system, lacks (or has) a limited presence on the World Wide Web, lacks linguistic expertise specific to that language, and/or lacks electronic resources such as corpora (monolingual and parallel), vocabulary lists, etc. NLP researchers have used the availability of data (in either labelled, unlabelled or auxiliary data), and the NLP tools and resources as the criteria to define LRLs~.\nOver the years, there have been many initiatives to categorise languages according to the aforementioned different criteria~. Given that the category of a language may change with time, we rely on the language categorization  recently proposed by  to identify LRLs. As shown in Table~\\ref{tab:language_categories},~ categorised 2485 languages into six groups based on the amount of publicly available data.\n\\begin{table}[htp]\n\\centering\n{\\small \n    \\begin{tabular}{p{.04\\textwidth}p{.7\\textwidth}p{.2\\textwidth}}\n    \\hline\n    Class & Description & Language Examples \\\\ \\hline \\hline\n    0 & Have exceptionally limited resources, and have rarely been considered in language technologies. & Slovene, Sinhala  \\\\ \\hline\n    1 & Have some unlabelled data; however, collecting labelled data is challenging. & Nepali, Telugu\\\\ \\hline\n    2 &  A small set of labeled datasets has been collected, and language support communities are there to support the language. & Zulu, Irish  \\\\ \\hline \n    3 & Has a strong web presence, and a cultural community that backs it. Have been highly benefited by unsupervised pre-training. & Afrikaans, Urdu \\\\ \\hline\n    4 & Have a large amount of unlabeled data, and lesser, but still a significant amount of labelled data. have  dedicated NLP communities researching these languages. & Russian, Hindi \\\\ \\hline\n    5 &  Have a dominant online presence. There have been massive investments in the development of resources and technologies. & English, Japanese\\\\ \\hline\n   \\end{tabular}\n   }\n  \\caption{Language Categories identified by~}~\\label{tab:language_categories}\n\\end{table}\nUnlike other NLP tasks, MT take place between two languages. Thus, in MT the resourcefulness of a language pair is determined by the available amount of parallel corpora between the considered languages. The terms `high-resource', `low-resource', as well as `extremely low-resource' have been commonly used  when referring to the parallel corpora at hand. However, there is no  minimum requirement in the size of the parallel corpora to categorise a language pair as high, low, or extremely low-resource. Some early research considered even $1$ million parallel sentences as LR~. More recent research seems to consider a language pair as LR or extremely LR if the available parallel corpora for the considered pair for NMT experiments is below $0.5$ Million, and below $0.1$ Million, respectively~; however, these are not absolute values for the size of the corpora.\\\\\nEven if a particular language has a large number of monolingual corpora while still having a small parallel corpus  with another language, this language pair is  considered as LR for the NMT task. We assume that languages that have been labelled as LR by~ have very small parallel corpora with other languages, or have no parallel corpora at all.\n\\iffalse", "cites": [7873, 2489, 7874, 4967, 4968, 4966, 4969], "cite_extract_rate": 0.7, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 2.0, "abstraction": 2.7}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of low-resource languages, citing various papers to support the definitions and classifications. It synthesizes a basic narrative around language resource criteria but lacks deeper connections or integration of the cited works. There is minimal critical analysis or evaluation of the cited papers, and while it attempts to generalize by introducing a language categorization framework, the abstraction remains limited to surface-level observations."}}
{"id": "2250caaf-a646-446f-b481-dd5c6b4bd159", "title": "Related Terminology and Definitions", "level": "subsection", "subsections": [], "parent_id": "a8eff206-d8a5-42d4-94ac-5f2cf5ed20dc", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "Background"], ["subsection", "Related Terminology and Definitions"]], "content": "\\label{related_defs}\n\\begin{enumerate}\n    \\item Monolingual Corpus - A body of text in one language.\n    \\item Parallel Data - A collection of text, each of which is translated into one or more other languages than the original\\footnote{\\url{http://www.ilc.cnr.it/EAGLES96/corpustyp/node20.html#:~:text=A\\%20parallel\\%20corpus\\%20is\\%20a,however\\%2C\\%20exist\\%20in\\%20several\\%20languages.}}.\n    \\item Bilingual Machine Translation - Machine translation between a pair of languages. Similarly a bilingual parallel corpus is between a pair of languages.\n    \\item Zero-shot translation and zero-resource translation. Some researchers~ consider zero-shot to be synonymous to extremely LR case.  However, ~ and  distinguished these two concepts from the concept of extremely LR setting. In zero-shot, there is no parallel data, and the model is trained with no parallel data for the considered language pair. In the latter case, although no parallel data is available for the considered language pair, synthetic data is generated, and the model is trained with this data for the considered language pair. Hence, in the perspective of the model, learning is not zero-shot.\n\\end{enumerate}\n\\fi", "cites": [4970, 7875], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides minimal synthesis of the cited papers, merely listing definitions and terminology with only superficial connections to the cited works. It lacks critical evaluation or analysis of the papers' contributions, limitations, or effectiveness. There is no abstraction to broader patterns or principles in the field of low-resource NMT."}}
{"id": "26a330b6-f4c2-4764-93e6-c7d10c626cb7", "title": "Related Work", "level": "subsection", "subsections": [], "parent_id": "a8eff206-d8a5-42d4-94ac-5f2cf5ed20dc", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "Background"], ["subsection", "Related Work"]], "content": "\\label{label-Related-Work}\nSome of the previous survey papers discussed different\nNMT architectures~. They did not contain any reference of LRL-NMT, except for~, which briefly identified Multilingual NMT (multi-NMT), unsupervised, and semi-supervised LRL-NMT techniques.\nAnother set of papers surveyed only one possible NMT methodology, for example multi-NMT~, leveraging monolingual data for\nNMT~, use of pre-trained embeddings for NMT~, or domain adaptation techniques for NMT ~. Out of these surveys,~ specifically focused on LR settings, may be because monolingual data is more useful in that scenario.\nWe also observed that some surveys focused on the broader MT, both SMT and NMT, in problem domains such as document-level MT~, while others focused on MT for a selected set of languages~. On a different front, we found surveys that discussed LRL scenarios in the general context of NLP, but did not have a noticeable focus on NMT or even MT~.\nTable~\\ref{tab:survey_papers} categorises the survey papers discussed above. In conclusion, although there are surveys that dedicated a brief section on LRL-NMT and others that explicitly focus on LRLs for a selected NMT technique, there is no comprehensive survey on leveraging NMT for LRLs.\\\\\n  \\begin{table} [htp]\n\\centering\n{\\small \n    \\begin{tabular}{p{.3\\textwidth}p{.7\\textwidth}}\n    \\hline\n    Type of survey & Examples\\\\ \\hline \\hline\n    NMT Architectures & , , , , ~\\\\ \\hline\n    Specific NMT Methodologies & , , , \\\\ \\hline\n    Specific MT Problem Domain &  \\\\ \\hline\n    Specific Language & \\\\ \\hline\n    LRL NLP & \\\\ \\hline\n   \\end{tabular}}\n  \\caption{Type of Survey papers}~\\label{tab:survey_papers}\n\\end{table}", "cites": [4974, 4969, 4973, 7873, 4972, 4965, 7200, 4971], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview by categorizing existing surveys and noting their focus areas, such as NMT architectures, specific methodologies, or low-resource NLP. It highlights gaps, such as the lack of a comprehensive survey on NMT for low-resource languages. However, synthesis is limited, and the analysis remains somewhat high-level without deeper integration or meta-level insights."}}
{"id": "bc72059f-1a7d-4a91-a3b2-f8dc30d71caf", "title": "Scope of the Survey", "level": "subsection", "subsections": [], "parent_id": "a8eff206-d8a5-42d4-94ac-5f2cf5ed20dc", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "Background"], ["subsection", "Scope of the Survey"]], "content": "Most of the NMT techniques discussed in this paper can be used in the context of LRL translation as well as LR domain translation. However, an LR domain, such as medical or finance, can exist for a high-resource language, such as English as well~. In that case, additional language resources (e.g. WordNet, Named Entity Recognisers) can be utilised in developing the solution. However, such resources might not be available for LRL pairs. Thus, solutions that only apply for LR domains are considered out of scope for this paper. In this paper, we use the phrase low-resource language NMT (LRL-NMT) to refer to NMT techniques that are applicable for translation between LRL pairs.\nSimilarly, we omit techniques that focused on NMT in general, without any specific focus on the translation of LRL pairs. We also omit techniques that focus on speech translation only, and multimodal translation (which is typically between images and text), as such research is not common in the context of LRL pairs. Some techniques (e.g. data augmentation (Section~\\ref{sec:dataAug}) and pivoting(Section~\\ref{sec:zero_shot})) have been used in the context of SMT as well, which is not discussed in this review.\nNMT solutions for zero-shot translation  (no parallel data to train an MT model) are included because of its relationship to the task of transation of LRL pairs, with an overlap between the techniques used.", "cites": [4975, 4969], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear analytical explanation of the survey's boundaries, distinguishing between low-resource language and low-resource domain translation. It synthesizes the general context of low-resource NLP and connects it to the specific focus of the survey. While it identifies some limitations (e.g., omitting multimodal and speech translation techniques), the analysis remains relatively surface-level and does not delve deeply into evaluating or contrasting the cited works."}}
{"id": "7cdcb411-627c-4731-ba2b-6ee0b4c77d4f", "title": "Overview", "level": "subsection", "subsections": [], "parent_id": "04e32557-9bb1-48a3-8d8c-57a570286aa7", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "NMT Techniques for Low-Resource Languages"], ["subsection", "Overview"]], "content": "NMT methodologies fall broadly into supervised, semi-supervised, and unsupervised.  Supervised NMT is the default architecture that relies on large-scale parallel datasets. Recurrent neural architecture with attention~, as well as the recently introduced transformer architecture~, are commonly used. However, due to space limitations, we do not detail out these techniques, and interested readers can refer to the aforementioned references.\nBoth these neural architectures rely on large parallel corpora, an advantage not available to LRLs. A solution is to synthetically generate data, which is called \\textbf{data augmentation} (Section~\\ref{sec:dataAug}). These techniques can be applied irrespective of the NMT architecture used.    \nIn the extreme case where no parallel data is available, \\textbf{unsupervised} NMT techniques (Section~\\ref{unsupervised_nmt}) can be employed. Even if the available parallel corpora is small, it is possible to combine them with the monolingual data of the languages concerned, in a \\textbf{semi-supervised} manner (Section~\\ref{semi-supervised}). \nEven if parallel data is available, building (bilingual) NMT models between each pair of languages is not practical. As a solution, \\textbf{multi-NMT} models (Section~\\ref{sec:multiNMT}) were introduced, which  facilitate the translation between more than one language pair using a single model. Most of the multi-NMT models are based on supervised NMT, while some research is available on the applicability of semi-supervised, and unsupervised NMT in a multilingual setting. Although multi-NMT models were initially introduced to avoid the need to build individual bilingual translation models, their capability in the translation of LRL pairs is shown to be promising. \n\\textbf{Transfer learning} (Section~\\ref{transfer_learning_nmt}) is a technique that is commonly used in low-resource NLP, including NMT. Here, an NMT model trained on a high-resource language pair is used to initialize a child model, which reduces the amount of time taken to train the latter, while guaranteeing better performances over training the child model from scratch. In particular, transfer learning on multi-NMT models have shown very good performance for LRL pairs. This is a very promising development, as it is time-consuming to train a multi-NMT model every time a dataset for a new language pair comes up.\n\\textbf{Zero-shot} NMT (Section~\\ref{sec:zero_shot}) is a problem related to LRL-NMT. In zero-shot, there is no parallel data, and the model is trained with no parallel data for the considered language pair. While some researchers~ consider zero-shot to be synonymous to extremely LR case, others ~ disagree.   Promising solutions for zero-shot translation that have been presented include pivoting, multi-NMT, unsupervised NMT, and transfer learning. Zero-shot translation is extremely useful because it eliminates the requirement of the existence of parallel data between every pair of languages. \nFigure~\\ref{fig:NMT_techniques} gives an overview of these techniques. Note that it does not cover all the possible scenarios. For example, semi-supervised NMT techniques can work with monolingual data available either at the source or target side, and multi-NMT works with more than three languages.\\\\\nThe following sub-sections discuss the aforementioned techniques at length. At the end of each sub-section, we discuss how the technique has been employed with respect to LRLs. \n\\begin{figure}\n    \\centering\n    \\includegraphics[scale = 0.45]{Figures/Figure1.png}\n    \\caption{NMT techniques applicable for the translation of LRL pairs. $L_1 - L_3$ refer to languages.  Dashed lines indicate translation task, and solid lines indicate the availability of parallel corpora. Double circles in (a) and (b) indicate the languages have monolingual data. (a) Bilingual supervised NMT, (b) Bilingual semi-supervised NMT, (c) Bilingual unsupervised NMT, (d) Multi-NMT, (e) Pivoting. }\n    \\label{fig:NMT_techniques}\n\\end{figure}", "cites": [7875, 168, 4970, 38], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of NMT techniques for low-resource languages, citing relevant papers but without substantial synthesis or critical evaluation. It briefly introduces concepts like data augmentation, unsupervised, semi-supervised, multi-NMT, transfer learning, and zero-shot translation, linking some ideas (e.g., multi-NMT with LRLs), but does not connect them in a novel or deeply integrated framework. There is minimal critique of the approaches or identification of broader trends, limiting its analytical depth."}}
{"id": "0432fa8e-8177-477c-9c23-7ac3c2857358", "title": "Data Augmentation Techniques", "level": "subsection", "subsections": ["fca0288a-59c1-43bd-8c9a-827609a8c944"], "parent_id": "04e32557-9bb1-48a3-8d8c-57a570286aa7", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "NMT Techniques for Low-Resource Languages"], ["subsection", "Data Augmentation Techniques"]], "content": "\\label{sec:dataAug}\nData augmentation (DA) is a set of techniques that is used to create additional data either by modifying existing data or adding data from different sources, to be used in training Machine Learning models. For the problem of MT, data augmentation is used to generate \\emph{synthetic} parallel sentence pairs to train data-centric MT models, such as SMT and NMT. In contrast to the other techniques discussed in this section, data augmentation techniques usually do not alter the NMT architecture but generate data to train these neural architectures. Data augmentation techniques for NMT could be divided into 3 categories: i) word or phrase replacement based augmentation, ii) back-translation based augmentation, and iii) parallel corpus mining.\n\\\\\n\\textbf{1. Word or phrase replacement based augmentation:} In this technique, a subset of sentences from an existing parallel or monolingual corpus is selected, and new synthetic sentences are generated by replacing words or phrases in that selected set of sentences. One solution is to use a bilingual dictionary and replace all the words~ or rare words~ in the selected  sentences of a monolingual corpus, with the words in the other language in order to generate its translation. Another solution is to replace frequent words in the target sentences with rare words in the target vocabulary and then modifying the aligned source words accordingly~. The main problem with such synthetic data is a lack of fluency. There have been subsequent attempts to select the best set of words considering fluency ~. Alternatively, instead of replacing words, phrases can be replaced, which preserves the context and in turn, improves the fluency of resulting sentences~. To further improve fluency, syntactic rules (e.g.~morphological, POS, or dependency rules) have been imposed during word replacement~.\n\\textbf{2. Back-Translation based Data Augmentation: } Back-Translation is the process of translating a monolingual corpus in the target language by pre-existing MT system, in the reverse translation direction, into the source language. Then the obtained synthetic source language sentences along with their respective target language sentences are used to construct a synthetic parallel corpus~. Usually, target-side sentences are selected to be back-translated, because monolingual target data helps improve the fluency of the translation model.~  empirically showed that starting with the source side has a lesser success. \\\\\n   Synthetic data generated using BT tends to be noisier than the original parallel data, especially if the MT system used to generate the synthetic data is suboptimal. This is particularly the case with MT systems trained with very small amounts of data. Thus, subsequent research improved BT using  data selection, data filtering, distinguishing between synthetic and original data, sampling and iterative BT.  These improvements are further discussed below.   \\\\\n     \\textbf{Iterative back-translation}: In one form of iterative back-translation, source and target monolingual data are back-translated using source to target and target to source NMT models, respectively. This procedure is continued iteratively, where the same set of sentences is back-translated several times until no improvement is observed in both translation direction~. Another option is to improve the forward and backward translators in an iterative manner~. However, in these systems, the two translators are trained independently. As a solution,~ jointly trained the two translators. \n   \\\\\n   \\textbf{Monolingual data selection}: In BT, simply back-translating all the available monolingual data would not guarantee optimal results. One factor that determines the performance of back-translation is the original-synthetic data ratio~. Thus, the synthetic to original parallel data ratio has to be selected carefully. The purpose of data selection is to select the best subset from the available monolingual corpora to be back-translated~.\n   \\\\\n    \\textbf{Synthetic parallel data filtering}: Even if a subset of monolingual data is selected to be back-translated, the resulting synthetic data could contain noise. Data filtering refers to the process of selecting a subset of the generated synthetic parallel sentences (the highest quality ones) to be used alongside the original data to train the NMT system~. \n    \\\\\n   \\textbf{Distinguishing between original and back-translated data}: Even after monolingual and parallel data filtering discussed above, it is highly likely that this data would be of lesser quality, compared to the original parallel data. It has been shown that adding a tag to the back-translated data to distinguish them from original data gives better results~. An alternative is to distinguish these two types of data by assigning a weight according to the quality of sentences.\n  \\\\\n \\textbf{Sampling}: In sampling, multiple source sentences are generated per target sentence in an attempt to average out errors in synthetic sentences~. Note that this technique as well as the previously discussed two techniques can be applied with other forms of DA techniques as well. However, we find experiments reported only in the context of BT.  \\\\\n{\\textbf{3. Parallel Data Mining (bitext mining) from comparable corpora:}}\nComparable corpora refer to text on the same topic that is not direct translations of each other but may contain fragments that are translation equivalents (e.g.~Wikipedia or news articles reporting the same facts in different languages).  Parallel sentences extracted from comparable corpora have been long identified as a good source of synthetic data for MT. \nBecause recently introduced multilingual sentence embeddings have become the common technique for generating parallel data to train NMT models, we only discuss those techniques\\footnote{We only discuss multilingual sentence embedding techniques that have been evaluated on NMT tasks. Some techniques have been evaluated on some other NLP tasks s.a. Natural Language Inference.}. In these techniques, a multilingual sentence embedding representation is first learnt between two or more languages. Then, during the sentence ranking step, for each given sentence in one language, a set of nearest neighbours is identified as parallel sentences from the other language, using a sentence similarity measurement technique.\n\\textbf{Multilingual Embedding generation: }Early research used NMT inspired encoder-decoder architectures to generate multilingual sentence embeddings. These include bilingual dual encoder architectures~, and shared multilingual encoder-decoder architectures~.~ leveraged the shared encoder-decoder model across 93 languages, which is publicly released under the LASER toolkit. This toolkit was the base for subsequent massive-scale parallel corpus extraction projects~ .\nThe above-discussed multilingual embedding generation techniques require large parallel corpora during the training process. As a result, unsupervised~, as well as self-learning~ NMT architectures have been used to generate multilingual embeddings.  \nA notable development is the use of pre-trained multilingual embedding models such as multilingual BERT (mBERT) or XLM-R~.  \\\\\n\\textbf{Sentence Ranking: }The choice of the sentence similarity measurement technique has been largely unsupervised (cosine similarity was the simplest one employed). However, this simple method is suboptimal, and improved cosine similarity measurements are available~. In addition, supervised sentence measurement techniques have been employed to a lesser extent~.", "cites": [4989, 4981, 4992, 8866, 7876, 8865, 4977, 4980, 4982, 4985, 4978, 8864, 4990, 4983, 860, 4984, 4993, 4986, 4976, 4979, 4991, 8867, 4988, 4987], "cite_extract_rate": 0.6153846153846154, "origin_cites_number": 39, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a well-structured synthesis of data augmentation techniques for low-resource NMT, categorizing them into three distinct approaches and connecting relevant papers to each category. It shows moderate critical analysis by highlighting limitations such as fluency issues and noise in synthetic data. The abstraction is strong, as it generalizes patterns in how different DA methods are applied and their effects, especially in relation to training data quality and model performance."}}
{"id": "fca0288a-59c1-43bd-8c9a-827609a8c944", "title": "Data Augmentation for Low-Resource Language NMT", "level": "subsubsection", "subsections": [], "parent_id": "0432fa8e-8177-477c-9c23-7ac3c2857358", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "NMT Techniques for Low-Resource Languages"], ["subsection", "Data Augmentation Techniques"], ["subsubsection", "Data Augmentation for Low-Resource Language NMT"]], "content": "The above three data augmentation techniques have shown promising results for translation between LRL pairs. However, each technique has its practical limitations when applied to the translation of LRL pairs.  BT assumes that an MT system is already available between the given language pair. Moreover, as evidenced by many empirical studies, the success of BT depends on many factors such as the original-synthetic parallel data ratio, and the domain relatedness of the parallel and monolingual data~. In addition, there have been attempts to leverage high-resource language data for LRL with BT; however, its success depends on the relatedness of the high-resource language and LRL~ or the availability of bilingual lexicons~.  Word or phrase replacement based augmentation techniques rely on language-specific resources (e.g.~bilingual dictionaries, Part of Speech (POS) taggers, dependency parsers) that many LRLs would not have. One possibility to explore the use of neural language models trained on monolingual data (e.g. BERT and its variants) to increase the fluency of the synthetic data.  For parallel data mining, the applicability of pre-trained multilingual models such as LASER or mBERT is restricted to the languages already included in the pre-trained model. Thus, it is worthwhile to investigate the heuristic and statistical-based parallel corpus mining techniques employed in early SMT research, in the context of LRLs.", "cites": [4992, 7164, 4994], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section analytically discusses data augmentation techniques for low-resource language NMT, integrating insights from the cited works by highlighting limitations and contextual dependencies of methods like back-translation. It critically evaluates the conditions under which these techniques are effective and suggests revisiting earlier SMT approaches for potential relevance. The section abstracts from specific papers to identify broader challenges and considerations in data augmentation for LRLs."}}
{"id": "500a7ba1-62dc-4a3c-9cc9-6cc93bd8c606", "title": "Unsupervised NMT", "level": "subsection", "subsections": ["67781361-7535-4644-bd90-d3b914dc874a"], "parent_id": "04e32557-9bb1-48a3-8d8c-57a570286aa7", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "NMT Techniques for Low-Resource Languages"], ["subsection", "Unsupervised NMT"]], "content": "\\label{unsupervised_nmt} \nThe generation of parallel corpora for language translation is expensive and resource-intensive. In contrast, monolingual corpora are often easier to obtain. As a result, unsupervised NMT using monolingual corpora or cross-lingual word embeddings (i.e., cross-lingual representations of words in a joint embedding space) is  less data-intensive for LRL-NMT \\footnote{The input corpora used for training data for unsupervised NMT is assumed to be monolingual corpora, while testing uses parallel corpora to evaluate the true translation}. \nGenerally, the architecture for unsupervised NMT makes use of Generative Adversarial Networks (GANs) and contains the following three steps~: i) initialization, ii) back-translation, and iii) discriminative classifier. \n\\textbf{Initialization}:\nThe underlying goal of the initialization step is to bridge the gap between the input representations of the different languages in an unsupervised manner. \nAs shown in Figure~\\ref{fig:unsupervised}, the unsupervised NMT model is initialized by learning a mapping between two or more languages. \nThe intuition is that human beings live a shared common experience in the same physical world. Thus, the embedding of different languages should have a shared mathematical context. Researchers have experimented with various linguistic resources  and neural input representations for initialisation. \\\\\nThe traditional lexical resources include bilingual dictionaries  or word-by-word gloss  (inferred by aligning words , phrases , or sub-words ).\\\\\nNeural representations include cross-lingual n-grams, word embeddings, language models (LMs) or dependency embeddings . \nThese neural input representations are either jointly trained by concatenating the source and target monolingual corpora or by learning the  transformation between the separate monolingual embeddings to map them into the shared space. One such technique is to leverage the available bilingual dictionaries by initialising the model with bilingual word embedding . Instead of words, using sub-word representations such as Byte Pair Encoding (BPE) has shown more promise~.\nIn recent works, it has been shown  that the cross-lingual masked LMs could be more effective in initialising the models . During training, the LM tries to predict the percentage of tokens that are randomly masked in the input.  further extended on the same lines by using n-grams instead of BPE tokens and inferred the cross-lingual n-gram translation tables. \n\\textbf{Back-Translation}:  \nNext, the generative step uses back-translation (discussed previously in Section~\\ref{sec:dataAug})  by a denoising autoencoder that combines forward and backward translation from source to target, then target back to the source.  The loss function compares the original source text against the doubly translated text.  Again, the intuition is that there exists a common latent space between two languages so that the model can reconstruct the sentence in a given noisy version and then reconstruct the source sentence given noisy translation.\\\\\nRecent back-translation based on multi-NMT (Section~\\ref{sec:multiNMT}) models have shown much better results compared to the bilingual counterpart     ~.\n\\textbf{Adversarial Architecture}:   \nFinally, a discriminative step uses a binary classifier to differentiate the source language from the target language by distinguishing translated target text from original target text. An adversarial loss function trades-off between the reconstruction loss from the back-translation against the discrimination loss from the classifier.  The result of this step is a high-quality translation that is more fluent for LRLs.\\\\\nExisting methods in the unsupervised NMT literature modifies the adversarial framework by incorporating additional adversarial steps or additional loss functions into the optimization step. These include dual cycle-GAN architecture  and local-global GAN . On the other hand, those methods that add a loss function include embedding agreement  , edit and extract , and comparative translations .\n \\begin{figure}\n    \\centering\n    \\includegraphics[scale = 0.45]{Figures/Figure2.png}\n    \\caption{The initialization step of unsupervised NMT, where two languages are mapped to a common space.  The input is a trained embedding for each language and a dictionary of mapped words.  The dictionary pairs help guide the two embeddings by a) rotation and  b) alignment.  The resulting embedding space of the dictionary pairs is matched. }\n    \\label{fig:unsupervised}\n\\end{figure}", "cites": [5005, 8868, 5001, 7877, 5004, 1551, 4999, 4997, 5002, 4995, 5003, 8869, 4996, 4998, 5000, 8870], "cite_extract_rate": 0.8, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple unsupervised NMT techniques by organizing them into a coherent framework of initialization, back-translation, and adversarial classification. It abstracts the core ideas (e.g., shared latent space, cross-lingual embeddings) and identifies trends such as the shift from word-based to sub-word and n-gram representations. While it includes some critical evaluation (e.g., limitations of pseudo-sentences in back-translation), the critique is not as deep as it could be."}}
{"id": "67781361-7535-4644-bd90-d3b914dc874a", "title": "Unsupervised NMT for Low-Resource Languages", "level": "subsubsection", "subsections": [], "parent_id": "500a7ba1-62dc-4a3c-9cc9-6cc93bd8c606", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "NMT Techniques for Low-Resource Languages"], ["subsection", "Unsupervised NMT"], ["subsubsection", "Unsupervised NMT for Low-Resource Languages"]], "content": "The majority of the early unsupervised techniques focused on high-resource languages that have monolingual data in abundance ; except for English-Russian and English-Romanian . \nHowever, having the required input representation for the LRLs is a limitation  because some LRLs do not have bilingual dictionaries or proper word alignments.  On the other hand, to build a neural representation, large monolingual corpora are needed.  How these resources perform in extreme LRLs has not been properly studied. More recent work that explored the conditions for using unsupervised NMT for LRLs having less monolingual data is a promising development~.   \\\\ \nVarious researchers have found a reduced translation quality for LRL pairs that are not from similar linguistic families or similar domains . \nFour areas of concerns are: i) different script and dissimilar language, ii) imperfect domain alignment or domain mismatch, iii) diverse datasets, and iv) extremely LRLs .\nTo resolve issues i and ii,  proposed robust training by using language models that are agnostic to language similarity and domain similarity, while  resolved issue i by combining transfer learning from HRL to LRL with unsupervised NMT to improve translations on a low-resource supervised setup.", "cites": [8870, 8869, 5008, 5006, 5000, 5007, 4995], "cite_extract_rate": 0.5833333333333334, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates several cited papers to highlight challenges and recent developments in applying unsupervised NMT to low-resource languages. It connects ideas like domain mismatch, linguistic dissimilarity, and the use of cross-lingual pre-training. However, the synthesis is somewhat limited in depth and the critical evaluation is moderate, focusing on methodological shortcomings rather than offering a nuanced critique or a novel framework."}}
{"id": "69fceb92-f017-465f-b68d-572e7cfdc976", "title": "Semi-Supervised NMT", "level": "subsection", "subsections": ["c6078c0c-a15b-41c6-9dbf-8778e3333722"], "parent_id": "04e32557-9bb1-48a3-8d8c-57a570286aa7", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "NMT Techniques for Low-Resource Languages"], ["subsection", "Semi-Supervised NMT"]], "content": "\\label{semi-supervised}\nIn contrast to unsupervised techniques, semi-supervised techniques assume the availability of some amount of parallel corpora alongside monolingual corpora. Semi-supervised techniques can be categorised according to the way the monolingual data is utilised: \n\\textbf{Using monolingual data to generate synthetic parallel data:} The simplest strategy is to create the synthetic parallel corpus (this is essentially data augmentation) either by 1) copying monolingual data of one language as the translated text~, or 2) creating the source side with a null token~. A better way to generate synthetic data is through back-translation, as discussed in Section~\\ref{sec:dataAug}.\n\\textbf{Using monolingual data to generate a language model (LM):} A LM can be integrated into the target-side of the NMT to improve the fluency of the generated text. This is named LM fusion, which can be broadly categorised as shallow fusion and deep fusion~. In shallow fusion, the LM is used to score the candidate words generated by the decoder of the NMT system either during inference time~, or training time~. In deep fusion, the NMT architecture is modified to concatenate the LM with the decoder. Deep fusion provides better performance. However, LM model fusion has few limitations: 1) The NMT model and LM are trained independently and are not fine-tuned, 2) LM is only used at the decoder, 3) in deep fusion, only the final layers of the LM are integrated, disregarding the low-level LM features, and 4) the NMT architecture has to be changed to integrate the LM~. \nInstead of LM fusion,~ used the trained LM model as a weakly informative prior, which drives the output distributions of the NMT model to be probable under the distributions of the LM. This does not require any change to the NMT architecture.  \nAnother alternative is to use LMs to initialize the NMT model.~ initialized both encoder and decoder with the LMs of respective languages, while~ used source embeddings to initialize the encoder. Following this line of research, recently there have been initiatives to incorporate BERT fine-tuning for NMT~. \nA promising extension is the use of pre-training multilingual LMs, such as mBART, in the form of an autoregressive sequence-to-sequence model~. \n\\textbf{Changing the NMT training objective to incorporate monolingual data: }\n appended a reconstruction term to the training objective, which reconstructs the observed monolingual corpora using an autoencoder. This method assumes both source and target monolingual corpora are available. They jointly train source-to-target and target-to-source translation models, which serve as the encoder and decoder (respectively) for the autoencoder.  also made use of both source and target monolingual data and employed source-to-target and target-to-source translation models. They introduced a new training objective by adding a joint Expectation Maximization (EM) estimation over the monolingual data to the Maximum Likelihood Estimation (MLE) over parallel data. \n\\textbf{Multi-task learning:} Here, separate NMT models are used.  trained one model on the aligned sentence pairs to predict the target sentence from the source sentence, while the other is trained on the source monolingual data to predict the reordered source sentence from original source sentences. In essence, they strengthened the encoder using source-side monolingual data.~ followed a similar approach, however, they strengthened the decoder using target-side monolingual data. A similar technique is joint learning, where the source-to-target and target-to-source translation models, as well as language models, are aligned through a shared latent semantic space~.\n\\textbf{Dual Learning:} Dual learning is based on the concept of Reinforcement Learning (RL) and requires monolingual data on both sides. Parallel data is used to build two weak source-to-target and target-to-source translation models. Then, monolingual data of both sides undergo a two-hop translation. For example, source side data is first translated using the source-to-target model, the output of which is again translated by the target-to-source model. This final output is evaluated against the original monolingual sentence and is used as a reward signal to improve the translation models~. This process is carried out iteratively and shows some resemblance to iterative BT~. However, RL based techniques are known to be very inefficient~.~ also argued that the above RL based technique does not properly exploit the monolingual data, and suggested several improvements.~ transferred the knowledge learned in this dual translation task into the primary source-to-target translation task.", "cites": [7878, 5009, 5010, 7475, 2489, 5011, 9121, 5012, 4983, 4988], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by grouping diverse methods under coherent categories (e.g., data generation, LM fusion, training objective changes). It provides critical analysis by pointing out limitations of LM fusion and RL-based methods. The abstraction is evident in the identification of broader patterns, such as the role of monolingual data in enhancing translation systems and the trade-offs between different integration strategies."}}
{"id": "c6078c0c-a15b-41c6-9dbf-8778e3333722", "title": "Semi-supervised NMT for Low-Resource Languages", "level": "subsubsection", "subsections": [], "parent_id": "69fceb92-f017-465f-b68d-572e7cfdc976", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "NMT Techniques for Low-Resource Languages"], ["subsection", "Semi-Supervised NMT"], ["subsubsection", "Semi-supervised NMT for Low-Resource Languages"]], "content": "Although semi-supervised techniques have been presented as a solution to the scarcity of parallel data, we note the following concerns with respect to their applicability in the context of LRLs: 1) A LRL translation scenario has been simulated by taking small amounts of parallel data from high-resource languages such as English, French, German, and Chinese. 2) Some research has employed very large monolingual corpora. Although many LRLs have monolingual corpora with sizes larger than parallel corpora, it is difficult to assume they would have such large amounts of monolingual corpora, 3) Lack of comparative evaluations across the different semi-supervised techniques. Except for a few research~, most of the others compared with back-translation only. Interestingly some reported results less than BT~ and iterative BT~, while some reported only marginal gains over BT~. This makes one doubt the actual benefit of these sophisticated techniques. Thus to establish the usefulness of these techniques for true LRLs, experiments should be carried out concerning a wide array of languages and different monolingual dataset sizes.\\\\\nAlthough integrating massive language models such as BERT have shown promising results, these techniques have also been tested with high-resource languages. How these models would work with models built with rather small amounts of monolingual data should be investigated\\footnote{Note that there is a long list of very recent publications in this line. However, due to this reason, above we cited only one.}. However, multilingual models such as mBART are indeed very promising for the translation of LRL pairs, which has been already proven~ as further discussed in Section~\\ref{sec:multiNMT}.", "cites": [7475, 5013, 9121, 5014], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes insights from multiple papers to highlight common limitations and trends in semi-supervised NMT for low-resource languages. It critically evaluates the use of techniques like back-translation and newer approaches, pointing out inconsistencies in reported results and the need for broader validation. The section abstracts these findings into general concerns about data assumptions and experimental rigor, rather than focusing solely on individual methods."}}
{"id": "6c52df28-c470-4f26-b8bb-ea0b68c915b7", "title": "Multilingual NMT", "level": "subsection", "subsections": ["2da4949d-805f-4954-9164-a1dc9be68496", "283c33fe-3ad4-4944-9563-e4b9b6e797b8"], "parent_id": "04e32557-9bb1-48a3-8d8c-57a570286aa7", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "NMT Techniques for Low-Resource Languages"], ["subsection", "Multilingual NMT"]], "content": "\\label{sec:multiNMT}\nMultilingual NMT (multi-NMT) systems are those handling translation between more than one language pair~. Recent research has shown multilingual models outperform their bilingual counterpart, in particular when the number of languages in the system is small and those languages are related . Particularly, in English-centric datasets, multi-NMT models trained with roughly $50$ languages have shown clear performance gains over bilingual models for LRLs~. This is mainly due to the capability of the model to learn an \\textit{interlingua} (shared semantic representation between languages)~. Training multi-NMT models is a more practical solution as opposed to building separate bilingual models in a real-world setting~.\nDespite these benefits, multi-NMT faces challenging problems such as i) Inclusion of a large number of languages that have varying differences among them, ii) noise especially in the parallel data used, iii) data imbalance (some languages just having a fraction of parallel sentences compared to high-resource languages), and iv) other discrepancies concerning factors such as writing style and topic~.  \nWith respect to the translation task, multi-NMT can be categorised into three types (Figure~\\ref{fig:multiNMT}): \n\\begin{enumerate}\n    \\item Translating from one source language to multiple target languages, (one-to-many) (Figure~\\ref{fig:multiNMT} (a)): This is essentially a multi-task problem, where each target becomes a new task .\n    \\item Translating from multiple source languages to a single target language, (many-to-one) (Figure~\\ref{fig:multiNMT} (b)): This can be considered as a multi-source problem, considered relatively easier than the multi-task problem .\n    \\item Translating from multiple languages to multiple languages,(many-to-many) (Figure~\\ref{fig:multiNMT} (c) and (d)). This is the multi-source, multi-target problem, and the most difficult scenario .\n\\end{enumerate}\nSupervised multi-NMT architectures introduced to tackle the aforementioned translation tasks can be broadly categorised into three paradigms: i) single encoder-decoder for all the languages (all source sentences are fed into the encoder irrespective of the language, and the decoder can generate any of the target languages (Figure~\\ref{fig:multiNMT}. (c)); ii) per-language encoder-decoder (each source language has its own encoder, and each target language has its own decoder (Figure~\\ref{fig:multiNMT}. (d)); and iii) shared (a single) encoder/decoder at one side, with per-language decoder/encoder at the other side (Figure~\\ref{fig:multiNMT}. (a) and (b)). The main objective of these different architectures is to maximize the common information shared across languages while retaining language-specific information to distinguish between different languages. This mainly depends on how parameters are shared between individual encoders and decoders.  All of these architectures are based on either the recurrent model with attention , or the transformer-based model~. Comparison of the recurrent model against the transformer model under the same settings has shown that the latter is better . Almost all the recent multi-NMT architectures are based on the transformer model.\n   \\textbf{Single encoder-decoder for all the languages:}\nFor large-scale multi-NMT implementations, this is currently the state-of-the-art, especially in real-world industry-level systems~. Since all the source languages share the same encoder and all the target languages share the same decoder, while simultaneously supporting one-to-many, many-to-one, and many-to-many cases, this model is commonly known as the `\\textit{universal NMT model}'. The main advantage of a universal model is lower model complexity compared to per language encoder-decoder models (discussed next) because it has a lower parameter count.\nMoreover, as demonstrated by , this universal model is capable of learning a form of interlingua, which is crucial in facilitating zero-shot translation (see Section~\\ref{sec:zero_shot}). \nA major challenge in using this architecture is enabling the decoder to distinguish the target language. The common practice is to add a  language identification tag to the source sentence~. An alternative is to add the language name as an input feature~.\nMore recent work has used language-dependent positional embeddings representations ~.\n\\textbf{Per-language encoder-decoder:} \nIn this architecture, there is a separate encoder per source language, as well as a separate decoder per each target language. As opposed to the universal NMT models described above, the requirement to capture language-independent features can be easily achieved by this setup. However, sharing common information across languages is a challenge. The commonly applied solution is the use of shared parameters in the model, employing shared attention~.\n extended this idea even further, and introduced a contextual parameter generator, which enables the model to learn language-specific parameters, while sharing information between similar languages. \n \\begin{figure}\n    \\centering\n    \\includegraphics[scale = 0.5]{Figures/Figure3.png}\n    \\caption{Supervised multi-NMT architectures.}\n    \\label{fig:multiNMT}\n\\end{figure}\n\\textbf{Single encoder with per-language decoder / per-language encoder with single decoder:} \nThe single encoder per language with multiple decoder architecture supports the one-to-many scenario, where a single source language gets translated to multiple languages via multiple decoders (multi-task). The multiple encoders, single decoder architecture supports the many-to-one scenario, where multiple encoders process separate source languages while using a single decoder to translate into one target language (multi-source).\nIn the one-to-many scenario, each decoder has its attention mechanism, so no parameter sharing takes place at the decoder side~. A more effective model is to allow partial sharing of parameters across decoders~. \nWhen there are multiple encoders alongside a single decoder, encoder output has to be combined to be sent to the decoder. Initial solutions assumed the used corpus is multi-way parallel~. Ways to relax this restriction are either to mark a corresponding sentence as null if a particular language does not have that sentence~ or to generate the missing sentences with a pre-trained model~ . \nMany research has evaluated the pros and cons of these different architectures. For example,  showed that a unique decoder for each target language or unique decoder attention parameters for each target language outperform models with fully shared decoder parameters.~ obtained better results with partial parameter sharing in the transformer model, over the full-parameter sharing recurrent model of . However, the best model selection would depend on the nature of the task at hand. For example, if the model is expected to deal with hundreds of languages, it is desirable to have maximum parameter sharing like in , to reduce the model complexity .", "cites": [8872, 5025, 7879, 4970, 5019, 5016, 5017, 5022, 8871, 5020, 5024, 4507, 5015, 5021, 5018, 5023, 4968], "cite_extract_rate": 0.6296296296296297, "origin_cites_number": 27, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers to categorize and explain multilingual NMT architectures, connecting ideas around parameter sharing, attention mechanisms, and language representation. It provides a critical evaluation of different models (e.g., universal vs. per-language encoders/decoders) and highlights trade-offs like model complexity and performance. It abstracts these findings into a broader framework of multilingual translation paradigms and their suitability for different language settings."}}
{"id": "2da4949d-805f-4954-9164-a1dc9be68496", "title": "Multi-NMT for Low-Resource Languages", "level": "subsubsection", "subsections": [], "parent_id": "6c52df28-c470-4f26-b8bb-ea0b68c915b7", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "NMT Techniques for Low-Resource Languages"], ["subsection", "Multilingual NMT"], ["subsubsection", "Multi-NMT for Low-Resource Languages"]], "content": "\\label{Low-Resource_Languages}\nAll of the three supervised multi-NMT techniques discussed above have been leveraged for LRL translation. In addition, the multilingual version of unsupervised and semi-supervised NMT models, as well as transfer learning on multi-NMT parent models have been used for LRL translation. \n\\textbf{1. Supervised multi-NMT architectures:} In the available multilingual datasets, the LRL pairs are heavily under-represented. Thus, the results of supervised multi-NMT models for the LRL pairs are far below the results for high-resource languages, even though they use the same multi-NMT model~. The simplest strategy to alleviate this problem would be to over-sample the parallel corpus related to the LRL pair. There are different sampling strategies such as simple over-sampling and temperature-based sampling~. Sampling data into mini-batches also has to be given careful consideration, to avoid any form of bias. Some strategies include scheduling (cycle through the bilingual language pairs)~, using mini-batches that consist of different languages~, or using mini-batches that contain data from the same target~. Data augmentation~ (Section~\\ref{sec:dataAug}) and pivoting~ (Section~\\ref{sec:zero_shot}) can also be used to generate parallel data for LRL pairs.\n\\textbf{2. Unsupervised multi-NMT:} When a multi-NMT model is built entirely upon monolingual data, it is referred to as unsupervised multi-NMT, which are trained following a similar process to that of bilingual unsupervised models (see Section~\\ref{unsupervised_nmt}). The difference between bilingual and multilingual NMT comes from the way the input representation is constructed. In the English-centric unsupervised model proposed by , first, the embeddings of non-English languages are mapped into the latent space of English embeddings.~  constructed a multilingual masked language model using only a single encoder. Better results over the pure unsupervised model can be obtained if at least one language has parallel data with some other language~.\n\\textbf{3. Semi-supervised multi-NMT} In semi-supervised multi-NMT, monolingual data is used to create an additional training objective on top of the supervised translation training objective. While~ used the MASS objective~ for this purpose,~ employed two monolingual auxiliary tasks: masked language modelling (MLM) for the source-side, and denoising autoencoding for the target side. Semi-supervised NMT is further discussed in Section~\\ref{semi-supervised}.\n\\textbf{4. Transfer Learning on a pre-trained multi-NMT model:} Transfer Learning is discussed in Section~\\ref{transfer_learning_nmt}. Here we note that transfer learning using a multilingual parent has been identified as a promising approach for LRL-NMT~. In particular, some LRL data may not be available during multi-NMT training time and very large multilingual models cannot be re-trained every time parallel data for a new language pair becomes available  .\n\\textbf{Input Representation:} Despite the multi-NMT methodology selected, a major factor that decides the success of multi-NMT for LRLs is the input representation. The input representation determines the ability to group semantically similar words from different languages in the embedding space. Input representation can be broadly broken down into two categories: surface form (word-level) representation, and embedding-based representation.\nWhen the surface form representation is used, a semantic grouping of words is achieved by adding a language token to each word~, or by using additional information s.a POS of words~. However, using word-level input results in large vocabulary sizes that are difficult to scale . Even if there are linguistically similar languages that share a common script, the amount of vocabulary overlap is minimal~. LRLs are severely affected by this~.\nAs a solution, sub-word level encoding (BPE~, sentence piece representation~, or transliteration~ was used. \n noted that even sub-word level encoding does not create enough overlap for extremely LRLs, since it still uses the surface form of the word. Further, as  pointed out, with such sub-word based techniques, semantically similar and similarly spelt words could get split into different sub-words for different languages. \nAn alternative is to use input representations based on cross-lingual embeddings.  argued that, when the input languages are in the same semantic space, the encoder has to learn a relatively simple transform of the input. Moreover, in such shared spaces, LRLs get enriched with more semantic information with the help of high-resource languages. Such universal embedding representations have shown very promising results for LR as well as extremely LRL pairs~.\nA very interesting development is the use of multilingual denoising models pre-trained on monolingual data of a large number of languages (e.g. mBART~), which can be fine-tuned on multilingual translation tasks. This has shown very promising results for LRL pairs~.  \nIn summary, we see research efforts on multiple fronts to leverage multi-NMT for LRLs. While earlier research experimented with datasets sub-sampled from high-resource language pairs, later research has experimented with actual LRLs~. Moreover, it is encouraging to see very promising results from transfer learning over multi-NMT models, as training large multi-NMT models is time-consuming. However, most of this research has been carried out holistically, without focusing on individual language, or language-family characteristics. How these characteristics can be exploited to better leverage NMT for LRLs would result in multi-NMT models that are more focused on a required set of languages.\n\\iffalse", "cites": [8872, 2489, 5033, 5026, 5001, 4970, 8873, 5017, 5031, 8871, 5020, 5030, 7873, 5034, 5002, 5028, 7096, 5003, 5015, 5027, 4998, 4968, 5032, 5014, 8874, 5029], "cite_extract_rate": 0.7647058823529411, "origin_cites_number": 34, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple cited works to highlight trends in using multi-NMT for low-resource languages, integrating sampling strategies, input representations, and transfer learning. It provides some critical insights, such as limitations of surface-based representations and the need to focus on language-specific characteristics. The abstraction level is strong, as it generalizes across methods to present broader patterns like the shift from high-resource sub-sampling to true low-resource experimentation and the role of semantic alignment in embeddings."}}
{"id": "283c33fe-3ad4-4944-9563-e4b9b6e797b8", "title": "Zero-shot and zero-resource translation", "level": "subsubsection", "subsections": [], "parent_id": "6c52df28-c470-4f26-b8bb-ea0b68c915b7", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "NMT Techniques for Low-Resource Languages"], ["subsection", "Multilingual NMT"], ["subsubsection", "Zero-shot and zero-resource translation"]], "content": "\\label{multi-NMT_zero}\nZero-shot and zero-resource scenarios can be considered as a special case of low-resource language translation. However, we believe they need a separate discussion, as zero-shot translation may take place between high-resource languages that do not have large parallel corpora, just with the considered languages. To be more specific, as mentioned in Section~\\ref{Pivoting} zero-shot translation may not be viable when the considered languages are not having large parallel corpora with any other language in the model.  \\\\\nFor zero-shot translation to be plausible, the model should be capable of learning a good interlingua . As long as this requirement is satisfied, both shared encoder-decoder models, as well as per language encoder-decoder models should work. Although some early research has shown the latter model being better , later research focused on improving the former model, initially introduced by . These continuous efforts finally guaranteed that zero-shot translation using single encoder-decoder models outperform the pivot-based zero-shot translation, which has been the de facto solution for a long time . Moreover, truly many-to-many models have recently shown to beat the English-centric models in zero-shot settings .  \\\\\nIf the multi-NMT model is truly capable of learning a language-independent interlingua, there should be less correlation between the source and target languages. However, when the model is trained with a large amount of languages, the modelling capacity has to be distributed across all the languages, suggesting that the overall model capacity is faced with a bottleneck . Thus, the learned interlingua is not fully language independent. Two solutions have been presented to solve this problem: to explicitly make the source and target languages independent, and to improve model capacity.\\\\\n\\begin{enumerate}\n\\item make the source and target languages independent:\n\\begin{itemize}\n   \\item pre-train the decoder as a multilingual language model~,\n    \\item iterative training of a multilingual model from source-target and target-source, conditioning the output on pre-trained strong source and target language models of the zero-shot language pair~,\n   \\item use regularization during the training process~, and \n   \\item use an additional loss function that serves as the objective to a new translation direction from source to source~.\n\\end{itemize}\n\\fi\n\\iffalse\n\\item improve model capacity:\n\\begin{itemize}\n    \\item add language-aware layer normalization and linear transformation in-between the encoder and decoder~, and \n    \\item add a parallel transformer layer, parameters of which are split by language or language group~.\n\\end{itemize}\n\\end{enumerate}\nWhile all the above methods assume that all the languages exist in the multi-NMT model as parallel data with some other language, ~ showed that this necessarily does not have to be. Using an additional training objective from the monolingual data of an unseen language along with the multi-NMT training objective, they showed very promising results for zero-shot translation.\\\\ \nTransfer learning can be considered as a form of zero-shot translation, if at least one of the  pair of languages corresponding to child model is not included in the parent model.~ experimented with zero-shot transfer learning, where the child model has one unseen language (either at the source or target side). They demonstrate how the unseen language translation is derived from carefully selected set of languages in the parent model.\\\\\nAs mentioned in Section~\\ref{multi-NMT_zero}, in the zero-resource case, there is no authentic parallel data, however synthetic parallel data is generated to train the multi-NMT model. Thus, in the eye of the model, it is no longer a zero-shot problem. Multiple methods have been explored for synthetic data generation:\n\\begin{itemize}\n    \\item pivoting ,\n    \\item back-translation ,\n    \\item bridging (a language pair that has the same language as the source and target), and\n    \\item First generate zero-shot translations using the trained multi-NMT model on some portion of the training data; then re-start the training process on both the generated translations and the original parallel data. At each iteration, the original training data is augmented only with the last batch of generated translations .\n\\end{itemize}\n\\fi", "cites": [5040, 5036, 5038, 5030, 8872, 5022, 5035, 4966, 5039, 5034, 5014, 5037, 5029, 5020], "cite_extract_rate": 0.875, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from multiple papers on zero-shot and zero-resource translation, organizing them into structured themes like language independence and model capacity. It provides a critical perspective by discussing limitations (e.g., model bottlenecks) and comparing the effectiveness of shared vs. per-language encoder-decoder models. While it identifies patterns such as the role of synthetic data and model adaptation, it does not fully abstract to a meta-level principle or propose a novel conceptual framework."}}
{"id": "797cd8b4-4a35-44d9-96d1-49176cef9068", "title": "Transfer Learning in NMT", "level": "subsection", "subsections": ["4ad7965b-c608-4132-9a2f-f43017b6b8cf"], "parent_id": "04e32557-9bb1-48a3-8d8c-57a570286aa7", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "NMT Techniques for Low-Resource Languages"], ["subsection", "Transfer Learning in NMT"]], "content": "\\label{sec:transfer_learning}\n\\label{transfer_learning_nmt}\nTransfer learning is a sub-area in Machine Learning that reuses (i.e.~transfers or adapts) knowledge that is gained from solving one particular task, problem, or model (parent) by applying it to a different but related one (child)~.~ first introduced the viability of transfer learning for NMT. In NMT, the parent model is first trained on a large corpus of parallel data from a high-resource language pair (or pairs), which is then used to initialize the parameters of a child model that is trained on a relatively smaller parallel corpus of the LRL pair (Figure~\\ref{fig:transfer_leraning}).  \nThe advantages of transferring knowledge from the parent model to the child model include i) reducing the size requirement on child training data, ii) improving the performance of the child task, and iii) faster convergence compared to child models trained from scratch.\nThe transfer process in NMT models can be broadly categorised as either warm-start and cold-start~. Due to the availability of child parallel data during parent model training, warm-start systems are more accurate and has been the focus of most of the previous work~. However, cold-start systems are also of importance due to their resemblance to a real-life scenario where child parallel data is not always available at parent model training time~. \nAs shown in Figure~\\ref{fig:transfer_leraning}, the first step in transfer learning is to train a parent model, which could be either bilingual or multilingual (note that the source and target in multi-NMT models can be many-to-one, one-to-many, or many-to-many. A special case of multi-NMT based transfer learning is fine-tuning large-scale multilingual language models such as mBART using small amounts of parallel data~, as already mentioned in Section~\\ref{sec:multiNMT}). However, the bilingual parent model is more common. The majority of the time, the parent and child have the same target language ~, while others use the same source language for both the parent and child ~.  However, it is also possible for the parent and child not to have shared languages in common~. Often,  multi-NMT models used as parents in transfer learning have been trained on the many-to-one setting~. Despite the parent model being trained on either bilingual or multilingual source-target languages, the child has always been bilingual with the exception of~, which progressively fine-tuned a parent model in order to build a model that adequately performs on multiple language pairs.\n\\begin{figure}\n    \\centering\n    \\includegraphics[scale = 0.5]{Figures/Figure4.png}\n    \\caption{Transfer Learning process.}\n    \\label{fig:transfer_leraning}\n\\end{figure}\nImprovements in transfer learning for NMT corresponds to three main aspects: i) minimizing the language space mismatch between languages, ii) fine-tuning technique and iii) the transfer protocol. \n\\textbf{Minimizing the language space mismatch}: Transfer learning systems have to address the problem of language space mismatch, since parent and child languages may not have the same feature distribution~. When the surface form is used as input, this language mismatch problem becomes a vocabulary mismatch between parent and child models. In the warm-start systems, sub-word segmentation models can be applied to the parent and child training data to build joint vocabularies~.~ took this idea even further and introduced a universal vocabulary for the parent to train on.  \n~ showed that a vocabulary based on sub-wording can be employed even in the cold-start scenarios by building a dynamic vocabulary. However, for cold-start scenarios, the better alternative is to pre-train a universal input representation, including child monolingual data, if available~.       \n\\textbf{Fine-tuning technique:} Transferring knowledge from the parent model to the child model requires fine-tuning  the parameters trained in the parent model on the child dataset. Conversely, when a particular layer of the parent model is not fine-tuned, this is called freezing that layer of the parent model.\nBelow we list the research experiments with differing fine-tuning strategies, where the best freezing setup depends on factors such as the neural architecture employed, the translation task, and the dataset size. Thus we do not draw any conclusions on the best fine-tuning strategy.\n\\textit{No fine-tuning: }The whole parent model is frozen (in other words, copied) to the child~. \n\\textit{Fine-tune the embedding layer:} Similarity between parent and child language pairs (e.g. whether parent and child have the same target) determines which embedding has to be fine-tuned. For example, if the parent and child translate to the same target language, parent decoder embeddings can be transferred to the child~. When the surface form input is used, the most naive way of transferring the embedding layer is to randomly initialize the parent embedding layer before training the child model~.  A better alternative is to take the parent and child vocabulary overlap  while replacing the rest of the parent embeddings with child embeddings~.\n\\textit{Fine-tune all of the parent model:} No layer of the parent model is freezed~. \n\\textit{Fine-tune a custom set of layers:} This includes fine-tuning a selected combination on input, and inner layers of the encoder and decoder ~. \n\\textbf{Transfer Protocol: }Varying the transfer protocol is also a promising way to improve NMT transfer learning. This can be done in different forms: \n\\begin{itemize}\n    \\item Train a chain of consecutive NMT models by transferring the parameters of a parent model to new LRL pairs~. \n    \\item Train the initial NMT model on a parallel corpus for a resource-rich language pair, fine-tune it with the combined corpus of parent and child (can be more than one child), and finally, fine-tune further with the selected child data only~.\n    \\item First train the unrelated high-resource language pair, then fine-tune it on a similar intermediate language pair and finally fine-tune on the LRL pair~.\n\\end{itemize}\n    \\iffalse\n\\begin{itemize}\n    \\item~ used the concept of Similar Language Regularization, which makes use of data from a high-resource language along with data from the LRL. \n    \\eal{not clear what the regularlization does}\n    \\item~ introduced progAdapt and progGrow protocols. In the former, a chain of consecutive NMT models are trained by transferring the parameters of a parent model to new language pairs. The latter progressively adds new pairs to the model.\n    \\eal{say upfront for each one what the improvement is and how does it help; knowing the \"name\" doesn't help much...they all have their own unique name}\n    \\item~ first trained a multilingual NMT model on two out-of-domain high-resource pairs that have the same target, and fine-tune it on two low-resource in-domain pairs that have the same languages as the parent. Then this model is further fine-tuned on an in-domain very small parallel corpus. The source and target languages are the source languages of the previous models.\n    \\item~ introduced a multistage transfer protocol where an initial NMT model is trained on a parallel corpus for a resource rich language pair, which in turn is fine-tuned with the combined corpus of parent and child (can be more than one child). Finally, fine-tuning is further continued with the selected child data only.\n    \\item~ first trained the unrelated high-resource language pair, the similar intermediate language pair and the LRL pair in turn.\n    \\item~ first trained the parent model, fine-tuned it on a hybrid corpus that combined parent and child data, and finally fine-tuned on the child corpus.    \n\\end{itemize}\n\\fi\nMultiple factors determine the success of transfer learning. The relationship between the languages used in parent and child models has been identified as the most crucial~. \nHigh relatedness between languages guarantees high vocabulary overlap when the surface form is used as input and would result in more meaningful cross-lingual embeddings as well. Much research has exploited vocabulary overlap of related languages using sub-word segmentation to achieve good results with transfer learning~, even when parent and child have no language in common~. An important consideration is the size of the sub-word vocabulary. It should be selected in such a way that the child is not overwhelmed by the parent~. For related languages, transliteration has shown to reduce lexical divergence~. The syntactic divergence between parent and child can be reduced by re-ordering the parent data~. Other factors that influence transfer learning include the size of the parent and child corpora, domains of the parent and child data, the number of shared words (vocabulary overlap), and the language script~.", "cites": [5044, 8873, 5031, 5043, 5030, 4967, 5041, 5042, 5045, 5046, 5032, 8874], "cite_extract_rate": 0.5217391304347826, "origin_cites_number": 23, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes transfer learning techniques in NMT, integrating insights from multiple papers into a structured narrative covering warm-start, cold-start, fine-tuning strategies, and transfer protocols. It also identifies broader patterns, such as the importance of language space and vocabulary alignment. While it provides some critical commentary on methods, it could offer deeper comparative evaluation and limitations for a more robust analysis."}}
{"id": "4ad7965b-c608-4132-9a2f-f43017b6b8cf", "title": "Transfer Learning for Low-Resource Languages", "level": "subsubsection", "subsections": [], "parent_id": "797cd8b4-4a35-44d9-96d1-49176cef9068", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "NMT Techniques for Low-Resource Languages"], ["subsection", "Transfer Learning in NMT"], ["subsubsection", "Transfer Learning for Low-Resource Languages"]], "content": "Transfer learning was originally introduced as a solution to low-resource (both domain and language) NMT. With respect to translation of LRL pairs, transfer learning using a high-resource pair always yielded better results than training the child model from scratch. This holds even for extremely LR children as well~. Interestingly, some research has shown that transfer learning is better than training a child pair (or a set of pairs ) with one or more parent pairs in a multi-NMT manner~. However, that research has been conducted against an early multi-NMT model~, considering very few languages. Whether the same observation would hold if a more novel multi-NMT model (discussed in Section~\\ref{sec:multiNMT}) is used along with a large number of language pairs should be subject to more research. On the other hand, transfer learning using pre-trained multi-NMT parent models has received only limited attention~. As mentioned above, multiple factors affect the success of transfer learning. Thus the impact of these factors should be evaluated extensively to determine their exact impact on LRL-NMT. Zero-shot translation adds an extra condition to the cold-start scenario, meaning that child parallel data unavailable, as discussed in Section~\\ref{sec:zero_shot}. \n\\iffalse\ndomain-shift problem - transfer learning has no explicit\ntraining process to guarantee that the source and pivot languages\nshare the same feature distributions, causing that the\nchild model inherited from the parent model fails in such\na situation (Ji 2020)\nfine-tuning technique, which forces the model to\nforget the specific knowledge from parent data and learn new\nfeatures from child data. - (Ji) - but this needs child data, and will not work in a zero-shot case.\nHere, the vocabulary mismatch between\nlanguages is still a problem, which seriously limits\nthe performance especially for distant languages.\n In the second\nscenario, training data covering different language directions\nare not available at the same time (most real-world MT training\nscenarios fall in this category, in which new data or new\nneeds in terms of domains or language coverage emerge over\ntime). In such cases, either: i) new MT models are trained\nfrom scratch with new vocabularies built from the incoming\ntraining data, or ii) the word segmentation rules of a prior\n(parent) model are applied on the new data to continue the\ntraining as a fine-tuning task. In all the scenarios, accurate\nword segmentation is crucial to avoid out-of-vocabulary\n(OOV) tokens. However, different strategies for the different\ntraining conditions can result in longer training time or performance\ndegradations. More specifically, limiting the target\ntask with the initial model vocabulary will result in: i) a word\nsegmentation that is unfavorable for the new language directions\nand ii) a fixed vocabulary/model dimension despite the\nvarying language and training dataset size.\nadapting from seed models is a good\nstrategy for rapid construction of MT systems in\nnew languages.(neubig)\nconcern 1 - (reduce lexical divergence between source and target) finding a representation of the data that ensures a sufficient overlap between the vocabularies of the languages.\nintial word-based translation (zoph) does not support this overlap. solutions BPE- transliteration (neubig).\nconcern 2 - language relatedness. this improves vocab overlap between source and target models even if parent is also LR(neubig).\nwhat to transfer - everything except the target is the best(zoph)\ntrainig strategies - 1.similar language regularization (Neubig)\nmulti-parent beter than bilingual parent, especially in cold-start (Neubig). \nAlythough it has been shown that TL is better than multiNMT (Kim) these evaluations worked with small nmber of languages. so difficult to make that claim. \nproblem of parent-child shared vocab is child has to be always there. A shared vocabulary is also problematic in that\nit must be divided into language-specific portions.\nWhen many languages share it, an allocated portion\nfor each will be smaller and accordingly less\nexpressive. This is the reason why the vocabulary\nis usually shared only for linguistically related languages,\neffectively increasing the portion of common\nsurface forms. (Kim)\neven langauge scrpipt is a concern (Kokmi - wfficiently reusing )\n\\fi", "cites": [5030, 5032, 8874, 5042], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of transfer learning in low-resource NMT, connecting several cited works to highlight trends and limitations. It synthesizes key ideas such as the benefits of pre-trained models, vocabulary mismatch, and language relatedness. However, the critique is somewhat limited, and the section does not fully abstract beyond the specific examples to present a higher-level framework or principle."}}
{"id": "6cc6b627-54f2-4960-be95-cd6d02584470", "title": "Zero-shot NMT", "level": "subsection", "subsections": [], "parent_id": "04e32557-9bb1-48a3-8d8c-57a570286aa7", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "NMT Techniques for Low-Resource Languages"], ["subsection", "Zero-shot NMT"]], "content": "\\label{sec:zero_shot}\nIn the zero-shot scenario, no parallel corpus is available for the considered source(X)-target(Z) language pair. We have identified pivoting, transfer learning, multi-NMT and unsupervised NMT as existing solutions in the literature for zero-shot NMT. \n\\textbf{Pivot-based solutions:}\nAn initial solution for zero-shot translation was the pivot-based translation, also known as pivoting. Pivoting relies on the availability of an intermediate high-resource language (Y), called the `pivot language'. In pivoting, the translation of X-Z is decomposed into the problem of training the two high-resource independent models: source-pivot (X-Y) and pivot-target (Y-Z). A source sentence is first translated using the X-Y model, the output of which is again translated using the Y-Z model to obtain the target sentence.\\\\\nThis basic form of pivoting has two main limitations. First, it suffers from the error propagation problem. Since the source-pivot and pivot-target models are independently trained, errors made in the first phase are propagated into the second phase. This is particularly the case when the source-pivot and pivot-target languages are distantly related. Second,  as models have to be independently trained, the total time complexity is increased. \nTo reduce the problem of error propagation, source-pivot and pivot-target models can be allowed to interact with each other during training by sharing the word embedding of the pivot language~. Another solution is to combine pivoting with transfer learning~. Here, the high-resource source-pivot and pivot-target models are first independently trained as in the basic pivoting technique, acting as the parent models. Then the source-target model (child model) is initialized with the source encoder from the pre-trained source-pivot model, and the target decoder from the pivot-target model. In addition to reducing the error propagation, this method reduces time complexity, since only one trained model is used for translation. Another way to reduce error propagation is to use a source-pivot parallel corpus to guide the learning process of a pivot-target model~.~ proposed a similar approach by training the source-target model via Maximum Likelihood Estimation , where the training objective is to maximize the expectation concerning a pivot-source model for the intended source-to-target model on a pivot-target parallel corpus.\nIt has been shown that adding even small amounts of true parallel source-target sentences (thus the extremely low-resource scenario) does increase the translation accuracy in pivoting~ . \nAnother possibility is to make use of monolingual data to generate synthetic parallel data. Pivot monolingual data is preferred because compared to source or target, pivot language would have much more monolingual data~.\nA common observation of the above discussed pivoting research (with the exception of~) is that, although the focus is on zero-shot translation between a source and target, large parallel corpora have been employed for source-pivot and pivot-target pairs. However, some LRLs may not have large parallel datasets even with a high-resource language such as English. Moreover, as empirically shown by~, the performance of pivoting depends on the relatedness of the selected languages. Thus, we believe that more research is needed to determine the impact of pivoting for zero-shot NMT in the context of LRL pairs.\n\\textbf{Transfer Learning-based Solutions}:\nAs mentioned in Section~\\ref{sec:transfer_learning}, transfer learning can be considered a form of zero-shot translation, when no parallel data is available for the child model.~ explored transfer learning for zero-shot translation by mimicking the pivoting technique, assuming a high-resource pivot language. They use source-pivot data to build a universal encoder, which is then used to initialize a pivot-target model. This model is used to directly translate source sentences into target sentences.\n\\textbf{Multi-NMT-based Solutions:}\nAlthough pivot-based models have been the solution for zero-shot NMT for a long time, recent research showed that multi-NMT models can outperform the pivot-based zero-shot translation~. Many-to-many models have recently shown to beat the English-centric multi-NMT models in zero-shot settings~.  \nA multi-NMT model can provide a reasonable translation between a source-target pair when the two languages are included in the model in the form of parallel data with any other language because the multi-NMT model is capable of learning an `interlingua' (see Section~\\ref{sec:multiNMT}). If the multi-NMT model is truly capable of learning a language-independent interlingua, there should be less correlation between the source and target languages. However, when the model is trained with a large number of languages, the modelling capacity (loosely measured in terms of the number of free parameters for neural networks~) has to be distributed across all the languages, suggesting that the overall model capacity is faced with a bottleneck~. Thus, the learned interlingua is not fully language independent. Two solutions have been presented to solve this problem: to explicitly make the source and target languages independent~, and to improve model capacity~. \nAs another solution, synthetic data can be generated between zero-shot language pairs using techniques such as pivoting  and back-translation . This synthetic parallel data is included in the multilingual corpus used to train the multi-NMT model. \n\\textbf{Unsupervised NMT-based solutions:}\nUnsupervised NMT techniques discussed in Section~\\ref{unsupervised_nmt} rely only on monolingual data. Thus, this translation task can be considered as a zero-shot translation task. \n\\iffalse\n\\begin{enumerate}\n\\item make the source and target languages independent:\n\\begin{itemize}\n   \\item pre-train the decoder as a multilingual language model~,\n    \\item iterative training of a multilingual model from source-target and target-source, conditioning the output on pre-trained strong source and target language models of the zero-shot language pair~,\n   \\item use regularization during the training process~, and \n   \\item use an additional loss function that serves as the objective to a new translation direction from source to source~.\n\\end{itemize}\n\\item improve model capacity:\n\\begin{itemize}\n    \\item add language-aware layer normalization and linear transformation in-between the encoder and decoder~, and \n    \\item add a parallel transformer layer, parameters of which are split by language or language group~.\n\\end{itemize}\n\\end{enumerate}\n\\fi", "cites": [5040, 5036, 5038, 5022, 5037, 7875, 8871, 5014, 5029, 5043, 7880, 5047, 5020], "cite_extract_rate": 0.7222222222222222, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.8, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key concepts from multiple cited papers on zero-shot NMT, integrating ideas like pivoting, transfer learning, multi-NMT, and unsupervised approaches into a coherent narrative. It critically discusses limitations such as error propagation and model capacity bottlenecks and identifies research gaps, particularly in the context of LRL pairs. The section also abstracts these methods into broader patterns, such as the role of interlingua and the trade-offs between model complexity and performance."}}
{"id": "8037047d-efca-4036-926d-d5e9d133f87a", "title": "Analysis on the Popularity of LRL-NMT Techniques ", "level": "subsection", "subsections": [], "parent_id": "04e32557-9bb1-48a3-8d8c-57a570286aa7", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "NMT Techniques for Low-Resource Languages"], ["subsection", "Analysis on the Popularity of LRL-NMT Techniques "]], "content": "\\label{techniques_trend}\nSo far we have discussed seven different techniques that can be used for the translation of LRL pairs, as well as zero-shot translation . This section provides a quantitative view of the use of these techniques in the related literature.      \n Figure~\\ref{fig:tech_time_GS} shows how the use of different techniques varied from 2014 onwards based on the research papers indexed in Google Scholar. For each of the technique, Google Scholar was searched with the following query: ``\\textit{<technique\\_name>}'' +  ``\\textit{low-resource}''  + ``\\textit{neural machine translation}'' for the year range 2014-2020. However, we  acknowledge that the search results contain noise. For example, in certain cases, unsupervised NMT research was referred in unsupervised text generation papers. However, here we are only interested in a comparative view, thus we assume the noise is equally distributed across the search results for all the techniques. \n\\iffalse\n\\sr{admit that there is noise}Figure xx shows the total number of search results from 2014 to 2020 for every technique.  It can be observed that the multilingual and unsupervised NMT techniques have gained the highest popularity among the low-resource NMT research community.  The lowest number of papers have been obtained for pivoting. This may be attributed to the fact that pivoting requires large parallel corpora between source and pivot language as well as  pivot and target language to achieve good performance. For low-resource languages, this is generally not the case and hence, this technique is not yet popular among the low-resource NMT community. \\sr{need to link this to pivoting section. same argument was made there. pivoting is optimal for HR languages in the zero-shot case. also very recently, MNMT was shown better for zero-shot }\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.5\\textwidth, keepaspectratio=True ]{Figures/Techniques_GS.pdf}\n    \\label{fig:1}\n    \\caption{No of Google Scholar results for different techniques }\n\\end{figure}\n\\fi\nFigure~\\ref{fig:tech_time_GS} shows that multi-NMT had the highest number of papers till 2019, however, unsupervised techniques have surpassed it marginally after that. \nIn particular, the use of multi-NMT for LRL pairs started growing with the promising results shown by~ and~ around 2016 and 2017. Transfer learning, and semi-supervised had similar growth till 2018, whereas from 2019 onwards transfer learning has seen a steep increase in popularity. Data augmentation techniques have gained popularity in 2020 as well. However, pivoting seems to have lost its traction, which may be due to the recent advancements in multi-NMT that outperformed pivoting for zero-shot translation~. Overall, it can be seen that the interest of the NMT research community towards LRLs is steadily increasing irrespective of the type of technique. \n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{Figures/Figure5.pdf}\n    \\caption{Number of Google Scholar search results($N_{GS}$) for different techniques from 2014-2020}\n    \\label{fig:tech_time_GS}\n\\end{figure}", "cites": [5020, 5017], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section attempts to provide an analytical view by examining trends in LRL-NMT techniques using Google Scholar data, but the synthesis is limited as it only references two papers and lacks deeper integration of ideas. Critical analysis is minimal, as it does not thoroughly evaluate the limitations or compare the effectiveness of the techniques. The abstraction is moderate, as it identifies general trends in popularity, but does not offer overarching principles or meta-level insights."}}
{"id": "d44ca8d5-e84a-4945-99d8-4b2a743f628f", "title": "Landscape of Low-Resource Languages and NMT Research", "level": "section", "subsections": ["821b6860-1647-4c8f-8b6f-b06326c22e0a", "219c10ea-b64c-4082-a033-74f5b58fa214"], "parent_id": "109ef38f-b3f2-41dd-a765-4a141b2d87cf", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "Landscape of Low-Resource Languages and NMT Research"]], "content": "\\label{section:trend_analysis}\nThere are over 7000 languages being spoken around the world. A look into the related research reported in Section~\\ref{section:label-NMT-Techniques-for-Low-Resource-Languages} reveals the NMT techniques have mostly been tested on the same set of languages. Identifying reasons for this imbalance in language selection would lead to efforts for more language diversity and inclusion in NMT research. We built upon the work by   in which $2485$ languages have been divided into 6 classes (see Table ~\\ref{tab:language_categories}) based on the amount of publicly available un-annotated and annotated corpora. Although  did not specifically refer to parallel data available for NMT, we hypothesize that there exists a strong correlation between the language class (and consequently the amount of publicly available data for that language) and the amount of NMT research available for this language.", "cites": [7874], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates a single cited paper to frame the discussion on language diversity in NMT research, showing basic synthesis. It offers some critical thought by hypothesizing a correlation between data availability and research focus, but does not deeply evaluate or contrast the cited work. The section begins to generalize by pointing to broader trends in NMT research and the need for language inclusion, offering meta-level insight at a moderate level."}}
{"id": "821b6860-1647-4c8f-8b6f-b06326c22e0a", "title": "Methodology", "level": "subsection", "subsections": [], "parent_id": "d44ca8d5-e84a-4945-99d8-4b2a743f628f", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "Landscape of Low-Resource Languages and NMT Research"], ["subsection", "Methodology"]], "content": "We queried Google Scholar with the query ``\\textit{neural machine translation}'' + ``\\textit{language}'' (e.g. ``\\textit{neural machine translation}'' + ``\\textit{Hindi}''). We excluded results before 2014 along with patents and citations. It should be noted that the search results were noisy; the most common among them being the ambiguity in the language name, where the language name is the same as the other entities such as location or author name. For example, the language `Swati', a Bantoid language spoken in Africa is also a common Indian name. Therefore, we manually checked and removed 240 such languages from our analysis.  \nIn order to find the LRLs that have been frequently used by the NMT community, we studied the outlier languages in language Classes 0-2 in , using the obtained Google Scholar search results ($N_{GS}$). For each class $c$, the outlier languages were identified using the following equation:\n\\begin{equation}\n N_{GS}^l > Q_3^c + 1.5IQR^c\n\\label{eq:outliers}\n\\end{equation} \nwhere $N_{GS}^l$ represents the number of Google Scholar results obtained for a language $l$, $Q_3^c$ is the third quartile and $IQR^c$ is the interquartile range of Google Scholar results for a language class $c$. In order to ascertain the factors responsible for the interest of the researchers towards these languages, we manually selected some of these outliers with geographical variations and plotted the number of search results with respect to the year. \nThese languages are selected such that it has a diverse mix of language class\\footnote{Hausa and Swahili (from African macroarea) were not among the outlier languages, yet they were included in the plot to show geographical diversity.}.", "cites": [7874], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates the cited paper by using its language classification framework to analyze outlier languages in NMT research. It shows some synthesis by connecting the data on Google Scholar results to patterns in research interest. However, the analysis is limited in depth and critical evaluation of the cited work is minimal. It identifies broader trends in language usage and interest, contributing some level of abstraction."}}
{"id": "219c10ea-b64c-4082-a033-74f5b58fa214", "title": "Results and Discussions", "level": "subsection", "subsections": [], "parent_id": "d44ca8d5-e84a-4945-99d8-4b2a743f628f", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "Landscape of Low-Resource Languages and NMT Research"], ["subsection", "Results and Discussions"]], "content": "We found 12.6\\%, 11.2\\% and 7.1\\% of languages as outliers for Class 0-2 respectively. A few random outlier examples are Sinhala \\& Slovene (Class 0),  Nepali \\&  Telugu (Class 1), and Irish (Class 2), as shown in Figure~\\ref{fig:boxplot_time}(a). We identified the possible factors responsible for the growth of research for some languages and put them into four categories: geographic considerations, dataset availability, open source frameworks and models, and community involvement. \n\\textbf{Geographic considerations }: We hypothesize that the geographical location where a language is spoken might play an important role in the growth of that language.\nTo validate the importance of geography, we looked at the outlier languages from Class 0-2 with respect to their geographical location\\footnote{The geographical area of a language is determined by using WALS data~}. In Figure~\\ref{fig:geo_corr}(a), we plot the percentage of outlier vs its geographical region. We found that in Class 0, approximately 25\\% of the outliers are languages from the European region, whereas in Class 1, approx 7\\% of the total languages from Europe are outliers.\nThus it is safe to assume that the early growth for NMT was mostly driven by the geographical location of the language. This could be due to the availability of funds, resources, and regional level joint projects. One of the prominent examples is the growth of European languages. Some of the recent trends are also supporting this. For example, the steady increase in the research activity for Irish and Slovene, outliers from Class 2 and Class 0 respectively (see Figure ~\\ref{fig:boxplot_time}) might be due to their presence in the European region.  \n\\begin{figure}\n    \\begin{subfigure}\n        \\centering\n        \\includegraphics[width=0.45\\textwidth, height=4.5cm]{Figures/Figure7a.pdf}\n    \\end{subfigure}\n    \\begin{subfigure}\n         \\centering\n        \\includegraphics[width=0.45\\textwidth, height=4.5cm]{Figures/Figure7b.pdf}\n    \\end{subfigure}\n    \\caption{ a) Boxplot showing the distribution of Google Scholar search results ($N_{GS}$) for different language classes. Outliers are marked by black markers and have been calculated independently for each class b)Time-varying trends of a few selected outlier languages from Classes 0-2. Y-axis represents the year and the x-axis represents the number of Google Scholar search results($N_{GS}$) }\n    \\label{fig:boxplot_time}\n\\end{figure}\n\\textbf{Dataset availability}: The next source of growth is driven by the availability of the datasets. For example, Sinhala and Nepali, outlier languages from Class 0 and Class 1, respectively have seen a steep rise from 2018-19 onwards (see Fig~\\ref{fig:boxplot_time}(b)). One reason could be due to the release of the FLoRes Evaluation Datasets ~, which includes both Sinhala-English and Nepali-English.  Our analysis revealed that the inclusion of a language in the standard yearly challenge such as WMT has a considerable impact on its growth in terms of NMT. For example, WMT-2019~ shared task had Nepali-Hindi parallel corpus. Similarly, Nepali and Sinhala were also part of Google's 102 languages corpus~.\nSimilarly, the increase in the number of publications for the Gujarati language from 2019 onwards could be attributed to the fact that the Gujarati-English language pair was included in the Shared Machine Translation task in WMT 2019~.\nTo quantify the relationship between the availability of datasets and research activity around that language, we used the resource matrix\\footnote{More information on this source: http://matrix.statmt.org/resources/matrix?set=all}. \nThis contains the details of the number of monolingual corpora and parallel corpora for 64 languages. Even though the list is not exhaustive, it is helpful for the growth analysis as it contains the languages from all the classes.  In Figure~\\ref{fig:geo_corr}(b), we plot the total number of datasets available v.s. the research activity (number of Google Scholar results for NMT) for a particular language. The number of datasets (X-axis) has been calculated by summing the number of monolingual datasets available for a source language and parallel corpora available between a source language and the other target languages.  It can be observed that the availability of datasets is directly correlated with the research activity ($r=0.88$), which further strengthens our claim that the NMT growth for a particular language is directly proportional to the data availability.  \n\\textbf{Open-source frameworks and models accessibility}\nThe availability of open-source frameworks and models is a major contributing factor towards the growth of research in the area of NMT. Frameworks such as OpenNMT~, and fairseq , as well as pre-trained models such as mBART  provide an easy and scalable implementation that helps in building a baseline and improving it for existing and new languages. These open-source projects are periodically maintained, flexible, and provide most of the latest NMT-related techniques. Since these projects provide standardized codes, it becomes easy to adapt for the LRLs even by novice researchers. It eliminates the need to develop the codes from scratch and helps in accelerating the research process. \n\\textbf{Community involvement}:  A recent development is a group of like-minded researchers coming together to increase the visibility of MT systems in the context of languages used in a particular region. It consists of both dataset building and the development of the standardized code and also focuses on training a new generation of enthusiasts to carry forward the work. One of the prominent examples is the Masakhane project~, which aims to put the Africa AI, specifically African language MT, into the world map. Within about two years, the Masakhane community has covered more than 38 African languages and resulted in multiple publications~. \nAs we could see from Figure~\\ref{fig:boxplot_time}(b), two of the representative languages, Swahili and Hausa, have a steep growth after 2018, which coincides with the inception of the Masakhane project. \n\\begin{figure}\n    \\centering\n        \\begin{subfigure}\n         \\centering\n           \\includegraphics[width=0.45\\textwidth, height=4.5cm]{Figures/Figure8a.pdf}\n        \\end{subfigure}\n    \\begin{subfigure}{}\n         \\includegraphics[width=0.45\\textwidth, height= 4.5cm ]{Figures/Figure8b.pdf}\n        \\end{subfigure}\n    \\caption{ a) Percentage of outliers from 7 different geographical regions for Classes 0-2 b) Relationship between the number of datasets available and number of Google Scholar search results ($N_{GS}$)}\n\\label{fig:geo_corr}\n\\end{figure}\nOur results and analysis highlight i) the importance of community building and region-level projects, ii) the inclusion of LRL datasets into yearly challenges and large multilingual datasets, and iii) the availability of open source models and frameworks to increase the focus on LRLs in the NMT landscape. This analysis could provide a cue to the researchers and funding agencies worldwide for the development of LRL resources.", "cites": [7881, 2489, 8871, 7882, 4751], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes findings from multiple sources, particularly connecting the availability of datasets (e.g., WMT, Google's 102 languages) and open-source tools (e.g., mBART, OpenNMT, fairseq) to the growth in low-resource NMT research. It provides a critical analysis by evaluating the role of geographic and community-driven factors in shaping research trends. The section abstracts beyond individual papers to identify broader patterns such as the influence of region-specific projects and standardization of tools on the low-resource NMT landscape."}}
{"id": "90aa37a6-013e-4588-97e2-8bc2a4edcf50", "title": "Model Capacity to Include LRLs", "level": "paragraph", "subsections": [], "parent_id": "e95a4d68-d978-47dd-ac03-bfac25ef9ccc", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "Discussion"], ["subsection", "Open Questions for LRL-NMT and Future Directions"], ["subsubsection", "Model Improvements"], ["paragraph", "Model Capacity to Include LRLs"]], "content": "Massive multi-NMT models is a promising technique, especially for systems produced by multinational tech companies ~. In particular, multi-NMT for zero-shot translation is an important line of research, as it eliminates the need for parallel datasets between every possible language. The work of~ is of particular interest, as it deals with a non-English-centric multilingual dataset, yet managed to outperform English-centric models in zero-shot translation.\nHowever, despite multi-NMT being able to cover about 100 languages ~, only a small fraction of LRLs are included from the more than 7000 possible languages. Therefore, more efforts need to be invested in scaling multi-NMT models that are capable of handling a larger number of languages, which would inherently cover LRLs and extremely LRLs. It is important to investigate how these LRLs are better represented in these massive models without compromising the performance of high-resource languages.~ recently reported promising results on this line, which should be further explored.", "cites": [8871, 5014, 5020], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes key findings from three cited papers to highlight the potential and limitations of multi-NMT for low-resource languages, particularly in zero-shot settings. It offers a critical view by pointing out the limited coverage of LRLs despite advances in multi-NMT. While it moves beyond mere description, the abstraction is moderate as it focuses on implications for model design rather than broader theoretical or methodological frameworks."}}
{"id": "a4888160-a1d3-4816-9976-3128b6e3693e", "title": "Model Robustness to Limiting Input Factors:", "level": "paragraph", "subsections": [], "parent_id": "e95a4d68-d978-47dd-ac03-bfac25ef9ccc", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "Discussion"], ["subsection", "Open Questions for LRL-NMT and Future Directions"], ["subsubsection", "Model Improvements"], ["paragraph", "Model Robustness to Limiting Input Factors:"]], "content": "Techniques discussed in Section~\\ref{section:label-NMT-Techniques-for-Low-Resource-Languages} are limited by various input requirements such as the size of the datasets, domain of the datasets, and language relatedness. Although some ablation studies~ have been done to understand the effect of input requirements, they are not sufficient or not exhaustive in considering model robustness. \nFor the example of language similarity, LRLs that have syntactic differences between spoken and written forms (e.g. Sinhala) is a challenge. Another challenge is translating code-mixed data.\nIn terms of domain relatedness, most publicly available datasets are from sources that do not resemble real-life translations. Thus, more focus should be given to multilingual domain adaption research, where pre-trained multi-MNT models can be fine-tuned to the task at hand (e.g.~translating legal or medical records). \nThus, empirical experimentation with the goal to extend the field to new approaches and architectures to address these limitations must be conducted.", "cites": [7164, 5020], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section touches on general challenges like language similarity and domain mismatch but does not deeply synthesize the cited papers into a coherent framework. It identifies some gaps, such as insufficient ablation studies on model robustness, indicating a moderate level of critical analysis. The discussion hints at broader research directions but lacks a strong meta-level abstraction or integration of ideas across sources."}}
{"id": "8f89745a-a706-48db-af31-157764247170", "title": "Model Interpretability and Explainability:", "level": "paragraph", "subsections": [], "parent_id": "e95a4d68-d978-47dd-ac03-bfac25ef9ccc", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "Discussion"], ["subsection", "Open Questions for LRL-NMT and Future Directions"], ["subsubsection", "Model Improvements"], ["paragraph", "Model Interpretability and Explainability:"]], "content": "Explaining and interpreting neural models is a challenge for researchers in Deep Learning.  \nFor multi-NMT, there have been experiments  that delve into models to examine how the learning takes place, and how different languages behave. There have also been attempts to provide theoretical interpretations on aspects of NMT models~. However, we believe more research in NMT model interpretability (such as interlingua learning and parent-child transfer) would help developing solutions specific to LRLs.", "cites": [7883, 5048], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section briefly synthesizes two papers on multilingual NMT and representation learning, connecting their findings to the theme of model interpretability in the context of low-resource languages. However, it lacks deeper critical evaluation or comparison of the cited works and only hints at potential future research areas without fully abstracting broader principles or frameworks."}}
{"id": "960a4c69-e893-473d-be0e-39cdf2bed7ce", "title": "Mitigating Model Bias:", "level": "paragraph", "subsections": [], "parent_id": "e95a4d68-d978-47dd-ac03-bfac25ef9ccc", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "Discussion"], ["subsection", "Open Questions for LRL-NMT and Future Directions"], ["subsubsection", "Model Improvements"], ["paragraph", "Mitigating Model Bias:"]], "content": "It has already been shown that gender bias exists in NMT models when trained on high-resource languages~. Recently, the existence of biases in the context of LRL MT is a problem that has not come into light as far as we know. In particular, when translating between languages belonging to different regions and cultures in multilingual settings, there can be undesirable influences from high-resource languages. As discussed later in this section, parallel data extracted from the web tends to contain bias. While there should be parallel efforts to free datasets of bias, it is important to develop models that are robust to dataset bias in order to prevent the ramification of propagating stereotypes from the social context of high-resource languages in to LRLs.", "cites": [5049, 5050], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes the cited papers to highlight the issue of bias in NMT, particularly gender bias in high-resource settings and the potential for bias propagation in low-resource scenarios. While it makes a basic connection between the two papers and the broader issue of dataset bias, it lacks deeper comparative or critical analysis of their methodologies or findings. The abstraction is limited to general concerns about bias in multilingual and low-resource settings without identifying broader principles or frameworks."}}
{"id": "90aa4a2e-2cf7-4491-bcdc-01ceee0a0f76", "title": "Creation  of Datasets:", "level": "paragraph", "subsections": [], "parent_id": "48c2e5ce-c935-420b-9f92-9cde5b66379a", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "Discussion"], ["subsection", "Open Questions for LRL-NMT and Future Directions"], ["subsubsection", "Equitable and Inclusive Access"], ["paragraph", "Creation  of Datasets:"]], "content": "Most of the datasets used in NMT have originated in a small number of regions in the world focusing on English-centric translations, however about 40\\% the content on the Internet is now non-English\\footnote{\\url{https://www.visualcapitalist.com/the-most-used-languages-on-the-internet/}}.\nAlthough LRL-NMT was initially applied to large corpora by sub-sampling HRLs such as English, French, and German, \nit was later adapted for LRL pairs such as English-Esperanto, ,\nEnglish-Urdu, English-Romanian, and English-Russian \\footnote{ excluded because low-resource was suggested but not experimented}.\nThis focus shifted to European LRLs, and more recently to non-European languages such as Indian, African, East Asian, and Middle Eastern languages ~.\nHowever, the amount of such datasets is less than high resource counterparts. \nTherefore, on par with current trends in some Machine Learning communities, more focus can be given to those that present new LRL datasets rather than the novelty of the employed technique, when accepting papers to conferences and evaluating value of this type of research. Contribution of conferences such as LREC, and journals such as the LREC journal is commendable, in this regard.  Projects such as ParaCrawl~ have automatically mined a large amount of parallel data from the web for multiple language pairs, including LRL such as Irish and Nepalese.  \nIn addition, regional communities can take the lead in dataset creation due to their expertise in their own cultural context thus could provide better judgement on the bias (discussed in the next point). \nMore recently, the problem of dataset bias has received significant attention from the community .  For example, the public crawl data available demonstrate a narrow sub-segment of population and may encode values and perspectives that exist within Western society.  More specifically, it has been shown that scrapped text contains geographical bias~, as well as an age and gender bias~. \nFurthermore, these web crawl datasets contain the potential to harm due to abusive language, hate speech, microaggressions, dehumanization, and social-political biases~. Thus, data pre-processing mechanisms can be employed with input from experts such as social scientists, regional communities, and linguists familiar with different languages.", "cites": [7881, 1551, 8868, 7695, 7877, 4997, 8875, 5004], "cite_extract_rate": 0.5, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes ideas from multiple papers to highlight the issue of dataset creation and bias in low-resource NMT. It connects the dots between data scarcity, geographic bias, and harmful content in web-mined datasets. While it offers some critical analysis of biases and limitations in current data practices, the critique remains moderate and does not deeply evaluate individual papers. The section abstracts some broader patterns, such as the uneven geographic representation and the need for expert input in preprocessing, but lacks a more comprehensive theoretical or methodological framework."}}
{"id": "1e1bc7b5-ad46-4285-89b8-d611294d4885", "title": "Availability of Trained models:", "level": "paragraph", "subsections": [], "parent_id": "48c2e5ce-c935-420b-9f92-9cde5b66379a", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "Discussion"], ["subsection", "Open Questions for LRL-NMT and Future Directions"], ["subsubsection", "Equitable and Inclusive Access"], ["paragraph", "Availability of Trained models:"]], "content": "Although the community has a recent focus on developing computationally efficient NMT models~, the public release of large-scale multi-NMT models has been limited, with the exception of the work by~.  By giving public access to these models, these time-consuming and expensive models can be used as the parent models to be transferred to the LRL child models.  \nTherefore publicly releasing NMT models including massive multi-NMT models would be tremendously beneficial to those working in LRL-NMT as well as advancing the field in other areas.", "cites": [2489, 5051], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section briefly mentions the limited public release of multi-NMT models and highlights their potential value for LRL-NMT, referencing two relevant papers. While it begins to connect the broader issue of model accessibility with practical implications for low-resource settings, the synthesis is minimal and lacks a deeper comparative or evaluative analysis of the cited works. It does, however, generalize to the importance of shared resources for advancing the field."}}
{"id": "a92021b0-98cb-44e3-9182-4eac14c1aaea", "title": "Availability of Computational Resources:", "level": "paragraph", "subsections": [], "parent_id": "48c2e5ce-c935-420b-9f92-9cde5b66379a", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "Discussion"], ["subsection", "Open Questions for LRL-NMT and Future Directions"], ["subsubsection", "Equitable and Inclusive Access"], ["paragraph", "Availability of Computational Resources:"]], "content": "The community has a recent focus on developing computationally efficient NMT models and providing computational resources for researchers \\footnote{Some tech giants also provide research grants to use their cloud GPUs platforms.}. However, more efforts need to be put forth by the research community, industry organizations, and governments in distributing resources and attention. Furthermore, computational resources can also be made available as part of conferences and challenges to further encourage LRLs researchers to participate.\n\\iffalse", "cites": [5051], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a brief analytical discussion on the availability of computational resources for LRL-NMT, noting recent trends and the need for more support. However, it lacks detailed synthesis of the cited paper and does not go into depth on specific limitations or comparisons. The abstraction is limited to general observations rather than broader principles or frameworks."}}
{"id": "7a91fca2-9bcf-44d5-8506-8c71f41f61f3", "title": "Model Interpretability:", "level": "paragraph", "subsections": [], "parent_id": "48c2e5ce-c935-420b-9f92-9cde5b66379a", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "Discussion"], ["subsection", "Open Questions for LRL-NMT and Future Directions"], ["subsubsection", "Equitable and Inclusive Access"], ["paragraph", "Model Interpretability:"]], "content": "\\sr{this should be written in a general sense, eithout specifically focusing on mnmt. We can use Aji's paper on tranfer learning}\\eal{I think we should focus on LRL-NMT, there are plenty of work that are general in NLP} \\sr{i mean, write this wrt nmt. currently it talks abut mnmt only }The learning of interlingua by multi-NMT models while maintaining language-specific representations is still not properly explained. In fact, this is owing to the fact that most Deep Learning models are treated as black boxes.  and  reported initial experiments to `look inside' the multi-NMT models to see how the learning actually takes place, and  how different languages behave. We believe more research in this line would help developing solutions specific to different language-groups, in particular the under-represented ones.\\sr{check on model interpretability}\\\\", "cites": [7883, 5048], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section touches on the issue of model interpretability in multilingual NMT and cites two relevant papers, but the synthesis is limited as it only loosely connects these works without deeply integrating their findings. There is minimal critical analysis of the cited works, focusing instead on the general problem of black-box models. Some abstraction is attempted by suggesting that interpretability research could benefit under-represented language groups, though the section remains somewhat focused on specific papers rather than forming broader meta-level insights."}}
{"id": "990a89e4-44a3-4075-afbe-1688e31abd63", "title": "LR Dataset Creation:", "level": "paragraph", "subsections": [], "parent_id": "6762be07-e814-45fe-acf7-e19054b513a2", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "Discussion"], ["subsection", "Open Questions for LRL-NMT and Future Directions"], ["subsubsection", "Equitable and Inclusive Access:"], ["paragraph", "LR Dataset Creation:"]], "content": "\\eal{note that there is also a more focus on dataset in the ML community, neurips has a workshop on dataset creation this year...}\nMost of the NMT research focus on English-centric translation. Even most of the parallel datasets are English-centric. However, with about  50\\% the content in the Internet is now non-English, it is important truly many-many models and datasets are introduced.  NMT has originally focused on HRL then moved into European language families.  More recently, there has been a focus non-European languages such as Indian, African, East Asian, and Middle Eastern languages.    had set the first step, by reporting a series of experiments on MBART, which is not English-centric. In fact, this dataset has shown much better promise for zero-shot translation, compared to the English-centric counter part.\\sr{fix the flow. i.e. say initial dataset were all European. then non-european datasets came out. however stillmost datsaets are english-centris. btw, after MBART now there is mT5 by Google. check their one too }\\\\\nMost of the publicly available datasets are from sources such as TED talks, Bible or open subtitles. These datasets do not resemble the type of data that needs to get translated in real-life. In particular, for (low-resource) languages that have syntactic differences between spoken and written forms (e.g. Sinhala) this is a major concern. Thus, more focus should be given to multilingual domain adaption research, where low-resource languages would be able to fine-tune a pre-trained multi-MNT model to the task at hand (e.g.~translating official government documents, or translating code-mixed social media data).", "cites": [5014], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates the cited paper (Beyond English-Centric Multilingual Machine Translation) to highlight the shift from English-centric to more globally representative datasets in NMT. It critically points out the limitations of current datasets and emphasizes the need for domain adaptation for low-resource languages, suggesting future research directions. While it identifies a broader trend and its implications, the synthesis and abstraction are not fully developed and remain somewhat at the level of summarizing and reacting to a single paper."}}
{"id": "39a6c395-f8c2-479d-9482-d32c5e36a04a", "title": "Public Availability of Models:", "level": "paragraph", "subsections": [], "parent_id": "6762be07-e814-45fe-acf7-e19054b513a2", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "Discussion"], ["subsection", "Open Questions for LRL-NMT and Future Directions"], ["subsubsection", "Equitable and Inclusive Access:"], ["paragraph", "Public Availability of Models:"]], "content": "As discussed in Section~\\ref{sec:multiNMT}, multi-NMT is a very promising avenue forward. Our trend analysis confirmed the same. However, only few such models have been publicly released. Given that training large multi-NMT models is very much time-consuming, the unavailability of such pre-trained models prevent the less-privileged researchers working in on NMT for their low-resource languages to take benefit of transfer learning on these large models.\\\\", "cites": [5014], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes a limited amount of information, primarily referencing one paper (doc_id: 5014) to discuss the English-centric nature of multi-NMT. It offers a basic critique by highlighting the issue of public model availability and its impact on less-privileged researchers. While it touches on broader implications for inclusivity, it lacks a more comprehensive abstraction or meta-level insight that could tie together multiple trends or principles."}}
{"id": "b7c63987-00ec-48cd-bc30-200310594b53", "title": "Gender Bias:", "level": "paragraph", "subsections": [], "parent_id": "d4b234cc-9f43-49eb-80ea-4e09bb9ae8b3", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "Discussion"], ["subsection", "Open Questions for LRL-NMT and Future Directions"], ["subsubsection", "Bias and Fairness:"], ["paragraph", "Gender Bias:"]], "content": "\\sr{isnt geneder bias a sub-category of dataset bias?}\nRecent works in high-resource NLP and NMT has identified bias and fairness as a priority for the field.  More specifically, gender-bias has been studied extensively in high-resource NMT, where gender is not faithfully translated.  For example,  identified that occupations such as nurse is related to female and doctor is related to male irregardless of the original association.  This incorrect inference is attributed to the inherent social bias in the dataset, which encodes the stereotypes into the model  .  These biases (not just gender, but also possibly race and religion) is then perpetuated into applications such as job postings or news articles, which causes discrimination in the wider social context.", "cites": [5049, 7695], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces the issue of gender bias in NMT, referencing two relevant papers, but does not deeply synthesize their findings into a broader narrative. It lacks critical analysis of the cited works and does not generalize the concept of gender bias to a meta-level understanding or broader patterns in LRL-NMT."}}
{"id": "f663be56-c11a-4f7d-b0f8-927527456463", "title": "Newly found papers", "level": "subsection", "subsections": [], "parent_id": "2c4eae1c-0595-4c9b-9650-df8b610079cd", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "Language wise Analysis"], ["subsection", "Newly found papers"]], "content": "\\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n     Pre-training via Leveraging Assisting Languages and Data Selection for Neural Machine Translation\\\\\n    Participatory Research for Low-resourced Machine Translation: A Case Study in African Languages \\\\ \n     Findings of the 2019 Conference on Machine Translation ({WMT}19)\\\\\n     Neural machine translation of rare words with subword units\\\\\n     Neural Machine Translation with Reconstruction \\\\\n      Parallel corpora for medium density languages\\\\\n      Translating a Language You Don{'}t Know In the {C}hinese Room\\\\ \n     JASS: Japanese-specific Sequence to Sequence Pre-training for Neural Machine Translation\\\\ \n     SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing,  not LR (English-Japanese)\\\\\n     Multilingual Neural Machine Translation involving {I}ndian Languages\\\\\n     English-Myanmar Supervised and Unsupervised NMT: NICTs Machine Translation Systems at WAT-2019\\\\ \n    The {IIIT}-{H} {G}ujarati-{E}nglish Machine Translation System for {WMT}19\\\\\n     Malay-Corpus-Enhanced Indonesian-Chinese Neural Machine Translation\\\\\n     Language Revitalization: A Benchmark for Akan-to-English Machine Translation\\\\\n    {E}nglish-{M}yanmar Supervised and Unsupervised {NMT}: {NICT}{'}s Machine Translation Systems at {WAT}-2019\\\\\n    A grounded unsupervised universal part-of-speech tagger for low-resource languages\\\\\n    No Data to Crawl? Monolingual Corpus Creation from {PDF} Files of Truly low-Resource Languages in {P}eru\\\\\n     {L}atin-{S}panish Neural Machine Translation: from the {B}ible to Saint Augustine\n     PidginUNMT: Unsupervised Neural Machine Translation from West African Pidgin to English\\\\\n    Chinese-Japanese Unsupervised Neural Machine Translation Using Sub-character Level Information\\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    \\\\\n    The FLoRes evaluation datasets for low-resource machine translation: Nepali-english and sinhala-english\\\\\n    The TALP-UPC System for the WMT Similar Language Task: Statistical vs Neural Machine Translation\\\\\n    ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization\\\\\n    Low Resource Neural Machine Translation: A Benchmark for Five African Languages\\\\\n     {CVIT}{'}s submissions to {WAT}-2019\\\\\n    \\\\\n     Effectively Aligning and Filtering Parallel Corpora under Sparse Data Conditions\\\\", "cites": [5054, 8559, 5055, 5060, 7884, 5059, 7885, 8876, 5056, 2414, 7886, 5058, 5057, 5052, 5053, 7881], "cite_extract_rate": 0.24242424242424243, "origin_cites_number": 66, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.2, "abstraction": 1.4}, "insight_level": "low", "analysis": "The section primarily lists the titles and brief contexts of papers without synthesizing their content or establishing connections between them. There is minimal critical evaluation or abstraction of broader themes or trends in low-resource NMT research, making the section mostly descriptive in nature with low insight quality."}}
{"id": "fbe8a8c8-27d1-447b-a82e-c35f0ae3c908", "title": "newly found papers", "level": "subsection", "subsections": [], "parent_id": "2c4eae1c-0595-4c9b-9650-df8b610079cd", "prefix_titles": [["title", "Neural Machine Translation for Low-Resource Languages: A Survey"], ["section", "Language wise Analysis"], ["subsection", "newly found papers"]], "content": " The NiuTrans machine translation systems for WMT19\\\\\n    Edinburgh Neural Machine Translation Systems for WMT 16\\\\\n    Syntax-directed attention for neural machine translation\\\\\n     Character-Aware Low-Resource Neural Machine Translation with Weight Sharing and Pre-training\\\\\n     Sentence-Level Adaptation for Low-Resource Neural Machine Translation, supervised learning\\\\\n    Efficient Low-Resource Neural Machine Translation with Reread and Feedback Mechanism\\\\\n    Low resource languages - 2020 papers \\\\\n\\cite {valeev2019application} Application of Low-resource Machine Translation Techniques to Russian-Tatar Language Pair \\\\\n Low Resource Neural Machine Translation: A Benchmark for Five African Languages \\\\\n\\\\\n\\\\\n\\cite {duh2020benchmarking} Benchmarking Neural and Statistical Machine Translation on Low-Resource African Languages \\\\\nThe {LMU} {M}unich Unsupervised Machine Translation System for {WMT}19\\\\\n\\fi\n\t\\bibliographystyle{ACM-Reference-Format}\n\t\\bibliography{lowMT.bib}\n\\end{document}", "cites": [863, 3146, 8876], "cite_extract_rate": 0.3, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.2}, "insight_level": "low", "analysis": "The section simply lists several paper titles and one citation with abstract details, without synthesizing their contributions, making critical comparisons, or identifying broader trends or principles. It lacks a coherent narrative and fails to provide analytical depth or abstraction beyond the individual studies."}}
