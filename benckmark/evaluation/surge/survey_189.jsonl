{"id": "5f1b580b-053d-4e74-9cf8-577ec189f2e0", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "df6e31e1-1c33-44b0-a2a3-972d7f8830eb", "prefix_titles": [["title", "Generative adversarial networks in time series: A survey and taxonomy"], ["section", "Introduction"]], "content": "\\label{sec:intro}\nThis review paper is designed for those interested in GANs applied to time series data generation. We provide a review of current state-of-the-art and novel time series GANs and their solutions to real-world problems. The applicability of GANs to this type of data can solve many issues that current dataset holders face. Data shortage is often an issue, and GANs can augment smaller datasets by generating new, previously unseen data. Data can be missing or corrupted in cases; GANs can impute data, i.e. replace the artefacts with information representative of clean data. GANs are also capable of denoising signals in the case of corrupted data. Data protection, privacy, and sharing have become heavily regulated; GANs can ensure an extra layer of data protection by generating differentially private datasets containing no risk of linkage from source to generated datasets. \nSeveral methods have been used in the past to generate synthetic data. One such method is the autoencoder (AE) which is designed to efficiently learn an informative representation of an input in a small dimensional space and reconstruct the encoded data back such that the reconstructed input is similar as possible to the original one. The AE model is made of an encoder and decoder neural network, as shown in Figure \\ref{fig:AE_model}. However, other generative models have emerged as front-runners due to the quality of the generated data and inherent privacy protection measures.\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{images/AE.pdf}\n    \\caption{Autoencoder Model}\n    \\label{fig:AE_model}\n\\end{figure}\nGenerative Adversarial Networks have been gaining a lot of traction amongst the deep learning research community since their inception in 2014 . Their ability to generate and manipulate data across multiple domains has contributed to their success. While the main focus of GANs to date has been in the computer vision (CV) domain, they have also been successfully applied to others, such as natural language processing (NLP). There has also been a movement towards the use of GANs for time series and sequential data generation, and forecasting.\nA GAN is a generative model consisting of a generator and discriminator, typically two neural networks (NN) models. In recent years GANs have demonstrated their ability to produce high-quality image and video generation, style-transfer, and image completion. They have also been successfully used for audio generation, sequence forecasting, and imputation.\nHowever, one of the significant challenges of GANs lies in their inherent instability, which makes it difficult to train. GAN models suffer from issues such as non-convergence, diminishing/vanishing gradients, and mode collapse. A non-converging model does not stabilise and continuously oscillates, causing it to diverge. Diminishing gradients prevents the generator from learning anything as the discriminator becomes too successful. Mode collapse is when the generator collapses, producing only uniform samples with little to no variety.\nThe second challenge of GANs lies in its evaluation process. With image-based GANs, researchers have reached a loose consensus  surrounding the evaluation of the generated distribution estimated from the training data distribution. Unfortunately for time series GANs, due to the comparatively low numbers of papers published, there has not been an agreement reached on the generated data's evaluation metrics. There have been different approaches put forward, but none established as a front runner in the metrics space as of yet.\nWe define a time series as a sequence of vectors dependant on time $(t)$ and can be represented as $xt = {x1,..., xn}$ for continuous/real-time and discrete-time. The time series' values can either be defined as continuous or discrete and, depending on the number of values recorded, are univariate or multivariate. In most cases, the time series will take either an integer value or a real value. \nAs Dorffner states, a time series can be viewed, from a practical perspective, as a value sampled at discrete steps in time . This time-step can be as long as years to as short as milliseconds, for example. We define a continuous time series as a signal sampled from a continuous process, i.e. the function's domain is from an uncountable set. In contrast, a discrete time series has a countable domain. \nIn this review, we present the first complete review and taxonomy of time series GANs, namely discrete and continuous variants, their applications, architecture, loss functions and how they have improved on their predecessors in terms of variety and quality of their generated data. We also contribute by including experiments for the majority of time series GAN architectures applied to time series synthesis.", "cites": [1013], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic overview of GANs and their application to time series, citing one paper on evaluation measures. While it integrates some information (e.g., discussing evaluation challenges in time series GANs), it does so in a general way without connecting multiple sources or presenting a novel synthesis. It mentions limitations of GANs but lacks deeper critical evaluation or comparative analysis. Some abstract concepts like continuous vs. discrete time series are introduced, but the section remains largely focused on describing the field and its challenges."}}
{"id": "e384c871-31be-4ce4-acf9-3ab15c61fbd5", "title": "Related Work", "level": "section", "subsections": [], "parent_id": "df6e31e1-1c33-44b0-a2a3-972d7f8830eb", "prefix_titles": [["title", "Generative adversarial networks in time series: A survey and taxonomy"], ["section", "Related Work"]], "content": "\\label{sec:reviews}\nThere has been a handful of high-quality GAN review papers published in the past few years. For example, Wang \\textit{et al.} takes a taxonomic approach to GANs in Computer Vision . The authors split GANs into architecture variants and loss variants. While they include applications of GANs and mention their applicability to sequential data generation, the work is heavily focused on media manipulation and generation. The authors in  breakdown GANs into their constituent parts. They begin by discussing the algorithms and architecture of various GANs and their evaluation metrics, then list their surrounding theory and problems such as mode collapse, amongst others. Finally, they discuss the applications of GANs and provide a very brief account of GANs used for sequential data. Gonog and Zhou  provide a short introduction to GANs, their theory and explores the variety of plausible models, again listing their applications in image and video manipulation with a mention of sequential data (NLP). In another review, paper  the authors give an overview of GAN fundamentals, variants, and applications. Sequential data applications are mentioned in the form of music and speech synthesis.\nAs with most review papers, Yinka-Banjo and Ugot give an introduction and overview of Generative Adversarial Networks . However, they also review GANs as adversarial detectors and discuss their limitations applied to cybersecurity. Yi, Walia, and Babyn  give a review of GANs and their applications in medical imaging, how they can be used in clinical research and potentially deployed to help practising clinicians. There is no mention of time series data use cases.\nA recurring theme in these papers focuses on GAN variants which have mostly been applied to the computer vision domain. To the best of our knowledge, no review paper has been conducted with the main focus on time series GANs. While these reviews have mentioned the application of these GANs in generating sequential data, they have scratched the surface of what is becoming a growing body of research. \nWe contribute to lessening this gap by presenting our work which is concerned with presenting the latest up-to-date research around time series GANs, their architecture, loss functions, evaluation metrics, trade-offs and approaches to privacy preservation of their datasets.", "cites": [5906, 978, 87], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from several GAN review papers, connecting common themes such as the focus on computer vision and only brief mentions of sequential or time series data. It provides a critical perspective by identifying the lack of depth in covering time series applications and the absence of dedicated reviews in this area. The abstraction is moderate, as it highlights the general trend but does not fully explore overarching principles or theoretical implications specific to time series GANs."}}
{"id": "46f809d4-6b81-4969-8a41-91c813a0a65d", "title": "Background", "level": "subsection", "subsections": [], "parent_id": "e3144c4d-28bc-49df-b689-0f351b2878a8", "prefix_titles": [["title", "Generative adversarial networks in time series: A survey and taxonomy"], ["section", "Generative Adversarial Networks"], ["subsection", "Background"]], "content": "The introduction of GANs facilitated a significant breakthrough in the generation of synthetic data. These deep learning models typically consist of two neural networks, a generator and a discriminator. The generator \\textit{G} takes in random noise $\\textbf{\\textit{z}} \\in \\mathbb{R}^{r}$ and attempts generates synthetic data that is similar to the training data distribution. The discriminator \\textit{D} attempts to determine if the generated data is real or fake. The generator aims to maximise the failure rate of the discriminator, while the discriminator aims to minimise it. Figure \\ref{fig:GAN_model} shows a simple example of the GAN architecture and the game that the neural network models play. The two networks are locked in a two-player minimax game defined by the value function \\textit{V(G,D)} (\\ref{eq:1}), where \\textit{D(\\textbf{x})} is the probability that \\textit{x} comes from the real data rather than the generated data .\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{images/GAN.pdf}\n    \\caption{Generative Adversarial Network}\n    \\label{fig:GAN_model}\n\\end{figure}\n\\begin{equation}\n\\mathop{min}_{G} \\mathop{max}_{D}V(G,D)= \\mathbb{E}_{x \\sim p_{data}(x)}[logD(\\textbf{x})] + \\mathbb{E}_{z \\sim p_{\\textbf{z}}(z)}[log(1-D(G(\\textbf{z})))]\n\\label{eq:1}\n\\end{equation}\nGANs belong to the family of generative models and are an alternative method of generating synthetic data that do not require domain expertise. They were conceived in the paper by Goodfellow in 2014, where a multi-layer perceptron was used for both the discriminator and the generator . Radford \\textit{et al.} (2015) subsequently developed the deep convolutional generative adversarial network (DCGAN) to generate synthetic images . Since then, researchers have continuously improved on the early GAN architectures, loss functions, and evaluation metrics while innovating on their potential contributions to real-world applications. To appreciate why there has been such concerted activity in the further development of GAN technologies it is important to understand the limitations of early architectures and the challenges these presented. We describe these next, and in so doing, prepare the reader for the particular manifestation of these challenges in the more specific context of time series.", "cites": [1003], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic descriptive overview of GANs, including their architecture and foundational papers. It integrates a few key works (e.g., Goodfellow 2014, Radford et al. 2015) to present a coherent background, but does not connect multiple ideas in a novel or deep way. There is limited critical analysis or abstraction, as the focus remains largely on explaining the components and early developments of GANs without evaluating their broader implications or limitations."}}
{"id": "3a9fee16-f08e-47be-88ad-abbd4b299c2a", "title": "Challenges", "level": "subsection", "subsections": [], "parent_id": "e3144c4d-28bc-49df-b689-0f351b2878a8", "prefix_titles": [["title", "Generative adversarial networks in time series: A survey and taxonomy"], ["section", "Generative Adversarial Networks"], ["subsection", "Challenges"]], "content": "\\label{sec:challenges}\nThere are three main challenges in the area of time series GANs, i.e., training stability, evaluation and privacy risk associated with synthetic data created by GANs. We are going to explain these three challenges as follows.\n\\textbf{Training stability.}\\hspace{5pt} The original work~ has already proved the global optimality and the convergence of GANs during training. It still highlights the instability problem that can arise when training a GAN. Two problems are well-studied in the literature 1. vanishing gradients and 2. mode collapse. The vanishing gradient is caused by directly optimizing loss presented in equation~\\eqref{eq:1}. When $D$ reaches the optimality, optimizing the equation~\\eqref{eq:1} for $G$ can be converted to minimising the Jensen-Shannon (JS) divergence (details of derivation can refer to section 5 in~) between $p_r$ and $p_g$:\n\\begin{equation}\n    \\mathcal{L}_{G}=2\\cdot JS(p_{r}\\|p_{g}) - 2 \\cdot\\mathrm{log}2\n    \\label{eq:JS-loss-G}\n\\end{equation}\nJS divergence stays constant ($log2=0.693$) when there is no overlap between $p_r$ and $p_g$, which indicates the gradient for $G$ using this loss is 0 in this situation. Non-zero gradient for $G$ only exists when $p_r$ and $p_g$ have substantial overlap. In practice, the possibility that $p_r$ and $p_g$ are not intersected or have negligible overlap is very high~. In order to get rid of the vanishing gradient problem for $G$, the original GAN work~ highlights that the minimization of\n\\begin{equation}\n    \\mathcal{L}_{G}=-\\mathbb{E}_{\\mathbf{x}\\sim p_{g}}\\mathrm{log}[D(\\mathbf{x})]\n    \\label{eq:KL-loss-G}\n\\end{equation}\nfor updating $G$. This strategy is able to avoid the vanishing gradient problem but lead to the mode collapse issue. Optimizing equation~\\eqref{eq:KL-loss-G} can be converted to optimizing the reverse Kullback–Leibler (KL) divergence i.e., $KL(p_g||p_r)$ (details can also refer to~). When $p_r$ contains multiple modes, $p_g$ chooses to recover a single mode and ignores other modes when optimizing the reverse KL divergence. Considering this case, $G$ trained using equation~\\eqref{eq:KL-loss-G} might be only able to generate few modes from real data. These problems can be amended by changing the architecture or the loss function, which are reviewed by Wang~\\textit{et al.}~ in detail. \n\\textbf{Evaluation.}\\hspace{5pt} A wide range of evaluation metrics has been proposed to evaluate the performance of GANs~. Current evaluations of GANs in computer vision are normally designed to consider two perspectives i.e., quality and quantity of generated data. The most representative qualitative metric is to use human annotation to determine the visual quality of the generated images. Quantitative metrics compare statistical properties between generated and real images i.e., two-sample tests such as maximum mean discrepancy (MMD)~, Inception Score~ and Fr\\'{e}chet Inception Distance (FID)~. Contrary to evaluating image-based GANs, it is difficult to evaluate time series data from human psycho-perceptual sense qualitatively. In terms of qualitatively evaluating time series based GANs, it normally conducts\nt-SNE~ and PCA~ analyses to visualize how well the generated distributions resemble the original distributions~. Quantitative evaluation for time series based GANs can be done by deploying two-sample tests similar to image-based GANs.\n\\textbf{Privacy risk.}\\hspace{5pt} Apart from evaluating the performance of GANs, a wide range of methods have been used to asses the privacy risk associated with synthetic data created by GANs. Choi \\textit{et al.} performed tests for presence disclosure and attribute disclosure. In contrast, others utilised a three-sample test on the training, test, and synthetic data to identify if the synthetic data has overfitted to the training data . It has been shown that common methods of de-identifying data do not prevent attackers from re-identifying individuals using additional data . Sensitive data is usually de-identified by removing personally identifiable information (PII). However, work is ongoing to create frameworks to link different sources of publicly available information together using alternative information to PII. Malin \\textit{et al.} developed a software program, REID, to connect individuals contained in publicly available hospital discharge data with their unique DNA records . Culnane \\textit{et al.} re-identified individuals in a de-identified open dataset of Australian medical billing records using unencrypted parts of the records and known information about individuals from other sources . Hejblum \\textit{et al.} developed a probabilistic method to link de-identified EHR data of patients with rheumatoid arthritis . The re-identification of individuals in publicly available datasets can lead to the exposure of their sensitive health information. Health data has been categorised as special personal data by General Data Protection Regulation (GDPR) and is subject to a higher level of protection under the Data Protection Act 2018 (Section36(2)) . Consequently, concerned researchers must find alternative methods of protecting sensitive health data to minimise the risk of re-identification. This will be addressed in Section \\ref{sec:privacy}.", "cites": [8294, 1013, 8293, 60, 6812, 64, 6811, 5906, 117, 989], "cite_extract_rate": 0.5, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates information from multiple cited papers to discuss three key challenges in time series GANs: training stability, evaluation, and privacy risks. It synthesizes concepts from foundational GAN theory (e.g., JS and KL divergence) and connects them with relevant literature. However, the critical analysis is somewhat limited, as it does not deeply compare or contrast different approaches or evaluate their relative merits. The abstraction is moderate, as it outlines general patterns like the difficulty of human-based evaluation in time series but does not fully elevate to a meta-level understanding."}}
{"id": "3e5bc8dd-80ff-47d5-a030-fb1c8c0d3d68", "title": "Popular Datasets", "level": "subsection", "subsections": [], "parent_id": "e3144c4d-28bc-49df-b689-0f351b2878a8", "prefix_titles": [["title", "Generative adversarial networks in time series: A survey and taxonomy"], ["section", "Generative Adversarial Networks"], ["subsection", "Popular Datasets"]], "content": "Unlike image-based datasets (CIFAR, MNIST, ImageNet ) there are no standardised or commonly used benchmarking datasets for time series generation. However, we have compiled a list of some of the more popular datasets implemented in the reviewed works, and they are listed in Table \\ref{table:Popular Datasets}. There exist two repositories; the UCR Time Series Classification/Clustering database , and the UCI Machine Learning repository  that make available several time series datasets. Despite this, there is still no consensus on a standardised dataset used for benchmarking time series GANs, which may be due to the `continuous' nature of the architecture dimensions. GANs designed for continuous time series generation often differ in the length of their input sequence due to either author preference or the constraints placed on their architecture for the generated data's downstream tasks.\n\\begin{table}[ht]\n    \\centering\n    \\caption{Popular Datasets used in the reviewed works.}\n    \\label{table:Popular Datasets}\n    \\begin{tabular}{  p{0.35\\columnwidth}  p{0.2\\columnwidth}  p{0.1\\columnwidth} p{0.1\\columnwidth} }\n        \\toprule\n\\textbf{Name (Year)}      \n& \\textbf{Data Type}   \n& \\textbf{Instances}\n& \\textbf{Attributes} \\\\\\midrule\nOxford-Man Institute \"realised library\" (Updated Daily)\n& Real Multivariate Time Series\n& >2,689,487\n& 5\\\\\\hline\nEEG Motor Movement/Imagery Dataset (2004)\n& Real Multivariate Time Series \n& 1,500\n& 64 \\\\\\hline\nECG 200 (2001)\n& Real Univariate Time Series \n& 200\n& 1 \\\\\\hline\nEpileptic Seizure Recognition Dataset (2001)\n& Real Multivariate Time Series \n& 11,500\n& 179 \\\\\\hline\nTwoLeadECG (2015)\n& Real Multivariate Time Series \n& 1,162\n& 2 \\\\\\hline\nMIMIC-III (-)\n& Real, Integer \\& Categorical Multivariate Time Series \n& -\n& - \\\\\\hline\nEPILEPSIAE project database (-)\n& Real Multivariate Time Series \n& 30\n& - \\\\\\hline\nPhysioNet/CinC (2015)\n& Real Multivariate Time Series \n& 750\n& 4 \\\\\\hline\nWrist PPG During Exercise (2017)\n& Real Multivariate Time Series \n& 19\n& 14 \\\\\\hline\nMIT-BIH Arrhythmia Database (2001)\n& Real Multivariate Time Series \n& 201\n& 2 \\\\\\hline\nPhysioNet/CinC (2012)\n& Real, Integer \\& Categorical Multivariate Time Series \n& 12000\n& 43 \\\\\\hline\nKDD Cup Dataset (2018)\n& Real, Integer \\& Categorical Multivariate Time Series \n& 282\n& 3 \\\\\\hline\nPeMS Database (Updated Daily)\n& Integer \\& Categorical Multivariate Time Series \n& -\n& 8 \\\\\\hline\nNottingham Music Database (2003)\n& Special Text Format Time Series \n& 1000\n& - \\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}", "cites": [8295], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes the lack of standard benchmark datasets for time series GANs and lists several datasets used in the literature. It integrates minimal information from the cited papers, focusing more on factual presentation than synthesis. There is little critical evaluation of the datasets or the works using them, and no abstraction or generalization beyond the data properties."}}
{"id": "08de9f3b-a14c-4fb9-8aad-ff6fc492a5b3", "title": "Taxonomy of Time Series based GANs", "level": "section", "subsections": ["c2f850ad-915d-493d-be0c-96dec7dc01c0", "d237200e-793c-4e85-bc55-b07cbf38c1d3", "8733946e-9293-41bc-b51c-2f898c30a4f3", "53fcc6c7-8058-4cff-b374-4c5a7854aa37"], "parent_id": "df6e31e1-1c33-44b0-a2a3-972d7f8830eb", "prefix_titles": [["title", "Generative adversarial networks in time series: A survey and taxonomy"], ["section", "Taxonomy of Time Series based GANs"]], "content": "\\label{sec:time-series-gans}\nWe propose a taxonomy of the following time series based GANs based on two distinct variant types: \\textbf{discrete variants} (discrete time series) and \\textbf{continuous variants} (continuous time series). A discrete time series consists of data points separated by time intervals. This type of data might have 1. a data-reporting interval that is infrequent (e.g., 1 point per minute) or irregular (e.g., whenever a user logs in) and 2. gaps where values are missing due to reporting interruptions (e.g., intermittent server or network downtime in a network traffic application). Discrete time series generation involves generating sequences that may have a temporal dependency but contain discrete tokens; these can be commonly found in electronic health records (International Classification of Diseases 9 codes) and text generation. A continuous time series has a data value corresponding to every moment in time. Continuous data generation is concerned with generating a real-valued signal x with temporal dependencies where x $\\in \\mathbb{R}$. See Figure \\ref{fig:Time_Series_Types} for examples of discrete and continuous time series signals.\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.49\\columnwidth]{images/DTS.pdf}\n    \\includegraphics[width=0.49\\columnwidth]{images/CTS.pdf}\n    \\caption{Example plots of discrete (left) and continuous time series (right).}\n    \\label{fig:Time_Series_Types}\n\\end{figure}\n\\textbf{Challenges with discrete time series generation.}\\hspace{5pt} GANs struggle with discrete data generation due to the zero gradient nearly everywhere, i.e., the distribution on discrete objects are not differentiable with respect to their parameters .  This limitation makes the generator untrainable using backpropagation alone. The generator starts with a random sampling and a deterministic transform guided via the gradient of the loss from the discriminator with respect to the output produced by $G$ and the training dataset. This loss leads to a slight change in $G$'s output, pushing it closer to the desired output. Making slight changes to continuous numbers make sense; adding 0.001 to a value of 10 in financial time series data will bring it to 10.001. However, a discrete token such as the word ‘penguin’ cannot simply undergo the addition of 0.001 as the sum ‘penguin+0.001’ makes no sense.  What's important here is the impossibility for the generator to jump from one discrete token to the next because the small change gives the token a new value that does not correspond to any other token over that limited discrete space . This is because there exists 0 probability in the space between these tokens, unlike with continuous data. \n\\textbf{Challenges with continuous time series generation.}\\hspace{5pt} Modelling continuous time series data presents a different problem for GANs, which are inherently designed to model continuous data, albeit most commonly in the form of images. The temporal nature of continuous data in time series presents an extra layer of difficulty. Complex correlations exist between the temporal features and their attributes, e.g., if using multichannel biometric/physiological data, the ECG characteristics will depend on the individual's age and/or health. Also, long-term correlations exist in the data, which are not necessarily fixed in dimension compared to image-based data under a fixed dimension.\nTransforming image dimensions may lead to a degradation in image quality, but it is a recognised practice. This operation becomes more difficult with continuous time series data as there is no standardised dimension used across time series GANs architectures, which means that benchmarking their performances becomes difficult. \nSince their inception in 2014, GANs have shown great success in generating high-quality synthetic images indistinguishable from real images . While the focus to date has been on developing GANs for improved media generation, there is a growing consensus that GANs can be used for more than image generation and manipulation, which has led to a movement towards generating time series data with GANs. \nRecurrent neural networks (RNNs) (Figure \\ref{fig:SimpleRNN}), due to their loop-like structure, are perfect for sequential data applications but by themselves lack the ability to learn long-term dependencies that might be crucial in forecasting future values based on past. Long-short Term Memory networks (LSTM) (Figure \\ref{fig:SimpleRNN} are a specific kind of RNN that have the ability to remember information for long periods of time and, in turn, learn these long-term dependencies that the standard RNN is not capable of doing. In most work reviewed in this paper, the majority of the RNN based architectures are utilising the LSTM cell.\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.85\\columnwidth]{images/LSTM.pdf}\n    \\caption{Block Diagram of (left) a standard RNN and (right) LSTM cell}\n    \\label{fig:SimpleRNN}\n\\end{figure}\nRNNs can model sequential data such as financial data, medical data, text, and speech, and they have been the foundational architecture for time series GANs. A recurrent GAN (RGAN) was first proposed in 2016. The generator contained a recurrent feedback loop that used both the input and hidden states at each time step to generate the final output . Recurrent GANs often utilise Long Short-Term Memory neural networks in their generative models to avoid the vanishing gradient problem associated with more traditional recurrent networks . In the section that follows, we chronologically present time series GANs that have either contributed significantly to this space or have made some of the most recent novel advancements.", "cites": [8296, 8295, 8297, 5781], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from the cited papers to explain the challenges and characteristics of discrete and continuous time series generation using GANs. It integrates concepts like the zero-gradient problem for discrete data and the use of LSTM in addressing long-term dependencies. While it provides some critical analysis, particularly on the limitations of GANs for discrete data, it is not deeply evaluative across all cited works. The section abstracts broader principles of GAN limitations and RNN-based solutions, suggesting a framework for understanding time series GANs, but stops short of offering a novel or comprehensive theoretical synthesis."}}
{"id": "ee7368cc-00c0-45a2-abb4-a7e82c29bdac", "title": "Sequence GAN (SeqGAN) (Sept. 2016)", "level": "subsubsection", "subsections": [], "parent_id": "c2f850ad-915d-493d-be0c-96dec7dc01c0", "prefix_titles": [["title", "Generative adversarial networks in time series: A survey and taxonomy"], ["section", "Taxonomy of Time Series based GANs"], ["subsection", "Discrete-variant GANs"], ["subsubsection", "Sequence GAN (SeqGAN) (Sept. 2016)"]], "content": "Yu \\textit{et al.} proposed a sequential data generation framework  that could address the issues with generating discrete data as previously mentioned in \\ref{sec:time-series-gans}. This approach outperformed previous methods for generative modelling on real-world tasks, including;  a maximum likelihood estimation (MLE) trained LSTM, scheduled sampling , and Policy Gradient with bilingual evaluation understudy (PG-BLEU) . SeqGAN's generative model comprises RNNs with LSTM cells, and its discriminative model is a convolutional neural network (CNN). Given a dataset of structured sequences the authors train $G$ to produce a synthetic sequence $Y_{1:T} = (y_{1}...,y_{t}...,y_{T}), y_{t} \\in \\mathcal{Y}$ where $\\mathcal{Y}$ is defined as the vocabulary of candidate tokens. $G$ is updated by a policy gradient and Monte Carlo (MC) search on the expected reward from $D$, see Figure \\ref{fig:SeqGAN}. The authors used two datasets for their experiments. A Chinese poem dataset  and a Barack Obama Speech dataset  with Adam optimisers and a batch size of 64. Their experiments are available online\\footnote{SeqGAN GitHub: https://github.com/LantaoYu/SeqGAN/}.\nAlthough the purpose of SeqGAN is to generate discrete sequential data, it opened the door to other GANs in generating continuous sequential and time series data. The authors use a synthetic dataset whose distribution is generated from a randomly initialised LSTM following a normal distribution. They also compare the generated data to real-world examples of poems, speech-language and music. SeqGAN showed competitive performance in generating the sequences and contributed heavily towards the further development of the continuous sequential GANs. \n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.85\\columnwidth]{images/SeqGAN.pdf}\n    \\caption{SeqGAN: \\textit{D} (left) is trained over real and generated data, whereas \\textit{G} (right) is trained by policy gradient where the final reward signal is provided by \\textit{D} and is passed back to the intermediate action value via Monte Carlo search .}\n    \\label{fig:SeqGAN}\n\\end{figure}", "cites": [1106], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the SeqGAN framework, its components, and experimental setup. It makes minimal connections to broader trends or other cited works, and while it mentions that SeqGAN outperformed other methods, it does not critically analyze these comparisons or the limitations of the cited papers. The abstraction remains low as it does not generalize or extract overarching principles from the described work."}}
{"id": "04161189-1928-4977-83f3-10f7da486fe9", "title": "Continuous RNN-GAN (C-RNN-GAN) (Nov. 2016)", "level": "subsubsection", "subsections": [], "parent_id": "d237200e-793c-4e85-bc55-b07cbf38c1d3", "prefix_titles": [["title", "Generative adversarial networks in time series: A survey and taxonomy"], ["section", "Taxonomy of Time Series based GANs"], ["subsection", "Continuous-variant GANs"], ["subsubsection", "Continuous RNN-GAN (C-RNN-GAN) (Nov. 2016)"]], "content": "\\label{sec:c-rnn-gan}\nIn previous works, RNNs have been applied to modelling music but have generally used a symbolic representation to model this type of sequential data. Mogren proposed the C-RNN-GAN (Figure \\ref{fig:C-RNN-GAN_model}), one of the first examples of using GANs to generate continuous sequential data. The generator is an RNN, and the discriminator a bidirectional RNN, which allows the discriminator to take the sequence context in both directions. The RNNs used in this work were two stacked LSTMs layers, with each cell containing 350 hidden units. The loss functions can be seen in (\\ref{eq:C-RNN-GAN_G},\\ref{eq:C-RNN-GAN_D}), where $z^{(i)}$ is a sequence of uniform random vectors in [0,1]$^k$, and $x^{(i)}$ is a sequence from the training data. $k$ is the dimensionality of the data in the random sequence.\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.85\\columnwidth]{images/C-RNN-GAN.pdf}\n    \\caption{Structure of C-RNN-GAN's generator and discriminator.}\n    \\label{fig:C-RNN-GAN_model}\n\\end{figure}\n\\begin{equation}\n{L}_{G} = \\frac{1}{m} \\sum_{i=1}^{m} log(1-D(G(z^{(i)})))\n\\label{eq:C-RNN-GAN_G}\n\\end{equation}\n\\begin{equation}\n{L}_{D} = \\frac{1}{m} \\sum_{i=1}^{m} [-logD(x^{(i)}) - log(1-D(G(z^{(i)})))]\n\\label{eq:C-RNN-GAN_D}\n\\end{equation}\nThe C-RNN-GAN is trained with backpropagation through time (BPTT) and mini-batch stochastic gradient descent with L2 regularisation on the weights of both $G$ and $D$. Freezing was applied to both $G$ and $D$ when one network becomes too strong relative to the other. The dataset used was 3697 midi files from 160 different composers of classical music with a batch size of 20. Adam and Gradient Descent Optimisers were used during training; full implementation details are available online\\footnote{C-RNN-GAN GitHub: https://github.com/olofmogren/c-rnn-gan/}. Overall the C-RNN-GAN was capable of learning the characteristics of continuous sequential data and, in turn, generate music. However, the author stated that their approach still needs work, particularly in rigorous evaluation of the generated data quality.", "cites": [992], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual summary of the C-RNN-GAN model, including its architecture, training methodology, and application to music generation. However, it lacks deeper synthesis with other works in the survey, minimal critical evaluation (beyond a brief mention of the need for better evaluation), and no abstraction to broader patterns or principles in time series GANs."}}
{"id": "e28cf9cf-9081-4596-93ff-3d2095d3ba9b", "title": "Recurrent Conditional GAN (RCGAN) (2017)", "level": "subsubsection", "subsections": [], "parent_id": "d237200e-793c-4e85-bc55-b07cbf38c1d3", "prefix_titles": [["title", "Generative adversarial networks in time series: A survey and taxonomy"], ["section", "Taxonomy of Time Series based GANs"], ["subsection", "Continuous-variant GANs"], ["subsubsection", "Recurrent Conditional GAN (RCGAN) (2017)"]], "content": "RCGAN for continuous data generation  differs architecturally from the C-RNN-GAN. Although the RNN LSTM is used, the discriminator is unidirectional, and the outputs of $G$ are not fed back as inputs at the next time step. There is also additional information that the model is conditioned on, which makes for a conditional RGAN; see the layout of the model in Figure \\ref{fig:RCGAN_model}. The purpose of the RCGAN and RGAN in this work is to generate continuous time series with a focus on medical data intended for use in downstream tasks, and this was one of the first works in this area. The loss functions can be seen in Equations (\\ref{eq:RCGAN_Dloss}, \\ref{eq:RCGAN_Gloss}) where CE is the average cross-entropy between two sequences. $X_n$ are samples drawn from the training dataset. $y_n$ is the adversarial ground truth; for real sequences, it is a vector of 1s, and conversely, for generated or synthetic sequences, it is a vector of 0s. $Z_n$ is a sequence of points sampled from the latent space, and the valid adversarial ground truth is written here as \\textbf{1}.\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{images/RCGAN.pdf}\n    \\caption{RCGAN architecture with conditional input \\textbf{c}, input data \\textbf{x} and latent variable \\textbf{z}.}\n    \\label{fig:RCGAN_model}\n\\end{figure}\n\\begin{equation}\n{D}_{loss}(X_n, y_n) = -CE(D(X_n), y_n)\n\\label{eq:RCGAN_Dloss}\n\\end{equation}\n\\begin{equation}\n{G}_{loss}(Z_n) = D_{loss}(G(Z_n),\\textbf{1}) =  -CE(D(G(Z_n)), \\textbf{1})\n\\label{eq:RCGAN_Gloss}\n\\end{equation}\nIn the conditional case, the inputs to $D$ and $G$ are concatenated with some conditional information $c_n$. This variant of an RNN-GAN facilitates the generation of a synthetic continuous time series dataset with associated labels. Experiments were carried out on generated sine waves, smooth functions sampled from a Gaussian process with a zero-valued mean function, MNIST dataset as a sequence, and the Philips eICU database . A batch size of 28 with Adam and Gradient Descent Optimisers were used for training. The authors propose a novel method for evaluating their model, which is discussed further in Section \\ref{sec:eval}. Full experimental details can be found online\\footnote{RCGAN GitHub: https://github.com/ratschlab/RGAN/}.", "cites": [989], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of the RCGAN architecture and its purpose for continuous time series generation, particularly in medical data. It integrates some information from the cited paper, such as the model structure and loss functions, but lacks deeper synthesis with other works or broader context. There is little critical evaluation or abstraction beyond the specific paper's contributions."}}
{"id": "2a3d9354-83f9-49f1-81fe-0df61417c501", "title": "Time GAN (Dec. 2019)", "level": "subsubsection", "subsections": [], "parent_id": "8733946e-9293-41bc-b51c-2f898c30a4f3", "prefix_titles": [["title", "Generative adversarial networks in time series: A survey and taxonomy"], ["section", "Taxonomy of Time Series based GANs"], ["subsection", "Noise Reduction GAN (NR-GAN) (Oct. 2019)"], ["subsubsection", "Time GAN (Dec. 2019)"]], "content": "TimeGAN provides a framework that utilises both the conventional unsupervised GAN training method and the more controllable supervised learning approach . By combining an unsupervised GAN network with a supervised autoregressive model, the network aims to generate time series with preserved temporal dynamics. The architecture of the TimeGAN framework is illustrated in Figure \\ref{fig:TimeGAN_model}. The input to the framework is considered to consist of two elements, a static feature and a temporal feature. \\textbf{s} represents a vector of static features and \\textbf{x} of temporal features at the input to the encoder. The generator takes a tuple of static and temporal random feature vectors drawn from a known distribution. The real and synthetic latent codes $\\textbf{h}$ and $\\hat{\\textbf{h}}$  are used to calculate the supervised loss element of this network. The discriminator receives the tuple of real and synthetic latent codes and classifies them as either real ($y$) or synthetic ($\\hat{y}$), the $\\Tilde{}$ operator denotes the sample as either real or fake.\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{images/TimeGAN.pdf}\n    \\caption{TimeGAN architecture.  }\n    \\label{fig:TimeGAN_model}\n\\end{figure}\nThe three losses used in TimeGAN are calculated as follows:\n\\begin{equation}\n{L}_{reconstruction} = \\mathbb{E}_{s,x_{1:T} \\sim p}[ \\|s-\\Tilde{s}\\|_{2} + \\sum_{t} \\| x_{t} - \\Tilde{x}_{t}\\|_{2}] \n\\label{eq:TimeGAN_R-loss}\n\\end{equation}\n\\begin{equation}\n{L}_{unsupervised} = \\mathbb{E}_{s,x_{1:T} \\sim p}[log(y_{S}) + \\sum_{t} log(y_{t}) ] + \\mathbb{E}_{s,x_{1:T} \\sim{\\hat{p}}} [log(1- \\hat{y}_{S}) + \\sum_{t} log(1- \\hat{y}_{t})]\n\\label{eq:TimeGAN_U-loss}\n\\end{equation}\n\\begin{equation}\n{L}_{supervised} = \\mathbb{E}_{s,x_{1:T} \\sim p}[ \\sum_{t} \\|h_{t} -g_{X}(h_{S}, h_{t-1}, z_{t})\\|_{2}]\n\\label{eq:TimeGAN_S-loss}\n\\end{equation}\nThe creators of TimeGAN conducted experiments on generating sine waves, stocks (daily historical Google stocks data from 2004 to 2019), energy (UCI Appliances energy prediction dataset) , and event (private lung cancer pathways dataset) datasets. A batch size of 128 and Adam optimiser were used for training, implementation details are available online\\footnote{TimeGAN GitHub: https://github.com/jsyoon0823/TimeGAN}. The authors demonstrated improvements over other state-of-the-art time series GANs such as  RCGAN C-RNN-GAN and WaveGAN.", "cites": [8295], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the TimeGAN model, its components, and the datasets used for evaluation. While it integrates some concepts (e.g., unsupervised and supervised learning), the synthesis is minimal and lacks deeper connections to the cited work (e.g., Boundary-Seeking GANs). There is no critical evaluation of the model's limitations or comparative insights against the cited papers, and abstraction remains at a basic level without identifying broader trends or principles."}}
{"id": "10b07d56-e4d5-4108-a059-c23e97bb23ca", "title": "Conditional Sig-Wasserstein GAN (SigCWGAN) (Jun. 2020) ", "level": "subsubsection", "subsections": [], "parent_id": "8733946e-9293-41bc-b51c-2f898c30a4f3", "prefix_titles": [["title", "Generative adversarial networks in time series: A survey and taxonomy"], ["section", "Taxonomy of Time Series based GANs"], ["subsection", "Noise Reduction GAN (NR-GAN) (Oct. 2019)"], ["subsubsection", "Conditional Sig-Wasserstein GAN (SigCWGAN) (Jun. 2020) "]], "content": "A problem addressed by  is that long time series data streams can greatly increase the dimensionality requirements of generative modelling, which may render such approaches infeasible. To counter this problem, the authors develop a metric named Signature Wasserstein-1 (Sig-$W_1$) that captures time series models' temporal dependency and uses it as a discriminator in a time series GAN. It provides an abstract and universal description of complex data streams and does not require costly computation like the Wasserstein metric. A novel generator is also presented that is named conditional autoregressive feed-forward neural network (AR-FNN) that captures the auto-regressive nature of the time series. The generator is capable of mapping past series and noise into future series. For a rigorous mathematical description of their method, the interested reader should consult .\nFor the AR-FNN generator the idea is to obtain the step-q estimator $\\hat{X}^{(t)}_{t+1:t+q}$. The loss function for $D$ is defined as:\n\\begin{equation}\nL(\\theta)= \\sum_{t}|\\mathbb{E}_{\\mu}[S_M(X_{t+1:t+q})|X_{t-p+1:t}] - \\mathbb{E}_v[S_M(\\hat{X}_{t+1:t+q}^{(t)})|X_{t-p+1:t}]|\n\\label{eq:SIGCWGAN_D}\n\\end{equation}\nWhere $v$ and $\\mu$ are the conditional distributions induced by the real data and synthetic generator, respectively, further details of the author's algorithm can be found in the appendix of the original paper.\nThe authors state that SigCWGAN eliminates the problem of approximating a costly $D$ and simplifies training. It is reported to achieve state-of-the-art results on synthetic and empirical datasets compared to TimeGAN, RCGAN and Generative Moment Matching Networks (GMMN) . The empirical dataset consists of the S\\&P 500 index (SPX) and Dow Jones index (DJI) and their realized volatility, which is retrieved from the Oxford-Man Institute’s \"realised library\" . A batch size of 200 with the Adam optimiser was used for training\\footnote{SigCWGAN GitHub: https://github.com/SigCGANs/Conditional-Sig-Wasserstein-GANs/}.", "cites": [6813, 8276], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates the key contributions of the SigCWGAN paper, explaining both the novel Sig-Wasserstein metric and the conditional AR-FNN generator in the context of addressing high-dimensional time series challenges. It makes some connections to broader GAN issues like computational cost and training complexity, and briefly compares the model's performance to other methods. However, it lacks deeper critical evaluation of limitations or systematic comparison of underlying assumptions."}}
{"id": "863f38b4-f7a9-471d-8dac-168d6558b411", "title": "Decision Aware Time series conditional GAN (DAT-CGAN) (Sept. 2020)", "level": "subsubsection", "subsections": [], "parent_id": "8733946e-9293-41bc-b51c-2f898c30a4f3", "prefix_titles": [["title", "Generative adversarial networks in time series: A survey and taxonomy"], ["section", "Taxonomy of Time Series based GANs"], ["subsection", "Noise Reduction GAN (NR-GAN) (Oct. 2019)"], ["subsubsection", "Decision Aware Time series conditional GAN (DAT-CGAN) (Sept. 2020)"]], "content": "This framework is designed to provide support for end-users decision processes, specifically in financial portfolio choices. It uses a multi-Wasserstein loss on structured decision-related quantities . The discriminator loss and generator loss are defined in Equations (\\ref{eq:DAT-CGAN_D}) and (\\ref{eq:DAT-CGAN_G}) respectively. For further details on the loss functions, see Section 3 of the original paper.\n\\begin{equation}\n\\mathop{inf}_{\\eta} \\mathop{sup}_{\\gamma_{k},\\theta_{j,k}} \\sum_{k=1}^{K} \\omega_k(\\mathbb{E}_{k}^r - \\mathbb{E}_{k}^{G_{\\eta}})+ \\sum_{k=1}^{K} \\sum_{j=1}^{J} \\lambda_{j,k} (\\mathbb{E}_{j,k}^{f,R} - \\mathbb{E}_{j,k}^{f, G_{\\eta}}) \n\\label{eq:DAT-CGAN_D}\n\\end{equation}\n\\begin{equation}\n\\mathop{inf}_{\\eta} - \\sum_{k} \\omega_k\\mathbb{E}_{k}^{G_{\\eta}} - \\sum_{k,j} \\lambda_{j,k}  \\mathbb{E}_{j,k}^{f, G_{\\eta}}\n\\label{eq:DAT-CGAN_G}\n\\end{equation}\nThe generator is a two-layer feed-forward neural network for each input which are assets in this case. $G$ outputs asset returns that are used to compute decision-related quantities. These quantities are fed into $D$, which is also a two-layer feed-forward NN. Further details about the architecture can be found in the appendix of . The dataset used is daily price data for each of four U.S. Exchange-traded fund (ETFs), i.e. Material (XLB), Energy (XLE), Financial (XLF) and Industrial (XLI) ETFs, from 1999 to 2016. The authors found this model capable of high-fidelity time series generation that supports decision processes by end-users due to incorporating a decision-aware loss function. However, this approach's limitation is that the computational complexity of this model is vast and requires one month of training time for a single generative model.", "cites": [6814], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes the DAT-CGAN framework and its components without synthesizing it with other GAN variants or providing broader insights. While it mentions the model's purpose and a limitation (computational complexity), it lacks in-depth comparison or critical evaluation of the approach relative to others. There is minimal abstraction beyond the specific paper being summarized."}}
{"id": "605724a7-299f-429d-96ff-90d9d427d808", "title": "Data Augmentation", "level": "subsection", "subsections": [], "parent_id": "037f789a-925c-44de-8b40-310423fd6896", "prefix_titles": [["title", "Generative adversarial networks in time series: A survey and taxonomy"], ["section", "Applications"], ["subsection", "Data Augmentation"]], "content": "It is common knowledge in the deep learning community that GANs are among the methods of choice when discussing data augmentation. Reasons for augmenting datasets range from increasing the size/variety of small and imbalanced datasets  to reproducing restricted datasets for dissemination.\nA well-defined solution to the data shortage problem is transfer learning, and it works well in domain adaptation which has led to advancements in classification and recognition problems . However, it has been found that augmenting datasets with GANs can lead to further improvements in certain classification and recognition tasks . Data synthesised by a GAN can adhere to stricter privacy measures discussed in Section \\ref{sec:privacy}. This further demonstrates the advantages of augmenting your training dataset with GANs over implementing transfer learning with a pre-trained model from a different domain on a smaller dataset.\nMany researchers find that accessing datasets for their deep learning research and models to be time-consuming, laborious work, particularly when the research is concerned with personal sensitive data. Often medical and clinical data are presented as continuous sequential data that can only be accessed by a small contingent of researchers who are not at liberty to disseminate their research openly. This, in turn, may lead to stagnation in the research progress in these domains. \nFortunately, we are beginning to see the uptake of GANs applied to time series with these types of medical and physiological data  . With  showing dependent multivariate continuous high-fidelity physiological signal generation is capable via GANs, demonstrating the impressive capability of these networks. See Figure \\ref{fig:Generated_ECG} for an example of the real input and synthetic generated data. \n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.49\\columnwidth]{images/Input_ECG_brophy.pdf}\n    \\includegraphics[width=0.49\\columnwidth]{images/Generated_ECG_brophy.pdf}\n    \\caption{An example of dependent multichannel ECG data (left) and  generated ECG from a multivariate GAN (right) . NSR indicates the training dataset which is the Normal Sinus Rhythm. The generated data is produced by a GAN named by the authors as LSGAN-DTW.}\n    \\label{fig:Generated_ECG}\n\\end{figure}\nOf course, this is not comprehensive coverage of the research using time series GANs for data synthesis and augmentation. GANs have been applied to time series data for a plethora of use cases. \nAudio generation (both music and speech) and text-to-speech (TTS)  has been a popular area for researchers to explore with GANs. The C-RNN-GAN described in Section \\ref{sec:c-rnn-gan} was one of the seminal works to apply GANs to generating continuous sequential data in the form of music.\nIn the financial sector, GANs have been implemented to generate data and predict/forecast values. Wiese \\textit{et al.} implemented a GAN to approximate financial time series in discrete-time . In , the authors designed a decision-aware GAN that generates synthetic data and supports decision processes to financial portfolio selection of end-users. \nOther time series generation/prediction methods range from estimating soil temperature  to predicting medicine expenditure based on the current state of patients .", "cites": [6816, 989, 6817, 6815, 8298, 6813], "cite_extract_rate": 0.35294117647058826, "origin_cites_number": 17, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of GAN applications in data augmentation, particularly in medical, audio, and financial time series. It cites multiple papers but does not meaningfully synthesize or connect their contributions into a cohesive narrative. There is minimal critical analysis or identification of broader patterns or principles in the field."}}
{"id": "1da83d1c-f030-4b1e-b428-0e4b941b1f34", "title": "Imputation", "level": "subsection", "subsections": [], "parent_id": "037f789a-925c-44de-8b40-310423fd6896", "prefix_titles": [["title", "Generative adversarial networks in time series: A survey and taxonomy"], ["section", "Applications"], ["subsection", "Imputation"]], "content": "In real-world datasets, missing or corrupt data is an all too common problem that leads to downstream problems. These issues manifest themselves in further analytics of the dataset and can induce biases in the data. Common methods of dealing with missing or corrupted data in the past have been the deletion of data streams containing the missing segments, statistical modelling of the data, or machine learning imputation approaches. Looking at the latter, we review the work in imputing these data using GANs. Guo {et al.} designed a GAN based approach for multivariate time series imputation , see Figure \\ref{fig:MTS_imputation} for an example of imputed data from a toy experiment .\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.5\\columnwidth]{images/ECG.pdf}\n    \\caption{An example of the incomplete corrupted time series (top) and imputed signal (bottom).}\n    \\label{fig:MTS_imputation}\n\\end{figure}", "cites": [6817], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides minimal synthesis by merely mentioning Guo et al.'s work and referencing a figure. It lacks integration with the cited paper (doc_id: 6817), which is not even discussed in the text. There is no critical analysis of the approaches, and no abstraction or generalization of patterns or principles across GAN-based imputation methods."}}
{"id": "203e11b1-bc3f-4dcf-af15-6eba7c0927e0", "title": "Anomaly Detection", "level": "subsection", "subsections": [], "parent_id": "037f789a-925c-44de-8b40-310423fd6896", "prefix_titles": [["title", "Generative adversarial networks in time series: A survey and taxonomy"], ["section", "Applications"], ["subsection", "Anomaly Detection"]], "content": "Detecting outliers or anomalies in time series data is an important part of many real-world systems and sectors. Whether it is detecting unusual patterns in physiological data that may be a precursor to some more malicious condition or detecting irregular trading patterns on the stock exchange, anomaly detecting can be vital to keeping us informed on important information. Statistical measures of non-stationary time series signals may achieve good performance on the surface, but they might also miss some important outliers present in deeper features. They may also struggle in exploiting large unlabelled datasets; this is where the unsupervised deep learning approaches can outperform the conventional methods. Zhu \\textit{et al.} designed a GAN algorithm for anomaly detection in time series data (ECG and taxi dataset) with LSTMs and GANs, which achieved superior performance compared to conventional, more shallow approaches. Similar approaches have been applied to detect cardiovascular diseases , in cyber-physical systems to detect nefarious players  and even irregular behaviours such as stock manipulation on the stock markets  .", "cites": [8404], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of GAN applications in anomaly detection for time series, mentioning one specific paper (MAD-GAN) and listing other applications without in-depth discussion. It begins to synthesize the idea that GANs offer advantages over traditional and shallow methods, but lacks integration of multiple cited works into a cohesive framework. There is minimal critical evaluation or abstraction beyond individual examples."}}
{"id": "bac9897f-2a45-4d2b-abd9-e5c161879ee8", "title": "Other Applications", "level": "subsection", "subsections": [], "parent_id": "037f789a-925c-44de-8b40-310423fd6896", "prefix_titles": [["title", "Generative adversarial networks in time series: A survey and taxonomy"], ["section", "Applications"], ["subsection", "Other Applications"]], "content": "Some works have utilised image-based GANs for time series and sequential data generation by first converting their sequences to images via some transformation function and training the GAN on these images. Once the GAN converges, similar images can be generated; then, a sequence can be retrieved using the inverse of the original transformation function. For example, this approach has been implemented in audio generation with waveforms , anomaly detection  and physiological time series generation .", "cites": [5904, 6817, 8299], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of how image-based GANs have been used for time series generation by transforming sequences into images. It mentions a few applications and cites three papers but does not synthesize or connect their approaches in a meaningful way. There is little critical evaluation or abstraction to broader principles or frameworks in the field."}}
{"id": "eecc1a53-9228-46a0-a242-f6b741a0979e", "title": "Evaluation Metrics", "level": "section", "subsections": [], "parent_id": "df6e31e1-1c33-44b0-a2a3-972d7f8830eb", "prefix_titles": [["title", "Generative adversarial networks in time series: A survey and taxonomy"], ["section", "Evaluation Metrics"]], "content": "\\label{sec:eval}\nAs mentioned in Section \\ref{sec:gan} GANs can be difficult to evaluate, and researchers are yet to agree on what metrics reflect the GANs performance best. There has been plenty of metrics proposed in the literature  with most of them suited to the computer vision domain. Work is still ongoing to suitably evaluate time series GANs. We can break down evaluation metrics into two categories: qualitative and quantitative. Qualitative evaluation is another term for human visual assessment via the inspection of generated samples from the GAN. However, this cannot be deemed a full evaluation of GAN performance due to the lack of a suitable \\textit{objective} evaluation metric. The quantitative evaluation includes the use of metrics associated with statistical measures used for time series analytics and similarity measures such as; Pearson Correlation Coefficient (PCC), percent root mean square difference (PRD), (Root) Mean Squared Error MSE and RMSE, Mean Relative Error (MRE) and Mean Absolute Error (MAE). These metrics are among the most commonly used for time series evaluation and, as such, used as a suitable GAN performance metric as they can reflect the stability between the training data and synthetic generated data. \nSeveral metrics have become well-established choices in evaluating image-based GANs, and some of these have permeated through to the sequential and time series GANs such as Inception Score (IS) , Fr\\'{e}chet (Inception) Distance (FD and FID) . Structural Similarity Index (SSIM) is a measure of similarity between two images. However,  use this with time series data as SSIM does not exclude itself from comparing aligned sequences of fixed length. Of course, some of these metrics are measures of similarities/dissimilarities between two probability distributions, suitable for many types of data, particularly the maximum mean discrepancy (MMD)  is very suitable to this task across domains. Another metric that generalises well to the sequential data case is the Sliced-Wasserstein Distance approximating the Wasserstein distance by computing Wasserstein distances between all 1d-projections of two distributions.\nThe data generated from GANs have been used in downstream classification tasks. Using the generated data together with the training data has lead to the Train on Synthetic, Test on Real (TSTR) and Train on Real, Test on Synthetic (TRTS) evaluation methods, first proposed by Esteban \\textit{et al.} . In scoring downstream classification applications that use both real and generated data, studies have adopted the precision, recall, and F1 scores to determine the classifier's quality and, in turn, the quality of the generated data. Other accuracy measures of classifier performance include the weighted accuracy (WA) and unweighted average recall (UAR).\nOften used distance and similarity measures in time series data are the Euclidean Distance (ED) and Dynamic Time Warping (DTW) algorithms. Multivariate (in)dependent DTW (MVDTW), implemented in , can determine similarity measures across both dependent and independent multichannel time series signals.\nOther metrics used across different applications include: \n\\begin{itemize}\n  \\item \\textbf{Financial Sector}; autocorrelation function (ACF) score, DY metric. \n  \\item \\textbf{Temperature Estimation}; Nash-Sutcliffe model efficiency coefficient (NS), Willmott index of agreement (WI) and the Legates and McCabe index (LMI).\n  \\item \\textbf{Audio Generation}; Normalised Source-to-Distortion Ratio (NSDR), Source-to Interference Ratio (SIR), Source-to-artifact ratio (SAR) and t-SNE .\n\\end{itemize}\nFor a full list of GAN architectures reviewed in this work, their applications, evaluation metrics, and datasets used in their respective experiments, see Table \\ref{table:GANs}. Results for the sine wave and ECG generation using variants of GAN architectures can be found in Tables \\ref{table:GAN-res-SINE} and \\ref{table:GAN-res-MITBIH}, respectively.\n\\begin{table}[ht]\n    \\centering\n    \\caption{A list of GAN architectures, their applications, and datasets used in their experiments and evaluation metrics used to judge the quality of the respective GANs. For novel approaches, the GAN name is given as they have been covered already in Section \\ref{sec:time-series-gans}} \n    \\label{table:GANs}\n    \\begin{tabular}{  p{0.2\\columnwidth}  p{0.25\\columnwidth}  p{0.2\\columnwidth} p{0.2\\columnwidth} }\n        \\toprule\n\\textbf{Application}      \n& \\textbf{GAN Architecture(s)}   \n& \\textbf{Dataset(s)}\n& \\textbf{Evaluation Metrics} \\\\\\midrule\nMedical/Physiological Generation \n& LSTM-LSTM,   , , ,  \nLSTM-CNN,  \nBiLSTM-CNN, \nBiGridLSTM-CNN,  \nCNN-CNN, , \nAE-CNN, \nFCNN \n& EEG, ECG, EHRs, PPG, EMG, Speech, NAF, MNIST, Synthetic sets\n& TSTR, MMD, Reconstruction error, DTW, PCC, IS, FID, ED, S-WD, RMSE, MAE, FD, PRD, Averaging Samples, WA, UAR, MV-DTW\\\\\\hline\nFinancial time series generation/prediction\n&TimeGAN  \nSigCWGAN \nDAT-GAN \nQuantGAN \n& S\\&P500 index (SPX), Dow Jones Index (DJI), ETFs\n& Marginal Distributions, Dependencies, TSTR, Wasserstein Distance, EM distance, DY Metric, ACF score, leverage effect score, discriminative score, predictive score \\\\\\hline\nTime series Estimation/Prediction      \n& LSTM-NN \nLSTM-CNN  \nLSTM-MLP                        \n& Meteorological data, Truven MarketScan dataset\n& RMSE, MAE, NS, WI, LMI\\\\\\hline\nAudio Generation      \n& C-RNN-GAN  \nTGAN (variant)  \nRNN-FCN  \nDCGAN (variant) \nCNN-CNN \n& Nottingham dataset, Midi music files, MIR-1K, TheSession, Speech\n& Human perception, Polyphony, Scale Consistency, Tone Span, Repetitions, NSDR, SIR, SAR, FD, t-SNE, Distribution of notes\\\\\\hline\nTime series Imputation/Repairing   \n& MTS-GAN  \nCNN-CNN  \nDCGAN variant \nAE-GRUI \nRGAN \nFCN-FCN \nGRUI-GRUI \n& TEP, Point Machine, Wind Turbin data, PeMS, PhysioNet Challenge 2012, KDD CUP 2018, Parking lot data, \n& Visually, MMD, MAE, MSE, RMSE, MRE, Spatial Similarity, AUC score\\\\\\hline\nAnomaly Detection      \n& LSTM-LSTM  \nLSTM-(LSTM\\&CNN)  \nLSTM-LSTM (MAD-GAN)  \n& SET50, NYC Taxi data, ECG, SWaT, WADI\n& Manipulated data used as a test set, ROC Curve, Precision, Recall, F1, Accuracy\\\\\\hline\nOther time series generation \n& VAE-CNN       \n& Fixed length time series 'vehicle and engine speed' \n& DTW, SSIM\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\\begin{table}[ht]\n\\caption{Experimental results comparing the performance of time series GANs for sinewave generation}\n\\begin{center}\n\\begin{tabular}{|c||c||c|c|c|}\n    \\hline\n    \\multirow{2}{*}{Architecture} &\\multirow{2}{*}{Loss Function} &\\multicolumn{3}{|c|}{Toy Sine Dataset}\\\\\n    \\cline{3-5}& & MMD &DTW &MSE\\\\\n    \\hline \\hline\n    \\multirow{2}{*}{LSTM-LSTM} &BCE &-- &-- &--\\\\ \t\t\n        &MSE &0.007805404\t&54.16447988\t&\\textbf{0.148043822}\\\\\n    \\hline\n    \\multirow{2}{*}{BiLSTM-LSTM} &BCE &0.121597451\t&428.43108\t&3.070050828\\\\\n        &MSE &0.951539381\t&79.56070154\t&0.236244962\\\\\n    \\hline\n    \\multirow{2}{*}{LSTM-CNN} &BCE &0.006319945\t&55.36204204\t&0.31545636\\\\\n        &MSE &0.575763914\t&86.73579748\t&0.564357309\\\\\n    \\hline\n    \\multirow{2}{*}{BiLSTM-CNN} &BCE &\\textbf{1.12958E-05}\t&129.9257789\t&0.919376137\\\\\n        &MSE &0.489165394\t&43.26942611\t&0.186902029\\\\\n    \\hline\n    \\multirow{2}{*}{GRU-CNN} &BCE &0.024468314\t&\\textbf{37.1630491}\t&0.230310881\\\\\n        &MSE &0.372734849\t&42.7348549\t&0.228260543\\\\\n    \\hline\n    \\multirow{2}{*}{FC-CNN} &BCE &0.003933644\t&58.35650673\t&0.304803267\\\\\n        &MSE &0.011720286\t&43.36115622\t&0.297288744\\\\\n    \\hline\n\\end{tabular}\n\\end{center}\n\\label{table:GAN-res-SINE}\n\\end{table}\n\\begin{table}[ht]\n\\caption{Experimental results comparing the performance of time series GANs for ECG generation on MIT-BIH Dataset}\n\\begin{center}\n\\begin{tabular}{|c||c||c|c|c|}\n    \\hline\n    \\multirow{2}{*}{Architecture} &\\multirow{2}{*}{Loss Function} &\\multicolumn{3}{|c|}{MIT-BIH Arrhythmia Dataset}\\\\\n    \\cline{3-5}& & MMD &DTW &MSE\\\\\n    \\hline \\hline\n    \\multirow{2}{*}{LSTM-LSTM} &BCE &0.993149132\t&30.18161173\t&0.086768344\\\\ \t\t\n        &MSE &0.8842204\t&44.45535914\t&0.138977587\\\\\n    \\hline\n    \\multirow{2}{*}{BiLSTM-LSTM} &BCE &0.991669978\t&22.86345584\t&0.069901973\\\\\n        &MSE &0.973720273\t&23.55338573\t&0.080641843\\\\\n    \\hline\n    \\multirow{2}{*}{LSTM-CNN} &BCE &0.551958508\t&\\textbf{13.0158286}\t&\\textbf{0.015101118}\\\\\n        &MSE &\\textbf{0.0005273}\t&24.73064185\t&0.045702711\\\\\n    \\hline\n    \\multirow{2}{*}{BiLSTM-CNN} &BCE &0.924689511\t&117.3994295\t&0.227219626\\\\\n        &MSE &0.068707491\t&22.67402118\t&0.058676694\\\\\n    \\hline\n    \\multirow{2}{*}{GRU-CNN} &BCE &0.005577049\t&20.48459161\t&0.033519309\\\\\n        &MSE &0.770406707\t&108.4124982\t&0.194816291\\\\\n    \\hline\n    \\multirow{2}{*}{FC-CNN} &BCE &0.206880394\t&23.99100792\t&0.030977119\\\\\n        &MSE &0.308249889\t&18.23405152\t&0.021292498\\\\\n    \\hline\n\\end{tabular}\n\\end{center}\n\\label{table:GAN-res-MITBIH}\n\\end{table}", "cites": [992, 6816, 6814, 60, 8404, 8298, 996, 6813, 8299, 117, 989, 6818, 1013], "cite_extract_rate": 0.3170731707317073, "origin_cites_number": 41, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a structured overview of evaluation metrics for time series GANs, synthesizing common methods like MMD, DTW, and FID across multiple applications. It identifies domain-specific adaptations and categorizes metrics by application areas, showing some abstraction. However, while it mentions limitations of certain metrics (e.g., SSIM for non-fixed-length time series), it lacks deeper critical analysis or a unified framework for evaluating these approaches."}}
{"id": "96971a91-78bd-408e-9d11-058c4f87a89e", "title": "Differential Privacy", "level": "subsection", "subsections": [], "parent_id": "fa175aea-1e64-4018-9aa0-9f39540d4289", "prefix_titles": [["title", "Generative adversarial networks in time series: A survey and taxonomy"], ["section", "Privacy"], ["subsection", "Differential Privacy"]], "content": "The goal of Differential Privacy is to preserve the underlying privacy of a database. An algorithm or, more specifically, a GAN achieves differential privacy if, by looking at the generated samples, we cannot identify whether the samples were included in the training set. As GANs attempts to model the training dataset, the problem of privacy lies in capturing and generating useful information about the training set population without the possibility of linkage from generated sample to an individuals data . \nAs we have previously addressed, one of the main goals of GANs is to augment existing under-resourced datasets for use in further downstream applications such as upskilling of clinicians where healthcare data is involved. These personal sensitive data must contain privacy guarantees, and the rigorous mathematical definition of DP  offers this assurance. \nWork is ongoing to develop machine learning methods with privacy-preserving mechanisms such as differential privacy (DP). Abadi \\textit{et al.}  demonstrate the ability to train deep neural networks with DP and implement a mechanism for tracking privacy loss . Xie \\textit{et al.} proposed a differentially private GAN (DPGAN) that achieved differential privacy by adding noise gradients to the optimiser during the training phase .", "cites": [8300, 7608], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of differential privacy and its relevance to GANs in time series, but it does not synthesize or connect the cited works in a deeper way. There is minimal critical evaluation of the methods or their limitations, and no abstraction to broader principles or trends in privacy-preserving GANs."}}
{"id": "22fa1be1-b9f2-4e7e-b539-c278133c3be5", "title": "Decentralised/Federated Learning", "level": "subsection", "subsections": [], "parent_id": "fa175aea-1e64-4018-9aa0-9f39540d4289", "prefix_titles": [["title", "Generative adversarial networks in time series: A survey and taxonomy"], ["section", "Privacy"], ["subsection", "Decentralised/Federated Learning"]], "content": "Distributed or decentralised learning is another method for limiting the privacy risk associated with personal and personal sensitive data in machine learning. Standard approaches to machine learning require that all training data be kept on one server. Decentralised/distributed approaches to GAN algorithms require large communication bandwidth to ensure convergence  and are also subject to strict privacy constraints. A new method that enables communication efficient collaborative learning on a shared model while keeping all the training data decentralised is known as \\textit{Federated learning} . Rasouli \\textit{et al.} applied federated learning algorithm to a GAN for communication efficient distributed learning and proved the convergence of their federated learning GAN (FedGAN) . However, it should be noted that they did not experiment with differential privacy in this study but note it as an avenue of future work. \nCombining the above techniques of federated learning and differential privacy in developing new GAN algorithms would lead to a fully decentralised private GAN capable of generating data without leakage of private information to the source data. This is clearly an open research avenue for the community.", "cites": [9077, 6819, 582, 8301], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers on federated learning and GANs, connecting them to highlight the potential for combining these methods in a privacy-preserving context. It includes some critical points, such as noting the absence of differential privacy in FedGAN, but does not go into deeper evaluation or comparison. The abstraction level is moderate, identifying a broader research direction (fully decentralised private GANs) and the interplay between communication efficiency and privacy."}}
{"id": "5d898b77-8a10-4667-860f-8d9c6ac547a6", "title": "Assessment of Privacy Preservation", "level": "subsection", "subsections": [], "parent_id": "fa175aea-1e64-4018-9aa0-9f39540d4289", "prefix_titles": [["title", "Generative adversarial networks in time series: A survey and taxonomy"], ["section", "Privacy"], ["subsection", "Assessment of Privacy Preservation"]], "content": "We can also assess how well the generative model was able to protect our privacy through tests known as attribute and presence disclosure . The latter test is more commonly known in the machine learning space as a membership inference attack. This has become a quantitative assessment of how machine learning models leak information about the individual data records on which they were trained . \nHayes \\textit{et al.} carried out membership inference attacks on synthetic images and concluded that for acceptable levels of privacy in the GAN, the quality of the data generated is sacrificed . Conversely, others have followed this approach and found that DP networks can successfully generate data that adheres to differential privacy and resists membership inference attacks without too much degradation in the quality of the generated data .", "cites": [8294, 603, 989, 6059, 8298], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes two contrasting findings from the literature regarding the trade-off between privacy and data quality in GANs. It integrates the work of Hayes et al. and others to form a basic comparison. However, the critical analysis is limited, as it does not deeply evaluate the methodologies or implications of these findings. Abstraction is minimal, focusing on specific results rather than broader principles or patterns."}}
{"id": "96303e7c-ae24-4b8b-a769-3c400ea2b883", "title": "Conclusion", "level": "section", "subsections": [], "parent_id": "df6e31e1-1c33-44b0-a2a3-972d7f8830eb", "prefix_titles": [["title", "Generative adversarial networks in time series: A survey and taxonomy"], ["section", "Conclusion"]], "content": "This paper reviews a niche but growing use of Generative Adversarial Networks for time series data based mainly around architectural evolution and loss function variants. We see that each GAN provides application-specific performance and doesn't necessarily generalise well to other applications, e.g. a GAN for generating high-quality physiological time series may not produce high-fidelity audio due to some limitation imposed by the architecture or loss function. A detailed review of the applications of time series GANs to real-world problems has been provided, along with their datasets and the evaluation metrics used for each domain. As stated in , GAN-related research for time series lags that of computer vision both in terms of performance and defined rules for generalisation of models. In conclusion, this review has highlighted the open challenges in this area and offers directions for future work and technological innovation, particularly for those GAN aspects related to evaluation, privacy, and decentralised learning. \n\\section*{Acknowledgements}\nThis work is funded by Science Foundation Ireland under grant numbers 17/RC-PhD/3482 and SFI/12/RC/2289\\_P2.\n\\bibliographystyle{IEEEtran}\n\\bibliography{template}\n\\end{document}", "cites": [5906], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a coherent synthesis of the reviewed literature by highlighting architectural and loss function variations, as well as application-specific performance limitations. It offers a critical perspective by noting the lack of generalization and the lag in GAN research for time series compared to computer vision. While it identifies broader challenges and future directions, the abstraction remains somewhat limited to the GAN domain without deeper meta-level generalizations."}}
