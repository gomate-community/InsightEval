{"id": "6d6e1c41-d730-4443-9f55-c97f70207f24", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "6220d89f-a486-420a-914e-59957895a313", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Introduction"]], "content": "Machine learning (ML) models implement intelligence in computing systems. Different ML models are widely used in several important domains including computer vision (object classification  and detection ), natural language processing  , media generation , recommendation systems , medical diagnosis , large-scale scientific computing , embedded systems , mobile and edge processing , and even for designing or optimizing hardware and software systems . Domain-customized accelerators can significantly speed up their execution in an energy-efficient manner . However, the computational and memory requirements for processing these models have surged drastically . Moreover, ML models can be deeper and larger, which improves learning accuracy, but significant redundancy may exist in these often over-parameterized models . Therefore, recent techniques for efficient learning and inference have proposed compressing tensors of ML models. Tensors are compressed by inducing and leveraging: \\textit{(a) sparsity (zero values in tensors)} , \\textit{(b) size reduction (tensor decomposition, dimension reduction, and shape reduction)} , and \\textit{(c) quantization (precision lowering and leveraging value similarity)} . With significantly lowered computational, storage, and communication requirements, efficient processing of \\textit{compressed tensors (sparse, size-reduced, and quantized)} offers notable acceleration and energy efficiency opportunities .\n\\begin{figure*}[t!]\n\\centering\n\\centerline{\\includegraphics[width=0.95\\linewidth]{figures/sparse-accel-block-diagram}}\n\\caption{Overview of the accelerator system for processing sparse and irregular tensor computations. (Section \\ref{sec::overview-accel-compressed-tensors} provides further discussion.)}\n\\label{fig::sparse-accel-block-diagram}\n\\end{figure*}\nHardware accelerators can efficiently process tensor computations of ML models. In particular, coarse-grain spatial architectures are a common choice for hardware accelerator designs. They contain an array of processing elements (PEs) with local registers/memory and shared memory. These accelerators feature interconnects like mesh or multicast for communicating data to PEs and reusing the data spatially, which reduces the accesses to the memory hierarchy. With simple PE designs and effective spatial and temporal management of the data and computations, such architectures achieve high speedups and energy-efficiency .\nSpecial mechanisms are needed to exploit the acceleration benefits due to tensor sparsity, size reduction, and quantization. This is because, while hardware accelerators for ML can process low-precision tensors, they inherently cannot benefit from sparsity . They are designed for performing structured computations with regular memory accesses and communication patterns. Without special support for sparse tensors, they fetch all the data, including zero values from memory and feed into PEs, thereby wasting the execution time. Sparsity, especially unstructured, induces irregularity in processing since non-zeros (NZs) or blocks of NZs are scattered across tensors. So, leveraging sparsity necessitates additional mechanisms to store, extract, communicate, compute, and load-balance the NZs and the corresponding hardware or software support. The goal of exploiting sparsity is to exploit all forms of sparsity possible to considerably reduce computation, communication, and storage of zeros while avoiding adding performance, power, and area overheads. Exploiting sparsity effectively depends on tailoring the data encoding and extraction, dataflow, memory banking structure, interconnect design, and write-back mechanisms. Further, it requires new representations and enables new opportunities for hardware/software/model co-designs. In this survey, we mainly discuss different accelerator designs that have leveraged the sparsity of different tensors and different opportunities for performance gains and energy efficiency. \nTensor decomposition and dimension reduction yield tensors of various sizes and asymmetric shapes . Dataflow mechanisms for executing layers of the models are typically optimized well for some commonly used layers (symmetric dimensions). They often become ill-suited for processing tensors with reduced dimensions  and different functionality. So, we describe how configurable designs and flexible dataflows can help to achieve efficient execution. Sparse tensors quantized with value sharing require additional support to index a dictionary for obtaining shared values. The survey also discusses how accelerators leverage value similarity across inputs, weights, or outputs and support variable bit-widths of sparse tensors.\n\\textbf{Contributions:} This paper provides a comprehensive survey of different techniques for efficiently executing sparse and irregular tensor computations of the compact ML models on hardware accelerators. It describes corresponding enhancements in the hardware architecture and the required software support. In specific, \n\\begin{itemize}\n    \\item For inference and training of different ML models, we summarize various sources of the sparsity of tensors.\n    \\item We highlight challenges in accelerating computations of sparse (especially unstructured) and irregular-shaped tensors (e.g., dot product, convolution, and matrix multiplication) on spatial-architecture-based hardware accelerators that execute with dataflow mechanisms.\n    \\item We present an overview of the accelerator system along with the different hardware/software modules for sparse and irregular computations, their interfacing, and the execution flow. We provide an in-depth discussion of the need of each module, different design choices, and qualitative analysis of the different choices.\n    \\item We survey different accelerator systems and execution techniques for sparse tensors of ML models and provide taxonomies to categorize them based on the various hardware/software aspects of the designs.\n     \\item We analyze how variations in sparsity and tensor shapes of different models impact the storage efficiency of different sparsity-encodings and the reuse of tensors. \n    \\item For designing these accelerator modules and overall accelerator system, we discuss recent trends and outline further opportunities for hardware/software/model co-designs.\n\\end{itemize} \n\\textbf{Paper organization:}  \n\\begin{itemize}[leftmargin=*]\n    \\item Section \\ref{sec::background} provides a brief background on different ML models, hardware accelerators for their tensor computations, and the need for further efficiency by reducing computation, storage, and communication requirements.\n    \\item Section \\ref{sec::tensor-compression} discusses tensor compression and opportunities due to sparse, size-reduced, and quantized tensors and why their efficient processing  requires special support.\n    \\item Section \\ref{sec::overview-accel-compressed-tensors} provides an overview of the accelerator system with enhanced architectural modules and software support for sparse and irregular tensor computations (Fig. \\ref{fig::sparse-accel-block-diagram}). It also presents a case study of accelerations of recent, sparse DNNs and analyzes execution bottlenecks. In-depth discussions of individual modules follow through sections \\ref{sec::sparse-data-coding}--\\ref{sec::software-support}. Opportunities for optimizing each module further are discussed at the end of corresponding sections or subsections.\n    \\item Section \\ref{sec::sparse-data-coding} illustrates common sparse data encodings, analyzes their implications in terms of storage and coding overheads, and describes the group-wise encoding of tensors.\n    \\item Section \\ref{sec::NZ-data-extraction} discusses techniques for extracting matching NZs from tensors for computations. It analyzes the advantages and limitations of the centralized and in-PE extractions.\n    \\item Section \\ref{sec::memory-management} discusses managing non-coherent, multi-banked, global scratchpad and hiding the memory access latency behind computations. It also discusses data reuse of the sparse tensors and cross-layer reuse opportunities. \n    \\item Section \\ref{sec::comm-networks} discusses interconnect designs for distributing data from memory and reducing partial outputs, their bandwidth requirements, spatial data reuse, and their configurability to support multiple dataflows for execution. \n    \\item Section \\ref{sec::PEArch} describes sparsity-aware dataflows and pipelined PE architecture including tailoring functional units for sparsity, bit-adaptive computing, and leveraging value similarity. \n    \\item Section \\ref{sec::load-balance} discusses sources of the inter-PE and intra-PE imbalance due to sparsity and their impact, software-directed balancing, and hardware structures for dynamic balancing. \n    \\item Section \\ref{sec::post-processing} describes different write-back mechanisms for collecting data from PEs and assembling the data locally in PEs or on a central module. It also discusses data layout transformations and on-the-fly encoding of sparse outputs.\n    \\item Section \\ref{sec::software-support} discusses compiler support for targeting hardware accelerators, including intermediate representations for deep learning models, compiler optimizations and their automation, and ISAs and code generation for accelerators.\n    \\item Section \\ref{sec::future-directions} describes recent trends and future directions in terms of developing tools and techniques for systematic exploration of hardware/software/model co-designs.\n    \\item Section \\ref{sec::related-works} discusses relevant surveys that describe additional details (domain-specific models, tensor compression techniques, etc.) and can be useful to readers. \n\\end{itemize}", "cites": [679, 7167, 7, 836, 9103, 7005, 885, 547, 4335, 4334, 38, 1759, 2912, 784, 4333, 97, 7168, 7634, 7299, 842, 4331, 194, 4332, 4330, 7430], "cite_extract_rate": 0.5813953488372093, "origin_cites_number": 43, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes diverse cited works to present a coherent narrative on ML model compression and hardware acceleration needs. It abstracts common themes such as sparsity, quantization, and irregular tensor patterns across domains like vision, NLP, and recommendation systems. While it offers analytical insights into challenges and design requirements, it stops short of deep comparative or critical evaluation of individual approaches."}}
{"id": "1fe75192-0778-4883-abf0-6e82d54675a3", "title": "Domain-Specific Machine Learning Models", "level": "subsection", "subsections": [], "parent_id": "9808a39b-5577-4a19-8e94-4d9dd70a65c5", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Background: Need for Efficient Execution of ML Models on Hardware Accelerators"], ["subsection", "Domain-Specific Machine Learning Models"]], "content": "Learning through ML models can be supervised (where labeled data is available), unsupervised (training samples are unlabeled), or semi-supervised. We refer non-expert readers to surveys  for a detailed discussion on different learning approaches and inference and training of various models. Discussions through this survey mainly focus on accelerating different \\textit{deep neural networks (DNNs)} that are commonly used for supervised learning. \n\\textbf{Convolutional neural networks (CNNs)} are used for object classification and detection in image processing, video analysis, and autonomous vehicle systems. CNNs majorly consist of many \\textit{convolution layers (CONV)} and a \\textit{few fully-connected (FC)} layers. Early CONV layers capture low-level features from the images (e.g., edges and corners), which are used for constructing high-level features (e.g., shapes) by subsequent layers. Finally, the classifier aka FC layer determines the type of the objects .\n\\textbf{Sequence-to-sequence models} include recurrent neural networks (RNNs), gated recurrent units (GRU), long-short term memory (LSTM) , and attention mechanisms . These models are used for \\textit{natural language processing (NLP)} and media processing tasks. They essentially use unidirectional or bidirectional recurrent cells at their core and process \\textit{multi-layer perceptrons (MLP)} aka FC structures. \n\\textbf{Models for semantic segmentation and language translation} use encoder-decoder structures with convolutions , recurrent cells, or attention layers , respectively.\n\\textbf{Generative adversarial networks (GANs)}  are used by media generation applications. GANs use generators and discriminative networks that consist of convolution layers. \n\\textbf{Graph neural networks (GNNs)} and other graph learning models  are used for applications such as text classification and translation, node classification and link predictions in large social graphs, etc. They learn graph properties and infer about unforeseen information. To achieve this objective, each node contains an embedding feature vector with the information mixture about own and neighborhood features. The nodes then recurrently aggregate features of local neighbors, perform neural network computations on aggregated data (e.g., MLP for down-scaling embeddings), and update their embeddings. \n\\textbf{Recommendation system models} consist of embedding layers (look-ups and matrix operations) , CNNs for object detection and video understanding, and RNNs for processing language models . \nPrimitives like MLP or GEMM (general matrix multiply) and CONV are at the core of many models and dominate the execution. So, ML frameworks like PyTorch , TensorFlow , and Intel MKL  provide efficient implementations of these primitives for execution on commodity hardware (CPUs, GPUs, FPGAs) or even specialized accelerators. So, our discussions mainly focus on efficiently accelerating tensor computations of MLP, CONV, and RNN operators.", "cites": [194, 1759, 9103, 4333, 41, 7157, 7007, 7633, 7, 38], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 15, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of domain-specific ML models and their applications, with brief mentions of cited papers. However, it lacks deep synthesis of ideas across the sources and offers little critical evaluation or abstraction beyond individual model descriptions. It mainly serves as a summary of common ML model types."}}
{"id": "7dbbc9d2-5da7-400f-b7e4-603692ededcf", "title": "Hardware Accelerators for Machine Learning", "level": "subsection", "subsections": [], "parent_id": "9808a39b-5577-4a19-8e94-4d9dd70a65c5", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Background: Need for Efficient Execution of ML Models on Hardware Accelerators"], ["subsection", "Hardware Accelerators for Machine Learning"]], "content": "\\begin{figure}[t]\n\\centering\n\\centerline{\\includegraphics[width=\\linewidth]{figures/dataflow-accelerators}}\n\\caption{Abstract accelerator design for processing sparse tensors of machine learning applications. Execution of applications require explicit management of computational, communication, and memory resources.}\n\\label{fig::ML-accelerators}\n\\end{figure}\nIn the “new golden age of computer architecture”, recent research efforts and commercial solutions have extensively demonstrated that domain-customized hardware accelerators significantly speed up the execution of ML models in an energy-efficient way . Typically, these specialized solutions feature \\textit{spatial architectures}, which are those that expose low-level aspects of the hardware's interconnect and storage to the hardware-software interface. Spatial architectures can be coarse-grained or fine-grained. Coarse-grained architectures feature arrays of interconnected PEs, and fine-grained designs are realized by programming FPGAs. \\textit{Coarse-grained spatial architectures} are a common implementation choice for designing hardware accelerators for ML . As Fig. \\ref{fig::ML-accelerators} illustrates, the accelerator comprises an array of PEs that may contain private register files (RFs) and shared buffers or a scratchpad memory. PEs are simple in design (functional units with little local control), and the shared scratchpad is non-coherent with software-directed execution. Therefore, these accelerators are a few orders of magnitude more power-efficient than out-of-order CPU or GPU cores . They lead to highly energy-efficient execution of ML models that are compute-intensive and memory-intensive. Performance-critical tensor computations of ML models are relatively simple operations like element-wise or tensor additions and multiplications. So, they can be processed efficiently with structured computations on the PE-array. Moreover, private and shared memories of PEs enable high temporal reuse of the data ; with efficient data management, PEs can be continuously engaged in tensor computations while the data is communicated via memories . Additionally, interconnects like mesh or multicast enable data communication among PEs and spatial reuse of the data, lowering the accesses to off-chip memory. Thus, with minimized execution time, spatial-architecture-based hardware accelerators yield very high throughput and low latency for processing ML models.", "cites": [7005], "cite_extract_rate": 0.09090909090909091, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from cited works by discussing the role of spatial architectures in ML accelerators, referencing the TPU as a representative example. It abstracts general principles about the benefits of structured computations, data reuse, and interconnect design for energy efficiency and throughput. However, it lacks deeper critical analysis of the limitations or trade-offs of the cited hardware approaches."}}
{"id": "df4922fd-4d93-44b1-926c-837577b79edd", "title": "Need for Further Efficient Execution", "level": "subsection", "subsections": [], "parent_id": "9808a39b-5577-4a19-8e94-4d9dd70a65c5", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Background: Need for Efficient Execution of ML Models on Hardware Accelerators"], ["subsection", "Need for Further Efficient Execution"]], "content": "\\label{sec::need-for-compression}\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=\\linewidth]{figures/openai-plot}\n  \\caption{Computation requirements for the training of AI algorithms almost double every few months (Figure adopted from ).}\n  \\label{fig::ML-model-compute-req}\n\\end{figure}\nWith recent advances in the development of ML models, their computational and memory requirements have increased drastically . Fig. \\ref{fig::ML-model-compute-req} provides an overview of this dramatic surge. One major reason is the rise of deeper models. For example, for processing ImageNet images, AlexNet  contained five CONV and three FC layers (eight parameter layers) with the model size of 61M parameters (weights and bias) and computation of 724 MFLOPs. DNNs like ResNet-101  achieved higher classification accuracy but contained 100+ parameter layers and required processing about 7.6 GFLOPs per image. \\insight{Memory requirements for NLP models have increased massively}, e.g., from 50M--100M parameters (Transformer , 2017) to 175 billion (GPT-3 , 2020).\nWhile deeper and larger models achieve high efficiency for various tasks , they consume high execution time, energy, and memory. Previous studies showed that \\insight{significant data redundancy exists in these often over-parameterized models} . So, researchers have developed techniques that compress tensors and obtain compact models, reducing computational, communication, and storage requirements significantly.", "cites": [679, 97, 7633, 4334, 38], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key trends in computational and memory demands from multiple papers, connecting the growth of model sizes (e.g., Transformer to GPT-3) with the need for hardware acceleration. It provides some abstract insights on the inefficiency of over-parameterized models and the potential of tensor compression. However, it lacks deeper critical analysis of the cited works, such as evaluating trade-offs or limitations in specific approaches."}}
{"id": "f772f077-d5f9-4a62-b8e3-6c1cf6e8e806", "title": "Acceleration Opportunities due to Compact Models and the Need for Special Support", "level": "section", "subsections": ["a09f7517-1ff1-4968-99cf-053b28f0c253", "3fb7d8ac-2c30-4eb5-9d3b-8956b4a4bcb8", "08935b60-7ccb-45a9-af71-db60dc826e34", "916ba4be-6ddc-490b-b26e-ccd9d49fedca"], "parent_id": "6220d89f-a486-420a-914e-59957895a313", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Acceleration Opportunities due to Compact Models and the Need for Special Support"]], "content": "\\label{sec::tensor-compression}\nEfficiency of executing ML models can be improved further by drastically reducing computation, communication, and memory requirements. This can be achieved by compressing tensors of ML models. Tensors are compressed by inducing and leveraging: (a) sparsity (zero values) , (b) size reduction (tensor decomposition, dimension reduction, and shape reduction) , and (c) quantization (precision lowering and value similarity) . Previous techniques have achieved highly compact models without incurring accuracy loss. For example, after applying pruning, quantization, and Huffman encoding, Deep Compression  reduced the model size of AlexNet and VGG-16 by 35$\\times$ and 49$\\times$ (e.g., from 552 MB to 11.3 MB), respectively. Accelerator-aware designs can compress the model further. For AlexNet and GoogLeNet models,  pruned 91\\% and 66\\% of weights and reduced computational requirements by 6.63$\\times$ and 3.43$\\times$, respectively. ADMM-NN  applied weight pruning and quantization, thereby reducing the model size of AlexNet, VGG-16, and ResNet-50 (with up to 0.2\\% accuracy loss) by 99$\\times$, 66.5$\\times$ and 25.3$\\times$, respectively.\nThis section describes various sources of tensor sparsity which is either inherent or induced by model architecture or regularization. It describes how sparsity reduces computations, storage, and communication requirements. It also discusses techniques for reducing the size and quantization of the tensors, and how they offer advantages in terms of storage/performance/energy-efficiency. Then, it describes how compression techniques may induce irregularity in the processing and why special support is needed for efficiently processing the compressed tensors on hardware accelerators.\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=\\linewidth]{figures/sparsity-structures}\n  \\caption{Common sparsity structures (e.g., for a 75\\% sparse 8$\\times$8 matrix).}\n  \\label{fig::sparsity-structures}\n\\end{figure}", "cites": [2912, 7167, 4332, 7168, 7299, 7430, 842], "cite_extract_rate": 0.5833333333333334, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of tensor compression techniques and cites multiple papers to support examples of model size reduction. While it connects some ideas (e.g., structured sparsity, quantization, and pruning), it primarily presents isolated results without a deeper synthesis or comparative framework. There is minimal critical evaluation or abstraction of broader principles beyond the cited examples."}}
{"id": "a09f7517-1ff1-4968-99cf-053b28f0c253", "title": "Opportunities Due to Sparse Tensors", "level": "subsection", "subsections": [], "parent_id": "f772f077-d5f9-4a62-b8e3-6c1cf6e8e806", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Acceleration Opportunities due to Compact Models and the Need for Special Support"], ["subsection", "Opportunities Due to Sparse Tensors"]], "content": "\\mysubsubsection{Sparsity Structures} Inherent sparsity is usually unstructured (e.g., of activations, gradients, or tensors of scientific computing applications), where NZ elements are randomly scattered (shaded elements in Fig. \\ref{fig::sparsity-structures}a). Applying ReLU, dropout, quantization, or fine-grain pruning also induces unstructured sparsity in input activations ($IA$) or weights ($W$). For improving execution efficiency, pruning techniques or model operators induce structured sparsity. For example, weights can be pruned in coarse-grain blocks where block shape can vary from 1-D (vector) to n-D for an n-dimension tensor . Fig. \\ref{fig::sparsity-structures}b shows 4$\\times$4 blocks for a block-sparse tensor, where each block contains all zeros or all NZs. With larger blocks, techniques often prune entire dimensions (e.g., channels or filters in CNN models) . The selection of block size and shape depends on task accuracy requirements. Alternatively, tensors are  sparsified with density bounded blocks (Fig. \\ref{fig::sparsity-structures}c), where each n-D block contains a fixed ($k$) number of NZs . It equally scatters NZs throughout the tensor. NZs are located arbitrarily in the whole block, or a fixed number of NZs can be induced across each dimension of the block. Values of $k$ can be selected based on the sensitivity of the pruning to accuracy. For example, analysis of  showed that for VGG-16 and ResNet-50, about 12 out of 16 elements can be pruned without any accuracy loss, and about 10 out of 16 elements for compact models like MobileNetV1 and SqueezeNetV1. To preserve accuracy while achieving high sparsity, a mixture of blocks (with different block sizes or sparsity) can also be introduced . Lastly, tensors can be pruned in patterns or conditionally with sophisticated rules (e.g., diagonally, as Fig. \\ref{fig::sparsity-structures}d shows).\n\\mysubsubsection{Sources of Sparsity}\nTensors of different ML models can be sparse due to multiple reasons:\n$\\bullet$ CNNs use the ReLU activation function  that clamps negative values to zero. So, sparsity of input activations ($IA$-sparsity) can be 40\\% in CNNs, on average  and higher in later layers (about up to 70\\% ). Cao et al.  reported that max-pooling can amplify it, e.g., up to 80\\% for VGG-16 layers. Lee et al.  showed that IA-sparsity eliminated about 40\\% and 55\\% of the multiply-and-accumulate (MAC) operations during CNN training and inference, respectively. For recent compact models like MobileNetV2 , IA-sparsity eliminates about 20\\% of the MACs.\n$\\bullet$ Neural networks use drop-out layers to avoid overfitting. After applying the drop-out, only partial activations are retained . Dropping the activations induces sparsity . \n$\\bullet$ Pruning techniques remove unimportant weights and alleviate the overfitting of the model while maintaining the classification accuracy. Typically, weights with the least significant values can be safely pruned  (in training or post-training). Pruning can bring regularity in the learning of the model and can even increase accuracy slightly . Pruning algorithms introduce significant sparsity, e.g., more than 60\\% weights of CONV and more than 90\\% of the weights of FC layers can be removed  ($W$-sparsity). For recent compact models like MobileNetV2 and EfficientNetB0, W-sparsity can be 50\\%--93\\% (80\\%--85\\% in point-wise convolutions) , which reduces MACs by 2.5$\\times$--4.2$\\times$. Similarly, more than 80\\% weights of RNN, GRU, or LSTMs can be pruned , especially for medium or large models, without significantly increasing error rate. For NLP models Transformers  and BERT , recent techniques induce 80\\%  and 93\\%  W-sparsity, which reduces total MACs by about 4.8$\\times$ and 12.3$\\times$, respectively. Besides, regularization of the models (e.g., L1 or group-lasso based) can induce unstructured or structured W-sparsity . \nPruning of activations is also shown as effective . DasNet  reported eliminating about 27\\% and 12\\% MACs by activation sparsification for AlexNet and MobileNet. It achieved 79\\% IA-sparsity for AlexNet FC layers along with pruning 81\\% weights, without dropping top-1 accuracy. Similarly, MASR  refactored batch normalization, achieving about 60\\% IA-sparsity for RNNs. For attention-based NLP models, SpAtten  pruned unimportant tokens and heads. It reported reducing computations and DRAM accesses by up to 3.8$\\times$ and 1.1$\\times$, respectively, without accuracy loss.\n$\\bullet$ CNNs use Atrous (dilated) convolutions where filters are upsampled by inserting zeros between weights . \n$\\bullet$ GANs use transposed convolution in a degenerator network, where input data is upscaled first by inserting zeros between values, and then convolution is applied. For transposed convolutions in different GANs, about 60\\% MACs can be zero . Additional sparsity is introduced when GANs are forced to forget generating specific objects .\n$\\bullet$ Input data for object detection tasks can be inherently sparse, as only specific regions of frames are valid . For example, object detection models of autonomous driving systems process 3D LiDAR data by constructing point clouds and projecting them from the bird's eye view (top view) . The resultant images are then fed to object detection algorithms for locating the regions of interest. Recent techniques have reported that the sparsity of the input data for object detection can be 80\\% or more .\n$\\bullet$ For efficient communication in distributed training, \\emph{gradients} ($Grad$) are sparsified and compressed. E.g., \\textit{Grad-sparsity} can be 99\\%+ for \\textit{computer vision (CV)} or language processing tasks  and 95\\%--99\\% for recommendation models .\n$\\bullet$ Input data for the tasks of recommendation systems (e.g., user-item matrix) can be inherently highly sparse, e.g., from 95\\%  to 99\\% . Recommendation models compute dot products on dense-sparse or sparse-sparse data .\n$\\bullet$ GNNs process large graphs, e.g., with thousands of vertices. Depending on the real-world interactions of objects (vertices), data contain high (e.g., 75\\%--99\\%) or hyper (99\\%+) unstructured sparsity . For example, in processing large graphs with GCNs, many features of vertices are local and lead to zeros in adjacency matrices for remote nodes . GNN computations involve aggregation on sparse data and multiplications of dense matrices with dense or sparse matrices , which are often processed on separate modules of the accelerator (e.g., in HyGCN  and EnGN ).\n$\\bullet$ Text corpus in text analytics applications leads to high sparsity since each document contains only a fraction of the words from the vocabulary. Such analytics applications include PCA for dimensionality reduction of the sparse data, support vector machines and regression for classification, collaborative filtering for the recommendation, and k-means for clustering the data . These operations involve multiplications of sparse matrices with dense or sparse vectors, where the matrix sparsity can vary from 67\\% to 99\\% .\nWhile we describe leveraging sparsity for ML models, applications of many domains, including linear algebra, graph processing, and scientific computing , can be accelerated by exploiting sparsity.\n\\mysubsubsection{Advantages}\nSparsity allows (i) \\textit{eliminating ineffectual computations}, i.e., reduces execution time and energy by processing only NZs, (ii) \\textit{reducing storage} by encoding only NZ values, so more data fits in on-chip memory and off-chip memory accesses (extremely energy-consuming ) are reduced, and (iii) improving speedup due to \\textit{reduced communication requirements} for data-intensive ML models.", "cites": [8767, 4343, 4342, 1003, 7280, 848, 4346, 7, 4339, 840, 7169, 4337, 4340, 1199, 38, 4344, 4348, 4347, 1759, 7168, 4345, 8766, 7634, 842, 4331, 4336, 4338, 844, 4349, 4341, 8768], "cite_extract_rate": 0.6078431372549019, "origin_cites_number": 51, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes a wide range of cited works to systematically categorize sparsity structures, their sources, and benefits in ML models. It abstracts these findings into general advantages of sparsity and identifies patterns across CNNs, RNNs, GANs, GNNs, and NLP models. While it provides a coherent narrative and highlights opportunities, critical evaluation of limitations or trade-offs between methods is somewhat limited, preventing a higher score on that dimension."}}
{"id": "3fb7d8ac-2c30-4eb5-9d3b-8956b4a4bcb8", "title": "Opportunities Due to Size-Reduced Tensors", "level": "subsection", "subsections": [], "parent_id": "f772f077-d5f9-4a62-b8e3-6c1cf6e8e806", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Acceleration Opportunities due to Compact Models and the Need for Special Support"], ["subsection", "Opportunities Due to Size-Reduced Tensors"]], "content": "Symmetric or high-dimensional tensors have large sizes and their processing requires more computation and memory. So, ML models are designed to reduce such requirements by using group or parallel operators , 1$\\times$1 or point-wise convolutions (PW-CONV) , or dimensionality reduction with PCA . Moreover, tensors can be decomposed with spatial factorization , depth-wise separation for convolutions , or low-rank approximations . Further, tensors can be ragged  to eliminate the need for structured or rectangular shapes. While these transformations significantly reduce storage and computations, they make tensors irregular-shaped (asymmetric).", "cites": [305, 7167, 502, 301, 7430, 842], "cite_extract_rate": 0.6, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section lists various methods for reducing tensor sizes in ML models and briefly mentions how they affect tensor regularity, but it does so in a largely descriptive manner without connecting the cited works to a broader theme or evaluating their relative merits. There is minimal synthesis and no critical analysis or meta-level abstraction."}}
{"id": "08935b60-7ccb-45a9-af71-db60dc826e34", "title": "Opportunities Due to Quantized Tensors", "level": "subsection", "subsections": [], "parent_id": "f772f077-d5f9-4a62-b8e3-6c1cf6e8e806", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Acceleration Opportunities due to Compact Models and the Need for Special Support"], ["subsection", "Opportunities Due to Quantized Tensors"]], "content": "Quantization includes precision lowering  and leveraging value similarity . Precision lowering allows representing tensors (weights, activations, gradients, weight updates) at much lower bit-width (e.g., 8b or lower for inference and 8b/16b for learning). Moreover, elements with similar values can be clustered and approximated by sharing common values (centroids of clusters). Further, similar values of outputs are reused with memoization (partially or the entire layer). In general, significant redundancy exists in tensor elements (particularly in the parameters of large models), and a successfully trained model is generalized and immune to noisy data. So, the error induced by quantization or approximation may often be tolerated by a well-trained model . It can also obviate over-fitting caused otherwise by excessive precision, thereby bringing generality in learning . For compensating accuracy drop due to quantization, learning algorithms fine-tune the model or use quantization-aware training . Thus, quantization or approximation techniques typically do not degrade inference accuracy  or trade it off for notable execution efficiency . \nQuantization significantly reduces storage requirements and accesses to off-chip memory. It also \\textit{reduces area and power}, since for quantized tensors, functional units can be simpler and energy-efficient  (e.g., int8 multiplier consumes 20$\\times$ less energy than FP32 multiplier  for a 45 nm process). Bus sizes can be smaller as bandwidth requirements are reduced. \nThus, with sparse, size-reduced, and quantized tensors, compact models can achieve higher accuracy as models with uncompressed tensors, while becoming amenable for deployment at the edge, mobile, or online-learning platforms  due to scope for low latency, energy, and storage. So, leveraging such opportunities is crucial for further accelerations.", "cites": [4330, 4352, 4351, 4350, 7299], "cite_extract_rate": 0.4166666666666667, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple quantization-related concepts from the cited papers into a coherent narrative about the benefits and implications of using quantized tensors in ML models. It abstracts these ideas to highlight broader patterns such as reduced memory, energy efficiency, and tolerance for approximation in well-trained models. However, while it discusses the impact of quantization, it lacks deeper critical evaluation of the limitations or trade-offs inherent in these techniques, such as potential accuracy degradation or hardware constraints."}}
{"id": "916ba4be-6ddc-490b-b26e-ccd9d49fedca", "title": "Need for Special Support to Accelerate Sparse and Irregular Tensor Computations", "level": "subsection", "subsections": [], "parent_id": "f772f077-d5f9-4a62-b8e3-6c1cf6e8e806", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Acceleration Opportunities due to Compact Models and the Need for Special Support"], ["subsection", "Need for Special Support to Accelerate Sparse and Irregular Tensor Computations"]], "content": "\\label{sec::need-special-support}\nHardware accelerators efficiently process different models . But, they inherently cannot benefit from the sparsity because all the data, including the zero values of activations, weights, and gradients, have to be fetched from memory and communicated to PEs; PEs are also unable to skip ineffectual computations, wasting the execution time. Sparsity, especially unstructured, induces irregularity in processing since NZs or blocks of NZs are scattered across the tensor. Therefore, leveraging sparsity necessitates additional mechanisms to store, extract, communicate, compute, and load-balance the NZs, and corresponding hardware and software support . Different sparsity levels and patterns from various sources lead to unique challenges and solutions in hardware/software co-design. Therefore, our discussions throughout this survey mainly focus on exploiting tensor sparsity for accelerating compact models.\nTensor dimension reduction and tensor decomposition make tensors irregular-shaped (asymmetric), and they may also modify the functionality of the computational primitives, e.g., depthwise convolution (DW-CONV). Since execution on hardware accelerators is typically well-optimized for processing symmetric tensors with a specific dataflow mechanism, these shape transformations and supporting different functionality (e.g., DW-CONV, randomized or approximated matrix multiply ) may introduce irregularity in processing requirements. To sustain high utilization of computational resources, it requires additional support including configurable hardware architectures and flexible mappings of the functionality onto architectural resources . \nHardware accelerators have supported low-precision tensors of fixed bit-widths, and even more recently, tensors with mixed precision . However, when sparse tensors are quantized with value sharing, it requires indexing the codebook through indices for approximated elements . Such irregular accesses are handled by implementing separate indirection tables in the pipelined hardware datapath . Moreover, value similarity is leveraged further by reusing computations with memoized outputs, which requires additional processing. Further, supporting different bit-widths of various sparse tensors of different models requires configurable architectures for \\emph{bit-adaptive} computing .  \nTo sum up, compressed tensors lead to sparse and irregular computations. Their efficient accelerations require special support, which is described in the next section. The appendix describes that exploiting sparsity (especially unstructured) is relatively hard for execution on CPUs and GPUs; with special support, hardware accelerators can achieve notable gains.", "cites": [4353, 7005, 7634, 4335], "cite_extract_rate": 0.26666666666666666, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key challenges related to sparsity and irregular tensor computations, drawing on multiple cited works to highlight the need for hardware and software co-design. It abstracts the issue to broader architectural and functional requirements such as configurability and bit-adaptive computing. While it provides some critical perspectives on how conventional accelerators fail to exploit sparsity, it does not deeply compare or evaluate the cited papers against one another."}}
{"id": "dfcc21bd-b4bb-440e-9f6f-0af65d5e6281", "title": "Overview", "level": "subsection", "subsections": [], "parent_id": "4bd89bd4-f378-4c43-b038-faf39e54d1ba", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Accelerator Design for Efficient Sparse and Irregular Tensor Computations"], ["subsection", "Overview"]], "content": "To efficiently process sparse and irregular tensor computations, designers of the accelerator systems can integrate special hardware or software modules. It enables orchestration of the structured computations while processing the tensors in compressed formats. Consequently, it can lead to efficient utilization of the accelerator resources and allows exploiting acceleration opportunities. Fig. \\ref{fig::sparse-accel-block-diagram} provides an overview of the accelerator system equipped with such modules. This section briefly describes these system modules.\n\\begin{table}\n\\centering\n\\caption{Accelerators for Processing Sparse Tensors.}\n\\label{tab:overview-sparse-accel}\n\\begin{tabular}{|c|m{4.85cm}|}\n\\hline\n\\thead{Objective} & \\thead{Techniques} \\\\ \\hline\n\\makecell{Compressed data \\\\ in off-chip \\\\ memory (storage)} \n&  \\\\ \\hline  \n\\makecell{Compressed data \\\\ in on-chip \\\\ memory (storage)} &   \\\\ \\hline \n\\makecell{Skip processing \\\\ zeros \\\\ (energy efficiency)} &  \\\\ \\hline , \n\\makecell{Reduce ineffectual \\\\ computation cycles \\\\ (performance \\& energy)} &  \\\\ \\hline \n\\makecell{Load balancing \\\\ (performance)} &  \\\\ \\hline  \n\\end{tabular}\n\\end{table}\n\\begin{table*}[!t]\n\\centering\n\\caption{Accelerator Systems Leveraging Sparsity of Different Tensors for Different ML Models.}\n\\label{tab:target-sparsity}\n\\begin{tabular}{|c|c|m{1.5cm}|m{10cm}|}\n\\hline\n\\multirow{3}{*}{\\makecell{Dynamicity \\\\ of Sparsity}} \n& Static & \\multicolumn{2}{l|}{} \\\\ \\cline{2-4} \n& \\multirow{2}{*}{Dynamic} \n& \\multicolumn{2}{l|}{} \\\\ \n& & \\multicolumn{2}{l|}{}  \\\\ \\hline\n\\multirow{4}{*}{\\makecell{Tensors Treated \\\\ as Sparse}} & \\multirow{2}{*}{Weight} & Unstructured & ,  \\\\ \\cline{3-4} \n&  & Structured &     \\\\ \\cline{2-4}\n& Activation & \\multicolumn{2}{l|}{} \\\\ \\cline{2-4}\n& Both & \\multicolumn{2}{l|}{} \\\\ \\hline\n\\multirow{5}{*}{\\makecell{Primitive \\\\ Operation}} \n& \\makecell{Matrix-Vector Multiply} & \\multicolumn{2}{l|}{} \\\\ \\cline{2-4} \n& \\makecell{Matrix-Matrix Multiply} &  \\multicolumn{2}{l|}{} \\\\ \\cline{2-4} \n& \\makecell{Convolution} &  \\multicolumn{2}{l|}{} \\\\ \\cline{2-4}\n& \\makecell{Recurrent / Attention Layer} &  \\multicolumn{2}{l|}{} \\\\ \\hline\n\\multicolumn{2}{|l|}{Accelerators for Learning} & \\multicolumn{2}{l|}{} \\\\ \\hline\n\\end{tabular}\n\\end{table*}\nSparse, size-reduced, and quantized tensors of ML models offer various opportunities for storage, performance, and energy efficiency. Hence, several accelerators have provided marginal or comprehensive support and leveraged some or all the opportunities. Table \\ref{tab:overview-sparse-accel} lists such common objectives and corresponding accelerator solutions that meet these objectives. \nDifferent accelerators for inference and learning exploit $W$-sparsity, $IA$-sparsity, or both, which impacts acceleration gains . Several accelerators, including Cambricon-X , exploit only static sparsity (Table \\ref{tab:target-sparsity}), e.g., when locations of zeros in weights are known beforehand for inference. Static sparsity allows offline encoding and data transformations for arranging structured computations (e.g., for systolic arrays ). Recent accelerators, including ZENA , SNAP , and EyerissV2 , leverage dynamic sparsity also. It requires determining locations of intersecting NZs in both tensors at run-time to feed functional units, on-the-fly decoding (encoding) NZs, and often balancing computations on PEs. Table \\ref{tab:target-sparsity} lists different accelerators that support static and dynamic sparsity of tensors. Now, we describe different hardware and software aspects of the accelerator system that help in leveraging sparsity effectively.\n\\textbf{Sparsity encodings:} Sparse tensors are compressed using encodings, where only NZ values are stored in a \"data\" tensor and one or more \"metadata\" tensors encode locations of NZs. Section \\ref{sec::sparse-data-coding} discusses different formats and associated costs for encoding and decoding. For different sparsity levels, it analyzes their effectiveness in terms of storage efficiency. E.g., tensors can be compressed by 1.8$\\times$ and 2.8$\\times$ for 50\\% and 70\\% sparsity (bitmap or RLC-2) and 7.6$\\times$ and 55$\\times$--60$\\times$ for 90\\% (RLC-4) and 99\\% sparsity (CSC or RLC-7). Structured sparsity (coarse-grain block-sparse) can alleviate the overheads of metadata and fine-grained data extraction by encoding indices for only large dense blocks. For accelerating ML models, sparse tensors are also quantized i.e., their precisions are lowered (typically int8 or int16 for inference  and FP16 for learning ) and often approximated by clustering data of similar values . Therefore, encoded sparse data contains quantized values of NZs. \n\\textbf{NZ detection and data extraction:} In processing sparse tensors of different primitives, corresponding elements of the weight and activation tensors are multiplied and accumulated. Depending on the sparsity, accelerators need to use data extraction logic that decodes compressed tensors, search within a window of NZs or index the buffer, and obtain matching pairs of NZs to feed the functional units for computation. Section \\ref{sec::NZ-data-extraction} provides a taxonomy of different data extraction mechanisms and analyzes their implications for various sparsity levels. Up to moderate $IA$-sparsity and high $W$-sparsity, these indexing or intersection-based mechanisms efficiently extract sufficient NZs at every cycle for keeping functional units engaged. For efficient compute-bounded executions at such sparsity, accelerators reported achieving near-ideal speedups (e.g., about 80\\%--97\\% of the speedup corresponding to reduced operations, i.e., \\textit{sparsity-speedup ratio}) . However, extraction becomes challenging at high (e.g., 90\\%+) or hyper sparsity as NZs are scattered at distant locations , and execution is usually memory-bounded with low arithmetic intensity. Section \\ref{sec::NZ-data-extraction} also discusses sharing of the data extraction mechanism among PEs or employing in PEs. Then, it discusses opportunities for further optimizations.\n\\textbf{Memory management:} Compressed tensors are often stored in the shared on-chip memory that is non-coherent, multi-banked, and often non-unified. For a pre-determined sequence of execution, a controller or PEs initiates the accesses between off-chip and on-chip memory; their latency needs to be hidden behind computations on PEs. Section \\ref{sec::memory-management} discusses corresponding memory architectures and techniques for hiding miss penalty for sparse tensors via double-buffering or asynchronous computation and memory accesses. It describes the data reuse opportunities for various sparsity and dimensions of tensors of common DNNs and how sparsity lowers the reuse. It also discusses techniques that leverage cross-layer reuse of intermediate output layers and reduce latency. \n\\textbf{Communication networks:} Once tensor blocks are fetched from memory, they are distributed to appropriate PEs via interconnect networks (often one per operand). Efficient designs ensure that sufficient data can be fed to PEs while they perform computations. Reuse is leveraged spatially by multicast or mesh networks that communicate common data blocks to multiple PEs. It lowers accesses to memory hierarchy and communication latency. However, spatial reuse opportunities vary depending on the sparsity, NZ extraction mechanism, and mapping of the functionality on the accelerator. Section \\ref{sec::comm-networks} discusses different designs for distributing sparse and quantized tensors and reducing partial outputs. It also describes challenges in executing inter-PE communications that may become unstructured due to sparsity and the temporal and spatial mechanisms for reduction/collection of the outputs. It describes how configurable designs support various communication patterns for different sparsity, reuse, and functionality. \n\\textbf{PE architecture:} Several accelerators consist of scalar PEs with fused MAC units (e.g., EIE , LNPU , and Envision ). Others contain SIMD PEs (multiple functional units) (e.g., EyerissV2 ) or vector PEs consisting of multiplier-arrays and adder-trees (e.g., Cambricon-X  and SNAP ). PE architectures either directly process pairs of matching NZs extracted from tensors or use hardware logic for data extraction or coordinate computation (Fig. \\ref{fig::ML-accelerators}). Effectively utilizing functional units can be challenging for variations in sparsity, precisions, and functionality, and it may require configurable designs. Section \\ref{sec::PEArch} provides corresponding discussions and describes sparsity-aware dataflow mechanisms (mapping of tensor computations on accelerator resources) used by different accelerators. It also describes how accelerators have leveraged value similarity of tensors and the corresponding modifications in the PE architecture.\n\\textbf{Load balancing:} Depending on the distribution of zeros, the execution may end up with processing a different amount of NZs on different PEs or their functional units, which creates inter-PE or intra-PE load imbalance. Section \\ref{sec::load-balance} analyzes such sources of the imbalance and introduces a taxonomy of different load balancing techniques. Accelerators achieve load balance through either software techniques (e.g., structured pruning or data reorganization) or by providing a hardware module for dynamic work balance (through asynchronous execution or work sharing), which provides further accelerations. For example, ZENA  leveraged the sparsity of both activation and weight tensors for AlexNet and VGG-16 models and reported about 32\\% additional performance gains through load balancing. Dynamic load balancing can provide notable speedups for high, unstructured sparsity .\n\\textbf{Write-back and post-processing:} Tensor elements produced by PEs need to be collected, post-processed for further operations, and written back to the memory. PEs in different accelerators either write back sequentially or asynchronously through a shared bus or via point-to-point links. In addition, accelerators usually contain a post-processing unit that re-organizes the data (as per the dataflow mechanism of the current and next layer of the model) and encodes sparse output on the fly. Section \\ref{sec::post-processing} discusses such mechanisms. \n\\textbf{Compilation support:} It is important to support the execution of various ML models on accelerators and easier programming of models from ML libraries. Section \\ref{sec::software-support} discusses compiler support for sparse models and hardware accelerators. It discusses polyhedral and non-polyhedral intermediate representations and their implications on the compiler’s ability to represent the code and apply code transformations. It describes challenges in supporting sparse tensors and DNN compilers that facilitate sparse tensor computations. Then, it discusses compiler optimizations including common loop optimizations and those specific to hardware intrinsics. It also describes semi-automatic optimizations for transforming the loops and data layout and automatic optimizations using cost models. Finally, it discusses ISAs used by accelerators and their code generation by using libraries of high-level primitives.", "cites": [4355, 8767, 4342, 7169, 7804, 4335, 4347, 4344, 4354, 7634, 7805, 4356, 4341], "cite_extract_rate": 0.2708333333333333, "origin_cites_number": 48, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates multiple cited works to discuss key aspects of accelerator design for sparse and irregular tensor computations, showing reasonable synthesis. It provides an analytical perspective by addressing the trade-offs and challenges (e.g., dynamic vs. static sparsity, memory reuse, and communication networks). However, the critical evaluation is limited, with few direct comparisons or critiques of the approaches. The abstraction level is moderate, as it identifies patterns and principles but does not offer a novel, overarching framework."}}
{"id": "c1d7323b-337a-4566-9a3e-103c4b3f9afa", "title": "Case Study: Acceleration of DNNs and Bottleneck Analysis", "level": "subsection", "subsections": [], "parent_id": "4bd89bd4-f378-4c43-b038-faf39e54d1ba", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Accelerator Design for Efficient Sparse and Irregular Tensor Computations"], ["subsection", "Case Study: Acceleration of DNNs and Bottleneck Analysis"]], "content": "\\label{sec::sparse-dnn-acceleration-case-study}\n\\begin{table}[!t]\n\\centering\n\\caption{Sparsity of Some Popular DNNs.}\n\\label{tab:sparsity-dnn-models}\n\\resizebox{0.48\\textwidth}{!}{\n\\addtolength{\\tabcolsep}{-3pt}\n\\begin{tabular}{|c|c|c|c|c|c|c|c|}\n\\hline\n\\multirow{2}{*}{Model} & \\multirow{2}{*}{Domain} & \\multirow{2}{*}{Dataset} & \\multirow{2}{*}{\\makecell{GOps \\\\ (dense)}} & \\multicolumn{3}{c|}{Sparsity \\%} & \\multirow{2}{*}{\\makecell{Sparse\\\\ Model}} \\\\ \\cline{5-7}\n &  &  &  & IA & W & Ops &  \\\\ \\hline\nMobileNetV2  & CV & ImageNet & 0.3 & 34 & 52 & 81 &  \\\\ \\hline\nEfficientNetB0  & CV & ImageNet & 0.5 & 0 & 68 & 60 &  \\\\ \\hline\nTransformer  & NLP & WMT En-De & 4.6 & 0 & 79 & 79 &  \\\\ \\hline\n\\makecell{BERT-base-uncased } & NLP & \\makecell{SQuAD} & 9.3 & 0 & 92 & 92 &  \\\\ \\hline\n\\end{tabular}\n\\addtolength{\\tabcolsep}{3pt}\n}\n\\end{table}\n\\begin{table*}[!t]\n\\centering\n\\caption{Architectural features of analyzed accelerators for sparse DNNs.}\n\\label{tab:microarch-features-sparse-DNN-accelerators}\n\\addtolength{\\tabcolsep}{-3pt}\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n\\multirow{3}{*}{ID} & \\multirow{3}{*}{\\begin{tabular}[c]{@{}c@{}}Reference \\\\ Architecture\\end{tabular}} & \\multirow{3}{*}{\\begin{tabular}[c]{@{}c@{}}Supported \\\\ Operators\\end{tabular}} & \\multicolumn{2}{c|}{\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Sparsity \\\\ Leveraged\\end{tabular}}} & \\multicolumn{3}{c|}{\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Non-zero Data \\\\ Extraction\\end{tabular}}} & \\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}PE \\\\ Architecture\\end{tabular}} & \\multirow{3}{*}{\\begin{tabular}[c]{@{}c@{}}Work \\\\ Synch-\\\\ ronization\\end{tabular}} & \\multirow{3}{*}{\\begin{tabular}[c]{@{}c@{}}Freq.\\\\ (GHz)\\end{tabular}} & \\multirow{3}{*}{\\begin{tabular}[c]{@{}c@{}}DRAM \\\\ BW\\\\ (GBPS)\\end{tabular}} & \\multicolumn{4}{c|}{Bit-width} \\\\ \\cline{13-16} \n &  &  & \\multicolumn{2}{c|}{} & \\multicolumn{3}{c|}{} &  &  &  &  & \\multicolumn{2}{c|}{data} & \\multicolumn{2}{c|}{metadata} \\\\ \\cline{4-9} \\cline{13-16} \n &  &  & IA & W & Encoding & Discovery & Loc. & FU &  &  &  & IA / O & W & IA & W \\\\ \\hline\nA1 & EIE  & GEMM & \\multicolumn{2}{c|}{\\begin{tabular}[c]{@{}c@{}}unstructured\\end{tabular}} & CSR & \\multirow{2}{*}{Indexing} & in-PE & Scalar & Prefetch & 0.8 & 256 & 16 & 4 & N/A & 4 \\\\ \\cline{1-6} \\cline{8-16} \nA2 & \\begin{tabular}[c]{@{}c@{}}Cambricon-X\\\\ \\end{tabular} & \\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}CONV, \\\\ GEMM\\end{tabular}} & dense & \\makecell{unstru-\\\\ctured} & \\begin{tabular}[c]{@{}c@{}}COO-\\\\ 1D\\end{tabular} &  & \\begin{tabular}[c]{@{}c@{}}central \\\\ (per PE)\\end{tabular} & \\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Vector (16 \\\\ multipliers \\&\\\\ adder tree)\\end{tabular}} & \\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Every \\\\ Output \\\\ Activation\\end{tabular}} & 1 & 256 & 16 & 16 & N/A & 8 \\\\ \\cline{1-2} \\cline{4-8} \\cline{11-16} \nA3 & \\begin{tabular}[c]{@{}c@{}}Cambricon-S\\\\ \\end{tabular} &  & \\makecell{unstru-\\\\ctured} & \\begin{tabular}[c]{@{}c@{}}block-\\\\ sparse\\end{tabular} & \\multirow{2}{*}{Bitmap} & \\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Inter-\\\\ section\\end{tabular}} & \\begin{tabular}[c]{@{}c@{}}central, \\\\ shared\\end{tabular} &  &  & 1 & 256 & 16 & 8 & 1 & 1 \\\\ \\cline{1-5} \\cline{8-16} \nA4 & \\begin{tabular}[c]{@{}c@{}}ZENA-\\\\ IA-W \\end{tabular} & CONV & \\multicolumn{2}{c|}{unstructured} &  &  & in-PE & Scalar & \\begin{tabular}[c]{@{}c@{}}Intra-\\\\ Workgroup\\end{tabular} & 0.2 & 12 & 16 & 16 & 1 & 1 \\\\ \\hline\n\\end{tabular}\n\\addtolength{\\tabcolsep}{3pt}\n\\end{table*}\nThis section analyzes the sparsity of recent DNN models (for NLP and CV) and the acceleration that can be achieved with some of the popular accelerators-alike architectures.\n\\textbf{DNN models:} Table \\ref{tab:sparsity-dnn-models} summarizes analyzed DNN models and their overall sparsity across all CONV, GEMM, and DW-CONV operations. For each of these DNN operations, $W$-sparsity was obtained from sparse DNN models (listed in the last column). $IA$-sparsity was obtained by performing inference with sample data (images and text sequences). $GOps$ corresponds to processing of a single image for CV models and sequences of 24\nand 107 tokens for Transformer and BERT. \n\\textbf{Accelerators:} Table \\ref{tab:microarch-features-sparse-DNN-accelerators} summarizes analyzed accelerators and their sparsity-centered features. Their architectures targeted unstructured or block sparsity of activations and/or weights. Their features represent variations across data encoding, data extraction, vector processing, memory hierarchy, NoC, and load balancing.\n\\textbf{Methodology:} To determine the impact of sparsity on achievable acceleration, we performed a data-driven analysis of the execution latency. For each DNN layer, zeros (or blocks of zeros) were induced randomly according to the sparsity of its tensors. The overall execution time was determined from the latency of processing on functional units, data decoding, extraction of non-zeros, work synchronization, and off-chip memory transfers, which were calculated based on analytical modeling of the microarchitectural features. The speedups were calculated over oracle processing of dense tensors at the accelerator's peak utilization of computational resources and off-chip bandwidth. In this study, we do not consider the processing of DW-CONV on these accelerators, since they are often not pruned, and their execution needs to be group-wise, which is extremely inefficient. Such unsupported performance-critical operators were assumed to be processed with dense tensors at peak utilization of hardware resources.\n\\begin{figure}[t]\n\\centering\n\\centerline{\\includegraphics[width=\\linewidth]{figures/sparsity-perf-analysis}}\n\\caption{(a) Obtained speedups for accelerators listed in Table \\ref{tab:microarch-features-sparse-DNN-accelerators}. (b) Analysis of execution time overheads for obtained accelerations.}\n\\label{fig::sparse-dnn-acceleration-analysis}\n\\end{figure}\n\\textbf{Analysis:} Fig. \\ref{fig::sparse-dnn-acceleration-analysis}(a) shows speedups of accelerators for targeted DNN models, for leveraging the sparsity of supported DNN operators. It illustrates speedups for (i) reduction in the operations due to sparsity (desired), (ii) peak utilization of accelerator's computational resources and off-chip bandwidth while leveraging sparsity, over such oracle processing of dense tensors (potential), and (iii) actual processing on accelerator over oracle processing of dense tensors (obtained). For understanding implications of execution overheads including those incurred by metadata processing and load imbalance, Fig. \\ref{fig::sparse-dnn-acceleration-analysis}(b) illustrates fractions for desired computation time and execution overheads in a stacked format. The overheads were extracted for layer-wise processing and then accumulated to determine the overall impact. Fractions include:\n$\\bullet$ Computation time: Minimum execution time required for processing at peak on accelerator's functional units.\n$\\bullet$ NZ extraction: Time required for decoding NZs from communicated operands and extracting matching operands for feeding the functional units. It also corresponds to balanced computations.\n$\\bullet$ Load imbalance: Time required for on-chip processing on the accelerator, considering the imbalanced computations subjected to the accelerator's work synchronization and work sharing schemes.\n$\\bullet$ DMA time: Time required for off-chip data communication via DMA transfers, in addition to on-chip processing.\nFig. \\ref{fig::sparse-dnn-acceleration-analysis}(a) shows that accelerators efficiently exploited moderate sparsity. E.g., for 4.8$\\times$ reductions in operations of Transformer due to $W$-sparsity, they achieved about 4$\\times$--4.2$\\times$ speedup. The exploitation of speedup lowers when activations are dense and weights are highly or hyper-sparse. This is because accelerators like EIE and Cambricon-X broadcast activations to PEs and extract matching pairs corresponding to NZ weights. So, communication of activations and extraction of matching NZ operands consume significant execution time, while there are fewer operations to feed the functional units (Fig. \\ref{fig::sparse-dnn-acceleration-analysis}b). E.g., for BERT-base-uncased  (92\\% sparse weights ) on SQuAD , they achieved about 7.7$\\times$--8.9$\\times$ speedup out of 12.2$\\times$ speedup for processing at peak. Due to block-sparse weights, computations on PEs of Cambricon-S are always balanced. Therefore, it achieved higher speedups. By using blocks of 16$\\times$16 or even 1$\\times$16 (across input and output channels) for pruning, inducing similar sparsity is not possible sometimes. So, the reduction in operations and potential for the speedup was slightly lower for Cambricon-S (e.g., for EfficientNetB0). In general, due to high DRAM bandwidth, overheads incurred by DMA transfers were hidden (for Cambricon-X/S) or negligible for non-interleaved transfers (e.g., for EIE).\nFig. \\ref{fig::sparse-dnn-acceleration-analysis}(a) also shows that Cambricon-S and ZENA-IA-W achieved higher speedups for CV models by leveraging unstructured sparsity of activations. High IA-sparsity amplified total sparsity during processing several layers (e.g., MobileNetV2), incurring considerable excess processing in data extraction for Cambricon-X/S and in load imbalance for ZENA-IA-W. With zero-aware static sorting of filters and dynamic load balance, ZENA  could overcome such imbalance. But, it would suffer through high on-chip communication time since it used only one shared bus for multicast via NoC and collecting outputs. We disregarded such communication overhead for ZENA-IA-W in this study, as most accelerators use separate NoCs or buses for alleviating communication overheads. Also, due to low DRAM bandwidth, overheads incurred by DMA transfers were higher for ZENA-IA-W, mainly for executing DW-CONVs with dense tensors.", "cites": [7, 4348, 848, 439, 7634, 842, 38, 840], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes sparsity characteristics from multiple DNN models and relates them to hardware accelerator designs, integrating insights from several cited papers. It provides a critical analysis by identifying how different accelerator features interact with sparsity types and by analyzing overheads and inefficiencies in sparsity exploitation. The section abstracts beyond individual papers to highlight general trends and bottlenecks in sparse DNN acceleration."}}
{"id": "76637357-2ce7-49d4-924f-7e20b7606e08", "title": "Encodings for Compressing Sparse Tensors", "level": "section", "subsections": ["7aaaec9f-b2f4-4e9c-b23d-5e1be0d4f563", "c38dbc5f-be9d-49a8-8c86-f1dc0a62356c", "88d6dbef-ac33-4b34-b43e-9ce569064200", "55296c8b-346d-4804-8382-bbbe3495d55b"], "parent_id": "6220d89f-a486-420a-914e-59957895a313", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Encodings for Compressing Sparse Tensors"]], "content": "\\label{sec::sparse-data-coding}\nA sparse tensor is compressed with an encoding format. An encoded tensor contains actual \\textit{data} (NZ values) and \\textit{metadata} (information about positions of NZs). Later, metadata is used by an accelerator's data indexing logic to locate and extract NZs. This section discusses commonly used encodings through an example (Fig. \\ref{fig::sparse-data-encodings}) and their implications on the storage and processing requirements. For different formats, Fig. \\ref{fig::sparse-encoding-schemes-overview} introduces a taxonomy for processing metadata during data extraction, and Table \\ref{tab:sparse-encodings-overhead} lists the corresponding storage overhead. Depending on the mapping of a layer onto the accelerator, tensors are divided into blocks (per PE-wise work) which are encoded separately. We refer to such processing as a \\textit{group-wise encoding}, which is discussed later. Finally, this section briefly describes encoding on the fly and further opportunities.\n\\begin{figure}[t]\n\\centering\n\\centerline{\\includegraphics[width=0.85\\linewidth]{figures/sparse-encoding-schemes-overview}}\n\\caption{A taxonomy for the required processing on the metadata during data extraction when a sparse tensor is encoded using different formats.}\n\\label{fig::sparse-encoding-schemes-overview}\n\\end{figure}\n\\begin{figure*}[t]\n\\centering\n\\centerline{\\includegraphics[width=0.9\\linewidth]{figures/sparse-data-encodings}}\n\\caption{Encodings to store sparse tensors in different formats. Elements with green shade encode the same NZ element. (Figure inspired by .)}\n\\label{fig::sparse-data-encodings}\n\\end{figure*}", "cites": [4357], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of sparse tensor encodings and references a paper on format abstraction, but it lacks in-depth synthesis of multiple sources, critical evaluation of the cited works, or abstraction into broader principles. The narrative is largely based on definitions and mentions of formats, with limited analysis or integration of insights from the referenced paper."}}
{"id": "7aaaec9f-b2f4-4e9c-b23d-5e1be0d4f563", "title": "Encoding Formats and Implications", "level": "subsection", "subsections": [], "parent_id": "76637357-2ce7-49d4-924f-7e20b7606e08", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Encodings for Compressing Sparse Tensors"], ["subsection", "Encoding Formats and Implications"]], "content": "\\mysubsubsection{Coordinate (COO)} It stores absolute positions of NZs. As Fig. \\ref{fig::sparse-data-encodings}(b) shows, all NZs of an uncompressed tensor $T$ are stored in a data vector $val$, and vectors $coord\\_y$ and $coord\\_x$ indicate the coordinates of each NZ value. So, COO is a natural way to express sparse tensors and is used commonly (e.g., in PyTorch). Formats adopted by FROSTT  and matrix market  closely resemble COO.\nThe COO format stores all coordinates in the uncompressed format. E.g., as Fig. \\ref{fig::sparse-data-encodings}(b) shows, the metadata for values '2' and '3' (same row) or '2' and '5' (same column) are not compressed, i.e., duplicate values of row and column indices exist in coordinate vectors. So, the overhead of storing $n$ coordinates per NZ value is about $\\sum_1^n { \\lceil \\log_2{d_i} \\rceil}$ bits (vector $d$ contains the tensor's dimensions). It makes COO inefficient for storing tensors with low or moderate sparsity. \nFig. \\ref{fig::sparse-data-encodings-overhead} shows storage benefits for encoding 2 MB matrices of various sparsity in different formats. We calculated storage requirements with the analysis presented in Table \\ref{tab:sparse-encodings-overhead} and normalized them to the matrix's size in dense format. We used the Scipy library  to generate matrices of various sparsity and encode them in COO, CSR, and CSC. Fig. \\ref{fig::sparse-data-encodings-overhead} shows that for a 2 MB matrix, COO achieves storage efficiency for 70\\%+ sparsity. However, COO may yield simple indexing logic, as both the data and metadata can be directly extracted.\n\\mysubsubsection{COO-1D} For \\emph{tile-wise processing} of an encoded tensor, accelerators often process only a block of NZs at a time, where block elements vary across only a single dimension. For example, Cnvlutin  processes the input activations and weights across the channel direction.  Therefore, the data block is encoded with COO-1D, which is just like COO, but there is only one $pos$ vector for storing coordinates of NZs in the flattened block. For instance, if we flatten $T$ and consider it a block, then the value '5' is indexed by position '3'. \n\\mysubsubsection{Run-Length Coding (RLC)} It compresses a sequence of values by replacing consecutive duplicate values with a single value and the number of repetitions (aka \\textit{run}). For RLC-encoded sparse tensor, ``run'' indicates a total number of zeros before (after) an NZ. Fig. \\ref{fig::sparse-data-encodings}(d) shows RLC encoding of $T$. Run values for '2' and '3' are '0' and '1', respectively. A few accelerators, including Eyeriss , encode both the NZs and runs altogether in the same vector $val$. For example, $T$ can be encoded as $val$: (0, 2, 1, 3, 0, 5, 4, 7).\nRLC requires a \\emph{step-processing} on metadata, as run-length needs to be calculated by accumulating runs and preceding \\textit{number of NZs (NNZs)}, for determining the position of an NZ. The storage overhead for RLC-B is NNZ $\\times$ B bits, where B is bit-width of the run. If a vector $d$ contains tensor dimensions, then B can be set as up to $\\lceil \\log_2{(\\prod_1^n d_i)} \\rceil$ bits for accommodating the number of leading zeros in a highly sparse tensor. When B is set lower, it cannot always capture the number of zeros as \\textit{run}. Fig. \\ref{fig::sparse-data-encodings}(d) shows RLC-2b encoding, where leading zeros before '7' are four. This cannot be expressed in 2 bits. As a work-around, padding zeros  are inserted and treated as NZs. In this example, a padding zero is inserted between '5' and '7'; run values corresponding to the padding zero and '7' are '3' and '0', which contributes to the total run of four.\nTo accelerate CNNs with 30\\%--90\\% sparsity of tensors, designers have set B as two or four bits. In general, setting the B as $\\lfloor \\log_2{(\\frac{sparsity}{density})} \\rfloor + 1$ bits can effectively compress tensors and provide a feasible bit-width to indicate leading zeros. Here, $sparsity$ and $density$ are fractional numbers indicating the actual or anticipated number of zeros and non-zeros in the tensor, respectively. Thus, setting the B as 1, 1, 1, 2, 4, and 7 efficiently encodes tensors with sparsity of 10\\%, 30\\%, 50\\%, 70\\%, 90\\%, and 99\\%, which is depicted in Fig. \\ref{fig::sparse-data-encodings-overhead}.\nAs RLC requires step-processing on metadata, the indexing logic needs an accumulator to determine the position of an NZ. When an encoded tensor is not processed block-wise but rather indexed by n-dimensions, the indexing logic may require performing division and modulo operations on the metadata. Alternatively, a multi-dimension representation can be used where $run$ for the coordinates of each dimension can be calculated separately and stored. The overall computational cost (arithmetic and logical operations realized in hardware) for such step-processing can be low. Therefore, several accelerator designs, including Eyeriss  and SCNN , used RLC or its variant. As run indicates repetition of a value, CompAct  used an enhanced RLC format for encoding both the sparse and similar-value activations. \n\\mysubsubsection{Bitmap} It stores all NZs in a tensor $val$ along with a tensor \\textit{flag} which contains 1-bit flags for all elements of an uncompressed tensor $T$. As Fig. \\ref{fig::sparse-data-encodings}(e) shows, a flag indicates whether an element is NZ or not. Storage overhead for the bitmap (aka bit-mask) is $\\prod_1^n d_i$ bits (where vector $d$ stores $n$ dimensions of $T$) . Since bitmap stores metadata for all elements, it is effective for compressing the tensors of low or moderate sparsity. Like RLC, decoding or indexing  bitmap also requires step-processing. The indexing logic to locate an NZ typically consists of at least an adder and a comparator . Due to moderate storage overhead and low encoding/decoding cost, several accelerators used bitmap, including Cambricon-X , SparTen , and SIGMA , as shown in Table \\ref{tab:accel-sparse-encodings}.\n\\begin{figure}[t]\n\\centering\n\\centerline{\\includegraphics[width=0.9\\linewidth]{figures/sparse-data-encodings-overhead}}\n\\caption{Storage benefits for encoding a sparse tensor (512$\\times$2048 matrix with 16b elements) in different formats, normalized to the size of the fully dense tensor (Figure inspired by ).}\n\\label{fig::sparse-data-encodings-overhead}\n\\end{figure}\n\\mysubsubsection{Compressed Sparse Row (CSR)} It compresses a matrix by processing each row as a sparse vector. In a CSR-coded tensor, an array $val$ contains all NZ values (ordered row-wise), and an array $idx$ stores their column indices . Array $ptr$ stores information about total NZs in each row $i$, which is obtained by calculating $ptr[i+1]$ - $ptr[i]$. The last element of $ptr$ contains the total number of NZs in $T$. Row-wise compression enables random accesses to any row.\nWhile COO redundantly stores row-coordinates of NZs in the same row, CSR compresses such metadata by storing NZs row-wise . For example, in Fig. \\ref{fig::sparse-data-encodings}(b) (COO), \\textit{coord-y} stores row indices '0' and '0' for NZs '2' and '3'. This redundancy is removed in the CSR coding of Fig. \\ref{fig::sparse-data-encodings}(f), as $ptr$ stores only total NZs in each row. For compressing an M$\\times$N matrix using CSR, the total storage overhead is {NNZ $\\times$ $\\lceil \\log_2{N} \\rceil$} (for $idx$) + {(M + 1) $\\times$ $\\lfloor \\log_2{NNZ} + 1 \\rfloor$} (for $ptr$). Due to high storage overhead (proportional to NNZs and size of the row), CSR coding is efficient at high sparsity , e.g., 90\\% or higher (Fig. \\ref{fig::sparse-data-encodings-overhead}). \nDecoding a CSR-encoded tensor can require a two-step processing of metadata. The first step locates NZs of a row by iterating over $ptr$, and the next step locates an NZ element in the NZs of the row through the column index. Accelerators efficiently process CSR-coded matrices row-wise such that $ptr$ is accessed once for fetching each row, and then the decoder iterates through $idx$ (to locate column positions).\nCSR variants can improve efficiency further. For example, $ptr$ stores duplicate values when consecutive rows are zero. Doubly CSR (DCSR)  eliminates this redundancy and achieves additional compression for hyper-sparse matrices. Block CSR (BCSR)  stores a block of elements in $val$, if the block contains at least one NZ. As Fig. \\ref{fig::sparse-data-encodings}(k) shows, in BCSR, $idx$ indicates the column index of a block, and $ptr$ informs about the number of dense blocks located in the same rows. BCSR avoids storing blocks of all zeros and populates dense regions, and hence suitable for encoding block-sparse structured weight tensors. Thus, BCSR-coded tensors can be efficiently executed not only on conventional processors but also on hardware accelerators (with additional support for appropriately indexing dense regions, e.g., ). \n\\begin{table}[t]\n\\centering\n\\caption{Storage overhead for common encodings. Vector $d$ stores $n$ dimensions of a tensor that contains $NNZ$ non-zero elements.}\n\\label{tab:sparse-encodings-overhead}\n\\begin{tabular}{|c|c|}    \\hline\nFormat & Storage Overhead (bits) \\\\ \\hline\nCOO & $NNZ \\times \\sum_1^n { \\lceil \\log_2{d_i} \\rceil}$ \\\\ \\hline\nCOO-1D & $NNZ \\times \\lceil \\log_2{\\prod_1^n d_i} \\rceil$ \\\\ \\hline\nRLC & $NNZ \\times B$ \\\\ \\hline\nBitmap & $\\prod_1^n d_i$ \\\\ \\hline\nCSR & \\makecell{$NNZ \\times \\lceil \\log_2{d_1} \\rceil$ + $(d_0 + 1) \\times \\lfloor \\log_2{NNZ} + 1 \\rfloor$} \\\\ \\hline\nCSC & \\makecell{$NNZ \\times \\lceil \\log_2{d_0} \\rceil$ + $(d_1 + 1) \\times \\lfloor \\log_2{NNZ} + 1 \\rfloor$} \\\\ \\hline\n\\end{tabular}\n\\end{table}\n\\mysubsubsection{Compressed Sparse Column (CSC)} CSC is similar to CSR, except that NZs are stored column-wise . As Fig. \\ref{fig::sparse-data-encodings}(g) shows, an array $val$ contains NZs (organized column-wise); $idx$ stores their row indices; $ptr$ informs about the total NZs in each column. The storage overhead and hardware costs for encoding/decoding tensors in CSC format are similar to those for CSR. Accelerators, including EIE  and Sticker , processed high-sparsity tensors with CSC format. \nFor alleviating the high storage overhead of CSR or CSC formats due to storing $idx$ and $ptr$ arrays, a few accelerators further encode the metadata $idx$ or $ptr$. For example, EIE  and EyerissV2  encode $idx$ in RLC such that elements in $idx$ indicate zeros between column indices of NZs (similar to $run$ in RLC for NZ values). Fig. \\ref{fig::sparse-data-encodings}(h) shows CSC encoding with such an RLC-encoded row index array. Values '2' and '5' have column index '0' and '1', respectively, which can be encoded as '0' and '0' since there are no leading zeros before NZs '2' and '5'. Similarly, if the first column of $T$ is (0, 2, 0, 0, 5), then the row indices for '2' and '5' can be encoded as '1' and '2'. $ptr$ can also be encoded likewise (store NZs per column instead of a cumulative number). However, encoding positions relatively requires additional step-processing on the metadata. Therefore, decoding a CSR or CSC encoded matrix with RLC-encoded metadata can require triple-step processing on metadata (additional hardware cost).\n\\mysubsubsection{Compressed Sparse Fiber (CSF)} CSF  provides a generalization of CSR for higher-order ($n$-dimensional) tensors by forming a tree (with $n$ levels). Nodes at level $l$ contain indices for $l$th mode (dimension) of an uncompressed tensor $T$. Path from a root to a leaf node encodes different coordinates of an NZ, which are stored in the nodes throughout the path; each leaf node stores an NZ value. So, the height of the tree is the total dimensions of $T$; the width is NNZs in $T$. \nFig. \\ref{fig::sparse-data-encodings}(i) illustrates a mode-0 tree and corresponding arrays of index pointers. Root nodes represent the major mode (0 or $y$), and their child nodes represent the consecutive dimension (1 or $x$). Like in CSR, $ptr$ informs about a group of indices corresponding to a dimension. For instance, $ptr$ array at the beginning informs that one group of three coordinates corresponds to the mode 0 ($idx$ stores coordinates). Similarly, the next $ptr$ array informs about three different groups of coordinates for the next mode (dimension 1). The corresponding $idx$ array stores the coordinates for mode 1, separated into three groups (marked by thick outer vertical borders). \nLayering the arrays of index pointers reduces duplication of indices . Each time when a node directs to children, it eliminates duplicating indices for the corresponding mode. Storage benefits increase with the increase in dimensions and redundancy among coordinates of NZs. The organization of the data also impacts storage efficiency. For example, Fig. \\ref{fig::sparse-data-encodings}(j) shows another ordering, which eliminates storing redundant coordinates of column (mode 1), achieving fewer nodes. For an n-mode CSF tensor, the storage overhead corresponds to more than $NNZ + n - 1$ coordinates and typically much less than $n \\times NNZ$ coordinates. Works  provide further details about managing higher-order tensors with CSF format. Processing metadata at each dimension requires two-step processing (just like processing $ptr$ and $idx$ in CSR), thereby up to 2n-step processing for an n-dimensional tensor. So, accelerator designers may opt for CSF format when processing high-dimensional tensors with high sparsity.\n\\mysubsubsection{Huffman coding} It typically is applied for compressing sparse tensors once they are quantized using precision lowering or value sharing. After quantization, values of the reduced range appear with different frequencies and can be compressed further with Huffman encoding . For example, Deep Compression  pruned and quantized weights of AlexNet  and VGG-16 , achieving 8b/5b indices with a codebook of 256/32 weights for CONV/FC layers. With Huffman encoding, it compressed the models further by 22\\% and 36\\% (total compression of 35$\\times$ and 49$\\times$).\n\\begin{table}[!t]\n\\centering\n\\caption{Commonly Used Sparsity Encodings by Accelerators}\n\\label{tab:accel-sparse-encodings}\n\\begin{tabular}{|c|m{6cm}|}\n\\hline\nCOO &  \\\\ \\hline\nCOO-1D &   \\\\ \\hline\nRLC &   \\\\ \\hline\nBitmap &   \\\\ \\hline\nCSR &  \\\\ \\hline \nCSC &  \\\\ \\hline\nCSF &  \\\\ \\hline\n\\end{tabular}\n\\end{table}\n\\mysubsubsection{Encodings for tensors with structured sparsity}\nDensity-bounded blocks (Fig. \\ref{fig::sparsity-structures}c) can be encoded similarly as blocks with unstructured sparsity, e.g., with bitmap , COO-1D , or RLC. So, for the same sparsity and block size, the overhead is similar to tile-wise processing of a tensor with unstructured sparsity. It is usually low for small block sizes (e.g., 8$\\times$1 , 1$\\times$4 in NVIDIA A100 Tensor Core GPU ), since the position of each NZ is indicated by a few bits. Coarse-grain block-sparse tensors (Fig. \\ref{fig::sparsity-structures}b) can be encoded at block-granularity, which can significantly reduce the metadata size (almost eliminated for dimensional pruning ). Cambricon-S  used bitmap to indicate the presence of each 1$\\times$16 dense block with a single bit. Similarly, ERIDANUS  used few bytes to process each 8$\\times$8 dense block on systolic arrays. Such encodings require indicating the position of a dense block across rows or columns and additional indices for higher dimensions that indicate dense blocks packed per dimension, e.g., in block CSR (Fig. \\ref{fig::sparse-data-encodings}-k).\n\\mysubsubsection{Other formats} Various encoding formats have been proposed, which improve the compression or efficiently access sparse tensors during execution on CPUs/GPUs (for high-performance and scientific computing). It includes compressed sparse blocks (CSB) , libsvm , ELLPACK , diagonal (DIA) , dynamic CSR , delta-coded CSR , and mode-generic and mode-specific formats . Prior works including , SPARSKIT , and  surveyed them along with additional formats and discussed their implications. Different libraries that provide support for encoding the tensors and for sparse tensor computations on CPUs or GPUs include MATLAB tensor toolbox , Intel MKL , SciPy , and cuSPARSE .", "cites": [4357, 514, 7169, 4358, 4335, 4344, 4347, 4354, 7634, 7805, 4359, 4341], "cite_extract_rate": 0.1875, "origin_cites_number": 64, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple encoding formats for sparse tensors and links them to hardware accelerators, drawing from several cited works (e.g., Eyeriss, SCNN, Cnvlutin2). It provides analysis of storage efficiency and indexing logic implications, showing some level of critical comparison (e.g., COO inefficiency at low sparsity). However, while it integrates concepts, it does not propose a novel framework or deeply critique limitations, remaining more focused on describing and comparing formats."}}
{"id": "c38dbc5f-be9d-49a8-8c86-f1dc0a62356c", "title": "Group-wise Encoding", "level": "subsection", "subsections": [], "parent_id": "76637357-2ce7-49d4-924f-7e20b7606e08", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Encodings for Compressing Sparse Tensors"], ["subsection", "Group-wise Encoding"]], "content": "One way of processing sparse tensors is to encode the whole tensor. Then, the accelerator's data management logic extracts an appropriate tile  (optionally decodes it) and communicates to the PEs. In contrast, for group-wise encoding, tensor tiles are encoded separately, based on pre-determined per-PE work. Depending on the mapping, each tile is typically communicated to a unique PE (or a PE-group) during execution. Thus, the encoding considers the dataflow, i.e., mapping of the tensor computations onto PEs. It can make the decoding and data extraction easier, as each group corresponds to execution on a distinct PE (or a PE-group). EIE , Cambricon-X , and CompAct  used group-wise encoding.", "cites": [7634], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of group-wise encoding and mentions systems that have used it, but it does not synthesize insights from the cited papers or place them in a broader context. It lacks critical evaluation of the methods and does not abstract to general principles or compare different approaches."}}
{"id": "88d6dbef-ac33-4b34-b43e-9ce569064200", "title": "On-the-fly Encoding", "level": "subsection", "subsections": [], "parent_id": "76637357-2ce7-49d4-924f-7e20b7606e08", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Encodings for Compressing Sparse Tensors"], ["subsection", "On-the-fly Encoding"]], "content": "Accelerator designers often target only static sparsity of weights and encode them off-line, e.g., DNN inference accelerators, including EIE , Cambricon-X , and . However, on-the-fly encoding is required for efficiently processing dynamically sparsified tensors (sparse activations in the inference and tensors in training the models). Therefore, accelerators, such as CompAct , SCNN , NullHop , Cnvlutin , and Sticker  employ an on-the-fly encoder. Typically, before encoding a tensor, the data is re-organized as per requirements of the group-wise encoding and dataflow mechanism for processing the subsequent layer. So, on-the-fly encoding is often combined with assembling the outputs from PEs (section \\ref{sec::post-process-encoding} provides further details).", "cites": [4354, 7634], "cite_extract_rate": 0.25, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions on-the-fly encoding as a requirement for dynamic sparsity and lists accelerators that use it, but it lacks deeper synthesis or comparison across the cited works. It does not critically evaluate their approaches or limitations, nor does it abstract the concept into broader principles or frameworks."}}
{"id": "2c2b4338-ab49-49e8-95e3-2e1350db0598", "title": "Extraction of Matching Data for Computations on Non-Zeros", "level": "section", "subsections": ["20ccd576-d8ff-4c83-bbfa-db2fd8b3f424", "6c46d7ee-8f0a-4a7a-aac7-d1bb719f64e7", "c3855c4e-a3f8-4153-9abf-839df6cb02ad"], "parent_id": "6220d89f-a486-420a-914e-59957895a313", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Extraction of Matching Data for Computations on Non-Zeros"]], "content": "\\label{sec::NZ-data-extraction}\n\\begin{table}[!t]\n\\centering\n\\caption{Classification of NZ Data Extraction Techniques}\n\\label{tab:data-extraction}\n\\begin{tabular}{|c|c|c|m{3cm}|}\n\\hline\n\\makecell{Target \\\\ Sparsity}    & \\makecell{PE Arch-\\\\itecture}   & \\makecell{Functional Unit \\\\ Operation} & \\makecell{Accelerators} \\\\ \\hline\n\\multirow{3}{*}{\\makecell{One \\\\ Tensor}}          \n& Scalar                    & MAC   &  \\\\ \\cline{2-4}\n& \\multirow{2}{*}{\\makecell{SIMD/\\\\Vector}}   & Sc-Vec-Mul   &  \\\\ \\cline{3-4} \n&                           & Vec-Vec-Mul   &  \\\\ \\hline\n\\multirow{3}{*}{\\makecell{Both \\\\ Tensors}} \n& Scalar                    & MAC   &  \\\\ \\cline{2-4} \n& \\multirow{2}{*}{\\makecell{SIMD/\\\\Vector}}   & Sc-Vec-Mul   &  \\\\ \\cline{3-4} \n&                           & Vec-Vec-Mul   &  \\\\ \\hline\n\\end{tabular}\n\\begin{tabular}{ccc}\n& & \\\\\n\\end{tabular}\n\\begin{tabular}{|c|m{5.6cm}|}\n\\hline\n\\makecell{Location of \\\\ Extraction Units} & \\makecell{Accelerators} \\\\ \\hline\n\\makecell{Centralized/\\\\Shared} &  \\\\ \\hline\nIn-PE       &  \\\\ \\hline\n\\end{tabular}\n\\end{table}\nTensors are typically stored in the compressed format in the accelerator's memory. Therefore, locations of NZs that need to be processed are determined from the metadata. Once a matching pair is extracted (elements of two tensors that need to be added or multiplied), a PE can proceed for computations. Identifying effective NZs is the primary step towards eliminating ineffectual computations due to the sparsity of weights and/or activations. \nThis section describes different data extraction mechanisms (Table \\ref{tab:data-extraction} provides a taxonomy), their management in PEs or centrally, and their trade-offs. Then, it discusses further acceleration opportunities to exploit various sparsity levels.", "cites": [7169, 4335, 4347, 4354, 7634, 7805, 4356], "cite_extract_rate": 0.2916666666666667, "origin_cites_number": 24, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.8}, "insight_level": "low", "analysis": "The section primarily presents a taxonomy of NZ data extraction techniques in a table and briefly describes the importance of identifying effective non-zero elements. While it references several relevant papers, it lacks integration and synthesis of their contributions into a cohesive framework. There is minimal critical evaluation of the cited works, and no abstraction to broader principles or patterns across the surveyed accelerators."}}
{"id": "20ccd576-d8ff-4c83-bbfa-db2fd8b3f424", "title": "Non-Zero Detection and Extraction Mechanisms", "level": "subsection", "subsections": [], "parent_id": "2c2b4338-ab49-49e8-95e3-2e1350db0598", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Extraction of Matching Data for Computations on Non-Zeros"], ["subsection", "Non-Zero Detection and Extraction Mechanisms"]], "content": "\\label{sec::data-extraction-HW}\n\\insight{A data extraction mechanism needs to feed functional units of PEs every cycle.} So, based on their processing of scalars or vectors of NZs, Table \\ref{tab:data-extraction} categorizes extraction mechanisms for (i) MAC operation on scalars, (ii) scalar-vector multiplication, and (iii) vector-vector multiplication.\n\\begin{figure}[!t]\n\\centering\n\\centerline{\\includegraphics[width=\\linewidth]{figures/NZ-detection-1-sparse-tensor-SIMDPE-svm}}\n\\caption{Data extraction in subunits of Cnvlutin PE (Figure adopted from ).}\n\\label{fig::NZ-detection-cnvlutin}\n\\end{figure}\n\\mysubsubsection{Indexing dense tensors by indices of NZs of a sparse tensor}\nDepending on sparsity, only one tensor may be treated as sparse and compressed (e.g., activations for Cnvlutin  or weights for Cambricon-X  and NVIDIA A100 ). So, the position of an NZ can be used for indexing the other (i.e., dense) tensor to extract the corresponding value. \n\\textit{MAC:} Consider the activation lane and filter lane 0 of subunit 0 in Fig. \\ref{fig::NZ-detection-cnvlutin}, which can be visualized as processing on a scalar PE. For an NZ streaming from the activation lane, matching weight can be looked up and provided to the multiplier or MAC unit. For COO-1D encoded blocks, absolute positions of NZs can be obtained directly from metadata. Otherwise, absolute positions of NZs need to be computed explicitly by decoding metadata (e.g., bitmap or RLC) through simple combinational logic consisting of AND gates, multiplexers, and adders (e.g., in   and ). \n\\textit{Sc-Vec Mul:} For SIMD processing, multiple arrays are indexed with the position of an NZ. Fig. \\ref{fig::NZ-detection-cnvlutin} shows such mechanism used in Cnvlutin PEs . Each of 16 subunits in Cnvlutin PE featured an activation lane (streamed an input channel vector), 16 multipliers, and 16 filter lanes. A common NZ activation was fetched from the activation lane, and its position was used for looking up in all 16 filter lanes to obtain corresponding weights for multiplication.  \n\\begin{figure}[!t]\n\\centering\n\\centerline{\\includegraphics[width=\\linewidth]{figures/NZ-detection-1-sparse-tensor-SIMDPE-vvm}}\n\\caption{Data extraction via central indexing module in Cambricon-X  accelerator. The indexing module decodes weights encoded in step-indexed COO-1D format to obtain the absolute positions of NZs. Then, it extracts the activations via a parallel look-up, which are later communicated to a PE via fat-tree NoC for a vector-vector multiplication. (Figure adopted from .)}\n\\label{fig::NZ-detection-cambriconX}\n\\end{figure}\n\\textit{Vec-Vec Mul:} PEs of some accelerators spatially process vectors at every cycle (e.g., with 16 multipliers and an adder tree in Cambricon-X). As Fig. \\ref{fig::NZ-detection-cambriconX} illustrates, based on positions of NZs of a vector, a combinational logic with multiplexers can select matching data elements to feed the arithmetic units (e.g., in ). \\insight{An associated challenge is overheads of parallel look-up}. To exploit high sparsity, larger multiplexers need to be used for indexing the dense tensor, as positions of scattered NZs are likely distant. With the search length set as 256 (supports 93.75\\% sparsity for fetching 16 NZ elements), a central indexing module in Cambricon-X occupied about 31\\% and 35\\% of total on-chip area and power, respectively (exceeded total power of all 16 PEs) . \n\\mysubsubsection{Compare metadata of sparse tensors for extracting matching pairs of NZs}\nFor effectual computations over multiple compressed tensors, the extraction logic determines pairs of NZs (intersections) by comparing indices either from metadata streams or in multi-stage indexing.\n\\textit{MAC:} Circuitry for extracting NZ scalars can consist of one or more comparators (or AND gates for comparing bitmaps) and an additional indexing logic (e.g., in ZENA  and SparTen ). The comparators match positions of NZs, and the indexing logic uses their outputs to extract the leading pair. Due to the diverse sparsity of tensors, positions of NZs may not match during comparison. Therefore, the detection logic uses several comparators to search within a large window, which usually can provide at least one pair at every cycle. Priority encoders provide the leading $n$-pairs for feeding $n$ computational units (n=1 for scalar PEs). The data extraction unit can use skip mechanisms (e.g., in ExTensor ) to quickly navigate through the lanes. \nAlternatively, multi-stage indexing logic is used for extracting the pair. The first stage obtains a position of an NZ from one tensor for indexing another tensor. Later stage checks if there is a corresponding NZ in another tensor and extracts it upon matching the positions. For example, in EIE , each PE loads an NZ activation from a queue; when it does not have any matching weights, it fetches the next activation from the queue in the next cycle. Depending on the sparsity level and pattern, the indexing-based design occasionally may not find the matching data, wasting the execution cycles, i.e., functional units in the pipeline are not utilized.  \n\\textit{Sc-Vec Mul:} PEs in EyerissV2  use multi-stage extraction. Each SIMD PE fetches an CSC-coded activation and its position, and checks positions of NZ weights. Upon match, it forwards the activation and weights to two MAC units. \n\\begin{figure}[!t]\n\\centering\n\\centerline{\\includegraphics[width=\\linewidth]{figures/NZ-detection-2-sparse-tensor-SIMDPE-vvm-SNAP}}\n\\caption{Associative index matching in SNAP (Figure adopted from ).}\n\\label{fig::NZ-detection-SNAP}\n\\end{figure}\n\\textit{Vec-Vec Mul:} The data extraction logic to feed multiple arithmetic units of a vector PE requires multiple comparators followed by priority encoders or multiplexers. For example, in SNAP architecture , an associate index matching module (AIM, Fig. \\ref{fig::NZ-detection-SNAP}) determines the positions of NZs in case of valid matches. Each PE of a row is interfaced with a shared AIM. Using comparison outcomes from AIM, a sequencer in each PE determines leading pairs of matching data, which are then fed to three multipliers within the PE. Cambricon-S  uses similar extraction logic, but its comparator array is just ANDing of the bits due to bitmap encoding.    \n\\mysubsubsection{Eliminating extraction of intersecting NZs} Some accelerators do not require extracting unstructured NZs.   \n\\begin{figure}[b]\n\\centering\n\\centerline{\\includegraphics[width=\\linewidth]{figures/orchestration-structured-sparsity}}\n\\caption{Computation of locally dense regions in ERIDANUS (Figure adopted from ). (a) Matrix multiplication with block-sparse weights. (b) Sub-matrices for processing on a 2$\\times$2 systolic array. (c) Multiplication of streaming blocks (NZs) with stationary data.}\n\\label{fig::orchestration-structured-sparsity}\n\\end{figure}\n\\textbf{Orchestrating structured computations:} A few techniques targeted high sparsity of single tensor (DNN weights). With data pruning or transformations, they achieved coarse-grain sparsity so that each PE can process a dense region of NZs. ERIDANUS  proposed a pruning algorithm to cluster the weights (Fig. \\ref{fig::orchestration-structured-sparsity}a). Blocks of NZ weights are streamed to PEs of systolic arrays for conventional processing (Fig. \\ref{fig::orchestration-structured-sparsity}c). Corresponding activations are kept stationary. Partial products computed by each row of PEs are added on a separate adder tree. When block width for structured pruning can be set as the height/width of the systolic array, dot products can be accumulated linearly over the systolic array itself. Thus, \\insight{structured sparsity allows executing denser blocks conventionally on accelerators, while requiring additional support to index and communicate the blocks.} Adaptive tiling  used a column-combining approach. For a sparse GEMM, NZ weights were statically combined such that each column of the systolic array could process multiple columns of input activations. Thus, it obviated the run-time data extraction and reduced total invocations of the systolic array by 2$\\times$--3$\\times$ for processing point-wise CONVs of MobileNet. \nCirCNN  and C-LSTM  proposed executing DNN operators as FFT (Fast Fourier Transform) on smaller block-circulant matrices.\n\\textbf{Coordinate computation unit:} SCNN  and SqueezeFlow  perform unit-strided convolutions as a Cartesian product where all elements of two blocks of tensors should be multiplied together. Due to all-to-all multiplication, no special support is required for extracting matching pairs of NZs. However, index computation is still required to determine which partial-sums should be accumulated with partial products. This calculation is performed in a ``coordinate computation unit'' that processes metadata (indices of NZs) and determines indices of outputs. These approaches require conflict detection in hardware since it can't be pre-determined which accumulators would be accessed in any cycle. Since coordinate computation unit facilitates direct processing on compressed tensors, it may also be used for computing block-level indices for processing a \\emph{coarse-grain block-sparse} tensor.", "cites": [4354, 7634, 4361, 4360, 4335, 7169], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key data extraction mechanisms from multiple cited works, such as Cnvlutin, Cambricon-X, EIE, EyerissV2, and SNAP, by categorizing them into distinct types of operations (MAC, scalar-vector, vector-vector). It also offers critical insights, such as the overheads of parallel look-up and the inefficiencies of indexing-based designs when matches are not found. Furthermore, it abstracts these mechanisms into broader design principles, such as the trade-offs between structured and unstructured sparsity and the impact of metadata encoding on performance and area/power costs."}}
{"id": "6c46d7ee-8f0a-4a7a-aac7-d1bb719f64e7", "title": "Centralized vs. Distributed Management", "level": "subsection", "subsections": [], "parent_id": "2c2b4338-ab49-49e8-95e3-2e1350db0598", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Extraction of Matching Data for Computations on Non-Zeros"], ["subsection", "Centralized vs. Distributed Management"]], "content": "\\mysubsubsection{Centralized} The data extraction unit can be either centralized (and shared among PEs) or within pipelines of PEs. Advantages of central mechanisms are: (i) PEs can be directly provided effective NZs for useful computations . It can also be used as a pre-processing unit for a PE-array that processes structured computations, e.g., systolic arrays or near-data accelerators. (ii) Centralized extraction in some architectures (e.g., Cambricon-X ) duplicates hardware for concurrent extractions for PEs. However, the module can be time-shared by multiple PEs (e.g., in SNAP ), which can reduce area and power. In fact, by leveraging structured $W$-sparsity, the module in Cambricon-S \\emph{shares} extracted indices among \\emph{all} PEs. (iii) Centralized logic extracts work for multiple PEs, and often it is coupled with a controller that allocates data to PEs. So, it can enable \\emph{run-time load balancing}. However, a major challenge is to maintain spatial data reuse. This is because, the centralized unit mostly extracts data on a per-PE basis for communication to a unique PE. So, the common data for multiple PEs cannot be multi-cast. SNAP overcomes this limitation by sharing a module with a row of PEs and multicasting data to PEs. The multicast occurs first, followed by PEs communicating their metadata to the extraction unit. Then, extracted indices are streamed back to a PE, which uses them to obtain data from its local RF for computations.\n\\mysubsubsection{In-PE} PEs of several accelerators, such as Cnvlutin , ZENA , and EyerissV2 , extract appropriate data.It allows a controller to multicast or broadcast tensor elements for spatial reuse. Then, in-PE logic extracts the data. However, challenges are: (i) in-PE logic may incur ineffectual cycles for extraction that cannot be hidden. (ii) employing inter-PE load-balancing in the hardware may be infeasible or costlier, as the actual work carried out by different PEs is unknown while offloading compressed tensors to PEs (until extraction in PE datapath).  \n\\begin{figure*}[!t]\n\\centering\n\\centerline{\\includegraphics[width=\\linewidth]{figures/analysis-data-reuse}}\n\\caption{Data reuse opportunities for executing different CNN layers (dense tensors) on hardware accelerators (Figure inspired by ).} \n\\label{fig::analysis-data-reuse}\n\\end{figure*}", "cites": [4335], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes information from multiple hardware accelerator designs, connecting centralized and in-PE data extraction mechanisms with their respective trade-offs. It provides critical analysis by highlighting the challenges and limitations of each approach, such as spatial reuse in centralized units and load-balancing in in-PE extraction. The discussion abstracts beyond individual papers to identify general principles like the impact of sparsity structures on design choices and communication patterns."}}
{"id": "0c7ea9d1-6b8d-48fe-91f9-fa72adc10e69", "title": "Memory Management of Compressed Tensors", "level": "section", "subsections": ["ecce2096-4015-48ff-97f9-355f9c2a3463", "8f654738-ca19-44ff-ae76-2a74f2ebe86c", "0afeddc8-8d83-41ab-9493-366288a6e53d", "5dbe3587-d8d2-4242-b7d0-700e4d7a42ae", "ccc7ce5f-8a10-49d7-adaf-8358c7cd8a5c", "b1a6e5c9-14b7-4609-a56e-1a10c9f815be"], "parent_id": "6220d89f-a486-420a-914e-59957895a313", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Memory Management of Compressed Tensors"]], "content": "\\label{sec::memory-management}\nAccelerators contain \\emph{multi-banked} scratchpads, which are usually shared among PEs. Either a scratchpad is \\emph{unified} , or separate buffers store different tensors . Their sizes vary from several tens of KBs  to several MBs . Effective management of shared and local memory highly reuses data and hides memory access latency behind computations on PEs. This section discusses how sparsity and reduced shapes of tensors lower reuse. \nHowever, compressed tensors help to achieve better speedups and energy efficiency, as more data fits in on-chip memory, reducing off-chip accesses. This section also describes how irregular accesses (e.g., arbitrating output activations) make management of the banks challenging. Then, it discusses reusing intermediate outputs via fused-layer executions and how sparsity affects it. \n\\begin{figure*}[!t]\n\\centering\n\\centerline{\\includegraphics[width=\\linewidth]{figures/analysis-data-reuse-sparse}}\n\\caption{Impact of sparsity on data reuse opportunities for accelerating CNNs and NLP models.}\n\\label{fig::analysis-data-reuse-sparse}\n\\end{figure*}", "cites": [7005, 4335], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes concepts from the cited papers by discussing how sparsity and irregular tensor shapes affect memory reuse and access patterns. It also introduces analytical insights by examining the implications of compression on speedup and energy efficiency. However, it lacks deeper critical evaluation or a comprehensive comparative framework, and while it begins to generalize, it does not yet reach a meta-level abstraction."}}
{"id": "ecce2096-4015-48ff-97f9-355f9c2a3463", "title": "Leveraging Data Reuse Opportunities", "level": "subsection", "subsections": [], "parent_id": "0c7ea9d1-6b8d-48fe-91f9-fa72adc10e69", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Memory Management of Compressed Tensors"], ["subsection", "Leveraging Data Reuse Opportunities"]], "content": "\\label{sec::data-reuse}\n\\mysubsubsection{Reuse characteristics} Depending on the functionality of layers, there can be significant reuse of tensors. Figures \\ref{fig::analysis-data-reuse} and \\ref{fig::analysis-data-reuse-sparse} depict reuse opportunities for different layers (early CONV layers, later CONV layers, MLPs, DW-CONVs, PW-CONVs, expand or reduce layers, attention mechanism). For each tensor, the data reuse is calculated as the total number of MACs per data element. For better visualization, reuse factors and layers are plotted on a logarithmic scale. \n\\textit{Input activations:} Reuse of input activations increases with going deeper in CNNs since the number of filters increases significantly. It is also high for 'expansion' layers in bottleneck blocks (Fig. \\ref{fig::analysis-data-reuse-sparse}). DW-CONVs are an exception and present very low reuse as there is only one filter. 'Squeeze' or 'reduce' layers present moderate reuse for dense tensors. Reuse in FC layers or MLPs (e.g., in encoder/decoder layers of Transformers ) depends on the sizes of weight matrices (i.e., sizes of output tensors). \n\\textit{Weights:} Since 2D feature maps in CNNs are usually much larger than 2D weights, weight reuse can be higher by an order of magnitude. With going deeper in CNNs, feature maps shrinks spatially, which lowers the reuse. There is no weight reuse for MLPs, but increasing the batch size linearly improves the weight reuse. Video processing applications use 3D CNNs (e.g., c3d ), which can further increase the reuse opportunities  for input activations and weights due to additional processing steps on consecutive frames. For NLP models such as Transformer  and BERT , Fig. \\ref{fig::analysis-data-reuse-sparse} illustrates weight reuse for executing a sequence of 24 and 107 tokens, respectively. $MatMul$s in the attention-based calculation are shown for a single head.\n\\textit{Partial summations:} Input channels are increased as we go deeper into CNNs. Similarly, 'reduction' layers in bottleneck blocks involve more input channels. Both improve the reuse of partial summations. MLPs also usually provide high reuse due to larger input vectors. DW-CONVs show very low reuse because partial summations are not accumulated across input channels. \n\\mysubsubsection{Impact of sparsity on reuse} Increase in sparsity can lead to lower reuse. To determine the impact of sparsity, we considered evaluations by Han et al.  for pruned AlexNet and VGG-16 models. For recent DNNs like MobileNetV2 or BERT models, we considered sparse models as listed in Table \\ref{tab:sparsity-dnn-models}. Then, we calculated the reuse as NZ MACs per NZ of a tensor. Fig. \\ref{fig::analysis-data-reuse-sparse} plots the reuse opportunities for both dense and sparse tensors of CNNs and NLP models. Since execution in encoder/decoder modules of NLP models is repetitive, unique layers of a single module are only shown (sparsity averaged across all encoder/decoder modules). \nThe figure shows that for sparse models, reuse characteristics are preserved, but the reuse factor decreases for almost all layers and tensors, as compared to processing dense tensors. Primarily, this is due to the reduced number of effectual MACs. For example, for MLPs without batching, weight reuse can drop below one. It means that even if a weight matrix consists of NZs, some of them are never used due to the unavailability of matching NZs in input activations. As an exception, reuse of weights remains the same, when activation sparsity is absent (e.g., EfficientNetB0 , BERT ). Similarly, with dense weights, low or moderate reuse of activations remains the same for DW-CONV or 'excite' layers, respectively.\nThe reuse of partial summations also decreases since effectual MACs per partial summation decrease with sparsity. Note that each output activation element still needs to be populated or assembled before ReLU/encoding. Due to sparsity and fewer input channels, the reuse is low or moderate in 'expansion' layers. Similarly, small matrices in processing individual attention heads exhibit low reuse. The reuse remains high for 'reduce' layers in CNNs or query and value processing and FC layers in NLP models. To sum up, although sparsity reduces the reuse of tensors, there can be high data reuse for many layers (up to $1E+04$), which should be exploited for efficient accelerations.\n\\mysubsubsection{Temporally reusing data through shared on-chip memory} Like CPUs, accelerators have memory hierarchies because applications have different working set sizes. Data reuse can be leveraged \\emph{temporally} (repeatedly accessing data from memory without accessing lower-level memory) and \\emph{spatially} (providing the same data to multiple PEs without repeatedly accessing memory). After exploiting high temporal reuse, the highest energy is spent in upper buffers .", "cites": [784, 4362, 7633, 7, 38], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple cited works to explain data reuse characteristics in different ML layers and under sparsity. It abstracts broader trends in reuse behavior for CNNs, NLP models, and specialized layers like MLPs and DW-CONVs. The critical evaluation is present but less deep, particularly in its focus on reuse limitations due to sparsity and implications for acceleration efficiency."}}
{"id": "8f654738-ca19-44ff-ae76-2a74f2ebe86c", "title": "Hiding Miss Latency Behind Computations", "level": "subsection", "subsections": [], "parent_id": "0c7ea9d1-6b8d-48fe-91f9-fa72adc10e69", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Memory Management of Compressed Tensors"], ["subsection", "Hiding Miss Latency Behind Computations"]], "content": "\\mysubsubsection{Management of tiled data in double-buffered memory} On-chip buffers are typically not large enough to accommodate all tensors. Therefore, loops are tiled for reusing some tensors from buffers, while repeatedly accessing other tensors from the off-chip memory . Since scratchpads are non-coherent and their management is software-directed, data is transferred by direct memory accesses (DMA) . PEs are kept engaged in useful computations by interleaving computations with memory accesses. Such an objective is usually achieved by double-buffering (aka ping-pong buffers) . Loop optimization techniques, like loop tiling and ordering, can determine the sizes of tensor blocks to be managed in memories and sequence of memory accesses for high reuse and reduced data transfers .\n\\mysubsubsection{Asynchronous communication} Some accelerators hide the latency of communicating data to the shared/local memory with an asynchronous mechanism that refills the memory after some data has been consumed (e.g., in Cambricon-X ). For such execution, PEs and a DMA controller may simultaneously produce/consume data either through different banks or at the granularity of small blocks in the same bank. Similarly, when accessing shared memories via configurable communication networks , PEs can execute in a dataflow fashion and request partial refilling of their memory with new data. Such \\insight{mechanisms for asynchronous communication and computations can alleviate work imbalance among PEs} that is caused by leveraging unstructured sparsity.\n\\mysubsubsection{Impact of sparsity on the latency of memory accesses and speedup} For memory-bounded execution (e.g., MLPs), even with effective prefetching, miss penalty may be significant. It restricts accelerators from achieving peak performance . When tensors are sparse, the amount of data that needs to be transferred from off-chip reduces significantly, leading to substantial performance gains. For example, Cambricon-S reported up to 59.6$\\times$ speedup of FC layers for hyper-sparse weights. However, higher $IA$-sparsity did not provide such gains (speedup saturated at about 14$\\times$) since the latency of accessing weights dominated total execution time. For processing high sparsity (e.g., 90\\%+) and low reuse, it becomes challenging to engage functional units into effectual computations. This is because, with \\emph{low} arithmetic intensity, required data \\emph{may not be prefetched} at available bandwidth.", "cites": [7005, 4354, 4335], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key concepts from multiple papers to present a coherent narrative on how latency from memory accesses can be hidden in sparse tensor computations. It critically evaluates the effectiveness of sparsity in reducing miss penalties and explains limitations, such as the diminishing returns with higher IA-sparsity. The section also abstracts the problem beyond individual systems to highlight general principles like arithmetic intensity and the challenge of low reuse in high sparsity scenarios."}}
{"id": "0afeddc8-8d83-41ab-9493-366288a6e53d", "title": "Management of Multi-Bank Memory", "level": "subsection", "subsections": [], "parent_id": "0c7ea9d1-6b8d-48fe-91f9-fa72adc10e69", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Memory Management of Compressed Tensors"], ["subsection", "Management of Multi-Bank Memory"]], "content": "\\label{sec::memory-bank-management}\n\\mysubsubsection{Concurrent accesses to memory banks} While single-bank memory can be easier to manage, it is infeasible to provide multiple ports for the PE-array with just one bank . Moreover, multi-port unified memory consumes very high power and longer latency . \nSo, on-chip memories are partitioned into smaller banks . For mapping a layer onto the accelerator, each bank is usually allocated to only one tensor (e.g., in EyerissV2 ). Banked buffers provide multiple read and write ports, allowing simultaneous accesses to different tensors stored in different banks . \nSometimes, a data layout reorganization is required before loading into memory banks. Such transformation is done after loading it from DRAM or before writing outputs to DRAM, which consumes additional execution time and energy. For compressed tensors, such transformation can be done along with the data encoding  at alleviated overheads.\n\\mysubsubsection{Arbitration and conflict management} Depending on the indexing logic and interconnect between memory and PEs, managing application data may require additional compilation support or hardware logic for data arbitration and conflict management . For regular memory accesses (e.g., dense or block-sparse data), allocation and accesses to banks can be determined for mappings of layers. However, computations on unstructured sparse data can lead to accessing \\emph{arbitrary} banks and require special support. E.g., outputs from PEs may need to be written to different banks. Moreover, accelerators contain accumulator-buffers , where PEs or their functional units are connected with memory banks via a crossbar. The crossbar arbitrates write-back of outputs to the appropriate bank . Since these partial outcomes can correspond to non-contiguous elements in an output tensor, bank conflicts are possible during arbitration, i.e., multiple outputs need to be simultaneously handled by the same bank . To obviate conflicts, the buffer contains more banks (e.g., 2$\\times$N banks for storing outputs from $N$ sources in SCNN ). It alleviates collisions in hashing irregular outputs into different memory banks. Consequently, the crossbar may require higher bandwidth and significant on-chip area (e.g., 21\\% for a 16$\\times$32 crossbar in each SCNN's PE).", "cites": [4354, 4335, 8769], "cite_extract_rate": 0.3, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from multiple papers (e.g., EyerissV2 and SCNN) to explain how multi-bank memory systems are used for efficient tensor processing. It provides some abstraction by discussing common design patterns such as data layout reorganization and crossbar arbitration. While it includes some critical points, such as the overhead of data transformations and the area cost of crossbars, the analysis remains somewhat focused on describing techniques rather than offering a deeper evaluative or comparative perspective."}}
{"id": "5dbe3587-d8d2-4242-b7d0-700e4d7a42ae", "title": "Reusing Intermediate Tensors", "level": "subsection", "subsections": [], "parent_id": "0c7ea9d1-6b8d-48fe-91f9-fa72adc10e69", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Memory Management of Compressed Tensors"], ["subsection", "Reusing Intermediate Tensors"]], "content": "\\mysubsubsection{Reusing intermediate tensors from large on-chip memory} Intermediate feature map in DNNs is an output of a layer that serves as input to later layers. It can be kept stationary and reused from on-chip memory to reduce off-chip traffic. Such reuse is amplified when input is the same for multiple layers due to residual connections  or high cardinality (e.g., ResNeXt ). Leveraging it can be important for latency-bounded real-time applications. \\insight{Sparsity-encoding and quantization significantly makes such reuse opportunities more feasible} due to reduced storage requirements. Accelerators with large memories (hundreds of KBs) such as SCNN  and Cnvlutin , can leverage such reuse. \n\\mysubsubsection{Overcoming static bank assignment} Many accelerators process models layer-by-layer and do not leverage cross-layer reuse, i.e., write outputs for layer $L$ in DRAM and load them back later as inputs for layer $L+1$. It is more prevalent among accelerators with small memories. Moreover, bank assignment for each tensor is often fixed at design time , which enforces write-back of outputs and reloading them later in other banks as inputs while processing next layers. Thus, in both cases, output activations are not reused on-chip, causing excessive off-chip memory traffic. To address this problem and exploit cross-layer reuse, shortcut-mining  used a flexible architecture with decoupled physical-logical buffers. \nFor pre-known sparsity, prior techniques for statically determining the data allocation to memory banks may work well by estimating sizes of encoded tensors. However, for dynamic sparsity, conservative estimations may lead to inefficient utilization of banks, and efficient banking for non-conflicting accesses can also be challenging. \n\\begin{figure}[!t]\n\\centering\n\\centerline{\\includegraphics[width=\\linewidth]{figures/fused-layer-CNNs}}\n\\caption{Fusing the execution of layers can significantly reuse intermediate activations  (Figure adopted from ).}\n\\label{fig::fused-layer-CNNs}\n\\end{figure}\n\\mysubsubsection{Fused-layer execution} Fused-layer CNNs  leveraged cross-layer reuse by processing a small tile of activations such that outputs for few layers can be computed alongside while retaining the corresponding data in the on-chip memory. Fig. \\ref{fig::fused-layer-CNNs} shows an example for processing an input tile of 5$\\times$5 activations (C\\_L input channels) for layer $L$ and finally obtaining 1$\\times$1 output activations (M\\_L+1 output channels) for layer $L+1$. Apart from reusing intermediate outputs for obtaining the output tile, corresponding tiles of intermediate activations and filters are maintained in the memory and reused partially for processing the next tiles (striding execution in the spatial direction). Alwani et al.  reported reducing off-chip transfers of input feature maps by 28\\% for the first two layers of AlexNet and by 95\\% for  the first five layers of VGG-19. Since the cascading by storing all the filters and input channels (dense tensors) requires high memory,  applied it to only early layers. However, encoded sparse tensors and further tiling across filters/channels allow fitting tensors for multiple layers in the small memory, making such reuse opportunities more feasible. The tile size and number of layers that can be fused are bounded by memory capacity. So, fusion parameters depend on the actual/anticipated sparsity levels. For efficient executions, fusion parameters need to be explored systematically with sparsity-aware dataflows.", "cites": [502, 4354, 97], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes insights from multiple papers to discuss intermediate tensor reuse strategies in accelerators, particularly in the context of sparsity and memory constraints. It critically evaluates limitations of static bank assignment and explores the implications of sparsity encoding and quantization. The section abstracts the concept of cross-layer reuse into a broader architectural and algorithmic design challenge, applicable to various DNNs and accelerators."}}
{"id": "ccc7ce5f-8a10-49d7-adaf-8358c7cd8a5c", "title": "Techniques for Further Energy-Efficiency", "level": "subsection", "subsections": [], "parent_id": "0c7ea9d1-6b8d-48fe-91f9-fa72adc10e69", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Memory Management of Compressed Tensors"], ["subsection", "Techniques for Further Energy-Efficiency"]], "content": "\\mysubsubsection{Look-ahead snoozing} Depending on the sparsity, encoding of tensors, and mapping of the layer, several banks can be unused or inactive for certain time intervals. Accelerators achieve further energy efficiency by power gating unused or inactive banks. For example, look-ahead snoozing in CompAct  targeted reducing the leakage power of large on-chip SRAMs. Each bank of its activation SRAM can be power-gated. Banks unutilized during the execution of a layer were put in the deep sleep mode (maximal savings in leakage power, while not preserving any data in unused banks). Further, the period of active cycles for each bank was determined based on the data movement schedule. Then, inactive banks were snoozed during execution (i.e., connecting to the data retention voltage for consuming lower leakage power). \n\\mysubsubsection{Skipping memory hierarchy} Some layers do not provide significant reuse. Data reuse is also lowered due to sparsity and architectural choice for extracting or communicating NZs. Therefore, a few accelerators (e.g., EIE  and Cambricon-X ) obviate storing non-reusable data in the shared memory and directly feed it to appropriate PEs (weights for MLPs).", "cites": [7634], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to explain general energy-efficiency strategies like look-ahead snoozing and skipping the memory hierarchy. It connects these techniques to the broader challenges of sparsity and data reuse. However, the critical analysis is limited to stating what the accelerators do, without evaluating trade-offs or limitations. The abstraction level identifies a few recurring patterns but does not provide a meta-level framework or overarching principles."}}
{"id": "b1a6e5c9-14b7-4609-a56e-1a10c9f815be", "title": "Optimization Opportunities", "level": "subsection", "subsections": [], "parent_id": "0c7ea9d1-6b8d-48fe-91f9-fa72adc10e69", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Memory Management of Compressed Tensors"], ["subsection", "Optimization Opportunities"]], "content": "\\textit{(i) Managing both data and metadata in unified memory:} Accelerators often contain separate buffers for metadata (positions of NZs, indirection tables for shared values). Although such designs are easy to manage for processing tensors of some models encoded in a specific format, they may not work well across different levels of sparsity and value similarity, as storage requirements vary significantly. \nSo, designers can explore unified memory architectures for managing both data and metadata (including memory partitioning and bank management) and their trade-offs. It can also be leveraged to tailor efficient designs for programming FPGAs.\n\\begin{figure}[!t]\n\\centering\n\\centerline{\\includegraphics[width=\\linewidth]{figures/common-NoC-designs}}\n\\caption{Common NoC designs (Figure adopted from ).}\n\\label{fig::NoC-designs}\n\\end{figure}", "cites": [4335], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section introduces a general concept of unified memory for managing compressed tensors and references one paper to support its discussion. While it highlights a design consideration, it does not deeply synthesize or compare multiple works. The analysis is somewhat evaluative but lacks nuanced critique or comprehensive comparisons. The mention of broader implications for FPGA programming indicates a modest level of abstraction."}}
{"id": "0ed84c84-7c0f-440f-a66c-b4d0e03b9b4c", "title": "Mechanisms for Distribution of Operands", "level": "subsection", "subsections": [], "parent_id": "4b82dfad-305c-4c60-9b38-8189a27dd3b9", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Interconnects for Distributing Non-Zeros and Reducing Partial Outputs"], ["subsection", "Mechanisms for Distribution of Operands"]], "content": "\\begin{table}[!t]\n\\centering\n\\caption{NoC Designs for Distribution of Sparse Tensors}\n\\label{tab:NoC-data-distribution}\n\\addtolength{\\tabcolsep}{-3pt}\n\\begin{tabular}{|c|c|m{5.7cm}|}\n\\hline\n\\multirow{5}{*}{\\makecell{Topo-\\\\logy}}      \n    & Unicast &  \\\\ \\cline{2-3} \n    & Multicast         &  \\\\ \\cline{2-3} \n    & Broadcast         &  \\\\ \\cline{2-3} \n    & Mesh              &  \\\\ \\cline{2-3}\n    & Configurable      &  \\\\ \\hline\n\\multirow{2}{*}{\\makecell{Spatial \\\\ Reuse}} \n    & Activations       &  \\\\ \\cline{2-3} \n    & Weights           & \\\\ \\hline \n\\end{tabular}\n\\addtolength{\\tabcolsep}{3pt}\n\\end{table}\nFig. \\ref{fig::NoC-designs} shows some common NoC designs for distributing the operands, their bandwidth, and achievable spatial reuse . Data can be reused spatially by distributing it to multiple PEs or functional units. For layers with high reuse opportunities (Fig. \\ref{fig::analysis-data-reuse}), it lowers communication and helps to hide the communication latency. Most accelerators leverage spatial reuse with multicast or broadcast NoC. They consist of configurable buses or trees that multicast the data to PEs (often in a single cycle) . In contrast, the mesh interconnect (e.g., in systolic arrays  ) or 1D buses communicate the data and reuse spatially with a store-and-forward mechanism. Low-reuse tensors are distributed with unicast NoCs. Table \\ref{tab:NoC-data-distribution} lists common interconnect topologies used by previous accelerators for data distribution.\nCommunication requirements vary significantly depending on the sparsity of tensors, available reuse, and adopted dataflow mechanism. Prior work  provides a detailed analysis of different NoC topologies and  characterizes the NoC bandwidth required for different dataflows. Similarly, analytical tools including  model implications of different dataflows on communication requirements and execution time. \n\\mysubsubsection{Broadcast} Accelerators, including Cnvlutin , EIE , and Cambricon-S , use broadcast NoC to reuse activations for processing CONVs or MLPs. Similarly, in SCNN , weights are broadcast to PEs for executing unit-strided convolutions with input stationary dataflow. For sparsity-encoded tensors, their NZs (and positions) can be broadcast for spatial reuse, as long as the NZs are indexed or extracted afterward (e.g., in-PE extraction in Cnvlutin and EIE). In Cambricon-S, positions of intersecting NZs are extracted centrally before broadcast, but due to structured sparsity, the same extracted positions are used by all PEs. So, NZ activations are broadcast to all PEs. \n\\mysubsubsection{Multicast} Eyeriss , ZENA , and SNAP  use multicast NoC to reuse multiple operands spatially. For example, Eyeriss processed tensors with row-stationary dataflow where PEs of a row processed the same spatial rows of filters, and diagonal PEs processed the same spatial row of feature maps. Eyeriss facilitated such multicasting through its configurable NoCs, which consisted of row-wise and column-wise controllers for 2D PE-array. Each controller could be configured with a pre-determined tag value, which was compared with the row or column tag of a packet. Upon matching the tags, a row-wise controller forwarded the packet to associated column-wise controllers, and a column-wise controller forwarded it to the associated PE. Similarly, for processing bitmap-coded tensors in ZENA , a block of activations was broadcast to a row of PEs, and a block of weights was multicast to PEs of the same column. \n\\mysubsubsection{Mesh} A few accelerators, including Compact , ERIDANUS , and , use systolic arrays with mesh interconnects. Since the same data is forwarded among PEs in the same row or column, such NoCs achieve the same amount of spatial reuse as multicast NoCs. But, for sparse tensors, efficient and correct processing becomes challenging. Hence, \\emph{pre-processing} is needed to cluster appropriate NZs or index appropriate block of structured-sparse tensor before feeding PEs of the systolic array .   \n\\mysubsubsection{Unicast} SCNN , Cambricon-X , and SqueezeFlow  use unicast NoC or point-to-point links. Such NoCs concurrently feed different elements to various PEs. They are used when spatial reuse of a tensor is infeasible (e.g., weights in MLPs, NZs are extracted beforehand, due to dataflow requirements) or outputs are collected simultaneously (section \\ref{sec::write-back}). \nWith high bandwidth, they reduce communication latency , but can incur high area and power. \n\\mysubsubsection{Configurable}\n\\insight{Communication requirements vary with different dataflows} that are effective for only some DNN layers (section \\ref{sec::dataflows} and Table \\ref{fig::layer-characteristics}). Further, while communication may consist of gather, scatter, forward, or reduction patterns , efficient execution may demand their combination or even non-uniform patterns including multi-hop communications among PEs . Therefore, configurable NoC designs are required, which can support various communication patterns that are amenable to different reuse and sparsity. Recent designs including EyerissV2 , microswitch-NoC , and SIGMA  address some of these challenges.\n\\begin{figure}[!t]\n\\centering\n\\centerline{\\includegraphics[width=\\linewidth]{figures/architecture-EyerissV2}}\n\\caption{EyerissV2 accelerator architecture  (Figure adopted from ).}\n\\label{fig::architecture-EyerissV2}\n\\end{figure}\nEyerissV2  uses a novel \\emph{hierarchical-mesh} NoC, which is illustrated in Fig. \\ref{fig::architecture-EyerissV2}. EyerissV2 contains 16 clusters (8$\\times$2 array) of PEs and global buffers (GLBs). Each PE-cluster contains 3$\\times$4 PEs, and each 12 kB GLB-cluster contains seven banks for input and output activations. At the top level, router clusters are connected through a 2D mesh, and they enable communication among different PE-clusters and GLB-clusters. For local communication among each PE-cluster and GLB-cluster, a router-cluster with ten routers is used. Each router connects PEs with a port of the GLB cluster for accessing GLB bank or off-chip memory (three, three, and four routers for managing input activations, weights, partial summations). Locally, an all-to-all NoC connects all PEs of a PE-cluster to the routers for each data type. As Fig. \\ref{fig::NoC-Hierarchical-Mesh-EyerissV2}(a)--(d) illustrates, it facilitates multiple communication patterns including multicast, broadcast, and unicast of the tensors. 2D mesh topology enables inter-cluster communications, allowing an interleaved-multicast or broadcast to all clusters.\n\\begin{figure}[!t]\n\\centering\n\\centerline{\\includegraphics[width=\\linewidth]{figures/NoC-Hierarchical-Mesh-EyerissV2}}\n\\caption{Different configuration modes of hierarchical mesh network in EyerissV2 architecture  (Figure adopted from ).}\n\\label{fig::NoC-Hierarchical-Mesh-EyerissV2}\n\\end{figure}\n\\begin{figure}[!b]\n\\centering\n\\centerline{\\includegraphics[width=\\linewidth]{figures/NoC-microswitches}}\n\\caption{(a) Microswich network . NoC configurations: (b) multicast (c) gather (d) local communication. (Figure adopted from .)}\n\\label{fig::NoC-microswitches}\n\\end{figure}\nFor an N-PE accelerator, an array of microswitches (Fig. \\ref{fig::NoC-microswitches}a) contains $\\log_{2}N + 1$ levels with $N$ micro-switches at each level. Each microswitch contains a small combinational logic for configuration and up to two FIFOs for buffering the data during routing conflict. With small logic and storage, data traverses through several microswitches within each cycle . All microswitches contain \\emph{gather} and \\emph{scatter} units, and bottom microswitches (level $\\log_{2}N$) also contain local units for inter-PE communication. In top microswitches (level 0), the scatter unit connects to memory banks, and the gather unit uses round-robin-based priority logic for arbitrating the incoming data in a pipelined manner. In middle microswitches, scatter units forward data to desired lower-level links, and gather units stream the data back. In bottom microswitches, scatter and gather units stream the data, and local units connect adjacent PEs. Fig. \\ref{fig::NoC-microswitches}(b)--(d) shows how configurable microswitches can enable various communication patterns. \n\\begin{figure}[!t]\n\\centering\n\\centerline{\\includegraphics[width=0.95\\linewidth]{figures/NoC-benes-SIGMA}}\n\\caption{(a) Flexible dot product engine in SIGMA accelerator  features a data distribution NoC with configurable switches interconnected via Benes topology. (b)--(c): Configuration of the interconnect facilitates different unicast and multicast communication patterns. (Figure adopted from .)}\n\\label{fig::NoC-benes-SIGMA}\n\\end{figure}\nSIGMA  used \\emph{Benes topology} with \\emph{configurable} switches (Fig. \\ref{fig::NoC-benes-SIGMA}a). For N source and destinations, the interconnect contains $2\\log_{2}N + 1$ levels, each with N number of 2$\\times$2 switches. Each switch receives two control signals to determine whether to forward data vertically and/or diagonally. After combining communication requirements for distributing all elements to desired multipliers, switches can be configured to forward the data, as shown in Fig. \\ref{fig::NoC-benes-SIGMA}(b)--(c).", "cites": [7169, 4363, 7005, 4335, 4347, 4354, 7634, 7805, 4356], "cite_extract_rate": 0.25, "origin_cites_number": 36, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes data distribution mechanisms across multiple cited papers, integrating examples of broadcast, multicast, mesh, and unicast NoC designs. It provides critical insights by discussing the trade-offs of each design, such as area and power for unicast, and the need for preprocessing in systolic arrays. The abstraction level is strong as it identifies broader principles of spatial reuse and the need for configurable NoCs to handle diverse dataflows and sparsity patterns."}}
{"id": "79d994f1-db90-4ba7-9daf-de3a3cd3da3c", "title": "Mechanisms for Reduction of Partial Outputs", "level": "subsection", "subsections": [], "parent_id": "4b82dfad-305c-4c60-9b38-8189a27dd3b9", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Interconnects for Distributing Non-Zeros and Reducing Partial Outputs"], ["subsection", "Mechanisms for Reduction of Partial Outputs"]], "content": "\\label{sec:reduction-partial-outputs}\nComputation primitives of ML models require reduction (accumulation) of partial outputs from PEs or their functional units. It can be done temporally and/or spatially  (Table \\ref{tab:reduction-mechanisms}).\n\\mysubsubsection{Temporal} All the reductions for computing an output scalar are performed on a \\emph{single} PE (or a functional unit) during \\emph{different} cycles. Accumulations are done temporally when different PEs compute distinct outputs, e.g., for output stationary dataflow. The temporal reduction makes the processing of sparse tensors \\emph{simple} since PEs update partial outputs in their private memory or registers \\emph{without communicating} to other PEs via an additional interconnect. Therefore, it is adopted by many accelerators, including EIE , ZENA , and SparTen . However, temporal accumulation requires \\emph{indexing} the buffer for reading/writing partial outputs or accumulating computations in the output register (of MAC unit). So, it involves register/memory read and write operations, which consume higher energy than integer arithmetic . Besides, using local accumulator buffers for vector/SIMD functional units (e.g., in SCNN ) requires support for arbitration of partial outputs.\n\\mysubsubsection{Spatial} Partial outputs can be reduced spatially for an output scalar. It can be either done by functional units within a PE (e.g., adder-trees in Cambricon-X/S to sum up partial products) or inter-PE communication via a \\emph{separate interconnect} (e.g., forwarding in systolic arrays). Inter-PE spatial reduction usually requires communication among neighboring PEs and is typically achieved through a mesh or similar topology . Spatial reduction obviates buffer accesses and improves energy efficiency (e.g., by 2$\\times$--3$\\times$ , as compared to the temporal reduction on scalar PEs). These linear or tree-based reductions are typically symmetric. However, a major challenge is to enable \\emph{asymmetric and asynchronous reductions} of a variable number of partial outputs, for adapting to high sparsity, tensor shape, or target functionality (e.g., DW-CONV). This is because, an efficient dataflow may require some of the interconnected functional units or PEs to process partial outputs for distinct output elements (e.g., different depth-wise groups); all partial outputs cannot be reduced altogether. Hence, configurable interconnects are needed. Otherwise, for high or hyper sparsity, functional units cannot be fed enough NZs and are poorly utilized. Note that structured sparsity can alleviate imbalance by inducing patterns such that all PEs process the same number of NZs. However, configurable mechanisms are still required to support different dataflows for the variations in functionalities or tensor shapes. \n\\mysubsubsection{Spatio-temporal} Partial outputs can be reduced spatially and temporally and \\emph{locally} (within PEs) and \\emph{globally} (across PEs). Spatial and temporal reduction of outputs depends on the mapping of computation graph onto PEs . In spatiotemporal reduction, different PEs or their functional units compute partial outputs at every cycle or a few, which are, at first, reduced spatially. The resultant partial output is then reduced temporally by updating the previous partial output in the memory. E.g., when data streams through PEs of a systolic array, there is an inter-PE spatial reduction of partial outputs (via PEs of each column). Then, the bottom PE-row provides the reduced partial outputs to accumulator buffers (CompAct , TPU ). PEs of SNAP  perform spatiotemporal accumulation locally, where partial products are first spatially accumulated through a configurable adder-tree and then accumulated in PE's memory over time.    \n\\mysubsubsection{Temporo-spatial} In temporospatial reduction, PEs compute partial outputs and reduce them locally over time. Then, they are collected later and accumulated spatially via interconnect before further processing (e.g., write-back, encoding). For example, PEs of a cluster in EyerissV2  first locally accumulate partial summations. Then, partial outputs can be accumulated across vertically connected clusters. SCNN  PEs compute output tiles corresponding to distinct input feature maps stored in their buffers. Outputs are temporally reduced by indexing the accumulator buffers. Then, overlapping fractions of incomplete outputs are exchanged among neighboring PEs for reduction. SNAP  also performs temporospatial reduction at PE-array (core) level. Its PEs accumulate outputs locally over time, which are reduced spatially across horizontal/diagonal PEs by a core-level reducer.\n\\begin{figure}[!t]\n\\centering\n\\centerline{\\includegraphics[width=\\linewidth]{figures/NoC-configurable-reduction}}\n\\caption{Configurable spatial reduction trees: (a) Augmented reduction tree in MAERI (Figure adopted from .) (b) Forwarding adder network in SIGMA (Figure adopted from .)}\n\\label{fig::NoC-configurable-reduction}\n\\end{figure}\n\\mysubsubsection{Configurable} \nMAERI  and SIGMA  employ configurable reduction trees for efficient and asymmetric spatial reduction of partial outputs. So, it can be useful for spatial processing of unstructured sparsity and variable-sized vectors for dot products. The augmented reduction tree in MAERI (Fig. \\ref{fig::NoC-configurable-reduction}a) allows an asymmetric reduction of partial outputs with configurable adder switches and bidirectional forwarding links. Each 3-input adder switch can receive two partial outputs from the previous level and one via a forwarding link, and it can add and forward them. Plus, upper-levels of the tree (near root) have double bandwidth than lower-levels, allowing simultaneous collection of multiple reduced outputs. The forwarding adder network in SIGMA (Fig. \\ref{fig::NoC-configurable-reduction}b) enables similar configurable reduction but at reduced area and power. Instead of 3-input adders, it uses 2-input adders and N:2 mux for selecting the inputs. Also, adders at the $0^{th}$ level allow bypassing of partial products to the next level.\n\\begin{table}[!t]\n\\centering\n\\caption{Mechanisms for Accumulations of Partial Outputs}\n\\label{tab:reduction-mechanisms}\n\\begin{tabular}{|c|m{5.1cm}|}\n\\hline\nTemporal            &  \\\\ \\hline\nSpatial (intra-PE)  &  \\\\ \\hline \nSpatial (inter-PE)  &  \\\\ \\hline \nSpatio-temporal     &  \\\\ \\hline\nTemporo-spatial     &  \\\\ \\hline\nConfigurable        &  \\\\ \\hline\n\\end{tabular}\n\\end{table}", "cites": [7633, 7005, 4335, 4347, 4354, 7634, 4356], "cite_extract_rate": 0.23333333333333334, "origin_cites_number": 30, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes information from multiple cited works to present a structured overview of partial output reduction mechanisms. It provides a coherent narrative by categorizing and comparing temporal, spatial, spatiotemporal, temporospatial, and configurable approaches, highlighting their trade-offs in terms of energy efficiency, communication requirements, and suitability for sparsity. It also abstracts broader design principles, such as the role of interconnects and dataflow in handling irregular and sparse computations."}}
{"id": "9fdf5ca0-0307-4f7d-a9ba-a9873050ebdc", "title": "Optimization Opportunities", "level": "subsection", "subsections": [], "parent_id": "4b82dfad-305c-4c60-9b38-8189a27dd3b9", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Interconnects for Distributing Non-Zeros and Reducing Partial Outputs"], ["subsection", "Optimization Opportunities"]], "content": "\\textit{i) Low-cost flexible interconnects for accommodating spatial reuse opportunities, dynamic communication requirements, various sparsity, and different precision:} Variations in data reuse (Fig. \\ref{fig::analysis-data-reuse-sparse}) are caused by the tensor size, functionality (e.g., stride, separable convolution), batch size, and sparsity of tensors. The communication  mechanism needs to leverage available reuse by supporting various multicast and unicast patterns . Moreover, the distribution, inter-PE communication, and collection of the outputs can be done \\emph{asynchronously} and \\emph{concurrently}. These require the interconnect switches to support \\emph{dynamic} management (priority arbitration and congestion) at low cost. Furthermore, communication among distant PEs may be required (e.g., for store-and-forward or exchanging outputs during sparse computations). Finally, depending on sparsity and precision, the bit-width of the metadata and NZ value can differ significantly. Communicating different sizes of data and metadata can be facilitated by configurable interconnect buses and their interfacing with PEs and memory. For instance, in EyerissV2 , a 24-bit bus can supply PEs either three 8b uncompressed values or two pairs of 8b NZ and 4b metadata. Thus, configurable interconnect topologies should be explored for effectively serving various communication requirements. FPGAs can also be leveraged for designing accelerators with tailored interconnects.\n\\textit{ii) Programming of configurable interconnects and design exploration:} Configurable interconnections can support various communication patterns and dynamic data movement for sparse computations. But, compilation support is needed to program them as they often contain parameterized multi-level switches and switches with many-to-many links between source and destination (e.g., ). Depending on the interconnect topology and optimized dataflow, the compiler may need to select efficient paths for distributing data from source to destination switches. Additionally, the underlying topology (e.g., lack of multi-hop connectivity) \\emph{may not} support some dataflows (e.g., spatiotemporal accumulation of partial outputs from distant PEs in the absence of multi-hop connectivity). Further, a systematic methodology for mapping communication onto interconnect topology can enable design space exploration of interconnects needed for accelerating target ML models, allowing minimum overhead of \\emph{run-time reconfiguration} of the interconnect to support various dataflows.", "cites": [4335], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from the cited paper (Eyeriss V2) and integrates it with broader architectural and communication considerations to present a coherent narrative on flexible interconnects. It identifies limitations, such as the need for compilation support and potential topological constraints, and provides abstract principles like dynamic management, configurability, and design exploration. While it offers insightful analysis, the critique and comparison across multiple papers are somewhat limited, as only one paper is cited."}}
{"id": "d9ab0fd8-41d0-4648-973b-6a9930053093", "title": "PE Architecture Design", "level": "section", "subsections": ["adae221e-0b86-440d-9247-a6adfbc6cb8f", "77a52b82-cfaf-475e-a733-c6109662e9aa", "cc845f17-b9e8-4c7d-8469-99a422bd86f0"], "parent_id": "6220d89f-a486-420a-914e-59957895a313", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "PE Architecture Design"]], "content": "\\label{sec::PEArch}\nPE architecture consists of functional units, local memory (RFs or SRAMs), and local control (instruction buffer or finite state machine). Fig. \\ref{fig::PE-pipeline} shows pipeline stages for processing sparse and value-approximated tensors. Depending upon PE's interface, it either gets data from the interconnect (typical) or directly accesses off-chip memory via DMA transfer. At every cycle or few, a PE (a) processes an instruction or events based on its state , (b) fetches data from local memory or interconnect, (c) computes tensor elements via functional unit, and (d) writes the intermediate result to local memory or interconnect. PEs may contain \\emph{special-function} modules (e.g., for ReLU or sigmoid computations ). \n\\begin{figure}[!t]\n\\centering\n\\centerline{\\includegraphics[width=0.9\\linewidth]{figures/PE-pipeline}}\n\\caption{Overview of the PE pipeline for processing sparse and value-approximated tensors (Figure adopted from ).} \n\\label{fig::PE-pipeline}\n\\end{figure}\nProcessing compressed tensors can impose significant maneuvering efforts for PE design. For example, reusing tensors temporally through local memory (e.g., in EyerissV2 , SNAP ) alleviates overheads of repeatedly accessing compressed tensors via memory hierarchy and decoding them. However, it requires communicating data to PEs before extracting NZs. Thus, the PE may require additional hardware for \\emph{extracting} or \\emph{correctly indexing} NZs (section \\ref{sec::NZ-data-extraction}). Additionally, \\insight{the selection of functional units is affected by the number of NZs that can be fed for various sparsity of tensors, support for mixed-precision, and functionality of the layers.} In such various scenarios, a single dataflow may not always be effective  and can lead to significant acceleration loss. So, PE datapath needs to be \\emph{adaptive} for supporting multiple dataflows optimized for different layers and sparsity. Further, techniques for leveraging computation reuse due to \\emph{value similarity} often require enhancements in the design. PEs may also post-process outputs or generate additional metadata for communicating outputs. So, an efficient pipeline needs to hide pre-processing and post-processing latency.", "cites": [4347, 4354, 4335], "cite_extract_rate": 0.375, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from EyerissV2, SNAP, and SCNN to discuss PE architecture design for sparse tensor processing, particularly focusing on data reuse and sparsity handling. While it includes some analytical statements (e.g., about functional unit selection and adaptive dataflows), it lacks deeper comparative or critical analysis of the cited papers. The discussion abstracts some design principles, such as the need for adaptive pipelines and value similarity exploitation, but stops short of forming a novel, overarching framework."}}
{"id": "adae221e-0b86-440d-9247-a6adfbc6cb8f", "title": "Functional Units", "level": "subsection", "subsections": [], "parent_id": "d9ab0fd8-41d0-4648-973b-6a9930053093", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "PE Architecture Design"], ["subsection", "Functional Units"]], "content": "\\mysubsubsection{Scalar PEs} Table \\ref{tab:PE-vectorization} lists accelerators based on their functional units for scalar, SIMD, or vector processing. Many architectures contain an array of scalar PEs; PE datapath contains a pipelined MAC unit (e.g., EIE , SparTen ). \n\\mysubsubsection{SIMD/Vector PEs} PEs of Cnvlutin  and Cambricon-S  contain multiplier arrays and adder trees. By performing dot products at every cycle, they can deliver high throughput. Moreover, accumulation through adder-trees reuses data spatially, which lowers energy consumption (by 2$\\times$--3$\\times$ ) as compared to temporal accumulation on scalar PEs by reading and writing partial summations via local memory. However, a major challenge is the inefficient utilization of multipliers and adders, which often leads to ineffectual computation cycles and acceleration loss. This is because, for high sparsity, enough NZs may not be extracted to feed all multipliers at every cycle. For example, a sensitivity analysis for Cambricon-X  determined that, for hyper $W$-sparsity, it accelerated CONVs by about 8$\\times$ (out of 16$\\times$ peak speedup). The utilization may be improved by employing larger indexing or extraction modules (increased on-chip area and power). Alternatively, PEs can be designed with fewer multipliers to sustain the scalability and efficiency over a wide sparsity range.\n\\begin{table}[!t]\n\\centering\n\\caption{PE Architectures for Sparse Tensor Computations}\n\\label{tab:PE-vectorization}\n\\begin{tabular}{|c|m{6.6cm}|}\n\\hline \nScalar &  \\\\ \\hline\n\\makecell{SIMD /\\\\Vector} &  \\\\ \\hline \n\\end{tabular}\n\\end{table}\nWhile SIMD or vector PEs achieve spatial reuse, due to fixed designs, they are utilized poorly when executing some functionalities like DW-CONV. The efficiency of SIMD PEs is further affected by high sparsity, as functional units of the PE require synchronization, and there may not be enough effectual NZs to feed all of them. \\emph{Configurable functional units} can overcome such limitations. For example, PEs of SNAP architecture  use a configurable adder-tree. It processes inputs from three multipliers and computes different combinations of partial summations. With multiple adders and multiplexers, the PE can concurrently process different partial summations (vs. gather in adder-tree) without high-bandwidth crossbars. Such configurable designs can support different DNN operators (e.g., DW-CONVs).\n\\mysubsubsection{Multiplier-free PEs} Accelerators, such as ZENA  and , use multiplier-free PEs for high energy efficiency. These PEs process tensors of very low-precision (binary or ternary values) or logarithmic quantization. So, they replace multipliers with \\emph{simpler arithmetic} like 2's complement (inverters and adders or subtractors)  or bit-wise shift and additions . However, one challenge is to \\emph{maintain} the accuracy of DNNs, as aggressive quantization often drops top-1 and top-5 accuracy, e.g., by 0.1\\%  -- 5\\% . By trading off the flexibility with simple hardware, supporting various models can be challenging.\n\\begin{figure}[!b]\n\\centering\n\\centerline{\\includegraphics[width=\\linewidth]{figures/bit-adaptive-computing}}\n\\caption{Bit-serial processing of sparse activations in Pragmatic . (a) Bit-parallel unit. (b) Bit-serial unit. (c) Bit-serial unit in Pragmatic for processing only essential bits. (Figure adopted from ).}\n\\label{fig::bit-serial-pragmatic}\n\\end{figure}\n\\mysubsubsection{Bit-adaptive computing} Precision requirements for targeted accuracy can vary for different models , which can be supported by PEs with bit-adaptive computing. \n\\textbf{Bit-serial computing:} Albericio et al.  showed that \\emph{zero bits in NZ activations} (8b or 16b precision) can be more than 50\\% and proposed the Pragmatic accelerator to leverage sparsity of activation bits. Fig. \\ref{fig::bit-serial-pragmatic}(b) shows the \\emph{bit-serial} computation of an inner product with AND gates, adder tree, and bit-wise shift of partial output. AND gates are serially fed 1b activations (variable precision) and bit-parallel 16b weights (fixed precision). Fig. \\ref{fig::bit-serial-pragmatic}(c) shows the processing of only NZ activations in Pragmatic (essential bits indicated by their positions). Laconic  achieved further accelerations by processing only NZ bits of both activations and weights. \n\\textbf{Bit-composable computing:} Bit-fusion  employed fusion units consisting of an array of BitBricks. The fusion units can be configured for processing multiplications of 2b, 4b, 8b, or 16b operands. For processing NZs, PEs of CNN accelerator Envision  used a single-cycle N-subword-parallel multiplier, followed by an N$\\times$48b/N reconfigurable adder. The subword-parallel design allowed the configuration of MAC units for processing the data of 4b, 8b, or 16b. SPU architecture  employed DGRA, a decomposable CGRA, for efficiently processing stream-join accesses. The DGRA PE and interconnect switches enabled decomposing up to four 16b sub-word operands. DGRA also supported accessing sub-word data from the scratchpad. For DNN training with mixed-precision and sparse tensors, PEs of LNPU contained configurable MAC units that can process FP8 or FP16 tensors. Table \\ref{tab:precision} lists precisions of sparse tensors that are supported by different accelerators. The precision indicates the bit-width of input operands (activations and weights). For MAC operations, accumulators usually produce high-precision output, which can be down-scaled or truncated afterward.  \n\\begin{table}[!t]\n\\centering\n\\caption{Precision of Sparse Tensors Supported by Accelerators}\n\\label{tab:precision}\n\\begin{tabular}{|c|m{5.8cm}|}\n\\hline \nbinary/ternary &  \\\\ \\hline\nint8        &  \\\\ \\hline\nint16       &  \\\\ \\hline\nlogarithmic &  \\\\ \\hline\nbit-adaptive   &  \\\\ \\hline\nFP8         &  \\\\ \\hline\nFP16        &  \\\\ \\hline\nFP32        &  \\\\ \\hline\nFP64        &  \\\\ \\hline \n\\end{tabular}\n\\end{table}\n\\mysubsubsection{Clock-gated PEs} PEs can be clock-gated when \\emph{not used} for executing a layer and for ineffectual computations. For example, Eyeriss , Thinker , and Minerva  use zero detection for clock gating the datapath in the PE pipeline. PEs check whether the value being read is zero (or compare with a threshold, e.g., in MNNFast ). Based on the comparator output, their clock gating logic prevent the MAC datapath from switching in the consecutive cycle, which reduces energy (e.g., it saved power consumption of Eyeriss PE by 45\\%). Zero-skipping through flags in Envision  and Sticker  achieved similar savings.\n\\mysubsubsection{Optimization opportunities}\n\\textit{(i) Exploring efficient designs of functional units for various sparsity ranges/patterns and functionality:} Utilization of vector/SIMD units can drop significantly due to unstructured sparsity  and functionality beyond structured dot products (e.g., DW-CONV). So, for exploring design hyperparameters such as the number of functional units, designers need to consider the impacts of sparsity, data extraction mechanism, required synchronization among computational units, and configurations required to support various functionalities. Moreover, for low sparsity, designs should deliver performance \\emph{at par} with a sparsity-oblivious design. For example, for processing dense tensors, SCNN  achieved 79\\% of the performance and consumed 33\\% higher energy as compared to the baseline accelerator for processing dense tensors. So, designers may ensure that additional features for exploiting sparsity and configurable components do not increase the critical path latency and are power-gated if not used.", "cites": [7169, 4335, 4365, 4347, 4354, 7634, 4364, 7805, 4356, 4366], "cite_extract_rate": 0.2127659574468085, "origin_cites_number": 47, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes concepts from multiple papers to discuss different types of functional units in processing elements (PEs), connecting their designs, benefits, and limitations across sparsity levels and DNN operations. It provides critical analysis of utilization inefficiencies and trade-offs in energy consumption and accuracy. The abstraction is strong, highlighting general principles like the need for configurability, bit-adaptive computing, and optimization under sparsity constraints."}}
{"id": "77a52b82-cfaf-475e-a733-c6109662e9aa", "title": "Dataflow Mechanisms", "level": "subsection", "subsections": [], "parent_id": "d9ab0fd8-41d0-4648-973b-6a9930053093", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "PE Architecture Design"], ["subsection", "Dataflow Mechanisms"]], "content": "\\label{sec::dataflows}\n\\mysubsubsection{Background} The efficiency of executing a layer onto hardware accelerator depends on the computational, communication, and memory access patterns, which are commonly referred to as dataflow mechanisms . A dataflow refers to the spatiotemporal execution of a model layer (nested loop) on architectural resources . Here, spatial execution corresponds to how PEs exploits parallelism in the computation graph and processes different subsets of tensors. Temporal execution drives the data accessed throughout memory hierarchy and data communication via interconnects. Thus, depending on the functionality and tensor dimensions, dataflow can significantly impact the utilization of resources, data reuse, and latency hiding for memory accesses and data communication, and consequently, the execution time and energy consumption . \n\\begin{figure}[!t]\n\\centering\n\\centerline{\\includegraphics[width=\\linewidth]{figures/dataflows}}\n\\caption{Commonly used dataflow mechanisms for executing convolution layers on hardware accelerators.}\n\\label{fig::dataflows}\n\\end{figure}\nOne way to classify dataflows is by what data is kept ``stationary'' in registers or local memory of PEs (and reused fully before eviction), while other data is being iterated over.\nSome commonly used dataflow mechanisms are output stationary, weight stationary, input stationary, row stationary, and no local reuse. Fig. \\ref{fig::dataflows} shows an example of convolution and the layout of the stationary data for mapping the convolution with these dataflows. In weight stationary dataflow, each weight (of a 2D filter) remains stationary on a unique PE, and reused many times, during processing activations (corresponding to the same input channel $C$). By processing a unique weight, each PE produces partial summations for output activations, which are communicated by PEs and accumulated before outputs are written back to the memory hierarchy. Thus, input and output activations are accessed from off-chip memory (via shared scratchpad and PE's local memory) several times, while weights are continuously reused. After reuse, a new set of weights is loaded from memory, and the execution repeats. Weight reuse is higher in processing larger feature maps (CNNs) and multi-folded for processing data in a batch (e.g., images for CNNs, tokens of sentences for NLP models). Fig. \\ref{fig::layer-characteristics} lists such characteristics of different layers. \nDataflows can be applied at a coarser level, where PEs process a data block or plane (1D/2D/3D). In a coarse weight stationary approach , each PE processes weights of an entire 2D filter (dimensions $C$ and/or $M$ are laid out spatially on PEs). Rows and columns of PEs process the data corresponding to unique input and output channels, respectively. So, activations need to be multicast to the PEs of a row, different weights need to be provided to each PE, and partial summations for output channels can be accumulated vertically . Similarly, in an input stationary dataflow, unique activations (or blocks of input feature maps) remain stationary and are reused. In an output stationary dataflow, each PE produces a unique output activation (corresponding to the same or different output channel) . By processing spatial data and input channels first, partial summations are accumulated and reused in the memory of each PE. With the temporal accumulation of outputs on PEs, the output stationary dataflow does not need to reduce partial outputs spatially by collecting them from appropriate PEs, which is otherwise challenging for unstructured sparse data (section \\ref{sec:reduction-partial-outputs}). Therefore, many accelerators opt for such dataflow. In no local reuse dataflow, input operands are streamed to PEs, but they are not stored in PE's memory . In row stationary dataflow, PEs of the same row process the same weights (a row of a filter), diagonal PEs process the same row of input activations, and partial summations for rows of the output feature map are accumulated through vertically connected PEs . \nThus, different dataflows uniquely exploit the spatial parallelism and reuse of different tensors. \n\\begin{figure}[!t]\n\\centering\n\\centerline{\\includegraphics[width=\\linewidth]{figures/dataflow-low-PE-utilization}}\n\\caption{Low utilization of a 16$\\times$16 PE-array in (a) coarse weight stationary dataflow when executing depth-wise layers and (b) input stationary dataflow for executing later layers of deep CNN models (Figure inspired by ).}\n\\label{fig::dataflow-low-PE-utilization}\n\\end{figure}\n\\textit{Dataflow optimization:} As dimensions of tensors are often large, many ways exist for spatiotemporally executing a layer onto the computational and memory resources of an accelerator. Optimization of the dataflow is important as it can significantly impact the performance and energy consumption . For instance, mappings with similar performance can consume an order of magnitude higher energy  or vice versa. Further, as Fig. \\ref{fig::layer-characteristics} shows, reuse characteristics, tensor dimensions, functionality, and sparsity can vary significantly for different DNN layers. Hence, a single dataflow may not always be effective for acceleration. Fig. \\ref{fig::dataflow-low-PE-utilization} provides two such examples that lead to low PE-array utilization. The coarse weight stationary dataflow processes different 2D filters on different PEs. So, it is inefficient for DW-CONV. Similarly, output-stationary or input-stationary dataflows can result in low utilization of PEs for processing later layers of deep CNNs. With the vast space of execution methods and the growing development of new models (with variations in tensor dimensions), it becomes hard for non-experts to figure out optimized execution methods and designs. Therefore, many optimization tools have been proposed recently including Timeloop , dMazeRunner , MAESTRO , and Interstellar . They analytically model the execution of accelerators to estimate execution metrics and evaluate a set of mappings from the pruned space of various dataflows. \n\\begin{figure}[!t]\n\\centering\n\\centerline{\\includegraphics[width=\\linewidth]{figures/layer-characteristics}}\n\\caption{Characteristics of different DNN layers pertaining to hardware execution (Figure inspired by ).}\n\\label{fig::layer-characteristics}\n\\end{figure}\n\\mysubsubsection{Sparsity-aware dataflows} Dataflows for processing sparse tensors are typically similar to those for dense tensors while processing the data in compressed format. For correct functionality, dataflow executions are facilitated by extraction/orchestration of NZs, which is done either in PEs , on a different module , or by a separate controller. For example, SCNN  used PT-IS-CP dataflow. It processed planar tiles of feature maps with input stationary dataflow. SCNN's PT-IS-CP-sparse dataflow extended the PT-IS-CP. It processed only NZ activations and weights in compressed format while accessing them from memory and performing computations. The coordinate computation module in each PE ensured that partial products generated by all-to-all multiplications of NZ inputs and weights were accumulated correctly and stored in appropriate buffers. Table \\ref{tab:dataflows} lists sparsity-aware dataflow mechanisms used by accelerators.\n\\begin{table}[!t]\n\\centering\n\\caption{Dataflow Mechanisms of Accelerators}\n\\label{tab:dataflows}\n\\begin{tabular}{|c|m{4.6cm}|}\n\\hline \nInput Stationary            &  \\\\ \\hline \nOutput Stationary           &  \\\\ \\hline\nWeight Stationary           &  \\\\ \\hline\nCoarse Weight Stationary    & ,  \\\\ \\hline\nRow Stationary              &  \\\\ \\hline\n\\end{tabular}\n\\end{table}\nEyerissV2  used an enhanced row-stationary dataflow. By using statically known sparsity of weights, more NZ weights were allocated in local memories and global scratchpads. For example, each PE can store up to 192 NZ weights. Mappings of CONV and FC layers of AlexNet with row-stationary dataflow allocated 64--174 NZ weights, which corresponded to a total of 132--480 weights in the dense format. With in-PE data extraction logic, each PE only processed NZ values from CSC-encoded data. Thus, sparsity-aware dataflow can be optimized with the pre-known (or expected bounds of) sparsity and value similarity.            \n\\mysubsubsection{Optimization opportunities}\n\\textit{(i) Dataflow optimizations accounting for storage and computational overheads for metadata and codebooks:} Sparse and value-shared tensors are processed along with metadata (indicates positions of NZs) and codebook (common values shared among tensor elements), respectively. It requires additional processing, e.g., buffer management, communication via interconnects, and indexing the appropriate values. Depending on the dataflow, such processing can amplify the execution costs, which needs to be optimized. Existing tools for optimizing dataflows target dense tensor computations. Accelerators EIE, SCNN, and Cambricon-S process sparse tensor computations but with customized dataflows. Hence, frameworks for mapping and design explorations need to consider the sparsity and value similarity of tensors and their variations across layers/models. Such tools can include additional costs corresponding to storage, communication, and extraction in their analytical models. Explorations supporting multiple dataflows can help to achieve efficient designs for handling different functionality and variations in sparsity, shapes, and quantizations of tensors.\n\\textit{(ii) Sparsity-aware resource partitioning:} Acceleration of deep learning models is scaled by simultaneously processing multiple layers. It is done either by partitioning resources  of a scaled-up accelerator or on multiple accelerators (scale-out) by leveraging model- or data-parallelism . Techniques for resource partitioning aim to highly reuse data from the on-chip memory of accelerators. It involves evaluating many-to-many mappings between layers and accelerators. Such optimizations can be crucial for several applications that require low latency, real-time processing, or high frame rates (e.g., processing the frames for multiple object detection models of an autonomous vehicle's perception system). Exploiting sparsity can provide further opportunities due to fewer computation, communication, and storage.", "cites": [7633, 4367, 8769, 7169, 7005, 4335, 4347, 4354, 7634, 7805, 4356], "cite_extract_rate": 0.3055555555555556, "origin_cites_number": 36, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes dataflow concepts from multiple cited papers (e.g., SCNN, EyerissV2, TPU) and integrates them into a structured explanation of how different dataflows exploit parallelism and reuse. It critically highlights limitations such as low PE utilization and discusses trade-offs in dataflow choices. While it abstracts some principles of dataflow optimization, it stops short of presenting a meta-level framework or deeply novel synthesis."}}
{"id": "cc845f17-b9e8-4c7d-8469-99a422bd86f0", "title": "Leveraging Value Similarity", "level": "subsection", "subsections": [], "parent_id": "d9ab0fd8-41d0-4648-973b-6a9930053093", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "PE Architecture Design"], ["subsection", "Leveraging Value Similarity"]], "content": "Several techniques have leveraged value similarity for accelerating DNNs by value sharing and computation reuse (Table \\ref{tab:value-similarity}). Video frames exhibit high similarity spatially (among neighboring pixels) and temporally (over consecutive frames) . After precision lowering, values of limited range repeat frequently , which are further compressed by maintaining a codebook of unique values . With repetition of values, computation (outputs) can be reused, either partially during processing a layer  or by skipping processing of a whole layer . This subsection describes such techniques and corresponding hardware enhancements.\n\\begin{table}[!t]\n\\centering\n\\caption{Techniques for leveraging value similarity.}\n\\label{tab:value-similarity}\n\\begin{tabular}{|c|c|m{3.5cm}|}\n\\hline\n\\multirow{2}{*}{\\makecell{Value sharing}}\n    & Weights     &    \\\\ \\cline{2-3} \n    & Activations &      \\\\ \\hline\n\\multirow{2}{*}{\\makecell{Computation reuse \\\\ and memoization}} \n    & Partial     &      \\\\ \\cline{2-3} \n    & Full        &  \\\\ \\hline\n\\multicolumn{2}{|c|}{Early termination of computations}  &    \\\\ \\hline \n\\end{tabular}\n\\end{table}\n\\mysubsubsection{Weight similarity} Prior studies have shown that weights can be approximated with a small set of values. \nHegde et al.  showed that for 8b weights of DNNs, each NZ value mostly repeated more than 10 times and even more than 100 times in later layers of AlexNet and ResNet-50 models. Han et al.  pruned weights of DNNs with k-means clustering for value sharing. Shared unique values were represented with 4 or 5 bits without dropping classification accuracy. Local quantization (applying clustering separately over different sub-tensors) can achieve even smaller codebooks . Leveraging the weight similarity can compress pruned models further by up to an order of magnitude .\nValue-shared weights are processed by augmenting the PE datapath with a weight decoder (e.g., in EIE ). For processing NZ weights, the PE provides the encoded index of the weight to the decoder and obtains shared value.  \nDepending on the lookup mechanism and total bits to be extracted at every cycle, the decoder can incur considerable area and power costs (e.g., for Cambricon-S , 32.56\\% and 3.98\\% of the total on-chip area and power, respectively). \n\\mysubsubsection{Input similarity} Audio or video frames can contain high similarity spatially or temporally. This is because a speech signal can be quasi-stationary for a short interval. Also, successive executions of DNNs process overlapping windows of audio frames for context extraction . Feature maps for CNNs exhibit high spatial correlation . High input similarity enables only storing unique values and reusing computations by \\emph{differential computing} over non-similar data. \nRiera et al.  showed that after uniform linear quantization of inputs of DNNs (e.g., C3D , EESEN , CNN for self-driving cars ), about 61\\% of input activations are the same as previous execution, and 66\\% computations can be avoided. Their accelerator maintains centroids of quantized inputs and the index corresponding to each input element. Then, consecutive frames are processed layer-wise with differential computing. For example, for each activation of an FC layer (of a new frame), the accelerator calculates centroid and index, and then it compares calculated centroid to memoized centroid. If the difference is zero, then output from the previous execution is reused, and the next activation is processed. Otherwise, a new value of the index is updated in the buffer, and new values for output activations are computed by accumulating multiplications of weights with the difference.\n\\begin{figure}[!t]\n\\centering\n\\centerline{\\includegraphics[width=\\linewidth]{figures/weight-similarity-UCNN}}\n\\caption{(a) Leveraging weight similarity and reuse of partial outputs . (b) Modifications in UCNN PE architecture (shaded blocks) for buffering indirection tables, partial summations of activation groups, and memoization of partial outputs. (Figure adopted from .)}\n\\label{fig::weight-similarity-UCNN}\n\\end{figure}\n\\mysubsubsection{Computation reuse (partial during processing a layer)} UCNN  leverages the repetition of weights by forming activation groups (summations of activations) that share the same weight. It also reuses activation sub-groups, i.e., memoizes partial summations of activations that can repeatedly appear across different filters. Fig. \\ref{fig::weight-similarity-UCNN}(a) illustrates an example. Weights $A$ and $C$ can be shared among corresponding activation groups. For producing activation groups, subgroups like (r+s) can be reused with memoization. So, an output activation is calculated by indexing a unique weight value and corresponding activation groups. Indirection tables provide indices of the unique weight and grouped activations. Fig. \\ref{fig::weight-similarity-UCNN}(b) shows corresponding modifications in the PE datapath. UCNN reported up to 24\\% area overhead for a PE and 1.8$\\times$ speedup for CNNs as compared to execution on a baseline accelerator without exploiting weight repetition.   \nSilfa et al.  showed that for RNNs (e.g., DeepSpeech2 , EESEN ), the relative difference between the output activations over consecutive frames was about 23\\%. Leveraging temporal similarity of outputs saved about 24\\% computations with negligible accuracy loss. For predicting whether an output activation leads to a value similar to the previous output, their technique extended each RNN layer with a binary neural network (BNN). With BNN outputs correlating to actual outputs, execution of much smaller BNN layers led to an efficient prediction of the temporal output similarity.   \n\\mysubsubsection{Computation reuse (skip processing of entire layer)} A few techniques predict outputs based on previous computations and skip heavy computations of some layers. Gon{\\c{c}}alves et al.  showed that 18\\%--81\\% of computations in AlexNet CONV layers could be reused due to spatial (intra-frame) and temporal (inter-frame) redundancy of the inputs. They leveraged such reuse with memory look-ups and avoided executing CONVs. \nFor YOLO-v3 , it processed only 22\\%--32\\% frames while incurring negligible accuracy loss.\nBuckler et al.  proposed skipping heavy processing of some CNN layers for several frames (predicted) and executing precise computations periodically for remaining (key) frames. For predicted frames, their algorithm estimated motion in the input frame. It used results for incrementally updating the output saved from the last key frame. Unlike other value similarity techniques that incur changes in PE datapath, such techniques can be efficiently executed on a separate module (e.g., EVA$^2$ ) or co-processor, while other modules of the same or different accelerator process sparse tensors of DNN layers. EVA$^2$ identified 78\\%--96\\% of the frames for AlexNet and 40\\%--71\\% of the frames for Faster-RCNN as predicted frames while processing the YouTube-BoundingBoxes dataset .\n\\mysubsubsection{Early termination of computations by predicting outputs} SnaPEA , SparseNN , and CompEND  reduce ineffectual computations by early prediction of the usefulness of outputs. They check whether computations contribute to the effective inputs for the subsequent layer (e.g., ReLU or max-pooling). If not, their PEs terminate such computations. To reduce computations corresponding to output sparsity,  statically re-ordered weights based on their signs. PEs of its SnaPEA architecture contained prediction activation units (with each MAC), which checked the sign-bit of the partial summation and raised a termination signal to notify the controller as the sign-bit was set. \n\\mysubsubsection{Optimization opportunities} \n\\textit{(i) Joint exploration of spatial and temporal similarity of inputs, weights, and outputs:} Depending on the model's configurations (depth, layer dimensions, cardinality) and domain-specific data, opportunities for value sharing and computation reuse (at both fine-grain and coarse-grain levels) in processing activations and weights can vary considerably. A joint exploration for different tensors can help to identify storage-Ops-accuracy trade-offs for efficient acceleration. \n\\textit{(ii) Leveraging value similarity through separate processing:} Determining value similarity and leveraging computation reuse often demands modifications in PE-array, increasing the accelerator's area, latency, or power. Designers may obviate it by providing a separate and compact module for differential computing that handles necessary pre-processing or post-processing and can be interfaced with the PE-array and on-chip memory. Upon requirement, it can trigger execution on PE-array for structured computations. Further, algorithms expressing the functionality of ML layers/models may be defined in terms of \\emph{differential computing} (i.e., execution is conditional to the input mismatch, reused otherwise). With efficient accelerator/model co-designs for differential computing of tensors, accelerators may attain structured effectual computations with fewer overheads of metadata or memoization.", "cites": [4370, 4372, 4371, 836, 4369, 887, 784, 7634, 4359, 4368, 4350], "cite_extract_rate": 0.4782608695652174, "origin_cites_number": 23, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes information from multiple cited works by connecting themes like weight, activation, and output similarity across different architectures and use cases (e.g., video frames, RNNs, and CNNs). It provides some critical analysis by discussing trade-offs such as area/power overheads and accuracy impact. The abstraction is moderate, identifying patterns in reuse and compression but not fully elevating them to a generalized framework."}}
{"id": "2c8f696c-91bc-46b7-a15a-683e0f85caef", "title": "Sources and Impact of Imbalance", "level": "subsection", "subsections": [], "parent_id": "ac4d8613-1421-4c83-a4a5-66deeb11b225", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Load Balancing of Effectual Computations"], ["subsection", "Sources and Impact of Imbalance"]], "content": "\\mysubsubsection{Inter-PE imbalance} Zeros in different tensors can be scattered, and their positions may not be determined statically (e.g., unstructured $IA$-sparsity). For most accelerators, work to be performed concurrently by PEs is fixed statically. Also, executions with conventional dataflows usually require synchronization among PEs (e.g., in SCNN , Cnvlutin ), which is achieved by barriers implemented in software via instructions or in hardware via PE architecture or controller logic. Consequently, computations per PE during each execution pass can vary drastically (inter-PE load imbalance). So, many PEs may finish their computations early, get stalled, and wait for the next set of data due to synchronized execution, while other PEs still process the previously allocated data. It increases execution time and energy consumption. Kim et al.  analyzed the distribution of NZ weights in AlexNet CONV3 filters and showed that in an execution pass, NZs processed by the leading and trailing PEs differed by up to 6.5$\\times$. Similarly, up to 40\\% cycles were idle for executions of PEs in SCNN architecture . Sensitivity analysis for EIE showed that, without any load balance, about 47\\% of the cycles were idle for the 64-PE accelerator .\n\\begin{table}[!t]\n\\centering\n\\caption{Classification of Load Balancing Techniques}\n\\label{tab:load-balance}\n\\begin{tabular}{|c|c|m{3.95cm}|}\n\\hline\n\\multirow{3}{*}{\\makecell{Software \\\\ Directed}} \n& Data Clustering &  \\\\  \\cline{2-3} \n& Data Reorganization &  \\\\ \\cline{2-3}\n& Model Regularization &  \\\\ \\hline \n\\multirow{2}{*}{\\makecell{Hardware \\\\ Module}} \n& Work Prefetching &  \\\\ \\cline{2-3} \n& Work Sharing & \n\\\\ \\hline\n\\end{tabular}\n\\end{table}\n\\mysubsubsection{Intra-PE imbalance} For SIMD or vector PEs, intra-PE load imbalance can also contribute to a significant acceleration loss. With unstructured sparsity of one or more tensors, enough NZs may not be extracted to feed all the functional units within PEs, which causes intra-PE load imbalance. Sensitivity analysis for the SNAP accelerator showed that with moderate sparsity, utilization of multipliers falls below 80\\% and up to 20\\% for 90\\% sparsity . Similarly, SCNN  reported below 80\\% utilization of multipliers for all GoogLeNet  layers and 20\\% for the last two inception modules. Moreover, a few architectures use PE designs with multiple subunits in each PE. For SIMD processing, a subunit works in synchronization with other subunits of the same PE, e.g., in Cnvlutin , , and . With unstructured sparsity, multipliers and accumulators in some subunits can often be idle, while trailing subunits process computations.", "cites": [4355, 4342, 7169, 305, 4347, 4354, 7634, 7805, 4341], "cite_extract_rate": 0.391304347826087, "origin_cites_number": 23, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple cited papers to identify and categorize the sources of load imbalance (inter-PE and intra-PE) in sparse tensor computations. It abstracts the problem into general architectural and execution challenges, and provides concrete examples of performance impacts from specific systems. While it includes some critical observations on the limitations of synchronization and utilization in PEs, it could offer deeper comparative evaluations of the proposed techniques."}}
{"id": "2c592133-299e-499f-805e-2d476151c4f4", "title": "Software Directed Load Balance", "level": "subsection", "subsections": [], "parent_id": "ac4d8613-1421-4c83-a4a5-66deeb11b225", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Load Balancing of Effectual Computations"], ["subsection", "Software Directed Load Balance"]], "content": "\\mysubsubsection{Clustering of NZs for populating dense data regions} As described in section \\ref{sec::data-extraction-HW}, a few techniques targeted high $W$-sparsity. They used structured pruning or data combining approaches for clustering the tensor elements in locally dense regions that can be dispatched to PEs for processing in a conventional manner . Thus, they achieve high PE utilization and lower invocations to accelerator resources. However, such techniques may not be effective when algorithms cannot generate or pack structured sparse data (e.g., dynamic unstructured sparsity).\nConcise convolution rules (CCR)  partitioned sparse convolutions into effective and ineffective sub-convolutions for processing locally dense regions of filters and input feature maps. It eliminated a majority of ineffectual computations and their storage (for VGG-16, achieving reduction of about 79\\% and 51\\%, respectively) . Sub-convolutions after CCR transformation were executed on the SqueezeFlow accelerator . However, with PEs performing only all-to-all multiplications, it may not support generic tensor operations; it can be challenging to extend CCR methodology for other algorithms.   \n\\begin{figure}[!t]\n\\centering\n\\centerline{\\includegraphics[width=\\linewidth]{figures/NZ-load-imbalance}}\n\\caption{Distribution of NZ weights for executing CONV1 of AlexNet  with coarse weight stationary dataflow on a 4$\\times$3 PE-array. Distribution shows NZ weights in different workgroups (each workgroup contains NZs for 12 PEs): (a) without load balance (b) after sorting. (Figure inspired by .)}\n\\label{fig::NZ-load-imbalance}\n\\end{figure}\n\\mysubsubsection{Data reorganization before work allocation} In ZENA , each PE processed a different set of filters for processing a sub-workgroup. For balancing computations among these PEs, filters were sorted by sparsity and allocated to PEs such that all PEs executed filters of similar sparsity.\nTo determine the efficacy of such sorting, we considered AlexNet  for ImageNet classification. We obtained the pruned model through the neural network distiller  with a pruning algorithm similar to . For accelerating AlexNet  CONV1 layer with coarse weight stationary dataflow, Fig. \\ref{fig::NZ-load-imbalance} presents distributions of NZs in filters before and after reorganization. For processing 64 filters of size 3$\\times$11$\\times$11 on 4$\\times$3 PEs, we consider execution through 16 different workgroups. Each workgroup contains NZ weights for concurrent processing of four filters and three channels on 12 PEs (up to 11$\\times$11 on a PE). The next workgroup is initiated once all PEs entirely use previously allocated weights. Fig. \\ref{fig::NZ-load-imbalance}(a) shows that before data re-organization, the total NZ weights allocated to PEs within workgroups differed by up to 21.4$\\times$ (5 vs. 107 for 11$\\times$11 filters) and 6.09$\\times$ on average. Fig. \\ref{fig::NZ-load-imbalance}(b) shows that after sorting the weights (both filter-wise and input channel-wise), it leads to an almost equal number of NZs for computations onto 12 PEs during each workgroup. The total allocated NZ weights differed by only 1.36$\\times$. \nAfter static sorting, ZENA achieved about 20\\%--32\\% more acceleration for CONV layers of AlexNet and VGG-16 . Depending on the sparsity, distribution of NZs, and achievable sorting granularity, the work allocated to PEs may differ considerably even after sorting. Moreover, such transformations are usually feasible only statically. So, ZENA also used dynamic work sharing, which we discuss in section \\ref{sec::dynamic-load-balance}.\n\\mysubsubsection{Accelerator-aware regularization of the model} Recent accelerators, including Sparse Tensor Cores in NVIDIA Ampere architecture , , and , execute models pruned with $k$:$n$ block-sparsity (e.g., 2:4 sparsity supported by Ampere ). Their PEs contain multiplexers that use indices of $k$ NZ weights to select $k$ out of $n$ dense activations. Then, functional units process extracted values. Like $k$:$n$ block-sparsity, ESE  used a load-balance aware pruning for RNNs. It considered sub-matrices to be processed by different PEs and induced the same sparsity into all sub-matrices.\nIn some architectures, all PEs receive the same set of NZ activations. They process them with their unique weights and produce distinct output activations. One such architecture is Cambricon-S  which used a coarse-grained pruning of weights. The block size for pruning depends on the total number of PEs (16). Over local regions, the pruning removed all connections between an input activation and all (16) output activations. So, when PEs processed output neurons, they processed the same number of NZ input activations and weights for computing MACs.", "cites": [4374, 4347, 4373, 7169], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple approaches for software-directed load balancing, integrating ideas from CCR, ZENA, ESE, and Cambricon-S to highlight common strategies and their effectiveness. It provides some critical analysis by pointing out limitations, such as the static nature of sorting and challenges in extending CCR to generic operations. While it identifies patterns across works, the abstraction remains limited to the context of sparse tensor computations rather than broader principles."}}
{"id": "a52c92e6-ede5-4d3a-999b-a2c59bcadf17", "title": "Load Balancing with Hardware Structures", "level": "subsection", "subsections": [], "parent_id": "ac4d8613-1421-4c83-a4a5-66deeb11b225", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Load Balancing of Effectual Computations"], ["subsection", "Load Balancing with Hardware Structures"]], "content": "\\label{sec::dynamic-load-balance}\n\\mysubsubsection{Facilitating asynchronous computations by prefetching allocated work} One way to improve PE utilization (in the presence of load imbalance) is to prefetch the allocated work for PEs and avoid their fine-grain synchronization. So, even if there is a different amount of work (e.g., MACs per input activation), all the PEs may perform effectual computations at the same time (e.g., work on different activations). Thus, each PE can be engaged in performing some computations, before it runs out of the available data. This can be achieved by offloading more data into the FIFO or memory of each PE. For example, in EIE , activations are broadcast to FIFOs of all PEs. Once a PE finishes multiplying an activation to corresponding NZ weights or does not find any matching weights, it processes the next activation from its queue. FIFO size of 8 or higher ensured each PE almost having an NZ activation to process (during 87\\%--91\\% of computation cycles) and lowered idle time of PEs from about 47\\% to 13\\% . \nCambricon-X  allows asynchronous communication of weights to PEs. A centralized data extraction mechanism provides NZ activations to each PE via a unicast network, and compressed weights are loaded in the memory of each PE (2 KB). The memory access port is assigned to each PE for a short period, where it fetches several chunks of weights via DMA transfer. Depending on the prefetching interval and unstructured sparsity, each PE may asynchronously work on useful computations in most of the execution cycles. \nWhile asynchronous execution improves the utilization of PEs, the work allocated to PEs is still fixed. Plus, in-PE data fetching mechanisms may restrict PEs from finding the pending work in other PEs and sharing it. For highly imbalanced computations, straggling PEs can still be the bottleneck.\n\\mysubsubsection{Centralized load balance} In some accelerators, data is multicast to one or more rows (or columns) of PEs. A central logic processes the metadata (indices) of the tensor tiles to be distributed along with control signals from PE-rows and finds out work distribution. Then, it feeds the fast-acting rows/lanes of PEs and facilitates work sharing. For instance, ZENA  allocates work dynamically through down counters. Different PE-groups (e.g., PE-rows) process the same filters with different activation tiles. A central distribution mechanism contains down counters that store the number of remaining activation tiles for each PE-group. When a leading PE-group finishes its work (counter is zero), it obtains an activation tile from a straggling group (has the biggest count value) and then continues processing output activations. The work sharing improved acceleration by about 10\\% for CONV layers of AlexNet and VGG-16 . Memory port contention may occur when multiple leading groups simultaneously attempt to fetch the same set of input activation tiles. ZENA's execution mechanism overcomes this problem by reassigning only one activation tile at a time (to the leading group) and performing reassignments only during bus idle time.\n\\begin{figure}[!t]\n\\centering\n\\centerline{\\includegraphics[width=\\linewidth]{figures/load-balance-LNPU}}\n\\caption{Load balance mechanism in LNPU  (Figure adopted from ).}\n\\label{fig::load-balance-LNPU}\n\\end{figure}\nLNPU  uses an input load balancer (ILB) which is shared among PE-rows. As Fig. \\ref{fig::load-balance-LNPU} shows, ILB contains address generator units to determine the indices of the compressed elements that need to be fetched. Once ILB fetches them, the skip-index decoder unit determines the appropriate indices for data extraction. It pushes them along with the NZ values into the FIFO of a PE-row. It also calculates bitmaps, which are used for pushing the data (indices and NZs) selectively into FIFOs of PE-rows at run time. Due to ILB, PE utilization in LNPU was increased by 2\\%--26\\% for 10\\%--90\\% sparsity of the inputs (activations or their gradients) . Thus, centralized load balancing mechanisms can leverage the information about data allocation for PEs and provide equal work to PEs or feed the fast-acting PEs during run-time.", "cites": [7634], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key approaches from EIE, Cambricon-X, and LNPU to present a coherent narrative on how hardware structures can be used for load balancing in sparse tensor computations. It also provides some critical analysis by pointing out limitations, such as fixed work allocation and memory port contention. The discussion generalizes the mechanisms to highlight broader principles like dynamic work distribution and PE utilization, but it remains somewhat focused on specific implementations rather than offering a fully meta-level abstraction."}}
{"id": "a6e6350a-7c3d-4517-bdb5-11afa0cc8a48", "title": "Optimization Opportunities", "level": "subsection", "subsections": [], "parent_id": "ac4d8613-1421-4c83-a4a5-66deeb11b225", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Load Balancing of Effectual Computations"], ["subsection", "Optimization Opportunities"]], "content": "\\textit{(i) Software-level or hardware/software/model co-design optimizations for low-cost load balance:} Most accelerators lack special support to balance computations among PEs, e.g., to avoid area and power overheads due to special hardware logic. (a) One technique is to reorganize the data . But, it can mostly exploit only static $W$-sparsity for inference at no/low hardware cost. So, we may require additional co-design optimizations for regularizing dynamic sparsity. (b) For pre-known or estimated sparsity, sparsity-aware mapping optimizations for accelerators may identify efficient dataflows that sustain high PE utilization. (c) When sparsity may be regularized at modest accuracy loss (e.g., for several DNNs), accelerator/model co-designs can induce the balance. It can be either done via structured pruning of activations or refactoring the operators (nonlinear activations, batch normalization , or quantization of outputs). Consequently, the co-designs may achieve structured computations over both activations and weights to an extent, leading to further accelerations.", "cites": [8767], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of optimization opportunities for load balancing in sparse and irregular tensor computations, drawing from a single cited paper (MASR). It integrates the paper's theme of handling dynamic sparsity into a broader architectural and co-design context. While it does not deeply compare or critique multiple works, it identifies general strategies and their limitations, showing some abstraction toward principles like structured pruning and operator refactoring for improved PE utilization."}}
{"id": "d451769e-a134-445a-a7af-73c10cbbd92a", "title": "Write-Back and Post-processing", "level": "section", "subsections": ["d7a904a1-024e-4031-87c5-cfa5487bd195", "70747015-64a2-4214-9646-0e7ae4610e46", "cff1bf65-03c5-45a4-b414-fd42cc60602a", "7e34c4f1-3645-438c-95fb-7faf7ec757a7"], "parent_id": "6220d89f-a486-420a-914e-59957895a313", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Write-Back and Post-processing"]], "content": "\\label{sec::post-processing}\nOnce PEs process allocated data, they write (partial) outputs back via interconnect. For unstructured sparsity, managing \\textit{write-backs (WBs)} can be challenging because different PEs can produce outputs of different sizes at different times. Moreover, operations like ReLU, pooling, and batch-normalization need to be performed on outputs. They are usually not performance-critical like CONV or MLP. So, they can be either executed on PEs before WBs of outputs (in SCNN , Cambricon-S , and EIE ) or post-processed on central modules (in MAERI , ZENA , and SqueezeFlow ). Central modules often assemble the outputs collected from PEs, transform data for the next layer, and encode sparse outputs on the fly.", "cites": [4354, 7634], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of write-back and post-processing strategies in hardware accelerators, mentioning a few systems like SCNN, EIE, and MAERI. It integrates some information from the cited papers but lacks deeper synthesis or a structured framework. There is minimal critical analysis or identification of broader principles or trends."}}
{"id": "d7a904a1-024e-4031-87c5-cfa5487bd195", "title": "Write-Back from PEs", "level": "subsection", "subsections": [], "parent_id": "d451769e-a134-445a-a7af-73c10cbbd92a", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Write-Back and Post-processing"], ["subsection", "Write-Back from PEs"]], "content": "\\label{sec::write-back}\n\\mysubsubsection{Simultaneous WB} Cambricon-X  and SCNN  use fat-tree networks or point-to-point links, which allows simultaneous WBs from multiple PEs. Whenever ready, PEs can execute in a dataflow manner and immediately write outputs back after computations. This is important for processing unstructured sparsity because different PEs may process a different number of NZs and produce different amounts of output values for WB at different time intervals. With such high bandwidth, communication time can be reduced and interleaved with computations, which is important for processing models with low arithmetic intensity. These PEs write to a central module for post-processing (e.g., in Cambricon-X ), the on-chip memory , or off-chip memory (e.g., in SCNN ). Although simultaneous WBs are faster, such a fat-tree network can incur considerable overhead due to increased bandwidth and inefficient bandwidth utilization in some scenarios. So, accelerator designs can instead use a common bus that is time-shared among multiple PEs; PEs can write the data back turn-wise or asynchronously.\n\\mysubsubsection{Sequential WB} PEs in several accelerator designs operate in a lock-stepped manner, where data blocks common to PEs are broadcast to them, and all PEs synchronize for processing the outputs (idle when done). Synchronized execution can allow WB in a specific sequence (e.g., a PE with the lowest PE-index writes the data first and so forth). It makes the programming of the accelerator easier. It also obviates overheads of specialized hardware/software support, which is required otherwise for asynchronous WB.  \n\\mysubsubsection{Asynchronous WB} With unstructured sparsity, PEs process a different amount of data and can asynchronously request WB during the execution. For facilitating such support, accelerator designs can employ additional hardware logic. For example, ZENA  used a common bus for multicasting blocks of filters and feature maps to PEs and collecting the output. Output buffers of PEs were flushed to the memory during the idle period of the bus, which avoided bus contention between broadcasting activations from memory and WB of partial summations. For prioritizing the requests from PEs to access the bus for WB, it determined the PE groups with a high number of pending output tiles.", "cites": [4354], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes information from multiple cited works, integrating different write-back strategies (simultaneous, sequential, asynchronous) and their implications on performance and design. It provides critical analysis by discussing trade-offs such as bandwidth overhead and efficiency, and it abstracts beyond specific papers to highlight general design principles and challenges in handling sparse and irregular tensor computations."}}
{"id": "70747015-64a2-4214-9646-0e7ae4610e46", "title": "Data Assembling", "level": "subsection", "subsections": [], "parent_id": "d451769e-a134-445a-a7af-73c10cbbd92a", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Write-Back and Post-processing"], ["subsection", "Data Assembling"]], "content": "PEs often process large output tiles. So, they perform fine-grained assembling of outputs locally. For example, SCNN  PEs use a coordinate computation unit that determines appropriate indices for arbitrating partial outputs to the local accumulator buffer. In other accelerators, PEs produce metadata and supplies it with outputs for correctly indexing the memory (e.g., in ZENA ) or assembling outputs on a central module (e.g., in Cambricon-X , CoNNA ). The central module uses the metadata (e.g., output coordinates) from PEs or pre-known indices of PEs to assemble collected outputs before WB or post-processing. In some designs, data assembling is done by global accumulators that reduce partial summations and update outputs into appropriate memory banks (e.g., SNAP ). The data assembling logic typically also handles data layout transformation (e.g., in ), which is required for processing the subsequent layer.", "cites": [4354], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of data assembling techniques in different accelerators, citing specific examples such as SCNN, ZENA, and SNAP. While it connects a few sources, it lacks deeper synthesis or critical evaluation of these approaches. It does not generalize to broader principles or trends, focusing instead on individual system components."}}
{"id": "cff1bf65-03c5-45a4-b414-fd42cc60602a", "title": "Data Layout Transformations", "level": "subsection", "subsections": [], "parent_id": "d451769e-a134-445a-a7af-73c10cbbd92a", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Write-Back and Post-processing"], ["subsection", "Data Layout Transformations"]], "content": "\\mysubsubsection{Data reorganization} Accelerators are often designed for efficient vector or matrix multiplications. So, for processing convolutions, they (e.g., ) require data layout in \\textit{NHWC (channels-first)} format , which is also used for processing on CPUs and GPUs. Fig. \\ref{fig::data-layout-xform}(b) shows data reorganization for striding execution of the convolution of Fig. \\ref{fig::data-layout-xform}(a). It shows iterative processing of the spatial data with channels-first processing. For example, an output activation $1A$ can be processed by fetching a block containing all channels of the first filter and ifmap. Vectors corresponding to channels can be processed iteratively. Sparse data blocks are also processed similarly but with computations on appropriate NZs.\n\\begin{figure}[t]\n\\centering\n\\centerline{\\includegraphics[width=\\linewidth]{figures/data-layout-xform}}\n\\caption{Data layout transformation for executing convolution. (a) Convolution of two 2$\\times$3$\\times$3 feature maps with two 2$\\times$2$\\times$2 filters. (b) Reorganizing data for striding execution. (c) Transforming feature maps into Toeplitz matrix.}\n\\label{fig::data-layout-xform}\n\\end{figure}\n\\mysubsubsection{Transformations to Toeplitz matrix} Processing with NHWC format allows executing CONVs as iterative vector-vector multiplications, but it requires hardware support to fetch appropriate blocks. So, for processing CONVs as sparse GEMMs, a few accelerators, including ERIDANUS  and , transform sparse feature maps into Toeplitz matrix with \\textit{im2col} transformation .Once transformed matrices are obtained and sparsity-encoded, accelerators compute sparse matrix multiplication. Fig. \\ref{fig::data-layout-xform}(c) illustrates the transformation for tensors of Fig. \\ref{fig::data-layout-xform}(a). It shows that neighborhood values for computing an output with a 2D CONV are combined as a vector. For multiple input channels of ifmaps or filters, corresponding elements are stacked in column-vectors or row-vectors. However, transforming ifmap into Toeplitz matrix duplicates neighborhood data and yields storage overhead (Fy$\\times$Fx times higher memory for unit-strided CONV).", "cites": [2671], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section describes data layout transformations for convolutional operations, referencing some papers but not in depth. It provides a basic explanation of NHWC format and im2col transformation but lacks critical evaluation or synthesis of ideas across multiple works. The discussion remains at a concrete level without abstracting broader principles or trends."}}
{"id": "0648e6ac-e21a-4f45-9a3c-833f135b255f", "title": "Intermediate Representations", "level": "subsection", "subsections": [], "parent_id": "cce74b6f-9cb1-4065-8a80-43648099f245", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Compiler Support"], ["subsection", "Intermediate Representations"]], "content": "IR determines which types of code can be represented by the compiler, whether it can support sparse tensor computations, the types of code transformations that can be done, and even the scalability of the compiler. \n\\mysubsubsection{Need for high-level representations} A common example of low-level IR is LLVM IR which is well suited for low-level code optimizations such as register allocation but not for many high-level code optimizations needed for optimizing sparse deep learning. This is mainly because low-level IRs do not preserve information about loop structures and data layouts, and reconstructing such information is not trivial . That is why many deep learning compilers such as TVM , Tiramisu , and Halide  apply many code optimizations on a high-level IR (an IR that has loops and represents multi-dimensional tensors). This is also one of the motivations for creating MLIR , which serves as a high-level IR for low-level compilers like LLVM.\n\\mysubsubsection{Mathematical abstractions of code} While previous IRs have focused on representing program statements and program structure, many compilers use an additional mathematical representation (abstraction) to represent iteration domains\\footnote{The iteration domain of loop iterators in a loop is all possible values that loop iterators can take.} and array accesses of statements. These mathematical representations are usually used in conjunction with the IR to simplify iteration domain and array access transformations. This subsection presents two major families of mathematical representations and compares their strengths and weaknesses. \n\\myparagraph{2.A. Polyhedral representation.}\nIt is a unified mathematical representation for the iteration domains of statements, code transformations, and dependencies. It relies on two main concepts: \\emph{integer sets} and \\emph{maps}. \\emph{Integer sets} represent iteration domains. \\emph{Maps} are used for representing memory accesses and transforming iteration domains and memory accesses.\nAn integer set is a set of integer tuples described using affine constraints.  An example of a set of integer tuples is \\polyc{$\\{(1,1); (2,1); (1,2); (2,2); (1,3); (2,3)\\}$}\nInstead of listing all the tuples, we can describe the set by using affine constraints over loop iterators and symbolic constants as follows: \\polyc{$\\{S(i,j): 1 \\leq i \\leq 2 \\wedge 1 \\leq j \\leq 3\\}$} where $i$ and $j$ are the dimensions of the tuples in the set.\nA map is a relation between two integer sets.  For example,\n\\polyc{$\\{S1(i,j) \\rightarrow S2(i+1,j+1) : 1 \\leq i \\leq 2 \\wedge 1 \\leq j \\leq 3\\}$}\nis a map between tuples in the set S1 and tuples in the set S2. More details about the polyhedral model and formal definitions can be found in .\n\\myparagraph{Polyhedral compilers:}\nNotable polyhedral compilers for deep learning include Tiramisu , Tensor Comprehensions , Diesel , and TensorFlow XLA  (through affine MLIR dialect ). General-purpose compilers that support deep learning include PENCIL , Pluto , Polly , PolyMage , AlphaZ , and CHiLL .\n\\myparagraph{Strengths of the polyhedral representation:}\n\\begin{itemize}[leftmargin=*]\n    \\item Unified representation: It eliminates friction within compiler IRs and greatly simplifies design of code transformations.\n    \\item Instance-wise representation: The representation granularity is instances of statement executions where each instance is a single execution of a statement during one loop iteration. Instance-wise representation includes iteration domains, data dependencies, data accesses, and code transformations, which allows the compiler to have a precise representation. \n    \\item Support for the whole class of affine transformations: It allows applying any affine transformation on iteration domain and data accesses. An example of a complex affine transformation is  iteration space skewing, which allows extracting parallelism from multi-layer recurrent neural networks to increase hardware occupancy.\n    \\item Non-rectangular iteration domains: The representation allows compilers to naturally express non-rectangular iteration domains (i.e., iteration domains with an affine conditional).\n\\end{itemize}\n\\myparagraph{Weaknesses of the polyhedral representation:}\n\\begin{itemize}[leftmargin=*]\n    \\item Limited support for non-affine code: The polyhedral model mainly represents code and transformations using sets and maps described using affine constraints. So, it does not naturally support code that leads to non-affine constraints. This includes code with non-affine loop bounds, non-affine array accesses, and non-affine conditional. While the classical polyhedral model does not support non-affine constraints, recent work has extended the polyhedral representation to support non-affine array accesses, non-affine loop bounds, non-affine conditionals , and parametric tiling . The efficiency of these techniques has been demonstrated in practice by PENCIL  and Tiramisu .\n    \\item Slower compilation: While polyhedral operations are precise, they are computationally expensive. So, polyhedral compilers are slower than non-polyhedral compilers. Recent techniques reduce the number of statements by clustering groups of statements into macro-statements and scheduling macro-statements instead of individual statements , reducing the compilation time notably. \\\\\n\\end{itemize} \n\\myparagraph{2.B Non-polyhedral representation.} A common non-polyhedral representation used in deep learning compilers is \\emph{interval}-based representation. It uses intervals and interval arithmetic to represent iteration domain and code transformations, respectively. Using intervals, N-dimensional loops are represented with N-dimensional boxes, e.g., iteration domain of a loop nest can be represented as: $(i,j) \\in$ ([0, N],[2, M-2]). \n\\myparagraph{Non-polyhedral DNN compilers:}\nTheir examples include TVM , Halide , DLVM , and Latte .\n\\myparagraph{Strengths of {interval}-based representations:}\n\\begin{itemize}[leftmargin=*]\n    \\item Better support for non-affine code: Non-polyhedral compilers can naturally support non-affine code transformations such as parametric tiling (loop tiling with parametric tile size). This is because the interval-based representation does not rely on using affine sets and affine relations to represent the code or dependencies. However, non-polyhedral compilers also have limited support for non-affine code (e.g., indirect memory accesses) and code transformations.\n    \\item Faster compilation: Operations on intervals are computationally less expensive than polyhedral equivalent operations on sets of integer points, which yields faster compilation.\n\\end{itemize}\n\\myparagraph{Weaknesses of {interval}-based representations:}\n\\begin{itemize}[leftmargin=*]\n    \\item Limited expressiveness: Interval-based non-polyhedral compilers cannot naturally represent non-rectangular iteration spaces (e.g., when bounds of loop iterators depend on a condition). It is also hard to perform certain complex affine transformations such as iteration space skewing. \n    \\item Lack of support for programs with cyclic data-flow graphs: To simplify checking the legality of a schedule, many interval-based compilers assume that the program has an acyclic dataflow graph. This prevents users from expressing many programs with cyclic dataflow. For example, when a value produced by a loop is read by another loop, Halide  does not allow fusion of the two loops (with \\texttt{compute\\_with} command). While it avoids illegal fusion, it prevents legal loop fusions in common cases. Polyhedral compilers avoid these over-conservative constraints by using dependence analysis  to check for the correctness of code transformations, which enables more schedules. While interval-based compilers can also implement non-polyhedral dependence analysis (by computing dependence distance vectors ), it is not as precise as polyhedral dependence analysis .\n\\end{itemize}", "cites": [4377, 882, 4376, 4375], "cite_extract_rate": 0.18181818181818182, "origin_cites_number": 22, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the key ideas from multiple cited papers to build a coherent narrative on high-level and mathematical IR representations in deep learning compilers. It critically compares polyhedral and non-polyhedral approaches, highlighting their strengths and weaknesses with specific examples and references to real systems. The abstraction is strong, as it generalizes these representations into broader categories and identifies trade-offs and limitations that apply across different compiler designs."}}
{"id": "fc3d1662-53d4-4119-840c-a54a80c25327", "title": "Support for Sparse Tensors", "level": "subsection", "subsections": [], "parent_id": "cce74b6f-9cb1-4065-8a80-43648099f245", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Compiler Support"], ["subsection", "Support for Sparse Tensors"]], "content": "\\mysubsubsection{Challenges in supporting sparse tensors}\nWhile compiler support is needed in general for targeting ML hardware accelerators with diverse features, sparse tensor computations with various dataflows especially need further support. The code for manipulating sparse tensors exhibits \\insight{non-static loop bounds, non-static array accesses, and conditionals}, which are difficult to analyze at compile time. The following pseudo-code shows one example of a direct convolution with sparse tensors (bounds of $j$ and accesses of $in$ are non-static).\n\\begin{lstlisting}\nfor each output channel c_o\n  for j in (W.row_ptr[c_o], W.row_ptr[c_o + 1])\n  {\n    coeff = W.value[j]\n    offset = W.col_idx[j]\n    for y in (0, out_H)\n      for x in (0, out_W)\n        out[c_o][y][x] += coeff*in[y*out_W+x+offset]\n  }\n\\end{lstlisting}\n\\mysubsubsection{DNN compilers supporting sparsity}\nTheir examples include Tiramisu , Acorns , and Taichi .\nTiramisu supports $W$-sparsity by extending the polyhedral model in a way similar to . For example, a non-affine conditional is transformed into a predicate that is attached to computation. The list of accesses of the computation is the union of the accesses of the computation in the two branches of the conditional, which is an over-approximation. During code generation, a pre-processing step inserts the conditional back into generated code. Non-static loop bounds and tensor accesses are represented as parameters in the polyhedral model. Statements that define those parameters are inserted just before the original statements that have non-static code. These techniques introduce approximations in the compiler. Their efficiency was demonstrated by  and confirmed by PENCIL  and Tiramisu .\nAcorns  optimizes the CNNs with \\textit{IA}-sparsity. It fuses operators in a computation graph of a deep CNN, followed by sparse layout conversion (which ensures that dense/sparse tensors produced by each operator are compatible with the next operation), followed by code optimization and code generation. Acorns introduces a data layout for exploiting the sparsity structure of input data in certain domains (face detection, LiDAR, etc.) where only certain data regions are NZs. For code optimization and generation, the compiler processes a set of template codes for CNN operators (e.g., convolution, pooling) and applies optimizations such as loop tiling, vectorization, and weight packing. It does not implement advanced loop-nest optimizations like iteration space skewing.\nTACO  uses a specific representation (iteration graphs) to generate code for sparse tensor operations and uses a scheduling language to guide the code optimization.", "cites": [4375], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from Tiramisu, Acorns, and TACO, connecting their compiler-level strategies for handling sparse tensors. It provides some abstraction by highlighting common challenges like non-static loop bounds and conditionals, but the critical analysis is limited to mentioning Tiramisu's use of over-approximations and Acorns' omission of advanced loop optimizations. The section offers an insightful overview but lacks deeper evaluation or a novel integrative framework."}}
{"id": "5cba33c7-04e8-4faa-a124-3d6bfae52a69", "title": "Compiler Optimizations", "level": "subsection", "subsections": [], "parent_id": "cce74b6f-9cb1-4065-8a80-43648099f245", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Compiler Support"], ["subsection", "Compiler Optimizations"]], "content": "To generate efficient code for NN operators, a compiler has to apply a large set of complex code optimizations. It includes operator fusion; multi-level tiling and register blocking which improve data reuse; loop reordering, array packing  and data prefetching; loop skewing which enables the extraction of wavefront parallelism from multi-layer RNNs; parallelization; loop unrolling; vectorization; full/partial tile separation; tuning optimization parameters to the target architecture (e.g., tile sizes or loop unrolling factors). There are two major families of optimizing compilers: compilers that allow semi-automatic code optimization and fully automatic compilers.\n\\mysubsubsection{Compilers with semi-automatic code optimization (scheduling languages)}\nThe main idea in these compilers is to separate the algorithm from optimizations. A program, in this case, has two parts: The first part specifies the algorithm without specifying how it is optimized. The second part specifies how the algorithm is optimized (transformed). This is done through a set of high-level scheduling commands for common optimizations. Halide , Tiramisu , and TVM  are examples of compilers that allow semi-automatic optimization. The main advantage of this approach is it allows a user to have full control over how code should be optimized. This is important because fully automatic optimization techniques do not always succeed in providing the best performance.\nSemi-automatic deep learning compilers usually provide a library of highly optimized deep learning operators. The compiler then only needs to decide automatically whether to apply certain optimizations such as operator fusion. All other optimizations are encoded manually in the library using scheduling commands. This minimizes the number of decisions that the compiler needs to make and thus guarantees the best possible performance. Note that semi-automatic compilers usually also have automatic optimization modules, but such modules can be disabled if necessary.\n\\mysubsubsection{Fully automatic compilers} Tensor Comprehensions  and Diesel  are examples of fully automatic compilers for deep learning. Other examples of fully automatic compilers include PENCIL , Pluto , and Polly . All of them use Pluto  algorithm to automatically optimize code (choosing the schedule of statements). The main idea of Pluto algorithm is to use integer linear programming to model the problem of automatic code optimization where constraints are dependencies of the program and the objective function is the minimization of the distance between producer and consumer statements. Other compilers such as PolyMage  use a custom algorithm for automatic optimization.\nAll these compilers do not have a scheduling language and therefore do not allow the user to have fine-grain control over optimizations. Although fully automatic compilers provide productivity, they may not always obtain the best performance. Performance can be sub-optimal because they do not have a precise cost model to decide which optimizations are profitable. For instance, the Pluto  algorithm does not consider the redundant computations, data layout, or the complexity of the control flow of generated code.\n\\myparagraph{Cost models for automatic code optimization:} The goal of an automatic code optimization pass in a compiler is to find the best combination of code optimizations that minimizes the execution time. This can be modeled as a search problem where the search space is a set of combinations of code optimizations. Then, the compiler needs a search technique and a cost model to evaluate each combination. Classical compilers use hand-tuned cost models , while others use machine learning to build cost models . Both of these models do not precisely capture hardware complexity (different memory hierarchies, out-of-order execution, hardware prefetching, communication latency, etc.). Instead, state-of-the-art models are built using deep learning for better accuracy . For example, Ithemal  is a cost model that predicts the throughput of a basic block of x86 instructions and gets less than half the error of state-of-the-art hand-tuned models (llvm-mca in LLVM  and Intel’s IACA).", "cites": [4375, 4378, 882], "cite_extract_rate": 0.2, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the cited papers by grouping them into two compiler categories and highlighting their design philosophies. It provides a critical analysis of the trade-offs between semi-automatic and fully automatic compilers, noting performance limitations of the latter, particularly with the Pluto algorithm. The abstraction is strong as it generalizes the use of scheduling languages and cost models, and even discusses how machine learning can improve cost modeling across hardware."}}
{"id": "4879cb49-5aea-4686-ba02-9effd2aedd5d", "title": "Accelerator ISAs and Code Generation", "level": "subsection", "subsections": [], "parent_id": "cce74b6f-9cb1-4065-8a80-43648099f245", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Compiler Support"], ["subsection", "Accelerator ISAs and Code Generation"]], "content": "Accelerators, such as Cambricon-X , Scaledeep , Thinker , and DnnWeaver , expose a high-level ISA where some instructions perform tensor operations (e.g., dot product, matrix-matrix multiplication, convolution, pooling, and sigmoid). They simplify the compiler's  mission because it can invoke high-level operations instead of generating and optimizing a low level-code. However, it still has to manage data copies automatically. This subsection describes such high-level ISAs used by accelerators and machine code generation.\n\\mysubsubsection{Instruction sets} For tensor computations on hardware accelerators, ISAs typically feature instructions for arithmetic, logical, and data transfer operations with matrix, vector, and scalar data. Layers of ML models feature loops iterating thousands of times; dynamic instances of repetitive instructions can significantly increase the bandwidth requirements for delivering them to PEs at each cycle and the energy consumption. To mitigate such overheads, accelerators are designed with an array of vector or SIMD PEs. It allows PEs to process a single instruction for performing multiple computations on the blocks of tensors. Alternatively, PEs contain additional control logic such that they process an instruction once, but repeatedly perform the sequence of execution for a certain interval. \nCambricon ISA for machine learning  contains instructions for matrix and vector processing with arithmetic and logic operations, control (conditional branch and jump), and data transfer. Each operand of the instruction is either an immediate value or one of the 64 32b general-purpose registers. The registers are used for temporarily storing scalars or register-indirect addressing of the on-chip scratchpad memory. The tensor blocks are communicated between computational units from the on-chip scratchpad that is transparent to the compiler and programmers. The instructions support commonly used primitives in various ML models, e.g., multiplication, addition, subtraction, and division operations on matrices and vectors. It also supports max-pooling with a vector-greater-than-merge instruction and provides dedicated instruction for random vector generation with uniform distribution of values within [0, 1]. For supporting weight update during the training of DNNs, Cambricon provides additional instructions such as outer product, scalar-matrix multiplication, and matrix-matrix addition. However, it lacks support for managing data in the local memory of PEs and configuring NoC for communication in various dataflows. Moreover, it does not provide specific instructions for handling sparsity, e.g., predicated execution of encoded sparse data. \nThe instruction set for Sticker  consists of instructions for high-level operations. For processing each layer, one of the instructions is executed only once. It configures instruction registers and common control signals that correspond to the sparsity levels and tensor dimensions. Then, at a certain time interval, a dynamic 32b instruction executes for computing convolution over data blocks on PE-array. Meanwhile, the accelerator controller distributes the next instruction, if there is no collision between the current and the next instruction. It allows hiding the execution of other dynamic instructions including the write-back and encoding of outputs and transferring data between on-chip and off-chip memory.\n\\mysubsubsection{Finite state machines (FSMs)} Some accelerators use FSMs for PE executions. The parameters of FSMs or PE's control logic correspond to tensor shapes and target functionality, and they are configured once (e.g., through bit-streams ) before executing a model or a layer. Accelerator controllers (which usually initiate the data movement between on-chip and off-chip memory and configure PEs and NoC) can also contain FSMs. For example, in Thinker architecture , a finite-state controller is used for configuring the accelerator at three levels, i.e., PE-array level, model layer level, and PE level. Configuration word for PE-array level handles partitioning of the PE-array, and it points to the memory address of configurations for model layers. Each configuration word for a layer contains information about tensor dimensions and their memory addresses. Lastly, layer configurations for PEs correspond to PE functionality and the interval (loop iterations) of computations and idle time.\n\\mysubsubsection{Library support and code generation} The instructions for cycle-level executions or primitives are usually obtained offline. Accelerator system designers often provide users a template library that defines high-level primitives such as model layers or low-level primitives such as vector/matrix operations. Using these primitives, users can construct the model of their interest. Then, the low-level code is obtained automatically by the compiler or using the pre-defined optimized code . For example, Zhang et al.  programmed Cambricon-X accelerator with a set of library functions (written in C/C++) for primitives like convolution and matrix/vector multiplication and addition. Chen et al.  proposed a programming framework consisting of assembly language, an assembler, and run-time support for executing ML models with their Cambricon ISA. For executing common layers, it also replaced the primitives with pre-defined code blocks. \nTVM  supports defining custom back-ends for accelerators, which was demonstrated using a vanilla accelerator with a matrix-multiply engine. For executing primitives on accelerators, TVM enables Tensorization , i.e.,  decoupling the target hardware intrinsic from the schedule while mapping ML operators. To demonstrate code generation for the vanilla accelerator, TVM enabled a driver library and runtime support that constructs the instructions and offloads them to the accelerator. Its code generation module translated the program into appropriate function calls of the runtime API. Moreau et al.  leveraged the TVM stack and proposed a JIT compiler and a runtime system to generate code for a programmable VTA accelerator. \nIt is important that the accelerator can support multiple front-ends corresponding to different ML frameworks such as TensorFlow , PyTorch , and MXNet . Integration of the programming, compilation, and runtime environment with the common frameworks for ML application development is necessary for supporting different compact ML models. Leveraging the existing system stack (e.g., TVM) can provide such opportunities to accelerator system developers. Note that although TVM supports defining custom accelerator back-ends and can lower optimized mappings to accelerator-specific code, it currently does not provide support for sparse tensors.", "cites": [3416, 7157, 41, 4379], "cite_extract_rate": 0.3076923076923077, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.5, "abstraction": 3.3}, "insight_level": "medium", "analysis": "The section synthesizes key aspects of instruction sets, FSMs, and library/code generation from multiple cited papers, forming a coherent narrative about compiler support for sparse and irregular tensor accelerators. It provides some critical insights, such as limitations in handling sparsity and local memory management. While it identifies patterns in how different accelerators manage control and execution, it lacks a more meta-level abstraction that could unify the principles across all cited works."}}
{"id": "6c2f34b5-5f91-4599-987d-4e0d10ea32a6", "title": "Hardware/Software/Model Co-designs", "level": "subsection", "subsections": [], "parent_id": "4b09d196-2359-424f-bc4b-e7c987fe6803", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Trends and Future Directions"], ["subsection", "Hardware/Software/Model Co-designs"]], "content": "\\mysubsubsection{Hardware-aware compression techniques} The framework for exploring efficient model compression (either of quantization, pruning, and size reduction) should be aware of hardware features and provide directed search accordingly. For example, bit-widths of tensors that can be efficiently processed by different hardware platforms vary considerably (e.g., from multiples of 8-bits to arbitrary bit-widths). Accelerators typically support only uniform widths of tensors (activations and weights), and many accelerators do not support value sharing. Also, when hardware only supports widths that are multiple of 4 or 8 bits, quantization with other bit-widths requires zero paddings, which incurs inefficient storage and processing. Instead, the compression algorithm can opt for improving the accuracy, increasing sparsity, or trading off the bit-widths among layers for achieving higher compression and acceleration. Similarly, depending on the hardware support for fine-grained or block-sparsity, hardware-aware pruning can better achieve the compression objectives (model exploration time, performance, and energy efficiency, while meeting target accuracy). Efficiency can be further improved when compression techniques leverage execution models of hardware accelerators (e.g., energy-aware pruning ). Relatively simple logic modules of hardware accelerators have enabled recent techniques to estimate execution metrics through analytical cost models. Accommodating such cost models (including for different sparsity levels/patterns, precisions) enables the compression algorithms to select effective pruning ratios/structures, tensor shapes, and tensor precisions, which can help to achieve desired accelerations.     \n\\begin{figure}[!t]\n\\centering\n\\centerline{\\includegraphics[width=0.75\\linewidth]{figures/codesign}}\n\\caption{Co-designs can enable efficient accelerations of compact models.}\n\\label{fig::codesign}\n\\end{figure}\n\\mysubsubsection{Joint and automated exploration of sparsity, precision, and value similarity} Recent compression techniques typically employ structured or fine-grained data pruning during training with a fixed precision of tensors. Techniques for adaptive quantization often do not explore pruning. Joint explorations of pruning and quantization may achieve high compression due to the interplay of these compression mechanisms. For instance, quantization can increase sparsity considerably , as more values can be represented as zero after compressing the range . Likewise, pruning may reduce bit-widths further since fewer non-zero values in the pruned model may be expressed with a much lower numeric range and precision. Moreover, such compression techniques do not leverage temporal and spatial value similarity in inputs, outputs, or weights. So, joint exploration algorithms may be developed that use multiple compression strategies during training and automatically explore combinations that compress the model further. Recent techniques for automated explorations include CLIP-Q , , and . Exploring a wide range of compression combinations during the training may not be feasible. Therefore, model designers may reduce the space of compression choices by limiting effective options before beginning resource-extensive training, and if required, further limiting the search space by quick evaluations with a pre-trained model and fine-tuning. \nCompression benefits achieved through joint explorations need to be translated into efficient hardware accelerations. So, the exploration heuristic should not preclude experts from expressing a directed search for hardware-friendly executions, e.g., specifying pruning with 1D or $k$:$n$ block-sparsity, constraints for bit-widths, tolerable accuracy loss, etc. Moreover, the heuristic should also provide automated optimization/exploration of hyperparameters (including using cost models of accelerators). This is because the compression algorithm needs to adjust the strategy of pruning or quantization and its hyperparameters. For instance, the pruning algorithm needs to find out the pruning ratio for each iteration (epoch); pruning mechanism (which values to prune, e.g., below a certain threshold); pruning pattern (fine-grain, block size); bit-widths of tensors (quantization). All such hyperparameters or strategies need to be adjusted automatically (to an extent) such that the memory footprint or computations are greatly reduced, with no or tolerable accuracy loss.\n\\mysubsubsection{Value-aware neural architecture search (NAS) and accelerator/model co-designs} Techniques for NAS or AutoML can automatically obtain efficient models that surpass the accuracy of models devised by human developers. However, there remains scope for considerably improving NAS for obtaining highly compact models. Recent techniques  have explored accelerator/model co-designs that support quantized models and layers of different shapes. However, the efficiency can be further amplified by including the sparsity and adaptive bit-widths of model layers and analytically considering their implications on hardware accelerators. \nA major challenge faced by the model search techniques and automated accelerator/model co-designs is the vast search space. As Fig. \\ref{fig::codesign} shows, explorations can be performed for (i) ML models (i.e., NAS) , (ii) compression strategies (e.g., automated pruning and quantization) , (iii) mappings of models on accelerators , and (iv) specifications of hardware accelerators . The explorations of (i) and (ii) directly impact compression and accuracy, while search optimizations for (iii) and (iv) affect the performance and energy-efficiency of the accelerator for given models. Among these exploration spaces, NAS can be significantly time-consuming (several GPU days ), followed by automated model compression (e.g., ). Therefore, the resultant joint space for value-aware NAS and accelerator/model co-designs is many-folded. So, it may require notable efforts for developing automated exploration of co-designs that can obtain extremely efficient and accelerator-friendly compact models.\n\\mysubsubsection{Facilitating structured computations of sparse tensors} Designers may opt for accelerators that are effective for structured computations of dense tensors, e.g., systolic arrays (as near-data accelerators or coupled to processor cores) and in-memory processing with resistive crossbars. While sparsity or size reduction of tensors may need to be leveraged, significant design modifications are often infeasible due to design requirements (area/power budgets) or the increase in complexity of the system stack. So, techniques for pre-processing can be developed, which can arrange structured dense regions for feeding underlying engines or achieve structured data through sparsification/reorganization at almost no accuracy loss. Such pre-processing can be done on additional hardware modules or the host processor that handles the non-performance-critical tasks. Such disjoint mechanisms can obviate heavy design modifications in systolic arrays (e.g., ) or in-memory/near-data processing engines (e.g., ReCom , SNNrram ) while leveraging various sparsity and value similarity opportunities across different models.", "cites": [7806, 4332, 4380, 4381, 849], "cite_extract_rate": 0.29411764705882354, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes insights from multiple cited papers, particularly connecting hardware-aware compression, joint exploration of sparsity and precision, and co-design methodologies. It provides a critical perspective by discussing limitations such as the vast search space and the need for directed exploration. The section abstracts these ideas into broader design principles, such as the interplay between model compression and hardware features, and the importance of pre-processing for structured sparsity."}}
{"id": "d52e14eb-0f98-4e17-bb3f-2760d39c8a3c", "title": "Design Tools and Frameworks", "level": "subsection", "subsections": [], "parent_id": "4b09d196-2359-424f-bc4b-e7c987fe6803", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Trends and Future Directions"], ["subsection", "Design Tools and Frameworks"]], "content": "\\mysubsubsection{Framework for analyzing performance gains of accelerators due to sparsity} Given that tensors of several ML models are sparse, it is important to design accelerator systems that can exploit performance gains for multiple models through low-cost hardware modules and enhanced software support. As we discussed in sections \\ref{sec::sparse-data-coding}--\\ref{sec::software-support}, each enhancement presents multiple implementation choices at the hardware or software level. Although crafting a cycle-level simulation infrastructure for such a wide design space may be infeasible, a data-driven quantitative model can be significantly helpful for design explorations. It can process the actual data (or discover distributions of zeros), provide high-level modeling of common choices, and estimate the performance gains for each combination of the implementation choices. For newer models or functionality, hardware designers can run through a set of implementation choices in an early design phase. They can explore the implications of sparsity for the desired choice of encoding, data extraction logic, functional units, NoC, load balancing, and dataflow mechanism. \n\\mysubsubsection{Accelerator design frameworks for compact models} Several frameworks for developing and simulating FPGA or ASIC based accelerators have recently been proposed, including DNNWeaver , DNNBuilder , T2S-Tensor , and HeteroCL  for FPGAs and NVDLA , VTA , MAGNet , MAERI , and AutoDNNChip  for specialized accelerators. Similarly, hardware construction languages or representations such as Chisel  and $\\mu$IR  enable expressing microarchitectural features through high-level primitives. Such infrastructures are key for the community since they can serve as a good learning resource for training the new professionals and provide a kick-starter baseline for developing new design features. \nHowever, most frameworks support designs for dense tensors of fixed bit-widths and lack support for sparsity-tailoring features. Such frameworks can provide some pre-built modules for encoding/extracting sparse data (e.g., with common formats like  RLC, bitmap, or for block-sparsity), dynamic load balancing or data reorganization, configurable functional units, and configurable interconnects for sparse and bit-adaptive computing, etc. Even with limited features, they may serve as reusable logic that can be leveraged by designers for quick prototyping and design explorations. Further, abstractions and specifications for constructing sparsity-tailored hardware and dataflows can enable automated and efficient design explorations and easier programming of accelerators.", "cites": [4363, 4379], "cite_extract_rate": 0.18181818181818182, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes insights from multiple cited papers to highlight the role of design frameworks in accelerator development, particularly in the context of sparse tensor processing. It provides a critical view by pointing out the lack of sparsity-tailoring features in most frameworks, and it abstracts the discussion to emphasize the broader need for design abstractions and automated exploration tools. While not deeply evaluative of individual papers, it constructs a coherent and forward-looking narrative around the topic."}}
{"id": "ad9f2e9a-5ed2-40f2-b737-e0f2fac672e7", "title": "Accelerating Training of ML Models", "level": "subsection", "subsections": [], "parent_id": "4b09d196-2359-424f-bc4b-e7c987fe6803", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Trends and Future Directions"], ["subsection", "Accelerating Training of ML Models"]], "content": "While there have been significant advances in performing inference on hardware accelerators, efficient training of the models on hardware accelerators has received relatively little attention. Training has been done in high-performance computing environments containing CPU and GPU platforms and recently on FPGAs and TPU accelerators. Hardware accelerators can offer significant benefits to the model training in both edge and datacenter-scale computing environments, and they can notably improve performance and energy efficiency, respectively. In particular, they are promising for enabling online learning on edge devices through compact models.\nAccelerators, such as , ScaleDeep , and HyPar , have been proposed for efficiently training the models. However, they either do not leverage sparsity, or may not be efficiently utilized for irregular-shaped tensors, or lack support for various precisions of weights, activations, gradients, and weight updates. This presents further opportunities for performance gains and energy efficiency. Additionally, designers can leverage cross-layer optimizations (e.g., by reusing the data of gradients during back-propagation) and support mixed-precision of tensors during the training of compact models.", "cites": [4367], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information by referencing specific accelerators (e.g., HyPar) and identifying common limitations such as lack of sparsity support and irregular tensor handling. It offers a critical perspective by pointing out these shortcomings and suggesting opportunities for improvement. The discussion abstracts some general principles, like the potential for cross-layer optimizations and mixed-precision training, but remains focused on specific systems without a broader meta-level analysis."}}
{"id": "92b659a6-eb60-494f-84a3-2859bf5b052c", "title": "Related Work", "level": "section", "subsections": [], "parent_id": "6220d89f-a486-420a-914e-59957895a313", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Related Work"]], "content": "\\label{sec::related-works}\n\\textbf{Deep learning models and their applications:} Surveys  described different deep learning models along with different frameworks and datasets. Gu et al.  discussed applications of CNNs in computer vision and language processing. Recent surveys have also discussed applications of deep learning in medical image analysis , biomedical applications , wireless and networking , and embedded systems . Elsken et al.  surveyed techniques for neural architecture search.\n\\textbf{Compact models:} Cheng et al.  surveyed techniques for parameter pruning and low-rank factorization. Wang et al.  surveyed techniques for pruning, precision lowering, weight sharing, low-rank factorization, and knowledge distillation. Deng et al.  described techniques to obtain compact models including sparsification, quantization, tensor decomposition, and joint-way compression. \n\\textbf{Hardware accelerators for dense ML models:} Shawahna et al.  surveyed FPGA accelerators for processing dense tensor computations of deep learning applications. Venieris et al.  discussed different CNN-to-FPGA toolchains and described their hardware architectures, design space exploration techniques, and support for different precisions of tensors. They also compared execution metrics of the designs obtained with various toolchains and those with the previously proposed FPGA accelerators for CNNs. Sze et al.  presented a survey about efficiently executing DNNs on hardware accelerators. It described different DNNs, different compression techniques for compact models, and optimized dataflows for spatial architectures. Reuther et al.  benchmarked executions of different ML accelerators. Li et al.  discussed different ML frameworks and compilers for deep learning models.\n\\textbf{Hardware accelerators for compact ML models:} Mittal  surveyed executing compact models, including BNNs, on FPGAs. It also discussed processing convolutions with the Winograd algorithm and executions on multiple FPGAs. Deng et al.  surveyed hardware accelerators that support bit-adaptive computing and the data extraction modules for leveraging the sparsity of inputs, weights, or outputs. Du et al.  recently proposed MinMaxNN system for dynamically switching NN models. They surveyed techniques for designing self-aware NN systems (which can continuously sense information from the environment and dynamically react), including leveraging sparsity and tensor quantization. Wang et al.  surveyed hardware implementations for processing tensors of lower precisions (binary, ternary, and logarithmic quantizations). Ignatov et al.  benchmarked executions of quantized deep learning models on mobile AI accelerators.\nIn contrast to the above surveys, this work highlights sources of sparsity and size reduction of tensors in ML models and challenges in efficiently executing them on hardware accelerators. Then, it surveys and discusses the corresponding hardware and software support, including encodings and extraction of sparse data, sparsity-aware dataflows, memory management and on-chip communication of sparse tensors while leveraging data reuse, load balancing of computations, and compiler support. It also discusses techniques for computations of mixed-precision and value-shared sparse tensors.", "cites": [7633, 4384, 4385, 7632, 4382, 885, 547, 8770, 872, 4333, 4383, 8771], "cite_extract_rate": 0.631578947368421, "origin_cites_number": 19, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of existing surveys on deep learning models, compact models, and hardware accelerators but lacks deep synthesis or abstraction. It briefly contrasts this work with prior surveys but does not connect or analyze ideas across the cited papers in a meaningful way. There is minimal critical evaluation of the limitations or strengths of the referenced works."}}
{"id": "e04212f9-2787-4a9d-94a3-01623a9139c0", "title": "Summary", "level": "section", "subsections": [], "parent_id": "6220d89f-a486-420a-914e-59957895a313", "prefix_titles": [["title", "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models:\\\\ A Survey and Insights"], ["section", "Summary"]], "content": "For efficient and hardware-friendly processing, compact deep learning models have been designed. They consume less storage and computations and consist of tensors with considerable sparsity, asymmetric shapes, and variable precisions. While these compact models can be accelerated on hardware accelerators efficiently, it requires special hardware and software support. We have highlighted challenges in efficiently accelerating their sparse and irregular tensor computations. Leveraging sparsity, especially unstructured, requires a significant redesign to store, extract, communicate, compute, and load-balance only non-zeros. Moreover, the sparsity levels and patterns due to various sources lead to unique challenges and solutions in hardware/software/model co-designs. \nIn this article, we have discussed how exploiting sparsity effectively depends on tailoring the data encoding and extraction, dataflow, memory bank structure, interconnect design, and write-back mechanisms. We provided an overview of corresponding enhancements in accelerator designs and their effectiveness in exploiting sparsity. Categorization of different techniques informs how they leveraged structured or unstructured sparsity of weight or activations during learning or inference of ML models (Tables \\ref{tab:overview-sparse-accel}, \\ref{tab:target-sparsity}). For recent DNNs, we analyzed achievable accelerations for a few popular accelerators (section \\ref{sec::sparse-dnn-acceleration-case-study}). The analysis showed that accelerators exploit moderate sparsity and achieve high speedups as sparsity increases. However, exploiting high or hyper sparsity can further provide considerable opportunities, which would also need efficient mechanisms for data extraction and load balancing. Also, configurable architectures for NoCs, functional units, and buffers are required for catering to various functionalities and metadata management.\nOur analysis of sparsity-encodings describes their storage efficiency for various sparsity and the decoding requirements. While bitmaps and RLC/CSC formats are commonly used for moderate and high sparsity, respectively, storage efficiency can be improved with block-sparse tensors (especially at hyper sparsity). We have introduced a taxonomy for non-zero extraction techniques that are used for feeding the functional units of PEs. Existing data extraction mechanisms (e.g., in EIE , EyerissV2 , Cambricon-X/S ) exploit moderate sparsity. But, they may not extract enough NZs at high or hyper sparsity of large tensors (e.g., sparse BERT ), achieving lower speedups. We also discuss how block-sparsity can simplify data extraction and facilitate balanced computations. For exploiting diverse sparsity across tensors of different models, designers can explore multiple or configurable mechanisms for decoding and extraction of non-zeros. \nData reuse opportunities in processing common DNNs vary significantly, and sparsity lowers the reuse due to fewer effectual computations. However, compressed tensors allow to fit and reuse more data in on-chip memory, which reduces accesses to off-chip memory and overall latency. We have discussed techniques for memory bank management to support unstructured accesses for sparse computations and hiding the memory access latency. At high or hyper sparsity, execution may become \\emph{bandwidth-bounded}, as enough data may not be prefetched always. Hence, techniques for efficient data management (e.g., cross-layer, on-chip data reuse) and exploiting high bandwidths need to be explored. Different accelerator designs have used various interconnects for the distribution of operands, reduction of partial outputs, and collecting the outputs. They vary in terms of the bandwidth requirement and exploiting spatial data reuse. \\emph{Configurable interconnects} (e.g., in EyerissV2 , SIGMA ) are required for accelerating different DNNs of diverse sparsity, functionality, and tensor shapes, since they can support a mix of communication patterns. They are important for enabling \\emph{asymmetric} spatial accumulations of partial outputs (for sparse tensor computations) and concurrent spatial processing of different groups, e.g., for DW-CONV.  \nProcessing compressed tensors can impose significant maneuvering efforts in the PE architecture design. We discuss further opportunities including configurable designs of functional units for efficient vector processing and flexible sparsity-aware dataflows for high utilization across variations in sparsity and functionality of different layers. We also surveyed techniques for approximated computing through multiplier-free PEs and leveraging temporal and spatial similarity of values, which improve execution efficiency further. Sparse tensor computations over different PEs can be highly imbalanced. We have surveyed different techniques that sustain the acceleration by balancing the work through hardware modules for asynchronous computations or work sharing (e.g., EIE , ZENA ). Software-directed regularization such as structured sparsity eliminates load imbalance, e.g., in leveraging weight/activation sparsity for Cambricon-S  and 50\\% weight sparsity for NVIDIA A100 . Techniques including data transformations and refactoring of DNN operators may achieve low-cost load balance, including for dynamic sparsity. We have also surveyed mechanisms for asynchronous write-backs of outputs and sparsity-aware encodings on the fly. Compilation for the accelerators requires the ability to efficiently express sparsity in intermediate representations, flexibly apply different compiler optimizations, and emit efficient accelerator-specific code. The survey has discussed techniques that can enable such support and open challenges. \nAccelerator/model co-designs can efficiently leverage various precision and value similarity of different tensors and induce sparsity for accelerator-friendly executions. Automated and joint explorations of accelerator-aware compression algorithms can advance acceleration opportunities further. We have highlighted future directions for such co-designs and the system stack development (section \\ref{sec::future-directions}). In individual sections, we have also discussed further opportunities for tailoring different hardware or software enhancements for sparsity. While our discussions focused on leveraging sparsity for ML models, exploiting diverse sparsity can also aid the efficient processing of applications of other domains .\nIn conclusion, while different accelerators and compression algorithms have been proposed for efficiently processing compact ML models, it remains an active research frontier. In particular, hardware/software/model co-designs and automated and joint explorations of tensor sparsity, adaptive quantization, shape reductions, and dataflow will likely provide further opportunities for innovations across the system. With a boost in energy-efficient accelerations of the learning and inference at the cloud and edge, they can be anticipated to further improve the intelligence of various systems or applications. \n\\newpage\n\\appendix[Hardware Accelerators Can \\\\ Exploit Sparsity Better]\nExploiting acceleration opportunities due to sparsity (especially unstructured) is relatively hard for execution on CPUs and GPUs . The performance of ML models can even degrade, as compared to the execution with dense data (e.g., for a GEMM, when unstructured $W$-sparsity is below 70\\% ). For executing AlexNet layers on GPUs,  analyzed speedup for processing CSR-encoded matrices with cuSPARSE and dense matrices with cuBLAS. Their experiments showed obtaining \\emph{limited} speedups (below 1.4$\\times$) or even slowdowns for high sparsity. This is because unstructured sparsity may yield poor data locality for scattered effectual NZs. Plus, it is challenging to skip ineffectual computations and equally distribute the work among multiple threads or computational units of processor cores. Zhang et al.  analyzed performance benefits of executing sparse models (LeNet, AlexNet, and VGG-16) on CPU (with sparse BLAS) and GPU (with cuSPARSE) platforms, as compared to processing dense models (with Caffe ). For average sparsity of 90.94\\%, they reported geomean speedup of only 23.34\\% for GPU and 110\\% more time on CPU. In their sparsity-sensitivity analysis, CPU and GPU showed marginal speedup only at moderate or high sparsity due to non-trivial costs of sparse data processing. But, for Cambricon-X  , performance gains were reported for 5\\% or more sparsity due to its design tailored for sparse tensor computations. For hyper sparsity, it achieved high speedups (e.g., 15.5$\\times$ for CONV and 48.5$\\times$ for FC layer), as compared to executing dense tensors . Thus, with special support for sparse and irregular tensor computations, hardware accelerators can achieve notable speedups and efficiency. \n\\section*{Acknowledgment}\nThis work was partially supported by funding from NSF grant CCF 1723476 - NSF/Intel joint research center for Computer Assisted Programming for Heterogeneous Architectures (CAPA). The authors thank anonymous reviewers for providing valuable feedback.\n\\ifCLASSOPTIONcaptionsoff\n  \\newpage\n\\fi\n\\bibliographystyle{IEEEtran}\n\\bibliography{ref.bib}\n\\end{document}", "cites": [4348, 7168, 2671, 4386, 7634, 4335], "cite_extract_rate": 0.46153846153846156, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes insights from multiple cited works, integrating ideas on sparsity exploitation, data encoding, and architectural enhancements. It critically evaluates the limitations of existing methods, such as the inefficiency of sparse GPU kernels at moderate sparsity and the need for configurable interconnects. The section abstracts these findings into broader patterns, such as the importance of hardware/software/model co-design and the role of data reuse and load balancing in achieving efficient sparse computation."}}
