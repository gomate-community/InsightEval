"""
Autosurvey Baseline Evaluation Script
======================================
ä½¿ç”¨ LLM è¯„ä¼° Autosurvey baseline ç”Ÿæˆçš„ survey å„ç« èŠ‚æ´å¯ŸåŠ›è´¨é‡ã€‚
Autosurvey çš„æ•°æ®æ ¼å¼ï¼šæ¯ä¸ªæ–‡ä»¶å¤¹ä¸­åŒ…å«ä¸€ä¸ª JSON æ–‡ä»¶ï¼ŒåŒ…å«ï¼š
  - "survey": markdown æ ¼å¼çš„ survey å…¨æ–‡
  - "references": å¼•ç”¨ç¼–å· -> arXiv ID çš„æ˜ å°„

Usage:
    python benckmark/evaluate_autosurvey.py \
        --input_dir benckmark/SurGE/baselines/Autosurvey/output \
        --output_dir benckmark/results/autosurvey \
        --cache_path benckmark/paper_info_cache.json

    # æŒ‡å®šè¯„ä¼°æŸäº›æ–‡ä»¶å¤¹
    python benckmark/evaluate_autosurvey.py \
        --input_dir benckmark/SurGE/baselines/Autosurvey/output \
        --folders 0 1 2
"""

import argparse
import asyncio
import json
import os
import re
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import httpx
from loguru import logger
from openai import AsyncOpenAI

# å¤ç”¨ evaluate_insight.py ä¸­çš„ prompt ä¸å·¥å…·
from evaluate_insight import (
    INSIGHT_EVAL_SYSTEM_PROMPT,
    INSIGHT_EVAL_USER_PROMPT,
    LLMClient,
    parse_llm_response,
)


# ============================================================
# 1. arXiv Paper Info Fetcher (with caching)
# ============================================================

class PaperInfoCache:
    """æœ¬åœ°ç¼“å­˜ arXiv è®ºæ–‡ä¿¡æ¯ï¼Œé¿å…é‡å¤ API è°ƒç”¨"""

    def __init__(self, cache_path: str = "paper_info_cache.json"):
        self.cache_path = Path(cache_path)
        self.cache: Dict[str, Dict[str, str]] = {}
        self._load()

    def _load(self):
        if self.cache_path.exists():
            with open(self.cache_path, "r", encoding="utf-8") as f:
                self.cache = json.load(f)
            logger.info(f"ğŸ“¦ ä»ç¼“å­˜åŠ è½½äº† {len(self.cache)} æ¡è®ºæ–‡ä¿¡æ¯")

    def save(self):
        self.cache_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.cache_path, "w", encoding="utf-8") as f:
            json.dump(self.cache, f, ensure_ascii=False, indent=2)

    def get(self, arxiv_id: str) -> Optional[Dict[str, str]]:
        return self.cache.get(arxiv_id)

    def set(self, arxiv_id: str, info: Dict[str, str]):
        self.cache[arxiv_id] = info


def fetch_paper_info(arxiv_id: str) -> Dict[str, str]:
    """
    é€šè¿‡ ArxivAPIWrapper è·å–è®ºæ–‡ä¿¡æ¯ã€‚
    ä¸ get_paper_info.py ä½¿ç”¨ç›¸åŒçš„æ–¹æ³•ã€‚
    """
    try:
        from langchain_community.utilities import ArxivAPIWrapper
        wrapper = ArxivAPIWrapper()
        results = wrapper._fetch_results(arxiv_id)
        for result in results:
            return {
                "Title": result.title,
                "Abstract": result.summary,
            }
        return {"Title": "", "Abstract": ""}
    except Exception as e:
        logger.warning(f"è·å–è®ºæ–‡ {arxiv_id} ä¿¡æ¯å¤±è´¥: {e}")
        return {"Title": "", "Abstract": ""}


async def fetch_papers_for_references(
    references: Dict[str, str],
    cache: PaperInfoCache,
    delay: float = 1.0,
) -> Dict[str, Dict[str, str]]:
    """
    æ‰¹é‡è·å– references ä¸­æ‰€æœ‰è®ºæ–‡çš„ä¿¡æ¯ã€‚
    ä½¿ç”¨ç¼“å­˜ + å»¶è¿Ÿæ¥é¿å… API é™æµã€‚
    è¿”å›: arxiv_id -> {Title, Abstract}
    """
    paper_info_map: Dict[str, Dict[str, str]] = {}
    fetch_count = 0

    for cite_num, arxiv_id in references.items():
        cached = cache.get(arxiv_id)
        if cached is not None:
            paper_info_map[arxiv_id] = cached
            continue

        # éœ€è¦ä» API è·å–
        logger.debug(f"  ä» arXiv è·å–: {arxiv_id}")
        info = await asyncio.to_thread(fetch_paper_info, arxiv_id)
        cache.set(arxiv_id, info)
        paper_info_map[arxiv_id] = info
        fetch_count += 1

        # æ¯è·å–ä¸€ç¯‡å»¶è¿Ÿä¸€ä¸‹ï¼Œé¿å… API é™æµ
        if delay > 0:
            await asyncio.sleep(delay)

    if fetch_count > 0:
        cache.save()
        logger.info(f"  æ–°è·å– {fetch_count} ç¯‡è®ºæ–‡ä¿¡æ¯ï¼Œå·²æ›´æ–°ç¼“å­˜")

    return paper_info_map


# ============================================================
# 2. Markdown Section Parser
# ============================================================

def parse_markdown_sections(markdown_text: str) -> List[Dict[str, Any]]:
    """
    è§£æ markdown æ–‡æœ¬ï¼ŒæŒ‰æ ‡é¢˜æ‹†åˆ†ä¸ºç« èŠ‚åˆ—è¡¨ã€‚
    è¿”å›: [{level, title, content, cites, section_path}, ...]
    """
    # åŒ¹é… markdown æ ‡é¢˜è¡Œ: ## Title, ### Title, etc.
    heading_pattern = re.compile(r'^(#{1,6})\s+(.+)$', re.MULTILINE)

    headings = list(heading_pattern.finditer(markdown_text))
    if not headings:
        return []

    sections = []
    # ç”¨äºæ„å»ºå±‚çº§è·¯å¾„
    title_stack: List[Tuple[int, str]] = []

    for i, match in enumerate(headings):
        level = len(match.group(1))  # '#' çš„æ•°é‡
        title = match.group(2).strip()

        # è·å–ç« èŠ‚å†…å®¹ï¼ˆä»å½“å‰æ ‡é¢˜åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜ä¹‹é—´ï¼‰
        start = match.end()
        end = headings[i + 1].start() if i + 1 < len(headings) else len(markdown_text)
        content = markdown_text[start:end].strip()

        # è·³è¿‡ç©ºå†…å®¹çš„ç« èŠ‚
        if not content:
            continue

        # æ›´æ–°æ ‡é¢˜æ ˆï¼Œæ„å»ºå±‚çº§è·¯å¾„
        while title_stack and title_stack[-1][0] >= level:
            title_stack.pop()
        title_stack.append((level, title))

        section_path = " > ".join(
            [f"[H{lvl}] {ttl}" for lvl, ttl in title_stack]
        )

        # æå–å¼•ç”¨ç¼–å· [1], [2], [3,4], [1, 5] ç­‰
        cites = extract_citations(content)

        sections.append({
            "level": level,
            "title": title,
            "content": content,
            "cites": cites,
            "section_path": section_path,
        })

    return sections


def extract_citations(text: str) -> List[str]:
    """
    ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰å¼•ç”¨ç¼–å·ã€‚
    æ”¯æŒ [1], [2], [3,4], [1, 5, 10] ç­‰æ ¼å¼ã€‚
    è¿”å›å»é‡åçš„å¼•ç”¨ç¼–å·åˆ—è¡¨ï¼ˆå­—ç¬¦ä¸²ï¼‰ã€‚
    """
    cite_ids = set()
    # åŒ¹é… [æ•°å­—] æˆ– [æ•°å­—, æ•°å­—, ...] æ ¼å¼
    pattern = re.compile(r'\[(\d+(?:\s*,\s*\d+)*)\]')
    for match in pattern.finditer(text):
        nums = match.group(1).split(",")
        for num in nums:
            num = num.strip()
            if num:
                cite_ids.add(num)
    return sorted(cite_ids, key=lambda x: int(x))


# ============================================================
# 3. Survey Loading & Processing
# ============================================================

def load_autosurvey_folders(
    input_dir: str, folders: Optional[List[str]] = None
) -> List[Dict[str, Any]]:
    """
    åŠ è½½ Autosurvey output ç›®å½•ä¸­çš„æ‰€æœ‰ survey æ•°æ®ã€‚
    è¿”å›: [{folder_id, survey_title, survey_text, references, json_path}, ...]
    """
    input_path = Path(input_dir)
    surveys = []

    if folders is None:
        # è‡ªåŠ¨å‘ç°æ‰€æœ‰æ•°å­—ç¼–å·çš„å­ç›®å½•
        folder_candidates = sorted(
            [d for d in input_path.iterdir() if d.is_dir() and d.name.isdigit()],
            key=lambda d: int(d.name),
        )
    else:
        folder_candidates = [input_path / f for f in folders]

    for folder in folder_candidates:
        if not folder.exists():
            logger.warning(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder}")
            continue

        # æŸ¥æ‰¾ JSON æ–‡ä»¶
        json_files = list(folder.glob("*.json"))
        if not json_files:
            logger.warning(f"æ–‡ä»¶å¤¹ {folder.name} ä¸­æ²¡æœ‰ JSON æ–‡ä»¶")
            continue

        json_path = json_files[0]  # å–ç¬¬ä¸€ä¸ª JSON æ–‡ä»¶
        try:
            with open(json_path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except Exception as e:
            logger.error(f"è¯»å– {json_path} å¤±è´¥: {e}")
            continue

        survey_text = data.get("survey", "")
        references = data.get("references", {})

        # ä» survey markdown çš„ç¬¬ä¸€ä¸ª H1 æ ‡é¢˜æå– survey title
        title_match = re.search(r'^#\s+(.+)$', survey_text, re.MULTILINE)
        survey_title = title_match.group(1).strip() if title_match else json_path.stem

        surveys.append({
            "folder_id": folder.name,
            "survey_title": survey_title,
            "survey_text": survey_text,
            "references": references,
            "json_path": str(json_path),
        })

    return surveys


def build_cited_papers_info_autosurvey(
    cite_ids: List[str],
    references: Dict[str, str],
    paper_info_map: Dict[str, Dict[str, str]],
) -> str:
    """æ„å»ºå¼•ç”¨è®ºæ–‡çš„ä¿¡æ¯æ–‡æœ¬ï¼ˆAutosurvey ç‰ˆï¼‰"""
    papers_info_parts = []
    for cite_id in cite_ids:
        arxiv_id = references.get(cite_id)
        if not arxiv_id:
            papers_info_parts.append(
                f"**Paper [{cite_id}]**:\n  - [Reference ID not found in references mapping]"
            )
            continue

        info = paper_info_map.get(arxiv_id, {})
        title = info.get("Title", "")
        abstract = info.get("Abstract", "")

        if title or abstract:
            abstract_display = abstract[:500] + "..." if len(abstract) > 500 else abstract
            papers_info_parts.append(
                f"**Paper [{cite_id}]** (arXiv: {arxiv_id}):\n"
                f"  - Title: {title}\n"
                f"  - Abstract: {abstract_display}"
            )
        else:
            papers_info_parts.append(
                f"**Paper [{cite_id}]** (arXiv: {arxiv_id}):\n"
                f"  - [Paper info not available]"
            )

    return "\n\n".join(papers_info_parts)


# ============================================================
# 4. Main Evaluation Logic
# ============================================================

async def evaluate_autosurvey_section(
    llm: LLMClient,
    survey_title: str,
    section: Dict[str, Any],
    references: Dict[str, str],
    paper_info_map: Dict[str, Dict[str, str]],
    semaphore: asyncio.Semaphore,
) -> Dict[str, Any]:
    """è¯„ä¼° Autosurvey ä¸­çš„å•ä¸ªç« èŠ‚"""
    async with semaphore:
        section_title = section.get("title", "")
        section_path = section.get("section_path", "")
        section_content = section.get("content", "")
        cite_ids = section.get("cites", [])

        cited_papers_info = build_cited_papers_info_autosurvey(
            cite_ids, references, paper_info_map
        )

        # æˆªæ–­è¿‡é•¿å†…å®¹
        if len(section_content) > 8000:
            section_content = section_content[:8000] + "\n... [content truncated]"

        # Autosurvey æ²¡æœ‰ç‹¬ç«‹ abstractï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
        user_prompt = INSIGHT_EVAL_USER_PROMPT.format(
            survey_title=survey_title,
            survey_abstract="(Abstract not available for baseline-generated survey)",
            section_path=section_path,
            section_title=section_title,
            section_content=section_content,
            cited_papers_info=cited_papers_info,
        )

        try:
            logger.info(
                f"  è¯„ä¼°ç« èŠ‚: [H{section.get('level', '')}] {section_title} "
                f"(cites: {len(cite_ids)} ç¯‡)"
            )
            response_text = await llm.chat(INSIGHT_EVAL_SYSTEM_PROMPT, user_prompt)
            insight_result = parse_llm_response(response_text)

            if insight_result is None:
                insight_result = {
                    "error": "Failed to parse LLM response",
                    "raw_response": response_text[:500],
                }
        except Exception as e:
            logger.error(f"  è¯„ä¼°å¤±è´¥: {section_title} - {str(e)}")
            insight_result = {"error": str(e)}

        result = dict(section)
        result["insight_result"] = insight_result
        return result


async def evaluate_one_survey(
    llm: LLMClient,
    survey: Dict[str, Any],
    paper_info_map: Dict[str, Dict[str, str]],
    output_dir: Path,
    semaphore: asyncio.Semaphore,
) -> None:
    """è¯„ä¼°å•ä¸ª Autosurvey survey çš„æ‰€æœ‰æœ‰å¼•ç”¨ç« èŠ‚"""
    folder_id = survey["folder_id"]
    survey_title = survey["survey_title"]
    references = survey["references"]
    output_path = output_dir / f"survey_{folder_id}.jsonl"

    # æ–­ç‚¹ç»­ä¼ 
    if output_path.exists():
        logger.info(f"â­ï¸  è·³è¿‡ survey_{folder_id}: {survey_title} (å·²æœ‰ç»“æœ)")
        return

    # è§£æ markdown sections
    sections = parse_markdown_sections(survey["survey_text"])
    sections_with_cites = [s for s in sections if s.get("cites")]

    if not sections_with_cites:
        logger.info(f"â­ï¸  è·³è¿‡ survey_{folder_id}: {survey_title} (æ— å¼•ç”¨ç« èŠ‚)")
        return

    logger.info(
        f"ğŸ“ è¯„ä¼° survey_{folder_id}: {survey_title} "
        f"(å…± {len(sections)} ä¸ªç« èŠ‚, {len(sections_with_cites)} ä¸ªæœ‰å¼•ç”¨ç« èŠ‚)"
    )

    # å¹¶å‘è¯„ä¼°å„ç« èŠ‚
    tasks = [
        evaluate_autosurvey_section(
            llm, survey_title, section, references, paper_info_map, semaphore
        )
        for section in sections_with_cites
    ]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # å†™å…¥ JSONL
    with open(output_path, "w", encoding="utf-8") as f:
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"  ç« èŠ‚è¯„ä¼°å¼‚å¸¸: {result}")
                continue
            f.write(json.dumps(result, ensure_ascii=False) + "\n")

    logger.info(
        f"âœ… survey_{folder_id} è¯„ä¼°å®Œæˆï¼Œ"
        f"ç»“æœå·²ä¿å­˜è‡³ {output_path} ({len(results)} æ¡)"
    )


async def main(args):
    """ä¸»æµç¨‹"""
    start_time = time.time()

    # 1. åŠ è½½ Autosurvey æ•°æ®
    logger.info("=" * 60)
    logger.info("ğŸ“‚ åŠ è½½ Autosurvey æ•°æ®...")
    folders = args.folders if args.folders else None
    surveys = load_autosurvey_folders(args.input_dir, folders)
    logger.info(f"   å…±åŠ è½½ {len(surveys)} ç¯‡ Autosurvey survey")

    if not surveys:
        logger.error("æœªæ‰¾åˆ°ä»»ä½• survey æ•°æ®ï¼Œè¯·æ£€æŸ¥ input_dir è·¯å¾„")
        return

    # 2. æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„ arXiv ID å¹¶è·å–è®ºæ–‡ä¿¡æ¯
    logger.info("ğŸ“š è·å–å¼•ç”¨è®ºæ–‡ä¿¡æ¯ (arXiv API)...")
    cache = PaperInfoCache(args.cache_path)

    # åˆå¹¶æ‰€æœ‰ references
    all_references: Dict[str, str] = {}
    for survey in surveys:
        for cite_num, arxiv_id in survey["references"].items():
            all_references[cite_num] = arxiv_id

    unique_arxiv_ids = set(all_references.values())
    logger.info(f"   å…± {len(unique_arxiv_ids)} ä¸ªå”¯ä¸€ arXiv ID")

    # æ„å»º arxiv_id -> info çš„æ˜ å°„
    # ä¸ºäº†å¤ç”¨ï¼Œè¿™é‡Œå¯¹æ¯ä¸ª survey çš„ references è¿›è¡Œæ‰¹é‡è·å–
    all_paper_info: Dict[str, Dict[str, str]] = {}
    for survey in surveys:
        paper_info_map = await fetch_papers_for_references(
            survey["references"], cache, delay=args.api_delay
        )
        all_paper_info.update(paper_info_map)

    cached_count = sum(1 for aid in unique_arxiv_ids if cache.get(aid) is not None)
    logger.info(f"   è®ºæ–‡ä¿¡æ¯è·å–å®Œæˆ: {cached_count}/{len(unique_arxiv_ids)} å·²ç¼“å­˜")

    # 3. åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
    llm = LLMClient(
        base_url=args.base_url,
        model_name=args.model_name,
        api_key=args.api_key,
        temperature=args.temperature,
    )
    logger.info(f"ğŸ¤– LLM å®¢æˆ·ç«¯åˆå§‹åŒ–: {args.model_name}")

    # 4. åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # 5. ç»Ÿè®¡ä¿¡æ¯
    total_sections = 0
    for survey in surveys:
        sections = parse_markdown_sections(survey["survey_text"])
        total_sections += len([s for s in sections if s.get("cites")])
    logger.info(f"ğŸ“Š å¾…è¯„ä¼°: {len(surveys)} ç¯‡ survey, {total_sections} ä¸ªæœ‰å¼•ç”¨ç« èŠ‚")
    logger.info("=" * 60)

    # 6. å¹¶å‘æ§åˆ¶
    semaphore = asyncio.Semaphore(args.concurrency)

    # 7. é€ä¸ªè¯„ä¼°
    for i, survey in enumerate(surveys):
        logger.info(f"\n--- [{i+1}/{len(surveys)}] ---")
        await evaluate_one_survey(llm, survey, all_paper_info, output_dir, semaphore)

    elapsed = time.time() - start_time
    logger.info(f"\nğŸ‰ å…¨éƒ¨è¯„ä¼°å®Œæˆ! è€—æ—¶: {elapsed:.1f}s")


# ============================================================
# 5. CLI Entry Point
# ============================================================

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Autosurvey Baseline Evaluation - ä½¿ç”¨ LLM è¯„ä¼° Autosurvey ç”Ÿæˆçš„ survey ç« èŠ‚æ´å¯ŸåŠ›"
    )
    parser.add_argument(
        "--input_dir",
        type=str,
        default="benckmark/SurGE/baselines/Autosurvey/output",
        help="Autosurvey è¾“å‡ºç›®å½•è·¯å¾„",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="benckmark/results/autosurvey",
        help="è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•",
    )
    parser.add_argument(
        "--cache_path",
        type=str,
        default="benckmark/paper_info_cache.json",
        help="arXiv è®ºæ–‡ä¿¡æ¯ç¼“å­˜è·¯å¾„",
    )
    parser.add_argument(
        "--folders",
        nargs="*",
        default=None,
        help="æŒ‡å®šè¦è¯„ä¼°çš„æ–‡ä»¶å¤¹ç¼–å· (å¦‚ 0 1 2)ï¼Œä¸æŒ‡å®šåˆ™è¯„ä¼°å…¨éƒ¨",
    )
    parser.add_argument(
        "--api_delay",
        type=float,
        default=1.0,
        help="arXiv API è°ƒç”¨é—´éš”æ—¶é—´/ç§’ (é»˜è®¤: 1.0)",
    )
    parser.add_argument(
        "--concurrency",
        type=int,
        default=3,
        help="LLM å¹¶å‘è¯·æ±‚æ•° (é»˜è®¤: 3)",
    )
    parser.add_argument(
        "--base_url",
        type=str,
        default="https://aicloud.oneainexus.cn:30013/inference/aicloud-yanqiang/qwen3-32b-server/v1",
        help="LLM API base URL",
    )
    parser.add_argument(
        "--model_name",
        type=str,
        default="Qwen/Qwen3-32B",
        help="æ¨¡å‹åç§°",
    )
    parser.add_argument(
        "--api_key",
        type=str,
        default="dummy_key",
        help="API å¯†é’¥",
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="ç”Ÿæˆæ¸©åº¦ (é»˜è®¤: 0.7)",
    )

    args = parser.parse_args()
    asyncio.run(main(args))



# python benckmark/evaluate_autosurvey.py --folders