{"id": "c8891f20-bfc3-4813-8482-d5a81ebe56db", "title": "Introduction", "level": "section", "subsections": ["b8c8dcc5-cf7f-4b12-bb64-6f170abb4c1f"], "parent_id": "d9a1f7b0-a2c3-420d-9fc5-182794e93cd2", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "Introduction"]], "content": "\\label{sec:intro}\nGenerative Adversarial Networks (GANs)  are a type of generative model which have received much attention in recent years due to their ability to model complex real-world data. Comprised of two models, a generator ($G$) and a discriminator ($D$) both typically represented as neural networks, the two models are trained in a process of adversarial optimization. GANs have been shown to achieve state-of-the-art results in the generation of images , natural language , time-series synthesis , and other domains .\nAlthough GANs achieve state-of-the-art results in many domains, they are known to be notoriously difficult to train, suffering significantly from instability problems . Common failures include mode collapse in which the generator maps different inputs to the same class of outputs which results in highly non-diversified samples; non-convergence due to oscillatory and diverging behaviors during the training of both generator and discriminator; and vanishing or exploding gradients leading to slow learning or a complete stop of the learning process . \nRecent years have seen a plethora of proposed methods to stailize GAN training, including modified network architectures, loss functions, convergence-aiding training strategies, and other methods . Despite the significant progress made, to date none of these methods fully solve the instability problem. This instability poses a significant obstacle to the wider adoption of GANs as an off-the-shelf method, because significant manual parameter tuning is typically required to make the method work .\nThe purpose of this article is to provide a comprehensive survey of GAN stabilization methods and to highlight open problems for the progress of the field. We describe the GAN stability problems, identify and categorize existing stabilization methods into five different categories, and include a comparative summary of methods. While each category is presented separately, it should be noted that many of the proposed methods are complementary and can potentially be jointly applied. \nAfter discussing related surveys in Section~\\ref{sec:relwork}, the reader is introduced to the concept of GANs and instability problems in Section~\\ref{sec:gans}. Section~\\ref{sec:gan-variants} surveys different GAN variants, outlining strengths and weaknesses of each of the approaches. Section~\\ref{sec:trainstab} describes widely employed heuristics for GAN stabilization. Finally, Section~\\ref{sec:openprob} outlines open problems which provide useful directions for future research.", "cites": [8430, 1001, 6809, 8265, 989, 117, 64, 148, 7022], "cite_extract_rate": 0.5625, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The introduction synthesizes key challenges in GAN training by referencing multiple papers, creating a coherent narrative of instability problems and stabilization efforts. It critically acknowledges that existing methods do not fully resolve the issues, emphasizing the need for further research. The section abstracts some broader patterns, such as the diversity of stabilization approaches and their partial success, but stops short of offering a novel or meta-level framework for understanding the field."}}
{"id": "b8c8dcc5-cf7f-4b12-bb64-6f170abb4c1f", "title": "Related Surveys", "level": "subsection", "subsections": [], "parent_id": "c8891f20-bfc3-4813-8482-d5a81ebe56db", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "Introduction"], ["subsection", "Related Surveys"]], "content": "\\label{sec:relwork}\nMost surveys devoted to the field of GANs offer a synopsis of the whole field, outlining key concepts, methods and applications of GANs, albeit without an in-detail description of the GAN training procedure and instability problems .  conduct a large-scale empirical study which focused on the performance of a variety of GANs. They demonstrated a need for more consistent and objective evaluation of GANs, as well as for constructing variants less sensitive to hyperparameter tuning.  provide a descriptive, limited overview of the GAN taxonomy, focusing solely on the architecture and loss function variants.  provide an overview of the current GAN landscape, but omit a number of aspects crucial for stabilization of GAN training. \nIn contrast to the surveys mentioned above, a number of studies focus solely on GAN training issues. This includes the work of  who provide a comprehensive outline of the GAN training issues and the underlying theory. Furthermore, a plethora of work deals with practical considerations  as well as theoretical aspects  towards training GANs. A concise practical study by  investigates the impact of a few chosen loss functions, architecture, normalization and regularization schemes. They explore individual and joint performance, as well as sensitivity to hyperparameter tuning.\nOur survey provides a comprehensive summary of current GAN techniques and an in-depth study on GAN training problems, providing an overview of the current state of the GAN research landscape with a specific focus on the problem of GAN training stability. It also aims to describe and categorize GAN variants into a holistic taxonomy, and includes an overview of previously overlooked game theory and gradient-based approaches, as well as general training methods.", "cites": [7190, 60, 6809, 7001, 135, 65, 99, 117, 148], "cite_extract_rate": 0.5294117647058824, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear analytical overview by distinguishing between general GAN surveys and those focused on training stability. It synthesizes key aspects of the cited papers, such as theoretical foundations, empirical studies, and practical considerations. However, the synthesis remains somewhat surface-level and lacks a deeper integration of ideas into a novel framework. Critical evaluation is present in identifying omissions and limitations in previous surveys, but not in a highly nuanced or systematic way. Some abstraction is attempted, particularly in highlighting broader themes like game theory and gradient-based approaches, but not at a meta-level."}}
{"id": "41be60b7-6bce-4f4b-9dbd-41eb26f7a25e", "title": "Generative Adversarial Networks", "level": "section", "subsections": ["96b0317a-4600-48b9-ac16-0eb9f8a77635"], "parent_id": "d9a1f7b0-a2c3-420d-9fc5-182794e93cd2", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "Generative Adversarial Networks"]], "content": "\\label{sec:gans}\nThe basic idea of GANs is a competitive game between the generator, $G$, and discriminator, $D$. The generator is given a Gaussian or uniform random noise $z \\sim p_{z}$, and transforms it in such a way that $G(z)$ resembles a target distribution $p_{r}$. Specifically, the generator is trained to maximize the expected log-probability with which the discriminator classifies $G(z)$ as a real sample, $\\mathbb{E}_{z \\sim p_{z}}\\log[D(G(z))]$. Conversely, the discriminator is provided with a set of unlabeled samples from $G(z)$ and $p_{r}$, and is trained to distinguish between the generated (fake) and real samples. Formally, the discriminator is trained to maximize the expected log-probability $\\mathbb{E}_{x \\sim p_{r}}\\log[D(x)]$ while simultaneously minimizing $\\mathbb{E}_{z \\sim p_{z}}\\log[D(G(z))]$. This competition between $G$ and $D$ can be formulated as a two-player zero-sum game in which the following loss function is maximized by the discriminator and minimized by the generator:\n\\begin{equation}\nL(D,G) = \\mathbb{E}_{x\\sim p_{r}}\\log[D(x)] + \\mathbb{E}_{z \\sim p_{z}}\\log[1-D(G(x))]\n\\end{equation}\nIn their original publication,  proved the existence of a unique solution at $D=\\frac{1}{2}$, which since GANs are a zero-sum two-player game, corresponds to a Nash equilibrium (NE) in which neither player can improve their cost unilaterally . In practice, however, GANs have been shown to struggle to reach this NE . Furthermore, the evaluation and benchmarking of GANs has been hampered by the lack of a single universal and comprehensive performance measure.", "cites": [117, 135], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of GANs and their training objective, integrating minimal information from the cited papers. While it mentions the existence of a Nash equilibrium and training instability, it lacks deeper synthesis of the cited works, critical evaluation of their contributions, and abstraction to broader principles or trends. The section reads more like a background introduction than an insightful analysis of the literature."}}
{"id": "96b0317a-4600-48b9-ac16-0eb9f8a77635", "title": "Instability Problems in GAN Training", "level": "subsection", "subsections": ["3fb3a2ee-6192-4199-899b-e6c54391e425", "482c8740-95f9-4ff3-b354-666c91ec1620", "fd2d28c4-035f-419f-8c38-33415be380e1"], "parent_id": "41be60b7-6bce-4f4b-9dbd-41eb26f7a25e", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "Generative Adversarial Networks"], ["subsection", "Instability Problems in GAN Training"]], "content": "\\label{sec:gan-probs}\nDespite the proof of existence of the unique equilibrium in the GAN game, the training dynamics of GANs have proven to be notoriously unstable . We now discuss the main sources of instability.\nIn their original formulation,  have shown that GANs optimize the Jensen-Shannon divergence (JSD)\n\\begin{equation}\n    \\text{JSD}(\\mathbb{P}_{r}||\\mathbb{P}_{g}) = \\frac{1}{2}\\text{KL}(\\mathbb{P}_{r}||\\mathbb{P}_{A}) + \\frac{1}{2}\\text{KL}(\\mathbb{P}_{g}||\\mathbb{P}_{A})\n\\end{equation}\nwhere KL denotes the Kullback-Leibler divergence and $\\mathbb{P}_{A}$ is defined as the ``average'' distribution $\\mathbb{P}_{A} = \\frac{P_{r}+P_{g}}{2}$. Using the definition of \\text{JSD} and $\\mathbb{P}_{A}$, the optimal discriminator $D^{*}$ is\n\\begin{equation}\n    D^* = \\frac{P_{r}(x)}{P_{r}(x) + P_{g}(x)}.\n\\end{equation}\nIncorporating $D^*$ into the discriminator's loss as a function of $\\theta$, it can be shown that at optimality, the discriminator is minimizing the following equation\n\\begin{equation}\n    L(D^{*}, g_{\\theta}) = \\text{2JSD}(\\mathbb{P}_{r}||\\mathbb{P}_{g}) - 2\\log2.\n\\end{equation}\nThe concept of an optimal discriminator is particularly important because its optimality would ensure that the discriminator produces meaningful feedback to the generator. Therefore to achieve this, it is reasonable to assume that a training procedure would include training the discriminator to optimality for each iteration of the generator. Nevertheless, extensive practical  and theoretical  studies have shown that training the discriminator to optimality works only in theory. The following sub-sections detail the major causes of this instability.", "cites": [64, 117], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides an analytical explanation of GAN instability by referencing the original GAN formulation and the role of the optimal discriminator. However, it only briefly mentions the cited papers without integrating their insights deeply into the discussion. There is limited critical evaluation or abstraction to broader principles."}}
{"id": "3fb3a2ee-6192-4199-899b-e6c54391e425", "title": "Convergence", "level": "subsubsection", "subsections": [], "parent_id": "96b0317a-4600-48b9-ac16-0eb9f8a77635", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "Generative Adversarial Networks"], ["subsection", "Instability Problems in GAN Training"], ["subsubsection", "Convergence"]], "content": "Although the existence of a global NE has been proven , arriving at this equilibrium is not straightforward. The proof of convergence described above does not hold in practice due to the fact that the generator and discriminator are modeled as neural networks; thus, the optimization procedure operates in the parameter space of these networks, instead of directly learning the probability density function. Furthermore, the non-convex-concave character of the game makes it especially hard for the gradient descent ascent (GDA) algorithm to converge, often resulting in diverging, oscillating, or cyclic behavior , and proneness to convergence to a local NE within the parameter space of the neural networks. Furthermore, the local NE can be arbitrarily far from the global NE, thus preventing the discriminator from being optimal. Finally, from a practical standpoint, training the discriminator to optimality for every generator iteration is highly computationally expensive.", "cites": [117, 8266], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the key ideas from the cited papers on GAN instability, particularly linking the theoretical convergence properties of GANs to practical challenges due to non-convexity and parameter space limitations. It provides a general abstraction by highlighting the computational and mathematical difficulties in achieving convergence. However, the critical analysis is limited to pointing out problems without deeper evaluation or contrast between the cited works."}}
{"id": "fd2d28c4-035f-419f-8c38-33415be380e1", "title": "Mode Collapse", "level": "subsubsection", "subsections": [], "parent_id": "96b0317a-4600-48b9-ac16-0eb9f8a77635", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "Generative Adversarial Networks"], ["subsection", "Instability Problems in GAN Training"], ["subsubsection", "Mode Collapse"]], "content": "One of the most common failures in GANs, \\textit{mode collapse} happens when the generator maps multiple distinct inputs to the same output, which means that the generator produces samples of low diversity . This is a problem for GAN training, since a generator without the ability to produce diverse samples is not useful for training purposes. Discovering and tackling mode collapse is a difficult problem, since the cause of mode collapse is rooted deeply in the concept of GANs.  argued that with the generator greedily optimizing its loss function in order to ``fool'' the discriminator, each generator update is a partial collapse towards mode collapse. An intuitive solution to mode collapse is a well trained discriminator capable of robustly identifying the symptoms of mode collapse. However, as discussed above, this runs counter to the vanishing or exploding gradients problem. A crucial aspect in addressing mode collapse is a discriminator with high generalization capabilities \nAll of the above problems contribute to the instability of the GAN training process. These problems are often highly linked with each other and addressing them often takes character of a trade-off. A crucial challenge is the accurate estimation of the generalization capabilities of the discriminator, which in turn would make it possible to properly evaluate the generator. A final obstacle is lack of a comprehensive evaluation metric. At the current state, a number of evaluation metrics have been proposed, such as \\textit{Inception} or \\textit{Frechet Inception Distance} (FID). Nevertheless, none of them offer a unique solution .", "cites": [5903, 7001, 8267], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of mode collapse in GANs, integrating concepts from the cited papers to explain its root causes and challenges. It connects the issue of mode collapse with vanishing/exploding gradients and the importance of discriminator generalization, showing basic synthesis. However, it lacks deeper critical evaluation or a novel framework, and while it hints at broader challenges (e.g., evaluation metrics), the abstraction remains limited to general observations rather than a meta-level analysis."}}
{"id": "e8f3f4a8-fe23-4a51-8845-d768128c22a9", "title": "Modified Architectures", "level": "subsection", "subsections": ["4e0b618c-d2f4-4e1c-a7cd-feee70d961ea", "16fc428e-76a1-420c-bcbd-efa99fc374bb", "3abe358c-0580-4473-96c6-96ce68b5b07e", "51c0c89e-b90f-4dcc-a433-a4b663980184", "ba235442-3698-453f-b214-356c3e7cd732"], "parent_id": "6dc30893-8a8b-406f-816b-8783a6b2765f", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "GAN Variants and Stabilization Techniques"], ["subsection", "Modified Architectures"]], "content": "The most extensively studied category is architecture variant GANs. This category covers methods in which the proposed improvements involve networks or training structure while keeping the loss function largely intact from the previously proposed methods. The majority of the architecture-based variants have been established for a specific application . Here, we focus on the more general approach, which has shown to improve stability of the training procedure in GANs.", "cites": [6809, 7022, 8268], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a brief introduction to architecture-based GAN variants but lacks in-depth synthesis of the cited works. It does not clearly connect the specific contributions of the papers to the broader theme of training stabilization. There is minimal critical analysis or abstraction, and the section primarily describes the category without offering a comparative or analytical perspective."}}
{"id": "4e0b618c-d2f4-4e1c-a7cd-feee70d961ea", "title": "Convolutional Architectures", "level": "subsubsection", "subsections": [], "parent_id": "e8f3f4a8-fe23-4a51-8845-d768128c22a9", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "GAN Variants and Stabilization Techniques"], ["subsection", "Modified Architectures"], ["subsubsection", "Convolutional Architectures"]], "content": "One of the most popular and successful architecture-variant GANs are inspired by computer vision techniques, specifically Convolutional Neural Networks (CNNs) . An example which has been shown to significantly improve the quality of generated images is the Deep Convolutional GAN (DCGAN) . DCGAN applies a series of convolutional operations including a spatial up-sampling operation to improve the generator. Having shown to enhance the quality of the produced samples solely through architectural changes, DCGAN is often treated as a baseline for modeling other GAN variants . A further improvement was proposed by  in Self-Attention GAN (SAGAN), who incorporated a self-attention mechanism into the design of both networks in order to capture local and global dependencies of the target distribution. Finally, BigGAN  builds on SAGAN and introduces a number of incremental improvements. In particular, both the batch size and number of model parameters are increased, contributing to BigGAN obtaining state-of-the-art results on the ImageNet.", "cites": [7194, 56], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of convolutional GAN variants, citing DCGAN, SAGAN, and BigGAN. While it mentions improvements introduced by each, it lacks critical evaluation or comparison of these approaches. There is minimal synthesis or abstraction, as the discussion remains at a surface level without deeper insights or a broader conceptual framework."}}
{"id": "16fc428e-76a1-420c-bcbd-efa99fc374bb", "title": "Hierarchical Architectures", "level": "subsubsection", "subsections": [], "parent_id": "e8f3f4a8-fe23-4a51-8845-d768128c22a9", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "GAN Variants and Stabilization Techniques"], ["subsection", "Modified Architectures"], ["subsubsection", "Hierarchical Architectures"]], "content": "A difficult problem in GAN training is the discriminator overpowering the generator in the early stages of the game, thus failing to properly train the generator. This is particularly challenging for generating high resolution images. A heuristic approach designed to tackle this problem is to allow the generator to make a number of updates per one discriminator update . Nevertheless, this technique has been shown to be limited and often leading to other instability problems. Progressive GAN (PROGAN)  addresses this problem by training the networks in multiple phases, starting from a low-resolution image and minimal network architecture, progressively expanding both networks. Additionally, PROGAN proposes a number of training techniques improving stability for image generation, including pixel normalization and mini-batch standard deviation.\nAnother hierarchical approach is the simultaneous training of multiple GAN pairs. Stacked GAN (SGAN)  stacks pairs of generator and discriminator, each consisting of an encoder and decoder on top of each pair. Apart from calculating standard adversarial loss, at each stack the encoder and decoder calculate the discrepancy between generated and real image (referred to as conditional loss) as well as entropy loss, which prevents mode collapse. All three losses are then combined into a composite loss and used in the joint training of all stacks. Similar to Stacked GAN, GoGAN  trains pairs of WGANs  (see Section~\\ref{sec:intprob}), but uses margin based Wasserstein distance, instead of standard Wasserstein distance. This makes it possible for the discriminator to focus on samples with smaller margin and progressively reduce the margin at each stage of the game. Therefore, steadily decreasing the discrepancy between the generated and target distribution.", "cites": [8430, 8269, 62, 2574, 64], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple papers to explain the concept of hierarchical architectures in GAN stabilization, linking PROGAN, SGAN, and GoGAN under a common theme. It offers a critical view by pointing out the limitations of earlier heuristics and describing how different approaches address specific issues like mode collapse and distribution discrepancy. While it provides some abstraction by framing these methods as hierarchical solutions, the analysis remains focused on specific techniques without deeper meta-level generalization."}}
{"id": "3abe358c-0580-4473-96c6-96ce68b5b07e", "title": "Autoencoder Architectures", "level": "subsubsection", "subsections": [], "parent_id": "e8f3f4a8-fe23-4a51-8845-d768128c22a9", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "GAN Variants and Stabilization Techniques"], ["subsection", "Modified Architectures"], ["subsubsection", "Autoencoder Architectures"]], "content": "Another approach designed to prevent the discriminator from optimizing greedily (which may overpower the generator at early stages of the training) is the adoption of an autoencoder architecture for the discriminator. Instead of outputting a probability to distinguish between generated (fake) and real samples, the discriminator assigns an energy value to given samples. The energy model is attained by feeding a sample (generated or real) to a discriminator consisting of an encoder and a decoder, calculating its reconstruction cost, and training it to output high energy level for generated and low for real samples. Intuitively, the autoencoder architecture takes into account the complexity of the image. Furthermore, as the autoencoder is trained to focus on reconstructing the most important features (i.e. near the data manifold), it provides feedback on these features for the generator. The autoencoder architecture was initially proposed by , with the loss function being the energy level. Additionally, in order to stabilize training the discriminator ignored the sample outliers with high reconstruction error. The margin restriction was later relaxed by  in MAGAN, which argued that a fixed margin has a negative influence on the dynamics of the GAN. More complex Boundary Equilibrium GAN (BEGAN)  uses Wasserstein distance to match the distributions between reconstructed and generated or real examples. Moreover, it adds a hyperparameter $\\gamma = \\frac{\\mathbb{E}[L(G(z))]}{\\mathbb{E}[L(x)]}$ to balance the training between the generator and the discriminator.", "cites": [8270], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates the concept of autoencoder-based discriminators from multiple works, showing some synthesis by explaining how these architectures aim to stabilize GAN training by using energy levels and reconstruction costs. It includes a basic comparison between different methods (e.g., MAGAN relaxing margin restrictions and BEGAN using Wasserstein distance), but lacks deeper critical evaluation of their trade-offs or limitations. The abstraction is moderate, as it generalizes the purpose of autoencoder discriminators but does not elevate the discussion to a meta-level theoretical framework."}}
{"id": "51c0c89e-b90f-4dcc-a433-a4b663980184", "title": "Latent Space Decomposition Architectures", "level": "subsubsection", "subsections": [], "parent_id": "e8f3f4a8-fe23-4a51-8845-d768128c22a9", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "GAN Variants and Stabilization Techniques"], ["subsection", "Modified Architectures"], ["subsubsection", "Latent Space Decomposition Architectures"]], "content": "One of the reasons behind the instability of training is the low-dimensional support of generated and target distributions, which can lead to the greedy optimization of the discriminator . While the dimensions of $p_{r}$ appear to be high, in reality they are usually concentrated in lower dimensional manifolds, which often mirror the most important features of the distribution (for example, a feature of a human face on an image). Such lower dimensional manifolds establish a large number of restrictions for the generator, often making it unattainable for the generator to follow them. This contributes to the disjointedness of generated and real distribution, thus making it easier for the discriminator to perfectly discriminate between the samples.\nLatent space decomposition methods aim at providing additional information to the input latent vector $z$, enabling it to disentangle the relevant features from the latent space. InfoGAN  adopts an unsupervised method which supplies a latent variable $c$ capturing the meaningful features of the real sample on top of the standard input noise vector $z$. InfoGAN then maximizes the mutual information $I(c, G(z,c)$ between $z$ and $c$, transforming the generated sample with $G(z, c)$. Due to intractability of estimating the posterior $p(c|x)$, the algorithm adopts a variational approach maximizing a lower bound of $I(c, G(z,c)$, instead of directly calculating it.\nIn contrast to InfoGAN, Conditional GAN (CGAN)  proposes a supervised method, adding a class label $c$ to a generator and discriminator. This approach has been shown to produce high quality images and prevent mode collapse. However, this is with the strong restriction of possessing a labeled dataset.", "cites": [1002, 1001, 148], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic synthesis by contrasting InfoGAN and CGAN, and it connects them to the broader issue of GAN instability. It includes a simple comparison by highlighting the supervised vs. unsupervised nature of the approaches and mentions a limitation of CGAN (need for labeled data). However, it lacks deeper abstraction or critical evaluation of the methods' theoretical underpinnings or broader implications."}}
{"id": "162ea370-8ec3-4505-a113-9d50d5c22821", "title": "Modified Loss Functions", "level": "subsection", "subsections": ["767ff531-44f8-421e-98be-6ac1ab383334", "911d2e8b-d59e-404f-81b8-8780047c15e5", "3a008917-3c2f-4f98-a59c-8bf2d18ebaa8", "66714035-313b-4f13-96b8-fc889f6d1890"], "parent_id": "6dc30893-8a8b-406f-816b-8783a6b2765f", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "GAN Variants and Stabilization Techniques"], ["subsection", "Modified Loss Functions"]], "content": "The shortcomings of the standard GAN loss function were already identified by Goodfellow et al. , who showed that KL divergence and JS divergence under idealized conditions  contributes to oscillating and cyclical behavior, especially when the distributions of generated and target data are disjoint. This leads to the non-convergence and vanishing gradients problems. A large number of studies have been devoted to this problem . In this section, we introduce a number of alternative loss functions employed for GANs and highlight the most important ones.", "cites": [5903, 8271, 135, 148, 8265], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a concise analytical overview of the role of modified loss functions in GAN stabilization, referencing key theoretical insights from multiple papers. It synthesizes the core idea that standard GAN loss leads to instability due to divergence issues, and links this to alternative approaches. However, it lacks deeper critical evaluation or detailed comparison of the specific loss functions and their trade-offs, limiting its insight quality."}}
{"id": "767ff531-44f8-421e-98be-6ac1ab383334", "title": "f", "level": "subsubsection", "subsections": [], "parent_id": "162ea370-8ec3-4505-a113-9d50d5c22821", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "GAN Variants and Stabilization Techniques"], ["subsection", "Modified Loss Functions"], ["subsubsection", "f"]], "content": "-Divergence}\nThe KL and JSD divergences discussed in the standard GAN  belong to an $f$-divergence family, which measures the difference between two given probability distributions. Given distributions $p_{r}$ and $p_{g}$, the $f$-divergence with a function $f$ is defined as:\n\\begin{equation}\nD_{f}(p_{r}\\parallel p_{g}) = \\int_{\\mathcal{X}}p_{g}(x)f\\bigg(\\frac{p_{r}(x)}{p_{g}(x)}\\bigg)dx\n\\end{equation}\nwhere the generator function $f$ is a convex function and satisfies the condition $f(1) = 0$. In other words, the function $f$ becomes 0 when two distributions are equivalent. The $f$-divergence family can be indirectly calculated by estimating the expectation of the lower bound, which circumvents the intractability problem of unknown probability distributions.  discussed the GAN loss function in terms of various divergences and their efficacy under an arbitrary function $f$, proposing f-GAN, which generalizes the GAN loss function through the estimation of various $f$-divergences given $f$. Another f-divergence based GAN is the Least Squares GAN (LSGAN), which tries to remedy the problem of vanishing gradient by replacing the sigmoid cross-entropy loss present in the standard GAN with the least-squares loss, penalizing generated samples far from the decision boundary . As a divergence, LSGAN has been proven to use the Pearson $\\mathcal{X}^{2}$ divergence.\nNevertheless, the $f$-divergence family is subject to a number of limitations. In particular, the growing dimension of data makes it harder to estimate the divergence and the support of the two distributions becomes low dimensional, resulting in the divergence value going to infinity .", "cites": [54, 63], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the concepts of f-divergence and its application in GAN stabilization by integrating the core ideas from both the f-GAN and LSGAN papers. It provides a critical analysis by discussing the limitations of f-divergence in high-dimensional settings. Additionally, it abstracts the discussion by framing these techniques within the broader theoretical context of probability divergence estimation in GANs."}}
{"id": "911d2e8b-d59e-404f-81b8-8780047c15e5", "title": "Integral Probability Metric", "level": "subsubsection", "subsections": [], "parent_id": "162ea370-8ec3-4505-a113-9d50d5c22821", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "GAN Variants and Stabilization Techniques"], ["subsection", "Modified Loss Functions"], ["subsubsection", "Integral Probability Metric"]], "content": "\\label{sec:intprob}\nDesigned to address the restrictions of the $f$-divergence family, integral probability metrics (IPM) methods are not subject to the data dimension and distribution disjointedness problems, resulting in a consistent distance between the data distributions. Furthermore, in contrast to $f$-divergence in which the discriminator is a binary classifier, the discriminator in an IPM framework is a regression task providing a scalar value and henceforth opening a new avenue for research by treating GANs as an actor-critic problem . Defined as a critic function $f$ in a function class $\\mathcal{F}$ consisting of real-valued, bounded, measurable functions, an IPM measures the maximal distance between two distributions. Given $p_{r}$ and $p_{g}$ on a compact space $\\mathcal{X}$, the IPM metric can be denoted as:\n\\begin{equation}\nd_{\\mathcal{F}}(p_{r}, p_{g}) = sup_{f \\in \\mathcal{F}}\\mathbb{E}_{x \\sim p_{r}}[f(x)] - \\mathbb{E}_{x \\sim p_{g}}[f(x)]\n\\end{equation}\nOne of the distances which belong to IPM is the \\textit{Wasserstein} or Earth Mover's (EM) distance, which is used in Wasserstein GAN (WGAN)  as the new loss measure. Wasserstein distance is defined as:\n\\begin{equation}\nW(p_{r}, p_{g}) = \\inf_{\\gamma\\in\\sqcap(p_{r},p_{g})}\\mathbb{E}_{(x,y)\\sim\\gamma}[\\parallel x - y \\parallel]\n\\end{equation}\nWasserstein distance has been shown to significantly improve training stability and convergence, dealing particularly well with the distributions support lying on low dimensional manifolds. However, due to the intractability of the infimum term, the critic $f$ needs to be parameterized and weight clipping needs to be applied. A significant drawback is that weight clipping incurs pathological behavior that contributes to slow convergence and not entirely stable training. These problems have been partly addressed by including a gradient penalty instead of weight clipping in the loss function of WGAN-GP . Another shortcoming of the WGAN was discussed by  who argued that the WGAN incurs biased gradients. Their solution to this problem suggests an energy function similar to Wasserstein distance, but without the biased gradients. Furthermore, the Cramer distance is also linked with the kernel embedded space distance, discussed in the next paragraph on Maximum Mean Discrepancy.\nDerived from Generative moment matching networks (GMMN) , Maximum Mean Discrepancy (MMD) distance is an alternative to Wasserstein distance. Continuous and differentiable like the Wasserstein distance, MMD distance has been applied in Maximum Mean Discrepancy GAN (MMDGAN), which can be seen as a combination of GMMNs and GANs . Essentially, MMD measures the distance between the means of the embedding space of two distributions, using the kernel trick with a Gaussian kernel. In MMDGAN, the Gaussian kernels are replaced by adversarial kernel learning techniques, which is claimed to better represent the feature space. This can be seen as advantageous over WGAN. However, the computational complexity of MMDGAN which increases exponentially with the number of samples is a significant drawback.\nA different IPM approach is adopted in Relativistic GAN (RGAN) , in which the authors argue for a general approach to devising new GAN loss functions. This approach estimates the probability that a generated sample is real, taking into account the probability of a corresponding real sample being real. In other words, we measure the distance between the probability of generated sample being real and the probability of real sample being real. It has been shown that the RGAN approach contributes to overcoming the greedy optimization of the discriminator. Furthermore, the technique outlined in RGAN could be potentially expanded to other GANs.\nOther IPM approaches include Geometric GAN  which draw on the Support Vector Machine (SVM) algorithm , utilizing separating hyperplanes to update the generator and discriminator and encouraging the generator to move toward the separating hyperplane and the discriminator to move away from it. Although proven to improve training stability, the Geometric GAN requires expensive  matrix computation. Loss Sensitive GAN (LS-GAN)  is based on the idea that the loss for a real sample should be smaller than the loss for a generated sample, and aims at minimizing the margins between the two. The reasoning behind LS-GAN is that the non-parametric assumption of the discriminator having infinite capacity  leads to instability problems, including vanishing gradients, and argues for a more general margin-based approach. Finally, a more stable and computationally efficient approach has been proposed in Fisher GAN , which incorporates a data-dependent constraint into the loss function. Fisher GAN aims at not only reducing the distance between two distributions, but also the in-class variance of them. Furthermore, it avoids adding weight clipping or gradient penalty.", "cites": [8276, 92, 1925, 8274, 8317, 8273, 64, 8272, 8275, 121], "cite_extract_rate": 0.7692307692307693, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes the concept of Integral Probability Metrics (IPM) by integrating multiple works, highlighting how methods like WGAN, MMDGAN, RGAN, and Fisher GAN differ in their approaches and trade-offs. It provides critical analysis by discussing limitations such as weight clipping, computational complexity, and biased gradients, and also compares alternatives. While not reaching the highest level of abstraction, it identifies key patterns in how IPM-based GANs address training instability and introduces a broader analytical framework for loss function design in GANs."}}
{"id": "3a008917-3c2f-4f98-a59c-8bf2d18ebaa8", "title": "Auxiliary Loss Functions", "level": "subsubsection", "subsections": [], "parent_id": "162ea370-8ec3-4505-a113-9d50d5c22821", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "GAN Variants and Stabilization Techniques"], ["subsection", "Modified Loss Functions"], ["subsubsection", "Auxiliary Loss Functions"]], "content": "The modifications in loss function are not limited to devising new divergences from the $f$-divergence or IPM family. An important avenue is providing supplementary terms to the already existing GAN loss function in order to improve the training procedure. Here, we highlight two such methods. The first one is inspired by the recursive reasoning opponent modeling methods , in which the generators ``unrolls'' possible discriminator reactions and accounts for them by adding a term to the gradient. Given the parameters of the discriminator $\\theta_{D}$ and generator $\\theta_{G}$, the surrogate loss by unrolling $N$ steps can be defined as:\n\\begin{equation}\nf_{N}(\\theta_{D}, \\theta_{G}) =  f(\\theta_{D}^{N}(\\theta_{D}, \\theta_{G}), \\theta_{G})\n\\end{equation}\nThe intuition is that capturing how a discriminator would react given a change in discriminator and feeding it into a generator would allow the generator to react, and prevent mode collapse. A different approach towards preventing mode collapse is offered in Mode Regularized GAN (MRGAN), which uses an encoder $E$ to produce the latent vector $z$ from a real sample $x$, $E(x):x \\rightarrow z$ instead of random input noise and alters the loss function by including a term ensuring that different modes are captured in the latent vector $z$.", "cites": [5903], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from a single cited paper and introduces a second method (MRGAN) with a general explanation. It provides some analytical insight by explaining the intuition behind the methods and their goals, such as preventing mode collapse. However, the critical analysis is limited, and the abstraction remains at a moderate level, identifying general patterns but not overarching principles or a novel framework."}}
{"id": "66714035-313b-4f13-96b8-fc889f6d1890", "title": "Summary", "level": "subsubsection", "subsections": [], "parent_id": "162ea370-8ec3-4505-a113-9d50d5c22821", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "GAN Variants and Stabilization Techniques"], ["subsection", "Modified Loss Functions"], ["subsubsection", "Summary"]], "content": "Loss function variant GANs are the most heavily studied type of GANs after the architecture type to date. Some of the loss function variant GANs have managed to predominantly overcome the vanishing gradient problem . Furthermore, in comparison to architecture types, they offer more generalization properties, with methods such as WGAN or SN-GAN being widely applied and offering avenues for further research . Additionally, the trade-off between the quality of generated data and computational complexity does not show to be as proportional in comparison to some of the architecture variant GANs such as BigGAN . However, loss function variant GANs often work only under specific restrictions , and proper hyperparameter tuning of architectural design approaches often appear to yield superior results . Hence, the stabilization of the training procedure remains a challenge.", "cites": [1925, 8317, 63, 64, 78, 56, 121], "cite_extract_rate": 0.7, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview by discussing key advantages and limitations of loss function variant GANs, referencing several relevant papers. It connects ideas such as the impact of weight clipping in WGAN (Paper 2) and spectral normalization (Paper 5), while also touching on trade-offs between quality and complexity (Paper 6). However, the integration remains somewhat surface-level, and the critique or synthesis of concepts could be more nuanced for a higher insight level."}}
{"id": "c4a93b68-f430-4434-8758-c5f35cd26515", "title": "Game Theory for GANs", "level": "subsection", "subsections": ["815398ba-7156-4f43-b60e-c0f02ad57458"], "parent_id": "6dc30893-8a8b-406f-816b-8783a6b2765f", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "GAN Variants and Stabilization Techniques"], ["subsection", "Game Theory for GANs"]], "content": "At the core of all GANs is a competition between generator and discriminator. Game theory GANs focus specifically on that aspect, drawing on the rich literature in the field of two-player competitive games  to aid convergence.\nA leading theme in this research area is the computation of mixed strategy Nash equilibria (MNE) . In contrast to the pure strategy Nash equilibrium used in standard GAN, which has been proven to guarantee convergence only to a local Nash equilibrium , under certain circumstances an MNE is always equal to a global NE. A proof for of the existence of the MNE in the GAN game together with a sketch of the suitable algorithm was given by . Drawing on this work,  proposed a convergent algorithm called ChekhovGAN for finding MNE using a regret minimization algorithm . However, the ChekhovGAN is only provably convergent under the heavy restrictions that the discriminator is a single-layered network. The authors of  formulated the game as a finite zero-sum game with a finite number of parameters and proposed using a Parallel Nash Memory algorithm  to arrive at the MNE. Their work was further extended by a less restrictive use of Mirror Descent in computing MNEs . A relatively simpler yet effective method to compute MNEs was proposed by , which trains the model using a mixture of historical models. Some of the game theory inspired methods are also present in other approaches .", "cites": [8277, 6997, 8278, 60, 8279, 8280, 8281, 117], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple game theory-inspired GAN stabilization techniques, connecting them through the concept of mixed strategy Nash equilibria. It provides a critical evaluation by highlighting the limitations (e.g., heavy restrictions on ChekhovGAN) and theoretical contributions of each approach. The abstraction is strong as it frames the GAN training problem in terms of game-theoretic principles, identifying broader patterns in how convergence can be improved."}}
{"id": "5fef70ef-6a78-4376-a3b2-4f165c4ecc06", "title": "Multi-Agent GANs", "level": "subsection", "subsections": ["5355da57-2f83-411a-82e1-e9fe41433bbc"], "parent_id": "6dc30893-8a8b-406f-816b-8783a6b2765f", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "GAN Variants and Stabilization Techniques"], ["subsection", "Multi-Agent GANs"]], "content": "Multi-agent type GANs extend the idea of using a single pair of generator and discriminator to the multi-agent setting and show how employing such methods can provide better convergence properties and prevent mode collapse. The majority of the literature focuses on introducing a larger number of generators  or discriminators .\nMAD-GAN  uses multiple generators and one discriminator not only to identify fake images but also to determine the specific generator which produced the images fed to it. Aimed at preventing mode collapse, MAD-GAN modifies the generator's objective function to encourage different generators to produce different samples. It also changes the discriminator to account for these generators and provide feedback to them accordingly to various modes. An extension of MAD-GAN is MGAN  which uses a separate classifier in parallel with discriminator to match the generated output with an appropriate generator. Also providing some theoretical justification for the MGAN enforcing the generators to produce diverse samples, thus prevent mode collapse. Stackelberg GAN , drawing on the concept of \\textit{Stackelberg} competition , offers an architecture design where multiple generators act as followers to the discriminator (leader), utilizing sampling schemes beyond the mixture model.\nAt the other side of the spectrum,  use many discriminators to boost the learning of the generator, while  train exactly two discriminators which separately compute KL and reverse KL divergence to place a fair distribution across the data modes.\nMIX+GAN  uses a mixture of multiple generators and discriminators with multiple different parameters to compute mixed strategies. The total reward is then calculated by taking a weighted average of rewards over the pairs of generators and discriminators. An interesting approach is offered by  who examine how multi-agent communication can regularize the training of both networks. Previously discussed SGAN  and GoGAN  train GANs in local pairs to establish a global GAN pair trained against all of these pairs at the same time. Similarly,  also adopts the concept of multiple GAN pairs, but with discriminators dynamically exchanged during training, thus reducing the effect of coupling which can lead to mode collapse.", "cites": [7194, 8284, 8282, 8286, 2574, 8269, 8283, 8285], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of multi-agent GANs by listing different approaches and their general mechanisms. It mentions some methods (e.g., MAD-GAN, SGAN, GoGAN) but lacks deeper connections or a unifying narrative to synthesize the cited works. Critical evaluation is minimal, with no discussion of trade-offs or limitations beyond superficial mentions. The abstraction level is low, as it mainly describes specific architectures without identifying broader principles or trends in multi-agent GAN design."}}
{"id": "5355da57-2f83-411a-82e1-e9fe41433bbc", "title": "Summary", "level": "subsubsection", "subsections": [], "parent_id": "5fef70ef-6a78-4376-a3b2-4f165c4ecc06", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "GAN Variants and Stabilization Techniques"], ["subsection", "Multi-Agent GANs"], ["subsubsection", "Summary"]], "content": "The current literature on multi-agent type GANs mainly revolves around the relatively simple idea of hindering mode collapse through the introduction of a variety of generators and discriminators. Although the results are promising, often the computational costs pose a significant problem. Moreover, it is not yet entirely clear how an appropriate number of generators or discriminators should be chosen. Finally, there remain a variety of methods in multi-agent learning literature which have not been exhausted in terms of stabilizing GAN training .", "cites": [8287, 6810, 3442], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a brief analytical overview of multi-agent GANs, mentioning the purpose of these models and key challenges such as computational costs and the lack of guidance on choosing the number of agents. While it does connect the idea of multi-agent GANs to the broader goals of stabilization and mode collapse mitigation, the synthesis is limited and does not deeply integrate the cited multi-agent reinforcement learning papers. The critical perspective is moderate, as it identifies some limitations but does not engage in deep evaluation or comparison of the methods."}}
{"id": "c0202de5-807a-4a66-b1f7-db10bf90c217", "title": "Modified Gradient Optimization", "level": "subsection", "subsections": ["16a59057-b06d-40e0-85e4-06466306346c"], "parent_id": "6dc30893-8a8b-406f-816b-8783a6b2765f", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "GAN Variants and Stabilization Techniques"], ["subsection", "Modified Gradient Optimization"]], "content": "An interesting research avenue is the development of alternatives to the regular gradient descent ascent (GDA) that are more suitable for general games with multiple losses interacting with each other. Regular GANs trained with GDA have demonstrated diverging and oscillating behaviors during training , and although its convergence has been proven using a two-time scale update rule , this requires mild assumptions and can only be guaranteed to arrive at a local Nash equilibrium.   used Optimistic Mirror Descent (OMD) to approximately converge, but only under a restriction that the game is a two-player bilinear zero-sum game.  proposed the \\textit{ConOpt} algorithm that adds a penalty for non-convergence. Based on this result,  and later  decomposed the game into two components: a \\textit{potential game} which can be solved by GDA, and a \\textit{Hamiltonian game} suitable for solving with \\textit{ConOpt} . Finally,  combined the approaches above and drew on multi-agent methods  to create a \\textit{Competitive Gradient Descent}, suitable specifically for two-player competitive games. This algorithm was later applied to competitive regularization in GANs , where the generator and discriminator act as agents under local information showing more consistent and stable performance.", "cites": [8288, 60, 8266, 8289, 99, 6810, 7003], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes several papers by connecting different gradient optimization techniques used in GAN training, such as OMD, ConOpt, and Competitive Gradient Descent. It shows how these methods aim to address instability and provides a coherent narrative on the evolution of these techniques. While it includes some critical evaluation of assumptions and limitations (e.g., bilinear zero-sum game restrictions), it does not go into deep comparative analysis or meta-level abstraction, though it begins to categorize the types of games these methods are suitable for."}}
{"id": "8ba0456c-f374-4289-aa44-89dce9c58b41", "title": "Other Modifications", "level": "subsection", "subsections": [], "parent_id": "6dc30893-8a8b-406f-816b-8783a6b2765f", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "GAN Variants and Stabilization Techniques"], ["subsection", "Other Modifications"]], "content": "This section mentions other GAN methods which have no clear correspondence to the previous categories. One is the Evolutionary GAN , which adapts the GAN to the environment through a series of operations on the population of generators and discriminators. AdaGAN  uses a boosting technique to iteratively address the problem of missing modes. An interesting and deeply theoretical approach was proposed by  who provided a proof and a method to escape local equilibria using energy-based potential fields. Another technique for preventing mode collapse includes marginalizing the weights of the generator and discriminator offered in the Bayesian GAN .", "cites": [8290, 8291], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic, factual overview of a few GAN variants without clearly connecting them to each other or to broader themes in GAN stabilization. It lacks critical evaluation of the cited methods and does not abstract beyond the individual papers to highlight overarching principles or trends."}}
{"id": "45d4d19f-7f99-4836-b523-0353e9cead69", "title": "Comparative Summary", "level": "subsection", "subsections": [], "parent_id": "6dc30893-8a8b-406f-816b-8783a6b2765f", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "GAN Variants and Stabilization Techniques"], ["subsection", "Comparative Summary"]], "content": "To conclude the review of the GAN variants, we outline the advantages and disadvantages as well as the outlook of each of the types. Currently, the approach showing the highest empirical improvement is the architecture type. Nevertheless, this is also due to a large number of publications in the area of modified network architecture relative to other GAN variants, low entry barrier, and some of the designs resulting in meticulous parameter optimization. Furthermore, much of the research of this type lacks rigorous theoretical justification and struggles to alleviate all of the training problems, usually showing improvement in only one or two of them . At the same time, loss function variant GANs, although often well justified, such as WGAN , show that their results could also be attained by appropriate hyperparameter optimization of architecture variant GANs such as DCGAN. A comprehensive comparison of GANs  has demonstrated a small improvement in general metrics by state-of-the-art loss type GANs in comparison to architectures such as DCGAN , and that it can be more worthwhile to focus on tuning hyperparameters rather than introducing novel methods.\nAnother survey on convergence in GANs  highlighted that much of the rigorous theoretical analysis is often not fulfilled in reality; GANs can converge in situations they theoretically should not, and vice versa. It is hypothesized that this is mostly due to the nature of gradient descent ascent. Loss function variant GANs hold a big promise, addressing an obvious issue which is the unsuitability of the standard objective function to the GAN game. Furthermore, they often demonstrate more general improvement in training than architecture type GANs. We believe, however, that a significant breakthrough in loss function GANs would require incorporating an alternative to GDA. This could possibly be addressed by the gradient-based variants, which might have the broadest influence, enhancing all types of GANs by introducing a gradient design relevant to the GAN game. Nevertheless, gradient-based methods are still at a too early stage to form any reliable predictions. Similarly to loss function variants, game theory variants could provide an approach capable of the general improvement in the training procedure, addressing the problem of dealing with the non-convex-concave character of the game. However, current methods in this field work only under strict limitations and cannot compete with architecture and loss function type GANs. It should be noted that this area is also in early stages, with only a handful of studies published. Finally, multi-agent GANs have mostly focused on preventing mode collapse, showing promising but not state-of-the-art results in this problem. A vast area of multi-agent learning including actor-critic methods and opponent modeling  remains unexplored.", "cites": [135, 64], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes insights from different GAN stabilization approaches, integrating information about architecture, loss function, gradient-based, game theory, and multi-agent methods. It offers critical evaluation by discussing limitations such as lack of theoretical justification in architecture variants and the potential for hyperparameter tuning to match loss function improvements. The section abstracts beyond individual papers, identifying broader patterns and trends in the field, such as the early stage of gradient-based and game theory approaches."}}
{"id": "9882f9d8-9a73-4ee1-aa55-435c9ae86707", "title": "General Stabilization Heuristics", "level": "section", "subsections": ["3ca6111f-3930-439d-850b-b3e62bb83fe6", "6de2092a-66ac-4e96-9150-32f9cf4f2d06", "b2f82a06-a677-4a56-8399-6985aa140ef6"], "parent_id": "d9a1f7b0-a2c3-420d-9fc5-182794e93cd2", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "General Stabilization Heuristics"]], "content": "\\label{sec:trainstab}\nA large scale study on GANs  has shown that optimizing hyperparameters tends to yield superior results compared to applying specific methods such as modified architectures and loss functions. Furthermore, other studies in this area  have demonstrated the impact of regularization and normalization techniques on various loss function and architecture GAN variants. In this section, we discuss general heuristics which have demonstrated consistent stabilization improvements in GAN training. These heuristics can potentially be applied to different GAN types.", "cites": [65, 135], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions the importance of hyperparameters, regularization, and normalization based on the cited papers but fails to integrate or synthesize these ideas into a broader narrative. It lacks critical evaluation of the cited works and does not identify overarching patterns or principles, remaining largely descriptive in nature."}}
{"id": "3ca6111f-3930-439d-850b-b3e62bb83fe6", "title": "Regularization", "level": "subsection", "subsections": [], "parent_id": "9882f9d8-9a73-4ee1-aa55-435c9ae86707", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "General Stabilization Heuristics"], ["subsection", "Regularization"]], "content": "The most prominent regularization technique in GANs is the gradient penalty, which has been initially applied to WGAN in WGAN-GP  as a soft penalty constraining the objective function to 1-Lipschitzness. In this study the authors used linear interpolation between real and generated distributions to calculate the gradient norm used for the penalty. The gradient penalty in WGAN was later extended by , who looked at the gradient penalty from the perspective of regret minimization. Therefore, using a no-regret type algorithm to constrain the gradient penalty to be calculated only around a data manifold, thus limiting the discriminator functions to the set of linear functions. A different approach was adopted by  who proposed a gradient penalty on the weighted gradient-norm of the discriminator, designed specifically to tackle the issue of non-overlapping support between the generated and target distribution in GANs from \\textit{f}-divergence family.", "cites": [6997, 101, 8317], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple regularization approaches in GANs by connecting the gradient penalty in WGAN-GP to extensions from other works, showing some integration of ideas. It provides an analytical view by explaining how each method addresses specific training issues, such as non-overlapping support or Lipschitz constraints. However, it lacks deeper critical evaluation or meta-level abstraction to present overarching principles of regularization in GANs."}}
{"id": "6de2092a-66ac-4e96-9150-32f9cf4f2d06", "title": "Normalization", "level": "subsection", "subsections": [], "parent_id": "9882f9d8-9a73-4ee1-aa55-435c9ae86707", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "General Stabilization Heuristics"], ["subsection", "Normalization"]], "content": "Normalization of the discriminator can stabilize the GAN training procedure from two perspectives. First, it can lead to more stable optimization through inducing the discriminator to produce better quality feedback. Second, it can enrich the representation within the layers of the network of the discriminator.\nOptimization-wise, two techniques have been particularly applied to the GAN stability problem. These are batch normalization  and layer normalization . Batch normalization, introduced by  and popularized by , has been shown to greatly improve the optimization of neural networks through normalizing the activations of each layer, allowing each layer to learn more stable distribution of inputs. Batch normalization was further extended by  who proposed virtual batch normalization, which aims to make the input sample less dependent on other samples in the batch, thereby alleviating the mode collapse problem. In contrast to batch normalization, the statistics for layer normalization  are independent of other samples in the batch. However, the normalization of inputs is done across the features, resulting in different samples being normalized differently depending on its features. Layer normalization was initially used in the work of .\nFrom a representation standpoint, the most effective method proposed up-to-date has been spectral normalization . Introduced initially in the Spectral Normalization GAN (SN-GAN), spectral normalization normalizes the weights of the discriminator, contributing to a more stable training. Spectral normalization constrains the spectral norm of the discriminator's layers, which imposes a Lipschitz condition in a more efficient and adaptable way.", "cites": [71, 8317, 117, 78, 2573, 57], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear analytical overview of normalization techniques in GANs, grouping them under optimization and representation perspectives. It connects relevant papers like batch normalization and spectral normalization, but synthesis could be improved by more explicitly linking the different methods' goals and effects. The analysis includes basic comparisons and identifies benefits but lacks deeper evaluation of trade-offs or limitations."}}
{"id": "b2f82a06-a677-4a56-8399-6985aa140ef6", "title": "Architectural Features and Training Procedures", "level": "subsection", "subsections": [], "parent_id": "9882f9d8-9a73-4ee1-aa55-435c9ae86707", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "General Stabilization Heuristics"], ["subsection", "Architectural Features and Training Procedures"]], "content": "In addition to the regularization and normalisation techniques described above, in recent years the community devised a number of heuristic techniques which have shown to further improve the stability of the training. These include:\n\\begin{description}\n    \\item[Feature matching ] --- Proposed to prevent the generator from overtraining on the given discriminator, feature matching requires the generator to not only maximize the probability of the generated sample being classified as real by the discriminator, but also account for the generated data to match with the target distribution. This is achieved by training the generator on the intermediate layer of the discriminator.\n    \\item[Minibatch discrimination ] --- A method aimed at hindering mode collapse, minibatch discrimination offers a method for penalizing strong similarity between generated samples. By adding a layer in the discriminator, minibatch discrimination is able to compute the cross-sample distance and measure the diversity of generated data. The discriminator still outputs a single probability, but offers the data computed with the use of minibatch as side information.\n    \\item[Historical averaging ] --- Designed to avoid the oscillating and cyclical behavior of GANs, historical averaging incorporates past values of parameters into the player's cost function by adding a term $\\parallel\\theta - \\frac{1}{t}\\sum_{i=1}^{t}\\theta[i]\\parallel^{2}$, with $\\theta[i]$ being the value of the parameters at past time $i$.\n    \\item[One-sided label smoothing ] --- Hindering the discriminator from becoming overconfident and squashing generator's gradients, one-sided label smoothing restricts the discriminator to output the probabilities between 0.1 and 0.9, instead of 0 and 1, thus handicapping the discriminator and alleviating the vanishing gradient problem. \n    \\item[Noise injection ] --- Designed to tackle the training instability problem, in particular, vanishing gradients problem, injecting continuous noise to the inputs of the discriminator leads to smoothing the distribution of the probability mass, thus reducing the problem of non-overlapping support, problematic especially for standard GAN. Noise injection has been shown to be computationally efficient and practical , however, leading to a lower quality of the generated image .\n\\end{description}", "cites": [8292, 64, 117, 101], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a clear description of several heuristic techniques for GAN stabilization, with references to relevant papers. However, it lacks deeper synthesis by not connecting these methods to a broader framework or to each other. Critical analysis is minimal, as it does not compare the methods or evaluate their trade-offs in depth. The abstraction level is low, as the discussion remains at the method-specific level without generalizing to underlying principles or trends."}}
{"id": "9b9eef00-c7b3-439d-9001-8876b17585c5", "title": "Combining Training Stabilization Methods", "level": "subsection", "subsections": [], "parent_id": "86735d72-5116-451e-8962-1662a43f1a67", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "Open Problems"], ["subsection", "Combining Training Stabilization Methods"]], "content": "With the number of approaches coming from different disciplines and perspectives proposed in recent years, it is unclear how these techniques align with each other and how combinations affect the overall performance. Thus, an interesting avenue of research is the investigation of how these methods could be potentially applied jointly. An example of this idea is BigGAN , which achieved state-of-the-art results by combining already-known methods and subsequently scaling them. A proper combination of training techniques, architectural changes, and loss function modifications could prove to yield superior results. This is especially important in the context of game theory and gradient-based GAN variants, which have been consistently omitted in the architectural and loss function GAN literature . Although these type of GANs often posses significant limitations, the methods outlined by them such as fictitious play or no-regret algorithms could provide an additional advantage in stabilizing the GAN training procedure.", "cites": [56, 6809], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section attempts to connect different GAN stabilization methods and suggests the value of combining them, using BigGAN and game theory-based approaches as examples. While it integrates a few key ideas, the synthesis is limited and does not present a novel framework. It offers some critical perspectives by pointing out omissions in the literature and limitations of certain GAN variants, but the critique is not deeply nuanced. The section abstracts somewhat by highlighting potential broader advantages of combining methods, yet the generalization remains modest."}}
{"id": "697c7658-6b42-4710-9b83-179fdec8e4c4", "title": "Actor-Critic Methods for GAN Training", "level": "subsection", "subsections": [], "parent_id": "86735d72-5116-451e-8962-1662a43f1a67", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "Open Problems"], ["subsection", "Actor-Critic Methods for GAN Training"]], "content": "The link between GANs and reinforcement learning has already been described ; in particular,  discussed the connection between GANs and actor-critic methods. However, numerous possible connections remain unexplored. One of the potentially beneficial stabilization technique known to improve the stability in actor-critic methods is the use of replay buffers, enabling off-policy updates based on the action chosen. Although not directly applicable, an extension of replay buffers, prioritized experience replay  could serve as a conceptual inspiration for a different method for identifying and improving the networks on the outliers, with outliers being the samples with large discrepancy between real and generated samples.", "cites": [3586, 1925], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section analytically explores the connection between GANs and actor-critic methods, integrating ideas from two cited papers. It identifies a potential stabilization technique (prioritized experience replay) and suggests how it might conceptually inspire GAN training improvements. However, the synthesis remains at a moderate level with limited exploration of multiple frameworks, and the critique or comparison is not deeply developed."}}
{"id": "216bf857-eab5-4baa-9b91-66300f643aed", "title": "Discriminator Generalization", "level": "subsection", "subsections": [], "parent_id": "86735d72-5116-451e-8962-1662a43f1a67", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "Open Problems"], ["subsection", "Discriminator Generalization"]], "content": "GAN training relies on the idea of the discriminator producing useful feedback to the generator. This, however, is often hard to attain. Furthermore, it is not entirely possible to evaluate the quality of the discriminator's feedback and performance solely on the basis of its loss . For example, a theoretically robust discriminator could have in practice learnt to discern real from generated images only based on a small set of features with no proper generalization. A crucial aspect of the optimal discriminator is its ability to generalize to unseen samples. A proper GAN objective should induce the discriminator towards the optimal discriminator with good generalization capabilities. This issue has been explored more in-depth only recently, proposing new distances  and regularization techniques . Nevertheless, constructing a GAN objective that fully incorporates the discriminator's generalization capabilities remains a challenge.", "cites": [7001], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a concise analytical discussion on the issue of discriminator generalization in GANs, drawing from recent research. It synthesizes the idea that the original GAN loss leads to poor generalization and integrates this with broader concerns about training instability. While it introduces a critical perspective by highlighting the limitations of existing approaches, it lacks deeper comparative evaluation or a novel framework, keeping the abstraction at a moderate level."}}
{"id": "232ffcfe-bc22-4327-85c4-19003281848e", "title": "Opponent Modeling in GANs", "level": "subsection", "subsections": [], "parent_id": "86735d72-5116-451e-8962-1662a43f1a67", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "Open Problems"], ["subsection", "Opponent Modeling in GANs"]], "content": "The two-player zero-sum game is a subject of rich literature in the field of opponent modeling . Nevertheless, very few studies have explored the connection between the GAN game and opponent modeling or multi-agent learning broadly. An interesting avenue would be to model the GAN game as a game between two agents with limited information and devise mechanisms that would allow to incorporate the awareness of the other agent into their training procedure. Such an approach could potentially lead to more stable convergence .", "cites": [8289], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical perspective by suggesting that GAN training could benefit from opponent modeling concepts. It cites one paper and uses it as a basis to generalize the idea of modeling the GAN game as a strategic interaction between agents. While it identifies a potential research direction, it lacks a synthesis of multiple works and a deeper comparative or critical analysis of existing limitations in the cited paper."}}
{"id": "2117ca71-2f09-4fa4-8c51-641548ee5f26", "title": "Variational Autoencoder GAN Hybrid", "level": "subsection", "subsections": [], "parent_id": "86735d72-5116-451e-8962-1662a43f1a67", "prefix_titles": [["title", "Stabilizing Generative Adversarial Networks: A Survey"], ["section", "Open Problems"], ["subsection", "Variational Autoencoder GAN Hybrid"]], "content": "Similar to GANs, Variational Autoencoders (VAEs) are also a type of generative model . However, in contrast to the former, which implicitly model a target distribution, VAEs follow an explicit approach to approximately estimating the data distribution through the use of an encoder and a decoder. VAEs are known to produce images of lesser quality in comparison to GANs. Nevertheless, they possess stronger theoretical groundings and have been shown to be less prone to mode collapse than GANs. Drawing on VAEs, VAEGAN  and $\\alpha$-GAN  combined the approaches to alleviate the mode collapse problem. This area at the intersection of VAEs and GANs, although not yet entirely explored, provides promising results and could prove to achieve significant results, especially in terms of preventing mode collapse.", "cites": [5680, 8962], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the general concepts of VAEs and GANs, and integrates the ideas from VAEGAN and -GAN to form a narrative about hybrid models. It provides a basic comparison between the two generative models and identifies a potential research direction. However, it lacks deeper critical evaluation of the cited works or broader theoretical abstraction."}}
