{"id": "fa093594-1cb8-4a7d-b1b3-e0f7759f2a69", "title": "Introduction", "level": "section", "subsections": ["ff3fd66d-7158-4818-8216-f75841b7bf36"], "parent_id": "c4299bf7-fa65-4214-968a-c213c95e6d07", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Introduction"]], "content": "\\label{section:introduction}\nNatural Language Generation (NLG)  is one of the crucial yet challenging sub-fields of Natural Language Processing (NLP). NLG techniques are used in many downstream tasks such as summarization, dialogue generation, generative question answering (GQA), data-to-text generation, and machine translation. Recently, the rapid development of NLG has captured the imagination of many thanks to the advances in deep learning technologies, especially  Transformer~-based models like BERT~, BART~, GPT-2~, and GPT-3~. The conspicuous development of NLG tasks attracted the attention of many researchers, leading to an increased effort in the field. \nAlongside the advancement of NLG models, attention towards their limitations and potential risks has also increased.\nSome early works~ focus on the potential pitfalls of utilizing the standard likelihood maximization-based objective in training and decoding of NLG models. They discovered that such likelihood maximization approaches could result in \\textit{degeneration}, which refers generated output that is bland, incoherent, or gets stuck in repetitive loops. \nConcurrently, it is discovered that NLG models often generate text that is nonsensical, or unfaithful to the provided source input~. Researchers started referring to such undesirable generation as \\textit{hallucination}~~\\footnote{The term ``hallucination'' first appeared in Computer Vision (CV) in~ and carried more positive meanings, such as superresolution~, image inpainting~, and image synthesizing~. Such hallucination is something we take advantage of rather than avoid in CV. \nNevertheless, recent works have started to refer to a specific type of error as \"hallucination\" in image captioning~ and object detection~, which denotes non-existing objects detected or localized incorrectly at their expected position. The latter conception is similar to ``hallucination'' in NLG.}.\nHallucination in NLG is concerning because it hinders performance and raises safety concerns for real-world applications.\nFor instance, in medical applications, a hallucinatory summary generated from a patient information form could pose a risk to the patient. \nIt may provoke a life-threatening incident for a patient if the instructions of a medicine generated by machine translation are hallucinatory.\nHallucination can also lead to potential privacy violations.  demonstrate that language models can be prompted to recover and generate sensitive personal information from the training corpus (e.g., email address, phone/fax number, and physical address). Such memorization and recovery of the training corpus is considered a form of hallucination because the model is generating text that is not ``faithful'' to the source input content (i.e., such private information does not exist in the source input).\nCurrently, there are many active efforts to address hallucination for various NLG tasks. Analyzing hallucinatory content in different NLG tasks and investigating their relationship would strengthen our understanding of this phenomenon and encourage the unification of efforts from different NLG fields.\nHowever, to date, little has been done to understand hallucinations from a broader perspective that encompasses all major NLG tasks. To the best of our knowledge, existing surveys have only focused specific tasks like abstractive summarization~ and translation~.  \nThus, in this paper, we present a survey of the research progress and challenges in the hallucination problem in NLG. And offer a comprehensive analysis of existing research on the phenomenon of hallucination in different NLG tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation.\nWe mainly discussed hallucination of the unimodal NLG tasks that have textual input sources upon which the generated text can be assessed. We also briefly summarize hallucinations in multi-modal settings such as visual-language tasks~.\nThis survey can provide researchers with a high-level insight derived from the similarities and differences of different approaches. Furthermore, given the various stages of development in studying hallucination from different tasks, the survey can assist researchers in drawing inspiration on concepts, metrics, and mitigation methods.", "cites": [2336, 679, 2338, 2337, 2340, 7565, 8542, 2339, 38, 42, 2335, 6979, 2341, 2342, 1291, 1877], "cite_extract_rate": 0.6956521739130435, "origin_cites_number": 23, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the cited works by connecting concepts of hallucination across multiple NLG tasks and highlighting its implications for real-world applications. It also traces the origin and evolution of the term 'hallucination' across disciplines. While it identifies some limitations (e.g., in training objectives and evaluation metrics), the critical analysis is not as deep as it could be, and it stops short of proposing a novel framework or deeply generalized principles."}}
{"id": "8a937c6a-663f-436e-8440-8c482962a1cb", "title": "Definitions", "level": "section", "subsections": ["51f0bf53-bad8-486f-9f2f-f7ce30a46eed", "82fa3312-c888-4cb1-83e1-971b2f72e75c", "518ebb18-7011-496d-8724-03128b75ed86"], "parent_id": "c4299bf7-fa65-4214-968a-c213c95e6d07", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Definitions"]], "content": "\\label{section:definition}\nIn the general context outside of NLP, hallucination is a psychological term referring to a particular type of perception~.  define hallucination as \\textbf{``a percept, experienced by a waking individual, in the absence of an appropriate stimulus from the extracorporeal world''}.\nSimply put, a hallucination is an unreal perception that feels real. \nThe undesired phenomenon of \\textbf{``NLG models generating unfaithful or nonsensical text''} shares similar characteristics with such psychological hallucinations -- explaining the choice of terminology. Hallucinated text gives the impression of being fluent and natural despite being unfaithful and nonsensical. It appears to be grounded in the real context provided, although it is actually hard to specify or verify the existence of such contexts. Similar to psychological hallucination, which is hard to tell apart from other ``real'' perceptions, hallucinated text is also hard to capture at first glance.\nWithin the context of NLP, the above definition of hallucination, \\textit{the generated content that is nonsensical or unfaithful to the provided source content}~, is the most inclusive and standard.\nHowever, there do exist variations in definition across NLG tasks, which will be further described in the later task-specific sections.", "cites": [2344, 2343, 2345, 1291], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to establish a coherent and inclusive definition of hallucination in NLG. It connects the psychological concept with the NLP phenomenon and references relevant studies to support the discussion. While it offers some generalization by identifying variations across NLG tasks, it lacks deeper critical analysis of the cited works' strengths or limitations."}}
{"id": "51f0bf53-bad8-486f-9f2f-f7ce30a46eed", "title": "Categorization", "level": "subsection", "subsections": [], "parent_id": "8a937c6a-663f-436e-8440-8c482962a1cb", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Definitions"], ["subsection", "Categorization"]], "content": "\\label{subsec:category}\nFollowing the categorization from previous works~, there are two main types of hallucinations, namely intrinsic hallucination and extrinsic hallucination.\nTo explain the definition and categorization more intuitively, we give examples of each category of hallucinations for each NLG downstream task in Table \\ref{Table: example}.\n\\begin{enumerate}\n\\item \\textbf{Intrinsic Hallucinations}: The generated output that contradicts the source content. \nFor instance, in the abstractive summarization task from Table \\ref{Table: example}, the generated summary ``\\textit{The first Ebola vaccine was approved in \\underline{2021}}'' contradicts the source content ``\\textit{The first vaccine for Ebola was approved by the FDA in \\underline{2019}.}''. \n\\item \\textbf{Extrinsic Hallucinations}: The generated output that cannot be verified from the source content (i.e., output that can neither be supported nor contradicted by the source). \nFor example, in the abstractive summarization task from Table \\ref{Table: example}, the information ``\\textit{China has already started clinical trials of the COVID-19 vaccine.}'' is not mentioned in source. We can neither find evidence for the generated output from the source nor assert that it is wrong.\nNotably, the extrinsic hallucination is not always erroneous because it could be from factually correct external information~. Such factual hallucination can be helpful because it recalls additional background knowledge to improve the informativeness of the generated text. \nHowever, in most of the literature, extrinsic hallucination is still treated with caution because its unverifiable aspect of this additional information increases the risk from a factual safety perspective. \n\\end{enumerate}", "cites": [1291, 8543, 8542, 2346], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the concept of hallucination by integrating the intrinsic and extrinsic categories from multiple works, creating a coherent framework. It also critically addresses the nuance that extrinsic hallucinations may not always be harmful, showing an analytical perspective. The abstraction is strong as it generalizes these categories across NLG tasks and highlights broader implications for factual safety."}}
{"id": "82fa3312-c888-4cb1-83e1-971b2f72e75c", "title": "Task Comparison", "level": "subsection", "subsections": [], "parent_id": "8a937c6a-663f-436e-8440-8c482962a1cb", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Definitions"], ["subsection", "Task Comparison"]], "content": "The previous subsection is about the definition and categorization of hallucination commonly shared by many NLG tasks. Yet, there are some task-specific differences. \nFor the abstractive summarization, data-to-text, and dialogue tasks, the main difference is in what serves as the ``source'' and the level of tolerance towards hallucinations. \n The source in abstractive summarization is the input source text that is being summarized~, while the source in data-to-text is non-linguistic data~, and the source(s) in the dialogue system is dialogue history and/or the external knowledge sentences.\nTolerance towards hallucinations is very low in both the summarization~ and data-to-text tasks~ because it is essential to provide faithful generation. In contrast, the tolerance is relatively higher in dialogue systems because the desired characteristics are not only faithfulness but also user engagement, especially in open-domain dialogue systems~. \nFor the generative question answering (GQA) task, the exploration of hallucination is at its early stage, so there is no standard definition or categorization of hallucination yet. However, we can see that the GQA literature mainly focuses on ``intrinsic hallucination'' where the source is the world knowledge~. Lastly, unlike the aforementioned tasks, the categorizations of hallucinations in machine translation vary within the task. Most relevant literature agrees that translated text is considered a hallucination when the source text is completely disconnected from the translated target~.\nFor further details, please refer to Section~\\ref{section:translation}.", "cites": [8544, 2345, 2351, 2348, 2335, 2350, 2349, 2347], "cite_extract_rate": 0.6153846153846154, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes information from multiple cited papers to highlight task-specific differences in hallucination definitions and tolerance levels. It abstracts key patterns across NLG tasks (summarization, data-to-text, dialogue, GQA, and translation) to provide a meta-level understanding of how hallucination manifests differently depending on the source and task objectives. While it identifies some gaps (e.g., early-stage exploration in GQA), it could offer more critical analysis of mitigation techniques or limitations in the discussed approaches."}}
{"id": "518ebb18-7011-496d-8724-03128b75ed86", "title": "Terminology Clarification", "level": "subsection", "subsections": [], "parent_id": "8a937c6a-663f-436e-8440-8c482962a1cb", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Definitions"], ["subsection", "Terminology Clarification"]], "content": "\\label{subsec:terminology}\nMultiple terminologies are associated with the concept of hallucination. We provide clarification of the commonly used terminologies \\textit{hallucination}, \\textit{faithfulness}, and \\textit{factuality} to resolve any confusion. \n\\textit{Faithfulness} is defined as staying consistent and truthful to the provided source -- an antonym to \"hallucination.\" Any work that tries to maximize faithfulness thus focuses on minimizing hallucination. For this reason, our survey includes all those works that address the faithfulness of machine-generated outputs. \n\\textit{Factuality} refers to the quality of being actual or based on fact. Depending on what serves as the ``fact'', \"factuality\" and \"faithfulness\" may or may not be the same.  differentiate \"factuality\" from \"faithfulness\" by defining the ``fact'' to be the world knowledge.\nIn contrast,  use the source input as the ``fact'' to determine the factual correctness, making \"factuality\" indistinguishable from \"faithfulness\". \nIn this paper, we adopt the definition from  because we believe having such a distinction between source knowledge and world knowledge provides a more clear understanding. \nNote that the judging criteria for what is considered faithful or hallucinated (i.e., the definition of hallucination) can differ across tasks. For more details of these variation definitions, you can find in the later task-specific sections.\n\\begin{sidewaystable}[]\n\\centering\n\\resizebox{0.92\\textwidth}{!}{\n\\footnotesize\n\\begin{tabular}{clclccccccl}\n\\toprule\n\\textbf{Task}                           & \\multicolumn{1}{c}{\\textbf{Sub-Task}}                                                         & \\textbf{Type}              & \\multicolumn{7}{c}{\\textbf{Source}}                                                                                                                                                                                                                                                                                                                                                                                                                & \\multicolumn{1}{c}{\\textbf{Output}}                                                                                                                                                                                                                                                                                                                          \\\\ \\hline\n\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Abstractive\\\\Summarization\\\\~\\end{tabular}}       &                                                                                               & Intrinsic                  & \\multicolumn{7}{l}{\\begin{tabularx}{0.35\\textwidth}{X}The first vaccine for Ebola was approved by the FDA in 2019 in the US, five years after the initial outbreak in 2014. To produce the vaccine, scientists had to sequence the DNA\\end{tabularx}}                                                                                                                                                                                                 & \\begin{tabularx}{0.35\\textwidth}{X}The first Ebola vaccine was approved in \\textbf{2021}.\\end{tabularx}                                                                                                                                                              \\\\\n                              &                                                                                               & Extrinsic                  & \\multicolumn{7}{l}{\\begin{tabularx}{0.35\\textwidth}{X}of Ebola, then identify possible vaccines, and finally show successful clinical trials. Scientists say a vaccine for COVID-19 is unlikely to be ready this year, although clinical trials have already started.\\end{tabularx}}                                                                                                                                                               & \\begin{tabularx}{0.35\\textwidth}{X}\\textbf{China} has already started clinical trials of the COVID-19 vaccine. \\end{tabularx}                                                                                                                                                                                                                                                                                         \\\\ \\hline\n\\multirow{8}{*}{Dialogue}      & \\multicolumn{1}{c}{\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Task-\\\\ oriented\\\\~\\end{tabular}}} & Intrinsic                  & \\multicolumn{7}{l}{\\multirow{2}{*}{\\begin{tabularx}{0.35\\textwidth}{X}inform (NAME = pickwick hotel, PRICERANGE = moderate)\\end{tabularx}}}                                                                                                                                                                                                                                                                                                                                                          & \\begin{tabularx}{0.35\\textwidth}{X}the hotel named pickwick hotel is in a \\textbf{high} price range.\\\\ \\\\\\end{tabularx}                                                                                                                                                                                                                                                                                                    \\\\\n                              & \\multicolumn{1}{c}{}                                                                          & Extrinsic                  & \\multicolumn{7}{l}{}                                                                                                                                                                                                                                                                                                                                                                                                                               & \\begin{tabularx}{0.35\\textwidth}{X}the pickwick hotel \\textbf{in san diego} is a moderate price range\\\\\\\\ \\end{tabularx}                                                                                                                                                                                                                                                                                               \\\\ \\cline{2-11} \n                              & \\multicolumn{1}{c}{\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Open-\\\\ domain \\\\\\end{tabular}}}   & Intrinsic                  & \\multicolumn{7}{l}{\\begin{tabularx}{0.35\\textwidth}{X}Dialog History: \\\\ Speaker 1: What do you think about Murray? \\\\ Speaker 2: I think Murray is a great player he just needs to stay healthy in order to compete more. Who do you like best?\\end{tabularx}} &\\begin{tabularx}{0.35\\textwidth}{X} Speaker 1: I like \\textbf{Roger Nadal} the most!\\\\\\\\\\\\\\ \\end{tabularx}                                                                                                                                                                                                                                                                                                                                  \\\\\n                              & \\multicolumn{1}{c}{}                                                                          & Extrinsic                  & \\multicolumn{7}{l}{\\begin{tabularx}{0.35\\textwidth}{X}Knowledge: In tennis, the term Big Four refers to the quartet of men’s singles players comprising Roger Federer, Rafael Nadal, Novak Djokovic, and Andy Murray.\\end{tabularx}}                                                                                                                                                                                                                                                                                                                                                                                                                            & \\begin{tabularx}{0.35\\textwidth}{X}Speaker 1: I like Djokovic. He has played \\textbf{in the top ten singles players of the world}.\\end{tabularx}                                                                                                                                                                                                                                                                                 \\\\ \\hline\n\\multirow{6}{*}{\\begin{tabular}[c]{@{}c@{}}Generative QA\\\\~\\end{tabular}} &                                                                                               & Intrinsic                  & \\multicolumn{7}{l}{\\begin{tabularx}{0.35\\textwidth}{X}Question: dow jones industrial average please? \\end{tabularx}}                                                                                                                                                                                                                                                                                                                                                                                  & \\begin{tabularx}{0.35\\textwidth}{X}Answer: The Dow Jones Industrial Average(DJIA) is an index of \\textbf{30 major U.S.stock indexes}. The DJIAwas created in 1896 by Charles Dow, a statistician and editor of The Wall Street Journal. The index was first published in the mid-1880s, and \\textbf{was last updated on May 26,1896.}\\end{tabularx}                         \\\\\n                              &                                                                                               & Extrinsic                  & \\multicolumn{7}{l}{Question: definition of sadduction}                                                                                                                                                                                                                                                                                                                                                                                             & \\begin{tabularx}{0.35\\textwidth}{X}Answer: Sadducees. noun. \\textbf{The definition of a sadducee is a person who acts in a deceitful or duplicitous manner. An example of a Sadduceee is a politician who acts deceitfully in order to gain political power}.1 a  member of a Jewish sect that was active during the Second Temple.\\end{tabularx}                        \\\\ \\hline\n\\multirow{3}{*}{\\begin{tabular}[c]{@{}c@{}}Data2text\\\\~\\end{tabular}} &                                                                                               & \\multirow{2}{*}{Intrinsic} & \\multicolumn{1}{c}{TEAM}                                  & CITY                                  & WIN                              & LOSS                              & PTS                              & FG\\_PCT                              & BLK                             & \\multirow{2}{*}{\\begin{tabularx}{0.35\\textwidth}{X}The Houston Rockets \\textbf{(18-4)} defeated the Denver Nuggets (10-13) 108-96 on Saturday.\\end{tabularx}}                                                                                                                                                                                          \\\\ \\cline{4-10}\n                                                                                                                  &                                                                                               &                            & \\multicolumn{1}{c}{Rockets}                               & Houston                               & 18                               & 5                                 & 108                              & 44                                   & 7                               &                                                                                                                                                                                                                                                                                                                                         \\\\\n                                                                                                                  &                                                                                               & Extrinsic                  & \\multicolumn{1}{c}{Nuggets}                               & Denver                                & 10                               & 13                                & 96                               & 38                                   & 7                               & \\begin{tabularx}{0.35\\textwidth}{X} \\textbf{Houston has won two straight games and six of their last seven.}\\end{tabularx}                                                                                                                                                                                                                                                                         \\\\ \\hline\n\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Translation\\\\~\\end{tabular}}  &                                                                                               & Intrinsic                 & \\multicolumn{7}{l}{\\begin{tabularx}{0.35\\textwidth}{X} \\begin{CJK*}{UTF8}{gbsn}迈克周四去书店。\\end{CJK*} (Michael went to the bookstore on Thursday.) \\end{tabularx}}                                                                                                                                                                                                                                                                               & \\begin{tabularx}{0.35\\textwidth}{X}\\textbf{Jerry didn't go} to the bookstore.\\end{tabularx}                                                                                                                                                                                                           \\\\\n                              &                                                                                               & Extrinsic                & \\multicolumn{7}{l}{\\begin{tabularx}{0.35\\textwidth}{X}\\begin{CJK*}{UTF8}{gbsn}迈克周四去书店。\\end{CJK*} (Michael went to the bookstore on Thursday.) \\end{tabularx}}                                                                                                                                                                                                                                                                                                                 & \\begin{tabularx}{0.35\\textwidth}{X}Michael \\textbf{happily} went to the bookstore \\textbf{with his friend.} \\end{tabularx}                                                                                                                                                                         \\\\ \\bottomrule\n\\end{tabular}\n}\n\\caption{Examples of each category of hallucinations for each task.\n In the Data2Text task, H/A: home/away, MIN: minutes, PTS: points, REB: rebounds, AST: assists, BLK: blocks, FG\\_PCT: field goals percentage. The examples for VL tasks are shown in Figure~\\ref{fig:captioning_examples} and Figure~\\ref{fig:vqa_examples}}\n\\label{Table: example}\n\\end{sidewaystable}\n\\begin{table*}[]\n\\resizebox{\\linewidth}{!}{\n\\footnotesize\n\\centering\n\\begin{tabular}{cccl}\n\\toprule\n\\multicolumn{1}{c}{}                                                          & \\textbf{Category}                                                                   & \\multicolumn{1}{c}{\\textbf{Task}} & \\multicolumn{1}{c}{\\textbf{Works}} \\\\ \\midrule\n\\multirow{16}{*}{\\begin{tabular}[c]{@{}c@{}}\\textbf{Automatic}\\\\ \\textbf{Metrics}\\end{tabular}}                                                  & \\multirow{4}{*}{\\begin{tabular}[c]{@{}c@{}}Statistical \\end{tabular}}                                                         & Dialogue                    &   \\\\\\cline{3-4}   \n                                                                              &                                                                                  & Data2Text                 &    \\\\     \\cline{3-4} \n                                                                              &\n                                                                       &\n                                         Translation                &    \\\\   \\cline{3-4}  \n                                         & & Captioning & \\\\\n                                         \\cline{2-4} \n                                         &\n                                         \\multirow{10}{*}{\\begin{tabular}[c]{@{}c@{}}Model-\\\\ based\\end{tabular}}                                                        &     \\begin{tabular}[c]{@{}c@{}}Abstractive\\\\Summarization\\end{tabular}                         & \\begin{tabular}[c]{@{}l@{}}\\\\\\\\\\end{tabular}     \\\\\\cline{3-4} \n                                                                              &                                                                                  &Dialogue                    &     \\begin{tabular}[c]{@{}l@{}} \\\\ \\end{tabular} \\\\\\cline{3-4} \n                                                                              &                                                                                  & Generative QA                  &  \\begin{tabular}[c]{@{}l@{}}$^*$,$^*$,$^*$\\\\$^*$, \\end{tabular}   \\\\\\cline{3-4} \n                                                                              &                                                                                  & Data2Text                 &    \n                                                                              \\begin{tabular}[c]{@{}l@{}}\\\\ \\end{tabular} \\\\\\cline{3-4}  \n                                                                              &                                                                                  & Translation               &   \n                                                   \\begin{tabular}[c]{@{}l@{}}\\\\ \\\\ \\end{tabular}                           \\\\\\cline{3-4} \n                                                                              &                                                                                  & Task-Agnostic               &     \\\\  \n\\midrule\n\\multirow{22}{*}{\\begin{tabular}[c]{@{}c@{}}\\textbf{Mitigation}\\\\ \\textbf{Method}\\end{tabular}} & \n                                                                             \\multirow{8}{*}{\\begin{tabular}[c]{@{}c@{}}Data-\\\\Related\\end{tabular}}        &  \\begin{tabular}[c]{@{}c@{}}Abstractive\\\\Summarization\\end{tabular}                    &  \\begin{tabular}[c]{@{}l@{}}\\\\\\end{tabular} \\\\ \\cline{3-4} \n                                                                              &                                                                                  & Dialogue                    & \\begin{tabular}[c]{@{}l@{}}\\\\  \\end{tabular}\\\\\\cline{3-4} \n                                                                              &                                                                                  & Generative QA                        &                          \\\\\\cline{3-4} \n                                                                              &                                                                                 & Data2Text                 &  \\begin{tabular}[c]{@{}l@{}}\\\\ \\end{tabular} \\\\\\cline{3-4} \n                                                                              &                                                                                  & Translation               &  \\begin{tabular}[c]{@{}l@{}} \\\\  \\end{tabular}  \\\\ \\cline{3-4} \n                                                                              & & Captioning &  \\\\  \\cline{2-4}\n                                                                            & \\multirow{12}{*}{\\begin{tabular}[c]{@{}c@{}}Modeling\\\\and\\\\ Inference\\end{tabular}}     &  \\begin{tabular}[c]{@{}c@{}}Abstractive\\\\Summarization\\end{tabular}                    & \\begin{tabular}[c]{@{}l@{}}\\\\ \\\\\\end{tabular}\n                                                                         \\\\\\cline{3-4} \n                                                                              &                                                                                  &Dialogue                    & \n                                                                              \\begin{tabular}[c]{@{}l@{}}\\\\ \\end{tabular}        \\\\\\cline{3-4} \n                                                                              &                                                                                  & Generative QA                        &          \\begin{tabular}[c]{@{}l@{}}\\\\ \\end{tabular}                  \\\\\\cline{3-4} \n                                                                              &                                                                                  & Data2Text                 & \n                                                                              \\begin{tabular}[c]{@{}l@{}}\\\\\\\\\\end{tabular}  \\\\\\cline{3-4} \n                                                                              &                                                                                  & Translation               & \n                                                                              \\begin{tabular}[c]{@{}l@{}}\\\\\\\\\\\\ \\end{tabular} \\\\ \\cline{3-4}\n                                                                              & & Captioning &  \\\\\n                                                                              \\bottomrule\n\\end{tabular}}\n\\caption{Evaluation metrics and mitigation methods for each task. *The hallucination metrics are not specifically proposed for generative question answering (GQA), but they can be adapted for that task. } \n\\label{Table: overall}\n\\end{table*}", "cites": [2380, 2379, 2386, 8545, 2353, 2377, 2339, 2370, 2368, 8550, 1291, 1112, 446, 8547, 2340, 2373, 2363, 1998, 2371, 2366, 2335, 2349, 2369, 7569, 2344, 2357, 8544, 2367, 2361, 2382, 2354, 2374, 2345, 2387, 2381, 2352, 2359, 2355, 2378, 2372, 2343, 2356, 2362, 2348, 2364, 1106, 432, 2365, 2385, 8543, 2384, 2360, 8546, 7570, 2376, 2383, 2006, 2358, 2375, 8549, 8548, 2341, 7189], "cite_extract_rate": 0.7764705882352941, "origin_cites_number": 85, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section demonstrates strong synthesis by clearly distinguishing between faithfulness and factuality, and linking these concepts to the broader issue of hallucination across various NLG tasks. It also references multiple papers to illustrate the nuanced definitions and their implications. While it provides some critical analysis by acknowledging that definitions and criteria can vary across tasks, it does not deeply critique the methods or compare their effectiveness. The abstraction is moderate, identifying the importance of source and world knowledge but not offering a fully meta-level insight into overarching principles."}}
{"id": "d0956bbf-f013-4ee0-8810-1864c57e7353", "title": "Heuristic data collection", "level": "paragraph", "subsections": ["cab16c74-c973-4a67-83c3-04481091e3c2"], "parent_id": "3797754b-7c09-45c3-bbda-2e4f79919e4d", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Contributors to Hallucination in NLG"], ["subsection", "Hallucination from Data"], ["paragraph", "Heuristic data collection"]], "content": "When collecting large-scale datasets, some works heuristically select and pair real sentences or tables as the source and target~. As a result, the target reference may contain information that cannot be supported by the source~. \nFor instance, when constructing WIKIBIO~, a dataset for generating biographical notes based on the infoboxes of Wikipedia, the authors took the Wikipedia infobox as the source and the first sentence of the Wikipedia page as the target ground-truth reference. However, the first sentence of the Wikipedia article is not necessarily equivalent to the infobox in terms of the information they contain. Indeed,  points out that 62\\% of the first sentences in WIKIBIO have additional information not stated in the corresponding infobox. Such mismatch between source and target in datasets can lead to hallucination.\nAnother problematic scenario is when duplicates from the dataset are not properly filtered out. It is almost impossible to check hundreds of gigabytes of text corpora manually.  show that duplicated examples \nfrom the pretraining corpus bias the model to favor generating repeats of the memorized phrases from the duplicated examples.", "cites": [2345, 2385, 2388, 448, 2372], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to explain how heuristic data collection practices contribute to hallucination in NLG, particularly in datasets like WIKIBIO and ToTTo. It offers a critical perspective by pointing out specific problems such as unmatched information between source and target and the effects of duplication. However, the analysis remains focused on concrete examples and does not rise to the level of presenting a novel framework or overarching principle."}}
{"id": "cab16c74-c973-4a67-83c3-04481091e3c2", "title": "Innate divergence", "level": "paragraph", "subsections": [], "parent_id": "d0956bbf-f013-4ee0-8810-1864c57e7353", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Contributors to Hallucination in NLG"], ["subsection", "Hallucination from Data"], ["paragraph", "Heuristic data collection"], ["paragraph", "Innate divergence"]], "content": "Some NLG tasks by nature do not always have factual knowledge alignment between the source input text and the target reference, especially those that value diversity in generated output. \nFor instance, it is acceptable for open-domain dialogue systems to respond in chit-chat style, subjective style~, or with a relevant fact that is not necessarily present in the user input, history or provided knowledge source -- this improves the engagingness and diversity of the dialogue generation. However, researchers have discovered that such dataset characteristic leads to inevitable extrinsic hallucinations.", "cites": [8545], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic analytical perspective by acknowledging that certain NLG tasks inherently encourage hallucination due to their design. It cites one relevant paper to support the idea that retrieval-augmented methods can help mitigate hallucination in dialogue systems, but it does not deeply synthesize or compare multiple sources. The abstraction is moderate as it touches on the general nature of task design influencing hallucination, but lacks a broader framework or deep pattern identification."}}
{"id": "2f8e4b4c-facd-40d2-bd2e-1761993f766f", "title": "Hallucination from Training and Inference", "level": "subsection", "subsections": ["3ed9fe1a-6ed2-480a-8372-66b4deb723da"], "parent_id": "e1f0a692-0b42-4ca4-a3e0-b58a67996dbe", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Contributors to Hallucination in NLG"], ["subsection", "Hallucination from Training and Inference"]], "content": "As discussed in the previous subsection, source-reference divergence existing in dataset is one of the contributors of hallucination. However,  show that hallucination problem still occurs even when there is very little divergence in dataset. This is because there is another contributor of hallucinations -- training and modeling choices of neural models~.", "cites": [2345, 2339, 1877, 2335, 2340], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section begins with a brief analytical point by noting that hallucination persists even with low source-reference divergence, but it lacks detailed synthesis of the cited papers. It mentions the relevance of training and modeling choices without elaborating on how the cited works connect or contribute to this understanding. The analysis is minimal, and no broader abstraction or framework is presented."}}
{"id": "3ed9fe1a-6ed2-480a-8372-66b4deb723da", "title": "Imperfect representation learning", "level": "paragraph", "subsections": ["a16f0078-6ca0-4bdf-9010-c75ffa110095", "247ac41d-9750-4458-af11-c2e10b72aebc", "184e3dfb-5a4e-4990-970a-25e8dca6fba4"], "parent_id": "2f8e4b4c-facd-40d2-bd2e-1761993f766f", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Contributors to Hallucination in NLG"], ["subsection", "Hallucination from Training and Inference"], ["paragraph", "Imperfect representation learning"]], "content": "The encoder has the role of comprehending and encoding input text into meaningful representations. An encoder with a defective comprehension ability could influence the degree of hallucination~.\nWhen encoders learn wrong correlations between different parts of the training data, it could result in erroneous generation that diverges from the input~.", "cites": [2345, 2359, 8548, 2364], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a minimal synthesis of the cited works, merely stating that imperfect representation learning in encoders can lead to hallucination. It lacks critical evaluation of the papers or their methods, and does not abstract beyond the immediate observation to offer broader theoretical or conceptual insights."}}
{"id": "a16f0078-6ca0-4bdf-9010-c75ffa110095", "title": "Erroneous decoding", "level": "paragraph", "subsections": [], "parent_id": "3ed9fe1a-6ed2-480a-8372-66b4deb723da", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Contributors to Hallucination in NLG"], ["subsection", "Hallucination from Training and Inference"], ["paragraph", "Imperfect representation learning"], ["paragraph", "Erroneous decoding"]], "content": "The decoder takes the encoded input from the encoder and generates the final target sequence. Two aspects of decoding contribute to hallucinations. First, decoders can attend to the wrong part of the encoded input source, leading to erroneous generation~. \nSuch wrong association results in generation with facts mixed up between two similar entities~.\nSecond, the design of the decoding strategy itself can contribute to hallucinations. \n illustrate that a decoding strategy that improves the generation diversity, such as top-k sampling, is positively correlated with increased hallucination. We conjecture that deliberately added ``randomness'' by sampling from the top-k samples instead of choosing the most probable token increase the unexpected nature of the generation, leading to a higher chance of containing hallucinated content.", "cites": [8545, 8543, 2364], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates ideas from three different papers to discuss how decoding strategies contribute to hallucinations in NLG. It connects the concept of attention misalignment and decoding diversity as common sources of hallucination. While it provides some analytical insight, it does not offer a novel framework or deep critique, and the abstraction remains at a moderate level, focusing on general patterns without reaching a meta-level synthesis."}}
{"id": "247ac41d-9750-4458-af11-c2e10b72aebc", "title": "Exposure Bias", "level": "paragraph", "subsections": [], "parent_id": "3ed9fe1a-6ed2-480a-8372-66b4deb723da", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Contributors to Hallucination in NLG"], ["subsection", "Hallucination from Training and Inference"], ["paragraph", "Imperfect representation learning"], ["paragraph", "Exposure Bias"]], "content": "Regardless of decoding strategy choices, the exposure bias problem~, defined as the discrepancy in decoding between training and inference time, can be another contributor to hallucination. \nIt is common practice to train the decoder with teacher-forced maximum likelihood estimation (MLE) training, where the decoder is encouraged to predict the next token conditioned on the ground-truth prefix sequences. However, during the inference generation, the model generates the next token conditioned on the historical sequences previously generated by itself~. Such a discrepancy can lead to increasingly erroneous generation, especially when the target sequence gets longer.", "cites": [2389, 1106], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the concept of exposure bias by linking the training-inference discrepancy to the broader issue of hallucination in NLG. It references two key papers that address this problem but does not go beyond identifying the issue to critically evaluate or compare the proposed solutions. The abstraction is moderate as it generalizes the problem to the training and inference process, but does not elevate the discussion to a meta-level or overarching framework."}}
{"id": "184e3dfb-5a4e-4990-970a-25e8dca6fba4", "title": "Parametric knowledge bias", "level": "paragraph", "subsections": [], "parent_id": "3ed9fe1a-6ed2-480a-8372-66b4deb723da", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Contributors to Hallucination in NLG"], ["subsection", "Hallucination from Training and Inference"], ["paragraph", "Imperfect representation learning"], ["paragraph", "Parametric knowledge bias"]], "content": "Pre-training of models on a large corpus is known to result in the model memorizing knowledge in its parameters~. This so-called parametric knowledge helps improve the performance of downstream tasks but also serves as another contributor to hallucinatory generation. Large pre-trained models used for downstream NLG tasks are powerful in providing generalizability and coverage, but  have discovered that such models prioritize parametric knowledge over the provided input. In other words, models that favor generating output with their parametric knowledge instead of the information from the input source can result in the hallucination of excess information in the output. \nOn the other hand, current research works~ highlight a discrepancy between surface realization and inherent knowledge of the model in NLG tasks. Models can realize they are generating something hallucinated in some way.", "cites": [7092, 2390, 2391, 1565, 8551, 2392], "cite_extract_rate": 1.0, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a reasonable synthesis by connecting the concept of parametric knowledge from multiple papers to explain how it contributes to hallucination. It abstracts the idea that models may prioritize internal knowledge over input, leading to inaccuracies, but does not go beyond the cited works to form a novel framework. Critical analysis is limited, as it mainly states findings without evaluating trade-offs or limitations of the methods."}}
{"id": "49fa7ebf-cc16-4d0a-985c-014265d655bf", "title": "Metrics Measuring Hallucination", "level": "section", "subsections": ["d2806810-0d49-41ee-afad-b32c8ec7b313", "1d238477-6a4e-4ed4-ae23-8683c86b28ff", "33cd558d-85b9-45e7-bd14-91d058fafb41"], "parent_id": "c4299bf7-fa65-4214-968a-c213c95e6d07", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Metrics Measuring Hallucination"]], "content": "\\label{section:metric}\nRecently, various studies have illustrated that most conventional metrics used to measure the quality of writing are not adequate for quantifying the level of hallucination~. It has been shown that state-of-the-art abstractive summarization systems, evaluated with metrics such as ROUGE, BLEU, and METEOR, have hallucinated content in 25\\% of their generated summaries~. A similar phenomenon has been shown in other NLG tasks, where it has been discovered that traditional metrics have a poor correlation with human judgment in terms of the hallucination problem~. \nTherefore, there are active research efforts to define effective metrics for quantifying hallucination. \n\\ziwei{FRANK~ surveys the faithfulness metrics for summarization and compares these metrics' correlations with human judgments. To assess the example-level accuracy of metrics in diverse tasks, TRUE~ reports their Area Under the ROC Curve (ROC AUC) in regard to hallucinated example detection.\n}", "cites": [2393, 2343, 2349, 2376, 2372], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the limitations of traditional NLG evaluation metrics and integrates findings from multiple papers to argue for the need for more effective hallucination metrics. It briefly introduces key frameworks such as FRANK and TRUE, showing their purpose and relevance, but lacks in-depth comparison or critique. The abstraction is limited to a general observation about metric shortcomings rather than broader theoretical or conceptual generalizations."}}
{"id": "d2806810-0d49-41ee-afad-b32c8ec7b313", "title": "Statistical Metric", "level": "subsection", "subsections": [], "parent_id": "49fa7ebf-cc16-4d0a-985c-014265d655bf", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Metrics Measuring Hallucination"], ["subsection", "Statistical Metric"]], "content": "\\label{section:statistical_metric}\nOne of the simplest approaches is to leverage lexical features (n-grams) to calculate the information overlap and contradictions between the generated and the reference texts -- the higher the mismatch counts, the lower the faithfulness and thus the higher the hallucination score. \nGiven that many traditional metrics leverage the target text as the ground-truth reference (e.g., ROUGE, BLEU, etc.),  build upon this idea and propose PARENT (Precision And Recall of Entailed n-grams from the Table)~\\footnote{Note that PARENT is a general metric like ROUGE and BLEU, not only constrained to hallucination}, a metric which can also measure hallucinations using \\textit{both} the source and target text as references.\nParticularly, PARENT n-gram lexical entailment matches generated text with both the source table and target text. The F1-score that combines the precision and recall of the entailment reflects the accuracy of the table-to-text task.\nThe source text is additionally used because it is not guaranteed that the output target text contains the complete set of information available in the input source text.\nIt is common for NLG tasks to have multiple plausible outputs from the same input, which is known as one-to-many mapping~. \nIn practice, however, covering all the possible outputs is too expensive and almost impossible. Thus, many works simplify the hallucination evaluation setup by relying on the source text as the sole reference. Their metrics just focus on the information referred by input sources to measure hallucinations, especially intrinsic hallucinations.\nFor instance,  propose PARENT-T, which simplifies PARENT by only using table content as the reference. \nSimilarly, Knowledge F1~ -- a variant of unigram F1 -- has been proposed for knowledge-grounded dialogue tasks to measure the overlap between the model’s generation and the knowledge used to ground the dialogue during dataset collection. \nFurthermore,  proposed a bag-of-vectors sentence similarity (BVSS) metric for measuring sentence adequacy in machine translation, that only refers to the target text. This statistical metric helps to determine whether the MT output has a different amount of information than the translation reference. \nAlthough simple and effective, one potential limitation of lexical matching is that it can only handle lexical information. Thus, it fails to deal with syntactic or semantic variations~.", "cites": [2394, 2361, 2348, 8545, 1984, 2372], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively synthesizes information from multiple papers, particularly those related to PARENT and its variants, while connecting the concept of one-to-many mapping to the limitations of relying solely on target text. It also critically evaluates the shortcomings of lexical-based metrics, such as their inability to capture syntactic or semantic variations. The abstraction is moderate, as it identifies general challenges in hallucination evaluation but does not offer a high-level conceptual framework beyond the statistical metrics."}}
{"id": "5e9b919e-064f-47fe-a489-68235670134a", "title": "Information Extraction (IE)-based", "level": "subsubsection", "subsections": [], "parent_id": "1d238477-6a4e-4ed4-ae23-8683c86b28ff", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Metrics Measuring Hallucination"], ["subsection", "Model-based Metric"], ["subsubsection", "Information Extraction (IE)-based"]], "content": "It is not always easy to determine which part of the generated text contains the knowledge that requires verification. IE-based metrics use IE models to represent the knowledge in a simpler relational tuple format (e.g., \\textit{subject, relation, object}), then verify against relation tuples extracted from the source/reference. \nHere, the IE model is identifying and extracting the ``facts\" that require verification. In this way, words containing no verifiable information (e.g., stopwords, conjunctions, etc) are not included in the verification step.\nFor example, ground-truth reference text \\texttt{``Brad Pitt was born in 1963''} and generated text \\texttt{``Brad Pitt was born in 1961''} will be mapped to the relation triples \\texttt{(Brad Pitt, born-in, 1963)} and \\texttt{(Brad Pitt, born-in, 1961)} respectively~\\footnote{This is an example from~}. The mismatch between the dates (1963$\\neq$1961) indicates that there is hallucination. \nOne limitation associated with this approach is the potential error propagation from the IE model.", "cites": [2374], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of IE-based metrics for hallucination by explaining their methodology and use in identifying verifiable facts. It synthesizes the approach from the cited paper by describing how IE models extract relational tuples and compares them to ground truth for hallucination detection. A minor limitation is mentioned, but the analysis could be deeper with more comparative or evaluative discussion of related works."}}
{"id": "69ef0f8d-3ea8-4a07-960e-4ce1b82240f9", "title": "QA-based", "level": "subsubsection", "subsections": [], "parent_id": "1d238477-6a4e-4ed4-ae23-8683c86b28ff", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Metrics Measuring Hallucination"], ["subsection", "Model-based Metric"], ["subsubsection", "QA-based"]], "content": "This approach implicitly measures the knowledge overlap or consistency between the generation and the source reference. This is based on the intuition that similar answers will be generated from a same question if the generation is factually consistent with the source reference. It is already put in use to evaluate hallucinations in many tasks, such as summarization \\ziwei{~, dialogue~, and data2text generation~}.\nQA-based metric that measures the faithfulness of the generated text is consisted of three parts: First, given a generated text, a question generation (QG) model generates a set of question-answer pairs. \nSecond, a question answering (QA) model answers the generated questions given a ground-truth source text as the reference (containing knowledge). Lastly, the hallucination score is computed based on the similarity of the corresponding answers.\nSimilar to the IE-based metrics, the limitation of this approach is the potential error that might arise and propagated from either the QG model or the QA model.", "cites": [8549, 2358, 2379], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear explanation of QA-based metrics for hallucination detection, referencing three papers that use similar question-answering paradigms across different NLG tasks. It synthesizes the core idea of using QA models to assess factual consistency and highlights the shared limitation of model errors. However, it lacks deeper comparative analysis or broader theoretical generalization, limiting its abstraction and critical depth."}}
{"id": "711aa3e5-4137-4564-852c-b4871c48d67d", "title": "Natural Language Inference (NLI) Metrics", "level": "subsubsection", "subsections": [], "parent_id": "1d238477-6a4e-4ed4-ae23-8683c86b28ff", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Metrics Measuring Hallucination"], ["subsection", "Model-based Metric"], ["subsubsection", "Natural Language Inference (NLI) Metrics"]], "content": "There are not many labelled datasets for hallucination detection tasks, especially at the early stage when the hallucination problem starts to gain attention. As an alternative, many works leverage the NLI dataset to tackle hallucinations. Note that NLI is a task that determines whether a “hypothesis” is true (entailment), false (contradiction), or undetermined (neutral) given a “premise”. These metrics are based on the idea that only the source knowledge reference should entail the entirety of the information in faithful and hallucination-free generation ~. \nMore specifically, NLI-based metrics define the hallucination/faithfulness score to be the entailment probability between the source and its generated text, also known as the percentage of times generated text entails, neutral to, and contradicts the source. \nAccording to~, NLI-based approaches are more robust to lexical variability than token matching approaches such as IE-based and QA-based metrics. Nevertheless, as illustrated by~, off-the-shelf NLI models tend to transfer poorly to the abstractive summarization task. Thus, there is a line of research in improving and extending the NLI paradigm specifically for hallucination evaluation purposes~. Apart from generalizability,  point out the potential limitation of using sentence-level entailment models, namely their incapability to pinpoint and locate which parts of the generation are erroneous. In response, the authors propose a new dependency-level entailment and attempt to identify factual inconsistencies in a more fine-grained manner.", "cites": [2370, 8542, 2381, 7189, 2374, 2383], "cite_extract_rate": 0.6, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple cited works to highlight the use and limitations of NLI-based metrics in measuring hallucination. It critically examines the generalizability and localization issues of such models and introduces a broader analytical perspective by discussing the evolution of NLI approaches from sentence-level to dependency-level entailment."}}
{"id": "502ea404-9791-4733-9ad1-7cdfce2b1f9f", "title": "Faithfulness Classification Metrics", "level": "subsubsection", "subsections": [], "parent_id": "1d238477-6a4e-4ed4-ae23-8683c86b28ff", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Metrics Measuring Hallucination"], ["subsection", "Model-based Metric"], ["subsubsection", "Faithfulness Classification Metrics"]], "content": "To improve upon NLI-based metrics, task-specific datasets are constructed to improve from the NLI-based metrics.\n constructed syntactic data by automatically inserting hallucinations into training instances. \n\\ziwei{ and  construct new corpora for faithfulness classification in dialogue responses. They manually annotate the Wizard-of-Wikipedia dataset~, a knowledge grounded dialog dataset, by judging whether each response is hallucinated.}\nFaithfulness specific datasets can be better than NLI datasets because entailment or neutral labels of NLI datasets and faithfulness are not equivalent. \nFor example, the hypothesis ``Putin is U.S. president'' can be considered to be either neutral to or entailed from the premise ``Putin is president''. However, from the faithfulness perspective, the hypothesis contains unsupported information ``U.S.'', which is deemed to be hallucination.", "cites": [2344, 8546, 457], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a coherent synthesis by linking the limitations of NLI-based metrics to the development of faithfulness-specific datasets, using examples from the cited works. It offers some critical analysis by pointing out the mismatch between NLI labels and faithfulness judgments. However, it stops short of deeper evaluation or identifying broader frameworks or principles, remaining somewhat focused on specific examples rather than generalizing more extensively."}}
{"id": "5334ae06-b6bc-44d4-97ab-6ee74cc193f2", "title": "LM-based Metrics", "level": "subsubsection", "subsections": [], "parent_id": "1d238477-6a4e-4ed4-ae23-8683c86b28ff", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Metrics Measuring Hallucination"], ["subsection", "Model-based Metric"], ["subsubsection", "LM-based Metrics"]], "content": "These metrics leverage two language models (LMs) to determine if each token is supported or not:\nAn unconditional LM is only trained on the targets (ground-truth references) in the dataset, while a conditional language model $LM_x$ is trained on both source and target data.\nIt is assumed that the next token is inconsistent with the input if unconditional LM gets a smaller loss than conditional $LM_x$ during forced-path decoding~. \nWe classify the generated token as hallucinatory if the loss from LM is lower. The ratio of hallucinated tokens to the total number of target tokens $|y|$ can reflect the hallucination degree.", "cites": [2343, 2364], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of LM-based metrics for hallucination, explaining the general approach using two LMs. It integrates the idea from both cited papers but lacks in-depth critical evaluation or abstraction into broader principles. The explanation is coherent but does not go beyond summarizing the methods presented in the papers."}}
{"id": "33cd558d-85b9-45e7-bd14-91d058fafb41", "title": "Human Evaluation", "level": "subsection", "subsections": [], "parent_id": "49fa7ebf-cc16-4d0a-985c-014265d655bf", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Metrics Measuring Hallucination"], ["subsection", "Human Evaluation"]], "content": "\\label{subsec:human_evaluation}\nDue to the challenging and imperfect nature of the current automatic evaluation of hallucinations in NLG, human evaluation~ is still one of the most commonly used approaches. \nThere are two main forms of human evaluation: (1) scoring, where human annotators rate the hallucination level in a range; \nand (2) comparing, where human annotators compare the output texts with baselines or ground-truth references .\nMultiple terminologies, such as\n \\textit{faithfulness}~, \n \\textit{factual consistency}~, \\textit{fidelity}~, \n \\textit{factualness}\\footnote{uses the source input as the “fact”. \\label{fact}}~, \n \\textit{factuality}$^4$~,\n or on the other hand, \\textit{hallucination}~, \n \\textit{fact contradicting}~\n  are used in the human evaluation of hallucination to rate whether the generated text is in accord with the source input.\n  use finer-grained metrics for \\textit{intrinsic hallucination} and \\textit{extrinsic hallucination} separately. \nMoreover, there are some broad metrics, such as \\textit{Correctness}~,\n\\textit{Accuracy}~,\nand \\textit{Informativeness}~ \nconsidering both missing and additional contents (extrinsic hallucinations) compared to the input source.", "cites": [2344, 2343, 8544, 2367, 2364, 446, 8547, 2386, 2365, 8543, 8545, 2353, 1291, 2363, 2345, 1998, 2387, 2006, 2371, 2366, 2369, 8550, 2352, 8552], "cite_extract_rate": 0.75, "origin_cites_number": 32, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of human evaluation approaches in the context of hallucination in NLG and lists terminology and methods from cited papers. It mentions some distinctions like intrinsic/extrinsic hallucinations and broader metrics such as Correctness and Accuracy, but lacks deeper synthesis or critical analysis of the papers. The abstraction level remains limited to identifying basic themes rather than deriving overarching principles."}}
{"id": "b519ce9b-6ca2-470a-89b1-d04b4ee861a9", "title": "Building a Faithful Dataset", "level": "subsubsection", "subsections": [], "parent_id": "f5777035-e36a-4ccc-8b98-bd9bd8fb434f", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination Mitigation Methods"], ["subsection", "Data-Related Methods"], ["subsubsection", "Building a Faithful Dataset"]], "content": "Considering that noisy data encourage hallucinations, constructing faithful datasets manually is an intuitive method, and there are various ways to build such datasets:\nOne way is employing annotators to write clean and faithful targets from scratch given the source~, which may lack diversity~.\nAnother way is employing annotators to rewrite real sentences on the web~, \nor targets in the existing dataset~. \nBasically, the revision strategy consists of three stages: \n(1) phrase trimming: removing phrases unsupported by the source in the exemplar sentence;\n(2) decontextualization: resolving co-references and deleting phrases dependent on context;\n(3) syntax modification: making the purified sentences flow smoothly.\nMeanwhile, other works~ leverage the model to generate data and instruct annotators to label whether these outputs contain hallucinations or not. \nWhile this approach is typically used to build diagnostic evaluation datasets, it has the potential to build faithful datasets.", "cites": [2345, 2396, 2385, 2395, 1963], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes different data-related methods for building faithful datasets without offering in-depth synthesis or critical analysis. It briefly mentions strategies like phrase trimming, decontextualization, and syntax modification, but does not elaborate on how these relate across the cited works. There is minimal abstraction or identification of broader patterns, and the discussion remains at a surface level without evaluating the effectiveness or limitations of the approaches."}}
{"id": "39111c49-39e3-46ef-96a0-8216e7078793", "title": "Cleaning Data Automatically", "level": "subsubsection", "subsections": [], "parent_id": "f5777035-e36a-4ccc-8b98-bd9bd8fb434f", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination Mitigation Methods"], ["subsection", "Data-Related Methods"], ["subsubsection", "Cleaning Data Automatically"]], "content": "In order to alleviate semantic noise issues, another approach is to find information that is irrelevant or contradictory to the input from the existing parallel corpus and then filter or correct the data. This approach is suitable for the case where there is a low or moderate level of noise in the original data~.\nSome works~ have dealt with the hallucination issue at the instance level by using a score for each source-reference pair and filtering out hallucinated ones.\n This corpus filtering method consists of several steps:\n (1) measuring the quality of the training samples in terms of hallucination utilizing the metrics described above; \n (2) ranking these hallucination scores in descending order;\n (3) selecting and filtering out the untrustworthy samples at the bottom.\nInstance-level scores can lead to a signal loss because divergences occur at the word level; i.e., parts of the target sentence are loyal to the source input, while others diverge~.\nConsidering this issue, other works~ correct paired training samples, specifically the input data, according to the references. This method is mainly applied in the data-to-text task because structured data are easier to correcte than utterances.\nThis method consists of two steps: (1) utilizing a model to parse the meaning representation (MR), such as attribute-value pairs, from original human textual references; (2) using the MR extracted from the reference to correct the input MR through slot matching.\nThis method will enhance the semantic consistency between input and output without abandoning a part of the dataset.", "cites": [2343, 2386, 2367, 2371, 2335], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates several data-related methods for hallucination mitigation, particularly those involving automatic data cleaning. It connects the instance-level filtering approach with correction-based methods, providing some level of synthesis. However, while it identifies limitations such as signal loss from word-level divergences, it does not deeply critique or evaluate the methods in comparison to one another. The abstraction level is moderate, as it identifies general patterns in data cleaning strategies but stops short of proposing overarching theoretical principles."}}
{"id": "fda6e98d-ec06-4c8d-aec3-3aca44182033", "title": "Information Augmentation", "level": "subsubsection", "subsections": [], "parent_id": "f5777035-e36a-4ccc-8b98-bd9bd8fb434f", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination Mitigation Methods"], ["subsection", "Data-Related Methods"], ["subsubsection", "Information Augmentation"]], "content": "\\label{subsec:augment information}\nIt is intuitive that augmenting the inputs with external information will obtain a better representation of the source. Because the external knowledge, explicit alignment, extra training data, etc., can improve the correlation between the source and target and help the model learn better task-related features. Consequently, a better semantic understanding helps alleviate the divergence from the source issue.\nExamples of the augmented information include entity information~, \nextracted relation triples from source document~ obtained by Fact Description Extraction, \\ziwei{pre-executed operation results~,}\n synthetic data generated through replacement or perturbation ~,\nretrieved external knowledge~, and retrieved similar training samples~.\nThese methods enforce a stronger alignment between inputs and outputs. \nHowever, they will bring challenges due to the gap between the original source and augmented information, such as the semantic gap between an ambiguous utterance and a distinct MR of structured data, \nand the format discrepancy between the structured knowledge graph and natural language.", "cites": [8550, 2357, 2375, 2363, 2367, 8545, 2366, 2378, 2369], "cite_extract_rate": 0.75, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section briefly mentions several papers but does not deeply integrate their findings into a coherent framework. It lists the types of augmented information and notes challenges like semantic and format gaps, but lacks critical evaluation or comparative analysis of the approaches. While it attempts to generalize a bit by highlighting common themes (e.g., alignment improvement), the synthesis remains superficial."}}
{"id": "d514dbef-51c2-482b-a616-ad5b5e4e35f0", "title": "Encoder", "level": "paragraph", "subsections": [], "parent_id": "e4b43313-4dfe-4285-82f2-c2595072f369", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination Mitigation Methods"], ["subsection", "Modeling and Inference Methods"], ["subsubsection", "Architecture"], ["paragraph", "Encoder"]], "content": "\\vspace{-0.5em} \nThe encoder learns to encode a variable-length sequence from input text into a fixed-length vector representation. \nAs we mentioned above in Section \\ref{subsec:augment information}, \n\\ziwei{hallucination appears when the models lack semantic interpretation over the input.}\nSome works have modified the encoder architecture in order to make it more compatible with input \\ziwei{and learn a better representation}.\nFor example,  and  propose a dual encoder, consisting of a sequential document encoder and a structured graph encoder to deal with the additional knowledge.\n\\vspace{-0.5em}", "cites": [2363, 2369], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic analytical overview by connecting the encoder architecture modification to the issue of hallucination, as discussed in prior sections. It synthesizes the key idea from the cited papers that hallucination arises from poor semantic interpretation and that structured representations can help. However, it lacks deeper comparative or critical analysis and does not generalize these ideas to broader principles or frameworks."}}
{"id": "e32d1ade-975c-4022-a3c8-eef8d05781c1", "title": "Attention", "level": "paragraph", "subsections": [], "parent_id": "e4b43313-4dfe-4285-82f2-c2595072f369", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination Mitigation Methods"], ["subsection", "Modeling and Inference Methods"], ["subsubsection", "Architecture"], ["paragraph", "Attention"]], "content": "The attention mechanism is an integral component in neural networks that selectively concentrates on some parts of sequences while ignoring others based on dependencies~.\nIn order to encourage the generator to pay more attention to the source,  introduce a short circuit from the input document to the vocabulary distribution via source-conditioned bias.\n employ sparse attention to improve the model‘s long-range dependencies in the hope of modeling more retrieved documents so as to mitigate the hallucination in the answer.\n adopt inductive attention, which removes potentially uninformative attention links by injecting pre-established structural information to avoid hallucinations.\n\\vspace{-0.5em}", "cites": [2359, 2376, 38, 1998], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of how attention mechanisms have been used in various works to mitigate hallucination but lacks deeper synthesis, critical evaluation, or abstraction. It lists methods introduced in different papers without comparing them, discussing their effectiveness, or identifying broader trends in attention-based hallucination mitigation."}}
{"id": "2cb4d456-5631-4405-8f1f-31272ae1083d", "title": "Decoder", "level": "paragraph", "subsections": [], "parent_id": "e4b43313-4dfe-4285-82f2-c2595072f369", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination Mitigation Methods"], ["subsection", "Modeling and Inference Methods"], ["subsubsection", "Architecture"], ["paragraph", "Decoder"]], "content": "The decoder is responsible for generating the final output in natural language given input representations~. \nSeveral work modified the decoder structures to mitigate hallucination, such as \nthe multi-branch decoder~, \nuncertainty-aware decoder~,\ndual decoder, consisting of a sequential decoder and a tree-based decoder~,\nand constrained decoder with lexical or structural limitations~.\n\\yeon{Based on the observation that the ``randomness'' from sampling-based decoding, especially near the end of sentences, can lead to hallucination,  propose to iteratively reduce the ``randomness'' through time. }\nThese decoders improve the possibility of faithful tokens while reducing the possibility of hallucinatory ones during inference by figuring out the implicit discrepancy and dependency between tokens or restricted by explicit constraints.\n\\ziwei{Since such decoders may have more difficulty generating fluent or diverse text, there is a balance to be struck between them.}", "cites": [2397, 2355, 2371, 2353, 38], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of various decoder architectures aimed at mitigating hallucination in NLG. It lists several modified decoders but does not synthesize their underlying principles or connect them into a cohesive framework. There is minimal critical analysis or abstraction, as the section focuses primarily on summarizing methods without evaluating their effectiveness or limitations."}}
{"id": "208b708e-73d1-40a7-981f-e8fb4e8be98d", "title": "Planning/Sketching", "level": "paragraph", "subsections": [], "parent_id": "d4582d46-76a9-4a93-9878-2fbd3041d2f0", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination Mitigation Methods"], ["subsection", "Modeling and Inference Methods"], ["subsubsection", "Training"], ["paragraph", "Planning/Sketching"]], "content": "Planning is a common method to control and restrict what the model generates by informing the content and its order\\ziwei{~}. Planning can be a separate step in a two-step generator\\ziwei{~}\\ziwei{, which is prone to progressive amplification of the hallucination problem.}\nOr be injected into the end-to-end model during generation~.\n Sketching has a similar function to planning, and can also be adopted for handling hallucinations~. The difference is that the skeleton is treated as a part of the final generated text.\n \\ziwei{While providing more controllability, such methods also need to strike a balance between faithfulness and diversity.}\n\\vspace{-0.5em}", "cites": [8550, 2356, 8544, 2367, 7570, 8547, 2398], "cite_extract_rate": 1.0, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of planning and sketching methods in hallucination mitigation, referencing several papers but without a clear synthesis or integration of their findings into a cohesive framework. There is limited critical analysis of the methods or their limitations, and the generalization to broader principles or patterns is minimal. The content remains largely descriptive of the approaches and their purposes."}}
{"id": "43764f46-043a-4b3f-be89-e16e02ca4e06", "title": "Reinforcement Learning (RL)", "level": "paragraph", "subsections": [], "parent_id": "d4582d46-76a9-4a93-9878-2fbd3041d2f0", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination Mitigation Methods"], ["subsection", "Modeling and Inference Methods"], ["subsubsection", "Training"], ["paragraph", "Reinforcement Learning (RL)"]], "content": "As pointed out by , word-level maximum likelihood training leads to the problem of exposure bias.\nSome works~ adopt RL to solve the hallucination problem, which utilizes different rewards to optimize the model.\nThe purpose of RL is for the agent to learn an optimal policy that maximizes the reward that accumulates from the environment~.\nThe reward function is critical to RL and, if properly designed, it can provide training signals that help the model accomplish its goal of hallucination reduction.\nFor example,  propose a slot consistency reward which is the cardinality of the difference between generated template and the slot-value pairs extracted from input dialogue act. Improving the slot consistency can help reduce the hallucination phenomenon of missing or misplacing slot values in generated templates.\n attain persona consistency sub-reward via an NLI model to reduce the hallucinations in personal facts.\n use a combination of ROUGE and the multiple-choice cloze score as the reward function to improve the faithfulness of summarization outputs. The cloze score is similar to the QA-based metric, measuring how well a QA model can address the questions by reading the generated summary (as context), where the questions are automatically constructed from the reference summary.\nAs the above examples show, some RL reward functions for mitigating hallucination are inspired by existing automatic evaluation metrics.\n\\ziwei{Although RL is challenging to learn and converge due to the extremely large search space, this method has the potential to obtain the best policy for the task without an oracle.}", "cites": [2399, 8547, 2384, 446, 2389, 2369], "cite_extract_rate": 1.0, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple approaches using RL to mitigate hallucination, connecting them through the common theme of reward design for improving faithfulness. It demonstrates some abstraction by highlighting how reward functions are inspired by existing evaluation metrics. However, the critical analysis is limited—while it notes the potential of RL and its challenges (e.g., large search space), it does not deeply evaluate the trade-offs, effectiveness, or limitations of the different methods compared."}}
{"id": "8e031514-5ae3-499c-adbd-fd722d7296c5", "title": "Multi-task Learning", "level": "paragraph", "subsections": [], "parent_id": "d4582d46-76a9-4a93-9878-2fbd3041d2f0", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination Mitigation Methods"], ["subsection", "Modeling and Inference Methods"], ["subsubsection", "Training"], ["paragraph", "Multi-task Learning"]], "content": "\\vspace{-0.5em} \nMulti-task learning is also utilized for handling hallucinations in different NLG tasks. \nIn this training paradigm, a shared model is trained on multiple tasks simultaneously to learn the commonalities of the tasks. The hallucination problem may be derived from the reliance of the training process on a single dataset, leading to the fact that the model fails to learn the actual task features. By adding proper additional tasks along with the target task during training, the model can suffer less from the hallucination problem.\nFor example,  and  incorporate a word alignment task into the translation model to improve the alignment accuracy between the input and output, and thus faithfulness.\n combine an entailment task with abstractive summarization to encourage models to generate summaries entailed by and faithful to the source.\n incorporate rationale extraction and the answer generation, which allows more confident and correct answers and reduces the hallucination problem.\nThe Multi-task approach has several advantages, such as data efficiency improvement, overfitting reduction, and fast learning. It is crucial to choose which tasks should be learned jointly, and learning multiple tasks simultaneously presents new challenges of design and optimization~.\n\\vspace{-0.5em}", "cites": [2360, 325], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers by connecting their use of multi-task learning to the goal of reducing hallucinations, showing a general trend rather than listing isolated studies. It provides a basic level of abstraction by highlighting common advantages like data efficiency and reduced overfitting, but lacks deeper critical evaluation of the specific methods or limitations. The content is analytical in nature, as it integrates the concept across different NLG tasks and explains the rationale behind using multi-task learning for hallucination mitigation."}}
{"id": "ba90d2d1-e80f-4dba-a959-c972631a46ce", "title": "Controllable Generation", "level": "paragraph", "subsections": [], "parent_id": "d4582d46-76a9-4a93-9878-2fbd3041d2f0", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination Mitigation Methods"], ["subsection", "Modeling and Inference Methods"], ["subsubsection", "Training"], ["paragraph", "Controllable Generation"]], "content": "Current works treat the hallucination level as a controllable attribute in order to remain the hallucination in outputs at a low level. \nControllable generation techniques \\ziwei{such as controlled re-sampling~, control codes that can be provided manually~, or predicted automatically~} are leveraged to improve faithfulness.\n\\ziwei{This method may require some annotated datasets for training.}\nConsidering that hallucination is not necessarily harmful and may bring some benefits, controllable methods can be further adapted to change the degree of hallucination to meet the demands of different real-world applications. \nOther general training methods such as regularization~ and loss reconstruction~ have also been proposed to tackle the hallucination problem.", "cites": [2343, 8553, 2362, 1998, 2348, 2377, 8545, 2350], "cite_extract_rate": 0.8888888888888888, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a brief description of controllable generation as a method to manage hallucination but lacks deeper synthesis of the cited works. It mentions various techniques and papers in passing without connecting their ideas cohesively. There is minimal critical analysis or identification of broader patterns or principles in the field."}}
{"id": "340496a2-7143-4749-b3df-b77d684d1c66", "title": "Post-Processing", "level": "subsubsection", "subsections": [], "parent_id": "4d2e3577-5e43-43c1-8d8e-ccfcea005cd1", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination Mitigation Methods"], ["subsection", "Modeling and Inference Methods"], ["subsubsection", "Post-Processing"]], "content": "Post-processing methods can correct hallucinations in the output, and this standalone task requires less training data. Especially for noisy datasets where a large proportion of the ground truth references suffer from hallucinations, modeling correction is a competitive choice to handle the hallucination problem~. \n, and  follow a generate-then-refine strategy. \n\\ziwei{While the post-processing correction step tends to result in ungrammatical texts, this method} allows researchers to utilise SOTA models which perform best in respect of other attributes, such as fluency, and then correct the results specifically for faithfulness by using small amounts of automatically generated training data.", "cites": [8550, 8543, 2365, 2387], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions post-processing as a method to correct hallucinations and refers to the cited papers in a general way. However, it lacks synthesis by not clearly connecting the specific approaches or findings across the papers. It also does not provide critical evaluation or limitations of the methods, and offers minimal abstraction beyond the individual paper descriptions, resulting in a low insight level overall."}}
{"id": "f0003f40-44f4-4635-8691-d56faa1d9792", "title": "Fact-Checking", "level": "paragraph", "subsections": [], "parent_id": "e99b06de-3d9f-4278-b4aa-65c02ee64602", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Future Directions"], ["subsection", "Future Directions in Metrics Design"], ["paragraph", "Fine-grained Metrics"], ["paragraph", "Fact-Checking"]], "content": "The factual verification of extrinsic hallucinations requires fact-checking against world knowledge, which can be time consuming and laborious.\nLeveraging an automatic fact-checking system for extrinsic hallucination verification is, thus, other future work that requires attention.\nFact-checking consists of the knowledge evidence selection and claim verification sub-tasks, and the following are the remaining challenges associated with each sub-task. \nThe main research problem associated with the evidence selection sub-task is how to retrieve evidence from the \\textit{world} knowledge. Most of the literature leverages Wikipedia as the knowledge source~, which is only a small part of world knowledge. Other literature attempts to use the whole web as the knowledge source\n~. However, this method leads to another research problem -- ``how to ensure the trustworthiness of the information we use from the web''~. Source-level methods that leverages the meta-information of the web source (e.g., web traffic, PageRank or URL structure) have been proposed to deal with this trustworthiness issue~.\nAddressing the aforementioned issues to allow evidence selection against world knowledge will be an important future research direction.\nFor the verification subtask, verification models perform relatively well if given correct evidence~. However, it has been shown that verification models are prone to adversarial attacks and are not robust to negation, numerical or comparative words~.\nImproving this weakness of verification models would also be crucial because the factuality of a sentence can easily be changed by small word changes (i.e., changes in negations, numbers, and entities).", "cites": [2403, 2400, 2402, 2401], "cite_extract_rate": 0.45454545454545453, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section shows moderate analytical depth by discussing the challenges in both evidence selection and claim verification subtasks of fact-checking, and linking them to cited papers. It integrates multiple ideas (e.g., use of Wikipedia, web-scale sources, and adversarial attacks) into a coherent narrative about the limitations and future work in automatic fact-checking. However, the analysis remains somewhat surface-level and does not fully synthesize or abstract beyond the individual contributions of the papers."}}
{"id": "346843c5-9fa2-45e4-b938-05b3a9fed0b1", "title": "General and robust data pre-processing approaches", "level": "paragraph", "subsections": ["0f9e025b-160b-48ad-bf0b-1c516ec06218", "fd2c139c-3b76-4341-bbe1-af7321a83ade", "acf0c01c-1fa9-4a74-947d-83059c05d9a0", "8e449fa7-9500-46a8-93a7-e577ddec88c2", "11d47282-1504-40b5-b04f-a7dbd728eae4"], "parent_id": "e9c963da-1f7f-427e-be5e-83ea98a7cd94", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Future Directions"], ["subsection", "Future Directions in Mitigation Methods"], ["paragraph", "General and robust data pre-processing approaches"]], "content": "Since the data format varies between downstream tasks, there is still a gap for data processing methods between tasks, and currently, no universal method is effective for all NLG tasks~.\nData pre-processing might result in grammatical errors or semantic transformation between the original and processed data, which can negatively affect the performance of generation.\nTherefore, we believe that general and robust data pre-processing methods can help mitigate the hallucinations in NLG.", "cites": [2404], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section mentions the importance of data pre-processing for mitigating hallucinations and cites a survey on data augmentation, but it does not integrate or synthesize the paper's content effectively. It lacks critical evaluation of the cited work's strengths or limitations and only abstracts at a minimal level, pointing out general challenges without deeper conceptual framing."}}
{"id": "0f9e025b-160b-48ad-bf0b-1c516ec06218", "title": "Hallucinations in numerals", "level": "paragraph", "subsections": [], "parent_id": "346843c5-9fa2-45e4-b938-05b3a9fed0b1", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Future Directions"], ["subsection", "Future Directions in Mitigation Methods"], ["paragraph", "General and robust data pre-processing approaches"], ["paragraph", "Hallucinations in numerals"]], "content": "Most existing mitigation methods do not focus on the hallucination of numerals. However, the correctness of numerals in generated text, such as date, quantities and scalars are important for readers~. \nFor example, given the source document ``\\textit{The optimal oxygen saturation ($SpO_2$) in adults with COVID-19 who are receiving supplemental oxygen is unknown. However, a target $SpO_2$ of 92\\% to 96\\% seems logical, considering that indirect evidence from patients without COVID-19 suggests that an $SpO_2$ of <92\\% or >96\\% may be harmful.}~\\footnote{\\url{https://www.covid19treatmentguidelines.nih.gov/management/critical-care/oxygenation-and-ventilation/}}'', the summary ``\\textit{The target oxygen saturation range for patients with COVID-19 is 82–86\\%.}'' includes wrong numbers, which could be fatal.\nCurrently, some works~ point out that using commonsense knowledge can help to gain better numeral representation. And  alleviate numeral hallucinations by re-ranking candidate-generated summaries based on the verification score of quantity entities.\nTherefore, we believe that explicitly modeling numerals to mitigate hallucinations is a potential direction.", "cites": [2357, 2386, 2406, 2405], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the content of multiple papers to highlight the importance of numeral correctness in NLG and the current lack of focus on this issue. It offers a general argument for the need to explicitly model numerals, but the critical analysis is limited—papers are mentioned more as supporting points than evaluated for their strengths or weaknesses. The section identifies a broader pattern regarding the neglect of numeracy in NLG but stops short of presenting a novel, meta-level framework."}}
{"id": "fd2c139c-3b76-4341-bbe1-af7321a83ade", "title": "Extrinsic Hallucination Mitigation", "level": "paragraph", "subsections": [], "parent_id": "346843c5-9fa2-45e4-b938-05b3a9fed0b1", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Future Directions"], ["subsection", "Future Directions in Mitigation Methods"], ["paragraph", "General and robust data pre-processing approaches"], ["paragraph", "Extrinsic Hallucination Mitigation"]], "content": "Though many works on mitigating hallucinations have been published, most do not distinguish between intrinsic and extrinsic hallucination. Moreover, the main research focus has been on dealing with intrinsic hallucination, while extrinsic hallucination has been somewhat overlooked as it is more challenging to reduce~. Therefore, we believe it is worth exploring different mitigation methods for intrinsic and extrinsic hallucinations, and relevant methods in fact-checking can be potentially used for this purpose.", "cites": [8542], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a brief analytical perspective by highlighting the lack of distinction in existing hallucination mitigation research between intrinsic and extrinsic hallucination. It cites one paper to support the claim and suggests that methods from fact-checking could be adapted. While it begins to identify a research gap, it does not deeply synthesize multiple sources, nor does it offer a comprehensive framework or deep critique of the cited work."}}
{"id": "acf0c01c-1fa9-4a74-947d-83059c05d9a0", "title": "Hallucination in long text", "level": "paragraph", "subsections": [], "parent_id": "346843c5-9fa2-45e4-b938-05b3a9fed0b1", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Future Directions"], ["subsection", "Future Directions in Mitigation Methods"], ["paragraph", "General and robust data pre-processing approaches"], ["paragraph", "Hallucination in long text"]], "content": "Many tasks in NLG require the model to process long input texts, such as multi-document summarization and generative question answering. We think adopting existing approaches to a Longformer -based model could help encode long inputs. Meanwhile, part of dialogue systems need to generate long output text, in which the latter part may contradict history generation. Therefore, reducing self-contradiction is also an important future direction.", "cites": [7298], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic analytical perspective by identifying a specific issue (hallucination in long text) and suggesting a potential solution (adopting Longformer-based models). It integrates the cited paper to some extent but lacks deeper connections to other relevant works. There is little critical evaluation of the cited approach or alternative methods, and the abstraction is limited to general observations without overarching principles or frameworks."}}
{"id": "8e449fa7-9500-46a8-93a7-e577ddec88c2", "title": "Reasoning", "level": "paragraph", "subsections": [], "parent_id": "346843c5-9fa2-45e4-b938-05b3a9fed0b1", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Future Directions"], ["subsection", "Future Directions in Mitigation Methods"], ["paragraph", "General and robust data pre-processing approaches"], ["paragraph", "Reasoning"]], "content": "Misunderstanding facts in the source context will lead to intrinsic hallucination and errors. To help models understand the facts correctly requires reasoning over the input table or text. Moreover, if the generated text can be reasoned backwards to the source, we can assume it is faithful. There are some reasoning works in the area of dialogue~, but few in reducing hallucinations. Moreover, tasks with quantities, such as logical table-to-text generation, require numerical reasoning. Therefore, adding reasoning ability to the hallucination mitigation methods is also an interesting future direction.", "cites": [2408, 2407, 1153], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section makes a basic synthesis by connecting the cited works to the broader theme of reasoning in dialogue systems, but it does not deeply integrate them into a cohesive framework. It lacks critical evaluation of the cited methods and instead focuses on identifying a general need for reasoning in hallucination mitigation. The section abstracts reasoning as a necessary component for faithful generation, which provides some higher-level insight, though it remains limited in scope."}}
{"id": "11d47282-1504-40b5-b04f-a7dbd728eae4", "title": "Controllability", "level": "paragraph", "subsections": [], "parent_id": "346843c5-9fa2-45e4-b938-05b3a9fed0b1", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Future Directions"], ["subsection", "Future Directions in Mitigation Methods"], ["paragraph", "General and robust data pre-processing approaches"], ["paragraph", "Controllability"]], "content": "Controllability means the ability of models to control the level of hallucination and strike a balance between faithfulness and diversity~. As mentioned in Section~\\ref{section:contributors}, it is acceptable for chit-chat models to generate a certain level of hallucinatory content as long as it is factual. Meanwhile, for the abstractive summarization task, there is no agreement in the research community about whether factual hallucinations are desirable or not~.\nTherefore, we believe controllability merits attention when exploring hallucination mitigation methods.", "cites": [1291, 8543, 2340], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical perspective by linking the concept of controllability to the broader issue of balancing faithfulness and diversity in NLG. It synthesizes ideas from cited papers on abstractive summarization and dialogue systems to highlight the differing acceptability of hallucination across tasks. However, the analysis is limited and lacks deeper critical evaluation or a novel framework."}}
{"id": "f4da818f-6913-4ff2-b06d-742463b266ca", "title": "Hallucination in Abstractive Summarization", "level": "section", "subsections": ["4e744786-0ac7-4291-9b34-ded31dc9f86f", "23331ad3-4972-4095-8fc7-cbf91c98cd3a", "761272e0-4835-4911-8ff3-8acd998c00be", "5477e27e-9a6c-46be-83b2-4b0023db6564"], "parent_id": "c4299bf7-fa65-4214-968a-c213c95e6d07", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Abstractive Summarization"]], "content": "\\label{section:summarization}\nAbstractive summarization aims to extract essential information from source documents and to generate short, concise, and readable summaries~. Neural networks have achieved remarkable results on abstractive summarization. However,  observe that neural abstractive summarization models are likely to generate hallucinatory content that is unfaithful to the source document.  analyze three recent abstractive summarization systems and show that 25\\% of the summaries generated from state-of-the-art models have hallucinated content. In addition,  mention that even if a summary contains a large amount of hallucinatory content, it can achieve a high ROUGE~ score. This has encouraged researchers to actively devise ways to improve the evaluation of abstractive summarization, especially from the hallucination perspective.\nIn this section, we review the current progress in automatic evaluation and the mitigation of hallucination, and list the remaining challenges for future work. In addition, it is worth mentioning that researchers have used various terms to describe the hallucination phenomenon, such as faithfulness, factual errors, and factual consistency, and we will use the original terms from their papers in the remainder of this section.", "cites": [2344, 1291, 7189], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates information from three papers to highlight the prevalence and evaluation challenges of hallucination in abstractive summarization, showing some synthesis. It identifies the issue of high ROUGE scores despite hallucination and sets up the need for better evaluation methods, indicating a moderate level of critical analysis. While it begins to generalize by mentioning broader evaluation issues, it does not yet present a meta-level framework or deep abstraction beyond the cited works."}}
{"id": "4e744786-0ac7-4291-9b34-ded31dc9f86f", "title": "Hallucination Definition in Abstractive Summarization", "level": "subsection", "subsections": [], "parent_id": "f4da818f-6913-4ff2-b06d-742463b266ca", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Abstractive Summarization"], ["subsection", "Hallucination Definition in Abstractive Summarization"]], "content": "The definition of hallucination in abstractive summarization follows that in Section~\\ref{section:definition}. Specifically, we adopt the definition from : given a document and its abstractive summary, a summary is hallucinated if it has any spans not supported by the input document. \nOnce again, intrinsic hallucination refers to output content that contradicts the source, while extrinsic hallucination refers to output content that the source cannot verify.\nFor instance, in Table~\\ref{Table: example}, given the input article shown in the caption,\nan example of intrinsic hallucination is ``\\textit{The Ebola vaccine was rejected by the FDA in 2019,}'' because this statement contradicts the given content ``\\textit{The first vaccine for Ebola was approved by the FDA in 2019 in the US}''.\nAnd an example of extrinsic hallucination is ``\\textit{China has already started clinical trials of the COVID-19 vaccine,}'' because this statement is not mentioned in the given content. We can neither find evidence of it from the input article nor assert that it is wrong.\n define fine-grained types of factual errors in summaries. \nAs mentioned in \\ref{subsec:terminology}, since the ``fact'' here refers to source knowledge, ``factual error'' can be treated as hallucination, and we can adopt this classification as a sub-type of hallucination.\nThey establish three categories as semantic frame error, discourse error, and content verifiability error.", "cites": [1291, 2349], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes definitions and classifications from the cited papers to present a coherent understanding of hallucination in abstractive summarization. It connects intrinsic and extrinsic hallucination concepts with specific examples and introduces a fine-grained typology of factual errors. While it includes some critical references, such as the treatment of factuality as a binary concept, it lacks deeper comparative or evaluative analysis of the approaches or limitations. The abstraction is moderate, as it generalizes the error types but stops short of offering a meta-level synthesis."}}
{"id": "23331ad3-4972-4095-8fc7-cbf91c98cd3a", "title": "Hallucination Metrics in Abstractive Summarization", "level": "subsection", "subsections": ["391d0736-ca8b-4d65-b4c1-3de811e149a7", "f3bdcacf-5e35-455a-b3db-a9103a565dc3"], "parent_id": "f4da818f-6913-4ff2-b06d-742463b266ca", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Abstractive Summarization"], ["subsection", "Hallucination Metrics in Abstractive Summarization"]], "content": "Existing metrics for hallucination in abstractive summarization are mainly model-based. Following~, we divide the hallucination metrics into two categories: (1) unsupervised metrics and (2) semi-supervised metrics.\nNote that existing hallucination metrics evaluate both intrinsic and extrinsic hallucinations together in one metric because it is difficult to automatically distinguish between them.", "cites": [8542], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a minimal description of hallucination metrics, categorizing them into unsupervised and semi-supervised. It briefly mentions the difficulty in distinguishing intrinsic and extrinsic hallucinations but does not offer substantial synthesis, critical evaluation, or abstraction beyond individual papers. The content remains largely factual and lacks deeper analysis or insights."}}
{"id": "6b973fc4-d077-4c0e-af45-b3be3b023782", "title": "IE-based Metrics", "level": "paragraph", "subsections": [], "parent_id": "391d0736-ca8b-4d65-b4c1-3de811e149a7", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Abstractive Summarization"], ["subsection", "Hallucination Metrics in Abstractive Summarization"], ["subsubsection", "Unsupervised Metrics"], ["paragraph", "IE-based Metrics"]], "content": "As mentioned in Section~\\ref{section:metric}, IE-based metrics leverage IE models to extract knowledge as relation tuples (\\textit{subject, relation, object}) from both the generation and knowledge source to analyze the factual accuracy of the generation~. However, IE models are not 100\\% reliable yet (making errors in the identification of the relation tuples). Therefore,  propose an entity-based metric relying on the Named-Entity Recognition model, which is relatively more robust. Their metric builds on the assumption that there will be a different set of named entities in the gold and generated summary if there exists hallucination.", "cites": [2374, 2354], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes two papers by connecting the general idea of IE-based metrics to their specific implementations in abstractive summarization. It provides a critical point by noting the limitations of IE models and highlights the relative robustness of NER-based alternatives. However, the abstraction is limited, as it does not extend to broader principles or meta-level insights about the role of entity-level metrics in NLG evaluation."}}
{"id": "45bc7443-d402-4f83-8b4d-0d0645c19f27", "title": "NLI-based Metrics", "level": "paragraph", "subsections": [], "parent_id": "391d0736-ca8b-4d65-b4c1-3de811e149a7", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Abstractive Summarization"], ["subsection", "Hallucination Metrics in Abstractive Summarization"], ["subsubsection", "Unsupervised Metrics"], ["paragraph", "NLI-based Metrics"]], "content": "As mentioned in Section~\\ref{section:metric}, the NLI-model (textual entailment model) can be utilized to measure hallucination based on the assumption that a faithful summary will be entailed by the gold source. \nHowever,  discover that models trained on NLI datasets can not transfer well to abstractive summarization tasks, degrading the reliability of NLI-based hallucination metrics. To improve NLI models for hallucination evaluation, they release collected annotations as additional test data. Other efforts have also been made to further improve NLI models.   find that the low performance of NLI-based metrics is mainly caused by the length of the premises in NLI datasets being shorter than the source documents in abstractive summarization. Thus, the authors propose to convert multiple-choice reading comprehension datasets into long premise NLI datasets automatically. The results indicate that long-premise NLI datasets help the model achieve a higher performance than the original NLI datasets. In addition,  introduce a simple but efficient method called SUMMAC$_{Conv}$ by applying NLI models to sentence units that are segmented from documents. The performance of their model is better than applying NLI models to the whole document.", "cites": [2381], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of NLI-based metrics, highlighting their limitations in abstractive summarization and describing how certain works attempted to address these issues. It synthesizes the core idea from the cited paper by connecting the input granularity mismatch to poor performance, and offers a comparison of methods. However, the analysis is somewhat constrained by not exploring deeper critiques or broader theoretical implications."}}
{"id": "0532e799-b283-4bb5-808d-cdabf8a2bfce", "title": "QA-based Metrics", "level": "paragraph", "subsections": [], "parent_id": "391d0736-ca8b-4d65-b4c1-3de811e149a7", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Abstractive Summarization"], ["subsection", "Hallucination Metrics in Abstractive Summarization"], ["subsubsection", "Unsupervised Metrics"], ["paragraph", "QA-based Metrics"]], "content": "QA-based metrics measure the knowledge overlap or consistency between summaries and the source documents based on the intuition that QA models will achieve similar answers if the summaries are factually consistent with the source documents. QA-based metrics such as FEQA~, QAGS~\\ziwei{, and QuestEval~ follow} three steps to obtain a final score: (1) a QG model generates questions from the summaries, (2) a QA model obtains answers from the source documents, and (3) calculate the score by comparing the set of answers from source documents and the set of answers from summaries.\nThe results show that \\ziwei{these reference-free metrics} have substantially higher correlations with human judgments of faithfulness than the baseline metrics. \n further analyze the FEQA and find that the effectiveness of QA-based metrics depends on the question. They also provide a meta-evaluation framework that includes QA metrics.", "cites": [2379, 2358], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the key idea of QA-based metrics from the cited papers and explains their shared methodology. It also notes their effectiveness relative to baseline metrics. However, it lacks deeper comparative or evaluative analysis of the limitations of these metrics, and while it introduces a general framework, the abstraction is limited without broader conceptual insights or a meta-level discussion."}}
{"id": "f3bdcacf-5e35-455a-b3db-a9103a565dc3", "title": "Semi-Supervised Metrics", "level": "subsubsection", "subsections": [], "parent_id": "23331ad3-4972-4095-8fc7-cbf91c98cd3a", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Abstractive Summarization"], ["subsection", "Hallucination Metrics in Abstractive Summarization"], ["subsubsection", "Semi-Supervised Metrics"]], "content": "Semi-supervised metrics are trained on the synthetic data generated from summarization datasets. Trained on these task-specific corpora, models can judge whether the generated summaries are hallucinatory.\n propose a weakly supervised model named FactCC for evaluating factual consistency. The model is trained jointly for three tasks: (1) checking whether the synthetic sentences remain factually consistent, (2) extracting supporting spans in the source documents, and (3) extracting inconsistent spans in the summaries, if any exist.\nThey transfer this model to check whether the summaries generated from summarization models are factually consistent. Results show that the performance of their FactCC model surpasses the classifiers trained on the MNLI or FEVER datasets.  introduce a method to fine-tune a pre-trained language model on synthetic data with automatically inserted hallucinations in order to detect the hallucinatory content in summaries. The model can classify whether spans in the machine-generated summaries are faithful to the article. This method shows higher correlations with human factual consistency evaluation than the baselines.", "cites": [2344, 2383], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes the key ideas from the two cited papers, both of which focus on training models on synthetic data to evaluate factual consistency in abstractive summaries. It connects these approaches by highlighting their shared use of semi-supervised learning and synthetic hallucinations. However, while it notes that these methods outperform baselines, it lacks deeper critical analysis of their limitations or broader implications, limiting the abstraction and critique to a moderate level."}}
{"id": "fbd75aaf-d148-4d20-a294-359419d9196e", "title": "Architecture Method.", "level": "subsubsection", "subsections": ["7e94b097-6a76-4c69-8b8c-e6b3fd4c0839", "8008741f-4b82-4c5a-b9c5-7f72e507a63d", "d828090b-1a54-4093-86ec-de1452342770"], "parent_id": "761272e0-4835-4911-8ff3-8acd998c00be", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Abstractive Summarization"], ["subsection", "Hallucination Mitigation in Abstractive Summarization"], ["subsubsection", "Architecture Method."]], "content": "Seq-to-seq  models are widely used and achieve state-of-the-art performance in abstractive summarization. Researchers have made modifications to the architecture design of the seq-to-seq models to reduce hallucinated content in the summaries. We describe various efforts made to improve the encoder, decoder, or both the encoder and decoder of the seq-to-seq models.", "cites": [2401], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section merely introduces the general idea of modifying seq-to-seq architectures to reduce hallucinations and cites one foundational paper without integrating or synthesizing insights from it. There is no critical analysis or abstraction to broader principles; it reads as a minimal description of an approach rather than a deeper synthesis of the literature."}}
{"id": "7e94b097-6a76-4c69-8b8c-e6b3fd4c0839", "title": "Encoder", "level": "paragraph", "subsections": [], "parent_id": "fbd75aaf-d148-4d20-a294-359419d9196e", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Abstractive Summarization"], ["subsection", "Hallucination Mitigation in Abstractive Summarization"], ["subsubsection", "Architecture Method."], ["paragraph", "Encoder"]], "content": " propose to use an explicit graph neural network (GNN) to encode the fact tuples extracted from source documents. In addition to an explicit graph encoder,  further design a multiple-choice cloze test reward to encourage the model to better understand entity interactions. Moreover,  use external knowledge from Wikipedia to make knowledge embeddings, which the results show improve factual consistency.", "cites": [2357, 2375, 2369], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 1.7, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of the encoder-based hallucination mitigation methods from three papers, mentioning each paper's approach without deep integration or comparison. It lacks critical evaluation of the methods' strengths and weaknesses, and offers minimal abstraction beyond the specific techniques described."}}
{"id": "8008741f-4b82-4c5a-b9c5-7f72e507a63d", "title": "Decoder", "level": "paragraph", "subsections": [], "parent_id": "fbd75aaf-d148-4d20-a294-359419d9196e", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Abstractive Summarization"], ["subsection", "Hallucination Mitigation in Abstractive Summarization"], ["subsubsection", "Architecture Method."], ["paragraph", "Decoder"]], "content": " present the incorporation of a sequential decoder with a tree-based decoder to generate a summary sentence and its syntactic parse. This joint generation is performed improve faithfulness.  introduce the Focus Attention Mechanism, which encourages decoders to generate tokens similar or topical to the source documents. The results on the BBC extreme summarization task show that models augmented with the Focus Attention Mechanism generate more faithful summaries.", "cites": [2359, 2355], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of two papers addressing hallucination in abstractive summarization, focusing on decoder-based methods. It integrates the papers minimally by noting their shared goal of improving faithfulness but does not synthesize their approaches into a broader framework. There is little critical analysis or generalization to overarching principles or trends in the field."}}
{"id": "d828090b-1a54-4093-86ec-de1452342770", "title": "Encoder-decoder", "level": "paragraph", "subsections": [], "parent_id": "fbd75aaf-d148-4d20-a294-359419d9196e", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Abstractive Summarization"], ["subsection", "Hallucination Mitigation in Abstractive Summarization"], ["subsubsection", "Architecture Method."], ["paragraph", "Encoder-decoder"]], "content": " extract fact descriptions from the source text and apply a dual-attention seq-to-seq framework to force the summaries to be conditioned on both source documents and the extracted fact descriptions.  propose an entailment-aware encoder and decoder with multi-task learning which incorporates the entailment knowledge into abstractive summarization models.", "cites": [2363], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of two methods for hallucination mitigation in abstractive summarization but lacks synthesis of ideas or broader context. It does not compare these approaches or critically evaluate their strengths and limitations. The content remains at a concrete level without abstracting principles or trends across the cited work."}}
{"id": "403c3d3e-587c-4af5-9b43-97c8754051bb", "title": "Training Method", "level": "subsubsection", "subsections": [], "parent_id": "761272e0-4835-4911-8ff3-8acd998c00be", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Abstractive Summarization"], ["subsection", "Hallucination Mitigation in Abstractive Summarization"], ["subsubsection", "Training Method"]], "content": "Aside from architecture modification, some works improved the training approach to reduce hallucination.  introduce a contrastive learning method to train summarization models. The positive training data are reference summaries, while the negative training data are automatically generated hallucinatory summaries, and the contrastive learning system is trained to distinguish between them. In the dialogue summarization field,  propose another contrastive fine-tuning strategy, named CONFIT, that can improve the factual consistency and overall quality of summaries.", "cites": [2352, 42], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly describes two papers that use contrastive learning for hallucination mitigation in summarization but lacks deeper synthesis of ideas or comparative analysis. It does not generalize the methods or provide critical evaluation of their strengths, weaknesses, or implications. The content remains at the level of surface-level description without offering analytical or meta-level insights."}}
{"id": "a6fe7d90-d630-4fc3-8120-5499ed499379", "title": "Post-Processing Method", "level": "subsubsection", "subsections": [], "parent_id": "761272e0-4835-4911-8ff3-8acd998c00be", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Abstractive Summarization"], ["subsection", "Hallucination Mitigation in Abstractive Summarization"], ["subsubsection", "Post-Processing Method"]], "content": "Some works carry out post-editing to reduce the hallucination of the model-generated summaries, which are viewed as draft summaries.  propose SpanFact, a pair of factual correction models that use knowledge learned from QA models to correct the spans in the generated summaries. Similar to SpanFact,  introduce a post-editing corrector module to identify and correct hallucinatory content in generated summaries. The corrector module is trained on synthetic data which are created by adding a series of heuristic transformations to reference summaries.\n present HERMAN, a system that learns to recognize quantities (dates, amounts of money, etc.) in the generated summary and verify their factual consistency with the source text. According to the quantity hallucination score, the system chooses the most faithful summary where the source text supports its quantity terms from the candidate-generated summaries.  introduce a contrast candidate generation and selection system to do post-processing. The contrast candidate generation model replaces the named entities in the generated summaries with ones present in the source documents, and the contrast candidate selection model will select the best candidate as the final output summary.", "cites": [8550, 2365, 2357, 2387], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of post-processing methods for hallucination mitigation in abstractive summarization but lacks synthesis of ideas across the cited works. It summarizes each paper's approach without evaluating their effectiveness or limitations. There is minimal abstraction or identification of broader patterns or principles."}}
{"id": "6e289bdd-ce50-44e7-9249-21805ccfd7bd", "title": "Hallucination in Dialogue Summarization", "level": "paragraph", "subsections": [], "parent_id": "350e5418-3757-4a5b-b2b0-9f08eb35a4cb", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Abstractive Summarization"], ["subsection", "Future Directions in Abstractive Summarization"], ["paragraph", "Factual Hallucination Evaluation"], ["paragraph", "Hallucination in Dialogue Summarization"]], "content": "In conversational data, the discourse relations between utterances and co-references between speakers are more complicated than from, say, news articles. For example,  show that 74\\% of samples in the QMSum dataset consist of inconsistent facts. We believe exploring the hallucination issue in dialogue summarization is an important and special component of research into hallucination in abstractive summarization.", "cites": [7187], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly mentions a finding from one cited paper regarding factual inconsistency in dialogue summarization but does not synthesize this with other relevant works or provide a broader analysis. It lacks critical evaluation of the cited study and does not abstract the issue into wider patterns or principles within NLG hallucination research."}}
{"id": "d780be6b-bf59-40c5-96f8-75987ba7f5e8", "title": "Open-domain Dialogue Generation", "level": "subsection", "subsections": ["7e92eb2f-5fe2-4874-af47-a4833a63a51b", "37e7c95a-a8a1-48d6-8f15-e6944e73fbf8", "d85e4335-5d68-458c-a346-98c1cea924ff", "f9520360-fd53-407e-8a9a-0393a9520015"], "parent_id": "2a695764-0bf3-4ec3-b5d1-617938f2ceba", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Dialogue Generation"], ["subsection", "Open-domain Dialogue Generation"]], "content": "While the term ``hallucination'' seems to have newly emerged in the NLP field, a related behavior, ``inconsistency'', of neural models has been widely discussed. This behavior has been pointed out as a shortcoming of generation-based approaches for open-domain chatbots~. \nTwo possible types of inconsistency occur in open-domain dialogue generation: (1) inconsistency among the system utterances, such as when the system contradicts its previous utterance; (2) inconsistency with an external source, such as factually incorrect utterances. Whereas the first type is described using the term \"inconsistency\"~ or \"incoherence\"~, some have recently started to call the second type \"hallucination\"~.  Self-inconsistency can be considered as an intrinsic hallucination problem, while the external inconsistency involves both intrinsic and extrinsic hallucinations, depending on the reference source. \nAs mentioned earlier, a certain level of hallucination may be acceptable in open-domain chit-chat as long as it does not involve severe factual issues. Moreoever, it is almost impossible to verify factual correctness since the system usually lacks a connection to external resources. With the introduction of knowledge-grounded dialogue tasks~, which provide an external reference, however, there has been more active discussion of hallucination in open-domain dialogue generation.", "cites": [457, 2351, 2409, 2399, 2050, 2410], "cite_extract_rate": 0.6363636363636364, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates the concept of hallucination with prior notions like inconsistency and incoherence, drawing connections across the cited works to form a coherent narrative. It abstracts these ideas to define two types of inconsistencies and relates them to the broader hallucination problem. However, while it provides some analytical depth, it lacks deeper critical evaluation or novel synthesis of the cited papers."}}
{"id": "7e92eb2f-5fe2-4874-af47-a4833a63a51b", "title": "Self-Consistency", "level": "subsubsection", "subsections": [], "parent_id": "d780be6b-bf59-40c5-96f8-75987ba7f5e8", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Dialogue Generation"], ["subsection", "Open-domain Dialogue Generation"], ["subsubsection", "Self-Consistency"]], "content": "In end-to-end generative open-domain dialogue systems, the inconsistency among system utterances has been pointed out as the bottleneck to human-level performance ~. We often observe an inconsistency in the answers to semantically similar yet not identical questions. For example, a system may answer the questions of ``What is your name?'' and ``May I ask your name?\" with different responses. Persona consistency has been the center of attention~ and it is one of the most obvious cases of self-contradiction regarding the character of the dialogue system. \"Persona\" is defined as the character that a dialogue system plays during a conversation, and can be composed of identity, language behavior, and an interaction style~. While some works has set their objective as teaching models to utilize speaker-level embeddings~, others condition generation with a set of descriptions about a persona, which we will discuss in detail in the next section.", "cites": [1877, 650, 2411, 1989], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the self-consistency issue in open-domain dialogue generation and mentions relevant papers, but it does not synthesize their contributions into a broader narrative. There is minimal critical evaluation of the methods or limitations of the cited works, and no clear abstraction or generalization to overarching principles in the field."}}
{"id": "37e7c95a-a8a1-48d6-8f15-e6944e73fbf8", "title": "External Consistency", "level": "subsubsection", "subsections": [], "parent_id": "d780be6b-bf59-40c5-96f8-75987ba7f5e8", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Dialogue Generation"], ["subsection", "Open-domain Dialogue Generation"], ["subsubsection", "External Consistency"]], "content": "Besides self-consistency, an open-domain dialogue system should also generate persona-consistent and informative responses corresponding so as to user utterances to further engage with the user during conversation. In this process, an external resource containing explicit persona information or world knowledge is introduced into the system to assist the model generation process.\nThe PersonaChat datasets~ have accelerated research into persona consistency~. In PersonaChat datasets, each conversation has persona descriptions such as ``I like to ski'' or ``I am a high school teacher'' attached. By conditioning the response generation on the persona description, a chit-chat model is expected to acquire an ability to generate a more persona-consistent response. Lately, the application of NLI methods~ or reinforcement learning frameworks~ have been investigated. Although these methods conditioned on the PersonaChat datasets have been successful, further investigation of approaches that do not rely on a given set of persona descriptions is necessary because such descriptions are not always available, and covering every aspect of a persona with them is impossible. \nIn addition to PersonaChat-related research, the knowledge-grounded dialogue (KGD) task in the open-domain requires the model to generate informative responses with the help of an external knowledge graph (KG) or knowledge corpus~. Hallucination in conversations, which is also considered as a factual consistency problem, has raised much research interest recently~. Here, we continue to split the hallucination problem in the KGD task into intrinsic hallucination and extrinsic hallucination. Most of the KGD works tackle the hallucination problem when responses contain information that contradicts (intrinsic) or cannot be found in the provided knowledge input (extrinsic). \nSince world knowledge is enormous and ever-changing, the extrinsic hallucination may be factual but hard to verify.  further adopt the same definition of hallucination as mentioned above to the knowledge graph-grounded dialogue task, where intrinsic hallucination indicates the case of misusing either the subject or object of the knowledge triple; \nand extrinsic hallucination indicates that there is no corresponding valid knowledge triple in the gold reference knowledge.\nRecently, there have been some attempts to generate informative responses without explicit knowledge inputs, but with the help of the implicit knowledge inside large pre-trained language models instead~ during the inference time. Under this setting, the study of extrinsic hallucination is of great value but still poorly investigated.", "cites": [8543, 2399, 8545, 2050, 650, 2412, 457, 8523], "cite_extract_rate": 0.75, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several papers to discuss external consistency in open-domain dialogue generation, connecting the use of persona descriptions, knowledge graphs, and implicit model knowledge. It identifies limitations of relying on explicit persona or knowledge inputs and points to underexplored areas, showing moderate critical analysis. The discussion abstracts some broader patterns, such as the distinction between intrinsic and extrinsic hallucination, but the insights remain grounded rather than highly generalized."}}
{"id": "d85e4335-5d68-458c-a346-98c1cea924ff", "title": "Hallucination Metrics", "level": "subsubsection", "subsections": ["3938bf6c-1000-4b01-bede-0c8c0dd700c0", "a0b689b6-c67a-426e-930d-9d83a0051faf"], "parent_id": "d780be6b-bf59-40c5-96f8-75987ba7f5e8", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Dialogue Generation"], ["subsection", "Open-domain Dialogue Generation"], ["subsubsection", "Hallucination Metrics"]], "content": "For generation-based dialogue systems, especially open-domain chatbots, the hallucination evaluation method remains an open problem~. As of now, there is no standard metric. Therefore, chatbots are usually evaluated by humans on factual consistency or factual correctness~. We also introduce some automatic statistical and model-based metrics as a reference, which will be described in more detail below.", "cites": [2410, 1998], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the current state of hallucination metrics in open-domain dialogue generation, mentioning the lack of standard metrics and the use of human evaluation. It cites two relevant papers but does not deeply integrate their ideas or compare different approaches. There is minimal critical analysis or abstraction to broader principles, keeping the content primarily descriptive."}}
{"id": "3938bf6c-1000-4b01-bede-0c8c0dd700c0", "title": "Variants of F1 Metrics", "level": "paragraph", "subsections": [], "parent_id": "d85e4335-5d68-458c-a346-98c1cea924ff", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Dialogue Generation"], ["subsection", "Open-domain Dialogue Generation"], ["subsubsection", "Hallucination Metrics"], ["paragraph", "Variants of F1 Metrics"]], "content": "\\textbf{Knowledge F1 (KF1)} measures the overlap between the generated responses and the gold knowledge sentences to which the human referred for conversation during dataset collection~. \nKF1 attempts to capture whether a model can generate knowledgable responses by correctly utilizing the relevant knowledge. \nKF1 is only available for datasets with labeled ground-truth knowledge.  further propose \\textbf{Rare F1 (RF1)}, which only considers the infrequent words in the dataset when calculating F1 to avoid influence from the common uni-grams. The authors define an infrequent word as being in the lower half of the cumulative frequency distribution of the reference corpus.", "cites": [8545], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly describes two F1-based hallucination metrics (KF1 and RF1) but does not synthesize or connect them to broader themes or other works in the field. It lacks critical evaluation of their strengths and limitations and does not abstract to general principles or trends in hallucination measurement."}}
{"id": "a0b689b6-c67a-426e-930d-9d83a0051faf", "title": "Model-based Metric", "level": "paragraph", "subsections": [], "parent_id": "d85e4335-5d68-458c-a346-98c1cea924ff", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Dialogue Generation"], ["subsection", "Open-domain Dialogue Generation"], ["subsubsection", "Hallucination Metrics"], ["paragraph", "Model-based Metric"]], "content": "\\yan{Natural language has its natural on the flexibility of the surface forms with the same semantics, so overlap-based metrics cannot provide the comprehensive evaluation.} \nRecently, several works have proposed evaluation metrics for measuring consistency, such as using natural language inference (NLI)~, training learnable evaluation metrics~, or releasing an additional test set for coherence~.\n\\yan{These methods are more flexible and supports the generated responses with different surface forms.} \nFor the KGD task,  propose the BEGIN benchmark, which consists of samples taken from  with additional human annotation and a new classification task extending the NLI paradigm.  present a trainable metric for the KGD task, which also applies NLI. It is also noteworthy that  propose datasets that can benefit fact-checking systems specialized for dialogue systems.\nThe Conv-FEVER corpus~ is a factual consistency detection dataset, which was created by adapting the Wizard-of-Wikipedia dataset~. It consists of both factually consistent and inconsistent responses and can be used to train a classifier to detect factually inconsistent responses with respect to the knowledge provided.", "cites": [457, 2399], "cite_extract_rate": 0.25, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of model-based hallucination metrics, mentioning some approaches like NLI and trainable metrics. However, it lacks synthesis by not clearly connecting these ideas or explaining how they address limitations of overlap-based metrics. There is minimal critical analysis or abstraction, and the content appears largely descriptive with limited insight into trends or principles."}}
{"id": "f9520360-fd53-407e-8a9a-0393a9520015", "title": "Mitigation Methods", "level": "subsubsection", "subsections": [], "parent_id": "d780be6b-bf59-40c5-96f8-75987ba7f5e8", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Dialogue Generation"], ["subsection", "Open-domain Dialogue Generation"], ["subsubsection", "Mitigation Methods"]], "content": "The hallucination issue can be mitigated by data pre-processing, which includes introducing extra information into the data.  propose a measurement based on seven attributes of the dialogue quality, including self-consistency. Based on this measurement, the untrustworthy samples which get lower scores are filtered out from the training set to improve the model performance in terms of self-consistency (i.e., intrinsic hallucination). \n conduct a comprehensive investigation on a retrieval-augmented KGD task where a retriever is introduced to the system for knowledge selection. The authors study several key problems, such as whether retrieval helps reduce hallucinations and how the generation should be augmented with the retrieved knowledge. The experimental results show that retrieval helps substantially in improving performance on KGD tasks and in reducing the hallucination in conversations without sacrificing conversational ability.\n introduce a set of control codes and concatenate them with dialogue inputs to reduce the hallucination by forcing the model to be more aware of how the response relies on the knowledge evidence in the response generation.\nSome researchers have also tried to reduce hallucinated responses during generation by improving dialogue modeling.  apply inductive attention into transformer-based dialogue models, and potentially uninformative attention links are removed with respect to a piece of pre-established structural information between the dialogue context and the provided knowledge.\nInstead of improving the dialogue response generation model itself,  present a response refinement strategy with a token-level hallucination critic and entity-mention retriever, so that the original dialogue model is left without retraining. The former module is designed to label the hallucinated entity mentioned in the generated responses, while the retriever is trained to retrieve more faithful entities from the provided knowledge graph.\nRHO  is a framework that uses three mechanisms to tackle hallucinations, namely, local knowledge grounding, global knowledge grounding, and response re-ranking to tackle hallucinations in open-domain dialogues, and has been empirically shown to perform this.", "cites": [1998, 8543, 8545, 364], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section describes several mitigation methods for hallucination in open-domain dialogue generation, referencing the cited papers. However, it primarily lists the methods without deeply connecting or synthesizing the underlying ideas. There is minimal critical evaluation or comparison of the approaches, and abstraction to broader principles is limited."}}
{"id": "fa155eb0-e462-41ca-9937-30b1301298dc", "title": "Task-oriented Dialogue Generation", "level": "subsection", "subsections": ["2ecd3942-cee9-480b-ab5b-428c3fdc3d8d", "c72faec9-ad6e-407b-9aff-0f584fe17e30"], "parent_id": "2a695764-0bf3-4ec3-b5d1-617938f2ceba", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Dialogue Generation"], ["subsection", "Task-oriented Dialogue Generation"]], "content": "A task-oriented dialogue system is often composed of several modules: a natural language understanding (NLU) module, a dialogue manager (DM), and a natural language generation (NLG) module~. Intrinsic hallucination can occur between the DM and NLG, where a dialogue act such as \\texttt{recommend(NAME=}\\textit{peninsula hotel}\\texttt{, AREA=}\\textit{tsim sha tsui}\\texttt{)} is transformed into a natural language representation ``the hotel named \\textit{peninsula hotel} is located in \\textit{tsim sha tsui} area.''~.", "cites": [446], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of how intrinsic hallucination can occur in task-oriented dialogue systems, using an example from one cited paper (MS MARCO). However, it does not synthesize or integrate multiple sources, lacks critical evaluation of the cited work or alternative approaches, and offers minimal abstraction or generalization beyond the specific example."}}
{"id": "2ecd3942-cee9-480b-ab5b-428c3fdc3d8d", "title": "Hallucination Metrics", "level": "subsubsection", "subsections": [], "parent_id": "fa155eb0-e462-41ca-9937-30b1301298dc", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Dialogue Generation"], ["subsection", "Task-oriented Dialogue Generation"], ["subsubsection", "Hallucination Metrics"]], "content": "To evaluate hallucination,  and  combine traditional metrics such as the BLEU score and human evaluation as well as hallucination-specific automatic metrics. Following previous works such as~, and ,  use the slot error rate, which is computed by $(p+q)/N$, where $N$ represents the total number of slots extracted by another model in the dialogue act. Here, $p$ stands for the missing slots in the generated template, and $q$ is the number of redundant slots. \nOn the other hand,  introduce a novel metric called the tree accuracy, which determines whether the prediction's tree structure is identical to that of the input meaning representations.", "cites": [446], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal synthesis of cited works by briefly introducing two metrics—slot error rate and tree accuracy—but fails to connect these concepts meaningfully. There is no critical analysis or evaluation of the strengths and limitations of these metrics. The content remains at a concrete level, describing individual methods without generalizing to broader patterns or principles in hallucination measurement."}}
{"id": "c72faec9-ad6e-407b-9aff-0f584fe17e30", "title": "Mitigation Methods", "level": "subsubsection", "subsections": [], "parent_id": "fa155eb0-e462-41ca-9937-30b1301298dc", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Dialogue Generation"], ["subsection", "Task-oriented Dialogue Generation"], ["subsubsection", "Mitigation Methods"]], "content": "While  propose to adopt tree-structured semantic representations and add constraints on decoding,  frame a reinforcement learning problem to which they apply a bootstrapping algorithm to sample training instances and then leverage a reward related to slot consistency. \nRecently, there has emerged another line of research in task-oriented dialogue, which is to build a single end-to-end system rather than connecting several modules (e.g., ). As discussed in previous sections of this paper, there is potential for such end-to-end systems to produce extrinsic hallucinations, yet this remains less explored. For example, a model might generate a response with an entity that appears out of nowhere. In the example of hotel recommendation in Hong Kong given above, a model could generate a response such as ``the hotel named \\textit{raffles hotel} is located in \\textit{central} area,\\footnote{Raffles Hotel is a hotel located in Downtown Core, Singapore.}'' which cannot be verified from the knowledge base of the system.", "cites": [8554, 446], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.5, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section briefly introduces two distinct mitigation approaches for hallucinations in task-oriented dialogue generation and acknowledges the potential for end-to-end systems to produce such hallucinations. However, it lacks deeper synthesis between the cited papers and does not fully analyze their strengths or limitations. The abstraction is limited, as it does not generalize to broader trends or frameworks."}}
{"id": "72f828e0-9bd2-452a-8fb1-8ef08dfb18d0", "title": "Self-Contradiction in Dialogue Systems", "level": "paragraph", "subsections": ["1d062e05-2f9e-4371-8f09-526253333c06"], "parent_id": "7f130716-a52d-47c9-aa1c-52919fe53f2a", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Dialogue Generation"], ["subsection", "Future Directions in Dialogue Generation"], ["paragraph", "Self-Contradiction in Dialogue Systems"]], "content": "One of the possible reasons for self-contradiction is that current dialogue systems tend to have a short memory of dialogue history~. \nFirstly, common dialogue datasets provide several turns of conversation, yet these are not long enough to assess a model's ability to deal with a long context. To overcome this,  introduce a new dataset that consists of, on average, over 40 utterances per episode. Secondly, we often truncate dialogue history into fewer turns to fit into models such as Transformer-based architectures, which makes it difficult for a model to memorize the past. In addition to the works on dialogue summarization, e.g.,~, it would be beneficial to apply other works which are aiming to grasp the longer context but do not focus on dialogue generation~.", "cites": [7298, 8555, 1499, 2410], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from multiple papers to highlight the issue of short memory in dialogue systems and its relation to self-contradiction. It abstracts the problem to the broader context of long-term conversation modeling. However, the critical analysis is limited, as it does not deeply evaluate or compare the cited works or their effectiveness in addressing the issue."}}
{"id": "124ebc0b-881a-4988-a79c-25a62bcfe7c2", "title": "Hallucination in Generative Question Answering", "level": "section", "subsections": ["d900572d-c4c7-44ad-be1e-366a4547f5a1", "47d4bdc7-4ac3-4e04-b03a-e143ec695a1b", "6e98bf37-4aed-41b4-9d73-bfb861c46536", "46c0d48c-0a23-4701-9820-221d626f9d0a"], "parent_id": "c4299bf7-fa65-4214-968a-c213c95e6d07", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Generative Question Answering"]], "content": "\\label{section:QA}\nGenerative question answering (GQA) aims to generate an abstractive answer rather than extract an answer to a given question from provided passages~. It is an important task since many of the everyday questions that humans deal with and pose to search engines require in-depth explanations~ (e.g., \\textit{why/how..?}), and the answers are normally long and cannot be directly extracted from existing phrase spans. A GQA system can be integrated with a search engine~ to empower more intelligent search or combined with a virtual conversation agent to enhance user experience. \nNormally, a GQA system involves searching an external knowledge source for information relevant to the question. Then it generates the answer based on the retrieved information~. In most cases, no single source (document) contains the answer, and multiple retrieved documents will be considered for answer generation. Those documents may contain redundant, complementary, or contradictory information. Thus, hallucination is common in the generated answers. \nThe hallucination problem is one of the most important challenges in GQA. Since an essential goal of a GQA system is to provide factualy-correct answers given the question, hallucination in the answer will mislead the user and damage the system performance dramatically.", "cites": [442, 2376], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of generative question answering and mentions the hallucination problem, but it does not effectively synthesize the two cited papers into a cohesive narrative. It lacks critical evaluation of the papers or their methodologies and only briefly introduces the broader issue of hallucination without abstracting to general principles or trends in the field."}}
{"id": "d900572d-c4c7-44ad-be1e-366a4547f5a1", "title": "Hallucination Definition in GQA", "level": "subsection", "subsections": [], "parent_id": "124ebc0b-881a-4988-a79c-25a62bcfe7c2", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Generative Question Answering"], ["subsection", "Hallucination Definition in GQA"]], "content": "As a challenging yet under-explored task, there is no standard definition of hallucination in GQA. However, almost all the works on GQA~ involve a human evaluation process, in which the \\textit{factual correctness} measuring the faithfulness of the generated answer can be seen as a measurement of the hallucination; i.e., the more faithful the answer is, the less hallucinated content it contains. The most recent such work ~ uses the term \\textit{semantic drift}, which indicates how the answer drifts away from a correct one during generation, and this can also be seen as a specific definition of hallucination in GQA. \nIn line with the general categorization of hallucination in Section ~\\ref{subsec:category}, we give two concrete hallucination examples in GQA in Table ~\\ref{Table: example}. The sources of both questions are Wikipedia web pages. For the first question, ``\\textit{dow jones industrial average please?}'', the generated answer ``\\textit{index of 30 major U.S. stock indexes}'' conflicts with the statement ``\\textit{of 30 prominent companies listed on stock exchanges in the United States}'' from Wikipedia. So we categorize it as an {intrinsic hallucination}. For the second example, the sentences ``\\textit{The definition of a Sadducee is a person who acts in a deceitful or duplicitous manner. An example of a Sadduceee is a politician who acts deceitfully in order to gain political power}'' in the generated answer can not be verified from the source documents; thus, we categorize it as an {extrinsic hallucination}.", "cites": [432, 7569, 442, 2376], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section attempts to synthesize the concept of hallucination in GQA by referencing a general categorization from earlier in the survey and applying it to specific examples. It abstracts to a degree by using the terms 'intrinsic' and 'extrinsic hallucination' to classify generated content. However, the critical analysis is limited, as it does not deeply evaluate or compare the cited works or identify significant limitations in how hallucination is currently defined or measured."}}
{"id": "47d4bdc7-4ac3-4e04-b03a-e143ec695a1b", "title": "Hallucination-related Metrics in GQA", "level": "subsection", "subsections": [], "parent_id": "124ebc0b-881a-4988-a79c-25a62bcfe7c2", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Generative Question Answering"], ["subsection", "Hallucination-related Metrics in GQA"]], "content": "Currently, there is no automatic metric to evaluate hallucination in QGA specifically. While most works on GQA use automatic evaluation metrics such as ROUGE score and F1 to measure the quality of the answer, these N-gram overlap-based metrics are not a meaningful way to evaluate hallucination due to their poor correlation with human judgments, as indicated by . On the other hand, almost all the GQA-related work involves a human evaluation process as a complement to the automatic evaluation. Normally, human annotators will be asked to assign a score indicating the faithfulness of the answer, which can also be viewed as a measurement of the answer hallucination. However, the metrics obtained via human evaluation come normally from a small sample of the data.\nMetrics such as \\textit{semantic overlap} ~, a learned evaluation metric based on BERT that models human judgments, could be considered a better measurement of hallucination for GQA. Other metrics such as the \\textit{factual correctness} can also be considered as a way to measure hallucination in GQA.  propose to explicitly measure the factual correctness of a generated text against the reference by first extracting facts via an information extraction (IE) module. Then they define and measure the factual accuracy score to be the ratio of facts in the generation text equal to the corresponding facts in the reference. \n\\textit{Factual consistency}, which measures the faithfulness of the generated answer given its source documents, can be employed as another way to measure hallucination in GQA.  propose an automatic QA-based metric to measure faithfulness in summary, leveraging the recent advances in machine reading comprehension. They first use a question generation model to construct question-answer pairs from the summary, and then a QA model is applied to extract short answer spans from the given source document for the question. The extracted answers that do not match the provided answers indicate unfaithful information in the summary. While these metrics were first proposed in summarization works, they can be easily adopted in generative QA to measure hallucinations in the generated long-form answer.\nThe most recent work on GQA by ~ proposed to estimate the faithfulness of the generated long-form answer via \\textit{zero-shot short answer recall} on extractive QA datasets. They first generate long-form answers for questions from two extractive QA datasets Natural Questions(NQ)~ and HotpotQA~, both of which contains large-scale question-answer pairs, then they measure the ratio of golden short answer span contained in the generated long answer as an estimation of faithfulness of the generated long-answer. While the idea is similar to the factual consistency metric in summarization work~, and also matches with our intuition to some extent, its correlation with human evaluation on faithfulness has not been verified.", "cites": [2380, 2361, 7569, 460, 2379, 2376], "cite_extract_rate": 0.75, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers, connecting ideas like factual correctness and factual consistency across summarization and GQA. It critically evaluates the limitations of traditional metrics and highlights the lack of correlation with human judgment. The section abstracts these approaches into broader themes like the need for faithfulness in generative tasks, though the critique could be deeper."}}
{"id": "6e98bf37-4aed-41b4-9d73-bfb861c46536", "title": "Hallucination Mitigation in GQA", "level": "subsection", "subsections": [], "parent_id": "124ebc0b-881a-4988-a79c-25a62bcfe7c2", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Generative Question Answering"], ["subsection", "Hallucination Mitigation in GQA"]], "content": "Unlike conditional text generation tasks such as summarization, or data-to-text generation, in which the source documents are provided and normally related to the target generation, the hallucination problem in GQA is more complicated. Generally speaking, it might come from two sources: 1) the incompetency of the retriever, which retrieves documents irrelevant to the answer, and 2) the \\textit{intrinsic} and \\textit{extrinsic} hallucination in the conditional generation model itself. Normally these two parts are interconnected and cause hallucinations in the answer.\nEarly works on GQA mostly tried to improve the faithfulness of the answer by investigating reliable external knowledge sources or incorporating multiple information sources.\n propose Neural Generative Question Answering (GENQA), an end-to-end model that generates answers to simple factoid questions based on the knowledge base, while  propose the Knowledge-Enriched Answer Generator (KEAG) to generate a natural answer by integrating facts from four different information sources, namely, questions, passages, vocabulary, and knowledge. \n\\ziwei{Nevertheless, these methods rely on the existence of high-quality, relevant resources which are not easily available.}\nRecent works focus more on the conditional generation model.  construct a local knowledge graph for each question to compress the information and reduce redundancy from the retrieved documents, which can be viewed as an early trial to mitigate hallucination.  propose Rationale-Enriched Answer Generator (REAG), in which they add an extraction task to obtain the rationale for an answer at the encoding stage, and the decoder is expected to generate the answer based on both the extracted rationale and original input. The recent work~ employs a Routing Transformer (RT), a sparse attention-based Transformer-based model that employs local attention and mini-batch k-means clustering for long-range dependence, as the answer generator in the hope of modeling more retrieved documents to mitigate the hallucination in the answer.  propose a framework named RBG (\\textbf{r}ead \\textbf{b}efore \\textbf{g}enerate), to jointly models answer generation with machine reading. They augment the generation model with fine-grained, answer-related salient information predicted by the MRC module, to enhance answer faithfulness.\n\\ziwei{Such methods can exploit and utilize the information in the original input better, while they require the extra effort of building models to extract that information.}\nMost recently,~ propose a benchmark, which comprises 817 questions that span 38 categories, to measure the truthfulness of a language model in the QA task. This work investigates the performances of GPT-3~, GPT-Neo/J~, GPT-2~ and a T5-based model~. The results suggest that simply scaling up the model is less promising than fine-tuning it in terms of improving truthfulness since larger models are better at learning the training distribution from web data and thus tend to produce more imitative falsehoods. In another recent work,  fine-tune GPT-3 to answer long-form questions with a web-browsing environment, which allows the model to navigate the web as well as use human feedback to optimize answer quality directly using imitation learning~. \n\\ziwei{While this method seems promising, it also hinges on how that feedback is processed.}", "cites": [679, 432, 2006, 2217, 7569, 9, 2366, 2378, 2376], "cite_extract_rate": 0.6923076923076923, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.5, "abstraction": 3.3}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers on hallucination mitigation in GQA, grouping them into early external knowledge-based methods and more recent generation-focused approaches. It provides some critical analysis by highlighting limitations such as the need for high-quality resources or extra modeling effort. While it identifies trends like the shift from relying on external knowledge to improving the generation process, the abstraction remains moderate, as it does not fully generalize into overarching theoretical frameworks."}}
{"id": "88b99036-1cad-4b0d-958a-98dce00c74e0", "title": "Hallucination in Data-to-Text Generation", "level": "section", "subsections": ["dbc9c61e-f290-4ce4-b0c8-0313e03520c2", "dbd064ed-1891-436a-846e-a35812a9c4aa", "963e4673-7257-411e-80a3-51554da3056c", "83e23e5d-b9c7-4c30-b8d3-6d30a93766d5"], "parent_id": "c4299bf7-fa65-4214-968a-c213c95e6d07", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Data-to-Text Generation"]], "content": "\\label{section:data2text}\nData-to-Text Generation is the task of generating natural language descriptions conditioned on structured data~, such as tables~, database records~, and knowledge graphs~.\nAlthough this field has been recently boosted by neural text generation models, it is well known that these models are prone to hallucinations~ because of the gap between structured data and text, which may cause semantic misunderstanding and erroneous correlation.\nMoreover, the tolerance of hallucination is very low when this task is applied to the real world, such as in the case of patient information table description~, and analysis of experimental results tables in a scientific report.\nRecent years have seen a growth of interest in hallucinations in Data-to-Text Generation, and researchers have proposed works from the aspect of evaluation and mitigation.", "cites": [2345, 2346, 2413], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces the concept of hallucination in data-to-text generation and mentions the cited papers in a general way, but does not synthesize or integrate their contributions into a broader narrative. It lacks critical evaluation of the works and does not abstract beyond the specific examples to highlight overarching patterns or principles. As such, it remains largely descriptive."}}
{"id": "dbc9c61e-f290-4ce4-b0c8-0313e03520c2", "title": "Hallucination Definition in Data-to-Text Generation", "level": "subsection", "subsections": [], "parent_id": "88b99036-1cad-4b0d-958a-98dce00c74e0", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Data-to-Text Generation"], ["subsection", "Hallucination Definition in Data-to-Text Generation"]], "content": "The definition and categories of hallucination in Data-to-Text Generation follow the descriptions in Section \\ref{section:definition}. We follow the general hallucination definition in this task:\n(1) Intrinsic Hallucinations: the generated text contains information that is contradicted by the input data~. For example, in Table \\ref{Table: example}, ``\\textit{The Houston Rockets (18-4)}'' uses the information ``\\textit{[TEAM: Rockets, CITY:Houston, WIN:18, LOSS: 5]}'' in the source table. However, ``\\textit{(18-4)}'' is contradicted by ``\\textit{[LOSS: 5]}'' and it should be ``\\textit{(18-5)}''.\n(2) Extrinsic Hallucinations: the generated text contains extra information irrelevant to the input ~. For example, in Table \\ref{Table: example}, ``\\textit{Houston has won two straight games and six of their last seven.}'' is not mentioned in the source table~.", "cites": [2385, 2386, 2372], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic definition of hallucinations in data-to-text generation and cites three relevant papers. However, it does not effectively synthesize these sources into a broader narrative, nor does it critically analyze their contributions or limitations. The content remains at the descriptive level, offering minimal abstraction or insight beyond reporting definitions and examples."}}
{"id": "51203880-ea0d-45f4-a470-b97f70024700", "title": "Statistical", "level": "paragraph", "subsections": ["1f8b565f-e96f-4584-b328-061b6cac2fc6", "a5053db2-3566-4c8b-aadf-d873ae201645", "233c7ef4-6ccd-42d3-b56e-682e2e46e520", "d93f5d53-2305-438c-807a-9334afb83909"], "parent_id": "dbd064ed-1891-436a-846e-a35812a9c4aa", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Data-to-Text Generation"], ["subsection", "Hallucination Metrics in Data-to-Text Generation"], ["paragraph", "Statistical"]], "content": "PARENT~ measures the accuracy of table-to-text generation by aligning n-grams from the reference description \\ziwei{$R$ and generated texts $G$ to the table $T$}. And it is the average F-score by combining the entailment precision and recall. \n modify PARENT and denote this table-focused version as PARENT-T.  \nDifferent from PARENT, which evaluates \\ziwei{i-th} instance $(T_i, R_i, G_i)$, PARENT-T ignores the reference description R and evaluates each instance $(T_i, G_i)$.", "cites": [2348, 2372], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section describes the PARENT and PARENT-T metrics and their use in evaluating hallucination in data-to-text generation, but it does so in a largely descriptive manner. It integrates minimal information from the cited papers and does not provide a broader framework or critical evaluation of the methods. There is little abstraction or synthesis beyond the specific metrics."}}
{"id": "1f8b565f-e96f-4584-b328-061b6cac2fc6", "title": "IE-based", "level": "paragraph", "subsections": [], "parent_id": "51203880-ea0d-45f4-a470-b97f70024700", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Data-to-Text Generation"], ["subsection", "Hallucination Metrics in Data-to-Text Generation"], ["paragraph", "Statistical"], ["paragraph", "IE-based"]], "content": "\\vspace{-0.5em} \n estimate hallucination with two entity-centric metrics: table record coverage (the ratio of covered records in a table) and hallucinated ratio (the ratio of hallucinated entities in text).\nThis metric firstly uses entity recognition to extract the entities of input and generated output,\nthen aligns these entities by heuristic matching strategies,\nand finally calculates the ratios of faithful and hallucinated entities separately.\nMoreover, there are some general post-hoc IE-based metrics that could be applied to hallucination evaluation, such as Slot Error Rate (SER)~, \nContent Selection (CS), Relation Generation (RG), and Content Ordering (CO)~.", "cites": [2367, 2385, 7570], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of IE-based hallucination metrics in data-to-text generation, referencing specific papers but without deeper analysis or comparison. It outlines the general approach of entity recognition and alignment but does not critically evaluate the strengths, weaknesses, or broader implications of these methods across the cited works."}}
{"id": "a5053db2-3566-4c8b-aadf-d873ae201645", "title": "QA-based", "level": "paragraph", "subsections": [], "parent_id": "51203880-ea0d-45f4-a470-b97f70024700", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Data-to-Text Generation"], ["subsection", "Hallucination Metrics in Data-to-Text Generation"], ["paragraph", "Statistical"], ["paragraph", "QA-based"]], "content": "\\vspace{-0.5em} \n\\ziwei{Data-QuestEval~ adapt QuestEval~ from summarization into data-to-text generation. \nFirst, a \\textit{textual QG model} is trained on a textual QA dataset. \nFor each sample (structured data, textual descriptions), the \\textit{textual QG model} generates synthetic problems based on the descriptions.\nThe structured data, textual descriptions (answers), and synthetic questions make up a synthetic QG/QA dataset to train \\textit{synthetic QA/QG models}.\nThen, the \\textit{synthetic QG} model generates questions based on the textual description to be evaluated. The \\textit{synthetic QA} model then generates answers based on a synthetic question and the structured input data.\nFinally, BERTScore~ measures the similarity between the generated answer and description, indicating faithfulness.\n}", "cites": [8549, 2358], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a clear description of the QA-based hallucination metric Data-QuestEval and its adaptation of QuestEval. However, it primarily summarizes the methodology without critically evaluating its strengths, weaknesses, or comparing it to other approaches. There is limited abstraction or synthesis with broader themes in hallucination research."}}
{"id": "233c7ef4-6ccd-42d3-b56e-682e2e46e520", "title": "NLI-based", "level": "paragraph", "subsections": [], "parent_id": "51203880-ea0d-45f4-a470-b97f70024700", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Data-to-Text Generation"], ["subsection", "Hallucination Metrics in Data-to-Text Generation"], ["paragraph", "Statistical"], ["paragraph", "NLI-based"]], "content": "\\vspace{-0.5em} \n recognize the textual entailment between the input data and the output text for both omissions and hallucinations with an NLI model. This work measures the semantic accuracy in two directions: check omissions by inferring whether the input fact is entailed by the generated text and check hallucinations by inferring the generated text from the input.", "cites": [7189], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal description of NLI-based hallucination metrics by mentioning the use of NLI models to check omissions and hallucinations. It lacks synthesis of ideas from the cited paper (or others), critical evaluation of the approach, and abstraction to broader principles or trends. The explanation is brief and does not contextualize the method within the broader field of hallucination detection in NLG."}}
{"id": "d93f5d53-2305-438c-807a-9334afb83909", "title": "LM-based", "level": "paragraph", "subsections": [], "parent_id": "51203880-ea0d-45f4-a470-b97f70024700", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Data-to-Text Generation"], ["subsection", "Hallucination Metrics in Data-to-Text Generation"], ["paragraph", "Statistical"], ["paragraph", "LM-based"]], "content": "\\vspace{-0.5em} \n are based on the intuition that when an unconditional LM, only trained on the targets, gets a smaller loss than a conditional $LM_x$, trained on both sources and targets, the token is predicted unfaithfully. Thus, they calculate the ratio of hallucinated tokens to the total target length to measure the hallucination level.", "cites": [2343, 2364], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly describes a concept from the cited papers regarding LM-based hallucination metrics but does not synthesize or integrate the ideas meaningfully. It lacks critical evaluation or comparison of the approaches, and does not abstract or generalize the concept into broader patterns or principles, remaining at a descriptive level."}}
{"id": "468649d6-ba5e-4de1-a974-8e72b92bb59e", "title": "Data-Related Methods", "level": "paragraph", "subsections": ["30344d72-008f-4c57-a4c2-4ce07478a859"], "parent_id": "963e4673-7257-411e-80a3-51554da3056c", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Data-to-Text Generation"], ["subsection", "Hallucination Mitigation in Data-to-Text Generation"], ["paragraph", "Data-Related Methods"]], "content": "Several clean and faithful corpora are collected to tackle the challenges from data infidelity. \nTOTTO~ is an open-domain faithful table-to-text dataset, where each sample includes a Wikipedia table with several highlighted cells and a description. \nTo ensure that targets exclude hallucinations, the annotators revise existing Wikipedia candidate sentences and clear the parts unsupported by the table.  \nMoreover, RotoWire-FG (Fact-Grounding)~ is a purified and enlarged and enriched version of RotoWire~ generating NBA game summaries from score tables.\nAnnotators trim the hallucination part in target texts and extract the mapped table records as content plans to better align input tables and output summaries.\nFor data processing, \\ziwei{OpAtt~ designs a gating mechanism and a quantization module for the symbolic operation to augment the record table with pre-calculated results.}\n utilize a language understanding module to improve the equivalence between the input MR and the reference utterance in the dataset.\nThey train an NLU model with an iterative relabeling procedure: \nFirst, they train the model on original data; parse the MR by model inference; train the model on new paired data with high confidence; and then repeat the above processes.\n select training instances based on faithfulness ranking. Finer-grained than the above instance-level method,  label tokens according to co-occurrence analysis and sentence structure through dependency parsing in the pre-processing step to explicate the correspondence between the input table and the text. \n\\ziwei{Generally, the data-related methods are appropriate when the training dataset is noisy.}", "cites": [2386, 2345, 2367, 2385, 2371], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of data-related methods for mitigating hallucinations in data-to-text generation by listing examples such as TOTTO and RotoWire-FG. While it integrates some ideas (e.g., data cleaning and iterative relabeling), the synthesis is limited and does not develop a broader framework. There is little critical analysis of the methods' effectiveness or limitations, and abstraction is minimal, focusing mainly on specific techniques and datasets."}}
{"id": "30344d72-008f-4c57-a4c2-4ce07478a859", "title": "Modeling and Inference Methods", "level": "paragraph", "subsections": [], "parent_id": "468649d6-ba5e-4de1-a974-8e72b92bb59e", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Data-to-Text Generation"], ["subsection", "Hallucination Mitigation in Data-to-Text Generation"], ["paragraph", "Data-Related Methods"], ["paragraph", "Modeling and Inference Methods"]], "content": "\\vspace{-0.5em} \nPlanning and skeleton generation are common methods to improve the faithfulness to the input in data-to-text tasks. \n propose a two-step generator with a separate text planner augmented by auxiliary entity information.\nThe planner predicts the plausible content plan based on the input data. Then, given the above input data and the content plan, the sequence generator generates the text. \nSimilarly, Plan-then-Generate~ also consists of a content planner and a sequence generator.\nIn addition, this work adopts a structure-aware RL training to generate output text following the generated content plan faithfully. \n\\ziwei{ first induce a macro plan consisting of multiple sequences of entities and events from the input table and its corresponding multi-paragraph long document. The predicted macro plan then serves as the input to an encoder-decoder model for surface realization.}\nSANA~ is a skeleton-based two-stage model that includes skeleton generation to select key tokens from the source table and edit-based generation to produce texts via iterative insertion and deletion operations.\nIn contrast to the above two-step model using planning or skeleton, AGGGEN~ is an end-to-end model that jointly learns to plan and generate at the same time.\nThis architecture with a Hidden Markov Model and Transformer encoder-decoder reintroduces explicit sentence planning stages into neural systems by aligning facts in the target text to input representations.\nOther modeling methods have also been proposed to mitigate the hallucination problem.\nConjecturing that hallucinations can be caused by inattention to the source,  propose a confidence score and a variational Bayes training framework to learn the score from data.  \n introduce a table-text optimal-transport matching loss and an embedding similarity loss to encourage faithfulness.\nThe hallucination degree can also be treated as a controllable factor in generating texts.\nIn , the hallucination degree of each training sample is estimated and converted into a categorical value which is a part of the inputs as a controlled setting. \nThis approach does not require the dismissal of any input or modification of the model structure.\nTo mitigate hallucinations at the inference step,~ propose a Multi-Branch Decoder that leverages word-level alignment labels between the input table and paired text to learn the relevant parts of the training instance. These word-level labels are gained through dependency parsing during the pre-processing step.\nThe branches separately integrate three co-dependent control factors: content, hallucination, and fluency.\nUncertainty-aware beam search (UABS)~ is an extension to beam search to reduce hallucination. Considering that the hallucination probability is positively correlated with predictive uncertainty, this work adds a weighted penalty term in the beam search which is able to balance the predictive probability and uncertainty. \\ziwei{This approach is task-agnostic and can also be applied to other tasks, such as image captioning.}\n\\ziwei{These various types of methods do not necessarily conflict and can collaborate to solve the hallucination problem in data-to-text generation. }", "cites": [2343, 2356, 8544, 2367, 2348, 2371, 2353, 2364, 7570, 8547], "cite_extract_rate": 1.0, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several papers by categorizing them into planning-based, skeleton-based, and end-to-end modeling methods, which creates a coherent narrative around hallucination mitigation strategies. It offers some critical analysis by discussing the assumptions behind certain approaches (e.g., hallucination linked to inattention, predictive uncertainty) and the use of control factors. However, while there is a generalization of patterns (e.g., planning improves faithfulness), the analysis remains somewhat focused on methodological descriptions rather than deeper theoretical or comparative insights."}}
{"id": "83e23e5d-b9c7-4c30-b8d3-6d30a93766d5", "title": "Future Directions in Data-to-Text Generation", "level": "subsection", "subsections": [], "parent_id": "88b99036-1cad-4b0d-958a-98dce00c74e0", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Data-to-Text Generation"], ["subsection", "Future Directions in Data-to-Text Generation"]], "content": "Given the challenges brought by the discrepancy between structure data and natural text, and the low fault tolerance in the Data-to-Text Generation task, there are several potential directions worth exploring in terms of hallucination. \nFirstly, numbers contain information about scales and are common and crucial in the Data-to-Text task ~.\nIt is frequent to have errors in numbers, which results in hallucinations and infidelity. \nThis is a serious problem for Data-to-Text generation, yet models rarely give special consideration to the numbers found in the table or text~. The current automatic metrics of hallucinations also do not specifically treat numbers. \nThis indiscriminate treatment contradicts findings in cognitive neuroscience, where numbers are known to be represented differently from lexical words in a different part of the brain~.\nThus, considering or highlighting numbers when mitigating and assessing hallucinations is worth exploring. This requires the generative model to learn a better numerical presentation and capture scales, which will reduce the hallucinations caused by the misunderstanding of numbers. \nMoreover, for the logical data-to-text generation task, rather than surface-level generation, logical inference, calculation, and comparison are required, which is challenging and causes hallucinations more easily.   \nThus, reasoning (including numerical reasoning), which is usually combined with graph structures~ is another direction to improve the accuracy of entity relationships and alleviate hallucinations.", "cites": [2405, 8552, 2406], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes insights from three different papers to highlight the unique challenges of handling numbers and logic in Data-to-Text generation. It identifies a critical gap in how models treat numbers and logical inference, and links these to cognitive neuroscience and broader implications for hallucination mitigation. The abstraction is strong as it moves beyond individual methods to propose general research directions and underlying principles."}}
{"id": "834833c3-36eb-4e30-8dc3-efe4af6e32ee", "title": "Hallucinations Definition and Categories in NMT", "level": "subsection", "subsections": ["bfe8f69f-3918-45fa-bb7a-db7a83403528"], "parent_id": "b5d00b7b-aadc-4051-b146-dd50d420d75a", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucinations in Neural Machine Translation"], ["subsection", "Hallucinations Definition and Categories in NMT"]], "content": "The problem of hallucination was identified with the deployment of the first NMT models. Early work comparing SMT and NMT systems~, without explicitly using the term “hallucination”, mentioned that NMT models tend to “sacrifice adequacy for the sake of fluency” especially when evaluated with out-of-domain test sets. Following further development of NMT, most of the relevant research papers agree that translated text is considered a hallucination when it is completely disconnected from the source ~. \n  The categorization of hallucination in NMT is unlike that in any other NLG tasks, and uses various terms that are often overlapping. In order to maintain consistency with other NLG tasks, in this section we use the intrinsic and extrinsic hallucination categories applied to the NMT task by~. After a formal definition, we will describe other identified types of hallucinations and hallucination categories mentioned in the relevant literature.", "cites": [2339, 2344, 2350], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key definitions of hallucination in NMT from early observations to more recent terminology, integrating insights from multiple papers to form a coherent narrative. It provides some critical context by contrasting NMT hallucinations with the behavior of SMT systems, though it does not deeply evaluate the limitations of specific methods. The section attempts abstraction by aligning NMT hallucination categories with broader NLG frameworks, but the insights remain somewhat surface-level without deeper theoretical generalization."}}
{"id": "bfe8f69f-3918-45fa-bb7a-db7a83403528", "title": "Intrinsic and Extrinsic Hallucinations", "level": "paragraph", "subsections": ["6b371292-c57d-4404-aa35-78f10f517b9f"], "parent_id": "834833c3-36eb-4e30-8dc3-efe4af6e32ee", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucinations in Neural Machine Translation"], ["subsection", "Hallucinations Definition and Categories in NMT"], ["paragraph", "Intrinsic and Extrinsic Hallucinations"]], "content": "Following the idea that hallucinations are outputs that are disconnected from the source,~ suggest categorizing the hallucinatory content based on the way the output is disconnected:\n\\begin{itemize}\n    \\item Intrinsic Hallucination: translations contain incorrect information compared to information present in the source. \n    In Table \\ref{Table: categories_in_mt}, the example of such hallucination is “Jerry doesn't go”, since the original name in the source is “Mike” and the verb “to go” is not negated. \n    \\item Extrinsic Hallucination: translations produce additional content without any regard to the source. In Table \\ref{Table: categories_in_mt},  “happily” and “with his friend” are the two examples of the hallucinatory content since they are added without any apparent connection to the input.\n\\end{itemize}", "cites": [2344], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of intrinsic and extrinsic hallucinations in Neural Machine Translation (NMT), primarily using an example from a cited paper. However, it lacks synthesis of multiple sources, deeper critical evaluation, and abstraction to broader principles, remaining at a descriptive level with minimal insight."}}
{"id": "6b371292-c57d-4404-aa35-78f10f517b9f", "title": "Other Categories and Types of Hallucinations", "level": "paragraph", "subsections": [], "parent_id": "bfe8f69f-3918-45fa-bb7a-db7a83403528", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucinations in Neural Machine Translation"], ["subsection", "Hallucinations Definition and Categories in NMT"], ["paragraph", "Intrinsic and Extrinsic Hallucinations"], ["paragraph", "Other Categories and Types of Hallucinations"]], "content": " propose an alternative categorization of hallucinations. They divide hallucinations into hallucinations under perturbations and natural hallucinations. Hallucinations under perturbation are those that can be observed if a model tested on the perturbed and unperturbed test set returns drastically different content. Their work on hallucinations under perturbation strictly follows the algorithm proposed by ; see Section \\ref{sec: entropy_measure} on the entropy measure. The second category, natural hallucinations, are created with a connection to the noise in the dataset and can be further divided into detached and oscillatory, where detached hallucinations mean that a target translation is semantically disconnected from a source input, and oscillatory hallucinations mean those that are decoupled from the source by manifesting a repeating n-gram.  and  analyze this phenomenon under the name “over-translation”, that is, a repetitive appearance of words that were not in the source text. Conversely, under-translation is skipping the words that need to be translated~. Finally, abrupt jumps to the end of the sequence and outputs that remain mostly in the source language are also examples of hallucinatory content~.\n\\begin{table}[]\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{@{}llll@{}}\n\\toprule\n  \\multicolumn{1}{c}{\\textbf{Category}} &\n  \\multicolumn{1}{c}{\\textbf{Source}} &\n  \\multicolumn{1}{c}{\\textbf{Correct Translation}} &\n  \\multicolumn{1}{c}{\\textbf{Hallucinatory Translation}} \\\\ \\midrule\n  Intrinsic &\n  \\begin{CJK*}{UTF8}{gbsn}  迈克周四去书店。\\end{CJK*}   &\n  \\begin{tabularx}{0.4\\textwidth}{X}Mike goes to the bookstore on Thursday.\\end{tabularx} &\n  \\begin{tabularx}{0.4\\textwidth}{X}Jerry doesn't go to the bookstore on Thursday.\\end{tabularx} \\\\\n  Extrinsic &\n  \\begin{CJK*}{UTF8}{gbsn}  迈克周四去书店。\\end{CJK*}   &\n  \\begin{tabularx}{0.4\\textwidth}{X}Mike goes to the bookstore on Thursday.\\end{tabularx} &\n  \\begin{tabularx}{0.4\\textwidth}{X}Mike happily goes to the bookstore on Thursday with his friend.\\end{tabularx} \\\\\\midrule\n  Detached &\n  \\begin{tabularx}{0.4\\textwidth}{X}Das kann man nur feststellen, wenn die kontrollen mit  einer großen  intensität  durchgeführt werden.\\end{tabularx} &\n  \\begin{tabularx}{0.4\\textwidth}{X}This can only be detected if controls undertaken are more rigorous.\\end{tabularx} &\n  \\begin{tabularx}{0.4\\textwidth}{X}Blood alone moves the wheel of history, i say to you and you will understand, it is a privilege to fight.\\end{tabularx} \\\\\n  Oscillatory &\n  \\begin{tabularx}{0.4\\textwidth}{X}1995 das produktionsvolumen von 30 millionen pizzen wird erreicht.\\end{tabularx}&\n  \\begin{tabular}[c]{@{}l@{}}1995 the production \\\\ reached 30 million pizzas.\\end{tabular} &\n  \\begin{tabularx}{0.4\\textwidth}{X}The US, for example, has been in the past two decades, but has been in the same position as the US, and has been in the United States.\\end{tabularx} \\\\\\bottomrule\n\\end{tabular}}\n\\caption{Categories and examples of hallucinations in MT by  and }\n\\label{Table: categories_in_mt}\n\\end{table}", "cites": [2344, 1112, 2384, 2335], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple categorizations of hallucinations from different papers, presenting a structured overview including intrinsic, extrinsic, detached, and oscillatory hallucinations. While it integrates these concepts and provides definitions with examples, it lacks deeper critical evaluation of the methodologies or limitations of the cited works. Some abstraction is evident in grouping similar phenomena under broader types like 'over-translation' and 'under-translation,' but it remains focused on the specific context of NMT without identifying overarching principles."}}
{"id": "71b6c531-6315-4bcb-86b6-993cdc2a1133", "title": "Hallucination Metrics in NMT", "level": "subsection", "subsections": ["9f6fc251-5d53-4c7e-8a59-466e20270563", "7e741643-0bb0-481c-a8d3-ac6f7b84918f"], "parent_id": "b5d00b7b-aadc-4051-b146-dd50d420d75a", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucinations in Neural Machine Translation"], ["subsection", "Hallucination Metrics in NMT"]], "content": "The definition of hallucinations in machine translation (MT) tends to be qualitative and subjective, and thus researchers often identify hallucinated content manually. Most detrimentally, the appearance of hallucinations is found not to affect the BLEU score of the translated text~. There are, nevertheless, several notable efforts to automatize and quantify the search for hallucinations using statistical methods.", "cites": [2344, 2364], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the challenges in defining hallucinations in NMT and briefly mentions efforts to automate their detection. It cites two papers but does not integrate their ideas into a coherent framework or compare their approaches. There is no deep analysis or generalization of broader patterns or principles related to hallucination metrics in NMT."}}
{"id": "b09f1662-f5b9-481f-94c2-90cfcbf39de4", "title": "Auxiliary Decoder", "level": "paragraph", "subsections": [], "parent_id": "7e741643-0bb0-481c-a8d3-ac6f7b84918f", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucinations in Neural Machine Translation"], ["subsection", "Hallucination Metrics in NMT"], ["subsubsection", "Model-Based Metrics"], ["paragraph", "Auxiliary Decoder"]], "content": "\\label{sec: auxiliary decoder}\n“Faithfulness” refers to the amount of source meaning that is faithfully expressed in the translation, and it is used interchangeably with the term “adequacy”~.  propose adding another “evaluation decoder” apart from the standard translation decoder.  In their work, faithfulness is based on word-by-word translation probabilities, and is calculated in the evaluation module along with translation fluency. The loss returned by the evaluation module helps to adjust the probability returned by the translation module.", "cites": [8548, 2414], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the auxiliary decoder method and its relation to faithfulness in NMT, citing two papers. However, it lacks synthesis across the works, does not compare or critically evaluate the approaches, and does not abstract to broader principles or trends in hallucination measurement. The narrative is minimal and mostly parrots key ideas without deeper insight."}}
{"id": "f7aff4c7-6182-4a71-83cf-e9d4fe8954b7", "title": "Entropy Measure", "level": "paragraph", "subsections": [], "parent_id": "7e741643-0bb0-481c-a8d3-ac6f7b84918f", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucinations in Neural Machine Translation"], ["subsection", "Hallucination Metrics in NMT"], ["subsubsection", "Model-Based Metrics"], ["paragraph", "Entropy Measure"]], "content": "\\label{sec: entropy_measure}\nIn scenarios where the ground truth of a translation is not available, an entropy measure of the average attention distribution can be used to detect hallucinations.  and  show that hallucinations are visible in attention matrices. When the model outputs correct translation, the attention mechanism attends to the entire input sequence throughout decoding. However, it tends to concentrate on one point when the model outputs hallucinatory content. \nThe entropy is calculated on the average attention weights when the model does or does not produce hallucinations during testing. \nFor comparison, a clean test set is used along with the purposefully perturbed one, which is created to incite hallucinations (test sets featuring multiple repetitions). \nThe mean entropy returned by hallucinatory models diverges from the mean of the models that do not produce hallucinations spontaneously~.", "cites": [1112, 2360], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes two papers by connecting their findings on attention mechanisms and coverage in NMT to explain how entropy in attention can indicate hallucination. It offers a basic analytical perspective by describing how attention behavior differs in correct versus hallucinatory outputs. However, it lacks deeper critical evaluation of the methods or broader abstraction beyond the specific entropy measure."}}
{"id": "ab8150e5-1266-45f5-b7b2-66f0379e18ff", "title": "Token Level Hallucination Detection", "level": "paragraph", "subsections": [], "parent_id": "7e741643-0bb0-481c-a8d3-ac6f7b84918f", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucinations in Neural Machine Translation"], ["subsection", "Hallucination Metrics in NMT"], ["subsubsection", "Model-Based Metrics"], ["paragraph", "Token Level Hallucination Detection"]], "content": "\\label{paragraph: token_level}\n propose a method for detecting hallucinated tokens within a sentence, making the search more fine-grained. They use a synthetic dataset that is created by adding noise to the source data, more specifically it is generated by a language model with certain tokens of correct translations masked. Tokens in synthetic data are labeled as hallucinated (1) or not (0). Then the authors compute the hallucination prediction loss between binary labels and the tokens from the hallucinated sentence.\nThis work further employs a word alignment-based method and overlap-based method as baselines for hallucination.", "cites": [2344], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the token-level hallucination detection method from the cited paper, including the use of a synthetic dataset and loss computation. However, it lacks synthesis of broader ideas, critical evaluation of the method's strengths or limitations, and abstraction to general principles or trends in hallucination detection."}}
{"id": "37d5dd3e-e7a4-4067-9b66-88370348e729", "title": "Similarity-based Mathods", "level": "paragraph", "subsections": [], "parent_id": "7e741643-0bb0-481c-a8d3-ac6f7b84918f", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucinations in Neural Machine Translation"], ["subsection", "Hallucination Metrics in NMT"], ["subsubsection", "Model-Based Metrics"], ["paragraph", "Similarity-based Mathods"]], "content": " use an unsupervised model that extracts alignments from similarity matrices of word embeddings~, and then predicts the target token as hallucinated if it is not aligned to the source.\n propose calculating faithfulness by computing similarity scores between perturbed source sentence and target sentence after applying the same perturbation.", "cites": [2344, 2339], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section lacks synthesis, critical evaluation, and abstraction. It briefly mentions two papers but only provides minimal paraphrasing of their methods without connecting them or situating them within a broader context of hallucination detection in NMT."}}
{"id": "6f6c2758-8647-45b7-8ae4-b21a055809d6", "title": "Overlap-based Mathods", "level": "paragraph", "subsections": [], "parent_id": "7e741643-0bb0-481c-a8d3-ac6f7b84918f", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucinations in Neural Machine Translation"], ["subsection", "Hallucination Metrics in NMT"], ["subsubsection", "Model-Based Metrics"], ["paragraph", "Overlap-based Mathods"]], "content": " predict that the target token is hallucinated if it does not appear in the source. Since the target and source are two different languages, the authors use the density matching method for bilingual synonyms from .\n suggest the Coverage Difference Ratio (CDR) as the metric to evaluate adequacy, which is especially successful in finding cases of under-translation. It is estimated by comparing source words covered by generated translation with human translations. \nThe overlap-based methods for detecting hallucinations are heuristics based on the assumption that all translated words should appear in the source. However, this is not always the case, e.g., when paraphrasing or using synonyms. Using word embeddings as similarity-based methods helps avoid such simplifications and allows more diverse, synonymous translations.", "cites": [2344, 2415, 2384], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates three different papers to explain overlap-based methods for hallucination detection in NMT, connecting the concept of source-target token matching to the use of word embeddings for handling synonyms. It also critiques the limitations of strict overlap assumptions, showing awareness of real-world translation needs. However, the analysis remains somewhat surface-level and does not deeply compare or contrast the methods or propose a new conceptual framework."}}
{"id": "0ddd5459-2beb-445e-8488-3d5391f73065", "title": "Approximate Natural Hallucination Detection", "level": "paragraph", "subsections": [], "parent_id": "7e741643-0bb0-481c-a8d3-ac6f7b84918f", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucinations in Neural Machine Translation"], ["subsection", "Hallucination Metrics in NMT"], ["subsubsection", "Model-Based Metrics"], ["paragraph", "Approximate Natural Hallucination Detection"]], "content": " propose Approximate Natural Hallucination (ANH) detection based on the fact that hallucinations often occur as oscillations (repeating n-grams) and the lower unique bigram count indicates a higher appearance of oscillatory hallucinations. Furthermore, the ANH detection method searches for repeated targets in the translation output. Their method finds translation above a certain n-gram threshold and searches for repeated targets in the output translation, following the assumption that if hallucinations are often incited by aligning unique sources to the same target, then repeating targets will also appear during the inference~.", "cites": [1112, 2335], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly describes the ANH detection method but does not synthesize or integrate the cited papers meaningfully. It lacks critical analysis of the method's strengths, limitations, or comparative performance. The content remains concrete and does not generalize to broader patterns or principles, resulting in a low insight level."}}
{"id": "dac06e83-16e2-4fdc-9046-ae2a4c3ff07a", "title": "Data-Related", "level": "subsubsection", "subsections": [], "parent_id": "c31437d9-f81e-4a3c-82f8-c668236d1025", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucinations in Neural Machine Translation"], ["subsection", "Hallucination Mitigation Methods in NMT"], ["subsubsection", "Data-Related"]], "content": "Data augmentation appears to be one of the most common methods for removing hallucination.  and  suggest addition of perturbed sentences. Furthermore, perturbation, where the insertions of most common tokens are placed at the beginning of the sentence, seems to be the most successful in hallucination mitigation. A disadvantage of this method is the need to understand different types of hallucinations produced by the model in order to apply a correct augmentation method.\nCorpus filtering is a method of mitigating hallucinations caused by the noise in the dataset by removing the repetitive and mismatching source and target sequences~. \n\\rita{ implements a cross-entropy data filtering method for bilingual data, which uses cross-entropy scores calculated for noisy pairs according to two translation models trained on the clean data. The scores that suggest dissagreament between sentence pairs from two models are subsequently penalized.}\nWhile~ and~ define noise as mismatched source and target sentences, ~ analyzes the influence of fine-grained semantic divergences on NMT outputs. The authors consequently propose a mitigation method for fine-grained divergences based on semantic factors. The tags are applied to each source and target sentence to inform about the position of divergent tokens. Factorizing divergence not only helps to mitigate hallucinations, but improves the overall performance of the NMT. This shows that tagging small semantic divergences can provide useful information for the network during training.", "cites": [2335, 2371], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple cited works to discuss data-related mitigation strategies for hallucinations in NMT, connecting ideas about data augmentation and filtering. It includes a critical note on the limitations of data augmentation, such as the need to understand different types of hallucinations. However, the synthesis is somewhat limited in scope and could benefit from a more explicit framework or deeper comparative analysis to generalize broader principles."}}
{"id": "ad8de037-efef-45d1-87bc-edaa8456b3f9", "title": "Modeling and Inference", "level": "subsubsection", "subsections": [], "parent_id": "c31437d9-f81e-4a3c-82f8-c668236d1025", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucinations in Neural Machine Translation"], ["subsection", "Hallucination Mitigation Methods in NMT"], ["subsubsection", "Modeling and Inference"]], "content": "Overexposure bias is a common problem in NMT, amplified by the teacher-forcing technique used in sequence-to-sequence models. The models are trained on the ground truth, but during inference, they attend to the past predictions, which can be incorrect~.\nTo mitigate this problem,  propose substituting MLE as a training objective with minimum risk training (MRT)~. \nScheduled sampling is a classic method of mitigating overexposure bias first proposed by~. Based on that method,~ create a differentiable approximation to greedy decoding that shows a good performance in the NMT task.~ propose further improvement of the scheduled sampling algorithm for NMT by optimizing the probability of source and target word alignments. This improvement helps to address the issue flexibility in word order between a source and target language when performing scheduled sampling.\n propose a method of improving self-training of NMT based on hallucination detection. They create hallucination labels (see Section \\ref{paragraph: token_level}), and then discard losses of tokens predicted as hallucinations, which is known as token loss truncation. This is similar to the method proposed by , the latter for full sentences in the summarization task. Furthermore, instead of adjusting losses,  mask the hidden states of the discarded losses in the decoder in a procedure called decoder HS masking. Experimental results show both a translation quality improvement in terms of BLEU and also a large reduction in hallucination. The token loss truncation method shows good results in the low-resource languages scenario.\nAnother method to mitigate the impact of noisy datasets is tilted empirical risk minimization (TERM), a training objective proposed by .\n mentions that techniques such as dropout, L2E regularization, and clipping tend to decrease the number of hallucinations. Lastly, several authors propose methods of improving phrase alignment that are helpful both in increasing translation accuracy and identifying content that did not appear in the source translation~.", "cites": [2344, 2362, 2373, 2384, 2360, 2382, 2377, 1106, 8553, 2389, 2416], "cite_extract_rate": 0.7857142857142857, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers to connect overexposure bias in NMT with hallucination mitigation, integrating methods like MRT, scheduled sampling, token loss truncation, and TERM. It identifies how these techniques aim to reduce exposure to incorrect previous predictions during inference. However, the critical analysis is limited, as it does not deeply evaluate trade-offs or limitations of the approaches. The section provides some abstraction by framing the mitigation strategies in terms of general training and inference challenges, but not at a high meta-level."}}
{"id": "6f3894c0-f7b0-49a7-b7f0-f70d99e50933", "title": "Future Directions in NMT", "level": "subsection", "subsections": [], "parent_id": "b5d00b7b-aadc-4051-b146-dd50d420d75a", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucinations in Neural Machine Translation"], ["subsection", "Future Directions in NMT"]], "content": "The future work on hallucinations in NMT is to define hallucinations in a quantifiable manner; i.e., to specify a cut-off value between translation error and hallucinated content using a particular metric. \n propose a threshold between fluency and adequacy which is the closest to this ideal. They, however, do not concentrate on hallucinated content as such, and thus fluent but inadequate sentences may not always indicate hallucinations but also other types of translation errors.\n mention constrained decoding as a method to mitigate hallucinations in dialogue systems, but it could also be applied in NMT.~ and~ use constrained decoding to incorporate specific terminology into MT, but the above methods can be repurposed to mitigate hallucinations. \nAnother direction for future work on hallucinations is improving existing methods of searching for hallucinatory content, such as the algorithms proposed by  and , that are computationally expensive~ or require the creation of an additional perturbed test-set~. \nSimilarly, for mitigation of lack of faithfulness and fluency, the method proposed by  requires the creation of a one-to-many architecture (one encoder and two decoders), which is also computationally expensive. \nFuture directions would therefore include simplification of existing hallucination evaluation methods, applying them to different architectures like CNNs and transformers, and possibly conducting research on finding simpler hallucination search methods.", "cites": [1999, 2417, 2419, 2418, 2421, 8548, 2420, 2335], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section attempts to synthesize various approaches to constrained decoding and hallucination mitigation in NMT, connecting them to broader challenges in defining and addressing hallucinations. It critically points out the computational inefficiencies and limitations of existing methods, such as the need for additional resources or architectures. While it identifies general patterns, such as the trade-off between fluency and fidelity, it stops short of offering a novel framework or deeper theoretical abstraction."}}
{"id": "d2d13288-b44b-4b69-a8fa-0b14b1f34c11", "title": "Hallucination in Vision-Language Generation", "level": "section", "subsections": ["4b20840e-6b2c-49f1-ae64-61482d9190cc", "f0941cf9-6da5-4880-8fa3-bcea35500adb", "bdfc5126-a335-439f-9814-2f295e72e43e"], "parent_id": "c4299bf7-fa65-4214-968a-c213c95e6d07", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Vision-Language Generation"]], "content": "\\label{section:VL}\nWith the vast advancement of the Transformer architecture~ in both CV and NLP, there is a trend to pre-train large-scale unified vision-language (VL) models~ to perform vision grounded text generation tasks, such as image captioning and visual question answering (VQA). Generally, there are two common schemas for vision-language pre-training: 1) pre-train from scratch with a massive amount of image-text pairs as well as optionally a large text-only corpus; or 2) initialize model parameters from a large pre-trained LM and then adapt it to the VL domain with adequate image-text pairs. Either way, the learned vision and language representations are aligned in the same multimodal space and the resulting model can be seen as a LM that understands visual information. Therefore, the hallucination problem is also observed in VL models due to similar reasons as found in NLG. \nIn the VL domain, the research on hallucination is still in its very early stage and how to measure and mitigate hallucination is an open question. In this section, we first review the hallucination in image captioning as it is the only VL task that has corresponding previous research works. Then, we introduce hallucination phenomena found in other VL tasks. Finally, we discuss potential future research directions on this problem.", "cites": [7565, 38, 2422, 1582, 732], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of hallucination in vision-language generation and mentions the relevance of cited papers without deeply integrating or connecting their ideas. It lacks critical evaluation or comparison of the approaches and focuses on high-level phenomena without abstracting overarching principles or trends."}}
{"id": "20fb62c3-f5e4-4e86-90e6-121323fd1a1d", "title": "Metrics", "level": "paragraph", "subsections": [], "parent_id": "9de55480-e3c8-4f55-bd4b-64cac046a531", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Vision-Language Generation"], ["subsection", "Object Hallucination in Image Captioning"], ["paragraph", "Definition."], ["paragraph", "Metrics"]], "content": "To automatically measure object hallucination,  propose the CHAIR (Caption Hallucination Assessment with Image Relevance) metric, which calculates what proportion of object words generated are actually in the image according to the ground truth captions. Specifically, there are two variants of it, which are CHAIR$_i$ and CHAIR$_s$ defined as follows,\n\\begin{align*}\n    \\textrm{CHAIR}_i = \\frac{\\textrm{\\# \\{hallucinated objects\\}}}{\\textrm{\\# \\{all objects in ground truth\\}}}, \n    \\textrm{CHAIR}_s = \\frac{\\textrm{\\# \\{hallucinated captions\\}}}{\\textrm{\\# \\{all captions\\}}}.\n\\end{align*}\n\\noindent CHAIR$_i$ measures per-instance object hallucination, i.e. what fraction of object instances in each generated caption are hallucinated. CHAIR$_s$ measures per-sentence object hallucination, i.e. what fraction of generated captions include at least one hallucinated object. For example, to calculate CHAIR scores for the MSCOCO dataset~,  apply the 80-object list used in the MSCOCO segmentation challenge  and find exact matches of object words or phrases in captions.", "cites": [486, 2423, 2340], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section primarily describes the CHAIR metric and its variants, citing the relevant paper for their definitions and usage. It provides some synthesis by explaining the two variants and how they are applied (e.g., using the MSCOCO object list), but lacks deeper integration with the other cited works. There is minimal critical evaluation or comparison of the proposed metric with alternative approaches, and the abstraction level remains limited to specific definitions and applications without broader conceptual generalization."}}
{"id": "688f6021-73aa-49a8-9ef5-0ccab69a3012", "title": "Mitigation", "level": "paragraph", "subsections": [], "parent_id": "9de55480-e3c8-4f55-bd4b-64cac046a531", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Vision-Language Generation"], ["subsection", "Object Hallucination in Image Captioning"], ["paragraph", "Definition."], ["paragraph", "Mitigation"]], "content": "As a research problem that is in its early stage, there are currently a limited number of approaches proposed to mitigate object hallucination in image captioning.  hypothesize that the main cause of object hallucination is the systematic co-occurence of particular object categories in input images. They propose three simple yet effective ways of data augmentation to make the co-occurence statistics matrix more uniform to mitigate object hallucination. Results show that their introduced method can reduce object hallucination without changing model architectures.  From another perspective,  propose an uncertainty-aware beam search method for decoding and exhibit that reducing uncertainty can lead to less hallucination. Specifically, a weighted penalty term is added to the beam search objective to balance between log probability and predictive uncertainty of the selected word candidates. More recently,  analyze object hallucination in VL pre-training and propose a novel pre-training objective named object masked language modeling to alleviate this problem.", "cites": [2353, 2341, 2368], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly describes three distinct approaches from the cited papers but does not effectively synthesize or connect them into a cohesive narrative or framework. It lacks critical evaluation of the methods' strengths and weaknesses, and does not abstract beyond the specific techniques to highlight broader patterns or principles in object hallucination mitigation."}}
{"id": "f0941cf9-6da5-4880-8fa3-bcea35500adb", "title": "Hallucination in Other VL Tasks", "level": "subsection", "subsections": [], "parent_id": "d2d13288-b44b-4b69-a8fa-0b14b1f34c11", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Vision-Language Generation"], ["subsection", "Hallucination in Other VL Tasks"]], "content": "\\label{sec:hall_VL_other}\nIn addition to image captioning, hallucination has also been observed in other VL tasks and raised as an open research question. For example, in open-ended visual question answering, Figure~\\ref{fig:vqa_examples} (left and right) shows that the model could generate seem likely answers when we only see the text, however wrong when given the image. Moreover, Figure~\\ref{fig:vqa_examples} (middle) indicates that hallucination can also be triggered by adversarially prompting an unanswerable question. The model will imagine an unsupported answer that commonly matches the given visual scene. \n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{images/vqa_examples.pdf}\n    \\caption{Examples of hallucination in visual question answering (taken from ). The bold text is the output generated by the model and the part before it is the input prompt.}\n    \\label{fig:vqa_examples}\n\\end{figure}", "cites": [7565], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly mentions hallucination in visual question answering and provides an example with a figure, but it lacks synthesis of multiple sources and does not integrate ideas beyond one paper. It offers minimal critical evaluation and no abstraction or identification of broader patterns or principles across works."}}
{"id": "bdfc5126-a335-439f-9814-2f295e72e43e", "title": "Future Directions in VL", "level": "subsection", "subsections": [], "parent_id": "d2d13288-b44b-4b69-a8fa-0b14b1f34c11", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Vision-Language Generation"], ["subsection", "Future Directions in VL"]], "content": "For future research on the hallucination problem in VL, we summarize three promising directions. \nFirstly, hallucination detection and mitigation in VL is still in the early stage. There is a lack of empirical and theoretical analyses in many tasks, such as visual storytelling, visual commonsense reasoning, video captioning, etc. Secondly, more effective evaluation metrics are needed. For example, although CHAIR can automatically evaluate the degree of object hallucination in image captioning, it requires a pre-defined list of object categories, which does not generalize well. Furthermore, currently there is no automatic metric for the hallucination types discussed in Section~\\ref{sec:hall_VL_other}. Therefore, we cannot perform quantitative evaluations for them. Thirdly, we believe how to perform controlled generation~ with visual grounding is a promising direction to mitigate hallucination in VL.", "cites": [2371, 2368], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview by identifying key gaps and directions in vision-language hallucination research, integrating insights from the cited papers. While it connects the concept of controlled generation with visual grounding, the synthesis is not fully novel but builds on existing ideas. Critical analysis is present, particularly in pointing out limitations of evaluation metrics like CHAIR, but deeper evaluation of methods is limited. The abstraction is reasonable as it generalizes the problem to broader VL tasks and identifies a principle of the need for better control in generation."}}
{"id": "d89e5465-3eec-484a-9fa1-383de676a03c", "title": "Hallucination in Large Language Models", "level": "section", "subsections": ["832380a5-1f16-47e5-a6f3-9bbe42b3b49d", "ca961c5a-714d-41df-a1e1-63e2460a8e6d", "87a0a996-c597-49e1-a125-8a3267a56f1f", "dd1c1d98-f881-491e-aba5-b0164777f160"], "parent_id": "c4299bf7-fa65-4214-968a-c213c95e6d07", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Large Language Models"]], "content": "\\label{section: LLM}\nScaling up models and data sizes for language models has shown great empirical success. The resulting Large Language Models (LLMs) not only have achieved significant performance improvements upon previous PLMs across a diverse array of NLP tasks, but have also demonstrated many emerging abilities and strong steerability after instruction tuning and Reinforcement Learning from Human Feedback (RLHF)~. However, they still exhibit the hallucination problem. Even worse, since LLMs generate highly fluent and convincing responses, their hallucinations become more difficult to identify, and more likely to have harmful consequences. The launch of ChatGPT as an LLM with a conversational user interface led to the popularity of LLMs and their wide range of real-world applications. The research on the identification and mitigation of hallucination in LLMs has intensified. LLMs are usually open-ended general-purpose systems, which differ much from task-specific models, and their architectural designs, data coverage, training methodologies, and model behaviors are also different from PLMs mentioned in previous sections. Therefore, in the following sections, we specifically discuss the hallucination problem in LLMs, by covering more recent works, introducing novel strategies for measuring and mitigating hallucination, as well as listing unsolved questions and future directions.~\\footnote{This section was updated in Jan 2024.}", "cites": [8461, 8556], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates key ideas from the two cited papers to frame hallucination in LLMs as a distinct and challenging issue due to their scale and fluency. It provides a coherent narrative by connecting model scaling, emergent abilities, and the implications for hallucination. However, it lacks deeper critical evaluation or systematic comparison of the cited works and offers limited meta-level abstraction beyond specific observations."}}
{"id": "c2b69e3f-f4d3-4c81-b032-e7c4eb41a388", "title": "Reference-dependent Metrics", "level": "subsubsection", "subsections": ["009942ed-0e3b-41ce-bce3-d5026750ab4e", "ea53eee5-a2f7-403a-b9c0-8957f7491418"], "parent_id": "ca961c5a-714d-41df-a1e1-63e2460a8e6d", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Large Language Models"], ["subsection", "Hallucination Metrics for LLMs "], ["subsubsection", "Reference-dependent Metrics"]], "content": "Reference-dependent metrics are often formatted as ``hallucination benchmarks'' that measure the hallucination level not risks of different LLMs. Most benchmarks utilize multiple choice QA or fill-in-the-blank QA to assess model hallucination, while some efforts further consider the ratio of partially incorrect answers~ or instances where the model admits that it cannot answer~. Some benchmarks adopt the open domain and open-ended QA setting, where exact-match-based evaluations generally fail to recognize semantically equivalent answers~. Free-form responses from LLMs can be analyzed based on atomized decomposition (FactualityPrompt~ and FActScore~), LLM-based evaluation (self-judgment~ or GPT-4-based judgment~), or by measuring the discrepancy of the vanilla response and the response giving the knowledge for reference~.", "cites": [2425, 2397, 2427, 9151, 2217, 410, 9118, 2424, 2426], "cite_extract_rate": 0.9, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of reference-dependent hallucination metrics for LLMs, listing various methods and benchmarks from the cited papers. While it mentions different evaluation settings (e.g., multiple choice QA, free-form responses), it does not synthesize the findings into a cohesive framework or highlight how these metrics differ in effectiveness. There is minimal critical analysis or abstraction to broader principles."}}
{"id": "009942ed-0e3b-41ce-bce3-d5026750ab4e", "title": "Human-annotated Hallucination Benchmarks", "level": "paragraph", "subsections": [], "parent_id": "c2b69e3f-f4d3-4c81-b032-e7c4eb41a388", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Large Language Models"], ["subsection", "Hallucination Metrics for LLMs "], ["subsubsection", "Reference-dependent Metrics"], ["paragraph", "Human-annotated Hallucination Benchmarks"]], "content": "\\label{SEC:Human-annotated Hallucination Benchmarks} \n    Collecting human-verified QA samples to test the factual knowledge of LLMs is the most straightforward method. Since the training corpora of LLMs are extremely large, high frequency count knowledge and basic commonsense can be easily memorized. As a result, examining low frequency, long-tailed knowledge becomes a primary focus of these hallucination benchmarks. Testing samples for long-tail knowledge can be selected based on frequency of appearance (\\textit{e.g.,} in pretraining corpora~ or Wikitext~), popularity measurements based on website traffic data (\\textit{e.g.,} PopQA~, Head-to-Tail~), recency (RealTimeQA~), or from specific domains (\\textit{e.g.,} ExpertQA~ and Med-HALT~). In addition to long-tail knowledge, another way to build challenging benchmarks is to gather questions that are more likely to lead to hallucinations -- a methodology similar to adversarial prompting~ and red teaming~. The most representative example is TruthfulQA~, a benchmark containing challenging questions that some humans would answer falsely due to false beliefs or misconceptions (\\textit{e.g.,} question \\textit{``If it's cold outside, what does that tell us about global warming?''} leads to GPT-3 answers \\textit{``It tells us that global warming is a hoax''}). \n    Similarly, the DecodingTrust~ evaluated the trustworthiness of GPT models in terms of adversarial robustness, OOD knowledge, vulnerability of jail-breaking, misleading instructions, etc.\n    Known-Unknown~ and SeflAware~ evaluated LLMs' awareness of uncertainty for question without definitive answer (\\textit{e.g.,} \\textit{``If the Universe started at the Big Bang, what existed before then?''})", "cites": [9108, 2434, 2431, 2432, 2217, 2430, 7573, 2429, 7572, 2433, 2428, 7574, 7571], "cite_extract_rate": 0.8666666666666667, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers by categorizing hallucination benchmarks into methods such as long-tail knowledge testing and adversarial/red-teaming approaches. It provides a coherent narrative by highlighting different design philosophies and motivations behind the benchmarks. While it offers some critical evaluation (e.g., noting the limitations of models on known-unknown questions), it stops short of deeper comparative or methodological critique. It also identifies some broader patterns, such as the focus on rare or uncertain knowledge, but does not elevate the discussion to a fully meta-level framework."}}
{"id": "ea53eee5-a2f7-403a-b9c0-8957f7491418", "title": "Automated Hallucination Benchmarking", "level": "paragraph", "subsections": [], "parent_id": "c2b69e3f-f4d3-4c81-b032-e7c4eb41a388", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Large Language Models"], ["subsection", "Hallucination Metrics for LLMs "], ["subsubsection", "Reference-dependent Metrics"], ["paragraph", "Automated Hallucination Benchmarking"]], "content": "Manual question collection and answer annotation are labor intensive, hard to scale, and hard to adapt to new domains. To address this issue, annotation-free benchmarking methods have been proposed, which can be further divided into 1) automated benchmark generation and 2) automated evidence retrieval. For the first type of methods~, factual but unstructured corpora (for example, Wikipedia and reliable news articles) are transformed into a unified QA format based on information extraction and answer candidate generation. The second type of methods automatically retrieves evidence related to LLM generations from the Internet, similar to automated fact checking~. They typically adopt a chained pipeline, where search queries are first generated (by an LLM) based on the content that needs to be verified, and then a model determines whether it contains hallucinations according to the retrieved evidence~). This pipeline can be further enhanced by upgrading the verification model from an NLI model~ to prompted/fine-tuned LLMs (as in AttributionScore~ and~), or by adding additional steps to the pipeline, such as tool interaction (as in FactTool~ and CRITIC~), claim decomposition (Self-Checker~), additional second stage fine-grained retrieval and claim-focused summarization (as in~), or chain-of-verification~.", "cites": [2435, 7575, 2437, 2440, 468, 2436, 423, 2439, 2438], "cite_extract_rate": 0.75, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers into two main categories—automated benchmark generation and automated evidence retrieval—highlighting their shared goals and workflows. It abstracts these methods into a general pipeline and discusses enhancements, such as using prompted/fine-tuned LLMs or adding claim decomposition and tool interaction. While it provides a coherent framework and identifies broader trends, it could offer a more critical evaluation of the limitations or trade-offs between these approaches."}}
{"id": "fc28593f-098e-443f-8b86-add68fe3fcf7", "title": "Reference-free Metrics", "level": "subsubsection", "subsections": ["bf77905c-b925-40e7-9612-69a3108a06ef", "e9e0e22e-0980-4dfe-b63b-7c1d2177ef32"], "parent_id": "ca961c5a-714d-41df-a1e1-63e2460a8e6d", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Large Language Models"], ["subsection", "Hallucination Metrics for LLMs "], ["subsubsection", "Reference-free Metrics"]], "content": "Reference-free metrics aim to assess the hallucination risk of LLMs without relying on external references or ground truth. Although not as reliable as reference-based ones, these metrics are particularly useful when golden references are unavailable, expensive to obtain, or when hallucination identification needs to be performed in real-time. Reference-free metrics generally fall into two categories: uncertainty-based and consistency-based hallucination detection. Uncertainty-based methods rely on the token probabilities assigned by the LLM during generation, whereas consistency-based methods evaluate the coherence of multiple completions generated by the LLM. There are also some benchmarks to measure the effectiveness of these reference-free hallucination metrics, which are also termed ``hallucination detection benchmarks'', such as HaluEval~ and HaDes~ These benchmarks serve as meta-metrics of hallucination metrics, offering valuable resources for developing more effective methods to identify hallucinations.", "cites": [8546, 7576], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the main categories of reference-free hallucination metrics (uncertainty-based and consistency-based) and references two relevant benchmarks, connecting them to the broader theme of hallucination detection. It provides a general analytical perspective by explaining the motivation and limitations of reference-free metrics, but lacks deeper comparisons or critiques of the specific cited papers. The abstraction level is moderate as it introduces a conceptual categorization and discusses the role of benchmarks in a meta-metric sense."}}
{"id": "bf77905c-b925-40e7-9612-69a3108a06ef", "title": "Uncertainty-based Hallucination Detection", "level": "paragraph", "subsections": [], "parent_id": "fc28593f-098e-443f-8b86-add68fe3fcf7", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Large Language Models"], ["subsection", "Hallucination Metrics for LLMs "], ["subsubsection", "Reference-free Metrics"], ["paragraph", "Uncertainty-based Hallucination Detection"]], "content": "This type of method assumes that LLMs assign high probabilities to tokens that they are confident of, and low probabilities to uncertain tokens, which usually contain hallucinated information. It has a close relationship with classical uncertainty estimation methods~, since the generation of each token can be viewed as a vocabulary size classification problem. The difference here is that autoregressive text generation of LLM is a chained classification process, therefore individual token (subword) probabilities need to be aggregated so that they can reflect word-/sentence-/passage-level uncertainty. Such aggregation can be done by average-/max-/min-pooling on token probabilities, calculating the normalized product of the token probabilities, or calculating the maximum/averaged entropy~. These methods can be further extended by adding prompts like \\textit{``Generate factually consistent summary for the following text: {<source-text> <generated-text>}''}~ for fine-grained control (factuality evaluation on summarization), or \\textit{``{<question> <generated-answer>} The answer is true.''}~ for self-evaluation of QA samples.", "cites": [1585, 2441, 8551, 2442], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key concepts from multiple papers, particularly linking uncertainty estimation in deep learning to hallucination detection in LLMs. It abstracts the common idea that high token probabilities reflect confidence and introduces general aggregation methods. While it briefly mentions possible applications and extensions, it could offer deeper critical comparisons between the different methods or highlight limitations more explicitly."}}
{"id": "e9e0e22e-0980-4dfe-b63b-7c1d2177ef32", "title": "Consistency-based Hallucination Detection", "level": "paragraph", "subsections": [], "parent_id": "fc28593f-098e-443f-8b86-add68fe3fcf7", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Large Language Models"], ["subsection", "Hallucination Metrics for LLMs "], ["subsubsection", "Reference-free Metrics"], ["paragraph", "Consistency-based Hallucination Detection"]], "content": "is the more reliable and common approach. In practical applications, many LLMs only offer an API while internal logits remain inaccessible (\\textit{i.e.,} the ``black-box setting''), thus making uncertainty-based methods inapplicable. Efforts have been made to create surrogate approximations of token probability. Specifically, LLMs first generate multiple completions with stochastic sampling given a fixed context, and then the consistency of these completions is used to reflect the uncertainty. Consistency can be measured by the BLUE-based variation ratio~, BERTScore~, n-gram approximation~, NER-based overlap ratio~, NLI model~, or LLM-based judgment~.", "cites": [2443, 7577, 8551, 2442, 2444], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly describes consistency-based hallucination detection methods and lists several metrics used, but does not synthesize or connect the ideas from the cited papers into a deeper narrative. It lacks critical evaluation of the methods or their limitations and remains at a concrete level without generalizing to broader principles or trends."}}
{"id": "138d304e-7fbe-4d3f-8595-0851f278e45c", "title": "Data for Pretraining", "level": "paragraph", "subsections": [], "parent_id": "4106a64f-147b-4ef3-9545-fc4c758a1360", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Large Language Models"], ["subsection", "Hallucination Mitigation in LLMs"], ["subsubsection", "Data-Related Methods"], ["paragraph", "Data for Pretraining"]], "content": "Recent studies~ show that the knowledge within LLMs is almost entirely acquired during pretraining. Thus, a strong emphasis should be placed on ensuring the quality of the pretraining data. This can be performed through the collection of pretraining data from credible sources~ and the minimization of defective and noisy data such as those that are unreliable or unverifiable~. A representative example of such data-driven methods can be found in the development of Llama-2~, where the most factual sources within the pretraining data were up-sampled in order to reduce hallucinations. Similarly, Falcon LLM~ also demonstrated that their data refinement methods including rigorously filtering and deduplicating can significantly boost LLM performance.  Due to the extensive pretraining datasets, some approaches have been developed to overcome the impracticalities that arise from manually selecting them. For instance, for the pretraining of GPT-3~, a classifier-based automatic filtering method was applied to remove low-quality documents. However, as pretraining data is being continuously scaled up, it is becoming increasingly costly for the research community to even access the entire pretraining dataset. As an alternative, small LLMs like phi-1.5~, which are trained on strictly controlled ``textbooks-style'' small-scale corpus, provide a valuable opportunity for doing rigors ablations to investigate the relationship between pretraining data and model hallucination.", "cites": [7558, 679, 366, 2207, 1552, 2445], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the role of pretraining data in mitigating hallucinations by drawing on Llama-2, Falcon, and phi-1.5, and connects their approaches to broader data quality considerations. While it includes some critical points (e.g., cost of accessing full datasets), it lacks deeper evaluation of methodological trade-offs. It abstracts to a degree by discussing general trends in data selection and filtering but remains grounded in specific models and techniques."}}
{"id": "7f5de1b1-c631-4462-ab8f-e21f3aaf9a2a", "title": "Data for Instruction-tuning", "level": "paragraph", "subsections": [], "parent_id": "4106a64f-147b-4ef3-9545-fc4c758a1360", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Large Language Models"], ["subsection", "Hallucination Mitigation in LLMs"], ["subsubsection", "Data-Related Methods"], ["paragraph", "Data for Instruction-tuning"]], "content": "During the fine-tuning stage, data refinement can also be used to dampen hallucinations. As the size of fine-tuning data is significantly smaller, manual approaches can also be practically used~. For example, the authors of LIMA collected a diverse dataset of 1,000 carefully curated prompts and responses which are aligned with each other~. Conversely,  point out that widely used fine-tuning datasets e.g. Alpaca’s 52k data contain low-quality instances with incorrect or irrelevant responses which are detrimental to fine-tuning, used an automated approach leveraging strong LLMs to identify and discard such low-quality data. Alongside, the authors of InstructMining~, proposed linear rules for selecting high-quality instruction fine-tuning data and avoided the need for human or machine annotations.", "cites": [2446, 7558, 366, 7578], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes three distinct papers to present a narrative on the use of data refinement for hallucination mitigation in instruction-tuning. It connects the ideas of manual curation (LIMA), automated filtering (AlpaGasus), and rule-based selection (InstructMining), showing a basic level of integration. While it offers some critical evaluation by highlighting the limitations of widely used datasets like Alpaca’s, it does not deeply analyze trade-offs or compare the effectiveness of the methods. The abstraction level is moderate, as it identifies a general theme of improving data quality for instruction-tuning but does not elevate the discussion to a meta-level framework."}}
{"id": "92d62501-285a-4d9b-8f0d-0a9799dd77fb", "title": "Data for Reward Model Training", "level": "paragraph", "subsections": [], "parent_id": "4106a64f-147b-4ef3-9545-fc4c758a1360", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Large Language Models"], ["subsection", "Hallucination Mitigation in LLMs"], ["subsubsection", "Data-Related Methods"], ["paragraph", "Data for Reward Model Training"]], "content": "Training reliable reward models such that desirable and undesirable outputs are distinguished is an effective method for mitigating hallucination. These models can subsequently be incorporated into reinforcement learning pipelines. Since the effectiveness of the system is constrained by the performance of the reward model itself, emphasis has been placed on determining the most effective training methods~. Thus, OpenAI released a dataset, namely, PRM800K, which resulted in a SOTA reward model~. In the development of GPT-4~, two different approaches were implemented. Firstly, in tackling open-domain hallucinations, OpenAI collected real-world ChatGPT data that were flagged as non-factual by users and used it together with additional labeled comparison data to train their reward models. Secondly, for closed-domain hallucinations, OpenAI utilized GPT-4 itself to generate and subsequently utilize synthetic data for reward model training.", "cites": [9115, 8557], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes methods used by OpenAI in reward model training, drawing from the GPT-4 Technical Report. It mentions two approaches but does not synthesize broader insights or connect them to the general framework of hallucination mitigation. There is minimal critical analysis or abstraction beyond the specific techniques outlined."}}
{"id": "2120a274-c304-4ecc-a169-3465aade9e8a", "title": "Retrieval Augmented Generation (RAG)", "level": "paragraph", "subsections": [], "parent_id": "4106a64f-147b-4ef3-9545-fc4c758a1360", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Large Language Models"], ["subsection", "Hallucination Mitigation in LLMs"], ["subsubsection", "Data-Related Methods"], ["paragraph", "Retrieval Augmented Generation (RAG)"]], "content": "RAG is a technique that leverages external sources to enhance the reliability of generative models~, which is a popular method used to mitigate hallucination~. For example, Gemini incorporates the search engine to provide external references during generation. \nHowever, RAG faces several challenges~: \nIn the retrieval stage, the model is required to discern the noise, irrelevant or fake information. \nIn the augmentation stage, integrating heterogeneous, independent, and even conflicting information presents challenges. \nAdditionally, the model needs to reject generation when insufficient information is given.\nRecent works make efforts to address these challenges.\nFor example, MixAlign~ enhances the alignment between user questions and stored knowledge to improve retrieval and reduce hallucination.", "cites": [468, 466], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a concise overview of RAG as a method for hallucination mitigation and mentions one recent work (MixAlign) that addresses its challenges. It integrates the general concept of RAG from the cited papers and highlights common challenges in retrieval and augmentation stages, showing some synthesis. However, it lacks deeper critical evaluation of these papers and offers only a moderate level of abstraction by touching on recurring issues rather than presenting a novel framework or meta-level insights."}}
{"id": "3073d770-da90-4533-9e00-9a69e9875f84", "title": "Safety Fine-Tuning and Reinforcement Learning with Human Feedback (RLHF)", "level": "paragraph", "subsections": [], "parent_id": "6a4fe2c9-ef2a-41a0-8741-bd463e71bfe3", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Large Language Models"], ["subsection", "Hallucination Mitigation in LLMs"], ["subsubsection", "Modelling and Inference Methods"], ["paragraph", "Safety Fine-Tuning and Reinforcement Learning with Human Feedback (RLHF)"]], "content": "Safety fine-tuning is a prevalent method to mitigate risks associated with LLMs, such as hallucinations. A prominent technique in this domain is Reinforcement Learning with Human Feedback (RLHF).\nThe application of RLHF in the fine-tuning process of GPT-3 leads to the development of InstructGPT, which significantly reduces the hallucination rate from 41\\% to 21\\% in closed domain tasks~.\nSimilarly, Llama2~ demonstrates that post-RLHF, the model consistently generated factual responses to prompts on factual information. The study also highlighted the use of RLHF in enhancing the overall safety of LLMs, including a reduction in hallucinations~.\nDespite the advantages, Safety Fine-Tuning and RLHF can incur `alignment tax' and `catastrophic forgetting' where the models lose diverse previously acquired abilities after alignment~. To counterbalance these issues,  proposes Adaptive Model Averaging (AMA) optimizing the averaging ratios of different model layers to maximize alignment reward.", "cites": [364, 2445], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes information from two papers (InstructGPT and Llama2) by connecting the use of RLHF to the reduction of hallucinations. It provides a critical perspective by mentioning limitations such as 'alignment tax' and 'catastrophic forgetting.' However, the abstraction is limited, as it does not generalize beyond these two models or identify broader principles of safety fine-tuning across the literature."}}
{"id": "c28e96fa-8548-46df-99d1-5b2578c56f30", "title": "Model Editing", "level": "paragraph", "subsections": [], "parent_id": "6a4fe2c9-ef2a-41a0-8741-bd463e71bfe3", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Large Language Models"], ["subsection", "Hallucination Mitigation in LLMs"], ["subsubsection", "Modelling and Inference Methods"], ["paragraph", "Model Editing"]], "content": "Editing Models such as parameter adapting can also serve to reduce hallucination.\nGiven that some parameters are more responsible for causing hallucinations, EWR (Elastic Weight Removal)~ weighs their individual importance via Fisher Information matrix and performs parameter interpolation to remove the undesired behaviors.", "cites": [2447], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal synthesis by briefly mentioning one approach, EWR, and its mechanism. It lacks critical evaluation of the method or comparison with others. The content remains largely descriptive and concrete, without abstraction or generalization to broader patterns or principles in hallucination mitigation."}}
{"id": "47d3eb71-6fdf-4bfa-9cf2-2d944cb4c199", "title": "Decoding Methods", "level": "paragraph", "subsections": [], "parent_id": "6a4fe2c9-ef2a-41a0-8741-bd463e71bfe3", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Large Language Models"], ["subsection", "Hallucination Mitigation in LLMs"], ["subsubsection", "Modelling and Inference Methods"], ["paragraph", "Decoding Methods"]], "content": "Without editing architecture or additional fine-tuning, novel decoding strategies are proposed to reduce hallucinations.\nInspired by the finding that factual knowledge is localized to particular layers, DoLa (Decoding by Contrasting Layers)~ utilizes the difference in logits from the later layers versus earlier layers to amplify the factual knowledge probability. \nSimilarly, CAD (Context-Aware Decoding)~ exploits the difference in logits with and without context.\nIn addition,~ observe the gap between surface generation and internal knowledge and propose ITI (Inference-Time Intervention) to reduce this gap. It first identifies attention heads with high linear probing accuracy for truthfulness and then shifts activations along these truth-correlated directions during inference.", "cites": [2449, 2450, 2448], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section describes three decoding methods for mitigating hallucinations in LLMs but does so in a largely descriptive manner without connecting them to a broader framework or trend. It mentions the core mechanisms of each method (e.g., contrasting layers, context-aware decoding, inference-time intervention) but lacks deeper comparative or critical analysis of their strengths, weaknesses, or trade-offs. There is minimal abstraction or synthesis beyond the surface-level description of individual techniques."}}
{"id": "ecac1f5f-71d4-40a9-bec9-b559b246275a", "title": "Chain-of-Thought and Variants", "level": "paragraph", "subsections": [], "parent_id": "6a4fe2c9-ef2a-41a0-8741-bd463e71bfe3", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Large Language Models"], ["subsection", "Hallucination Mitigation in LLMs"], ["subsubsection", "Modelling and Inference Methods"], ["paragraph", "Chain-of-Thought and Variants"]], "content": "Chain-of-Thought and its variants encourage LLMs to reason prior to arriving at a final reply, which improves performance and reduces hallucinations by leveraging the model's internal knowledge~. \nFor example, Chain-of-Verification (COVE)~ consists of four steps where an LLM first drafts an initial response, plans verification questions to fact check it, answers the questions independently to ensure the answers are not biased by other responses, and finally generates the verified response. \nChain of Natural Language Inference (CoNLI)~ detect hallucinations in the initial response by prompting the LLM to conduct sentence/entity-level NLI, and then refine the response according to the detection result.\n propose iterative self-reflection loops that involve generating relevant background knowledge for a given question followed by a factuality evaluation and self-correction. During answering, a similar generation-score-refine strategy is used for its entailment with the question.\nSolo Performance Prompting (SPP)~ prompts a single LLM to engage in multi-turn self-collaboration with multiple personas, such that their individual strengths and knowledge are combined to boost the problem-solving and performance in tasks.", "cites": [7094, 423, 7093, 2451], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from multiple papers by framing them under the broader concept of Chain-of-Thought and its variants for hallucination mitigation, creating a coherent narrative. It abstracts some common patterns, such as the use of multi-step reasoning and verification, across the cited works. However, the analysis remains largely descriptive with limited comparison or critique of the methods' strengths and weaknesses."}}
{"id": "56364de4-21c8-41ce-b76d-57c719841ccb", "title": "Post-processing", "level": "paragraph", "subsections": [], "parent_id": "6a4fe2c9-ef2a-41a0-8741-bd463e71bfe3", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Large Language Models"], ["subsection", "Hallucination Mitigation in LLMs"], ["subsubsection", "Modelling and Inference Methods"], ["paragraph", "Post-processing"]], "content": "Petite Unsupervised Research and Revision (PURR)~ fine-tune an editor to denoise the corruptions and use the editor to correct the hallucinations in the initial outputs.\nOther works like REFEED~ leverage retrieved documents for post-correction.\nSimilarly, CRITIC~ uses external tools such as search engines and code interpreters to verify and post-correct the initial output.", "cites": [2452, 2453, 7575], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly describes three post-processing methods for mitigating hallucinations in LLMs but lacks synthesis of their underlying principles or integration into a broader narrative. It provides minimal critical evaluation and does not identify limitations or compare the approaches. The content remains concrete and focused on specific systems without generalizing to broader patterns or principles."}}
{"id": "23aa731a-4a46-42d9-ad51-1d819b2af1b4", "title": "Ensemble", "level": "paragraph", "subsections": [], "parent_id": "6a4fe2c9-ef2a-41a0-8741-bd463e71bfe3", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Large Language Models"], ["subsection", "Hallucination Mitigation in LLMs"], ["subsubsection", "Modelling and Inference Methods"], ["paragraph", "Ensemble"]], "content": "Ensembling multiple models to mitigate hallucination is another way. \nFor example, in~, multiple LLMs individually produce responses and then jointly debate their responses and reasoning, ultimately reaching a single common answer. This approach has been demonstrated to improve the factuality of generated content.", "cites": [2454], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly describes an ensembling approach from one cited paper but lacks synthesis with other works, critical evaluation of the method's strengths and weaknesses, or abstraction to broader patterns. It provides a minimal descriptive account of the technique without deeper insight or comparative analysis."}}
{"id": "17a04644-d3a0-4ce8-9209-4f4426697b5c", "title": "Hallucination in Large Multimodal Models", "level": "subsubsection", "subsections": [], "parent_id": "dd1c1d98-f881-491e-aba5-b0164777f160", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Large Language Models"], ["subsection", "Future Directions of Hallucination Mitigation in LLMs"], ["subsubsection", "Hallucination in Large Multimodal Models"]], "content": "Large multimodal models (LMMs) such as GPT-4V can generate rich and detailed responses for visual inputs, therefore being inherently riskier compared to previous VLMs for single-sentence captioning and short-answer VQA. Furthermore,  images convey different levels of information encompassing object existence, attributes (color, shape, etc), spatial relationships, and sometimes high-level emotional responses they elicit (peaceful, beautiful, etc.), which makes mitigating hallucination in LMM even more challenging. The exploration of this area is still in its early stage, with a primary focus on object existence hallucination~. However, it is imperative for LMMs to analyze images well beyond that surface level, otherwise, the use of LMM would be redundant and a simple object detector would suffice. The most crucial factor in this problem is the availability and quality of data and supervision. Language-supervised representations (\\textit{e.g.,} CLIP) have demonstrated inherent shortcomings in recognizing certain types of visual patterns, such as object orientation, quantity, and viewpoint~. Even with perfect representation, imperfect vision-language alignment can result in a significant gap between the visual recognition abilities of the LMM and its visual backbone~. The cost of annotation during the mitigation of multimodal hallucination should also be considered. While there are works on the visual extension of faithfulness-oriented RLHF~, these essentially involve labeling more data, which is expensive and difficult to scale. The versatility of the visual domains also makes it challenging to guarantee robustness and generalization. Large-scale self-supervision may be the path forward, as supervised learning has proven to be insufficient for both the CV and NLP in the past decade.", "cites": [2455, 2456, 2457], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key findings from the cited papers, connecting the limitations of CLIP-based visual representations with the broader issue of hallucination in LMMs. It critically examines current approaches, noting their scalability and robustness issues, and offers a forward-looking perspective by suggesting large-scale self-supervision as a potential solution. The discussion abstracts beyond individual papers to highlight overarching challenges like data quality, vision-language alignment, and generalization across modalities."}}
{"id": "9282d644-9ab0-48d6-afde-cd2be11b69a1", "title": "Estimating the Knowledge Boundary and Expressing the Uncertainty", "level": "subsubsection", "subsections": [], "parent_id": "dd1c1d98-f881-491e-aba5-b0164777f160", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Large Language Models"], ["subsection", "Future Directions of Hallucination Mitigation in LLMs"], ["subsubsection", "Estimating the Knowledge Boundary and Expressing the Uncertainty"]], "content": "Despite the vast size and continuous scaling of pretraining datasets for LLMs, it is impossible for them to encompass all the world's knowledge. Consequently, teaching models to express their inability to answer specific questions and honestly admit ``I don't know'' is crucial, and it requires LLMs to accurately model their knowledge boundaries. Notably, this task lies in a different dimension from other objectives that we expect LLMs to achieve (\\textit{e.g.,} improving QA task accuracy) since it is not explicitly included in the pretraining corpora, making it more challenging. Furthermore, the diverse sources and complex distributions of LLM pretraining data, along with varying training corpora for different LLMs, exacerbate the difficulty of this task. Related research is still in its early stages, with existing methods including calibration-based uncertainty estimation~ and posterior approaches based on QA testing results~, but none of them can satisfactorily solve this issue. For instances near the knowledge boundary, the situation is no longer a binary opposition between knowing and not knowing. In these cases, accurately expressing the model's uncertainty level with fine granularity becomes a critical research question that warrants further exploration.", "cites": [2391, 2458], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates ideas from two papers to discuss the challenge of uncertainty estimation and knowledge boundary modeling in LLMs, showing reasonable synthesis. It offers some critical perspective by noting that current methods are insufficient for fine-grained uncertainty expression, particularly near the knowledge boundary. The content generalizes beyond specific papers by framing the issue as a broader research question, indicating a moderate level of abstraction."}}
{"id": "ce42d4c2-fbcc-428a-9ce4-2bdde527db4a", "title": "Minimizing the Alignment Tax During Hallucination Mitigation", "level": "subsubsection", "subsections": [], "parent_id": "dd1c1d98-f881-491e-aba5-b0164777f160", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Large Language Models"], ["subsection", "Future Directions of Hallucination Mitigation in LLMs"], ["subsubsection", "Minimizing the Alignment Tax During Hallucination Mitigation"]], "content": "Several studies have confirmed that the performance of ChatGPT and GPT-4 degrades over time due to continuous safety fine-tuning. This phenomenon is a typical example of an alignment tax. Similarly, when addressing hallucination problems in low-resource languages, the rate of hallucination in the original languages may increase. In the process of enhancing the faithfulness of LLMs' multi-modal perception through visual instruction tuning on VQA datasets, the ``politeness'' of LLM responses may decrease due to the overly succinct nature of VQA answer annotations~. Furthermore, the alignment tax problem is not confined to fine-tuning-based methods but also extends to prompting-based methods such as Retrieval-Augmented Generation (RAG). These methods may also compromise the quality of responses when the quality of retrieved evidence is sub-optimal. In light of these findings, it is clear that we must explore strategies to minimize the cost during hallucination mitigation and prevent LLMs from becoming overly conservative, losing their creativity, or suffering from catastrophic forgetting in task performance.", "cites": [2459], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the cited paper and relates it to broader observations about alignment tax in hallucination mitigation, showing some integration. It offers a critical perspective by highlighting the trade-offs and negative side effects of mitigation strategies, though not in great depth. The abstraction is moderate, as it generalizes the idea of alignment tax across multiple domains (safety fine-tuning, low-resource languages, and RAG) but stops short of proposing a meta-framework."}}
{"id": "b6ebfd5a-cdba-4f6a-aee4-e2d998be2ad3", "title": "Understanding Hallucination in LLMs", "level": "subsubsection", "subsections": [], "parent_id": "dd1c1d98-f881-491e-aba5-b0164777f160", "prefix_titles": [["title", "Survey of Hallucination in Natural Language Generation"], ["section", "Hallucination in Large Language Models"], ["subsection", "Future Directions of Hallucination Mitigation in LLMs"], ["subsubsection", "Understanding Hallucination in LLMs"]], "content": "Although the research community has put extensive efforts into developing empirical methods for measuring and mitigating hallucinations, our understanding of LLM hallucination still remains limited. The absence of reliable theoretical frameworks and rigorous mathematical formulations hinders our ability to answer fundamental questions and make further advancements. For instance, how does hallucination correlate with model and data size (\\textit{e.g.}, scaling laws)? What is the relationship between hallucination and auto-regressive generation? Do SFT and RLHF employ different mechanisms to mitigate hallucinations? What is the lower bound of hallucination across different tasks and domains? These questions are also intrinsically linked to a broader question -- although now there are a lot of effort on empirically investigating what LLM can and cannot do from different dimensions, we still don't understand why and how it works. There has been some attempts from various perspectives, such as knowledge encoding~, compression~, model selection~, compositionality~, computational perspective~, but we are still a long way from truly understanding the mechanism behind various LLM behaviors.", "cites": [2460, 2463, 2461, 2462], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section draws on several cited papers to frame the lack of theoretical understanding around LLM hallucination, linking them to broader questions about model behavior. It integrates ideas about compositionality, compression, and model structure, but does so in a general and somewhat fragmented manner without a novel unifying framework. The section identifies gaps in current understanding but does not deeply critique or compare the cited works."}}
