{"id": "ecbafaf3-1565-4746-bf9c-a9182b235789", "title": "Introduction", "level": "section", "subsections": ["2ea37eb2-3734-4b08-ad7a-6c1226af2e9b", "b87f5810-e203-4606-8bb5-00623ec5335b", "7291e51c-caa8-4bd7-9c08-2d393eaf83dc"], "parent_id": "5020eb09-4f12-4d1a-8dae-076499c8173a", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "Introduction"]], "content": "\\label{sec:introduction}}\n\\IEEEPARstart{A}{nomalies} were first defined by Grubbs in 1969~ as \\textit{``one that appears to deviate markedly from other members of the sample in which it occurs''} and the studies on anomaly detection were initiated by the statistics community in the 19th century.\nTo us, anomalies might appear as social spammers or misinformation in social media; fraudsters, bot users or sexual predators in social networks; network intruders or malware in computer networks and broken devices or malfunctioning blocks in industry systems, and they often introduce huge damage to the real-world systems they appear in.\nAccording to FBI's 2014 Internet Crime Report\\footnote{https://www.fbi.gov/file-repository/2014\\_ic3report.pdf/view}, the financial loss due to crime on social media reached more than \\$60 million in the second half of the year alone and a more up-to-date report\\footnote{https://www.zdnet.com/article/online-fake-news-costing-us-78-billion-globally-each-year/} indicates that the global economic cost of online fake news reached around \\$78 billion a year in 2020.\nIn computer science, the research on anomaly detection dates back to the 1980s, and detecting anomalies on graph data has been an important data mining paradigm since the beginning.\nHowever, the extensive presence of connections between real-world objects and advances in graph data mining in the last decade have revolutionized our understanding of the graph anomaly detection problems such that this research field has received a dramatic increase in interest over the past five years.\nOne of the most significant changes is that graph anomaly detection has evolved from relying heavily on human experts' domain knowledge into machine learning techniques that eliminate human intervention, and more recently, to various deep learning technologies.\nThese deep learning techniques are not only capable of identifying potential anomalies in graphs far more accurately than ever before, but they can also do so in real-time.\n\\begin{figure}[!t]\n\\setlength{\\belowcaptionskip}{-0.5cm}\n\\centering\n\\subfigure[Conventional Anomaly Detection]{\n\\label{Fig1a}\n\\includegraphics[scale=0.45]{pics/Introduction/Conventional_AD.pdf}}\n\\subfigure[Graph Anomaly Detection]{\n\\label{Fig1b}\n\\includegraphics[scale=0.45]{pics/Introduction/Toy_Example.pdf}} \n\\caption{Toy Examples of Conventional Anomaly Detection and Graph Anomaly Detection. Apart from anomalies shown in (b), graph anomaly detection also identifies graph-level anomalies, detailed in Sections~\\ref{sec:anosgd:db} and~\\ref{sec:anosgd:dynamic}.} \n\\label{Toy}\n\\end{figure}\nFor our purposes today, anomalies, which are also known as outliers, exceptions, peculiarities, rarities, novelties, etc., in different application fields, refer to abnormal objects that are significantly different from the standard, normal, or expected. \nAlthough these objects rarely occur in real-world, they contain critical information to support downstream applications. For example, the behaviors of fraudsters provide evidences for anti-fraud detection and abnormal network traffics reveal signals for network intrusion protection. Anomalies, in many cases, may also have real and adverse impacts, for instance, fake news in social media can create panic and chaos with misleading beliefs~, untrustworthy reviews in online review systems can affect customers' shopping choices~, network intrusions might leak private personal information to hackers~, and financial frauds can cause huge damage to economic systems~.\nAnomaly detection is the data mining process that aims to identify the unusual patterns that deviate from the majorities in a dataset~. In order to detect anomalies, conventional techniques typically represent real-world objects as feature vectors (\\eg news in social media are represented as bag-of-words~, and images in web pages are represented as color histograms~), and then detect outlying data points in the vector space~, as shown in Fig.~\\ref{Fig1a}. \nAlthough these techniques have shown power in locating deviating data points under tabulated data format, they inherently discard the complex relationships between objects~.\nYet, in reality, many objects have rich relationships with each other, which can provide valuable complementary information for anomaly detection.\nTake online social networks as an example, fake users can be created using valid information from normal users or they can camouflage themselves by mimicking benign users' attributes~. \nIn such situations, fake users and benign users would have near-identical features, and conventional anomaly detection techniques might not be able to identify them using feature information only. \nMeanwhile, fake users always build relationships with a large number of benign users to increase their reputation and influence so they can get unexpected benefits, whereas benign users rarely exhibit such activities~.\nHence, these dense and unexpected connections formed by fake users denote their deviations to the benigns and more comprehensive detection techniques should take these structural information into account to pinpoint the deviating patterns of anomalies.\nTo represent the structural information, \\textit{Graphs}, in which nodes/vertices denote real objects, and the edges denote their relationships, have been prevalently used in a range of application fields~, including social activities, e-commerce, biology, academia and communication. With the structural information contained in graphs, detecting anomalies in graphs raises a more complex anomaly detection problem in non-Euclidean space - graph anomaly detection (GAD) that aims to identify anomalous graph objects (\\ie nodes, edges or sub-graphs) in a single graph as well as anomalous graphs among a set/database of graphs~.\nAs a toy example shown in Fig.~\\ref{Fig1b}, given an online social network, graph anomaly detection aims to identify anomalous nodes (\\ie malicious users), anomalous edges (\\ie abnormal relations) and anomalous sub-graphs (\\ie malicious user groups).\nBut, because the copious types of graph anomalies cannot be directly represented in Euclidean feature space, it is not feasible to directly apply traditional anomaly detection techniques to graph anomaly detection, and researchers have intensified their efforts to GAD recently.\nAmongst earlier works in this area, the detection methods relied heavily on handcrafted feature engineering or statistical models built by domain experts~.\nThis inherently limits these techniques' capability to detect unknown anomalies, and the exercise tended to be very labor-intensive. \nMany machine learning techniques, such as matrix factorization~ and SVM~, have also been applied to detect graph anomalies.\nHowever, real-world networks often contain millions of nodes and edges that result in extremely high dimensional and large-scale data, and these techniques do not easily scale up to such data efficiently.\nPractically, they exhibit high computational overhead in both the storage and execution time~.\nThese general challenges associated with graph data are significant for the detection techniques, and we categorize them as data-specific challenges (Data-CHs) in this survey.\nA summary of them is provided in Appendix~\\ref{appendix:data-challenges}.\n\\begin{table*}[!h]\n\\centering \n\\footnotesize\n\\renewcommand\\arraystretch{1}\n\\setlength{\\tabcolsep}{2.8mm}{\n\\caption{A Comparison Between Existing Surveys on Anomaly Detection. We mark edge, sub-graph and graph detection as \\includegraphics[scale=0.2]{pics/Introduction/4.pdf} in our survey because we review more deep learning based works than any previous surveys.} \n\\resizebox{0.96\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c}\n\\toprule[1 pt]\n\\multirow{2}{*}{\\textbf{Surveys}} & \\multirow{2}{*}{\\textbf{AD}} & \\multirow{2}{*}{\\textbf{DAD}} & \\multirow{2}{*}{\\textbf{GAD}} \n& \\multicolumn{4}{c|}{\\textbf{GADL}} & \\multirow{2}{*}{\\textbf{Source Code}} & \\multicolumn{2}{c}{\\textbf{Dataset}}  \\\\ \\cline{5-8}  \\cline{10-11}\n& & & &\\textbf{Node}  &\\textbf{Edge}  & \\textbf{Sub-graph} & \\textbf{Graph} & &\\textbf{Real-world}  &\\textbf{Synthetic} \\\\ \n\\midrule[1 pt]\nOur Survey & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf}& \\includegraphics[scale=0.15]{pics/Introduction/4.pdf}& \\includegraphics[scale=0.15]{pics/Introduction/4.pdf}& \\includegraphics[scale=0.15]{pics/Introduction/4.pdf}& \\includegraphics[scale=0.15]{pics/Introduction/4.pdf}& \\includegraphics[scale=0.15]{pics/Introduction/4.pdf}& \\includegraphics[scale=0.15]{pics/Introduction/4.pdf}& \\includegraphics[scale=0.15]{pics/Introduction/4.pdf}& \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} \\\\\n\\midrule[1 pt]\nChandola \\etal~ & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & - & - & - & - & - & -  & - & - & - \\\\\nBoukerche \\etal~ & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & \\includegraphics[scale=0.15]{pics/Introduction/2.pdf} & - & - & - & - & -  & - & - & - \\\\\nBulusu \\etal~ & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & - & - & - & - & - & - & - & - \\\\\nThudumu \\etal~ & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & \\includegraphics[scale=0.15]{pics/Introduction/1.pdf} & \\includegraphics[scale=0.15]{pics/Introduction/1.pdf} & - & - & - & -  & - & - & - \\\\\n\\midrule[1 pt]\nPang \\etal~ & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & \\includegraphics[scale=0.15]{pics/Introduction/1.pdf} & \\includegraphics[scale=0.15]{pics/Introduction/1.pdf} & \\includegraphics[scale=0.15]{pics/Introduction/1.pdf} & - & - & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & - \\\\\nChalapathy and Chawla~ & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & - & - & - & - & -  & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & - \\\\\n\\midrule[1 pt]\nAkoglu \\etal~ & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & - & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & - & - & - & - & - & - & - \\\\\nRanshous \\etal~ & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & - & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & - & - & -& -  & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & - & - \\\\\nJennifer and Kumar~ & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & - & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & - & - & - & - & - & - & - \\\\\n\\midrule[1 pt]\nEltanbouly \\etal~ & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & \\includegraphics[scale=0.15]{pics/Introduction/2.pdf} & \\includegraphics[scale=0.15]{pics/Introduction/1.pdf} & - & - & - & -  & - & - & - \\\\\nFernandes \\etal~ & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & \\includegraphics[scale=0.15]{pics/Introduction/2.pdf} & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & - & - & - & -  & - & - & - \\\\\nKwon \\etal~ & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & \\includegraphics[scale=0.15]{pics/Introduction/2.pdf} & - & - & - & -  & - & - & \\includegraphics[scale=0.15]{pics/Introduction/1.pdf} & - \\\\\nGogoi \\etal~ & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & \\includegraphics[scale=0.15]{pics/Introduction/1.pdf} & - & - & - & - & - & - & - & - \\\\\n\\midrule[1 pt]\nSavage \\etal~ & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & - & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & - & - & - & - & - & - & - \\\\\nYu \\etal~ & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & - & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & - & - & - & - & - & - & - \\\\\nHunkelmann \\etal~ & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & - & \\includegraphics[scale=0.15]{pics/Introduction/1.pdf} & - & - & - & - & - & - & - \\\\\nPourhabibi \\etal~ & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & \\includegraphics[scale=0.15]{pics/Introduction/2.pdf} & \\includegraphics[scale=0.15]{pics/Introduction/4.pdf} & \\includegraphics[scale=0.15]{pics/Introduction/2.pdf} & - & - & -  & - & - & - \\\\\n\\bottomrule[1 pt]\n\\multicolumn{11}{l}{* AD: Anomaly Detection, DAD: Anomaly Detection with Deep Learning, GAD: Graph Anomaly Detection.} \\\\\n\\multicolumn{11}{l}{* GADL: Graph Anomaly Detection with Deep Learning.}\\\\\n\\multicolumn{11}{l}{* -: not included, \\includegraphics[scale=0.2]{pics/Introduction/1.pdf} (1-2 references included), \\includegraphics[scale=0.2]{pics/Introduction/2.pdf} (3-10 references included), \\includegraphics[scale=0.2]{pics/Introduction/4.pdf} (10+ references included).}\n\\end{tabular}\n\\label{table:comparison}}}\n\\end{table*}\nNon-deep learning based techniques also lack the capability to capture the non-linear properties of real objects~.\nHence, the representations of objects learned by them are not expressive enough to fully support graph anomaly detection.\nTo tackle these problems, more recent studies seek the potential of adopting deep learning techniques to identify anomalous graph objects. \nAs a powerful tool for data mining, deep learning has achieved great success in data representation and pattern recognition~.\nIts deep architecture with layers of parameters and transformations appear to suit the aforementioned problems well. The more recent studies, such as deep graph representation learning and graph neural networks (GNNs), further enrich the capability of deep learning for graph data mining~. \nBy extracting expressive representations such that graph anomalies and normal objects can be easily separated, or the deviating patterns of anomalies can be learned directly through deep learning techniques, graph anomaly detection with deep learning (GADL) is starting to take the lead in the forefront of anomaly detection.\nAs a frontier technology, graph anomaly detection with deep learning, hence, is expected to generate more fruitful results on detecting anomalies and secure a more convenient life for the society.", "cites": [6272, 3207, 1671, 553, 3344, 6271, 6277, 6275, 217, 1662, 6274, 6276, 6270, 6273, 6278], "cite_extract_rate": 0.24193548387096775, "origin_cites_number": 62, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a factual overview of graph anomaly detection, citing relevant prior work, but synthesis is limited to basic connections between conventional and graph-based techniques. Critical analysis is minimal, with only brief mentions of limitations in traditional methods and a lack of deeper evaluation of cited works. Some abstraction is present in discussing the evolution from human-driven to deep learning methods, but broader patterns or meta-level insights are not fully developed."}}
{"id": "2ea37eb2-3734-4b08-ad7a-6c1226af2e9b", "title": "Challenges in GAD with Deep Learning", "level": "subsection", "subsections": [], "parent_id": "ecbafaf3-1565-4746-bf9c-a9182b235789", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "Introduction"], ["subsection", "Challenges in GAD with Deep Learning"]], "content": "\\label{sec:introdution:challenges}\nDue to the complexity of anomaly detection and graph data mining~, in addition to the prior mentioned data-specific challenges, adopting deep learning techniques for graph anomaly detection also faces a number of challenges from the technical side. \nThese challenges associated with deep learning are categorized as technique-specific challenges (Tech-CHs), and they are summarized as follows.\n\\textbf{Tech-CH1. Anomaly-aware training objectives.} Deep learning models rely heavily on the training objectives to fine-tune all the trainable parameters. \nFor graph anomaly detection, this necessitates appropriate training objectives or loss functions such that the GADL models can effectively capture the differences between benign and anomalous objects.\nDesigning anomaly-aware objectives is very challenging because there is no prior knowledge about the ground-truth anomalies as well as their deviating patterns versus the majority.\nHow to effectively separate anomalies from normal objects through training remains critical for deep learning-based models.\n\\textbf{Tech-CH2. Anomaly interpretability.} In real-world scenarios, the interpretability of detected anomalies is also vital because we need to provide convincing evidence to support the subsequent anomaly handling process. For example, the risk management department of a financial organization must provide lawful evidence before blocking the accounts of identified anomalous users. \nAs deep learning has been limited for its interpretability~, how to justify the detected graph anomalies remains a big challenge for deep learning techniques.\n\\textbf{Tech-CH3. High training cost.} Although D(G)NNs are capable of digesting rich information (\\eg structural information and attributes) in graph data for anomaly detection, these GADL models are more complex than conventional deep neural networks or machine learning methods due to the anomaly-aware training objectives. Such complexity inherently leads to high training costs in both time and computing resources.\n\\textbf{Tech-CH4. Hyperparameter tuning.} D(G)NNs naturally exhibit a large set of hyperparameters, such as the number of neurons in each neural network layer, the learning rate, the weight decay and the number of training epochs. Their learning performance is significantly affected by the values of these hyperparameters. However, it remains a serious challenge to effectively select the optimal/sub-optimal settings for the detection models due to the lack of labeled data in real scenarios.\nBecause deep learning models are sensitive to their associated hyperparameters, setting well-performing values for the hyperparameters is vital to the success of a task.\nTuning hyperparameter is relatively trivial in supervised learning when labeled data are available. For instance, users can find an optimal/sub-optimal set of hyperparameters (\\eg through random search, grid search) by comparing the model's outputs with the ground-truth. However, unsupervised anomaly detection has no accessible labeled data to judge the model's performance under different hyperparameter settings~. Selecting the ideal hyperparameter values for unsupervised detection models persists as a critical obstacle to applying them in a wide range of real scenarios.", "cites": [6279, 4539, 6280], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a coherent analytical overview of key challenges in using deep learning for graph anomaly detection, and it integrates insights from the cited papers, particularly linking the meta-learning approach for hyperparameter tuning (Paper 1) and the interpretability issues of deep learning (Paper 2). However, the synthesis remains somewhat surface-level, and the critical analysis is limitedâ€”few limitations or evaluations of the cited works are discussed. The abstraction shows some generalization by framing challenges in a broader technical context, though it stops short of proposing a novel framework."}}
{"id": "b87f5810-e203-4606-8bb5-00623ec5335b", "title": "Existing Anomaly Detection Surveys", "level": "subsection", "subsections": [], "parent_id": "ecbafaf3-1565-4746-bf9c-a9182b235789", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "Introduction"], ["subsection", "Existing Anomaly Detection Surveys"]], "content": "\\begin{figure*}[!t]\n\\setlength{\\belowcaptionskip}{-0.25cm}\n    \\centerline{\\includegraphics[width=0.98\\textwidth]{pics/Introduction/Timeline.pdf}}\n    \\caption{A Timeline of Graph Anomaly Detection and Reviewed Techniques.}\n    \\label{pic:timeline}\n\\end{figure*} \nRecognizing the significance of anomaly detection, many review works have been conducted in the last ten years covering a range of anomaly detection topics: anomaly detection with deep learning, graph anomaly detection, graph anomaly detection with deep learning, and particular applications of graph anomaly detection such as social media, social networks, fraud detection and network security, etc.\nThere are some representative surveys on generalized anomaly detection techniques -~,~ and~.\nBut only the most up-to-date work in Thudumu \\etal~ covers the topic of graph anomaly detection.\nRecognizing the power of deep learning, the three contemporary surveys, Ruff \\etal~, Pang \\etal~ and Chalapathy and Chawla~ specifically review deep learning based anomaly detection techniques specifically.\nAs for graph anomaly detection, Akoglu \\etal~, Ranshous \\etal~, and Jennifer and Kumar~ put their concentration on graph anomaly detection, reviewing many conventional approaches in this area, including statistical models and machine learning techniques.\nOther surveys are dedicated to particular applications of graph anomaly detection, such as computer network intrusion detection and anomaly detection in online social networks, \\eg~, and~.\nThese works provided solid reviews of the application of anomaly detection/graph anomaly detection techniques in these high demand and vital domains. \nHowever, none of the mentioned surveys are dedicated to techniques on graph anomaly detection with deep learning, as shown in Table~\\ref{table:comparison}, and hence do not provide a systematic and comprehensive review of these techniques.", "cites": [6275, 6274, 3207, 3206, 3344], "cite_extract_rate": 0.29411764705882354, "origin_cites_number": 17, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic description of existing surveys in the field of anomaly detection, categorizing them by topic and approach. While it makes some effort to synthesize by grouping works into generalized detection, deep learning-based detection, and application-specific surveys, it lacks deeper integration or a novel framework. Critical analysis is minimal, primarily pointing out that no prior survey focuses on deep learning for graph anomalies without evaluating strengths or weaknesses. Abstraction is limited to surface-level observations of trends and gaps."}}
{"id": "8ea06e54-3d89-4768-967e-6b68c2d22f53", "title": "Preliminaries", "level": "section", "subsections": [], "parent_id": "5020eb09-4f12-4d1a-8dae-076499c8173a", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "Preliminaries"]], "content": "\\label{sec:preliminaries}\nIn this section, we provide definitions of different types of graphs mostly used in node/edge/sub-graph-level anomaly detection (Section~\\ref{sec:AND} to Section~\\ref{sec:subgraph}).\nFor consistency, we have followed the conventional categorization of graphs as in existing works~ and categorize them as static graphs, dynamic graphs, and graph databases. Unless otherwise specified, all graphs mentioned in the following sections are static. Meanwhile, as graph-level anomaly detection is discussed far away on page 13, to enhance readability, the definition for the graph database is given closer to the material in Section~\\ref{sec:anosgd:db}.\n\\textbf{\\textit{Definition 1 (Plain Graph)}}. A static plain graph $G = \\{V, E\\}$ comprises a node set $V = \\{ v_{i} \\}_{1}^{n}$ and an edge set $E = \\{e_{i,j}\\}$ where $n$ is the number of nodes and $e_{i,j} = (v_i,v_j)$ denotes an edge between nodes $v_{i}$ and $v_{j}$. The adjacency matrix $A = [a_{i,j}]_{n\\times n}$ restores the graph structure, where $a_{i,j} = 1$ if node $v_{i}$ and $v_{j}$ is connected, otherwise $a_{i,j} = 0$.\n\\textbf{\\textit{Definition 2 (Attributed Graph)}}. A static attributed graph $G = \\{V, E, X\\}$ comprises a node set $V$, an edge set $E$ and an attribute set $X$. In an attributed graph, the graph structure follows the definition in Definition 1. The attribute matrix $X = [\\mathbf{x}_{i}]_{n\\times k}$ consists of nodes' attribute vectors, where $\\mathbf{x}_{i}$ is the attribute vector associated with node $v_{i}$ and $k$ is the vector's dimension. Hereafter, the terms attribute and feature are used interchangeably.\n\\textbf{\\textit{Definition 3 (Dynamic Graph)}}. A dynamic graph $G(t) = \\{V(t), E(t), X_{v}(t), X_{e}(t) \\}$ comprises nodes and edges changing overtime. $V(t)$ is the nodes set in the graph at a specific time step $t$, $E(t)$ is the corresponding edge set, $X_{v}(t)$ and $X_{e}(t)$ are the node attribute matrix and edge attribute matrix at time step $t$ in the graph if existed.\nIn reality, the nodes or edges might also be associated with numerical or categorical labels to indicate their classes (\\eg normal or abnormal). When label information is available/partially-available, supervised/semi-supervised detection models could be effectively trained.", "cites": [3344], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily offers definitions of graph types used in anomaly detection and briefly references existing categorizations from cited works, but it lacks substantial synthesis of ideas. It does not compare or evaluate different approaches or highlight limitations, and the level of abstraction is minimal, focusing only on structural definitions rather than broader patterns or principles in the field."}}
{"id": "699af85c-49af-4ca0-a394-878a65f78e4e", "title": "Network Representation Based Techniques", "level": "subsubsection", "subsections": [], "parent_id": "5e100893-099a-4201-b53e-ba07115d4307", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "Anomalous node detection (ANOS ND)"], ["subsection", "ANOS ND on Plain Graphs"], ["subsubsection", "Network Representation Based Techniques"]], "content": "\\label{sec:spg:nr}\nTo capture more valuable information from the graph structure for anomaly detection, network representation techniques have been widely exploited. \nTypically, these techniques encode the graph structure into an embedded vector space and identify anomalous nodes through further analysis. Hu \\etal~, for example, proposed an effective embedding method to detect structural anomalies that are connecting with many communities. \nIt first adopts a graph partitioning algorithm (\\eg METIS~) to group nodes into $d$ communities ($d$ is a user-specified number). \nThen, the method employs a specially designed embedding procedure to learn node embeddings that could capture the link information between each node and $d$ communities.\nDenoting the embedding for node $i$ as $Z_i = \\{z_{i}^{1},\\cdots,z_{i}^{d}\\}$, the procedure initializes each $z_{i}^{c} \\in Z_i$ with regard to the membership of node $i$ to community $c$ (if node $i$ belongs to the community, then $z_{i}^{c} = \\frac{1}{\\sqrt{2}}$; otherwise, 0.) and optimizes node embeddings such that directly linked nodes have similar embeddings and unconnected nodes are dissimilar.\nAfter generating the node embeddings, the link information between node $i$ and $d$ communities is quantified for further anomaly detection analysis.\nFor a given node $i$, such information is represented as:\n\\begin{equation} \\label{eq:hu:nbi}\n  \\overline{NB(i)} = (y_{i}^{1},...,y_{i}^{d}) = \\mathop{\\sum}\\limits_{j\\in NB(i)}(1- \\|Z_{i} - Z_{j}\\|)\\cdot Z_{j},\n\\end{equation}\nwhere $NB(i)$ comprises node $i$'s neighbors.\nIf $i$ has many links with community $c$, then the value in the corresponding dimension $y_{i}^{c}$ will be large.\nIn the last step, Hu \\etal~ formulate a scoring function to assign anomalousness scores, calculated as:\n\\begin{equation} \\label{eq:hu:as}\n  AScore(i) = \\mathop{\\sum}\\limits_{k=1}^{d}\\frac{y_{i}^{k}}{y_{i}^{*}}, y_{i}^{*}=\\max\\{y_{i}^{1},...,y_{i}^{d}\\}.\n\\end{equation}\nAs expected, structural anomalies receive higher scores as they connect to different communities.\nIndeed, given a predefined threshold, nodes with above-threshold scores are identified as anomalies.\nTo date, many plain network representation methods such as Deepwalk~, Node2Vec~ and LINE~ have shown their effectiveness in generating node representations and been used for anomaly detection performance validation~.\nBy pairing the conventional anomaly detection techniques such as density-based techniques~ and distance-based techniques~ with node embedding techniques, anomalous nodes can be identified with regard to their distinguishable locations (\\ie low-density areas or far away from the majorities) in the embedding space.", "cites": [282, 6281, 218], "cite_extract_rate": 0.2727272727272727, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of network representation-based techniques for anomalous node detection, with a focus on the method by Hu et al. It integrates the concept of embedding with community-based analysis but lacks deeper synthesis across all cited papers. There is minimal critical evaluation or identification of limitations in the discussed methods. The section offers some general ideas, such as pairing embedding with traditional anomaly detection techniques, but does not elevate to a meta-level abstraction or trend analysis."}}
{"id": "bc2ebce7-d1c1-4f0b-881b-6a0ace25800e", "title": "Reinforcement Learning Based Techniques", "level": "subsubsection", "subsections": [], "parent_id": "5e100893-099a-4201-b53e-ba07115d4307", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "Anomalous node detection (ANOS ND)"], ["subsection", "ANOS ND on Plain Graphs"], ["subsubsection", "Reinforcement Learning Based Techniques"]], "content": "The success of reinforcement learning (RL) in tackling real-world decision making problems has attracted substantial interests from the anomaly detection community. Detecting anomalous nodes can be naturally regarded as a problem of deciding which class a node belongs to - anomalous or benign. As a special scenario of the general selective harvesting task, the anomalous node detection problem can be approached by a recent work in~ that intuitively combines reinforcement learning and network embedding techniques for selective harvesting. The proposed model, NAC, is trained with labeled data without any human intervention. Specifically, it first selects a seed network consisting of partially observed nodes and edges. Then, starting from the seed network, NAC adopts reinforcement learning to learn a node selection plan such that anomalous nodes in the undiscovered area can be identified. This is achieved by rewarding selection plans that can choose labeled anomalies with higher gains. Through offline training, NAC will learn an optimal/suboptimal anomalous node selection strategy and discover potential anomalies in the undiscovered graph step by step.", "cites": [6282], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of reinforcement learning-based techniques for anomalous node detection, focusing on a single paper (NAC). It lacks synthesis with other works and offers minimal critical evaluation or abstraction beyond the specific method discussed. The narrative is straightforward and does not contextualize the approach within broader trends or frameworks."}}
{"id": "9b59f8e2-fd33-4bf3-bf31-9d82848223fa", "title": "ANOS ND on Attributed Graphs", "level": "subsection", "subsections": ["9201dd97-4956-4c75-840c-212d3ec5e5ae", "73bf642c-6661-4550-b563-f83087e0c906", "be74c2e6-73e6-42d6-9154-063b0fce612a"], "parent_id": "59eed45d-0f6f-4983-a1d0-48c2fb48a606", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "Anomalous node detection (ANOS ND)"], ["subsection", "ANOS ND on Attributed Graphs"]], "content": "\\label{lb:node:sag}\nIn addition to the structural information, real-world networks also contain rich attribute information affiliated with nodes~. These attributes provide complementary information about real objects and together with graph structure, more hidden anomalies that are non-trivial can now be detected. \n\\begin{figure*}[!t]\n    \\setlength{\\belowcaptionskip}{-0.25cm} \n    \\centerline{\\includegraphics[width=0.98\\textwidth]{pics/Node/GCN.pdf}}\n    \\caption{ANOS ND on attributed graphs -- GCN based approaches. Node representations are generated through GCN layers. Anomalies are then detected according to their reconstruction loss (\\ding{172}) or embedding distribution in the embedding space (\\ding{173}).}\n    \\label{pic:GCNbased_framework}\n\\end{figure*}\nFor clarity, we distinguish between deep neural networks and graph neural networks in this survey. \nWe review deep neural network (Deep NN) based techniques, GCN based techniques, and reinforcement learning based techniques for ANOS ND as follows. \nDue to page limitations, other existing works including traditional non-deep learning techniques, GAT~ based techniques, GAN based techniques, and network representation based techniques are surveyed in Appendix~\\ref{appendix:node:static}.", "cites": [7007, 180, 242], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces the concept of attributed graph anomaly detection but lacks a deeper synthesis of the cited papers. It mentions different approaches (e.g., GCN-based, Deep NN, reinforcement learning) without elaborating on their relationships or contributions. There is no critical evaluation of the methods, nor any abstraction to broader principles or trends in the field."}}
{"id": "73bf642c-6661-4550-b563-f83087e0c906", "title": "GCN Based Techniques", "level": "subsubsection", "subsections": [], "parent_id": "9b59f8e2-fd33-4bf3-bf31-9d82848223fa", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "Anomalous node detection (ANOS ND)"], ["subsection", "ANOS ND on Attributed Graphs"], ["subsubsection", "GCN Based Techniques"]], "content": "\\label{sec:AND:GCN}\nGraph convolutional neural networks (GCNs)~ have accomplished decent success in many graph data mining tasks (\\eg link prediction, node classification, and recommendation) owing to its capability of capturing comprehensive information in the graph structure and node attributes.\nTherefore, many anomalous node detection techniques start to investigate GCNs. \nFig.~\\ref{pic:GCNbased_framework} illustrates a general framework of existing works in this line.\nIn~, Ding \\etal measured an anomaly score for each node using the network reconstruction errors of both the structure and attribute.\nThe proposed method, DOMINANT, comprises three parts, namely, the graph convolutional encoder, the structure reconstruction decoder, and the attribute reconstruction decoder. The graph convolutional encoder generates node embeddings through multiple graph convolutional layers. The structure reconstruction decoder tends to reconstruct the network structure from the learned node embeddings, while the attribute reconstruction decoder reconstructs the node attribute matrix.\nThe whole neural network is trained to minimize the following loss function:\n\\begin{equation} \\label{eq:ad}\n    \\begin{split}\n    \\mathcal{L}_{DOMINANT} &= (1-\\alpha)\\mathcal{R}_{S} + \\alpha \\mathcal{R}_{A}\\\\\n                &= (1-\\alpha)||A - \\hat{A}||_{F}^{2} + \\alpha||X - \\hat{X}||_{F}^{2},\n    \\end{split}\n\\end{equation}\nwhere $\\alpha$ is the coefficient, $A$ depicts the adjacency matrix of the graph, $\\mathcal{R}_{S}$ and $\\mathcal{R}_{A}$ quantify reconstruction errors with regard to the graph structure and node attributes, respectively.\nWhen the training is finished, an anomaly score is then assigned to each node according to its contribution to the total reconstruction error, which is calculated by:\n\\begin{equation} \\label{eq:nodescore}\n  \\textit{score}(i) \n      = (1-\\alpha)||\\mathbf{a}_{i} - \\mathbf{\\hat{a}}_{i}||_{2} + \\alpha||\\mathbf{x}_{i} - \\mathbf{\\hat{x}}_{i}||_{2},\n\\end{equation}\nwhere $\\mathbf{a}_{i}$ and $\\mathbf{x}_{i}$ are the structure vector and attribute vector of node $i$, $\\mathbf{\\hat{a}}_{i}$ and $\\mathbf{\\hat{x}}_{i}$ are their corresponding reconstructed vectors. \nThe nodes are then ranked according to their anomaly scores in descending order, and the top-k nodes are recognized as anomalies.\nTo enhance the performance of anomalous node detection, later work by Peng \\etal~ further explores node attributes from multiple attributed views to detect anomalies.\nThe multiple attributed views are employed to describe different perspectives of the objects~.\nFor example, in online social networks, user's demographic information and posted contents are two different attributed views, and they characterize the personal information and social activities, respectively.\nThe underlying intuition of investigating different views is that anomalies might appear to be normal in one view but abnormal in another view.\nFor the purpose of capturing these signals, the proposed method, ALARM, applies multiple GCNs to encode information in different views and adopts a weighted aggregation of them to generate node representations.\nThis model's training strategy is similar to DOMINANT~ in that it aims to minimize the network reconstruction loss and attribute reconstruction loss and can be formulated as:\n\\begin{equation} \\label{eq:alarmloss}\n\\begin{split}\n\\mathcal{L}_{ALARM} = & \\sum_{i=1}^n \\sum_{j=1}^n - [\\gamma A_{ij}\\log\\hat{A}_{ij} + (1-A_{ij})\\log(1-\\hat{A}_{ij})] \\\\\n                      & + ||X - \\tilde{X}||^{F}_{2},\n\\end{split}\n\\end{equation}\nwhere $\\gamma$ is coefficient to balance the errors, $A_{ij}$ is the element at coordinate $(i,j)$ in the adjacency matrix $A$, $\\hat{A}_{ij}$ is the corresponding element in the reconstructed adjacency matrix $\\hat{A}$, $X$ is the original node feature matrix and $\\tilde{X}$ is the reconstructed node feature matrix.\nLastly, ALARM adopts the same scoring function as~, and nodes with top-k highest scores are anomalous.\nInstead of spotting unexpected nodes using their reconstruction errors, Li \\etal~ proposed SpecAE to detect global anomalies and community anomalies via a density estimation approach, Gaussian Mixture Model (GMM).\nGlobal anomalies can be identified by only considering the node attributes. \nFor community anomalies, the structure and attributes need to be jointly considered because of their distinctive attributes to the neighbors.\nAccordingly, SpecAE investigates a graph convolutional encoder to learn node representations and reconstruct the nodal attributes through a deconvolution decoder.\nThe parameters in the GMM are then estimated using the node representations. \nDue to the deviating attribute patterns of global and community anomalies, normal nodes are expected to exhibit greater energies in GMM, and the k nodes with the lowest probabilities are deemed to be anomalies.\nIn~, Wang \\etal developed a novel detection model that identify fraudsters using their relations and features. Their proposed method, Fdgars, first models online users' reviews and visited items as their features, and then identifies a small portion of significant fraudsters based on these features. In the last step, a GCN is trained in a semi-supervised manner by using the user-user network, user features, and labeled users. After training, the model can directly label unseen users.\nA more recent work, GraphRfi~, also explores the potential of combining anomaly detection with other downstream graph analysis tasks.\nIt targets on leveraging anomaly detection to identify malicious users and provide more accurate recommendations to service benign users by alleviating the impact of these untrustworthy users. \nSpecifically, a GCN framework is deployed to encode users and items into a shared embedding space for recommendation and users are classified as fraudsters or normal users through an additional neural random forest using their embeddings. \nFor rating prediction between users and items, the framework reduces the corresponding impact of suspicious users by assigning less weights to their training loss. \nAt the same time, the rating behavior of users also provides auxiliary information for fraudster detection. \nThe mutually beneficial relationship between these two applications (anomaly detection and recommendation) indicates the potential of information sharing among multiple graph learning tasks.", "cites": [6283, 6284], "cite_extract_rate": 0.2222222222222222, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple GCN-based techniques for anomalous node detection on attributed graphs, connecting ideas from DOMINANT, ALARM, SpecAE, Fdgars, and GraphRfi to illustrate a coherent narrative around the use of reconstruction errors and multi-view analysis. It provides some abstract insights, particularly in highlighting the role of attributes and structure in detecting different anomaly types and the synergy between anomaly detection and recommendation. However, the critical analysis is limited to brief mentions of methods' goals and techniques without in-depth evaluation of their limitations or trade-offs."}}
{"id": "12f2fc41-29d6-4189-b62f-ff980f20a825", "title": "ANOS ND on Dynamic Graphs", "level": "section", "subsections": ["0af4432d-d88f-455d-b4ca-b342865e37f7", "75faec9f-6952-423b-8944-196564d6aaa7"], "parent_id": "5020eb09-4f12-4d1a-8dae-076499c8173a", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "ANOS ND on Dynamic Graphs"]], "content": "\\label{sec:node:dg}\nReal-world networks can be modeled as dynamic graphs to represent evolving objects and the relationships among them.\nIn addition to structural information and node attributes, dynamic graphs also contain rich temporal signals~, \\eg the evolving patterns of the graph structure and node attributes. \nOn the one hand, these information inherently makes anomalous node detection on dynamic graphs more challenging. \nThis is because dynamic graphs usually introduce large volume of data and temporal signals should also be captured for anomaly detection.\nBut, on the other hand, they could provide more details about anomalies~. \nIn fact, some anomalies might appear to be normal in the graph snapshot at each time stamp, and, only when the changes in a graph's structure are considered, do they become noticeable.\nIn this section, we review the network representation based techniques and GAN based techniques as follows. Relevant techniques from traditional non-deep learning approaches are reviewed in Appendix~\\ref{appendix:node:dynamic}.\n\\begin{table*}[!t]\n\\centering\n\\footnotesize\n\\renewcommand\\arraystretch{1}\n\\setlength{\\tabcolsep}{2.8mm}\n\\caption{Summary of Anomalous Node Detection Techniques.}\n\\resizebox{0.96\\textwidth}{!}{\n\\begin{tabular}{m{2.1cm}<{\\centering}|m{2.3cm}<{\\centering}|m{1.1cm}<{\\centering}|m{4cm}<{\\centering}|m{1.8cm}<{\\centering}|m{2.9cm}<{\\centering}}\n\\toprule[1 pt]\n\\textbf{Graph Type} & \\textbf{Approach} & \\textbf{Category} & \\textbf{Objective Function} & \\textbf{Measurement} & \\textbf{Outputs}  \\\\ \n\\midrule[1 pt]\n\\multirow{3}{*}{\\shortstack{Static Graph - \\\\ Plain}} & ~ & NR & $\\mathop{\\sum}\\limits_{(i,j)\\in E} \\|\\mathbf{Z_{i}} - \\mathbf{Z_{j}}\\|^{2} + \\alpha \\mathop{\\sum}\\limits_{(i,j)\\notin E}(\\|\\mathbf{Z_{i}} - \\mathbf{Z_{j}}\\| -1)^{2}$  & Anomaly Score  & $ \\mathop{\\sum}\\limits_{k=1}^{d}\\frac{y_{i}^{k}}{y_{i}^{*}}$ \\\\ \\cline{2-6}\n& DCI~ & NR &$\\frac{1}{K}\\mathop{\\sum}\\limits_{k=1}^K\\mathcal{L}_{DCI}^{k}$ & Anomaly Prediction & Predicted Label \\\\\\cline{2-6}\n&NAC~ & RL & Cumulative reward  & -  & Anomalies \\\\ \n\\midrule[1 pt]\n\\multirow{24}{*}{\\shortstack{Static Graph - \\\\ Attributed}} &ALAD~ & Non-DP & $\\min\\limits_{W,H} \\|A-WW^{T}\\|_{F}^{2} + \\alpha \\|X-WH\\|_{F}^{2} + \\gamma (\\|W\\|_{F}^{2} + \\|H\\|_{F}^{2})$  & Anomaly Score & $\\frac{\\mathbf{W}_{n,c}}{\\sum_{c}\\mathbf{W}_{n,c}} cos(\\mathbf{A}_{n*}, \\mathbf{H}_{c*})$ \\\\   \\cline{2-6}\n&Radar~ & Non-DP & $\\min\\limits_{W,R}\\|X - W^{T} X - R\\|_{F}^{2}+ \\alpha \\|W\\|_{2,1} +\\beta \\|R\\|_{2,1} +\\gamma tr(R^{T}LR)$  & Residual Analysis & Residual Value \\\\  \\cline{2-6}\n&ANOMALOUS~ & Non-DP & $\\min\\limits_{W,\\tilde{R}}||X - XWX - \\tilde{R}||_{F}^{2} + \\alpha ||W||_{2,1} + \\beta ||W^{T}||_{2,1} +  \\gamma ||\\tilde{R}^{T}||_{2,1} + \\varphi tr(\\tilde{R}L\\tilde{R}^{T})$ & Residual Analysis & Residual Value   \\\\  \\cline{2-6}\n&SGASD~ & Non-DP & $\\min\\limits_{\\mathbf{w,c}}\\frac{1}{2}\\sum_{i=1}^{m}\\mathbf{c_{i}}(V_{i,*}\\mathbf{w} - y_i)^2 + \\frac{\\lambda_{1}}{2}||\\mathbf{w}||_{2}^{2} + \\lambda_{2} \\sum_{i=0}^{d}\\sum_{j=1}^{n_{i}}||\\mathbf{c}_{G_{j}^{i}}||_{2}$  & Anomaly Prediction & Predicted Label \\\\  \\cline{2-6}\n&DONE~ & DNN & $\\alpha_{1}\\mathcal{L}_{str}^{Recs} + \\alpha_{2}\\mathcal{L}_{attr}^{Recs} + \\alpha_{3}\\mathcal{L}_{str}^{Hom} + \\alpha_{4}\\mathcal{L}_{attr}^{Hom} + \\alpha_{5}\\mathcal{L}^{Com}$  & Anomaly Scores & $o_{i}^{s},o_{i}^{a},o_{i}^{com}$   \\\\  \\cline{2-6}\n&DOMINANT~ & GCN & $(1-\\alpha)\\mathcal{R}_{S} + \\alpha \\mathcal{R}_{A}$  & Anomaly Score & $(1-\\alpha)||\\mathbf{a}_{i} - \\mathbf{\\hat{a}}_{i}||_{2} + \\alpha||\\mathbf{x}_{i} - \\mathbf{\\hat{x}}_{i}||_{2}$ \\\\  \\cline{2-6}\n&ALARM~ & GCN & $\\sum_{i=1}^n \\sum_{j=1}^n - [\\gamma A_{ij}\\log\\hat{A}_{ij} + (1-A_{ij})\\log(1-\\hat{A}_{ij})] + \\mathcal{L}_a$ & Anomaly Score & $(1-\\alpha)||\\mathbf{a}_{i} - \\mathbf{\\hat{a}}_{i}||^2_{2} + \\alpha||\\mathbf{x}_{i} - \\mathbf{\\hat{x}}_{i}||^2_{2}$ \\\\  \\cline{2-6}\n&SpecAE~ & GCN & $\\mathbb{E}[dis(X,\\hat{X})] + \\mathbb{E}[dis(X,\\tilde{X})] + \\lambda_{1}\\mathbb{E}(E(Z)) + \\lambda_{2}KL$ & Density Estimation & Anomalousness Rank  \\\\  \\cline{2-6}\n&Fdgars~ & GCN & $\\mathcal{L}_{GCN}$ & Anomaly Prediction & Predicted Label  \\\\  \\cline{2-6}\n&GraphRfi~ & GCN & $\\mathcal{L}_{rating} + \\lambda \\mathcal{L}_{fraudster}$ & Anomaly Prediction & Predicted Label  \\\\  \\cline{2-6}\n&ResGCN~ & GCN & $(1-\\alpha)||A - \\hat{A}||^2_{F} + \\alpha||X - \\hat{X} - \\lambda R||^2_{F}$ & Anomaly Score & $||R_{i,:}||_{2}$  \\\\  \\cline{2-6}\n&GraphUCB~ & RL & Expert Judgment  & - & Anomalies \\\\  \\cline{2-6}\n&AnomalyDAE~ & GAT & $\\alpha ||(A - \\hat{A})\\odot \\bm{\\theta}||_{F}^{2} + (1-\\alpha)||(X - \\hat{X})\\odot \\bm{\\eta}||_{F}^{2}$ & Reconstruction Loss & Anomalousness Rank \\\\  \\cline{2-6}\n&SemiGNN~ & GAT & $\\alpha \\mathcal{L}_{sup} + (1- \\alpha) \\mathcal{L}_{unsup} + \\lambda \\mathcal{L}_{reg}$ & Anomaly Prediction & Predicted Label \\\\  \\cline{2-6}\n&AEGIS~ & GAN & $\\mathcal{L}_{AE} + \\mathcal{L}_{GAN}$ & Anomaly Score & $1 - D(\\mathbf{z}_i)$ \\\\  \\cline{2-6}\n&REMAD~ & NR & $\\mathcal{L}_{res} + \\beta\\| R^{T}\\|_{2,1}$  & Residual Analysis & Residual Value \\\\  \\cline{2-6}\n&CARE-GNN~ & NR & $\\mathcal{L}_{GNN} + \\lambda_{1}\\mathcal{L}_{Simi}^{(1)} + \\lambda_{2}\\mathcal{L}_{reg}$  & Anomaly Prediction & Predicted Label \\\\  \\cline{2-6}\n&SEANO~ & NR & $-\\sum_{i \\in V_{L}} \\log p(y_{i}|\\mathbf{x}_{i},\\Bar{\\mathbf{x}}_{N_{i}}) - \\sum_{i \\in V}\\sum_{v\\prime \\in C_i} \\log p(v\\prime|\\mathbf{x}_{i},\\Bar{\\mathbf{x}}_{N_{i}})$  & Anomaly Score & Discriminator's Output \\\\ \\cline{2-6}\n&OCGNN~ & NR & $\\frac{1}{\\beta K}\\sum\\limits_{v_{i}\\in\\mathbf{V}_{tr}}[||g(X,A;\\mathcal{W})_{v_i}-c||^2 -r^2]^{+}+ r^2 +\\frac{\\lambda}{2}\\sum\\limits_{l=1}^{L}||W^{(l)}||^2$  & Location in Embedding Space & Distance to Hypersphere Center \\\\ \\cline{2-6}\n&GAL~ & NR &  $\\max\\{0,\\max\\limits_{y_{v^{\\prime}} \\neq y_{u}}g(u,v^\\prime) - \\min\\limits_{y_{v} = y_{u}}g(u,v) + \\Delta_{y_u}\\}$  & Anomaly Prediction & Predicted Label \\\\ \\cline{2-6}\n&CoLA~& NR & $-\\sum\\limits_{i=1}^{N}y_{i}\\log(CLM(v_i,\\mathcal{G}_{i})) + (1-y_i)\\log(1-CLM(v_i,\\mathcal{G}_{i}))$  & Anomaly Score & $\\frac{\\sum\\limits_{r=1}^{R}(s_{i,r}^{(-)}-s_{i,r}^{(+)})}{R}$ \\\\ \\cline{2-6}\n&COMMANDER~ & NR & $-\\mathcal{L}_{D} + \\mathcal{L}_{C} + \\mathcal{L}_{R}$  & Anomaly Score & $\\bar{y_i}||\\mathbf{\\tilde{x}_i} - \\mathbf{x}_i||_2^2$ \\\\ \\cline{2-6}\n&FRAUDRE~& NR & $\\sum\\limits_{i=1}^{n}f^{*}(y_i,\\mathbf{h}_{i}^{(final)}\\mathbf{W}_2)$  & Anomaly Prediction & Predicted Label \\\\ \\cline{2-6}\n&Meta-GDN~ & NR & $(1-y_i)\\cdot|dev(v_i)| + y_i\\cdot\\max(0,dev(v_i))$  & Anomaly Score & $\\mathbf{u}_{s}^{T}\\mathbf{o}_{i} + b_s$ \\\\ \n\\midrule[1 pt]\nDynamic Graph - Plain & NetWalk~ & DNN & $\\gamma\\mathcal{L}_{AE} + \\mathcal{L}_{Clique} + \\lambda\\|W\\|_{F}^{2} + \\beta KL$ & Anomaly Score & Nearest Distance to Cluster Centers \\\\ [2ex]\n\\midrule[1 pt]\n\\multirow{2}{*}{\\shortstack{Dynamic Graph - \\\\ Attributed}} &MTHL~ & Non-DP & $\\mathop{min}_{\\mathcal{P}}f(\\mathcal{P})$  & Anomaly Score & Distance to Hypersphere Centroid \\\\  \\cline{2-6}\n&OCAN~ & GAN & $\\mathcal{L}_{LSTM-AE} + \\mathcal{L}_{GAN}$  & Anomaly Score & Discriminator's Output  \\\\\n\\bottomrule[1 pt]\n\\multicolumn{6}{l}{* Non-DP: Non-Deep Learning Techniques, DNN: Deep NN Based Techniques, GCN: GCN Based Techniques, RL: Reinforcement Learning Based Techniques.} \\\\\n\\multicolumn{6}{l}{* GAT: GAT Based Techniques, NR: Network Representation Based Techniques, GAN: Generative Adversarial Network Based Techniques.}\n\\end{tabular}\n}\n\\label{table:ANOSND}\n\\end{table*}\n\\begin{figure*}[!t]\n\\setlength{\\belowcaptionskip}{-0.25cm} \n\\centerline{\\includegraphics[width=0.98\\textwidth]{pics/Edge/AddGraph.pdf}}\n\\caption{ANOS ED on dynamic graphs -- GCN based approaches. GCN is employed to learn node embeddings from the temporal graph at each timestamp. The attention-based GRU generates the current hidden state using the node embeddings and previous hidden states. The edge scoring function, such as a FCN, is learned to assign anomaly scores, and the top-k edges are depicted as anomalies.}\n\\label{pic:Dynamic_Edge_Comparison}\n\\end{figure*}", "cites": [6285, 6290, 6288, 6287, 6283, 6289, 6282, 3344, 6271, 6284, 6291, 8991, 6286], "cite_extract_rate": 0.38235294117647056, "origin_cites_number": 34, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes various deep learning techniques for anomalous node detection on dynamic graphs, listing each method with its objective function, measurement, and output. There is minimal synthesis of ideas across papers or critical evaluation of their strengths and weaknesses. The content remains largely at a concrete level, focusing on categorization and technical details rather than abstract patterns or overarching principles."}}
{"id": "75faec9f-6952-423b-8944-196564d6aaa7", "title": "GAN Based Techniques", "level": "subsection", "subsections": [], "parent_id": "12f2fc41-29d6-4189-b62f-ff980f20a825", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "ANOS ND on Dynamic Graphs"], ["subsection", "GAN Based Techniques"]], "content": "\\label{sec:ND:GAN}\nIn practice, anomaly detection is facing great challenges from the shortage of ground-truth anomalies.\nConsequently, many research efforts have been invested in modeling the features of anomalies or regular objects such that anomalies can be identified effectively.\nAmong these techniques, generative adversarial networks (GAN)~ have received extensive attention because of its impressive performance in capturing real data distribution and generating simulated data.\nMotivated by the recent advances in ``bad\" GAN~, Zheng \\etal~ circumvented the fraudster detection problem using only the observed benign users' attributes.\nThe basic idea is to seize the normal activity patterns and detect anomalies that behave significantly differently.\nThe proposed method, OCAN, starts by extracting the benign users' content features using their historical social behaviors (\\eg historical posts, posts' URL), for which this method is classified into the dynamic category.\nA long short-term memory (LSTM) based autoencoder~ is employed to achieve this and as assumed, benign users and malicious users are in separate regions in the feature space.\nNext, a novel one-class adversarial net comprising a generator and a discriminator is trained.\nSpecifically, the generator produces complementary data points that locate in the relatively low density areas of benign users.\nThe discriminator, accordingly, aims to distinguish the generated samples from the benign users.  \nAfter training, benign users' regions are learned by the discriminator and anomalies can hence be identified with regard to their locations.\nBoth NetWalk~ and OCAN~ approach the anomalous node detection problem promisingly, however, they respectively only consider the structure or attributes.\nBy the success of static graph anomaly detection techniques that analyze both aspects, when the structure and attribute information in dynamic graphs are jointly considered, an enhanced detection performance can be foreseen.\nWe therefore highlight this unexplored area for future works in Section~\\ref{sec:futures}.", "cites": [2626, 6285, 6292], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates GAN-based techniques for graph anomaly detection by connecting the use of LSTM autoencoders (Paper 1) and one-class adversarial nets (Paper 2), while also referencing theoretical insights from Paper 3 on the limitations of GANs. It provides a coherent narrative and identifies a gap in existing approaches by pointing out the lack of joint consideration of structure and attributes in dynamic graph anomaly detection. However, it stops short of offering deeper critical evaluation or a novel conceptual framework."}}
{"id": "678bd7af-5bf2-44f6-ae32-df413829eb80", "title": "Anomalous edge detection (ANOS ED) ", "level": "section", "subsections": ["509789f2-b5ef-4288-92b5-0d64443ed4af", "e519d916-fe62-4029-8d13-0a7fc3954141", "231d1246-99ce-4de5-9270-25f30b2ceb5d"], "parent_id": "5020eb09-4f12-4d1a-8dae-076499c8173a", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "Anomalous edge detection (ANOS ED) "]], "content": "\\label{sec:edge:static}\nIn contrast to anomalous node detection, which targets individual nodes, ANOS ED aims to identify abnormal links.\nThese links often inform the unexpected or unusual relationships between real objects~, such as the abnormal interactions between fraudsters and benign users shown in Fig.~\\ref{Toy}, or suspicious interactions between attacker nodes and benign user machines in computer networks. Following the previous taxonomy, in this section, we review the state-of-the-art ANOS ED methods for static graphs, and Section~\\ref{sec:edge:dynamic} summarizes the techniques for dynamic graphs. A summary is provided in Table~\\ref{tb:edgeandsubgraph}. This section includes methods based on deep NNs, GCNs and network representations. The non-deep learning techniques are reviewed in Appendix~\\ref{appendix:edge}.", "cites": [6293], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section introduces the concept of anomalous edge detection and briefly categorizes the methods used, but it lacks synthesis of the cited papers into a coherent narrative. It only mentions one paper (F-FADE) in passing without integrating its contributions or contrasting it with other methods. There is minimal critical analysis or abstraction beyond individual examples."}}
{"id": "9ba15010-a3db-4588-8e7f-56a7def289f5", "title": "Anomalous graph detection (ANOS GD)", "level": "section", "subsections": ["49385046-24ac-4a52-8807-655e25c7942e", "c5bcea84-0572-429e-a5e7-37efbed81759"], "parent_id": "5020eb09-4f12-4d1a-8dae-076499c8173a", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "Anomalous graph detection (ANOS GD)"]], "content": "\\label{sec:anosgd:db}\nBeyond anomalous node, edge, and sub-graph, graph anomalies might also appear as abnormal graphs in a set/database of graphs.\nTypically, a graph database is defined as:\n\\textbf{\\textit{Definition 4 (Graph Database)}}. A graph database $\\mathcal{G} = \\{ G_i = (V_i,E_i,X_v(i), X_e(i)) \\}^N_{i=1}$ contains $N$ individual graphs. Here, each graph $G_i$ is comprised of a node set $V_i$ and an edge set $E_i$. $X_v(i)$ and $X_e(i)$ are the node attribute matrix and edge attribute matrix of $G_i$ if it is an attributed graph.\nThis graph-level ANOS GD aims to detect individual graphs that deviate significantly from the others.\nA concrete example of ANOS GD is unusual molecule detection.\nWhen chemical compounds are represented as molecular/chemical graphs where the atoms and bonds are represented as nodes and edges~, unusual molecules can be identified because their corresponding graphs have structures and/or features that deviate from the others.\nBrain disorders detection is another example.\nA brain disorder can be diagnosed by analyzing the dynamics of brain graphs at different stages of aging in sequence and finding an inconsistent snapshot at a specific time stamp.\nThe prior reviewed techniques (\\ie ANOS ND/ED/SGD) are not compatible with ANOS GD because they are dedicated to detecting anomalies in a single graph, whereas ANOS GD is directed at detecting graph-level anomalies.\nThis problem is commonly approached by: 1) measuring the pairwise proximities of graphs using graph kernels~; 2) detecting the appearance of anomalous graph signals created by abnormal groups of nodes~; or 3) encoding graphs using frequent motifs~.\nHowever, none of these methods are deep learning-based.\nAs the time of writing, very few studies in ANOS GD with deep learning have been undertaken. As such, this is highlighted as a potential future direction in Section~\\ref{sec:future:ED}.", "cites": [8992], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section introduces the concept of graph-level anomaly detection but primarily serves as a descriptive overview. It provides a definition and examples of ANOS GD and lists general approaches (graph kernels, graph signals, motifs), but does not synthesize these ideas in depth or compare them. The only cited paper is mentioned briefly without analysis, and the section lacks a critical or abstract perspective on the field's state or potential."}}
{"id": "49385046-24ac-4a52-8807-655e25c7942e", "title": "GNN Based Techniques", "level": "subsection", "subsections": [], "parent_id": "9ba15010-a3db-4588-8e7f-56a7def289f5", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "Anomalous graph detection (ANOS GD)"], ["subsection", "GNN Based Techniques"]], "content": "Motivated by the success of GNNs in various graph classification tasks, the most recent works in ANOS GD employ GNNs to classify single graphs as normal/abnormal in the given graph database. Specifically, Dou \\etal~ transformed fake news detection into an ANOS GD problem by modeling news as tree-structured propagation graphs where the root nodes denote pieces of news, and child nodes denote users who interact with the root news. Their end-to-end framework, UPFD, extracts two embeddings for the news piece and users, respectively, via a text embedding model (e.g. word2vec, BERT) and a user engagement embedding process. For each news graph, its latent representation is a flattened concatenation of these two embeddings, which is input to train a neural classifier with the label of the news. \nCorresponding propagation graphs that are labeled as fake by the trained model are regarded as anomalous.\nAnother representative work by Zhao and Akoglu~ employed a GIN model and one-class classification (\\ie DeepSVDD~) loss to train a graph-level anomaly detection framework in an end-to-end manner.\nFor each individual graph in the graph database, its graph-level embedding is generated by applying mean-pooling over its nodes' node-level embeddings.\nA graph is eventually depicted as anomalous if it lies outside the learned hypersphere, as shown in Fig.~\\ref{pic:graph_level}.", "cites": [8149], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual summary of two GNN-based techniques for anomalous graph detection, focusing on their methodologies and architectures. It integrates basic information from the cited papers but does not establish deeper connections between the approaches. There is limited critical evaluation or abstraction to broader trends, keeping the insight level low."}}
{"id": "c5bcea84-0572-429e-a5e7-37efbed81759", "title": "Network Representation Based Techniques", "level": "subsection", "subsections": [], "parent_id": "9ba15010-a3db-4588-8e7f-56a7def289f5", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "Anomalous graph detection (ANOS GD)"], ["subsection", "Network Representation Based Techniques"]], "content": "It is also possible to apply general graph-level network representation techniques to ANOS GD.\nWith these methods, the detection problem is transformed into a conventional outlier detection problem in the embedding space.\nIn contrast to D(G)NN based techniques that can detect graph anomalies in an end-to-end manner, adopting these representation techniques for anomaly detection is two-staged.\nFirst, graphs in the database are encoded into a shared latent space using graph-level representation techniques, such as Graph2Vec~, FGSD~.\nThen, the anomalousness of each single graph is measured by an off-the-shelf outlier detector.\nEssentially, this kind of approach involves pairing existing methods in both stages, yet, the stages are disconnected from each other and, hence, the detection performance can be subpar since the embedding similarities are not necessarily designed for the sake of anomaly detection.", "cites": [3953], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from cited papers to frame network representation-based techniques as a two-stage process for graph anomaly detection. It critically notes that these methods are disconnected and may lead to subpar performance, though it stops short of a deep evaluation or identifying specific trends. It provides some abstraction by distinguishing between embedding techniques and outlier detection, but not at a meta-level that would unify broader principles."}}
{"id": "ae3bbf88-f1cb-408b-8ded-ef8445178dfa", "title": "ANOS GD on Dynamic Graphs", "level": "section", "subsections": [], "parent_id": "5020eb09-4f12-4d1a-8dae-076499c8173a", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "ANOS GD on Dynamic Graphs"]], "content": "\\label{sec:anosgd:dynamic}\nFor dynamic graph environments, graph-level anomaly detection endeavors to identify abnormal graph snapshots/temporal graphs. Similar to ANOS ND and ED on dynamic graphs, given a sequence of graphs, anomalous graphs can be distinguished regarding their unusual evolving patterns, abnormal graph-level features, or other characteristics.\n\\begin{table*}[!h]\n\\centering \n\\footnotesize\n\\renewcommand\\arraystretch{1}\n\\setlength{\\tabcolsep}{2.8mm}\n\\caption{Summary of Anomalous Edge, Sub-graph and Graph Detection Techniques.} \n\\resizebox{0.96\\textwidth}{!}{\n\\begin{tabular}{m{2.1cm}<{\\centering}|m{2.3cm}<{\\centering}|m{1.1cm}<{\\centering}|m{4cm}<{\\centering}|m{1.8cm}<{\\centering}|m{2.9cm}<{\\centering}}\n\\toprule[1 pt]\n\\textbf{Graph Type} & \\textbf{Approach} & \\textbf{Category} & \\textbf{Objective Function} & \\textbf{Measurement} & \\textbf{Outputs}  \\\\ \n\\midrule[1 pt]\n\\multicolumn{6}{c}{Anomalous Edge Detection Techniques} \\\\\n\\midrule[1 pt]\n\\multirow{2}{*}{\\shortstack{Static Graph - \\\\ Plain}} &UGED~ & DNN & $\\text{cross-entropy}(f(u,N(u)), v)$  & Anomaly Score  & $\\text{mean}(1-P(v|u,N(u)), 1-P(u|v,N(v)))$ \\\\  \\cline{2-6}\n&AANE~ & GCN & $\\mathcal{L} = \\mathcal{L}_{afl} + \\gamma\\mathcal{L}_{aal}$  & Anomaly Ranking  & Edge Existing Probability \\\\ \n\\midrule[1 pt]\nStatic Graph - Attributed &eFraudCom~ & NR & $\\mathcal{L}_{MAIN} - \\lambda\\mathcal{L}_{MI}$ & Anomaly Prediction & Predicted Label \\\\ \n\\midrule[1 pt]\nDynamic Graph - Plain & NetWalk~ & NR & $\\gamma\\mathcal{L}_{AE} + \\mathcal{L}_{Clique} + \\lambda\\|W\\|_{F}^{2} + \\beta KL$ & Anomaly Score & Nearest Distance to Cluster Centers \\\\\n\\midrule[1 pt]\nDynamic Graph - Attributed & AddGraph~ & GCN & $\\text{min} \\sum_{{e\\in \\varepsilon^{t}}}\\sum_{{e^{\\prime}\\notin \\varepsilon^{t}}} \\text{max}\\{0, \\gamma + f(i,j,w) - f(i^{\\prime},j^{\\prime},w)\\} + \\lambda\\mathcal{L}_{reg}$  & Anomaly Score & $f(i,j,w) = w\\cdot\\sigma(\\beta\\cdot (||\\mathbf{a} \\odot \\mathbf{h}_{i} + \\mathbf{b} \\odot \\mathbf{h}_{j}||^{2}_{2} - \\mu))$ \\\\ \n\\midrule[1 pt]\n\\midrule[1 pt]\n\\multicolumn{6}{c}{Anomalous Sub-graph Detection Techniques} \\\\\n\\midrule[1 pt]\n\\multirow{2}{*}{\\shortstack{Static Graph - \\\\ Plain}} &DeepFD~ & NR & $\\mathcal{L}_{recon} +\\alpha \\mathcal{L}_{sim} + \\gamma \\mathcal{L}_{reg}$& Density-based Method (DBSCAN)  & Dense sub-graphs \\\\   \\cline{2-6}\n&FraudNE~ & NR & $\\mathcal{L}_{res}^{source} + \\mathcal{L}_{res}^{sink} + \\alpha \\mathcal{L}_{share} + \\eta \\mathcal{L}_{reg}$  & Density-based Method (DBSCAN)  & Dense sub-graphs \\\\\n\\midrule[1 pt]\n\\midrule[1 pt]\n\\multicolumn{6}{c}{Anomalous Graph Detection Techniques} \\\\\n\\midrule[1 pt]\n\\multirow{2}{*}{\\shortstack{Graph Database - \\\\ Attributed}}&UPFD~ & NR & $ -(y\\log(p) + (1-y)\\log(1-p))$ & Anomaly Prediction & Predicted Label \\\\ \\cline{2-6}\n& OCGIN~ & GNN & $ \\min\\limits_{W}\\frac{1}{N}\\sum\\limits_{i=1}^{N}||GIN(G_i,W)-c||^2 + \\frac{\\lambda}{2}\\sum\\limits_{l=1}^{L}||W^l||^2_{F}$ & Location in Embedding Space & Distance to Hypersphere Center \\\\\n\\midrule[1 pt]\n\\multirow{2}{*}{\\shortstack{Dynamic Graph - \\\\ Plain}} & DeepSphere~ & DNN & $\\mathcal{L} = \\mathcal{L}_{h} + \\lambda\\mathcal{L}_{res}$ & Location in Embedding Space  & Anomalous Label  \\\\ \\cline{2-6}\n& GLAD-PAW~ & GNN & $\\text{cross-entropy}(\\mathbf{y},\\mathbf{\\hat{y}})$ & Anomaly Prediction  & Predicted Label  \\\\ \n\\bottomrule[1 pt]\n\\multicolumn{6}{l}{* DNN: Deep NN Based techniques, GCN: GCN Based Techniques, NR: Network Representation Based Techniques.}\\\\\n\\multicolumn{6}{l}{* GNN: Graph Neural Network Based Techniques.}\n\\end{tabular}\n\\label{tb:edgeandsubgraph}\n}\n\\end{table*}\nIn order to derive each graph snapshot/temporal graphâ€™s characteristics, the commonly used GNN, LSTM and autoencoder are feasible to apply. For instance, Teng \\etal~ applied a LSTM-autoencoder to detect abnormal graph snapshots, as shown in Fig.~\\ref{pic:GDdynamic}.\nIn their proposed model, DeepSphere, a dynamic graph is described as a collection of three-order tensors, $\\{\\mathcal{X}_k, k=1,2...\\}$ where each $\\mathcal{X} \\in \\mathcal{R}^{N \\times N \\times T}$, and the slices along the time dimension are the adjacency matrices of graph snapshots.\nTo identify abnormal tensors, DeepSphere first embeds each graph snapshot into a latent space using an LSTM autoencoder, and then leverages a one-class classification objective~ that learns a hypersphere such that normal snapshots are covered, and anomalous snapshots lay outside.\nThe LSTM autoencoder takes the adjacency matrices as input sequentially and attempts to reconstruct these input matrices through training.\nThe hypersphere is learned through a single neural network layer and its objective function is formulated as:\n\\begin{equation} \\label{eq:ds:hsp}\n    \\mathcal{L}_{h} = r^{2} + \\gamma\\sum_{k=1}^{m} \\epsilon_{k} + \\frac{1}{m}\\sum_{k=1}^{m} \\|\\textbf{z}_{k} - \\textbf{a}\\|^{2},\n\\end{equation}\nwhere $\\textbf{z}_{k}$ is the latent representation generated by the LSTM autoencoder, $\\textbf{a}$ is the centroid of the hypersphere, $r$ is the radius, $\\epsilon_{k}$ is the outlier penalty ($\\epsilon_{k} = \\|\\textbf{z}_{k} - \\textbf{a}\\|^{2} - r^2$), $m$ is the number of training graph snapshots, and $\\gamma$ is a hyperparameter.  \nThe overall objective function of DeepSphere is represented as:\n\\begin{equation} \\label{eq:ds:obj}\n    \\mathcal{L} = \\mathcal{L}_{h} + \\lambda\\mathcal{L}_{res},\n\\end{equation}\nwhere $\\mathcal{L}_{res}$ is the reconstruction loss of the LSTM autoencoder.\nWhen the training is finished, DeepSphere spots a given unseen data $\\mathcal{X}$ as anomalous if its embedding lies outside the learned hypersphere with a radius of $r$.\nIn addition to all ANOS ND, ED, SGD, and GD techniques reviewed above, it is worth mentioning that perturbed graphs, which adversarial models generate to attack graph classification algorithms or GNNs~, can also be regarded as (intensional) anomalies. In a perturbed graph, the nodes and edges are modified deliberately to deviate from the others. We have not reviewed these in this survey because their main purpose is to attack a GNN model. The key idea behind these methods is the attacking/perturbation strategy, and studies in this sphere seldom focus on a detection or reasoning module to identify the perturbed graph or its sub-structures, \\ie anomalous nodes, edges, sub-graphs, or graphs.\n\\begin{table*}[!t]\n\\centering \n\\footnotesize\n\\renewcommand\\arraystretch{1}\n\\setlength{\\tabcolsep}{2.8mm}\n\\caption{Published Algorithms and Models} \n\\resizebox{0.96\\textwidth}{!}{\n\\begin{tabular}{c|c|c|c|l}\n\\toprule[1 pt]\n\\textbf{Model} & \\textbf{Language} & \\textbf{Platform} &  \\textbf{Graph} & \\textbf{Code Repository} \\\\ \n\\midrule[1 pt]\nAnomalyDAE~  & Python & Tensorflow   & Static Attributed Graph & https://github.com/haoyfan/AnomalyDAE   \\\\ \\hline\nMADAN~    & Python & -   & Static Attributed Graph & https://github.com/leoguti85/MADAN  \\\\ \\hline\nPAICAN~  & Python & Tensorflow  & Static Attributed Graph & http://www.kdd.in.tum.de/PAICAN/   \\\\ \\hline\nONE~ & Python & -  & Static Attributed Graph & https://github.com/sambaranban/ONE   \\\\ \\hline\nDONE\\&AdONE~  & Python & Tensorflow    & Static Attributed Graph & https://bit.ly/35A2xHs \\\\ \\hline\nSLICENDICE~  & Python & -  & Static Attributed Graph & http://github.com/hamedn/SliceNDice/ \\\\ \\hline\nFRAUDRE~  & Python & Pytorch  & Static Attributed Graph & https://github.com/FraudDetection/FRAUDRE \\\\ \\hline\nSemiGNN~  & Python & Tensorflow  & Static Attributed Graph & https://github.com/safe-graph/DGFraud \\\\ \\hline\nCARE-GNN~  & Python & Pytorch & Static Attributed Graph & https://github.com/YingtongDou/CARE-GNN \\\\ \\hline\nGraphConsis~  & Python & Tensorflow  & Static Attributed Graph & https://github.com/safe-graph/DGFraud \\\\ \\hline\nGLOD~  & Python & Pytorch  & Static Attributed Graph & https://github.com/LingxiaoShawn/GLOD-Issues \\\\ \\hline\nOCAN~ & Python & Tensorflow  & Static Graph &https://github.com/PanpanZheng/OCAN \\\\ \\hline\nDeFrauder~ & Python & -  & Static Graph & https://github.com/LCS2-IIITD/DeFrauder \\\\ \\hline\nGCAN~  & Python & Keras & Heterogeneous Graph & https://github.com/l852888/GCAN \\\\ \\hline\nHGATRD~ & Python & Pytorch   & Heterogeneous Graph & https://github.com/201518018629031/HGATRD  \\\\\\hline\nGLAN~  & Python & Pytorch  & Heterogeneous Graph & https://github.com/chunyuanY/RumorDetection \\\\ \\hline\nGEM~ & Python & -  & Heterogeneous Graph & https://github.com/safe-graph/DGFraud/tree/master/algorithms/GEM \\\\ \\hline\neFraudCom~ & Python & Pytorch & Heterogeneous Graph & https://github.com/GeZhangMQ/eFraudCom \\\\ \\hline\nDeepFD~ & Python & Pytorch  & Bipartite Graph & https://github.com/JiaWu-Repository/DeepFD-pyTorch \\\\ \\hline\nANOMRANK~ & C++ & - & Dynamic Graph & https://github.com/minjiyoon/anomrank \\\\ \\hline\nMIDAS~ & C++ & - & Dynamic Graph & https://github.com/Stream-AD/MIDAS \\\\ \\hline\nSedanspot~    & C++ &  -   & Dynamic Graph & https://www.github.com/dhivyaeswaran/sedanspot \\\\ \\hline\nF-FADE~ & Python & Pytorch  & Dynamic Graph & http://snap.stanford.edu/f-fade/ \\\\ \\hline\nDeepSphere~  & Python & Tensorflow   & Dynamic Graph & https://github.com/picsolab/DeepSphere   \\\\ \\hline\nChangedar~  & Matlab & -   & Dynamic Graph & https://bhooi.github.io/changedar/  \\\\\\hline\nUPFD~ & Python & Pytorch  & Graph Database & https://github.com/safe-graph/GNN-FakeNews \\\\ \\hline\nOCGIN~ & Python & Pytorch  & Graph Database & https://github.com/LingxiaoShawn/GLOD-Issues \\\\ \\hline\nDAGMM~ & Python & Pytorch  &  Non Graph & https://github.com/danieltan07/dagmm \\\\ \\hline\nDevNet~ & Python & Tensorflow  & Non Graph &https://github.com/GuansongPang/deviation-network \\\\ \\hline\nRDA~ & Python & Tensorflow & Non Graph &https://github.com/zc8340311/RobustAutoencoder \\\\ \\hline\nGAD~ & Python & Tensorflow  & Non Graph &https://github.com/raghavchalapathy/gad \\\\ \\hline\nDeep SAD~ &Python & Pytorch  & Non Graph & https://github.com/lukasruff/Deep-SAD-PyTorch \\\\ \\hline\nDATE~ &Python & Pytorch  &Non Graph & https://github.com/Roytsai27/Dual-Attentive-Tree-aware-Embedding \\\\ \\hline\nSTS-NN~ & Python & Pytorch  & Non Graph & https://github.com/JiaWu-Repository/STS-NN \\\\\n\\bottomrule[1 pt]\n\\multicolumn{5}{l}{* -: No Dedicated Platforms.}\n\\end{tabular}\n\\label{tb:publishedalgs}\n}\n\\end{table*}\n\\begin{table*}[!t]\n\\centering \n\\footnotesize\n\\renewcommand\\arraystretch{1}\n\\setlength{\\tabcolsep}{2.8mm}\n\\caption{Published Datasets.}\n\\resizebox{0.96\\textwidth}{!}{\n\\begin{tabular}{m{1.6cm}<{\\centering}|m{1.4cm}<{\\centering}|m{0.5cm}<{\\centering}|m{0.5cm}<{\\centering}|m{0.5cm}<{\\centering}|m{0.5cm}<{\\centering}|m{0.5cm}<{\\centering}|m{2cm}<{\\centering}|l}\n\\toprule[1 pt]\n\\textbf{Category} & \\textbf{Dataset} &  \\textbf{\\#G} & \\textbf{\\#N} & \\textbf{\\#E} & \\textbf{\\#FT} & \\textbf{\\#AN} & \\textbf{REF} & \\textbf{URL} \\\\ \n\\midrule[1 pt]\n\\multirow{5}{*}{\\shortstack{Citation \\\\ Networks}} \n& ACM & 1 & 16K & 71K & 8.3K & - & &\\text{http://www.arnetminer.org/open-academic-graph} \\\\   \\cline{2-9}\n& Cora & 1 & 2.7K & 5.2K & 1.4K & - & &http://linqs.cs.umd.edu/projects/projects/lbc \\\\    \\cline{2-9}\n& Citeseer & 1& 3.3K & 4.7K & 3.7K & - & & http://linqs.cs.umd.edu/projects/projects/lbc \\\\    \\cline{2-9}\n& Pubmed & 1& 19K & 44K & 500 & - & &http://linqs.cs.umd.edu/projects/projects/lbc \\\\   \\cline{2-9}\n& DBLP & 1& - & - & - & -  & &http://www.informatik.uni-trier.de/Ëœley/db/ \\\\\n\\midrule[1 pt]\n\\multirow{10}{*}{\\shortstack{Social \\\\ Networks}}\n&Enron & - & 80K & - & - & - &   & http://odds.cs.stonybrook.edu/\\#table2 \\\\    \\cline{2-9}\n&UCI Message & 1& 5K & - & - & -  & &http://archive.ics.uci.edu/ml \\\\  \\cline{2-9}\n&Google+ & 4 & 75M & 11G & - & -  & -  & https://wangbinghui.net/dataset.html \\\\ \\cline{2-9}\n&Twitter Sybil & 3 & 41M & - & - & 100K  & -  & https://wangbinghui.net/dataset.html \\\\ \\cline{2-9}\n&Twitter WorldCup2014 & - & 54K & - & - & -  &   & http://shebuti.com/SelectiveAnomalyEnsemble/ \\\\ \\cline{2-9}\n&Twitter Security2014 & - & 130K & - & - & -  &   & http://shebuti.com/SelectiveAnomalyEnsemble/ \\\\ \\cline{2-9}\n&Reality Mining & - & 9.1K & - & - & -  &   & http://shebuti.com/SelectiveAnomalyEnsemble/ \\\\ \\cline{2-9}\n&NYTNews & - & 320K & - & - & -  &   & http://shebuti.com/SelectiveAnomalyEnsemble/ \\\\ \\cline{2-9}\n&Politifact & 314 & 41K & 40K & - & 157  &  &https://github.com/safe-graph/GNN-FakeNews \\\\ \\cline{2-9}\n&Gossipcop & 5.4K & 314K & 308K & - & 2.7K &  &https://github.com/safe-graph/GNN-FakeNews \\\\ \n\\midrule[1 pt]\n\\multirow{5}{*}{\\shortstack{Co-purchasing \\\\ Networks}}\n& Disney & 1 & 124 & 334 & 30 & 6  & &https://www.ipd.kit.edu/mitarbeiter/muellere/consub/ \\\\  \\cline{2-9}\n&Amazon-v1 & 1 & 314K & 882K & 28 & 6.2K  & &https://www.ipd.kit.edu/mitarbeiter/muellere/consub/ \\\\ \\cline{2-9}\n&Amazon-v2  & 1 & 11K & - & 25 & 821  & -&https://github.com/dmlc/dgl/blob/master/python/dgl/data/fraud.py \\\\ \\cline{2-9}\n&Elliptic & 1 & 203K & 234K & 166 & 4.5K  & - &https://www.kaggle.com/ellipticco/elliptic-data-set \\\\\\cline{2-9}\n&Yelp  & 1 & 45K & - & 32 & 6.6K  & -&https://github.com/dmlc/dgl/blob/master/python/dgl/data/fraud.py \\\\ \n\\midrule[1 pt]\nTransportation Networks &  New York City Taxi& - & - & - & - & - & &http://www.nyc.gov/html/tlc/html/about/triprecorddata.shtml \\\\\n\\bottomrule[1 pt]\n\\multicolumn{9}{l}{* -: Not Given, \\#G: Number of Graphs, \\#N: Number of Nodes, \\#E: Number of Edges, \\#FT: Number of Features, \\#AN: Number of Anomalies, REF: References.}\n\\end{tabular}\n\\label{tb:publisheddataset}\n}\n\\end{table*}", "cites": [8993, 6285, 6281, 6302, 6297, 3333, 6283, 6303, 6296, 6289, 3700, 6294, 6304, 6299, 6271, 6301, 1420, 6277, 6295, 6300, 6293, 5645, 8149, 6291, 6286, 6298], "cite_extract_rate": 0.4126984126984127, "origin_cites_number": 63, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "low", "analysis": "The section primarily describes various models and techniques for dynamic graph anomaly detection, listing methods and their objective functions in a tabular format. It provides minimal synthesis by grouping papers by graph type but does not deeply connect their underlying ideas or themes. There is limited critical analysis, and the abstraction remains at a surface level, focusing on categorization rather than extracting broader principles or trends."}}
{"id": "4da63f48-4501-4170-981d-8fe6cdbe1f52", "title": "Synthetic Dataset Generation", "level": "subsection", "subsections": [], "parent_id": "8147e459-f864-454f-86f6-52d0b0cd6b52", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "Published Algorithms and Datasets"], ["subsection", "Synthetic Dataset Generation"]], "content": "\\label{sec:synthetic}\nGiven the rarity of ground-truth anomalies, many researchers have employed synthetic datasets to investigate the effectiveness of their proposed methods~. \nTypically, these datasets can be categorized as follows: \n\\begin{itemize}\n    \\item \\textit{Synthetic graphs with injected anomalies.} Pursuing this strategy, graphs are created to simulate real-world networks. All the nodes and edges are manually added with well-known benchmarks (\\eg Lanchinetti-Fornunato-Radicchi (LFR)~, small-world~, scale-free graphs~). Once built, ground-truth anomalies are planted into the network. For the feasibility of generating expected scale of networks, this strategy is mostly used by previous works to validate their underlying intuitions in anomaly detection.\n    \\item \\textit{Real-world datasets with injected anomalies.} These datasets are built based on the real-world networks. In particular, anomalies are created either by modifying the topological structure or the attributes of existing nodes/edges/sub-graphs, or by inserting non-existent graph objects. \n    \\item \\textit{Downsampled graph classification datasets.} The widely-used graph classification datasets (\\eg NCI1, IMDB, ENZYMES in~) can be easily converted into sets suitable for anomaly detection through two steps. Firstly, one particular class and its data records are chosen to represent normal objects. Then, the other data records are downsampled as anomalies at a specified downsampling rate. By this, the generated graph anomaly detection dataset is, in fact, a subset of the original dataset. The most significant strength of this strategy is that no single data record has been modified.\n\\end{itemize}", "cites": [6281], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section primarily describes the categories of synthetic datasets used in graph anomaly detection, with minimal synthesis of the cited papers. It briefly mentions a few benchmarks and methods but does not connect or integrate ideas across multiple sources. The analysis remains surface-level, with no critical evaluation or identification of broader trends or principles in the field."}}
{"id": "32fa0363-7074-47b3-9e03-5211f2f264be", "title": "Anomalous Edge, Sub-graph, and Graph Detection", "level": "subsection", "subsections": [], "parent_id": "e97bdf9c-475c-4ebc-8114-7af7a8b0aa87", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "Future Directions"], ["subsection", "Anomalous Edge, Sub-graph, and Graph Detection"]], "content": "\\label{sec:future:ED}\nIn real-world graphs, anomalies also appear as unusual relationships between objects, sub-structures formed by abnormal groups, or abnormal graphs, which are known as anomalous edges, sub-graphs, and graphs respectively.\nAs indicated in our review, there is a huge gap between the existing anomalous edge/sub-graph/graph detection techniques and the emerging demands for more advanced solutions in various application domains (\\eg social networks, computer networks, financial networks).\nWhen detecting anomalous edges/sub-graphs/graphs, the proposed methods should be capable of leveraging the rich information contained in graphs to find clues and characteristics that can distinguish normal objects and anomalies in specific applications.\nTypically, this involves extracting edge/sub-graph/graph-level features, modeling the patterns of these features, and measuring the abnormalities accordingly.\nHowever, current deep learning based graph anomaly detection techniques put forward very little effort in this regard.\n\\textit{Opportunities}: We believe more research efforts can be done on anomalous edge, sub-graph, and graph detection with regard to their significance in real-world applications. Possible solutions to this gap might to be first consider the application domain and explore domain knowledge to find complementary clues as a basis for these problems. Then, motivated by recent advances in deep learning for edge, sub-graph, and graph-level representation learning~, extensive work can be done to learn an anomaly-aware embedding space such that it is feasible to extract abnormal patterns of anomalies. Although this direction seems quite straightforward, the true challenge lies in the specific application domains. Hence, domain knowledge, anomalous pattern recognition and anomaly-aware deep learning techniques should be enforced simultaneously.", "cites": [6305], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section identifies a gap in deep learning-based graph anomaly detection for edge, sub-graph, and graph-level anomalies, referencing one cited paper to support the claim. While it provides a general analytical perspective on the limitations of current methods, it does not deeply synthesize multiple sources or provide detailed comparisons. It does generalize somewhat by emphasizing the need for domain knowledge and anomaly-aware embeddings, suggesting a broader research direction."}}
{"id": "239988e3-c594-47d4-adb9-69b272543369", "title": "Anomaly Detection in Dynamic Graphs", "level": "subsection", "subsections": [], "parent_id": "e97bdf9c-475c-4ebc-8114-7af7a8b0aa87", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "Future Directions"], ["subsection", "Anomaly Detection in Dynamic Graphs"]], "content": "Dynamic graphs provide powerful machinery with which to capture the evolving relationships between real objects and their attributes.\nTheir ever-changing structure and attribute information inherently make anomaly detection very challenging in these scenarios, leading to two primary concerns for the task.\nThe first is to consider the spatial and temporal information contained in each graph snapshot at different time stamps, and the second is to explore the evolving patterns of nodes, edges, sub-graphs and graphs, as well as their interaction with the node/edge attributes over time.\nWhen these challenges have been tackled with mature solutions, detection techniques will achieve better results.\n\\textit{Opportunities}: From our observations, most of deep learning based dynamic graph anomaly detection techniques are built on DeepWalk~, GCN~ or other deep models that are intuitively designed for static graphs. This means other information, like evolving patterns in attributes~), are not adequately used in the detection task.\nWe can therefore identify the following directions for future studies to target.\n\\begin{itemize}\n    \\item \\textit{Using dynamic graph mining tools.} As a popular research topic, deep learning for dynamic graph data mining~ has shown its effectiveness in supporting dynamic graph analysis, such as node clustering and graph classification~. More future works can be foreseen that adopt these techniques for anomaly detection.\n    \\item \\textit{Deriving solid evidence for anomaly detection.} The rich structural, attribute and temporal information in dynamic graphs are valuable resources for identifying anomalies. Apart from the indicators widely used in current works, such as burst of connections between node pairs or suddenly vanishing connections, we suggest exploring structural and attribute changes in depth. From such studies, we may derive additional information to enhance the detection performance, such as the appearance of abnormal attributes.\n    \\item \\textit{Handling complex dynamics.} Real-world networks always exhibit changes in both the network structure and node attributes, but only very few studies address this circumstance. Most of the state-of-arts only consider changes in one of these aspects. Although this `double' scenario is extremely complex and detecting anomalies in this kind of dynamic graph is very challenging, it is worth studying because these graphs are highly reflective of real network data.\n\\end{itemize}", "cites": [217, 218], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates the concepts from the cited papers to highlight the limitations of current deep learning methods for dynamic graph anomaly detection. It synthesizes the role of DeepWalk and GCN in static graph analysis and points out their underutilization in dynamic settings. While it offers some abstraction by framing broader challenges and opportunities, the critical analysis is moderate and primarily identifies gaps rather than providing in-depth evaluation of the cited works."}}
{"id": "aff7afb2-0d0e-4d85-baf6-f135a679f1c2", "title": "Anomaly Detection in Heterogeneous Graphs", "level": "subsection", "subsections": [], "parent_id": "e97bdf9c-475c-4ebc-8114-7af7a8b0aa87", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "Future Directions"], ["subsection", "Anomaly Detection in Heterogeneous Graphs"]], "content": "Heterogeneous graphs are a specific type of graph that contain diverse types of nodes and edges.\nFor instance, Twitter can be intuitively modeled as a heterogeneous graph comprised of tweets, users, words, etc.\n\\textit{Opportunities}: To use the complex relationships between different types of nodes in heterogeneous graphs for anomaly detection, representative works, such as HGATRD~, GCAN~ and GLAN~, typically decompose a heterogeneous graph into individual graphs according to meta-paths, \\eg one with tweets and users, and another with tweets and words. They then use D(G)NNs to learn the embeddings for graph anomaly detection. Such a decomposition inherently overlooks the direct inter-relations among diverse types of nodes/edges and downgrades the effectiveness of the embeddings. A possible solution is to reveal the complex relations between different types of nodes and edges, and encode them into a unique representation for boosted detection performance.", "cites": [6296, 6299, 6300], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.3, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview by identifying a common methodological approach (meta-path-based decomposition) across the cited papers and pointing out its limitation in ignoring direct inter-relations. It also proposes a potential solution, showing some level of critical evaluation and abstraction beyond individual works. However, the synthesis is limited to a general discussion without deep integration into a novel framework."}}
{"id": "094e633e-5d3b-45d7-bca3-171ad2d8492b", "title": "Anomaly Detection in Large-scale Graphs", "level": "subsection", "subsections": [], "parent_id": "e97bdf9c-475c-4ebc-8114-7af7a8b0aa87", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "Future Directions"], ["subsection", "Anomaly Detection in Large-scale Graphs"]], "content": "The scalability of methods to high-dimensional and large-scale data is an ongoing and significant challenge to anomaly detection techniques.\nIn face of large-scale networks, such as Facebook and Twitter that contain billions of users and friendship links, the size of data in terms of both graph size and number of node attributes is extremely high.\nHowever, most of the existing works lack the ability to detect anomalies in such large-scale data because they are transductive models and need to take the whole graph as input for further analysis.\nComputation time and memory cost increase dramatically as the network scales up, and this stops existing techniques from being used on large-scale networks.\n\\textit{Opportunities}: Accordingly, there is a need for scalable graph anomaly detection techniques.\nOne possible approach would be an inductive learning scheme that first trains a detection model on part of the whole graph and then applies the model to detect anomalies in the unseen data.\nAs some inductive learning models, such as GraphSAGE~, have shown their effectiveness on link prediction and node classification in large-scale graphs, this approach is expected to provide a basis for graph anomaly detection in large-scale graphs and similar techniques can be investigated in the future.", "cites": [242], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section highlights a key limitation of current graph anomaly detection methodsâ€”lack of scalabilityâ€”and proposes inductive learning as a potential solution, referencing GraphSAGE. While it connects the concept of inductive learning to the anomaly detection challenge, it does not deeply synthesize multiple sources or evaluate various techniques. It offers a clear direction for future research, indicating some level of analysis and abstraction beyond individual papers."}}
{"id": "e773dc9f-5c7d-4700-bcf9-1e7e0772172c", "title": "Multi-view Graph Anomaly Detection", "level": "subsection", "subsections": [], "parent_id": "e97bdf9c-475c-4ebc-8114-7af7a8b0aa87", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "Future Directions"], ["subsection", "Multi-view Graph Anomaly Detection"]], "content": "In real-world networks, objects might form different kinds of relationships with others (\\eg user's followership and friendship on Twiter). And their attribute information might be collected from different resources, such as user's profile, historical posts.\nThis results in two types of multi-view graphs: 1) multi-graph that contains more than one type of edges between two nodes~; and 2) multi-attributed-view graph that stores node attributes in different attributed views~.\n\\textit{Opportunities}: These multi-views basically allow us to analyze real objects' characteristics from different perspectives.\nEach view also provides complementary information to other views, and they might have different significance on anomaly detection. \nFor instance, anomalies might be indistinguishable in one view but are obviously divergent from the majority in another view.\nThere are a variety of work in data mining on multi-view learning~. However, work that can accommodate multi-view graphs along with multi-view attributes on nodes for anomaly detection purposes is nascent.\nMoreover, the rich information contained in multiple views and the inconsistency among them are overlooked in these works. \nTo this end, we believe more research effort in this direction is needed. Digesting the relationships between views will be vital to their success, as two views might provide contrary/supplementary information for anomaly detection.", "cites": [8994], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a coherent analytical overview of multi-view graph anomaly detection by introducing the concept and highlighting its potential. It synthesizes the cited paper by framing it within the broader context of multi-view networks and attributes. While it identifies some research gaps and the need for further exploration, it does not deeply evaluate or compare the cited work with others. The abstraction level is moderate, as it introduces a general understanding of multi-view graphs and their implications for anomaly detection."}}
{"id": "cc214041-b949-4f3c-95ae-5deb83ec1573", "title": "Camouflaged/Adversarial Anomaly Detection", "level": "subsection", "subsections": [], "parent_id": "e97bdf9c-475c-4ebc-8114-7af7a8b0aa87", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "Future Directions"], ["subsection", "Camouflaged/Adversarial Anomaly Detection"]], "content": "The easy accessibility of online platforms has made them convenient targets for fraudsters, attackers and other malevolent agents to carry out malicious activities.\nAlthough various anomaly detection systems have been deployed to protect benign objects, anomalies can still conceal themselves to evade detection~.\nKnown as camouflaged anomalies, these entities typically disguise themselves as regular objects.\nIf the detection techniques are not robust against such cases, \\ie if they cannot quickly and effectively adapt to the evolving behavior of evasion-seeking attackers, the anomalies are simply left to cause their damage.\n\\textit{Opportunities}: In the face of camouflage, the boundary between anomalies and regular objects is blurred, making anomalies much harder to be identified.\nWe believe extensive effort should be placed on detecting these anomalies because, as yet, very few studies have looked at handling camouflaged anomalies in graphs~.\nTo fulfill this gap, one major direction might be to jointly analyze the attributes, co-relations, such as the triadic, tetradic, or high-order relationships between objects in hypergraphs~, and other information comprised in graphs.\nBy this, anomalies that only camouflage their local structures or attributes can be identified effectively.\nEnhancing existing techniques might be another direction. \nThis involves incorporating additional detection mechanisms or function blocks particularly designed for distinguishing camouflaged anomalies with existing detection techniques.\nConsequently, these techniques will bridge most existing works and camouflaged anomaly detection.", "cites": [6307, 6306, 6271, 8995], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from the cited papers to frame the problem of camouflaged anomalies in graphs and proposes directions for future research, such as joint analysis of attributes and high-order relationships. While it identifies gaps and suggests possible solutions, the critical analysis is somewhat limited, and the abstraction provides some general insights but not a fully novel or meta-level framework."}}
{"id": "1224de5d-ffc7-41d5-bee2-9722227a3ca0", "title": "Multi-task Anomaly Detection", "level": "subsection", "subsections": [], "parent_id": "e97bdf9c-475c-4ebc-8114-7af7a8b0aa87", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "Future Directions"], ["subsection", "Multi-task Anomaly Detection"]], "content": "Graph anomaly detection has close relations with other graph mining tasks including community detection~ and node classification~, and link prediction~. \nFor a concrete example, when detecting community anomalies, community detection techniques are usually used to extract the community structures prior to anomaly detection. Meanwhile, the anomaly detection results can be used to optimize the community structure.\nSuch mutually beneficial collaborations between anomaly detection and other tasks inherently suggest an opportunity for multi-task learning that can handle diverse tasks simultaneously and share information among tasks.\n\\textit{Opportunities}: Multi-task learning provides effective machinery with which incorporate associated tasks~. Its utmost advantage is that the training signal from another task could yield complementary information to distinguish anomalies from non-anomalies. The result would be enhanced detection performance. However, very few attempts focus on this at present.\nBeyond current works, such as~ that jointly perform anomalous node detection and personalized recommendation, explorations into combining other learning tasks with graph anomaly detection are likely to emerge as a fruitful future direction.", "cites": [6308, 1671, 6309, 6284], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section offers some synthesis by connecting graph anomaly detection to related tasks such as community detection and link prediction, and highlights how these tasks can inform and improve one another. However, the integration remains at a general level without a novel framework. It shows limited critical analysis by noting the scarcity of multi-task approaches but does not deeply critique the cited works. The abstraction is moderate, as it generalizes the idea of task interdependencies and hints at future opportunities for multi-task learning in the field."}}
{"id": "d42a5aab-3ff4-4f52-8576-f381cddddc12", "title": "Graph Anomaly Interpretability", "level": "subsection", "subsections": [], "parent_id": "e97bdf9c-475c-4ebc-8114-7af7a8b0aa87", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "Future Directions"], ["subsection", "Graph Anomaly Interpretability"]], "content": "The interpretability of anomaly detection techniques is vital to the subsequent anomaly handling process.\nWhen applying these techniques to real applications, such as financial and insurance systems, it is essential to provide explainable and lawful evidence to support the detection results.\nHowever, most of the existing works lack the ability to provide such evidence.\nTo identify the anomalies, the most commonly used metrics are top-k rankings and simple anomaly scoring functions.\nThese metrics are flexible enough to label objects as being either an anomaly or not an anomaly, but they cannot derive solid explanations.\nMoreover, as deep learning techniques have also been criticized for their low interpretability, future works on graph anomaly detection with deep learning should pay much more attention to this~.\n\\textit{Opportunities}: To bridge this gap, integrating specially designed interpretation algorithms or mechanisms~ into the detection framework would be a possible solution, noting that this would inherently induce a higher computational cost.\nFuture works should therefore balance the cost of anomaly detection performance and interpretability.\nVisualization-based approaches, \\eg dashboards, charts, might also be feasible for showing the distinction between anomalies and non-anomalies in a human-friendly manner.\nFurther research in this direction will be successful if interpretable visualization results can be given~.", "cites": [8996, 6310], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a general analytical perspective on the interpretability challenge in graph anomaly detection using deep learning, but it only weakly synthesizes the cited papers. It mentions issues of interpretability and suggests potential solutions, which reflects some critical thinking, yet the integration of the two papers is minimal and does not form a strong, coherent narrative. The abstraction level is moderate, as it identifies a broader need for interpretability but does not offer deep meta-level insights or a novel framework."}}
{"id": "4eb29545-f452-443d-b7b6-8accd75e515c", "title": "Graph Anomaly Identification Strategies", "level": "subsection", "subsections": [], "parent_id": "e97bdf9c-475c-4ebc-8114-7af7a8b0aa87", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "Future Directions"], ["subsection", "Graph Anomaly Identification Strategies"]], "content": "Amongst existing unsupervised graph anomaly detection techniques, anomalies are mainly identified based on residual analysis~, reconstruction loss~, distance-based statistics~, density-based statistics~, graph scan statistics~, and one-class classification~.\nThe underlying intuition of these identification strategies is that anomalies have inconsistent data patterns with regular objects, and they will, therefore: 1) introduce more residual errors or be harder to reconstruct; or 2) be located in low-density areas or far away from the majority class in an anomaly-aware feature space.\nEffort toward designing novel loss functions for GNNs for anomaly detection is currently quite limited~\n\\textit{Opportunities}: Although these strategies could capture the deviating data patterns of anomalies, they also have different limitations. Specifically, the residual analysis, one-class classification and reconstruction loss strategies are sensitive to noisy training data. Noisy nodes, edges or sub-graphs also exhibit large residuals, a large distance to the origin/hypersphere center and high reconstruction losses. Meanwhile, the distance-based and density-based strategies can only be applied when anomalies and non-anomalies are well separated in lower-dimensional space. Detection performance also downgrades dramatically if the gap between anomalies and non-anomalies is not that evident.\nIt calls for extensive future efforts to break these limitations and explore new anomaly identification strategies.", "cites": [6289, 6287, 6283, 8997], "cite_extract_rate": 0.36363636363636365, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple identification strategies (residual, reconstruction, distance, density, scan statistics, one-class classification) and relates them to the cited works, such as AnomalyDAE and SpecAE, which focus on reconstruction and representation learning. It critically evaluates the limitations of each strategy, highlighting sensitivity to noise and separation assumptions. The abstraction is strong as it moves beyond individual papers to identify overarching challenges and opportunities in the design of anomaly identification strategies for GNNs."}}
{"id": "9a4d2cc0-0208-4633-a227-f43cf17e07e4", "title": "Challenges in Graph Anomaly Detection", "level": "section", "subsections": [], "parent_id": "5020eb09-4f12-4d1a-8dae-076499c8173a", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "Challenges in Graph Anomaly Detection"]], "content": "\\label{appendix:data-challenges}\nDue to the complexity of anomaly detection and graph data mining, adopting deep learning technologies for graph anomaly detection faces a number of challenges:\n\\textbf{Data-CH1. Ground-truth is scarce.} In most cases, there is little or no prior knowledge about the features or patterns of anomalies in real applications. Ground-truth anomalies are often identified by domain experts, and this generally cost-prohibitive. As a result, labeled ground-truth anomalies are often unavailable for analysis in a wide range of disciplines.\n\\textbf{Data-CH2. Various types of graphs.} Different graphs model different real-world data. For instance, plain graphs contain only structural information, attributed graphs contain both structural and attribute information, and heterogeneous graphs represent the complex relations between different types of objects.\nThese graphs reflect the real-world data in different forms, and graph anomalies will show different deviating patterns in different types of graphs.\n\\textbf{Data-CH3. Various types of graph anomalies.} Given a specific type of graph, graph anomalies could appear as a specific node, edge, sub-graph, or an entire graph, and each type of these anomalies is significantly different from others. This means detection methods must involve concise definitions of anomalies and be able to identify concrete clues about the deviating patterns of anomalies.\n\\textbf{Data-CH4. High dimensionality and large scale.} Representing the structure information of real-world networks usually results in high dimensional and large-scale data~ because real-world network often contain millions or billions of nodes. Graph anomaly detection techniques, hence, should be capable of handling such high dimensional and large scale data; this includes the ability to extract anomalous patterns under the constraints of execution time and feasible computing resources. \n\\textbf{Data-CH5. Interdependencies and dynamics.} The relationships between real objects reveal their interdependencies and they can no longer be treated individually for anomaly detection. That is to say, the detection techniques need to consider the deviating patterns of anomalies by assessing the pairwise, triadic, and higher relationships among objects restored in conventional graphs or hypergraphs~.\nIn addition, the dynamic nature of real-world networks makes detection problems much more challenging.\n\\textbf{Data-CH6. Class imbalance.} As anomalies are rare occurrences, only a very small proportion of the real-world data might be anomalous. This naturally introduces a critical class imbalance problem to anomaly detection because the number of normal objects is far greater than anomalies in the training data. If no further actions are taken to tackle this challenge, learning-based anomaly detection techniques might overlook the patterns of anomalies, leading to sub-optimal results.\n\\textbf{Data-CH7. Unknown and camouflage of anomalies.} In reality, knowledge about anomalies mainly stems from human expertise. There are still many unknown anomalies across different application domains, and new types of anomalies might appear in the future. Nevertheless, real-world anomalies can hide or be camouflaged as benign objects to bypass existing detection systems. In graphs, anomalies might hide themselves by connecting with many normal nodes or by mimicking their attributes. Detection methods, therefore, need to be adaptive to unknown and novel anomalies and robust to camouflaged anomalies.\nThese data-specific challenges and technical-specific challenges (discussed in Section~\\ref{sec:introdution:challenges}) are summarized in Table.~\\ref{tb:challenges}, along with the corresponding articles that aim to address them.\n\\begin{table}\n\\centering \n\\footnotesize\n\\renewcommand\\arraystretch{1}\n\\setlength{\\tabcolsep}{2.8mm}\n\\caption{Challenges and Methods.}\n\\resizebox{0.48\\textwidth}{!}{\n\\begin{tabular}{m{1.1cm}<{\\centering}|m{2cm}<{\\centering}|m{2.9cm}<{\\centering}}\n\\toprule[1 pt]\n\\textbf{Challenges} & \\textbf{Details} &  \\textbf{Methods} \\\\ \n\\midrule[1 pt]\n\\multicolumn{3}{c}{\\textbf{Data-specific Challenges}} \\\\\n\\midrule[1 pt]\nData-CH1 & Ground-truth is scarce              &  \\\\ \n\\cline{1-3}\nData-CH2 & Various types of graphs             &  \\\\ \n\\cline{1-3}\nData-CH3 & Various types of graph anomalies    &  \\\\ \n\\cline{1-3}\nData-CH4 & High dimensionality and large scale &  \\\\ \n\\cline{1-3}\nData-CH5 & Interdependencies and dynamics      &  \\\\ \n\\cline{1-3}\nData-CH6 & Class imbalance                     &  \\\\ \n\\cline{1-3}\nData-CH7 & Unknown and camouflage of anomalies &  \\\\ \n\\midrule[1 pt]\n\\midrule[1 pt]\n\\multicolumn{3}{c}{\\textbf{Techniques-specific Challenges}} \\\\\n\\midrule[1 pt]\nTech-CH1 & Anomaly-aware training objectives   &  \\\\\\cline{1-3}\nTech-CH2 & Anomaly interpretability            &  \\\\\\cline{1-3}\nTech-CH3 & High training cost                  &  \\\\\\cline{1-3}\nTech-CH4 & Hyperparameter tuning              &  \\\\\n\\bottomrule[1 pt]\n\\end{tabular}\n\\label{tb:challenges}\n}\n\\end{table}", "cites": [6285, 6281, 6283, 6289, 6307, 6279, 6271, 6306, 6293, 5645, 8149, 6291, 6286], "cite_extract_rate": 0.3170731707317073, "origin_cites_number": 41, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section lists several challenges in graph anomaly detection and briefly mentions the need for methods to address them, but it does not deeply synthesize or integrate insights from the cited papers. It provides minimal critical evaluation or comparison of approaches, and the abstraction level remains low as it focuses on enumerating challenges rather than deriving overarching principles or trends."}}
{"id": "ac5b6d2a-da5e-45c0-bd9e-1a63dbed603a", "title": "GAT Based Techniques", "level": "subsection", "subsections": [], "parent_id": "dfc7ea98-0750-495a-b3c3-8183220b6042", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "ANOS ND on Static Graphs"], ["subsection", "GAT Based Techniques"]], "content": "\\begin{figure*}[!t]\n\\setlength{\\belowcaptionskip}{-0.25cm}\n    \\centerline{\\includegraphics[width=0.98\\textwidth]{pics/Node/GAT.pdf}}\n    \\caption{ANOS ND on attributed graphs -- GAT based approaches. Given the input graph, these techniques employ graph attention neural network to learn node embeddings. The unsupervised technique, AnomalyDAE~, scores each node based on the reconstruction loss and mark the top-k nodes as anomalies, while the semi-supervised technique, SemiGNN~, trains a classifier to predict node labels.}\n    \\label{pic:GAT}\n\\end{figure*}\nAlthough GCN provides an effective solution to incorporating graph structure with node attributes for ANOS ND (reviewed in Section~\\ref{sec:AND:GCN}), its ability to capture the most relevant information from neighboring is subpar.\nThis is due to the simple convolution operation that aggregates neighbor information equally to the target node.\nRecently, graph attention mechanism (GAT)~ is employed to replace the traditional graph convolution.\nFor instance, Fan \\etal~ applied graph attention neural network to encode the network structure information (structure encoding).\nThe method, AnomalyDAE, also adopts a separate attribute autoencoder to embed the node attributes (attribute encoding).\nThrough an unsupervised encoding-decoding process, each node is ranked according to its corresponding reconstruction loss, and the top-k nodes introducing the greatest losses are identified as anomalies.\nSpecifically, the attribute decoding process takes both node embeddings learned through the structure and attribute encoding processes to reconstruct node attributes, as shown in Fig.~\\ref{pic:GAT}, while the graph topology is reconstructed only using the embeddings output by the GAT.\nTo acquire better reconstruction results, AnomalyDAE is trained to minimize the overall loss function, denoted as:\n\\begin{equation} \\label{eq:anomalydae}\n\\begin{split}\n    \\mathcal{L}_{AnomalyDAE} = & \\alpha ||(A - \\hat{A})\\odot \\bm{\\theta}||_{2}^{2} + \\\\\n    &(1-\\alpha)||(X - \\hat{X})\\odot \\bm{\\eta}||_{2}^{2},\n\\end{split}\n\\end{equation}\nwhere $\\alpha$ is the coefficient, $A$ and $X$ is the input adjacency matrix and attribute matrix, $\\hat{A}$ and $\\hat{X}$ are the reconstructed matrices. Each $\\theta_{i,j} \\in \\bm{\\theta}$ and $\\eta_{i,j} \\in \\bm{\\eta}$ is 1, if the corresponding element $A_{ij}$ and $X_{ij}$ equals 0, otherwise, their values are defined by hyperparameters greater than 1.\nAnother decent work is SemiGNN~, in which Wang \\etal proposed a semi-supervised attention-based graph neural network for detecting fraudulent users in online payment platforms.\nThis work further explores user information collected from various sources (\\eg transaction information and user profiles), and represents real-networks as multi-view graphs.\nEach view in the graph is modeled to reflect the relationship between users or the correlation between user attributes.\nFor anomaly detection, SemiGNN first generates node embedding $h_u^v$ from each view $v$ by aggregating neighbor information through a node-level attention mechanism.\nIt then employs view-level attention to aggregate node embeddings from each view and generates a unified representation $a_u$ for each node.\nLastly, the class of each node is predicted through a softmax classifier.\nIndeed, Wang \\etal designed a supervised classification loss and an unsupervised graph reconstruction loss to jointly optimize the model by fully utilizing labeled and unlabeled data.\nThe classification loss can be denoted as:\n\\begin{equation} \n    \\mathcal{L}_{sup} = - \\frac{1}{|U_L|}\\sum_{u \\in U_L} \\sum_{i=1}^{k}I(y_u=i)\\log\\frac{\\exp(a_u\\cdot\\theta_i)}{\\sum_{j=1}^{k}\\exp(a_u\\cdot\\theta_j)},\n\\end{equation}\nwhere $U_L$ is the labeled user set and its size is $|U_L|$, $I(\\cdot)$ is an indicator function, $k$ is the number of labels to be predicted (in most cases, the label is either anomalies or non-anomalies, and $k=2$), and $\\theta$ represents the trainable variables.\nMeanwhile, the unsupervised loss encourages unlabeled nodes (users) that can be reached by labeled nodes through random walks to obtain similar representations and vice versa.\nThis is achieved by negative sampling (unlabeled nodes that cannot be reached by random walks are negative samples) and the loss can be formulated as:\n\\begin{equation}\n  \\begin{split}\n    \\mathcal{L}_{unsup}  & = \\sum_{u\\in U}\\sum_{v\\in N_u \\cup Neg_{u}} -\\log(\\sigma(a_{u}^{T}a_{v})) \\\\\n    &- 3\\cdot E_{q\\sim P_{neg}(u)}\\log(\\sigma(a_{u}^{T}a_{q})),\n  \\end{split}\n\\end{equation}\nwhere $U$ denotes the user set, $N_u$ denotes the neighbor set of $u$, $Neg_{u}$ represents negative samples, $P$ is the sampling distribution, and $\\sigma(\\cdot)$ is the sigmoid function. The total loss takes the sum of them and is formulated as:\n\\begin{equation} \n    \\mathcal{L}_{SemiGNN} = \\alpha \\mathcal{L}_{sup} + (1- \\alpha) \\mathcal{L}_{unsup} + \\lambda \\mathcal{L}_{reg},\n\\end{equation}\nwhere $\\alpha$ is a balancing parameter and $\\mathcal{L}_{reg}$ regularizes all trainable variables.", "cites": [6291, 180, 6289], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from two GAT-based papers (AnomalyDAE and SemiGNN) by integrating their methodologies and highlighting how they differ in terms of supervision (unsupervised vs. semi-supervised). It provides a clear analytical structure by explaining the role of attention mechanisms and their benefits over GCNs. However, the critical analysis is limited to a brief mention of GCNs' shortcomings without deeper evaluation of the limitations or trade-offs in the GAT-based techniques."}}
{"id": "99113f23-1403-4b3d-bae2-8db2bf051152", "title": "Network Representation Based Techniques", "level": "subsection", "subsections": [], "parent_id": "dfc7ea98-0750-495a-b3c3-8183220b6042", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "ANOS ND on Static Graphs"], ["subsection", "Network Representation Based Techniques"]], "content": "With network representation, graphs are first encoded into a vector space before the anomalies detection procedure takes place.\nAs outlined in Section~\\ref{sec:spg:nr}, numerous studies on ANOS ND in attributed graphs have exploited deep network representation techniques.\nFor instance, Zhang \\etal~ detected abnormal nodes that have attributes significantly deviating from their neighbors through a 3-layer neural network, REMAD, and residual analysis.\nThey explicitly divide the original node attribute matrix into a residual attribute matrix $R$ that captures the abnormal characters of anomalies and a structurally relevant attribute matrix $\\hat{X}$ for network representation learning. Both matrices are jointly updated throughout the representation learning process so nearby nodes are encouraged to have similar representations.\nSpecifically, these node embeddings are generated by aggregating neighbor information with each node's own attributes, formulated as:\n\\begin{equation} \n    h_{i}^{l} = \\sigma \\left( W^l \\cdot \\text{CONCAT}\\{h_{i}^{l-1},h_{N_i}^{l}\\} + b^2\\right),\n\\end{equation}\nwhere $h_{i}^{l}$ is node $i$'s representation generated by the $l$-th layer ($h_{i}^{0} = \\hat{X}$), $N_i$ contains $i$'s neighbors, $\\sigma()$ is the activation function, $W^{l}$ and $b$ are the trainable variables. Finally, the residual matrix $R$ will contain the abnormal information of each node and the top-k nodes with the largest norms are considered anomalies.\nGiven partial node labels, Liang \\etal~ developed a semi-supervised representation model, SEANO, that incorporates graph structure, node attributes and label information.\nSimilar to REMAD, SEANO also aggregates neighbor information to center nodes, and the node representations are obtained through an embedding layer, formulated as:\n\\begin{equation} \\label{SEANO:embedding}\n    z_{i} = \\lambda_{i}h^{l_{1}}(x_{i}) + (1-\\lambda_{i})h^{l_{1}}(\\Bar{x}_{N_{i}}),\n\\end{equation}     \nwhere $z_{i}$ is $i$'s representation, $\\lambda_{i}$ is a trainable variable that identifies the weight of $i$'s own attributes ($x_{i}$), $\\Bar{x}_{N_{i}}$ is the average of node $i$'s neighbors' representations, and the function $h^{k}(x_{i}) = \\phi(W^kh^{k-1}(x_i)+b^k)$ maps original node attributes into lower dimensional vectors.\nThen, a supervised component, which takes the representations as input, predicts node labels through a softmax classifier, and an unsupervised component is trained to reconstruct node contexts (node sequences).\nThe context of each node is not only generated through random walks on the graph but also from the labeled nodes that belong to the same class. After training, SEANO interprets $\\lambda_{i}$ as the normality score of node $i$ and the top-k nodes with the highest scores are classed as anomalies.\nLearning node representations via aggregating neighbor information has proven effective for capturing comprehensive information from graph structure and node attributes. \nBut, Liu \\etal~ demonstrated such an approach can help anomalies aggregate features from regular nodes, making them look normal and leading to sub-optimal detection performance. \nThey identified three concrete issues that should be considered when applying aggregation operations for anomaly detection: \n1) Anomalies are rare objects in a network. Hence, directly aggregating neighborhood information will smooth the difference between the anomalies and normal instances, blurring boundaries between them. \n2) Directly connected nodes have distinctive features, and the assumption that connected nodes share similar features, which serves as the basis for feature aggregation, no longer holds in this scenario.\n3) Real objects also form multiple types of relations with others, which means aggregation results for different types of relations will be distinctive.\nWith regard to these concerns, their proposed method, GraphConsis, follows a sampling strategy to avoid potential anomalous neighbors when aggregating node features.\nThis method also adopts an attention mechanism to aggregate neighbor information following different links.\nThe learned node representations, therefore, are more robust to anomalies. As such, GraphConsis takes them as input to train a classifier for predicting labels.\nDou \\etal~ further considered camouflage behaviors of fraudsters in their proposed model CARE-GNN to enhance detection performance.\nAs specified, the camouflages can be categorized as either feature camouflage or relation camouflage.\nRespectively, anomalies either adjust their feature information or form connections with many benign objects to gloss over suspicious information.\nHence, directly employing aggregation will overlook the camouflages and smooth the abnormal patterns of anomalies, eliminating the distinctions between anomalies and normal objects. \nTo alleviate over-smoothness, CARE-GNN also adopts a neighbor sampling strategy, as is the case with GraphConsis, to filter camouflaged anomalies and explores different types of relations formed between users.\nSpecifically, under each relation, Dou \\etal employed a MLP to predict node labels using their features and measure the similarity ($l1$ distance) between each node and its neighbors according to the MLP's output.\nThen, the top-k most similar neighbors are selected for feature aggregation, and CARE-GNN generates each node's representation through a combination of latent representations that are learned under different relations.\nA classifier is eventually trained using the representations to predict the node labels.\nAs can be seen, the performance of these network representation based techniques is decided by their training objectives/loss functions.\nEnhanced detection performance is probable if the loss function is able to separate normal nodes from abnormal nodes reasonably well. \nMotivated by this, a more recent work in~ emphasizes the importance of anomaly-aware loss functions.\nIn order to adjust margins for the anomalies, the authors proposed a novel loss function to guide the representation learning process.\nSpecifically, this loss function is designed to find the relative scales between the margins of outlier nodes and normal nodes.\nAn MLP-based classifier is finally trained using the node representations generated by the anomaly-aware loss-guided GNNs and node labels. \nFor unseen nodes, the classifier will label them upon their representations.", "cites": [6286, 6298, 6271], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes key techniques from multiple papers (REMAD, SEANO, GraphConsis, CARE-GNN) and connects their design choices and innovations in a coherent narrative. It also provides critical analysis by identifying limitations of aggregation-based methods and explaining how specific techniques address these issues. While the section offers strong analytical depth, it slightly falls short of abstraction by not fully generalizing these insights into broader principles or frameworks."}}
{"id": "ef880176-a742-4e90-bb20-77c1c0e4b593", "title": "ANOS ED With Traditional Non-Deep Learning Techniques", "level": "section", "subsections": [], "parent_id": "5020eb09-4f12-4d1a-8dae-076499c8173a", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "ANOS ED With Traditional Non-Deep Learning Techniques"]], "content": "\\label{appendix:edge}\nTraditional non-deep learning based approaches mainly focus on using temporal signals (\\eg changes in graph structure), and applying specially designed statistical metrics to detect anomalous edges on dynamic graphs~. \nAs a concrete example, Eswaran and Faloutsos~ modeled a dynamic graph as a stream of edges and exploited the graph structure as well as the structure evolving patterns. They identified two signs of anomalous edges: 1) connecting regions of the graph that were disconnected; and 2) connections that appear in bursts. For incoming edges, their model assigns anomaly scores to each edge, and the top-k edges with highest scores are anomalies. Another most recent work by Chang \\etal~ proposed a novel frequency factorization algorithm, aiming to spot anomalous incoming edges based on their likelihood of observed frequency. This method merges the advantages of both probabilistic models and matrix factorization for capturing both temporal and structural changes of nodes, and as reported, it only requires constant memory to handle edge streams.", "cites": [6293], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of two traditional non-deep learning edge anomaly detection techniques, but it lacks deeper synthesis of ideas across the cited works. It does not critically analyze the methods or identify limitations, nor does it abstract to broader patterns or principles in the field. The narrative remains at a surface level, primarily summarizing each paper's approach."}}
{"id": "a68d74d8-bc4c-4876-9c25-1421d3078bb4", "title": "ANOS SGD on Static Graphs", "level": "subsection", "subsections": [], "parent_id": "6dcc483a-2e1d-4f81-abe8-97ae152ee771", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "ANOS SGD With Traditional Non-Deep Learning Techniques"], ["subsection", "ANOS SGD on Static Graphs"]], "content": "\\label{appendix:subgraph:static}\nOne motivation of ANOS SGD in static graphs is that anomalous sub-graphs often exhibit significantly different attribute distributions.\nTherefore, traditional non-deep learning techniques, such as gAnomaly~, AMEN~, and SLICENDICE~, focus on modeling the attribute distributions and measuring the normality of sub-graphs.\nAnother line of investigation is graph residual analysis. The rich attribute information contained in real-world networks provides insight into the relationships formed between objects. Thus, the motivation behind several studies to spot anomalous sub-graphs has been to measure the residual between the expected structures and observed structures~.", "cites": [6277, 6297], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive summary of two lines of research in ANOS SGD on static graphs: modeling attribute distributions and graph residual analysis. It integrates the cited papers at a minimal level by grouping them under shared motivations but lacks deeper synthesis, critical evaluation, or abstraction to broader principles. The analysis remains surface-level and does not offer a comparative or evaluative perspective."}}
{"id": "ab11e3f8-fc96-4814-ac49-21cd01dfc491", "title": "ANOS SGD on Dynamic Graphs", "level": "subsection", "subsections": [], "parent_id": "6dcc483a-2e1d-4f81-abe8-97ae152ee771", "prefix_titles": [["title", "A Comprehensive Survey on\\\\ Graph Anomaly Detection with Deep Learning"], ["section", "ANOS SGD With Traditional Non-Deep Learning Techniques"], ["subsection", "ANOS SGD on Dynamic Graphs"]], "content": "\\label{appendix:subgraph:dynamic}\nDevising metrics for ANOS SGD has been the subject of many traditional works. \nFor instance, Chen \\etal~ introduced six metrics to identify community-based anomalies, namely: grown community, shrunken community, merged community, split community, born community and vanished community.\nAlthough these hand-crafted features or statistical patterns well fit some particular types of existing anomalies, their abilities to detect unseen and camouflage anomalies are limited and applying them directly might introduce high false negative rate, which is not optimal for applications like financial security. \nOther works, such as SPOTLIGHT by Eswaran \\etal~ and another by Liu \\etal~, explore sudden changes in dynamic graphs and identify anomalous sub-graphs that are related to such changes.\nMotivated by the phenomena that social spam and fraud groups often form dense temporal sub-graphs in online social networks, plenty of works, including,~, use manually-extracted features and spot anomalous dense sub-graphs that have evolved significantly different from the reset of the graph. \nIn addition to these studies, a large number of works discuss uses of various graph scan statistics for anomalous sub-graph detection, such as the Kulldorff statistic~, Poisson statistic~, elevated mean scan statistic~ and Berk-Jones statistic~. Specifically, Shao \\etal~ proposed a non-parametric method to detect anomalous sub-graphs in dynamic graphs where the network structure is constant, but the node attributes change overtime. This approach measures the anomalous score of each sub-graph with regard to the p-values of nodes it comprises. Sub-graphs with higher scores are more anomalous. Another work, GBGP~, instead, adopts the elevated mean scan statistic to identify nodes that might form anomalous sub-graphs and detects anomalous groups that follow predefined irregular structures.\n\\end{document}", "cites": [6272, 8997], "cite_extract_rate": 0.18181818181818182, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of various traditional methods for detecting anomalies in dynamic graphs. While it integrates several approaches and mentions their application contexts, the synthesis remains limited and does not present a novel framework. The critical analysis is minimal, with only a passing mention of limitations like high false negative rates. The abstraction level is low, focusing on individual methods rather than broader patterns or principles."}}
