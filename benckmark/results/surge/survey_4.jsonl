{"id": "e40a3a7a-c96c-4593-8997-9422d0c66615", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "2c20cd26-fec7-4463-b98c-798fc974562d", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Introduction"]], "content": "}\n\\begin{figure*}\n    \\centering\n    {\n        \\setlength{\\fboxrule}{1pt}\n        \\includegraphics[width=0.95\\linewidth]{resource/fig_taxonomy}\n    }\n    \\caption{\n        Hierarchically-structured taxonomy of this survey.\n    }\n    \\label{fig_sr_methodology}\n\\end{figure*}\n\\IEEEPARstart{I}{mage} super-resolution (SR), which refers to the process of recovering high-resolution (HR) images from low-resolution (LR) images, is an important class of image processing techniques in computer vision and image processing.\nIt enjoys a wide range of real-world applications, such as medical imaging , surveillance and security ), amongst others.  \nOther than improving image perceptual quality, it also helps to improve other computer vision tasks .  \nIn general, this problem is very challenging and inherently ill-posed since there are always multiple HR images corresponding to a single LR image.\nIn literature, a variety of classical SR methods have been proposed, including prediction-based methods , edge-based methods , statistical methods , patch-based methods  and sparse representation methods , etc.\nWith the rapid development of deep learning techniques in recent years, deep learning based SR models have been actively explored and often achieve the state-of-the-art performance on various benchmarks of SR.\nA variety of deep learning methods have been applied to tackle SR tasks, ranging from the early Convolutional Neural Networks (CNN) based method (e.g., SRCNN ) to recent promising SR approaches using Generative Adversarial Nets (GAN)  (e.g., SRGAN ).\nIn general, the family of SR algorithms using deep learning techniques differ from each other in the following major aspects: different types of network architectures , different types of loss functions , different types of learning principles and strategies , etc.\nIn this paper, we give a comprehensive overview of recent advances in image super-resolution with deep learning.\nAlthough there are some existing SR surveys in literature, our work differs in that we are focused in deep learning based SR techniques, while most of the earlier works  aim at surveying traditional SR algorithms or some studies mainly concentrate on providing quantitative evaluations based on full-reference metrics or human visual perception .\nUnlike the existing surveys, this survey takes a unique deep learning based perspective to review the recent advances of SR techniques in a systematic and comprehensive manner.\nThe main contributions of this survey are three-fold:\n\\begin{enumerate}\n    \\item We give a comprehensive review of image super-resolution techniques based on deep learning, including problem settings, benchmark datasets, performance metrics, a family of SR methods with deep learning, domain-specific SR applications, etc.\n    \\item We provide a systematic overview of recent advances of deep learning based SR techniques in a hierarchical and structural manner, and summarize the advantages and limitations of each component for an effective SR solution.\n    \\item We discuss the challenges and open issues, and identify the new trends and future directions to provide an insightful guidance for the community.\n\\end{enumerate}\nIn the following sections, we will cover various aspects of recent advances in image super-resolution with deep learning.\nFig. \\ref{fig_sr_methodology} shows the taxonomy of image SR to be covered in this survey in a hierarchically-structured way.\nSection 2 gives the problem definition and reviews the mainstream datasets and evaluation metrics.\nSection 3 analyzes main components of supervised SR modularly.\nSection 4 gives a brief introduction to unsupervised SR methods.\nSection 5 introduces some popular domain-specific SR applications, and Section 6 discusses future directions and open issues.", "cites": [7029, 7028, 480, 483, 481, 482, 7030, 7254, 8358], "cite_extract_rate": 0.23684210526315788, "origin_cites_number": 38, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a good synthesis by categorizing SR techniques into supervised, unsupervised, and domain-specific approaches and highlighting the evolution from early CNN-based methods to more advanced GAN-based ones. It also introduces broader themes such as loss functions and network architectures, which show abstraction. However, while it mentions limitations like over-smoothed images and computational costs, it does not deeply critique or compare specific cited works."}}
{"id": "53549365-a742-4965-8411-f673b3d3f2e5", "title": "Problem Definitions", "level": "subsection", "subsections": [], "parent_id": "a6cc8412-517b-4e0a-a1f4-0cf417b15a19", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Problem Setting and Terminology"], ["subsection", "Problem Definitions"]], "content": "\\label{sec_problem_definitions}\nImage super-resolution aims at recovering the corresponding HR images from the LR images.\nGenerally, the LR image $I_x$ is modeled as the output of the following degradation:\n\\begin{equation}\n    I_x = \\mathcal{D} (I_y ; \\delta),\n\\end{equation}\nwhere $\\mathcal{D}$ denotes a degradation mapping function, $I_y$ is the corresponding HR image and $\\delta$ is the parameters of the degradation process (e.g., the scaling factor or noise).\nGenerally, the degradation process (i.e., $\\mathcal{D}$ and $\\delta$) is unknown and only LR images are provided.\nIn this case, also known as blind SR, researchers are required to recover an HR approximation $\\hat{I_y}$ of the ground truth HR image $I_y$ from the LR image $I_x$, following:\n\\begin{equation}\n    \\hat{I_y} = \\mathcal{F} (I_x ; \\theta),\n\\end{equation}\nwhere $\\mathcal{F}$ is the super-resolution model and $\\theta$ denotes the parameters of $\\mathcal{F}$.\nAlthough the degradation process is unknown and can be affected by various factors (e.g., compression artifacts, anisotropic degradations, sensor noise and speckle noise), researchers are trying to model the degradation mapping.\nMost works directly model the degradation as a single downsampling operation, as follows:\n\\begin{equation}\n    \\label{eq_degradation_impl_simple}\n    \\mathcal{D} (I_y ; \\delta) = (I_y) \\downarrow_s, \\{s\\} \\subset \\delta,\n\\end{equation}\nwhere $\\downarrow_s$ is a downsampling operation with the scaling factor $s$.\nAs a matter of fact, most datasets for generic SR are built based on this pattern, and the most commonly used downsampling operation is bicubic interpolation with anti-aliasing.\nHowever, there are other works  modelling the degradation as a combination of several operations:\n\\begin{equation}\n    \\label{eq_degradation_impl_combine}\n    \\mathcal{D} (I_y ; \\delta) = (I_y \\otimes \\kappa) \\downarrow_s + n_\\varsigma, \\{\\kappa,s,\\varsigma\\} \\subset \\delta,\n\\end{equation}\nwhere $I_y \\otimes \\kappa$ represents the convolution between a blur kernel $\\kappa$ and the HR image $I_y$, and $n_\\varsigma$ is some additive white Gaussian noise with standard deviation $\\varsigma$.\nCompared to the naive definition of Eq. \\ref{eq_degradation_impl_simple}, the combinative degradation pattern of Eq. \\ref{eq_degradation_impl_combine} is closer to real-world cases and has been shown to be more beneficial for SR .\nTo this end, the objective of SR is as follows:\n\\begin{equation}\n    \\hat{\\theta} = \\mathop{\\arg \\min}_{\\theta} \\mathcal{L} (\\hat{I_y}, I_y) + \\lambda \\Phi (\\theta),\n\\end{equation}\nwhere $\\mathcal{L} (\\hat{I_y}, I_y)$ represents the loss function between the generated HR image $\\hat{I_y}$ and the ground truth image $I_y$, $\\Phi (\\theta)$ is the regularization term and $\\lambda$ is the tradeoff parameter.\nAlthough the most popular loss function for SR is pixel-wise mean squared error (i.e., pixel loss), more powerful models tend to use a combination of multiple loss functions, which will be covered in Sec. \\ref{sec_loss}.", "cites": [484], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the core concepts of image super-resolution by integrating the degradation modeling approaches from cited papers into a coherent mathematical framework. It provides a critical evaluation by pointing out limitations in assuming bicubic downsampling and highlights the benefits of more realistic degradation models. The abstraction level is strong, as it generalizes degradation patterns and introduces a unified objective function, offering a meta-level understanding of SR methodologies."}}
{"id": "d1d57480-8772-4298-b729-a54a8a65eac9", "title": "Datasets for Super-resolution", "level": "subsection", "subsections": [], "parent_id": "a6cc8412-517b-4e0a-a1f4-0cf417b15a19", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Problem Setting and Terminology"], ["subsection", "Datasets for Super-resolution"]], "content": "\\label{sec_datasets}\n\\begin{table*}[t]\n    \\renewcommand{\\arraystretch}{1.3}\n    \\caption{List of public image datasets for super-resolution benchmarks.}\n    \\label{table_datasets}\n    \\centering\n    \\begin{tabular}{|l|c|c|c|c|l|}\n        \\hline\n        Dataset & Amount & Avg. Resolution & Avg. Pixels & Format & Category Keywords \\\\\n        \\hline\n        BSDS300          &   $300$ & $( 435,  367)$ &    $154,401$ & JPG & animal, building, food, landscape, people, plant, etc. \\\\\n        BSDS500        &   $500$ & $( 432,  370)$ &    $154,401$ & JPG & animal, building, food, landscape, people, plant, etc. \\\\\n        DIV2K           &  $1000$ & $(1972, 1437)$ &  $2,793,250$ & PNG & environment, flora, fauna, handmade object, people, scenery, etc. \\\\\n        General-100    &   $100$ & $( 435,  381)$ &    $181,108$ & BMP & animal, daily necessity, food, people, plant, texture, etc. \\\\\n        L20                &    $20$ & $(3843, 2870)$ & $11,577,492$ & PNG & animal, building, landscape, people, plant, etc. \\\\\n        Manga109      &   $109$ & $( 826, 1169)$ &    $966,011$ & PNG & manga volume \\\\\n        OutdoorScene     & $10624$ & $( 553,  440)$ &    $249,593$ & PNG & animal, building, grass, mountain, plant, sky, water \\\\\n        PIRM                  &   $200$ & $( 617,  482)$ &    $292,021$ & PNG & environments, flora, natural scenery, objects, people, etc. \\\\\n        Set5              &     $5$ & $( 313,  336)$ &    $113,491$ & PNG & baby, bird, butterfly, head, woman \\\\\n        Set14               &    $14$ & $( 492,  446)$ &    $230,203$ & PNG & humans, animals, insects, flowers, vegetables, comic, slides, etc. \\\\\n        T91                    &    $91$ & $( 264,  204)$ &     $58,853$ & PNG & car, flower, fruit, human face, etc. \\\\\n        Urban100            &   $100$ & $( 984,  797)$ &    $774,314$ & PNG & architecture, city, structure, urban, etc. \\\\\n        \\hline\n    \\end{tabular}\n\\end{table*}\nToday there are a variety of datasets available for image super-resolution, which greatly differ in image amounts, quality, resolution, and diversity, etc.\nSome of them provide LR-HR image pairs, while others only provide HR images, in which case the LR images are typically obtained by \\textit{imresize} function with default settings in MATLAB (i.e., bicubic interpolation with anti-aliasing).\nIn Table \\ref{table_datasets} we list a number of image datasets commonly used by the SR community, and specifically indicate their amounts of HR images, average resolution, average numbers of pixels, image formats, and category keywords.\nBesides these datasets, some datasets widely used for other vision tasks are also employed for SR, such as ImageNet , MS-COCO , VOC2012 , CelebA .  \nIn addition, combining multiple datasets for training is also popular, such as combining T91 and BSDS300 , combining DIV2K and Flickr2K .", "cites": [485, 7031, 7256, 7255, 7028, 7032, 482, 487, 7030, 486], "cite_extract_rate": 0.45454545454545453, "origin_cites_number": 22, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes publicly available datasets for super-resolution, listing their properties and some common practices (e.g., using imresize for LR generation). While it references several papers, it does so in a minimal and non-integrative manner, focusing on the datasets themselves rather than synthesizing insights from the cited works. There is little critical evaluation or abstraction of broader trends in the field."}}
{"id": "f971a9d2-8d17-455c-93e6-da6aefec6037", "title": "Mean Opinion Score", "level": "subsubsection", "subsections": [], "parent_id": "37eac979-1c0e-4e61-9440-a289c60a8cdd", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Problem Setting and Terminology"], ["subsection", "Image Quality Assessment"], ["subsubsection", "Mean Opinion Score"]], "content": "\\label{sec_iqa_mos}\nMean opinion score (MOS) testing is a commonly used subjective IQA method, where human raters are asked to assign perceptual quality scores to tested images.\nTypically, the scores are from $1$ (bad) to $5$ (good).\nAnd the final MOS is calculated as the arithmetic mean over all ratings.\nAlthough the MOS testing seems a faithful IQA method, it has some inherent defects, such as non-linearly perceived scales, biases and variance of rating criteria.\nIn reality, there are some SR models performing poorly in common IQA metrics (e.g., PSNR) but far exceeding others in terms of perceptual quality, in which case the MOS testing is the most reliable IQA method for accurately measuring the perceptual quality .", "cites": [489, 8359, 488, 487, 483], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the Mean Opinion Score (MOS) as a subjective IQA method, highlighting its reliability in measuring perceptual quality despite limitations in linear perception and variability in human ratings. It connects the concept of MOS with the shortcomings of objective metrics like PSNR, referencing multiple SR papers that emphasize perceptual realism. While it integrates some key ideas, it lacks deeper synthesis into a novel framework and offers only moderate abstraction and critique."}}
{"id": "98ccac76-f3e5-49c7-8cc8-645bbdba6750", "title": "Learning-based Perceptual Quality", "level": "subsubsection", "subsections": [], "parent_id": "37eac979-1c0e-4e61-9440-a289c60a8cdd", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Problem Setting and Terminology"], ["subsection", "Image Quality Assessment"], ["subsubsection", "Learning-based Perceptual Quality"]], "content": "\\label{sec_iqa_learning_perceptual}\nIn order to better assess the image perceptual quality while reducing manual intervention, researchers try to assess the perceptual quality by learning on large datasets.\nSpecifically, Ma \\textit{et al.}  and Talebi \\textit{et al.}  propose no-reference Ma and NIMA, respectively, which are learned from visual perceptual scores and directly predict the quality scores without ground-truth images.\nIn contrast, Kim \\textit{et al.}  propose DeepQA, which predicts visual similarity of images by training on triplets of distorted images, objective error maps, and subjective scores.\nAnd Zhang \\textit{et al.}  collect a large-scale perceptual similarity dataset, evaluate the perceptual image patch similarity (LPIPS) according to the difference in deep features by trained deep networks, and show that the deep features learned by CNNs model perceptual similarity much better than measures without CNNs.\nAlthough these methods exhibit better performance on capturing human visual perception, what kind of perceptual quality we need (e.g., more realistic images, or consistent identity to the original image) remains a question to be explored, thus the objective IQA methods (e.g., PSNR, SSIM) are still the mainstreams currently.", "cites": [491, 8360, 490], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.3, "critical": 3.0, "abstraction": 3.3}, "insight_level": "medium", "analysis": "The section synthesizes information from three different no-reference perceptual quality assessment methods, integrating their approaches and purposes (e.g., predicting quality scores vs. perceptual similarity). It includes a critical note on the unresolved question of what kind of perceptual quality is needed in SR tasks, highlighting limitations in current methods. While it begins to abstract the broader trend of using deep features to model perceptual similarity, it does not offer a deeper, meta-level framework or extensive comparative analysis."}}
{"id": "c7f6f3a2-41a1-48d0-82c4-2fd4aa7a7551", "title": "Task-based Evaluation", "level": "subsubsection", "subsections": [], "parent_id": "37eac979-1c0e-4e61-9440-a289c60a8cdd", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Problem Setting and Terminology"], ["subsection", "Image Quality Assessment"], ["subsubsection", "Task-based Evaluation"]], "content": "\\label{sec_iqa_task}\nAccording to the fact that SR models can often help other vision tasks , evaluating reconstruction performance by means of other tasks is another effective way.  \nSpecifically, researchers feed the original and the reconstructed HR images into trained models, and evaluate the reconstruction quality by comparing the impacts on the prediction performance.\nThe vision tasks used for evaluation include object recognition , face recognition , face alignment and parsing , etc.", "cites": [7029, 7033, 492, 493, 483], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly introduces task-based evaluation for SR but primarily functions as a descriptive overview of the concept. It lists the types of vision tasks used (e.g., object recognition, face recognition) and references relevant papers without critically analyzing their approaches or limitations. There is minimal synthesis or abstraction, as the discussion remains superficial and does not connect the cited works into a broader conceptual framework."}}
{"id": "3f34f12a-efa9-4a1c-90a0-042ac955589a", "title": "Operating Channels", "level": "subsection", "subsections": [], "parent_id": "a6cc8412-517b-4e0a-a1f4-0cf417b15a19", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Problem Setting and Terminology"], ["subsection", "Operating Channels"]], "content": "In addition to the commonly used RGB color space, the YCbCr color space is also widely used for SR.  \nIn this space, images are represented by Y, Cb, Cr channels, denoting the luminance, blue-difference and red-difference chroma components, respectively.\nAlthough currently there is no accepted best practice for performing or evaluating super-resolution on which space, earlier models favor operating on the Y channel of YCbCr space , while more recent models tend to operate on RGB channels .\nIt is worth noting that operating (training or evaluation) on different color spaces or channels can make the evaluation results differ greatly (up to $4$ dB) .", "cites": [8358, 7256, 481, 493, 482, 7255, 7028], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the operating channels used in SR models, referencing multiple papers to highlight a shift from YCbCr to RGB color spaces. It synthesizes the information by connecting trends in different works and emphasizes the impact of color space choice on performance. While it offers a comparison between earlier and more recent models, it lacks deeper critical evaluation of the cited works' methodologies or limitations."}}
{"id": "907bce52-c45d-42aa-9cd6-d94aed9e1c55", "title": "Super-resolution Challenges", "level": "subsection", "subsections": [], "parent_id": "a6cc8412-517b-4e0a-a1f4-0cf417b15a19", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Problem Setting and Terminology"], ["subsection", "Super-resolution Challenges"]], "content": "In this section, we will briefly introduce two most popular challenges for image SR, NTIRE  and PIRM .\n\\textbf{NTIRE Challenge.}\nThe New Trends in Image Restoration and Enhancement (NTIRE) challenge  is in conjunction with CVPR and includes multiple tasks like SR, denoising and colorization.\nFor image SR, the NTIRE challenge is built on the DIV2K  dataset and consists of bicubic downscaling tracks and blind tracks with realistic unknown degradation.\nThese tracks differs in degradations and scaling factors, and aim to promote the SR research under both ideal conditions and real-world adverse situations.\n\\textbf{PIRM Challenge.}\nThe Perceptual Image Restoration and Manipulation (PIRM) challenges are in conjunction with ECCV and also includes multiple tasks.\nIn contrast to NTIRE, one sub-challenge  of PIRM focuses on the tradeoff between generation accuracy and perceptual quality, and the other  focuses on SR on smartphones.\nAs is well-known , the models target for distortion frequently produce visually unpleasing results, while the models target for perceptual quality performs poorly on information fidelity.\nSpecifically, the PIRM divided the perception-distortion plane into three regions according to thresholds on root mean squared error (RMSE).\nIn each region, the winning algorithm is the one that achieves the best perceptual quality , evaluated by NIQE  and Ma .\nWhile in the other sub-challenge , SR on smartphones, participants are asked to perform SR with limited smartphone hardwares (including CPU, GPU, RAM, etc.), and the evaluation metrics include PSNR, MS-SSIM and MOS testing.\nIn this way, PIRM encourages advanced research on the perception-distortion tradeoff, and also drives lightweight and efficient image enhancement on smartphones.", "cites": [490, 494], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of the NTIRE and PIRM challenges, integrating basic information from the cited papers to outline their focus and evaluation strategies. While it mentions the perception-distortion tradeoff and the constraints of smartphone-based SR, it lacks deeper synthesis of ideas or critical evaluation of the approaches. It offers some level of categorization but does not abstract to broader principles or frameworks in image super-resolution research."}}
{"id": "519339a2-242b-44d5-84f0-42bc4f8e8d0e", "title": "Pre-upsampling Super-resolution", "level": "subsubsection", "subsections": [], "parent_id": "689493e0-ec64-420a-b3d0-7d947940e262", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Super-resolution Frameworks"], ["subsubsection", "Pre-upsampling Super-resolution"]], "content": "\\label{sec_framework_pre}\nOn account of the difficulty of directly learning the mapping from low-dimensional space to high-dimensional space, utilizing traditional upsampling algorithms to obtain higher-resolution images and then refining them using deep neural networks is a straightforward solution.\nThus Dong \\textit{et al.}  firstly adopt the pre-upsampling SR framework (as Fig. \\ref{fig_framework_pre} shows) and propose SRCNN to learn an end-to-end mapping from interpolated LR images to HR images.\nSpecifically, the LR images are upsampled to coarse HR images with the desired size using traditional methods (e.g., bicubic interpolation), then deep CNNs are applied on these images for reconstructing high-quality details.\nSince the most difficult upsampling operation has been completed, CNNs only need to refine the coarse images, which significantly reduces the learning difficulty.\nIn addition, these models can take interpolated images with arbitrary sizes and scaling factors as input, and give refined results with comparable performance to single-scale SR models .\nThus it has gradually become one of the most popular frameworks , and the main differences between these models are the posterior model design (Sec. \\ref{sec_network_design}) and learning strategies (Sec. \\ref{sec_learning_strategies}).\nHowever, the predefined upsampling often introduce side effects (e.g., noise amplification and blurring), and since most operations are performed in high-dimensional space, the cost of time and space is much higher than other frameworks .", "cites": [7031, 495, 8358, 7256, 7028], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic description of the pre-upsampling super-resolution framework and references multiple papers, but it does not effectively synthesize or connect the ideas across these papers. It briefly mentions limitations (e.g., noise amplification, higher computational cost), but lacks deeper critical analysis or comparison. The abstraction is minimal, as it focuses on the general structure of the framework without identifying broader trends or principles in the field."}}
{"id": "49399822-0598-45df-ac83-415cc84fbe69", "title": "Post-upsampling Super-resolution", "level": "subsubsection", "subsections": [], "parent_id": "689493e0-ec64-420a-b3d0-7d947940e262", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Super-resolution Frameworks"], ["subsubsection", "Post-upsampling Super-resolution"]], "content": "\\label{sec_framework_post}\nIn order to improve the computational efficiency and make full use of deep learning technology to increase resolution automatically, researchers propose to perform most computation in low-dimensional space by replacing the predefined upsampling with end-to-end learnable layers integrated at the end of the models.\nIn the pioneer works  of this framework, namely post-upsampling SR as Fig. \\ref{fig_framework_post} shows, the LR input images are fed into deep CNNs without increasing resolution, and end-to-end learnable upsampling layers are applied at the end of the network.\nSince the feature extraction process with huge computational cost only occurs in low-dimensional space and the resolution increases only at the end, the computation and spatial complexity are much reduced.\nTherefore, this framework also has become one of the most mainstream frameworks .\nThese models differ mainly in the learnable upsampling layers (Sec. \\ref{sec_upsampling_methods}), anterior CNN structures (Sec. \\ref{sec_network_design}) and learning strategies ({Sec. \\ref{sec_learning_strategies}), etc.", "cites": [7256, 482, 496], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of post-upsampling super-resolution frameworks and mentions the main differences between models, such as upsampling layers, CNN structures, and learning strategies. However, it lacks deeper synthesis of the cited works, critical evaluation of their strengths or weaknesses, and broader abstraction or generalization of the underlying principles. It primarily serves as a factual overview rather than offering analytical or comparative insights."}}
{"id": "5b9686f8-a7e5-4eb6-9df5-54d91e1f02d5", "title": "Progressive Upsampling Super-resolution", "level": "subsubsection", "subsections": [], "parent_id": "689493e0-ec64-420a-b3d0-7d947940e262", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Super-resolution Frameworks"], ["subsubsection", "Progressive Upsampling Super-resolution"]], "content": "\\label{sec_framework_progressive}\nAlthough post-upsampling SR framework has immensely reduced the computational cost, it still has some shortcomings.\nOn the one hand, the upsampling is performed in only one step, which greatly increases the learning difficulty for large scaling factors (e.g., 4, 8).\nOn the other hand, each scaling factor requires training an individual SR model, which cannot cope with the need for multi-scale SR.\nTo address these drawbacks, a progressive upsampling framework is adopted by Laplacian pyramid SR network (LapSRN) , as Fig. \\ref{fig_framework_progressive} shows.\nSpecifically, the models under this framework are based on a cascade of CNNs and progressively reconstruct higher-resolution images.\nAt each stage, the images are upsampled to higher resolution and refined by CNNs.\nOther works such as MS-LapSRN  and progressive SR (ProSR)  also adopt this framework and achieve relatively high performance.\nIn contrast to the LapSRN and MS-LapSRN using the intermediate reconstructed images as the ``base images'' for subsequent modules, the ProSR keeps the main information stream and reconstructs intermediate-resolution images by individual heads.\nBy decomposing a difficult task into simple tasks, the models under this framework greatly reduce the learning difficulty, especially with large factors, and also cope with the multi-scale SR without introducing overmuch spacial and temporal cost.\nIn addition, some specific learning strategies such as curriculum learning (Sec. \\ref{sec_curriculum_learning}) and multi-supervision (Sec. \\ref{sec_multi_supervision}) can be directly integrated to further reduce learning difficulty and improve final performance.\nHowever, these models also encounter some problems, such as the complicated model designing for multiple stages and the training stability, and more modelling guidance and more advanced training strategies are needed.", "cites": [7030, 489, 7254], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple progressive upsampling SR approaches, connecting the core idea of multi-stage refinement across LapSRN, MS-LapSRN, and ProSR. It provides a critical analysis by highlighting the limitations of single-step upsampling and the challenges in model design and training stability. The abstraction is strong as it generalizes the concept of decomposing complex tasks into simpler ones, and notes broader learning strategies like curriculum learning and multi-supervision that can be integrated."}}
{"id": "c84341e6-19ad-464b-80f1-c98cb381ad5f", "title": "Iterative Up-and-down Sampling Super-resolution", "level": "subsubsection", "subsections": [], "parent_id": "689493e0-ec64-420a-b3d0-7d947940e262", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Super-resolution Frameworks"], ["subsubsection", "Iterative Up-and-down Sampling Super-resolution"]], "content": "\\label{sec_framework_up_down}\nIn order to better capture the mutual dependency of LR-HR image pairs, an efficient iterative procedure named back-projection  is incorporated into SR .\nThis SR framework, namely iterative up-and-down sampling SR (as Fig. \\ref{fig_framework_up_down} shows), tries to iteratively apply back-projection refinement, i.e., computing the reconstruction error then fusing it back to tune the HR image intensity.\nSpecifically, Haris \\textit{et al.}  exploit iterative up-and-down sampling layers and propose DBPN, which connects upsampling and downsampling layers alternately and reconstructs the final HR result using all of the intermediately reconstructions.\nSimilarly, the SRFBN  employs a iterative up-and-down sampling feedback block with more dense skip connections and learns better representations.\nAnd the RBPN  for video super-resolution extracts context from continuous video frames and combines these context to produce recurrent output frames by a back-projection module.\nThe models under this framework can better mine the deep relationships between LR-HR image pairs and thus provide higher-quality reconstruction results.\nNevertheless, the design criteria of the back-projection modules are still unclear.\nSince this mechanism has just been introduced into deep learning-based SR, the framework has great potential and needs further exploration.", "cites": [498, 497, 7032, 7255], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers into a coherent narrative about the iterative up-and-down sampling framework in SR, highlighting the role of back-projection refinement. It provides some critical perspective by noting the unclear design criteria of back-projection modules and the novelty of the mechanism in deep learning-based SR. The abstraction is moderate, as it identifies a broader pattern in using feedback mechanisms but does not reach a meta-level theoretical understanding."}}
{"id": "e83198c5-8c15-4bd4-bf6e-cc354c2ace47", "title": "Learning-based Upsampling", "level": "subsubsection", "subsections": [], "parent_id": "f6adcd75-0abb-4db7-a8b4-0d6afdfa320e", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Upsampling Methods"], ["subsubsection", "Learning-based Upsampling"]], "content": "\\label{sec_learning_based_upsampling}\n\\begin{figure}[!t]\n    \\centering\n    \\subfloat[Starting]{\\includegraphics[height=60pt]{resource/fig_deconv_0}\n    \\label{fig_deconv_0}}\n    \\hfil\n    \\subfloat[Expanding]{\\includegraphics[height=60pt]{resource/fig_deconv_1}\n    \\label{fig_deconv_1}}\n    \\hfil\n    \\subfloat[Convolution]{\\includegraphics[height=60pt]{resource/fig_deconv_2}\n    \\label{fig_deconv_2}}\n    \\caption{\n        Transposed convolution layer.\n        The \\textcolor{CornflowerBlue}{blue} boxes denote the input, and the \\textcolor{LimeGreen}{green} boxes indicate the kernel and the convolution output.\n    }\n    \\label{fig_deconv}\n\\end{figure}\n\\begin{figure}[!t]\n    \\centering\n    \\subfloat[Starting]{\\includegraphics[height=45pt]{resource/fig_sub_pixel_0}\n    \\label{fig_sub_pixel_0}}\n    \\hfil\n    \\subfloat[Convolution]{\\includegraphics[height=45pt]{resource/fig_sub_pixel_1}\n    \\label{fig_sub_pixel_1}}\n    \\hfil\n    \\subfloat[Reshaping]{\\includegraphics[height=45pt]{resource/fig_sub_pixel_2}\n    \\label{fig_sub_pixel_2}}\n    \\caption{\n        Sub-pixel layer.\n        The \\textcolor{CornflowerBlue}{blue} boxes denote the input, and the boxes with other colors indicate different convolution operations and different output feature maps.\n    }\n    \\label{fig_sub_pixel}\n\\end{figure}\n\\begin{figure}[!t]\n    \\centering\n    {\n        \\setlength{\\fboxrule}{1pt}\n        {\\includegraphics[width=0.35\\textwidth]{resource/fig_meta_upscale}\n        }\n    }\n    \\caption{\n        Meta upscale module.\n        The \\textcolor{CornflowerBlue}{blue} boxes denote the projection patch, and the \\textcolor{LimeGreen}{green} boxes and lines indicate the convolution operation with predicted weights.\n    }\n    \\label{fig_meta_upscale}\n\\end{figure}\nIn order to overcome the shortcomings of interpolation-based methods and learn upsampling in an end-to-end manner, transposed convolution layer and sub-pixel layer are introduced into the SR field.\n\\textbf{Transposed Convolution Layer.}\nTransposed convolution layer, a.k.a. deconvolution layer , tries to perform transformation opposite a normal convolution, i.e., predicting the possible input based on feature maps sized like convolution output.\nSpecifically, it increases the image resolution by expanding the image by inserting zeros and performing convolution.\nTaking $2\\times$ SR with $3 \\times 3$ kernel as example (as Fig. \\ref{fig_deconv} shows), the input is firstly expanded twice of the original size, where the added pixel values are set to $0$ (Fig. \\ref{fig_deconv_1}).\nThen a convolution with kernel sized $3 \\times 3$, stride $1$ and padding $1$ is applied (Fig. \\ref{fig_deconv_2}).\nIn this way, the input is upsampled by a factor of 2, in which case the receptive field is at most $2 \\times 2$.\nSince the transposed convolution enlarges the image size in an end-to-end manner while maintaining a connectivity pattern compatible with vanilla convolution, it is widely used as upsampling layers in SR models .\nHowever, this layer can easily cause ``uneven overlapping'' on each axis , and the multiplied results on both axes further create a checkerboard-like pattern of varying magnitudes and thus hurt the SR performance.\n\\textbf{Sub-pixel Layer.}\nThe sub-pixel layer , another end-to-end learnable upsampling layer, performs upsampling by generating a plurality of channels by convolution and then reshaping them, as Fig. \\ref{fig_sub_pixel} shows.\nWithin this layer, a convolution is firstly applied for producing outputs with $s^2$ times channels, where $s$ is the scaling factor (Fig. \\ref{fig_sub_pixel_1}).\nAssuming the input size is $h \\times w \\times c$, the output size will be $h \\times w \\times s^2 c$.\nAfter that, the reshaping operation (a.k.a. \\textit{shuffle} ) is performed to produce outputs with size $sh \\times sw \\times c$ (Fig. \\ref{fig_sub_pixel_2}).\nIn this case, the receptive field can be up to $3 \\times 3$.\nDue to the end-to-end upsampling manner, this layer is also widely used by SR models .\nCompared with transposed convolution layer, the sub-pixel layer has a larger receptive field, which provides more contextual information to help generate more realistic details.\nHowever, since the distribution of the receptive fields is uneven and blocky regions actually share the same receptive field, it may result in some artifacts near the boundaries of different blocks.\nOn the other hand, independently predicting adjacent pixels in a blocky region may cause unsmooth outputs.\nThus Gao \\textit{et al.}  propose PixelTCL, which replaces the independent prediction to interdependent sequential prediction, and produces smoother and more consistent results.\n\\textbf{Meta Upscale Module.}\nÂ The previous methods need to predefine the scaling factors, i.e., training different upsampling modules for different factors, which is inefficient and not in line with real needs.\nSo that Hu \\textit{et al.}  propose meta upscale module (as Fig. \\ref{fig_meta_upscale} shows), which firstly solves SR of arbitrary scaling factors based on meta learning.\nSpecifically, for each target position on the HR images, this module project it to a small patch on the LR feature maps (i.e., $k \\times k \\times c_{in}$), predicts convolution weights (i.e., $k \\times k \\times c_{in} \\times c_{out}$) according to the projection offsets and the scaling factor by dense layers and perform convolution.\nIn this way, the meta upscale module can continuously zoom in it with arbitrary factors by a single model.\nAnd due to the large amount of training data (multiple factors are simultaneously trained), the module can exhibit comparable or even better performance on fixed factors.\nAlthough this module needs to predict weights during inference, the execution time of the upsampling module only accounts for about 1\\% of the time of feature extraction .\nHowever, this method predicts a large number of convolution weights for each target pixel based on several values independent of the image contents, so the prediction result may be unstable and less efficient when faced with larger magnifications.\nNowadays, these learning-based layers have become the most widely used upsampling methods.\nEspecially in the post-upsampling framework (Sec. \\ref{sec_framework_post}), these layers are usually used in the final upsampling phase for reconstructing HR images based on high-level representations extracted in low-dimensional space, and thus achieve end-to-end SR while avoiding overwhelming operations in high-dimensional space.", "cites": [499, 7034, 484, 7257, 481, 7255, 496], "cite_extract_rate": 0.5, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key learning-based upsampling techniques (transposed convolution, sub-pixel, and meta upscale) by connecting their mechanisms and performance implications. It critically evaluates their pros and cons, such as checkerboard artifacts and receptive field limitations. The discussion abstracts from individual papers to highlight broader design choices and efficiency considerations in SR frameworks."}}
{"id": "e2bf59f0-9d13-40a4-aacd-65cd2ed1ed45", "title": "Residual Learning", "level": "subsubsection", "subsections": [], "parent_id": "40ccdbeb-49e2-41b8-849a-4470b011cae3", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Network Design"], ["subsubsection", "Residual Learning"]], "content": "\\label{sec_residual_learning}\nBefore He \\textit{et al.}  propose ResNet for learning residuals instead of a thorough mapping, residual learning has been widely employed by SR models , as Fig. \\ref{fig_residual_learning} shows.\nAmong them, the residual learning strategies can be roughly divided into global and local residual learning.\n\\textbf{Global Residual Learning.}\nSince the image SR is an image-to-image translation task where the input image is highly correlated with the target image, researchers try to learn only the residuals between them, namely global residual learning.\nIn this case, it avoids learning a complicated transformation from a complete image to another, instead only requires learning a residual map to restore the missing high-frequency details.\nSince the residuals in most regions are close to zero, the model complexity and learning difficulty are greatly reduced.\nThus it is widely used by SR models .\n\\textbf{Local Residual Learning.}\nThe local residual learning is similar to the residual learning in ResNet  and used to alleviate the degradation problem  caused by ever-increasing network depths, reduce training difficulty and improve the learning ability.\nIt is also widely used for SR .\nIn practice, the above methods are both implemented by shortcut connections (often scaled by a small constant) and element-wise addition, while the difference is that the former directly connects the input and output images, while the latter usually adds multiple shortcuts between layers with different depths inside the network.", "cites": [97, 7031, 500, 493, 7028, 496], "cite_extract_rate": 0.5, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the concept of residual learning from multiple SR papers, integrating it into a structured discussion of global and local strategies. It abstracts the idea into a general framework involving shortcut connections and element-wise addition. However, it lacks deeper critical analysis, such as evaluating the trade-offs between these strategies or the limitations of the cited methods."}}
{"id": "8f7faf9a-44bf-4a4a-a876-6b36ac5bd458", "title": "Recursive Learning", "level": "subsubsection", "subsections": [], "parent_id": "40ccdbeb-49e2-41b8-849a-4470b011cae3", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Network Design"], ["subsubsection", "Recursive Learning"]], "content": "\\label{sec_recursive_learning}\nIn order to learn higher-level features without introducing overwhelming parameters, recursive learning, which means applying the same modules multiple times in a recursive manner, is introduced into the SR field, as Fig. \\ref{fig_recursive_learning} shows.\nAmong them, the 16-recursive DRCN employs a single convolutional layer as the recursive unit and reaches a receptive field of $41 \\times 41$, which is much larger than $13 \\times 13$ of SRCNN , without over many parameters.\nThe DRRN  uses a ResBlock  as the recursive unit for 25 recursions and obtains even better performance than the 17-ResBlock baseline.\nLater Tai \\textit{et al.}  propose MemNet based on the memory block, which is composed of a 6-recursive ResBlock where the outputs of every recursion are concatenated and go through an extra $1 \\times 1$ convolution for memorizing and forgetting.\nThe cascading residual network (CARN)  also adopts a similar recursive unit including several ResBlocks.\nRecently, Li \\textit{et al.}  employ iterative up-and-down sampling SR framework, and propose a feedback network based on recursive learning, where the weights of the entire network are shared across all recursions.\nBesides, researchers also employ different recursive modules in different parts.\nSpecifically, Han \\textit{et al.}  propose dual-state recurrent network (DSRN) to exchange signals between the LR and HR states.\nAt each time step (i.e., recursion), the representations of each branch are updated and exchanged for better exploring LR-HR relationships.\nSimilarly, Lai \\textit{et al.}  employ the embedding and upsampling modules as recursive units, and thus much reduce the model size at the expense of little performance loss.  \nIn general, the recursive learning can indeed learn more advanced representations without introducing excessive parameters, but still can't avoid high computational costs.\nAnd it inherently brings vanishing or exploding gradient problems, consequently some techniques such as residual learning (Sec. \\ref{sec_residual_learning}) and multi-supervision (Sec. \\ref{sec_multi_supervision}) are often integrated with recursive learning for mitigating these problems .", "cites": [489, 97, 7031, 495, 497, 481, 496], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes recursive learning approaches across several key papers, showing how similar concepts are applied in different models (e.g., DRCN, DRRN, MemNet). It also critically addresses limitations such as high computational costs and vanishing/exploding gradients, suggesting mitigation strategies like residual learning and multi-supervision. While it identifies patterns in the use of recursion, it stops short of offering a meta-level framework or deeper theoretical analysis."}}
{"id": "1dadc8f5-aa09-4f5b-9d6f-f5a916d558a5", "title": "Multi-path Learning", "level": "subsubsection", "subsections": [], "parent_id": "40ccdbeb-49e2-41b8-849a-4470b011cae3", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Network Design"], ["subsubsection", "Multi-path Learning"]], "content": "\\label{sec_multi_path_learning}\nMulti-path learning refers to passing features through multiple paths, which perform different operations, and fusing them back for providing better modelling capabilities.\nSpecifically, it could be divided into global, local and scale-specific multi-path learning, as bellow.\n\\textbf{Global Multi-path Learning.}\nGlobal multi-path learning refers to making use of multiple paths to extract features of different aspects of the images.\nThese paths can cross each other in the propagation and thus greatly enhance the learning ability.\nSpecifically, the LapSRN  includes a feature extraction path predicting the sub-band residuals in a coarse-to-fine fashion and another path to reconstruct HR images based on the signals from both paths.\nSimilarly, the DSRN  utilizes two paths to extract information in low-dimensional and high-dimensional space, respectively, and continuously exchanges information for further improving learning ability.\nAnd the pixel recursive super-resolution  adopts a conditioning path to capture the global structure of images, and a prior path to capture the serial dependence of generated pixels.\nIn contrast, Ren \\textit{et al.}  employ multiple paths with unbalanced structures to perform upsampling and fuse them at the end of the model.\n\\textbf{Local Multi-path Learning.}\nMotivated by the inception module , the MSRN  adopts a new block for multi-scale feature extraction, as Fig. \\ref{fig_multi_path_learning} shows.\nIn this block, two convolution layers with kernel size $3\\times 3$ and $5\\times 5$ are adopted to extract features simultaneously, then the outputs are concatenated and go through the same operations again, and finally an extra $1\\times 1$ convolution is applied.\nA shortcut connects the input and output by element-wise addition.\nThrough such local multi-path learning, the SR models can better extract image features from multiple scales and further improve performance.\n\\textbf{Scale-specific Multi-path Learning.}\nConsidering that SR models for different scales need to go through similar feature extraction, Lim \\textit{et al.}  propose scale-specific multi-path learning to cope with multi-scale SR with a single network.\nTo be concrete, they share the principal components of the model (i.e., the intermediate layers for feature extraction), and attach scale-specific pre-processing paths and upsampling paths at the beginning and the end of the network, respectively (as Fig. \\ref{fig_multi_path_learning_specific} shows).\nDuring training, only the paths corresponding to the selected scale are enabled and updated.\nIn this way, the proposed MDSR  greatly reduce the model size by sharing most of the parameters for different scales and exhibits comparable performance as single-scale models.\nThe similar scale-specific multi-path learning is also adopted by CARN  and ProSR .", "cites": [7254, 481, 482, 488, 7030, 305, 496], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes information from multiple papers by categorizing multi-path learning into global, local, and scale-specific types, and clearly connects their design principles. It identifies patterns in how different models use multi-path architectures to enhance performance. However, the critical analysis is limited to mentioning performance improvements and model size reductions without deeper evaluation or comparison of trade-offs. The abstraction level is moderate, with some generalization but not reaching meta-level insights."}}
{"id": "9d2cd66e-cbba-47ff-ac53-21cc3e8b6ed5", "title": "Dense Connections", "level": "subsubsection", "subsections": [], "parent_id": "40ccdbeb-49e2-41b8-849a-4470b011cae3", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Network Design"], ["subsubsection", "Dense Connections"]], "content": "\\label{sec_dense_connections}\nSince Huang \\textit{et al.}  propose DenseNet based on dense blocks, the dense connections have become more and more popular in vision tasks.\nFor each layer in a dense block, the feature maps of all preceding layers are used as inputs, and its own feature maps are used as inputs into all subsequent layers, so that it leads to $l \\cdot (l - 1) / 2$ connections in a $l$-layer dense block ($l \\ge 2$).\nThe dense connections not only help alleviate gradient vanishing, enhance signal propagation and encourage feature reuse, but also substantially reduce the model size by employing small growth rate (i.e., number of channels in dense blocks) and squeezing channels after concatenating all input feature maps.\nFor the sake of fusing low-level and high-level features to provide richer information for reconstructing high-quality details, dense connections are introduced into the SR field, as Fig. \\ref{fig_dense_connections} shows.\nTong \\textit{et al.}  not only adopt dense blocks to construct a 69-layers SRDenseNet, but also insert dense connections between different dense blocks, i.e., for every dense block, the feature maps of all preceding blocks are used as inputs, and its own feature maps are used as inputs into all subsequent blocks.\nThese layer-level and block-level dense connections are also adopted by MemNet , CARN , RDN  and ESRGAN .\nThe DBPN  also adopts dense connections extensively, but their dense connections are between all the upsampling units, as are the downsampling units.", "cites": [7031, 7034, 96, 481, 7255, 7258], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the concept of dense connections by linking their origins in DenseNet to their adoption in various SR models, establishing a coherent narrative. It abstracts the idea to show how dense connections facilitate feature reuse and model efficiency. However, the critical analysis is limitedâthere is no evaluation of the trade-offs or limitations of dense connections across the cited works."}}
{"id": "d97c223a-4bc5-4a47-8979-2535eb24b334", "title": "Attention Mechanism", "level": "subsubsection", "subsections": [], "parent_id": "40ccdbeb-49e2-41b8-849a-4470b011cae3", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Network Design"], ["subsubsection", "Attention Mechanism"]], "content": "\\label{sec_attention_mechanism}\n\\textbf{Channel Attention.}\nConsidering the interdependence and interaction of the feature representations between different channels, Hu \\textit{et al.}  propose a ``squeeze-and-excitation'' block to improve learning ability by explicitly modelling channel interdependence, as Fig. \\ref{fig_channel_attention} shows.\nIn this block, each input channel is squeezed into a channel descriptor (i.e., a constant) using global average pooling (GAP), then these descriptors are fed into two dense layers to produce channel-wise scaling factors for input channels.\nRecently, Zhang \\textit{et al.}  incorporate the channel attention mechanism with SR and propose RCAN, which markedly improves the representation ability of the model and SR performance.\nIn order to better learn the feature correlations, Dai \\textit{et al.}  further propose a second-order channel attention (SOCA) module.\nThe SOCA adaptively rescales the channel-wise features by using second-order feature statistics instead of GAP, and enables extracting more informative and discriminative representations.\n\\textbf{Non-local Attention.}\nMost existing SR models have very limited local receptive fields.\nHowever, some distant objects or textures may be very important for local patch generation.\nSo that Zhang \\textit{et al.}  propose local and non-local attention blocks to extract features that capture the long-range dependencies between pixels.\nSpecifically, they propose a trunk branch for extracting features, and a (non-)local mask branch for adaptively rescaling features of trunk branch.\nAmong them, the local branch employs an encoder-decoder structure to learn the local attention, while the non-local branch uses the embedded Gaussian function to evaluate pairwise relationships between every two position indices in the feature maps to predict the scaling weights.\nThrough this mechanism, the proposed method captures the spatial attention well and further enhances the representation ability.\nSimilarly, Dai \\textit{et al.}  also incorporate the non-local attention mechanism to capture long-distance spatial contextual information.", "cites": [493, 501], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the attention mechanisms from two cited papers (RCAN and Residual Non-local Attention Networks) and connects their contributions to the broader context of SR network design. It provides a clear analytical structure by distinguishing between channel and non-local attention, and explains how these mechanisms address limitations in traditional CNN approaches. However, it offers limited critical evaluation of the methods' shortcomings or broader implications, and the abstraction is modest, focusing mainly on the technical aspects of attention rather than overarching design principles or theoretical insights."}}
{"id": "678fc038-cc30-476b-9384-4e4afbdc9b84", "title": "Advanced Convolution", "level": "subsubsection", "subsections": [], "parent_id": "40ccdbeb-49e2-41b8-849a-4470b011cae3", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Network Design"], ["subsubsection", "Advanced Convolution"]], "content": "\\label{sec_advanced_convolution}\nSince convolution operations are the basis of deep neural networks, researchers also attempt to improve convolution operations for better performance or greater efficiency.\n\\textbf{Dilated Convolution.}\nIt is well known that the contextual information facilitates generating realistic details for SR.\nThus Zhang \\textit{et al.}  replace the common convolution by dilated convolution in SR models, increase the receptive field over twice and achieve much better performance.\n\\textbf{Group Convolution.}\nMotivated by recent advances on lightweight CNNs , Hui \\textit{et al.}  and Ahn \\textit{et al.}  propose IDN and CARN-M, respectively, by replacing the vanilla convolution by group convolution.\nAs some previous works have proven, the group convolution much reduces the number of parameters and operations at the expense of a little performance loss .\n\\textbf{Depthwise Separable Convolution}\nSince Howard \\textit{et al.}  propose depthwise separable convolution for efficient convolution, it has been expanded to into various fields.\nSpecifically, it consists of a factorized depthwise convolution and a pointwise convolution (i.e., $1 \\times 1$ convolution), and thus reduces plenty of parameters and operations at only a small reduction in accuracy .\nAnd recently, Nie \\textit{et al.}  employ the depthwise separable convolution and much accelerate the SR architecture.", "cites": [503, 500, 505, 481, 502, 494, 504], "cite_extract_rate": 1.0, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of advanced convolution types used in image super-resolution, citing relevant papers but without establishing strong connections between them. It lacks critical evaluation of the trade-offs (e.g., performance vs. efficiency) and does not abstract these techniques into broader design principles or trends in SR research."}}
{"id": "b527778b-723c-49d8-9ab4-5333d03ca380", "title": "Region-recursive Learning", "level": "subsubsection", "subsections": [], "parent_id": "40ccdbeb-49e2-41b8-849a-4470b011cae3", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Network Design"], ["subsubsection", "Region-recursive Learning"]], "content": "\\label{sec_region_recursive_learning}\nMost SR models treat SR as a pixel-independent task and thus cannot source the interdependence between generated pixels properly.\nInspired by PixelCNN , Dahl \\textit{et al.}  firstly propose pixel recursive learning to perform pixel-by-pixel generation, by employing two networks to capture global contextual information and serial generation dependence, respectively.\nIn this way, the proposed method synthesizes realistic hair and skin details on super-resolving very low-resolution face images (e.g., $8\\times 8$) and far exceeds the previous methods on MOS testing  (Sec. \\ref{sec_iqa_mos}).\nMotivated by the human attention shifting mechanism , the Attention-FH  also adopts this strategy by resorting to a recurrent policy network for sequentially discovering attended patches and performing local enhancement.\nIn this way, it is capable of adaptively personalizing an optimal searching path for each image according to its own characteristic, and thus fully exploits the global intra-dependence of images.\nAlthough these methods show better performance to some extent, the recursive process requiring a long propagation path greatly increases the computational cost and training difficulty, especially for super-resolving HR images.", "cites": [506, 507, 488], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates the concept of region-recursive learning across multiple papers, connecting PixelCNN's influence with the contributions of Pixel Recursive Super Resolution and Attention-FH. It highlights the shift from pixel-independent to sequential generation and emphasizes the benefits and challenges of these approaches. While it provides some level of critical analysis by mentioning increased computational cost and training difficulty, it does not deeply compare or critique the methods. The abstraction is moderate, as it generalizes the idea of sequential pixel generation and contextual interdependence."}}
{"id": "991fb891-2f49-4b32-8d73-a582b4734886", "title": "Pyramid Pooling", "level": "subsubsection", "subsections": [], "parent_id": "40ccdbeb-49e2-41b8-849a-4470b011cae3", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Network Design"], ["subsubsection", "Pyramid Pooling"]], "content": "\\label{sec_pyramid_pooling}\nMotivated by the spatial pyramid pooling layer , Zhao \\textit{et al.}  propose the pyramid pooling module to better utilize global and local contextual information.\nSpecifically, for feature maps sized $h\\times w\\times c$, each feature map is divided into $M\\times M$ bins, and goes through global average pooling, resulting in $M\\times M\\times c$ outputs.\nThen a $1\\times 1$ convolution is performed for compressing the outputs to a single channel.\nAfter that, the low-dimensional feature map is upsampled to the same size as the original feature map via bilinear interpolation.\nBy using different $M$, the module integrates global as well as local contextual information effectively.\nBy incorporating this module, the proposed EDSR-PP model  further improve the performance over baselines.", "cites": [509, 508], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the pyramid pooling module and its application in the EDSR-PP model, drawing from the cited papers. While it mentions the purpose of the module and its components, it lacks deeper synthesis of ideas across the sources and offers minimal critical evaluation or abstraction to broader trends in SR or CNN design."}}
{"id": "fda2defb-1f41-4ab9-ad3c-c031428143aa", "title": "Wavelet Transformation", "level": "subsubsection", "subsections": [], "parent_id": "40ccdbeb-49e2-41b8-849a-4470b011cae3", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Network Design"], ["subsubsection", "Wavelet Transformation"]], "content": "\\label{sec_wavelet_transformation}\nAs is well-known, the wavelet transformation (WT)  is a highly efficient representation of images by decomposing the image signal into high-frequency sub-bands denoting texture details and low-frequency sub-bands containing global topological information.\nBae \\textit{et al.}  firstly combine WT with deep learning based SR model, take sub-bands of interpolated LR wavelet as input and predict residuals of corresponding HR sub-bands.\nWT and inverse WT are applied for decomposing the LR input and reconstructing the HR output, respectively.\nSimilarly, the DWSR  and Wavelet-SRNet  also perform SR in the wavelet domain but with more complicated structures.\nIn contrast to the above works processing each sub-band independently, the MWCNN  adopts multi-level WT and takes the concatenated sub-bands as the input to a single CNN for better capturing the dependence between them.\nDue to the efficient representation by wavelet transformation, the models using this strategy often much reduce the model size and computational cost, while maintain competitive performance .", "cites": [7035], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic overview of wavelet transformation in the context of deep learning for image super-resolution, mentioning a few key papers but without substantial integration or comparison of their approaches. It highlights the use of WT and its benefits (e.g., reduced model size and cost) but does not critically evaluate the limitations or trade-offs of these methods. The abstraction is minimal, focusing mainly on the specific role of WT in SR without generalizing to broader trends or principles."}}
{"id": "c306a6a1-8f8c-4859-987d-2a95118d2c98", "title": "Desubpixel", "level": "subsubsection", "subsections": [], "parent_id": "40ccdbeb-49e2-41b8-849a-4470b011cae3", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Network Design"], ["subsubsection", "Desubpixel"]], "content": "\\label{sec_desubpixel}\nIn order to speed up the inference speed, Vu \\textit{et al.}  propose to perform the time-consuming feature extraction in a lower-dimensional space, and propose desubpixel, an inverse of the shuffle operation of sub-pixel layer (Sec. \\ref{sec_learning_based_upsampling}).  \nSpecifically, the desubpixel operation splits the images spatially, stacks them as extra channels and thus avoids loss of information.\nIn this way, they downsample input images by desubpixel at the beginning of the model, learn representations in a lower-dimensional space, and upsample to the target size at the end.  \nThe proposed model achieves the best scores in the PIRM Challenge on Smartphones  with very high-speed inference and good performance.", "cites": [494], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly describes the desubpixel technique and its purpose but does not synthesize it with other methods or provide deeper analysis. It lacks critical evaluation of the approach and offers no abstraction or generalization beyond the specific technique. The mention of performance in a challenge is factual but not contextualized or compared with other works."}}
{"id": "a87c4558-b49d-41c3-b3f4-d81e2530643c", "title": "xUnit", "level": "subsubsection", "subsections": [], "parent_id": "40ccdbeb-49e2-41b8-849a-4470b011cae3", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Network Design"], ["subsubsection", "xUnit"]], "content": "\\label{sec_xunit}\nIn order to combine spatial feature processing and nonlinear activations to learn complex features more efficiently, Kligvasser \\textit{et al.}  propose xUnit for learning a spatial activation function.\nSpecifically, the ReLU is regarded as determining a weight map to perform element-wise multiplication with the input, while the xUnit directly learn the weight map through convolution and Gaussian gating.\nAlthough the xUnit is more computationally demanding, due to its dramatic effect on the performance, it allows greatly reducing the model size while matching the performance with ReLU.\nIn this way, the authors reduce the model size by nearly 50\\% without any performance degradation.", "cites": [510], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the xUnit by explaining its design and purpose in relation to traditional ReLU activation. It synthesizes the key idea from the cited paper and briefly contrasts xUnit with ReLU. However, it lacks deeper critical analysis of the method's limitations or broader comparisons with other activation units, and abstraction is limited to a single technique without generalizing to broader trends in network design."}}
{"id": "5f7b8408-097b-4ff9-9bbe-75059eeb4009", "title": "Loss Functions", "level": "subsubsection", "subsections": [], "parent_id": "d654c603-6009-4a1e-89b5-b983ee09835f", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Learning Strategies"], ["subsubsection", "Loss Functions"]], "content": "\\label{sec_loss}\nIn the super-resolution field, loss functions are used to measure reconstruction error and guide the model optimization.\nIn early times, researchers usually employ the pixel-wise L2 loss, but later discover that it cannot measure the reconstruction quality very accurately.\nTherefore, a variety of loss functions (e.g., content loss , adversarial loss ) are adopted for better measuring the reconstruction error and producing more realistic and higher-quality results.\nNowadays these loss functions have been playing an important role.\nIn this section, we'll take a closer look at the loss functions used widely.\nThe notations in this section follow Sec. \\ref{sec_problem_definitions}, except that we ignore the subscript $y$ of the target HR image $\\hat{I_y}$ and generated HR image $I_y$ for brevity.\n\\textbf{Pixel Loss.}\nPixel loss measures pixel-wise difference between two images and mainly includes L1 loss (i.e., mean absolute error) and L2 loss (i.e., mean square error):\n\\begin{align}\n    \\mathcal{L}_{\\text{pixel\\_l1}} (\\hat{I}, I) &= \\frac{1}{hwc} \\sum_{i,j,k} |\\hat{I}_{i,j,k} - I_{i,j,k}| , \\\\\n    \\mathcal{L}_{\\text{pixel\\_l2}} (\\hat{I}, I) &= \\frac{1}{hwc} \\sum_{i,j,k} (\\hat{I}_{i,j,k} - I_{i,j,k})^2 ,\n\\end{align}\nwhere $h$, $w$ and $c$ are the height, width and number of channels of the evaluated images, respectively.\nIn addition, there is a variant of the pixel L1 loss, namely Charbonnier loss , given by:.\n\\begin{equation}\n    \\mathcal{L}_{\\text{pixel\\_Cha}} (\\hat{I}, I) = \\frac{1}{hwc} \\sum_{i,j,k} \\sqrt{(\\hat{I}_{i,j,k} - I_{i,j,k})^2 + \\epsilon^2} ,\n\\end{equation}\nwhere $\\epsilon$ is a constant (e.g., $10^{-3}$) for numerical stability.\nThe pixel loss constrains the generated HR image $\\hat{I}$ to be close enough to the ground truth $I$ on the pixel values.\nComparing with L1 loss, the L2 loss penalizes larger errors but is more tolerant to small errors, and thus often results in too smooth results.\nIn practice, the L1 loss shows improved performance and convergence over L2 loss .\nSince the definition of PSNR (Sec. \\ref{sec_iqa_psnr}) is highly correlated with pixel-wise difference and minimizing pixel loss directly maximize PSNR, the pixel loss gradual becomes the most widely used loss function.\nHowever, since the pixel loss actually doesn't take image quality (e.g., perceptual quality , textures ) into account, the results often lack high-frequency details and are perceptually unsatisfying with oversmooth textures .\n\\textbf{Content Loss.}\nIn order to evaluate perceptual quality of images, the content loss is introduced into SR .\nSpecifically, it measures the semantic differences between images using a pre-trained image classification network.\nDenoting this network as $\\phi$ and the extracted high-level representations on $l$-th layer as $\\phi^{(l)}(I)$, the content loss is indicated as the Euclidean distance between high-level representations of two images, as follows:\n\\begin{equation}\n    \\mathcal{L}_{\\text{content}} (\\hat{I}, I ; \\phi, l) = \\frac{1}{h_l w_l c_l} \\sqrt{\\sum_{i,j,k} (\\phi^{(l)}_{i,j,k}(\\hat{I}) - \\phi^{(l)}_{i,j,k}(I))^2} ,\n\\end{equation}\nwhere $h_l$, $w_l$ and $c_l$ are the height, width and number of channels of the representations on layer $l$, respectively.\nEssentially the content loss transfers the learned knowledge of hierarchical image features from the classification network $\\phi$ to the SR network.\nIn contrast to the pixel loss, the content loss encourages the output image $\\hat{I}$ to be perceptually similar to the target image $I$ instead of forcing them to match pixels exactly.\nThus it produces visually more perceptible results and is also widely used in this field , where the VGG  and ResNet  are the most commonly used pre-trained CNNs.\n\\textbf{Texture Loss.}\nOn account that the reconstructed image should have the same style (e.g., colors, textures, contrast) with the target image, and motivated by the style representation by Gatys \\textit{et al.} , the texture loss (a.k.a style reconstruction loss) is introduced into SR.\nFollowing , the image texture is regarded as the correlations between different feature channels and defined as the Gram matrix $G^{(l)} \\in \\mathbb{R}^{c_l \\times c_l}$, where $G^{(l)}_{ij}$ is the inner product between the vectorized feature maps $i$ and $j$ on layer $l$:\n\\begin{equation}\n    G^{(l)}_{ij}(I) = \\operatorname{vec}(\\phi_i^{(l)}(I)) \\cdot \\operatorname{vec}(\\phi_j^{(l)}(I)) ,\n\\end{equation}\nwhere $\\operatorname{vec}(\\cdot)$ denotes the vectorization operation, and $\\phi_i^{(l)}(I)$ denotes the $i$-th channel of the feature maps on layer $l$ of image $I$.\nThen the texture loss is given by:\n\\begin{equation}\n    \\mathcal{L}_{\\text{texture}} (\\hat{I}, I ; \\phi, l) = \\frac{1}{c_l^2} \\sqrt{\\sum_{i,j} (G^{(l)}_{i,j}(\\hat{I}) - G^{(l)}_{i,j}(I))^2} .\n\\end{equation}\nBy employing texture loss, the EnhanceNet  proposed by Sajjadi \\textit{et al.} creates much more realistic textures and produces visually more satisfactory results.\nDespite this, determining the patch size to match textures is still empirical.\nToo small patches lead to artifacts in textured regions, while too large patches lead to artifacts throughout the entire image because texture statistics are averaged over regions of varying textures.\n\\textbf{Adversarial Loss.}\nIn recent years, due to the powerful learning ability, the GANs  receive more and more attention and are introduced to various vision tasks.\nTo be concrete, the GAN consists of a generator performing generation (e.g., text generation, image transformation), and a discriminator which takes the generated results and instances sampled from the target distribution as input and discriminates whether each input comes from the target distribution.\nDuring training, two steps are alternately performed:\n(a) fix the generator and train the discriminator to better discriminate,\n(b) fix the discriminator and train the generator to fool the discriminator.\nThrough adequate iterative adversarial training, the resulting generator can produce outputs consistent with the distribution of real data, while the discriminator can't distinguish between the generated data and real data.\nIn terms of super-resolution, it is straightforward to adopt adversarial learning, in which case we only need to treat the SR model as a generator, and define an extra discriminator to judge whether the input image is generated or not.\nTherefore, Ledig \\textit{et al.}  firstly propose SRGAN using adversarial loss based on cross entropy, as follows:\n\\begin{align}\n    \\mathcal{L}_{\\text{gan\\_ce\\_g}} (\\hat{I} ; D) &= - \\log D(\\hat{I}) , \\\\\n    \\mathcal{L}_{\\text{gan\\_ce\\_d}} (\\hat{I}, I_s ; D) &= - \\log D(I_s) - \\log (1 - D(\\hat{I})) ,\n\\end{align}\nwhere $\\mathcal{L}_{\\text{gan\\_ce\\_g}}$ and $\\mathcal{L}_{\\text{gan\\_ce\\_d}}$ denote the adversarial loss of the generator (i.e., the SR model) and the discriminator $D$ (i.e., a binary classifier), respectively, and $I_s$ represents images randomly sampled from the ground truths.\nBesides, the Enhancenet  also adopts the similar adversarial loss.\nBesides, Wang \\textit{et al.}  and Yuan \\textit{et al.}  use adversarial loss based on least square error for more stable training process and higher quality results , given by:\n\\begin{align}\n    \\mathcal{L}_{\\text{gan\\_ls\\_g}} (\\hat{I} ; D) &= (D(\\hat{I}) - 1)^2 , \\\\\n    \\mathcal{L}_{\\text{gan\\_ls\\_d}} (\\hat{I}, I_s ; D) &= (D(\\hat{I}))^2 + (D(I_s) - 1)^2 .\n\\end{align}\nIn contrast to the above works focusing on the specific forms of adversarial loss, Park \\textit{et al.}  argue that the pixel-level discriminator causes generating meaningless high-frequency noise, and attach another feature-level discriminator to operate on high-level representations extracted by a pre-trained CNN which captures more meaningful attributes of real HR images.  \nXu \\textit{et al.}  incorporate a multi-class GAN consisting of a generator and multiple class-specific discriminators.\nAnd the ESRGAN  employs relativistic GAN  to predict the probability that real images are relatively more realistic than fake ones, instead of the probability that input images are real or fake, and thus guide recovering more detailed textures.\nExtensive MOS tests (Sec. \\ref{sec_iqa_mos}) show that even though the SR models trained with adversarial loss and content loss achieve lower PSNR compared to those trained with pixel loss, they bring significant gains in perceptual quality .\nAs a matter of fact, the discriminator extracts some difficult-to-learn latent patterns of real HR images, and pushes the generated HR images to conform, thus helps to generate more realistic images.\nHowever, currently the training process of GAN is still difficult and unstable.\nAlthough there have been some studies on how to stabilize the GAN training , how to ensure that the GANs integrated into SR models are trained correctly and play an active role remains a problem.\n\\textbf{Cycle Consistency Loss.}\nMotivated by the CycleGAN proposed by Zhu \\textit{et al.} , Yuan \\textit{et al.}  present a cycle-in-cycle approach for super-resolution.  \nConcretely speaking, they not only super-resolve the LR image $I$ to the HR image $\\hat{I}$ but also downsample $\\hat{I}$ back to another LR image $I'$ through another CNN.\nThe regenerated $I'$ is required to be identical to the input $I$, thus the cycle consistency loss is introduced for constraining their pixel-level consistency:\n\\begin{equation}\n    \\mathcal{L}_{\\text{cycle}} (I', I) = \\frac{1}{hwc} \\sqrt{\\sum_{i,j,k} (I'_{i,j,k} - I_{i,j,k})^2} .\n\\end{equation}\n\\textbf{Total Variation Loss.}\nIn order to suppress noise in generated images, the total variation (TV) loss  is introduced into SR by Aly \\textit{et al.} .\nIt is defined as the sum of the absolute differences between neighboring pixels and measures how much noise is in the images, as follows:\n\\begin{equation}\n    \\mathcal{L}_{\\text{TV}} (\\hat{I}) = \\frac{1}{hwc} \\sum_{i,j,k} \\sqrt{\n        (\\hat{I}_{i,j+1,k} - \\hat{I}_{i,j,k})^2 +\n        (\\hat{I}_{i+1,j,k} - \\hat{I}_{i,j,k})^2} .\n\\end{equation}\nLai \\textit{et al.}  and Yuan \\textit{et al.}  also adopt the TV loss for imposing spatial smoothness.\n\\textbf{Prior-Based Loss.}\nIn addition to the above loss functions, external prior knowledge is also introduced to constrain the generation.\nSpecifically, Bulat \\textit{et al.}  focus on face image SR and introduce a face alignment network (FAN) to constrain the consistency of facial landmarks.  \nThe FAN is pre-trained and integrated for providing face alignment priors, and then trained jointly with the SR.\nIn this way, the proposed Super-FAN improves performance both on LR face alignment and face image SR.\nAs a matter of fact, the content loss and the texture loss, both of which introduce a classification network, essentially provide prior knowledge of hierarchical image features for SR.\nBy introducing more prior knowledge, the SR performance can be further improved.\nIn this section, we introduce various loss functions for SR.\nIn practice, researchers often combine multiple loss functions by weighted average  for constraining different aspects of the generation process, especially for distortion-perception tradeoff .\nHowever, the weights of different loss functions require a lot of empirical exploration, and how to combine reasonably and effectively remains a problem.", "cites": [8317, 516, 513, 515, 512, 121, 514, 480, 7258, 483, 97, 481, 482, 487, 7030, 63, 78, 7254, 511, 518, 7022, 517], "cite_extract_rate": 0.6285714285714286, "origin_cites_number": 35, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a coherent synthesis of different loss functions in deep learning-based SR, connecting pixel, content, texture, and adversarial losses across multiple works. It includes critical points such as the limitations of pixel loss (oversmoothing, lack of perceptual quality) and adversarial training issues (instability, noise artifacts). While it identifies patterns in the usage and evolution of loss functions, the abstraction remains somewhat constrained to the specific context of SR rather than reaching broader theoretical principles."}}
{"id": "c7f23b9b-1823-4ada-ab4a-2eeb93847ad0", "title": "Batch Normalization", "level": "subsubsection", "subsections": [], "parent_id": "d654c603-6009-4a1e-89b5-b983ee09835f", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Learning Strategies"], ["subsubsection", "Batch Normalization"]], "content": "In order to accelerate and stabilize training of deep CNNs, Sergey \\textit{et al.}  propose batch normalization (BN) to reduce internal covariate shift of networks.\nSpecifically, they perform normalization for each mini-batch and train two extra transformation parameters for each channel to preserve the representation ability.\nSince the BN calibrates the intermediate feature distribution and mitigates vanishing gradients, it allows using higher learning rates and being less careful about initialization.\nThus this technique is widely used by SR models .\nHowever, Lim \\textit{et al.}  argue that the BN loses the scale information of each image and gets rid of range flexibility from networks.\nSo they remove BN and use the saved memory cost (up to 40\\%) to develop a much larger model, and thus increase the performance substantially.\nSome other models  also adopt this experience and achieve performance improvements.", "cites": [7254, 7031, 519, 71, 482, 484, 7035, 7258], "cite_extract_rate": 0.7272727272727273, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of batch normalization in the context of SR, explaining its purpose and impact on training. It synthesizes information from multiple papers by contrasting BN's benefits (as introduced in Paper 4) with its limitations (as discussed in Paper 5). The critical perspective is evident in highlighting how BN can lose scale information and reduce range flexibility, which led to performance gains through model redesign. The abstraction is moderate, focusing on the implications of BN for SR rather than broader architectural trends."}}
{"id": "47efaa78-c242-4ecd-8e17-3885d537c601", "title": "Curriculum Learning", "level": "subsubsection", "subsections": [], "parent_id": "d654c603-6009-4a1e-89b5-b983ee09835f", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Learning Strategies"], ["subsubsection", "Curriculum Learning"]], "content": "\\label{sec_curriculum_learning}\nCurriculum learning  refers to starting from an easier task and gradually increasing the difficulty.\nSince super-resolution is an ill-posed problem and always suffers adverse conditions such as large scaling factors, noise and blurring, the curriculum training is incorporated for reducing learning difficulty.\nIn order to reduce the difficulty of SR with large scaling factors, Wang \\textit{et al.} , Bei \\textit{et al.}  and Ahn \\textit{et al.}  propose ProSR, ADRSR and progressive CARN, respectively, which are progressive not only on architectures (Sec. \\ref{sec_framework_progressive}) but also on training procedure.\nThe training starts with the $2\\times$ upsampling, and after finishing training, the portions with $4\\times$ or larger scaling factors are gradually mounted and blended with the previous portions.\nSpecifically, the ProSR blends by linearly combining the output of this level and the upsampled output of previous levels following , the ADRSR concatenates them and attaches another convolutional layer, while the progressive CARN replace the previous reconstruction block with the one that produces the image in double resolution.\nIn addition, Park \\textit{et al.}  divide the $8\\times$ SR problem to three sub-problems (i.e., $1\\times$ to $2\\times$, $2\\times$ to $4\\times$, $4\\times$ to $8\\times$) and train independent networks for each problem.\nThen two of them are concatenated and fine-tuned, and then with the third one.\nBesides, they also decompose the $4\\times$ SR under difficult conditions into $1\\times$ to $2\\times$, $2\\times$ to $4\\times$ and denoising or deblurring sub-problems.\nIn contrast, the SRFBN  uses this strategy for SR under adverse conditions, i.e., starting from easy degradation and gradually increasing degradation complexity.\nCompared to common training procedure, the curriculum learning greatly reduces the training difficulty and shortens the total training time, especially for large factors.", "cites": [7254, 62, 497], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple papers by highlighting how curriculum learning is used to address the challenge of large scaling factors and adverse conditions in SR, showing a coherent narrative across ProSR, ADRSR, progressive CARN, and SRFBN. While it offers some abstraction by identifying the common theme of progressive training, the critical analysis is limitedâthere is no deep evaluation of the limitations or trade-offs of these methods. It remains mostly analytical by identifying trends and strategies rather than just describing individual systems."}}
{"id": "f7b986d5-4b1c-4a86-9dca-a3d11ae4ee40", "title": "Multi-supervision", "level": "subsubsection", "subsections": [], "parent_id": "d654c603-6009-4a1e-89b5-b983ee09835f", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Learning Strategies"], ["subsubsection", "Multi-supervision"]], "content": "\\label{sec_multi_supervision}\nMulti-supervision refers to adding multiple supervision signals within the model for enhancing the gradient propagation and avoiding vanishing and exploding gradients.\nIn order to prevent the gradient problems introduced by recursive learning (Sec. \\ref{sec_recursive_learning}), the DRCN  incorporates multi-supervision with recursive units.\nSpecifically, they feed each output of recursive units into a reconstruction module to generate an HR image, and build the final prediction by incorporating all the intermediate reconstructions.  \nSimilar strategies are also taken by MemNet  and DSRN , which are also based on recursive learning.\nBesides, since the LapSRN  under the progressive upsampling framework (Sec. \\ref{sec_framework_progressive}) generates intermediate results of different scales during propagation, it is straightforward to adopt multi-supervision strategy.\nSpecifically, the intermediate results are forced to be the same as the intermediate images downsampled from the ground truth HR images.\nIn practice, this multi-supervision technique is often implemented by adding some terms in the loss function, and in this way, the supervision signals are back-propagated more effectively, and thus reduce the training difficulty and enhance the model training.", "cites": [489, 7031, 495, 7030, 496], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes information from multiple papers by connecting the use of multi-supervision to address gradient issues in recursive networks and progressive upsampling. It provides a coherent explanation of how this technique is implemented across different architectures. However, while it acknowledges limitations (e.g., training difficulty), it does not deeply critique or evaluate the effectiveness of multi-supervision in each case. The abstraction level is moderate, as it identifies a general strategy but does not propose a higher-level theoretical framework."}}
{"id": "f78a8e94-a3ba-457e-b64b-540649b45ed6", "title": "Context-wise Network Fusion", "level": "subsubsection", "subsections": [], "parent_id": "130e5bca-8421-423a-bba5-191d29e16c10", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Other Improvements"], ["subsubsection", "Context-wise Network Fusion"]], "content": "Context-wise network fusion (CNF)  refers to a stacking technique fusing predictions from multiple SR networks (i.e., a special case of multi-path learning in Sec. \\ref{sec_multi_path_learning}).\nTo be concrete, they train individual SR models with different architectures separately, feed the prediction of each model into individual convolutional layers, and finally sum the outputs up to be the final prediction result.\nWithin this CNF framework, the final model constructed by three lightweight SRCNNs  achieves comparable performance with state-of-the-art models with acceptable efficiency .", "cites": [8358], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a basic description of context-wise network fusion, referencing one paper and situating it within a previously introduced multi-path learning framework. However, it lacks deeper synthesis across multiple works, critical evaluation of the approachâs strengths or limitations, and broader abstraction to highlight underlying principles or trends in SR research."}}
{"id": "11702c98-62eb-4c24-a3b8-74b1e5f297a8", "title": "Data Augmentation", "level": "subsubsection", "subsections": [], "parent_id": "130e5bca-8421-423a-bba5-191d29e16c10", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Other Improvements"], ["subsubsection", "Data Augmentation"]], "content": "Data augmentation is one of the most widely used techniques for boosting performance with deep learning.\nFor image super-resolution, some useful augmentation options include cropping, flipping, scaling, rotation, color jittering, etc. .\nIn addition, Bei \\textit{et al.}  also randomly shuffle RGB channels, which not only augments data, but also alleviates color bias caused by the dataset with color unbalance.", "cites": [500, 7032, 482, 7030, 496], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of data augmentation techniques used in deep learning for image super-resolution. It mentions augmentation methods like cropping, flipping, scaling, and color jittering, and cites one paper (Paper 2) explicitly discussing data augmentation as a technique. However, it fails to synthesize insights across the cited papers, lacks critical evaluation of the methods or limitations, and offers minimal abstraction beyond the specific examples."}}
{"id": "58b1b25b-13ef-43e1-b293-798744d77254", "title": "Multi-task Learning", "level": "subsubsection", "subsections": [], "parent_id": "130e5bca-8421-423a-bba5-191d29e16c10", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Other Improvements"], ["subsubsection", "Multi-task Learning"]], "content": "\\label{sec_mtl}\nMulti-task learning  refers to improving generalization ability by leveraging domain-specific information contained in training signals of related tasks, such as object detection and semantic segmentation , head pose estimation and facial attribute inference .\nIn the SR field, Wang \\textit{et al.}  incorporate a semantic segmentation network for providing semantic knowledge and generating semantic-specific details.  \nSpecifically, they propose spatial feature transformation to take semantic maps as input and predict spatial-wise parameters of affine transformation performed on the intermediate feature maps.\nThe proposed SFT-GAN thus generates more realistic and visually pleasing textures on images with rich semantic regions.  \nBesides, considering that directly super-resolving noisy images may cause noise amplification, the DNSR  proposes to train a denoising network and an SR network separately, then concatenates them and fine-tunes together.\nSimilarly, the cycle-in-cycle GAN (CinCGAN)  combines a cycle-in-cycle denoising framework and a cycle-in-cycle SR model to joint perform noise reduction and super-resolution.\nSince different tasks tend to focus on different aspects of the data, combining related tasks with SR models usually improves the SR performance by providing extra information and knowledge.", "cites": [520, 518, 487], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of multi-task learning in SR, connecting the cited works by highlighting how combining SR with other vision tasks (e.g., segmentation, denoising) enhances performance. It abstracts the general idea that different tasks focus on different data aspects, though it stops short of offering a novel framework or deeper comparative analysis. Critical evaluation is limited to brief observations like noise amplification in direct SR of noisy images."}}
{"id": "728e90bf-43c6-4b78-99b6-c265092e2016", "title": "Network Interpolation", "level": "subsubsection", "subsections": [], "parent_id": "130e5bca-8421-423a-bba5-191d29e16c10", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Other Improvements"], ["subsubsection", "Network Interpolation"]], "content": "PSNR-based models produce images closer to ground truths but introduce blurring problems, while GAN-based models bring better perceptual quality but introduce unpleasant artifacts (e.g., meaningless noise making images more ``realistic'').\nIn order to better balance the distortion and perception, Wang \\textit{et al.}  propose a network interpolation strategy.\nSpecifically, they train a PSNR-based model and train a GAN-based model by fine-tuning, then interpolate all the corresponding parameters of both networks to derive intermediate models.\nBy tuning the interpolation weights without retraining networks, they produce meaningful results with much less artifacts.", "cites": [7258, 521], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical perspective by contrasting PSNR-based and GAN-based models, and introduces the concept of network interpolation as a solution to balance their trade-offs. It integrates the core idea from both cited papers to present a coherent problem-solution narrative, and offers a basic comparison of model outcomes. However, it stops short of deeper critical evaluation or abstraction into broader principles, limiting the overall insight quality."}}
{"id": "d5df91d7-a2d3-4c1f-aae4-1079cb0e0b0f", "title": "Self-Ensemble", "level": "subsubsection", "subsections": [], "parent_id": "130e5bca-8421-423a-bba5-191d29e16c10", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "Other Improvements"], ["subsubsection", "Self-Ensemble"]], "content": "\\label{sec_self_ensemble}\nSelf-ensemble, a.k.a. enhanced prediction , is an inference technique commonly used by SR models.\nSpecifically, rotations with different angles (0$^\\circ$, 90$^\\circ$, 180$^\\circ$, 270$^\\circ$) and horizontal flipping are applied on the LR images to get a set of 8 images.\nThen these images are fed into the SR model and the corresponding inverse transformation is applied to the reconstructed HR images to get the outputs.\nThe final prediction result is conducted by the mean  or the median  of these outputs.\nIn this way, these models further improve performance.\n\\begin{table*}[t]\n    \\renewcommand{\\arraystretch}{1.3}\n    \\caption{\n        Super-resolution methodology employed by some representative models.\n        The ``Fw.'', ``Up.'', ``Rec.'', ``Res.'', ``Dense.'', ``Att.'' represent SR frameworks, upsampling methods, recursive learning, residual learning, dense connections, attention mechanism, respectively.\n    }\n    \\label{tab_sota_sr_models}\n    \\rowcolors{2}{gray!0}{gray!8}\n    \\centering\n    {\n        \\setlength{\\fboxrule}{1pt}\n        \\begin{tabular}{|l|l|c|c|c|c|c|c|c|c|l|}\n            \\hline\n            Method                                      & Publication & Fw.   & Up.       & Rec. & Res. & Dense& Att. & $\\mathcal{L}_{\\text{L1}}$ & $\\mathcal{L}_{\\text{L2}}$ & Keywords \\\\\n            \\hline\n            SRCNN            & 2014, ECCV  & Pre.  & Bicubic   & ~    & ~    & ~    & ~    & ~    & \\yes & ~ \\\\\n            DRCN                & 2016, CVPR  & Pre.  & Bicubic   & \\yes & \\yes & ~    & ~    & ~    & \\yes & Recursive layers \\\\\n            FSRCNN       & 2016, ECCV  & Post. & Deconv    & ~    & ~    & ~    & ~    & ~    & \\yes & Lightweight design \\\\\n            ESPCN           & 2017, CVPR  & Pre.  & Sub-Pixel & ~    & ~    & ~    & ~    & ~    & \\yes & Sub-pixel \\\\\n            LapSRN                & 2017, CVPR  & Pro.  & Bicubic   & ~    & \\yes & ~    & ~    & \\yes & ~    & $\\mathcal{L}_{\\text{pixel\\_Cha}}$ \\\\\n            DRRN                 & 2017, CVPR  & Pre.  & Bicubic   & \\yes & \\yes & ~    & ~    & ~    & \\yes & Recursive blocks \\\\\n            SRResNet           & 2017, CVPR  & Post. & Sub-Pixel & ~    & \\yes & ~    & ~    & ~    & \\yes & $\\mathcal{L}_{\\text{Con.}}$, $\\mathcal{L}_{\\text{TV}}$ \\\\\n            SRGAN              & 2017, CVPR  & Post. & Sub-Pixel & ~    & \\yes & ~    & ~    & ~    & ~    & $\\mathcal{L}_{\\text{Con.}}$, $\\mathcal{L}_{\\text{GAN}}$ \\\\\n            EDSR             & 2017, CVPRW & Post. & Sub-Pixel & ~    & \\yes & ~    & ~    & \\yes & ~    & Compact and large-size design \\\\\n            EnhanceNet  & 2017, ICCV  & Pre.  & Bicubic   & ~    & \\yes & ~    & ~    & ~    & ~    & $\\mathcal{L}_{\\text{Con.}}$, $\\mathcal{L}_{\\text{GAN}}$, $\\mathcal{L}_{\\text{texture}}$ \\\\\n            MemNet              & 2017, ICCV  & Pre.  & Bicubic   & \\yes & \\yes & \\yes & ~    & ~    & \\yes & Memory block \\\\\n            SRDenseNet          & 2017, ICCV  & Post. & Deconv    & ~    & \\yes & \\yes & ~    & ~    & \\yes & Dense connections \\\\\n            DBPN                & 2018, CVPR  & Iter. & Deconv    & ~    & \\yes & \\yes & ~    & ~    & \\yes & Back-projection \\\\\n            DSRN                 & 2018, CVPR  & Pre.  & Deconv    & \\yes & \\yes & ~    & ~    & ~    & \\yes & Dual state \\\\\n            RDN             & 2018, CVPR  & Post. & Sub-Pixel & ~    & \\yes & \\yes & ~    & \\yes & ~    & Residual dense block \\\\\n            CARN                  & 2018, ECCV  & Post. & Sub-Pixel & \\yes & \\yes & \\yes & ~    & \\yes & ~    & Cascading \\\\\n            MSRN                  & 2018, ECCV  & Post. & Sub-Pixel & ~    & \\yes & ~    & ~    & \\yes & ~    & Multi-path \\\\\n            RCAN               & 2018, ECCV  & Post. & Sub-Pixel & ~    & \\yes & ~    & \\yes & \\yes & ~    & Channel attention \\\\\n            ESRGAN            & 2018, ECCVW & Post. & Sub-Pixel & ~    & \\yes & \\yes &      & \\yes & ~    & $\\mathcal{L}_{\\text{Con.}}$, $\\mathcal{L}_{\\text{GAN}}$ \\\\\n            RNAN            & 2019, ICLR  & Post. & Sub-Pixel & ~    & \\yes & ~    & \\yes & \\yes & ~    & Non-local attention \\\\\n            Meta-RDN               & 2019, CVPR  & Post. & Meta Upscale & ~    & \\yes & \\yes & ~    & \\yes & ~    & Magnification-arbitrary \\\\\n            SAN                 & 2019, CVPR  & Post. & Sub-Pixel & ~    & \\yes & ~    & \\yes & \\yes & ~    & Second-order attention \\\\\n            SRFBN              & 2019, CVPR  & Post. & Deconv & \\yes & \\yes & \\yes & ~    & \\yes & ~    & Feedback mechanism \\\\\n            \\hline\n        \\end{tabular}\n    }\n\\end{table*}\n\\begin{figure*}\n    \\centering\n    {\n        \\setlength{\\fboxrule}{1pt}\n        \\includegraphics[width=0.95\\linewidth]{fig_sr_benchmark}\n    }\n    \\caption{\n        Super-resolution benchmarking.\n        The $x$-axis and the $y$-axis denote the Multi-Adds and PSNR, respectively, and the circle size represents the number of parameters.\n    }\n    \\label{fig_sr_benchmarking}\n\\end{figure*}", "cites": [7031, 7256, 497, 7257, 7255, 495, 7258, 483, 522, 7034, 493, 7032, 481, 482, 7030, 496, 501, 7254], "cite_extract_rate": 0.6923076923076923, "origin_cites_number": 26, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes the self-ensemble technique and lists various SR models in a table without a detailed synthesis of their contributions. It lacks critical evaluation of the cited works and fails to abstract broader trends or principles, making it more of a factual summary than an insightful analysis."}}
{"id": "fc2fb2c1-1bc8-4bc6-b0f3-577d02a87eea", "title": "State-of-the-art Super-resolution Models", "level": "subsection", "subsections": [], "parent_id": "7a2c21dd-b0fb-4435-8e94-827357705759", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Supervised Super-resolution"], ["subsection", "State-of-the-art Super-resolution Models"]], "content": "\\label{sec_sota_sr_models}\nIn recent years, image super-resolution models based on deep learning have received more and more attention and achieved state-of-the-art performance.\nIn previous sections, we decompose SR models into specific components, including model frameworks (Sec. \\ref{sec_sr_frameworks}), upsampling methods (Sec. \\ref{sec_upsampling_methods}), network design (Sec. \\ref{sec_network_design}) and learning strategies (Sec. \\ref{sec_learning_strategies}), analyze these components hierarchically and identify their advantages and limitations.\nAs a matter of fact, most of the state-of-the-art SR models today can basically be attributed to a combination of multiple strategies we summarize above.\nFor example, the biggest contribution of the RCAN  comes from the channel attention mechanism (Sec. \\ref{sec_attention_mechanism}), and it also employs other strategies like sub-pixel upsampling (Sec. \\ref{sec_learning_based_upsampling}), residual learning (Sec. \\ref{sec_residual_learning}), pixel L1 loss (Sec. \\ref{sec_loss}), and self-ensemble (Sec. \\ref{sec_self_ensemble}).\nIn similar manners, we summarize some representative models and their key strategies, as Table \\ref{tab_sota_sr_models} shows.\nIn addition to SR accuracy, the efficiency is another very important aspect and different strategies have more or less impact on efficiency.\nSo in the previous sections, we not only analyze the accuracy of the presented strategies, but also indicate the concrete impacts on efficiency for the ones with a greater impact on efficiency, such as the post-upsampling (Sec. \\ref{sec_framework_post}), recursive learning (Sec. \\ref{sec_recursive_learning}), dense connections (Sec. \\ref{sec_dense_connections}), xUnit (Sec. \\ref{sec_xunit}).\nAnd we also benchmark some representative SR models on the SR accuracy (i.e., PSNR), model size (i.e., number of parameters) and computation cost (i.e., number of Multi-Adds), as shown in Fig. \\ref{fig_sr_benchmarking}.\nThe accuracy is measured by the mean of the PSNR on 4 benchmark datasets (i.e., Set5 , Set14 , B100  and Urban100 ).\nAnd the model size and computational cost are calculated with PyTorch-OpCounter , where the output resolution is 720p (i.e., $1080 \\times 720$).\nAll statistics are derived from the original papers or calculated on official models, with a scaling factor of 2.\nFor better viewing and comparison, we also provide an interactive online version\\footnote{https://github.com/ptkin/Awesome-Super-Resolution}.", "cites": [493], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from previous sections and RCAN to highlight how state-of-the-art models combine multiple strategies. It abstracts by identifying broader trends in model design and efficiency. While it provides a good analytical overview, it could offer deeper critical evaluation of the limitations of specific strategies and their trade-offs."}}
{"id": "7b9b2d2b-ed23-46f2-a7a0-ddbcbd346afd", "title": "Weakly-supervised Super-resolution", "level": "subsection", "subsections": [], "parent_id": "a53a0c61-637a-47c6-bcab-c20c491483f3", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Unsupervised Super-resolution"], ["subsection", "Weakly-supervised Super-resolution"]], "content": "To cope with super-resolution without introducing predefined degradation, researchers attempt to learn SR models with weakly-supervised learning, i.e., using unpaired LR-HR images.\nAmong them, some researchers first learn the HR-to-LR degradation and use it to construct datasets for training the SR model, while others design cycle-in-cycle networks to learn the LR-to-HR and HR-to-LR mappings simultaneously.\nNext we'll detail these models.\n\\textbf{Learned Degradation.}\nSince the predefined degradation is suboptimal, learning the degradation from unpaired LR-HR datasets is a feasible direction.\nBulat \\textit{et al.}  propose a two-stage process which firstly trains an HR-to-LR GAN to learn degradation using unpaired LR-HR images and then trains an LR-to-HR GAN for SR using paired LR-HR images conducted base on the first GAN.\nSpecifically, for the HR-to-LR GAN, HR images are fed into the generator to produce LR outputs, which are required to match not only the LR images obtained by downscaling the HR images (by average pooling) but also the distribution of real LR images.\nAfter finishing training, the generator is used as a degradation model to generate LR-HR image pairs.\nThen for the LR-to-HR GAN, the generator (i.e., the SR model) takes the generated LR images as input and predicts HR outputs, which are required to match not only the corresponding HR images but also the distribution of the HR images.\nBy applying this two-stage process, the proposed unsupervised model effectively increases the quality of super-resolving real-world LR images and obtains large improvement over previous state-of-the-art works.\n\\textbf{Cycle-in-cycle Super-resolution.}\nAnother approach for unsupervised super-resolution is to treat the LR space and the HR space as two domains, and use a cycle-in-cycle structure to learn the mappings between each other.\nIn this case, the training objectives include pushing the mapped results to match the target domain distribution and making the images recoverable through round-trip mappings.\nMotivated by CycleGAN , Yuan \\textit{et al.}  propose a cycle-in-cycle SR network (CinCGAN) composed of 4 generators and 2 discriminators, making up two CycleGANs for \\textit{noisy LR} $\\rightleftharpoons$ \\textit{clean LR} and \\textit{clean LR} $\\rightleftharpoons$ \\textit{clean HR} mappings, respectively.\nSpecifically, in the first CycleGAN, the noisy LR image is fed into a generator, and the output is required to be consistent with the distribution of real clean LR images.\nThen it's fed into another generator and required to recover the original input.\nSeveral loss functions (e.g., adversarial loss, cycle consistency loss, identity loss) are employed for guaranteeing the cycle consistency, distribution consistency, and mapping validity.\nThe other CycleGAN is similarly designed, except that the mapping domains are different.\nBecause of avoiding the predefined degradation, the unsupervised CinCGAN not only achieves comparable performance to supervised methods, but also is applicable to various cases even under very harsh conditions.\nHowever, due to the ill-posed essence of SR problem and the complicated architecture of CinCGAN, some advanced strategies are needed for reducing the training difficulty and instability.", "cites": [523, 518, 7022], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes two key approaches for weakly-supervised SRâlearned degradation and cycle-in-cycle networksâby connecting their goals and mechanisms in a coherent narrative. It provides a critical perspective by highlighting the limitations of predefined degradation and the challenges posed by the ill-posed nature of the SR problem and CinCGAN's complexity. While it identifies some broader patterns, such as the use of GANs for unsupervised learning, its abstraction remains grounded in the specific techniques described without reaching a high level of meta-insight."}}
{"id": "b5ed866c-3103-4967-9bd8-995336da6184", "title": "Deep Image Prior", "level": "subsection", "subsections": [], "parent_id": "a53a0c61-637a-47c6-bcab-c20c491483f3", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Unsupervised Super-resolution"], ["subsection", "Deep Image Prior"]], "content": "Considering that the CNN structure is sufficient to capture a great deal of low-level image statistics prior for inverse problems, Ulyanov \\textit{et al.}  employ a randomly-initialized CNN as handcrafted prior to perform SR.\nSpecifically, they define a generator network which takes a random vector $z$ as input and tries to generate the target HR image $I_y$.\nThe goal is to train the network to find an $\\hat{I_y}$ that the downsampled $\\hat{I_y}$ is identical to the LR image $I_x$.\nSince the network is randomly initialized and never trained, the only prior is the CNN structure itself.\nAlthough the performance of this method is still worse than the supervised methods ($2$ dB), it outperforms traditional bicubic upsampling considerably ($1$ dB).\nBesides, it shows the rationality of the CNN architectures itself, and prompts us to improve SR by combining the deep learning methodology with handcrafted priors such as CNN structures or self-similarity.", "cites": [524], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear explanation of the Deep Image Prior method and integrates the key idea from the cited paper that CNN structure itself can serve as a prior without training. It also offers a brief critical comparison with supervised methods and traditional upsampling, though the depth of critique is limited. The section abstracts the method's significance to suggest broader implications for SR research, such as the potential for combining deep learning with handcrafted priors."}}
{"id": "4249a5d4-6210-4dbd-8e01-2f372b42cfdf", "title": "Depth Map Super-resolution", "level": "subsection", "subsections": [], "parent_id": "93f3d938-3f64-432f-856b-181ab4d5675a", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Domain-Specific Applications"], ["subsection", "Depth Map Super-resolution"]], "content": "Depth maps record the depth (i.e., distance) between the viewpoint and objects in the scene, and plays important roles in many tasks like pose estimation  and semantic segmentation .  \nHowever, due to economic and production constraints, the depth maps produced by depth sensors are often low-resolution and suffer degradation effects such as noise, quantization and missing values.\nThus super-resolution is introduced for increasing the spatial resolution of depth maps.\nNowadays one of the most popular practices for depth map SR is to use another economical RGB camera to obtain HR images of the same scenes for guiding super-resolving the LR depth maps.\nSpecifically, Song \\textit{et al.}  exploit the depth field statistics and local correlations between depth maps and RGB images to constrain the global statistics and local structures.\nHui \\textit{et al.}  utilize two CNNs to simultaneously upsample LR depth maps and downsample HR RGB images, then use RGB features as the guidance for upsampling depth maps with the same resolution.\nAnd Haefner \\textit{et al.}  further exploit the color information and guide SR by resorting to the shape-from-shading technique.\nIn contrast, Riegler \\textit{et al.}  combine CNNs with an energy minimization model in the form of a powerful variational model to recover HR depth maps without other reference images.", "cites": [527, 525, 526, 7259], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from four cited papers to present a coherent narrative on RGB-guided and model-based approaches to depth map super-resolution. While it provides an analytical overview by contrasting methods that use RGB guidance with those that rely on variational models, it stops short of deeper critical evaluation or proposing a novel framework. The abstraction level is moderate, as it identifies common themes (e.g., use of RGB features, integration of variational methods) but does not generalize to broader principles or trends beyond the specific techniques described."}}
{"id": "6d553ba0-1664-4a99-bd40-41c57c208850", "title": "Face Image Super-resolution", "level": "subsection", "subsections": [], "parent_id": "93f3d938-3f64-432f-856b-181ab4d5675a", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Domain-Specific Applications"], ["subsection", "Face Image Super-resolution"]], "content": "Face image super-resolution, a.k.a. face hallucination (FH), can often help other face-related tasks .  \nCompared to generic images, face images have more face-related structured information, so incorporating facial prior knowledge (e.g., landmarks, parsing maps, identities) into FH is a very popular and promising approach.\nOne of the most straightforward way is to constrain the generated images to have the identical face-related attributes to ground truth.\nSpecifically, the CBN  utilizes the facial prior by alternately optimizing FH and dense correspondence field estimation.\nThe Super-FAN  and MTUN  both introduce FAN to guarantee the consistency of facial landmarks by end-to-end multi-task learning.\nAnd the FSRNet  uses not only facial landmark heatmaps but also face parsing maps as prior constraints.\nThe SICNN , which aims at recovering the real identity, adopts a super-identity loss function and a domain-integrated training approach to stable the joint training.\nBesides explicitly using facial prior, the implicit methods are also widely studied.\nThe TDN  incorporates spatial transformer networks  for automatic spatial transformations and thus solves the face unalignment problem.\nBased on TDN, the TDAE  adopts a decoder-encoder-decoder framework, where the first decoder learns to upsample and denoise, the encoder projects it back to aligned and noise-free LR faces, and the last decoder generates hallucinated HR images.\nIn contrast, the LCGE  employs component-specific CNNs to perform SR on five facial components, uses k-NN search on an HR facial component dataset to find corresponding patches, synthesizes finer-grained components and finally fuses them to FH results.\nSimilarly, Yang \\textit{et al.}  decompose deblocked face images into facial components and background, use the component landmarks to retrieve adequate HR exemplars in external datasets, perform generic SR on the background, and finally fuse them to complete HR faces.\nIn addition, researchers also improve FH from other perspectives.\nMotivated by the human attention shifting mechanism , the Attention-FH  resorts to a recurrent policy network for sequentially discovering attended face patches and performing local enhancement, and thus fully exploits the global interdependency of face images.\nThe UR-DGN  adopts a network similar to SRGAN  with adversarial learning.\nAnd Xu \\textit{et al.}  propose a multi-class GAN-based FH model composed of a generic generator and class-specific discriminators.\nBoth Lee \\textit{et al.}  and Yu \\textit{et al.}  utilize additional facial attribute information to perform FH with the specified attributes, based on the conditional GAN .", "cites": [529, 528, 531, 506, 7033, 492, 530], "cite_extract_rate": 0.3684210526315789, "origin_cites_number": 19, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section organizes the cited papers into explicit and implicit facial prior approaches, providing a basic synthesis of the methods. However, it primarily describes each paper's method without deeper comparative analysis or critical evaluation of their strengths and limitations. Some patterns are identified (e.g., multi-task learning, component-based SR), but the analysis remains at a surface level without meta-level insights."}}
{"id": "f10fbcca-4576-41af-b398-54da4a876ac6", "title": "Hyperspectral Image Super-resolution", "level": "subsection", "subsections": [], "parent_id": "93f3d938-3f64-432f-856b-181ab4d5675a", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Domain-Specific Applications"], ["subsection", "Hyperspectral Image Super-resolution"]], "content": "Compared to panchromatic images (PANs, i.e., RGB images with 3 bands), hyperspectral images (HSIs) containing hundreds of bands provide abundant spectral features and help various vision tasks . \nHowever, due to hardware limitations, collecting high-quality HSIs is much more difficult than PANs and the resolution is also lower.\nThus super-resolution is introduced into this field, and researchers tend to combine HR PANs and LR HSIs to predict HR HSIs.\nAmong them, Masi \\textit{et al.}  employ SRCNN  and incorporate several maps of nonlinear radiometric indices for boosting performance.\nQu \\textit{et al.}  jointly train two encoder-decoder networks to perform SR on PANs and HSIs, respectively, and transfer the SR knowledge from PAN to HSI by sharing the decoder and applying constraints such as angle similarity loss and reconstruction loss.\nRecently, Fu \\textit{et al.}  evaluate the effect of camera spectral response (CSR) functions for HSI SR and propose a CSR optimization layer which can automatically select or design the optimal CSR, and outperform the state-of-the-arts.", "cites": [533, 532], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a brief description of hyperspectral image super-resolution and mentions three representative papers, but it lacks deeper synthesis of their contributions. It does not compare methods or identify overarching trends, and there is minimal critical evaluation or abstraction of principles beyond the individual techniques described."}}
{"id": "ee943b58-26cc-44a8-ad44-ee0b138dc3f4", "title": "Real-world Image Super-resolution", "level": "subsection", "subsections": [], "parent_id": "93f3d938-3f64-432f-856b-181ab4d5675a", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Domain-Specific Applications"], ["subsection", "Real-world Image Super-resolution"]], "content": "Generally, the LR images for training SR models are generated by downsampling RGB images manually (e.g., by bicubic downsampling).\nHowever, real-world cameras actually capture 12-bit or 14-bit RAW images, and performs a series of operations (e.g., demosaicing, denoising and compression) through camera ISPs (image signal processors) and finally produce 8-bit RGB images.\nThrough this process, the RGB images have lost lots of original signals and are very different from the original images taken by the camera.\nTherefore, it is suboptimal to directly use the manually downsampled RGB image for SR.\nTo solve this problem, researchers study how to use real-world images for SR.\nAmong them, Chen \\textit{et al.}  analyze the relationships between image resolution (R) and field-of-view (V) in imaging systems (namely R-V degradation), propose data acquisition strategies to conduct a real-world dataset City100, and experimentally demonstrate the superiority of the proposed image synthesis model.\nZhang \\textit{et al.}  build another real-world image dataset SR-RAW (i.e., paired HR RAW images and LR RGB images) through optical zoom of cameras, and propose contextual bilateral loss to solve the misalignment problem.  \nIn contrast, Xu \\textit{et al.}  propose a pipeline to generate realistic training data by simulating the imaging process and develop a dual CNN to exploit the originally captured radiance information in RAW images.\nThey also propose to learn a spatially-variant color transformation for effective color corrections and generalization to other sensors.", "cites": [534, 535, 536], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the cited papers effectively by highlighting a common problemâloss of information in RGB imagesâand how each paper addresses it through different approaches (realistic data generation, dual CNN, and spatially-variant color correction). It also provides a critical comparison by noting that existing methods fail in real-world settings due to unrealistic training data. The abstraction is moderate, as it identifies a trend in the use of RAW images but does not generalize to a broader theoretical or methodological framework."}}
{"id": "4f792960-ffed-4436-a2b8-5beb48b58693", "title": "Video Super-resolution", "level": "subsection", "subsections": [], "parent_id": "93f3d938-3f64-432f-856b-181ab4d5675a", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Domain-Specific Applications"], ["subsection", "Video Super-resolution"]], "content": "For video super-resolution, multiple frames provide much more scene information, and there are not only intra-frame spatial dependency but also inter-frame temporal dependency (e.g., motions, brightness and color changes).\nThus the existing works mainly focus on making better use of spatio-temporal dependency, including explicit motion compensation (e.g., optical flow-based, learning-based) and recurrent methods, etc.\nAmong the optical flow-based methods, Liao \\textit{et al.}  employ optical flow methods to generate HR candidates and ensemble them by CNNs.\nVSRnet  and CVSRnet  deal with motion compensation by Druleas algorithm , and uses CNNs to take successive frames as input and predict HR frames.\nWhile Liu \\textit{et al.}  perform rectified optical flow alignment, and propose a temporal adaptive net to generate HR frames in various temporal scales and aggregate them adaptively.\nBesides, others also try to directly learn the motion compensation.\nThe VESPCN  utilizes a trainable spatial transformer  to learn motion compensation based on adjacent frames, and enters multiple frames into a spatio-temporal ESPCN  for end-to-end prediction.\nAnd Tao \\textit{et al.}  root from accurate LR imaging model and propose a sub-pixel-like module to simultaneously achieve motion compensation and super-resolution, and thus fuse the aligned frames more effectively.\nAnother trend is to use recurrent methods to capture the spatial-temporal dependency without explicit motion compensation.\nSpecifically, the BRCN  employs a bidirectional framework, and uses CNN, RNN, and conditional CNN to model the spatial, temporal and spatial-temporal dependency, respectively.\nSimilarly, STCN  uses a deep CNN and a bidirectional LSTM  to extract spatial and temporal information.\nAnd FRVSR  uses previously inferred HR estimates to reconstruct the subsequent HR frames by two deep CNNs in a recurrent manner.\nRecently the FSTRN  employs two much smaller 3D convolution filters to replace the original large filter, and thus enhances the performance through deeper CNNs while maintaining low computational cost.\nWhile the RBPN  extracts spatial and temporal contexts by a recurrent encoder-decoder, and combines them with an iterative refinement framework based on the back-projection mechanism (Sec. \\ref{sec_framework_up_down}).\nIn addition, the FAST  exploits compact descriptions of the structure and pixel correlations extracted by compression algorithms, transfers the SR results from one frame to adjacent frames, and much accelerates the state-of-the-art SR algorithms with little performance loss.  \nAnd Jo \\textit{et al.}  generate dynamic upsampling filters and the HR residual image based on the local spatio-temporal neighborhoods of each pixel, and also avoid explicit motion compensation.", "cites": [531, 7260, 538, 522, 539, 498, 537], "cite_extract_rate": 0.3684210526315789, "origin_cites_number": 19, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section organizes the cited works around key themes like optical flow-based methods and recurrent architectures, showing moderate synthesis. However, it primarily functions as a descriptive overview with minimal critical evaluation or identification of broader trends. There is some abstraction in grouping methods by motion compensation strategies, but deeper insight or evaluation is lacking."}}
{"id": "cd62f88e-a3c1-433d-b251-fc32a0c25662", "title": "Other Applications", "level": "subsection", "subsections": [], "parent_id": "93f3d938-3f64-432f-856b-181ab4d5675a", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Domain-Specific Applications"], ["subsection", "Other Applications"]], "content": "Deep learning based super-resolution is also adopted to other domain-specific applications and shows great performance.\nSpecifically, the Perceptual GAN  addresses the small object detection problem by super-resolving representations of small objects to have similar characteristics as large objects and be more discriminative for detection.\nSimilarly, the FSR-GAN  super-resolves small-size images in the feature space instead of the pixel space, and thus transforms the raw poor features to highly discriminative ones, which greatly benefits image retrieval.\nBesides, Jeon \\textit{et al.}  utilize a parallax prior in stereo images to reconstruct HR images with sub-pixel accuracy in registration.\nWang \\textit{et al.}  propose a parallax-attention model to tackle the stereo image super-resolution problem.\nLi \\textit{et al.}  incorporate the 3D geometric information and super-resolve 3D object texture maps.\nAnd Zhang \\textit{et al.}  separate view images in one light field into groups, learn inherent mapping for every group and finally combine the residuals in every group to reconstruct higher-resolution light fields.\nAll in all, super-resolution technology can play an important role in all kinds of applications, especially when we can deal with large objects well but cannot handle small objects.", "cites": [540, 541, 542], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a descriptive overview of various domain-specific applications of deep learning for image super-resolution, mentioning several methods and their uses. However, it lacks synthesis by not connecting the underlying principles or techniques across the cited papers. There is minimal critical analysis or abstraction, and the discussion remains largely at the level of individual contributions without deeper insights or evaluation of their effectiveness or limitations."}}
{"id": "b189992d-d4f2-4bdb-8aa8-56df37859ed4", "title": "Network Design", "level": "subsection", "subsections": [], "parent_id": "d99390bf-c1ab-42f0-a206-de70277ab044", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Conclusion and Future Directions"], ["subsection", "Network Design"]], "content": "Good network design not only determines a hypothesis space with great performance upper bound, but also helps efficiently learn representations without excessive spatial and computational redundancy.\nBelow we will introduce some promising directions for network improvements.\n\\textit{Combining Local and Global Information.}\nLarge receptive field provides more contextual information and helps generate more realistic results.\nThus it is promising to combine local and global information for providing contextual information of different scales for image SR.\n\\textit{Combining Low- and High-level Information.}\nShallow layers in CNNs tend to extract low-level features like colors and edges, while deeper layers learn higher-level representations like object identities.\nThus combining low-level details with high-level semantics can be of great help for HR reconstruction.\n\\textit{Context-specific Attention.}\nIn different contexts, people tend to care about different aspects of the images.\nFor example, for the grass area people may be more concerned with local colors and textures, while in the animal body area people may care more about the species and corresponding hair details.\nTherefore, incorporating attention mechanism to enhance the attention to key features facilitates the generation of realistic details.  \n\\textit{More Efficient Architectures.}\nExisting SR modes tend to pursue ultimate performance, while ignoring the model size and inference speed.\nFor example, the EDSR  takes 20s per image for $4\\times$ SR on DIV2K  with a Titan GTX GPU , and DBPN  takes 35s for $8\\times$ SR .\nSuch long prediction time is unacceptable in practical applications, thus more efficient architectures are imperative.\nHow to reduce model sizes and speed up prediction while maintaining performance remains a problem.\n\\textit{Upsampling Methods.}\nExisting upsampling methods (Sec. \\ref{sec_upsampling_methods}) have more or less disadvantages:\ninterpolation methods result in expensive computation and cannot be end-to-end learned, the transposed convolution produces checkerboard artifacts, the sub-pixel layer brings uneven distribution of receptive fields, and the meta upscale module may cause instability or inefficiency and have further room for improvement.\nHow to perform effective and efficient upsampling still needs to be studied, especially with high scaling factors.\nRecently, the neural architecture search (NAS) technique for deep learning has become more and more popular, greatly improving the performance or efficiency with little artificial intervention .\nFor the SR field, combining the exploration of the above directions with NAS is of great potential.", "cites": [543, 544, 8361, 482, 7255], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes insights from multiple papers to present a coherent narrative on the future of network design in image SR. It abstracts key challenges and opportunities, such as combining local/global and low/high-level information, context-specific attention, and the need for more efficient architectures. While it identifies limitations of existing methods, the critique remains focused on performance and efficiency issues without deeper evaluation of underlying assumptions or trade-offs."}}
{"id": "6e1445d8-2320-4c43-a72b-a4e4b5144a80", "title": "Learning Strategies", "level": "subsection", "subsections": [], "parent_id": "d99390bf-c1ab-42f0-a206-de70277ab044", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Conclusion and Future Directions"], ["subsection", "Learning Strategies"]], "content": "Besides good hypothesis spaces, robust learning strategies are also needed for achieving satisfactory results.\nNext we'll introduce some promising directions of learning strategies.\n\\textit{Loss Functions.}\nExisting loss functions can be regarded as establishing constraints among LR/HR/SR images, and guide optimization based on whether these constraints are met.\nIn practice, these loss functions are often weighted combined and the best loss function for SR is still unclear.\nTherefore, one of the most promising directions is to explore the potential correlations between these images and seek more accurate loss functions.\n\\textit{Normalization.}\nAlthough BN is widely used in vision tasks, which greatly speeds up training and improves performance, it is proven to be sub-optimal for super-resolution .\nThus other effective normalization techniques for SR are needed to be studied.", "cites": [7254, 482], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates and discusses general trends in learning strategies for image super-resolution, referencing the role of loss functions and normalization. It connects ideas across sources, such as the limitations of batch normalization and the need for better loss functions, but does not develop a novel framework or deeply evaluate the cited works. The abstraction level is moderate, as it identifies broader challenges without offering overarching theoretical insights."}}
{"id": "a669a929-2c70-4689-9f55-1e58edd497df", "title": "Evaluation Metrics", "level": "subsection", "subsections": [], "parent_id": "d99390bf-c1ab-42f0-a206-de70277ab044", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Conclusion and Future Directions"], ["subsection", "Evaluation Metrics"]], "content": "Evaluation metrics are one of the most fundamental components for machine learning.\nIf the performance cannot be measured accurately, researchers will have great difficulty verifying improvements.\nMetrics for super-resolution face such challenges and need more exploration.\n\\textit{More Accurate Metrics.}\nNowadays the PSNR and SSIM have been the most widely used metrics for SR.\nHowever, the PSNR tends to result in excessive smoothness and the results can vary wildly between almost indistinguishable images.\nThe SSIM  performs evaluation in terms of brightness, contrast and structure, but still cannot measure perceptual quality accurately .\nBesides, the MOS is the closest to human visual response, but needs to take a lot of efforts and is non-reproducible.\nAlthough researchers have proposed various metrics (Sec. \\ref{sec_iqa}), but currently there is no unified and admitted evaluation metrics for SR quality.\nThus more accurate metrics for evaluating reconstruction quality are urgently needed.\n\\textit{Blind IQA Methods.}\nToday most metrics used for SR are all-reference methods, i.e., assuming that we have paired LR-HR images with perfect quality.\nBut since it's difficult to obtain such datasets, the commonly used datasets for evaluation are often conducted by manual degradation.\nIn this case, the task we perform evaluation on is actually the inverse process of the predefined degradation.\nTherefore, developing blind IQA methods also has great demands.", "cites": [483], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a critical discussion of current evaluation metrics for image super-resolution, highlighting limitations of PSNR, SSIM, and MOS, and emphasizing the need for more accurate and unified metrics. It references Paper 1 to support its critique of PSNR's poor correlation with human perception. While the section identifies important gaps, it does not deeply synthesize multiple sources or present a novel framework, and the abstraction is limited to general observations rather than meta-level insights."}}
{"id": "6168fcfd-be9a-4482-a1b8-fa019e1df3c7", "title": "Towards Real-world Scenarios", "level": "subsection", "subsections": [], "parent_id": "d99390bf-c1ab-42f0-a206-de70277ab044", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", "Conclusion and Future Directions"], ["subsection", "Towards Real-world Scenarios"]], "content": "Image super-resolution is greatly limited in real-world scenarios, such as suffering unknown degradation, missing paired LR-HR images.\nBelow we'll introduce some directions towards real-world scenarios.\n\\textit{Dealing with Various Degradation.}\nReal-world images tend to suffer degradation like blurring, additive noise and compression artifacts.\nThus the models trained on datasets conducted manually often perform poorly in real-world scenes.\nSome works have been proposed for solving this , but these methods have some inherent drawbacks, such as great training difficulty and over-perfect assumptions.\nThis issue is urgently needed to be resolved.\n\\textit{Domain-specific Applications.}\nSuper-resolution can not only be used in domain-specific data and scenes directly, but also help other vision tasks greatly (Sec. \\ref{sec_domain_specific_apps}).\nTherefore, it is also a promising direction to apply SR to more specific domains, such as video surveillance, object tracking, medical imaging and scene rendering.\n\\appendices\n\\ifCLASSOPTIONcaptionsoff\n    \\newpage\n\\fi\n\\section*{Acknowledgment}\nProf. Jian Chen is supported by the Guangdong special branch plans young talent with scientific and technological innovation (Grant No. 2016TQ03X445), the Guangzhou science and technology planning project (Grant No. 201904010197) and Natural Science Foundation of Guangdong Province, China (2016A030313437).\n\\bibliographystyle{IEEEtran}\n\\bibliography{reference}\n\\begin{IEEEbiography}\n    [{\\includegraphics[width=1.1in,clip,keepaspectratio]{resource/bio_zhihao_wang.jpg}}]\n    {Zhihao Wang} received the BE degree in South China University of Technology (SCUT), China, in 2017, and is working toward the ME degree at the School of Software Engineering, SCUT.\n    Now he is as a visiting student at the School of Information Systems, Singapore Management University, Singapore. His research interests are computer vision based on deep learning, including visual recognition and image super-resolution.\n\\vspace{-0.2in}\n\\end{IEEEbiography}\n\\begin{IEEEbiography}\n    [{\\includegraphics[width=1.1in,clip,keepaspectratio]{resource/bio_jian_chen.png}}]\n    {Jian Chen} is currently a Professor of the School of Software Engineering at South China University of Technology where she started as an Assistant Professor in 2005.\n    She received her B.S. and Ph.D. degrees, both in Computer Science, from Sun Yat-Sen University, China, in 2000 and 2005 respectively.\n    Her research interests can be summarized as developing effective and efficient data analysis techniques for complex data and the related applications.\n\\vspace{-0.2in}\n\\end{IEEEbiography}\n\\begin{IEEEbiography}\n    [{\\includegraphics[width=1.1in,clip,keepaspectratio]{resource/bio_steven_hoi.pdf}}]\n    {Steven C. H.~Hoi} is currently the Managing Director of Salesforce Research Asia, and an Associate Professor (on leave) of the School of Information Systems, Singapore Management University, Singapore. Prior to joining SMU, he was an Associate Professor with Nanyang Technological University, Singapore. He received his Bachelor degree from Tsinghua University, P.R. China, in 2002, and his Ph.D degree in computer science and engineering from The Chinese University of Hong Kong, in 2006.\n    His research interests are machine learning and data mining and their applications to multimedia information retrieval (image and video retrieval), social media and web mining, and computational finance, etc., and he has published over 150 refereed papers in top conferences and journals in these related areas.\n    He has served as the Editor-in-Chief for Neurocomputing Journal, general co-chair for ACM SIGMM Workshops on Social Media (WSM'09, WSM'10, WSM'11), program co-chair for the fourth Asian Conference on Machine Learning (ACML'12), book editor for ``Social Media Modeling and Computing'', guest editor for ACM Transactions on Intelligent Systems and Technology (ACM TIST), technical PC member for many international conferences, and external reviewer for many top journals and worldwide funding agencies, including NSF in US and RGC in Hong Kong. He is an IEEE Fellow and ACM Distinguished Member.\n\\vspace{-0.2in}\n\\end{IEEEbiography}\n\\clearpage\n\\if 0", "cites": [484, 523, 518], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of challenges in applying SR to real-world scenarios, such as unknown degradation and lack of LR-HR pairs. It integrates ideas from cited papers to highlight these issues and mentions promising application domains. However, it offers only basic synthesis and limited critical evaluation of the cited works, such as their assumptions and training difficulties, without deeper comparison or meta-level generalization."}}
{"id": "8f0f5d78-51f4-4970-b0a8-a53685116af3", "title": "", "level": "section", "subsections": [], "parent_id": "2c20cd26-fec7-4463-b98c-798fc974562d", "prefix_titles": [["title", "Deep Learning for Image Super-resolution:\\\\A Survey"], ["section", ""]], "content": "For better reading of this survey, we provide all the notations used in this survey and their detailed definitions in Table \\ref{tab_notations}.\nAnd we also list the full text of all the abbreviations used herein in Table \\ref{tab_abbreviations}.\n\\fi\n\\setcounter{table}{0}\n\\setcounter{page}{1}\n\\begin{table*}[h]\n    \\renewcommand{\\arraystretch}{1.3}\n    \\caption{Notations.}\n    \\label{tab_notations}\n    \\centering\n    \\begin{tabular}{|c|l|}\n        \\hline\n        Notation        & Description \\\\\n        \\hline\n        $I_x$           & LR image \\\\\n        $I_y$           & ground truth HR image, abbreviated as $I$ \\\\\n        $\\hat{I_y}$     & reconstructed HR image, abbreviated as $\\hat{I}$ \\\\\n        $I_s$           & randomly sampled HR image from the real HR images \\\\\n        \\hline\n        $I(i)$          & intensity of the $i$-th pixel of image $I$ \\\\\n        $D$             & discriminator network of GAN \\\\\n        $\\phi$          & image classification network \\\\\n        $\\phi^{(l)}$    & extracted representations on $l$-th layer by $\\phi$ \\\\\n        $\\operatorname{vec}$ &  vectorization operation \\\\\n        $G^{(l)}$       & Gram matrix of representations on $l$-th layer \\\\\n        $l$             & layer of CNN \\\\\n        $h$, $w$, $c$   & width, height and number of channels of feature maps \\\\\n        $h_l$, $w_l$, $c_l$  & width, height and number of channels of feature maps in $l$-th layer \\\\\n        \\hline\n        $\\mathcal{D}$   & degradation process \\\\\n        $\\delta$        & parameters of $\\mathcal{D}$ \\\\\n        $\\mathcal{F}$   & super-resolution process \\\\\n        $\\theta$        & parameters of $\\mathcal{F}$ \\\\\n        \\hline\n        $\\otimes$       & convolution operation \\\\\n        $\\kappa$        & convolution kernel \\\\\n        $\\downarrow$    & downsampling operation \\\\\n        $s$             & scaling factor \\\\\n        $n$             & Gaussian noise \\\\\n        $\\varsigma$     & standard deviation of $n$ \\\\\n        $z$             & a random vector \\\\\n        \\hline\n        $\\mathcal{L}$   & loss function \\\\\n        $\\mathcal{L}_{\\text{content}}$     & content loss \\\\\n        $\\mathcal{L}_{\\text{cycle}}$       & content consistency loss \\\\\n        $\\mathcal{L}_{\\text{pixel\\_l1}}$   & pixel L1 loss \\\\\n        $\\mathcal{L}_{\\text{pixel\\_l2}}$   & pixel L2 loss \\\\\n        $\\mathcal{L}_{\\text{pixel\\_Cha}}$  & pixel Charbonnier loss \\\\\n        $\\mathcal{L}_{\\text{gan\\_ce\\_g}}$, $\\mathcal{L}_{\\text{gan\\_ce\\_d}}$  & adversarial loss of the generator and discriminator based on cross entropy \\\\\n        $\\mathcal{L}_{\\text{gan\\_hi\\_g}}$, $\\mathcal{L}_{\\text{gan\\_hi\\_d}}$  & adversarial loss of the generator and discriminator based on hinge error \\\\\n        $\\mathcal{L}_{\\text{gan\\_ls\\_g}}$, $\\mathcal{L}_{\\text{gan\\_ls\\_g}}$  & adversarial loss of the generator and discriminator based on least square error \\\\\n        $\\mathcal{L}_{\\text{TV}}$          & total variation loss \\\\\n        $\\Phi$          & regularization term \\\\\n        $\\lambda$       & tradeoff parameter of $\\Phi$ \\\\\n        $\\epsilon$      & small instant for stability \\\\\n        \\hline\n        $\\mu_I$         & luminance of image $I$, i.e., mean of intensity \\\\\n        $\\sigma_I$      & contrast of image $I$, i.e., standard deviation of intensity \\\\\n        $\\sigma_{I,\\hat{I}}$ & covariance between images $I$ and $\\hat{I}$ \\\\\n        $\\mathcal{C}_l$, $\\mathcal{C}_c$, $\\mathcal{C}_s$ & comparison function of luminance, contrast, structure \\\\\n        $\\alpha$, $\\beta$, $\\gamma$                       & weights of $\\mathcal{C}_l$, $\\mathcal{C}_c$, $\\mathcal{C}_s$ \\\\\n        $C_1$, $C_2$, $C_3$  & constants \\\\\n        $k_1$, $k_2$         & constants \\\\\n        \\hline\n        $L$  & maximum possible pixel value \\\\\n        $N$  & number of pixels \\\\\n        $M$  & number of bins \\\\\n        \\hline\n    \\end{tabular}\n\\end{table*}\n\\begin{table*}[h]\n    \\renewcommand{\\arraystretch}{1.3}\n    \\caption{Abbreviations.}\n    \\label{tab_abbreviations}\n    \\centering\n    \\begin{tabular}{|clcl|}\n        \\hline\n        Abbreviation & Full name   & Abbreviation & Full name \\\\\n        \\hline\n        FH & face hallucination    & PAN & panchromatic image \\\\\n        HR & high-resolution       & SR & super-resolution \\\\\n        HSI & hyperspectral image  & TV & total variation \\\\\n        HVS & human visual system  & WT & wavelet transformation \\\\\n        LR & low-resolution        & & \\\\\n        \\hline\n        FSIM  & feature similarity                     & MS-SSIM  & multi-scale SSIM \\\\\n        IQA & image quality assessment                                        & NIQE  & natural image quality evaluator \\\\\n        MOS & mean opinion score                                              & PSNR & peak signal-to-noise ratio \\\\\n        MSSIM  & mean SSIM                             & SSIM  & structural similarity \\\\\n        \\hline\n        BN  & batch normalization           & GAN  & generative adversarial net \\\\\n        CNN & convolutional neural network                            & LSTM & long short term memory network \\\\\n        CycleGAN  & cycle-in-cycle GAN      & ResNet  & residual network \\\\\n        DenseNet  & densely connected CNN  & SENet  & squeeze-and-excitation network \\\\\n        FAN & face alignment network                                  & SPMC  & sub-pixel motion compensation \\\\\n        \\hline\n        ADRSR  & automated decomposition and reconstruction     & LCGE  & learn FH via component generation and enhancement \\\\\n        Attention-FH  & attention-aware FH                 & MemNet  & memory network \\\\\n        BRCN  & bidirectional recurrent CNN & MS-LapSRN  & multi-scale LapSRN \\\\\n        CARN  & cascading residual network                      & MSRN  & multiscale residual network \\\\\n        CARN-M  & CARM based on MobileNet                       & MTUN  & multi-task upsampling network \\\\\n        CBN  & cascaded bi-network                              & MWCNN  & multi-level wavelet CNN \\\\\n        CinCGAN  & cycle-in-cycle GAN                 & ProSR  & progressive SR \\\\\n        CNF  & context-wise network fusion                    & RBPN  & recurrent back-projection network \\\\\n        CVSRnet  & compressed VSRnet                      & RCAN  & residual channel attention networks \\\\\n        DBPN  & deep back-projection network                  & RDN  & residual dense network \\\\\n        DNSR  & denoising for SR                                & RNAN  & residual non-local attention networks \\\\\n        DRCN  & deeply-recursive CNN                          & SAN  & Second-order Attention Network \\\\\n        DRRN  & deep recursive residual network                & SFT-GAN  & GAN with spatial feature transformation \\\\\n        DSRN  & dual-state recurrent network                   & SICNN  & super-identity CNN \\\\\n        DWSR  & deep wavelet prediction for SR                 & SOCA  & second-order channel attention \\\\\n        EDSR  & enhanced deep SR network                   & SRCNN  & SR CNN \\\\\n        EDSR-PP  & EDSR with pyramid pooling             & SRFBN  & SR feedback network \\\\\n        ESPCN  & efficient sub-pixel CNN                  & SRGAN  & SR GAN \\\\\n        ESRGAN  & enhanced SRGAN                            & SRDenseNet  & SR DenseNet \\\\\n        FAST  & free adaptive SR via transfer                & STCN  & spatial-temporal CNN \\\\\n        FRVSR  & frame-recurrent video SR                  & TDAE  & transformative discriminative auto-encoder \\\\\n        FSRCNN  & fast SRCNN                           & TDN  & transformative discriminative network \\\\\n        FSR-GAN  & feature SRGAN                             & Super-FAN  & SR with FAN \\\\\n        FSRNet  & face SR network                            & UR-DGN  & ultra-resolving by discriminative generative networks \\\\\n        FSTRN  & fast spatio-temporal ResNet                     & VESPCN  & video ESPCN \\\\\n        IDN  & information distillation network                 & VSRnet  & video SR network \\\\\n        LapSRN  & Laplacian pyramid SR network & ZSSR  & zero-shot SR \\\\\n        MDSR  & multi-scale deep SR system                 & & \\\\\n        \\hline\n    \\end{tabular}\n\\end{table*}\n\\end{document}", "cites": [7031, 7256, 497, 528, 7255, 506, 7260, 7033, 500, 538, 495, 7258, 522, 489, 97, 7034, 71, 481, 493, 482, 539, 487, 7035, 7030, 496, 501, 7254, 498, 8358, 537, 96, 492, 518, 7022, 530], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 63, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.0, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section does not provide any synthesis, critical analysis, or abstraction. It is a purely descriptive section that lists tables of notations and abbreviations, referencing the cited papers only by name without integrating or discussing their contributions, limitations, or relationships."}}
