{"id": "ed4f3055-e5dc-420d-8009-404bb4d82113", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "4fd906cb-4aa6-49a2-a5b7-ec10dfbd9280", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "Introduction"]], "content": "\\label{sec:introduction}}\n\\IEEEPARstart{D}{eep} neural networks (DNNs) are playing an important role in various areas including computer vision (CV)  and natural language processing (NLP) . \nRecent years have witnessed many successful deep models such as AlexNet , VGG , GoogleNet , ResNet , DenseNet  and Transformer . These {architectural innovations} have enabled the training of deeper, more accurate and more efficient models. The recent research on neural architecture search (NAS)  further speeds up the process of designing more powerful structures. However, most of the prevalent deep learning models perform inference in a static manner, i.e., both the computational graph and the network parameters are fixed once trained, which may limit their representation power, efficiency and interpretability . \nDynamic networks, as opposed to static ones, can adapt their structures or parameters to the input during inference, and therefore enjoy favorable properties that are absent in static models. In general, dynamic computation in the context of deep learning has the following advantages: \n\\begin{figure*}\n  \\centering\n  \\vspace{-2ex}\n    \\includegraphics[width=0.875\\linewidth]{1_overall.pdf}\n    \\vskip -0.175in\n    \\centering\\caption{Overview of the survey. {We first review the dynamic networks that perform adaptive computation at three different granularities (i.e. sample-wise, spatial-wise and temporal-wise). Then we summarize the decision making strategy, training technique and applications of dynamic models. Existing open problems in this field together with some future research directions are finally discussed. Best viewed in color.}}\n    \\label{framework}\n    \\vspace{-3ex}\n\\end{figure*}\n\\textbf{1) Efficiency.} One of the most notable advantages of dynamic networks is that they are able to allocate computations on demand at test time, by selectively activating model components (e.g. layers , channels  or sub-networks ) \\emph{conditioned} on the input. Consequently, less computation is spent on canonical samples that are relatively easy to recognize, or on less informative spatial/temporal locations of an input. {In addition to \\emph{computational efficiency}, dynamic models have also shown promising results for improving \\emph{data efficiency} in the scenario of few-shot learning .}\n\\textbf{2) Representation power.} Due to the data-dependent network architecture/parameters, dynamic networks have significantly enlarged parameter space and improved representation power. For example, with a minor increase of computation, model capacity can be boosted by applying feature-conditioned attention weights on an ensemble of convolutional kernels . It is worth noting that the popular soft attention mechanism could also be unified in the framework of dynamic networks, as different channels , spatial areas  or temporal locations  of features are dynamically re-weighted at test time. \n\\textbf{3) Adaptiveness.} Dynamic models are able to achieve a desired trade-off between accuracy and efficiency for dealing with varying computational budgets on the fly. Therefore, they are more adaptable to different hardware platforms and changing environments, compared to static models with a fixed computational cost.\n\\textbf{4) Compatibility.} Dynamic networks are compatible with most advanced techniques in deep learning, including architecture design , optimization algorithms  and data preprocessing , which ensures that they can benefit from the most recent advances in the field to achieve state-of-the-art performance. For example, dynamic networks can inherit architectural innovations in lightweight models , or be designed via NAS approaches . Their efficiency could also be further improved by acceleration methods developed for static models, such as network pruning , weight quantization , knowledge distillation  and low-rank approximation .\n\\textbf{5) Generality.} As a substitute for static deep learning techniques, many dynamic models are general approaches that can be applied seamlessly to a wide range of applications, such as image classification , object detection  and semantic segmentation . Moreover, the techniques developed in CV tasks are proven to transfer well to language models in NLP tasks , and vice versa.\n\\textbf{6) Interpretability.} We finally note that the research on dynamic networks {may bridge} the gap between the underlying mechanism of deep models and brains, as it is believed that the brains process information in a dynamic way~. It is possible to analyze which components of a dynamic model are activated  when processing an input sample, and to observe which parts of the input are accountable for certain predictions . These properties may shed light on interpreting the decision process of DNNs.\n\\begin{table}\n  \\vspace{-1ex}\n  \\caption{Notations used in this paper.}\n  \\label{tab:notations}\n  \\vspace{-4ex}\n  \\begin{center}\n    \\begin{tabular}{c|c}\n      \\hline\n      \\textbf{Notations} & \\textbf{Descriptions} \\\\\n      \\hline\n      $\\mathbb{R}^m$ & $m$-dimensional real number domain \\\\\n      \\hline\n      $a, \\mathbf{a}$ & Scalar, vector/matrix/tensor \\\\\n      \\hline\n      $\\mathbf{x,y}$ & Input, output feature \\\\\n      \\hline\n      $\\mathbf{x}^{\\ell}$ & Feature at layer $\\ell$ \\\\\n      \\hline\n      $\\mathbf{h}_t$ & Hidden state at time step $t$ \\\\\n      \\hline\n      $\\mathbf{x(p)}$ & Feature at spatial location $\\mathbf{p}$ on $\\mathbf{x}$ \\\\\n      \\hline\n      $\\bm{\\Theta}$ & Learnable parameter \\\\\n      \\hline\n      $\\bm{\\hat{\\Theta}}|\\mathbf{x}$ & Dynamic parameter conditioned on $\\mathbf{x}$ \\\\\n      \\hline\n      $\\mathbf{x}\\star\\mathbf{W}$ & Convolution of feature $\\mathbf{x}$ and weight $\\mathbf{W}$ \\\\\n      \\hline\n      $\\otimes$ & Channel-wise or element-wise multiplication \\\\\n      \\hline\n      $\\mathcal{F}(\\cdot,\\bm{\\Theta})$ & Functional Operation parameterized by $\\bm{\\Theta}$ \\\\\n      \\hline\n      $\\mathcal{F}\\circ\\mathcal{G}$ & Composition of function $\\mathcal{F}$ and $\\mathcal{G}$ \\\\\n      \\hline\n    \\end{tabular}\n  \\end{center}\n  \\vskip -0.35in\n\\end{table}\nIn fact, adaptive inference, the key idea underlying dynamic networks, has been studied before the popularity of modern DNNs. The most classical approach is building a model ensemble through a cascaded  or parallel  structure, and selectively activating the models conditioned on the input. Spiking neural networks (SNNs)  also perform data-dependent inference by propagating pulse signals. However, the training strategy for SNNs is quite different from that of popular convolutional neural networks (CNNs), and they are less used in vision tasks. Therefore, we leave out the work related to SNNs in this survey. \n\\begin{figure}\n  \\centering\n    \\vspace{-1ex}\n    \\includegraphics[width=\\linewidth]{2_dynamic_cascade.pdf}\n    \\vskip -0.15in\n    \\caption{Two early-exiting schemes. The dashed lines and shaded modules are not executed, conditioned on the decisions made by the routers.}\n    \\label{cascading_models}\n    \\vskip -0.2in\n\\end{figure}\n\\begin{figure*}\n  \\centering\n  \\vspace{-2ex}\n    \\includegraphics[width=\\linewidth]{3_multi_scale_SuperNet.pdf}\n    \\vskip -0.175in\n    \\caption{{Multi-scale architectures with dynamic inference graphs. The first three models (a, b, c) perform adaptive early exiting with specific architecture designs and exiting policies. Dynamic routing is achieved inside a SuperNet (d) to activate data-dependent inference paths.}}\n    \\label{multi_scale}\n    \\vspace{-3ex}\n\\end{figure*}\nIn the context of deep learning, dynamic inference with modern deep architectures, has raised many new research questions and has attracted great research interests in the past three years.\nDespite the extensive work on designing various types of dynamic networks, a systematic and comprehensive review on this topic is still lacking. This motivates us to write this survey, to review the recent advances in this rapidly developing area, with the purposes of 1) providing an overview as well as new perspectives for researchers who are interested in this topic; 2) pointing out the close relations of different subareas and reducing the risk of reinventing the wheel; and 3) summarizing the key challenges and possible future research directions.\n{This survey is organized as follows (see \\figurename~\\ref{framework} for an overview). In Sec. \\ref{sec_sample_wise}, we introduce the most common \\emph{sample-wise} dynamic networks which adapt their architectures or parameters conditioned on each input sample. Dynamic models working on a finer granularity, i.e., \\emph{spatially} adaptive and \\emph{temporally} adaptive models, are reviewed in Sec. \\ref{sec_spatially_adaptive} and Sec.\\ref{sec_temporal_adaptive}, respectively\\footnote{{These two categories can also be viewed as sample-wise dynamic networks as they perform adaptive computation within each sample at a finer granularity, and we adopt such a split for narrative convenience.}}. Then we investigate the decision making strategies and the training techniques of dynamic networks in Sec. \\ref{inference_and_train}. The applications of dynamic models are further summarized in Sec. \\ref{sec_tasks}. Finally, we conclude this survey with a discussion on a number of open problems and future research directions in Sec. \\ref{sec:discussion}. For better readability, we list the notations that will be used in this survey in Table \\ref{tab:notations}.}\n\\vspace{-2ex}", "cites": [71, 96, 679, 681, 687, 544, 305, 682, 683, 675, 38, 504, 97, 686, 684, 680, 677, 676, 7266, 8372, 7268, 7, 678, 7267, 674, 685, 306], "cite_extract_rate": 0.6428571428571429, "origin_cites_number": 42, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes a wide range of cited papers to present a coherent narrative around dynamic neural networks, categorizing them by adaptive inference types. It abstracts key advantages (efficiency, adaptiveness, compatibility) that unify the diverse approaches. While it identifies limitations (e.g., SNNs being less used in vision tasks), it could offer more in-depth critical evaluation of these methods' trade-offs and challenges."}}
{"id": "3befdcee-686f-4399-9d86-c1c11ef02509", "title": "Dynamic Architectures", "level": "subsection", "subsections": ["ca8e1af0-84b8-4669-8aed-b0ba1c15924a", "9747d6b0-66bd-4364-a0a9-4ece0a43fe6e", "ad51ce7b-9d07-4edc-88e3-0822d3ae0397"], "parent_id": "58436ad0-55e1-4da8-b7e8-6bf9edce3fb1", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "{Sample-wise Dynamic Networks"], ["subsection", "Dynamic Architectures"]], "content": "\\label{dynamic_arch}\nConsidering different inputs may have diverse computational demands, it is natural to perform inference with dynamic architectures conditioned on each sample. Specifically, one can adjust the network depth (Sec. \\ref{dynamic_depth}), width (Sec. \\ref{dynamic_width}), or perform dynamic routing within a super network (SuperNet) that includes multiple possible paths (Sec. \\ref{SuperNets}). Networks with dynamic architectures not only save redundant computation for canonical (\"easy\") samples, but also preserve their representation power when recognizing non-canonical (\"hard\") samples. Such a property leads to remarkable advantages in efficiency compared to the acceleration techniques for static models , which handle \"easy\" and \"hard\" inputs with identical computation, and fail to reduce intrinsic computational redundancy.\n\\vspace{-1.5ex}", "cites": [688, 685, 687], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the concept of dynamic architectures by integrating ideas from different types of efficiency improvements (depth, width, routing). It abstracts these into a general framework of dynamic computation per sample but does not provide deep comparative or critical analysis of the cited papers. The insights are primarily conceptual rather than evaluative."}}
{"id": "ca8e1af0-84b8-4669-8aed-b0ba1c15924a", "title": "Dynamic Depth", "level": "subsubsection", "subsections": [], "parent_id": "3befdcee-686f-4399-9d86-c1c11ef02509", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "{Sample-wise Dynamic Networks"], ["subsection", "Dynamic Architectures"], ["subsubsection", "Dynamic Depth"]], "content": "\\label{dynamic_depth}\nAs modern DNNs are getting increasingly deep for recognizing more \"hard\" samples, a straightforward solution to reducing redundant computation is performing inference with dynamic depth, which can be realized by 1) \\emph{early exiting}, i.e. allowing \"easy\" samples to be output at shallow exits without executing deeper layers ; or 2) \\emph{layer skipping}, i.e. selectively skipping intermediate layers conditioned on each sample .\n\\begin{figure*}\n  \\vspace{-2ex}\n  \\centering\n    \\includegraphics[width=\\linewidth]{4_skip_layers.pdf}\n    \\vskip -0.175in\n    \\caption{Dynamic layer skipping. Feature $\\mathbf{x}_4$ in (a) are not calculated conditioned on the halting score, and the gating module in (b) decides whether to execute the block based on the intermediate feature. The policy network in (c) generates the skipping decisions for all layers in the main network.}\n    \\label{skipping_layers}\n    \\vspace{-3ex}\n\\end{figure*}\n\\noindent\\textbf{1) Early exiting.} The complexity (or \"difficulty\") of input samples varies in most real-world scenarios, and shallow networks are capable of correctly identifying many canonical inputs. Ideally, these samples should be output at certain early exits without executing deeper layers.\nFor an input sample $\\mathbf{x}$, the forward propagation of an $L$-layer deep network $\\mathcal{F}$ could be represented by \n\\begin{equation}\n  \\setlength{\\abovedisplayskip}{3pt}\n  \\mathbf{y} = \\mathcal{F}^{L}\\circ\\mathcal{F}^{L-1}\\circ\\cdots\\circ\\mathcal{F}^1(\\mathbf{x}),\n  \\setlength{\\belowdisplayskip}{3pt}\n\\end{equation}\n\\noindent where $\\mathcal{F}^{\\ell}$ denotes the operational function at layer $\\ell,1\\!\\le \\ell\\!\\le L$. In contrast, early exiting allows to terminate the inference procedure at an intermediate layer. For the $i$-th input sample $\\mathbf{x}_i$, the forward propagation can be written as\n\\begin{equation}\n  \\setlength{\\abovedisplayskip}{3pt}\n  \\mathbf{y}_i = \\mathcal{F}^{\\ell_i}\\circ\\mathcal{F}^{\\ell_i-1}\\circ\\cdots\\circ\\mathcal{F}^1(\\mathbf{x}_i), 1\\!\\le \\ell_i\\!\\le L.\n  \\setlength{\\belowdisplayskip}{3pt}\n\\end{equation}\nNote that $\\ell_i$ is adaptively determined based on $\\mathbf{x}_i$. Extensive architectures have been studied to endow DNNs with such early exiting behaviors, as discussed in the following.\na) \\emph{Cascading of DNNs.} The most intuitive approach to enabling early exiting is cascading multiple models (see \\figurename~\\ref{cascading_models} (a)), and adaptively retrieving the prediction of an early network without activating latter ones. For example, Big/little-Net  cascades two CNNs with different depths. After obtaining the \\emph{SoftMax} output of the first model, early exiting is conducted when the score margin between the two largest elements exceeds a threshold. {Moreover, a number of classic CNNs  are cascaded in  and }. After each model, a decision function is trained to determine whether the obtained feature should be fed to a linear classifier for immediate prediction, or be sent to subsequent models.\nb) \\emph{Intermediate classifiers.}\nThe models in the aforementioned cascading structures are mutually independent. Consequently, \nonce a \"difficult\" sample is decided to be fed to a latter network, a whole inference procedure needs to be executed from scratch without reusing the already learned features. A more compact design is involving intermediate classifiers within one backbone network (see \\figurename~\\ref{cascading_models} (b)), so that early features can be propagated to deep layers if needed. Based on such a  multi-exit architecture, adaptive early exiting is typically achieved according to confidence-based criteria  or learned functions .\nc) \\emph{Multi-scale architecture with early exits.} Researchers  have observed that in chain-structured networks, the multiple classifiers may interfere with each other, which degrades the overall performance. A reasonable interpretation could be that in regular CNNs, the high-resolution features lack the coarse-level information that is essential for classification, leading to unsatisfying results for early exits. Moreover, early classifiers would force the shallow layers to generate \\emph{task-specialized} features, while a part of \\emph{general} information is lost, resulting in degraded performance for deep exits. To tackle this issue, multi-scale dense network (MSDNet)  adopts 1) a \\emph{multi-scale} architecture, {which consists of multiple sub-networks for processing feature maps with different resolutions (scales)}, to quickly generate coarse-level features that are suitable for classification; 2) \\emph{dense connections}, to reuse early features and improve the performance of deep classifiers (see \\figurename~\\ref{multi_scale} (a)). Such a specially-designed architecture effectively enhances the overall accuracy of all the classifiers in the network. \n{Based on the multi-scale architecture design, researchers have also studied the exiting policies  (see \\figurename~\\ref{multi_scale} (b)) and training schemes  of early-exiting dynamic models. More discussion about the inference and training schemes for dynamic models will be presented in Sec. \\ref{inference_and_train}.}\nPrevious methods typically achieve the adaptation of network depths. From the perspective of exploiting spatial redundancy in features, resolution adaptive network (RANet, see \\figurename~\\ref{multi_scale} (c))  first processes each sample with low-resolution features, while high-resolution representations are conditionally utilized based on early predictions.\nAdaptive early exiting is also extended to language models (e.g. BERT ) for improving their efficiency on NLP tasks . In addition, it can be implemented in recurrent neural networks (RNNs) for \\emph{temporally} dynamic inference when processing sequential data such as videos  and texts  (see Sec. \\ref{sec_temporal_adaptive}).\n\\noindent\\textbf{2) Layer skipping.} The general idea of the aforementioned early-exiting paradigm is skipping the execution of all the deep layers after a certain classifier. More flexibly, the network depth can also be adapted on the fly by strategically skipping the calculation of \\emph{intermediate layers} without placing extra classifiers. Given the $i$-th input sample $\\mathbf{x}_i$, dynamic layer skipping could be generally written as\n\\begin{equation}\n  \\setlength{\\abovedisplayskip}{3pt}\n  \\mathbf{y}_i = (\\mathds{1}^{L}\\circ\\mathcal{F}^{L})\\circ (\\mathds{1}^{L-1}\\circ\\mathcal{F}^{L-1})\\circ\\cdots\\circ (\\mathds{1}^1\\circ\\mathcal{F}^1)(\\mathbf{x}_i),\n  \\setlength{\\belowdisplayskip}{3pt}\n\\end{equation}\nwhere $\\mathds{1}^{\\ell}$ denotes the indicator function determining the execution of layer $\\mathcal{F}^{\\ell}, 1\\!\\le\\!\\ell\\!\\le\\!L$.\nThis scheme is typically implemented on structures with skip connections (e.g. ResNet ) to guarantee the continuity of forward propagation, and here we summarize three common approaches.\na) \\emph{Halting score} is first proposed in , where an accumulated scalar named halting score adaptively decides whether the hidden state of an RNN will be directly fed to the next time step. The halting scheme is extended to vision tasks by viewing residual blocks within a ResNet stage \\footnote{Here we refer to a stage as a stack of multiple residual blocks with the same feature resolution.} as linear layers within a step of RNN  (see \\figurename~\\ref{skipping_layers} (a)).\nRather than skipping the execution of layers with independent parameters, multiple blocks in each ResNet stage could be replaced by one weight-sharing block, leading to a significant reduction of parameters . In every stage, the block is executed for an adaptive number of steps according to the halting score.\nIn addition to RNNs and CNNs, the halting scheme is further implemented on Transformers  by  and  to achieve dynamic network depth on NLP tasks.\nb) {\\emph{Gating function} is also a prevalent option for dynamic layer skipping due to its plug-and-play property.} Take ResNet as an example (see \\figurename~\\ref{skipping_layers} (b)), let $\\mathbf{x}^{\\ell}$ denote the input feature of the $\\ell$-th residual block, gating function $\\mathcal{G}^{\\ell}$ generates a binary value to decide the execution of residual block $\\mathcal{F}^{\\ell}$. This procedure could be represented by\\footnote{For simplicity and without generality, the subscript for sample index will be omitted in the following.}\n\\begin{equation}\n  \\setlength{\\abovedisplayskip}{3pt}\n  \\mathbf{x}^{\\ell+1} = \\mathcal{G}^{\\ell}(\\mathbf{x}^{\\ell})\\mathcal{F}^{\\ell}(\\mathbf{x}^{\\ell}) + \\mathbf{x}^{\\ell}, \\mathcal{G}^{\\ell}(\\mathbf{x}^{\\ell})\\in\\{0,1\\}.\n  \\setlength{\\belowdisplayskip}{3pt}\n\\end{equation}\nSkipNet  and convolutional network with adaptive inference graph (Conv-AIG)  are two typical approaches to enabling dynamic layer skipping. Both methods induce lightweight computational overheads to efficiently produce the binary decisions on whether skipping the calculation of a residual block. Specifically, Conv-AIG utilizes two FC layers in each residual block, while the gating function in SkipNet is implemented as an RNN for parameter sharing.\n\\begin{figure*}\n  \\centering\n  \\vspace{-2ex}\n    \\includegraphics[width=0.85\\linewidth]{5_multi_branch.pdf}\n    \\vskip -0.175in\n    \\caption{MoE with soft weighting (a) and hard gating (b) schemes both adopt an auxiliary module to generate the weights or gates. In the tree structure (c), features (nodes) and transformations (paths) are represented as circles and lines with arrows respectively. Only the full lines are activated.}\n    \\label{multi_branch}\n    \\vspace{-3ex}\n\\end{figure*}\nRather than skipping layers in classic ResNets, dynamic recursive network  iteratively executes one block with shared parameters in each stage. Although the weight-sharing scheme seems similar to the aforementioned IamNN , the skipping policy of  differs significantly: gating modules are exploited to decide the recursion depth.\nInstead of either skipping a layer, or executing it thoroughly with a full numerical precision, a line of work  studies adaptive \\emph{bit-width} for different layers conditioned on the \\emph{resource budget}. Furthermore, fractional skipping  adaptively selects a bit-width for each residual block by a gating function based on \\emph{input features}.\nc) {\\emph{Policy network} can be built to take in an input sample, and directly produces the skipping decisions for all the layers in a backbone network  (see \\figurename~\\ref{skipping_layers} (c)).}\n\\vspace{-1.5ex}", "cites": [698, 695, 7273, 305, 693, 683, 690, 691, 7269, 8373, 38, 7270, 697, 97, 692, 694, 696, 676, 7272, 7271, 7266, 689, 7], "cite_extract_rate": 0.6216216216216216, "origin_cites_number": 37, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.3, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by organizing diverse approaches (early exiting, layer skipping, multi-scale designs) into a coherent framework, showing how they relate to dynamic depth. It provides some critical analysis by acknowledging limitations such as interference among classifiers and the need for specialized training schemes. The abstraction is evident in the general formulation of dynamic depth and the discussion of broader design principles like spatial redundancy and model efficiency."}}
{"id": "9747d6b0-66bd-4364-a0a9-4ece0a43fe6e", "title": "Dynamic Width", "level": "subsubsection", "subsections": [], "parent_id": "3befdcee-686f-4399-9d86-c1c11ef02509", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "{Sample-wise Dynamic Networks"], ["subsection", "Dynamic Architectures"], ["subsubsection", "Dynamic Width"]], "content": "\\label{dynamic_width}\nIn addition to dynamic network \\emph{depth} (Sec. \\ref{dynamic_depth}), a finer-grained form of conditional computation is performing inference with dynamic \\emph{width}: although every layer is executed, its multiple units (e.g. neurons, channels or branches) are selectively activated conditioned on the input.\n\\noindent\\textbf{1) {Skipping neurons in fully-connected (FC) layers.}} The computational cost of an FC layer is determined by its input and output dimensions. It is commonly believed that different neuron units are responsible for representing different features, and therefore not all of them need to be activated for every sample. Early studies learn to adaptively control the neuron activations by auxiliary branches  or other techniques such as low-rank approximation .\n\\noindent\\textbf{2) {Skipping branches in mixture-of-experts (MoE).}} In Sec. \\ref{dynamic_depth}, adaptive model ensemble is achieved via a \\emph{cascading} way, and later networks are conditionally executed based on early predictions. An alternative approach to improving the model capacity is the MoE  structure, which means that multiple network branches are built as experts in \\emph{parallel}. \nThese experts could be selectively executed, and their outputs are fused with data-dependent weights.\nConventional \\emph{soft} MoEs  adopt real-valued weights to dynamically rescale the representations obtained from different experts (\\figurename~\\ref{multi_branch} (a)). In this way, all the branches still need to be executed, and thus the computation cannot be reduced at test time. \\emph{Hard} gates with only a fraction of non-zero elements are developed to increase the inference efficiency of the MoE structure (see \\figurename~\\ref{multi_branch} (b)) : let $\\mathcal{G}$ denote a gating module whose output is a $N$-dimensional vector $\\bm{\\alpha}$ controlling the execution of $N$ experts $\\mathcal{F}_1,\\mathcal{F}_2, \\cdots, \\mathcal{F}_N$, the final output can be written as\n\\begin{equation} \\label{eq_moe}\n  \\setlength{\\abovedisplayskip}{3pt}\n  \\mathbf{y} = \\sum\\nolimits_{n=1}^N [\\mathcal{G}(\\mathbf{x})]_n \\mathcal{F}_n (\\mathbf{x}) = \\sum\\nolimits_{n=1}^N \\alpha_n \\mathcal{F}_n (\\mathbf{x}),\n  \\setlength{\\belowdisplayskip}{3pt}\n\\end{equation}\nand the $n$-th expert will not be executed if $\\alpha_n\\! =\\!0$. \nHard MoE has been implemented in diverse network structures. For example, HydraNet  replaces the convolutional blocks in a CNN by multiple branches, and selectively execute them conditioned on the input. For another example, dynamic routing network (DRNet)  performs a branch selection in each cell structure which is commonly used in NAS . On NLP tasks, sparely gated MoE  and switch Transformer  embeds hard MoE in a long short-term memory (LSTM)  network and a Transformer , respectively. Instead of making choice with \\emph{binary} gates as in , only the branches corresponding to the \\emph{top-K} elements of the real-valued gates are activated in .\n\\noindent\\textbf{{3) Skipping channels in CNNs.}}\nModern CNNs usually have considerable channel redundancy. Based on the common belief that the same feature channel can be of disparate importance for different samples, adaptive width of CNNs could be realized by dynamically activating convolutional channels. Compared to the \\emph{static} pruning methods  which remove \"unimportant\" filters permanently, such a \\emph{data-dependent} pruning approach improves the inference efficiency without degrading the model capacity.\na) \\emph{Multi-stage architectures along the channel dimension.} Recall that the early-exiting networks  discussed in Sec. \\ref{dynamic_depth} can be viewed as multi-stage models along the \\emph{depth} dimension, where late stages are conditionally executed based on early predictions. One can also build multi-stage architectures along the \\emph{width} (channel) dimension, and progressively execute these stages on demand.\nAlong this direction, an optimal architecture is searched among multiple structures with different widths, and any sample can be output at an early stage when a confident prediction is obtained . Channel gating network (CGNet)  first executes a subset of convolutional filters in every layer, and the remaining filters are only activated on strategically selected areas.\nb) \\emph{Dynamic pruning with gating functions.} In the aforementioned progressive activation paradigm, the execution of a later stage is decided based on previous output. As a result, a complete forward propagation is required for every stage, which might be suboptimal for reducing the practical inference latency. Another prevalent solution is to decide the execution of channels at every layer by gating functions. For example, runtime neural pruning (RNP)  models the layer-wise pruning as a Markov decision process, and an RNN is used to select specific channel groups at every layer. Moreover, pooling operations followed by FC layers are utilized to generate {\\emph{channel-wise hard attention} (i.e. making discrete decisions on whether to activate each channel)} for each sample . The recent work  uses a gate module to decide the width for a whole stage of a ResNet. Different reparameterization and optimizing techniques are required for training these gating functions, which will be reviewed in Sec. \\ref{training}.\nRather than placing plug-in gating modules \\emph{inside} a CNN, GaterNet  builds an \\emph{extra} network, which takes in the input sample and directly generates all the channel selection decisions for the backbone CNN. This implementation is similar to BlockDrop  that exploits an additional policy network for dynamic layer skipping (Sec. \\ref{dynamic_depth}).\nc) \\emph{Dynamic pruning based on feature activations directly} has also been realized without auxiliary branches and computational overheads , where a regularization item is induced in training to encourage the sparsity of features.\n{On basis of the existing literature on dynamically skipping either network \\emph{layers}  or convolutional \\emph{filters} , recent work  has realized dynamic inference with respect to network \\emph{depth} and \\emph{width} simultaneously: only if a layer is determined to be executed, its channels will be selectively activated, leading to a more flexible adaptation of computational graphs.}\n\\vspace{-1ex}", "cites": [706, 688, 705, 707, 8375, 544, 691, 8374, 710, 8373, 712, 713, 709, 699, 38, 711, 703, 680, 696, 702, 7266, 700, 701, 708, 704, 687], "cite_extract_rate": 0.7647058823529411, "origin_cites_number": 34, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.5, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes dynamic width mechanisms across multiple papers, integrating concepts like neuron skipping, MoE, and channel pruning into a structured taxonomy. It provides critical analysis by contrasting soft and hard MoE, progressive vs. per-layer pruning, and discussing trade-offs in computational efficiency. The section abstracts these ideas into a broader framework of data-dependent computation, showing how dynamic width relates to overall network adaptability."}}
{"id": "ad51ce7b-9d07-4edc-88e3-0822d3ae0397", "title": "Dynamic Routing", "level": "subsubsection", "subsections": [], "parent_id": "3befdcee-686f-4399-9d86-c1c11ef02509", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "{Sample-wise Dynamic Networks"], ["subsection", "Dynamic Architectures"], ["subsubsection", "Dynamic Routing"]], "content": "\\label{SuperNets}\nThe aforementioned methods mostly adjust the depth (Sec. \\ref{dynamic_depth}) or width (Sec. \\ref{dynamic_width}) of \\emph{classic architectures} by activating their computational units (e.g. layers  or channels ) conditioned on the input. {Another line of work develops different forms of SuperNets with various possible inference paths, and performs dynamic routing inside the SuperNets to adapt the computational graph to each sample.}\nTo achieve dynamic routing, there are typically routing nodes that are responsible for allocating features to different paths. For node $s$ at layer $\\ell$, let $\\alpha_{s\\rightarrow j}^{\\ell}$ denote the probability of assigning the reached feature $\\mathbf{x}^{\\ell}_s$ to node $j$ at layer $\\ell+1$, the path $\\mathcal{F}_{s\\rightarrow j}^{\\ell}$ will be activated only when $\\alpha_{s\\rightarrow j}^{\\ell}\\!>\\!0$. The resulting feature reaching node $j$ is represented by\n\\begin{equation}\\label{eq_supernet}\n  \\setlength{\\abovedisplayskip}{3pt}\n  \\mathbf{x}^{\\ell+1}_j = \\sum\\nolimits_{s\\in\\left\\{s: \\alpha_{s\\rightarrow j}^{\\ell}>0\\right\\}} \\alpha_{s\\rightarrow j}^{\\ell}\\mathcal{F}_{s\\rightarrow j}^{\\ell}(\\mathbf{x}^{\\ell}_s).\n  \\setlength{\\belowdisplayskip}{3pt}\n\\end{equation}\nThe probability $\\alpha_{s\\rightarrow j}^{\\ell}$ can be obtained in different manners. \nNote that the dynamic early-exiting networks  are a special form of SuperNets, where the routing decisions are only made at intermediate classifiers. The CapsuleNets  also perform dynamic routing between capsules, i.e. groups of neurons, to character the relations between (parts of) objects. Here we mainly focus on specific architecture designs of the SuperNets and their routing policies.\n\\noindent\\textbf{1) Path selection in multi-branch structures.} The simplest dynamic routing can be implemented by selectively executing \\emph{one} of multiple candidate modules at each layer , which is equivalent to producing a one-hot probability distribution $\\alpha_{s\\rightarrow \\cdot}^{\\ell}$ in Eq. \\ref{eq_supernet}. The main difference of this approach to hard MoE (\\figurename~\\ref{multi_branch} (b)) is that only one branch is activated without any fusion operations.\n\\noindent\\textbf{2) Neural trees and tree-structured networks.} \nAs decision trees always perform inference along one forward path that is dependent on input properties, combining tree structure with neural networks can naturally enjoy the adaptive inference paradigm and the representation power of DNNs simultaneously. Note that in a tree structure, the outputs of different nodes are routed to \\emph{independent} paths rather than being \\emph{fused} as in MoE structures (compare \\figurename~\\ref{multi_branch} (b), (c)).\na) \\emph{Soft decision tree} (SDT)  adopts neural units as routing nodes (blue nodes in \\figurename~\\ref{multi_branch} (c)), which decides the portion that the inputs are assigned to their left/right sub-tree. Each leaf node generates a probability distribution over the output space, and the final prediction is the expectation of the results from all leaf nodes. Although the probability for a sample reaching each leaf node in an SDT is data-dependent, all the paths are still executed, which limits the inference efficiency.\nb) \\emph{Neural trees with deterministic routing policies}  are designed to make hard routing decisions during inference, avoiding computation on those unselected paths.\nc) \\emph{Tree-structured DNNs.} Instead of developing decision trees containing neural units, a line of work builds special network architectures to endow them with the routing behavior of decision trees. For instance, a small CNN is first executed to classify each sample into coarse categories, and specific sub-networks are conditionally activated based on the coarse predictions . A subsequent work  not only partitions samples to different sub-networks, but also divides and routes the feature channels.\nDifferent to those networks using neural units only in routing nodes , or routing each sample to pre-designed sub-networks , adaptive neural tree (ANT)  adopts CNN modules as feature transformers in a hard neural tree (see lines with arrows in \\figurename~\\ref{multi_branch} (c)), and learns the tree structure together with the network parameters simultaneously in the training stage.\n\\noindent\\textbf{3) Others.} \n{Performing dynamic routing within more general SuperNet architectures is also a recent research trend. Representatively, an architecture distribution with partly shared parameters is \\emph{searched} from a SuperNet containing $\\sim \\!\\! 10^{25}$ sub-networks . During inference, every sample is allocated by a controller network to one sub-network with appropriate computational cost. Instead of training a standalone controller network, gating modules are plugged inside the \\emph{hand-designed} SuperNet (see \\figurename~\\ref{multi_scale} (d)) to decide the routing path based on intermediate features . \n\\begin{figure*}\n  \\centering\n  \\vspace{-2ex}\n    \\includegraphics[width=0.9\\linewidth]{6_dynamic_parameter.pdf}\n    \\vskip -0.2in\n    \\caption{{Three implementations of dynamic parameters: adjusting (a) or generating (b) the backbone parameters based on the input, and (c) dynamically rescaling the features with the attention mechanism.}}\n    \\label{dynamic_params}\n    \\vskip -0.2in\n\\end{figure*}\n\\vspace{-1.5ex}", "cites": [715, 714, 8374, 8373, 719, 720, 718, 696, 7266, 7274, 716, 717, 306], "cite_extract_rate": 0.6842105263157895, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple cited papers on dynamic routing, organizing them into clear subcategories (e.g., path selection, neural trees, and general SuperNet routing). It provides abstraction by identifying distinctions between soft and hard routing and how different structures (e.g., trees vs. MoE) influence inference behavior. While it includes some critical observations (e.g., SDT's limitation in inference efficiency), a more in-depth evaluation of trade-offs between approaches would strengthen its insight."}}
{"id": "8a3a1403-da1a-48be-b1e9-80d1325e0909", "title": "Parameter Adjustment", "level": "subsubsection", "subsections": [], "parent_id": "fdd3c2e8-dc39-4dd8-8a63-1d07741f61b4", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "{Sample-wise Dynamic Networks"], ["subsection", "Dynamic Parameters"], ["subsubsection", "Parameter Adjustment"]], "content": "\\label{dynamic_param_adjust}\n\\vspace{-0.5ex}\nA typical approach to parameter adaptation is adjusting the weights based on their input during inference as presented in \\figurename~\\ref{dynamic_params} (a). This implementation usually evokes little computation to obtain the adjustments, e.g., attention weights  or sampling offsets .\n\\noindent\\textbf{1) Attention on weights.} {To improve the representation power without noticeably increasing the computation, \nsoft attention can be performed on multiple convolutional kernels, producing an adaptive ensemble of parameters .}\nAssuming that there are $N$ kernels $\\mathbf{W}_n, n\\! =1,2,\\!\\cdots\\!,N$, such a dynamic convolution \ncan be formulated as \n\\begin{equation}\n  \\setlength{\\abovedisplayskip}{3pt}\n\\mathbf{y} = \\mathbf{x} \\star \\mathbf{\\tilde{W}} = \\mathbf{x} \\star (\\sum\\nolimits_{n=1}^N \\alpha_n\\mathbf{W}_n). \n\\setlength{\\belowdisplayskip}{3pt}\n\\end{equation}\nThis procedure increases the model capacity yet remains high efficiency, as the result obtained through fusing the outputs of $N$ convolutional branches (as in MoE structures, see \\figurename~\\ref{multi_branch} (a)) is equivalent to that produced by performing once convolution with $\\mathbf{\\tilde{W}}$. However, only $\\sim\\!1/N$ times of computation is consumed in the latter approach.\nWeight adjustment could also be achieved by performing soft attention over the \\emph{spatial locations} of convolutional weights . For example, segmentation-aware convolutional network  applies locally masked convolution to aggregate information with larger weights from similar pixels, which are more likely to belong to the same object.\nUnlike  that requires a sub-network for feature embedding, pixel-adaptive convolution (PAC)  adapts the convolutional weights based on the attention mask generated from the input feature at each layer.\n{Instead of adjusting weights conditioned on every sample itself, meta-neighborhoods  adapt the network parameters to each input sample based on its similarity to the neighbors stored in a dictionary.}\n\\noindent\\textbf{2) Kernel shape adaptation.} \nApart from adaptively scaling the weight \\emph{values}, parameter adjustment can also be realized to reshape the convolutional kernels and achieve \\emph{dynamic reception of fields}. Towards this direction, \ndeformable convolutions  sample feature pixels from adaptive locations when performing convolution on each pixel. Deformable kernels  samples weights in the kernel space to adapt the \\emph{effective} reception field (ERF) while leaving the reception field unchanged. Table \\ref{tab:deform_kernels} summarizes the formulations of the above three methods. \n{Due to their irregular memory access and computation pattern, these kernel shape adaptation approaches typically require customized CUDA kernels for the implementation on GPUs. However, recent literature has shown that the practical efficiency of deformable convolution could be effectively improved by co-designing algorithm and hardware based on embedded devices such as FPGAs .}\n\\begin{table*}\n  \\vspace{-2ex}\n  \\caption{{Kernel shape adaptation by dynamically sampling feature pixels  or convolutional weights .}}\n  \\vspace{-4ex}\n  \\label{tab:deform_kernels}\n  \\begin{center}\n    \\begin{tabular}{c|c|c|c}\n      \\hline\n      \\textbf{Method} & \\textbf{Formulation} & \\textbf{Sampled Target} & \\textbf{Dynamic Mask} \\\\\n      \\hline\n      Regular Convolution & $\\mathbf{y(p)} = \\sum\\nolimits_{k=1}^K \\mathbf{W}(\\mathbf{p}_k) \\mathbf{x}(\\mathbf{p+p}_k)$ & - & -  \\\\\n      \\hline\n      Deformable ConvNet-v1  & $\\mathbf{y(p)} = \\sum\\nolimits_{k=1}^K \\mathbf{W}(\\mathbf{p}_k) \\mathbf{x}(\\mathbf{p+p}_k+\\Delta \\mathbf{p}_k)$ & Feature map & No  \\\\\n      Deformable ConvNet-v2  & $\\mathbf{y(p)} = \\sum\\nolimits_{k=1}^K \\mathbf{W}(\\mathbf{p}_k) \\mathbf{x}(\\mathbf{p+p}_k+\\Delta \\mathbf{p}_k)\\Delta \\mathbf{m}_k$ & Feature map & Yes  \\\\\n      Deformable Kernels  & $\\mathbf{y(p)} = \\sum\\nolimits_{k=1}^K \\mathbf{W}(\\mathbf{p}_k+\\Delta \\mathbf{p}_k) \\mathbf{x}(\\mathbf{p+p}_k)$ & Conv kernel & No \\\\\n      \\hline\n    \\end{tabular}\n  \\end{center}\n  \\vspace{-5ex}\n\\end{table*}\n\\vspace{-1ex}", "cites": [675, 8376, 721, 7276, 7275, 722], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers by categorizing dynamic parameter adjustment into attention-based methods and kernel shape adaptation, connecting them under a coherent narrative. It provides some critical commentary, such as noting the computational challenges of kernel shape adaptation and referencing improvements via hardware co-design. While it identifies patterns like the use of attention mechanisms and dynamic receptive fields, it stops short of formulating a meta-level framework or theory."}}
{"id": "81d9301c-7d1b-4044-b144-7027a7b2165b", "title": "Weight Prediction", "level": "subsubsection", "subsections": [], "parent_id": "fdd3c2e8-dc39-4dd8-8a63-1d07741f61b4", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "{Sample-wise Dynamic Networks"], ["subsection", "Dynamic Parameters"], ["subsubsection", "Weight Prediction"]], "content": "\\label{weight_predict}\n\\vspace{-0.25ex}\nCompared to making modifications on model parameters on the fly (Sec. \\ref{dynamic_param_adjust}), weight prediction  is more straightforward: it directly generates (a subset of) input-adaptive parameters with an independent model at test time (see \\figurename~\\ref{dynamic_params} (b)). This idea was first suggested in , where both the weight prediction model and the backbone model were feedforward networks. Recent work has further extended the paradigm to modern network architectures and tasks.\n\\noindent\\textbf{1) General architectures.} Dynamic filter networks (DFN)  and HyperNetworks  are two classic approaches realizing runtime weight prediction for CNNs and RNNs, respectively. Specifically, a filter generation network is built in DFN  to produce the filters for a convolutional layer. As for processing sequential data (e.g. a sentence), the weight matrices of the main RNN are predicted by a smaller one at each time step conditioned on the input (e.g. a word) . WeightNet  unifies the dynamic schemes of  and  by predicting the convolutional weights via simple grouped FC layers, achieving competitive results in terms of the accuracy-FLOPs\\footnote{{Floating point operations, which is widely used as a measure of inference efficiency of deep networks.}} and accuracy-parameters trade-offs.\n{Rather than generating standard \\emph{convolutional} weights, LambdaNetworks  learns to predict the weights of \\emph{linear} projections based on the contexts of each pixel together with the relative position embeddings, showing advantages in terms of computational cost and memory footprint.}\n\\noindent\\textbf{2) Task-specific information} \nhas also been exploited to predict model parameters on the fly, enabling dynamic networks to generate task-aware feature embeddings. For example, edge attributes are utilized in  to generate filters for graph convolution, and camera perspective is incorporated in  to generate weights for image convolution. {Such task-aware weight prediction has been shown effective in improving the data efficiency on many tasks, including visual question answering  and few-shot learning .}\n\\vspace{-1ex}", "cites": [675, 726, 7277, 724, 7215, 678, 727, 725, 8377, 677, 723], "cite_extract_rate": 0.7857142857142857, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from multiple cited papers to present a structured overview of weight prediction in dynamic neural networks, distinguishing general architectures from task-specific approaches. It offers some level of abstraction by identifying overarching themes like input-adaptivity and computational trade-offs. However, the critical analysis is limitedâ€”there are no clear evaluations of strengths/weaknesses or in-depth comparisons between methods."}}
{"id": "0651cec9-e0a5-4c53-aca3-aa4ed8ac7d94", "title": "Dynamic Features", "level": "subsubsection", "subsections": [], "parent_id": "fdd3c2e8-dc39-4dd8-8a63-1d07741f61b4", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "{Sample-wise Dynamic Networks"], ["subsection", "Dynamic Parameters"], ["subsubsection", "Dynamic Features"]], "content": "\\label{sec:attention}\n\\vspace{-0.25ex}\nThe main goal of either \\emph{adjusting} (Sec. \\ref{dynamic_param_adjust}) or \\emph{predicting} (Sec. \\ref{weight_predict}) model parameters is producing more dynamic and informative features, and therefore enhancing the representation power of deep networks. A more straightforward solution is rescaling the features with input-dependent soft attention (see \\figurename~\\ref{dynamic_params} (c)), which requires minor modifications on computational graphs. Note that for a linear transformation $\\mathcal{F}$, applying attention $\\bm{\\alpha}$ on the output is equivalent to performing computation with re-weighted parameters, i.e.\n\\begin{equation}\\label{dynamic_feature_key}\n  \\setlength{\\abovedisplayskip}{3pt}\n    \\mathcal{F}(\\mathbf{x},\\bm{\\Theta})\\otimes \\bm{\\alpha}= \\mathcal{F}(\\mathbf{x},\\bm{\\Theta}\\otimes\\bm{\\alpha}).\n    \\setlength{\\belowdisplayskip}{3pt}\n\\end{equation}\n\\noindent\\textbf{1) Channel-wise attention} is one of the most common soft attention mechanisms. Existing work typically follows \nthe form in squeeze-and-excitation network (SENet) :\n\\begin{equation}\\label{se_attention}\n  \\setlength{\\abovedisplayskip}{3pt}\n\\mathbf{\\tilde{y}}\\! = \\!\\mathbf{y} \\otimes \\bm{\\alpha}\\! =\\!\\mathbf{y} \\otimes \\mathcal{A}(\\mathbf{y}),  \\bm{\\alpha}\\in\\left[0,1\\right]^C.\n\\setlength{\\belowdisplayskip}{3pt}\n\\end{equation} \nIn Eq. \\ref{se_attention}, $\\mathbf{y}\\! =\\!\\mathbf{x} \\star \\mathbf{W}$ is the output feature of a convolutional layer with $C$ channels, and $\\mathcal{A}(\\cdot)$ is a lightweight function composed of pooling and linear layers for producing $\\bm{\\alpha}$.\nTaking the convolution into account, the procedure can also be written as $\\mathbf{\\tilde{y}}\\! = \\!(\\mathbf{x} \\star \\mathbf{W}) \\otimes \\bm{\\alpha}\\! =\\! \\mathbf{x} \\star (\\mathbf{W} \\otimes \\bm{\\alpha})$, from which we can observe that applying attention on features is equivalent to performing convolution with dynamic weights.\nOther implementations for attention modules have also been developed, including using standard deviation to provide more statistics , or replacing FC layers with efficient 1D convolutions .\nThe empirical performance of three computational graphs for soft attention is studied in : 1) $\\mathbf{\\tilde{y}}\\! =\\!\\mathbf{y}\\otimes \\mathcal{A}(\\mathbf{y})$, 2) $\\mathbf{\\tilde{y}}\\! =\\!\\mathbf{y}\\otimes \\mathcal{A}(\\mathbf{x})$ and 3) $\\mathbf{\\tilde{y}}\\! =\\!\\mathbf{y}\\otimes \\mathcal{A}(\\mathrm{Conv}(\\mathbf{x}))$. It is found that the three forms yield different performance in different backbone networks.\n\\noindent\\textbf{2) Spatial-wise attention}. Spatial locations in features could also be dynamically rescaled with attention to improve the representation power of deep models . Instead of using pooling operations to efficiently gather global information as in channel-wise attention, convolutions are often adopted in spatial-wise attention to encode local information. Moreover, these two types of attention modules can be integrated in one framework  (see \\figurename~\\ref{dynamic_params} (c)).\n\\noindent\\textbf{3) Dynamic activation functions.} The aforementioned approaches to generating dynamic features usually apply soft attention before static activation functions. A recent line of work has sought to increase the representation power of models with dynamic activation functions . For instance, \nDY-ReLU  replaces ReLU $(\\mathbf{y}_c\\! =\\!\\max (\\mathbf{x}_c, 0))$ with the max value among $N$ linear transformations $\\mathbf{y}_c\\! =\\!\\max_{n} \\left\\{a_c^n \\mathbf{x}_c + b_c^n\\right\\}$, where $c$ is the channel index, and $a_c^n, b_c^n$ are linear coefficients calculated from $\\mathbf{x}$. On many vision tasks, these dynamic activation functions can effectively improve the performance of different network architectures with negligible computational overhead.\nTo summarize, soft attention has been exploited in many fields due to its simplicity and effectiveness. Moreover, it can be incorporated with other methods conveniently. E.g., by replacing the weighting scalar $\\alpha_n$ in Eq. \\ref{eq_moe} with channel-wise  or spatial-wise  attention, the output of multiple branches with independent kernel sizes  or feature resolutions  are adaptively fused.\nNote that we leave out the detailed discussion on the self attention mechanism, which is widely studied in both NLP  and CV fields  to re-weight features based on the similarity between queries and keys at different locations (temporal or spatial). \nReaders who are interested in this topic may refer to review studies . In this survey, we mainly focus on the feature re-weighting scheme in the framework of dynamic inference. \n\\vspace{-2ex}", "cites": [736, 730, 729, 7278, 8378, 38, 734, 8379, 733, 731, 7040, 728, 735, 7268, 7, 732], "cite_extract_rate": 0.7619047619047619, "origin_cites_number": 21, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key attention-based dynamic feature approaches from multiple papers, establishing a coherent narrative around channel-wise, spatial-wise, and dynamic activation methods. It provides abstract insights by linking attention to dynamic weight adaptation and identifying broader design patterns. While it includes some critical evaluation (e.g., noting performance differences across backbones), it does not deeply critique limitations or compare trade-offs in detail."}}
{"id": "d1dfc651-19ab-46ab-9db8-02eef3371ca0", "title": "Spatial-wise Dynamic Networks", "level": "section", "subsections": ["bb93a738-d7fa-4ffe-811c-d2e5d3062ddd", "0d52450e-77a8-4861-a2eb-00ddb1ece863", "ab0c50b9-9d18-4c18-976a-34aae739e7ac"], "parent_id": "4fd906cb-4aa6-49a2-a5b7-ec10dfbd9280", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "Spatial-wise Dynamic Networks"]], "content": "\\vspace{-0.25ex}\n\\label{sec_spatially_adaptive}\nIn visual learning, it has been found that not all locations contribute equally to the final prediction of CNNs , which suggests that \\emph{spatially} dynamic computation has great potential for reducing computational redundancy.\nIn other words, making a correct prediction may only require processing a fraction of pixels or regions with an adaptive amount of computation. Moreover, based on the observations that low-resolution representations are sufficient to yield decent performance for most inputs , the static CNNs that take in all the input with the same resolution may also induce considerable redundancy.\nTo this end, spatial-wise dynamic networks are built to perform adaptive inference with respect to different spatial locations of images.\nAccording to the granularity of dynamic computation, we further categorize the relevant approaches into three levels: {\\emph{pixel level} (Sec. \\ref{pixel_level}), \\emph{region level} (Sec. \\ref{region_level}) and \\emph{resolution level} (Sec. \\ref{dynamic_resolution}).}\n\\vspace{-2ex}", "cites": [737, 504], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section introduces the concept of spatial-wise dynamic networks but primarily summarizes general observations and motivations without deeply synthesizing the cited papers. It lacks critical evaluation of the cited works and does not establish a comparative or analytical framework. While it hints at broader categories (pixel, region, resolution levels), these are not fully developed or contextualized with the cited research."}}
{"id": "fa853b1f-afd4-40d2-bed1-be3a1d6cb128", "title": "Pixel-wise Dynamic Architectures", "level": "subsubsection", "subsections": [], "parent_id": "bb93a738-d7fa-4ffe-811c-d2e5d3062ddd", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "Spatial-wise Dynamic Networks"], ["subsection", "Pixel-level Dynamic Networks"], ["subsubsection", "Pixel-wise Dynamic Architectures"]], "content": "\\label{pixel_dynamic_arch}\n\\vspace{-0.25ex}\nBased on the common belief that foreground pixels are more informative and computational demanding than those in the background, some dynamic networks learn to adjust their architectures for each pixel. Existing literature generally achieves this by 1) \\emph{dynamic sparse convolution}, which only performs convolutions on a subset of sampled pixels; 2) \\emph{additional refinement}, which strategically allocates extra computation (e.g. layers or channels) on certain spatial positions.\n\\begin{figure}\n  \\centering\n  \\vspace{-1ex}\n    \\includegraphics[width=\\linewidth]{8_spatial_pixel_sparse.pdf}\n    \\vskip -0.15in\n    \\caption{{Dynamic convolution on selected spatial locations. The 1 elements (black) in the spatial mask determine the pixels (green) that require computation in the output feature map.}}\n    \\label{fig_spatial_location_specific}\n    \\vspace{-3ex}\n\\end{figure}\n\\noindent\\textbf{1) Dynamic sparse convolution.} To reduce the unnecessary computation on less informative locations, convolution can be performed only on strategically sampled pixels. Existing sampling strategies include 1) making use of the intrinsic sparsity of the input ; 2) predicting the positions of zero elements on the output ; and 3) estimating the saliency of pixels . A typical approach is using an extra branch to generate a spatial mask, determining the execution of convolution on each pixel (see \\figurename~\\ref{fig_spatial_location_specific}). {Pixel-wise dynamic depth could also be achieved based on a halting scheme } (see Sec. \\ref{dynamic_depth}). These dynamic convolutions usually neglect the unselected positions, which might degrade the network performance. Interpolation is utilized in  to efficiently fill those locations, therefore alleviating the aforementioned disadvantage.\n\\noindent\\textbf{2) Dynamic additional refinement.} Instead of only sampling certain pixels to perform convolutions, another line of work first conducts relatively cheap computation on the whole feature map, and adaptively activate extra modules on selected pixels for further \\emph{refinement}. Representatively, dynamic capacity network  generates coarse features with a shallow model, and salient pixels are sampled based on the gradient information. \nFor these salient pixels, extra layers are applied to extract finer features. Similarly, specific positions are additionally processed by a fraction of convolutional filters in . These methods adapt their network architectures in terms of \\emph{depth} or \\emph{width} at the pixel level, achieving a spatially adaptive allocation of computation.\n{The aforementioned dynamic additional refinement approaches  are mainly developed for image classification.} On the semantic segmentation task, pixel-wise \\emph{early exiting} (see also Sec. \\ref{dynamic_depth}) is proposed in , where the pixels with high prediction confidence are output without being processed by deeper layers. PointRend  shares a similar idea, and applies additional FC layers on selected pixels with low prediction confidence, which are more likely to be on borders of objects. All these researches demonstrate that by exploiting the spatial redundancy in image data, dynamic computation at the pixel level beyond sample level significantly increases the model efficiency.\n\\vspace{-1ex}", "cites": [701, 738, 7279, 7280, 676, 739, 7267, 7281], "cite_extract_rate": 0.7272727272727273, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple cited papers to present a coherent narrative on pixel-wise dynamic architectures, grouping them into two main approaches: dynamic sparse convolution and dynamic additional refinement. It identifies broader abstraction principles, such as exploiting spatial redundancy and adaptive allocation of computation. While it does provide some limitations (e.g., neglecting unselected pixels), the critical analysis is moderate and more descriptive than deeply evaluative."}}
{"id": "530cae77-537a-4dca-80a6-e035f5680e22", "title": "Pixel-wise Dynamic Parameters", "level": "subsubsection", "subsections": [], "parent_id": "bb93a738-d7fa-4ffe-811c-d2e5d3062ddd", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "Spatial-wise Dynamic Networks"], ["subsection", "Pixel-level Dynamic Networks"], ["subsubsection", "Pixel-wise Dynamic Parameters"]], "content": "\\label{pixel_dynamic_param}\n\\vspace{-0.25ex}\nIn contrast to entirely skipping the convolution operation on a subset of pixels, dynamic networks can also apply data-dependent parameters on different pixels for improved representation power or adaptive reception fields.\n\\noindent\\textbf{1) Dynamic weights.} {Similar to the sample-wise dynamic parameter methods (Sec. \\ref{adaptive_params}), pixel-level dynamic weights are achieved by test-time \\emph{adjustment} , \\emph{prediction}  or \\emph{dynamic features} . Take weight prediction as an example,} typical approaches generate an $H\\!\\times\\!W\\!\\times\\! k^2$ kernel map to produce spatially dynamic weights ($H,W$ are the spatial size of the output feature and $k$ is the kernel size). Considering the pixels belonging to the same object may share identical weights, dynamic region-aware convolution (DRConv)  generates a segmentation mask for an input image, dividing it into $m$ regions, for each of which a weight generation network is responsible for producing a data-dependent kernel.\n\\noindent\\textbf{2) Dynamic reception fields.} Traditional convolution operations usually have a fixed shape and size of kernels (e.g. the commonly used $3\\!\\times\\!3$ 2D convolution). The resulting uniform reception field across all the layers may have limitations for recognizing objects with varying shapes and sizes. {To tackle this, a line of work learns to adapt the reception field for different feature pixels , as discussed in Sec. \\ref{dynamic_param_adjust}.} Instead of adapting the sampling location of features or kernels, adaptive connected network  realizes a dynamic trade-off among self transformation (e.g. $1\\!\\times\\!1$ convolution), local inference (e.g. $3\\!\\times\\!3$ convolution) and global inference (e.g. FC layer). The three branches of outputs are fused with data-dependent weighted summation. \nBesides images, the local and global information in non-Euclidean data, such as graphs, could also be adaptively aggregated.\n\\vspace{-1.5ex}", "cites": [7257, 740, 741, 7268, 8376, 731, 722, 7282, 7276, 742, 7278, 7275], "cite_extract_rate": 0.8, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple cited papers to present a coherent discussion on pixel-wise dynamic parameters and adaptive reception fields. It connects related methods such as DRConv, ACNet, and PAC, and relates them to broader dynamic network concepts. While it includes some critical insights (e.g., limitations of fixed kernels), it primarily analyzes and explains the mechanisms rather than offering deep comparative critiques or identifying major research gaps."}}
{"id": "0d52450e-77a8-4861-a2eb-00ddb1ece863", "title": "Region-level Dynamic Networks", "level": "subsection", "subsections": ["b67eaf7b-932e-433e-a7e5-732f3b779d92", "27095343-62ad-48ed-a110-4d5657e3a267"], "parent_id": "d1dfc651-19ab-46ab-9db8-02eef3371ca0", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "Spatial-wise Dynamic Networks"], ["subsection", "Region-level Dynamic Networks"]], "content": "\\label{region_level}\n\\vspace{-0.5ex}\n{Pixel-level dynamic networks mentioned in Sec. \\ref{pixel_level} often require specific implementations for sparse computation, and consequently may face challenges in terms of achieving real acceleration on hardware .} An alternative approach is performing adaptive inference on \\emph{regions/patches} of input images. There mainly exists two lines of work along this direction (see \\figurename~\\ref{fig_spatial_select_regions}): one performs parameterized \\emph{transformations} on a region of feature maps for more accurate prediction (Sec. \\ref{dynamic_transofrm}), and the other learns patch-level \\emph{hard attention}, with the goal of improving the effectiveness and/or efficiency of models (Sec. \\ref{hard_attention_pathces}).\n\\vspace{-1ex}", "cites": [7281], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces region-level dynamic networks and refers to two sub-sections for more details, but it does not engage deeply with the cited paper. It lacks synthesis of multiple ideas, critical evaluation of strengths or limitations, and abstraction beyond the specific methods mentioned. As a result, the insight quality remains low."}}
{"id": "b67eaf7b-932e-433e-a7e5-732f3b779d92", "title": "Dynamic Transformations", "level": "subsubsection", "subsections": [], "parent_id": "0d52450e-77a8-4861-a2eb-00ddb1ece863", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "Spatial-wise Dynamic Networks"], ["subsection", "Region-level Dynamic Networks"], ["subsubsection", "Dynamic Transformations"]], "content": "\\label{dynamic_transofrm}\n\\vspace{-0.25ex}\nDynamic transformations (e.g. affine/projective/thin plate spline transformation) can be performed on images to undo certain variations  for better generalization ability, or to exaggerate the salient regions  for discriminative feature representation. For example, spatial transformer  adopts a localization network to generate the transformation parameters, and then applies the parameterized transformation to recover the input from the corresponding variations. \nMoreover, transformations are learned to adaptively zoom-in the salient regions on some tasks where the model performance is sensitive to a small portion of regions. \n\\begin{figure}\n  \\centering\n  \\vspace{-1ex}\n    \\includegraphics[width=\\linewidth]{9_spatial_select_regions.pdf}\n    \\vskip -0.15in\n    \\caption{{Region-level dynamic inference. The region selection module generates the transformation/localization parameters, and the subsequent network performs inference on the transformed/cropped region.}}\n    \\label{fig_spatial_select_regions}\n    \\vspace{-4ex}\n\\end{figure}\n\\vspace{-1ex}", "cites": [743, 531], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of dynamic transformations in spatial-wise dynamic networks, referencing two key papers. However, it lacks synthesis by not connecting these ideas into a broader framework or discussing their relationships in depth. There is little critical analysis or abstraction, as it primarily summarizes methods without evaluating their strengths, limitations, or general principles."}}
{"id": "27095343-62ad-48ed-a110-4d5657e3a267", "title": "Hard Attention on Selected Patches", "level": "subsubsection", "subsections": [], "parent_id": "0d52450e-77a8-4861-a2eb-00ddb1ece863", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "Spatial-wise Dynamic Networks"], ["subsection", "Region-level Dynamic Networks"], ["subsubsection", "Hard Attention on Selected Patches"]], "content": "\\label{hard_attention_pathces}\n\\vspace{-0.25ex}\nInspired by the fact that informative features may only be contained in certain regions of an image, dynamic networks with hard spatial attention are explored to strategically select patches from the input for improved efficiency.\n\\noindent\\textbf{1) Hard attention with RNNs.} The most typical approach is formulating a classification task as a sequential decision process, {and adopting RNNs to make iterative predictions based on selected patches .} For example, images are classified within a fixed number of steps, and at each step, the classifier RNN only sees a cropped patch, deciding the next attentional location until the last step is reached . An adaptive step number is further achieved by including early stopping in the action space . Glance-and-focus network (GFNet)  builds a general framework of region-level adaptive inference by sequentially focusing on a series of selected patches, and is compatible with most existing CNN architectures. The recurrent attention mechanism together with the early exiting paradigm enables both \\emph{spatially} and \\emph{temporally} adaptive inference .\n\\noindent\\textbf{2) Hard attention with other implementations.} Rather than using an RNN to predict the region position that the model should pay attention to, class activation mapping (CAM)  is leveraged in  to iteratively focus on salient patches. At each iteration, the selection is performed on the previously cropped input, leading to a progressive refinement procedure. A multi-scale CNN is built in , where the sub-network in each scale takes in the cropped patch from the previous scale, and is responsible for simultaneously producing 1) the feature representations for classification and 2) the attention map for the next scale. {Without an iterative manner, the recent differentiable patch selection  adopts a differentiable top-K module to select a fixed number of patches in one step.}\n\\vspace{-2ex}", "cites": [747, 745, 737, 744, 674, 746], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of approaches using hard attention in region-level spatial-wise dynamic networks, mentioning the use of RNNs, CAM, multi-scale CNNs, and differentiable patch selection. It lists key papers and their contributions but lacks deep synthesis, critical evaluation, or abstraction into broader principles. The narrative remains largely descriptive and does not offer a nuanced analysis of trade-offs or trends among the methods."}}
{"id": "ab0c50b9-9d18-4c18-976a-34aae739e7ac", "title": "Resolution-level Dynamic Networks", "level": "subsection", "subsections": ["6a7330af-c2c7-4497-af0d-0ab86ce108a0", "16545c48-ec80-4bbd-871e-b151b745999d"], "parent_id": "d1dfc651-19ab-46ab-9db8-02eef3371ca0", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "Spatial-wise Dynamic Networks"], ["subsection", "Resolution-level Dynamic Networks"]], "content": "\\label{dynamic_resolution}\n\\vspace{-0.5ex}\nThe researches discussed above typically divide feature maps into different areas (pixel-level or region-level) for adaptive inference.\nOn a coarser granularity, some dynamic networks could treat each image as a whole by processing feature representations with adaptive resolutions. Although it has been observed that a low resolution might be sufficient for recognizing most \"easy\" samples , conventional CNNs mostly process all the inputs with the same resolution, inducing considerable redundancy. Therefore, resolution-level dynamic networks exploit spatial redundancy from the perspective of feature resolution rather than the saliency of different locations. Existing approaches mainly include 1) scaling the inputs with adaptive ratios (Sec. \\ref{adaptive_scaling_ratio}); 2) selectively activating the sub-networks with different resolutions in a multi-scale architecture (Sec. \\ref{dynamic_res_multiscale}).\n\\vspace{-1ex}", "cites": [504], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section introduces resolution-level dynamic networks by briefly describing the general motivation and two main approaches. It integrates the concept of spatial redundancy and references a paper (MobileNets) related to efficient network design but does not explicitly connect it to the discussion of resolution adaptation. The analysis remains superficial, lacking in-depth evaluation, comparison, or abstraction beyond individual methods."}}
{"id": "6a7330af-c2c7-4497-af0d-0ab86ce108a0", "title": "Adaptive Scaling Ratios", "level": "subsubsection", "subsections": [], "parent_id": "ab0c50b9-9d18-4c18-976a-34aae739e7ac", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "Spatial-wise Dynamic Networks"], ["subsection", "Resolution-level Dynamic Networks"], ["subsubsection", "Adaptive Scaling Ratios"]], "content": "\\label{adaptive_scaling_ratio}\n\\vspace{-0.25ex}\nDynamic resolution can be achieved by scaling features with adaptive ratios. For example, a small sub-network is first executed to predict a scale distribution of faces on the face detection task, then the input images are adaptively zoomed, so that all the faces fall in a suitable range for recognition . A plug-in module is used by  to predict the stride for the first convolution block in each ResNet stage, producing features with dynamic resolution.\n\\vspace{-1ex}", "cites": [748], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly describes two methods for achieving dynamic resolution through adaptive scaling but lacks synthesis of broader ideas or comparison between them. It does not critically analyze the approaches or identify limitations, and offers no abstraction or generalization to higher-level principles or patterns in the field."}}
{"id": "16545c48-ec80-4bbd-871e-b151b745999d", "title": "Dynamic Resolution in Multi-scale Architectures", "level": "subsubsection", "subsections": [], "parent_id": "ab0c50b9-9d18-4c18-976a-34aae739e7ac", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "Spatial-wise Dynamic Networks"], ["subsection", "Resolution-level Dynamic Networks"], ["subsubsection", "Dynamic Resolution in Multi-scale Architectures"]], "content": "\\label{dynamic_res_multiscale} \n\\vspace{-0.25ex}\nAn alternative approach to achieving dynamic resolution is building multiple sub-networks in a parallel  or cascading  way. These sub-networks with different feature resolutions are selectively activated conditioned on the input during inference. For instance, Elastic  realizes a \\emph{soft} selection from multiple branches at every layer, where each branch performs a downsample-convolution-upsample procedure with an independent scaling ratio. To practically avoid redundant computation, a \\emph{hard} selection is realized by , which allows each sample to conditionally activate sub-networks that process feature representations with resolution from low to high (see \\figurename~\\ref{multi_scale} (c) in Sec. \\ref{dynamic_depth}).\n\\vspace{-2ex}\n\\begin{figure*}\n  \\centering\n  \\vspace{-2ex}\n    \\includegraphics[width=0.9\\linewidth]{10_temporal_skim.pdf}\n    \\vskip -0.15in\n    \\caption{{Temporally adaptive inference. The first three approaches dynamically allocate computation in each step by (a) skipping the update, (b) partially updating the state, or (c) conditional computation in a hierarchical structure. The agent in (d) decides where to read in the next step.}}\n    \\label{fig_temporal_skim}\n    \\vspace{-3ex}\n\\end{figure*}", "cites": [7266], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"error": "Failed to parse LLM response", "raw_response": "{\n    \"type\": \"descriptive\",\n    \"scores\": {\"synthesis\": 2.5, \"critical\": 2.0, \"abstraction\": 2.0},\n    \"insight_level\": \"low\",\n    \"analysis\": \"The section provides a basic descriptive overview of dynamic resolution in multi-scale architectures, mentioning methods like Elastic and \\emph{hard} selection, but does not synthesize these ideas into a deeper narrative. There is minimal critical analysis of the cited work (e.g., RANet), and no clear abstraction or identification of overarching princip"}}
{"id": "7450af29-777b-40c4-bbdb-5820eb26595d", "title": "Dynamic Update of Hidden States", "level": "subsubsection", "subsections": [], "parent_id": "d0476612-fbf1-40f0-a9de-cc704c8eb7d7", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "Temporal-wise Dynamic Networks"], ["subsection", "RNN-based Dynamic Text Processing"], ["subsubsection", "Dynamic Update of Hidden States"]], "content": "\\vspace{-0.25ex}\n\\label{skimming_text}\nSince not all the tokens are essential for capturing the task-relevant information in a sequence, dynamic RNNs can be built to adaptively update their hidden states at each time step. Less informative tokens will be coarsely \\emph{skimmed}, i.e. the states are updated with cheaper computation.\n\\noindent\\textbf{1) Skipping the update.} For unimportant inputs at certain temporal locations, dynamic models can learn to entirely skip the update of hidden states (see \\figurename~\\ref{fig_temporal_skim} (a)), i.e.\n\\begin{equation}\n  \\setlength{\\abovedisplayskip}{3pt}\n  \\mathbf{h}_t = \\alpha_t\\mathcal{F}(\\mathbf{x}_t, \\mathbf{h}_{t-1}) + (1-\\alpha_t)\\mathbf{h}_{t-1}, \\alpha_t\\in\\left\\{0,1\\right\\}.\n  \\setlength{\\belowdisplayskip}{3pt}\n\\end{equation}\nFor instance, Skip-RNN  updates a controlling signal in every step to determine whether to update or \\emph{copy} the hidden state from the previous step. An extra agent is adopted by Structural-Jump-LSTM  to make the skipping decision conditioned on the previous state and the current input. Without training the RNNs and the controllers jointly as in  and , a predictor is trained in  to estimate whether each input will make a \"significant change\" on the hidden state. The update is identified worthy to be executed only when the predicted change is greater than a threshold.\n\\noindent\\textbf{2) Coarse update. } As directly skipping the update may be too aggressive, dynamic models could also update the hidden states with adaptively allocated operations. In specific, a network can adapt its architecture in every step, i.e.  \n\\begin{equation}\n  \\setlength{\\abovedisplayskip}{3pt}\n  \\mathbf{h}_t = \\mathcal{F}_t(\\mathbf{x}_t, \\mathbf{h}_{t-1}), t=1,2,\\cdots, T,\n  \\setlength{\\belowdisplayskip}{3pt}\n\\end{equation}\nwhere $\\mathcal{F}_t$ is determined based on the input $\\mathbf{x}_t$. One implementation is selecting a subset of dimensions of the hidden state to calculate, and copying the remaining from the previous step , as shown in \\figurename~\\ref{fig_temporal_skim} (b). To achieve the partial update, a subset of rows in weight matrices of the RNN is dynamically activated in , while Skim-RNN  makes a choice between two independent RNNs.\nWhen the hidden states are generated by a multi-layer network, the update could be interrupted at an intermediate layer based on an accumulated halting score .\nTo summarize, a coarse update can be realized by data-dependent network \\emph{depth}  or \\emph{width} . \n{\\noindent\\textbf{3) Selective updates in hierarchical RNNs.} Considering the intrinsic hierarchical structure of texts (e.g. sentence-word-character), researchers have developed hierarchical RNNs to encode the temporal dependencies with different timescales using a dynamic update mechanism . During inference, the RNNs at higher levels will selectively update their states conditioned on the output of low-level ones (see \\figurename~\\ref{fig_temporal_skim} (c)).\nFor example, when a character-level model in  detects that the input satisfies certain conditions, it will \\emph{\"flush\"} (reset) its states and feed them to a word-level network. Similar operations have also been realized by a gating module on question answering tasks .}\n\\vspace{-1ex}", "cites": [683, 7284, 750, 7283, 749], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key approaches for dynamic hidden state updates in RNNs, grouping them into skipping, coarse, and selective updates. It abstracts these methods into broader principles like data-dependent depth and width. While it provides a clear analytical framework, the critical evaluation is moderate, focusing more on method descriptions and less on nuanced limitations or trade-offs."}}
{"id": "64d0b8b6-25b1-481c-a175-60d486a5210d", "title": "Jumping in Texts", "level": "subsubsection", "subsections": [], "parent_id": "d0476612-fbf1-40f0-a9de-cc704c8eb7d7", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "Temporal-wise Dynamic Networks"], ["subsection", "RNN-based Dynamic Text Processing"], ["subsubsection", "Jumping in Texts"]], "content": "\\vspace{-0.25ex}\n\\label{text_jumping}\nAlthough early exiting in Sec. \\ref{temporal_early_exit} can largely reduce redundant computation, all the tokens must still be fed to the model one by one. More aggressively, dynamic RNNs could further learn to decide \\emph{\"where to read\"} by strategically skipping some tokens without reading them, and directly jumping to an arbitrary temporal location (see \\figurename~\\ref{fig_temporal_skim} (d)).\nSuch dynamic jumping, together with early exiting, is realized in  and . Specifically, LSTM-Jump  implements an auxiliary unit to predict the jumping stride within a defined range, and the reading process ends when the unit outputs zero. The model in  first decides whether to stop at each step. If not, it will further choose to re-read the current input, or to skip a flexible number of words. Moreover, structural information is exploited by Structural-Jump-LSTM , which utilizes an agent to decide whether to jump to the next punctuation. Apart from looking ahead, LSTM-Shuttle  also allows backward jumping to supplement the missed history information.\n\\vspace{-1.5ex}", "cites": [7283, 7285], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of how dynamic RNNs can skip tokens in text processing by referencing several specific models. It mentions the mechanisms of LSTM-Jump, Structural-Jump-LSTM, and LSTM-Shuttle, but does not synthesize their ideas into a broader framework or highlight comparative strengths and weaknesses. There is minimal critical evaluation or abstraction beyond the described systems."}}
{"id": "b2f1acb3-4b7a-41ce-96e3-e8abe77bc9b2", "title": "Video Recognition with Dynamic RNNs", "level": "subsubsection", "subsections": [], "parent_id": "844590b4-9a40-42f9-8052-47ad9e4a7783", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "Temporal-wise Dynamic Networks"], ["subsection", "Temporal-wise Dynamic Video Recognition"], ["subsubsection", "Video Recognition with Dynamic RNNs"]], "content": "\\label{recurrent_video}\n\\vspace{-0.25ex}\nVideo recognition is often conducted via a recurrent procedure, where the video frames are first encoded by a 2D CNN, and the obtained frame features are fed to an RNN sequentially for updating its hidden state. Similar to the approaches introduced in Sec. \\ref{temporal_text}, RNN-based adaptive video recognition is typically realized by 1) treating unimportant frames with relatively cheap computation (\\emph{\"glimpse\"}) ; 2) \\emph{early exiting} ; and 3) performing dynamic \\emph{jumping} to decide {\"where to see\"} .\n\\noindent\\textbf{1) Dynamic update of hidden states.} To reduce redundant computation at each time step, LiteEval  makes a choice between two LSTMs with different computational costs. ActionSpotter  decides whether to update the hidden state according to each input frame. {AdaFuse  selectively reuses certain feature channels from the previous step to efficiently make use of historical information.} Recent work has also proposed to adaptively decide the numerical precision  or modalities  when processing the sequential input frames. Such a \\emph{glimpse} procedure (i.e. allocating cheap operations to unimportant frames) is similar to the aforementioned text \\emph{skimming} .\n\\noindent\\textbf{2) Temporally early exiting.} Humans are able to comprehend the contents easily before watching an entire video. Such early stopping is also implemented in dynamic networks to make predictions only based on a portion of video frames . Together with the \\emph{temporal} dimension, the model in  further achieves early exiting from the aspect of network \\emph{depth} as discussed in Sec. \\ref{dynamic_depth}.\n\\noindent\\textbf{3) Jumping in videos.} Considering encoding those unimportant frames with a CNN still requires considerable computation, a more efficient solution could be dynamically skipping some frames without watching them. Existing arts  typically learn to predict the location that the network should jump to at each time step.\nFurthermore, both early stopping and dynamic jumping are allowed in , where the jumping stride is limited in a discrete range.\nAdaptive frame (AdaFrame)  generates a continuous scalar within the range of $[0,1]$ as the relative location. \n\\vspace{-1ex}", "cites": [7286, 755, 7270, 752, 751, 754, 7283, 8380, 753], "cite_extract_rate": 0.6, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to categorize and explain dynamic RNN approaches for video recognition, creating a coherent narrative around three main strategies: dynamic hidden state updates, early exiting, and jumping. It provides some critical context by highlighting efficiency gains and human-like behavior in video comprehension. While it offers a framework for understanding these methods, it does not go significantly beyond specific papers to form a meta-level abstraction or provide deep critique of limitations."}}
{"id": "ce2a3a82-4498-49fd-b6ea-af13eecfa56b", "title": "Dynamic Key Frame Sampling", "level": "subsubsection", "subsections": [], "parent_id": "844590b4-9a40-42f9-8052-47ad9e4a7783", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "Temporal-wise Dynamic Networks"], ["subsection", "Temporal-wise Dynamic Video Recognition"], ["subsubsection", "Dynamic Key Frame Sampling"]], "content": "\\label{frame_sampling}\n\\vspace{-0.25ex}\nRather than processing video frames recurrently as in Sec. \\ref{recurrent_video}, another line of work first performs an adaptive \\emph{pre-sampling} procedure, and then makes prediction by processing the selected subset of key frames or clips.\n\\noindent{\\textbf{1) Temporal attention} is a common technique for networks to focus on salient frames.} For face recognition, neural aggregation network  uses \\emph{soft} attention to adaptively aggregate frame features. To improve the inference efficiency, \\emph{hard} attention is realized to remove unimportant frames iteratively with RL for efficient video face verification .\n\\noindent{\\textbf{2) Sampling module} is also a prevalent option for dynamically selecting the key frames/clips in a video.} For example, the frames are first sampled uniformly in , and discrete decisions are made for each selected frame to go forward or backward step by step. As for clip-level sampling, SCSample  is designed based on a trained classifier to find the most informative clips for prediction. Moreover, dynamic sampling network (DSN)  segments each video into multiple sections, and a sampling module with shared weights across the sections is exploited to sample one clip from each section.\n{Adjusting multiple factors of deep models simultaneously has attracted researches in both static  and dynamic networks . For example, together with \\emph{temporal-wise} frame sampling, \\emph{spatially} adaptive computation can be achieved by spatial /temporal  resolution adaptation and patch selection . It would be promising to exploit the redundancy in both \\emph{input data} and \\emph{network structure} for further improving the efficiency of deep networks.}\n\\vspace{-2ex}", "cites": [8381, 756, 6975, 8382, 757, 7288, 7287, 686, 758], "cite_extract_rate": 0.6428571428571429, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers on dynamic key frame sampling, grouping techniques under 'temporal attention' and 'sampling module' and connecting them to broader dynamic network concepts. It also identifies the potential for combining spatial and temporal adaptivity. The critical evaluation is moderate, as it mentions efficiency improvements and limitations of hand-crafted methods, but does not deeply critique trade-offs or shortcomings. The abstraction is strong, with general insights into how redundancy in data and structure can be exploited for efficiency."}}
{"id": "4ffefefb-4651-456b-a07d-75e465a728b0", "title": "Inference and Training", "level": "section", "subsections": ["8d1ee291-ab1d-4e7f-8c16-431c07d68081", "2bd26a27-bb2c-4752-8590-7d1e502a061d"], "parent_id": "4fd906cb-4aa6-49a2-a5b7-ec10dfbd9280", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "Inference and Training"]], "content": "\\label{inference_and_train}\n\\vspace{-0.25ex}\nIn previous sections, we have reviewed three different types of dynamic networks (sample-wise (Sec. \\ref{sec_sample_wise}), spatial-wise (Sec. \\ref{sec_spatially_adaptive}) and temporal-wise (Sec. \\ref{sec_temporal_adaptive})). It can be observed that making data-dependent \\emph{decisions} at the inference stage is essential to achieve high efficiency and effectiveness. Moreover, \\emph{training} dynamic models is usually more challenging than optimizing static networks.\nNote that since parameter adaptation (Sec. \\ref{adaptive_params}) could be conveniently achieved by differentiable operations, models with dynamic parameters  can be directly trained by stochastic gradient descent (SGD) without specific techniques. Therefore, in this section we mainly focus on discrete decision making (Sec. \\ref{inference}) and its training strategies (Sec. \\ref{training}), which are absent in most static models.\n\\vspace{-1.5ex}", "cites": [675, 7277], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the concept of dynamic neural networks by distinguishing between different types of decision-making during inference, and connects this to the training challenges, particularly for discrete decisions. It references Paper 1 (CondConv) and Paper 2 (WeightNet) to contextualize parameter adaptation and training strategies, but the critical evaluation is limited. The abstraction is moderate, as it generalizes discrete decision-making and its training implications but stops short of identifying overarching theoretical principles."}}
{"id": "16951c8b-a4dd-48e3-a004-db0aa23dc8c4", "title": "Confidence-based Criteria", "level": "subsubsection", "subsections": [], "parent_id": "8d1ee291-ab1d-4e7f-8c16-431c07d68081", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "Inference and Training"], ["subsection", "Decision Making of Dynamic Networks"], ["subsubsection", "Confidence-based Criteria"]], "content": "\\vspace{-0.25ex}\nMany dynamic networks  are able to output \"easy\" samples at early exits if a certain confidence-based criterion is satisfied. These methods generally require estimating the confidence of intermediate predictions, which is compared to a predefined threshold for decision making.\nIn classification tasks, the confidence is usually represented by the maximum element of the \\emph{SoftMax} output . Alternative criteria include the entropy  and the score margin . On NLP tasks, a \\emph{model patience} is proposed in : when the predictions for one sample stay unchanged after a number of classifiers, the inference procedure stops. \nIn addition, the halting score in  could also be viewed as confidence for whether the current feature could be output to the next time step or calculation stage.\nEmpirically, the confidence-based criteria are easy to implement, and generally require no specific training techniques. A trade-off between accuracy and efficiency is controlled by manipulating the thresholds, which are usually tuned on a validation dataset. Note that the \\emph{overconfidence} issue in deep models  might affect the effectiveness of such decision paradigm, when the incorrectly classified samples could obtain a high confidence at early exits.\n\\vspace{-1ex}", "cites": [7266, 689, 683, 759, 7269, 676, 7273], "cite_extract_rate": 0.6363636363636364, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several papers to explain how confidence-based criteria function in dynamic neural networks, linking concepts like early exits, model patience, and halting scores. It provides a critical mention of the overconfidence issue in deep models, indicating an awareness of limitations. While it abstracts to a general principle (confidence as a decision criterion), the abstraction is not fully meta-level but focuses primarily on classification and NLP tasks."}}
{"id": "2a7772db-4dcb-4d14-b74b-bd6ac1bb74cf", "title": "Policy Networks", "level": "subsubsection", "subsections": [], "parent_id": "8d1ee291-ab1d-4e7f-8c16-431c07d68081", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "Inference and Training"], ["subsection", "Decision Making of Dynamic Networks"], ["subsubsection", "Policy Networks"]], "content": "\\label{inference_policy}\n\\vspace{-0.25ex}\nIt is a common option to build an additional policy network learning to adapt the network topology based on different samples. \nSpecifically, each input sample is first processed by the policy network, whose output directly determines which parts of the main network should be activated. For example, BlockDrop  and GaterNet  use a policy network to adaptively decide the \\emph{depth} and \\emph{width} of a backbone network. More generally, dynamic routing in a \\emph{SuperNet} can also be controlled by a policy network .\nOne possible limitation of this scheme is that the architectures and the training process of some policy networks are developed for a specific backbone , and may not be easily adapted to different architectures. \n\\vspace{-1ex}", "cites": [702, 718, 691], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly describes how policy networks function in dynamic neural networks and provides a few examples. While it mentions BlockDrop, GaterNet, and SuperNet, it does not meaningfully synthesize their contributions or compare their approaches. It includes a minor critical observation about the specificity of policy networks to backbones, but lacks deeper analysis or abstraction to broader principles."}}
{"id": "6e023be0-dfce-4cb5-b69e-f84dd0c01501", "title": "Gating Functions", "level": "subsubsection", "subsections": [], "parent_id": "8d1ee291-ab1d-4e7f-8c16-431c07d68081", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "Inference and Training"], ["subsection", "Decision Making of Dynamic Networks"], ["subsubsection", "Gating Functions"]], "content": "\\label{inference_gating}\n\\vspace{-0.25ex}\nGating function is a general and flexible approach to decision making in dynamic networks. It can be conveniently adopted as a plug-in module at arbitrary locations in any backbone network.\nDuring inference, each module is responsible for controlling the local inference graph of a layer or block. The gating functions take in intermediate features and efficiently produce binary-valued gate vectors to decide: 1) which channels need to be activated  \\emph{width}, 2) which layers need to be skipped , 3) which paths should be selected in a SuperNet , or 4) what locations of the input should be allocated computations .\nCompared to the aforementioned decision policies, the gating functions demonstrate notable generality and applicability. However, due to their lack of differentiability, these gating functions usually need specific training techniques, which will be introduced in the following Sec. \\ref{training}.\n\\vspace{-2ex}", "cites": [7274, 708, 751, 8374, 710, 8373, 696, 7281, 713, 699], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a clear analytical overview of gating functions in dynamic neural networks, synthesizing their general role and use cases across different types of dynamic models. While it connects multiple papers to highlight the versatility of gating functions, it lacks in-depth comparison or critique of specific approaches. The abstraction level is moderate, as it identifies the broader purpose of gating functions without delving into deeper theoretical or methodological generalizations."}}
{"id": "2bd26a27-bb2c-4752-8590-7d1e502a061d", "title": "Training of Dynamic Networks", "level": "subsection", "subsections": ["4d7d59fa-e730-411f-b178-1c7c2fc0254e", "34c67f80-85f1-4cea-8d35-f3e767fa3802"], "parent_id": "4ffefefb-4651-456b-a07d-75e465a728b0", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "Inference and Training"], ["subsection", "Training of Dynamic Networks"]], "content": "\\vspace{-0.5ex}\n\\label{training}\nBesides architecture design, training is also essential for dynamic networks. Here we summarize the existing training strategies for dynamic models from the perspectives of objectives and optimization.\n\\begin{table*}\n  \\scriptsize\n  \\vspace{-2ex}\n  \\caption{Applications of Dynamic Networks. For the type column, Sa, Sp and Te stand for sample-wise, spatial-wise and temporal-wise respectively.}\n  \\label{tab_tasks}\n  \\begin{center}\n    \\vspace{-6ex}\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{cccc}\n      \\toprule\n      \\textbf{Fields} & \\textbf{Data} & \\textbf{Type} & \\textbf{Subfields \\& references} \\\\\n      \\midrule\n      & \\multirow{6}*{\\textbf{Image}} & \\multirow{2}*{Sa} & Object detection (face , facial point , pedestrian , general ) \\\\\n      & & & Image segmentation , Super resolution , Style transfer ,  Coarse-to-fine classification \\\\ \n      \\cmidrule{3-4}\n      & &  \\multirow{3}*{Sa \\& Sp} & Image segmentation , Image-to-image \\\\\n      & & & translation , Object detection ,  Semantic image synthesis , \\\\\n      \\multirow{1}*{\\textbf{Computer}} & & & Image denoising , Fine-grained classification  Eye tracking , Super resolution  \\\\\n      \\cmidrule{3-4}\n      \\multirow{1}*{\\textbf{Vision}}& &  Sa \\& Sp \\& Te & General classification , Multi-object classification , Fine-grained classification  \\\\\n      \\cmidrule{2-4}\n      & \\multirow{4}*{\\textbf{Video}} & Sa & Multi-task learning (human action recognition and frame prediction)  \\\\\n      \\cmidrule{3-4}\n      & &  \\multirow{2}*{Sa \\& Te} & Classification (action recognition) , Semantic segmentation \\\\\n      & & & Video face recognition , Action detection , Action spotting  \\\\\n      \\cmidrule{3-4}\n      & &  Sa \\& Sp \\& Te & Classification , Frame interpolation , Super resolution , Video deblurring , Action prediction \\\\\n      \\cmidrule{2-4}\n      & \\textbf{Point Cloud} & Sa \\& Sp  & 3D Shape classification and segmentation, 3D scene segmentation , 3D semantic scene completion  \\\\\n      \\midrule\n      \\multirow{2}*{\\textbf{Natural}} & \\multirow{3}*{\\textbf{Text}} & Sa  & Neural language inference, Text classification, Paraphrase similarity matching, and Sentiment analysis  \\\\\n      \\cmidrule{3-4}\n      \\textbf{Language} & &  \\multirow{2}*{Sa \\& Te} & Language modeling , Machine translation , Classification , \\\\\n      \\textbf{Processing} & & &  Sentiment analysis , Question answering  \\\\\n      \\midrule\n      \\textbf{Cross-Field} & \\multicolumn{3}{c}{Image captioning , {Video captioning , Visual question answering , Multi-modal sentiment analysis }}  \\\\\n      \\midrule\n      \\multirow{2}{*}{\\textbf{Others}} &  \\multicolumn{3}{c}{{Time series forecasting} , Link prediction , {Recommendation system} }\\\\\n      & \\multicolumn{3}{c}{Graph classification , {Document classification} , Stereo confidence estimation }\\\\\n  \\bottomrule\n    \\end{tabular}}\n  \\end{center}\n  \\vskip -0.3in\n\\end{table*}\n\\vspace{-1ex}", "cites": [779, 725, 722, 7273, 683, 770, 771, 776, 7257, 778, 741, 7294, 7283, 7267, 674, 8381, 7290, 772, 746, 7285, 150, 8382, 760, 7281, 7289, 7272, 761, 747, 752, 757, 7288, 8383, 764, 750, 727, 743, 765, 748, 762, 8380, 777, 775, 769, 686, 680, 767, 7295, 7291, 763, 773, 744, 774, 7286, 7284, 7282, 7278, 749, 7215, 7292, 726, 766, 768, 676, 7293, 738, 7274, 780, 7287], "cite_extract_rate": 0.5811965811965812, "origin_cites_number": 117, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.8}, "insight_level": "low", "analysis": "The section presents a large number of cited papers but lacks synthesis and integration of their core ideas into a coherent narrative. There is minimal critical analysis of the methods or their limitations, and no clear abstraction or identification of overarching principles in dynamic network training."}}
{"id": "4d7d59fa-e730-411f-b178-1c7c2fc0254e", "title": "Training Objectives for Efficient Inference", "level": "subsubsection", "subsections": [], "parent_id": "2bd26a27-bb2c-4752-8590-7d1e502a061d", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "Inference and Training"], ["subsection", "Training of Dynamic Networks"], ["subsubsection", "Training Objectives for Efficient Inference"]], "content": "\\label{sec_train_objectives}\n\\vspace{-0.25ex}\n\\noindent\\textbf{1) Training multi-exit networks.} We first notice that early-exiting dynamic networks  are generally trained by minimizing a weighted cumulative loss of intermediate classifiers. One challenge for training such models is the joint optimization of multiple classifiers, which may interfere with each other. MSDNet  alleviates the problem through its special architecture design. Several improved training techniques  are proposed for multi-exit networks, including a gradient equilibrium algorithm to stable the training process, and a bi-directional knowledge transfer approach to boost the collaboration of classifiers. {For temporal-wise early exiting, the training of the policy network in FrameExit  is supervised by pseudo labels.}\n\\noindent\\textbf{2) Encouraging sparsity.} Many dynamic networks adapt their inference procedure by conditionally activating their computational units  or strategically sampling locations from the input .\nTraining these models without additional constraints would result in superfluous computational redundancy, as a network could tend to activate all the candidate units for minimizing the task-specific loss. \nThe overall objective function for restraining such redundancy are typically written as $\\mathfrak{L}\\! =\\!\\mathfrak{L}_\\mathrm{task}\\!+\\!\\gamma \\mathfrak{L}_\\mathrm{sparse}$, where $\\gamma$ is the hyper-parameter balancing the two items for the trade-off between accuracy and efficiency. In real-world applications, the second item can be designed based on the gate/mask values of candidate units (e.g. channels , layers  or spatial locations ). Specifically, one may set a target activation rate  or minimizing the $\\mathcal{L}_1$ norm of the gates/masks . It is also practical to directly optimize a resource-aware loss (e.g. FLOPs) , which can be estimated according to the input and output feature dimension for every candidate unit.\n\\noindent{\\textbf{3) Others.} Note that extra loss items are mostly designed for but not limited to improving efficiency. Take  as an example, the model progressively focuses on a selected region, and is trained with an additional \\emph{inter-scale pairwise ranking loss} for proposing more discriminative regions. Moreover, knowledge distilling is utilized to boost the co-training of multiple sub-networks in  and .}\n\\vspace{-1ex}", "cites": [7266, 701, 693, 7274, 708, 8374, 8373, 696, 7281, 753], "cite_extract_rate": 0.7692307692307693, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information well by grouping training objectives under three categories (multi-exit, sparsity, and others) and linking them to their broader goals in dynamic neural networks. It integrates ideas from multiple papers to form a coherent narrative on how efficiency can be achieved during training. Critical analysis is moderate, as it mentions issues like interference between classifiers and practical trade-offs but does not deeply critique limitations or evaluate competing approaches. The section identifies some abstraction, particularly in framing efficiency objectives as a combination of task-specific and sparsity-based losses, but the insights remain somewhat grounded in specific methods rather than reaching a meta-level understanding."}}
{"id": "34c67f80-85f1-4cea-8d35-f3e767fa3802", "title": "Optimization of Non-differentiable Functions", "level": "subsubsection", "subsections": [], "parent_id": "2bd26a27-bb2c-4752-8590-7d1e502a061d", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "Inference and Training"], ["subsection", "Training of Dynamic Networks"], ["subsubsection", "Optimization of Non-differentiable Functions"]], "content": "\\vspace{-0.25ex}\nA variety of dynamic networks contain non-differentiable functions that make discrete decisions to modify their architectures or sampling spatial/temporal locations from the input. These functions can not be trained directly with back-propagation. Therefore, specific techniques are studied to enable the end-to-end training as follows.\n\\noindent\\textbf{1) Gradient estimation} is proposed to approximate the gradients for those non-differentiable functions and enable back-propagation. In , straight-through estimator (STE) is exploited to heuristically copy the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the \\emph{Sigmoid} argument.\n\\noindent\\textbf{2) Reparameterization} is also a popular technique to optimize the discrete decision functions. For instance, the gating functions controlling the network width  or depth  can both be trained with \\emph{Gumbel SoftMax} , which is also used for pixel-level dynamic convolution .  An alternative technique is \\emph{Improved SemHash}  adopted in  and  to train their hard gating modules.\n{Note that although these reparameterization techniques enable joint optimizing dynamic models together with gating modules in an end-to-end fashion, they usually lead to a longer training process for the decision functions to converge into a stable situation . Moreover, the model performance might be sensitive to some extra hyper-parameters (e.g. temperature in \\emph{Gumbel SoftMax}), which might also increase the training cost for these dynamic networks.}\n\\noindent\\textbf{3) Reinforcement learning (RL)} is widely exploited for training non-differentiable decision functions. In specific, the backbones are trained by standard SGD, while the agents (either policy networks in Sec. \\ref{inference_policy} or gating functions in Sec. \\ref{inference_gating}) are trained with RL to take discrete actions for dynamic inference graphs  or spatial/temporal sampling strategies .\n{One challenge for RL-based training is the design of reward functions, which is important to the accuracy-efficiency tradeoff of dynamic models. Commonly seen reward signals are usually constructed to minimize a penalty item of the computational cost . Moreover, the training could be costly due to a multi-stage procedure: a pre-training process may be required for the backbone networks before the optimization of decision  or sampling  modules, and joint finetuning may be indispensable finally.}\n\\vspace{-2ex}", "cites": [8381, 708, 702, 7279, 696, 781, 691, 782, 750, 8373, 8375, 674, 7281, 699], "cite_extract_rate": 0.8235294117647058, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes several techniques for optimizing non-differentiable functions in dynamic neural networks, including gradient estimation, reparameterization, and reinforcement learning. It connects these methods across multiple cited works to form a coherent discussion. While it offers some critical observations (e.g., training cost and sensitivity to hyperparameters), these are not deeply analyzed. The section identifies general patterns but stops short of forming a meta-level framework or deeper abstraction."}}
{"id": "3cd25c7f-359c-4b70-8c02-f2738746b35c", "title": "Application of Dynamic Networks", "level": "section", "subsections": [], "parent_id": "4fd906cb-4aa6-49a2-a5b7-ec10dfbd9280", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "Application of Dynamic Networks"]], "content": "\\label{sec_tasks}\n\\vspace{-0.25ex}\nIn this section, we summarize the applications of dynamic networks. Representative methods are listed in Table \\ref{tab_tasks} based on the input data modality.\nFor image recognition, most dynamic CNNs are designed to conduct \\emph{sample-wise} or \\emph{spatial-wise} adaptive inference on classification tasks, and many inference paradigms can be generalized to other applications. Note that as mentioned in Sec. \\ref{region_level}, the object recognition could be formulated as a sequential decision problem . By allowing early exiting in these approaches, \\emph{temporally} adaptive inference procedure could also be enabled.\nFor text data, reducing its intrinsic temporal redundancy has attracted great research interests, and the inference paradigm of \\emph{temporal-wise} dynamic RNNs (see Sec. \\ref{temporal_text}) is also general enough to process audios . Based on large language models such as Transformer  and BERT , adaptive depths  are extensively studied to reduce redundant computation in network architectures.\nFor video-related tasks, the three types of dynamic inference can be implemented simultaneously . However, for the networks that do not process videos recurrently, e.g. 3D CNNs , most of them still follow a static inference scheme. Few researches have been committed to building dynamic 3D CNNs , which might be an interesting future research direction.\n{Moreover, dynamic networks (especially the attention mechanism) have also been applied to dynamically fuse the features from different modalities in some multi-modal learning tasks, e.g. RGB-D image segmentation  and image/video captioning .}\nFinally, dynamic networks have also been exploited to tackle some fundamental problems in deep learning. For example, multi-exit models can be used to: 1) alleviate the \\emph{over-thinking} issue while reducing the overall computation ; 2) perform \\emph{long-tailed classification}  by inducing early exiting in the training stage; and 3) improve the model \\emph{robustness} . For another example, the idea of dynamic routing is implemented for: 1) {reducing the \\emph{training cost} under a multi-task setting } and 2) finding the optimal fine-tuning strategy for per example in \\emph{transfer learning} .\n\\vspace{-2ex}", "cites": [764, 744, 785, 7273, 765, 770, 7269, 777, 758, 38, 786, 692, 768, 783, 7296, 7295, 7272, 7271, 7, 757, 674, 784], "cite_extract_rate": 0.8148148148148148, "origin_cites_number": 27, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple cited works to categorize dynamic networks by their application domains (image, text, video) and modality-specific strategies, showing strong integration. It critically identifies gaps such as the limited exploration of dynamic 3D CNNs for video tasks and the use of dynamic networks to address broader issues like over-thinking and robustness. The abstraction is evident in generalizing inference paradigms across data types and linking them to overarching deep learning challenges."}}
{"id": "3fc7f7a7-7403-4a95-918e-d6e40acaa928", "title": "Theories for Dynamic Networks", "level": "subsection", "subsections": [], "parent_id": "e9013abd-3d22-4eab-a882-4ba5e86719f2", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "{Challenges and Future Directions"], ["subsection", "Theories for Dynamic Networks"]], "content": "\\vspace{-0.5ex}\nDespite the success of dynamic neural networks, relatively few researches has been committed to analyze them from the theoretical perspective. In fact, theories for a deep understanding of current dynamic learning models and further improving them in principled ways are highly valuable. {Notably, it has been proven that a dynamic network with an adaptive width can preserve the representation power of an unsparsified model .} However, there are more theoretical problems that are fundamental for dynamic networks. Here we list several of them as follows.\n\\noindent\\textbf{1) Optimal decision in dynamic networks.}\nAn essential operation in most dynamic networks (especially those designed for improving computational efficiency) is making data-dependent decisions, e.g., determining whether a module should be evaluated or skipped. Existing solutions either use confidence-based criteria, or introduce policy networks and gating functions. Although being effective in practice (as mentioned in Sec. \\ref{inference_and_train}), they may not be optimal and lack theoretical justifications. Take early exiting as an example, the current heuristic methods  might face the issues of overconfidence, high sensitivity for threshold setting and poor transferability. As for policy networks or gating modules, runtime decisions can be made based on a learned function.\nHowever, they often introduce extra computations, and usually require a long and unstable training procedure. Therefore, principled approaches with theoretical guarantees for decision function design in dynamic networks is a valuable research topic.\n\\noindent\\textbf{2) Generalization issues.} \nIn a dynamic model, a sub-network might be activated for a set of test samples that are not uniformly sampled from the data distribution, e.g., smaller sub-networks tend to handle ``easy'' samples, while larger sub-networks are used for ``hard'' inputs . This brings a divergence between the training data distribution and that of the inference stage, and thus violates the common \\emph{i.i.d.} assumption in classical machine learning. Therefore, it would be interesting to develop new theories to analyze the generalization properties of dynamic networks under such distribution mismatch. Note that transfer learning also aims to address the issue of distributional shift at test time, but the samples of the target domain are assumed to be accessible in advance. In contrast, for dynamic models, the test distribution is not available until the training process is finished, when the network architecture and parameters are finalized. This poses greater challenges than analyzing the generalization issues in transfer learning.", "cites": [7266, 706], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates the theoretical aspects of dynamic networks, drawing on concepts from the cited papers to discuss decision-making and generalization issues. It critically examines the limitations of existing methods, such as overconfidence and distribution mismatch, and highlights the need for principled approaches. While it provides some abstraction by identifying broader theoretical challenges, the synthesis remains somewhat focused on individual problems rather than a novel overarching framework."}}
{"id": "9458c790-c005-432b-a25a-958ab55678a8", "title": "Architecture Design for Dynamic Networks", "level": "subsection", "subsections": [], "parent_id": "e9013abd-3d22-4eab-a882-4ba5e86719f2", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "{Challenges and Future Directions"], ["subsection", "Architecture Design for Dynamic Networks"]], "content": "Architecture design has been proven to be essential for deep networks. Existing researches on architectural innovations are mainly proposed for static models , while relatively few are dedicated to developing architectures specially for dynamic networks. It is expected that architectures developed specifically for dynamic networks may further improve their effectiveness and efficiency. For example, the interference among multiple classifiers in an early-exiting network could be mitigated by a carefully designed multi-scale architecture with dense connections . \nPossible research direction include designing dynamic network structures either by hand (as in ), or by leveraging the NAS techniques (as in ). Moreover, considering the popularity of Transformers , {recent work has proposed dynamic vision Transformers with adaptive early exiting  or token sparsification .} Developing a dynamic version of this family of models could also be an interesting direction.\n{Note that the research on dynamic networks differs from a seemingly close topic, i.e. model compression . One common goal of them is improving the network efficiency with minimal accuracy drop. However, model compression may focus on reducing the \\emph{size} of deep networks, while dynamic networks pay more attention to the \\emph{computation}, even at the price of slightly \\emph{increasing} model size . Moreover, model compression typically adopts pruning  or quantization  techniques to produce compact \\emph{static} models, which treat all the inputs in the same way. In contrast, dynamic networks perform data-dependent computation on different inputs, which can effectively reduce the intrinsic redundancy in static models.}\n\\vspace{-1ex}", "cites": [7266, 96, 700, 8372, 718, 7297, 97, 787, 732, 696, 685, 504, 687], "cite_extract_rate": 0.7647058823529411, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple papers to highlight architectural considerations for dynamic networks, such as multi-scale designs, NAS techniques, and dynamic token sparsification. It also draws a critical distinction between dynamic networks and model compression, evaluating their differing focuses. While it identifies some patterns and broader implications, it does not fully generalize to a meta-level framework or deeply critique individual methods."}}
{"id": "baaa0a8b-88d7-44dc-8c2b-1c3a26c2c457", "title": "Applicability for More Diverse Tasks", "level": "subsection", "subsections": [], "parent_id": "e9013abd-3d22-4eab-a882-4ba5e86719f2", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "{Challenges and Future Directions"], ["subsection", "Applicability for More Diverse Tasks"]], "content": "\\vspace{-0.5ex}\nMany existing dynamic networks (e.g., most of the sample-wise adaptive networks) are designed specially for classification tasks, and cannot be applied to other vision tasks such as object detection and semantic segmentation. The difficulty arises from the fact that for these tasks there is no simple criterion to assert whether an input image is easy or hard, as it usually contains multiple objects and pixels that have different levels of difficulty. Although many efforts, e.g., spatially adaptive models  and soft attention based models , have been made to address this issue, it remains a challenging problem to develop a unified and elegant dynamic network that can serve as an off-the-shelf backbone for a variety of tasks.\n\\vspace{-1.5ex}", "cites": [7281, 675, 7268, 674, 676], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several relevant papers to highlight a common challenge: the limited applicability of dynamic neural networks beyond classification. It connects ideas from spatially adaptive models and attention-based approaches to frame the issue. While it identifies a general problem and some attempts to address it, it does not offer a comprehensive comparative analysis or deeply critique the limitations of each method. The abstraction is reasonable, as it points to the need for a unified dynamic network framework for diverse vision tasks."}}
{"id": "23446088-fa89-4b21-8a64-51ee553e33cb", "title": "Gap between Theoretical \\& Practical Efficiency", "level": "subsection", "subsections": [], "parent_id": "e9013abd-3d22-4eab-a882-4ba5e86719f2", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "{Challenges and Future Directions"], ["subsection", "Gap between Theoretical \\& Practical Efficiency"]], "content": "\\label{discuss_implement}\n\\vspace{-0.5ex}\nThe current deep learning hardware and libraries are mostly optimized for static models, and they may not be friendly to dynamic networks. Therefore, we usually observe that the practical runtime of dynamic models lags behind the theoretical efficiency. For example, some spatially adaptive networks involve sparse computation, {which is known to be inefficient on modern computing devices due to the memory access bottleneck . A recent line of work focuses on the codesign of algorithm and hardware for accelerating deep models on platforms with more flexibility such as FPGA . Many input-dependent operations, including pixel-level dynamic computation , adaptive channel pruning  and early exiting , have also been tailored together with hardware for further improving their practical efficiency. It is an interesting research direction to simultaneously optimize the algorithm, hardware and deep learning libraries to harvest the theoretical efficiency gains of dynamic networks.}\n{In addition, a data-dependent inference procedure, especially for the dynamic \\emph{architectures}, usually requires a model to handle input samples sequentially, which also poses challenge for parallel computation. Although inference with batches has been enabled for early-exiting networks , the conflict between adaptive computational graph and parallel computation still exists for other types of dynamic architectures. This issue is mitigated in the scenario of mobile/edge computing, where the input signal by itself is sequential and the computing hardware is less powerful than high-end platforms. However, designing dynamic networks that are more compatible with existing hardware and software is still a valuable and challenging topic.}\n\\vspace{-1.5ex}", "cites": [721, 788, 7281], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the cited papers by connecting algorithm-hardware co-design (Papers 1 and 2) with spatially adaptive computation (Paper 3) to explain the inefficiency of dynamic models on current hardware. It provides critical analysis by highlighting the limitations of parallel computation and the mismatch between adaptive architectures and existing systems. The discussion abstracts the issue to a broader challenge in deep learning deployment, emphasizing the need for holistic optimization of algorithms, hardware, and libraries."}}
{"id": "a8fb9eb8-777e-40f9-a6cc-da50d402237b", "title": "Robustness Against Adversarial Attack", "level": "subsection", "subsections": [], "parent_id": "e9013abd-3d22-4eab-a882-4ba5e86719f2", "prefix_titles": [["title", "Dynamic Neural Networks: A Survey"], ["section", "{Challenges and Future Directions"], ["subsection", "Robustness Against Adversarial Attack"]], "content": "\\vspace{-0.5ex}\nDynamic models may provide new perspectives for the research on adversarial robustness of deep neural networks. {For example, recent work  has leveraged the multi-exit structure to improve the robustness against adversarial attacks. Moreover, traditional attacks are usually aimed at causing \\emph{misclassification}. For dynamic networks, it is possible to launch attacks on \\emph{efficiency} . Specifically, by adjusting the objective function of the adversarial attack, input-adaptive models could be fooled to activate all their intermediate layers  or yielding confusing predictions at early exits  even for \"easy\" samples. It has also been observed that the commonly used adversarial training is not effective to defend such attacks.} The robustness of dynamic network is an interesting yet understudied topic.\n\\vspace{-1.5ex}", "cites": [789], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the key point from Paper 1 regarding slowdown attacks on adaptive multi-exit networks, and abstracts the idea to suggest that dynamic models introduce new challenges in robustness, particularly concerning efficiency. However, the critical analysis is limited to stating that adversarial training is ineffective without delving deeper into why or exploring alternative defenses, restricting its depth."}}
