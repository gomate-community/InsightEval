{"id": "58c254c5-72b9-4976-a493-e0824a0d96b5", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "21139f68-9b71-4857-a410-85c84d8234d4", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "Introduction"]], "content": "\\label{sec:introduction}}\nThe initial inspiration of Artificial Intelligence (AI)\nis {to imitate human perception}, \\eg, seeing, hearing, touching, smelling.\nIn general, a {modality} is often associated with a specific sensor that creates a unique communication channel, such as vision and language  .\nIn humans,\na fundamental mechanism in our sensory perception is the ability to leverage multiple modalities of perception data collectively in order to engage ourselves properly with the world under dynamic unconstrained circumstances, with each modality serving as a distinct information source characterized by different statistical properties.\nFor example, an image gives the visual appearance of an ``elephants playing in water'' scene via thousands of pixels, whilst the corresponding text describes this moment with a sentence using discrete words.\nFundamentally, a multimodal AI system needs to ingest, interpret, and reason about multimodal information sources to realize similar human level perception abilities.\n{Multimodal learning} (MML) is a general approach to building AI models that can extract and relate information from multimodal data .\nThis survey focuses on multimodal learning with Transformers  (as demonstrated in Figure \\ref{fig:transformer}), inspired by their intrinsic advantages and scalability in modelling different modalities (\\eg, language, visual, auditory) and tasks (\\eg, language translation, image recognition, speech recognition) with fewer  modality-specific architectural assumptions (\\eg, translation invariance and local grid attention bias in vision) .\nConcretely, the input to a Transformer could encompass one or multiple sequences of tokens, and each sequence's attribute (\\eg, the modality label, the sequential order),\nnaturally allowing for MML without architectural modification .\nFurther, learning per-modal specificity and inter-modal correlation \ncan be simply realized by controlling the input pattern of self-attention.\nCritically, there is a recent surge of research attempts and activities across distinct disciplines  exploring the Transformer architectures, resulting in a large  number of novel MML methods being developed in recent years, along with significant and diverse advances in various areas\n.\nThis calls for a timely review and summary of representative methods to enable researchers to understand the global picture of the MML field across related disciplines and more importantly to capture a holistic structured picture of  current achievements as well as major challenges.\n\\keypoint{Taxonomy}\nFor better readability and reachability from and across different disciplines, \nwe adopt a {two-tier} structured taxonomy\nbased on the application and challenge dimensions respectively.\nThis has several benefits:\n(1) Researchers with expertise in specific applications\ncan find \nthose applications appropriate to their own research domain\nbefore connecting to other related domains.\n(2) Similar model designs and architectures developed in different domains\ncan be summarized in an abstract, formula-driven perspective so that the mathematical ideas of various models formed in different applications can be correlated and contrasted on common ground, crossing domain-specific restrictions.\nCrucially, our taxonomy offers an interesting stereo-view of individual works\nwith the insights in both application specificity and formulation generality.\nIt is hoped that this can help to break down domain boundaries\nand foster more effective idea communication and exchange across modalities.\nBy using the prompt modelling strategy   as a basis for investigation, we also include \nthe classical classification problem (\\eg, image classification) -- usually regarded\nas a single modality learning application in conventional MML surveys  --\nas a special MML application.\nThis has the potential to significantly enrich MML, as the classification problem\nis an AI topic amongst the most extensive studies in the literature .\n\\keypoint{Scope}\nThis survey will discuss the multimodality specific designs of Transformer architecture including, but not limited to, the following modalities:\nRGB image , depth image , \\blue{multispectral image }, video , audio/speech/music ,\ntable , scene graph/layout ,\npose skeleton , SQL , \nrecipe , programming language ,\nsign language ,\npoint cloud ,\nsymbolic knowledge (graph) , multimodal knowledge graph , sketch drawing , 3D object/scene , document , programming code  and Abstract Syntax Tree (AST) -- a kind of graph , optical flow ,\n medical knowledge (\\eg, diagnosis code ontology ).\nNote that this survey will not discuss the multimodal papers where Transformer is used simply  as the feature extractor without multimodal designs.\n\\keypoint{Related Surveys}\nWe relate this paper to existing surveys \nof the two specific dimensions MML and Transformers.\nThere exist a few MML surveys .\nIn particular,  proposed a structured, acknowledged taxonomy by five challenges, which we also adopt\nas part of our structure.\nUnlike , and , which review general machine learning models,\nwe instead focus on Transformer architectures and their self-attention mechanisms.\nSeveral surveys dedicated to Transformers have been recently introduced, with a range of emphases including \ngeneral Transformers ,\nefficient designs ,\nvisualization ,\ncomputer vision tasks ,\nmedical imaging ,\nvideo tasks , and \nvision language pretraining .\nWhile  consider MML,\ntheir reviews are somewhat  limited in the scope, taxonomy, and coverage.\nTo our knowledge, only a few surveys on video-language pretraining (VLP)   are relevant to MML.\nHowever, VLP is only a subdomain of MML.\nIn this survey, we focus solely on the intersection of multimodal learning and Transformers.\n\\begin{figure}[!t]\n\t\\centering\t\n\t\\includegraphics[width=0.5\\linewidth]{./figures/transformer.png}\n\t\\caption{Overview of Transformer .}\n\t\\label{fig:transformer}\n\\end{figure}\n\\keypoint{{Features}}\nTo our knowledge, this paper is the first comprehensive review of the state of Transformer based multimodal machine learning.\nThe major features of this survey include \n{\\textbf{(1)}} {We highlight that Transformers have the advantage that they can work in a modality-agnostic way. Thus, they are compatible with various modalities (and combinations of modalities). }\n{To support this view, we, for the first time, offer an understanding of the intrinsic traits of Transformers in a multimodal context from a geometrically topological perspective.}\nWe suggest that self-attention be treated as a graph style modelling, which models the input sequence (both uni-modal and multimodal) as a fully-connected graph. \nSpecifically, self-attention models the embedding of arbitrary tokens from an arbitrary modality as a graph node. \n{\\textbf{(2)}} We discuss the key components of Transformers in a multimodal context as mathematically as possible.\n{\\textbf{(3)}} Based on Transformers, cross-modal interactions (\\eg,\nfusion, alignment)  are essentially  processed by self-attention\nand its variants.\nIn this paper, we extract the mathematical essence and formulations of  Transformer based MML practices, from the perspective of self-attention designs.\n\\keypoint{{Contributions}}\nHaving presented our review of the landscape of multimodal learning, Transformer ecosystem, and multimodal big data era in Section \\ref{sec:background},\nwe summarize our main contributions as the follows. \n{{\\textbf{(1)}} In Section \\ref{sec:multi-modal-transformer}, we present a \n{systematic} reviewing of \\vani{} Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective.} \n{\\textbf{(2)}} We contribute a taxonomy for Transformer based MML from two complementary perspectives, \\ie, application based and challenge based.\nIn Section \\ref{sec:applications-and-representative-models},\nwe provide a review of multimodal Transformer applications, via\ntwo important paradigms, \\ie, for multimodal pretraining and for specific multimodal tasks.\nIn Section \\ref{sec:challenges-and-designs}, \nwe summarize the common challenges and designs shared by the various multimodal Transformer models and applications. \n{\\textbf{(3)}} In Section \\ref{sec:discussion-and-outlook}, we discuss current bottlenecks, existing  problems, and potential research directions for Transformer based MML.", "cites": [4765, 5208, 7905, 5219, 5221, 5233, 4862, 5222, 864, 5216, 7903, 5213, 5217, 5231, 5224, 7360, 7904, 4832, 7302, 5215, 4769, 5230, 5232, 5225, 5234, 5235, 38, 5218, 7907, 5220, 5227, 5214, 5223, 5226, 5210, 7298, 7040, 768, 5229, 7, 7906, 5212, 7908, 7078, 1639, 5209, 1030, 732, 5228, 5211], "cite_extract_rate": 0.847457627118644, "origin_cites_number": 59, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section provides strong synthesis by integrating diverse multimodal Transformer applications and design approaches into a coherent framework, particularly through its two-tier taxonomy. It offers critical analysis by noting limitations in existing surveys and highlighting the novelty of treating self-attention as graph-style modeling. The abstraction is high, as it generalizes across modalities and emphasizes the mathematical and structural principles underlying multimodal learning with Transformers."}}
{"id": "9a29f2bd-6dfd-4aa9-84a3-29ec53a79083", "title": "Multimodal Learning (MML)", "level": "subsection", "subsections": [], "parent_id": "2e3f79e1-6ad8-40a4-ae3d-6d29a207351d", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "Background"], ["subsection", "Multimodal Learning (MML)"]], "content": "MML  has been an important research area in recent decades; an early multimodal application --\naudio-visual speech recognition was studied in 1980s .\nMML is key to human societies.\nThe world we humans live in is a \\md{} environment, thus both our observations and behaviours are  \\md{} . \nFor instance, an AI navigation robot \nneeds multimodal sensors to perceive the real-world environment , \\eg, camera, LiDAR, radar, ultrasonic, GNSS, HD Map, odometer.\nFurthermore, human behaviours, emotions, events, actions, and humour are multimodal, thus various human-centred MML tasks are widely studied, including \nmultimodal emotion recognition , multimodal event representation ,\nunderstanding multimodal humor ,\nface-body-voice based video person-clustering , \\etc.\nThanks to the development of the internet and a wide variety of intelligent devices in recent years, increasing amounts of multimodal data are being transmitted over the internet, thus an increasing number of multimodal application scenarios are emerging.\nIn modern life, we can see various multimodal applications, including commercial services (\\eg, e-commerce/commodity retrieval , vision-and-language navigation (VLN) ), communication (\\eg, lip reading , sign language translation ), human-computer interaction , healthcare AI , surveillance AI , \\etc. \nMoreover, in the era of Deep Learning, deep neural networks greatly promote the development of MML, and\nTransformers  are a highly competitive architecture family,  bringing new challenges and opportunities to  MML.\n\\blue{In particular, the recent success of large language models and their multimodal derivatives  further demonstrates the potential of Transformers in multimodal foundation models.}", "cites": [5221, 5237, 7565, 8885, 5241, 5240, 3003, 5235, 38, 9131, 7467, 5236, 5242, 5238, 1030, 5239, 1582], "cite_extract_rate": 0.5483870967741935, "origin_cites_number": 31, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a general overview of multimodal learning and lists several application domains and tasks, citing relevant papers. However, it does not deeply synthesize or connect the cited works into a cohesive narrative, nor does it critically evaluate or compare them. It remains largely descriptive, with minimal abstraction or identification of broader principles."}}
{"id": "1a8106cc-4753-45af-b6d5-2744346344ec", "title": "Transformers: a Brief History and Milestones", "level": "subsection", "subsections": [], "parent_id": "2e3f79e1-6ad8-40a4-ae3d-6d29a207351d", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "Background"], ["subsection", "Transformers: a Brief History and Milestones"]], "content": "Transformers are emerging as promising learners.\n\\vani{} Transformer  benefits from a self-attention mechanism, and is a breakthrough model for sequence-specific representation learning that was originally proposed for NLP, achieving the \\sota{} on various NLP tasks.\n{Following the great success of \\vani{} Transformer, a lot of derivative models have been proposed, \\eg, BERT , BART , GPT , \nLongformer ,\nTransformer-XL ,\nXLNet .}\nTransformers currently  stand at the dominant position in NLP domains, and this motivates\nresearchers try to apply Transformers to other modalities, such as visual domains.\nIn early attempts for visual domain, the general pipeline \nis ``CNN features + standard Transformer encoder'', and researchers achieved BERT-style pretraining, via preprocessing raw images by resizing to a low resolution and reshaping into a 1D sequence .\nVision Transformer (ViT)  is a seminal work that contributes an end-to-end solution by applying the encoder of Transformer to images. \nBoth ViT and its variants have been widely  applied to various computer vision tasks, including\nlow-level tasks , recognition , detection , segmentation , \\etc, and also work well for both supervised  and self-supervised  visual learning.\nMoreover, some recently-released works provide further theoretical understanding for ViT, \\eg, its internal representation robustness , the continuous behaviour of its latent representation propagation .\nMotivated by the great success of Transformer, \nVideoBERT  is a breakthrough work that is the first work to extend Transformer to the multimodal tasks. VideoBERT demonstrates the great potential of Transformer in multimodal context.\nFollowing VideoBERT, a lot of Transformer based multimodal pretraining models (\\eg, ViLBERT , \nLXMERT , VisualBERT , VL-BERT , UNITER , CBT , Unicoder-VL , B2T2 , VLP , 12-in-1 , Oscar , Pixel-BERT , ActBERT , ImageBERT , HERO , UniVL ) have become research topics of increasing interest  in the field of machine learning.\nIn 2021, CLIP  \nwas proposed. It is a new milestone that uses \\md{} pretraining to convert classification as a retrieval task that enables the pretrained models to tackle zero-shot recognition.\nThus, CLIP is a successful practice that makes full use of large-scale multimodal pretraining to enable zero-shot learning.\nRecently, the idea of CLIP is further studied,\n\\eg, CLIP pretrained model based zero-shot semantic segmentation ,\nALIGN , CLIP-TD , ALBEF , and CoCa .", "cites": [1275, 7080, 2514, 2023, 2012, 2011, 7070, 2017, 2015, 8886, 5248, 7535, 1278, 38, 1272, 5243, 7298, 768, 7, 5247, 1501, 5249, 2527, 9149, 5246, 1639, 7370, 7850, 732, 5244, 7590, 7909, 11, 5245, 1279], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 42, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a chronological and descriptive overview of the Transformer architecture and its evolution into multimodal learning. It synthesizes key developments and milestones, connecting some ideas (e.g., BERT to Vision Transformer and then to multimodal pretraining). However, it lacks critical evaluation of the models' strengths and weaknesses and offers minimal abstraction beyond listing applications and techniques."}}
{"id": "b46863a5-1c8c-4ffc-bc75-abb3a937511e", "title": "\\MM{", "level": "subsection", "subsections": [], "parent_id": "2e3f79e1-6ad8-40a4-ae3d-6d29a207351d", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "Background"], ["subsection", "\\MM{"]], "content": "Big Data} \n{In the past decade, with the rapid development of internet applications such as social media and online retail,\nmassive multimodal \ndatasets have been proposed, \\eg,\nConceptual Captions , COCO , VQA , Visual Genome , SBU Captions , Cooking312K , LAIT , \ne-SNLI-VE ,\nARCH\n, Adversarial VQA ,\nOTT-QA ,\nMULTIMODALQA (MMQA) ,  \nVALUE ,\nFashion IQ ,\nLRS2-BBC \n,\nActivityNet ,\nVisDial .}\nSome emergent new trends among the recently released multimodal datasets are:   \n{\\textbf{(1)}} {Data scales are larger.}\nVarious recently released datasets are million-scale, \\eg, \nProduct1M ,\nConceptual 12M ,\nRUC-CAS-WenLan  (30M),\nHowToVQA69M ,\nHowTo100M ,\nALT200M ,\nLAION-400M .\n{\\textbf{(2)}} {More modalities.} \nIn addition to the general modalities of vision, text, and audio, further diverse modalities are emerging, \\eg, \nPano-AVQA  -- the first large-scale spatial and audio-visual question\nanswering dataset on $360^{\\circ}$ videos,\nYouTube-360 (YT-360)  ($360^{\\circ}$ videos),\nAIST++  (a new multimodal dataset of 3D\ndance motion and music),\nArtemis  (affective language for visual arts). In particular, MultiBench~ provides a dataset including 10 modalities. \n{\\textbf{(3)}}\n{More scenarios.} \nIn addition to common caption and QA datasets,\nmore applications and scenarios have been studied, \\eg, \nCIRR  (real-life images), \nProduct1M ,\nBed and Breakfast (BnB)   (vision-and-language navigation),\nM3A  (financial dataset),\nX-World  (autonomous drive). \n{\\textbf{(4)}}\n{Tasks are more difficult.} \nBeyond the straightforward tasks,\nmore abstract multimodal tasks are proposed,\n\\eg, MultiMET  (a multimodal dataset for metaphor understanding),\nHateful Memes  (hate speech in multimodal memes). \n{\\textbf{(5)}} {Instructional videos have become increasingly popular}, \\eg, cooking video YouCookII .   Aligning a sequence of instructions to a video of someone carrying out a task is an example of a powerful pretraining pretext task .\n\\blue{Pretext tasks are pre-designed problems to force the models to learn representation by solving them.} \n\\input{tex/datasets}\nSimilar to other deep neural network architectures, Transformers are also data hungry.\nTherefore, their high-capacity models and multimodal big data basis co-create the prosperity of the Transformer based multimodal machine learning.\nFor instance, big data bring zero-shot learning capability to VLP Transformer models.", "cites": [5252, 5259, 7911, 7796, 486, 5271, 5251, 7910, 5254, 5266, 5262, 5269, 7535, 5255, 5261, 5257, 768, 8887, 5212, 5268, 5253, 5267, 2901, 5256, 5250, 5260, 5264, 5270, 5258, 5263, 5265], "cite_extract_rate": 0.8378378378378378, "origin_cites_number": 37, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes a wide range of multimodal datasets and identifies several trends, such as larger data scales, more modalities, and complex tasks. It integrates these into a coherent narrative about the evolution of multimodal big data. However, critical analysis is limited to a brief mention of challenges, and abstraction provides some general observations but stops short of forming a meta-level framework."}}
{"id": "af1c8fea-49d0-4414-b29b-01d4ab51cdd8", "title": "{Transformers", "level": "section", "subsections": ["7cb80339-995d-4e4d-b955-76baae247a0f", "27e5d64b-d25c-4141-9328-b27fa5837922", "b140ace7-aa64-4e9e-a5a9-29d3cb6942b7"], "parent_id": "21139f68-9b71-4857-a410-85c84d8234d4", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "{Transformers"]], "content": "}\n\\label{sec:multi-modal-transformer}\nIn this section, we use mathematical formulations to review the key techniques of \\vani{} Transformer , \nVision Transformer , and multimodal Transformers \\footnote{In this survey, ``multimodal Transformer'' means ``Transformer in multimodal learning context''.}, including tokenized inputs, self-attention, multi-head attention, basic Transformer layers/blocks, \\etc.\nWe highlight that \\vani{} Transformers can be understood from a geometrically topological perspective , because due to the self-attention mechanism, given each tokenized input from any modalities, \\vani{} self-attention (Transformer) can model it as a fully-connected graph in topological geometry space .\nCompared with other deep networks (for instance, CNN is restricted in the aligned grid spaces/matrices),\nTransformers intrinsically have a more general and flexible modelling space.\nThis is a notable  advantage of Transformers for multimodal tasks.\nSections \\ref{sec:vanilla-transformer}, \\ref{sec:vit}, and \\ref{sec:transformer-in-multimodal-context} will review the key designs of \\vani{} Transformer, Vision Transformer, and multimodal Transformers, respectively.", "cites": [732, 5272, 5273, 38], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key Transformer concepts from multiple papers, especially by framing the Transformer as a graph-based model and comparing it to CNNs. It provides a geometrically topological abstraction that ties together Vision Transformers and graph-based generalizations, offering a novel perspective. However, it lacks deeper critical analysis of limitations or trade-offs in the cited approaches."}}
{"id": "7cb80339-995d-4e4d-b955-76baae247a0f", "title": "\\vani{", "level": "subsection", "subsections": ["57521590-9cef-456e-9029-fb345788adf9", "44ba2e1d-5109-4df5-9aef-f3c3c66af41d", "048cfeab-9b3f-464f-9b5a-62792d05f734"], "parent_id": "af1c8fea-49d0-4414-b29b-01d4ab51cdd8", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "{Transformers"], ["subsection", "\\vani{"]], "content": "Transformer}\n\\label{sec:vanilla-transformer}\n\\input{tex/transformer}\n\\vani{} Transformer has an encoder-decoder structure and is the origin of the  Transformer-based research field.\nIt takes tokenized input (see Section \\ref{sec:tokenized-input}).\nBoth its encoder and decoder are stacked by the Transformer layers/blocks,\nas demonstrated in Figure \\ref{fig:transformer}.\nEach block has two sub-layers, \\ie, a multi-head self-attention (MHSA) layer (see Section \\ref{sec:self-attention-and-multi-head-attention}) and a position-wise fully-connected feed-forward network (FFN) (see Section \\ref{sec:ffn}).\nTo help the back propagation of the gradient, both MHSA and FFN use Residual Connection  (given an input $x$, the residual connection of any mapping $f(\\cdot)$ is defined as $x \\gets f(x) + x$), followed by normalization layer.\nThus, assuming that the input tensor is $\\mathbf{Z}$,  the output of MHSA and FFN sub-layers can be formulated as:\n\\begin{equation}\n\\label{eq:mhsa-and-ffn}\n    \\mathbf{Z} \\gets N ( {sublayer} (\\mathbf{Z}) + \\mathbf{Z}),\n\\end{equation}\nwhere ${sublayer}(\\cdot)$ is the mapping  implemented by the sub-layer\nitself and $N(\\cdot)$ denotes normalization, \\eg, $BN(\\cdot)$ , $LN(\\cdot)$ .\n\\keypoint{Discussion}\n\\magenta{There is an important unsolved problem that is post-normalization versus pre-normalization.} \nThe original \\vani{} Transformer uses post-normalization for each MHSA and FFN sub-layer.\nHowever, if we consider this from the mathematical perspective, pre-normalization makes more sense {.} \n{This is similar to the basic principle of the theory of matrix, that normalization should be performed before projection, \\eg, Gramâ€“Schmidt process \\footnote{\\url{https://en.wikipedia.org/wiki/Gram\\%E2\\%80\\%93Schmidt_process}}.}\nThis problem should be studied further by both theoretical research and experimental validation.", "cites": [97, 1511, 57, 71], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a basic explanation of the vanilla Transformer's structure and introduces the normalization debate from a mathematical analogy. It integrates concepts from multiple papers (residual connections, batch and layer normalization) but only loosely connects them. The critical analysis is present in identifying an unresolved issue (post-normalization vs. pre-normalization) and suggesting directions for further study, though deeper comparative evaluation is missing. The abstraction is modest, drawing a general analogy to matrix theory but not offering a broader theoretical framework."}}
{"id": "57521590-9cef-456e-9029-fb345788adf9", "title": "Input Tokenization", "level": "subsubsection", "subsections": [], "parent_id": "7cb80339-995d-4e4d-b955-76baae247a0f", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "{Transformers"], ["subsection", "\\vani{"], ["subsubsection", "Input Tokenization"]], "content": "\\label{sec:tokenized-input}\n\\keypoint{{Tokenization}}\n\\vani{} Transformer was originally proposed for machine translation as a sequence-to-sequence model, thus it is straightforward to take the vocabulary sequences as input.\nAs mentioned previously, the original self-attention can model an arbitrary input as a  fully-connected graph, independently of modalities.\nSpecifically, both \\vani{} and variant Transformers take in the tokenized sequences, where each token can be regarded as a node of the graph.\n\\keypoint{{Special/Customized Tokens}}\n{In Transformers,\nvarious special/customized tokens can be semantically defined as place-holders in the token sequences,\n\\eg, mask token \\texttt{[MASK]} .\nSome common special tokens are summarized in appendix. Special tokens can be used in both uni-modal and multimodal Transformers.}\n\\keypoint{{Position Embedding}}\n{  Position embeddings are added\nto the token embeddings to retain positional information .\n\\vani{} Transformer uses sine and cosine functions to produce position embedding.\nTo date, various implementations of position embedding have been proposed.\nThe concrete solutions are outside  the focus of this survey.\n}\n\\keypoint{Discussion}\n{The main advantages of input tokenization include the following:} \n{\\textbf{(1)}} Tokenization is a more general approach from a geometrically topological perspective, achieved by minimizing constraints caused by different modalities. \n{In general, every modality has intrinsic constraints on modelling.\nFor instance, sentences have  sequential structures that are well-suited by RNN, and photos are restricted in aligned grid matrices that CNN works well for.\nTokenization helps Transformers  inherently to process different modalities universally via irregular sparse structures.  Thus even \\vani{} Transformer can encode multimodal inputs flexibly by just concatenation, weighted summation, even without any multimodal tailor-made modifications. }\n{\\textbf{(2)}} Tokenization is a more flexible approach to organize the input information via concatenation/stack, weighted summation, \\etc.\n\\vani{} Transformer injects temporal information to the token embedding by summing position embedding.\nFor instance, when use Transformer to model free-hand sketch drawing , each input token can integrate various drawing stroke patterns, \\eg, stroke coordinates, stroke ordering, pen state (start/end). \n{\\textbf{(3)}} {Tokenization is compatible with the task-specific customized tokens, \\eg,  \\texttt{[MASK]} token  for Masked Language Modelling, \\texttt{[CLASS]} token  for classification.} \n\\keypoint{Discussion} \n\\magenta{How to understand position embedding to Transformers is an open problem.}\nIt can be understood as a kind of implicit coordinate basis of feature space, to provide  temporal or spatial information to the Transformer.\nFor cloud point  and sketch drawing stroke , their token element is already a coordinate, meaning that position embedding is optional, not necessary. \nFurthermore, position embedding can be regarded as a kind of general additional information.\nIn other words, from  a mathematical point of view, any additional information can be added, such as detail of the manner of position embedding, \\eg,\nthe pen state of sketch drawing stroke ,\ncameras and viewpoints in surveillance .\nThere is a comprehensive survey  discussing the position information in Transformers.\nFor both sentence structures (sequential) and general graph structures (sparse, arbitrary, and irregular),\nposition embeddings help Transformers to learn or encode the underlying structures.\nConsidered from the mathematical perspective of self-attention, \\ie, scaled\ndot-product attention,\nattentions are invariant to the positions of words (in text) or nodes (in graphs), if  position embedding information is missing.\n\\blue{Thus, in most cases, position embedding is necessary for Transformers.}", "cites": [7853, 5274, 5275, 732, 7, 4826], "cite_extract_rate": 1.0, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.2, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes tokenization strategies from multiple papers (e.g., BERT for [MASK], Vision Transformer for images, and the Multi-Graph Transformer for sketches) to present a unified view of how Transformers process multimodal data. It abstracts the concept of tokenization and position embedding into a geometrically topological perspective, highlighting their modality-agnostic nature and flexibility. While it offers a critical perspective by pointing to the optional nature of position embeddings in certain modalities, it could more explicitly contrast different approaches or highlight limitations in the cited works."}}
{"id": "44ba2e1d-5109-4df5-9aef-f3c3c66af41d", "title": "Self-Attention and Multi-Head Self-Attention", "level": "subsubsection", "subsections": [], "parent_id": "7cb80339-995d-4e4d-b955-76baae247a0f", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "{Transformers"], ["subsection", "\\vani{"], ["subsubsection", "Self-Attention and Multi-Head Self-Attention"]], "content": "\\label{sec:self-attention-and-multi-head-attention}\nThe core component of \\vani{} Transformer is the Self-Attention (SA) operation  that is also termed  ``Scaled Dot-Product Attention''.\nAssume  that\n$\\mathbf{X} = [\\mathbf{x}_1, \\mathbf{x}_2, \\cdots] \\in \\mathbb{R}^{N \\times d}$ is an input sequence of $N$ elements/tokens,\nand an optional preprocessing is positional encoding by point-wise summation $\\mathbf{Z} \\gets \\mathbf{X} \\oplus Position Embedding$ or concatenation $\\mathbf{Z} \\gets concat( \\mathbf{X}, Position Embedding)$.\n\\keypoint{Self-Attention (SA)}\nAfter preprocessing, embedding $\\mathbf{Z}$ will\ngo through three projection matrices ($\\mathbf{W}^{Q} \\in \\mathbb{R}^{d \\times d_q}$, $\\mathbf{W}^{K} \\in \\mathbb{R}^{d \\times d_k}$, and $\\mathbf{W}^{V} \\in \\mathbb{R}^{d \\times d_v}$, $d_q = d_k$) to generate three embeddings $\\mathbf{Q}$ (Query), $\\mathbf{K}$ (Key), and $\\mathbf{V}$ (Value):\n\\begin{equation}\n\\label{eq:projecting}\n    \\mathbf{Q} = \\mathbf{Z} \\mathbf{W}^{Q}, \\mathbf{K} = \\mathbf{Z} \\mathbf{W}^{K}, \\mathbf{V} = \\mathbf{Z} \\mathbf{W}^{V}.\n\\end{equation}\nThe output of self-attention is defined as\n\\begin{equation}\n\\label{eq:attention}\n    \\mathbf{Z}  = {SA} (\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = {Softmax}\\left( \\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_q}} \\right) \\mathbf{V}.\n\\end{equation}\nGiven an input sequence, self-attention allows each element to attend to all the other elements, so that  self-attention encodes the input as a fully-connected graph. Therefore, the encoder of \\vani{} Transformer can be regarded as a fully-connected GNN encoder, \nand the Transformer family has the non-local ability of global perception, similar to the Non-Local Network .\n\\keypoint{Masked Self-Attention (MSA)}\nIn practice, modification \nof  self-attention is needed to help the decoder of Transformer to learn  contextual dependence, to prevent positions from attending to subsequent positions, as\n\\begin{equation}\n\\label{eq:masked-attention}\n    \\mathbf{Z} = {MSA} (\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = {Softmax}\\left( \\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_q}} \\odot \\mathbf{M} \\right) \\mathbf{V},\n\\end{equation}\nwhere $\\mathbf{M}$ is a masking matrix.\nFor instance, in GPT , an upper triangular mask to enable look-ahead attention\nwhere each token can only look at the past tokens.\nMasking can be used in both encoder  and decoder of Transformer, and has flexible implementations, \\eg, 0-1 hard mask , soft mask .\nIn both uni-modal and multimodal practices,\nspecific masks are designed based on domain knowledge and prior knowledge.\nEssentially, MSA is used to inject additional knowledge to Transformer models, \\eg, . \n\\keypoint{Multi-Head Self-Attention (MHSA)}\nIn practice, multiple self-attention sub-layers can be stacked in parallel and their concatenated outputs are fused by a projection matrix $\\mathbf{W}$, to form a structure named Multi-Head Self-Attention:\n\\begin{equation}\n\\label{eq:multihead-attention}\n    \\mathbf{Z} = {MHSA} (\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = {concat} (\\mathbf{Z}_{1}, \\cdots, \\mathbf{Z}_{H}) \\textbf{W},\n\\end{equation}\nwhere each head $\\mathbf{Z}_{h} = {SA} (\\mathbf{Q}_{h}, \\mathbf{K}_{h} \\mathbf{V}_{h})$ and $h \\in [1, H]$,\nand $\\textbf{W}$ is a linear projection matrix.\nThe idea of MHSA is a kind of ensemble.\nMHSA helps the model to jointly attend to information from multiple representation\nsub-spaces.", "cites": [5275, 4764, 38, 5276, 5277, 5230], "cite_extract_rate": 0.75, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a clear and factual description of self-attention and multi-head self-attention mechanisms in the vanilla Transformer. However, it primarily summarizes concepts and equations without critically evaluating or comparing the cited papers. There is minimal synthesis of ideas from the cited works to form a broader narrative or framework."}}
{"id": "048cfeab-9b3f-464f-9b5a-62792d05f734", "title": "Feed-Forward Network", "level": "subsubsection", "subsections": [], "parent_id": "7cb80339-995d-4e4d-b955-76baae247a0f", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "{Transformers"], ["subsection", "\\vani{"], ["subsubsection", "Feed-Forward Network"]], "content": "\\label{sec:ffn}\nThe output of the multi-head attention sub-layer will go through the position-wise Feed-Forward Network (FFN) that consists of successive linear layers with non-linear activation. \nFor instance, a two-layer FFN can be formulated as\n\\begin{equation}\n\\label{eq:attention}\n    FFN (\\mathbf{Z}) =  \\sigma (  \\mathbf{Z} \\mathbf{W}_{1} + \\mathbf{b}_1) \\mathbf{W}_{2} + \\mathbf{b}_{2},\n\\end{equation}\nwhere $\\mathbf{W}_{1}$, $\\mathbf{b}_{1}$, $\\mathbf{W}_{2}$, and $\\mathbf{b}_{2}$ denote the weights and biases of the two linear transformations, while $\\sigma(\\cdot)$ is non-linear activation, \\eg, $\\text{ReLU}(\\cdot)$ , $GELU (\\cdot)$ . In some Transformer literature, FFN is also termed  Multi-Layer Perceptron (MLP).", "cites": [1512], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the Feed-Forward Network in Transformers and cites one paper for the GELU activation function. However, it does not synthesize multiple sources or connect broader ideas. There is minimal critical analysis or abstraction, focusing instead on a straightforward presentation of definitions and formulas."}}
{"id": "27e5d64b-d25c-4141-9328-b27fa5837922", "title": "Vision Transformer", "level": "subsection", "subsections": [], "parent_id": "af1c8fea-49d0-4414-b29b-01d4ab51cdd8", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "{Transformers"], ["subsection", "Vision Transformer"]], "content": "\\label{sec:vit}\n{Vision Transformer (ViT) } \n{has an image-specific input pipeline in which the input image must be split into fixed-size (\\eg, $16 \\times 16$, $32 \\times 32$) patches.}\nAfter going through the linearly embedded layer and adding the position embeddings, all the patch-wise sequences will be encoded by a standard Transformer encoder.\nGiven an image $\\mathbf{X} \\in \\mathbb{R}^{H \\times W \\times C}$ ($H$ height, $W$ width, $C$ channels), ViT needs to reshape $\\mathbf{X}$ into a sequence of flattened 2D patches: $\\mathbf{x}_{p} \\in \\mathbb{R}^\\mathbf{N \\times (P^{2} \\cdot C)}$, where $(P \\times P)$ is the patch resolution and $N = HW/P^2$.\nTo perform classification, a standard approach is to\nprepend an extra learnable embedding ``classification token'' \\texttt{[CLASS]} to the sequence of embedded patches:\n\\begin{equation}\n\\label{eq:attention}\n    \\textbf{Z} \\gets concat(\\texttt{[CLASS]}, \\mathbf{X} \\mathbf{W}),\n\\end{equation}\nwhere $\\mathbf{W}$ denotes the projection.\n\\input{tables/multi-modal-inputs}", "cites": [732], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of the Vision Transformer (ViT) architecture, primarily paraphrasing the core components and methodology from the cited paper. It outlines the input pipeline, embedding process, and classification token usage but lacks deeper synthesis across sources or critical evaluation of the approach. Some general terms like 'patch-wise sequences' and 'Transformer encoder' are mentioned, but the section does not abstract broader principles or trends in multimodal Transformer design."}}
{"id": "561ec406-3a38-4acd-838c-2a2cd4760ec0", "title": "\\MD{", "level": "subsubsection", "subsections": [], "parent_id": "b140ace7-aa64-4e9e-a5a9-29d3cb6942b7", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "{Transformers"], ["subsection", "Multimodal Transformers"], ["subsubsection", "\\MD{"]], "content": "Input}\n\\label{sec:multimodal-input}\nThe Transformer family is a general architecture that can be formulated as a type of general graph neural network.\nSpecifically, self-attention can process each input as a fully-connected graph, by attending to the global (non-local) patterns.\nTherefore, this intrinsic trait helps Transformers can work in a  modality\nagnostic pipeline that is compatible with various modalities by treating the embedding of each token as a node of the graph.\n\\keypoint{{Tokenization and Embedding Processing}}\nGiven an input from an arbitrary modality, \nusers only need to perform two main steps, (1) tokenize the input, and (2) select an embedding space to represent the tokens, before inputting the data into Transformers.\nIn practice, both the tokenizing input and selecting embedding for the token are vital for Transformers but highly flexible, with many alternatives.\nFor instance, given an image, the solution of tokenizing and embedding is not unique.\nUsers can choose or design tokenization at multiple granularity levels -- coarse-grained vs. fine-grained.\n\\eg, use ROIs (obtained by an object detector) and CNN features as tokens and token embeddings , use patches and linear projection as tokens and token embeddings , or use graph node (obtained by object detector and graph generator) and GNN features as tokens and token embeddings .\nGiven a tokenization plan, the subsequent embedding approaches can be diverse.\nFor example, for video input, a common  tokenization is to treat the non-overlapping\nwindows (down-sampled) over the video as tokens, and  their embeddings can then be extracted by various 3D CNNs, \\eg, \nVideoBERT , CBT , and UniVL  use S3D ,\nActBERT uses ResNet-3D .\nTable~\\ref{table:multi-modal-input} summarizes some common  practices of multimodal inputs for Transformers, including\n RGB,\n video,\n audio/speech/music,\n text,\n graph, \\etc.\n\\keypoint{Discussion}\nWhen considered from the perspective of geometric topology,\neach of the modalities listed in Table~\\ref{table:multi-modal-input} can be regarded as a graph. \nAn RGB image is essentially a neat grid graph in the pixel space.\nBoth video and audio are clip/segment based graphs over a complex space involving  temporal and semantic patterns.\nBoth 2D and 3D drawing sketches  are a kind of sparse graph if we consider their key points along the drawing strokes.\nSimilar to sketches, the human pose also is a kind of graph.\n3D point cloud is a graph in which each coordinate is a node.\n\\magenta{Other abstract modalities also can be interpreted as graphs, \\eg, \nsource code ,\ndata flow of source code ,\ntable ,\nSQL database schema ,\ntext question graph , and\nelectronic health records (EHRs) .}\n\\keypoint{Token Embedding Fusion}\nIn practice, \nTransformers allow each token position to contain multiple embeddings.\nThis is essentially a kind of early-fusion of embeddings, for both uni-modal and multimodal Transformer models. (This will be discussed further in subsequent sections.) \nThe most common fusion is the token-wise summing of the multiple embeddings, \\eg, a specific token embedding $\\oplus$ position embedding.\nSimilar to the flexible tokenization, token embedding fusion is also flexible and widely applied to both uni-modal and multimodal Transformer applications.\nIn , token-wise weighted summing is used to perform early-fusion of RGB and grey-scale images for multimodal surveillance AI.\nIn particular, token embedding fusion has an important role in multimodal Transformer applications as various embeddings can be fused by token-wise operators, \\eg,\nin VisualBERT  and Unicoder-VL , segment embeddings are token-wise added to indicate which modality (vision or language) each token is from,\nVL-BERT  injects global visual context to linguistic domain by ``linguistic token embedding $\\oplus$ full image visual feature\nembedding'',\nInterBERT  adds location information for ROI by ``ROI embedding $\\oplus$ location embedding'',\nin ImageBERT , five kinds of embeddings are fused ``image embedding $\\oplus$ position embedding $\\oplus$ linguistic embedding $\\oplus$ segment embedding $\\oplus$ sequence position embedding''.\n\\input{tables/fusion-comparision}", "cites": [2023, 5224, 2015, 8886, 7535, 5230, 1278, 8888, 1272, 5226, 9131, 5275, 768, 5212, 5278, 732], "cite_extract_rate": 0.7619047619047619, "origin_cites_number": 21, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section synthesizes diverse multimodal inputs into a coherent framework by interpreting them as graphs, linking ideas from multiple papers. It critically evaluates tokenization and embedding fusion strategies, highlighting their flexibility and importance in multimodal Transformers. The abstraction is strong, particularly through the geometric-topological perspective that generalizes across modalities and provides high-level insights into multimodal learning design."}}
{"id": "09a69d19-bfd1-4ce6-924b-308217b49548", "title": "Self-Attention Variants in \\MD{", "level": "subsubsection", "subsections": [], "parent_id": "b140ace7-aa64-4e9e-a5a9-29d3cb6942b7", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "{Transformers"], ["subsection", "Multimodal Transformers"], ["subsubsection", "Self-Attention Variants in \\MD{"]], "content": "Context}\n\\label{sec:self-attention-in-multimodal-context}\n{In multimodal Transformers,\ncross-modal interactions (\\eg, fusion, alignment) are essentially processed by self-attention and its variants. \nThus, in this section, we will review the main multimodal modelling practices of Transformers, from a perspective of self-attention designs, including\n(1) early summation (token-wise, weighted),\n(2) early concatenation,\n(3) hierarchical attention (multi-stream to one-stream),\n(4) hierarchical attention (one-stream to multi-stream),\n(5) cross-attention, and\n(6) cross-attention to concatenation.} See Table \\ref{table:fusion-comparison} and Figure \\ref{fig:self-attention-fusion}.\nFor brevity, we will state and compare the mathematical formulations in two-modality cases. Please note that all discussed self-attention and its variants are such flexible that can be extended to multiple modality cases.\n{Specifically, the following formulations are modality-, tokenization-, and embedding- agnostic, as self-attention models the embedding of arbitrary token from arbitrary modality as a node of a graph.}   \nGiven inputs  $\\mathbf{X}_\\texttt{A}$ and $\\mathbf{X}_\\texttt{B}$ from two arbitrary modalities,\n$\\mathbf{Z}_{(\\texttt{A})}$ and $\\mathbf{Z}_{(\\texttt{B})}$ denote their respective token embeddings.\nLet $\\mathbf{Z}$ denoting the token embedding (sequence) produced by the multimodal interactions.\n$Tf(\\cdot)$ stands for the processing of Transformer layers/blocks.\n\\begin{figure*}[!t]\n\t\\centering\n\t\\subfigure[]{\n\t\t\\label{fig:fusion-1}\n\t\t\\includegraphics[width=0.08\\textwidth]{./my-figures/fusion-1.pdf}}\n\t\\subfigure[]{\n\t\t\\label{fig:fusion-2}\n\t\t\\includegraphics[width=0.17\\textwidth]{./my-figures/fusion-2.pdf}}\n\t\\subfigure[]{\n\t\t\\label{fig:fusion-3}\n\t\t\\includegraphics[width=0.17\\textwidth]{./my-figures/fusion-3.pdf}}\n\t\\subfigure[]{\n\t\t\\label{fig:fusion-4}\n\t\t\\includegraphics[width=0.17\\textwidth]{./my-figures/fusion-4.pdf}}\n\t\\subfigure[]{\n\t\t\\label{fig:fusion-5}\n\t\t\\includegraphics[width=0.17\\textwidth]{./my-figures/fusion-5.pdf}}\n\t\\subfigure[]{\n\t\t\\label{fig:fusion-6}\n\t\t\\includegraphics[width=0.17\\textwidth]{./my-figures/fusion-6.pdf}}\n\t\\caption{Transformer-based cross-modal interactions\\blue{: (a) Early Summation, (b) Early Concatenation, (c) Hierarchical Attention (multi-stream to one-stream), (d) Hierarchical Attention (one-stream to multi-stream), (e) Cross-Attention, and (f) Cross-Attention to Concatenation}.  ``Q'': Query embedding; ``K'': Key embedding; ``V'': Value embedding. ``TL'': Transformer Layer. Best viewed in colour. \n }\n\t\\label{fig:self-attention-fusion}\n\\end{figure*}\n\\keypoint{(1) Early Summation}\nIn practice, early summation  is a simple and effective multimodal interaction, where the token embeddings from multiple modalities can be weighted summed at each token position and then processed by Transformer layers:\n\\begin{equation}\n\\label{eq:early-summation}\n   \\mathbf{Z} \\gets Tf(\\alpha \\mathbf{Z}_{(\\texttt{A})} \\oplus \\beta \\mathbf{Z}_{(\\texttt{B})}) = {MHSA} (\\mathbf{Q}_{(\\texttt{AB})}, \\mathbf{K}_{(\\texttt{AB})}, \\mathbf{V}_{(\\texttt{AB})}), \n\\end{equation}\nwhere $\\oplus$ is element-wise sum, and $\\alpha$ and $\\beta$ are weightings.\nConcretely, $\\mathbf{Q}_{(\\texttt{AB})} = (\\alpha \\mathbf{Z}_{(\\texttt{A})} \\oplus \\beta \\mathbf{Z}_{(\\texttt{B})}) \\mathbf{W}^{Q}_{(\\texttt{AB})}$, $\\mathbf{K}_{(\\texttt{AB})} = (\\alpha \\mathbf{Z}_{(\\texttt{A})} \\oplus \\beta \\mathbf{Z}_{(\\texttt{B})}) \\mathbf{W}^{K}_{(\\texttt{AB})}$, and $\\mathbf{V}_{(\\texttt{AB})} = (\\alpha \\mathbf{Z}_{(\\texttt{A})} \\oplus \\beta \\mathbf{Z}_{(\\texttt{B})}) \\mathbf{W}^{V}_{(\\texttt{AB})}$.\n{Its main advantage is that it does not increase computational complexity.\nHowever, its main disadvantage is due to the manually set weightings.} \nAs discussed in Section \\ref{sec:tokenized-input} and \\ref{sec:multimodal-input}, summing position embedding is intrinsically a case of early summation.\n\\keypoint{(2) Early Concatenation}\nAnother straightforward solution is early concatenation  that the token embedding sequences from multiple modalities are concatenated and input into Transformer layers as\n\\begin{equation}\n\\label{eq:early-concatenation}\n    \\mathbf{Z} \\gets Tf(\\mathcal{C}(\\mathbf{Z}_{(\\texttt{A})}, \\mathbf{Z}_{(\\texttt{B})})).\n\\end{equation}\n{Thus, all the multimodal token positions can be attended as a whole sequence, such that the positions of each modality can be encoded well by conditioning the context of other modalities.}\nVideoBERT  is the one of the first multimodal Transformer works, where video and text are fused via early concatenation {that can encode the global multimodal context well .\nHowever, the longer sequence after concatenation will increase computational complexity.}\nEarly concatenation is also termed  ``all-attention'' or ``Co-Transformer'' .\n\\keypoint{(3) Hierarchical Attention} (multi-stream to one-stream)\nTransformer layers can be combined hierarchically to attend to the cross-modal interactions.\nA common practice is that multimodal inputs are encoded by independent Transformer streams and their outputs are concatenated and fused by another Transformer :\n\\begin{equation}\n\\label{eq:hierarchical-attention-2-to-1}\n    \\mathbf{Z} \\gets Tf_3(\\mathcal{C}(Tf_1(\\mathbf{Z}_{(\\texttt{A})}), Tf_2(\\mathbf{Z}_{(\\texttt{B})}))).\n\\end{equation}\nThis kind of hierarchical attention is an implementation of late interaction/fusion, and  can be treated as a special case of early concatenation.\n\\keypoint{(4) Hierarchical Attention} (one-stream to multi-stream)\nInterBERT  is another good practice of hierarchical attention where concatenated multimodal inputs are encoded by a shared single-stream Transformer that is followed by two separate Transformer streams.\nThis flow can be formulated as\n\\begin{equation}\n\\label{eq:hierarchical-attention-1-to-2}\n    \\left\\{ \n    \\begin{aligned}\n        \\blue{\\mathcal{C}(}\\mathbf{Z}_{(\\texttt{A})}, \\mathbf{Z}_{(\\texttt{B})}\\blue{)} & \\gets Tf_{1}(\\mathcal{C}(\\mathbf{Z}_{(\\texttt{A})}, \\mathbf{Z}_{(\\texttt{B})})), \\\\\n        \\mathbf{Z}_{(\\texttt{A})} & \\gets Tf_{2}(\\mathbf{Z}_{(\\texttt{A})}), \\\\\n        \\mathbf{Z}_{(\\texttt{B})} & \\gets Tf_{3}(\\mathbf{Z}_{(\\texttt{B})}).\n    \\end{aligned}\n    \\right.\n\\end{equation}\n{This method perceives the cross-modal interactions and meanwhile preserves the independence of uni-modal representation.}\n\\keypoint{(5) Cross-Attention}\nFor two-stream Transformers, if the $\\mathbf{Q}$ (Query) embeddings are exchanged/swapped in a cross-stream manner, the cross-modal interactions can also be perceived.\nThis method is termed cross-attention or co-attention , which was first proposed in VilBERT :\n\\begin{equation}\n\\label{eq:cross-attention}\n    \\left\\{ \n    \\begin{aligned}\n        \\mathbf{Z}_{(\\texttt{A})} & \\gets {MHSA} (\\mathbf{Q}_{\\texttt{B}}, \\mathbf{K}_{\\texttt{A}}, \\mathbf{V}_{\\texttt{A}}), \\\\\n        \\mathbf{Z}_{(\\texttt{B})} & \\gets {MHSA} (\\mathbf{Q}_{\\texttt{A}}, \\mathbf{K}_{\\texttt{B}}, \\mathbf{V}_{\\texttt{B}}).\n    \\end{aligned}\n    \\right.\n\\end{equation}\n{Cross-attention attends to each modality\nconditioned on the other and does not cause higher computational complexity, however if considered for each modality, this method fails to perform  cross-modal attention globally and thus loses the whole context.\nAs discussed in , \ntwo-stream cross-attention can learn cross-modal interaction, whereas there is no self-attention to the self-context inside each modality.}\n\\keypoint{(6) Cross-Attention to Concatenation}\nThe two streams of cross-attention  can be further concatenated and processed by another Transformer to model the global context.\nThis kind of hierarchically cross-modal interaction is also widely studied\n, and {alleviates the drawback of cross-attention.}\n\\begin{equation}\n\\label{eq:cross-attention-to-concatenation}\n    \\left\\{ \n    \\begin{aligned}\n        \\mathbf{Z}_{(\\texttt{A})} & \\gets {MHSA} (\\mathbf{Q}_{\\texttt{B}}, \\mathbf{K}_{\\texttt{A}}, \\mathbf{V}_{\\texttt{A}}), \\\\\n        \\mathbf{Z}_{(\\texttt{B})} & \\gets {MHSA} (\\mathbf{Q}_{\\texttt{A}}, \\mathbf{K}_{\\texttt{B}}, \\mathbf{V}_{\\texttt{B}}), \\\\\n        \\mathbf{Z} & \\gets Tf(\\mathcal{C}(\\mathbf{Z}_{(\\texttt{A})}, \\mathbf{Z}_{(\\texttt{B})})).\n    \\end{aligned}\n    \\right.\n\\end{equation}\n\\keypoint{Discussion}\n{All these aforementioned self-attention variants for multimodal interactions are modality-generic, and can be applied in flexible strategies and for multi-granular tasks.\nSpecifically, these interactions can be flexibly combined and nested.}\nFor instance, multiple cross-attention streams are used in hierarchical attention (one-stream to multi-stream) that in a two-stream decoupled model  $Tf_2$ and $Tf_3$ of Eq. \\ref{eq:hierarchical-attention-1-to-2} are implemented by cross-attention defined in Eq. \\ref{eq:cross-attention}. \n{Moreover, they can be extended to multiple ($\\geq 3$) modalities.}\nTriBERT  is a tri-modal cross-attention (co-attention) for vision, pose, and audio, where given a Query embedding, its Key and Value embeddings are the concatenation from the other modalities. Cross-attention to concatenation is applied to three modalities (\\ie, language,\nvideo, and audio) in .", "cites": [5252, 5281, 5224, 5278, 4832, 5280, 9131, 768, 5269, 5282, 1278, 5283, 5279], "cite_extract_rate": 0.9285714285714286, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple multimodal Transformer variants and connects them through a coherent framework centered on self-attention. It critically evaluates each method's pros and cons (e.g., computational complexity, global context modeling). The abstraction level is strong as it generalizes modality-agnostic principles and identifies broader architectural patterns."}}
{"id": "763237b6-0d0e-4651-970e-cd2e510ab10f", "title": "Network Architectures", "level": "subsubsection", "subsections": [], "parent_id": "b140ace7-aa64-4e9e-a5a9-29d3cb6942b7", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "{Transformers"], ["subsection", "Multimodal Transformers"], ["subsubsection", "Network Architectures"]], "content": "\\label{sec:architectures}\nEssentially,\nvarious multimodal Transformers work due to their internal multimodal attentions that are \nthe aforementioned self-attention variants.\nMeanwhile, as illustrated in Figure \\ref{fig:self-attention-fusion}, these attentions determine the external network structures of the multimodal Transformers where they are embedded.  \nIn general, if we consider from the angle of network structures, (1) early summation and early concatenation work in single-stream, (2) cross-attention work in multi-streams, (3) hierarchical attention and  cross-attention to concatenation work in hybrid-streams.\nThus,\nmultimodal Transformers can be divided into\nsingle-stream (\\eg, Uniter , Visualbert ,  Vl-bert  , Unified VLP ), multi-stream (\\eg, ViLBERT , Lxmert ,  ActBERT ),  hybrid-stream (\\eg, InterBERT ), \\etc.\nFrom the perspective of timing of interaction, these multimodal attentions fall into three categories, \\ie, early interaction: early summation, early concatenation, and hierarchical attention (one-stream to multi-stream), late interaction: hierarchical attention (multi-stream to one-stream), or throughout interaction: cross-attention,  cross-attention to concatenation.\nAs demonstrated in Figure 2 in , the multimodal Transformer models have another architecture taxonomy based on the computational size of the components.", "cites": [5284, 7909, 2012, 5278, 1278, 1279, 1272], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers by organizing multimodal Transformers based on architectural paradigms (single-stream, multi-stream, hybrid-stream) and interaction timing, which provides a coherent and structured narrative. It abstracts key patterns in how attention mechanisms influence model design. However, the critical analysis is limited, as it does not deeply evaluate the limitations or trade-offs of these architectures."}}
{"id": "5b8dde6d-0134-49ff-8114-3677f36acfcc", "title": "Transformers for Multimodal Pretraining", "level": "subsection", "subsections": ["fe91bb70-88c5-4914-8596-78a4c61b8b55", "01084df1-deec-4ba7-8763-5e3a45eeaee5"], "parent_id": "5f41fbc9-6717-4317-8a00-37d207d6aced", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "{Application Scenarios"], ["subsection", "Transformers for Multimodal Pretraining"]], "content": "\\label{sec:transformers-for-multi-modal-pretraining}\nInspired by the great success of Transformer based pretraining in NLP community,\nTransformers are also widely studied for multimodal pretraining as the various large-scale multimodal corpora is emerging.\nRecent work has demonstrated that if pretrained on large scale multimodal corpora Transformer based models  clearly outperform other competitors in a wide range of multimodal down-stream tasks, and moreover achieve the zero-shot generalization ability. \nThese superiorities have led Transformer-based multimodal pretraining to become a hot topic, which \nhas two main directions, \\ie, general pretraining for agnostic down-stream tasks (Section \\ref{sec:task-agnostic-multi-modal-pretraining}), goal-oriented pretraining for specific down-stream tasks (Section \\ref{sec:task-specific-multi-modal-pretraining}).\nWe focus on these key points: \n(1) What trends are emerging? \n(2) Where/how do the cross-modal  interactions take place during pretraining? \n(3) How to sort out and understand the pretraining pretext objectives? \nHow can they drive Transformers to learn the cross-modal interactions?", "cites": [2012, 1279, 768, 1278, 1272], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by grouping the cited papers under two main pretraining directions (task-agnostic and task-specific), and connecting them through a shared focus on cross-modal interactions. It also abstracts from specific models to highlight overarching questions and trends in multimodal pretraining. While it does mention key features of each paper, the critical evaluation is moderate, focusing more on summarizing their contributions than deeply comparing or critiquing their limitations."}}
{"id": "fe91bb70-88c5-4914-8596-78a4c61b8b55", "title": "Task-Agnostic Multimodal Pretraining", "level": "subsubsection", "subsections": [], "parent_id": "5b8dde6d-0134-49ff-8114-3677f36acfcc", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "{Application Scenarios"], ["subsection", "Transformers for Multimodal Pretraining"], ["subsubsection", "Task-Agnostic Multimodal Pretraining"]], "content": "\\label{sec:task-agnostic-multi-modal-pretraining}\nRecently Transformer-oriented pretraining has been widely studied involving diverse modality combinations, \\eg, video-text , image-text , acoustic-text .\nAmong existing work,\nthe following main trends are emerging: \n{\\textbf{(1)}} {Vision-language pretraining (VLP) is a major research problem in this field.} VLP is including both ``image + language'' and ``video + language'', also termed  visual-linguistic pretraining. A great deal of excellent work has been proposed, \\eg, \nVideoBERT ,\nViLBERT ,\nLXMERT ,\nVisualBERT ,\nVL-BERT ,\nUNITER ,\nCBT ,\nUnicoder-VL ,\nB2T2 ,\nVLP ,\n12-in-1 ,\nOscar ,\nPixel-BERT ,\nActBERT ,\nImageBERT ,\nHERO ,\nUniVL , SemVLP . \n{\\textbf{(2)}} {Speech can be used as text.} Thanks to  recent advances in  automatic speech recognition (ASR) techniques, in a multimodal context, speech can be converted to text by the\n off-the-shelf speech\nrecognition tools. \nFor instance, VideoBERT  and CBT  make full use of speech rather than low-level sounds as a source of cross-modal supervision, by extracting  high-level semantic text.\n{\\textbf{(3)}} {Overly dependent on the well-aligned multimodal data.} A majority of Transformer-based multimodal pretraining works in a self-supervised manner, however, it is overly dependent on the well-aligned multimodal sample pairs/tuples.\nFor instance, large amount of image-language pretraining Transformer models are pretrained on large-scale image-text pairs, \\eg,\nVisualBERT , VL-BERT ,  ViLBERT ,\nLXMERT , UNITER .\nFor another example,\nthe instructional videos (\\eg, cooking) \\footnote{{Note that instructional videos also have weakly aligned cases .}} are widely used as the pretraining corpora, \\eg,\nHowToVQA69M ,\nHowTo100M ,\nas {in general,}\ntheir visual clues/content and the spoken words have a higher probability to align with each other, {if compared with other videos.} \nHowever, using  cross-modal alignment as cross-modal supervision is costly for large-scale applications.\nThus, how to use the weakly-aligned or even unpaired/unaligned  multimodal data as the pretraining corpora is still understudied. Some recent attempts  study the use of weakly-aligned cross-modal supervision  to train Transformers to learn the cross-modal interactions. \n{\\textbf{(4)}} \n{Most of the existing pretext tasks transfer well across modalities.}\nFor instance, Masked Language Modelling (MLM) in the text domain has been applied to audio and image, \\eg,  Masked Acoustic Modelling , Masked Image Region Prediction ,\nwhile both Sentence Ordering Modelling (SOM)  in text domain and Frame Ordering Modelling (FOM)  in video domain share the same idea.  \nWe will further discuss the pretext tasks for multimodal Transformer pretraining in the follows.\n{\\textbf{(5)}} {Model structures are mainly in three categories.} Essentially, in multimodal pretraining scenarios, Transformer models work based on those self-attention variants that are discussed in Section \\ref{sec:self-attention-in-multimodal-context}. Thus, if considered from the perspective of model structures, the existing Transformers for multimodal pretraining are also mainly in three categories, \\ie, single-stream, multi-stream, hybrid-stream. \n{\\textbf{(6)}}\n{Cross-modal interactions can perform within various components/levels in the pretraining pipelines.}\nFor  Transformer based multimodal pretraining, the key is to drive the Transformer (encoder w/, w/o decoder) to learn the cross-modal interactions.\nIn the existing Transformer-based multimodal pretraining practices, the cross-modal interactions are flexible, which can perform within various components/levels in the pretraining pipelines.\nIn general, Transformer-based multimodal pretraining pipelines have three key components, from bottom to top, \\ie, tokenization, Transformer representation, objective supervision. \nFor not only the multimodal pretraining but also the specific multimodal tasks,\nthe cross-modal interactions can perform within arbitrary component(s) of the three.\nAs discussed in Section \\ref{sec:self-attention-in-multimodal-context},\nbecause self-attention models the embedding of an arbitrary token from an arbitrary modality as a node of a graph,\n the existing pretraining pipelines\ncan, in general, be transferred independently across\nmodalities,\nunless considered with modality-specific objectives.\n\\keypoint{Discussion} Vision Language Pretraining (VLP) follows two general pipelines: two-stage (need object detector, \\eg, Faster R-CNN ) (\\eg, LXMERT , ViLBert , VL-Bert , UNITER ) and end-to-end (\\eg, Pixel-Bert , SOHO , KD-VLP , Simvlm ). \nTwo-stage pipelines have a main advantage -- object-aware perceiving, by using the supervised pre-trained visual detectors, \nhowever these are based on a strong assumption that the visual representations can be fixed.\n\\keypoint{Discussion}\nHow to look for more corpora that intrinsically have well-aligned cross-modal supervision, such as instructional videos, is still an open problem.\nHowever,\nweakly-aligned cross-modal samples are popular in the real-life scenarios, for instance, enormous weakly aligned multimodal\ndata samples are emerging in e-commerce , due to fine-grained categories, complex combinations, and fuzzy correspondence.\nWell labelled/aligned cross-modal datasets are very costly in collecting and annotating;  \nhow to use weakly-aligned or even unaligned corpora crawled from the web is a promising question. \nSome recently successful practice \nused weakly aligned image-text pairs\nto perform pretraining, and achieve both competitive performance and zero-shot learning capability for image classification, image-text retrieval, and open-ended visual question answering, \\etc.\nBecause these practices in weak supervision  make full use of large-scale pretraining corpora, they yield greater promise of zero-shot generalization.\n\\keypoint{Pretext Tasks}\nIn Transformer based multimodal pretraining,\nthe pretraining tasks/objectives are also termed  pretext tasks/objectives.\nTo date, various pretext tasks have been studied, \\eg, masked language modelling (MLM) ,\nmasked image region prediction/{classification} (also termed  masked object classification (MOC)) ,\nmasked region regression (MRR) ,\nvisual-linguistic matching (VLM) (\\eg, imageâ€“text\nmatching (ITM) , image text matching (ITM), phrase-region alignment (PRA) , word-region alignment (WRA) , video-subtitle matching (VSM) ),\nmasked frame modelling (MFM) ,\nframe order modelling (FOM) ,\nnext sentence prediction (NSP) ,\nmasked sentence generation (MSG) ,\nmasked group modelling (MGM) , \nprefix language modelling (PrefixLM) ,\nvideo conditioned masked\nlanguage model ,\ntext conditioned masked frame\nmodel ,\nvisual translation language modelling\n(VTLM) , and\nimage-conditioned masked language modelling (also termed  image-attended masked language modelling) .\n These down-stream task -agnostic pretext pretraining is optional,\nand the down-stream task objectives can be trained directly,\nwhich will be discussed in Section~\\ref{sec:task-specific-multi-modal-pretraining}.\n{Table~\\ref{table:pretext-tasks} provides the common and representative pretext tasks for Transformer based multimodal pretraining.}\n\\blue{In practice, pretext tasks can be combined, and some representative cases are summarized in Table 3 of , Table 2 of .} \n\\input{tables/pretext-tasks}\nThe pretext tasks have multiple taxonomies: \n{\\textbf{(1)}} {Supervision.}\n{The common multimodal pretraining Transformers use well-aligned,  weakly-aligned, and even unaligned multimodal sample pairs/tuples, to work in supervised, weakly-supervised, and unsupervised manners, respectively.}\nMeanwhile, \nif we consider the definitions of their pretext tasks/objectives from supervision, the pretexts can be sorted into unsupervised/self-supervised (\\eg, masked language modelling (MLM) )  and supervised (\\eg, image-text matching (ITM)     ), \\etc.\nNowadays, self-supervised attempts are the majority. \n{\\textbf{(2)}} {Modality.} Considering the mathematical formulations, some pretexts are defined on single modality, \\eg, masked language modelling , masked acoustic modelling , masked region regression (MRR) ,\nwhile other pretexts are defined on multiple modalities, \\eg, image-conditioned masked language modelling (IMLM) , image-text matching (ITM) , video-subtitle matching (VSM) .\nThus, from this mathematical view, the pretext tasks can be  divided into two categories, \\ie, uni-modal and multimodal.  \nHowever, this classification is not really accurate.\nIt should be highlighted that in multimodal pretraining Transformer models, even if the pretext objective formulations only include uni-modal elements, pretexts can still involve other modalities, essentially conditioned on the clues from other modalities, by (a) prepositive token level interactions and/or Transformer level interactions, \n(b) co-training with other pretexts that involve other modalities. \nFor instance, VL-BERT  uses two dual pretext tasks, \\ie, masked language modelling and masked RoI classification.\n{\\textbf{(3)}} {Motivation.} If consider their motivations, the pretext tasks include masking, describing, matching, ordering, \\etc.\nSome recent surveys focus on VLP and compare the existing VLP Transformer models from the angles of domain (image-text or video-text), vision feature extraction, language feature extraction, architecture (single- or dual- stream), decoder (w/, w/o), pretext tasks/objectives, pretraining datasets, and down-stream tasks, \\eg, Table 3 of , Table 2 of .\nDifferent from these views,\nin this survey,\nwe would propose our comparisons from some new perspectives.\nSpecifically:\n{\\textbf{(1)}}\nThe core of Transformer ecosystem is self-attention, thus we would compare the existing multimodal pretraining Transformer models from the angles of how and when the self-attention or its variants perform cross-modal interactions. \n{\\textbf{(2)}} Considering from a geometrically topological perspective, self-attention helps Transformers intrinsically \nwork in a modality agnostic pipeline that is compatible\nwith various modalities by taking in the embedding of each\ntoken as a node of graph, thus we would highlight that the existing VLP can be applied to other modalities, beyond visual and linguistic domains. \n{\\textbf{(3)}} \nWe suggest to treat the Transformer-based multimodal pretraining pipelines having three key components, from bottom to top, \\ie, tokenization, Transformer representation, objective supervision. \n\\keypoint{Discussion}\nIn spite of the recent advances,\nmultimodal pretraining Transformer methods still have some obvious bottlenecks.\n\\magenta{For instance,\nas discussed by  in VLP field,\nwhile the BERT-style cross-modal pretraining models produce excellent results on various down-stream vision-language tasks, they fail to be applied to generative tasks directly.}\nAs discussed in ,\nboth VideoBERT  and CBT  have to train a separate video-to-text decoder for video captioning.\nThis is a significant gap between the pretraining models designed for discriminative and generative tasks, as the main reason is discriminative task oriented pretraining models do not involve the decoders of Transformer.\nTherefore, how to design more unified pipelines that can work for both discriminative and generative down-stream tasks is also an open problem to be solved.\nAgain for instance,\ncommon multimodal pretraining models often underperform for fine-grained/instance-level tasks as discussed by .\n\\keypoint{Discussion}\n\\magenta{As discussed in ,\nthe masked language\nand region modelling as pre-training task have a main advantage that the Transformer encoder learned from these supervisions can encode both vision and language patterns based on bidirectional context and it is naturally fit for the semantic understanding tasks, \\eg, VQA, image-text retrieval.}\n\\keypoint{Discussion}\nHow to boost the performance for multimodal pretraining Transformers is an open problem.\nSome practices demonstrate that \nmulti-task training (by adding auxiliary loss)  and adversarial training  improve multimodal pretraining Transformers to further boost the performance. \nMeanwhile, overly compound\npretraining objectives potentially upgrade the challenge of balancing among different loss terms, thus complicate the training optimization .\nMoreover, \nthe difficulty of the pretexts is also worth discussing.\nIn general, if aim to learn more explicit object concepts, more complex pretext losses will be used . However, for pretexts, whether more complexity is better remains a question.", "cites": [5252, 7905, 1275, 7080, 7913, 2018, 5285, 7912, 2011, 2012, 2023, 5283, 5287, 5289, 2422, 2017, 2015, 8886, 5266, 5290, 7535, 1278, 5281, 1272, 5286, 5288, 5292, 768, 7906, 5282, 7, 7121, 2022, 7339, 5293, 5278, 1639, 209, 5260, 5244, 5291, 7909, 1279], "cite_extract_rate": 0.9347826086956522, "origin_cites_number": 46, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers to highlight trends in multimodal pretraining, particularly in the use of weakly-aligned data and pretext tasks. It critically examines limitations, such as over-reliance on well-aligned corpora, and abstracts these into broader patterns like model structure categories and cross-modal interaction levels."}}
{"id": "01084df1-deec-4ba7-8763-5e3a45eeaee5", "title": "Task-Specific Multimodal Pretraining", "level": "subsubsection", "subsections": [], "parent_id": "5b8dde6d-0134-49ff-8114-3677f36acfcc", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "{Application Scenarios"], ["subsection", "Transformers for Multimodal Pretraining"], ["subsubsection", "Task-Specific Multimodal Pretraining"]], "content": "\\label{sec:task-specific-multi-modal-pretraining}\nIn practices of multimodal Transformers,\nthe aforementioned down-stream task -agnostic pretraining is optional, not necessary,\nand down-stream task specific pretraining is also widely studied .\nThe main reasons include:\n(1) Limited by the existing  technique, it is extremely difficult to design a set of highly universal network architectures, pretext tasks, and corpora that work for all the various down-stream applications.\n(2) There are non-negligible gaps among various down-stream applications, \\eg, task logic, data form, making it difficult to transfer from pretraining to down-stream applications.\nTherefore,\na large number of down-stream tasks still need tailor-made pretraining to improve the performance.\nGuhur \\etal  propose in-domain pretraining for vision-and-language navigation, as \nthe general VLP\nfocuses on learning vision-language correlations, not designed for sequential decision making as required\nin embodied VLN.\nMurahari \\etal  present a visual dialogue oriented approach to leverage pretraining on general\nvision-language datasets. \nXGPT  is tailor-made for image captioning, to overcome the limitation that BERT-based cross-modal pre-trained models fail to be applied\nto generative tasks directly.\nERNIE-ViLG  is designed for bidirectional image-text generation with Transformers.\nSpecial modalities have their own unique domain knowledge that can be used to design the specific pretrain pretexts.\nGraphCodeBERT  uses\ntwo structure-aware pretext tasks (\\ie, predict where a variable is identified from, data flow edge\nprediction between variables) for programming source code.\nTo learn from the spatial cues in $360^{\\circ}$ video,\nMorgado \\etal  propose to perform contrastive audio-visual spatial alignment of $360^{\\circ}$ video and spatial audio.\nMed-BERT  is a contextualized embedding model pretrained on a structured electronic health record dataset of two million patients. \nKaleido-BERT  is a VLP Transformer model tailor-made for the fashion domain.", "cites": [5267, 5224, 5294, 5259, 5282, 8889], "cite_extract_rate": 0.75, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a list of task-specific pretraining approaches and their applications, citing relevant papers but without deep integration or a unifying framework. It makes minimal effort to connect the cited works into a broader narrative, and the critical analysis is limitedâ€”papers are described in terms of their goals and methods, but their relative strengths, weaknesses, or implications are not thoroughly evaluated. Some generalization is attempted (e.g., mention of domain-specific pretext tasks), but it remains shallow."}}
{"id": "95c9bf40-869d-42e1-9367-1698471f2615", "title": "Transformers for Specific Multimodal Tasks", "level": "subsection", "subsections": [], "parent_id": "5f41fbc9-6717-4317-8a00-37d207d6aced", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "{Application Scenarios"], ["subsection", "Transformers for Specific Multimodal Tasks"]], "content": "\\label{sec:transformers-for-specific-multi-modal tasks}\nRecent work has demonstrated that Transformer models can encode various multimodal inputs in both classical and novel discriminative applications, \\eg, RGB \\& optical flow , \n{RGB \\& depth ,\nRGB \\& point \\blue{cloud} ,\nRGB \\& LiDAR ,}\ntextual description \\& point cloud , acoustic \\& text , audio \\& visual observation for Audio-Visual Navigation ,\nspeech query \\& schema of SQL database ,\ntext question/query \\& the schema SQL database ,\naudio \\& tags ,\nmultimodal representation for video  ,\ntext query \\& video ,\naudio \\& video for audio visual speech enhancement (AVSE) ,\naudio \\& video for Audio-Visual Video Parsing ,\naudio \\& video for audio-visual speech recognition ,\nvideo \\& text for Referring Video Object Segmentation (RVOS) ,\nsource code \\& comment \\& data flow ,\nimage \\& text for retrieval .\nMeanwhile, Transformers also contribute to various multimodal generative tasks, including single-modality to single-modality (\\eg, raw audio to 3D mesh sequence ,\nRGB to 3D scene ,\nsingle image to 3D human texture estimation ,\nRGB to scene graph ,\ngraph to graph ,\nknowledge graph to text ,\nvideo to scene graph ,\nvideo to caption ,\nimage to caption  ,\ntext to speech ,\ntext to image ,\ntext to shape ,\nRGB to 3D human pose and mesh ,\nmusic to dance ),\nmultimodality to single modality (\\eg, \nimage \\& text to scene graph ,\nVideo Dialogue (text \\& audio \\& visual to text) ,\nMono Audio \\& Depth to Binaural Audio ,\nmusic piece \\& seed 3D motion to long-range future 3D motions  ,\nX-raying image \\& question to answer ,\nvideo \\& text \\& audio to text ),\nand multimodality to multimodality (\\eg, ).", "cites": [7361, 5312, 5314, 5296, 5297, 5303, 1515, 5283, 5298, 5216, 7914, 5307, 5295, 5224, 8890, 4832, 5306, 5304, 1274, 5313, 5254, 5269, 4769, 5311, 5230, 5232, 5218, 5315, 5226, 5310, 5316, 5308, 5229, 5300, 5302, 5236, 7339, 8615, 5305, 5309, 3016, 5299, 5301], "cite_extract_rate": 0.8269230769230769, "origin_cites_number": 52, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a broad list of specific multimodal tasks where Transformers have been applied, supported by relevant citations. However, it synthesizes little beyond grouping these tasks by modality combinations, lacks critical evaluation of the cited works' methodologies or limitations, and offers minimal abstraction or generalization of patterns across the applications."}}
{"id": "7fd39549-23f3-4dac-b3b1-996a2a33f076", "title": "Challenges and Designs", "level": "section", "subsections": ["d09231d3-5fdd-4151-9975-ded41a5290e2", "f7fc30fc-556d-4d35-9a46-031944e81028", "bad9d95a-67dc-4e7c-8d54-dbd6cc1d9fb2", "114808c7-9385-47a0-aeb9-0420bcf760c2", "2c73f720-1e86-467b-8745-426df8275bae", "4a41ea0c-3835-4244-80e6-f7158103a48e", "67c2f447-557d-4017-98b2-a7cf956f3b77"], "parent_id": "21139f68-9b71-4857-a410-85c84d8234d4", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "Challenges and Designs"]], "content": "\\label{sec:challenges-and-designs}\nComplementing the application {scenario taxonomy} discussed in Section \\ref{sec:applications-and-representative-models},\nwe further survey prior work from the perspective of technical challenges.\nWe discuss seven challenges of Transformer based multimodal learning, including fusion, alignment, transferability,\nefficiency,\nrobustness,\nuniversalness, and interpretability.\nThis further extends the taxonomy introduced in  to tackle the higher diversity and wider scopes of \nexisting Transformer based MML works in recent years.", "cites": [1030], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a structured analytical view by identifying seven key challenges in Transformer-based multimodal learning. It integrates the taxonomy from a prior survey to contextualize its discussion, showing some synthesis of ideas. However, it lacks detailed comparisons or critical evaluation of the cited works, focusing more on categorization than on deep analysis. The abstraction level is moderate, as it generalizes these challenges to a broader scope of MML research."}}
{"id": "d09231d3-5fdd-4151-9975-ded41a5290e2", "title": "Fusion", "level": "subsection", "subsections": [], "parent_id": "7fd39549-23f3-4dac-b3b1-996a2a33f076", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "Challenges and Designs"], ["subsection", "Fusion"]], "content": "In general, MML Transformers fuse  information across \nmultiple modalities primarily at three levels:\ninput (\\ie, early fusion), intermediate representation (\\ie, middle fusion), and prediction (\\ie, late fusion).\nCommon early fusion based MML Transformer models \n are also known as {one-stream architecture},\nallowing the adoption of the merits of BERT\ndue to minimal architectural modification.\nThe main difference between  these one-stream models\nis the usage of problem-specific modalities with variant masking techniques.\nWith attention operation, a noticeable fusion scheme\nis introduced based on a notion of bottleneck tokens .\nIt applies for both early and middle fusion\nby simply choosing to-be-fused layers.\nWe note that the simple prediction-based late fusion  is less adopted \nin MML Transformers.\nThis makes sense considering the motivations of learning stronger multimodal contextual representations and great advance of computing power. \nFor enhancing and interpreting the fusion of MML,\nprobing the interaction and measuring the fusion between modalities \nwould be an interesting direction to explore.", "cites": [2015, 768, 8891, 5317, 1272], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates key concepts from multiple papers to outline fusion levels in MML Transformers, showing a reasonable synthesis. It introduces an analytical perspective by discussing the adoption trends of fusion methods and highlights the potential for probing and measuring fusion. However, it lacks in-depth comparative or critical evaluation of the cited works and does not fully abstract to overarching principles."}}
{"id": "f7fc30fc-556d-4d35-9a46-031944e81028", "title": "Alignment", "level": "subsection", "subsections": [], "parent_id": "7fd39549-23f3-4dac-b3b1-996a2a33f076", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "Challenges and Designs"], ["subsection", "Alignment"]], "content": "Cross-modal alignment is the key to \na number of real-world multimodal applications.\nTransformer based cross-modal alignment has been studied for various tasks, \\eg,\nspeaker localization in multi-speaker videos ,\nspeech translation ,\ntext-to-speech alignment ,\ntext-to-video retrieval ,\nand visual grounding of natural language .\nRecently, Transformer based alignment\n\nhas led to a surge of leveraging large quantities of web data (\\eg, image-text pairs) for vision and language tasks.\nA representative practice is to map two modalities into a common representation space with contrastive learning over paired samples.\nThe models based on this idea are often enormous in size\nand expensive to optimize from millions or billions of training data.\nConsequently, successive works mostly exploit pretrained models\nfor tackling various down-stream tasks .\nThese alignment models\nhave the ability of zero-shot transfer\nparticularly for image classification via \\blue{prompt engineering~}.\nThis novel perspective is mind-blowing, given that image classification is conventionally regarded as a unimodal learning problem\nand zero-shot classification remains an unsolved challenge despite extensive research .\nThis has been  studied\nfor more challenging and fine-grained tasks (\\eg, object detection , visual question answering , and instance retrieval )\nby \nimposing region (semantic parts such as objects) level alignment.\nFine-grained alignment will however incur more  computational costs\nfrom explicit region detection \nand how to eliminate this whilst keeping the region-level learning capability becomes a challenge. \nSeveral ideas introduced recently include\nrandom sampling ,\nlearning concept dictionary ,\nuniform masking ,\npatch projection ,\njoint learning of a region detector , and\nrepresentation aligning before mask prediction .", "cites": [7915, 8892, 1275, 5318, 5329, 5324, 2012, 5283, 2017, 5284, 5321, 5330, 1273, 5325, 5328, 5292, 5322, 5247, 5323, 5326, 5327, 5320, 7916, 7917, 5319, 1639, 6983, 5331], "cite_extract_rate": 0.8484848484848485, "origin_cites_number": 33, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers to form a coherent narrative around cross-modal alignment in multimodal Transformers, emphasizing contrastive learning, representation spaces, and fine-grained alignment challenges. It critically highlights limitations like computational costs and the dependency on region detection. Furthermore, it abstracts key trends and strategies (e.g., sparse sampling, patch projection, prompt engineering) into broader design principles in multimodal alignment."}}
{"id": "bad9d95a-67dc-4e7c-8d54-dbd6cc1d9fb2", "title": "Transferability", "level": "subsection", "subsections": [], "parent_id": "7fd39549-23f3-4dac-b3b1-996a2a33f076", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "Challenges and Designs"], ["subsection", "Transferability"]], "content": "\\label{sec:transferability}\nTransferability is a major challenge for Transformer based multimodal learning, involving the question of how to transfer models across different datasets and applications.\nData augmentation and adversarial perturbation strategies help multimodal Transformers to improve the generalization ability. VILLA  is a two-stage  strategy (task-agnostic adversarial pretraining, followed by task-specific adversarial finetuning) that improves VLP Transformers.\nIn practice,\nthe distribution gap between training data and practical data is noticeable.\nFor instance,\nsupervised data samples (well-labelled, well-aligned) are costly in practical applications, thus\nhow to transfer the supervised multimodal Transformers pretrained on well-aligned cross-modal pairs/tuples to the weakly aligned test bed is challenging .  \nCLIP  is an inspiring solution that transfers knowledge across modalities by learning a shared multimodal embedding space, enabling zero-shot transfer\nof the model to down-stream tasks.\nThe main inspiration that CLIP presents the community is that the pretrained multimodal (image and text) knowledge can be transferred to down-stream zero-shot image prediction by using a prompt template ``\\texttt{A photo of a \\{label\\}.}'' to bridge the distribution gap between training and test datasets.\nOver-fitting is a major obstacle to transfer. Multimodal Transformers can be overly fitted to the dataset\nbiases during training, due to the large modelling capability.\nSome recent practices exploit how to transfer the oracle model trained on noiseless dataset to real dataset.\nFor instance, \nKervadec \\etal  explore \nhow transferable reasoning patterns are in VQA,\nand demonstrate that for LXMERT /BERT-like\nreasoning patterns can be partially transferred from an ideal dataset to a real dataset.\nCross-task gap is another major obstacle to transfer , due to the different reasoning and input-output workflows, \\eg,\nhow to use multimodal datasets to finetune the language pretrained model is difficult .\nIn real applications, \nmultimodal pretrained Transformers sometimes need to handle the uni-modal data at inference stage due to the issue of missing modalities. One solution is using knowledge\ndistillation, \\eg, distilling\nfrom multimodal to uni-modal attention in Transformers , distilling from multiple uni-modal Transformer teachers to a shared Transformer encoder .\nThere is a huge gap across discriminative and generative multimodal tasks.\nAs discussed in ,\nthe BERT-like encoder-only multimodal Transformers (\\eg, VideoBERT , CBT ) need\nseparately to train decoders for generation tasks. This could create a pretrain-finetune discrepancy detrimental to the\ngenerality.\nRecently, more and more attempts study this issue further, \\eg,\nGilBERT  is a generative VLP models for\na discriminative task, \\ie, image-text retrieval.\nCross-lingual gap also should be considered for the transferability of Transformer based multimodal learning, \\eg,\nuniversal cross-lingual generalization from English to non-English multimodal contexts .", "cites": [5252, 5333, 5332, 5334, 1639, 5335, 8893, 2018, 768, 7912, 2023, 2012, 8383], "cite_extract_rate": 0.8666666666666667, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers to form a coherent narrative on transferability challenges in multimodal Transformers, integrating concepts like adversarial training (VILLA), zero-shot transfer (CLIP), and knowledge distillation (e.g., UC2). It critically analyzes issues such as over-fitting, cross-task and cross-lingual gaps, and pretrain-finetune discrepancies. The abstraction level is strong as it identifies overarching principles like the need for shared embedding spaces and universal representations."}}
{"id": "114808c7-9385-47a0-aeb9-0420bcf760c2", "title": "Efficiency", "level": "subsection", "subsections": [], "parent_id": "7fd39549-23f3-4dac-b3b1-996a2a33f076", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "Challenges and Designs"], ["subsection", "Efficiency"]], "content": "\\label{sec:efficiency}\nMultimodal Transformers suffer from two major efficiency issues:\n(1) Due to the large model parameter capacity, they are data hungry and thus dependent  on huge scale training datasets.\n(2) They are limited by the time and memory complexities that grow quadratically with the input sequence length, which are caused by the self-attention.\nIn multimodal contexts, \ncalculation explosion will become worse due to jointly high dimension representations.\nThese two bottlenecks are interdependent and should be considered together.\nTo improve the training and/or inferring efficiency for multimodal Transformers,\n recent efforts have attempted to find various solutions, to use fewer training data and/or parameters.\nThe main ideas can be summarized as the follows. \n{\\textbf{(1)}} {Knowledge distillation.}\nDistill the knowledge from the trained larger Transformers to smaller Transformers .\nMiech \\etal  conduct distillation from a slower model (early concatenation based Transformers, $\\mathcal{O}((N_{(\\texttt{A})} + N_{(\\texttt{B})})^{2})$) to a faster one (independently dual branch Transformers, $\\mathcal{O}(N_{(\\texttt{A})}^{2})$).\n{\\textbf{(2)}} {Simplifying and compressing model.}\nRemove the components to simplify the pipelines.\nTaking the VLP Transformer models as an example,\ntwo-stage pipeline is costly as they need object detector.\nOne simplifying is processing the visual input in convolution-free manner, \\eg, E2E-VLP ,  ViLT  . \nDropToken   reduces\nthe training complexity via\nrandom dropping a portion of the video and audio tokens from input sequence during training.\nDropToken can be treated as an implementation of dropout or adversarial training.\nWeight-sharing is also a common practice for simplifying multimodal Transformer models.\nWen \\etal  present a weight-sharing Transformer on top of the visual and\ntextual encoders to align text and image.\nLee \\etal  propose a novel parameter sharing scheme based on low-rank approximation.\n{\\textbf{(3)}} {Asymmetrical network structures.}\nAssign different model capacities and computational size properly for different modalities, to save parameters. See Figure 2 in .\n{\\textbf{(4)}} {Improving utilization of training samples.} \nLiu \\etal  train a simplified LXMERT by making full use of fewer samples at different granularities.\nLi \\etal  use fewer data to train CLIP by  fully mining the  potential self-supervised signals of (a) self-supervision within each modality, (b) multi-view supervision across modalities, and (c) nearest-neighbour  supervision from other similar pairs.\n{\\textbf{(5)}} {Compressing and pruning model.}\nSearch the optimal sub-structures/sub-networks of multimodal Transformers, \\eg,\nplaying Lottery Tickets with the VLP Transformer models , adaptively freezing some layers during training .\n{\\textbf{(6)}} {Optimizing the complexity of self-attention.}\nTransformers cost time and memory that grows quadratically with the input sequence length~.\nOne potential solution is optimizing the $\\mathcal{O}(N^{2})$ complexity, \\eg,\nChild \\etal  present sparse factorizations of the attention matrix to reduce the quadratical complexity to $\\mathcal{O}(n \\sqrt{n})$,\nTransformer-LS  is an efficient Transformer for both language and vision long sequence, with linear computational and memory complexity.\n{\\textbf{(7)}} {Optimizing the complexity of self-attention based multimodal interaction/fusion.}\nNagrani \\etal   propose Fusion via Attention Bottlenecks  (FSN, fusion bottleneck) to improve the early concatenation based multimodal interaction.\nFSN passes on the messages through a small number of bottleneck \nlatents, thus requiring the model to purify the most necessary information\nfrom each modality for cross-modal sharing.\nThis strategy uses the fusion bottleneck as a bridge, and not only\nimproves fusion performance, but also reduces computational cost.\n{\\textbf{(8)}} {Optimizing other strategies.}\nUse optimal strategies to perform the common Transformer based multimodal interactions.\nGiven the quadratic complexity of self-attention,\nusing early concatenation based multimodal interaction to synchronously fuse the inputs from multiple modalities/views is costly.\nYan \\etal  present an efficient solution that sequentially fuses information between all pairs of\ntwo adjacent views in ascending order of sequence length.\nThis is intrinsically a greedy strategy.", "cites": [5284, 5341, 5339, 4735, 5342, 793, 5336, 5338, 5337, 8891, 5340, 7590, 5320, 7562], "cite_extract_rate": 0.875, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes a range of papers effectively, grouping them into coherent categories such as knowledge distillation, model compression, and attention optimization. It provides critical analysis by discussing the interdependencies of efficiency challenges and the trade-offs between accuracy and computational cost. The abstraction level is strong as it identifies broader strategies for improving Transformer efficiency in multimodal settings beyond individual papers."}}
{"id": "2c73f720-1e86-467b-8745-426df8275bae", "title": "Robustness", "level": "subsection", "subsections": [], "parent_id": "7fd39549-23f3-4dac-b3b1-996a2a33f076", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "Challenges and Designs"], ["subsection", "Robustness"]], "content": "\\label{sec:robustness}\nMultimodal Transformers pretrained on large-scale corpora achieve the state-of-the-art for various multimodal applications, while their robustness is still unclear and understudied.\nThis at least involves two key challenges, \\ie, how to theoretically analyse the robustness, how to improve the robustness.   \nAlthough that recent attempts  study and evaluate how the Transformer components/sub-layers contribute to the robustness, \nthe main bottleneck is that the community lacks theoretical tools to analyse the Transformer family. \nRecently, the common practices to analyse robustness are mainly based on experiment evaluations , \\eg, cross-dataset evaluations, perturbation-based evaluations.\nThus, some multimodal datasets    are proposed for evaluating the robustness.\nRecent attempts mainly use two straightforward methods to improve the robustness for multimodal Transformer models: (1) augmentation and adversarial learning based strategies ,\n(2) fine-grained loss functions .\nFor instance:\nVILLA   is a generic adversarial training framework that can be applied to various multimodal Transformers.\nAkula \\etal  empirically demonstrate that\nViLBERT fails to exploit linguistic structure, and they propose two methods to improve the robustness of ViLBERT, one based on contrastive learning and the other based on multi-task learning.", "cites": [4844, 5344, 5345, 5343, 5346, 8894, 2018, 5261, 5249], "cite_extract_rate": 0.8181818181818182, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of robustness in multimodal Transformers by identifying key challenges and summarizing methods used to address them, including adversarial training and fine-grained loss functions. It integrates multiple papers to highlight trends in evaluation methods and robustness improvements, but the synthesis remains somewhat fragmented and lacks a deeper, unifying framework. The critical perspective is present but limited, as the section mainly notes the lack of theoretical tools rather than offering nuanced comparisons or critiques of specific approaches."}}
{"id": "4a41ea0c-3835-4244-80e6-f7158103a48e", "title": "Universalness", "level": "subsection", "subsections": [], "parent_id": "7fd39549-23f3-4dac-b3b1-996a2a33f076", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "Challenges and Designs"], ["subsection", "Universalness"]], "content": "\\label{sec:universalness}\nDue to the highly diversity of tasks and modalities of multimodal learning, universalness is an important problem for multimodal Transformer models.\nA large amount of recent attempts  study how to use as unified as possible pipelines to handle various modalities and multimodal tasks.\nIdeally, the unified multimodal Transformers can be compatible with various data (\\eg,\naligned and unaligned, uni-modal and multimodal) and tasks (\\eg, supervised and unsupervised, uni-modal and multimodal, discriminative and generative), and meanwhile have either few-shot or even zero-shot generalization ability. Thus, the current solutions for universalness goal for multimodal Transformers are preliminary probes.\nThe currently unifying-oriented attempts mainly include: \n{\\textbf{(1)}} {Unifying the pipelines for both uni-modal and multimodal inputs/tasks.} \nAs discussed Section \\ref{sec:transferability}, in practical scenarios,\nmultimodal Transformers need to\nhandle  uni-modal data due to the issue of missing modalities.\nDistilling multimodal knowledge into  small models that are adaptable to uni-modal data and tasks is a successful practice .\n{\\textbf{(2)}} {Unifying the pipelines for both  multimodal understanding and generation.}\nIn general, for multimodal Transformer pipelines,\nunderstanding and discriminative tasks require Transformer encoders only, while generation/generative tasks require both Transformer encoders and decoders.\nExisting attempts use multi-task learning to combine the understanding and generation workflows, where two kinds of workflows are jointly trained by multi-task loss functions.\nFrom the perspective of model structures, typical solutions include:\n(a) encoder + decoder, \\eg, E2E-VLP .\n(b) separate encoders + cross encoder + decoder, \\eg, UniVL , CBT .\n(c) single unified/combined encoder-decoder, \\eg, VLP .\n(d) \\blue{two-stream} decoupled design . \n{\\textbf{(3)}} {Unifying and converting the tasks themselves}, \\eg, CLIP  converts zero-shot recognition to retrieval, thus reduces the costs of modifying the model.\nHowever, the aforementioned practices suffer some obvious challenges and  bottlenecks, at least including: \n{\\textbf{(1)}} Due to modality and task gaps, universal models should consider the trade-off between universalness and cost. Unifying the pipelines of different modalities and tasks generally cause larger or more complicated model configuration, whereas for a specific modality or task, some components are redundant. \n{\\textbf{(2)}} Multi-task loss functions increase the complexity of training. How to co-train multiple objectives properly  and effectively is challenging, due to that different objectives generally should be optimized in different strategies.", "cites": [5333, 1639, 8886, 8893, 2023, 5281, 5320, 1279], "cite_extract_rate": 1.0, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers to build a coherent narrative on the universalness of multimodal Transformers, categorizing them into design strategies. It provides critical analysis by highlighting challenges such as trade-offs between universalness and cost, and the complexity of multi-task learning. The abstraction level is strong, as it generalizes these efforts into broader design principles and identifies key limitations affecting the field."}}
{"id": "67c2f447-557d-4017-98b2-a7cf956f3b77", "title": "Interpretability", "level": "subsection", "subsections": [], "parent_id": "7fd39549-23f3-4dac-b3b1-996a2a33f076", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "Challenges and Designs"], ["subsection", "Interpretability"]], "content": "\\label{sec:interpretability}\nWhy and how  Transformers perform so well in multimodal learning has been investigated .\nThese attempts mainly use probing task and ablation study. \nCao \\etal  design a set of probing tasks on UNITER  and LXMERT , to evaluate what patterns are learned in pretraining.\nHendricks \\etal  probe the  imageâ€“language Transformers by fine-grained imageâ€“sentence pairs, and find that verb understanding is harder than subject or object understanding.\n\\magenta{Chen \\etal  examine the optimal combination of pretraining\ntasks via ablation study, to compare how different pretexts contribute to the Transformers.}\nDespite these attempts,\nthe interpretability of multimodal Transformers is still under-studied to date.", "cites": [5348, 5347, 7918, 8895, 8896, 5351, 5349, 5350, 2012], "cite_extract_rate": 0.9, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section shows some analytical effort by identifying interpretability as a challenge and highlighting methods like probing tasks and ablation studies used by different works. However, the synthesis remains basic, connecting only a few papers with similar themes without creating a novel narrative. The critical analysis is limited, with only a brief mention of the 'under-studied' nature of interpretability rather than a deeper evaluation of the cited approaches."}}
{"id": "d60d1615-9fb4-46ed-9d46-853e64b9762a", "title": "Discussion and Outlook", "level": "section", "subsections": [], "parent_id": "21139f68-9b71-4857-a410-85c84d8234d4", "prefix_titles": [["title", "Multimodal Learning with Transformers: \\\\ A Survey"], ["section", "Discussion and Outlook"]], "content": "\\label{sec:discussion-and-outlook}\nDesigning the universal MML models  to excel across all the unimodal and multimodal down-stream tasks with different characteristics simultaneously  is a non-trivial challenge.\nFor instance, two-stream architectures  are typically preferred over one-stream ones \nfor cross-modal retrieval-like tasks in efficiency, since the representation of each modality can be pre-computed beforehand and reused repeatedly. \nThat being said, how to design task-agnostic MML architectures is still an open challenge, in addition to other design choices such as pretext and objective loss functions.\nFurthermore, a clear gap remains\nbetween the state-of-the-art and this ultimate goal.\nIn general, existing multimodal Transformer models \nare superior only for specific MML tasks,\nas they are designed specifically for only a subset of specific tasks\n.\nEncouragingly, several recent studies towards { universal modality learning} in terms of modality-agnostic network design  and more task-generic architecture design  have been introduced,\nand it is hoped this will spark further investigation.\nTo that end, instead of exhaustively exploring the vast model design space, {seeking in-depth understanding and interpretation of a MML model's behaviour might be insightful for superior algorithm design}, even though the interactions and synergy across different modalities are intrinsically complex and even potentially inconsistent over tasks\n.\nFor more fine-grained MML, it is widely acknowledged that discovering the latent semantic alignments across modalities is critical.\nAn intuitive strategy is to leverage semantic parts (\\eg, objects) pre-extracted by an off-the-shelf detector for MML .\nThis, however, is not only complex and error-prone, but computationally costly .\nSeveral remedies introduced recently include\nrandom sampling ,\nlearning concept dictionary ,\njointly learning a region detector , and\nrepresentation aligning before mask prediction .\nGiven the scale of MML training data, exploring this direction\nneeds exhaustive computational costs, and\nit is supposed that industrial research teams with rich resources are more likely to afford. \n{Ideally, a favourable MML method would leave fine-grained\nsemantic alignment across modalities to emerge on its own},\nwhich is worthy of careful investigation in the future.\nAs the learning scale expands exponentially,\nthe training data become inevitably noisy and heterogeneous .\nIt has been recently shown that  properly tackling  the noise issue is useful .\nAnother related facet is training strategy,\n\\eg, how many stages of training is superior over the common one-stage policy .\nFurther, the quadratic complexity with Transformers\nbecomes more acute for multimodal data due to longer input.\nDespite extensive research on efficient variants ,\n{dedicated efficiency study for MML is still underestimated even empirically and call for more investigation}.\nIdentifying the strengths of Transformers for multimodal machine learning is a big open problem.\nThe following main points can be summarized from the literature:\n{\\textbf{(1)}} Transformers can encode implicit knowledge .\n{\\textbf{(2)}} The multi-head brings multiple modelling sub-spaces that can further enhance the expressive ability of the model.\nIdeally, multiple heads after training are good and different.\nThis is essentially a good practice of ensemble learning.\n{\\textbf{(3)}} Transformers intrinsically have a nature of global aggregation that perceives the non-local patterns. \n{\\textbf{(4)}} Thanks to the large model capacity, Transformer models handle the challenging domain gaps and shifts (\\eg, linguistic and visual) better via effective pretraining on large-scale corpora .\n{\\textbf{(5)}} Transformers can represent the inputs as graphs, which are\nintrinsically compatible with more modalities, \\eg, table and SQL.\n{\\textbf{(6)}} For modelling series and sequence patterns (\\eg, time-series), Transformers have better training and inference efficiency against RNN-based models,\nthanks to their parallel computation in training and/or inference. \nTransformers are inherently permutation invariant for processing a sequence of points, \\eg, well-suited for point cloud learning .\n{\\textbf{(7)}} Tokenization makes Transformers flexible to organize  multimodal inputs, as discussed in Section \\ref{sec:tokenized-input}.", "cites": [5252, 1275, 4862, 5285, 2012, 4826, 2422, 2017, 5321, 5294, 7302, 7535, 1272, 7920, 5328, 8896, 5292, 5322, 5352, 5323, 7919, 5320, 5345, 5293, 7916, 5256, 1639, 5211], "cite_extract_rate": 0.875, "origin_cites_number": 32, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section synthesizes insights from multiple papers to form a coherent narrative on challenges and future directions in multimodal Transformer design. It critically evaluates approaches such as two-stream vs. one-stream architectures, external object detectors, and training strategies, highlighting limitations and trade-offs. The section abstracts these findings into broader principles like universal modality learning, computational efficiency, and semantic alignment, offering a high-level understanding of the field."}}
