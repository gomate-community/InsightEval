{"id": "0b94b38e-9f41-4523-9736-29bbf66ca9e0", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "08af7ca6-1a67-48c9-9735-3113b8e18727", "prefix_titles": [["title", "Analysis of the hands in egocentric vision:\\\\ A survey"], ["section", "Introduction"]], "content": "\\label{sec:introduction}}\n\\IEEEPARstart{T}{he} hands are of primary importance for human beings, as they allow us to interact with objects and environments, communicate with other people, and perform activities of daily living (ADLs) such as eating, bathing, and dressing. It is not a surprise that in individuals with impaired or reduced hand functionality (e.g., after a stroke or cervical spinal cord injury -- cSCI) the top recovery priority is to regain the function of the hands . Given their importance, computer vision researchers have tried to analyze the hands from multiple perspectives: localizing them in the images , inferring the types of actions they are involved in , as well as enabling interactions with computers and robots . Wearable cameras (e.g., cameras mounted on the head or chest) have allowed studying the hands from a point of view (POV) that provides a first-person perspective of the world. This field of research in computer vision is known as egocentric or first-person vision (FPV). Although some studies were published as early as the 1990s , FPV gained more importance after 2012 with the emergence of smart glasses and action cameras (i.e., Google Glass and GoPro cameras). For an overview of the evolution of FPV methods, the reader is referred to the survey published by Betancourt et al. .\nEgocentric vision presents many advantages when compared with third person vision, where the camera position is usually stable and disjointed from the user. For example: the device is recording exactly what the users have in front of them; camera movement is driven by the camera-wearer’s activity and attention; hands and manipulated objects tend to appear at the center of the image and hand occlusions are minimized . These advantages made the development of novel approaches for studying the hands very appealing. However, when working in FPV, researchers must also face an important issue: the camera is not stable, but is moving with the human body. This causes fast movements and sudden illumination changes that can significantly reduce the quality of the video recordings and make it more difficult to separate the hand and objects of interest from the background.\nBetancourt et al.  clearly summarized the typical processing steps of hand-based methods in FPV. The authors proposed a unified and hierarchical framework where the lowest levels of the hierarchy concern the detection and segmentation of the hands, whereas the highest levels are related to interaction and activity recognition. Each level is devoted to a specific task and provides the results to higher levels (e.g., hand identification builds upon hand segmentation and hand detection, activity recognition builds upon the identification of interactions, etc.). Although clear and concise, this framework could not cover some of the recent developments in this field, made possible thanks to the availability of large amounts of annotated data and to the advent of deep learning . Other good surveys closely related to the topics discussed in our paper were published in the past few years . The reader should refer to the work of Del Molino et al.  for an introduction into video summarization in FPV, to the survey of Nguyen et al.  for the recognition of ADLs from egocentric vision, and to the work of Bola\\~{n}os et al.  for a review on visual lifelogging. Hand pose estimation and hand gesture recognition methods are analyzed in  and , respectively.\nIn this survey we define a comprehensive taxonomy of hand-based methods in FPV expanding the categorization proposed in  and classifying the existing literature into three macro-areas: localization, interpretation, and application. For each macro-area we identify the main sub-areas of research, presenting the most prominent approaches published in the past 10 years and discussing advantages and disadvantages of each method. Moreover, we summarize the available datasets published in this field. Our focus in defining a comprehensive taxonomy and comparing different approaches is to propose an updated and general framework of hand-based methods in FPV, highlighting the current trends and summarizing the main findings, in order to provide guidelines to researchers who want to improve and expand this field of research.\nThe remainder of the paper is organized as follows: Section 2 presents a taxonomy of hand-based methods in FPV following a novel categorization that divides these approaches into three macro-areas: localization, interpretation, and application; Section 3 describes the approaches developed for solving the localization problem; Section 4 summarizes the work focused on interpretation; Section 5 summarizes the most important applications of hand-based methods in FPV; Section 6 reviews the available datasets published so far; and, finally, Section 7 concludes with a discussion of the current trends in this field.", "cites": [4255, 4254, 4256], "cite_extract_rate": 0.1875, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple sources to present a coherent narrative about the development and challenges of hand analysis in FPV. It builds on the framework from Betancourt et al. while identifying its limitations in covering recent deep learning advancements. The section also abstracts the field into three macro-areas (localization, interpretation, application) and provides a broader context, including trends and future directions."}}
{"id": "cf1c83dd-47c4-4b1b-a55c-c4ed114cb2ed", "title": "Interpretation -- What are the hands doing?", "level": "subsection", "subsections": [], "parent_id": "6f70ffa8-b87c-41c9-a278-be2c90617e0b", "prefix_titles": [["title", "Analysis of the hands in egocentric vision:\\\\ A survey"], ["section", "Hand-based methods in FPV -- An updated framework"], ["subsection", "Interpretation -- What are the hands doing?"]], "content": "The interpretation area includes those approaches that, starting from lower level information (i.e., detection, segmentation, pose estimation, etc.), try to infer information with higher semantic content. The main sub-areas are:\n\\begin{itemize}[noitemsep, wide=0pt, leftmargin=\\dimexpr\\labelwidth + 2\\labelsep\\relax]\n    \\item \\textbf{Hand grasp analysis} -- Detection of the dominant hand postures during hand-object interactions.\n    \\item \\textbf{Hand gesture recognition} -- Classification of hand gestures, usually as input for virtual reality (VR) and augmented reality (AR) systems, as will be discussed in Section \\ref{sec:application}.\n    \\item \\textbf{Action/Interaction recognition} -- Predicting what type of action or interaction the hands are involved in. Following the taxonomy of Tekin et al. , an action is defined as a verb (e.g. “pour”), whereas an interaction as a verb-noun pair (e.g. “pour water”). This task is called \\textit{interaction detection} if the problem is reduced to a binary classification task (i.e., predicting whether or not the hands are interacting).\n    \\item \\textbf{Activity recognition} -- Identification of the activities, defined as set of temporally-consistent actions . For example, preparing a meal is an activity composed of several actions and interactions, such as cutting vegetables, pouring water, opening jars, etc.\n\\end{itemize}\nWe can qualitatively compare these sub-areas according to the two dimensions described above (i.e., amount of detail and semantic content). Hand grasp analysis and gesture recognition have lower semantic content than action/interaction recognition that, in turn has lower semantic content than activity recognition. Activity recognition, although with higher semantic content than action recognition, produces results with lower detail. This is because the information is summarized towards the upper end of the semantic content dimension.\nFollowing these considerations, we represent the localization and interpretation areas of this framework on a two-dimensional plot whose axes are the amount of detail and the semantic content (see Figure \\ref{fig_framework}).", "cites": [4257], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a structured analytical overview of the interpretation sub-areas in FPV, organizing them based on semantic content and detail. It synthesizes the cited paper's framework (e.g., Tekin et al.) and the H+O method to create a coherent narrative. However, the critical analysis is somewhat limited, with only a qualitative comparison and no in-depth evaluation of limitations or trade-offs."}}
{"id": "7a1e900d-f2db-4f24-95e0-36ae47590399", "title": "Localization", "level": "section", "subsections": ["c4df2472-ba86-4695-a0a8-db5f944cf9a0", "67e9bc8d-2f82-4946-b81d-c801a0d448bc", "1ca9512a-d587-441b-8b06-b07a116734fd", "196b63d0-2f44-4a3a-8cdf-0d660c3f4a77"], "parent_id": "08af7ca6-1a67-48c9-9735-3113b8e18727", "prefix_titles": [["title", "Analysis of the hands in egocentric vision:\\\\ A survey"], ["section", "Localization"]], "content": "\\label{sec:localization}\nThe localization of hands (or parts of them) is the first and most important processing step of many hand-based methods in FPV. A good hand localization algorithm allows estimating the accurate position of the hands within the image, boosting the performance of higher-level inference . For this reason, hand localization has been the main focus of researchers in egocentric vision. Although many hand detection, pose-estimation, and segmentation algorithms were developed in third person vision , the egocentric POV presents notable challenges that do not allow a direct translation of these methods. Rogez et al.  demonstrated that egocentric hand detection is considerably harder in FPV, and methods developed specifically for third person POV may fail when applied to egocentric videos.\nHand segmentation and detection are certainly the two most extensively studied sub-areas. They are often used in combination, for example to classify as “hand” or “not hand” previously segmented regions , or to segment ROIs previously obtained with a hand detector . However, considering the extensive research behind these two sub-areas, we summarize them separately.\n\\begin{figure*}[t]\n\\begin{center}\n\\includegraphics[width=0.9\\linewidth]{HandLocalization.png}\n\\end{center}\n\\caption{Diagram of hand localization tasks in egocentric vision. Hand detection and segmentation have often been used in combination, for example to segment ROIs previously obtained with a hand detector, or to classify as “hand” or “not hand” previously segmented regions. Since they provide the global position of the hands within the frame, they are chosen as the basis for other localization approaches, such as hand identification, hand tracking, hand pose estimation, fingertip detection (\\textbf{*}: Hand identification is now typically incorporated within the hand detection step).}\n\\label{fig_localization}\n\\end{figure*}", "cites": [4258, 4259], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section begins with a clear explanation of the importance of hand localization in egocentric vision and highlights its distinct challenges compared to third-person vision. It synthesizes the cited papers by noting their focus on hand detection and segmentation, and how these are foundational for higher-level tasks. While it identifies some limitations (e.g., variability of hand appearance, unsolved problems in RGB-D hand pose estimation), the critical analysis is limited in depth. The abstraction is moderate, as it introduces a diagram showing the relationship between different localization tasks, but does not offer a novel theoretical framework."}}
{"id": "399a335c-bc4f-460e-ba50-e81579708799", "title": "Discriminating hands from objects and background", "level": "subsubsection", "subsections": [], "parent_id": "c4df2472-ba86-4695-a0a8-db5f944cf9a0", "prefix_titles": [["title", "Analysis of the hands in egocentric vision:\\\\ A survey"], ["section", "Localization"], ["subsection", "Hand segmentation"], ["subsubsection", "Discriminating hands from objects and background"]], "content": "\\label{subsubsec:segHandsObjBack}\nTraditional hand segmentation approaches (i.e., not based on deep learning) rely on the extraction of features from an image patch, classifying the central pixel or the entire patch as skin or no-skin using a binary classifier or regression model. The vast majority of approaches combined color with gradient and/or texture features, whereas random forest has been the most popular classification algorithm . The use of texture and gradient features allows capturing salient patterns and contours of the hands that, combined with the color features, help discriminate them from background and objects with similar color.\n\\textbf{Pixel-based classification}. Li and Kitani  tested different combinations of color (HSV, RGB, and LAB color spaces) and local appearance features (Gabor filters , HOG , SIFT , BRIEF , and ORB  descriptors) to capture local contours and gradients of the hand regions. Each pixel was classified as skin or no-skin using a random forest regression. When using color features alone, the LAB color space provided the best performance, whereas gradient and texture features, such as HOG and BRIEF, improved the segmentation performance when combined with the color information . Zariffa and Popovic  used a mixture of Gaussian skin model with dilation and erosion morphological operators to detect a coarse estimate of the hand regions. The initial region was refined by removing small isolated blobs with texture different from the skin, by computing the Laplacian of the image within each blob. Lastly, pixel-level segmentation was achieved by backprojecting using an adaptively selected region in the colour space. In , the coarse segmentation obtained with a mixture of Gaussian skin model  was refined by using a structured forest edge detection , specifically trained on available datasets . \n\\textbf{Patch-based classification}. Other authors classified image patches instead of single pixels, in order to produce segmentation masks more robust to pixel-level noise . Serra et al.  classified clusters of pixels (i.e., super-pixels) obtained with the simple linear iterative clustering (SLIC) algorithm . For each super-pixel, they used a combination of color (HSV and LAB color spaces) and gradient features (Gabor filters and histogram of gradients) to train a random forest classifier. The segmentation was refined by assuming temporal coherence between consecutive frames and spatial consistency among groups of super-pixels. Similarly, Singh et al.  computed the hand binary mask by extracting texture and color features (Gabor filters with RGB, HSV, and LAB color features) from the super-pixels, whereas Urabe et al.  used the same features in conjunction with the centroid location of each super-pixel to train a support vector machine (SVM) for segmenting the skin regions. Instead of classifying the whole patch from which color, gradient and texture features are extracted, Zhu et al.  learned the segmentation mask within the image patch, by using a random forest framework (i.e., shape-aware structured forest).\n\\textbf{Deep learning} may help solve hand segmentation problems in FPV. However, its use is still hampered by the lack of large annotated datasets with pixel-level annotations. Some deep learning approaches for hand segmentation  tackled this issue by using the available annotations in combination with other image segmentation techniques (e.g., super-pixels or GrabCut ) to generate new hand segmentation masks for expanding the dataset and fine-tuning pre-trained networks (see Section \\ref{subsubsec:segLackAnnot} for more details). The availability of pre-trained convolutional neural networks (CNNs) for semantic object segmentation  was exploited in . Wang et al.  tackled the hand segmentation problem in a recurrent manner by using a recurrent U-NET architecture . The rationale behind this strategy is to imitate the saccadic movements of the eyes that allow refining the perception of a scene. The computational cost can be another issue in CNN-based hand segmentation. To reduce this cost, while achieving good segmentation accuracy, Li et al.  implemented the deep feature flow (DFF)  with an extra branch to make the approach more robust against occlusions and distortion caused by DFF.", "cites": [4262, 4261, 4258, 3807, 4256, 2571, 4260, 810, 825], "cite_extract_rate": 0.2727272727272727, "origin_cites_number": 33, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes a range of traditional and deep learning-based hand segmentation approaches, effectively organizing them into pixel-based, patch-based, and deep learning categories, and highlighting common features and techniques across works. It provides a critical perspective by noting the limitations of one-shot segmentation and the challenges of computational cost and dataset size. While it identifies patterns such as the use of color and texture features, it stops short of proposing a novel framework or deeper theoretical abstraction."}}
{"id": "a3813302-f199-4718-b46d-78d483ed0975", "title": "Lack of pixel-level annotations", "level": "subsubsection", "subsections": [], "parent_id": "c4df2472-ba86-4695-a0a8-db5f944cf9a0", "prefix_titles": [["title", "Analysis of the hands in egocentric vision:\\\\ A survey"], ["section", "Localization"], ["subsection", "Hand segmentation"], ["subsubsection", "Lack of pixel-level annotations"]], "content": "\\label{subsubsec:segLackAnnot}\nAnnotating images at pixel-level is a very laborious and costly work that refrains many authors from publishing large annotated datasets. Thus, the ideal solution to the hand segmentation problem would be a self-supervised approach able to learn the appearance of the hands on-the-fly, or a weakly supervised method that relies on the available training data to produce new hand masks.\nUsually, methods for online hand segmentation made assumptions on the hand motion  and/or required the user to perform a calibration with pre-defined hand movements . In this way, the combination of color and motion features facilitates the detection of hand pixels, in order to train segmentation models online. Kumar et al.  proposed an on-the-fly hand segmentation, where the user calibrated the systems by waving the hands in front of the camera. The combination of color and motion segmentation (Horn–Schunck optical flow ) and region growing, allowed locating the hand regions for training a GMM-based hand segmentation model. Region growing was also used by Huang et al. . The authors segmented the frames in super-pixels  and extracted ORB descriptors  from each super-pixel to find correspondences between regions of consecutive frames, which reflect the motion between two frames. Hand-related matches were distinguished from camera-related matches based on the assumption that camera-related matches play a dominant role in the video. These matches were estimated using RANSAC  and after being removed, those left were assumed to belong to the hands and used to locate the seed point for region growing. Zhao et al.  based their approach on the typical motion pattern during actions involving the hands: preparatory phase (i.e., the hands move from the lower part of the frame to the image center) and interaction phase. During the preparatory phase they used a motion-based segmentation, computing the TV-L1 optical flow . As the preparatory phase ends, the motion decreases and the appearance becomes more important. A super-pixel segmentation  was then performed and a super-pixel classifier, based on the initial motion mask, was trained using color and gradient features.\nTransfer learning has also been used to deal with the paucity of pixel-level annotations. The idea is to exploit the available pixel-level annotations in combination with other image segmentation techniques (e.g., super-pixels or GrabCut ) to generate new hand segmentation masks and fine-tune pre-trained networks. Zhou et al.  trained a hand segmentation network using a large amount of bounding box annotations and a small amount of hand segmentation maps . They adopted a DeconvNet architecture  made up of two mirrored VGG-16 networks  initialized with 1,500 pixel-level annotated frames from . Their approach iteratively selected and added good segmentation proposals to gradually refine the hand map. The hand segmentation proposals were augmented by applying super-pixel segmentation  and Grabcut  to generate the hand probability map within the ground truth bounding boxes. DeconvNet was trained in an Expectation-Maximization manner: 1) keeping the network parameter fixed, they generated a set of hand masks and selected the best segmentation proposals (i.e., those with largest match with the ground truth mask); 2) they updated the network weights by using the best segmentation hypotheses. Similarly, Li et al.  relied on the few available pixel-level annotations to train Deeplab-VGG16 . Their training procedure was composed of multiple steps: 1) Pre-segmentation -- the CNN, pre-trained using the available pixel-level annotations, was applied on the target images to generate pre-segmentation maps; 2) Noisy mask generation -- the pre-segmentation map was combined with a super-pixel segmentation ; and 3) Model retraining -- the new masks were used as ground truth to fine tune the pre-trained Deeplab-VGG16.", "cites": [1766, 514, 4263], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the key methods used to overcome the lack of pixel-level annotations for hand segmentation in egocentric vision, integrating techniques like motion-based segmentation, region growing, and transfer learning. It critically evaluates the assumptions and limitations of each approach, such as the need for user calibration or the reliance on limited annotations. The analysis abstracts these methods into broader strategies (e.g., self-supervised learning, EM-based refinement), offering a meta-level view of how the field has adapted to annotation constraints."}}
{"id": "0efbda79-3c9e-45d8-b941-5fc1abedb38f", "title": "Remarks on hand segmentation", "level": "subsubsection", "subsections": [], "parent_id": "c4df2472-ba86-4695-a0a8-db5f944cf9a0", "prefix_titles": [["title", "Analysis of the hands in egocentric vision:\\\\ A survey"], ["section", "Localization"], ["subsection", "Hand segmentation"], ["subsubsection", "Remarks on hand segmentation"]], "content": "\\label{subsubsec:segRemarks}\nBecause of the high amount of detail obtained with hand segmentation algorithms, this task is the hardest one among hand-based methods in FPV. The pixel- or super-pixel-level accuracy required for this task, combined with the intrinsic problems of egocentric vision, made this sub-area the most challenging and debated of this field of research. The effort of many researchers in finding novel and powerful approaches to obtain better results is justified by the possibility to improve not only the localization accuracy, but also to boost the performance of higher-level inference. In fact, it was demonstrated that a good hand segmentation mask can be sufficient for recognizing actions and activities involving the hands with high accuracy . For this reason, pixel-level segmentation has often been used as basis of higher-inference methods.\nRGB-D information can certainly improve and simplify the hand segmentation task. However, these methods are a minority with respect to the 2D counterpart, since no depth cameras have been developed for specific egocentric applications. With the recent miniaturization of depth sensors (e.g., iPhone$^{\\circledR}$ X and 11) the 3D segmentation is still an area worth exploring and expanding within the next few years.\nMany authors considered detection and segmentation as two steps of the same task. We preferred to split these two sub-areas given the large amount of work produced in the past few years. However, as it will be illustrated in the next section, many hand detection approaches, especially those using region-based CNNs, used the segmentation mask for generating region proposals. Perhaps, with the possibility to re-train powerful object detectors, this process has become inefficient and instead of having a “detection over segmentation”, it will be more convenient to have a “segmentation over detection”, unless the specific problem calls for a pixel-level segmentation of the entire frame. Following the great success of mask R-CNN , an interesting approach in this direction would be to address hand segmentation as an instance segmentation task, embedding bounding box detection and semantic segmentation of the hands in a single model.", "cites": [520], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key ideas around hand segmentation in egocentric vision, integrating the role of RGB-D data and the relationship between detection and segmentation. It offers a critical perspective by questioning the efficiency of traditional detection-over-segmentation pipelines and suggesting a shift to segmentation-over-detection. The discussion also abstracts to broader methodological trends, such as the potential of instance segmentation frameworks like Mask R-CNN."}}
{"id": "fbee5de2-daa6-42b7-b9f9-bd4fa7295323", "title": "Hand detection as object detection", "level": "subsubsection", "subsections": [], "parent_id": "67e9bc8d-2f82-4946-b81d-c801a0d448bc", "prefix_titles": [["title", "Analysis of the hands in egocentric vision:\\\\ A survey"], ["section", "Localization"], ["subsection", "Hand detection and tracking"], ["subsubsection", "Hand detection as object detection"]], "content": "\\label{subsubsec:detObjDet}\nHand detection performed within an object localization framework presents notable challenges. Given the flexibility, the continuous variation of poses, and the high number of degrees of freedom, the hand appearance is highly variable and classical object detection algorithms (e.g., Haar like features with adaboost classification) may work only in constrained situations, such as detection of hands in a specific pose . For these reasons, and thanks to the availability of large annotated datasets with bounding boxes, this is the area that most benefited from the advent of deep learning.\n\\textbf{Region-based approaches}. Many authors proposed region-based CNNs to detect the hands, exploiting segmentation approaches summarized in Section \\ref{subsec:handSeg} to generate region proposals. Bambach et al.  proposed a probabilistic approach for region proposal generation that combined spatial biases (e.g., reasoning on the position of the shape of the hands from training data) and appearance models (e.g., non-parametric modeling of skin color in the YUV color space). To guarantee high coverage, they generated 2,500 regions for each frame that were classified using CaffeNet . Afterwards, they obtained the hand segmentation mask within the bounding box, by applying GrabCut . Zhu et al.  used a structured random forest to propose pixel-level hand probability maps. These proposals were passed to a multitask CNN to locate the hand bounding box, the shape of the hand within the bounding box, and the position of wrist and palm. In , the authors generated region proposals by segmenting skin regions with  and determining if the set of segmented blobs correspond to one or two arms. This estimation was performed by thresholding the fitting error of a straight line. K-means clustering, with \\textit{k} = 2 if two arms are detected, was applied to split the blobs into two separate structures. The hand proposals were selected as the top part of a rectangular bounding box fitted to the arm regions and passed to CaffeNet for the final prediction. To generate hand region proposals, Cruz et al.  used a deformable part model (DPM) to make the approach robust to different gestures. DPM learns the hand shape by considering the whole structure and its parts (i.e., the fingers) using HOG features. CaffeNet  was used for classifying the proposals. Faster R-CNN was used in . In particular, Likitlersuang et al.  fine-tuned the network on videos from individuals with cSCI performing ADLs. False positives were removed based on the arm angle information computed by applying a Haar-like feature rotated 360 degrees around the bounding box centroid. The resulting histogram was classified with a random forest to determine whether the bounding box actually included a hand. Furthermore, they combined color and edge segmentation to re-center the bounding box, in order to promote maximum coverage of the hands while excluding parts of the forearm. \n\\textbf{Regression-based approaches} were also used for detecting the hands. Mueller et al.  proposed a depth-based approach for hand detection, implemented using the Intel$^{\\circledR}$ RealSense\\textsuperscript{TM} SR300 camera. A Hand Localization Network (HALNet -- architecture derived from ResNet50  and trained on synthesized data) was used to regress the position of the center of the hand. The ROI was then cropped around this point based on its distance from the camera (i.e., the higher the depth, the smaller the bounding box). Recently, the \\textit{You Only Look Once} (YOLO) detector  was applied for localizing hands in FPV , demonstrating better trade-off between computational cost and localization accuracy than Faster R-CNN and single-shot detector (SSD) .", "cites": [4265, 4258, 206, 2671, 802, 4260, 4264, 97], "cite_extract_rate": 0.42105263157894735, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes several papers by grouping methods into 'region-based' and 'regression-based' approaches, highlighting commonalities and strategies used in hand detection. It provides a critical perspective by noting the limitations of classical methods and the improvements brought by deep learning. While it identifies some patterns (e.g., use of CaffeNet, integration of segmentation and classification), it stops short of presenting a deeper, unifying abstraction of the field."}}
{"id": "1496c192-b64d-463e-923a-aca0a7bc9006", "title": "Hand tracking", "level": "subsubsection", "subsections": [], "parent_id": "67e9bc8d-2f82-4946-b81d-c801a0d448bc", "prefix_titles": [["title", "Analysis of the hands in egocentric vision:\\\\ A survey"], ["section", "Localization"], ["subsection", "Hand detection and tracking"], ["subsubsection", "Hand tracking"]], "content": "\\label{subsubsec:detTrack} \nHand tracking allows estimating the position of the hands across multiple frames, reconstructing their trajectories in time. Theoretically, every hand detection and segmentation approach seen above, with the exception of the binary classification algorithms of section \\ref{subsubsec:detImClass}, can be used as tracker as well, by performing a frame-by-frame detection. This is the most widely used choice for tracking the hand position over time. However, some authors tried to combine the localization results with temporal models to predict the future hand positions. This strategy has several advantages, such as decreasing the computational cost by avoiding to run the hand detection every frame , disambiguate overlapping hands by exploiting their previous locations , and refining the hand location .\nLee et al.  studied the child-parent social interaction from the child’s POV, by using a graphical model to localize the body parts (i.e., hands of child and parent, head of the parent). The model was composed of inter-frame links to enforce temporal smoothness of the hand positions over time, shift links to model the global shifts in the field of view caused by the camera motion, and intra-frame constraints based on the spatial configuration of the body parts. Skin color segmentation in the YUV color space was exploited to locate the hands and define intra-frame constraints on their position. This formulation forced the body parts to remain in the neighborhood of the same position between two consecutive frames, while allowing for large displacement due to global motion (i.e., caused by head movements) if this displacement is consistent with all parts. Liu et al.  demonstrated that the hand detection is more accurate in the central part of the image due to a center bias (i.e., higher number of training examples with hands in the center of the frame). To correct this bias and obtain homogeneous detection accuracy in the whole frame, they proposed an attention-based tracker (AHT). For each frame, they estimated the target location of the hand by exploiting the result at the previous frame. Then, the estimated hand region was translated to the image center, where a CNN fine-tuned on frames with centralized hands was applied. After segmenting the hand regions using , Cai et al.  used the temporal tracking method  to discriminate them in case of overlap. \nRegression-based CNNs in conjunction with object tracking algorithms were used in . Kapidis et al.  fine-tuned YOLOv3  on multiple datasets to perform the hand detection, discriminating the right and left hand trajectories over time using the simple online real-time tracking (SORT) . For each detected bounding box, this algorithm allowed predicting its next position, also assigning it to existing tracks or to new ones. Visée et al.  combined hand detection and tracking to design an approach for fast and reliable hand localization in FPV. Motivated by the slow detection performance of YOLOv2 without GPU, they proposed to combine YOLOv2 with the Kernelized Correlation Filter (KCF)  as a trade-off between speed and accuracy. The authors used the detector to automatically initialize and reset the tracker in case of failure or after a pre-defined number of frames.", "cites": [4265, 836, 4266], "cite_extract_rate": 0.3, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers by connecting hand detection, tracking, and the use of temporal models, showing how different approaches address common challenges like computational cost and occlusion. It provides some critical analysis by highlighting limitations (e.g., center bias in detection) and proposing solutions (e.g., attention-based tracking). It abstracts to a degree by identifying broader strategies (e.g., combining detection with tracking) but does not fully articulate overarching principles or deep theoretical insights."}}
{"id": "1ca9512a-d587-441b-8b06-b07a116734fd", "title": "Hand identification", "level": "subsection", "subsections": [], "parent_id": "7a1e900d-f2db-4f24-95e0-36ae47590399", "prefix_titles": [["title", "Analysis of the hands in egocentric vision:\\\\ A survey"], ["section", "Localization"], ["subsection", "Hand identification"]], "content": "\\label{subsec:handIdent} \nHand identification is the process of disambiguating the left and right hands of the camera wearer, as well as the hands of other persons in the scene. The egocentric POV has intrinsic advantages that allow discriminating the hands by using simple spatial and geometrical constraints . Usually, the user’s hands appear in the lower part of the image, with the right hand to the right of the user’s left hand, and vice versa. By contrast, other people’s hands tend to appear in the upper part of the frame . The orientation of the arm regions was used in  to distinguish the left from the right user’s hand. To estimate the angle, the authors rotated a Haar-like feature around the segmented hand region, making this approach robust to the presence of sleeves and different skin colors, since it did not require any calibrations . To identify the hands, they split the frame into four quadrants. The quadrant with the highest sum of the Haar-like feature vector determined the hand type: “user’s right” if right lower quadrant; “user’s left” if left lower quadrant; “other hands” if upper quadrants . The angle of the forearm/hand regions was also used by Betancourt et al. . The authors fitted an ellipse around the segmented region, calculating the angle between the arm and the lower frame border and the normalized distance of the ellipse center from the left border. The final left/right prediction was the result of a likelihood ratio test between two Maxwell distributions.\nAlthough simple and effective, spatial and geometric constraints may fail in case of overlapping hands. In this case, the temporal information help disambiguate the hands . Cai et al.  were interested in studying the grasp of the right hand. After segmenting the hand regions , they implemented the temporal tracking method proposed in  to handle the case of overlapping hands, thus tracking the right hand. Kapidis et al.  used the SORT tracking algorithm . This approach combines the Kalman filter to predict the future position of the hand and the Hungarian algorithm to assign the next detection to existing tracks (i.e., left/right) or new ones.\nWith the availability of powerful and accurate CNN-based detectors, the hand identification as separated processing step is deprecated, being incorporated within hand detection (see Section \\ref{subsubsec:detObjDet}) . To this end, both region-based (e.g., Faster R-CNN) and regression-based methods (e.g., YOLO and SSD) have been used. These models were trained or fine-tuned to recognize two or more classes of hands, predicting the bounding box coordinates along with its label (i.e., left, right, and other hands) .", "cites": [4265, 4266, 4267, 4260], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple methods for hand identification, integrating spatial, geometric, and temporal approaches from different papers into a coherent narrative. It also provides some critical discussion by pointing out limitations of spatial constraints in overlapping hand scenarios and notes the shift toward CNN-based solutions. However, while it identifies trends (e.g., the move from explicit segmentation to integrated detection), it does not fully abstract these into broader theoretical principles or meta-level insights."}}
{"id": "196b63d0-2f44-4a3a-8cdf-0d660c3f4a77", "title": "Hand pose estimation and fingertip detection", "level": "subsection", "subsections": ["e6845dc3-03e2-4c93-a275-97a57b89ebe5", "c00b234c-0e95-4415-9439-50614d10e067", "9a22616e-7bec-48df-9430-93812346de96", "4a2e01d3-ec0f-4262-baee-f1ede9a05c2e"], "parent_id": "7a1e900d-f2db-4f24-95e0-36ae47590399", "prefix_titles": [["title", "Analysis of the hands in egocentric vision:\\\\ A survey"], ["section", "Localization"], ["subsection", "Hand pose estimation and fingertip detection"]], "content": "\\label{subsec:handPose} \nHand pose estimation consists in the localization of the hand parts (e.g., the hand joints) to reconstruct the articulated hand pose from the images (see Figure \\ref{fig_localization}). The possibility to obtain the position of fingers, palm, and wrist, simplifies higher inference tasks such as grasp analysis and hand gesture recognition, since the dimensionality of the problem is reduced yet keeping high-detail information. An important difficulty in hand pose estimation lies in object occlusions and self-occlusions that make it hard to localize hidden joints/parts of the hand. Some authors proposed the use of depth cameras in conjunction with sensor-based techniques to train hand pose estimators more robust to self-occlusions . However, as discussed above, the use of RGB-D imaging techniques might not be easily translated to FPV. Thus, several attempts have also been made to estimate the hand pose using only color images . In this section, we summarize the previous work distinguishing between hand pose estimation approaches with depth sensors and hand pose estimation using monocular color images. Moreover, we summarize approaches for fingertip detection, which can be seen as an intermediate step between hand detection and hand pose estimation.", "cites": [4255, 4257, 4264, 4259], "cite_extract_rate": 0.4, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the cited papers by distinguishing between hand pose estimation approaches using depth sensors and those relying solely on color images, and also connects fingertip detection as an intermediate step. It includes a critical point about the difficulty of translating RGB-D techniques to FPV. While it provides some abstraction by framing the problem in terms of occlusions and sensor limitations, it stops short of offering a novel, meta-level framework or deeper comparative analysis of the methodologies."}}
{"id": "e6845dc3-03e2-4c93-a275-97a57b89ebe5", "title": "Hand pose estimation using depth sensors", "level": "subsubsection", "subsections": [], "parent_id": "196b63d0-2f44-4a3a-8cdf-0d660c3f4a77", "prefix_titles": [["title", "Analysis of the hands in egocentric vision:\\\\ A survey"], ["section", "Localization"], ["subsection", "Hand pose estimation and fingertip detection"], ["subsubsection", "Hand pose estimation using depth sensors"]], "content": "\\label{subsubsec:pose3D}\nOne of the advantages of using depth information for estimating the hand pose is that it is easier to synthesize depth maps that closely resemble the ones acquired by real sensors, when compared to real versus synthetic color images . In , the authors tackled hand pose estimation as a multiclass classification problem by using a hierarchical cascade architecture. The classifier was trained on synthesized depth maps by using HOG features and tested on depth maps obtained with a ToF sensor. Instead of estimating the joint coordinates independently, they predicted the hand pose as whole, in order to make the system robust to self-occlusions. Similarly, in , the authors predicted the upper arm and hand poses simultaneously, by using a multiclass linear SVM for recognizing K poses from depth data. However, instead of classifying scanning windows on the depth maps, they classified the whole egocentric work-space, defined as the 3D volume seen from the egocentric POV. Mueller et al.  proposed a CNN architecture (Joint Regression Net -- JORNet) to regress the 3D locations of the hand joints within the cropped colored depth maps captured with a structured light sensor (Intel$^{\\circledR}$ RealSense\\textsuperscript{TM} SR300). Afterwards, a kinematic skeleton was fitted to the regressed joints, in order to refine the hand pose. Yamazaki et al.  estimated the hand pose from hand point clouds captured with the Kinect v2 sensor. The authors built a dataset by collecting pairs of hand point clouds and ground truth joint positions obtained with a motion capture system. The pose estimation was performed by aligning the test point cloud to the training examples and predicting its pose as the one that minimizes the alignment error. The sample consensus initial alignment  and iterative closest point algorithms  were used for aligning the point clouds. Garcia-Hernando et al.  evaluated a CNN-based hand pose estimator  for regressing the 3D hand joints from RGB-D images recorded with the Intel$^{\\circledR}$ RealSense\\textsuperscript{TM} SR300 camera. The authors demonstrated that state-of-the-art hand pose estimation performance can be reached by training the algorithms on datasets that include hand-object interactions, in order to improve its robustness to self-occlusions or hand-object occlusions.", "cites": [4255, 4264, 4268, 4259], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes several works on hand pose estimation using depth sensors, connecting them through common themes such as the use of RGB-D data, handling occlusions, and sensor-specific approaches. It provides an analytical overview of the methodologies, including hierarchical classification, CNN-based regression, and point cloud alignment. While it mentions limitations (e.g., unsolved monocular hand pose estimation in RGB-D), it lacks deeper comparison or critique of the approaches. The abstraction is moderate, identifying patterns such as the role of depth in improving robustness to occlusions but not elevating the discussion to broader meta-principles."}}
{"id": "c00b234c-0e95-4415-9439-50614d10e067", "title": "Hand pose estimation from monocular color images", "level": "subsubsection", "subsections": [], "parent_id": "196b63d0-2f44-4a3a-8cdf-0d660c3f4a77", "prefix_titles": [["title", "Analysis of the hands in egocentric vision:\\\\ A survey"], ["section", "Localization"], ["subsection", "Hand pose estimation and fingertip detection"], ["subsubsection", "Hand pose estimation from monocular color images"]], "content": "\\label{subsubsec:poseColor}\nIn general, hand pose estimation from monocular color images allows locating the parts of the hands either in the form of 2D joints or semantic sub-regions (e.g., fingers, palm, etc.). This estimation is performed within previously detected ROIs, obtained by either a hand detection or segmentation algorithm. Liang et al.  used a conditional regression forest (CRF) to estimate the hand pose from hand binary masks. Specifically, they trained a set of pose estimators separately, conditioned on different distances from the camera, since the hand appearance and size can change dramatically with the distance from the camera. Thus, they synthesized a dataset in which the images were sampled at discretized intervals. The authors also proposed an intermediate step for improving the joint localization, by segmenting the binary silhouette into twelve semantic parts corresponding to different hand regions. The semantic part segmentation was performed with a random forest for pixel-level classification exploiting binary context descriptors. Similarly, Zhu et al.  built a structured forest to segment the hand region into four semantic sub-regions: thumb, fingers, palm, and forearm. This semantic part segmentation was performed extending the structured regression forest framework already used for hand segmentation (as discussed in Section \\ref{subsec:handSeg}) to a multiclass problem .\nOther studies adapted CNN architectures developed for human pose estimation (e.g., OpenPose ) for solving the hand pose estimation problem  and localizing 21 hand joints. Tekin et al.  used a fully convolutional network (FCN) architecture to simultaneously estimate the 3D hand and object pose from RGB images. For each frame, the FCN produced a 3D discretized grid. The 3D location of the hand joints in camera coordinate system was then estimated combining the predicted location within the 3D grid and the camera intrinsic matrix.", "cites": [4257, 4269], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual overview of methods for hand pose estimation from monocular color images, mentioning the use of conditional regression forests and CNNs. It integrates a few ideas by referencing both traditional structured forests and deep learning approaches but lacks deeper synthesis or comparative analysis. There is limited critical evaluation or abstraction of broader trends or principles in the field."}}
{"id": "4a2e01d3-ec0f-4262-baee-f1ede9a05c2e", "title": "Remarks on hand pose estimation", "level": "subsubsection", "subsections": [], "parent_id": "196b63d0-2f44-4a3a-8cdf-0d660c3f4a77", "prefix_titles": [["title", "Analysis of the hands in egocentric vision:\\\\ A survey"], ["section", "Localization"], ["subsection", "Hand pose estimation and fingertip detection"], ["subsubsection", "Remarks on hand pose estimation"]], "content": "\\label{subsubsec:poseRemarks}\nAmong the hand localization tasks, hand pose estimation allows obtaining high-detail information with high semantic content at the same time (see Figure \\ref{fig_framework}). This task, if performed correctly, can greatly simplify higher inference steps (e.g., hand gesture recognition and grasp analysis), but may be more prone to low robustness against partial hand occlusions.\nCompared to other localization tasks, hand pose estimation presents a higher proportion of approaches that use depth sensors. This choice has several advantages: 1) the possibility to use motion capture methods for automatically obtaining the ground truth joint positions ; 2) the availability of multiple streams (i.e., color and depth) that can be combined to refine the estimations ; and 3) the possibility to synthesize large datasets of realistic depth maps . In the past few years, human pose estimation approaches  have been successfully adapted to the egocentric POV, in order to estimate the hand and arm pose from monocular color images . This opens new possibilities to streamline and improve the performance of localization and hand-based higher inference tasks, such as grasp analysis. To further facilitate the adaptation of existing pose estimation approaches, large annotated datasets with hand joint information are needed. To this end, a combination of 2D and 3D information may be beneficial, in order to get accurate and extensive ground truth annotations in 3D that will allow solving the occlusion problems even when using color images alone.", "cites": [4264, 4268, 4259, 4269], "cite_extract_rate": 0.4, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple papers to highlight the unique role and challenges of hand pose estimation in egocentric vision. It critically discusses the reliance on depth sensors, their advantages, and limitations when applied to real-world color-only scenarios. The section abstracts these findings into broader considerations for dataset development and future research directions."}}
{"id": "b9165edf-9ff3-408d-bef3-b6156eb2abdd", "title": "Hand grasp recognition", "level": "subsubsection", "subsections": [], "parent_id": "5ddb3ce5-826a-4ba9-a2ca-c25abfceb9c5", "prefix_titles": [["title", "Analysis of the hands in egocentric vision:\\\\ A survey"], ["section", "Interpretation"], ["subsection", "Hand grasp analysis and gesture recognition"], ["subsubsection", "Hand grasp recognition"]], "content": "\\label{subsubsec:graspRec}\nSupervised approaches for grasp recognition are based on the extraction of features from previously segmented hand regions  and their multiclass classification following one of the taxonomies proposed in the literature . \nCai et al.  used HOG features to represent the shape of the hand and a combination of HOG and SIFT to capture the object context during the manipulation. These features were classified with a multi-class SVM using a subset of grasp types from Feix’s taxonomy . The authors extended their approach in  by introducing CNN-based features extracted from the middle layers of  and features derived from the dense hand trajectory (DHT)  such as the displacement, gradient histograms, histogram of optical flow, and motion boundary histograms. The superior performance of CNN- and DHT-based features and their robustness across different tasks and users  suggested that high-level feature representation and motion and appearance information in the space-time volume may be important cues for discriminating different hand configurations. In , the authors used a graph-based approach to discriminate 8 grasp types. Specifically, the binary hand mask was used to produce a graph structure of the hand with an instantaneous topological map neural network. The eigenvalues of the graph’s Laplacians were used as features to represent the hand configurations, which were recognized using an SVM. \nThe use of depth sensors was explored by Rogez et al. . The authors recognized 71 grasp types  using RGB-D data, by training a multi-class SVM with deep-learned features  extracted from both real and synthetic data. Moreover, the grasp recognition results were refined by returning the closest synthetic training example, namely the one that minimized the distance with the depth of the detected hand region.", "cites": [514], "cite_extract_rate": 0.08333333333333333, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes several grasp recognition approaches, connecting them under the theme of feature extraction and classification. It provides an analytical overview by highlighting the shift from traditional features (HOG, SIFT) to CNN- and DHT-based features, and notes their improved performance. However, the critical evaluation is limited to performance claims without deeper discussion of assumptions, limitations, or trade-offs. Some abstraction is present in grouping methods by feature type, but broader principles or unifying frameworks are not fully articulated."}}
{"id": "1d9e4188-817c-44e1-9353-32f1bd20d4c3", "title": "Hand grasp clustering and abstraction", "level": "subsubsection", "subsections": [], "parent_id": "5ddb3ce5-826a-4ba9-a2ca-c25abfceb9c5", "prefix_titles": [["title", "Analysis of the hands in egocentric vision:\\\\ A survey"], ["section", "Interpretation"], ["subsection", "Hand grasp analysis and gesture recognition"], ["subsubsection", "Hand grasp clustering and abstraction"]], "content": "\\label{subsubsec:graspCluster}\nThe first attempt to discover hand grasps in FPV was . HOG features were extracted from previously segmented hand regions and grouped by means of a two-stage clustering approach. First, a set of candidate cluster centers was generated through the fast determinantal point process (DPP) algorithm . This step allowed generating a wide diversity of clusters to cover many possible hand configurations. Secondly, each segmented region was assigned to the nearest cluster center. The use of the DPP algorithm was proven to outperform other clustering approaches such as k-means and to be more appropriate in situations, like grasp analysis, where certain clusters are more recurrent than other ones. A hierarchical structure of the grasp types was learned using the same DPP-based clustering approach . A hierarchical clustering approach was also used in  to find the relationships between different hand configurations based on a similarity measure between pairs of grasp types. Similarly, in , the authors used a correlation index to measure the visual similarity between grasp types: grasp types with high correlation were clustered at the lower nodes, whereas low-correlated types were clustered higher in the hierarchy. The above approaches  were used to build tree-like structures of the grasp types. These structures can be exploited to define new taxonomies depending on the trade-off between detail and robustness of grasp classification, as well as to discover new grasp types not included in previous categorizations .", "cites": [7166], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple works to present a coherent narrative on hand grasp clustering and abstraction in egocentric vision. It connects the use of DPP-based clustering with hierarchical grasp structures and similarity measures, indicating an attempt to integrate different methods. While it includes some comparison (e.g., DPP vs. k-means), the critical analysis is limited and does not deeply evaluate the limitations of the approaches. The section identifies broader patterns in clustering strategies, suggesting a level of abstraction beyond individual papers."}}
{"id": "d10c8f97-0c05-48b1-b5bf-aa095e551ca2", "title": "Action/interaction and activity recognition", "level": "subsection", "subsections": ["3f4677e6-00f2-4237-bdad-78d04fddd478", "1c060e85-d65e-4167-a296-6b3307c1133c", "7a05e184-e91d-498c-b40f-afac9c314bf3", "266bd0ca-9a83-4f4a-b8a5-7d42f9e67711"], "parent_id": "2e67d397-7219-4ffa-88b7-5a8046d9d153", "prefix_titles": [["title", "Analysis of the hands in egocentric vision:\\\\ A survey"], ["section", "Interpretation"], ["subsection", "Action/interaction and activity recognition"]], "content": "\\label{subsec:actionActivity}\nAccording to Tekin et al. , an action is a verb (e.g., “cut”), whereas an interaction is a verb-noun pair (e.g., “cut the bread”). Both definitions refer to short-term events that usually last a few seconds . By contrast, activities are longer temporal events (i.e., minutes or hours) with higher semantic content, typically composed of temporally-consistent actions and interactions  (see Figure \\ref{fig_interpret}).\nIn this section, we summarize FPV approaches that relied on hand information to recognize actions, interactions, and activities from sequences of frames. Regarding the actions and interactions, two main types of approaches can be found in literature: those that used hands as the only cue for the prediction (see Section \\ref{subsubsec:actionHand}) and approaches that used a combination of object and hand cues (see Section \\ref{subsubsec:actionHandObj}). Although the second type of approaches might seem more suitable for interaction recognition (i.e., verb + noun prediction), some authors used them for predicting action verbs, exploiting the object information to prune the space of possible actions (i.e., removing unlikely verbs for a given object) . Likewise, other authors tried to use only hand cues to recognize interactions , in order to produce robust predictions without relying on object features or object recognition algorithms. Either way, the boundary between action and interaction recognition is not well defined and often depends on the nature of the dataset on which a particular approach has been tested.", "cites": [4257], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a conceptual distinction between actions, interactions, and activities, and integrates this with an overview of approaches in the literature. It connects ideas by discussing the use of hand-only and hand-object cues, showing how these relate to the tasks at hand. While it includes some critical discussion about the fuzzy boundary between action and interaction recognition, it does not deeply evaluate or critique the cited methods. The abstraction level is moderate, as it identifies a recurring theme in method design but stops short of offering a novel meta-framework."}}
{"id": "1c060e85-d65e-4167-a296-6b3307c1133c", "title": "Action/interaction recognition combining hand and object cues", "level": "subsubsection", "subsections": [], "parent_id": "d10c8f97-0c05-48b1-b5bf-aa095e551ca2", "prefix_titles": [["title", "Analysis of the hands in egocentric vision:\\\\ A survey"], ["section", "Interpretation"], ["subsection", "Action/interaction and activity recognition"], ["subsubsection", "Action/interaction recognition combining hand and object cues"]], "content": "\\label{subsubsec:actionHandObj}\nMany authors demonstrated that the combination of object and hand cues can improve the recognition performance . This is quite intuitive, since during an interaction the grasp type and hand movements strictly depend on the characteristics of the object that is being manipulated (e.g., dimension, shapes, functionality) . Thus, grasp type or hand pose/shape along with object cues can be used to recognize the actions and interactions .\nIn , the authors predicted the attributes of the manipulated object (i.e., object shape and rigidity) and the type of grasp to recognize hand’s actions. They proposed a hierarchical 2-stage approach where the lower layer -- visual recognition -- classified the grasp type and the object attributes and pass this information to the upper layer -- action modeling -- responsible for the action classification via linear SVM. Coskun et al.  implemented a recurrent neural network (RNN) to exploit the temporal dependencies of consecutive frames using a set of deep-learned features related to grasp, optical flow, object-object, and hand-object interactions, as well as the trajectories of the hands over the past few frames. Other authors , used hand cues with lower semantic content than hand grasp, such as shape and pose. Fathi et al.  extracted a set of object and hand descriptors (e.g., object and hand labels, optical flow, location, shape, and size) at super-pixel level and performed a 2-stage interaction recognition. First, they recognized actions using Adaboost; second, they refined the object recognition in a probabilistic manner by exploiting the predicted verb label and object classification scores. Likitlersuang et al.  detected the presence of interactions between the camera wearer’s hands and manipulated objects. This was accomplished by combining the hand shape, represented with HOG descriptors, with color and motion descriptors (e.g., color histogram and optical flow) for the hand, the background, and the object (i.e., regions around the hands). Random forest was used for classification. The articulated hand pose was used in . Garcia-Hernando et al.  passed the hand and object key-points to an LSTM that predicted the interactions over the video frames. This approach was extended in , where hand-object interactions were first modeled using a multi-layer perceptron and then used as input to the LSTM.\nOther approaches, instead of explicitly using the hand information for predicting actions and interactions, exploited the results of hand localization algorithms to guide the feature extraction within a neighborhood of the manipulation region . This strategy was motivated by the fact that the most important cues (i.e., motion, object, etc.) during an action are likely to be found in proximity of the hands and manipulated object. Li et al.  used a combination of local descriptors for motion and object cues in conjunction with a set of egocentric cues. The former, were extracted from the dense trajectories to represent the motion of the action (i.e., shape of the trajectories, MBH, HoF) and the object appearance (e.g., HOG, LAB color histogram, and LBP along the trajectories). The latter were used to approximate the gaze information, by combining camera motion removal and hand segmentation, in order to focus the attention on the area where the manipulation is happening. Ma et al.  used a multi-stream deep learning approach composed of an appearance stream to recognize the object and a motion stream to predict the action verb. The object recognition network predicted the object label by using as input the hand mask and object ROI, whereas the action recognition network used the optical flow map to infer the verb. A fusion layer combined verb and object labels and predicted the interactions. Zhou et al.  used the hand segmentation mask, object features extracted from middle layers of AlexNet , and optical flow to localize and recognize the active object using VGG-16 . Afterwards, object features were represented in a temporal pyramid manner and combined with motion characteristics extracted from improved dense trajectories, in order to recognize interactions using non-linear SVM. \nAlthough the above approaches might differ for the type of features and algorithm used to predict actions and interactions, most of them demonstrated that the combination of object and hand cues can provide better recognition performance than single modality recognition .", "cites": [4265, 4257, 4255, 4270, 514, 4260], "cite_extract_rate": 0.46153846153846156, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple approaches effectively by grouping them based on how they combine hand and object cues, and it identifies common strategies such as hierarchical modeling, feature fusion, and temporal modeling. It shows some critical analysis by pointing out that while methods differ in features and algorithms, they share a common theme of improved performance via multimodal cues. The abstraction level is strong as it generalizes the underlying principle of using hand-object interactions for action recognition beyond specific papers."}}
{"id": "7a05e184-e91d-498c-b40f-afac9c314bf3", "title": "Activity recognition", "level": "subsubsection", "subsections": [], "parent_id": "d10c8f97-0c05-48b1-b5bf-aa095e551ca2", "prefix_titles": [["title", "Analysis of the hands in egocentric vision:\\\\ A survey"], ["section", "Interpretation"], ["subsection", "Action/interaction and activity recognition"], ["subsubsection", "Activity recognition"]], "content": "\\label{subsubsec:activity}\nAs we climb the semantic content dimension in the proposed framework, the strong dependency on hand cues fades away. Other information comes into play and can be used in conjunction with the hands to predict the activities. This diversification becomes clear when we look at the review published by Nguyen et al. , which categorized egocentric activity recognition as: 1) combination of actions; 2) combination of active objects; 3) combination of active objects and locations; 4) combination of active objects and hand movements; and 5) combination of other information (e.g., gaze, motion, etc.). The description of all these approaches goes beyond the scope of this work, since we are interested in characterizing how hands can be used in activity recognition methods. For a more comprehensive description of activity recognition in FPV, the reader is referred to .\nThe boundary between the recognition of short and long temporal events (i.e., actions/interactions and activities, respectively) is not always well defined and, similar to action/interaction recognition, it may depend on the dataset used for training and testing a particular approach. In fact, some of the methods described in the previous subsections were also tested within an activity recognition framework .\nGenerally, we can identify two types of approaches: activity recognition based on actions and interactions  and approaches that used hand localization results to directly prectict the activities . \nApproaches that relied on actions and interactions learned a classifier for recognizing the activities using the detected actions or hand-object interactions as features for the classification. This can be performed by using the histogram of action frequency in the video sequence and its classification using adaboost . Nguyen et al.  used Bag of Visual Words representation to model the interactions between hands and objects, since these cues play a key role in the recognition of activities. Dynamic time warping was then used to compare a new sequence of features with the key training features.\nOther authors  investigated how good the hand segmentation map is in predicting a small set of social activities, such as 4 interactions between two individuals. The authors used a CNN-based approach using the binary hand segmentation maps as input. The prediction was performed on a frame-by-frame basis and using temporal integration implemented through a voting strategy among consecutive frames, with the latter approach providing better results (up to 73\\% of recognition accuracy) . This result confirms what was already shown for actions and interactions, namely the temporal information becomes essential when performing higher-level inference, especially when modeling relatively long term events like activities. However, this approach was tested only in case of a small sample of social activities. To the best of our knowledge, no experiments using hand cues only were conducted for predicting other types of activities, such as ADLs.", "cites": [4270, 4261], "cite_extract_rate": 0.25, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a general analytical overview of how hands are used in activity recognition, referencing Nguyen et al. to categorize approaches and integrating insights from both action-based and segmentation-based methods. It connects ideas across sources and identifies a reliance on temporal information. However, while it points to a limitation (lack of experiments with hand cues alone for ADLs), the critical analysis is limited in depth and does not systematically evaluate or contrast the cited works."}}
{"id": "266bd0ca-9a83-4f4a-b8a5-7d42f9e67711", "title": "Remarks on action/interaction and activity recognition", "level": "subsubsection", "subsections": [], "parent_id": "d10c8f97-0c05-48b1-b5bf-aa095e551ca2", "prefix_titles": [["title", "Analysis of the hands in egocentric vision:\\\\ A survey"], ["section", "Interpretation"], ["subsection", "Action/interaction and activity recognition"], ["subsubsection", "Remarks on action/interaction and activity recognition"]], "content": "\\label{subsubsec:actionActivityRemarks}\nMany authors demonstrated that action/interaction recognition performance can be improved by combining different cues, such as hand, object, and motion information. This was proven regardless of the actual method. In fact, both feature-based and deep-learning based methods implemented this strategy by combining multiple features or using multi-stream deep-learning approaches. Recently, multi-task learning approaches have also been proposed for solving the action recognition problem , demonstrating that predicting hand coordinates as an auxiliary task leads to an improvement in verb recognition performance with respect to the single-task approach. In the near future, it will be interesting to compare multi-task and multi-stream architectures to understand whether the joint prediction of action labels and hand positions can actually provide state-of-the-art performance in one or both tasks.\nAnother important aspect on which one should focus when developing novel approaches for action/interaction recognition is the temporal information. This was exploited by using 3DCNN and RNNs or, in case of feature-based approaches, by encoding it in the motion information. The same conclusion can be drawn for activity recognition where, considering the longer duration of the events, the temporal information becomes even more important .\nSometimes the literature is not consistent on the choice of the taxonomy to describe these sub-areas. Some of the approaches summarized above, even though not explicitly referred as action/interaction recognition, actually recognized short actions or interactions. We preferred to be consistent with the definition proposed by Tekin et al. , as we believe that a consistent taxonomy may help authors comparing different approaches and unify their efforts towards solving a specific problem. Moreover, the term “action” has often been used interchangeably with “activity”, which indicates a longer event with higher semantic content. The actions and interactions can rather be seen as the building blocks of the activities. This allowed some authors to exploit this mutual dependency, in order to infer activities in a hierarchical manner, using the methods described above .\nThe number of egocentric activity recognition approaches based on hand information is lower than the number of action and interaction recognition approaches. This difference is due to the fact that higher in the semantic content, authors have a wider choice of cues and features for recognizing a temporal event. In particular, over the past few years, more and more end-to-end approaches for activity recognition have been proposed, similarly to video recognition .", "cites": [4257, 8758], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes insights from multiple papers, particularly Paper 1 and Paper 2, to highlight trends in combining visual and motion cues for action/interaction recognition. It offers critical observations about taxonomy inconsistencies and the relative scarcity of hand-based activity recognition approaches. The abstraction level is strong, as it generalizes the role of temporal information and the relationship between actions and activities."}}
{"id": "5d6e752f-4696-44e4-a775-fe653f7e3245", "title": "Application", "level": "section", "subsections": ["7d8362b5-05f9-4ef1-b97f-8843021f46fe", "64defbc7-5678-446e-93db-a323cb379d13", "76b93dc9-1efe-4a29-a256-f54fb128cf09"], "parent_id": "08af7ca6-1a67-48c9-9735-3113b8e18727", "prefix_titles": [["title", "Analysis of the hands in egocentric vision:\\\\ A survey"], ["section", "Application"]], "content": "\\label{sec:application}\nThe hand-based approaches summarized so far can be implemented to design real-world FPV applications. Most of these applications relied on HCI and HRI and included AR and VR systems , robot control and learning , as well as healthcare applications .", "cites": [4260], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a minimal synthesis by only briefly listing the general domains where hand-based FPV approaches are applied, such as HCI, HRI, AR, VR, and healthcare. It does not integrate or connect specific insights from the cited paper or others. There is no critical analysis or evaluation of the works, and no abstraction to broader patterns or principles is attempted."}}
{"id": "64defbc7-5678-446e-93db-a323cb379d13", "title": "Robot control and learning", "level": "subsection", "subsections": [], "parent_id": "5d6e752f-4696-44e4-a775-fe653f7e3245", "prefix_titles": [["title", "Analysis of the hands in egocentric vision:\\\\ A survey"], ["section", "Application"], ["subsection", "Robot control and learning"]], "content": "In the HRI field, FPV hand-based approaches have mainly been used for two purposes: robot learning and robot control. Approaches for robot learning recognized movements and/or actions performed by the user's hands, in order to train a robot performing the same set of movements autonomously . Aksoy et al. , decomposed each manipulation into shorter chunks and encoded each manipulation into a semantic event chain (SEC), which encodes the spatial relationship between objects and hands in the scene. Each temporal transition in the SEC (e.g., change of state in the scene configuration) was considered as movement primitive for the robot imitation. In , the robot used the tracked hand locations of a human to learn the hand's future position and predict trajectories of the hands when a particular action has to be executed. By contrast, robot control approaches mainly relied on hand gesture recognition to give specific real-time commands to the robots . The hand gestures are seen as means of communication between the human and the robot and can encode specific commands such as the action to be performed by a robot arm  or the direction to be taken by a reconnaissance robot .", "cites": [4271], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a reasonable synthesis by grouping methods under two main purposes: robot learning and robot control. It connects the concept of hand tracking to specific robotic applications, such as trajectory prediction and gesture-based commands. However, it lacks deeper critical evaluation or comparison of the cited approaches, and while it identifies general trends, it does not offer a highly abstracted or novel framework."}}
{"id": "76b93dc9-1efe-4a29-a256-f54fb128cf09", "title": "Remote healthcare monitoring", "level": "subsection", "subsections": [], "parent_id": "5d6e752f-4696-44e4-a775-fe653f7e3245", "prefix_titles": [["title", "Analysis of the hands in egocentric vision:\\\\ A survey"], ["section", "Application"], ["subsection", "Remote healthcare monitoring"]], "content": "\\label{subsec:appHealth}\nEgocentric vision has demonstrated the potential to have an important impact in healthcare. The possibility to automatically analyze the articulated hand pose and recognize actions and ADLs have made these methods appealing for the remote assessment of upper limb functions  and AAL systems .\nThe assessment of upper limb function is an important phase in the rehabilitation after stroke or cSCI that allows clinicians to plan the optimal treatment strategy for each patient. However, geographical distances between patients and hospitals create barriers towards obtaining optimal assessments and rehabilitation outcomes. Egocentric vision has inspired researchers to develop video-based approaches for automatically studying hand functions at home . Studies have been conducted in individuals with cSCI, tackling the problem of hand function assessment from two perspectives: localization  and interpretation . Fine-tuning object detection algorithms to localize and recognize hands in people with SCI allowed developing hand localization approaches robust to impaired hand poses and uncontrolled situations . Moreover, strategies for improving the computational performance of hand detection algorithms have been adopted (e.g., combining hand detection and tracking), making this application suitable for the use at home. The automatic detection of hand-object manipulations allowed extracting novel measures reflective of the hand usage at home, such as number of interactions per hour, the duration of interactions, and the percentage of interaction over time . These measures, once validated against clinical scores, will help clinicians to better understand how individuals with cSCI and stroke use their hands at home while performing ADLs.\nAnother healthcare application is the development of AAL systems. The increasing ageing population is posing serious social and financial challenges in many countries. These challenges have stimulated the interest in developing technological solutions to help and support older adults with and without cognitive impairments during their daily life . Some of these applications used egocentric vision to provide help and support to older adults during ADLs at home . Egocentric vision AAL builds upon the action and activity recognition approaches illustrated in Section \\ref{subsec:actionActivity}. In particular, approaches have been proposed to automatically recognize how older adults perform ADLs at home, for example to detect early signs of dementia  or to support people in conducting the activities .\nIn these specific applications, the use of egocentric vision presents important advantages with respect to other solutions (e.g., sensor-based and third person vision):\n\\begin{itemize}[noitemsep, wide=0pt, leftmargin=\\dimexpr\\labelwidth + 2\\labelsep\\relax]\n    \\item FPV can provide high quality videos on how people manipulate objects. This is important when the aim is the recognition of hand-object manipulations and ADLs, since hand occlusions tend to be minimized.\n    \\item Egocentric vision provides more details of hand-object interactions than sensor-based technology, by capturing information about both the hand and the object being manipulated. Other sensor-based solutions such as sensor gloves, although providing highly accurate hand information, may limit movements and sensation, which are already reduced in individuals with upper limb impairment .\n\\end{itemize}", "cites": [4260], "cite_extract_rate": 0.1, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from the cited paper to build a narrative on how egocentric vision can support remote healthcare monitoring, particularly for cSCI and stroke patients. It also abstracts the key benefits of FPV over alternative methods like sensor-based systems. While it provides a critical comparison between FPV and other technologies, it does not deeply evaluate the cited works or identify significant limitations, keeping the analysis at a moderate level."}}
{"id": "c289979f-f5c8-4c7d-8017-03424c08e89e", "title": "FPV Datasets with hand annotation", "level": "section", "subsections": [], "parent_id": "08af7ca6-1a67-48c9-9735-3113b8e18727", "prefix_titles": [["title", "Analysis of the hands in egocentric vision:\\\\ A survey"], ["section", "FPV Datasets with hand annotation"]], "content": "\\label{sec:datasets}\n\\begin{table*}[]\n\\begin{center}\n\\begin{tabular}{|ccccccccccc|}\n\\hline\nDataset & Year &  Mode & Device & Location & Frames & Videos & Duration & Subjects & \\begin{tabular}[c]{@{}c@{}}Resolution\\\\ (pixels)\\end{tabular} & Annotation \\\\ \\hline\\hline\nGTEA  & 2011 & C & GoPro & H & $\\sim$31k & 28 & 34 min & 4 & 1280$\\times$720 & \\begin{tabular}[c]{@{}c@{}} act\\\\ msk\\end{tabular} \\\\ \\hline\nADL  & 2012 & C & GoPro & H & \\textgreater{}1M & 20 & $\\sim$10 h & 20 & 1280$\\times$960 & \\begin{tabular}[c]{@{}c@{}}act\\\\ obj\\end{tabular} \\\\ \\hline\nEDSH  & 2013 & C & - & H & $\\sim$20k & 3 & $\\sim$10 min & - & 1280$\\times$720 & msk\\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Interactive\\\\ Museum \\end{tabular} & 2014 & C & - & H & - & 700 & - & 5 & 800$\\times$450 & \\begin{tabular}[c]{@{}c@{}}gst\\\\ msk\\end{tabular} \\\\ \\hline\nEgoHands  & 2015 & C & Google Glass & H & $\\sim$130k & 48 & 72 min & 8 & 1280$\\times$720 & msk\\\\ \\hline\nMaramotti  & 2015 & C & - & H & - & 700 & - & 5 & 800$\\times$450 & \\begin{tabular}[c]{@{}c@{}}gst\\\\ msk\\end{tabular} \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}UNIGE\\\\ Hands \\end{tabular} & 2015 & C & \\begin{tabular}[c]{@{}c@{}}GoPro\\\\ Hero3+\\end{tabular} & H & $\\sim$150k & - & 98 min & - & 1280$\\times$720 & det\\\\ \\hline\nGUN-71  & 2015 & CD & \\begin{tabular}[c]{@{}c@{}}Creative\\\\ Senz3D\\end{tabular} & C & \\begin{tabular}[c]{@{}c@{}}$\\sim$12k\\\\ (annotated)\\end{tabular} & - & - & 8 & - & grs\\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}RGBD\\\\ Egocentric\\\\ Action \\end{tabular} & 2015 & CD & \\begin{tabular}[c]{@{}c@{}}Creative\\\\ Senz3D\\end{tabular} & H & - & - & - & 20 & \\begin{tabular}[c]{@{}c@{}}C:640$\\times$480\\\\ D:320$\\times$240\\end{tabular} & act\\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Fingerwriting\\\\ in mid-air \\end{tabular} & 2016 & CD & \\begin{tabular}[c]{@{}c@{}}Creative\\\\ Senz3D\\end{tabular} & H & $\\sim$8k & - & - & - & - & \\begin{tabular}[c]{@{}c@{}}ftp\\\\ gst\\end{tabular} \\\\ \\hline\nEgo-Finger  & 2016 & C & - & H & $\\sim$93k & 24 & - & 24 & 640$\\times$480 & \\begin{tabular}[c]{@{}c@{}}det\\\\ ftp\\end{tabular} \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}ANS\\\\ able-\\\\ bodied \\end{tabular} & 2016 & C & Looxcie 2 & - & - & - & 44 min & 4 & 640$\\times$480 & int\\\\ \\hline\nUT Grasp  & 2017 & C & \\begin{tabular}[c]{@{}c@{}}GoPro\\\\ Hero2\\end{tabular} & H & - & 50 & $\\sim$4 h & 5 & 960$\\times$540 & grs\\\\ \\hline\nGestureAR  & 2017 & C & \\begin{tabular}[c]{@{}c@{}}Nexus 6 and\\\\ Moto G3\\end{tabular} & H & - & 100 & - & 8 & 1280$\\times$720 & gst\\\\ \\hline\nEgoGesture  & 2017 & C & - & - & $\\sim$59k & - & - & - & - & \\begin{tabular}[c]{@{}c@{}}det\\\\ ftp\\\\ gst\\end{tabular} \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Egocentric\\\\ hand-action \\end{tabular} & 2017 & D & \\begin{tabular}[c]{@{}c@{}}Softkinetic\\\\ DS325\\end{tabular} & H & $\\sim$154k & 300 & - & 26 & 320$\\times$240 & gst\\\\ \\hline\nBigHand2.2M  & 2017 & D & \\begin{tabular}[c]{@{}c@{}}Intel\\\\ RealSense\\\\ SR300\\end{tabular} & - & $\\sim$290k & - & - & - & 640$\\times$480 & pos\\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Desktop\\\\ Action \\end{tabular} & 2018 & C & \\begin{tabular}[c]{@{}c@{}}GoPro\\\\ Hero2\\end{tabular} & H & $\\sim$324k & 60 & 3 h & 6 & 1920$\\times$1080 & \\begin{tabular}[c]{@{}c@{}}act\\\\ msk\\end{tabular} \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Epic\\\\ Kitchens \\end{tabular} & 2018 & C & GoPro & H & ~11.5M & - & 55 h & 32 & 1920$\\times$1080 & act\\\\ \\hline\nFPHA  & 2018 & CD & \\begin{tabular}[c]{@{}c@{}}Intel\\\\ RealSense\\\\ SR300\\end{tabular} & S & \\begin{tabular}[c]{@{}c@{}}$\\sim$105k\\\\   (annotated)\\end{tabular} & 1,175 & - & 6 & \\begin{tabular}[c]{@{}c@{}}C:1920$\\times$1080\\\\D:640$\\times$480\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}act\\\\ pos\\end{tabular} \\\\ \\hline\nEYTH  & 2018 & C & - & - & \\begin{tabular}[c]{@{}c@{}}1,290\\\\ (annotated)\\end{tabular} & 3 & - & - & - & msk\\\\ \\hline\nEGTEA+  & 2018 & C & \\begin{tabular}[c]{@{}c@{}}SMI wearable\\\\ eye-tracker\\end{tabular} & H & \\textgreater{}3M & 86 & $\\sim$28 h & 32 & 1280$\\times$960 & \\begin{tabular}[c]{@{}c@{}}act\\\\ gaz\\\\ msk\\end{tabular} \\\\ \\hline\nTHU-READ  & 2018 & CD & \\begin{tabular}[c]{@{}c@{}}Primesense\\\\ Carmine\\end{tabular} & H & $\\sim$343k & 1,920 & - & 8 & 640$\\times$480 & \\begin{tabular}[c]{@{}c@{}}act\\\\ msk\\end{tabular} \\\\ \\hline\nEgoGesture  & 2018 & CD & \\begin{tabular}[c]{@{}c@{}}Intel\\\\ RealSense\\\\ SR300\\end{tabular} & H & $\\sim$3M & 24,161 & - & 50 & 640$\\times$480 & gst\\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}EgoDaily \\end{tabular} & 2019 & C & \\begin{tabular}[c]{@{}c@{}}GoPro\\\\ Hero5\\end{tabular} & - & $\\sim$50k & 50 & - & 10 & 1920$\\times$1080 & \\begin{tabular}[c]{@{}c@{}}det\\\\ hid\\end{tabular} \\\\ \\hline\nANS SCI  & 2019 & C & \\begin{tabular}[c]{@{}c@{}}GoPro\\\\ Hero4\\end{tabular} & H & - & - & - & 17 & 1920$\\times$1080 & \\begin{tabular}[c]{@{}c@{}}det\\\\ int\\end{tabular} \\\\ \\hline\nKBH  & 2019 & C & HTC Vive & H & \\begin{tabular}[c]{@{}c@{}}$\\sim$12.5k\\\\ (annotated)\\end{tabular} & 161 & - & 50 & 230$\\times$306 & msk\\\\ \\hline\n\\end{tabular}\n\\end{center}\n    \\caption{List of available datasets with hand-based annotations in FPV. Image modality (Mode): Color (C); Depth (D); Color+Depth (CD). Camera location: Head (H); Chest (C); Shoulder (S). Annotation: actions/activities (\\textbf{act}); hand presence and location (\\textbf{det}); fingertip positions (\\textbf{ftp}); gaze (\\textbf{gaz}); grasp types (\\textbf{grs}); hand gestures (\\textbf{gst}); hand disambiguation (\\textbf{hid}); hand-object interactions (\\textbf{int}); hand segmentation masks (\\textbf{msk}); object classes (\\textbf{obj}); hand pose (\\textbf{pos}).}\n\\label{tab_datasets}\n\\end{table*}\nThe importance that this field of research gained in recent years is clear when we look at the number of available datasets published since 2015 (Table \\ref{tab_datasets}). Although the type of information and ground truth annotations made available by the authors is heterogeneous, it is possible to identify some sub-areas that are more recurrent than others. The vast majority of datasets provided hand segmentation masks , reflecting the high number of approaches proposed in this area and summarized in Section \\ref{sec:localization}. However, the high number of datasets is counterbalanced by a relative low number of annotated frames, usually in the order of a few hundreds or thousands of images. To expedite the lenghty pixel-level annotation process and build larger datasets for hand segmentation, some authors proposed semi-automated techniques, for example based on Grabcut .\nActions/activities  and hand gestures  are other common information that were captured and annotated in many datasets. This large amount of data has been used by researchers for developing robust HCI applications that relied on hand gestures. Compared to the amount of hand segmentation masks, action/activities and hand gestures datasets are usually larger, since the annotation process is easier and faster than pixel-level segmentation.\nThe vast majority of datasets included color information recorded from head-mounted cameras. The head position is usually preferred over the chest or shoulders, since it is easier to focus on hand actions and manipulations whenever the camera wearer's is performing a specific activity. GoPro cameras were the most widely used devices for recording the videos, since they are specifically designed for egocentric POV and are readily available on the market. Few datasets, usually designed for hand pose estimation , hand gesture recognition , and action/activity recognition , include depth or color and depth information. In most cases, videos were collected using Creative$^{\\circledR}$ Senz3D\\textsuperscript{TM} or Intel$^{\\circledR}$ RealSense\\textsuperscript{TM}  SR300 depth sensors, as these devices were small and lightweight. Moreover, these cameras were preferred over other depth sensors (e.g., Microsoft$^{\\circledR}$ Kinect\\textsuperscript{TM}) because they were originally developed for natural user interface that made them more suitable for studying hand movements in the short range (i.e., up to 1 m of distance from the camera).\nAlthough FPV is gaining a lot of interest for developing healthcare applications for remote monitoring of patients living in the community, only one dataset (i.e., the ANS-SCI dataset ) included videos from people with clinical conditions such as cSCI. This lack of available data is mainly due to ethical constraints that make it harder to share videos and images collected from people with diseases or clinical conditions. In the next few years researchers should try -- within the ethical and privacy constraints -- to build and share datasets for healthcare applications including videos collected from patients. This will benefit the robustness of the hand-based approaches in FPV against the inter-group and within group variability that can be encountered in many clinical conditions.", "cites": [4261, 4255, 4268, 4256, 4260, 4272, 2906], "cite_extract_rate": 0.25, "origin_cites_number": 28, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple datasets and connects them to broader research areas like hand localization, gesture recognition, and action analysis. It identifies patterns in the types of annotations and the use of specific devices. However, it lacks deeper critical evaluation of the cited papers and limitations in the datasets, focusing more on descriptive and comparative elements."}}
