{"id": "aed28b61-7567-46ed-896a-a6d1ddec0f8c", "title": "Introduction", "level": "section", "subsections": ["a6c42c1c-9b1a-4227-8773-697a4f5c0acd", "eb4b2e52-eb80-4448-b71f-5daaed837f6b"], "parent_id": "69f98c5d-4a32-4911-97b2-4b657d88f671", "prefix_titles": [["title", "A Survey of Deep Active Learning"], ["section", "Introduction"]], "content": "Both deep learning (DL) and active learning (AL) are a subfield of machine learning. DL is also called representation learning . It originates from the study of artificial neural networks and realizes the automatic extraction of data features. DL has strong learning capabilities due to its complex structure, but this also means that DL requires a large number of labeled samples to complete the corresponding training. With the release of a large number of large-scale data sets with annotations and the continuous improvement of computer computing power, DL-related research has ushered in large development opportunities. Compared with traditional machine learning algorithms, DL has an absolute advantage in performance in most application areas. \nAL focuses on the study of data sets, and it is also known as query learning . AL assumes that different samples in the same data set have different values for the update of the current model, and tries to select the samples with the highest value to construct the training set. Then, the corresponding learning task is completed with the smallest annotation cost. Both DL and AL have important applications in the machine learning community. Due to their excellent characteristics, they have attracted widespread research interest in recent years. More specifically, DL has achieved unprecedented breakthroughs in various challenging tasks; however, this is largely due to the publication of massive labeled datasets . Therefore, DL is limited by the high cost of sample labeling in some professional fields that require rich knowledge. In comparison, an effective AL algorithm can theoretically achieve exponential acceleration in labeling efficiency . This large potential saving in labeling costs is a fascinating development. However, the classic AL algorithm also finds it difficult to handle high-dimensional data . Therefore, the combination of DL and AL, referred to as DeepAL, is expected to achieve superior results. DeepAL has been widely utilized in various fields, including image recognition , text classification , visual question answering  and object detection , etc. Although a rich variety of related work has been published, DeepAL still lacks a unified classification framework. To fill this gap, in this article, we will provide a comprehensive overview of the existing DeepAL related work \\footnote{We search about 270 related papers on \\href{https://dblp.uni-trier.de/}{DBLP} using \"deep active learning\" as the keyword. We review the relevance of these papers to DeepAL one by one, eliminate irrelevant (just containing a few keywords) or information missing papers, and manually add some papers that do not contain these keywords but use DeepAL-related methods or relate to our current discussion. Finally, the survey references are constructed. The latest paper is updated to November 2020. The references include 103 conference papers, 153 journal papers, 3 books , 1 research report , and 1 dissertation . There are 28 unpublished papers.}, along with a formal classification method. The contributions of this survey are summarized as follows:\n\\begin{itemize}\n    \\item As far as we know, this is the first comprehensive review work in the field of deep active learning.\n    \\item We analyze the challenges of combining active learning and deep learning, and systematically summarize and categorize existing DeepAL-related work for these challenges.\n    \\item We conduct a comprehensive and detailed analysis of DeepAL-related applications in various fields and future directions.\n\\end{itemize}\nNext, we first briefly review the development status of DL and AL in their respective fields. Subsequently, in Section \\ref{sec:The necessity and challenge of combining DL and AL}, the necessity and challenges of combining DL and AL are explicated. In Section \\ref{sec: Deep Active Learning}, we conduct a comprehensive and systematic summary and discussion of the various strategies used in DeepAL. In Section \\ref{sec: Various applications of DeepAL}, we review various applications of DeepAL in detail. In Section \\ref{sec: Discussion and future directions}, we conduct a comprehensive discussion on the future direction of DeepAL. Finally, in Section \\ref{sec: Summary and conclusions}, we make a summary and conclusion of this survey.", "cites": [5507, 1044, 5511, 4023, 5509, 5508, 5510, 5506], "cite_extract_rate": 0.42105263157894735, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a general overview of Deep Active Learning (DeepAL), integrating key themes from the cited papers, such as the high annotation cost in deep learning and the potential of AL to improve efficiency. It connects ideas across different application domains (e.g., VQA, 3D object detection, text classification) and identifies the necessity and challenges of combining DL and AL. However, the analysis remains largely at a surface level, with limited in-depth comparison or evaluation of the cited works, and the abstraction to broader principles or trends is not fully developed."}}
{"id": "a6c42c1c-9b1a-4227-8773-697a4f5c0acd", "title": "Deep Learning", "level": "subsection", "subsections": [], "parent_id": "aed28b61-7567-46ed-896a-a6d1ddec0f8c", "prefix_titles": [["title", "A Survey of Deep Active Learning"], ["section", "Introduction"], ["subsection", "Deep Learning"]], "content": "DL attempts to build appropriate models by simulating the structure of the human brain. The McCulloch-Pitts (MCP) model proposed in 1943 by  is regarded as the beginning of modern DL. Subsequently, in 1986,  introduced backpropagation into the optimization of neural networks, which laid the foundation for the subsequent rapid development of DL. In the same year, Recurrent Neural Networks (RNNs)  were first proposed. In 1998, the LeNet  network made its first appearance, representing one of the earliest uses of deep neural networks (DNN). However, these pioneering early works were limited by the computing resources available at the time and did not receive as much attention and investigation as they should have . In 2006, Deep Belief Networks (DBNs)  were proposed and used to explore a deeper range of networks, which prompted the name of neural networks as DL. \nAlexNet  is considered the first CNN deep learning model, which greatly improves the image classification results on large-scale data sets (such as ImageNet). In the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC)-2012 competition , the AlexNet  won the championship in the top-5 test error rate by nearly 10\\% ahead of the second place. AlexNet uses the ReLUs (Rectified Linear Units)  activation function to effectively suppress the gradient disappearance problem, while the use of multiple GPUs greatly improves the training speed of the model. \nSubsequently, DL began to win championships in various competitions and demonstrated very competitive results in many fields, such as visual data processing, natural language processing, speech processing, and many other well-known applications . From the perspective of automation, the emergence of DL has transformed the manual design of features  in machine learning to facilitate automatic extraction . It is precisely because of this powerful automatic feature extraction capability that DL has demonstrated such unprecedented advantages in many fields. \nAfter decades of development, the research work related to DL is very rich. In Fig.\\ref{fig:DL}, we present a standard deep learning model example: convolutional neural network (CNN) . Based on this approach, similar CNNs are applied to various image processing tasks. In addition, RNNs and GANs (Generative Adversarial Networks)  are also widely utilized. Beginning in 2017, DL gradually shifted from the initial feature extraction automation to the automation of model architecture design ; however, this still has a long way to go. \n\\begin{figure}[!tp] \n\t\\centering \n\t\\subfloat[Structure diagram of convolutional neural network.] \n \t{\\centering \n \t\t\\includegraphics[width = 0.45\\textwidth]{figure/DL}\n \t\t\\label{fig:DL}\n \t}\\hspace{5mm}\n\t\\subfloat[The pool-based active learning cycle.] \n \t{\\centering \n \t\t\\includegraphics[width = 0.45\\textwidth]{figure/AL}\n \t\t\\label{fig:AL}\n \t}\\\\\n\t\\subfloat[A typical example of deep active learning.] \n \t{\\centering \n \t\t\\includegraphics[width = 0.95\\textwidth]{figure/DAL}\n \t\t\\label{fig:DeepAL}\n \t}\n\t\\caption{Comparison of typical architectures of DL, AL, and DeepAL. (a) A common DL model: Convolutional Neural Network. (b)The pool-based AL cycle: Use the query strategy to query the sample in the unlabeled pool $U$ and hand it over to the oracle for labeling, then add the queried sample to the labeled training dataset $L$ and train, and then use the newly learned knowledge for the next round of querying. Repeat this process until the label budget is exhausted or the pre-defined termination conditions are reached. (c) A typical example of DeepAL: The parameters $\\theta$ of the DL model are initialized or pre-trained on the labeled training set $L_0$, and the samples of the unlabeled pool $U$ are used to extract features through the DL model. Select samples based on the corresponding query strategy, and query the label in querying to form a new label training set $L$, then train the DL model on $L$, and update $U$ at the same time. Repeat this process until the label budget is exhausted or the pre-defined termination conditions are reached (see Section \\ref{sec: DeepAL Stopping Strategy} for stopping strategy details).} \n\t\\label{fig:AL_DL_DAL} \n\\end{figure}\nThanks to the publication of a large number of existing annotation datasets , in recent years, DL has made breakthroughs in various fields including machine translation , speech recognition , and image classification . However, this comes at the cost of a large number of manually labeled datasets, and DL has a strong greedy attribute to the data. While, in the real world, obtaining a large number of unlabeled datasets is relatively simple, the manual labeling of datasets comes at a high cost; this is particularly true for those fields where labeling requires a high degree of professional knowledge . For example, the labeling and description of lung lesion images of COVID-19 patients requires experienced clinicians to complete, and it is clearly impractical to demand that such professionals complete a large amount of medical image labeling. Similar fields also include speech recognition , medical imaging , recommender\nsystems , information extraction , satellite remote sensing  and robotics , machine translation  and text classification , etc. Therefore, a way of maximizing the performance gain of the model when annotating a small number of samples is urgently required.", "cites": [684, 5515, 5514, 2909, 514, 5512, 166, 8923, 4023, 5516, 7950, 7948, 7217, 4507, 1496, 97, 5513, 5509, 7300, 7949, 7584, 8924], "cite_extract_rate": 0.41509433962264153, "origin_cites_number": 53, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a chronological and factual overview of deep learning's development, citing various papers to highlight its applications and advancements. However, it lacks synthesis of key themes across the cited works and offers minimal critical evaluation or abstraction beyond specific examples. The focus remains primarily on describing DL and its implications for annotation costs without deeper analysis."}}
{"id": "eb4b2e52-eb80-4448-b71f-5daaed837f6b", "title": "Active Learning", "level": "subsection", "subsections": [], "parent_id": "aed28b61-7567-46ed-896a-a6d1ddec0f8c", "prefix_titles": [["title", "A Survey of Deep Active Learning"], ["section", "Introduction"], ["subsection", "Active Learning"]], "content": "AL is just such a method dedicated to studying how to obtain as many performance gains as possible by labeling as few samples as possible. More specifically, it aims to select the most useful samples from the unlabeled dataset and hand it over to the oracle (e.g., human annotator) for labeling, to reduce the cost of labeling as much as possible while still maintaining performance. AL approaches can be divided into membership query synthesis , stream-based selective sampling  and pool-based  AL from application scenarios . Membership query synthesis means that the learner can request to query the label of any unlabeled sample in the input space, including the sample generated by the learner. Moreover, the key difference between stream-based selective sampling and pool-based sampling is that the former makes an independent judgment on whether each sample in the data stream needs to query the labels of unlabeled samples, while the latter chooses the best query sample based on the evaluation and ranking of the entire dataset. Related research on stream-based selective sampling is mainly aimed at the application scenarios of small mobile devices that require timeliness, because these small devices often have limited storage and computing capabilities. The more common pool-based sampling strategy in the paper related to AL research is more suitable for large devices with sufficient computing and storage resources. In Fig.\\ref{fig:AL}, we illustrate the framework diagram of the pool-based active learning cycle. In the initial state, we can randomly select one or more samples from the unlabeled pool $U$, give this sample to the oracle query label to get the labeled dataset $L$, and then train the model on $L$ using supervised learning. Next, we use this new knowledge to select the next sample to be queried, add the newly queried sample to $L$, and then conduct training. This process is repeated until the label budget is exhausted or the pre-defined termination conditions are reached (see Section \\ref{sec: DeepAL Stopping Strategy} for stopping strategy details).\nIt is different from DL by using manual or automatic methods to design models with high-performance feature extraction capabilities. AL starts with datasets, primarily through the design of elaborate query rules to select the best samples from unlabeled datasets and query their labels, in an attempt to reduce the labeling cost to the greatest extent possible. Therefore, the design of query rules is crucial to the performance of AL methods. Related research is also quite rich. For example, in a given set of unlabeled datasets, the main query strategies include the uncertainty-based approach , diversity-based approach  and expected model change . In addition, many works have also studied hybrid query strategies , taking into account the uncertainty and diversity of query samples, and attempting to find a balance between these two strategies. Because separate sampling based on uncertainty often results in sampling bias , the currently selected sample is not representative of the distribution of unlabeled datasets. On the other hand, considering only strategies that promote diversity in sampling may lead to increased labeling costs, as may be a considerable number of samples with low information content will consequently be selected. More classic query strategies are examined in . Although there is a substantial body of existing AL-related research, AL still faces the problem of expanding to high-dimensional data (e.g., images, text, and video, etc.) ; thus, most AL works tend to concentrate on low-dimensional problems . In addition, AL often queries high-value samples based on features extracted in advance and does not have the ability to extract features.", "cites": [1044, 5517, 8716, 8812, 5518, 5519], "cite_extract_rate": 0.2222222222222222, "origin_cites_number": 27, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of active learning by discussing key query strategies and their trade-offs, and it connects the cited works to broader issues like scalability and feature extraction in deep learning. It integrates ideas across multiple papers to explain the strengths and limitations of current approaches. However, while it highlights challenges, it does not offer a deeply critical or novel synthesis of the field, nor does it generalize to a high-level theoretical framework."}}
{"id": "504a0a59-0bf6-4525-8c0b-ff9673369a89", "title": "The necessity and challenge of combining DL and AL", "level": "section", "subsections": [], "parent_id": "69f98c5d-4a32-4911-97b2-4b657d88f671", "prefix_titles": [["title", "A Survey of Deep Active Learning"], ["section", "The necessity and challenge of combining DL and AL"]], "content": "\\label{sec:The necessity and challenge of combining DL and AL}\nDL has a strong learning capability in the context of high-dimensional data processing and automatic feature extraction, while AL has significant potential to effectively reduce labeling costs. Therefore, an obvious approach is to combine DL and AL, as this will greatly expand their application potential. This combined approach, referred to as DeepAL, was proposed by considering the complementary advantages of the two methods, and researchers have high expectations for the results of studies in this field. However, although AL-related research on query strategy is quite rich, it is still quite difficult to apply this strategy directly to DL. This is mainly due to:\n\\begin{itemize}\n\\item \\textbf{Model uncertainty in Deep Learning.} The query strategy based on uncertainty is an important direction of AL research. In classification tasks, although DL can use the softmax layer to obtain the probability distribution of the label, the facts show that they are too confident. The SR (Softmax Response)  of the final output is unreliable as a measure of confidence, and the performance of this method will thus be even worse than that of random sampling .\n\\item \\textbf{Insufficient data for labeled samples.} AL often relies on a small amount of labeled sample data to learn and update the model, while DL is often very greedy for data . The labeled training samples provided by the classic AL method thus insufficient to support the training of traditional DL. In addition, the one-by-one sample query method commonly used in AL is also not applicable in the DL context .\n\\item \\textbf{Processing pipeline inconsistency.} The processing pipelines of AL and DL are inconsistent. Most AL algorithms focus primarily on the training of classifiers, and the various query strategies utilized are largely based on fixed feature representations. In DL, however, feature learning and classifier training are jointly optimized. Only fine-tuning the DL models in the AL framework, or treating them as two separate problems, may thus cause divergent issues .\n\\end{itemize}\nTo address the first problem, some researchers have applied Bayesian deep learning  to deal with the high-dimensional mini-batch samples with fewer queries in the AL context , thereby effectively alleviating the problem of the DL model being too confident about the output results.\nTo solve the problem of insufficient labelled sample data, researchers have considered using generative networks for data augmentation  or assigning pseudo-labels to high-confidence samples to expand the labeled training set . Some researchers have also used labeled and unlabeled datasets to combine supervised and semisupervised training across AL cycles . In addition, the empirical research in  shows that the previous heuristic-based AL  query strategy is invalid when it is applied to DL in batch settings; therefore, for the one-by-one query strategy in classic AL, many researchers focus on the improvement of the batch sample query strategy , taking both the amount of information and the diversity of batch samples into account.\nFurthermore, to deal with the pipeline inconsistency problem, researchers have considered modifying the combined framework of AL and DL to make the proposed DeepAL model as general as possible, an approach that can be extended to various application fields. This is of great significance to the promotion of DeepAL. For example,  embeds the idea of AL into DL and consequently proposes a task-independent architecture design.", "cites": [1044, 5521, 5517, 4641, 1042, 5523, 1050, 5520, 4618, 5518, 5522], "cite_extract_rate": 0.6875, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key challenges in combining AL and DL by integrating multiple cited works, highlighting common themes such as model overconfidence and pipeline mismatches. It also offers critical insights, such as the limitations of traditional AL strategies in DL settings and how recent approaches like BADGE and Bayesian methods address these. The abstraction is strong, as it generalizes the issues and solutions into broader categories like uncertainty, data sufficiency, and pipeline design."}}
{"id": "0d6ad266-3e42-4732-a729-183187f8427f", "title": "Query Strategy Optimization in DeepAL", "level": "subsection", "subsections": ["f812f3b9-8493-4d5c-9abd-f0111976583e", "7e186f19-788f-4246-9ae0-6cbed00c6a9e", "f610f09b-72e3-4867-aa7a-fd61d236efa2", "fb0f4ba8-e10b-45d4-bec6-9cbf53f668c0", "576fbe37-bef7-4cc1-9263-b1e9c6892881"], "parent_id": "3de3bdd7-51c6-4365-9c87-09efb8923a32", "prefix_titles": [["title", "A Survey of Deep Active Learning"], ["section", "Deep Active Learning"], ["subsection", "Query Strategy Optimization in DeepAL"]], "content": "\\label{sec: Query Strategy Optimization in DeepAL}\nIn the pool-based method, we define $U^n=\\{\\mathcal{X},\\mathcal{Y}\\}$ as an unlabeled dataset with $n$ samples; here, $\\mathcal{X}$ is the sample space, $\\mathcal{Y}$ is the label space, and $P(x,y)$ is a potential distribution, where $x\\in \\mathcal{X},y\\in \\mathcal{Y}$. $L^m=\\{X,Y\\}$ is the current labeled training set with $m$ samples, where $\\mathrm{x}\\in X,\\mathrm{y}\\in Y$. Under the standard supervision environment of DeepAL, our main goal is to design a query strategy $Q$, $U^n\\stackrel{Q}{\\longrightarrow}L^m$, using the deep model $f\\in \\mathcal{F },f:\\mathcal{X}\\rightarrow\\mathcal{Y}$. The optimization problem of DeepAL in a supervised environment can be expressed as follows:\n\\begin{equation}\n\\mathop{\\arg\\min}_{L^m\\subseteq U^n, (\\mathrm{x,y}) \\in L^m, (x,y) \\in U^n} \\mathbb{E}_{(x,y)}[\\ell(f(\\mathrm{x}),\\mathrm{y})],\n\\end{equation}\nwhere $\\ell(\\cdot)\\in \\mathcal{R}^+$ is the given loss equation, and we expect that $m\\ll n$. Our goal is to make $m$ as small as possible while ensuring a predetermined level of accuracy. Therefore, the query strategy $Q$ in DeepAL is crucial to reduce the labeling cost.\nNext, we will conduct a comprehensive and systematic review of DeepAL's query strategy from the following five aspects.\n\\begin{itemize}\n    \\item \\textit{Batch Mode DeepAL (BMDAL).} The batch-based query strategy is the foundation of DeepAL. The one-by-one sample query strategy in traditional AL is inefficient and not applicable to DeepAL, so it is replaced by batch-based query strategy.\n    \\item \\textit{Uncertainty-based and Hybrid Query Strategies.} Uncertainty-based query strategy refers to the model based on sample uncertainty ranking to select the sample to be queried. The greater the uncertainty of the sample, the easier it is to be selected. However, this is likely to ignore the relationship between samples. Therefore, the method that considers multiple sample attributes is called the hybrid query strategy.\n    \\item \\textit{Deep Bayesian Active Learning (DBAL).} Active learning based on Bayesian convolutional neural network  is called deep Bayesian active learning.\n    \\item \\textit{Density-based Methods.} The density-based method is a query strategy that attempts to find a core subset  representing the distribution of the entire dataset from the perspective of the dataset to reduce the cost of annotation.\n    \\item \\textit{Automated Design of DeepAL.} Automated design of DeepAL refers to a method that uses automated methods to design AL query strategies or DL models that have an important impact on DeepAL performance.\n\\end{itemize}", "cites": [5524, 4618], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section introduces query strategies in Deep Active Learning and lists five categories, but it lacks synthesis of the cited papers into a unified narrative. It briefly mentions the relevance of Bayesian CNNs and density-based methods but does not elaborate on how these ideas interrelate or build upon each other. There is minimal critical evaluation or abstraction beyond the individual strategies."}}
{"id": "f812f3b9-8493-4d5c-9abd-f0111976583e", "title": "Batch Mode DeepAL (BMDAL)", "level": "subsubsection", "subsections": [], "parent_id": "0d6ad266-3e42-4732-a729-183187f8427f", "prefix_titles": [["title", "A Survey of Deep Active Learning"], ["section", "Deep Active Learning"], ["subsection", "Query Strategy Optimization in DeepAL"], ["subsubsection", "Batch Mode DeepAL (BMDAL)"]], "content": "\\label{sec:Batch Mode DeepAL (BMDAL)}\nThe main difference between DeepAL and classical AL is that DeepAL uses batch-based sample querying. In traditional AL, most algorithms use a one-by-one query method, which leads to frequent training of the learning model but little change in the training data. The training set obtained by this query method is not only inefficient in the training of the DL model, but can also easily lead to overfitting. Therefore, it is necessary to investigate BMDAL in more depth. In the context of BMDAL, at each acquisition step, we score a batch of candidate unlabeled data samples $\\mathcal{B}=\\{x_1,x_2,...,x_b\\}\\subseteq U$ based on the acquisition function $a$ used  and the deep model $f_{\\theta}(L)$ trained on $L$, to select a new batch of data samples $\\mathcal{B}^*=\\{x_1^ *,x_2^*,...,x_b^*\\}$. This problem can be formulated as follows:\n\\begin{equation}\n\\mathcal{B}^*= \\mathop{\\arg\\max}_{\\mathcal{B}\\subseteq U} a_{batch}(\\mathcal{B},f_{\\theta}(L)),\n\\end{equation}\nwhere $L$ is labeled training set. In order to facilitate understanding, we also use $ D_{train}$ to represent the labeled training set. \n\\begin{figure}[!tp] \n\t\\centering \n\t\\subfloat[Batch query strategy considering only the amount of information.] \n \t{\\centering \n \t\t\\includegraphics[width = 0.4\\textwidth]{figure/One_by_one}\n \t\t\\label{fig:One_by_one}\n \t}\\hspace{8mm}\n\t\\subfloat[Batch query strategy considering both information volume and diversity.] \n \t{\\centering \n \t\t\\includegraphics[width = 0.4\\textwidth]{figure/Batch}\n \t\t\\label{fig:Batch}\n \t}\n\t\\caption{A comparison diagram of two batch query strategies, one that only considers the amount of information and one that considers both the amount and diversity of information. The size of the dots indicates the amount of information in the samples, while the distance between the dots represents the similarity between the samples. The points shaded in gray indicate the sample points to be queried in a batch.} \n\t\\label{fig:OneBatch} \n\\end{figure}\nA naive approach would be to continuously query a batch of samples based on the one-by-one strategy. For example,  adopts the method of batch acquisition and chooses BALD (Bayesian Active Learning by Disagreement)  to query top-$K$ samples with the highest scores. \nThe acquisition function $ a_{BALD} $ of this idea is expressed as follows:\n\\begin{equation}\n    \\label{eq: BALD}\n    \\begin{aligned}\n        &a_{\\mathrm{BALD}}\\left(\\left\\{x_{1}, \\ldots, x_{b}\\right\\}, \\mathcal{P}\\left(\\omega \\mid D_{train}\\right)\\right)=\\sum_{i=1}^{b} \\mathbb{I}\\left(y_{i} ; \\omega \\mid x_{i}, D_{train}\\right),\\\\\n        &\\mathbb{I}\\left(y ; \\boldsymbol{\\omega} \\mid x, D_{train}\\right)=\\mathbb{H}\\left(y \\mid x, D_{train}\\right)-\\mathbb{E}_{\\mathcal{P}\\left(\\boldsymbol{\\omega} \\mid D_{train}\\right)}\\left[\\mathbb{H}\\left(y \\mid x, \\boldsymbol{\\omega}, D_{train}\\right)\\right],\n    \\end{aligned}\n\\end{equation}\nwhere $ \\mathbb{I}\\left(y ; \\boldsymbol{\\omega}\\mid x, D_{train}\\right) $ used in BALD is to estimate the mutual information between model parameters and model predictions. The larger the mutual information value $\\mathbb{I}(*)$, the higher the uncertainty of the sample. The condition of $\\boldsymbol{\\omega}$ on $D_{train}$ indicates that the model has been trained with $D_{train}$. And $\\omega \\sim \\mathcal{P}\\left(\\omega \\mid D_{train}\\right)$ represents the model parameters of the current Bayesian model. $\\mathbb{H(*)}$ represents the entropy of the model prediction. $\\mathbb{E}[H(*)]$ is the expectation of the entropy of the model prediction over the posterior of the model parameters. Equation (\\ref{eq: BALD}) considers each sample independently and selects samples to construct a batch query dataset in a one-by-one way.\nClearly, however, this method is not feasible, as it is very likely to choose a set of information-rich but similar samples. The information provided to the model by such similar samples is essentially the same, which not only wastes labeling resources, but also makes it difficult for the model to learn genuinely useful information. In addition, this query method that considers each sample independently also ignores the correlation between samples. This is likely to lead to local decisions that make the batch sample set of queries insufficiently optimized. Therefore, how to simultaneously consider the correlation between different query samples is the primary problem for BMDAL. To solve the above problems, BatchBALD  expands BALD, which considers the correlation between data points by estimating the joint mutual information between multiple data points and model parameters. The acquisition function of BatchBALD can be expressed as follows:\n\\begin{equation}\n\\begin{aligned}\n    &a_{\\text {BatchBALD }}\\left(\\left\\{x_{1}, \\ldots, x_{b}\\right\\}, \\mathcal{P}\\left(\\omega \\mid D_ {train }\\right)\\right)=\\mathbb{I}\\left(y_{1}, \\ldots, y_{b} ; \\omega \\mid x_{1}, \\ldots, x_{b}, D_ {train }\\right),\\\\\n    &\\mathbb{I}\\left(y_{1: b} ; \\boldsymbol{\\omega} \\mid x_{1: b}, D_ {train }\\right)=\\mathbb{H}\\left(y_{1: b} \\mid x_{1: b}, D_ {train }\\right)-\\mathbb{E}_{\\mathcal{P}\\left(\\boldsymbol{\\omega} \\mid D_ {train }\\right)} \\mathbb{H}\\left(y_{1: b} \\mid x_{1: b}, \\boldsymbol{\\omega}, D_ {train }\\right),\n\\end{aligned}\n\\end{equation}\nwhere $x_{1}, \\ldots, x_{b}$ and $y_{1}, \\ldots, y_{b}$ are represented by joint random variables $x_{1: b}$ and $y_{1: b}$ in a product probability space, and $\\mathbb{I}\\left(y_{1: b} ; \\boldsymbol{\\omega} \\mid x_{1: b}, D_ {train }\\right)$ denotes the mutual information between these two random variables. BatchBALD considers the correlation between different query samples by designing an explicit joint mutual information mechanism to obtain a better query batch sample set. \nThe batch-based query strategy forms the basis of the combination of AL and DL, and related research on this topic is also very rich. We will provide a detailed overview and discussion of BMDAL query strategies in the following sections.", "cites": [8925, 4641, 8717], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a clear explanation of batch mode deep active learning (BMDAL), integrating key concepts from the cited papers, particularly BatchBALD. It synthesizes the idea of extending BALD to account for correlations between samples and explains the mathematical formulation effectively. While it identifies limitations of naive one-by-one query strategies and the value of joint mutual information, the critical evaluation remains somewhat high-level without deeper comparative analysis of alternative methods. The section also abstracts some principles, such as the importance of diversity and correlation in batch selection, but does not develop a novel overarching framework."}}
{"id": "7e186f19-788f-4246-9ae0-6cbed00c6a9e", "title": "Uncertainty-based and Hybrid Query Strategies", "level": "subsubsection", "subsections": [], "parent_id": "0d6ad266-3e42-4732-a729-183187f8427f", "prefix_titles": [["title", "A Survey of Deep Active Learning"], ["section", "Deep Active Learning"], ["subsection", "Query Strategy Optimization in DeepAL"], ["subsubsection", "Uncertainty-based and Hybrid Query Strategies"]], "content": "\\label{sec: Hybrid query strategy}\nBecause the uncertainty-based approach is simple in form and has low computational complexity, it is a very popular query strategy in AL. This query strategy is mainly used in certain shallow models (eg, SVM  or KNN ). This is mainly because the uncertainty of these models can be accurately obtained by traditional uncertainty sampling methods.\nIn uncertainty-based sampling, learners try to select the most uncertain samples to form a batch query set. For example, in the margin sampling , margin $M$ is defined as the difference between the predicted highest probability and the predicted second highest probability of an sample as follows: \n$M=P\\left(y_{1} \\mid x\\right)-P\\left(y_{2} \\mid x\\right),$\nwhere \\(y_1\\) and \\(y_2\\) are the first and second most probable labels predicted for the sample \\(x\\) under the current model. The smaller the margin $M$, the greater the uncertainty of the sample $x$. The AL algorithm selects the top-$K$ samples with the smallest margin $M$ as the batch query set by calculating the margin $M$ of all unlabeled samples.\nInformation entropy  is also a commonly used uncertainty measurement standard. For a $k$-class task, the information entropy $\\mathbb{E}(x)$ of sample $x$ can be defined as follows:\n\\begin{equation}\n\\label{eq: entropy}\n\\mathbb{E}(x)=-\\sum_{i=1}^{k} P(y_i\\mid x) \\cdot \\log \\left(P(y_i\\mid x)\\right),\n\\end{equation}\nwhere $P(y_i\\mid x)$ is the probability that the current sample $x$ is predicted to be class $y_i$. The greater the entropy of the sample, the greater its uncertainty. Therefore, the top-$K$ samples with the largest information entropy should be selected. More query strategies based on uncertainty can be found in .\nThere are many DeepAL  methods that directly utilize an uncertainty-based sampling strategy.\nHowever, DFAL (DeepFool Active Learning)  contends that these methods are easily fooled by adversarial examples; thus, it focuses on the study of examples near the decision boundary, and actively uses the information provided by these adversarial examples on the input spatial distribution in order to approximate their distance to the decision boundary. This adversarial query strategy can effectively improve the convergence speed of CNN training. \nNevertheless, as analyzed in Section \\ref{sec:Batch Mode DeepAL (BMDAL)}, this can easily lead to insufficient diversity of batch query samples (such that relevant knowledge regarding the data distribution is not fully utilized), which in turn leads to low or even invalid DL model training performance. \nA feasible strategy would thus be to use a hybrid query strategy in a batch query, taking into account both the information volume and diversity of samples in either an explicit or implicit manner.\nThe performance of early Batch Mode Active Learning (BMAL)  algorithms are often excessively reliant on the measurement of similarity between samples. In addition, these algorithms are often only good at exploitation (learners tend to focus only on samples near the current decision boundary, corresponding to high-information query strategies), meaning that the samples in the query batch sample set cannot represent the true data distribution of the feature space (due to the insufficient diversity of batch sample sets). To address this issue, Exploration-P  uses a deep neural network to learn the feature representation of the samples, then explicitly calculates the similarity between the samples. At the same time, the processes of exploitation and exploration (in the early days of model training, learners used random sampling strategies for exploration purposes) are balanced to enable more accurate measurement of the similarity between samples. \nMore specifically, Exploration-P uses the information entropy in Equation (\\ref{eq: entropy}) to estimate the uncertainty of sample $x$ under the current model. The uncertainty of the selected sample set $S$ can be expressed as $E(S) = \\sum_{x_i\\in S}\\mathbb{E}(x_i)$. Furthermore, to measure the redundancy between samples in the selected sample set $S$, Exploration-P uses $R(S)$ to represent the redundancy of selected sample set $S$:\n\\begin{equation}\n    R(S) = \\sum_{x_i\\in S}\\sum_{x_j\\in S}Sim(x_i,x_j),\\quad Sim(x_i,x_j) = f(x_i)\\mathcal{M}f(x_j),\n\\end{equation}\nwhere $f(x)$ represents the feature of sample $x$ extracted by deep learning model $f$, $Sim(x_i,x_j)$ measures the similarity between two samples, and $\\mathcal{M}$ is a similarity matrix (when $\\mathcal{M}$ is the identity matrix, the similarity of two samples is the product of their feature vectors. In addition, $\\mathcal{M}$ can also be learned as a parameter of $f$). Therefore, the selected sample set $S$ is expected to have the largest uncertainty and the smallest redundancy. For this reason, Exploration-P considers these two strategies, and the final goal equation is defined as:\n\\begin{equation}\n\\mathrm{I}(S)=E(S)-\\frac{\\alpha}{|S|} R(S),\n\\end{equation}\nwhere, $\\alpha$ is used to balance the weight of the hybrid query strategies, uncertainty and redundancy.\nMoreover, DMBAL (Diverse Mini-Batch Active Learning)  adds informativeness to the optimization goal of K-means by weight, and further presents an in-depth study of a hybrid query strategy that considers the sample information volume and diversity under the mini-batch sample query setting. DMBAL  can easily achieve expansion from the generalized linear model to DL; this not only increases the scalability of DMBAL  but also increases the diversity of active query samples in the mini-batch. Fig.\\ref{fig:OneBatch} illustrates a schematic diagram of this idea. This hybrid query strategy is quite popular.  For example, WI-DL (Weighted Incremental Dictionary Learning)  mainly considers the two stages of DBN. In the unsupervised feature learning stage, the key consideration is the representativeness of the data, while in the supervised fine-tuning stage, the uncertainty of the data is considered; these two indicators are then integrated, and finally optimized using the proposed weighted incremental dictionary learning algorithm.\nAlthough the above improvements have resulted in a good performance, there is still a hidden danger that must be addressed: namely, that, diversity-based strategies are not appropriate for all datasets. More specifically, the richer the category content of the dataset, the larger the batch size, and the better the effect of diversity-based methods; by contrast, an uncertainty-based query strategy will perform better with smaller batch sizes and less rich content. These characteristics depend on the statistical characteristics of the dataset. The BMAL context, whether the data are unfamiliar and potentially unstructured, makes it impossible to determine which AL query strategy is more appropriate. In light of this, BADGE (Batch Active learning by Diverse Gradient Embeddings)  samples point groups that are disparate and high magnitude when represented in a hallucinated gradient space, meaning that both the prediction uncertainty of the model and the diversity of the samples in a batch are considered simultaneously. Most importantly, BADGE can achieve an automatic balance between forecast uncertainty and sample diversity without the need for manual hyperparameter adjustments. \nMoreover, while BADGE  considers this hybrid query strategy in an implicit way, WAAL (Wasserstein Adversarial Active Learning)  proposes a hybrid query strategy that explicitly balances uncertainty and diversity. In addition, WAAL  uses Wasserstein distance to model the interactive procedure in AL as a distribution matching problem, derives losses from it, and then decomposes WAAL  into two stages: DNN parameter optimization and query batch selection. \nTA-VAAL (Task-Aware Variational Adversarial Active Learning)  also explores the balance of this hybrid query strategy. The assumption underpinning TA-VAAL is that the uncertainty-based method does not make good use of the overall data distribution, while the data distribution-based method often ignores the structure of the task. Consequently, TA-VAAL proposes to integrate the loss prediction module  and the concept of RankCGAN  into VAAL (Variational Adversarial Active Learning) , enabling both the data distribution and the model uncertainty to be considered. TA-VAAL has achieved good performance on various balanced and unbalanced benchmark datasets. The structure diagram of TA-VAAL and VAAL is presented in Fig.\\ref{fig:TA-VAAL}.\n\\begin{figure}[!tp] \n\t\\centering \n\t\\includegraphics[width = 0.95\\textwidth]{figure/TA-VAAL}\n\t\\caption{Structure comparison chart of VAAL  and TA-VAAL . 1) VAAL uses labeled data and unlabeled data in a semi-supervised way to learn the latent representation space of the data, then selects the unlabeled data with the largest amount of information according to the latent space for labeling. 2) TA-VAAL expands VAAL and integrates the loss prediction module  and RankCGAN  into VAAL in order to consider data distribution and model uncertainty simultaneously.} \n\t\\label{fig:TA-VAAL}\n\\end{figure}\nNotably, although the hybrid query strategy achieves superior performance, the uncertainty-based AL query strategy is more convenient to combine with the output of the softmax layer of DL. Thus, the query strategy based on uncertainty is still widely used.", "cites": [4024, 2009, 5517, 5523, 5525, 8926, 5526, 5519, 5518, 8924], "cite_extract_rate": 0.4, "origin_cites_number": 25, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers on uncertainty-based and hybrid query strategies in Deep Active Learning, connecting ideas such as margin sampling, entropy, and adversarial methods. It offers critical insights by pointing out limitations like diversity issues and over-reliance on similarity. The abstraction is strong as it identifies patterns (e.g., trade-offs between uncertainty and diversity) and discusses general principles for hybrid strategies across different approaches."}}
{"id": "f610f09b-72e3-4867-aa7a-fd61d236efa2", "title": "Deep Bayesian Active Learning (DBAL)", "level": "subsubsection", "subsections": [], "parent_id": "0d6ad266-3e42-4732-a729-183187f8427f", "prefix_titles": [["title", "A Survey of Deep Active Learning"], ["section", "Deep Active Learning"], ["subsection", "Query Strategy Optimization in DeepAL"], ["subsubsection", "Deep Bayesian Active Learning (DBAL)"]], "content": "As noted in Section \\ref{sec:The necessity and challenge of combining DL and AL}, which analyzes the challenge of combining DL and AL, the acquisition function based on uncertainty is an important research direction of many classic AL algorithms. Moreover, traditional DL methods rarely represent such model uncertainty.\n To solve the above problems, Deep Bayesian Active Learning appears. In the given input set $X$ and the output $Y$ belonging to class $c$, the probabilistic neural network model can be defined as $f(\\mathrm{x};\\theta)$, $p(\\theta)$ is a prior on the parameter space $\\theta$ (usually Gaussian), and the likelihood $p(\\mathrm{y} = c|\\mathrm{x},\\theta)$ is usually given by $softma\\mathrm{x}(f(\\mathrm{x};\\theta))$. Our goal is to obtain the posterior distribution over $\\theta$, as follows:\n \\begin{equation}\n p(\\theta|X, Y)=\\frac{p(Y | X, \\theta) p(\\theta)}{p(Y | X)}.\n \\end{equation}\n For a given new data point $\\mathrm{x}^*$, $\\hat{\\mathrm{y}}$ is predicted by:\n \\begin{equation}\n p\\left(\\hat{\\mathrm{y}} | \\mathrm{x}^{*}, X, Y\\right)=\\int p\\left(\\hat{\\mathrm{y}} | \\mathrm{x}, \\theta\\right) p(\\theta | X, Y) d \\theta=\\mathbb{E}_{\\theta \\sim p(\\theta | X, Y)}[f(\\mathrm{x} ; \\theta)].\n \\end{equation}\nDBAL  combines BCNNs (Bayesian Convolutional Neural Networks)  with AL methods to adapt BALD  to the deep learning environment, thereby developing a new AL framework for high-dimensional data. This approach adopts the above method to first perform Gaussian prior modeling on the weights of a CNN, and then uses variational inference to obtain the posterior distribution of network prediction. In addition, in practice, researchers often also use a powerful and low-cost MC-dropout (Monte-Carlo dropout)  stochastic regularization technique to obtain posterior samples, consequently attaining good performance on real-world datasets . Moreover, this regularization technique has been proven to be equivalent to variational inference . \nHowever, a core-set approach  points out that DBAL  is unsuitable for large datasets due to the need for batch sampling. It should be noted here that while DBAL  allows the use of dropout in testing for better confidence estimation, the analysis presented in  contends that the performance of this method is similar to the performance of using neural network SR  as uncertainty sampling, which requires vigilance. \nIn addition, \nDEBAL (Deep Ensemble Bayesian Active Learning)  argues that the pattern collapse phenomenon  in the variational inference method leads to the overconfident prediction characteristic of the DBAL method. For this reason, DEBAL combines the expressive power of ensemble methods with MC-dropout to obtain better uncertainty in the absence of trading representativeness.\nFor its part, BatchBALD  opts to expand BALD  to the batch query context; this approach no longer calculates the mutual information between a single sample and model parameters but rather recalculates the mutual information between the batch samples and the model parameters to jointly score the batch of samples. This enables BatchBALD to more accurately evaluate the joint mutual information. Inspired by the latest research on Bayesian core sets , ACS-FW (Active Bayesian CoreSets with Frank-Wolfe optimization)  reconstructed the batch structure to optimize the sparse subset approximation of the log-posterior induced by the entire dataset. Using this similarity, ACS-FW then employs the Frank-Wolfe  algorithm to enable effective Bayesian AL at scale, while its use of random projection has made it still more popular. Compared with other query strategies (e.g., maximizing the predictive entropy\n(MAXENT)  and BALD ), ACS-FW achieves better coverage across the entire data manifold. \nDPEs (Deep Probabilistic Ensembles)  introduces an expandable DPEs technology, which uses a regularized ensemble to approximate the deep BNN, and then evaluates the classification effect of these DPEs in a series of large-scale visual AL experiments.\nActiveLink (Deep Active Learning for Link Prediction in Knowledge Graphs)  is inspired by the latest advances in Bayesian deep learning . Adopting the Bayesian view of the existing neural link predictors, it expands the uncertainty sampling method by using the basic structure of the knowledge graph, thereby creating a novel DeepAL method. ActiveLink further noted that although AL can sample efficiently, the model needs to be retrained from scratch for each iteration in the AL process, which is unacceptable in the DL model training context. A simple solution would be to use newly selected data to train the model incrementally, or to combine it with existing training data ; however, this would cause the model to be biased either towards a small amount of newly selected data or towards data selected early in the process. In order to solve this bias problem, ActiveLink adopts a principled and unbiased incremental training method based on meta-learning. More specifically, in each AL iteration, ActiveLink uses the newly selected samples to update the model parameters, then approximates the meta-objective of the model's future prediction by generalizing the model based on the samples selected in the previous iteration. This enables ActiveLink to strike a balance between the importance of the newly and previously selected data, and thereby to achieve an unbiased estimation of the model parameters.\nIn addition to the above-mentioned DBAL work, due to the lesser parameter of BNN and the uncertainty sampling strategy being similar to traditional AL, the research on DBAL is quite extensive, and there are many works related to this topic .", "cites": [1044, 4641, 8717, 1042, 4599, 8316, 4610, 5530, 1050, 5529, 5528, 4618, 5527, 6991], "cite_extract_rate": 0.5185185185185185, "origin_cites_number": 27, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes multiple papers on Deep Bayesian Active Learning, connecting their contributions and limitations into a coherent narrative. It critically evaluates DBAL, MC-dropout, and related methods, noting issues like pattern collapse and scalability. The abstraction level is moderate, as it identifies broader trends in uncertainty estimation and Bayesian approaches but remains focused on specific techniques."}}
{"id": "fb0f4ba8-e10b-45d4-bec6-9cbf53f668c0", "title": "Density-based Methods", "level": "subsubsection", "subsections": [], "parent_id": "0d6ad266-3e42-4732-a729-183187f8427f", "prefix_titles": [["title", "A Survey of Deep Active Learning"], ["section", "Deep Active Learning"], ["subsection", "Query Strategy Optimization in DeepAL"], ["subsubsection", "Density-based Methods"]], "content": "The term, density-based method, mainly refers to the selection of samples from the perspective of the set (core set ). The construction of the core set is a representative query strategy. This idea is mainly inspired by the compression idea of the core set dataset and attempts to use the core set to represent the distribution of the feature space of the entire original dataset, thereby reducing the labeling cost of AL. \nFF-Active (Farthest First Active Learning)  is based on this idea and uses the farthest-first traversal in the space of neural activation over a representation layer to query consecutive points from the pool. It is worth noting here that FF-Active  and Exploration-P  resemble the way in which random queries are used in the early stages of AL to enhance AL's exploration ability, which prevents AL from falling into the trap of insufficient sample diversity. Similarly, to solve the sampling bias problem in batch querying, the diversity of batch query samples is increased. \nThe Core-set approach  attempts to solve this problem by constructing a core subset. A further attempt was made to solve the k-Center problem  by building a core subset so that the model learned on the selected core set will be more competitive than the rest of the data. However, the Core-set approach requires a large distance matrix to be built on the unlabeled dataset, meaning that this search process is computationally expensive; this disadvantage will become more apparent on large-scale unlabeled datasets .\nActive Palmprint Recognition  applies DeepAL to high-dimensional and complex palmprint recognition data. Similar to the core set concept,  regards AL as a binary classification task. It is expected that the labeled and unlabeled sample sets will have the same data distribution, making the two difficult to distinguish; that is, the goal is to find a labeled core subset with the same distribution as the original dataset. More specifically, due to the heuristic generative model simulation data distribution being difficult to train and unsuitable for high-dimensional and complex data such as palm prints, the author considers whether the sample can be positively distinguished from the unlabeled or labeled dataset with a high degree of confidence. Those samples that can be clearly distinguished are obviously different from the data distribution of the core annotation subset. These samples will then be added to the annotation dataset for the next round of training.\nPrevious core-set-based methods  often simply try to query data points as far as possible to cover all points of the data manifold without considering the density, which results in the queried data points overly representing sample points from manifold sparse areas. Similar to , DAL (Discriminative Active Learning)  also regards AL as a binary classification task and further aims to make the queried labeled dataset indistinguishable from the unlabeled dataset. The key advantage of DAL  is that it can sample from the unlabeled dataset in proportion to the data density, without biasing the sample points in the sparse popular domain. Moreover, the method proposed by DAL  is not limited to classification tasks, which are conceptually easy to transfer to other new tasks.\nIn addition to the corresponding query strategy, some researchers have also considered the impact of batch query size on query performance. For example,  focus primarily on the optimization of query strategies in smaller batches, while  recommended expanding the query scale of AL for large-scale sampling (10k or 500k samples at a time). Moreover, by integrating hundreds of models and reusing intermediate checkpoints, the distributed searching of training data on large-scale labeled datasets can be efficiently realized with a small computational cost.  also proved that the performance of using the entire dataset for training is not the upper limit of performance, as well as that AL based on subsets specifically may yield better performance.\nFurthermore, the attributes of the dataset itself also have an important impact on the performance of DeepAL. With this in mind, GA (Gradient Analysis)  assesses the relative importance of image data in common datasets and proposes a general data analysis tool design to facilitate a better understanding of the diversity of training examples in the dataset. GA  finds that not all datasets can be trained on a small sub-sample set because the relative difference of sample importance in some datasets is almost negligible; therefore, it is not advisable to blindly use smaller sub-datasets in the AL context.\nIn addition,  finds that compared with the Bayesian deep learning approach (Monte-Carlo dropout ) and density-based  methods, ensemble-based AL can effectively offset the imbalance of categories in the dataset during the acquisition process, resulting in more calibration prediction uncertainty, and thus better performance.\nIn general, density-based methods primarily consider the selection of core subsets from the perspective of data distribution. There are relatively few related research methods, which suggests a new possible direction for sample querying.\n\\begin{figure}[!tp] \n\t\\centering \n\t\\subfloat[Active learning pipeline.] \n \t{\\centering \n \t\t\\includegraphics[width = 0.27\\textwidth]{figure/ALp}\n \t\t\\label{fig:ALp}\n \t}\\hspace{2mm}\n\t\\subfloat[Reinforced Active Learning (RAL) .] \n \t{\\centering \n \t\t\\includegraphics[width = 0.27\\textwidth]{figure/RAL}\n \t\t\\label{fig:RAL}\n \t}\\hspace{2mm}\n \\subfloat[Deep Reinforcement Active\nLearning (DRAL) .] \n \t{\\centering \n \t\t\\includegraphics[width = 0.37\\textwidth]{figure/DRAL}\n \t\t\\label{fig:DRAL}\n \t}\n\t\\caption{Comparison of standard AL, RAL  and DRAL  pipelines.} \n\t\\label{fig:ALp_RAL} \n\\end{figure}", "cites": [5524, 1044, 5531, 8927, 1042, 4641, 5517, 5528, 5532, 5518], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.5, "abstraction": 3.7}, "insight_level": "high", "analysis": "The section synthesizes multiple density-based query strategies, connecting ideas from core sets, farthest-first traversal, and binary classification approaches to present a coherent narrative. It includes some critical observations, such as the computational expense of core-set methods and their bias toward sparse regions. The section also abstracts these methods into broader patterns, such as the importance of data distribution and diversity in query selection, indicating a strong analytical and insight-driven treatment."}}
{"id": "576fbe37-bef7-4cc1-9263-b1e9c6892881", "title": "Automated Design of DeepAL", "level": "subsubsection", "subsections": [], "parent_id": "0d6ad266-3e42-4732-a729-183187f8427f", "prefix_titles": [["title", "A Survey of Deep Active Learning"], ["section", "Deep Active Learning"], ["subsection", "Query Strategy Optimization in DeepAL"], ["subsubsection", "Automated Design of DeepAL"]], "content": "DeepAL is composed of two parts: deep learning and active learning. Manually designing these two parts requires a lot of energy and their performance is severely limited by the experience of researchers. Therefore, it has important significance to consider how to automate the design of deep learning models and active learning query strategies in DeepAL.\nTo this end,  redefines the heuristic AL algorithm as a reinforcement learning problem and introduces a new description through a clear selection strategy. In addition, some researchers have also noted that, in traditional AL workflows, the acquisition function is often regarded as a fixed known prior, and that it will not be known whether this acquisition function is appropriate until the label budget is exhausted. This makes it impossible to flexibly and quickly tune the acquisition function. Accordingly, one good option may be to use reinforcement learning to dynamically tune the acquisition function. \nRAL (Reinforced Active Learning)  proposes to use BNN as a learning predictor for acquisition functions. As such, all probability information provided by the BNN predictor will be combined to obtain a comprehensive probability distribution; subsequently, the probability distribution is sent to a BNN probabilistic policy network, which performs reinforcement learning in each labeling round based on the oracle feedback. This feedback will fine-tune the acquisition function, thereby continuously improving its quality. \nDRAL (Deep Reinforcement Active Learning)  adopts a similar idea and designs a deep reinforcement active learning framework for the person Re-ID task. This approach uses the idea of reinforcement learning to dynamically adjust the acquisition function so as to obtain high-quality query samples. Fig.\\ref{fig:ALp_RAL} presents a comparison between traditional AL, RAL and DRAL pipelines. \nThe pipeline of AL is shown in Fig.\\ref{fig:ALp}. The standard AL pipeline usually consists of three parts. The oracle provides a set of labeled data; the predictor (here, BNN) is used to learn these data and provides predictable uncertainty for the guide. The guide is usually a fixed, hard-coded acquisition function that picks the next sample for the oracle to restart the cycle. \nThe pipeline of RAL (Reinforced Active Learning)  is shown in Fig.\\ref{fig:RAL}. RAL replaces the fixed acquisition function with the policy BNN. The policy BNN learns in a probabilistic manner, obtains feedback from the oracle, and learns how to select the next optimal sample point (new parts in red) in a reinforcement learning-based manner. Therefore, RAL can adjust the acquisition function more flexibly to adapt to the existing dataset. \nThe pipeline of DRAL (Deep Reinforcement Active Learning)  is shown in Fig.\\ref{fig:DRAL}. DRAL utilizes a deep reinforcement active learning framework for the person Re-ID task. For each query anchor (probe), the agent (reinforcement active learner) will select sequential instances from the gallery pool during the active learning process and hand it to the oracle to obtain manual annotation with binary feedback (positive/negative). The state evaluates the similarity relationships between all instances and calculates rewards based on oracle feedback to adjust agent queries.\nOn the other hand, Active-iNAS (Active Learning with incremental Neural Architecture Search)  notices that most previous DeepAL methods  assume that a suitable DL model has been designed for the current task, meaning that their primary focus is on how to design an effective query mechanism; however, the existing DL model is not necessarily optimal for the current DeepAL task. Active-iNAS  accordingly challenges this assumption and uses NAS (neural architecture search)  technology to dynamically search for the most effective model architectures while conducting active learning. \nThere is also some work devoted to providing a convenient performance comparison platform for DeepAL; for example,  discusses and studies the robustness and reproducibility of the DeepAL method in detail, and presents many useful suggestions.\nIn general, these query strategies are not independent of each other but are rather interrelated. Batch-based BMDAL provides the basis for the update training of AL query samples on the DL model. Although the query strategies in DeepAL are rich and complex, they are largely designed to take the diversity and uncertainty of query batches in BMDAL into account. Previous uncertainty-based methods often ignore the diversity in the batch and can thus be roughly divided into two categories: those that design a mechanism that explicitly encourages batch diversity in the input or learning representation space, and those that directly measure the mutual information (MI) of the entire batch.", "cites": [7951, 5531, 5533, 5534, 5516, 5506], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.3, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several papers on automated DeepAL design, connecting the use of reinforcement learning and neural architecture search to improve query strategies and model design. It offers a critical perspective by highlighting limitations of traditional fixed acquisition functions and assumptions in prior methods. While it identifies broader patterns such as the shift from fixed to dynamic strategies, the abstraction remains grounded in specific methodological comparisons and does not reach a meta-level conceptualization."}}
{"id": "d6b7acac-8ffe-421c-a9ab-57e8404c786d", "title": "Data Expansion of Labeled Samples in DeepAL", "level": "subsection", "subsections": [], "parent_id": "3de3bdd7-51c6-4365-9c87-09efb8923a32", "prefix_titles": [["title", "A Survey of Deep Active Learning"], ["section", "Deep Active Learning"], ["subsection", "Data Expansion of Labeled Samples in DeepAL"]], "content": "\\label{sec: Insufficient Data in DeepAL}\nAL often requires only a small amount of labeled sample data to realize learning and model updating, while DL requires a large amount of labeled data for effective training. Therefore, the combination of AL and DL requires as much as possible to use the data strategy without consuming too much human resources to achieve DeepAL model training. Most previous DeepAL methods  often only train on the labeled sample set sampled by the query strategy. However, this ignores the existence of existing unlabeled datasets, meaning that the corresponding data expansion and training strategies are not fully utilized. These strategies help to improve the problem of insufficient labeled data in DeepAL training without adding to the manual labeling costs. Therefore, the study of these strategies is also quite meaningful.\n\\begin{figure}[!tp] \n\t\\centering \n\t\\includegraphics[width = 0.95\\textwidth]{figure/CEAL}\n\t\\caption{In CEAL , the overall framework of DeepAL is utilized. CEAL  gradually feeds the samples from the unlabeled dataset to the initialized CNN, after which the CNN classifier outputs two types of samples: a small number of uncertain samples and a large number of samples with high prediction confidence. A small number of uncertain samples are labeled through the oracle, and the CNN classifier is used to automatically assign pseudo-labels to a large number of high-prediction confidence samples. These two types of samples are then used to fine-tune the CNN, and the updated process is repeated.} \n\t\\label{fig:CEAL} \n\\end{figure}\nFor example, CEAL (Cost-Effective Active Learning)  enriches the training set by assigning pseudo-labels to samples with high confidence in model prediction in addition to the labeled dataset sampled by the query strategy. This expanded training set is then also used in the training of the DL model. This strategy is shown in Fig.\\ref{fig:CEAL}.\nAnother very popular strategy involves performing unsupervised training on labeled and unlabeled datasets and incorporating other strategies to train the entire network structure.\nFor example, WI-DL  notes that full DBN training requires a large number of training samples, and it is impractical to apply DBN to a limited training set in an AL context. Therefore, in order to improve the training efficiency of DBN, WI-DL employs a combination of unsupervised feature learning on all datasets and supervised fine-tuning on labeled datasets.\nAt the same time, some researchers have considered using GAN (Generative Adversarial Networks) for data augmentation. \nFor example, GAAL (Generative Adversarial Active Learning)  introduced the GAN to the AL query method for the first time. GAAL aims to use generative learning to generate samples with more information than the original dataset.\nHowever, random data augmentation does not guarantee that the generated samples will have more information than those contained in the original data, and could thus represent a waste of computing resources. \nAccordingly, BGADL (Bayesian Generative Active Deep Learning)  expands the idea of GAAL  and proposes a Bayesian generative active deep learning method. More specifically, BGADL combines the generative adversarial active learning , Bayesian data augmentation , ACGAN (Auxiliary-Classifier Generative Adversarial Networks)  and VAE (Variational Autoencoder)  methods, with the aim of generating samples of disagreement regions  belonging to different categories. Structure comparison between GAAL and BGADL is presented in Fig.\\ref{fig:GAAL_BGADL}.\n\\begin{figure}[!tp] \n\t\\centering \n\t\\subfloat[Generative adversarial active learning (GAAL).] \n \t{\\centering \n \t\t\\includegraphics[width = 0.4\\textwidth]{figure/GAAL}\n \t\t\\label{fig:GAAL}\n \t}\\hspace{3mm}\n\t\\subfloat[Bayesian generative active deep learning (BGADL).] \n \t{\\centering \n \t\t\\includegraphics[width = 0.5\\textwidth]{figure/BGADL}\n \t\t\\label{fig:BGADL}\n \t}\n\t\\caption{Structure comparison chart of GAAL  and BGADL . For more details, please see .} \n\t\\label{fig:GAAL_BGADL} \n\\end{figure}\nSubsequently, VAAL  and ARAL (Adversarial Representation Active Learning)  borrowed from several previous methods  not only to train the network using labeled and unlabeled datasets but also to introduce generative adversarial learning into the network architecture for data augmentation purposes, thereby further improving the learning ability of the network.\nIn more detail, VAAL  noticed that the batch-based query strategy based on uncertainty not only readily leads to insufficient sample diversity, but is also highly susceptible to interference from outliers. In addition, density-based methods  are susceptible to $p$-norm limitations when applied to high-dimensional data, resulting in calculation distances that are too concentrated . To this end, VAAL  proposes to use the adversarial learning representation method to distinguish between the potential spatial coding features of labeled and unlabeled data, thus reducing interference from outliers. \nVAAL  also uses labeled and unlabeled data to jointly train a VAE  in a semi-supervised manner; the goal here is to deceive the adversarial network  into predicting that all data points come from the labeled pool, in order to solve the problem of distance concentration. VAAL  can learn an effective low-dimensional latent representation on a large-scale dataset, and further provides an effective sampling method by jointly learning the representation form and uncertainty.\nSubsequently, ARAL  expanded VAAL , aiming to use as few manual annotation samples as possible while still making full use of the existing or generated data information in order to improve the model's learning ability. In addition to using labeled and unlabeled datasets, ARAL  also uses samples produced by deep production networks to jointly train the entire model. ARAL  comprises both VAAL  and adversarial representation learning . By using VAAL  to learn the potential feature representation space of the labeled and unlabeled data, the unlabeled samples with the largest amount of information can be selected accordingly. At the same time, both real and generated data are used to enhance the model's learning ability through confrontational representation learning . Similarly, TA-VAAL  also extends VAAL by using the global data structure from VAAL and local task-related information from the learning loss for sample querying purposes. We present the framework of ARAL  in Fig.\\ref{fig:VAAL_ARAL}. \n\\begin{figure}[!tp] \n\t\\centering \n\t\\includegraphics[width = 0.95\\textwidth]{figure/VAAL_ARAL}\n\t\\caption{The overall structure of ARAL . ARAL uses not only real datasets (both labeled and unlabeled), but also generated datasets to jointly train the network. The whole network consists of an encoder ($E$), generator ($G$), discriminator ($D$), classifier ($C$) and sampler ($S$), and all parts of the model are trained together.} \n\t\\label{fig:VAAL_ARAL} \n\\end{figure}\nUnlike ARAL  and VAAL , which use labeled and unlabeled datasets for adversarial representation learning, SSAL (Semi-Supervised Active Learning)  implements a new training method. More specifically, SSAL  uses unsupervised, supervised, and semi-supervised learning methods across AL cycles, and makes full use of existing information for training without increasing the cost of labeling as much as possible. In more detail, the process is as follows: before the AL starts, first use labeled and unlabeled data for unsupervised pretraining. In each AL learning cycle, first, perform supervised training on the labeled dataset, then perform semi-supervised training on all datasets. This represents an attempt to devise a wholly new training method. The author finds that, compared with the difference between the sampling strategies, this model training method yields a surprising performance improvement.\nAs analyzed above, this kind of exploration of training methods and data utilization skills is also essential; in fact, the resultant performance gains may even exceed those generated by changing the query strategy. Applying these techniques enables the full use of existing data without any associated increase in labeling costs, which helps in resolving the issue of the number of AL query samples being insufficient to support the updating of the DL model.", "cites": [7100, 1042, 8926, 1001, 7952, 5536, 1050, 5520, 8924, 5535, 5680, 5522, 5526], "cite_extract_rate": 0.7222222222222222, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple DeepAL methods, connecting them through their shared goal of improving data efficiency by leveraging unlabeled data. It provides a coherent narrative on data expansion strategies, such as pseudo-labeling and GAN-based augmentation. While some critical analysis is present (e.g., noting issues with random data augmentation and outlier susceptibility), it is not as deep as a full evaluation of trade-offs or limitations across all cited works. The section identifies patterns in how different methods utilize semi-supervised and adversarial learning, suggesting some abstraction, though it remains focused on specific algorithmic implementations."}}
{"id": "e325634b-5e6a-4c21-ae7a-d63f06cf344c", "title": "DeepAL Generic Framework", "level": "subsection", "subsections": [], "parent_id": "3de3bdd7-51c6-4365-9c87-09efb8923a32", "prefix_titles": [["title", "A Survey of Deep Active Learning"], ["section", "Deep Active Learning"], ["subsection", "DeepAL Generic Framework"]], "content": "\\label{sec: Common Framework DeepAL}\nAs mentioned in Section \\ref{sec:The necessity and challenge of combining DL and AL}, a processing pipeline inconsistency exists between AL and DL; thus, only fine-tuning the DL model in the AL framework, or simply combining AL and DL to treat them as two separate problems, may cause divergence. For example,  first conducts offline supervised training of the DL model on two different types of session datasets to grant basic conversational capabilities to the backbone network, then enables the online AL stage to interact with human users, enabling the model to be improved in an open way based on user feedback. AL-DL  proposes an AL method for DL models with DBNs, while ADN  further proposes an active deep network architecture for sentiment classification.  proposes an AL algorithm using CNN for captcha recognition. However, generally speaking, the above methods first perform routine supervised training on this depth model on the labeled dataset, then actively sample based on the output of the depth model. There are many similar related works  that adopt this split-and-splitting approach that treats the training of AL and deep models as two independent problems and consequently increases the possibility, which the two problems will diverge. Although this method achieved some success at the time, a general framework that closely combines the two tasks of DL and AL would play a vital role in the performance improvement and promotion of DeepAL.\nCEAL  is one of the first works to combine AL and DL in order to solve the problem of depth image classification. CEAL  merges deep convolutional neural networks into AL, and consequently proposes a novel DeepAL framework. It sends samples from the unlabeled dataset to the CNN step by step, after which the CNN classifier outputs two types of samples: a small number of uncertain samples and a large number of samples with high prediction confidence. A small number of uncertain samples are labeled by the oracle, and the CNN classifier is used to automatically assign pseudo-labels to a large number of high-prediction-confidence samples. Then, these two types of samples are used to fine-tune the CNN and the update process is repeated. In Fig.\\ref{fig:CEAL}, we present the overall framework of CEAL. Moreover, HDAL (Heuristic Deep Active Learning)  uses a similar framework for face recognition tasks: it combines AL with a deep CNN model to integrate feature learning and AL query model training.\nIn addition, Fig.\\ref{fig:DeepAL} illustrates a widespread general framework for DeepAL tasks. Related works include  , among others. More specifically,  proposes a framework that uses an FCN (Fully Convolutional Network)  and AL to solve the medical image segmentation problem using a small number of annotations. It first trains FCN on a small number of labeled datasets, then extracts the features of the unlabeled datasets through FCN, using these features to estimate the uncertainty and similarity of unlabeled samples. This strategy, which is similar to that described in Section \\ref{sec: Hybrid query strategy}, helps to select highly uncertain and diverse samples to be added to the labeled dataset in order to start the next stage of training. \nActive Palmprint Recognition  proposes a similar DeepAL framework as that for the palmprint recognition task. The difference is that inspired by domain adaptation , Active Palmprint Recognition  regards AL as a binary classification task: it is expected that the labeled and unlabeled sample sets have the same data distribution, making the two difficult to distinguish. Supervision training can be performed directly on a small number of labeled datasets, which reduces the burden associated with labeling. \n proposes a DeepAL framework for defect detection. This approach performs uncertainty sampling based on the output features of the detection model to generate a list of candidate samples for annotation. In order to further take the diversity of defect categories in the samples into account,  designs an average margin method to control the sampling ratio of each defect category.\n\\begin{figure}[!tp] \n\t\\centering \n\t\\includegraphics[width = 0.95\\textwidth]{figure/AL-MV}\n\t\\caption{Taking a common CNN as an example, this figure presents a comparison between the traditional uncertainty measurement method  and the uncertainty measurement method of synthesizing information in two stages  (i.e., the feature extraction stage and task learning stage).} \n\t\\label{fig:AL-MV} \n\\end{figure}\nDifferent from the above methods, it is common for the final output of the DL model to be used as the basis for determining the uncertainty or diversity of the sample (Active Palmprint Recognition  uses the output of the first fully connected layer).  also used the output of the DL model's middle hidden layer. As analyzed in Section \\ref{sec: Hybrid query strategy} and Section \\ref{sec:The necessity and challenge of combining DL and AL}, due to the difference in learning paradigms between the deep and shallow models, the traditional uncertainty-based query strategy cannot be directly applied to the DL model. In addition, unlike the shallow model, the deep model can be regarded as composed of two stages, namely the feature extraction stage and the task learning stage. It is inaccurate to use only the output of the last layer of the DL model as the basis for evaluating the sample prediction uncertainty; this is because the uncertainty of the DL model is in fact composed of the uncertainty of these two stages. A schematic diagram of this concept is presented in Fig.\\ref{fig:AL-MV}. \nTo this end, AL-MV (Active Learning with Multiple Views)  treats the features from different hidden layers in the middle of CNN as multiview data, taking the uncertainty of both stages into account, and the AL-MV algorithm is designed to implement adaptive weighting of the uncertainty of each layer, to enable more accurate measurement of the sampling uncertainty.\n LLAL (Learning Loss for Active Learning)  also used a similar idea. More specifically, LLAL designs a small parameter module of the loss prediction module to attach to the target network, using the output of multiple hidden layers of the target network as the input of the loss prediction module. The loss prediction module is learned to predict the target loss of the unlabeled dataset, while the top-$K$ strategy is used to select the query samples. LLAL achieves task-agnostic AL framework design at a small parameter cost and further achieves competitive performance on a variety of mainstream visual tasks (namely, image classification, target detection, and human pose estimation).\nSimilarly,  uses a similar strategy to implement a DeepAL framework for finger bone segmentation tasks.  uses Deeply Supervised U-Net  as the segmentation network, then subsequently uses the output of the multilevel segmentation hidden layer and the output of the last layer as the input of AL; this input information is then integrated to form the basis for the evaluation of the sample information size. \nWe take LLAL  as an example to explicate the overall network structure of this idea in Fig.\\ref{fig:LLAL}.\n\\begin{figure}[!tp] \n\t\\centering \n\t\\includegraphics[width = 0.95\\textwidth]{figure/LLAL}\n\t\\caption{The overall framework of LLAL . The black line represents the stage of training model parameters, optimizing the overall loss composed of target loss and loss-prediction loss. The red line represents the sample query phase of AL. The output of the multiple hidden layers of the DL model is used as the input of the loss prediction module, while the top-$K$ unlabeled data points are selected according to the predicted losses and assigned labels by the oracle.} \n\t\\label{fig:LLAL} \n\\end{figure}\nThe research on the general framework is highly beneficial to the development and promotion of DeepAL, as this task-independent framework can be conveniently transplanted to other fields. In the current fusion of DL and AL, DL is primarily responsible for feature extraction, while AL is mainly responsible for sample querying; thus, a deeper and tighter fusion will help DeepAL achieve better performance. Of course, this will require additional exploration and effort on the part of researchers. Finally, the challenges of combining DL and AL and related work on the corresponding solutions are summarized in Table \\ref{tab: DAL_challenges_solutions}.", "cites": [2009, 810, 8923, 5508, 5523, 5537, 1050, 825], "cite_extract_rate": 0.47058823529411764, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple DeepAL frameworks by highlighting their structural similarities and differences, such as the split-and-splitting approach versus integrated feature and task learning. It critically assesses the limitations of treating AL and DL as separate tasks and emphasizes the need for a unified framework. The discussion abstracts these methods into a broader understanding of how AL can be adapted to deep learning by leveraging multi-stage uncertainty and task-agnostic designs."}}
{"id": "334f2d49-93f9-4238-9f87-25f656785fde", "title": "DeepAL Stopping Strategy", "level": "subsection", "subsections": [], "parent_id": "3de3bdd7-51c6-4365-9c87-09efb8923a32", "prefix_titles": [["title", "A Survey of Deep Active Learning"], ["section", "Deep Active Learning"], ["subsection", "DeepAL Stopping Strategy"]], "content": "\\label{sec: DeepAL Stopping Strategy}\nIn addition to querying strategies and training methods, an appropriate stopping strategy has an important impact on DeepAL performance. At present, most DeepALs  often use the predefined stopping criterion, and when the criterion is satisfied, they stop querying labels from the oracle. These predefined stopping criteria include the maximum number of iterations, the minimum threshold for changing classification accuracy, the minimum number of labeled samples, and the expected accuracy value, etc.\nAlthough these stopping criteria are simple, these predefined stopping criteria are likely to cause DeepAL to fail to achieve optimal performance. This is because the premature termination of AL annotation querying leads to large performance losses in the model, and excessive annotation behavior wastes a lot of annotation budget. Therefore, Stabilizing Predictions (SP)  makes a comprehensive review of AL stopping strategies and proposes an AL stopping strategy based on stability prediction. Specifically, the SP predivides a part of the samples from the unlabeled dataset to form a stop set (the stop set does not need to be labeled), and the SP checks the prediction stability on the stop set in each iteration. When the prediction performance of the model on the stop set stabilizes, the iteration is stopped. A well-trained model often has a stable predictive ability, and SP takes advantage of this feature. The predivided stop set does not require specific labeling information, which avoids additional labeling costs contrary to the purpose of AL. Although SP is a stopping strategy proposed mainly for AL, it also is relevant for DeepAL.\n\\begin{table}[!tp]\n    \\centering\n    \\scriptsize\n    \\caption{The challenges of combining DL and AL, as well as a summary of related work on the corresponding solutions.}\n    \\begin{tabular}{|l|l|l|c|l|}\\toprule\n    \\hline\n         Challenges & Solutions & Foundation & Category& Publications\\\\\\hline\n         \\multirow{7}{*}{\\makecell[l]{Model\\\\uncertainty\\\\in Deep\\\\Learning}} & \\multirow{7}{*}{\\makecell[l]{Query\\\\strategy\\\\optimization}} & \\multirow{7}{*}{\\makecell[l]{Batch\\\\Mode\\\\DeepAL\\\\(BMDAL)}}& \\makecell[l]{Uncertainty-based\\\\ and Hybrid \\\\Query Strategies}&\\makecell[l]{}\\\\\n         \\cline{4-5}\n         &&&\\makecell[l]{Deep Bayesian\\\\Active Learning\\\\(DBAL)}& \\makecell[l]{\\\\}\\\\\n         \\cline{4-5}\n         &&&\\makecell[l]{Density-based\\\\Methods}& \\\\\n         \\cline{4-5}\n         &&&\\makecell[l]{Automated\\\\Design of DeepAL}& \\\\\n         \\hline\n         \\makecell[l]{Insufficient\\\\data for\\\\labeled samples} & \\makecell[l]{Data\\\\expansion of\\\\labeled samples}&\\multicolumn{2}{c|}{-}& \\\\\n         \\hline\n         \\makecell[l]{Processing\\\\pipeline\\\\inconsistency} & \\makecell[l]{Common\\\\framework\\\\DeepAL}&\\multicolumn{2}{c|}{-}&\\makecell[l]{\\\\}\\\\\n         \\hline\n    \\end{tabular}\n    \\label{tab: DAL_challenges_solutions}\n\\end{table}", "cites": [1044, 7951, 4641, 5537, 4024, 5522, 2009, 8923, 8927, 1042, 5517, 4599, 7952, 5523, 5533, 5532, 4610, 5531, 5536, 1050, 5528, 5527, 6991, 8926, 5526, 5509, 5508, 5520, 5518, 8924, 5519], "cite_extract_rate": 0.5849056603773585, "origin_cites_number": 53, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of stopping strategies in Deep Active Learning (DeepAL), focusing primarily on predefined criteria and introducing the SP approach. However, it lacks meaningful synthesis of the cited papers, offers minimal critical evaluation of their strengths or weaknesses, and does not abstract broader principles or trends in the field. The narrative remains surface-level and descriptive."}}
{"id": "ebf2ea49-34b5-4e93-9905-e894e39da2ba", "title": "Image classification and recognition", "level": "subsubsection", "subsections": [], "parent_id": "4e9307b4-41e6-4c33-a90c-b467ddf251a8", "prefix_titles": [["title", "A Survey of Deep Active Learning"], ["section", "Application of DeepAL in fields such as vision and NLP"], ["subsection", "Visual Data Processing"], ["subsubsection", "Image classification and recognition"]], "content": "As with DL, the classification and recognition of images in DeepAL form the basis for research into other vision tasks. One of the most important problems that DeepAL faces in the field of image vision tasks is that of how to efficiently query samples of high-dimensional data (an area in which traditional AL performs poorly) and obtain satisfactory performance at the smallest possible labeling cost.\nTo solve this problem, CEAL  assigns pseudo-labels to samples with high confidence and adds them to the highly uncertain sample set queried using the uncertainty-based AL method, then uses the expanded training set to train the DeepAL model image classifier.\n first integrated the criteria of AL into the deep belief network and subsequently conducted extensive research on classification tasks on a variety of real uni-modal and multi-modal datasets.\nWI-DL  uses the DeepAL method to simultaneously consider the two selection criteria of maximizing representativeness and uncertainty on hyperspectral image (HSI) datasets for remote sensing classification tasks. Similarly,  also studied the classification of HSI.  introduces AL to initialize HSI and then performs transfer learning. This work also recommends constructing and connecting higher-level features to source and target HSI data in order to further overcome the cross-domain disparity.  proposes a unified deep network combined with active transfer learning, thereby training the HSI classification well while using less labeled training data.\nMedical image analysis is also an important application. For example,  explores the use of AL rather than random learning to train convolutional neural networks for tissue (e.g., stroma, lymphocytes, tumor, mucosa, keratin pearls, blood, and background/adipose) classification tasks.\n conducted a comprehensive review of DeepAL-related methods in the field of medical image analysis.\nAs discussed above, since the annotation of medical images requires strong professional knowledge, it is usually both very difficult and very expensive to find well-trained experts willing to perform annotations. In addition, DL has achieved impressive performance on various image feature tasks. Therefore, a large number of works continue to focus on combining DL and AL in order to apply DeepAL to the field of medical image analysis .\nThe DeepAL method is also used to classify in situ plankton  and perform the automatic counting of cells .\nIn addition, DeepAL also has a wide range of applications in our daily life. For example,  proposes an AL algorithm that uses CNN for verification code recognition. It can use the ability to obtain labeled data for free to avoid human intervention and greatly improve the recognition accuracy when less labeled data is used.\nHDAL  combines the excellent feature extraction capabilities of deep CNN and the ability to save on AL labeling costs to design a heuristic deep active learning framework for face recognition tasks.", "cites": [5538, 1050, 8924, 5539], "cite_extract_rate": 0.21052631578947367, "origin_cites_number": 19, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of DeepAL applications in image classification, citing several papers but primarily listing them without substantial synthesis or deeper comparison. While it does attempt to highlight common themes, such as the integration of uncertainty and representativeness in sample selection, it lacks critical evaluation of the methods or their limitations. Some general patterns are mentioned, but the abstraction remains limited to surface-level observations."}}
{"id": "bb9e3757-032e-41ad-b515-b103736802f7", "title": "Object detection and semantic segmentation", "level": "subsubsection", "subsections": [], "parent_id": "4e9307b4-41e6-4c33-a90c-b467ddf251a8", "prefix_titles": [["title", "A Survey of Deep Active Learning"], ["section", "Application of DeepAL in fields such as vision and NLP"], ["subsection", "Visual Data Processing"], ["subsubsection", "Object detection and semantic segmentation"]], "content": "Object detection and semantic segmentation have important applications in various fields, including autonomous driving, medical image processing, and wildlife protection. However, these fields are also limited by the higher sample labeling cost. Thus, the lower labeling cost of DeepAL is expected to accelerate the application of the corresponding DL models in certain real-world areas where labeling is more difficult.\n designs a DeepAL framework for object detection, which uses the layered architecture where labeling is more difficult as an example of \"query by committee\" to select the image set to be queried, while at the same time introducing a similar exploration/exploitation trade-off strategy to .\nDeepAL is also widely used in natural biological fields and industrial applications. For example,  uses deep neural networks to quickly transferable and automatically extract information, and further combines transfer learning and AL to design a DeepAL framework for species identification and counting in camera trap images. \n uses unmanned aerial vehicles (UAV) to obtain images for wildlife detection purposes; moreover, to enable this wildlife detector to be reused,  uses AL and introduces transfer sampling (TS) to find the corresponding area between the source and target datasets, thereby facilitating the transfer of data to the target domain. \n proposes a DeepAL framework for deep object detection in autonomous driving to train LiDAR 3D object detectors.\n proposes the adaptation of a widespread DeepAL framework to defect detection in real industries, along with an uncertainty sampling method for use in generating candidate label categories. This work uses the average margin method to set the sampling scale of each defect category and is thus able to obtain the required performance with less labeled data.\nIn addition, DeepAL also has important applications in the area of medical image segmentation. For example,  proposes an AL-based transfer learning mechanism for medical image segmentation, which can effectively improve the image segmentation performance on a limited labeled dataset.\n combines FCN and AL to create a DeepAL framework for biological-image segmentation. This work uses the uncertainty and similarity information provided by the FCN to extend the maximum set cover problem, significantly reducing the required labeling workload by pointing out the most effective labeling areas.\nDASL (Deep Active Self-paced Learning)  proposes a deep region-based network, Nodules R-CNN, for pulmonary nodule segmentation tasks. This work generates segmentation masks for use as examples, and at the same time, combines AL and SPL (Self-Paced Learning)  to propose a new deep active self-paced learning strategy that reduces the labeling workload.\n proposes a Nodule-plus Region-based CNN for pulmonary nodule detection and segmentation in 3D thoracic Computed Tomography (CT). This work combines AL and SPL strategies to create a new deep self-paced active learning (DSAL) strategy, which reduces the annotation workload and makes effective use of unannotated data.\n further proposes a new deep-supervised active learning method for finger bone segmentation tasks. This model can be fine-tuned in an iterative and incremental learning manner and uses the output of the intermediate hidden layer as the basis for sample selection. Compared with the complete markup,  achieved comparable segmentation results using fewer samples.", "cites": [8923, 5508, 5537], "cite_extract_rate": 0.25, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of various DeepAL applications in object detection and semantic segmentation, mentioning the methods and domains without synthesizing key themes or trends. It lacks critical evaluation of the cited works and does not abstract the findings into broader principles or frameworks."}}
{"id": "af0a8362-ff08-4ae6-9a01-e0c6659ec7da", "title": "Video processing", "level": "subsubsection", "subsections": [], "parent_id": "4e9307b4-41e6-4c33-a90c-b467ddf251a8", "prefix_titles": [["title", "A Survey of Deep Active Learning"], ["section", "Application of DeepAL in fields such as vision and NLP"], ["subsection", "Visual Data Processing"], ["subsubsection", "Video processing"]], "content": "Compared with the image task that only needs to process information in the spatial dimension, the video task also needs to process the information in the temporal dimension.\nThis makes the task of annotating the video more expensive, which also means that the need to introduce AL has become more urgent. DeepAL also has broader application scenarios in this field.\nFor example,  proposes to use imitation learning to perform navigation tasks. The visual environment and actions taken by the teacher viewed from a first-person perspective are used as the training set. Through training, it is hoped that students will become able to predict and execute corresponding actions in their own environment. When performing tasks, students use deep convolutional neural networks for feature extraction, learn imitation strategies, and further use the AL method to select samples with insufficient confidence, which are added to the training set to update the action strategy.  significantly improves the initial strategy using fewer samples.\nDeActive  proposes a DeepAL activity recognition model. Compared with the traditional DL activity recognition model, DeActive requires fewer labeled samples, consumes fewer resources, and achieves high recognition accuracy.\n minimizes the annotation cost of the video-based person Re-ID dataset by integrating AL into the DL framework. Similarly,  proposes a deep reinforcement active learning method for person Re-ID, using oracle feedback to guide the agent (i.e. the model in the reinforcement learning process) in selecting the next uncertainty sample. The agent selection mechanism is continuously optimized through alternately refined reinforcement learning strategies.\n further proposes an active learning object detection method based on convolutional neural networks for pedestrian target detection in video and static images.", "cites": [5540, 5506], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual summary of several DeepAL applications in video processing, but lacks integration of the cited papers into a broader narrative or framework. There is minimal critical analysis or comparison of methods, and the generalization to broader principles or trends is not evident. The content is primarily descriptive with little synthesis or abstraction."}}
{"id": "6785e494-c738-484a-9553-4331052998ea", "title": "Machine translation", "level": "subsubsection", "subsections": [], "parent_id": "84077737-96d6-4cf9-bb3e-a190b2e3eb47", "prefix_titles": [["title", "A Survey of Deep Active Learning"], ["section", "Application of DeepAL in fields such as vision and NLP"], ["subsection", "Natural Language Processing (NLP)"], ["subsubsection", "Machine translation"]], "content": "Machine translation has very important application value, but it usually requires a large number of parallel corpora as a training set. For many low-resource language pairs, building such a corpus requires a very high cost.\nFor this reason,  proposes to use the AL framework to select information source sentences to construct a parallel corpus. It proposes two effective sentence selection methods for AL: selection based on semantic similarity and decoder probability. Compared with traditional methods, the two proposed sentence selection methods show considerable advantages.\n proposes a curriculum learning framework related to AL for machine translation tasks. It can decide which training samples to show to the model during different periods of training based on the estimated difficulty of a sample and the current competence of the model. This method not only effectively improves the training efficiency but also obtains a good accuracy improvement. This kind of thinking is also very valuable for DeepAL's sample selection strategy.", "cites": [7948, 5541], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the two cited papers by connecting their use of active learning (AL) to improve machine translation, particularly in low-resource settings, and highlights the value of their proposed selection and curriculum learning strategies. While it offers some analytical perspective by noting the advantages of these approaches over traditional methods, it lacks in-depth critical evaluation or comparison of their limitations. It begins to abstract by emphasizing the relevance of these ideas to DeepAL more broadly, but the abstraction remains limited in scope and depth."}}
{"id": "60de43bd-c79a-412f-89c5-24e11735adfa", "title": "Text classification", "level": "subsubsection", "subsections": [], "parent_id": "84077737-96d6-4cf9-bb3e-a190b2e3eb47", "prefix_titles": [["title", "A Survey of Deep Active Learning"], ["section", "Application of DeepAL in fields such as vision and NLP"], ["subsection", "Natural Language Processing (NLP)"], ["subsubsection", "Text classification"]], "content": "Text classification tasks also face the challenge of excessive labeling costs, such as patent classification  and clinical text classification . These labeling tasks often need to be completed by experts, and the number of datasets and texts in each document is often very large, which makes it difficult for human experts to complete the corresponding labeling tasks.\n claims to be the first AL method for text classification with CNNs.  focuses on selecting those samples that have the greatest impact on the embedding space. It proposes a method for sentence classification that selects instances containing words whose embeddings are likely to be updated with the greatest magnitude, thereby rapidly learning discriminative, task-specific embeddings. They also extend this method to text classification tasks, which outperformed the baseline AL method in sentence and text classification tasks.  also proposes a new DeepAL framework for text classification tasks. It uses RNN as the acquisition function in AL. The method proposed by  can effectively reduce the number of label instances required for deep learning while saving training time without reducing model accuracy. \n focuses on the problem of sampling bias in deep active classification and apply active text classification on the large-scale text corpora of . These methods generally show better performance than that of the traditional AL-based baseline methods, and more relevant DeepAL-based text classification applications can be found in .", "cites": [5509, 5542, 4023, 1096], "cite_extract_rate": 0.4, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of DeepAL applications in text classification but lacks deeper synthesis, critical evaluation, or abstraction. It briefly mentions methods from cited papers and indicates performance improvements, but does not connect these ideas into a broader framework or analyze their strengths and weaknesses."}}
{"id": "2110095a-f072-45b4-9a58-3fb5bf39160a", "title": "Information extraction", "level": "subsubsection", "subsections": [], "parent_id": "84077737-96d6-4cf9-bb3e-a190b2e3eb47", "prefix_titles": [["title", "A Survey of Deep Active Learning"], ["section", "Application of DeepAL in fields such as vision and NLP"], ["subsection", "Natural Language Processing (NLP)"], ["subsubsection", "Information extraction"]], "content": "Information extraction aims to extract and simplify the most important information from large texts, which is an important basis for correlation analysis between different concepts.\n uses relevant tweets from disaster-stricken areas to extract information that facilitates the identification of infrastructure damage during earthquakes. For this reason,  combines RNN and GRU-based models with AL, using AL-based methods to pre-train the model so that it will retrieve tweets featuring infrastructure damage in different regions, thereby significantly reducing the manual labeling workload. \nIn addition, entity resolution (ER) is the task of recognizing the same real entities with different representations across databases and represents a key step in knowledge base creation and text mining. \n uses the combination of DL and AL to determine how the technical level of NER (Named Entity Recognition) can be improved in the case of a small training set.  developed a DL-based ER method that combines transfer learning and AL to design an architecture that allows for the learning of a model that is transferable from high-resource environments to low-resource environments.\n proposes a novel ALPNN (Active Learning Policy Neural Network) design to recognize the concepts and relationships in large EEG (electroencephalogram) reports; this approach can help humans extract available clinical knowledge from a large number of such reports.", "cites": [5543], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of how Deep Active Learning is applied to information extraction in NLP, referencing specific applications in tweet analysis, entity resolution, and EEG report concept recognition. While it connects the papers to the general theme of DeepAL in NLP, it lacks in-depth synthesis, critical evaluation, or abstraction. It primarily summarizes individual applications without comparing methods, identifying trends, or offering deeper analysis."}}
{"id": "04105094-0a90-4d96-8404-fd3673003366", "title": "Question-answering", "level": "subsubsection", "subsections": [], "parent_id": "84077737-96d6-4cf9-bb3e-a190b2e3eb47", "prefix_titles": [["title", "A Survey of Deep Active Learning"], ["section", "Application of DeepAL in fields such as vision and NLP"], ["subsection", "Natural Language Processing (NLP)"], ["subsubsection", "Question-answering"]], "content": "Intelligent question-answering is also a common processing task in the NLP context, and DL has achieved impressive results in these areas. However, the performance of these applications still relies on the availability of massive labeled datasets; AL is expected to bring new hope to this challenge.\nThe automatic question-answering system has a very wide range of applications in the industry, and DeepAL is also highly valuable in this field. \nFor example,  uses the online AL strategy combined with the DL model to achieve an open domain dialogue by interacting with real users and learning incrementally from user feedback in each round of dialogue.\n finds that AL strategies designed for specific tasks (e.g., classification) often have only one correct answer and that these uncertainty-based measurements are often calculated based on the output of the model. Many real-world vision tasks often have multiple correct answers, which leads to the overestimation of uncertainty measures and sometimes even worse performance than random sampling baselines. For this reason,  proposes to estimate the uncertainty in the hidden space within the model rather than the uncertainty in the output space of the model in the Visual Question Answer (VQA) generation, thus overcoming the paraphrasing nature of language.", "cites": [2009, 5544], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section shows basic synthesis by connecting the use of DeepAL in dialogue generation (Paper 1) with the limitations of traditional AL in handling multiple correct answers (Paper 2). It provides a critical perspective by pointing out how uncertainty measures can be overestimated in tasks with multiple correct answers. However, the abstraction is limited to a general observation rather than identifying higher-level principles or trends."}}
{"id": "eedaf731-74b8-49c4-895b-ed1869063c9a", "title": "Other Applications", "level": "subsection", "subsections": [], "parent_id": "dcc2782c-e704-4c3c-9abb-4c1b9d13e14c", "prefix_titles": [["title", "A Survey of Deep Active Learning"], ["section", "Application of DeepAL in fields such as vision and NLP"], ["subsection", "Other Applications"]], "content": "The emergence of DeepAL is exciting, as it is expected to reduce the annotation costs by orders of magnitude while maintaining performance levels. For this reason, DeepAL is also widely used in other fields.\nThese applications include, but are not limited to, gene expression, robotics, wearable device data analysis, social networking, ECG signal analysis, etc.\nFor some more specific examples, MLFS (Multi-Level Feature Selection)  combines DL and AL to select genes/miRNAs based on expression profiles and proposes a novel multi-level feature selection method. MLFS also considers the biological relationship between miRNAs and genes and applies this method to miRNA expansion tasks.\nMoreover, the failure risk of real-world robots is expensive.  proposes a risk-aware resampling technique; this approach uses AL together with existing solvers and DL to optimize the robot's trajectory, enabling it to effectively deal with the collision problem in scenes with moving obstacles, and verify the effectiveness of the DeepAL method on a real nano-quadcopter.\n further proposes an active trajectory generation framework for the inverse dynamics model of the robot control algorithm, which enables the systematic design of the information trajectory used to train the DNN inverse dynamics module.\nIn addition,  uses sensors installed in wearable devices or mobile terminals to collect user movement information for human activity recognition purposes.  proposes a DeepAL framework for activity recognition with context-aware annotator selection. ActiveHARNet (Active Learning for Human Activity Recognition)  proposes a resource-efficient deep ensembled model that supports incremental learning and inference on the device, utilizes the approximation in the BNN to represent the uncertainty of the model, and further proves the feasibility of ActiveHARNet deployment and incremental learning on two public datasets.\nFor its part, DALAUP (Deep Active Learning for Anchor User Prediction)  designs a DeepAL framework for anchor user prediction in social networks that reduces the cost of annotating anchor users and improves the prediction accuracy.\nDeepAL is also using in the classification of electrocardiogram (ECG) signals. For example,  proposes an active DL-based ECG signal classification method.  proposed an AL-based ECG classification method using eigenvalues and DL. The use of the AL method enables the cost of marking ECG signals by medical experts to be effectively reduced.\nFurthermore, the cost of label annotation in the speech and audio fields is also relatively high.\n finds that a model trained on a corpus composed of thousands of recordings collected by a small number of speakers is unable to be generalized to new domains; therefore,  developed a practical scheme that involves using AL to train deep neural networks for speech emotion recognition tasks when label resources are limited.\nIn general, the current applications of DeepAL are mainly focused on visual image processing tasks, although there are also applications in NLP and other fields. Compared with DL and AL, DeepAL is still in the preliminary stage of research, meaning that the corresponding classic works are relatively few; however, it still has the same broad application scenarios and practical value as DL. In addition, in order to facilitate readers' access to specific applications of DeepAL in related fields, we have classified and summarized all application scenarios and datasets used by survey-related work in Section \\ref{sec: Various applications of DeepAL} in detail. The specific information is shown in Table \\ref{tab: Application of DeepAL}.\n\\begin{table}[!tp]\n\\caption{DeepAL's research examples in Vision, NLP and other fields.}\n    \\centering\n    \\scriptsize\n    \\begin{threeparttable}\n        \\begin{tabular}{|c|c|l|l|c|}\\toprule\n        \\hline\n             Field & Task & Publications & Datasets & Scenes\\\\ \\hline\n             \\multirow{21}{*}{\\makecell[c]{Vision}} & \\multirow{7}{*}{\\makecell[c]{Image \\\\classification \\\\ and\\\\ recognition}} & \\makecell[l]{} & \\makecell[l]{CACD , Caltech-256 , VidTIMIT ,\\\\ CK , MNIST , CIFAR 10 ,\\\\ emoFBVP , MindReading  \\\\Cool PHP CAPTCHA } & \\makecell[c]{Handwritten numbers, \\\\face, CAPTCHA\\\\ recognition, etc.}\\\\\\cline{3-5}\n             && & \\makecell[l]{PaviaC, PaviaU, Botswana ,\\\\ Salinas Valley, Indian Pines , \\\\Washington DC Mall, Urban } &  \\makecell{Hyperspectral\\\\ image}\\\\\n             \\cline{3-5}\n             &&\\makecell[l]{\\\\\\\\}& \\makecell[l]{Erie County , EEG , \\\\BreaKHis , \\\\ SVEB, SVDB } & Biomedical\\\\\n             \\cline{2-5}\n             & \\multirow{4}{*}{\\makecell[c]{\\makecell[c]{Object \\\\detection}}} & &VOC , Kitti  & --  \\\\\n             \\cline{3-5}\n             &&& \\makecell[l]{SS , eMML , NACTI\\tnote{1}, CCT\\tnote{2}, UAV\\tnote{3}} & \\makecell{Biodiversity survey}\\\\\n             \\cline{3-5}\n             &&  &KITTI  & Autonomous driving\\\\\n             \\cline{3-5}\n             && & NEU-DET  & Defect detection\\\\\n             \\cline{2-5}\n             & \\makecell{Semantic \\\\segmentation} && \\makecell[l]{SPIM , Confocal, LIDC-IDRI ,\\\\MICCAI, Lymph node }& Bio-medical image \\\\\\cline{2-5}\n             & \\multirow{5}{*}{\\makecell[c]{Video \\\\processing}} & & Mash-simulator\\tnote{4} & \\makecell[c]{Autonomous\\\\ navigation}\\\\\n             \\cline{3-5}\n             &&&\\makecell[l]{OPPORTUNITY , WISDM ,\\\\ SenseBox , Skoda Daphnet ,CASAS }&Smart home\\\\\n             \\cline{3-5}\n             &&&\\makecell[l]{PRID , MARS , BDD100K ,\\\\DukeMTMC-VideoReID ,\\\\ CityPersons , Caltech Pedestrian}&Person Re-id\\\\\n             \\hline\n             \\multirow{14}{*}{\\makecell[c]{NLP}} & \\makecell[c]{Machine\\\\ translation} & &\\makecell[l]{OPUS , UNPC ,\\\\IWSLT, WMT }&\\makecell[c]{Ind-En, Ch-En, En-Vi, \\\\Fr-En, En-De, etc.}\\\\\n             \\cline{2-5}\n             & \\makecell[c]{Text \\\\classification}&&\\makecell[l]{CR\\tnote{5}, Subj, MR\\tnote{6}, MuR\\tnote{7}, DR \\\\AGN, DBP, AMZP, AMZF, YRF }&-- \\\\ \\cline{2-5}\n             & \\multirow{3}{*}{\\makecell[c]{Semantic \\\\analysis}} & &MOV , BOO, DVDs, ELE, KIT &\\makecell[c]{Sentiment \\\\classification}\\\\\n             \\cline{3-5}\n             &&&\\makecell[l]{KDnuggets Fake News\\tnote{8}, \\\\Harvard Dataverse , Liar }&\\makecell[c]{News veracity \\\\detection}\\\\\n             \\cline{2-5}\n             &\\multirow{5}{*}{\\makecell[c]{Information \\\\extraction}}&&Italy, Iran-Iraq, Mexico earthquake dataset & Disaster assessment \\\\\n             \\cline{3-5}\n             &&&Temple University Hospital\\tnote{10}& \\makecell[c]{Electroencephalography\\\\(EEG) reports}\\\\\\cline{3-5}\n             &&&\\makecell[l]{CoNLL , NCBI , MedMentions ,\\\\OntoNotes , DBLP, FZ, AG , Cora } &\\makecell[c]{Named entity \\\\recognition (NER)}\\\\\n             \\cline{2-5}\n             &\\multirow{2}{*}{\\makecell[c]{Question\\\\answering}}&&CMDC , JabberWackys chatlogs\\tnote{9} &Dialogue generation\\\\\n             \\cline{3-5}\n             &&&Visual Genome , VQA &\\makecell[c]{Visual question answer (VQA)}\\\\\n             \\hline\n             \\multirow{7}{*}{Other} &\\multirow{7}{*}{--}& &BC, HCC, Lung&Gene expression\\\\\\cline{3-5}\n             &&&EATG , Crazyflie 2.0\\tnote{11} &Robotics\\\\\\cline{3-5}\n             &&&HHAR , NWFD & Smart device\\\\\\cline{3-5}\n             &&&Foursquare, Twitter & Social network\\\\\\cline{3-5}\n             &&&MIT-BIH , INCART, SVDB & \\makecell[c]{Electrocardiogram (ECG)\\\\signal classification}\\\\\\cline{3-5}\n             &&&MSP-Podcast  & \\makecell[c]{Speech emotion recognition}\\\\\\hline\n        \\end{tabular}\n        \\begin{tablenotes}\n           \\footnotesize\n           \\item[1] http://lila.science/datasets/nacti\n           \\item[2] http://lila.science/datasets/caltech-camera-traps\n           \\item[3] http://kuzikus-namibia.de/xe\\_index.html\n           \\item[4] https://github.com/idiap/mash-simulator \n           \\item[5] www.cs.uic.edu/liub/FBS/sentiment-analysis.html\n           \\item[6] Subj and MR datasets are available at: http://www.cs.cornell.edu/people/pabo/movie-review-data/\n           \\item[7] http://www.cs.jhu.edu/mdredze/datasets/sentiment/\n           \\item[8] https://github.com/GeorgeMcIntire/fake\\_real\\_news\\_dataset\n           \\item[9] http://www.jabberwacky.com/j2conversations. JabberWacky is an in-browser, open-domain, retrieval-based bot.\n           \\item[10] https://www.isip.piconepress.com/projects/tuh\\_eeg/\n           \\item[11] https://www.bitcraze.io/\n           \\item[--] Non-specific application scenarios\n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\label{tab: Application of DeepAL}\n\\end{table}", "cites": [5514, 5538, 4023, 5545, 5257, 7948, 1050, 5542, 5508, 5539, 5544, 2893, 2009, 5540, 7796, 4968, 8465, 5541, 5509, 7953, 5506, 5543, 8923, 8422, 1096, 8924], "cite_extract_rate": 0.24074074074074073, "origin_cites_number": 108, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of various DeepAL applications in non-vision/NLP fields, listing tasks, papers, and datasets. While it attempts to synthesize the information by grouping applications by domain (e.g., robotics, ECG signal analysis), it does so in a surface-level manner without deeper integration of ideas. There is little to no critical evaluation of the cited papers or identification of broader principles or limitations."}}
{"id": "eb711660-bd71-47e8-b216-91168204d5d2", "title": "Discussion and future directions", "level": "section", "subsections": [], "parent_id": "69f98c5d-4a32-4911-97b2-4b657d88f671", "prefix_titles": [["title", "A Survey of Deep Active Learning"], ["section", "Discussion and future directions"]], "content": "\\label{sec: Discussion and future directions}\nDeepAL combines the common advantages of DL and AL: it inherits not only DL's ability to process high-dimensional image data and conduct automatic feature extraction but also AL's potential to effectively reduce annotation costs. DeepAL, therefore, has fascinating potential especially in areas where labels require high levels of expertise and are difficult to obtain.\nMost recent work reveals that DeepAL has been successful in many common tasks. DeepAL has attracted the interest of a large number of researchers by reducing the cost of annotation and its ability to implement the powerful feature extraction capabilities of DL; consequently, the related research work is also extremely rich. However, there are still a large number of unanswered questions on this subject.\nAs  discovered, the results reported on the random sampling baseline (RSB) differ significantly between different studies. For example, under the same settings, using 20\\% of the label data of CIFAR 10, the RSB performance reported by  is 13\\% higher than that in . Secondly, the same DeepAL method may yield different results in different studies. For example, using 40\\% of the label data of CIFAR 100  and VGG16  as the extraction network, the reported results of  and  differ by 8\\%. Furthermore, the latest DeepAL research also exhibits some inconsistencies. For example,  and  point out that diversity-based methods have always been better than uncertainty-based methods, and that uncertainty-based methods perform worse than RSB; however, the latest research of  shows that this is not the case.\nCompared with AL's strategic selection of high-value samples, RSB has been regarded as a strong baseline . However, the above problems reveal an urgent need to design a general performance evaluation platform for DeepAL work, as well as to determine a unified high-performance RSB. Secondly, the reproducibility of different DeepAL methods is also an important issue. The highly reproducible DeepAL method helps to evaluate the performance of different DALs. A common evaluation platform should be used for experiments under consistent settings, and snapshots of experimental settings should be shared. In addition, multiple repetitive experiments with different initializations under the same experimental conditions should be implemented, as this could effectively avoid misleading conclusions caused by experimental setup problems. Researchers should pay sufficient attention to these inconsistent studies to enable them to clarify the principles involved. On the other hand, adequate ablation experiments and transfer experiments are also necessary. The former will make it easier for us to determine which improvements bring about performance gains, while the latter can help to ensure that the AL selection strategy does indeed enable the indiscriminate selection of high-value samples for the dataset.\nThe current research directions regarding DeepAL methods focus primarily on the improvement of AL selection strategies, the optimization of training methods, and the improvement of task-independent models. \nAs noted in Section \\ref{sec: Query Strategy Optimization in DeepAL}, the improvement of AL selection strategy is currently centered around taking into account the query strategy based on uncertainty and diversity explicitly or implicitly. Moreover, hybrid selection strategies are increasingly favored by researchers. \nMoreover, the optimization of training methods mainly focuses on labeled datasets, unlabeled datasets, or the use of methods such as GAN to expand data, as well as the hybrid training method of unsupervised, semi-supervised, and supervised learning across the AL cycle. This training method promises to deliver even more performance improvements than are thought to be achievable through changes to the selection strategy. In fact, this makes up for the issues of the DL model requiring a large number of labeled training samples and the AL selecting a limited number of labeled samples. In addition, the use of unlabeled or generated datasets is also conducive to making full use of existing information without adding to the annotation costs. Furthermore, the incremental training method is also an important research direction. From a computing resource perspective, it is unacceptable to train a deep model from scratch in each cycle. While simple incremental training will cause the deviation of model parameters, the huge potential savings on resources are quite attractive. Although related research remains quite scarce, this is still a very promising research direction. \nTask independence is also an important research direction, as it helps to make DeepAL models more directly and widely extensible to other tasks. However, the related research remains insufficient, and the corresponding DeepAL methods tend to focus only on the uncertainty-based selection method. Because DL itself is easier to integrate with the uncertainty-based AL selection strategy, we believe that uncertainty-based methods will continue to dominate research directions not related to these tasks in the future. On the other hand, it may also be advisable to explicitly take the diversity-based selection strategy into account; of course, this will also give rise to great challenges. \nIn addition, it should be pointed out that blindly pursuing the idea of training models on smaller subsets would be unwise, as the relative difference in sample importance in some datasets with a large variety of content and a large number of samples can almost be ignored.\nThere is no conflict between the above-mentioned improvement directions; thus, a mixed improvement strategy is an important development direction for the future. In general, DeepAL research has significant practical application value in terms of both labeling costs and application scenarios; however, DeepAL research remains in its infancy at present, and there is still a long way to go in the future.", "cites": [1042, 5523, 4024, 514, 5534, 8926, 5522], "cite_extract_rate": 0.875, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple cited papers to highlight inconsistencies in reported results and experimental practices, forming a coherent narrative about the current state of DeepAL. It critically analyzes issues such as reproducibility, the need for a unified baseline, and the limitations of current evaluation methods. Additionally, it abstracts beyond individual studies to outline broader research directions and the importance of generalization in DeepAL methods."}}
