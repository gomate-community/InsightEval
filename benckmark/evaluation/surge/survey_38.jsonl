{"id": "4b84ebce-23e0-40d4-9bb9-d39d905945f8", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "f7edebb9-d241-4d6d-820b-72864b086880", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Introduction"]], "content": "\\IEEEPARstart{L}{arge} language models (LLMs) have achieved remarkable success, though they still face significant limitations, especially in domain-specific or knowledge-intensive tasks~, notably producing ``hallucinations\"~ when handling queries beyond their training data or requiring current information. To overcome challenges, Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic similarity calculation. By referencing external knowledge, RAG effectively reduces the problem of generating factually incorrect content. Its integration into LLMs has resulted in widespread adoption, establishing RAG as a key technology in advancing chatbots and enhancing the suitability of LLMs for real-world applications.\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=0.85\\linewidth]{images/rag_tech_tree.png}\n    \\caption{Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs, research on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent research has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models in the pre-training stage through retrieval-augmented techniques.}\n    \\label{fig:rag_tech_tree}\n\\end{figure*}\nRAG technology has rapidly developed in recent years, and the technology tree summarizing related research is shown in Figure~\\ref{fig:rag_tech_tree}. The development trajectory of RAG in the era of large models exhibits several distinct stage characteristics. Initially, RAG's inception coincided with the rise of the Transformer architecture, focusing on enhancing language models by incorporating additional knowledge through Pre-Training Models (PTM). This early stage was characterized by foundational work aimed at refining pre-training techniques.The subsequent arrival of ChatGPT~ marked a pivotal moment, with LLM demonstrating powerful  in context learning (ICL) capabilities. RAG research shifted towards providing better information for LLMs to answer more complex and knowledge-intensive tasks during the inference stage, leading to rapid development in RAG studies. As research progressed, the enhancement of RAG was no longer limited to the inference stage but began to incorporate more with LLM fine-tuning techniques.\nThe burgeoning field of RAG has experienced swift growth, yet it has not been accompanied by a systematic synthesis that could clarify its broader trajectory. This survey endeavors to fill this gap by mapping out the RAG process and charting its evolution and anticipated future paths, with a focus on the integration of RAG within LLMs. This paper considers both technical paradigms and research methods, summarizing three main research paradigms from over 100 RAG studies, and analyzing key technologies in the core stages of ``Retrieval,\" ``Generation,\" and ``Augmentation.\" On the other hand, current research tends to focus more on methods, lacking analysis and summarization of how to evaluate RAG. This paper comprehensively reviews the downstream tasks, datasets, benchmarks, and evaluation methods applicable to RAG. Overall, this paper sets out to meticulously compile and categorize the foundational technical concepts, historical progression, and the spectrum of RAG methodologies and applications that have emerged post-LLMs. It is designed to equip readers and professionals with a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and speculate on upcoming trends and innovations.\nOur contributions are as follows:\n\\begin{itemize}\n\\item In this survey, we present a thorough and systematic review of the state-of-the-art RAG methods, delineating its evolution through paradigms including naive RAG, advanced RAG, and modular RAG. This review contextualizes the broader scope of RAG research within the landscape of LLMs.\n\\item We identify and discuss the central technologies integral to the RAG process, specifically focusing on the aspects of ``Retrieval\", ``Generation'' and ``Augmentation\", and delve into their synergies, elucidating how these components intricately collaborate to form a cohesive and effective RAG framework.\n\\item We have summarized the current assessment methods of RAG, covering 26 tasks, nearly 50 datasets, outlining the evaluation objectives and metrics, as well as the current evaluation benchmarks and tools. Additionally, we anticipate future directions for RAG, emphasizing potential enhancements to tackle current challenges.\n\\end{itemize}\nThe paper unfolds as follows: Section~\\ref{sec:overview}  introduces the main concept and current paradigms of RAG. The following three sections explore core components—``Retrieval'', ``Generation\" and ``Augmentation\", respectively.\nSection~\\ref{sec:retrieval} focuses on optimization methods in retrieval,including indexing, query and embedding optimization.\nSection~\\ref{sec:generation} concentrates on post-retrieval process and LLM fine-tuning in generation.\nSection~\\ref{sec:augmentation} analyzes the three augmentation processes.\nSection~\\ref{sec:evaluation} focuses on RAG's downstream tasks and evaluation system. Section~\\ref{sec:prospects}  mainly discusses the challenges that RAG currently faces and its future development directions. At last, the paper concludes in Section~\\ref{sec:conclusion}.", "cites": [366, 8347, 365, 364, 9108, 7225], "cite_extract_rate": 1.0, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The introduction effectively synthesizes the cited papers to provide a coherent narrative of RAG's development and its role in addressing LLM limitations like hallucination. It shows some critical analysis by highlighting gaps in current research, particularly the lack of systematic evaluation methods for RAG. The section abstracts beyond individual works to define broader paradigms and technical components of RAG, offering a structured overview of the field's progression and future directions."}}
{"id": "fcda3cd5-0fa0-4f07-b7b6-5828701f68da", "title": "Naive RAG", "level": "subsection", "subsections": [], "parent_id": "1b2aeec0-0b7a-4d9b-b74d-19b8e56fd983", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Overview of RAG "], ["subsection", "Naive RAG"]], "content": "The Naive RAG research paradigm represents the earliest methodology, which gained prominence shortly after the widespread adoption of ChatGPT. The Naive RAG follows a traditional process that includes indexing, retrieval, and generation, which is also characterized as a ``Retrieve-Read'' framework~.\n\\emph{Indexing} starts with the cleaning and extraction of raw data in diverse formats like PDF, HTML, Word, and Markdown, which is  then converted into a uniform plain text format. To accommodate the context limitations of language models,  text is segmented into smaller, digestible chunks. Chunks are then encoded into vector representations using an embedding model and stored in vector database. This step is crucial for enabling efficient similarity searches in the subsequent retrieval phase.\n\\emph{Retrieval}. Upon receipt of a user query,  the RAG system employs the same encoding model utilized during the indexing phase to transform the query into a vector representation. It then computes the similarity scores between the query vector and the vector of  chunks within the indexed corpus. The system prioritizes and retrieves the top K chunks that demonstrate the greatest similarity to the query. These chunks are subsequently used as the expanded context in prompt.\n\\emph{Generation}. The posed query and selected documents are synthesized into a coherent prompt to which a large  language model is tasked with formulating a response. The model's approach to answering may vary depending on task-specific criteria, allowing it to either draw upon its inherent parametric knowledge or restrict its responses to the information contained within the provided documents. In cases of ongoing dialogues, any existing conversational history can be integrated into the prompt, enabling the model to engage in multi-turn dialogue interactions effectively.\nHowever, Naive RAG encounters notable drawbacks:\n\\emph{Retrieval Challenges}. The retrieval phase often struggles with precision and recall, leading to the selection of misaligned or irrelevant chunks, and the missing of crucial information.\n\\emph{Generation Difficulties}. In generating responses, the model may face the issue of hallucination, where it produces content not supported by the retrieved context. This phase can also suffer from irrelevance, toxicity, or bias in the outputs, detracting from the quality and reliability of the responses.\n\\emph{Augmentation Hurdles}. Integrating retrieved information with the different task can be challenging, sometimes resulting in disjointed or incoherent outputs. The process may also encounter redundancy when similar information is retrieved from multiple sources, leading to repetitive responses. Determining the significance and relevance of various passages and ensuring stylistic and tonal consistency add further complexity. Facing complex issues, a single retrieval based on the original query may not suffice to acquire adequate context information.\nMoreover, there's a concern that generation models might overly rely on augmented information, leading to outputs that simply echo retrieved content without adding insightful or synthesized information.", "cites": [367], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a clear and factual description of the Naive RAG process, including indexing, retrieval, and generation. It integrates some ideas from the cited paper (e.g., the 'retrieve-then-read' pipeline) but does not connect or synthesize concepts across multiple sources. There is minimal critical analysis, and the abstraction level remains low, focusing on specific components rather than broader trends or principles."}}
{"id": "2b7e76a0-9ae5-4fc6-b7d2-bb95cf1a62c5", "title": "Advanced RAG", "level": "subsection", "subsections": [], "parent_id": "1b2aeec0-0b7a-4d9b-b74d-19b8e56fd983", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Overview of RAG "], ["subsection", "Advanced RAG"]], "content": "Advanced RAG introduces specific improvements to overcome the limitations of Naive RAG. Focusing on enhancing retrieval quality, it employs pre-retrieval and post-retrieval strategies. To tackle the indexing issues, Advanced RAG refines its indexing techniques through the use of a sliding window approach, fine-grained segmentation, and the incorporation of metadata. Additionally, it incorporates several optimization methods to streamline the retrieval process. \n\\emph{Pre-retrieval process}. In this stage, the primary focus is on optimizing the indexing structure and the original query. The goal of optimizing  indexing is to enhance the quality of the content being indexed. This involves strategies: enhancing data granularity, optimizing index structures, adding metadata, alignment optimization, and mixed retrieval. While the goal of query optimization is to make the user's original question clearer and more suitable for the retrieval task. Common methods include query rewriting query transformation, query expansion and other techniques~.\n\\emph{Post-Retrieval Process}. Once relevant context is retrieved, it's crucial to integrate it effectively with the query. The main methods in post-retrieval process include rerank chunks and context compressing.  Re-ranking the retrieved information to relocate the most relevant content to the edges of the prompt is a key strategy. This concept has been implemented in frameworks such as LlamaIndex\\footnote{\\url{https://www.llamaindex.ai}}, LangChain\\footnote{\\url{https://www.langchain.com/}}, and HayStack~. Feeding all relevant documents directly into LLMs can lead to information overload, diluting the focus on key details with irrelevant content.To mitigate this, post-retrieval efforts concentrate on selecting the essential information, emphasizing critical sections, and shortening the context to be processed. \n\\begin{figure*}\n    \\centering\n    \\includegraphics[scale=0.25]{images/RAG_FrameCompre_eng.png}\n    \\caption{Comparison between the three paradigms of RAG. (Left) Naive RAG  mainly consists of three parts: indexing, retrieval and generation. (Middle) Advanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a chain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the introduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and generation; it includes methods such as iterative and adaptive retrieval.}\n    \\label{fig:RAG_comp}\n\\end{figure*}", "cites": [368, 370, 367, 369], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates multiple ideas from the cited papers to explain pre- and post-retrieval strategies in Advanced RAG, showing moderate synthesis. It provides a general analytical framework by discussing optimization goals and techniques, but lacks deeper critical evaluation of the individual works or their trade-offs. The abstraction is reasonable, identifying broader strategies like query rewriting and reranking, but it remains focused on the technical structure rather than offering high-level, meta-insights."}}
{"id": "98a52f80-e795-45b2-bc69-920fb5afc426", "title": "Modular RAG", "level": "subsection", "subsections": ["c8ebbb5c-28b5-4655-b1a0-371e090ab250", "5d106d1d-b3ff-4921-942c-e3150b6f32b8"], "parent_id": "1b2aeec0-0b7a-4d9b-b74d-19b8e56fd983", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Overview of RAG "], ["subsection", "Modular RAG"]], "content": "The modular RAG architecture advances beyond the former two RAG paradigms, offering enhanced adaptability and versatility. It incorporates diverse strategies for improving its components, such as adding a search module for similarity searches and refining the retriever through fine-tuning. Innovations like restructured RAG modules~ and rearranged RAG pipelines~  have been introduced to tackle specific challenges. The shift towards a modular RAG approach is becoming prevalent, supporting both sequential processing and integrated end-to-end training across its components. Despite its distinctiveness, Modular RAG builds upon the foundational principles of Advanced and Naive RAG, illustrating a progression and refinement within the RAG family.", "cites": [371, 372], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic overview of Modular RAG and mentions cited papers as examples of its components, but it lacks deeper synthesis or integration of their contributions. It describes innovations and strategies without critically evaluating their effectiveness or limitations. The abstraction level is minimal, as it does not generalize these approaches into broader trends or principles."}}
{"id": "c8ebbb5c-28b5-4655-b1a0-371e090ab250", "title": "New Modules", "level": "subsubsection", "subsections": [], "parent_id": "98a52f80-e795-45b2-bc69-920fb5afc426", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Overview of RAG "], ["subsection", "Modular RAG"], ["subsubsection", "New Modules"]], "content": "The Modular RAG framework introduces additional specialized components to enhance retrieval and processing capabilities. The Search module adapts to specific scenarios, enabling direct searches across various data sources like search engines, databases, and knowledge graphs, using LLM-generated code and query languages~. RAG-Fusion addresses traditional search limitations by employing a multi-query strategy that expands user queries into diverse perspectives, utilizing parallel vector searches and intelligent re-ranking to uncover both explicit and transformative knowledge~. The Memory module leverages the LLM's memory to guide retrieval, creating an unbounded memory pool that aligns the text more closely with data distribution through iterative self-enhancement~. Routing in the RAG system navigates through diverse data sources, selecting the optimal pathway for a query, whether it involves summarization, specific database searches, or merging different information streams~. The Predict module aims to reduce redundancy and noise by generating context directly through the LLM, ensuring relevance and accuracy~. Lastly, the Task Adapter module tailors RAG to various downstream tasks, automating prompt retrieval for zero-shot inputs and creating task-specific retrievers through few-shot query generation~ .This comprehensive approach not only streamlines the retrieval process but also significantly improves the quality and relevance of the information retrieved, catering to a wide array of tasks and queries with enhanced precision and flexibility.", "cites": [373, 8348, 375, 376, 374, 372], "cite_extract_rate": 0.75, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section describes several new modules in Modular RAG frameworks and briefly associates each with a cited paper. While it introduces the components and their purposes, it primarily functions as a descriptive overview without substantial synthesis, critical comparison, or abstracting into broader principles. Some integration is present, but deeper analysis is lacking."}}
{"id": "5d106d1d-b3ff-4921-942c-e3150b6f32b8", "title": "New Patterns", "level": "subsubsection", "subsections": [], "parent_id": "98a52f80-e795-45b2-bc69-920fb5afc426", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Overview of RAG "], ["subsection", "Modular RAG"], ["subsubsection", "New Patterns"]], "content": "Modular RAG offers remarkable adaptability by allowing module substitution or reconfiguration to address specific challenges. This goes beyond the fixed structures of Naive and Advanced RAG, characterized by a simple ``Retrieve\" and ``Read\" mechanism. Moreover, Modular RAG expands this flexibility by integrating new modules or adjusting interaction flow among existing ones, enhancing its applicability across different tasks.\nInnovations such as the Rewrite-Retrieve-Read~model leverage the LLM's capabilities to refine retrieval queries through a rewriting module and a LM-feedback mechanism to update rewriting model., improving task performance. Similarly, approaches like Generate-Read~ replace traditional retrieval with LLM-generated content, while Recite-Read~ emphasizes retrieval from model weights, enhancing the model's ability to handle knowledge-intensive tasks. Hybrid retrieval strategies integrate keyword, semantic, and vector searches to cater to diverse queries. Additionally, employing sub-queries and hypothetical document embeddings (HyDE)~ seeks to improve retrieval relevance by focusing on embedding similarities between generated answers and real documents.\nAdjustments in module arrangement and interaction, such as the Demonstrate-Search-Predict (DSP)~ framework and the iterative Retrieve-Read-Retrieve-Read flow of ITER-RETGEN~, showcase the dynamic use of module outputs to bolster another module's functionality, illustrating a sophisticated understanding of enhancing module synergy. The flexible orchestration of Modular RAG Flow showcases the benefits of adaptive retrieval through techniques such as FLARE~ and Self-RAG~. This approach transcends the fixed RAG retrieval process by evaluating the necessity of retrieval based on different scenarios. Another benefit of a flexible architecture is that the RAG system can more easily integrate with other technologies (such as fine-tuning or reinforcement learning)~. For example, this can involve fine-tuning the retriever for better retrieval results, fine-tuning the generator for more personalized outputs, or engaging in collaborative fine-tuning~.", "cites": [380, 378, 377, 371, 367, 372, 369, 7226, 381, 379], "cite_extract_rate": 1.0, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes a range of recent papers, connecting diverse innovations such as query rewriting, hybrid retrieval, and self-reflection into a coherent narrative about the adaptability and flexibility of Modular RAG. It moves beyond description by analyzing how each module or pattern contributes to addressing specific limitations of earlier RAG approaches. The section abstracts these methods into broader architectural patterns, such as iterative processes and flexible retrieval decisions, indicating a strong insight level."}}
{"id": "632e22d7-7679-44de-bbb8-3b68f840a0a3", "title": "RAG vs Fine-tuning", "level": "subsection", "subsections": [], "parent_id": "1b2aeec0-0b7a-4d9b-b74d-19b8e56fd983", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Overview of RAG "], ["subsection", "RAG vs Fine-tuning"]], "content": "The augmentation of LLMs has attracted considerable attention due to their growing prevalence. Among the optimization methods for LLMs, RAG is often compared with Fine-tuning (FT) and prompt engineering. Each method has distinct characteristics as illustrated in Figure~\\ref{fig:ragft}. We used a quadrant chart to illustrate the differences among three methods in two dimensions: external knowledge requirements and model adaption requirements. Prompt engineering leverages a model's inherent capabilities with minimum necessity for external knowledge and model adaption. RAG can be likened to providing a model with a tailored textbook for information retrieval, ideal for precise information retrieval tasks. In contrast, FT is comparable to a student internalizing knowledge over time, suitable for scenarios requiring replication of specific structures, styles, or formats.  \nRAG excels in dynamic environments by offering real-time knowledge updates and effective utilization of external knowledge sources with high interpretability. However, it comes with higher latency and ethical considerations regarding data retrieval. On the other hand, FT is more static, requiring retraining for updates but enabling deep customization of the model's behavior and style. It demands significant computational resources for dataset preparation and training, and while it can reduce hallucinations, it may face challenges with unfamiliar data.\nIn multiple evaluations of  their performance on various knowledge-intensive tasks across different topics,   revealed that while unsupervised fine-tuning shows some improvement, RAG consistently outperforms it, for both existing knowledge encountered during training and entirely new knowledge. Additionally, it was found that LLMs struggle to learn new factual information through unsupervised fine-tuning. The choice between RAG and FT depends on the specific needs for data dynamics, customization, and computational capabilities in the application context. RAG and FT are not mutually exclusive and can complement each other, enhancing a model's capabilities at different levels.  In some instances, their combined use may lead to optimal performance. The optimization process involving RAG and FT may require multiple iterations to achieve satisfactory results.\n\\begin{figure*}[tb]\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{images/rag_FT.png}\n    \\caption{RAG compared with other model optimization methods in the aspects of ``External Knowledge Required\" and ``Model Adaption Required\". Prompt Engineering requires low modifications to the model and external knowledge, focusing on harnessing the capabilities of LLMs themselves. Fine-tuning, on the other hand, involves further training the model. In the early stages of RAG (Naive RAG), there is a low demand for model modifications. As research progresses, Modular RAG has become more integrated with fine-tuning techniques.}\n    \\label{fig:ragft}\n\\end{figure*}\n\\begin{table*}\n\\caption{Summary of RAG methods}\n    \\label{tab:RAG_summary}\n    \\centering\n    \\scalebox{0.8}{\n    \\begin{tabular}{cccccc} \n\\toprule\n\\multicolumn{1}{c}{Method} & \\multicolumn{1}{c}{Retrieval Source} & \\multicolumn{1}{c}{\\begin{tabular}[c]{@{}c@{}}Retrieval\\\\ Data Type\\end{tabular}} & \\multicolumn{1}{c}{\\begin{tabular}[c]{@{}c@{}}Retrieval\\\\ Granularity\\end{tabular}} & \\multicolumn{1}{c}{\\begin{tabular}[c]{@{}c@{}}Augmentation \\\\ Stage\\end{tabular}} & \\multicolumn{1}{c}{\\begin{tabular}[c]{@{}c@{}}Retrieval \\\\ process\\end{tabular}} \\\\\n\\midrule\nCoG~& Wikipedia& Text& Phrase& Pre-training&Iterative\\\\\nDenseX~& FactoidWiki& Text& Proposition& Inference&Once\\\\\nEAR~& Dataset-base& Text& Sentence& Tuning&Once\\\\\nUPRISE~& Dataset-base& Text& Sentence& Tuning&Once\\\\\nRAST~& Dataset-base& Text& Sentence& Tuning&Once\\\\\nSelf-Mem~& Dataset-base& Text& Sentence& Tuning&Iterative\\\\\nFLARE~& Search Engine,Wikipedia& Text& Sentence& Tuning&Adaptive\\\\\nPGRA~& Wikipedia& Text& Sentence& Inference&Once\\\\\nFILCO~& Wikipedia& Text& Sentence& Inference&Once\\\\\nRADA~& Dataset-base& Text& Sentence& Inference&Once\\\\\nFilter-rerank~& Synthesized dataset& Text& Sentence& Inference&Once\\\\\nR-GQA~& Dataset-base& Text& Sentence Pair& Tuning&Once\\\\\nLLM-R~& Dataset-base& Text& Sentence Pair& Inference&Iterative\\\\\nTIGER~& Dataset-base& Text& Item-base& Pre-training&Once\\\\\nLM-Indexer~& Dataset-base& Text& Item-base& Tuning&Once\\\\\nBEQUE~& Dataset-base& Text& Item-base& Tuning&Once\\\\\nCT-RAG~& Synthesized dataset& Text& Item-base& Tuning&Once\\\\\nAtlas~&   Wikipedia, Common Crawl&Text&  Chunk&  Pre-training&Iterative\\\\ \nRAVEN~& Wikipedia& Text& Chunk& Pre-training&Once\\\\\nRETRO++~& Pre-training Corpus& Text& Chunk& Pre-training&Iterative\\\\\nINSTRUCTRETRO~&Pre-training corpus& Text& Chunk & Pre-training&Iterative\\\\\nRRR~& Search Engine& Text& Chunk& Tuning&Once\\\\\nRA-e2e~& Dataset-base& Text& Chunk& Tuning&Once\\\\\nPROMPTAGATOR~& BEIR&Text&  Chunk&  Tuning&Once\\\\\nAAR~& MSMARCO,Wikipedia& Text& Chunk& Tuning&Once\\\\\nRA-DIT~& Common Crawl,Wikipedia& Text& Chunk& Tuning&Once\\\\\nRAG-Robust~& Wikipedia& Text& Chunk& Tuning&Once\\\\\nRA-Long-Form~ & Dataset-base& Text& Chunk& Tuning&Once\\\\\nCoN~& Wikipedia& Text& Chunk& Tuning&Once\\\\\nSelf-RAG~& Wikipedia& Text& Chunk& Tuning&Adaptive\\\\\nBGM~& Wikipedia&Text & Chunk& Inference&Once\\\\\nCoQ~& Wikipedia& Text& Chunk& Inference&Iterative\\\\\nToken-Elimination~& Wikipedia& Text& Chunk& Inference&Once\\\\\nPaperQA~& Arxiv,Online Database,PubMed& Text& Chunk& Inference&Iterative\\\\\nNoiseRAG~& FactoidWiki& Text& Chunk& Inference&Once\\\\\nIAG~& Search Engine,Wikipedia& Text& Chunk& Inference&Once\\\\\nNoMIRACL~& Wikipedia& Text& Chunk& Inference&Once\\\\\nToC~& Search Engine,Wikipedia& Text& Chunk& Inference&Recursive\\\\\nSKR~& Dataset-base,Wikipedia& Text& Chunk& Inference&Adaptive\\\\\nITRG~& Wikipedia& Text& Chunk& Inference&Iterative\\\\\nRAG-LongContext~& Dataset-base& Text& Chunk& Inference&Once\\\\\nITER-RETGEN~& Wikipedia& Text& Chunk& Inference&Iterative\\\\\nIRCoT~& Wikipedia& Text& Chunk& Inference&Recursive\\\\\nLLM-Knowledge-Boundary~& Wikipedia& Text& Chunk& Inference&Once\\\\\nRAPTOR~& Dataset-base&Text& Chunk& Inference&Recursive\\\\\nRECITE~&   LLMs&Text&  Chunk&  Inference&Once\\\\ \nICRALM~& Pile,Wikipedia& Text& Chunk& Inference&Iterative\\\\\nRetrieve-and-Sample~& Dataset-base& Text& Doc& Tuning&Once\\\\\nZemi~& C4& Text& Doc& Tuning&Once\\\\\nCRAG~& Arxiv& Text& Doc & Inference&Once\\\\\n1-PAGER~ & Wikipedia& Text& Doc& Inference&Iterative\\\\\nPRCA~& Dataset-base& Text& Doc& Inference&Once\\\\\nQLM-Doc-ranking~& Dataset-base& Text& Doc& Inference&Once\\\\\nRecomp~& Wikipedia& Text& Doc& Inference&Once\\\\\nDSP~& Wikipedia& Text& Doc& Inference&Iterative\\\\\nRePLUG~& Pile& Text& Doc& Inference&Once\\\\\nARM-RAG~& Dataset-base& Text& Doc& Inference&Iterative\\\\\nGenRead~&   LLMs&Text&  Doc&  Inference&Iterative\\\\ \nUniMS-RAG~& Dataset-base& Text& Multi& Tuning&Once\\\\\nCREA-ICL~& Dataset-base& Crosslingual,Text& Sentence& Inference&Once\\\\\nPKG~& LLM& Tabular,Text& Chunk& Inference&Once\\\\\nSANTA~& Dataset-base& Code,Text & Item &Pre-training&Once\\\\\nSURGE~&Freebase& KG& Sub-Graph& Tuning&Once\\\\\nMK-ToD~& Dataset-base& KG& Entity& Tuning&Once\\\\\nDual-Feedback-ToD~& Dataset-base& KG& Entity Sequence& Tuning&Once\\\\\nKnowledGPT~& Dataset-base& KG& Triplet& Inference&Muti-time\\\\\nFABULA~& Dataset-base,Graph& KG& Entity& Inference&Once\\\\\nHyKGE~& CMeKG& KG& Entity& Inference&Once\\\\\nKALMV~& Wikipedia& KG&Triplet & Inference&Iterative\\\\\nRoG~& Freebase& KG& Triplet& Inference&Iterative\\\\\nG-Retriever~& Dataset-base& TextGraph& Sub-Graph& Inference&Once\\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{table*}", "cites": [409, 400, 8350, 373, 378, 386, 383, 375, 399, 367, 393, 372, 394, 379, 397, 380, 406, 377, 368, 389, 7024, 407, 398, 376, 374, 415, 7226, 381, 418, 8351, 420, 412, 7025, 414, 402, 8348, 396, 7227, 419, 390, 401, 385, 405, 404, 408, 403, 8349, 411, 8352, 388, 410, 417, 387, 371, 7228, 413, 384, 416, 392, 382, 391, 395], "cite_extract_rate": 0.8611111111111112, "origin_cites_number": 72, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.7, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes a large number of cited papers to establish a nuanced comparison between RAG and fine-tuning. It constructs a conceptual framework (e.g., the quadrant chart) to highlight distinctions and interdependencies, offering meta-level insights. While it provides a strong analytical foundation, deeper critiques of individual papers or methodological trade-offs could be further elaborated."}}
{"id": "ae9e0220-9c05-4b64-b133-7d132ef7418c", "title": "Data Structure", "level": "subsubsection", "subsections": [], "parent_id": "621473ff-b47d-44b6-984b-aa3412f20f7d", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Retrieval"], ["subsection", "Retrieval Source"], ["subsubsection", "Data Structure"]], "content": "Initially, text is s the mainstream source of retrieval. Subsequently, the retrieval source expanded to include semi-structured data (PDF) and  structured data (Knowledge Graph, KG) for enhancement. In addition to retrieving from original external sources, there is also a growing trend in recent researches towards utilizing content generated by LLMs themselves for retrieval and enhancement purposes.\n\\emph{Unstructured Data}, such as text, is the most  widely used retrieval source, which are mainly gathered from corpus. For open-domain question-answering (ODQA) tasks, the primary retrieval sources are Wikipedia  Dump  with the current major versions including  HotpotQA~\\footnote{\\url{https://hotpotqa.github.io/wiki-readme.html}} (1st October , 2017), DPR\\footnote{\\url{https://github.com/facebookresearch/DPR}} (20 December, 2018). In addition to encyclopedic data, common unstructured data includes cross-lingual text~ and domain-specific data (such as medical~and legal domains~). \n\\emph{Semi-structured data}. typically refers to data that contains a combination of text and table information, such as PDF. Handling semi-structured data poses challenges for conventional RAG systems due to two main reasons. Firstly, text splitting processes may inadvertently separate tables, leading to data corruption during retrieval. Secondly, incorporating tables into the data can complicate semantic similarity searches. When dealing with semi-structured data, one approach involves leveraging the code capabilities of LLMs to execute Text-2-SQL queries on tables within databases, such as TableGPT~. Alternatively, tables can be transformed into text format for further analysis using text-based methods~. However, both of these methods are not optimal solutions, indicating substantial research opportunities in this area.\n\\emph{Structured data}, such as knowledge graphs (KGs)~ , which are typically verified and can  provide more precise information. KnowledGPT~ generates KB search queries and stores knowledge in a personalized base, enhancing the RAG model's knowledge richness. In response to the limitations of LLMs in understanding and answering questions about textual graphs, G-Retriever~  integrates Graph Neural Networks (GNNs), LLMs and RAG, enhancing graph comprehension and question-answering capabilities through soft prompting of the LLM, and employs the Prize-Collecting Steiner Tree (PCST) optimization problem for targeted graph retrieval. On the contrary, it requires additional effort to build, validate, and maintain structured databases. On the contrary, it requires additional effort to build, validate, and maintain structured databases.\n\\emph{LLMs-Generated Content.} Addressing the limitations of external auxiliary information in RAG, some research has focused on exploiting LLMs' internal knowledge. SKR~ classifies questions as known or unknown, applying retrieval enhancement selectively. GenRead~ replaces the retriever with an LLM generator, finding that LLM-generated contexts often contain more accurate answers due to better alignment with the pre-training objectives of causal language modeling. Selfmem~ iteratively creates an unbounded memory pool with a retrieval-enhanced generator, using a memory selector to choose outputs that serve as dual problems to the original question, thus self-enhancing the generative model. These methodologies underscore the breadth of innovative data source utilization in RAG, striving to improve model performance and task effectiveness.", "cites": [373, 419, 396, 375, 387, 421, 398, 399, 374, 372], "cite_extract_rate": 0.9090909090909091, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to explain the evolution and challenges of different retrieval data structures in RAG. It provides a structured overview of unstructured, semi-structured, and structured data, along with the use of LLM-generated content, and includes some analysis of limitations and opportunities. However, while it connects ideas, it does not fully develop a novel framework or provide deep, comparative critique across all cited works."}}
{"id": "23fc4048-afba-4197-ae9e-763ab75ae7ea", "title": "Retrieval Granularity", "level": "subsubsection", "subsections": [], "parent_id": "621473ff-b47d-44b6-984b-aa3412f20f7d", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Retrieval"], ["subsection", "Retrieval Source"], ["subsubsection", "Retrieval Granularity"]], "content": "Another important factor besides the data format of the retrieval source is the granularity of the retrieved data. Coarse-grained retrieval units theoretically can provide more relevant information for the problem, but they may also contain redundant content, which could distract the retriever and language models in downstream tasks~. On the other hand,  fine-grained retrieval unit granularity increases the burden of retrieval  and does not guarantee semantic integrity and meeting the required knowledge. Choosing the appropriate retrieval granularity during inference can be a simple and effective strategy to improve the retrieval and downstream task performance of dense retrievers.\nIn text, retrieval granularity ranges from fine to coarse, including Token, Phrase, Sentence, Proposition, Chunks, Document. Among them, DenseX~proposed the concept of using propositions as retrieval units. Propositions are defined as atomic expressions in the text, each encapsulating a unique factual segment and presented in a concise, self-contained natural language format. This approach aims to enhance retrieval precision and relevance. On the Knowledge Graph (KG), retrieval granularity includes Entity, Triplet, and sub-Graph. The granularity of retrieval can also be adapted to downstream tasks, such as retrieving Item IDs~in recommendation tasks and Sentence pairs~. Detailed information is illustrated in Table~\\ref{tab:RAG_summary}.", "cites": [8353, 393, 403, 394], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple cited papers to discuss retrieval granularity in RAG systems, connecting ideas about different units of retrieval (e.g., propositions, entities, triplets) and their impact on model performance. It includes some critical points, such as the trade-offs between coarse and fine granularity, but does not deeply evaluate limitations or compare methodologies. The abstraction is moderate as it identifies a broader pattern in the design choice of retrieval units and their influence on downstream tasks."}}
{"id": "e8a2dc98-283e-4e76-8d15-25edda188f71", "title": "Structural Index", "level": "subsubsection", "subsections": [], "parent_id": "dc04ad06-8f47-4a48-bc10-8e37b43137e8", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Retrieval"], ["subsection", "Indexing Optimization"], ["subsubsection", "Structural Index"]], "content": "One effective method for enhancing information retrieval is to establish a hierarchical structure for the documents. By constructing In structure, RAG system can expedite the retrieval and processing of pertinent data.\n\\emph{Hierarchical index structure}.  File are arranged in parent-child relationships, with chunks linked to them. Data summaries are stored at each node, aiding in the swift traversal of data and assisting the RAG system in determining which chunks to extract. This approach can also mitigate the illusion caused by block extraction issues.\n\\emph{Knowledge Graph index}. Utilize KG in constructing the hierarchical structure of documents contributes to maintaining consistency. It delineates the connections between different concepts and entities, markedly reducing the potential for illusions. Another advantage is the transformation of the information retrieval process into instructions that LLM can comprehend, thereby enhancing the accuracy of knowledge retrieval and enabling LLM to generate contextually coherent responses, thus improving the overall efficiency of the RAG system. To capture the logical relationship between document content and structure, KGP~ proposed a method of building an index between multiple documents using KG. This KG consists of nodes (representing paragraphs or structures in the documents, such as pages and tables) and edges (indicating semantic/lexical similarity between paragraphs or relationships within the document structure), effectively addressing knowledge retrieval and reasoning problems in a multi-document environment.", "cites": [422], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of structural indexing methods by integrating the concept of hierarchical and knowledge graph indices, particularly referencing the KGP paper. It connects the role of structure in improving retrieval and reducing hallucinations. However, the critical evaluation is limited, and while some general patterns are identified, the abstraction remains at a moderate level without deep meta-level insights."}}
{"id": "b6c9af91-9dd3-4f3f-9c77-772647faa4d2", "title": "Query Expansion", "level": "subsubsection", "subsections": [], "parent_id": "1d58e3f0-1df5-4407-a3cd-24b10784d8bc", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Retrieval"], ["subsection", "Query Optimization"], ["subsubsection", "Query Expansion"]], "content": "Expanding a single query into multiple queries enriches the content of the query, providing further context to address any lack of specific nuances, thereby ensuring the optimal relevance of the generated answers.\n\\emph{Multi-Query}. By employing prompt engineering to expand queries via LLMs, these queries can then be executed in parallel. The expansion of queries is not random, but rather meticulously designed. \n\\emph{Sub-Query}. The process of sub-question planning represents the generation of the necessary sub-questions to contextualize and fully answer the original question when combined. This process of adding relevant context is, in principle, similar to query expansion. Specifically, a complex question can be decomposed into a series of simpler sub-questions using the least-to-most prompting method~.\n\\emph{Chain-of-Verification(CoVe)}. The expanded queries undergo validation by LLM to achieve the effect of reducing hallucinations. Validated expanded queries typically exhibit higher reliability~.", "cites": [424, 423], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the key ideas from both cited papers by linking query expansion with sub-question planning and verification. It abstracts these methods under the broader theme of improving retrieval context for RAG. However, it lacks deeper critical analysis, such as evaluating trade-offs or limitations between the approaches."}}
{"id": "0584877a-293f-4a3d-9754-67ec7aa149ab", "title": "Query Transformation", "level": "subsubsection", "subsections": [], "parent_id": "1d58e3f0-1df5-4407-a3cd-24b10784d8bc", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Retrieval"], ["subsection", "Query Optimization"], ["subsubsection", "Query Transformation"]], "content": "The core concept is to retrieve chunks based on a transformed query instead of the user’s original query.\n\\emph{Query Rewrite}.The original queries are not always optimal for LLM retrieval, especially in real-world scenarios. Therefore, we can prompt LLM to rewrite the queries. In addition to using LLM for query rewriting, specialized smaller language models, such as RRR (Rewrite-retrieve-read)~. The implementation of the query rewrite method in the Taobao, known as BEQUE~ has notably enhanced recall effectiveness for long-tail queries, resulting in a rise in GMV.\nAnother query transformation method is to use prompt engineering to let LLM generate a query based on the original query for subsequent retrieval. HyDE~ construct  hypothetical documents  (assumed answers to the original query). It focuses on embedding similarity from answer to answer rather than seeking embedding similarity for the problem or query. Using the Step-back Prompting method~, the original query is abstracted to generate a high-level concept question (step-back question). In the RAG system, both the step-back question and the original query are used for retrieval, and both the results are utilized as the basis for language model answer generation.", "cites": [368, 367, 370, 369], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple query transformation approaches, including query rewrite and HyDE, and connects them under the broader theme of improving retrieval effectiveness. It offers some critical discussion by highlighting that original queries may not be optimal and that methods like BEQUE and Step-back Prompting address specific challenges. While it identifies patterns in how query transformation enhances performance, it stops short of deeper critique or proposing a novel meta-framework."}}
{"id": "ae59a4de-502e-4291-9885-2366b67caf8f", "title": "Embedding", "level": "subsection", "subsections": ["426544ed-f0ef-45df-973e-ba3284764b95", "22463a5a-ee65-44e3-b8dc-ae280706c074"], "parent_id": "624953b6-2a68-46e9-9c98-05b747ddf82a", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Retrieval"], ["subsection", "Embedding"]], "content": "In RAG, retrieval is achieved by calculating the similarity (e.g. cosine similarity) between the embeddings of the question and document chunks, where the semantic representation capability of embedding models plays a key role. This mainly includes a sparse encoder (BM25) and a dense retriever (BERT architecture Pre-training language models). Recent research has introduced prominent embedding models such as AngIE, Voyage, BGE,etc~, which are benefit from multi-task instruct tuning. Hugging Face’s MTEB leaderboard~\\footnote{\\url{https://huggingface.co/spaces/mteb/leaderboard}} evaluates embedding models across 8 tasks, covering 58 datasests. Additionally, C-MTEB  focuses on  Chinese capability, covering 6 tasks and 35 datasets. There is no one-size-fits-all answer to “which embedding model to use.” However,  some specific models are better suited for particular use cases.", "cites": [425], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic overview of embedding models used in retrieval for RAG, mentioning both sparse and dense methods and listing specific models like AnglE, Voyage, and BGE. It cites one paper but does not synthesize insights from multiple sources or provide a comparative analysis of the models. There is minimal critical evaluation or abstraction to broader principles, making the section primarily descriptive."}}
{"id": "22463a5a-ee65-44e3-b8dc-ae280706c074", "title": "Fine-tuning Embedding Model", "level": "subsubsection", "subsections": [], "parent_id": "ae59a4de-502e-4291-9885-2366b67caf8f", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Retrieval"], ["subsection", "Embedding"], ["subsubsection", "Fine-tuning Embedding Model"]], "content": "In instances where the context significantly deviates from pre-training corpus, particularly within highly specialized disciplines such as healthcare, legal practice, and other sectors replete with proprietary jargon, fine-tuning the embedding model on your own domain dataset becomes essential to mitigate such discrepancies.\nIn addition to supplementing domain knowledge, another purpose of fine-tuning is to align the retriever and generator, for example, using the results of LLM as the supervision signal for fine-tuning, known as LSR (LM-supervised Retriever). PROMPTAGATOR~ utilizes the LLM as a few-shot query generator to create task-specific retrievers, addressing challenges in supervised fine-tuning, particularly in data-scarce domains. Another approach, LLM-Embedder~, exploits LLMs to generate reward signals across multiple downstream tasks. The retriever is fine-tuned with two types of supervised signals: hard labels for the dataset and soft rewards from the LLMs. This dual-signal approach fosters a more effective fine-tuning process, tailoring the embedding model to diverse downstream applications. REPLUG~ utilizes a retriever and an LLM to calculate the probability distributions of the retrieved documents and then performs supervised training by computing the KL divergence. This straightforward and effective training method enhances the performance of the retrieval model by using an LM as the supervisory signal, eliminating the need for specific cross-attention mechanisms. Moreover, inspired by RLHF (Reinforcement Learning from Human Feedback), utilizing LM-based feedback to reinforce the retriever through reinforcement learning.", "cites": [426, 7228, 376], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from three cited papers to explain the motivations and methods for fine-tuning embedding models in RAG systems. It highlights connections between domain adaptation, alignment of retriever and generator, and the use of LLM-based supervision, which suggests a moderate level of integration. While it provides some analysis (e.g., addressing data-scarce domains, using KL divergence), it does not extensively critique or compare approaches in depth. The abstraction is reasonable, as it generalizes the role of fine-tuning for both domain alignment and performance improvement across diverse tasks."}}
{"id": "195d0107-7d76-4dbb-a1f9-9b17e3cd3ff8", "title": "Adapter", "level": "subsection", "subsections": [], "parent_id": "624953b6-2a68-46e9-9c98-05b747ddf82a", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Retrieval"], ["subsection", "Adapter"]], "content": "Fine-tuning models may present challenges, such as integrating functionality through an API or addressing constraints arising from limited local computational resources. Consequently, some approaches opt to incorporate an external adapter to aid in alignment.\nTo optimize the multi-task capabilities of LLM, UPRISE~ trained a lightweight prompt retriever that can automatically retrieve prompts from a pre-built prompt pool that are suitable for a given zero-shot task input. AAR (Augmentation-Adapted Retriver)~ introduces a universal adapter designed to accommodate multiple downstream tasks. While  PRCA~ add a pluggable reward-driven contextual adapter to enhance performance on specific tasks. BGM~ keeps the retriever and LLM fixed,and trains a bridge Seq2Seq model in between. The bridge model aims to transform the retrieved information into a format that LLMs can work with effectively, allowing it to not only rerank but also dynamically select passages for each query, and potentially employ more advanced strategies like repetition.  Furthermore, PKG introduces an innovative method for integrating knowledge into white-box models via directive fine-tuning~. In this approach, the retriever module is directly substituted to generate relevant documents according to a query. This method assists in addressing the difficulties encountered during the fine-tuning process and enhances model performance.", "cites": [8348, 398, 395, 380], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of several papers by briefly introducing their approaches to using adapters in RAG systems. However, the integration is minimal and lacks deeper analysis or comparison of the methods. There is no significant abstraction or identification of overarching principles, and the section does not critically evaluate the strengths or weaknesses of the cited works."}}
{"id": "7274850b-99c3-4927-8b38-0f7cfc310d3b", "title": "Context Curation", "level": "subsection", "subsections": ["fc293bde-0fb9-437c-af57-9d4d68feb0e1", "72df52f4-e58c-46f8-874e-5a388f7e93fe"], "parent_id": "91d4f115-a5e1-4f89-bcc4-44c2d69172f6", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Generation"], ["subsection", "Context Curation"]], "content": "Redundant information can interfere with the final generation of LLM, and overly long contexts can also lead LLM to the ``Lost in the middle\" problem~. Like humans, LLM tends to only focus on the beginning and end of long texts, while forgetting the middle portion. Therefore, in the RAG system, we typically need to further process the retrieved content.", "cites": [427], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical discussion by linking the issue of redundant and lengthy contexts to the 'Lost in the middle' problem, supported by a relevant cited paper. It integrates a key concept from the paper into a broader understanding of RAG challenges but does not deeply compare multiple approaches or synthesize a novel framework. Critical analysis is limited to identifying a limitation without extensive evaluation or contrast."}}
{"id": "fc293bde-0fb9-437c-af57-9d4d68feb0e1", "title": "Reranking", "level": "subsubsection", "subsections": [], "parent_id": "7274850b-99c3-4927-8b38-0f7cfc310d3b", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Generation"], ["subsection", "Context Curation"], ["subsubsection", "Reranking"]], "content": "Reranking fundamentally reorders document chunks to highlight the most pertinent results first, effectively reducing the overall document pool, severing a dual purpose in information retrieval, acting as both an enhancer and a filter, delivering refined inputs for more precise language model processing~. Reranking can be performed using rule-based methods that depend on predefined metrics like Diversity, Relevance, and MRR, or model-based approaches like Encoder-Decoder models from the BERT series (e.g., SpanBERT), specialized reranking models such as Cohere rerank or bge-raranker-large, and general large language models like GPT~.", "cites": [407, 428], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of reranking methods and mentions some models and metrics used, but it lacks substantial synthesis of the cited papers or deeper critical evaluation of their approaches. It identifies general categories (rule-based vs. model-based) but does not connect the specific contributions of the cited works to form a broader narrative or insight."}}
{"id": "72df52f4-e58c-46f8-874e-5a388f7e93fe", "title": "Context Selection/Compression", "level": "subsubsection", "subsections": [], "parent_id": "7274850b-99c3-4927-8b38-0f7cfc310d3b", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Generation"], ["subsection", "Context Curation"], ["subsubsection", "Context Selection/Compression"]], "content": "A common misconception in the RAG process is the belief that retrieving as many relevant documents as possible and concatenating them to form a lengthy retrieval prompt is beneficial. However, excessive context can introduce more noise, diminishing the LLM’s perception of key information .\n(Long) LLMLingua~ utilize small language models (SLMs) such as GPT-2 Small or LLaMA-7B, to detect and remove  unimportant tokens, transforming it into a form that is challenging for humans to comprehend but well understood by LLMs. This approach presents a direct and practical method for prompt compression, eliminating the need for additional training of LLMs while balancing language integrity and compression ratio. PRCA tackled this issue by training an information extractor~. Similarly, RECOMP adopts a comparable approach by training an information condenser using contrastive learning~. Each training data point consists of one positive sample and five negative samples, and the encoder undergoes training using contrastive loss throughout this process~ .\nIn addition to compressing the context,  reducing the number of documents aslo helps improve the accuracy of the model's answers. Ma et al.~ propose the ``Filter-Reranker\" paradigm, which combines the strengths of LLMs and SLMs. In this paradigm, SLMs serve as filters, while LLMs function as reordering agents. The research shows that instructing LLMs to rearrange challenging samples identified by SLMs leads to significant improvements in various Information Extraction (IE) tasks. Another straightforward and effective approach involves having the LLM evaluate the retrieved content before generating the final answer. This allows the LLM to filter out documents with poor relevance through LLM critique. For instance, in Chatlaw~, the LLM is prompted to self-suggestion on the referenced legal provisions to assess their relevance.", "cites": [430, 9, 429, 416, 7229], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates several papers to discuss context selection and compression in RAG, showing a basic level of synthesis by grouping related methods (e.g., using SLMs for filtering). It provides some critical perspective by pointing out the limitations of excessive context and the benefits of compression techniques. However, the abstraction remains moderate, as it does not fully generalize to meta-level principles but instead outlines patterns and practical approaches."}}
{"id": "e4ef74a1-5ec2-43ee-8e48-4a8dfcd5dea2", "title": "LLM Fine-tuning", "level": "subsection", "subsections": [], "parent_id": "91d4f115-a5e1-4f89-bcc4-44c2d69172f6", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Generation"], ["subsection", "LLM Fine-tuning"]], "content": "Targeted fine-tuning based on the scenario and data characteristics  on LLMs can yield better results. This is also one of the greatest advantages of using on-premise LLMs. When LLMs lack data in a specific domain, additional knowledge can be provided to the LLM through fine-tuning. Huggingface’s fine-tuning data can also be used as an initial step. \nAnother benefit of fine-tuning is the ability to adjust the model’s input and output. For example, it can enable LLM to adapt to specific data formats and generate responses in a particular style as instructed~. For retrieval tasks that engage with structured data, the SANTA framework~ implements a tripartite training regimen to effectively encapsulate both structural and semantic nuances. The initial phase focuses on the retriever, where contrastive learning is harnessed to refine the query and document embeddings.\nAligning LLM outputs with human or retriever preferences through reinforcement learning is a potential approach. For instance, manually annotating the final generated answers and then providing feedback through reinforcement learning. In addition to aligning with human preferences, it is also possible to align with the preferences of fine-tuned models and retrievers~. When circumstances prevent access to powerful proprietary models or larger parameter open-source models, a simple and effective method is to distill the more powerful models(e.g. GPT-4). Fine-tuning of LLM can also be coordinated with fine-tuning of the retriever to align preferences. A typical approach, such as RA-DIT~, aligns the scoring functions between Retriever and Generator using KL divergence.", "cites": [405, 383, 8352, 377], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates multiple cited works to explain the role of fine-tuning in RAG systems, particularly highlighting how it enhances domain adaptation and model alignment. It synthesizes ideas from SANTA and RA-DIT to connect LLM fine-tuning with retriever optimization. While it provides some analytical depth, it lacks deeper comparison or critique of the methods' limitations, and the abstraction remains at a moderate level, focusing on specific techniques rather than broader conceptual trends."}}
{"id": "4bb56ce1-ecf5-42d9-b2e6-c4ebb1c51a51", "title": "Augmentation process in RAG", "level": "section", "subsections": ["3e066fd7-d3c1-4cd3-8bec-43c829d6a2fb", "afdcbefb-1ed7-4bb9-ad53-7684f877c84f", "1c384c6b-c9d4-40f6-936e-b04b4f841aa7"], "parent_id": "f7edebb9-d241-4d6d-820b-72864b086880", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Augmentation process in RAG"]], "content": "\\label{sec:augmentation}\nIn the domain of RAG, the standard practice often involves a singular (once) retrieval step followed by generation, which can lead to inefficiencies and sometimes is typically insufficient for complex problems demanding multi-step reasoning, as it provides a limited scope of information~. Many studies have optimized the retrieval process in response to this issue, and we have summarised them in Figure~\\ref{fig:aug_process}.\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=1\\linewidth,height=8cm]{images/aug_process.png}\n    \\caption{In addition to the most common once retrieval, RAG also includes three types of retrieval augmentation processes. (left) Iterative retrieval involves alternating between retrieval and generation, allowing for richer and more targeted context from the knowledge base at each step. (Middle) Recursive retrieval involves gradually refining the user query and breaking down the problem into sub-problems, then continuously solving complex problems through retrieval and generation. (Right) Adaptive retrieval focuses on enabling the RAG system to autonomously determine whether external knowledge retrieval is necessary and when to stop retrieval and generation, often utilizing LLM-generated special tokens for control.}\n    \\label{fig:aug_process}\n\\end{figure*}", "cites": [415], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the augmentation process in RAG by introducing and categorizing three types—iterative, recursive, and adaptive retrieval. It integrates information from cited papers to highlight limitations of the standard retrieval process and presents a coherent framework. However, it lacks deeper critical evaluation of the cited work and does not extensively generalize to broader principles beyond RAG systems."}}
{"id": "3e066fd7-d3c1-4cd3-8bec-43c829d6a2fb", "title": "Iterative Retrieval", "level": "subsection", "subsections": [], "parent_id": "4bb56ce1-ecf5-42d9-b2e6-c4ebb1c51a51", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Augmentation process in RAG"], ["subsection", "Iterative Retrieval"]], "content": "Iterative retrieval is a process where the knowledge base is repeatedly searched based on the initial query and the text generated so far, providing a more comprehensive knowledge base for LLMs. This approach has been shown to enhance the robustness of subsequent answer generation by offering additional contextual references through multiple retrieval iterations. However, it may be affected by semantic discontinuity and the accumulation of irrelevant information. ITER-RETGEN~ employs a synergistic approach that leverages ``retrieval-enhanced generation'' alongside ``generation-enhanced retrieval'' for tasks that necessitate the reproduction of specific information. The model harnesses the content required to address the input task as a contextual basis for retrieving pertinent knowledge, which in turn facilitates the generation of improved responses in subsequent iterations.", "cites": [371], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates the concept of iterative retrieval and connects it to the broader RAG framework, showing some synthesis with the cited paper. It provides a brief analytical explanation of how iterative retrieval functions and its benefits, but lacks in-depth comparison or critique of different methods. The abstraction level is moderate, as it identifies a general approach but does not fully elevate to overarching principles or meta-level insights."}}
{"id": "afdcbefb-1ed7-4bb9-ad53-7684f877c84f", "title": "Recursive Retrieval", "level": "subsection", "subsections": [], "parent_id": "4bb56ce1-ecf5-42d9-b2e6-c4ebb1c51a51", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Augmentation process in RAG"], ["subsection", "Recursive Retrieval"]], "content": "Recursive retrieval is often used in information retrieval and NLP to improve the depth and relevance of search results. The process involves iteratively refining search queries based on the results obtained from previous searches. Recursive Retrieval aims to enhance the search experience by gradually converging on the most pertinent information through a feedback loop. IRCoT~ uses chain-of-thought to guide the retrieval process and refines the CoT with the obtained retrieval results. ToC~ creates a clarification tree that systematically optimizes the ambiguous parts in the Query. It can be particularly useful in complex search scenarios where the user's needs are not entirely clear from the outset or where the information sought is highly specialized or nuanced. The recursive nature of the process allows for continuous learning and adaptation to the user's requirements, often resulting in improved satisfaction with the search outcomes.\nTo address specific data scenarios, recursive retrieval and multi-hop retrieval techniques are utilized together. Recursive retrieval involves a structured index to process and retrieve data in a hierarchical manner, which may include summarizing sections of a document or lengthy PDF before performing a retrieval based on this summary. Subsequently, a secondary retrieval within the document refines the search, embodying the recursive nature of the process. In contrast, multi-hop retrieval is designed to delve deeper into graph-structured data sources, extracting interconnected information~.", "cites": [412, 386], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the key concepts of recursive retrieval from two cited papers, connecting IRCoT and ToC within a broader framework of iterative query refinement. It offers some critical perspective by contrasting recursive retrieval with multi-hop retrieval, but the critique remains relatively surface-level. The abstraction is moderate, as it identifies the general principle of iterative improvement in retrieval but does not fully explore deeper implications or unify the approaches into a meta-framework."}}
{"id": "1c384c6b-c9d4-40f6-936e-b04b4f841aa7", "title": "Adaptive Retrieval", "level": "subsection", "subsections": [], "parent_id": "4bb56ce1-ecf5-42d9-b2e6-c4ebb1c51a51", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Augmentation process in RAG"], ["subsection", "Adaptive Retrieval"]], "content": "Adaptive retrieval methods, exemplified by Flare~ and Self-RAG~, refine the RAG framework by enabling LLMs to actively determine the optimal moments and content for retrieval, thus enhancing the efficiency and relevance of the information sourced.\nThese methods are part of a broader trend wherein LLMs employ active judgment in their operations, as seen in model agents like AutoGPT, Toolformer, and Graph-Toolformer~. Graph-Toolformer, for instance, divides its retrieval process into distinct steps where LLMs proactively use retrievers, apply Self-Ask techniques, and employ few-shot prompts to initiate search queries. This proactive stance allows LLMs to decide when to search for necessary information, akin to how an agent utilizes tools.\nWebGPT~ integrates a reinforcement learning framework to train the GPT-3 model in autonomously using a search engine during text generation. It navigates this process using special tokens that facilitate actions such as search engine queries, browsing results, and citing references, thereby expanding GPT-3's capabilities through the use of external search engines. Flare automates timing retrieval by monitoring the confidence of the generation process, as indicated by the probability of generated terms~. When the probability falls below a certain threshold would activates the retrieval system to collect relevant information, thus optimizing the retrieval cycle. Self-RAG~ introduces ``reflection tokens'' that allow the model to introspect its outputs. These tokens come in two varieties: ``retrieve'' and ``critic''. The model autonomously decides when to activate retrieval, or alternatively, a predefined threshold may trigger the process. During retrieval, the generator conducts a fragment-level beam search across multiple paragraphs to derive the most coherent sequence. Critic scores are used to update the subdivision scores, with the flexibility to adjust these weights during inference, tailoring the model's behavior. Self-RAG's design obviates the need for additional classifiers or reliance on Natural Language Inference (NLI) models, thus streamlining the decision-making process for when to engage retrieval mechanisms and improving the model's autonomous judgment capabilities in generating accurate responses.", "cites": [378, 434, 431, 432, 433, 379], "cite_extract_rate": 1.0, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes several adaptive retrieval methods, including Flare, WebGPT, Graph-Toolformer, Toolformer, and Self-RAG, highlighting their shared goal of enabling LLMs to make active retrieval decisions. It provides some critical analysis by discussing how these methods streamline or enhance autonomous judgment, but does not deeply critique their limitations. The abstraction is strong, as it generalizes these methods under the broader trend of active judgment in LLMs, drawing connections to agent-like behavior and modular decision-making."}}
{"id": "df050af3-8c46-4388-83ef-fad1590e00d3", "title": "Downstream Task", "level": "subsection", "subsections": [], "parent_id": "be6e132b-039a-47c5-94bd-ae5c20ce7fc1", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Task and Evaluation"], ["subsection", "Downstream Task"]], "content": "The core task of RAG remains Question Answering (QA), including traditional single-hop/multi-hop QA, multiple-choice, domain-specific QA as well as  long-form scenarios suitable for RAG. In addition to QA, RAG is continuously being expanded into multiple downstream tasks, such as Information Extraction (IE), dialogue generation, code search, etc. The main downstream tasks of RAG and their corresponding datasets are summarized in Table ~\\ref{tab:Task}.\n\\begin{table*}[]\n\\caption{Downstream tasks and datasets of RAG\\label{tab:Task}}\n\\centering\n\\begin{tabular}{@{}lllll@{}}\n\\toprule\nTask                   & Sub Task                   & Dataset                   & Method                                                       &  \\\\ \\midrule\nQA &\n  Single-hop &\n  Natural Qustion(NQ)~ &\n  \\begin{tabular}[c]{@{}l@{}}\\\\  \\\\\n  \n  \\end{tabular} &\n   \\\\\n &\n   &\n  TriviaQA(TQA)~ &\n  \\begin{tabular}[c]{@{}l@{}} \\\\ \\\\ \\end{tabular} &\n   \\\\\n                       &                            & SQuAD~                   & &  \\\\\n                       &                            & Web Questions(WebQ)~       &                         &  \\\\\n                       &                            & PopQA~                     &                                            &  \\\\\n                       &                            & MS MARCO~                &                            &  \\\\ \\cmidrule(lr){2-4}\n &\n  Multi-hop &\n  HotpotQA~ &\n  \\begin{tabular}[c]{@{}l@{}}\\\\ \\end{tabular} &\n   \\\\\n                       &                            & 2WikiMultiHopQA~           &                   &  \\\\\n                       &                            & MuSiQue~                   &                                    &  \\\\ \\cmidrule(lr){2-4}\n                       & Long-form QA               & ELI5~                      &                         &  \\\\\n                       &                            & NarrativeQA(NQA)~          &                &  \\\\\n                       &                            & ASQA~                   &                                                     &  \\\\\n                       &                            & QMSum(QM)~                &                                    &  \\\\ \\cmidrule(lr){2-4}\n                       & Domain QA                  & Qasper~              &                                        &  \\\\\n                       &                            & COVID-QA~                  &                                                  &  \\\\\n                       &                            & CMB~,MMCU\\_Medical~         &                                                         &  \\\\ \\cmidrule(lr){2-4}\n                       & Multi-Choice QA            & QuALITY~                   &                                      &  \\\\\n                       &                            & ARC~                       &                                                  &  \\\\\n                       &                            & CommonsenseQA~             &                                                      &  \\\\ \\cmidrule(lr){2-4}\n                       & Graph QA                   & GraphQA~                   &                                                 &  \\\\ \\cmidrule(r){1-4}\nDialog                 & Dialog Generation          & Wizard of Wikipedia (WoW)~ &                                    &  \\\\\n                       & Personal Dialog            & KBP~                       &                                              &  \\\\\n                       &                            & DuleMon~                   &                                                     &  \\\\\n                       & Task-oriented Dialog           & CamRest~              &                                      &  \\\\\n                       & Recommendation             & Amazon(Toys,Sport,Beauty)~ &                                             &  \\\\ \\cmidrule(r){1-4}\nIE & Event Argument Extraction  & WikiEvent                 &                                    &  \\\\\n                       &                            & RAMS~                      &                                           &  \\\\\n                       & Relation Extraction                         & T-REx~,ZsRE~                &                                                    &  \\\\ \\cmidrule(r){1-4}\nReasoning              & Commonsense Reasoning      & HellaSwag~            &                                                   &  \\\\\n                       & CoT Reasoning              & CoT Reasoning~             &                                   &  \\\\\n                       & Complex Reasoning          & CSQA~                     &                                                           &  \\\\ \\cmidrule(r){1-4}\nOthers                 & Language Understanding     & MMLU~                     &                  &  \\\\\n                       & Language Modeling          & WikiText-103~            &                            &  \\\\\n                       &                            & StrategyQA~            &                     &  \\\\\n                        & Fact Checking/Verification & FEVER~                     &                            &  \\\\\n                       &                            & PubHealth~                 &                                            &  \\\\ \n                       & Text Generation            & Biography~                 &                                              &  \\\\\n                       & Text Summarization         & WikiASP~                   &                                                         &  \\\\\n                       &                            & XSum~                      &                                                     &  \\\\\n                       & Text Classification        & VioLens~                   &                                         &  \\\\\n                       &                            & TREC~                      &                                                          &  \\\\\n                       & Sentiment                  & SST-2~                   &                                             &  \\\\\n                       & Code Search                & CodeSearchNet~             &                                         &  \\\\\n                       & Robustness Evaluation      & NoMIRACL~                  &                                    &  \\\\\n                       & Math                       & GSM8K~                     &                                                       &  \\\\ \n                       & Machine Translation        & JRC-Acquis~                &                                                  & \\\\ \\cmidrule(l){5-5} \n                       \\midrule\n\\end{tabular}\n\\end{table*}", "cites": [409, 8350, 386, 446, 383, 453, 397, 459, 452, 445, 365, 8347, 451, 456, 7025, 462, 396, 448, 405, 8355, 438, 388, 410, 7228, 382, 441, 422, 400, 373, 439, 436, 399, 372, 380, 454, 7024, 460, 437, 8351, 419, 7187, 390, 385, 8352, 416, 455, 7230, 457, 450, 393, 389, 464, 374, 415, 418, 7227, 443, 404, 444, 403, 8349, 411, 417, 447, 395, 378, 449, 440, 367, 8354, 435, 379, 394, 377, 442, 458, 7225, 7226, 381, 463, 420, 412, 414, 402, 8348, 7026, 387, 371, 461, 384, 392], "cite_extract_rate": 0.8053097345132744, "origin_cites_number": 113, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section primarily describes various downstream tasks of RAG and lists corresponding datasets and methods, with minimal synthesis or integration of insights across the cited papers. It lacks critical analysis of the methods or limitations of the datasets. Some pattern recognition is evident (e.g., categorizing QA subtasks), but the analysis remains at a surface level without deeper abstraction or evaluation."}}
{"id": "6d6341ab-06db-48f8-8f1a-243cad4325db", "title": "Evaluation Target", "level": "subsection", "subsections": [], "parent_id": "be6e132b-039a-47c5-94bd-ae5c20ce7fc1", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Task and Evaluation"], ["subsection", "Evaluation Target"]], "content": "Historically, RAG models assessments have centered on their execution in specific downstream tasks. These evaluations employ established metrics suitable to the tasks at hand. For instance, question answering evaluations might rely on EM and F1 scores~, whereas fact-checking tasks often hinge on Accuracy as the primary metric~. BLEU and ROUGE metrics are also commonly used to evaluate answer quality~. Tools like RALLE, designed for the automatic evaluation of RAG applications, similarly base their assessments on these task-specific metrics~. Despite this, there is a notable paucity of research dedicated to evaluating the distinct characteristics of RAG models.The main evaluation objectives include:\n\\emph{Retrieval Quality}. Evaluating the retrieval quality is crucial for determining the effectiveness of the context sourced by the retriever component. Standard metrics from the domains of search engines, recommendation systems, and information retrieval systems are employed to measure the performance of the RAG retrieval module. Metrics such as Hit Rate, MRR, and NDCG are commonly utilized for this purpose~.\n\\emph{Generation Quality}. The assessment of generation quality centers on the generator's capacity to synthesize coherent and relevant answers from the retrieved context. This evaluation can be categorized based on the content's objectives: unlabeled and labeled content. For unlabeled content, the evaluation encompasses the faithfulness, relevance, and non-harmfulness of the generated answers. In contrast, for labeled content, the focus is on the accuracy of the information produced by the model~. Additionally, both retrieval and generation quality assessments can be conducted through manual or automatic evaluation methods~.", "cites": [7025, 397, 388, 396, 7228, 371, 389, 465, 382, 367, 7225, 380], "cite_extract_rate": 0.75, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a structured overview of evaluation targets in RAG models, synthesizing insights from multiple papers to distinguish between retrieval and generation quality. It introduces broader evaluation categories (e.g., faithfulness, relevance, accuracy), showing some abstraction. However, it lacks deeper comparative or critical evaluation of the cited methods and their relative strengths/weaknesses, limiting its analytical depth."}}
{"id": "67f62c7e-264c-4c8e-afce-1b4f0530650d", "title": "Quality Scores", "level": "subsubsection", "subsections": [], "parent_id": "ed78004a-faf5-4276-91c8-4e034f9f9dfe", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Task and Evaluation"], ["subsection", "Evaluation Aspects"], ["subsubsection", "Quality Scores"]], "content": "Quality scores include context relevance, answer faithfulness, and answer relevance. These quality scores evaluate the efficiency of the RAG model from different perspectives in the process of information retrieval and generation~. \n\\emph{Context Relevance} evaluates the precision and specificity of the retrieved context, ensuring relevance and minimizing processing costs associated with extraneous content.\n\\emph{Answer Faithfulness} ensures that the generated answers remain true to the retrieved context, maintaining consistency and avoiding contradictions.\n\\emph{Answer Relevance} requires that the generated answers are directly pertinent to the posed questions, effectively addressing the core inquiry.", "cites": [466, 467], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "low", "analysis": "The section provides a basic description of three quality scores used in evaluating RAG systems but does not synthesize or contrast the cited papers in any meaningful way. It lacks critical evaluation of the frameworks or their limitations and offers only minimal abstraction by listing the evaluation aspects without deeper generalization or meta-level insights."}}
{"id": "0f9aca2b-85ec-416c-b4a2-8eec493138e1", "title": "Required Abilities", "level": "subsubsection", "subsections": [], "parent_id": "ed78004a-faf5-4276-91c8-4e034f9f9dfe", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Task and Evaluation"], ["subsection", "Evaluation Aspects"], ["subsubsection", "Required Abilities"]], "content": "RAG evaluation also encompasses four abilities indicative of its adaptability and efficiency: noise robustness, negative rejection, information integration, and counterfactual robustness~. These abilities are critical for the model's performance under various challenges and complex scenarios, impacting the quality scores. \n\\emph{Noise Robustness} appraises the model's capability to manage noise documents that are question-related but lack substantive information.\n\\emph{Negative Rejection} assesses the model's discernment in refraining from responding when the retrieved documents do not contain the necessary knowledge to answer a question.\n\\emph{Information Integration} evaluates the model's proficiency in synthesizing information from multiple documents to address complex questions.\n \\emph{Counterfactual Robustness} tests the model's ability to recognize and disregard known inaccuracies within documents, even when instructed about potential misinformation. \nContext relevance and noise robustness are important for evaluating the quality of retrieval, while answer faithfulness, answer relevance, negative rejection, information integration, and counterfactual robustness are important for evaluating the quality of generation.\nThe specific metrics for each evaluation aspect are summarized in Table~\\ref{tab:metrics-evaluation aspects}. It is essential to recognize that these metrics, derived from related work, are traditional measures and do not yet represent a mature or standardized approach for quantifying RAG evaluation aspects. Custom metrics tailored to the nuances of RAG models, though not included here, have also been developed in some evaluation studies.\n\\begin{table*}[htbp]\n\\centering\n\\caption{Summary of metrics applicable for evaluation aspects of RAG}\n\\begin{tabulary}{\\linewidth}{@{}L*{7}{C}@{}}\n\\toprule\n                & \\thead{Context\\\\ Relevance} & \\thead{Faithfulness} & \\thead{Answer\\\\ Relevance} & \\thead{Noise\\\\ Robustness} & \\thead{Negative\\\\ Rejection} & \\thead{Information\\\\ Integration} & \\thead{Counterfactual\\\\ Robustness} \\\\ \\midrule\nAccuracy        & \\checkmark                  & \\checkmark           & \\checkmark                & \\checkmark                  & \\checkmark                    & \\checkmark                     & \\checkmark                       \\\\\nEM              &                             &                      &                           &                             & \\checkmark                             &                                &                                  \\\\\nRecall          & \\checkmark                  &                      &                           &                             &                               &                                &                                  \\\\\nPrecision       &    \\checkmark                         &           &                           &  \\checkmark                            &                               &                                &                                  \\\\\nR-Rate          &                             &                      &                 &                             &                               &                                & \\checkmark                       \\\\\nCosine Similarity &                           &            & \\checkmark                           &                             &                               &                                &                                  \\\\\nHit Rate        & \\checkmark                  &                      &                           &                             &                               &                                &                                  \\\\\nMRR             & \\checkmark                  &                      &                           &                             &                               &                                &                                  \\\\\nNDCG            & \\checkmark                  &                      &                           &                             &                               &                                &     \\\\\nBLEU              & \\checkmark          &       \\checkmark          &   \\checkmark       &  &  &    & \\\\\nROUGE/ROUGE-L      &  \\checkmark   & \\checkmark & \\checkmark  &  & & & \n\\\\ \\bottomrule\n\\end{tabulary}\n\\label{tab:metrics-evaluation aspects}\n\\end{table*}", "cites": [469, 468], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the required abilities in RAG evaluation by categorizing and explaining concepts such as noise robustness, negative rejection, and counterfactual robustness. It draws from the cited papers to contextualize these abilities, particularly in relation to handling unreliable or insufficient external knowledge. While it organizes these ideas into a coherent structure and links them to broader evaluation aspects, it does not offer a novel synthesis or deep critique of the cited works."}}
{"id": "a9244ab0-8bc1-4766-9742-993cdc414b1e", "title": "Evaluation Benchmarks and Tools", "level": "subsection", "subsections": [], "parent_id": "be6e132b-039a-47c5-94bd-ae5c20ce7fc1", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Task and Evaluation"], ["subsection", "Evaluation Benchmarks and Tools"]], "content": "A series of benchmark tests and tools have been proposed to facilitate the evaluation of RAG.These instruments furnish quantitative metrics that not only gauge RAG model performance but also enhance comprehension of the model's capabilities across various evaluation aspects. Prominent benchmarks such as RGB, RECALL and CRUD ~ focus on appraising the essential abilities of RAG models. Concurrently, state-of-the-art automated tools like RAGAS~, ARES~, and TruLens\\footnote{\\url{https://www.trulens.org/trulens_eval/core_concepts_rag_triad/}} employ LLMs to adjudicate the quality scores. These tools and benchmarks collectively form a robust framework for the systematic evaluation of RAG models, as summarized in Table~\\ref{tab:evaluation-frameworks}.\n\\begin{table*}[htbp]\n\\caption{Summary of evaluation frameworks}\n\\label{tab:evaluation-frameworks}\n\\centering\n\\begin{tabulary}{\\textwidth}{@{}CCCCC@{}}\n\\toprule\n\\thead{\\textbf{Evaluation Framework}} & \\thead{\\textbf{Evaluation Targets}} & \\thead{\\textbf{Evaluation Aspects}} & \\thead{\\textbf{Quantitative Metrics}} \\\\\n\\midrule\n\\makecell{RGB$^\\dagger$} & \\makecell{Retrieval Quality\\\\ Generation Quality} & \\makecell{Noise Robustness\\\\ Negative Rejection\\\\ Information Integration\\\\ Counterfactual Robustness} & \\makecell{Accuracy\\\\ EM\\\\ Accuracy\\\\ Accuracy} \\\\\n\\midrule\n\\makecell{RECALL$^\\dagger$} & Generation Quality & Counterfactual Robustness & R-Rate (Reappearance Rate) \\\\\n\\midrule\n\\makecell{RAGAS$^\\ddagger$} & \\makecell{Retrieval Quality\\\\ Generation Quality} & \\makecell{Context Relevance\\\\ Faithfulness\\\\ Answer Relevance} & \\makecell{* \\\\ * \\\\ Cosine Similarity} \\\\\n\\midrule\n\\makecell{ARES$^\\ddagger$} & \\makecell{Retrieval Quality\\\\ Generation Quality} & \\makecell{Context Relevance\\\\ Faithfulness\\\\ Answer Relevance} & \\makecell{Accuracy\\\\ Accuracy\\\\ Accuracy} \\\\\n\\midrule\n\\makecell{TruLens$^\\ddagger$} & \\makecell{Retrieval Quality\\\\ Generation Quality} & \\makecell{Context Relevance\\\\ Faithfulness\\\\ Answer Relevance} & \\makecell{* \\\\ * \\\\ *} \\\\\n\\midrule\n\\makecell{CRUD$^\\dagger$} & \\makecell{Retrieval Quality\\\\ Generation Quality} & \\makecell{Creative Generation\\\\ Knowledge-intensive QA\\\\ Error Correction \\\\Summarization} & \\makecell{BLEU\\\\ROUGE-L\\\\BertScore\\\\RAGQuestEval  } \\\\\n\\bottomrule\n\\end{tabulary}\n\\par\\bigskip\n\\textit{† represents a benchmark, and ‡ represents a tool. * denotes customized quantitative metrics, which deviate from traditional metrics. Readers are encouraged to consult pertinent literature for the specific quantification formulas associated with these metrics, as required.}\n\\end{table*}", "cites": [469, 467, 466, 468], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of RAG evaluation benchmarks and tools by listing their evaluation targets, aspects, and metrics in a table. While it connects the cited works to the general theme of RAG evaluation, it lacks deeper synthesis of their contributions or how they relate to one another. There is minimal critical analysis, and abstraction is limited to a surface-level categorization rather than identifying broader principles or trends."}}
{"id": "ac763708-7f3c-414d-93c3-251e88946127", "title": "RAG vs Long Context", "level": "subsection", "subsections": [], "parent_id": "0404c4da-7e09-49f9-a715-01c49b32f535", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Discussion and Future Prospects"], ["subsection", "RAG vs Long Context"]], "content": "With the deepening of related research, the context of LLMs is continuously expanding~. Presently, LLMs can effortlessly manage contexts exceeding 200,000 tokens~\\footnote{\\url{https://kimi.moonshot.cn}}. This capability signifies that long-document question answering, previously reliant on RAG, can now incorporate the entire document directly into the prompt. This has also sparked discussions on whether RAG is still necessary when LLMs are not constrained by context. In fact, RAG still plays an irreplaceable role. On one hand, providing LLMs with a large amount of context at once will significantly impact its inference speed, while chunked retrieval and on-demand input can significantly improve operational efficiency. On the other hand, RAG-based generation can quickly locate the original references for LLMs to help users verify the generated answers. The entire retrieval and reasoning process is observable, while generation solely relying on long context remains a black box. Conversely, the expansion of context provides new opportunities for the development of RAG, enabling it to address more complex problems and integrative or summary questions that require reading a large amount of material to answer~. Developing new RAG methods in the context of super-long contexts is one of the future research trends.", "cites": [470, 418, 400, 471], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes insights from multiple cited papers to present a coherent comparison between RAG and long context capabilities of LLMs. It offers critical analysis by pointing out limitations such as inference speed and transparency in long-context generation, while emphasizing the continued relevance and unique advantages of RAG. The section abstracts the discussion to broader implications, such as opportunities for RAG in handling complex and integrative questions."}}
{"id": "50e7ef84-cea1-4712-8af6-caebabba400c", "title": "RAG Robustness", "level": "subsection", "subsections": [], "parent_id": "0404c4da-7e09-49f9-a715-01c49b32f535", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Discussion and Future Prospects"], ["subsection", "RAG Robustness"]], "content": "The presence of noise or contradictory information during retrieval can detrimentally affect RAG's output quality. This situation is figuratively referred to as ``Misinformation can be worse than no information at all''. Improving RAG's resistance to such adversarial or counterfactual inputs is gaining research momentum and has become a key performance metric~. Cuconasu et al.~ analyze which type of documents should be retrieved, evaluate the relevance of the documents to the prompt, their position, and the number included in the context. The research findings reveal that including irrelevant documents can unexpectedly increase accuracy by over 30\\%, contradicting the initial assumption of reduced quality. These results underscore the importance of developing specialized strategies to integrate retrieval with language generation models, highlighting the need for further research and exploration into the robustness of RAG.", "cites": [415, 385, 384], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical perspective by highlighting the impact of retrieval noise on RAG robustness and referencing relevant studies. It synthesizes the findings of the cited papers to form a general point about the need for robust integration strategies. While it identifies a key insight—that irrelevant documents can sometimes improve accuracy—it stops short of deeper critique or a novel abstraction of the underlying principles."}}
{"id": "5c9a70ff-069e-4a1b-a29e-c4bcb50efade", "title": "Hybrid Approaches ", "level": "subsection", "subsections": [], "parent_id": "0404c4da-7e09-49f9-a715-01c49b32f535", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Discussion and Future Prospects"], ["subsection", "Hybrid Approaches "]], "content": "Combining RAG with fine-tuning is emerging as a leading strategy. Determining the optimal integration of RAG and fine-tuning whether sequential, alternating, or through end-to-end joint training—and how to harness both parameterized and non-parameterized advantages are areas ripe for exploration~. Another trend is to introduce SLMs with specific functionalities into RAG and fine-tuned by the results of RAG system. For example, CRAG~ trains a lightweight retrieval evaluator to assess the overall quality of the retrieved documents for a query and triggers different knowledge retrieval actions based on confidence levels.", "cites": [419, 377], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes two distinct approaches—CRAG and RA-DIT—by connecting their shared focus on improving RAG through the integration of fine-tuning and external knowledge. It identifies a broader trend of using hybrid strategies to enhance robustness and performance. However, the analysis remains somewhat high-level without deeper evaluation of trade-offs or limitations between these methods."}}
{"id": "13a38c0a-fb95-45bf-b120-5374a9005398", "title": "Scaling laws of RAG ", "level": "subsection", "subsections": [], "parent_id": "0404c4da-7e09-49f9-a715-01c49b32f535", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Discussion and Future Prospects"], ["subsection", "Scaling laws of RAG "]], "content": "End-to-end RAG models and pre-trained models based on RAG are still one of the focuses of current researchers~.The parameters of these models are one of the key factors.While scaling laws~ are established for LLMs, their applicability to RAG remains uncertain. Initial studies like RETRO++~ have begun to address this, yet the parameter count in RAG models still lags behind that of LLMs. The possibility of an Inverse Scaling Law~\\footnote{\\url{https://github.com/inverse-scaling/prize}}, where smaller models outperform larger ones, is particularly intriguing and merits further investigation.", "cites": [392, 472, 8356], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces the topic of scaling laws in RAG and references a few relevant papers, but it lacks a deep synthesis of their findings. It mentions RETRO++ and hints at a gap in parameter counts but does not meaningfully connect these works to form a broader narrative. The analysis is minimal, focusing only on the possibility of an inverse scaling law without elaboration or comparison of methodologies. The section offers only surface-level abstraction, touching on a concept without deriving deeper principles or trends."}}
{"id": "5df982fb-072c-4cac-9793-e471f8230f7c", "title": "Production-Ready RAG", "level": "subsection", "subsections": [], "parent_id": "0404c4da-7e09-49f9-a715-01c49b32f535", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Discussion and Future Prospects"], ["subsection", "Production-Ready RAG"]], "content": "RAG's practicality and alignment with engineering requirements have facilitated its adoption. However, enhancing retrieval efficiency, improving document recall in large knowledge bases, and ensuring data security—such as preventing inadvertent disclosure of document sources or metadata by LLMs—are critical engineering challenges that remain to be addressed~.\nThe development of the RAG ecosystem is greatly impacted by the progression of its technical stack. Key tools like LangChain and LLamaIndex have quickly gained popularity with the emergence of ChatGPT, providing extensive RAG-related APIs and becoming essential in the realm of LLMs.The emerging technology stack, while not as rich in features as LangChain and LLamaIndex, stands out through its specialized products. For example, Flowise AI prioritizes a low-code approach, allowing users to deploy AI applications, including RAG, through a user-friendly drag-and-drop interface. Other technologies like HayStack, Meltano, and Cohere Coral are also gaining attention for their unique contributions to the field.\nIn addition to AI-focused vendors, traditional software and cloud service providers are expanding their offerings to include RAG-centric services. Weaviate's Verba~\\footnote{\\url{https://github.com/weaviate/Verba}} is designed for personal assistant applications, while Amazon's Kendra~\\footnote{\\url{https://aws.amazon.com/cn/kendra/}} offers intelligent enterprise search services, enabling users to browse various content repositories using built-in connectors. In the development of RAG technology, there is a clear trend towards different specialization directions, such as: 1) Customization - tailoring RAG to meet specific requirements. 2) Simplification - making RAG easier to use to reduce the initial learning curve. 3) Specialization - optimizing RAG to better serve production environments.\nThe mutual growth of RAG models and their technology stacks is evident; technological advancements continuously establish new standards for existing infrastructure. In turn, enhancements to the technology stack drive the development of RAG capabilities. RAG toolkits are converging into a foundational technology stack, laying the groundwork for advanced enterprise applications. However, a fully integrated, comprehensive platform concept is still in the future, requiring further innovation and development.\n\\begin{figure*}[htbp]\n    \\centering\n    \\includegraphics[scale=0.22]{images/rag_summary.png}\n    \\caption{Summary of RAG ecosystem}\n    \\label{fig:rag_summary}\n\\end{figure*}", "cites": [473], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the RAG ecosystem, particularly its production readiness, by discussing trends in toolkits and platforms. It integrates some information from the cited paper (e.g., computational cost of retrieval), but the synthesis is limited to one source and lacks deeper connections to others. The section identifies engineering challenges and specialization directions, showing some critical awareness, but does not extensively evaluate or compare different approaches. It abstracts to a degree by identifying patterns like customization and specialization, but these insights remain relatively surface-level."}}
{"id": "5186b580-7c43-4669-b36f-7be3aca0c951", "title": "Multi-modal RAG", "level": "subsection", "subsections": [], "parent_id": "0404c4da-7e09-49f9-a715-01c49b32f535", "prefix_titles": [["title", "Retrieval-Augmented Generation for Large Language Models: A Survey"], ["section", "Discussion and Future Prospects"], ["subsection", "Multi-modal RAG"]], "content": "RAG has transcended its initial text-based question-answering confines, embracing a diverse array of modal data. This expansion has spawned innovative multimodal models that integrate RAG concepts across various domains:\n\\emph{Image}. RA-CM3~ stands as a pioneering multimodal model of both retrieving and generating text and images. BLIP-2~ leverages frozen image encoders alongside LLMs for efficient visual language pre-training, enabling zero-shot image-to-text conversions. The ``Visualize Before You Write'' method~ employs image generation to steer the LM's text generation, showing promise in open-ended text generation tasks.\n\\emph{Audio and Video}. The GSS method retrieves and stitches together audio clips to convert machine-translated data into speech-translated data~. UEOP marks a significant advancement in end-to-end automatic speech recognition by incorporating external, offline strategies for voice-to-text conversion~. Additionally, KNN-based attention fusion leverages audio embeddings and semantically related text embeddings to refine ASR, thereby accelerating domain adaptation. Vid2Seq augments language models with specialized temporal markers, facilitating the prediction of event boundaries and textual descriptions within a unified output sequence~.\n\\emph{Code}.  RBPS~ excels in small-scale learning tasks by retrieving code examples that align with developers' objectives through encoding and frequency analysis. This approach has demonstrated efficacy in tasks such as test assertion generation and program repair. For structured knowledge, the CoK method~ first extracts facts pertinent to the input query from a knowledge graph, then integrates these facts as hints within the input, enhancing performance in knowledge graph question-answering tasks.", "cites": [474, 475, 7027, 8357], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of various multimodal RAG approaches but lacks substantial synthesis, critical evaluation, or abstraction. It describes individual models and methods without comparing them or highlighting their strengths, weaknesses, or broader implications. The narrative remains surface-level and does not offer a unified framework or meta-insights into the multimodal RAG landscape."}}
