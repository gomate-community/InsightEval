{"id": "8f91a9db-0932-4ad2-8d81-8a09c1a2a034", "title": "Introduction", "level": "section", "subsections": ["d5f540df-029d-43dd-a7e5-321601e3f096", "7766badc-add8-4639-96b5-b3218a5670f0", "2da140bc-2788-4e0b-bd79-619ad3d1062b"], "parent_id": "8584763c-0388-415b-b9cf-b52c55e5eeb8", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Introduction"]], "content": "\\label{sec:introduction}}\n\\IEEEPARstart{``H}{ow} can a learning algorithm cope with incorrect training examples?'' \nThis is the question raised in Dana\nAngluin's paper entitled ``Learning From Noisy Examples'' \nin 1988~. \nShe made the statement that, ``when the teacher may make independent random errors in classifying the example data, \nthe strategy of selecting the most consistent rule for the sample is sufficient, and usually requires a feasibly small number of examples, \nprovided noise affects less than half the examples on average''. \nIn other words, she claimed that a learning algorithm can cope with incorrect training examples, \nonce the noise rate is less than one half under the random noise model. \nOver the last 30 years, her seminal research opened a new door to machine learning, \nsince standard machine learning assumes that the label information is fully clean and intact. \nMore importantly, her research echoed the real-world environment, as labels or annotations are often noisy and imperfect in real scenarios.\nFor example, the surge of deep learning comes from 2012, because Geoffrey Hinton's team leveraged AlexNet (i.e., deep neural networks)~ to win the ImageNet challenge~ with an obvious margin. However, due to the huge quantity of data, the ImageNet-scale dataset was necessarily annotated by distributed workers in Amazon Mechanical Turk~\\footnote{\\url{https://www.mturk.com/}}. Due to the limited knowledge, distributed workers cannot annotate specific tasks with 100\\% accuracy, which naturally brings noisy labels. Another vivid example locates in medical applications, where datasets are typically small. However, it requires domain expertise to label medical data, which often suffers from high inter- and intra-observer variability, leading to noisy labels. We should notice that, noisy labels will cause wrong model predictions, which might further influence decisions that impact human health negatively. Lastly, noisy labels are ubiquitous in speech domains, e.g., Voice-over-Internet-Protocol (VoIP) calls~. In particular, due to unstable network conditions, VoIP calls are easily prone to various speech impairments, which should involve the user feedback to identify the cause. Such user feedback can be viewed as the cause labels, which are highly noisy, since most of users lack the domain expertise to accurately articulate the impairment in the perceived speech.\nAll the above noisy cases stem from our daily life, which cannot be avoided. Therefore, it is urgent to build up a robust learning algorithm for handling noisy labels with theoretical guarantees. In this survey paper, we term such a robust learning paradigm \\emph{label-noise learning}, and the noisy training data $(x,\\bar{y})$ is sampled from a corrupted distribution $p(X,\\bar{Y})$, where we assume that the features are intact but the labels are corrupted.\nAs far as we know, label-noise learning spans over two important ages in machine learning: statistical learning (i.e., shallow learning) and representation learning (i.e., deep learning). In the age of statistical learning, label-noise learning focused on designing noise-tolerant losses or unbiased risk estimators~. However, in the age of representation learning, label-noise learning has more options to combat with noisy labels, such as designing biased risk estimators or leveraging memorization effects of deep networks~.", "cites": [3340, 4451], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides an analytical overview of label-noise learning, synthesizing insights from cited papers and real-world examples to establish a narrative on its importance and evolution. It abstracts the concept into two distinct machine learning eras (statistical and representation learning) and introduces the broader problem context. However, it offers limited critical evaluation of specific methods or their limitations, which slightly reduces its critical depth."}}
{"id": "d5f540df-029d-43dd-a7e5-321601e3f096", "title": "Motivation and Contribution", "level": "subsection", "subsections": [], "parent_id": "8f91a9db-0932-4ad2-8d81-8a09c1a2a034", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Introduction"], ["subsection", "Motivation and Contribution"]], "content": "Label-noise representation learning has become very important for both academia and industry. There are two reasons behind. First, from the essence of the learning paradigm, deep supervised learning requires a lot of well-labeled data, which may require too much cost, especially for many start-ups. However, deep unsupervised learning (even self-supervised learning) is too immature to work very well in complex real-world scenarios. Therefore, as deep weakly-supervised learning, label-noise representation learning naturally has attracted much attention and has become a hot topic. Second, from the aspect of data, many real-world scenarios lack purely clean annotations, such as financial data, web data, and biomedical data. These have directly motivated researchers to explore label-noise representation learning.\nAs far as we know, there indeed exist three pioneer surveys related to label noise. \nFrenay and Verleysen~ focused on discussing label-noise statistical learning, \ninstead of label-noise representation learning.\nAlthough Algan et al.~ and Karimi et al.~ focused on deep learning with noisy labels, \nboth of them only considered image (or medical image) classification tasks. \nMoreover, their surveys were written from the applied perspective, instead of discussing methodology and its beneath theory.\nTo compensate for them and go beyond, we want to contribute to the label-noise representation learning area as follows.\n\\footnote{An update-to-date list of papers related to label-noise representation learning is here: \\url{https://github.com/bhanML/label-noise-papers}.}\n\\begin{itemize}[leftmargin=*]\n\\item From the perspective of machine learning, we give the formal definition for label-noise representation learning (LNRL). \nThe definition is not only general enough to include the existing LNRL, but also specific enough to clarify what the goal of LNRL is and how we can solve it.\n\\item Via the lens of learning theory, \nwe provide a deeper understanding why noisy labels affect the performance of deep models. \nMeanwhile, we report the generalization of deep models under noisy labels, which coincides with our theoretical understanding.\n\\item We perform extensive literature review from the age of representation learning, \nand categorize them in a unified taxonomy in terms of data, objective and optimization. \nThe pros and cons of different categories are analyzed. \nWe also present a summary of insights for each category.\n\\item Based on the above observations, we can spark new directions in label-noise representation learning. Beyond label-noise representation learning, we propose several promising future directions, \nsuch as learning with noisy features, preferences, domains, similarities, graphs, and demonstrations. We hope they can provide some insights.\n\\end{itemize}", "cites": [4452, 4453], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section effectively synthesizes the cited papers by highlighting their common focus on noisy labels in deep learning, particularly in the context of medical image classification. It critically evaluates their limitations, such as narrow task focus and lack of theoretical depth, and positions this survey as a broader and more methodologically rigorous contribution. The section abstracts beyond specific works to define LNRL formally, propose a theoretical framework, and identify essential components and future research directions, offering meta-level insights."}}
{"id": "7766badc-add8-4639-96b5-b3218a5670f0", "title": "Position of the Survey", "level": "subsection", "subsections": [], "parent_id": "8f91a9db-0932-4ad2-8d81-8a09c1a2a034", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Introduction"], ["subsection", "Position of the Survey"]], "content": "The position of this survey is explained as follows.\nFrenay and Verleysen~ mainly summarized the methods of label-noise statistical learning (LNSL), which cannot be used for deep learning models directly. Note that although both the LNSL and LNRL approaches address the same problem setting, they are fundamentally different. First, the underlying theories should be different due to different hypothesis space (see Section~\\ref{sec:thm:obj}); Second, the potential solution should be different due to different models (see Section~\\ref{sec:opt}). Meanwhile, LNSL may fail to handle large-scale data with label noise, while LNRL is good at handling such data.\nAlthough Algan et al.~ and Karimi et al.~ respectively summarized some methods of label-noise representation learning, both of them discussed from the perspective of applications, i.e., (medical) image analysis. Recently, Song et al.~ summarized some methods of label-noise representation learning from the view of methodology. However, their categorization is totally different from ours in philosophy. In our survey, we first introduce label-noise representation learning from three general views: input data, objective functions and optimization policies, with more theoretical understanding.", "cites": [4452, 4453, 4454], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple surveys on label-noise learning, highlighting their differences in focus and methodology. It critically assesses the limitations of prior works, such as their application-specific focus or lack of theoretical depth, and positions the current survey as more comprehensive. The abstraction is strong, as it identifies broader distinctions between LNSL and LNRL, and introduces a novel theoretical framework for understanding LNRL."}}
{"id": "e51e8a4c-20a9-4b3e-bfc1-c3a76092ab77", "title": "Early Stage", "level": "subsection", "subsections": [], "parent_id": "49424193-a97c-418b-973a-fb5cf7ba8945", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Related Literature"], ["subsection", "Early Stage"]], "content": "Before delving into label-noise representation learning, \nwe give a brief overview of some milestone works in label-noise statistical learning. In 1988, Angluin et al.~ proved that a learning algorithm can handle incorrect training examples robustly, when the noise rate is less than one half under the random noise model. Lawrence and Sch\\\"olkopf~ constructed a kernel Fisher discriminant to formulate the label-noise problem as a probabilistic model. Bartlett et al.~ justified that most loss functions are not completely robust to label noise. This means that classifiers based on label-noise learning algorithms are still affected by label noise.\nDuring this period, a lot of works emerged and contributed to this area. For example, Crammer et al.~ proposed the online Passive-Aggressive perceptron algorithm to cope with label noise. Natarajan et al.~ formally formulated an unbiased risk estimator for binary classification with noisy labels. This work was very important to the area, since it is the first work to provide guarantees for risk minimization under random label noise. Meanwhile, Scott et al.~ studied the classification problem under the class-conditional noise model, and proposed a way to handle asymmetric label noise. In contrast, van Rooyen et al.~ proposed the unhinge loss to tackle symmetric label noise. Liu and Tao~ proposed a method using anchor points to estimate the noise rate, and further leveraged importance reweighting to design surrogate loss functions for class-conditional label noise.\nIn 2015, research of label-noise learning has been shifted from statistical learning to representation learning, since deep learning models have become a mainstream due to its better empirical performance. Therefore, it is urgent to design label-noise representation learning methods for robustly training deep models with noisy labels.", "cites": [3454, 8734, 3453], "cite_extract_rate": 0.375, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a chronological overview of early works in label-noise learning, mentioning key papers and their contributions without deep integration or synthesis of their ideas. While it identifies some methodological shifts (e.g., from statistical to representation learning), it lacks critical evaluation of the cited works or a clear framework that unifies their contributions. Some abstract themes are hinted at, such as the transition to deep learning, but broader patterns and principles are not thoroughly explored."}}
{"id": "d53a14f2-40a6-407f-87a3-8bc8c9e37b1b", "title": "Emerging Stage", "level": "subsection", "subsections": [], "parent_id": "49424193-a97c-418b-973a-fb5cf7ba8945", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Related Literature"], ["subsection", "Emerging Stage"]], "content": "There are three seminal works in label-noise representation learning with noisy labels from 2015. For example, Sukhbaatar et al.~ introduced an extra but constrained linear ``noise'' layer on top of the softmax layer, which adapts the network outputs to model the noisy label distribution. Reed et al.~ augmented the prediction objective with the notion of consistency via soft and hard bootstrapping. Intuitively, this bootstrapping procedure provides the learner to disagree with an inconsistent training label, and re-label the training data to improve its label quality. Azadi et al.~ proposed an auxiliary image regularization technique, which exploits the mutual context information among training data, and encourages the model to select reliable labels.\nFollowing the seminal works, Goldberger et al.~ introduced a nonlinear ``noise'' adaptation layer on top of the softmax layer. Patrini et al.~ proposed the forward and backward loss correction approaches simultaneously. Both Wang et al.~ and Ren et al.~ leveraged the same philosophy, namely data reweighting, to learn with label noise. Jiang et al.~ is the first to leverage small-loss tricks to handle label noise. However, they trained only a single network iteratively, which inherits the accumulated error. To alleviate this, Han et al.~ trained two deep neural networks, and each network backpropagated the data selected by its peer network and updated itself.\nIn the context of representation learning, classical methods, such as estimating the noise transition matrix, regularization and designing losses, are still prosperous for handling label noise. For instance, Hendrycks et al.~ leveraged trusted examples to estimate the gold transition matrix, which approximates the true transition matrix well. Han et al.~ proposed a ``human-in-the-loop'' idea to easily estimate the transition matrix. Zhang et al.~ introduced an implicit regularization called mixup, which constructs virtual training data by linear interpolations of features and labels in training data. Zhang et al.~ generalized both the categorical cross entropy loss and mean absoulte error loss by the negative Box-Cox transformation. Ma et al.~ developed a dimensionality-driven learning strategy, which can learn robust low-dimensional subspaces capturing the true data distribution.", "cites": [3340, 4139, 4130, 7162, 4183, 4145, 4455, 7773, 4128, 7191, 4136], "cite_extract_rate": 0.7857142857142857, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key early developments in LNRL by organizing them into a narrative that highlights methodological themes, such as noise adaptation, bootstrapping, and data reweighting. It also provides some critical observations, e.g., noting that Jiang et al.'s method inherits accumulated error and that Han et al. improved upon this by using two networks. While it identifies broader patterns (e.g., the shift from single-network to co-training), the level of abstraction is moderate and does not fully articulate overarching theoretical principles."}}
{"id": "d193ddb4-8216-4df5-ae96-889c7d05b2a7", "title": "Flourished Stage", "level": "subsection", "subsections": [], "parent_id": "49424193-a97c-418b-973a-fb5cf7ba8945", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Related Literature"], ["subsection", "Flourished Stage"]], "content": "\\label{sec:intro:mat}\nSince 2019, label-noise representation learning has become flourished in the top conference venues. Arazo et al.~ formulated clean and noisy samples as a two-component (clean-noisy) beta mixture model on the loss values. Hendrycks et al.~ empirically demonstrated that pre-training can improve model robustness against label corruption for large-scale noisy datasets. Under the criterion of balanced error rate (BER) minimization, Charoenphakdee et al.~ proposed the barrier hinge loss. In contrast to selected samples via small-loss tricks, Thulasidasan et al.~ introduced the abstention-based training, which allows deep networks to abstain from learning on confusing samples but to learn on non-confusing samples. Following the re-weighting strategy, Shu et al.~ parameterized the weighting function adaptively as a one-layer multilayer perceptron called Meta-Weight-Net.\nMenon et al.~ mitigated the effects of\nlabel noise from an optimization lens, which naturally introduced the partially Huberised loss. Nguyen et al.~ proposed a self-ensemble label filtering method to progressively filter out the wrong labels during training. Li et al.~ modeled the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples. Lyu et al.~ proposed a provable curriculum loss, which can adaptively select samples for robust stagewise training. Han et al.~ proposed a versatile approach called scaled stochastic integrated gradient underweighted ascent (SIGUA). SIGUA uses stochastic gradient decent on good data, while using scaled stochastic gradient ascent on bad data rather than dropping those data. 5 years after the birth of \\textit{Clothing1M}, Jiang et al.~ proposed a new but realistic type of noisy dataset called ``web-label noise'' (or \\textit{red noise}).", "cites": [4456, 4135, 4253, 4182, 3342, 7133, 7781, 4457, 7774], "cite_extract_rate": 0.8181818181818182, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of various LNRL methods introduced after 2019 but lacks a deeper synthesis of ideas or a unifying framework. While it mentions different techniques (e.g., beta mixture model, self-ensemble filtering), it does so in a list-like manner without substantial comparison or analysis. There is minimal abstraction or critical evaluation of the approaches, resulting in a primarily factual summary rather than insightful analysis."}}
{"id": "f8390a02-ef0e-4fe2-8beb-c43e76a2460c", "title": "Notation", "level": "subsection", "subsections": [], "parent_id": "0631056c-9b2b-4b86-b30a-fc08a6ef0620", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Overview"], ["subsection", "Notation"]], "content": "\\label{sec:notation}\nLet $x$ be features and $y$ be labels. Consider a supervised learning task $T$: LNRL deals with a data set $\\mathcal{D} = \\{\\bar{\\mathcal{D}}^{\\text{tr}},\\mathcal{D}^{\\text{te}}\\}$ consisting of training set $\\bar{\\mathcal{D}}^{\\text{tr}} = \\{(x_i,\\bar{y}_i)\\}_{i=1}^N$ and test set $\\mathcal{D}^{\\text{te}} = \\{x^{\\text{te}}\\}$, \nwhere training set $\\bar{\\mathcal{D}}^{\\text{tr}}= \\{(x_i,\\bar{y}_i)\\}_{i=1}^N$ is independently drawn from a corrupted distribution $\\bar{D} = p(X,\\bar{Y})$ ($\\bar{Y}$ denotes the corrupted label).\nNote that ($X,Y$) denotes the variable, while ($x,y$) denotes its sampled value.\nFor the corrupted distribution $p(X,\\bar{Y})$, we assume that the features are intact but the labels are corrupted. Let $p(X, Y)$ be the ground-truth (i.e., non-corrupted) joint probability distribution of features $x$ and label $y$, \nand $f^*$ be the (Bayes) optimal hypothesis from $x$ to $y$. \nTo approximate $f^*$, \nthe objective requires a hypothesis space $\\mathcal{H}$ of hypotheses \n$f_{\\theta}(\\cdot)$ parameterized by $\\theta$. \nAn algorithm contains the\noptimization policy to search through $\\mathcal{H}$ in order to find $\\theta^*$ that corresponds to the optimal function in the hypothesis for $\\bar{\\mathcal{D}}^{\\text{tr}}$: $f_{\\theta^*} \\in \\mathcal{H}$.\nIntuitively, LNRL learns to discover $f_{\\theta^*}$ by fitting $\\bar{\\mathcal{D}}^{\\text{tr}}$ robustly, which can assign correct labels for $\\mathcal{D}^{\\text{te}}$. LNRL methods robustly train deep neural networks with noisy labels, \nwhere hypotheses $f_{\\theta}(\\cdot)$ can be modeled by deep neural networks. Since the hypothesis space $\\mathcal{H}$ is sufficiently complex for deep neural networks, $f_{\\theta^*} \\in \\mathcal{H}$ is expected to approximate the Bayes optimal $f^*$ well~. \n\\begin{table*}[!tp]\n\\centering\n\\caption{Three LNRL examples based on Definition 2.2.}\n\\vspace{-10px}\n\\begin{tabular}{c|c|c}\n\t\\hline\n\t$T$ & $E = (x,y)$ & $P$ \\\\\\hline\n\tweb-scale image classification  &  (ImageNet, crowdsourced labels) &  test accuracy \\\\\\hline\n\tintelligent healthcare &  (medical data, annotations by variability) &  error rate \\\\\\hline\n\tservice call analysis &  (perceived speech, user rating) &  quality rate of call \\\\\\hline\n\\end{tabular}\n\\label{tab:LNRL-exp}\n\\end{table*}", "cites": [4458], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section introduces notation and definitions related to label-noise representation learning but does not synthesize insights from the cited paper or integrate multiple sources. It lacks critical analysis and instead merely describes the problem setup and theoretical background. There is minimal abstraction, focusing on a specific formulation rather than broader patterns or principles."}}
{"id": "b3a21496-800f-4f0a-881b-d3aec107b3de", "title": "Problem Definition", "level": "subsection", "subsections": [], "parent_id": "0631056c-9b2b-4b86-b30a-fc08a6ef0620", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Overview"], ["subsection", "Problem Definition"]], "content": "\\label{sec:pro-def}\nAs LNRL is naturally a sub-area in machine learning, \nbefore giving the definition of LNRL, let us recall how machine learning is defined in literature.\nWe borrow Tom Mitchell's definition here, \nwhich is shown in Definition~\\ref{def:clsml}.\n\\begin{definition}\n\\label{def:clsml}\n(Machine Learning~).\n A computer program is said to learn from experience $E$ with respect to some classes of task $T$ and performance measure $P$ if its performance can improve with $E$ on $T$ measured by $P$.\n\\end{definition}\nThe above definition is quite classical, which has been widely adopted in the machine learning community.\nIt means that a machine learning problem is defined by three key components: $E$, $T$ and $P$. \nFor instance, consider a speech recognition task ($T$, e.g., \nApple Siri\\footnote{\\url{https://en.wikipedia.org/wiki/Siri}}), \nmachine learning programs can improve its recognition accuracy ($P$) via training with a large-scale speech data set ($E$) offline.\nAnother example  of $T$ is the hot topic in the security area, \ncalled empirical defense~. \nIn a high-level, \nmachine learning algorithms can make deep neural networks defensive against malicious cases. \nSpecifically, a stop sign crafted by malicious people may cause an accident to autonomous vehicles, \nwhich employ deep neural networks to recognize the sign. \nHowever, \nafter adversarial training with adversarial examples ($E$), \nthe robust generalization ($P$) of deep neural networks can improve a lot, \nwhich may avoid the above accident with large probability. \nThe above-mentioned classical applications of machine learning require a lot of ``correctly'' supervised information $\\{(x^{(i)},y^{(i)})\\}_{i=1}^N$ for the given tasks. \nHowever, this may be difficult and even impossible. \nAs far as we know, LNRL is a special case of machine learning, which belongs to weakly supervised learning~. \nIntuitively, LNRL exactly targets at acquiring good learning performance with ``incorrectly'' (a.k.a., noisy) supervised information provided by data set $\\bar{D}$, \ndrawn independently from a corrupted distribution $p(X,\\bar{Y})$.\nThe noisy supervised information refers to training data set $\\bar{D}^{\\text{tr}}$, which consists of the intact input features $x^{(i)}$ but with corrupted labels $\\bar{y}^{(i)}$. More important, LNRL focuses on training deep neural networks robustly, which has many special characteristics, such as memorization effects~. Formally, we define LNRL in Definition~\\ref{LNRL}.\n\\begin{definition}\\label{LNRL}\n(Label-Noise Representation Learning (LNRL)). \nLNRL is a special but common case of machine learning problems (specified by $E$, $T$ and $P$), \nwhere $E$ contains noisy supervised information for the target $T$. \nMeanwhile, deep neural networks will be leveraged to model the target $T$ directly. \n\\end{definition}\nTo understand this definition better, let us show three classical scenarios of LNRL (Table \\ref{tab:LNRL-exp}):\n\\begin{itemize}[leftmargin=*]\n\\item \\textit{Image}: Large-scale image data (e.g., ImageNet~) \nis the key factor to drive the second surge of deep learning from 2012. Note that it is impossible to annotate such large-scale data individually, which motivates us to leverage crowdsourcing technique (e.g., Amazon Mechanical Turk). However, the quality of crowdsourced data is normally low with a certain degree of label noise. Therefore, an important task ($T$) is to robustly training deep neural networks with crowdsourced data ($E$), and the trained deep models can be evaluated via the test accuracy ($P$).\n\\item \\textit{Healthcare}: Healthcare is highly related to each individual, whose data requires machine learning technique to analyze deeply and intelligently. However, intelligent healthcare ($T$) requires domain expertise to label medical data first, which often suffers from high inter- and intra-observer variability, leading to noisy medical data ($E$). We should notice that, noisy labels will cause a high error rate ($P$) of deep model predictions, which might further influence decisions that impact human health negatively.\n\\item \\textit{Speech}: In the speech recognition task (e.g., Apple Siri), the machine learning program can improve its recognition accuracy via training with a large-scale speech data set offline. However, noisy labels are ubiquitous in speech domains, e.g., the task of rating for service calls ($T$). Due to the difference of the personal mood and understanding, service calls are easily prone to different rating ($E$) for the same service. Such rating can be viewed as labels, which are highly noisy since most of users lack the domain expertise to accurately rate the speech service. Therefore, it is critical to robustly train deep neural networks with the user rating ($E$), and evaluate the trained model via the quality rate of service calls ($P$).\n\\end{itemize}\nAs noisy supervised information related to $T$ is directly contained in $E$, it is quite natural that common deep supervised learning approaches will fail on LNRL problems. One of the recent findings (i.e., memorization effects~) in the deep learning area may explain this: due to the high model capacity, deep neural networks will eventually fit and memorize label noise. Therefore, when facing the noisy data $E$, LNRL methods make the learning of the target $T$ feasible by leveraging the intrinsic characteristics of deep neural networks, e.g., memorization effects.", "cites": [895, 892, 4115], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from cited papers to provide a coherent definition and context for LNRL, effectively connecting the broader ML framework to specific scenarios involving label noise. It demonstrates abstraction by identifying general principles like the memorization effect in deep networks, though the critical analysis is limited to pointing out high-level issues rather than deeper evaluation of individual papers. The analytical tone is evident as it builds a conceptual understanding of the problem and its implications."}}
{"id": "7b9dd4ab-a69b-48f1-a78f-013060d44af2", "title": "Relevant Learning Problems", "level": "subsection", "subsections": [], "parent_id": "0631056c-9b2b-4b86-b30a-fc08a6ef0620", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Overview"], ["subsection", "Relevant Learning Problems"]], "content": "\\label{sec:rl-pro}\nIn this section, we discuss the relevant learning problems of LNRL. The relatedness and difference with respect to LNRL are clarified as follows.\n\\begin{itemize}[leftmargin=*]\n\\item \\textit{Semi-supervised Learning (SSL)}~ learns the hypothesis $f$ from experience $E$ consisting of both labeled and unlabeled data, where unlabeled data will be normally given pseudo labels. Since the labeling process may be not fully correct and noisy, SSL has some relation with LNRL. However, standard SSL methods assumes that labeled data are fully clean, which is different from LNRL, where labeled data are still noisy to some degree.\n\\item \\textit{Positive-unlabeled Learning (PUL)}~ learns the hypothesis $f$ from experience $E$ consisting of only positive labeled and unlabeled data. Similar to SSL, unlabeled data will be normally given pseudo labels. However, PUL assumes that labeled data are fully clean and only positive.\n\\item \\textit{Complementary Learning (CL)}~ specifies a class that a pattern does NOT belong to. Namely, CL learns the hypothesis $f$ from experience $E$ consisting of only complementary data. Since the labeling process cannot fully exclude the uncertainty, namely belonging to which categories, CL has some relation with LNRL. However, CL requires that all diagonal entries of the transition matrix are zeros. Sometimes, the transition matrix is not required to be invertible empirically. \n\\item \\textit{Unlabeled-unlabeled Learning (UUL)}~ \nallows training a binary classifier from two unlabeled datasets with different class priors. \nDifferent from SSL/PUL, there are two sets of unlabeled data in UUL instead of one set.\n\\end{itemize}", "cites": [4459, 8794, 7778, 4461, 4462, 4176, 4464, 4460, 4463, 7191, 139], "cite_extract_rate": 0.6875, "origin_cites_number": 16, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic comparison of LNRL to related learning problems like SSL, PUL, CL, and UUL by highlighting their similarities and differences. While it integrates ideas from multiple cited works, the synthesis is limited to surface-level distinctions without deeper connections. There is some critical analysis (e.g., pointing out assumptions like clean labeled data in SSL), but the evaluation remains relatively shallow, and the abstraction level is low, with no clear identification of broader principles or trends."}}
{"id": "4bcba53a-4669-4302-85dd-bd4ebe0c203f", "title": "Theoretical Understanding", "level": "subsection", "subsections": ["40bbe856-0103-4142-b0b9-919e0afe4d65", "ba460c14-2e7c-4cf5-a4e6-b39c7e298384", "06e02b1d-b8d0-449d-9ce4-7aaaa853d7cc"], "parent_id": "0631056c-9b2b-4b86-b30a-fc08a6ef0620", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Overview"], ["subsection", "Theoretical Understanding"]], "content": "\\label{sec:theorem}\nIn contrast to , via the lens of learning theory, \nwe provide a systematic way to understand LNRL. \nOur focus is to explore why noisy labels affect the performance of deep models. To figure it out, we should rethink the essence of learning with noisy labels. Normally, there are three key ingredients in label-noise learning problems, including the data, the objective function and the optimization policy.\nIn a high-level, \nthere are three rules of thumb, which explain how to handle LNRL.\n\\begin{itemize}[leftmargin=*]\n\\item For \\textit{data}, \nthe key is to discover the underlying noise transition pattern, which directly links the clean class posterior and the noisy class posterior. Based on this insight, it is critical to design an accurate estimator of noise transition matrix $T$.\n\\item For the \\textit{objective function}, \nthe key is to design noise-tolerant $\\tilde{\\ell}$ in \\eqref{eq:noisy-risk}, which can be more robust than standard loss functions. Based on this insight, it is critical to learn a robust classifier on noisy data, which can provably converge to the learned classifier on clean data.\n\\item For the \\textit{optimization policy}, \nthe key is to explore the dynamic process of optimization, which relates to memorization. Based on this insight, it is critical to trade-off overfit/underfit in training deep networks, such as early stopping and small-loss tricks, where small-loss tricks backpropogate small-loss data based on memorization effects of deep networks.\n\\end{itemize}\n\\begin{figure*}\n\t\\centering\n\t\\includegraphics[width=0.65\\textwidth]{taxonomy.pdf}\n\t\\caption{A taxonomy of LNRL based on the focus of each method. For each technique branch, we list a few representative works here.}\n\t\\label{fig:taxonomy}\n\\end{figure*}", "cites": [4452, 4453], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section offers an analytical perspective by framing the problem through three key ingredients: data, objective function, and optimization policy. It synthesizes ideas from the cited papers by linking them to a structured understanding of noise handling in deep learning. However, it lacks deeper critical evaluation of the cited works and only hints at broader principles without fully abstracting them into a novel conceptual framework."}}
{"id": "40bbe856-0103-4142-b0b9-919e0afe4d65", "title": "Perspective of Data", "level": "subsubsection", "subsections": [], "parent_id": "4bcba53a-4669-4302-85dd-bd4ebe0c203f", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Overview"], ["subsection", "Theoretical Understanding"], ["subsubsection", "Perspective of Data"]], "content": "\\label{sec:thm:idata}\nSpecifically, \nfrom the perspective of the data, the focus is to build up the noise transition matrix, \nwhich models the process of label corruption. \nIn general, there are two types of label noise: \ninstance-dependent label noise (e.g., $p(\\bar{Y}|Y,X)$)  and instance-independent label noise (e.g., $p(\\bar{Y}|Y)$) . \nFor instance-dependent label noise, the noise transition matrix can be represented as \n$T(X)$, \nwhich depends on features. \nHowever, \nit can be ill-posed to learn the transition matrix $T(X)$ by only exploiting noisy data, \ni.e., the transition matrix is unidentiable~.\nTherefore, \nwe emphasize instance-independent label noise here, \nand the noise transition matrix can be represented as $T$, \nwhich is independent of features. \nIn this case, the noise transition matrix $T$ approximately models the process of label corruption. \nAn instance $x^i$ is said to be an anchor point of the $i$-th clean class if $p(Y = e_i|x^i)=1$, where $Y = e_i$ means $Y$ belongs to the $i$-th class. The transition matrix can be obtained via\n\\begin{align}\np(\\bar{Y} = e_j | x^i) \n& = \\sum\\nolimits_{k=1}^{C} p(\\bar{Y} = e_j | Y = e_k, x^i) p(Y = e_k|x^i),\\notag\n\\\\\n& = p(\\bar{Y} = e_j | Y = e_i, x^i)p(Y = e_i|x^i),\\notag\n\\\\\n& = p(\\bar{Y} = e_j | Y = e_i, x^i) = T_{ij}.\\label{eq:trans}\n\\end{align}\nNote that if \nanchor points are hard to identify, we can use $x^i = \\arg\\max_{x}p(\\bar{Y} =i|x)$ ~. This transition matrix is very important, since it can bridge the noisy class posterior and clean class posterior, i.e., $p(Y|x) = T^{-1}p(\\bar{Y}|x)$. \nIn practice, this transition matrix has been employed to build a risk-consistent estimator via loss correction or a classifier-consistent estimator via hypotheses correction~. \nBesides, for inconsistent algorithms, the diagonal entries of this matrix are used to select reliable examples for further robust training~.", "cites": [3340, 4120, 3453, 4465, 4152], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a clear analytical overview of the data perspective in label-noise representation learning, synthesizing concepts from multiple papers to discuss instance-dependent and instance-independent label noise. It abstracts the idea of the noise transition matrix and its role in bridging noisy and clean class posteriors. While it does not deeply critique individual papers, it identifies limitations such as the unidentifiability issue and highlights the importance of anchor points, showing a balanced evaluation."}}
{"id": "ba460c14-2e7c-4cf5-a4e6-b39c7e298384", "title": "Perspective of Objective Function", "level": "subsubsection", "subsections": [], "parent_id": "4bcba53a-4669-4302-85dd-bd4ebe0c203f", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Overview"], ["subsection", "Theoretical Understanding"], ["subsubsection", "Perspective of Objective Function"]], "content": "\\label{sec:thm:obj}\nFrom the perspective of the objective function, \nthe focus is to derive the statistical consistency \nguarantees for robust \n$\\tilde{\\ell}$~.\nLet $f$ be a deep network with $d$ layers and ReLU active function,\n$R^*=R_D(f^*)$ denote the Bayes risk for Bayes optimal classifier $f^*$ under the clean distribution $D$~,\nand $\\hat{f} = \\arg\\min_{f\\in\\mathcal{H}} \\widehat{R}_{\\tilde{\\ell},\\bar{D}}(f)$\nwhere $L_{\\rho}$ is the Lipschitz constant of $\\tilde{\\ell}$.\nAssume the Frobenius norm of the weight matrices $W_1,\\ldots,W_d$ are at most $M_1,\\ldots,M_d$\nand $x$ be upper-bounded by $B$, i.e., $\\Vert x \\Vert \\leq B$ for any $x$. \nWith probability at least $1-\\delta$, if $\\ell$ is classification-calibrated~, \nthere exists a non-decreasing function $\\xi_{\\ell}$ with $\\xi_{\\ell}(0)=0$ such that \n\\begin{equation*}\n\\begin{split}\n& R_D (\\hat{f})  - R^* \\leq \\xi_{\\ell}\\big(\\min\\nolimits_{f\\in\\mathcal{H}} R_{\\ell,D}(f) - \\min\\nolimits_f R_{\\ell,D}(f) \n\\\\\n& \\quad + 4L_{\\rho}\\mathcal{R}(\\mathcal{H}) + 2 \\sqrt{\\log(\\nicefrac{1}{\\delta}) / 2N}\\big), \n\\\\\n& \\! \\leq \\! \\xi_{\\ell}\\big(\\min_{f\\in\\mathcal{H}} R_{\\ell,D}(f) \n\\! - \\! \\min_f R_{\\ell,D}(f) \n\\! + \\! 4L_{\\rho} C \n\\! + \\! 2 \\sqrt{\\log(\\nicefrac{1}{\\delta})/2N}\\big),\n\\end{split}\n\\end{equation*}\nwhere \n$C = B(\\sqrt{2 d \\log 2} \\! + \\! 1) \\prod\\nolimits_{i=1}^d M_i/\\sqrt{N}$ and\n$R_D(\\hat{f}) = \\mathbb{E}_{(x,y) \\sim D}[1_{\\{\\text{sign}(\\hat{f}(x))\\neq y\\}}]$ \ndenotes the risk of $\\hat{f}$ w.r.t. the 0-1 loss.\nNote that\nfor a deep neural network, \nits Rademacher complexity $\\mathcal{R}(\\mathcal{H})$ of the function class $\\mathcal{H}$ is upper-bounded by $C$~.\n\\begin{remark}\nThe above conclusions denote that the learned $\\hat{f}$ (using noise-tolerant $\\tilde{\\ell}$ over noisy $\\bar{D}$) can approach the Bayes optimal $f^*$, when increasing the richness of the class $\\mathcal{H}$ and the data size~$N$.\n\\end{remark}\nNote that $\\min_{f\\in\\mathcal{H}} R_{\\ell,D}(f) - \\min_f R_{\\ell,D}(f)$ denotes the approximation error for employing the hypothesis class $\\mathcal{H}$.\nAccording to the universal approximation theorem~, \nif a certain deep network model is employed, $\\mathcal{H}$ will be a universal hypothesis class and thus contains the Bayes optimal classifier. \nThen, $\\min_{f\\in\\mathcal{H}} R_{\\ell,D}(f) - \\min_f R_{\\ell,D}(f)=0$. \nThis means that by employing a proper deep network, \nthe upper bound will converge to zero by increasing the training sample size $N$. Since $R_D(\\hat{f})$ is always bigger than or equal to $R^*$, \nwe have that $R_D(\\hat{f})$ will converge to $R^*$. \nThis further means that $\\hat{f}$ learned from noisy data (independently drawn from $\\bar{D}$) will converge to Bayes optimal $f^*$ defined by the clean data.", "cites": [3454, 4119, 4466], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section analytically discusses the theoretical underpinnings of label-noise robust learning through the lens of the objective function, integrating elements from the cited works on convex loss robustness, loss factorization, and neural network sample complexity. While it connects ideas to build a coherent theoretical narrative, it provides limited direct comparison or critique of the cited papers, and the abstraction is moderate, focusing on generalization properties rather than broader meta-principles."}}
{"id": "06e02b1d-b8d0-449d-9ce4-7aaaa853d7cc", "title": "Perspective of Optimization Policy", "level": "subsubsection", "subsections": [], "parent_id": "4bcba53a-4669-4302-85dd-bd4ebe0c203f", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Overview"], ["subsection", "Theoretical Understanding"], ["subsubsection", "Perspective of Optimization Policy"]], "content": "From the perspective of the optimization policy,\nthe focus is to explore the dynamic process of optimization.\nTake the early stopping~, \nwhich is a simple yet effective trick to avoid overfitting on noisy labels,\nas an example. \nAssume an \ninitial weight matrix having entries following the standard normal distribution, \nnamely $W^0 \\sim \\mathcal{N}(0,1)$ entries, and $W^{\\tau}$ is updated via stochastic gradient descent with step size $\\eta$, \ni.e., $W^{\\tau+1} = W^{\\tau} - \\eta \\nabla \\ell(W^{\\tau})$. If $\\varepsilon_0 \\leq \\delta\\lambda(C)^2/K^2$ and $\\rho \\leq \\delta/8$ (cf. Definition 1.2 in ), \nthen after $I \\propto \\nicefrac{\\lVert C \\rVert^2}{\\lambda(C)}$ steps, there are two conclusions  with high probability. First, the model $W^I$ predicts the true label function $\\hat{y}(x)$ for all input $x$ that lies within the $\\varepsilon_0$-neighborhood of a cluster center $\\{c_k\\}_{k=1}^K$.\nNamely, $ \\hat{y}(x) = \\arg\\min_{l}|f_{W^I}(x) - \\alpha_l|$, \nwhere $\\{\\alpha_l\\}_{l=1}^{\\bar{K}}\\in[-1,1]$ denotes the labels associated with each class, and each label belongs to $\\bar{K} (\\leq K$) classes (cf. Definition 1.1 in ). Second, for all training samples, the distance to the initial weight matrix satisfies\n\\begin{equation*}\n\\lVert W^{\\tau} - W^0 \\rVert_\\mathrm{F} \\lesssim \\big(\\sqrt{K} + \\nicefrac{\\tau \\varepsilon_0 K^2}{\\lVert C \\rVert^2} \\big),\n\\end{equation*}\nwhere $0 \\leq \\tau \\leq I$ and $A \\lesssim B$ denotes $A \\leq \\beta B$ with some constant $\\beta$. \n\\begin{remark}\nThe above conclusions demonstrate that gradient descent with early stopping (i.e., $I$ steps) \ncan be robust when training deep neural networks. \nMoreover, the final network weights do not stray far from the initial weights for robustness, \nsince the distance between the initial model and final model grows with the square root of the number of clusters $\\sqrt{K}$.\n\\end{remark}\nIntuitively, due to memorization effects, deep neural networks will eventually overfit noisy training data~. Thus, it is a good strategy to stop training early, when deep neural networks fit clean training data in first few epochs. This denotes the robust weights are not far away from the initial weights.", "cites": [4115, 3630], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section analytically discusses the optimization policy perspective of LNRL using mathematical formulations and draws on the concept of early stopping. It integrates ideas from the cited papers on memorization and generalization, particularly highlighting the memorization effect of DNNs. However, the synthesis is somewhat limited to a single approach (early stopping), and the critical analysis is minimalâ€”there is no explicit evaluation of limitations or comparison with alternative strategies."}}
{"id": "16fef8bc-4031-464d-8299-ecae7d1eb048", "title": "Taxonomy", "level": "subsection", "subsections": [], "parent_id": "0631056c-9b2b-4b86-b30a-fc08a6ef0620", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Overview"], ["subsection", "Taxonomy"]], "content": "\\label{sec:taxonomy}\nBased on the above theoretical understanding, we categorize these works into three general perspectives:\n\\begin{enumerate}[leftmargin=*]\n\\item \\textit{Data}~: \nFrom the perspective of data, \nthe key is to build up the noise transition matrix $T$, which explores the data relationship between clean and noisy labels. Take loss correction as an example. We first model and estimate $T$ between latent $Y$ and observed $\\bar{Y}$. Then, via the estimated matrix, different correction techniques can generate $\\tilde{\\ell}$ from the original $\\ell$. \nMathematically, \n$\\widehat{R}_{\\tilde{\\ell},\\bar{D}}(f_{\\theta}) := \\frac{1}{n}\\sum_{i=1}^n \\tilde{\\ell}(f_{\\theta}(x_i), \\bar{y}_i)$,\nwhere \n$\\ell \\xrightarrow{T} \\tilde{\\ell}$, and $\\tilde{\\ell}$ is a corrected loss transitioning from $\\ell$ via $T$.\n\\item \\textit{Objective}: From the perspective of the objective, we can construct $\\tilde{\\ell}$ by augmenting the objective function, \ni.e., the original $\\ell$, \nvia either explicit or implicit regularization.\nFor instance, we may augment $\\ell$ by an auxiliary regularizer explicitly. Meanwhile, we may augment $\\ell$ by designing implicit regularization algorithms, such as soft-/hard-bootstrapping~\nand virtual adversarial training (VAT)~. \nMeanwhile, we can construct $\\tilde{\\ell}$ by reweighting the objective function $\\ell$. Lastly, we can also construct and design $\\tilde{\\ell}$ directly. Thus, $\\tilde{\\ell}$ has three options:\n\\begin{itemize}\n\\item $\\tilde{\\ell} = \\ell + r$, where $r$ denotes a regularization; \n\\item $\\tilde{\\ell} = \\sum_i w_i\\ell_i$, where $\\ell_i$ denotes $i$-th sub-objective \nwith the coefficient $w_i$; \n\\item $\\tilde{\\ell}$ has a special format $\\ell'$ independent of $\\ell$.\n\\end{itemize}\n\\item \\textit{Optimization}~: From the perspective of optimization, we can construct $\\tilde{\\ell}$ by leveraging the memorization effects of deep models. For example, due to the memorization effects, deep models tend to fit easy (clean) patterns first, and then over-fit complex (noisy) patterns gradually. Based on this observation, we can backpropagate the small-loss examples, which is equal to constructing the restricted $\\tilde{\\ell}$\nwhere $\\tilde{\\ell} = \\text{sort}(\\ell,1-\\tau)$, namely, sorting $\\ell$ from small to large, and fetching $1-\\tau$ percentage of small losses ($\\tau$ is the noise rate).\n\\end{enumerate}\nAccordingly, existing works can be categorized into a unified taxonomy as shown in Figure~\\ref{fig:taxonomy}. \nWe will detail each category in the sequel. Note that the discussion of three perspectives can be found in Appendix 2.", "cites": [4139, 3630, 4115, 139], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes ideas from multiple cited papers into a unified taxonomy, linking theoretical concepts like noise transition matrices and memorization effects to practical methods such as bootstrapping and virtual adversarial training. It demonstrates abstraction by generalizing the construction of corrected loss functions and discusses different optimization strategies. While it includes some critical points (e.g., explicit vs. implicit regularization), it does not deeply critique or compare the limitations of the approaches."}}
{"id": "af17c79e-e64b-44a4-8276-3856c711a6d8", "title": "Noise Transition Matrix", "level": "subsection", "subsections": [], "parent_id": "75c16b96-db92-4306-8203-f235a688b5ca", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Data"], ["subsection", "Noise Transition Matrix"]], "content": "Before introducing three common ways, we first define what the noise transition matrix is, and explain why the noise transition matrix is important.\n\\begin{definition}\n(Noise transition matrix~) Suppose that the observed noisy label $\\bar{y}$ is drawn independently from a corrupted distribution $p(X,\\bar{Y})$, where features are intact. Meanwhile, there exists a corruption process, transition from the latent clean label $y$ to the observed noisy label $\\bar{y}$. Such a corruption process can be approximately modeled via a \\textit{noise transition matrix} $T$, where $T_{ij} = p(\\bar{y} = e_j| y = e_i)$. \n\\end{definition}\nTo further understand $T$, \nwe present two representative \nstructures of $T$ (Figure~\\ref{fig:repT}): \n(1) Sym-flipping~; \n(2) Pair-flipping~. \nThe definition of the corresponding $T$ is as follow,\nwhere $\\tau$ is the noise rate and $n$ is the number of the classes.\n\\begin{figure}[ht]\n\\centering\n\\subfigure[Sym-flipping.]\n{\\includegraphics[height=0.08\\textheight]{sym}}\n\\qquad\n\\subfigure[Pair-flipping.]\n{\\includegraphics[height=0.084\\textheight]{pair}}\n\\vspace{-8px}\n\\caption{Two representatives of transition matrix $T$.}\n\\label{fig:repT}\n\\end{figure}\nSpecifically, the Sym-flipping structure models the common classification scenario under label noise, where the class of clean label can uniformly flip into other classes. Meanwhile, the Pair-flipping structure models the fine-grained classification scenario, where the class (e.g., Norwich terrier) of a clean label can flip into its adjunct class (e.g., Norfolk terrier) instead of a far-away class (e.g., Australian terrier). In the area of label-noise learning, we normally leverage the above two structures to generate simulated noise, and explore the root cause why the proposed algorithms can work on the simulated noise. Nonetheless, the real-world scenarios are very complex, where the noise transition matrix may not have structural rules (i.e., irregular). For example, \\textit{Clothing1M}~ is a Taobao clothing dataset, where mislabeled clothing images often share similar visual patterns. The noise structure of \\textit{Clothing1M} is irregularly asymmetric, which is hard ti estimate.\nThe mathematical modeling of $T$ can be found in Section~\\ref{sec:thm:idata} (Eq.~\\eqref{eq:trans}),\nwhich has been widely studied to build statistically consistent classifiers.\nNormally, the clean class posterior can be inferred by using $T$ and the noisy class posterior, i.e., we have the important equation $p(\\bar{Y}|x)=T \\cdot p(Y|x)$, where  $T$ is a bridge between clean and noisy information.\nAs the noisy class posterior can be estimated by exploiting the noisy training data, \nthe key step remains how to effectively estimate $T$ and leverage the estimated matrix to combat  label noise. \nBased on this observation, there are three general ways as \nin Sections~\\ref{sec:data:ada}-\\ref{sec:data:pro}.", "cites": [3454, 7773], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear definition of the noise transition matrix and connects it to two representative structures, integrating insights from relevant literature. While it references the cited papers to support the discussion, it does not deeply compare or critique the approaches they propose. It abstracts the role of $ T $ in bridging clean and noisy data but stops short of deriving overarching principles or a novel theoretical framework."}}
{"id": "d7498655-7294-4dd3-bc96-852e85620e57", "title": "Linear Case", "level": "subsubsection", "subsections": [], "parent_id": "f273f1da-19be-4d85-b744-80cb017f8b56", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Data"], ["subsection", "Adaptation Layer"], ["subsubsection", "Linear Case"]], "content": "To realize this adaptation layer, Sukhbaatar et al.~ proposed a constrained linear layer (i.e., constrained to be a probability matrix) inserted between the base network and cross-entropy loss layer. This linear adaptation layer is parameterized by $T$, which is equivalent to the function of  $T$. \nBased on this idea, we can modify a classification model using a probability matrix $T$ that modifies its prediction to match the label distribution of the noisy data. \nThe training model consists of two independent parts: the base model parameterized by $\\omega$ and the noise model parameterized by $T$. Since the noise matrix $T$ has been modeled as a constrained linear layer,  update of the $T$ matrix can be easily carried out by back-propagating the cross-entropy loss.\nHowever, it is hard to achieve the optimal $T$ via minimizing the cross-entropy loss, which is jointly parameterized by $\\omega$ and $T$. \nTo achieve the optimal $T$, Sukhbaatar et al.~ leveraged a regularizer on $T$, \ne.g., the trace norm or ridge regression, which forces it to approximate the optimal $T$. \nThis work paves the way for deep learning with noisy labels, which directly motivates the following nonlinear case of the adaptation layer.", "cites": [4136], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the constrained linear adaptation layer introduced by Sukhbaatar et al. and its role in modeling the noise matrix. While it integrates the key idea from the cited paper, it lacks deeper connections to other works, critical evaluation of its effectiveness or limitations, and broader abstraction or generalization to label-noise learning principles."}}
{"id": "5624754a-8cc8-4b12-8e92-638e4e750678", "title": "Nonlinear Case", "level": "subsubsection", "subsections": [], "parent_id": "f273f1da-19be-4d85-b744-80cb017f8b56", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Data"], ["subsection", "Adaptation Layer"], ["subsubsection", "Nonlinear Case"]], "content": "Following the linear case, Goldberger et al.~ proposed a non-linear layer inserted between the base network and cross-entropy loss layer to realize this adaptation layer.\nBeyond the linear case, the training model consists of two independent parts: the base model parameterized by $\\omega$ and the noise model/channel parameterized by $\\theta$ (equal to the function of the noise transition matrix). Since the latent outputs \nof the base model are hidden, they proposed to leverage the expectation-maximization (EM) algorithm~ to estimate the hidden outputs (E-step) and the current parameters (M-step). Different from the linear case, the nonlinear case is free of strong assumptions (see Section 3.2 in~).\nHowever, there are several potential drawbacks to the EM-based approach, such as local optimality and scalability. To address these issues, Goldberger et al.~ proposed two noise modeling variants: the c-model and s-model. Specifically, the c-model predicts the noisy label based on both the latent true label and the input features; while the s-model predicts the noisy label only based on the latent true label. Since the EM algorithm is equivalent to the s-model, they regarded both $\\omega$ and $\\theta$ as components of the same network and optimized them simultaneously. Moreover, the s-model is similar to the linear case proposed by Sukhbaatar et al.~, although they proposed a different learning procedure. Note that though the c-model depends on the input features, and they still leverage network training in the M-step to update $\\theta$.", "cites": [4136], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a basic analytical overview of nonlinear adaptation layers, integrating ideas from the cited paper by Goldberger et al. and connecting them to prior work (e.g., linear case and Sukhbaatar et al.). It identifies limitations of the EM-based approach and introduces the c-model and s-model as alternatives, which shows some critical evaluation. However, the synthesis is limited to a few sources and the abstraction does not rise to the level of a meta-level insight or overarching principle."}}
{"id": "bd1d010d-b83f-416b-bcaa-fc753eb4b8be", "title": "Gold Correction", "level": "subsubsection", "subsections": [], "parent_id": "a2fe8b6a-bc6a-4d6f-a8b8-e7a1f65578df", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Data"], ["subsection", "Loss Correction"], ["subsubsection", "Gold Correction"]], "content": "Based on Forward Correction, \nHendrycks et al.~ proposed Gold Loss Correction to handle severe noise. When severe noise exists, the transition matrix can not be estimated accurately by purely noisy data. \nThe key motivation is to assume that a small subset of the training data is trusted and available.\nNormally, a large number of crowdsourced workers may produce an untrusted set $\\widetilde{D}$; \nwhile a small number of experts can produce a trusted set $D$. In a high-level, Hendrycks et al.~ aimed to leverage $D$ to estimate the $T$ accurately, \nand employed Forward Correction based on the estimated matrix. Then, they trained deep neural networks on $\\widetilde{D}$ via the corrected loss, \nwhile training on $D$ via the original loss. \nThis method is called Gold Loss Correction (GLC).\nThus, \nGLC's key step is to estimate $T$ accurately via a trusted set $D$. \nSpecifically, \nwe can estimate $T$ by $\\widehat{T}$ as follows.\n\\begin{equation*}\n\\widehat{T}_{ij} \n= \\frac{1}{A_i}\\sum\\nolimits_{x \\in A_i}\\widehat{p}(\\bar{Y} = e_j|Y = e_i,x),\n\\end{equation*}\nwhere $A_i$ is the subset of $x$ in $D$ with label $i$ and a classifier $\\widehat{p}(\\bar{y}|x)$ can be modeled by deep neural networks trained on $\\widetilde{D}$. Empirically, the better estimate $\\widehat{T}$ will lead to the better GLC's performance.", "cites": [4145], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of Gold Loss Correction (GLC) and its motivation from the cited paper, but it lacks deeper synthesis with other works or a broader theoretical context. It briefly explains the method and its key steps but does not critically analyze its limitations or compare it with other LNRL approaches. The abstraction is minimal, focusing primarily on the specific technique without identifying general principles or patterns in the field."}}
{"id": "f1ec57da-c6a2-4d3c-92a4-af20b71b432f", "title": "Label Smoothing", "level": "subsubsection", "subsections": [], "parent_id": "a2fe8b6a-bc6a-4d6f-a8b8-e7a1f65578df", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Data"], ["subsection", "Loss Correction"], ["subsubsection", "Label Smoothing"]], "content": "\\label{label-smoothing}\nThe technique of label smoothing is to smooth labels by mixing in a uniform label vector, whose label distribution belongs to the uniform distribution~, which is a means of regularization. Lukasik et al.~ relates label smoothing to a general family of loss-correction techniques, \nwhich demonstrates that label smoothing significantly improves performance under label noise. \nIn general,  and  can be unified into a label smearing framework:\n\\begin{equation*}\n\\ell^{\\text{SM}}(f_{\\theta}(X),Y) \n= M \\cdot \\ell(f_{\\theta}(X),Y),\n\\end{equation*}\nwhere $M$ is a smearing matrix~. Such a matrix is used for bridging the original loss $\\ell$ and the smeared loss $\\ell^{\\text{SM}}$.\nTherefore, in this framework, there are three examples:\n\\begin{itemize}[leftmargin=*]\n\\item Standard training: suppose $M = I$, where $I$ is the identity matrix.\n\\item Label smoothing: suppose $M = (1-\\alpha) I + \\frac{\\alpha J}{L} $, where $J$ is the all-ones matrix and $L$ is the number of classes.\n\\item Backward correction under symmetric noise: suppose $M = \\frac{1}{1-\\alpha}\\cdot(I - \\frac{\\alpha J}{L})$, where $M = T^{-1}$ in Theorem~\\ref{bc-theorem}. \n\\end{itemize}\nWe can clearly see the close connection between label smoothing and backward correction. Actually, label smoothing can have a similar effect to \nshrinkage regularization~.", "cites": [301, 4148], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 4.0}, "insight_level": "medium", "analysis": "The section analytically connects label smoothing to loss-correction techniques, using Paper 2 to frame it within a broader context of label-noise handling. It introduces a unifying label smearing framework and highlights the relationship between label smoothing and backward correction. While it offers abstraction by generalizing the method into a mathematical formulation, it lacks deeper critical evaluation of the limitations or broader empirical comparisons across works."}}
{"id": "322c9a77-495e-4564-a22f-7e01448a7b25", "title": "Human-in-the-Loop Estimation", "level": "subsubsection", "subsections": [], "parent_id": "77ea0429-d362-45c0-928b-10900efd468b", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Data"], ["subsection", "Prior Knowledge"], ["subsubsection", "Human-in-the-Loop Estimation"]], "content": "Han et al.~ proposed a human-assisted approach called ``Masking'', which decouples the structure and value of the transition matrix. Specifically, the structure can be viewed as a prior knowledge, coming from human cognition, since human can mask invalid class transitions (e.g., cat $\\nleftrightarrow$ car). Given the structure information, we can only focus on estimating the noise transition probability along the structure in an end-to-end system. \nTherefore, the estimation burden will be largely reduced. Actually, it makes sense that human cognition masks invalid class transitions and highlights valid class transitions, such as the column-diagonal, tri-diagonal and block-diagonal structures. Therefore, the remaining issue is how to incorporate such prior structure into an end-to-end learning system. The answer is a generative model.\nSpecifically, there are three steps in Masking. \nFirst, the latent ground-truth label is from $y\\sim p(y|x)$, \nwhere $p(y|x)$ is a categorical distribution. Second, the noise transition matrix variable $t$ is from $t\\sim p(t)$  \nand its structure \nis generated as $t_o\\sim p(t_o)$, \nwhere $p(t)$ is an implicit distribution modeled by a neural network without its explicit form, structure prior $p(t_o)=p(t)\\frac{dt}{dt_o}\\big|_{t_o=f(t)}$ provided by human cognition, \nand $f(\\cdot)$ is the mapping function from $t$ to $t_o$. \nThird, the noisy label is from $\\bar{y}\\sim p(\\bar{y}|y,t)$, where $p(\\bar{y}|y,t)$ models the transition from $y$ to $\\bar{y}$ given $t$. Based on this generative process, \nwe can deduce the evidence lower bound~\nto approximate the log-likelihood of the noisy data.", "cites": [7773], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the key idea from Han et al. effectively, explaining the ``Masking'' approach and its generative model framework in a coherent way. It provides some abstraction by highlighting the role of human-provided structure in reducing estimation burden and mentions broader structures like column-diagonal and block-diagonal. However, it lacks deeper critical analysis of the limitations or trade-offs of this method, and while it integrates one paper well, it does not connect to other works in the broader LNRL literature."}}
{"id": "9b02a7b8-a622-4f6f-a802-74327e756509", "title": "Fine-tuning Revision", "level": "subsubsection", "subsections": [], "parent_id": "77ea0429-d362-45c0-928b-10900efd468b", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Data"], ["subsection", "Prior Knowledge"], ["subsubsection", "Fine-tuning Revision"]], "content": "Xia et al.~ introduced a transition-revision method to effectively learn the transition matrix, \nwhich is called Reweight T-Revision (Reweight-R). \nSpecifically, they first initialized the transition matrix by exploiting data points that are similar to anchor points, having high noisy class posterior probabilities. Thus, in the Reweighted-R method, the initialized transition matrix is viewed as a prior knowledge. Then, they fine-tuned the initialized matrix by adding a slack variable, which is then learned and validated together with the classifier by using noisy data.\nSpecifically, given noisy training sample \n$\\bar{\\mathcal{D}}^{\\text{tr}}$ and noisy validation set $\\bar{\\mathcal{D}}^{\\text{v}}$, \nthere are two stages in Reweight-R. In the first stage, the unweighted loss is minimized to learn $\\hat{p}(\\bar{Y}|X=x)$ without a noise adaption layer. \nThen, the noise transition matrix $\\hat{T}$ is initialized, \nwhich can be viewed as a prior knowledge for further fine-tuning. \nNamely, $\\hat{T}$ is initialized according to (1) in  by using data with the highest $\\hat{p}(\\bar{Y} = e_i|X=x)$ as anchor points for the $i$-th class.\nIn the second stage, based on the prior $\\hat{T}$, the neural network is initialized by minimizing the weighted loss with a noisy adaptation layer $\\hat{T}^\\top$. \nFurthermore, the weighted loss is minimized to learn classifier $f$ and incremental $\\Delta T$ with a noisy adaptation layer $(\\hat{T} + \\Delta T)^\\top$. \nNamely, the second stage modifies $\\hat{T}$ gradually by adding a slack variable $\\Delta T$, \nand learns the classifier and $\\Delta T$ by minimizing the weighted loss. \nThe two stages alternate until converges, namely achieving minimum error on $\\bar{\\mathcal{D}}^{\\text{v}}$.", "cites": [4152], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear analytical explanation of the Reweight-R method, integrating it within the broader context of using prior knowledge in label-noise learning. It synthesizes the methodâ€™s two-stage process and its use of anchor points, drawing from the cited paper. However, while it explains the method, it does not deeply compare it with other approaches or critically evaluate its limitations, which constrains its insight quality."}}
{"id": "9565da23-d5ac-44bf-b242-08852418038e", "title": "Explicit Regularization", "level": "subsubsection", "subsections": [], "parent_id": "3c156707-13c8-4d41-a27b-abbe2cd56ce7", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Objective"], ["subsection", "Regularization"], ["subsubsection", "Explicit Regularization"]], "content": "\\label{exp-reg}\nAzadi et al.~ proposed a novel regularizer $r = \\Omega_{\\text{aux}}(w)$ to exploit the data structure for combating label noise, \nwhere $\\Omega_{\\text{aux}}(w) = \\left\\lVert Fw\\right\\rVert_\\mathrm{g}$. \nNote that $\\left\\lVert \\cdot \\right\\rVert_\\mathrm{g}$ denotes the group norm \nand $F^{\\top} = [X_1,\\ldots,X_n]$ represents the set of diagonal matrices (e.g., $X_i$) consisting of the (e.g., $i$-th) data features,\nwhich induces a non-overlapping group sparsity\nthat encourages  most coefficients to be zero. \nThis operation will encourage a small number of clean data to contribute to learning of the model, while filtering mislabeled and non-relevant data. \nIn other words, this regularizer enforces the features of the good data to be used for modeling learning, while noisy additional activations will be disregarded. \nIt is worth noted that Berthelot et al.~ introduced MixMatch for semi-supervised learning (SSL), and the empirical performance of MixMatch reaches the state-of-the-art. Most importantly, one of the key components in MixMatch is Minimum Entropy Regularization (MER), which belongs to explicit regularization in LNRL. Speicifically, MER was proposed by Grandvalet \\& Bengio~, and the key idea is to augment the cross-entropy loss with an explicit term encouraging the classifier to make predictions with high confidence on the unlabeled examples, namely minimizing the model entropy  \nfor unlabeled data $x$. Similar to MER, the pseudo-label method conducts entropy minimization implicitly~, which generates hard labels from high-confidence predictions on unlabeled data for further training. In particular, the pseudo-label method (i.e., label guessing) first computes the average of the modelâ€™s predicted class distributions across all augmentations, and then applies a temperature sharpening function to reduce the entropy of the label distribution.\nMiyato et al.~ also explored the smoothness for combating label noise, and they proposed the virtual adversarial loss, which is a new measure of local smoothness of the conditional label distribution given input. Specifically, their method enforces the output distribution to be isotropically smooth around each input data \nvia selectively smoothing the model in its most anisotropic direction. The\nbenefit of such smoothness makes the model output insensitive  to the input data. To realize such smoothness, they first design the virtual adversarial direction, which can greatly deviate from the current inferred output distribution, from the status quo without the label information. Based on such a direction, they defined local distributional smoothness, and then proposed a training method called virtual adversarial training (VAT).\nMathematically, \nthey first defined local distributional smoothness (LDS):\n\\begin{equation}\n\\text{LDS}(x^*,\\theta):=D[p(y|x^*,\\hat{\\theta}),p(y|x^*+r_{\\text{vadv}},\\theta)],\n\\end{equation}\nwhere\n$r_{\\text{vadv}}\n:= \\arg\\max\\nolimits_{\\left\\lVert r \\right\\rVert_2 \\leq \\epsilon} \nD[ p(y|x^*,\\hat{\\theta}),p(y|x^*+r) ]$,\n$D$ is a non-negative function that measures the distribution divergence, and\n$x^*$ represents either labeled or unlabeled features. We use $\\hat{\\theta}$ to\ndenote the vector of model parameters at a specific iteration step of the\ntraining process. Then, we have a regularization term:\n\\begin{equation*}\nR_{\\text{vadv}}(D_\\mathrm{l},D_\\mathrm{ul},\\theta)\n:=\\nicefrac{1}{N_l+N_{ul}}\n\\sum\\nolimits_{x^* \\in D_l,D_{ul}}\\text{LDS}(x^*,\\theta),\n\\end{equation*}\nwhere $D_l$ denotes the label dataset with size $N_l$; and $D_{ul}$ denotes the unlabeled dataset with size $N_{ul}$. \nTherefore, the full objective function of VAT is given by\n\\begin{equation*}\n\\ell(D_l,\\theta) + \\alpha R_{\\text{vadv}}(D_l,D_{ul},\\theta).\n\\end{equation*}", "cites": [7778, 4183, 139], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key regularization techniques from different papers (Azadi et al., Berthelot et al., Miyato et al.) and connects them under the umbrella of explicit regularization in LNRL. It provides mathematical formulations and explains the core ideas, showing some integration of concepts. However, it lacks deeper critical evaluation or comparison of the methods' strengths and weaknesses. There is some abstraction in linking the methods to the broader goal of robust learning, but the analysis remains grounded in the specific techniques rather than revealing more general principles."}}
{"id": "88555417-b9ef-43dc-8f04-429b8079ed3c", "title": "Implicit Regularization", "level": "subsubsection", "subsections": [], "parent_id": "3c156707-13c8-4d41-a27b-abbe2cd56ce7", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Objective"], ["subsection", "Regularization"], ["subsubsection", "Implicit Regularization"]], "content": "Recently, there are more and more implicit regularizers, which take the effects of regularization without the explicit form (cf.~Section~\\ref{exp-reg}).\nFor example,\n\\begin{itemize}[leftmargin=*]\n\\item Bootstrapping~: Reed et al.~\naugmented the prediction objective with a notion of consistency. In the\nhigh-level, this provides the learner justification to ``disagree'' with a\nperceptually-inconsistent training label, and effectively re-label the data.\nNamely, the learner bootstraps itself in this way, which uses a convex\ncombination of training labels and the current modelâ€™s predictions to generate\nthe training targets. Intuitively, as the learner improves over time, its\npredictions can be trusted more. Such bootstrapping can avoid modeling the noise\ndistribution. Specifically, Reed et al.~ proposed two ways\nto realize bootstrapping, such as soft and hard bootstrapping. The soft version\nloss $\\ell_{\\text{soft}}$ uses predicted class probabilities $q$ directly to\ngenerate regression targets for each batch as follows:\n\\begin{equation*}\n\\ell_{\\text{soft}}(q,t) \n= \\sum\\nolimits_{k=1}^{L}[\\beta t_k + (1-\\beta)q_k]\\log(q_k),\n\\end{equation*}\nwhere $t$ denotes the training labels.\nThe parameter $\\beta$ balances the prediction $q$ and target $t$, which can be found via cross validation. The soft version loss is equivalent to softmax regression with minimum entropy regularization, which encourages the model to have a high confidence in predicting labels. Meanwhile, the hard version loss $\\ell_{\\text{hard}}$ modifies targets using a MAP estimate of $q$ given $\\mathbf{x}$ as follows:\n\\begin{equation*}\n\\ell_{\\text{hard}}(q,t) \n= \\sum\\nolimits_{k=1}^{L}[\\beta t_k + (1-\\beta)z_k]\\log(q_k),\n\\end{equation*}\nwhere $z_k = \\mathbf{1}[k=\\arg\\max_{i = 1,\\ldots,L} q_i]$ and $\\mathbf{1}[\\cdot]$ denotes the indicator function. To solve the hard version via stochastic gradient descent, an EM-like method will be employed. In the E-step, the approximate-truth confidence targets are estimated as a convex combination of training labels and model predictions. In the M-step, the model parameters are updated in order to predict those generated targets better.\n\\item Mixup~: Motivated by vicinal risk minimization~, Zhang et al.~ introduced a data-agnostic regularization method called Mixup, which constructs virtual training examples $(\\tilde{x},\\tilde{y})$ as follows.\n\\begin{equation}\\label{mixup}\n\\tilde{x} = \\lambda x_i + (1-\\lambda)x_j\n\\;\\text{and}\\;\n\\tilde{y} = \\lambda y_i + (1-\\lambda)y_j,\n\\end{equation}\nwhere $(x_i,y_i)$ and $(x_j,y_j)$ are two examples randomly drawn from the training data and $\\lambda \\in [0,1]$ is a balanced parameter between two examples.\nIntuitively, Mixup conducts virtual data augmentation, \nwhich dilutes the noise effects and smooths the data manifold. \nThis simple but effective idea can be used in not only noisy labels but also adversarial examples, \nsince the smoothness happens in both features and labels according to \\eqref{mixup}.\n\\item SIGUA~: It is noted that, given data with noisy labels, \nover-parameterized deep networks can gradually fit the whole data~.\nTo relieve this issue, \nHan et al.~ proposed a gradient-ascent method called stochastic integrated gradient underweighted ascent (SIGUA): \nin a mini-batch, \nwe adopt gradient descent on good data as usual, and learning-rate-reduced gradient ascent on bad data; \nthe proposal is a versatile approach where data goodness or badness is w.r.t. desired or undesired memorization given a base learning method.\nTechnically, SIGUA is a specially designed regularization by pulling optimization back for generalization when their goals conflict with each other.\nA key difference between SIGUA and parameter shrinkage like weight decay is that SIGUA pulls optimization back on some data but parameter shrinkage does the same on all data; philosophically, SIGUA shows that forgetting undesired memorization can reinforce desired one, which provides a novel viewpoint on the inductive bias of neural networks.\n\\end{itemize}", "cites": [4139, 3630, 4115, 4457, 7191], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key implicit regularization techniques from multiple papers (Bootstrapping, Mixup, SIGUA) and integrates them under a common theme of mitigating label noise through model self-correction or data manipulation. It critically evaluates each methodâ€™s rationale and limitations, such as the need for cross-validation in Bootstrapping and the distinction between SIGUA and traditional regularization. The discussion abstracts the concept of implicit regularization by highlighting broader principles like inductive bias and forgetting as mechanisms to improve robustness."}}
{"id": "b8aa75c2-9b46-4f2b-a640-7d5aa8301191", "title": "Importance Reweighting", "level": "subsubsection", "subsections": [], "parent_id": "89d39c41-4b33-4f19-8ccf-5eb0810e7166", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Objective"], ["subsection", "Reweighting"], ["subsubsection", "Importance Reweighting"]], "content": "Liu and Tao  introduced importance reweighting  from domain adaptation to label-noise learning by treating the noisy training data as the source domain and the clean test data as the target domain. The idea is to rewrite the risk w.r.t. the clean data by exploiting the noisy data. Specifically, \n\\begin{align*}\n\\label{eq:importance}\n&R(f)=\\mathbb{E}_{(X,Y)\\sim D}[\\ell(f(X),Y)]\\\\\n&=\\int_{x}\\sum_ip_D(X=x,Y=i)\\ell(f(x),i)dx\\nonumber\\\\\n&=\\int_{x}\\sum_ip_{\\bar{D}}(X=x,\\bar{Y}=i)\\frac{p_D(X=x,\\bar{Y}=i)}{p_{\\bar{D}}(X=x,\\bar{Y}=i)}\\ell(f(x),i)dx\\nonumber\\\\\n&=\\int_{x}\\sum_ip_{\\bar{D}}(X=x,\\bar{Y}=i)\\frac{p_D(\\bar{Y}=i|X=x)}{p_{\\bar{D}}(\\bar{Y}=i|X=x)}\\ell(f(x),i)dx\\\\\n&=\\mathbb{E}_{(X,\\bar{Y})\\sim \\bar{D}}[\\beta(X,\\bar{Y}){\\ell}(f(X),\\bar{Y})],\\nonumber\n\\end{align*}\nwhere the second last equation holds because label noise is assumed to be independent of instances and $\\beta(X,\\bar{Y})=p_D(\\bar{Y}=i|X=x)/p_{\\bar{D}}(\\bar{Y}=i|X=x)$ denotes the weights which plays a core role in importance reweighting and can be learned by either exploiting the transition matrix or a small set of clean data.", "cites": [3453], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical explanation of the importance reweighting approach by formalizing the method and explaining its theoretical underpinnings using equations. It synthesizes the idea from domain adaptation into the label-noise learning context, showing conceptual integration. However, it lacks a deeper critical evaluation of the method's limitations or comparisons with alternatives, and while it introduces a general framework, it does not rise to the level of meta-level insights."}}
{"id": "d1e5a7e4-2062-42df-9878-dad524899233", "title": "Bayesian Method", "level": "subsubsection", "subsections": [], "parent_id": "89d39c41-4b33-4f19-8ccf-5eb0810e7166", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Objective"], ["subsection", "Reweighting"], ["subsubsection", "Bayesian Method"]], "content": "Wang et al.~ proposed reweighted probabilistic models (RPM) to combat label noise. \nThe idea is simple and intuitive: down-weighting on corrupted labels but up-weighting on clean labels, which brings us Bayesian data reweighting. The mathematical formulations include three steps:\n\\begin{itemize}[leftmargin=15px]\n\\item[1)] Define a probabilistic model $p_{\\beta}(\\beta)\\prod_{n=1}^{N}\\ell(y_n|\\beta)$.\n\\item[2)] Assign a positive latent weight $w_n$ to each likelihood $\\ell(\\cdot|\\beta)$,\nand choose a prior on the weights $p_w(w)$, where $w=(w_1,\\ldots, w_N)$. Thus, the RPM can be represented by\n\\begin{equation*}\np(y,\\beta,w)\n= \\nicefrac{1}{Z} \\cdot p_{\\beta}(\\beta)p_{w}(w)\n\\prod\\nolimits_{n=1}^{N}\\ell(y_n|\\beta)^{w_n},\n\\end{equation*}\nwhere $Z$ is the normalizing factor.\n\\item[3)] Infer the posterior of both the latent variables $\\beta$ and the weight $w$: $p(\\beta,w|y)$. The prior knowledge on the weights $p_w(w)$ trades off extremely low likelihood terms, where the options of $p_w(w)$ are a bank of Beta priors, a scaled Dirichlet prior and a bank of Gamma priors. Note that RPMs treat weights $w$ as latent variables, which are automatically inferred.\n\\end{itemize}\nArazo et al.~ introduced a two-component (clean-noisy) beta mixture model (BMM) for a mixture of clean and noisy data, which brings us a bootstrapping loss. Specifically, the posterior probabilities under BMM are leveraged to implement a dynamically-weighted bootstrapping loss, robustly dealing with noisy samples without discarding them. Mathematically, the probability density function \nof a mixture model of $K$ components \non the loss $\\ell$ is defined as\n\\begin{equation}\np(\\ell)\n= \\sum\\nolimits_{k=1}^K \\lambda_k p(\\ell|k),\n\\end{equation}\nwhere $K=2$ is our case, $\\lambda_k$ are mixing \nweights, and $p(\\ell|k)$ can be modeled by the beta distribution:\n\\begin{equation}\np(\\ell|\\alpha,\\beta)\n= \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\ell^{\\alpha-1} (1-\\ell)^{\\beta-1}.\n\\end{equation}\nThe above BMM can be solved by the EM procedure. Specifically, latent variables $\\gamma_k(\\ell) = p(k|\\ell)$ are introduced. In the E-step,  $\\lambda_k$, $\\alpha_k$, $\\beta_k$ are fixed and $\\gamma_k(\\ell)$ is updated via the Bayes rule. In the M-step, given fixed $\\gamma_k(\\ell)$, $\\alpha_k$ and $\\beta_k$ are estimated using the weighted moments. Meanwhile, the dynamic weights are updated in an easy way: $\\lambda_k = \\frac{1}{N}\\sum_{i=1}^{N}\\gamma_k(\\ell_i)$. Based on this BMM model, they further proposed dynamic hard/soft bootstrapping losses, where the weight $w_i$ of each sample is dynamically set to $p(k=1|\\ell_i)$.", "cites": [4455, 4135], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes two Bayesian-based reweighting methods, RPM and BMM, by connecting their probabilistic formulations and inference strategies. It provides a clear, integrated explanation of how these approaches model noisy and clean labels using Bayesian principles. However, it lacks deeper critical analysis of their limitations or trade-offs, and while it offers some abstraction through the lens of mixture models and latent variables, it remains primarily focused on methodological descriptions."}}
{"id": "290b05af-9b42-4afd-b17c-d3830ca9ffec", "title": "Neural Networks", "level": "subsubsection", "subsections": [], "parent_id": "89d39c41-4b33-4f19-8ccf-5eb0810e7166", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Objective"], ["subsection", "Reweighting"], ["subsubsection", "Neural Networks"]], "content": "Shu et al.~ introduced Meta-Weight-Net (MW-Net), which can adaptively learn an explicit weighting function from data. \nIn high-level, \nthe weighting function is an MLP with one hidden layer, mapping from a loss to weights. \nMathematically, the optimal parameter $w$ can be calculated by minimizing the weighted loss.\n\\begin{align*}\nw^*(\\theta) \n= \\arg\\min_{w}\\mathcal{\\ell}^{\\text{tr}}(w;\\theta)\n= \\nicefrac{1}{N}\\sum\\nolimits_{i=1}^{N}\\mathcal{V}(\\ell_i^{\\text{tr}}(w);\\theta)\\ell_i^{\\text{tr}}(w),\n\\end{align*}\nwhere $\\mathcal{V}(\\ell_i^{\\text{tr}}(w);\\theta)$ denotes MW-Net.\nHere, the parameters in MW-Net can be optimized by the meta learning idea. Given a small amount of clean and balanced meta-data $\\{x_i^{(\\text{meta})},y_i^{(\\text{meta})}\\}_{i=1}^M$, the optimal parameter $\\theta$ can be obtained by minimizing the meta-loss:\n\\begin{align*}\n\\theta^* \n= \\arg\\min_{\\theta}\\ell^{\\text{meta}}(w^*(\\theta))\n=\\nicefrac{1}{M}\\sum\\nolimits_{i=1}^{M}\\ell_i^{\\text{meta}}(w^*(\\theta)).\n\\end{align*}\nThen SGD is employed to update $w$ (parameters of classifier network) and $\\theta$ (parameters of MW-Net) iteratively.", "cites": [7774], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the Meta-Weight-Net method and its mathematical formulation, but does not integrate or synthesize ideas from other papers. It lacks critical analysis of the methodâ€™s strengths and limitations, and offers minimal abstraction beyond the specific paper cited."}}
{"id": "6b8e75d9-1c46-4d4a-9825-4e527b09ba1e", "title": "Loss Redesign", "level": "subsubsection", "subsections": [], "parent_id": "f89f4416-15b0-4666-9c8a-3d22e8bc1664", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Objective"], ["subsection", "Redesigning"], ["subsubsection", "Loss Redesign"]], "content": "In recent years, there are a lot of \nnew losses for combating label noise. Their designs are based on different principles, such as gradient clipping~ and curriculum learning~. \nHere, we choose several representative losses to explain.\n\\begin{itemize}[leftmargin=*]\n\\item Zhang et al.~ proposed a generalized cross-entropy loss called $\\ell_q$, which encompasses both the mean absolute error\n(MAE) and categorical cross entropy (CCE) loss. Since the $\\ell_q$ loss is a generalization of MAE and CCE, it enjoys the benefits of both the noise-robustness provided by MAE and the implicit weighting scheme of CCE, which can be well justified by theoretical analysis.\nMore importantly, it empirically works well for both closed-set noise~ and open-set noise~.\nMathematically, they used the negative Box-Cox transformation as a \n$\\ell_q$ loss function: \n\\begin{equation*}\n\\ell_q(f(x),e_j) = (1-f_j(x)^q) / q,\n\\end{equation*}\nwhere $q \\in (0,1]$ and $e_j$ is one-hot vector belonging to ghe $j$-th class. The $\\ell_q$ loss is reduced to CCE when\n$\\lim_{q\\rightarrow0}\\ell_q(f(x),e_j)$, and becomes MAE when $q = 1$. Since a\ntighter bound in $\\ell_q$ can bring stronger noise tolerance due to discarding many samples for training, they proposed the truncated $\\ell_q$ loss:\n\\begin{equation}\n\\ell_\\text{trunc}(f(x),e_j)\n=\n\\begin{cases}\n\\ell_q(k) & \\text{if} \\; f_j(x) \\leq k, \\\\\n\\ell_q(f(x),e_j) & \\text{otherwise}, \\\\\n\\end{cases}\n\\end{equation}\nwhere $0 < k < 1$, and $\\ell_q(k) = \\nicefrac{1-k^q}{q}$. When $k \\rightarrow 0$, the truncated $\\ell_q$ loss equals the original $\\ell_q$ loss.\n\\item\nCharoenphakdee et al.~ theoretically justified symmetric losses through the lens of theoretical tools, including the classification-calibration condition, excess risk bound, conditional risk minimizer and AUC-consistency. The key idea is to design a loss that does not have to satisfy the symmetric condition everywhere, which gives a high penalty in the non-symmetric area,\ni.e., $\\ell(z) + \\ell(-z)$ is a constant for every $z \\in \\mathbb{R}$. Motivated by this phenomenon, they introduced a barrier hinge loss, satisfying a symmetric condition not everywhere but gives a large penalty once $z$ is outside of the symmetric interval,\nwhich incentivizes to learn a prediction function inside of the symmetric interval. \nMathematically, a barrier hinge loss is defined as follows.\n\\begin{equation}\n\\ell(z) = \\max(-b(r+z)+r,\\max(b(z-r),r-z)),\n\\end{equation}\nwhere $b > 1$ and $r > 0$.\n\\item\nThulasidasan et al.~ proposed to abstain some confusing examples during training deep networks. \nIn practice, abstention has some relation with loss redesign. Based on abstention, they introduced a deep abstaining classifier (DAC). \nDAC has an additional output $p_{k+1}$, which indicates the probability of abstention.\nThe loss of DAC is as follows.\n\\begin{equation}\n\\ell(x_j) \n= - \\tilde{p}_{k + 1} \\sum\\nolimits_{i=1}^k t_i\\log( \\nicefrac{p_i}{\\tilde{p}_{k + 1}} )\n- \\alpha\\log \\tilde{p}_{k + 1},  \n\\end{equation}\nwhere $\\tilde{p}_{k + 1} = 1-p_{k+1}$. If $\\alpha$ is large, the penalty drives $p_{k+1}$ to zero, which leads the model not to abstain. If $\\alpha$ is small, the classifier may abstain on everything. They further proposed an auto-tuning algorithm to find the optimal $\\alpha$. Note that DAC can be used for both structured (e.g., asymmetric and pair-flipping) and unstructured (i.e., symmetric) label noise, where DAC works as a data cleaner.\n\\item\nAditya et al.~ leveraged gradient clipping to design a new loss. \nIntuitively, clipping the gradient prevents over-confident descent steps in the scenario of label noise. Motivated by gradient clipping, they proposed the partially Huberized loss\n\\begin{equation}\n\\tilde{\\ell}_{\\theta}(x,y)\n\\! = \\!\n\\begin{cases}\n-\\tau p_{\\theta}(x,y)\n\\! + \\! \\log\\tau \\! + \\! 1 & \\text{if} \\; p_{\\theta}(x,y) \\leq \\frac{1}{\\tau}, \n\\\\\n-\\log p_{\\theta}(x,y) & \\text{otherwise}.\n\\end{cases}\n\\end{equation}\n\\item\nLyu et al.~ proposed the curriculum loss (CL), which is a tighter \nupper bound of the 0-1 loss compared to the conventional surrogate of the 0-1 loss (cf. Eq. (8) in ). Moreover, CL can adaptively select samples for stagewise training. In particular, giving any base loss function $\\ell(u) \\geq \\mathbf{1} (u < 0), u \\in \\mathbb{R}$, CL is defined as follows.\n\\begin{align*}\nQ(\\mathbf{u}) \n=\n\\max\n\\left( \n\\min\\nolimits_{\\mathbf{v}\\in\\{0,1\\}^n} f_1(\\mathbf{v})\n,\n\\min\\nolimits_{\\mathbf{v}\\in\\{0,1\\}^n} f_2(\\mathbf{v})\n\\right),\n\\end{align*}\nwhere\n\\begin{align*}\nf_1(\\mathbf{v}) \n\\! = \\! \\sum_{i=1}^n v_i\\ell(u_i)\n\\;\\text{and}\\;\nf_2(\\mathbf{v})\n\\! = \\! n \\! - \\! \\sum_{i=1}^n v_i \\! + \\! \\sum_{i=1}^n \\mathbf{1}(u_i \\! < \\! 0).\n\\end{align*}\nTo adapt CL for deep learning models, they further introduced the noise pruned CL.\n\\end{itemize}", "cites": [4130, 3340, 4182, 8630, 7781], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of loss redesign approaches for label-noise robustness, connecting ideas from multiple papers (e.g., Zhang, Charoenphakdee, Thulasidasan, Aditya, Lyu). It integrates different principles like gradient clipping and abstention into a broader discussion of loss functions. While it offers some theoretical context and mathematical formulations, it lacks deeper comparative evaluation or meta-level generalization that would elevate it to a high insight level."}}
{"id": "4cdc34e5-5f24-43ec-806b-c7c7c4a2d158", "title": "Label Ensemble", "level": "subsubsection", "subsections": [], "parent_id": "f89f4416-15b0-4666-9c8a-3d22e8bc1664", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Objective"], ["subsection", "Redesigning"], ["subsubsection", "Label Ensemble"]], "content": "\\begin{itemize}[leftmargin=*]\n\\item Laine and Aila~ introduced self-ensembling in semi-supervised learning, including the $\\pi$-model and temporal ensembling, which can also be used to purify the label noise. The key idea of self-ensembling is to form a consensus prediction of the unknown labels using the outputs of the network in training. Specifically, the $\\pi$-model encourages consistent network output between two realizations of the same input, under two different dropout conditions. \nBeyond the $\\pi$-model, temporal ensembling is considered for the network predictions over multiple previous training epochs. \nThe loss function of the $\\pi$-model is\n\\begin{equation}\n\\ell = -\\nicefrac{1}{B}\\sum\\nolimits_i\n\\log z_i[y_i] + \\nicefrac{w(t)}{C|B|}\n\\sum\\nolimits_i \\left\\|z_i-\\tilde{z}_i\\right\\|^2,\n\\end{equation}\nwhere the first term handles labeled data via the standard cross-entropy loss, namely, $\\log z_i[y_i]$ calculates the cross-entropy loss value between model prediction $z_i$ and label $y_i$, and the second term handles unlabeled data. Note that $(x_i,y_i)$ denotes the pair of input and label, $B$ denotes the mini-batch size, and $C$ denotes the number of different classes. Both $z_i$ and $\\tilde{z}_i$ are transformed from the same input $x_i$, namely two predictions via the same network with different dropout conditions. The second term is also scaled by time-dependent weighting function $w(t)$. Temporal ensembling goes beyond $\\pi$-model by aggregating the predictions of multiple previous network evaluations into an ensemble prediction. Namely, the main difference from $\\pi$-model is that the network and augmentations are evaluated only once per input in each epoch, and the target $\\tilde{z}$ is based on prior network evaluations. \nAfter every training epoch, \nthe network outputs $z_i$ are accumulated into ensemble outputs $Z_i$ by updating $Z_i \\leftarrow \\alpha Z_i + (1-\\alpha)z_i$ and $\\tilde{z} \\leftarrow \\nicefrac{Z_i}{(1-\\alpha^t)}$, where $\\alpha$ is a momentum term.\n\\item\nNguyen et al.~ proposed a self-ensemble label filtering (SELF) method to progressively filter out the wrong labels during training. \nIn high-level, they leveraged the knowledge provided in the network's output over different training iterations to form a consensus of predictions, which progressively identifies and filters out the noisy labels from the labeled data. In the filtering strategy, the model can determine the set of potentially corrected samples $L_i$ based on agreement between the label $y$ and its maximum likelihood prediction $\\hat{y}_x$ with $L_i = \\{(y,x)|\\hat{y}_x = y; \\forall (y,x) \\in L_0\\}$,\nwhere $L_0$ is the sample set \nin the beginning. In the self-ensemble strategy, they maintained the two-level ensemble. First, they leveraged the model ensemble with Mean Teacher~, \nnamely an exponential running average of model snapshots. Second, they employed a\nprediction ensemble by collecting the sample predictions over multiple training\nepochs: $\\bar{z}_j = \\alpha \\bar{z}_{j-1} + (1-\\alpha)\\hat{z}_j$, where\n$\\bar{z}_j$ depicts the moving-average prediction of sample $k$ at epoch $j$ and\n$\\hat{z}_j$ is the model prediction for sample $k$ at epoch $j$.\n\\item\nMa et al.~ \ninvestigated the dimensionality of the deep representation subspace of training samples. \nThen, they developed a dimensionality-driven learning strategy, \nwhich monitors the dimensionality of subspaces during training and adapts the loss function accordingly. The key idea is to leverage the local intrinsic dimensionality (LID), which discriminates clean labels and noisy labels during training deep networks.\nMathematically, the estimation of LID can be defined as\n\\begin{equation}\n\\widehat{\\text{LID}} \n= -\n\\big( \\nicefrac{1}{k}\\sum\\nolimits_{i=1}^k\\log\\nicefrac{r_i(x)}{r_{\\max}(x)}\\big)^{-1},\n\\end{equation}\nwhere $r_i(x)$ denotes the distance between $x$ and its $i$-th nearest neighbor, and $r_{\\max}(x)$ denotes the maximum of the neighbor distance. Specifically, when learning with clean labels, the LID score is consistently decreasing and the test accuracy is increasing with the increase of training epochs. However, when learning with noisy labels, the LID score first decreases and then increases after a few epochs. In contrast, the test accuracy is totally opposite, which first increases and then decreases. Based on the LID score, the dynamics of deep networks is overseen.\n\\end{itemize}", "cites": [3342, 7162, 199], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"error": "Failed to parse LLM response", "raw_response": "{\n    \"type\": \"analytical\",\n    \"scores\": {\"synthesis\": 3.5, \"critical\": 2.5, \"abstraction\": 3.0},\n    \"insight_level\": \"medium\",\n    \"analysis\": \"The section provides an analytical overview of label ensemble methods by connecting ideas from multiple papers, particularly linking self-ensembling and label filtering strategies. It integrates concepts such as temporal ensembling and self-ensemble label filtering into a coherent narrative but does so in a somewhat surface-level way without deeper th"}}
{"id": "6c23077d-879e-4355-a4b2-e8036af47caf", "title": "Optimization Policy", "level": "section", "subsections": ["f18991bb-072a-4b3f-9199-093293d6c577", "8c053acd-540b-4e8b-876b-8eead263245c", "8a81ffb3-1ebf-45d8-bab8-ba236653dd4f", "fc0f77d5-54e3-4ca1-8bd4-c3e6e527e054"], "parent_id": "8584763c-0388-415b-b9cf-b52c55e5eeb8", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Optimization Policy"]], "content": "\\label{sec:opt}\nMethods in this section solve the LNRL problem by changing optimization policies, such as early stopping. The effectiveness of early stopping is due to memorization effects of deep neural networks, which avoid overfitting noisy labels to some degree. To combat with noisy labels using memorization effects, there exists another and possibly better way, namely small-loss tricks.\nThe structure of this section is arranged as follows. First, we explain what \nmemorization effects are and why this phenomenon is important. Then, we introduce\nseveral common ways to leverage  memorization effects for combating label\nnoise. The first common way is to self-train a single network robustly via\nsmall-loss tricks, which brings us MentorNet~ and Learning to\nReweight~. Furthermore, the second common way is to co-train two\nnetworks robustly via small-loss tricks, which brings us Co-teaching~ and Co-teaching+~. Lastly, there are several ways to\nfurther improve the performance of Co-teaching, such as by using cross-validation~,\nautomated learning~ and   Gaussian mixture model~.", "cites": [3340, 4140, 4126, 4253, 4128], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes several LNRL methods around the concept of optimization policies and memorization effects, connecting ideas from multiple papers (e.g., Co-teaching, MentorNet, Learning to Reweight). While it introduces a coherent structure and highlights key strategies, the critical analysis is limited to pointing out the convergence issue in Co-teaching without deeper evaluation of trade-offs or limitations. The abstraction level is moderate, as it identifies a general principle (memorization) but stops short of offering meta-level insights."}}
{"id": "f18991bb-072a-4b3f-9199-093293d6c577", "title": "Memorization Effects", "level": "subsection", "subsections": [], "parent_id": "6c23077d-879e-4355-a4b2-e8036af47caf", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Optimization Policy"], ["subsection", "Memorization Effects"]], "content": "Arpit et al.~ introduced a very critical work called ``A\ncloser look at memorization in deep networks'', which shapes a new direction\ntowards solving LNRL. In general, memorization effects can be defined as the\nbehavior exhibited by deep networks trained on noise data. Specifically, deep\nnetworks tend to memorize and fit easy (clean) patterns, and gradually over-fit\nhard (noisy) patterns~.  Here, we empirically reproduce a simulated experiment to justify this hypothesis, and experimental details can be found in Appendix 3.\n\\begin{figure}\n\\begin{center}\n\\centerline{\\includegraphics[width=0.35\\textwidth]{noise_mnist.pdf}}\n\\vspace{-10px}\n\\caption{A simulated experiment based on different noise rates ($0\\%$-$80\\%$). We chose \\textit{MNIST} with uniform noise as noisy data. The solid lines denote the training accuracy; while the dotted lines mean the validation accuracy.}\n\\label{sim-exp}\n\\end{center}\n\\end{figure}\nIn Fig.~\\ref{sim-exp}, we used the MNIST dataset, and added random noise on its labels. \nThe noise rate was chosen from the range between $0\\%$ and $80\\%$. We trained our deep networks on the corrupted training data. \nThen, we tested the trained networks on both (noisy) training data and\n(clean) validation data. \nWe can clearly see two phenomena in the graph:\n\\begin{itemize}[leftmargin=*]\n    \\item The training curve will  reach or approximate 100\\% accuracy eventually. All curves will converge the same.\n    \\item The validation curve will first reach a very high accuracy in \n    the first few epochs, \n    but drop gradually until convergence (after 40 epochs).\n\\end{itemize}\nSince deep networks tend to memorize and fit easy\n(clean) patterns in the corrupted data, the validation curve will first reach a peak.\nHowever, such overparameterized models will gradually over-fit hard (noisy)\npatterns. The validation curve will drop gradually, since the validation data is\nclean. This simple experiment not only justifies the hypothesis of memorization\neffects, but also opens a new door to the LNRL problem, namely small-loss\ntricks~.\nSpecifically, small-loss tricks mean deep networks regard small-loss examples as ``clean'' examples, \nand only back-propagate such examples to update the model parameters. \nMathematically, small-loss tricks are equivalent to constructing the restricted $\\tilde{\\ell}$,\nwhere $\\tilde{\\ell} = \\text{sort}(\\ell,1-\\tau)$. Namely, $\\tilde{\\ell}$ can be constructed by sorting $\\ell$ from small to large, and fetching $1-\\tau$ percentage of small loss ($\\tau$ is noise rate).", "cites": [4115], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the core idea from Arpit et al. regarding memorization in deep networks and links it to the LNRL problem, showing some integration. It provides an empirical reproduction to support the concept, but lacks deeper comparative or critical analysis of alternative viewpoints or limitations. The section abstracts the idea into the broader concept of small-loss tricks and their theoretical basis, suggesting a moderate level of abstraction."}}
{"id": "5c3eff96-4510-4c44-b427-200919a4f9f3", "title": "Learning to Reweight", "level": "subsubsection", "subsections": [], "parent_id": "8c053acd-540b-4e8b-876b-8eead263245c", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Optimization Policy"], ["subsection", "Self-training"], ["subsubsection", "Learning to Reweight"]], "content": "Ren et al.~ employed a meta-learning framework to assign different weights to the training examples based on their gradient directions. In general,  small-loss examples are assigned  more weights, since small-loss examples are more likely to be clean. In general, they believed that the best example weighting should minimize the loss of a set of unbiased clean validation examples. Namely, they performed validation at every training iteration to dynamically determine the example weights of the current batch. Mathematically, they hoped to learn a reweighting of the inputs via minimizing a weighted loss:\n\\begin{equation}\n\\theta^*(w) = \\arg\\min_{\\theta}\n\\sum\\nolimits_{i=1}^N w_i \\ell_i(\\theta),\n\\end{equation}\nwhere the training loss $\\ell_i$ associates with a training set\n$\\{(x_i,y_i)\\}_{i=1}^N$. Note that $w_i$ can be viewed as training hyperparameters, and the optimal selection of $w$ is based on its validation performance: \n\\begin{equation}\nw^* = \\arg\\min_{w}\\nicefrac{1}{M}\n\\sum\\nolimits_{i=1}^M \\ell_i^v(\\theta^*(w)),\n\\end{equation}\nwhere the validation loss $\\ell_i^v$ associates with a small validation set $\\{(x_i^v,y_i^v)\\}_{i=1}^M$. To realize ``Learning to Reweight'', there are three technical steps. First, they fed forward and backward noisy training examples via the training loss, which updates the model parameter $\\theta$ and calculates $\\nabla\\theta$. Second, the $\\nabla\\theta$ affects the validation networks, where they fed forward and backward clean validation examples via the validation loss. Lastly, training networks leverage meta-learning to update example weights $w$ via backward on backward (i.e., taking a second-order gradient for the example weights). Note that the same strategy can also be  used for class-imbalance problems, in which big-loss tricks are preferred, since they are more likely to be the minority class~.", "cites": [4128], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the key idea of Ren et al.'s method by explaining its technical components and purpose, integrating it into the broader theme of self-training under label noise. It offers some abstraction by noting the method's applicability to class-imbalance problems, though the analysis remains focused on a single paper. Critical evaluation is limited, with only a brief mention of contrasting 'big-loss tricks' for minority classes but no in-depth critique or comparison of limitations."}}
{"id": "e0afb962-b6c7-4f0b-95c9-0c2571634804", "title": "Co-teaching/Co-teaching+", "level": "subsubsection", "subsections": [], "parent_id": "8a81ffb3-1ebf-45d8-bab8-ba236653dd4f", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Optimization Policy"], ["subsection", "Co-training"], ["subsubsection", "Co-teaching/Co-teaching+"]], "content": "Han et al.~ proposed a new deep learning paradigm called ``Co-teaching''. In general, instead of training a single deep network, they trained two deep networks simultaneously, and let them teach each other given every mini-batch. \nSpecifically, each network feeds forward all data and selects some data with possibly clean labels; \nthen, two networks communicate with each other what data in this mini-batch should be used for training; \nlastly, each network back-propagates the data selected by its peer network and updates itself. The selection criterion is still based on the small-loss trick.\nIn MentorNet~, the error from one network will be directly transferred back to itself in the second mini-batch of data, and the error should be increasingly accumulated. However, in Co-teaching, since two networks have different learning abilities, they can filter different types of error introduced by noisy labels. Namely, in this exchange procedure, the error flows can be reduced by peer networks mutually. However, with the increase of training epochs, two networks will converge to a consensus and Co-teaching will reduce to the self-training MentorNet in function. Note that the principle of ensemble learning is to keep different classifiers diverged~.\nYu et al.~ introduced the ``update by disagreement''~ strategy to keep Co-teaching diverged and named their method Co-teaching+. \nIn general, Co-teaching+ consists of the disagreement-update step and cross-update step. In the disagreement-update step, two networks feed forward and predict all data first, and only keep prediction-disagreed data. This step indeed keeps two networks diverged. The cross-update step has been explored in Co-teaching. Note that both Co-teaching and Co-teaching+ share the same dropping rate for big-loss examples, which was hand-designed. Via both methods, we summarize three key factors in this line research: (1) using the small-loss trick; (2) cross-updating parameters of two networks; and (3) keeping two networks diverged.", "cites": [3340, 4140, 4137], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes insights from multiple papers (Co-teaching, Co-teaching+, and MentorNet) and connects them through a coherent narrative about how dual-network training can reduce error flow and combat label noise. It also critically analyzes the convergence issue in Co-teaching and the proposed solution in Co-teaching+, while abstracting key factors like the small-loss trick, cross-updating, and network divergence. This contributes to a broader understanding of the design principles in LNRL."}}
{"id": "cbc62583-0a86-4ce8-b601-1429d5d61454", "title": "Beyond Co-teaching", "level": "subsubsection", "subsections": [], "parent_id": "8a81ffb3-1ebf-45d8-bab8-ba236653dd4f", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Optimization Policy"], ["subsection", "Co-training"], ["subsubsection", "Beyond Co-teaching"]], "content": "After 2018, \nthere are several important works to further improve Co-teaching. \nHere, we specify two representative works based on Co-teaching and go beyond. \n\\begin{itemize}[leftmargin=*]\n\\item \nChen et al.~ used cross-validation to randomly split noisy datasets, which identifies most samples that have correct labels. In general, they designed the Iterative Noisy Cross-Validation (INCV) method to select a subset of samples, which has a much smaller noise ratio than the original dataset. Then, they leveraged Co-teaching for further training over a selected subset. Apart from selecting clean samples, INCV removes samples that have large losses at each iteration.\n\\item \nYao et al.  used automated machine learning (AutoML) \n\nto explore the memorization effect thus improve Co-teaching. \nIt is noted that both Co-teaching and Co-teaching+ share the same dropping rate for big-loss examples, \nwhich was hand-designed. However, such a rate is critical in training deep networks. \nSpecifically, Yao et al.~ designed a domain-specific search and proposed\na novel Newton algorithm to solve the bi-level optimization problem efficiently. \nTo explore the optimal rate $R(\\cdot)$, they formulated the problem as\n\\begin{align*}\nR^* & = \\arg\\min\\nolimits_{R(\\cdot) \\in \\mathcal{F}} \\mathcal{L}_{\\text{val}}(f(w^*;R),\\mathcal{D}_{\\text{val}})\n\\\\\n\\text{s.t.} \\;\n& w^* = \\arg\\min\\nolimits_{w} \\mathcal{L}_{\\text{tr}}(f(w^*;R),\\mathcal{D}_{\\text{tr}}),\n\\end{align*}\nwhere $\\mathcal{F}$ is the search space of $R(\\cdot)$ exploring a general pattern of the memorization effect. \nSuch a prior on $\\mathcal{F}$ not only allows \nefficient bi-level optimization\nbut also boosts the final learning performance. \n\\item\nMotivated by MixMatch,\nLi et al.~ promoted a novel framework termed DivideMix by leveraging semi-supervised learning techniques. \nIn high-level, DivideMix used a Gaussian Mixture Model (GMM) to dynamically\ndivides the training data into two parts. The first part includes labeled data\nwith clean labels; while the second part includes unlabeled data with noisy\nlabels. During the semi-supervised learning phase, they leveraged variants of\nCo-training, such as Co-refinement~ on labeled data and\nCo-guessing~ on unlabeled data. Specifically, once the data is divided into labeled and unlabeled data, they conducted Co-refinement for labeled data, which linearly combines the ground-truth label with the network's prediction and sharpens the refined label. Then they conducted Co-guessing for unlabeled data, which averages the predictions from both networks. After Co-refinement and Co-guessing, they followed the routine MixMatch to mix the data, and updated the model parameters.\n\\end{itemize}", "cites": [4126, 7778, 4253], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates multiple works (Chen et al., Yao et al., Li et al.) by connecting their contributions to the broader theme of improving Co-teaching with noisy labels. It explains how each paper builds upon or diverges from previous approaches, providing a synthesis of ideas. While there is some critical evaluation of design choices (e.g., hand-designed dropping rates), it does not extensively compare or critique the methods in depth. The section also identifies a pattern of using semi-supervised learning and optimization refinements, showing moderate abstraction."}}
{"id": "b105c0e1-9d65-4188-8e44-a44d89df5e64", "title": "Pre-training", "level": "subsubsection", "subsections": [], "parent_id": "fc0f77d5-54e3-4ca1-8bd4-c3e6e527e054", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Optimization Policy"], ["subsection", "Beyond Memorization"], ["subsubsection", "Pre-training"]], "content": "In many CV and NLP applications, the pre-training paradigm has become a commonplace, especially when data is scarce in the target domain. \nHendrycks et al.~ demonstrated that pre-training can improve model robustness and uncertainty, \nincluding adversarial robustness and label corruption.\nNormally, pre-training is carried out on a bigger dataset first, \nand fine-tuning is then applied to the pre-trained model on a smaller target dataset. \nFor example, if we design an LNRL method for image classification with label noise, we pre-train a model on ImageNet via an LNRL method first. \nThen, we fine-tune the pre-trained model on the target dataset via an LNRL method. \nNote that the pre-training approach has been demonstrated in many robustness and uncertainty scenarios~,\nincluding label noise, adversarial examples, class imbalance, out-of-distribution detection and calibration.", "cites": [7133, 7707], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the role of pre-training in improving robustness and uncertainty, integrating findings from multiple domains like adversarial robustness and OOD detection. It provides a general abstraction by showing how pre-training fits into broader robust learning paradigms but offers limited critical analysis of the cited works' assumptions or limitations."}}
{"id": "b55f4f67-0597-4eec-b1b4-790fddae9810", "title": "Deep k-NN", "level": "subsubsection", "subsections": [], "parent_id": "fc0f77d5-54e3-4ca1-8bd4-c3e6e527e054", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Optimization Policy"], ["subsection", "Beyond Memorization"], ["subsubsection", "Deep k-NN"]], "content": "Bahri et al.~ proposed the deep k-NN method, which works on an intermediate layer \nof a preliminary deep model $\\mathcal{M}$ to filter mislabeled training data. In\nhigh-level, deep k-NN filtering consists of two steps. In the first step, a model\n$\\mathcal{M}$ with architecture $\\mathcal{A}$ is trained \nto filter noisy data $\\mathcal{D}_{\\text{noisy}}$ via the k-NN algorithm, which identifies and removes examples whose labels disagree with their neighbors. After filtering $\\mathcal{D}_{\\text{noisy}}$, \nin the second step, a final model with architecture $\\mathcal{A}$ is re-trained on $\\mathcal{D}_{\\text{filtered}} \\cup \\mathcal{D}_{\\text{clean}}$.", "cites": [4467], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the deep k-NN method and its two-step process as outlined in the cited paper. It lacks synthesis with other works in the field and does not offer comparative or critical analysis of the method's advantages or limitations. The abstraction level is minimal, focusing only on a specific technique without broader conceptual generalization."}}
{"id": "1624c670-ba44-403d-9ff0-b3cd971304c6", "title": "Build up New Datasets", "level": "subsection", "subsections": [], "parent_id": "e9c1d328-7cdf-466e-af77-cbf803cadee3", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Future Works"], ["subsection", "Build up New Datasets"]], "content": "In LNRL, the first thing we should do is to construct new datasets with real noise, which is critical to the rapid development of LNRL. To our best knowledge, \nmost  researchers test their LNRL methods on simulated datasets, \nsuch as \\textit{MNIST} and \\textit{CIFAR-10}. To make further breakthroughs, we should build  new datasets with real noise, \nsuch as \\textit{Clothing1M} .\nNote that, \nsimilar to \\textit{ImageNet}, \nmany researchers train deep networks to overfit \\textit{Clothing1M} via different tricks, \nwhich may not touch the core issues of LNRL.\nThis motivates us to rethink real datasets in LNRL. 5 years after the birth of Clothing1M, Jiang et al.~ proposed a new but realistic type of noisy dataset called ``web-label noise'' (or red noise), which enables us to conduct controlled experiments systematically in realistic scenarios. Another interesting point is that benchmark datasets with real noise mainly focus on image classification, instead of natural language, speech processing, and various data collected from real sensors. Obviously, these directions also involve label noise, which need to be addressed further.\nTo sum up, we should build  new benchmark datasets with \\textit{real noise}, not only for images but also for language, speech, and various sensor data. Normally, the better datasets can boost  rapid development of LNRL.", "cites": [4456], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from a single cited paper (Jiang et al.) and contextualizes it within the broader challenge of real-world label noise in LNRL. It critically notes that existing datasets like Clothing1M are often overfitted using tricks rather than addressing core issues, and highlights the lack of real-noise datasets in non-image domains. While it identifies a trend and suggests a direction for future work, the analysis remains somewhat surface-level and does not offer a novel framework or deep evaluative critique."}}
{"id": "c8b26ed4-c11f-4e4d-bd30-63ea4b72881f", "title": "Instance-dependent LNRL", "level": "subsection", "subsections": [], "parent_id": "e9c1d328-7cdf-466e-af77-cbf803cadee3", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Future Works"], ["subsection", "Instance-dependent LNRL"]], "content": "\\label{sec:fu:idlnrl}\nPreviously in Section~\\ref{sec:thm:idata},\nwe have seen that CCN is a popular assumption in LNRL.\nHowever, the CCN model is only an approximation to the real-world noise, which may not always work well in practice.\nTo directly model  real-world noise, we should consider  features in the label corruption process. \nThis motivates us to explore the instance-dependent noise (IDN) model, which is formulated as $p(\\bar{Y}|Y,X)$~. \nSpecifically, the IDN model considers a more general noise, in which the probability that an instance is mislabeled depends on both its class and features. Intuitively, this noise is quite realistic, as poor-quality or ambiguous instances are more likely to be mislabeled in real-world datasets. However, it is much more complex to formulate the IDN model, since the probability of a mislabeled instance is a function of not only the label space but also the input space that can be very high-dimensional. Moreover, without some extra assumptions/information, IDN is unidentifiable~.\nTowards the IDN model, there are several seminal explorations. For instance, Menon et al.~ proposed the boundary-consistent noise model, which considers stronger noise for samples closer to the decision boundary of the Bayes optimal classifier. However, this model is restricted to binary classification and cannot estimate noise functions. Cheng et al.~ recently studied a particular case of the IDN model, in which the noise functions are upper-bounded. Nonetheless, their method is limited to binary classification and has only been tested on small datasets. Berthon et al.~ proposed to tackle the IDN model from the new perspective, by considering confidence scores to be available for the label of each instance. They term this new setting confidence-scored IDN (CSIDN). Based on CSIDN, they derived an instance-level forward correction algorithm. More recently, Xia et al.~, Cheng et al.~ and Zhu et al.~ explored this direction in depth.", "cites": [4469, 4120, 4470, 4468, 4465, 7823], "cite_extract_rate": 1.0, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key contributions from multiple papers on instance-dependent label noise, integrating their approaches into a narrative about modeling more realistic noise. It identifies limitations such as the unidentifiability of IDN and the reliance on strong assumptions or binary classification, showing some critical perspective. The discussion abstracts from specific methods to highlight the general challenges of modeling IDN, but the insights remain within the scope of existing frameworks without offering a novel or meta-level synthesis."}}
{"id": "814911f7-6a07-43b3-b969-435dacd6bc5a", "title": "Adversarial LNRL", "level": "subsection", "subsections": [], "parent_id": "e9c1d328-7cdf-466e-af77-cbf803cadee3", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Future Works"], ["subsection", "Adversarial LNRL"]], "content": "When discussing robustness, \none may think about adversarial robustness~. \nHowever, adversarial robustness is obtained via adversarial training, in which the features are adversarially perturbed while the labels are clean. \nIs this the optimal way to formulate the adversarial robustness? In other words, would it be more useful to consider the scenario in which the features are adversarially perturbed while the labels are noisy? We term this adversarial LNRL.\nTowards adversarial LNRL, there are two seminal works. For example, Wang et al.~ proposed a new defense algorithm called misclassification aware adversarial training (MART), which explicitly differentiates the misclassified examples (i.e., label noise) and correctly classified examples during  training. To address this issue, MART introduces misclassification aware regularization, namely $1/n\\sum_{i=1}^{n}\\mathbf{1}(h_{\\theta}(x_i)\\neq h_{\\theta}(\\hat{x}'_i))\\cdot\\mathbf{1}(h_{\\theta}(x_i)\\neq y_i)$, where $h_{\\theta}$ denotes a DNN classifer with model parameter $\\theta$, $(x_i, y_i) $ denotes the $i$th pair of features and label, and $\\hat{x}'_i$ denotes the adversarial example generated by (5) in . Intuitively, $\\mathbf{1}(h_{\\theta}(x_i)\\neq y_i)$ denotes the misclassified examples, which can be closely connected with label noise. Meanwhile, Zhang et al.~ considered the same issue, namely misclassified examples in adversarial training. Specifically, they proposed friendly adversarial training (FAT), which trains deep networks using the wrongly-predicted adversarial data minimizing the loss and the correctly-predicted adversarial data maximizing the loss. To realize FAT, they introduced early-stopped PGD. Namely, once the adversarial data is misclassified by the current model, they  stopped the PGD iterations early. In high-level, the objective of FAT is min-min optimization instead of min-max optimization in standard adversarial training.", "cites": [4471, 892], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes two seminal works on adversarial training with label noise, connecting their core idea of addressing misclassified examples during training. It provides some critical perspective by questioning the traditional formulation of adversarial robustness and introducing the concept of adversarial LNRL. The abstraction level is moderate, as it identifies a broader scenario (features and labels both being adversarially affected) but does not fully articulate overarching principles or theoretical implications."}}
{"id": "2cf80ce6-cd98-479c-9b73-9939550e2a16", "title": "Beyond Labels: Noisy Data", "level": "subsection", "subsections": [], "parent_id": "e9c1d328-7cdf-466e-af77-cbf803cadee3", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Future Works"], ["subsection", "Beyond Labels: Noisy Data"]], "content": "We have envisioned three promising directions above, which belong to the vertical domain in LNRL. However, we hope to explore the horizontal domain more, namely, noisy data instead of only noisy labels. Here, we summarize different formats of noisy data, and show some preliminary works.\n\\begin{itemize}[leftmargin=*]\n\\item \n\\textit{Feature}:\nNaturally, label noise can arouse us to consider feature noise, where the adversarial example is one of the special cases in feature noise~. The problem of feature noise is formulated as $p(\\bar{X}|Y)$, in which the features are corrupted but the labels are intact.\nTherefore, adversarial training can be the main tool to defend from adversarial examples. Note that there exists another feature noise called random perturbation~. To address this issue, Zhang et al.~ proposed a robust ResNet, which is motivated by dynamical systems. Specifically, they characterized ResNet based on an explicit Euler method. This allows us to exploit the step factor in the Euler method to control the robustness of ResNet. They  proved that a small step factor can benefit its training and generalization robustness during backward and forward propagation. Namely, controlling the step factor robustifies deep networks, which can alleviate feature noise.\n\\item \\textit{Preference}:\nHan et al.~ and Pan et al.~ tried to address preference noise in ranking problems,\nwhich plays an important role in our daily life, such as ordinal peer grading, online image-rating and online product recommendation.\nSpecifically, Han et al.~ proposed the ROPAL model, which integrates the Plackett-Luce model with a denoising vector. Based on the Kendall-tau distance, this vector corrects $k$-ary noisy preferences with a certain probability. However, ROPAL cannot handle the dynamic length of $k$-ary noisy preferences, which motivated Pan et al.~ to propose COUPLE, which leverages stagewise learning to break the limit of fixed length. To update the parameters of both models, they used online Bayesian inference.\n\\item \\textit{Domain}:\nDomain adaptation (DA) is one of the fundamental problems in machine learning, when the data volume in the target domain is scarce. Previous DA methods assume that labeled data in the source domain is  clean. However, in practice, labeled data in the source domain may come from amateur annotators or the Internet due to its large volume. This issue brings us a new setting, where labels in the source domain are noisy. We call this setting \\textit{wild domain adaptation (WDA)}.\nThere are two seminal works. Specifically, to handle WDA, Liu et al.~ proposed the Butterfly framework, \nwhich maintains four deep networks simultaneously. \nButterfly can obtain high-quality domain-invariant representations (DIR) and target-specific representations (TSR) in an iterative manner.\nMeanwhile, Yu et al.~ proposed a novel Denoising Conditional Invariant Component (DCIC) framework, \nwhich provably ensures extracting invariant representations and estimating the label distribution in the target domain with no bias.\n\\item \\textit{Similarity}:\nSimilarity-based learning~ is one of the emerging weakly-supervised problems, where similar data pairs (i.e., two examples belonging to the same class) and unlabeled data are available. Compared to class labels, similarity labels are easier to obtain, especially for some sensitive issues, e.g., religion and politics. For example, for sensitive matters, people often hesitate to directly answer ``What is your opinion on issue A?''; while they are more likely to answer ``With whom do you share the same opinion on issue A?''. Therefore, similarity labels are easier to obtain. However, for some cases, people may not be willing to provide their real thoughts even facing easy questions. Therefore, noisy-similarity-labeled data are very challenging. Wu et al.~ employed a noise transition matrix to model similarity noise, which has been integrated into a deep learning system.\n\\item \\textit{Graph}:\nGraph neural networks (GNNs) are very hot in the machine learning community~. \nHowever, are GNNs robust to noise? For example, once the node or edge is corrupted, the performance of GNNs will certainly deteriorate. Since GNNs are highly related to discrete and combinatorial optimization, LNRL methods cannot be directly deployed. Therefore, it is very meaningful to robustify GNNs under the node or edge noise, in which the noise can occur in labels and features. Recently, Wang et al.~ proposed a robust and unsupervised embedding framework called Cross-Graph, which can handle structural corruption in attributed graphs. Meanwhile, since Hendrycks et al.~ discovered that pre-training can improve  model robustness and uncertainty, we may leverage strategies for pre-training GNNs~ to overcome the issues of graph noise.\n\\item \\textit{Demonstration}:\nThe goal of imitation learning (IL) is to learn a good policy from high-quality demonstrations~. However, the quality of demonstrations in reality should be diverse, since it is easier and cheaper to collect demonstrations from a mix of experts and amateurs. This brings us a new setting of IL called diverse-quality demonstrations, where low-quality demonstrations are highly noisy~. When experts provide additional information about the quality, learning from diverse-quality demonstrations becomes relatively easy, since the quality can be estimated by their confidence scores~, ranking scores~ and a small number of high-quality demonstrations~. However, without the availability of experts, these methods may not work well. Recently, Tangkaratt et al.~ pushed forward this line, and proposed to model the quality with a probabilistic graphic model termed VILD. Specifically, they estimated the quality along with a reward function that represents an intention of experts' decision making. Moreover, they used a variational approach to handle large state-action spaces, and employed importance sampling to improve  data efficiency.\n\\end{itemize}", "cites": [7133, 892, 7824, 4473, 4475, 8795, 7007, 4474, 7335, 4472, 7825, 8696], "cite_extract_rate": 0.6, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by grouping related works under distinct types of noisy data (feature, preference, domain, similarity, graph, demonstration), and connecting them to broader LNRL challenges. It also offers critical analysis by pointing out limitations of specific models (e.g., ROPALâ€™s inability to handle dynamic preference length). The abstraction level is high, as it generalizes these works into a unified perspective of extending LNRL beyond label noise to other data imperfections, suggesting new research directions."}}
{"id": "cdd67a8f-6488-47f6-8cb3-9d172523c445", "title": "Early Stage", "level": "subsection", "subsections": [], "parent_id": "8583ac9c-4327-4ed2-a97f-e59cc1711cc5", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Appendix 1: Related Literature"], ["subsection", "Early Stage"]], "content": "Before delving into label-noise representation learning, we should briefly overview some of milestone works in label-noise statistical learning. Starting from 1988, Angluin et al. proved that a learning algorithm can handle incorrect training examples robustly, when the noise rate is less than one half under the random noise model~. Bylander further demonstrated that linear threshold functions are polynomially learnable in the presence of classification noise~. Lawrence and Sch\\\"olkopf constructed a kernel Fisher discriminant to formulate label-noise problem as a probabilistic model~, which is solved by Expectation Maximization algorithm. Although the above works explored to tackle noisy labels theoretically and empirically, Bartlett et al. justified that most loss functions are not completely robust to label noise~. It means that classifiers based on label-noise robust algorithms are still affected by label noise.\nDuring this period, a lot of works emerged and contributed to this area. For example, Crammer et al. proposed an online Passive-Aggressive perceptron algorithm to cope with label noise~. Dredze et al. proposed confidence weighted learning to weigh trusted labels more~. Freund proposed a boosting algorithm to combat against random label noise~. To handle label noise theoretically, Cesa-Bianchi et al. proposed an online learning algorithm, leveraging unbiased estimates of the gradient of the loss~. Until 2013, Natarajan et al. formally formulated an unbiased risk estimator for binary classification with noisy labels~. This work is very important to the area, since it is the first work to provide guarantees for risk minimization under random label noise. Moreover, this work provides an easy way to suitably modify any given surrogate loss function for handling label noise.\nMeanwhile, Scott et al. studied the classification problem under class-conditional noise model, and propose the way to handle asymmetric label noise~. In contrast, van Rooyen et al. proposed the unhinge loss to tackle symmetric label noise~. Liu and Tao proposed the method of anchor points to estimate the noise rate, and further leverage importance reweighting to design surrogate loss functions for class-conditional label noise~. Instead of designing ad-hoc losses, Patrini et al. introduced linear-odd losses, which can be factorized into an even and an odd loss function~. More importantly, they estimated the mean operator from noisy data, and plug this operator in linear-odd losses for empirical risk minimization, which is resistant to asymmetric label noise.\nIt is noted that, we move from label-noise statistical learning to label-noise representation learning after 2015. There are two reasons behind this phenomenon. First, label-noise statistical learning mainly focus on designing theoretically-robust methods for small-scale noisy data. However, such methods cannot empirically work well on large-scale noisy data in our daily life, such as Clothing1M~ emerging from 2015. Second, label-noise statistical learning mainly applies to shallow and convex models, such as support vector machines. However, deep and non-convex models, such as convolutional and recurrent neural networks, have become trendy and mainstream due to the better empirical performance, not only in vision, but also in language, speech and video tasks. Therefore, it is urgent to design label-noise representation learning methods for robustly training of deep models with noisy labels.", "cites": [3454, 8734, 4119, 8796, 3453], "cite_extract_rate": 0.35714285714285715, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes a range of foundational works on label-noise statistical learning, connecting theoretical and empirical contributions into a coherent narrative. It provides critical analysis by identifying limitations of earlier methods, such as their inapplicability to large-scale and deep learning contexts. The section also abstracts key trends, such as the shift from shallow to deep models and the need for more robust loss functions, offering broader insights into the evolution of the field."}}
{"id": "96627aa8-43f4-4b39-9b47-65f4980e189a", "title": "Emerging Stage", "level": "subsection", "subsections": [], "parent_id": "8583ac9c-4327-4ed2-a97f-e59cc1711cc5", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Appendix 1: Related Literature"], ["subsection", "Emerging Stage"]], "content": "There are three seminal works in label-noise representation learning with noisy labels. For example, Sukhbaatar et al. introduced an extra but constrained linear ``noise'' layer on top of the softmax layer, which adapts the network outputs to model the noisy label distribution~. Reed et al. augmented the prediction objective with a notion of consistency via a soft and hard bootstrapping~, where the soft version is equivalent to softmax regression with minimum entropy regularization and the hard version modifies regression targets using the MAP estimation. Intuitively, this bootstrapping procedure provides the learner to disagree with an inconsistent training label, and re-label the training data to improve its label quality. Azadi et al. proposed an auxiliary image regularization technique~. The key idea is to exploit the mutual context information among training data, and encourage the model to select reliable labels.\nFollowed by seminal works, Goldberger et al. introduced a nonlinear ``noise'' adaptation layer on top of the softmax layer~, which adapts to model the noisy label distribution. Patrini et al. proposed forward and backward loss correction approaches simultaneously~. Based on the corrected loss, they explored a robust two-stage training algorithm. A very interesting point is, both Wang et al. and Ren et al. leveraged the same philosophy, namely data reweighting, to learn with label noise. However, they tackled from different perspectives. Specifically, Wang et al. come from a view of Bayesian and propose robust probabilistic modeling~, where the posterior of reweighted model will identify uncorrupted data but ignore corrupted data. Ren et al. come from a view of meta-learning~, which assigns weights to training samples based on their gradient directions. Namely, their method performs a meta gradient descent step on the current mini-batch example weights (initialized from zero) to minimize the loss on a clean unbiased validation set.\nBesides the above works, there are many important works born in 2018, ranging in diverse directions. In high-level, there are several major directions, such as estimating transition matrix, regularization, designing losses and small-loss tricks. Among them, small-loss tricks are inspired by memorization effects of deep neural networks, where deep models will fit easy (clean) patterns first but over-fit hard (noisy) patterns eventually. Namely, small-loss tricks regard small-loss samples as relatively ``clean'' samples, and back-propagate such samples to update the model parameters. For example, Jiang et al. is the first to leverage small-loss tricks to handle label noise~. However, they train only a single network iteratively, which is similar to the self-training approach. Such approach inherits the same inferiority of accumulated error caused by the sample-selection bias. To address this issue, Han et al. train two deep neural networks simultaneously, and back propagates the data selected by its peer network and updates itself~.\nIn the context of representation learning, estimating transition matrix, regularization and designing losses are still prosperous for handling label noise. For instance, given that a small set of trusted examples are available, Hendrycks et al. proposed gold loss correction. Namely, they leveraged trusted examples to estimate the (gold) transition matrix perfectly~. Therefore, on noisy examples, they will train deep models via forward correction (by gold matrix); on trusted examples, they will train deep models normally. Han et al. proposed ``human-in-the-loop'' idea to easily estimate the transition matrix~. Specifically, they proposed a human-assisted approach called ``Masking'' that conveys human cognition of invalid class transitions and naturally speculates the structure of the noise transition matrix. Then, they regarded the matrix structure as prior knowledge, which is further incorporated into deep probabilistic modeling.\nMoreover, Zhang et al. introduced an implicit regularization called mixup~, which constructs virtual training data by linear interpolations of features and labels in training data. Mixup encourages the model to behave linearly in-between training examples, which reduces the amount of undesirable oscillations when predicting outside the training examples. Zhang et al. generalized both categorical cross entropy loss and mean absoulte error loss by the negative Box-Cox transformation~. Their proposed $\\mathcal{L}_q$ loss not only has theoretical justification, but also work for both closed-set and open-set noisy labels. Motivated by a dimensionality perspective, Ma [2018] developed a dimensionality-driven learning strategy, which can effectively learn robust low-dimensional subspaces that capture the true data distribution.", "cites": [3340, 4139, 4130, 4183, 4145, 4455, 7773, 4128, 7191, 4136], "cite_extract_rate": 0.7692307692307693, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by organizing early works into themes like noise adaptation, bootstrapping, and regularization, and connecting them to broader principles. It offers critical analysis by pointing out limitations such as sample-selection bias in self-training and the practical challenges of estimating transition matrices. The abstraction is evident in identifying overarching strategies (e.g., data reweighting, small-loss tricks, human-in-the-loop) and framing them as general principles for robust LNRL."}}
{"id": "09b509d2-df66-4c35-832d-b059e7d4d74d", "title": "Flourished Stage", "level": "subsection", "subsections": [], "parent_id": "8583ac9c-4327-4ed2-a97f-e59cc1711cc5", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Appendix 1: Related Literature"], ["subsection", "Flourished Stage"]], "content": "Starting from 2019, label-noise representation learning has become mature in the top conference venues. Arazo et al. formulated clean and noisy samples as a two-component (clean-noisy) beta mixture model on the loss values~, where the posterior probabilities are then used to implement a dynamically weighted bootstrapping loss. To boost the performance of Co-teaching, Chen et al. introduced the Iterative Noisy Cross-Validation (INCV) method to select a subset of most confident samples (with correct labels)~, while Yu et al. employed the ``Update by Disagreement'' strategy to keep two networks diverged~. Hendrycks et al. empirically demonstrated that pre-training (i.e., ``pre-train then tune'' paradigm) can improve model robustness against label corruption~, which is for large-scale noisy datasets.\nUnder the criteria of balanced error rate (BER) minimization and area under curve (AUC) maximization, Charoenphakdee et al. found that symmetric losses have many merits in combating with noisy labels, even without knowing the noise information. Based on such observation, they proposed Barrier Hinge Loss~. In contrast to selected samples via small-loss tricks, Thulasidasan et al. introduced the abstention-based training, which allows deep abstaining networks to abstain on confusing samples while learning on non-confusing samples~. Following the re-weighting strategy, Shu et al. parameterized the weighting function adaptively as one-layer multilayer perceptron called Meta-Weight-Net~, which is free of manually pre-specifying the weighting function.\nEntering 2020, Menon et al. mitigated the effects of label noise from an optimization lens, namely using composite loss-based gradient clipping, which naturally introduces the partially Huberised loss for training deep models~. Nguyen et al. proposed a self-ensemble label filtering method to progressively filter out the wrong labels during training~. Li et al. modeled the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples~. Lyu et al. proposed a provable curriculum loss, which can adaptively select samples for robust stagewise training~. Han et al. proposed a versatile approach called scaled stochastic integrated gradient underweighted ascent (SIGUA)~. SIGUA uses gradient decent on good data, while using scaled stochastic gradient ascent on bad data rather than dropping those data. After Clothing1M born in 5 years, Jiang et al. proposed a new but realistic type of noisy dataset called ``web-label noise'' (or red noise)~, which enables us to conduct controlled experiments systematically in more realistic scenario.", "cites": [4456, 4140, 4126, 4135, 4253, 4182, 3342, 7133, 7781, 4457, 7774], "cite_extract_rate": 0.8461538461538461, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section demonstrates analytical insight by discussing how different LNRL methods address label noise, drawing connections between them, such as the use of mixture models or abstention strategies. It also identifies broader trends like the shift from synthetic to realistic noise datasets. However, while it provides some comparisons and highlights methodological advantages (e.g., SIGUA vs. discarding bad data), it lacks deeper critical evaluation of limitations or trade-offs among the approaches."}}
{"id": "69589617-d16d-41a6-a4de-6a9c30467048", "title": "Data Perspective", "level": "subsection", "subsections": [], "parent_id": "e3c66c9a-8067-4068-981d-ba74f39dbf1c", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Appendix 2: Discussion for Three Perspectives"], ["subsection", "Data Perspective"]], "content": "It is a typical method to leverage the noise transition matrix for solving the LNRL problem. First, we can insert an adaptation layer into the original network, and this layer can mimic the function of the noise transition matrix. Second, we may keep the original network, but correct the cross-entropy loss via the estimated transition matrix. Lastly, since the accurate matrix estimation will lead to the better classification accuracy, we can use the prior knowledge to ease the estimation burden.\nNote that there are other related works from the \ndata perspective. For example,\nstructured noise modeling \ndemonstrated that the noise in human-centric annotations exhibits structure, which can be modeled by decoupling the human bias from the correct visually grounded label~;\nnoisy fine-grained recognition showed the potential to train effective models of fine-grained recognition using noisy data from the web only~;\ndistillation with side information built a unified distillation framework to use ``side'' information, including a small clean dataset and label relations in a knowledge graph, to combat noisy labels~;\nrank pruning addressed the fundamental problem of estimating the noise rates~;\nnegative learning trained deep networks using complementary labels, which decrease the risk of providing incorrect information~; \ncombinatorial inference reduced the noise level by simply constructing meta-classes and improved the accuracy via combinatorial inferences over multiple constituent classifiers~; robust generative adversarial networks (GANs) incorporated a noise transition model, which can learn a clean label conditional generative distribution even when training labels are noisy~;\nnoise-tolerant fairness enabled learning of fair classifiers given noisy sensitive features using the mean-difference score~; and latent class-conditional noise modeled the noise transition in a Bayesian form, namely projecting the noise transition in a Dirichlet-distributed space~.", "cites": [7783, 4116, 4174, 3345, 4177, 4476], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a list of methods from the data perspective, including a brief description of several cited papers, but does not deeply synthesize their findings or connect them into a broader narrative. There is minimal critical analysis of the methods or limitations, and while it hints at patterns (e.g., using prior knowledge, structured noise), it does not generalize these into overarching principles or a meta-level framework."}}
{"id": "3f9b8f1b-7b6c-4e13-a71a-9ae8d923ab7d", "title": "Objective Perspective", "level": "subsection", "subsections": [], "parent_id": "e3c66c9a-8067-4068-981d-ba74f39dbf1c", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Appendix 2: Discussion for Three Perspectives"], ["subsection", "Objective Perspective"]], "content": "Modifying the objective function is another mainstream method to solve the LNRL problem. First, we can augment the objective via either explicit regularizer, e.g., \nMinimum Entropy Regularization~, or \nimplicit regularizer, e.g., \nVirtual Adversarial Training~. Second, instead of treating all sub-objective functions equally, we can leverage the reweighting strategy to assign different weights to sub-objective functions. The more weights we assign, the more importance these sub-objective functions have. We can realize the reweighting strategy via different ways, e.g., importance reweighting, a Bayesian method, a mixture model and neural networks. Lastly, we can modify the objective function via redesigning the loss function, e.g., $\\ell_q$, barrier hinge loss, partial Huberized loss and curriculum loss. Moreover, we can conduct the label ensemble, e.g., the temporal ensembling and self-ensemble filtering.\nNote that there are other related works from the objective perspective. For instance, online crowdsourcing greatly reduces the number of redundant annotations, when crowdsourcing annotations such as bounding boxes, parts, and class labels~; an undirected graphical model represents the relationship between noisy and clean labels, where the inference over latent clean labels is tractable and regularized using auxiliary information~; the active-bias method trains robust deep networks by emphasizing high variance samples~; model bootstrapped EM jointly models labels and worker quality from noisy crowdsourced data~; the joint optimization framework corrects labels during training by alternating update of network parameters and labels~; the iterative learning framework trains deep networks with open-set noisy labels~; deep bilevel learning is based on the principles of cross-validation, where a validation set is used to limit the model over-fitting~; symmetric cross entropy (CE) boosts CE symmetrically with a noise robust counterpart,  Reverse Cross Entropy (RCE)~; the ubiquitous reweighting network learns a robust model from large-scale noisy web data, by considering five key challenges (i.e., imbalanced class sizes, high intra-classes diversity and inter-class similarity, imprecise instances, insufficient representative instances, and ambiguous class labels) in image classification~; the information-theoretic loss is a generalized version of mutual information, which is provably robust to instance-independent label noise~; the peer loss enables learning from noisy labels without requiring a priori specification of the noise rates~; and the normalized loss theoretically demonstrates that a simple normalization can make any loss function robust to noisy labels~.", "cites": [8743, 4166, 7775, 4151, 4131, 4477, 8630, 4184, 139, 4134], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 14, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of various objective-based LNRL methods, listing multiple approaches and techniques with brief mentions of their purposes. It shows minimal synthesis by grouping methods under broad categories like reweighting and loss redesign but does not deeply connect or contextualize them. Critical evaluation is limited, as it does not assess trade-offs or limitations of these methods. Abstraction is weak, with no attempt to generalize or identify overarching principles beyond the surface-level categorization."}}
{"id": "d3cd8bd3-94a7-4a9d-a6ec-5d68717e41d2", "title": "Optimization Perspective", "level": "subsection", "subsections": [], "parent_id": "e3c66c9a-8067-4068-981d-ba74f39dbf1c", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Appendix 2: Discussion for Three Perspectives"], ["subsection", "Optimization Perspective"]], "content": "Leveraging memorization effects is an emerging mainstream method to solve the LNRL problem. First, we can combine self-training with memorization effects, which brings us self-paced MentorNet and learning to reweight. Second, we can combine Co-training with memorization effects, which introduces Co-teaching, Co-teaching+, INCV Co-teaching and S2E. Lastly, we can combine Co-training with the SOTA semi-supervised learning MixMatch, which provides DivideMix. Meanwhile, besides memorization, pre-training and deep k-NN are new branches using overparamterized models.\nNote that there are other related works from the optimization policy perspective, namely changing training dynamics. For example, multi-task networks jointly learn to clean noisy annotations and accurately classify images~; the unified framework of random grouping and attention effectively reduces the negative impact of noisy web image annotations~; decoupling trains two deep networks simultaneously, and only updates parameters on examples, where there is a disagreement between the two classifiers~;\nCleanNet was designed to make label noise\ndetection and learning from noisy data with human supervision scalable through transfer learning~; CurriculumNet designs a training curriculum by measuring and ranking the complexity of data using its distribution in a feature space~; Co-mining combines Co-teaching with the Arcface loss~ for face recognition tasks~; O2U-Net only requires adjusting the hyper-parameters of deep networks to make their status transferred from overfitting to underfitting (O2U) cyclically~; deep self-learning is an iterative learning framework\nto relabel noisy samples and train deep networks on the real\nnoisy dataset, without using extra clean supervision~; the label-noise information strategy is a training method that controls memorization by regularizing label noise information in weights~; different from Co-teaching+, Co-regularization aims to reduce the diversity of two networks during training~; and the data coefficient method wisely takes advantage of a small trusted dataset to optimize exemplar weights and labels of mislabeled data, which distills effective supervision for robust training~.", "cites": [4157, 4178, 4238, 4144, 4186, 309, 4137, 8742, 4188, 4159], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section lists various methods that leverage memorization effects and other training strategies to address label noise, but it does so in a largely descriptive manner. It integrates some ideas (e.g., linking Co-teaching and co-regularization), but the synthesis is minimal and lacks deeper connections or a novel framework. There is little critical evaluation of the methods, and abstraction is limited to only surface-level observations such as the use of pre-training or co-training."}}
{"id": "7536cbcd-658f-4891-8cc0-58dab7f8ffac", "title": "Network Structure", "level": "subsection", "subsections": [], "parent_id": "5aa2deed-36b5-4ac8-be9c-4fe127b1b643", "prefix_titles": [["title", "A Survey of Label-noise Representation Learning: Past, Present and Future"], ["section", "Appendix 3: Experimental Details"], ["subsection", "Network Structure"]], "content": "We follow memorization effects~ to set up network structure as follows: Input $\\rightarrow$ Conv(200,5,5) $\\rightarrow$ BN $\\rightarrow$ ReLU $\\rightarrow$ MaxPool(3,3) $\\rightarrow$ Conv(200,5,5) $\\rightarrow$ BN $\\rightarrow$ ReLU $\\rightarrow$ MaxPool(3,3) $\\rightarrow$ FC(200,384) $\\rightarrow$ BN $\\rightarrow$ ReLU $\\rightarrow$ FC(384,192) $\\rightarrow$ BN $\\rightarrow$ ReLU $\\rightarrow$ FC(192,10) $\\rightarrow$ Softmax.", "cites": [4115], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a purely descriptive account of the network structure used in experiments, citing one paper mainly to justify the setup. There is minimal synthesis of ideas and no critical evaluation or broader abstraction of concepts related to label-noise robustness or network design."}}
