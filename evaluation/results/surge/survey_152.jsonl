{"id": "dcad3f0a-0453-4b35-a06e-a24a28d8d898", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "bab571dd-55e6-4f4b-ac15-157a4152a738", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Introduction"]], "content": "\\begin{table*}[!tp]\n\\begin{center}\n\\small\n \\begin{tabular}{||m{0.2\\textwidth} m{0.49\\textwidth} m{0.23\\textwidth}||}\n \\hline\n Category & Key Characteristic & Publications\\\\ [0.5ex] \n \\hline\\hline\n Autoregressive DGGs & Adopting a sequential generation strategy, either node-by-node or edge-by-edge & \\\\ \n \\hline\n Autoencoder-Based DGGs & Making the generation process dependent on latent space variables & \\\\\n \\hline\n RL-Based DGGs & Utilizing reinforcement learning algorithms to induce desired properties in the generated graphs & \\\\\n \\hline\n Adversarial DGGs & Employing generative adversarial networks (GANs)  to generate graph structures & \\\\ \n \\hline\n Flow-based DGGs & Learning a mapping from the complicated graph distribution into a distribution mostly modeled as a Gaussian for calculating the exact data likelihood & \\\\\n [1ex] \n \\hline\n\\end{tabular}\n\\caption{\\label{tab:categorization} Categorization,  Key Characteristic, and Representative Publications among Deep Graph Generators}\n\\end{center}\n\\end{table*}\nRecently, with the rapid development of data collection and storage technologies, an increasing amount of data that needs to be processed is available. In many research areas, including biology, chemistry, pharmacy, social networks, and knowledge graphs, there exist some relationships between data entities that, if taken into account, more valuable features can be extracted, yielding more accurate predictions. Using graph data structure is a common way to represent such data, and therefore graph analysis research has attracted considerable attention.\\par\nIn the past few years, graph-related studies have made significant progress, which mainly focus on graph representation learning  but also include other problems like graph matching , adversarial attack and defense on graph-based neural networks , and graph attention networks . Graph generation is also another research line aiming to generate new graph structures with some desired properties, which dates back to 1960  and is followed by several other approaches .  However, the early methods generally use hand-engineered processes to create graphs with predefined statistical properties and, despite their simplicity, are not capable enough to capture complicated graph dependencies.\\par\nThanks to the recent successes of deep learning techniques and algorithms, deep generative models, which aim to generate novel samples from a similar distribution as the training data, have received a lot of attention in various data domains such as image, text, and speech. Subsequently, studies related to deep learning-based graph generators have started a little later, which, unlike the traditional approaches, can directly learn from data and eliminate the need for using hand-designed procedures. Therefore, there are apparent horizons in this research area, with applications ranging from discovering new molecular structures to modeling social networks.\\par\nSo far, several surveys have reviewed deep graph-related approaches such as those mainly focusing on graph representation learning methods , graph attention models , attack and defense techniques on graph data , and graph matching approaches . Although most of these surveys have made a passing reference to the modern graph generation approaches, which we refer to as Deep Graph Generators (DGGs), this field requires individual attention due to its value and expanding development.\\par \nIn this paper, we conduct a survey on DGGs in order to exclusively review and categorize these methods and their applications. To this end, we first divide the existing approaches into five broad categories, namely, autoregressive DGGs, autoencoder-based DGGs, RL-based DGGs, adversarial DGGs, and flow-based DGGs, providing the readers with detailed descriptions of the methods in each category and comparing them from different aspects. This categorization is either based on the model architectures, adopted generation strategies, or optimization objectives and the categories may sometimes overlap so that a method can belong to more than one category. Table \\ref{tab:categorization} summarizes the main characteristics of these categories, along with the most prominent approaches belonging to each of them.\\par \nThe rest of this article is organized as follows. Section \\ref{sec:notations} briefly summarizes notations used in this survey and formulates the problem of deep graph generation. Sections \\ref{sec:autoregressive} to \\ref{sec:flow} provide a detailed review of the existing DGGs in each of the five categories discussed above. Section \\ref{sec:applications} classifies the current applications and suggest some potential future ones. Section \\ref{sec:implementations} goes through implementation details by summarizing commonly used datasets, widely utilized evaluation metrics, and available source codes. Section \\ref{sec:future} discusses future research directions. Finally, section \\ref{sec:conclusion} concludes the survey.", "cites": [242, 3993, 7013, 7763, 3988, 3996, 3980, 988, 3978, 8712, 550, 3987, 3989, 3995, 3971, 227, 3981, 222, 264, 8711, 3998, 3985, 3994, 1655, 229, 3983, 4000, 3977, 7159, 3984, 231, 7009, 1003, 3990, 3959, 1652, 553, 3997, 3972, 7158, 180, 3975, 3992, 236, 3982, 3976, 3979, 8710, 8709, 169, 3991, 3999, 3986, 7006, 3973, 3974, 7214], "cite_extract_rate": 0.7215189873417721, "origin_cites_number": 79, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a high-level categorization of deep graph generation approaches, referencing multiple papers within each category. However, the synthesis of ideas remains limited to listing characteristics and representative papers without deeper connections or integration. It offers minimal critical evaluation or identification of limitations in the cited works, and while it does present a framework for classification, it lacks meta-level abstraction or broader insights into trends and principles in the field."}}
{"id": "37fa1d36-0470-4829-a129-6f1585a7ab97", "title": "Recurrent DGGs", "level": "subsection", "subsections": ["326ca503-cf34-4d35-81a4-082711935bf1", "45c200fc-82a6-4c89-9acb-1ad2fd3fb7d0"], "parent_id": "809f22e3-659d-4a78-af8d-d801c6b3a53f", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Autoregressive Deep Graph Generators"], ["subsection", "Recurrent DGGs"]], "content": "Recurrent DGGs are a bunch of autoregressive deep graph generators that use RNNs, namely long short-term memory (LSTM)  or gated recurrent units (GRU), to exert the influence of the generation history on the current decision. Here, we provide a detailed review of these methods in two subcategories.", "cites": [243], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section briefly introduces recurrent DGGs and mentions their use of RNN variants like LSTM and GRU. However, it lacks synthesis by not connecting the cited paper's encoder-decoder framework to graph generation in a meaningful way. There is no critical analysis or abstraction beyond the surface-level description of methods, and it does not form a coherent narrative integrating multiple sources."}}
{"id": "326ca503-cf34-4d35-81a4-082711935bf1", "title": "Node-by-Node Generators", "level": "subsubsection", "subsections": [], "parent_id": "37fa1d36-0470-4829-a129-6f1585a7ab97", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Autoregressive Deep Graph Generators"], ["subsection", "Recurrent DGGs"], ["subsubsection", "Node-by-Node Generators"]], "content": "Most of the autoregressive methods append one new node at a time into the already generated graph. For example, Li et al.  propose to generate molecular graphs sequentially, where the generation process initiates by adding a node to an empty graph. It then continues by iteratively deciding whether to append a new node to the graph, connect the lastly added node to the previous ones, or terminate the process. To this end, the authors propose two architectures, namely MolMP and MolRNN, to determine probabilities for each of these three actions. More precisely, MolMP decides based on the graph's current state, modeling the generation as a Markov Decision Process. It first calculates an initial embedding for graph nodes followed by several convolutional layers and an aggregation operation to obtain a graph-level representation. It then passes both the node-level and graph-level embeddings through MLP and softmax layers to compute the probabilities required for action selection. MolRNN, on the other hand, exploits molecule level recurrent units to make the generation history affect the current decision, which improves the model's performance. It adopts the same approach as MolMP to obtain embeddings and then updates the recurrent units' hidden state as follows:\n\\begin{equation}\nh_i = f_{trans}(h_{i-1}, h_{v^*}, h_{G_{i-1}}),\n\\end{equation}\nwhere $f_{trans}$ is implemented using GRUs, $h_{v^*}$ is the latest appended node embedding, and $h_{G_{i-1}}$ denotes the representation for the graph generated before the $i$-th generation step. Next, the action probabilities are calculated similarly as MolMP, except that MolRNN replaces $h_{i}$ by the graph-level representation. Moreover, the authors make the conditional graph generation possible by first converting a given requirement to a conditional code and then modifying the graph convolution to include this code.\\par\nYou et al.  propose GraphRNN, another deep autoregressive model with a hierarchical architecture consisting of a graph-level RNN and an edge-level RNN which learns to sample $G \\sim p(G)$ without explicitly computing $p(G)$. For this purpose, GraphRNN first defines a mapping $f_S$ from graphs to sequences where for a graph $G$ with $n$ nodes under the node ordering $\\pi$, the mapping is defined as follows:\n\\begin{equation}\nS^\\pi = f_S(G, \\pi) = (S_1^\\pi, ..., S_n^\\pi),\n\\end{equation}\nwhere each element $S_i^\\pi \\in \\{0, 1\\}^{i-1}, i\\in\\{1, ..., n\\}$ represents the edges between node $\\pi(v_i)$\nand the previous nodes. Since for undirected graphs, there exists the mapping function $f_G(S^\\pi) = G$, it is possible to sample $G$ at inference time by first sampling $S^\\pi \\sim p(S^\\pi)$ and then applying $f_G$, which obviates the need to compute $p(G)$ explicitly. To learn $p(S^\\pi)$, due to the sequential nature of $S^\\pi$,\n$p(S^\\pi)$ can be further decomposed as in Eq. (\\ref{eq:2}), which is modeled by an RNN with state transition and output functions defined in Eq. (\\ref{eq:3}) and (\\ref{eq:4}), respectively:\n\\begin{equation}\\label{eq:2}\np(S^\\pi) =\\prod_{i=1}^{n+1}p(S_i^\\pi |S_1^\\pi, ..., S_{i-1}^\\pi)=\\prod_{i=1}^{n+1}p(S_i^\\pi |S_{<i}^\\pi),\n\\end{equation}\n\\begin{equation}\\label{eq:3}\nh^{node}_i = f_{trans}^{node}(h^{node}_{i-1}, \\text{In}^{node}_{i}), \\ \\ \\text{In}^{node}_{i} = S_{i-1}^\\pi,\n\\end{equation}\n\\begin{equation}\\label{eq:4}\n\\theta_i = f_{out}(h_i^{node}),\n\\end{equation}\nwhere $f_{trans}^{node}$ is implemented using a GRU and serves as the graph-level RNN that maintains the state of the graph generated so far. Furthermore, the authors propose two varients for the implementation of $f_{out}$. First, they propose GraphRNN-S, a simple variant that does not consider dependencies between edges and models $p(S_i^\\pi |S_{<i}^\\pi)$ as a multivariate Bernoulli distribution. Next, to fully capture the complex edge dependencies, they propose the full GraphRNN model as illustrated in Figure \\ref{fig:GraphRNN}, which approximates $f_{out}$ by another RNN (i.e., the edge-level RNN) formulated as follows:\n\\begin{equation}\\label{eq:7}\n\\small\nh_{i, j}^{edge} = f_{trans}^{edge}(h^{edge}_{i, j-1}, \\text{In}^{edge}_{j}), \\ \\ \\text{In}^{edge}_{j} = S_{i, j-1}^\\pi, \\ \\ h^{edge}_{i,0} = h^{node}_i.\n\\end{equation}\nFurthermore, GraphRNN introduces a BFS node ordering scheme to improve scalability with two benefits. First,  it will suffice to train the model on all possible BFS orderings, rather than all possible node permutations. Second, it reduces the number of edges to be predicted in the edge-level RNN. \n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=1\\textwidth]{figures/GraphRNN}\n\\vspace*{-1.3cm}\n\\begin{center}\n\\caption{An illustration of the graph generation procedure at inference time proposed\nin GraphRNN  (reprinted with permission). Green arrows denote the\ngraph-level RNN, and blue arrows represent the edge-level RNN.}\n\\label{fig:GraphRNN}\n\\end{center}\n\\end{figure}\n\\par\nSubsequently, several graph generation methods have been proposed inspired by GraphRNN. For example, Liu et al.  propose two further variants for the implementation of $f_{out}$ function in Eq. (\\ref{eq:4}), namely RNN-Transf and GraphRNN+Attn. Specifically, RNN-Transf replaces the edge-level RNN in the full GraphRNN model with a vanilla Transformer  decoder consisting of a self-attention sublayer and a graph-state attention sublayer with the memory from hidden states of the node-level RNN. GraphRNN+Attn, on the other hand, maintains the edge-level RNN. More precisely, it is an additive attention mechanism in the edge-level RNN that computes the attention weights in each step using the last hidden states of the node-level RNN as well as the current hidden state of the edge-level RNN.\\par\nMolecularRNN extends GraphRNN to generate realistic molecular graphs with desired chemical properties. As in molecular graphs, both nodes and edges have types, likelihood formulation in Equation (\\ref{eq:2}) is rewritten as follows:\n\\begin{equation}\\label{eq:5}\np(S^\\pi, C^\\pi) =\\prod_{i=1}^{n+1} p(C_i^\\pi|S_{<i}^\\pi , C_{<i}^\\pi)p(S_i^\\pi|C_i^\\pi, S_{<i}^\\pi,  C_{<i}^\\pi ),\n\\end{equation}\nwhere $S_{i,j}^\\pi \\in \\{0, 1, 2, 3\\}$ is the categorical edge type that corresponds to no, single, double, or triple bonds, and $C_{i}^\\pi \\in \\{1, 2, ..., K\\}$ determines node (atom) type. Then, MolecularRNN substitutes the graph-level RNN input in Eq. (\\ref{eq:3}) with the embeddings of categorical inputs as in Eq. (\\ref{eq:6}):\n\\begin{equation}\\label{eq:6}\n\\text{In}_{i}^{node} = [emb(S_{i-1}^\\pi),emb(C_{i-1}^\\pi)].\n\\end{equation}\nFurthermore, a two-layer MLP with softmax output activation is added on top of the hidden states of both graph-level and edge-level RNNs to predict node and edge types, respectively.\nAfter likelihood pretraining on the molecular datasets, the model is fine-tuned with the policy gradient algorithm to shift the distribution\nof the generated samples to some desired chemical properties, namely, lipophilicity, drug-likeness, and melting point. Thus, the MolecularRNN acts as a policy network to output the probability of the next action given the current state, where the set of states consists of all possible sub-graphs and the possible atom connections to the existing graph, for all the atom types, serve as the action set. Moreover, each valid molecule is considered as a final state $s_n$, where its corresponding final reward is denoted by $r(s_n)$. The intermediate rewards $r(s_i), 0 < i < n$ are also obtained by discounting $r(s_n)$ as in the following loss function formula:\n\\begin{equation}\\label{eq:8}\n\\mathcal{L}(\\theta) = -\\sum_{i=1}^n r(s_n). \\gamma^i . \\log p (s_i|s_{i-1}; \\theta),\n\\end{equation}\nwhere $\\gamma$ is the discount factor and the transition probabilities $p(s_i|s_{i-1}; \\theta)$ are the elements of the product in Eq(\\ref{eq:5}). Furthermore, MolecularRNN introduces the structural penalty for atoms violating valency constraints during training. It also adopts a valency-based rejection sampling method during inference, which guarantees the generated samples' validity.\n\\par\nSun et al.  learn a mapping from a source to a target graph by adopting an encoder-decoder based approach, where the encoder utilizes recurrent based models to encode the source graph and the decoder generates the target graph in a node-by-node fashion, which makes it necessary to consider an ordering over nodes. Therefore, the authors first introduce a procedure to transform a graph $G$ into a DAG (Directed Acyclic Graph) to provide the required node ordering. They then obtain embeddings for each of the DAG's nodes by proposing two encoders: an Energy-Flow encoder and a Topology-Flow encoder, where the former utilizes only the information of adjacent nodes, while the latter exploits both the adjacent and non-adjacent nodes' information. Afterward, the decoder sequentially generates the target graph conditioned on the source graph by adopting a relatively similar generation strategy as the GraphRNN.\\par\nSo far, we have studied GraphRNN  as one of the most widely used deep graph generators and then reviewed the subsequent graph generation approaches inspired by it; each generates different types of graphs from general to molecular ones. Moreover, there are also other methods that use GraphRNN as a basis for solving some application-specific problems. For example, REIN  proposes to autoregressively generate meshes from input point clouds inspired by GraphRNN so that in each generation step, it predicts edges from the newly introduced point to all the previous ones. The generated mesh can then be used for the task of 3D object reconstruction. DeepNC  is another GraphRNN-based approach that proposes a network completion algorithm to infer the missing parts of a network. Specifically, it first trains GraphRNN to learn a likelihood over the data distribution. The method then formulates an optimization problem to infer the missing parts of a partially observed input graph in such a way that maximizes the learned likelihood.", "cites": [3979, 3996, 8709, 4001, 38], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers (Li et al., You et al., Liu et al., and MolecularRNN) to explain the progression and variations of node-by-node autoregressive graph generation methods. It provides a coherent narrative by linking techniques such as GRUs, attention mechanisms, and conditional generation. While there is some critical analysis (e.g., computational expense, scalability improvements), it is not deeply evaluative. The section identifies patterns in model design and training objectives but does not reach a meta-level abstraction or propose a novel framework."}}
{"id": "45c200fc-82a6-4c89-9acb-1ad2fd3fb7d0", "title": "Edge-by-Edge Generators", "level": "subsubsection", "subsections": [], "parent_id": "37fa1d36-0470-4829-a129-6f1585a7ab97", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Autoregressive Deep Graph Generators"], ["subsection", "Recurrent DGGs"], ["subsubsection", "Edge-by-Edge Generators"]], "content": "In addition to the methods discussed so far, there also exist other approaches adopting an edge-based generation strategy.  Bacciu et al.  propose to generate a sequence of edges for each graph instead of generating graphs node-by-node. They first convert a graph $G$ under the node ordering $\\pi$ to an ordered edge sequence $S^{edge,\\ \\pi} = [S^{edge,\\ \\pi}_1, ..., S^{edge,\\ \\pi}_m]$, where $S^{edge,\\ \\pi}_i = (u_i^\\pi, v_i^\\pi)$ is the $i$-th edge in the sequence that connects the source node $u_i^\\pi$ to the destination node $v_i^\\pi$ (here $u_i^\\pi$ and $v_i^\\pi$ are IDs assigend to graph nodes by $\\pi$). Note that the sequence is ordered, that is $S_i^{edge,\\ \\pi} \\leq S_{i+1}^{edge,\\ \\pi}$ iff $u_i^\\pi < u_{i+1}^\\pi$ or ($u_i^\\pi = u_{i+1}^\\pi$ and $v_i^\\pi < v_{i+1}^\\pi$). Then, the authors define $U^\\pi = [u_1^\\pi, . . . , u_m^\\pi]$ and $V^\\pi = [v_1^\\pi, . . . , v_m^\\pi]$ as sequences of the source and destination node IDs, respectively and decompose the edge sequence probability as followes:\n\\begin{equation}\nS^{edge,\\ \\pi} = p(U^\\pi)p(V^\\pi|U^\\pi),\n\\end{equation}\nwhere $p(U^\\pi)$ and $p(V^\\pi|U^\\pi)$ are approximated with two RNNs. Specifically, \\texttt{RNN1} is used to estimate $p(U^\\pi)$ with the following transition and output functions:\n\\begin{equation}\n\\begin{split}\n&h_i^\\texttt{RNN1} = f_{trans}(h_{i-1}^\\texttt{RNN1}, \\text{In}_{i}^\\texttt{RNN1}), \\ \\ \\ \\text{In}_{i}^\\texttt{RNN1} = emb(u_{i-1}^\\pi)\n\\\\\n&p(u_i^\\pi|u_{i-1}^\\pi, h_{i-1}^\\texttt{RNN1}) = f_{out}(h_i^\\texttt{RNN1})=\\text{Softmax}(Lin(h_i^\\texttt{RNN1})),\n\\end{split}\n\\end{equation}\nwhere $f_{trans}$ is implemented as a GRU and $Lin$ is a linear projection to map the recurrent output to the node ID space. Once all of the $U^\\pi$'s elements are generated, the last recurrent state of \\texttt{RNN1} is used to initialize the state of \\texttt{RNN2}, and the process moves to \\texttt{RNN2} that is given $U^\\pi$ as input. Thus \\texttt{RNN2} computes the probability distribution of $p(V^\\pi|U^\\pi)$ with the same architecture as \\texttt{RNN1} by approximating $p(v_i | u_i, h_{i-1}^\\texttt{RNN2})$ each step. \n\\par\nSimilarly, GraphGen  proposes another edge-based generation strategy that adds a single edge to the already generated graph at each stage. To this end, the method first converts a graph $G$ to a sequence $ S^{edge} = [S_1^{edge},...,S_m^{edge}]$ using the minimum DFS code, where each $S_i^{edge}$ corresponds to an edge $e = (u,v)$ and is described using a 5-tuple $(t_u, t_v , L_u, L_e, L_v)$, where $t_u$ is the timestamp assigned to node $u$ during the DFS traversal, and $L_u$ and $L_e$ denote the node and edge labels, respectively. As the minimum DFS codes are canonical labels, and thus there is a one-to-one mapping between a graph and its corresponding sequence,  there is no longer need to deal with multiple representations for the same graph under different node permutations during training, which improves the method scalability. Then, GraphGen takes a similar approach to GraphRNN  to decompose $p(S^{edge})$ as follows:\n\\begin{equation}\\label{GraphGen-likelihood-eq}\n\\small\np(S^{edge}) =\\prod_{i=1}^{m+1}p(S_i^{edge} |S_1^{edge}, ..., S_{i-1}^{edge})=\\prod_{i=1}^{m+1}p(S_i^{edge} |S_{<i}^{edge}),\n\\end{equation}\nwhere $m$ is the number of edges, and making the simplifying assumption that $t_u$, $t_v$, $L_u$, $L_e$, and $L_v$ are independent, reduces  $p(S_i^{edge} |S_{<i}^{edge} )$ in Eq. (\\ref{GraphGen-likelihood-eq}) to:\n\\begin{equation}\\label{GraphGen-indep}\n\\begin{split}\np(S_i^{edge} |S_{<i}^{edge} ) &= p((t_u,t_v , L_u, L_e, L_v ) | S_{<i}^{edge} )\\\\&\n= p(t_u |S_{<i}^{edge} ) \\times p(t_v |S_{<i}^{edge} ) \\times p(L_u |S_{<i}^{edge} )\\\\&\n\\times p(L_e |S_{<i}^{edge} ) \\times p(L_v |S_{<i}^{edge} ).\n\\end{split}\n\\end{equation}\nTo capture conditional distributions in Eq. (\\ref{GraphGen-indep}), the authors propose to use a custom LSTM with the transition function $f_{trans}$ in Eq. (\\ref{GraphGen-lstm}), and five separate output functions for each component of the 5-tuple. For example, $f_{t_u}$ in Eq. (\\ref{GraphGen-out-prediction}) is utilized for predicting $t_u$, where $\\sim_M$ represents sampling from a multinomial distribution. \n\\begin{equation}\\label{GraphGen-lstm}\nh_i = f_{trans} (h_{i-1}, \\text{In}_{i}), \\ \\ \\text{In}_{i} = emb (S_{i-1}^{edge}),\n\\end{equation}\n\\begin{equation}\\label{GraphGen-out-prediction}\nt_u \\sim_M\\ \\theta_{t_u} = f_{t_u} (h_i ),\n\\end{equation}\n\\begin{equation}\nS_i^{edge} = concat(t_u, t_v, L_u, L_e, L_v).\n\\end{equation}\nFigure \\ref{fig:GraphGen} outlines the proposed pipeline.\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=1\\textwidth]{figures/GraphGen}\n\\vspace*{-1cm}\n\\begin{center}\n\\caption{Flowchart of GraphGen .}\n\\label{fig:GraphGen}\n\\end{center}\n\\end{figure}", "cites": [8710, 3972, 3996], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the technical approaches of two edge-based graph generation methods, Bacciu et al. and GraphGen, focusing on their architectures and probabilistic formulations. While it connects these methods under the broader theme of edge-based sequential generation, there is minimal synthesis of ideas across the cited works, and no critical evaluation or identification of broader trends or principles. The discussion remains largely concrete and method-focused without analytical depth."}}
{"id": "1038e4cf-22e2-43c0-bdbf-e3c3c7e4a28f", "title": "Attention-Based Methods", "level": "subsubsection", "subsections": [], "parent_id": "04e173b5-1150-40d6-94d4-de1028cb603d", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Autoregressive Deep Graph Generators"], ["subsection", "Non-Recurrent DGGs"], ["subsubsection", "Attention-Based Methods"]], "content": "Here, we review the methods in which the attention mechanism plays a key role. In this regard, GRAN  proposes to generate one block of nodes and associated edges at each generation step by optimizing the following likelihood:\n\\begin{equation}\\label{GRAN-likelihood}\np(L^\\pi)=\\prod_{t=1}^Tp(L^\\pi_{\\textbf{b}_{\\textbf{t}}}|L^\\pi_{\\textbf{b}_{\\textbf{1}}},..., L^\\pi_{\\textbf{b}_{\\textbf{t-1}}}),\n\\end{equation}\nwhere $L^\\pi$ is the lower triangular part of the adjacency matrix $A^\\pi$, $B$ denotes the block size, $\\textbf{b}_{\\textbf{t}} = \\{B(t - 1) + 1,..., Bt\\}$ is the set of row indices for the $t$-th block of $L^\\pi$, and $T = \\ceil{\\frac{N}{B}}$ is the number of graph generation steps. For the $t$-th step, GRAN adds $B$ new nodes to the already-generated subgraph and connects them with each other as well as the previous $B(t - 1)$ nodes to acquire an augmented graph as depicted in Figure \\ref{GRAN-fig}. The authors then apply the following graph neural network with attentive messages on the augmented graph to get updated node representations:\n\\begin{equation}\\label{GRAN-GNN}\n\\begin{split}\nm^r_{ij} = f(h^r_i - h^r_j)\\textbf{,} \\ \\ \\ \\tilde{h^r_i}&= [h^r_i, x_i]\\textbf{,}  \\ \\ \\ a^r_{ij} = \\sigma\\big( g(\\tilde h^r_i - \\tilde h^r_j)\\big)\\\\\nh^{r+1}_i &= \\text{GRU}(h^r_i, \\sum_{j\\in \\mathcal{N}(i)}a^r_{ij}m^r_{ij}),\n\\end{split}\n\\end{equation}\nwhere $h^r_i$ is the representation for node $i$ after round $r$, $m^r_{ij}$ is the message vector from node $i$ to $j$, $x_i$ indicates whether node $i$ is in the previously generated nodes or the newly added ones, and $a^r_{ij}$ is an attention weight associated with $edge (i, j)$. Both the message function $f$ and the attention function $g$ are implemented as 2-layer MLPs with ReLU nonlinearities. After $R$ rounds of message passing, the final node representation vectors $h^R_i$ for each node $i$ is obtained, and then GRAN models the conditional probability in Eq. (\\ref{GRAN-likelihood}) with a mixture of Bernoulli distributions to capture edge dependencies via $K$ latent mixture components:\n\\begin{equation}\\label{GRAN-Mixture}\n\\begin{split}\n&p(L^\\pi_{\\textbf{b}_{\\textbf{t}}}|L^\\pi_{\\textbf{b}_{\\textbf{1}}},..., L^\\pi_{\\textbf{b}_{\\textbf{t-1}}}) = \\sum_{k=1}^K\\alpha_k\\prod_{i\\in \\textbf{b}_{\\textbf{t}}}\\prod_{1\\leq j\\leq i} \\theta_{k, i, j}\n\\\\\n&\\alpha_1, ..., \\alpha_K = \\text{Softmax}\\ \\Big(\\sum_{i\\in \\textbf{b}_{\\textbf{t}}, 1\\leq j\\leq i}\\text{MLP}_{\\alpha}(h_i^R-h_j^R) \\Big)\n\\\\\n&\\theta_{1, i, j}, ...,  \\theta_{K,i,j} = \\sigma \\big(\\text{MLP}_\\theta(h^R_i - h^R_j)\\big).\n\\end{split}\n\\end{equation}\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=1.03\\textwidth]{figures/GRAN}\n\\vspace*{-1.3cm}\n\\begin{center}\n\\caption{An overview of GRAN  (reprinted with permission). Dashed lines are augmented edges. Nodes with the same color belong to the same block (block size = 2). }\n\\label{GRAN-fig}\n\\end{center}\n\\end{figure}\n\\par\nGRAM  proposes to combine graph convolutional networks with graph attention mechanisms to obtain richer features during the graph generation process, where the proposed graph attention mechanism extends the one in  by introducing bias terms as a function of the shortest path between nodes. In particular, GRAM tries to maximize the following likelihood, which is somewhat similar to Eq. (\\ref{eq:5}):\n\\begin{equation}\n\\small\n\\begin{split}\np(&A^\\pi, C^\\pi) =\\\\ &\\prod_{i=1}^{n+1} p(C_i^\\pi|A_{<i, <i}^\\pi, C_{<i}^\\pi) \\prod_{j=1}^{i-1}p(A_{j, i}^\\pi|A_{<j,i}^\\pi, C_i^\\pi, A_{<i,<i}^\\pi, C_{<i}^\\pi).\n\\end{split}\n\\end{equation}\nTo this end, the authors propose an architecture that consists of three networks, namely, feature extractor, node estimator, and edge estimator. Firstly, the feature extractor extracts the local and global information using graph convolution layers and graph attention layers, respectively, where an attention layer employs a self-attention mechanism with the query, key, and value that are set to the node feature vectors. A graph pooling layer then aggregates all node features into a graph feature vector, denoted as $h^G$, by summing them up. Next, the node estimator determines a label for the new node based on the feature vector of the graph generated so far. Thereafter, the edge estimator predicts labels for edges between the newly added node and those already exist in the graph one after the other using a source-target attention mechanism as follows:\n\\begin{equation}\nA_{j, i}^\\pi = \\text{Softmax}(g_{EE}(h^v_j,  h^G, h^v_i, h^e_{<j})),\n\\end{equation}\nwhere $g_{EE}$ is a three-layer feedforward network, $h^v_j$ and $h^v_i$ are label embeddings of node $v_j$ and the new node $v_i$, respectively, and $h^e_{<j}$ is computed using a source-target attention with $\\text{Concat}(h^v_j, h^v_i)$ as its query and $\\{\\text{Concat}(h^v_t, h^v_i, h^e_{t,i})|t = 1, ..., j - 1\\}$ as both the key and value. \\par\nAGE  introduces another attention-based generative model, which is conditioned on some input graphs. In other words, the method takes an existing source graph as input and generates a transformed version of it, modeling its evolution. To this end, the authors propose an encoder-decoder based architecture, where its decoder autoregressively generates the target graph in a node-by-node fashion. More specifically, the encoder first applies the self-attention mechanism to the source graph in order to learn its nodes' representations. Then, at each generation step, the decoder first adopts a similar self-attention mechanism as the encoder, followed by source-target attention, which discovers the correlations between the nodes in the source graph and the ones in the already generated target graph. This way the decoder computes a representation for the graph generated so far, which will be further used to predict the new node's label and connections.", "cites": [3997, 3987, 38], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual description of attention-based graph generation methods, including GRAN, GRAM, and AGE, with some integration of how attention mechanisms are used in each. However, it lacks deeper synthesis of overarching patterns or principles across the methods and offers minimal critical evaluation or comparison. The content is detailed but remains largely at the level of summarizing the individual works."}}
{"id": "b76d26e3-8315-4f92-9917-1fc08d448595", "title": "Other Methods", "level": "subsubsection", "subsections": [], "parent_id": "04e173b5-1150-40d6-94d4-de1028cb603d", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Autoregressive Deep Graph Generators"], ["subsection", "Non-Recurrent DGGs"], ["subsubsection", "Other Methods"]], "content": "\\begin{figure*}[tp]\n\\centering\n\\includegraphics[width=0.65\\textwidth]{figures/BiGG}\n\\vspace*{-0.2cm}\n\\centering\\caption{An overview of the edge generation procedure in  (reprinted with permission).}\n\\label{BiGG-fig}\n\\end{figure*}\n\\begin{table*}[ht!]\n\\begin{center}\n\\small\n \\begin{tabular}{||l l l l l l||}\n \\hline\n Method & Recurrent & \\makecell[l]{Generation\\\\ Strategy} & \\makecell[l]{Attention\\\\ Mechanism} & Features & \\makecell[l]{Conditional\\\\ Generation}\\\\ [0.5ex] \n \\hline\\hline\n MolMP & No & Node-by-node& No& Node/Edge& Yes \\\\ \n \\hline\n MolRNN & Yes & Node-by-node& No& Node/Edge& Yes \\\\ \n \\hline\n GraphRNN & Yes & Node-by-node& No& -& No \\\\ \n \\hline\n MolecularRNN & Yes & Node-by-node& No& Node/Edge& No \\\\\n \\hline\nBacciu et al.  & Yes & Edge-by-edge& No& -& No \\\\\n \\hline\nGraphGen  & Yes & Edge-by-edge& No& Node/Edge& No \\\\\n\\hline\nGRAN & No & Block of nodes& Yes& -& No \\\\\n \\hline\nGRAM  & No & Node-by-node& Yes& Node/Edge& No \\\\\n\\hline\nAGE  & No & Node-by-node & Yes& Node& Yes\\\\ \n\\hline\nDeepGMG & No & Node-by-node & Yes& Node/Edge& Yes\\\\ \n\\hline\nBiGG  & No & Node-by-node & No& -& No\\\\ \n[1ex] \n \\hline\n\\end{tabular}\n\\caption{\\label{tab:autoregressive} The Main Characteristics of Autoregressive Deep Graph Generators}\n\\end{center}\n\\end{table*}\nBesides the attention-based methods reviewed above, other autoregressive non-recurrent DGGs either do not use attention at all, or the attention mechanism does not play a decisive role in their generation process. For example, DeepGMG  proposes a sequential graph generation process which can be seen as the following sequence of decisions: (1) whether to add a new node of a particular type or not (with probabilities provided by the $f_{addnode}$ in Eq. (\\ref{DeepGMG-addnode}), where $h_G$ is the graph representation vector, and $f_{an}$ is an MLP that maps $h_G$ to the action output space), if a node type is selected (2) the model decides whether to continue connecting the newly added node to the existing graph or not (referring to Eq. (\\ref{DeepGMG-addedge}), where $h_v^{(T)}$ is embedding of the new node $v$ after $T$ rounds of propagation in a graph neural\nnetwork, and $f_{ae}$ is another MLP), if yes (3) it selects a node already in the graph and connects it to the new node (referring to the Eq. (\\ref{DeepGMG-nodes}), where $f_s$ maps pairs $h_u^{(T)}$ and $h_v^{(T)}$ to a score $s_u$). The algorithm goes back to step (2) and repeats until the model decides not to add another edge. Finally, the algorithm goes back to step (1) to add subsequent nodes or terminate the process.\n\\begin{equation}\\label{DeepGMG-addnode}\nf_{addnode}(G) = \\text{Softmax}\\ (f_{an}(h_G)),\n\\end{equation}\n\\begin{equation}\\label{DeepGMG-addedge}\nf_{addedge}(G, v) = \\sigma(f_{ae}(h_G, h_v^{(T)})),\n\\end{equation}\n\\begin{equation}\\label{DeepGMG-nodes}\n\\begin{split}\ns_u = f_s(h_u^{(T)}, h_v^{(T)}),\\ \\  \\forall u\\in V\\\\\nf_{nodes}(G, v) = \\text{Softmax}\\ (s).\n\\end{split}\n\\end{equation}\n\\par\nDeepGG further extends DeepGMG by adding the idea of finite state machines into the generation process. Furthermore, similar to the GraphRNN, DeepGG learns the graph distribution from a sequence called construction sequence, which consists of graph evolutionary actions such as node addition, edge addition, and node deletion.\\par\nRecently, BiGG  proposes an autoregressive model to increase scalability for generating sparse graphs. To learn a generative model, BiGG  uses a single canonical ordering $\\pi(G)$ to model each graph G, as in , aiming to learn a lower bound on p(G):\n\\begin{equation}\n\\small\np(G) = p(V)P(E|V)=p(|V|=n)\\sum_\\pi p(A^\\pi)\\approx p(|V|=n) p(A^{\\pi(G)}),\n\\end{equation}\nwhere $p(|V|=n)$ can be directly estimated using an empirical distribution over the graph size. Therefore the goal is only to model $p(A^{\\pi(G)})$ under a default canonical ordering, which will be denoted by $p(A)$ in the following. Considering that most real-world graphs are sparse, BiGG generates only the non-zero entries in $A$ in a row-wise manner to enhance efficiency and  scalability; thus, the method adopts a recursive strategy inspired by R-MAT , for generating each edge as illustrated in the left half of Figure \\ref{BiGG-fig}. To further improve efficiency, the authors propose to jointly generate all the connections of an arbitrary node $u$ (non-zero entries in the $u$-th row of $A$) by autoregressively generating an {\\it edge-binary tree}, as shown in the right half of Figure \\ref{BiGG-fig}. Finally, BiGG introduces the full autoregressive model that generates the entire adjacency matrix row by row. The full model utilizes the autoregressive models as building blocks:\n\\begin{equation}\np(A) = p(\\{\\mathcal{N}_u\\}_{u\\in V})=\\prod_{u\\in V}p(\\mathcal{N}_u|\\{\\mathcal{N}_{u'}:u'<u\\}),\n\\end{equation}\nwhere $\\mathcal{N}_u$ denotes the set of neighbors for node $u$. More specifically, inspired by Fenwick tree , the authors propose a data structure called {\\it row-binary forest} to encode all the {\\it edge-binary trees} generated so far, which will be used to generate new {\\it edge-binary tree} for the current step.", "cites": [3979, 3987, 8710, 3996, 3976, 8709, 3972, 3997, 222], "cite_extract_rate": 0.6428571428571429, "origin_cites_number": 14, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of various non-recurrent autoregressive deep graph generators, often referencing their specific generation strategies and mechanisms. While it organizes the methods into a table and briefly explains their workflows, the synthesis is limited to surface-level categorization rather than a deeper integration of ideas. There is little critical evaluation of the strengths and weaknesses of each method, and abstraction is modest, with only some general observations about attention and scalability."}}
{"id": "a983ba4e-07c2-4d0f-9631-099f8940639c", "title": "Autoencoder-Based Deep Graph Generators", "level": "section", "subsections": ["91080d8c-7685-44f9-a2e9-a6c4fc47608b", "bb7f0158-f989-4c81-8843-89c6936adc63", "208c4ac0-34b0-446e-994b-eb3349300f8c"], "parent_id": "bab571dd-55e6-4f4b-ac15-157a4152a738", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Autoencoder-Based Deep Graph Generators"]], "content": "\\label{sec:autoencoder}\nThis section reviews those approaches that employ whether autoencoders (AEs) or VAEs to generate graph structures. In particular, a common practice in these methods is first to encode an input graph into a latent space using GNN , GCN , or their variants and then start to generate the graph from this latent space embedding. We divide the existing approaches based on their generation granularity level (i.e., adopting an all-at-once generation strategy, using valid substructures as building blocks, or generating graphs in a node-by-node fashion) into the following three subsections. The main characteristics of the most prominent autoencoder-based graph generators are presented in Table \\ref{tab:autoencoder}.\\par", "cites": [5680], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of autoencoder-based graph generation methods, mentioning a common practice and categorizing them based on generation granularity. However, it lacks deeper synthesis of the cited works, critical evaluation of their strengths and weaknesses, and broader abstraction to higher-level principles or trends. The reference to Paper 1 is minimal and not fully integrated into the narrative."}}
{"id": "91080d8c-7685-44f9-a2e9-a6c4fc47608b", "title": "One-Shot Generators", "level": "subsection", "subsections": [], "parent_id": "a983ba4e-07c2-4d0f-9631-099f8940639c", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Autoencoder-Based Deep Graph Generators"], ["subsection", "One-Shot Generators"]], "content": "A series of autoencoder-based DGGs generate the entire graph all at once. VGAE proposes a graph generation model that primarily aims to perform unsupervised learning on graphs based on the variational autoencoder . Given a graph $G$ with adjacency matrix $A$ and node feature matrix $X$, VGAE infers the latent matrix \\textbf{Z} by a two-layer GCN :\n\\begin{equation}\\label{VGAE-1}\n\\small\nq(\\textbf{Z}|X,A)=\\prod_{i=1}^nq(\\textbf{z}_{i}|X,A),\\ \\text{with}\\ \\ q(\\textbf{z}_{i}|X,A)=\\mathcal{N}(\\textbf{z}_{i}|\\mu_i,\\text{diag}(\\sigma_{i}^2)),\n\\end{equation}\nwhere $\\mu=\\text{GCN}_{\\mu}(X,A)$ is the matrix of mean vectors $\\mu_{i}$; similarly $\\log\\sigma = \\text{GCN}_\\sigma(X, A)$.  Then, the generative model is designed as a simple inner product of latent variables as follows:\n\\begin{equation}\n\\small\np(A|\\textbf{Z})=\\prod_{i=1}^n\\prod_{j=1}^np(A_{ij}|\\textbf{z}_{i},\\textbf{z}_{j}),\\ \\text{with}\\ \\ p(A_{ij} = 1 |\\textbf{z}_{i}, \\textbf{z}_{j})=\\sigma(\\textbf{z}_{i}^{T}\\textbf{z}_{j}),\n\\end{equation}\nwhere $\\sigma (.)$ is the logistic sigmoid function. The model parameters are then learned by optimizing the VAE objective. However, the authors also proposed a more straightforward, non-probabilistic, and autoencoder-based version of the method called GAE. The main limitation of VGAE is that it can only learn from a single input graph. GraphVAE , on the other hand, proposes another VAE-based generative model that learns from a dataset of graphs. The method first embeds the input graph into continuous representation \\textbf{z} using a graph convolution network  as the encoder $q_{\\phi}(\\textbf{z}|G)$, where the dimensionality of \\textbf{z} is relatively small in order to learn a high-level compression of the input data. Then, the decoder outputs a probabilistic fully-connected graph with a fairly small predefined maximum size directly at once denoted by $\\tilde{G}$. The whole GraphVAE model is trained by minimizing the upper bound on negative log-likelihood as follows:\n\\begin{equation}\\label{GraphVAE}\n\\mathcal{L}_{\\theta, \\phi}(G)=\\mathbb{E}_{q_\\phi(\\textbf{z}|G)}[-\\log_{}p_{\\theta}(G|\\textbf{z})]+KL[q_{\\phi}(\\textbf{z}|G)||p(\\textbf{z})],\n\\end{equation}\nwhere $q_\\phi(\\textbf{z}|G)$ and $p_\\theta(G|\\textbf{z})$ are the encoder posterior and the decoder generative\ndistributions respectively, and $\\phi$ and $\\theta$ are parameters to be learned. Since no particular ordering of nodes is imposed in neither G nor $\\tilde{G}$, the authors further adopt an approximate graph matching algorithm for aligning G with $\\tilde{G}$ in order to compute the likelihood $p_\\theta(G|\\textbf{z})$ in Eq. (\\ref{GraphVAE}). However, the growth of GPU memory requirements, number of parameters, and graph matching complexity for larger graph sizes limit the applicability of GraphVAE only to generate smaller graphs.\\par\nMPGVAE  further improves GraphVAE by building a message passing neural network (MPNN) into the encoder and decoder of a VAE, eliminating the need for complex graph matching algorithms. In particular, the method first encodes a molecular graph using a variant of MPNNs  combined with a graph attention  to aggregate the information over each node's neighbors. The encoder then obtains a graph-level representation using the set2set model  and uses this representation to parametrize the posterior distribution $q_\\phi(\\textbf{z}|G)$. Next, the decoder samples $\\textbf{z}\\sim q_\\phi(\\textbf{z}|G)$ and projects it to a high dimensional space consisting of several vectors and then passes these vectors through an RNN to compute initial states for the graph nodes. Afterward, it uses an identical MPNN as the encoder to obtain the final representation for each edge and node. The decoder then reconstructs the graph by predicting the atom types and bond types based on these final representations.\\par\nRGVAE  regularizes the framework of variational autoencoders to generate semantically valid graphs. To impose validity constraints in the training of VAEs, RGVAE transforms a constrained optimization problem to a regularized, unconstrained one by adding inequality constraints to the objective function of VAEs, which forms a Lagrangian function. More precisely, RGVAE minimizes the following loss function in each parameter update:\n\\begin{equation}\n\\mathcal{L} = \\mathcal{L}_{\\theta, \\phi}(G)+\\mu \\sum_ig_i(\\theta, \\textbf{z})_+, \\ \\ \\ \\ \\text{where}\\ \\ \\ \\textbf{z}\\sim p_{\\theta}(\\textbf{z}),\n\\end{equation}\nwhere $g_i(\\theta, \\textbf{z}) \\leq 0$ denotes the $i$-th validity constraint,  $g_+ = max(g, 0)$, and $\\mathcal{L}_{\\theta, \\phi}(G)$ is the standard VAE loss function as in Eq. (\\ref{GraphVAE}). The training of RGVAE is illustrated in Figure \\ref{fig:RGVAE}, where $l$ denotes the index of a training example and $\\underline{l}$ denotes a synthetic example, utilized in the regularization term.\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.85\\textwidth]{figures/RGVAE}\n\\vspace*{-1cm}\n\\begin{center}\n\\caption{The framework of RGVAE  (reprinted with permission). The top flow corresponds to the standard VAE, while the bottom flow denotes the regularization, where a synthetic $\\textbf{z}^{(\\underline{l})}$ is decoded to compute the constraints $g_i(\\theta, \\textbf{z}^{(\\underline{l})})_+$.}\n\\label{fig:RGVAE}\n\\end{center}\n\\end{figure}\n\\par\nGraphite  proposes a latent variable generative model based on VAE for unsupervised representation learning in large graphs. The method only models graph structure, and any supplementary information such as node features $X\\in \\mathbb{R}^{n\\times k}$ is considered as conditioning evidence. To learn the model parameters $\\theta$, Graphite maximizes a lower bound on log-likelihood of the observed adjacency matrix conditioned on $X$:\n\\begin{equation}\n\\log p_{\\theta} (A|X)\\geq \\mathbb{E}_{q_\\phi(\\textbf{Z}|A, X)}\\Big[\\log \\frac{p_\\theta(A, \\textbf{Z}|X)}{q_\\phi(\\textbf{Z}|A, X)}\\Big].\n\\end{equation}\nIn more detail, the authors take an encoding approach based on the mean-field approximation, which represents graph nodes in the latent space using a graph neural network. Next, they propose an iterative two-step approach as the decoding part: it first constructs an intermediate weighted graph $\\hat{A}$ from the latent matrix \\textbf{Z}. Then, a parameterized graph neural network updates the latent matrix \\textbf{Z}$^*$. The process alternates between these two steps to refine the graph gradually. More formally, given \\textbf{Z} and $X$, Graphite iterates over the following two operations:\n\\begin{equation}\n\\hat{A}=\\frac{\\textbf{Z}\\textbf{Z}^\\top}{||\\textbf{Z}||^2} +\\textbf{11}^\\top,\\ \\ \\   \\textbf{Z}^{*}=\\text{GNN}_{\\theta}(\\hat{A},[\\textbf{Z},X]),\n\\end{equation}\nwhere an additional constant of \\textbf{1} is added into the first operation to ensure entries are non-negative. Finally, it should be noted that similar to VGAE , Graphite is also limited to learning from a single input graph.\\par\nIn addition to the aforementioned graph generative approaches, some initial steps have taken towards making DGGs interpretable. Stoehr et al.  propose to learn disentangled, interpretable latent variables corresponding to generative parameters of graphs. The main goal of learning such disentangled variables is to make the latent space more interpretable as each latent variable encodes one and only one data property. Therefore, the authors use a GCN as the encoder combined with a deconvolutional neural network as the decoder to minimize the loss function of $\\beta$-VAE . In this setting, a higher value of $\\beta$ yields the more orthogonalized latent space. To further enforce disentanglement of latent variables, the model also learns an additional \\textit{parameter decoder} $h$, which maps latent variables to generative parameters as illustrated in Figure \\ref{fig:Disentangle}. Recently, NED-VAE  proposes a more generalized generative approach for disentanglement learning on attributed graphs that uncovers the independent latent factors in both edges and nodes. In particular, NED-VAE aims to develop a model that can learn the joint distribution of the graph $G$ and three groups of generative independent latent variables, namely, $\\textbf{z}_{f}, \\textbf{z}_{e}$, and $\\textbf{z}_{g}$  each of which controls the properties of only nodes, only edges, and the joint patterns between them, respectively. Therefore, inspired by the $\\beta$-VAE  formulation and considering the independence resulted from the disentanglement assumption, the goal is to maximize the following objective function:\n\\begin{equation}\n\\small\n\\begin{split}\n   \\mathcal{L}(\\theta, \\phi, G, \\textbf{Z}, \\beta)&=\n\\mathop{\\mathbb{E}_{q_{\\phi}(\\textbf{Z}|G)}}[\\log p_{\\theta}(F|\\textbf{z}_{f},\\textbf{z}_{g})p_{\\theta}(E|\\textbf{z}_{e}, \\textbf{z}_{g})]\n    \\\\\n   &-\\beta D_{KL}(q_{\\phi}(\\textbf{z}_{f}|F)||p(\\textbf{z}_{f}))\n    -\\beta D_{KL}(q_{\\phi}(\\textbf{z}_{e}|E)||p(\\textbf{z}_{e}))\\\\\n    &-\\beta D_{KL}(q_{\\phi}(\\textbf{z}_{g}|E,F)||p(\\textbf{z}_{g})),\n\\end{split}\n\\end{equation}\nwhere $E \\in \\mathbb{R}^{n\\times n\\times d}$ is the edge attributes tensor, and $F \\in \\mathbb{R}^{n\\times k}$ refers to the node attribute matrix. Based on the above objective, the authors propose an architecture consisting of three sub-encoders, namely, a node encoder, an edge encoder, and a node-edge co-encoder to model the distributions $q_{\\phi}(\\textbf{z}_{f}|F)$, $q_{\\phi}(\\textbf{z}_{e}|E)$, and $q_{\\phi}(\\textbf{z}_{g}|E,F)$, respectively, as depicted in Figure \\ref{fig:NED-VAE}. The architecture also consists of two sub-decoders: a node decoder, and an edge decoder to model $p_{\\theta}(F|\\textbf{z}_{f},\\textbf{z}_{g})$ and $p_{\\theta}(E|\\textbf{z}_{e},\\textbf{z}_{g})$, respectively. The authors further propose multiple variant models to address different issues, including group-wise disentanglement, variable-wise disentanglement, and the trade-off between reconstruction error and disentanglement performance.\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.7\\textwidth]{figures/Disentangle}\n\\vspace*{-1cm}\n\\begin{center}\n\\caption{Architecture Overview of  (reprinted with permission).}\n\\label{fig:Disentangle}\n\\end{center}\n\\end{figure}\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.8\\textwidth]{figures/NED-VAE}\n\\vspace*{-1cm}\n\\begin{center}\n\\caption{The architecture of NED-VAE  consisting of three sub-encoders, as well as two sub-decoders (reprinted with permission).}\n\\label{fig:NED-VAE}\n\\end{center}\n\\end{figure}\nMore recently, DGVAE  proposes to replace the commonly used Gaussian distribution in VAE-based graph generation models by the Dirichlet distribution as a prior for the latent variables, causing them to describe the graph cluster memberships, which as a result adds interpretability to the model. For this purpose, the authors adopt the same formulation as VGAE  in Eq. (\\ref{VGAE-1}) for the encoding process except that they utilize a GNN variant, named Heatts, proposed by their own and employ the Laplace approximation  to model both $q(\\textbf{z}_{i}|X,A)$ and $p(\\textbf{z}_{i})$ as Dirichlet distributions. Furthermore, DGVAE adopts a somewhat similar decoding strategy as VGAE  and proves that maximizing the reconstruction term of the model is equivalent to minimizing \\textit{balanced graph cut}, which further gives the authors the motivation for designing the Heatts.", "cites": [3973, 227, 3981, 180, 236, 7215, 4002, 229, 255, 8713, 5680, 216], "cite_extract_rate": 0.75, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "high", "analysis": "The section synthesizes several autoencoder-based graph generation methods (VGAE, GraphVAE, RGVAE, Graphite, and disentanglement approaches) and integrates them into a coherent narrative focused on one-shot generation. It critically evaluates limitations, such as scalability and the need for graph matching, and highlights innovations like MPNN-based encoding/decoding and constraint regularization. While providing a solid analytical overview, it focuses more on specific model improvements rather than abstracting overarching principles."}}
{"id": "bb7f0158-f989-4c81-8843-89c6936adc63", "title": "Substructure-Based Generators", "level": "subsection", "subsections": [], "parent_id": "a983ba4e-07c2-4d0f-9631-099f8940639c", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Autoencoder-Based Deep Graph Generators"], ["subsection", "Substructure-Based Generators"]], "content": "There exists a number of works using valid chemical substructures as building blocks to generate more plausible molecular graphs. JT-VAE  adopts such a strategy by extending the variational autoencoder framework, which, as a consequence, avoids invalidity of the intermediate subgraphs. Specifically, JT-VAE first decomposes a molecular graph G into a junction tree $\\tau_G$ to make the graph cycle-free, where each node in the tree represents a substructure of the molecule. Then, both G and $\\tau_G$ are encoded to latent representations $\\textbf{z}_G$ and $\\textbf{z}_\\tau$, respectively, using different encoders. More precisely, $\\textbf{z}_\\tau$ encodes the junction tree without capturing the exact mutual connections between substructures, while $\\textbf{z}_G$ encodes the graph to capture the fine-grained connectivities. Afterward, JT-VAE reconstructs the junction tree from its latent representation $\\textbf{z}_\\tau$ using a tree-structured decoder, where a tree is generated node-by-node, in a top-down manner. Next, the authors introduce a graph decoder to reproduce the molecular graph based on the underlying predicted junction tree. Since there are potentially many molecules corresponding to the same junction tree, the graph decoder learns how to assemble the subgraphs (nodes in the tree) to reconstruct the molecular graph. Furthermore, to generate molecules with desired properties, the method performs Bayesian optimization in the latent space.\\par \nJT-VAE is basically designed to use small substructures as building blocks, which degrades its performance for generating larger molecules such as polymers. To address this issue, HierVAE  recently proposes a motif-based hierarchical graph encoder-decoder that employs significantly larger motifs as basic building blocks. To this end, the authors design an encoder that learns hierarchical representation for a given molecular graph $G$ in a fine-to-coarse fashion, from atoms to connected motifs. Then, it obtains the latent vector  $\\textbf{z}_G$ by sampling from a Gaussian distribution parametrized using the acquired motif representations. The decoder, on the other hand, autoregressively generates a molecular graph in a coarse-to-fine fashion conditioned on $\\textbf{z}_G$. More specifically, at each generation step, the decoder first predicts the next motif to be attached to the already generated graph. Then, it predicts the attachment points in the new motif, i.e., what atoms belong to the intersection of the new motif and its neighbor motifs. Finally, the decoder decides how the new motif should be attached to the current graph based on its predicted attachment points. The model parameters are learned by minimizing the VAE loss function formulated in Eq. (\\ref{GraphVAE}). Furthermore, the authors extend the architecture to graph-to-graph translation  in order to induce desired properties in the generated molecules.\n\\par \nMHG-VAE  guides VAE to always generate valid molecular graphs by proposing {\\it molecular hypergraph grammar} (MHG),  a special case of {\\it hyperedge replacement grammar} (HRG)  for generating {\\it molecular hypergraphs}, to encode chemical constraints. In particular, the proposed encoder consists of three parts as follows:\n\\begin{equation}\\label{eq:MHG-VAE}\nEnc = Enc_N \\circ Enc_G \\circ Enc_H,\n\\end{equation}\nwhere $Enc_H$ first encodes a molecular graph into a molecular hypergraph, and $Enc_G$ represents the molecular hypergraph as a parse tree by leveraging MHG. Then, $Enc_N$ encodes the previously generated parse tree into the latent continuous space using a seq2seq GVAE . The decoder, on the other hand, acts as an inversion to the encoder and applies production rules, including those with chemical substructure terminals. Finally, using Bayesian optimization, MHG-VAE optimizes the latent continuous space (and its corresponding molecules) towards desired properties.\n\\par\nMoleculeChef  proposes to generate molecular graphs using a set of common reactant molecules as building blocks to address the synthesizability issue. In particular, the encoder maps from a multiset of reactants to a distribution over latent space. This is done by using GGNNs  to embed each reactant molecule separately, which are further summed to form one embedding for the whole multiset. A feed-forward network is then used to parameterize a Gaussian distribution over the latent space. The decoder, on the other hand, autoregressively maps from the latent space to a multiset of reactants using an RNN, where the latent vector \\textbf{z} initializes its hidden layer, and at each generation step, it outputs one reactant or halts the process. Afterward, a reaction predictor  predicts how the previously generated reactants produce a final molecule as illustrated in Figure \\ref{fig:MoleculeChef}. To learn the model parameters, MoleculeChef minimizes the following  WAE  objective function:\n\\begin{equation}\\label{MoleculeChef}\n\\begin{split}\n\\mathcal{L}_{\\theta, \\phi}(G)&=\\mathbb{E}_{G\\sim \\mathcal{D}}\\mathbb{E}_{q_\\phi(\\textbf{z}|G)}[-\\log_{}p_{\\theta}(G|\\textbf{z})]\\\\\n&+\\lambda D(\\mathbb{E}_{G\\sim \\mathcal{D}}[q_{\\phi}(\\textbf{z}|G)], p(\\textbf{z})),\n\\end{split}\n\\end{equation}\nwhere $D$ is a divergence measure, namely the maximum mean discrepancy (MMD). Finally, the authors propose to optimize the molecular properties in the continuous latent space in a similar manner to CGVAE , which we will discuss in the following subsection. \n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.7\\textwidth]{figures/MoleculeChef}\n\\vspace*{-1cm}\n\\begin{center}\n\\caption{An overview of MoleculeChef  (reprinted with permission).}\n\\label{fig:MoleculeChef}\n\\end{center}\n\\end{figure}", "cites": [3974, 8335, 1652, 211, 2709, 3990, 7052, 3975, 3993], "cite_extract_rate": 0.8181818181818182, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.3, "abstraction": 3.8}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple methods (JT-VAE, HierVAE, MHG-VAE, MoleculeChef) and connects their shared objective of improving molecular graph generation through substructure-based approaches. It provides critical insights, such as noting that JT-VAE's use of small substructures limits performance on larger molecules, and how other approaches like HierVAE and MHG-VAE address this. The section abstracts the general idea of using substructures or motifs as building blocks to generate valid and synthesizable molecules, highlighting broader trends in the field."}}
{"id": "208c4ac0-34b0-446e-994b-eb3349300f8c", "title": "Node-by-Node Generators", "level": "subsection", "subsections": [], "parent_id": "a983ba4e-07c2-4d0f-9631-099f8940639c", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Autoencoder-Based Deep Graph Generators"], ["subsection", "Node-by-Node Generators"]], "content": "\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=0.7\\textwidth]{figures/GraphVRNN}\n\\vspace*{-0.2cm}\n\\centering\\caption{The framework of GraphVRNN  (reprinted with permission).}\n\\label{GraphVRNN}\n\\end{figure*}\nBesides the methods reviewed above, some other autoencoder-based DGGs use graph nodes as building blocks, and their decoders adopt autoregressive generation strategies. CGVAE  is one of these methods whose encoder first samples a latent vector $\\textbf{z}_v$ for each node $v$ of an input graph from a normal distribution parametrized using GGNNs. Then, the decoder starts from these vectors and sequentially generates a graph node-by-node with the help of two decision functions, namely, \\texttt{focus} and \\texttt{expand}. More precisely, the \\texttt{focus} function determines the next node to be added into the graph, and the \\texttt{expand} function iteratively chooses edges to add from the currently focused node until particular stop criteria met, and once the generated subgraph changes, all node representations get updated. Moreover, CGVAE employs a valency masking mechanism as part of \\texttt{expand} function in the case of molecule generation to guarantee chemical validity. The authors also propose to optimize graph properties by minimizing $L_2$ distance between some numerical property Q and a differentiable gated regression score $R(G)$ as formulated in Eq. (\\ref{CGVAE}), using gradient ascent in the continuous latent space:\n\\begin{equation}\\label{CGVAE}\nR(G) = \\sum_v\\sigma(g_1(\\textbf{z}_v)).g_2(\\textbf{z}_v),\n\\end{equation}\nwhere $g_1$ and $g_2$ are neural networks. \n\\par\nDEFactor  proposes an encoder-decoder based architecture with a recurrent autoregressive decoder for conditional graph generation. In this regard, the encoder first applies a GCN  to obtain node embeddings, which are then aggregated by an LSTM to compute the graph-level representation $\\textbf{z}$. Then, the two-step decoder first employs another LSTM in order to autoregressively generate embeddings for each of the graph nodes based on the computed $\\textbf{z}$:\n\\begin{equation}\nh_i = f_{trans}(g_{in}([\\textbf{z}, s_{i-1}]), h_{i-1}),\\ \\ \ns_i = f_{embed}([h_i, \\textbf{z}]),\n\\end{equation}\nwhere $f_{trans}$ is implemented by an LSTM , $g_{in}$ and $f_{embed}$ are MLPs, and $s_i$ is the node embedding generated at timestep $i$. Next, the decoder establishes an edge factorization approach to compute the existence probability of an edge of type $k$ between nodes $u$ and $v$ as follows:\n\\begin{equation}\np(E_{u,v,k}|s_u, s_v)=\\sigma(s_u^\\intercal D_k s_v),\n\\end{equation}\nwhere $D_k$ is the diagonal matrix of learnable factors for the $k$-th edge  type. Finally, DEFactor makes the generation process conditional by first concatenating the condition vector $C$ with $\\textbf{z}$. It then utilizes a pre-trained discriminator to assess the property $C$ in the graphs generated by the decoder in the training phase.\\par\nNeVAE  proposes a probabilistic and permutation invariant encoder that is relatively similar to other graph representation learning algorithms, such as GraphSAGE  and GCNs , except that it uses variational inference to learn the aggregator functions, which is further proved that makes the resulting embeddings well suited for the molecular graph generation task. The authors then introduce a probabilistic decoder that first samples the number of graph nodes from a Poisson distribution. It also samples a latent vector $\\textbf{z}_v$ per node $v\\in V$ from $\\mathcal{N}(\\textbf{0}, \\textbf{I})$. Then, for each node $v$, the decoder passes $\\textbf{z}_v$ through a neural network followed by a softmax classifier to determine node features, i.e., the atom type. Next, the total number of graph edges is sampled from a Poisson distribution parametrizes by another neural network conditioned on all latent vectors $\\textbf{Z}$. Thereafter, the decoder samples graph edges one by one from a softmax distribution among all potential edges not generated that far, and similar to CGVAE , uses a set of binary masks to guarantee some local structural and functional properties. Finally, it determines the edge type by sampling from another softmax distribution with different binary masks. These masks get updated every time the decoder generates a new edge. The model's objective is similar to that of conventional VAE-based methods plus maximizing the Poisson distribution log-likelihood, which models the number of graph nodes. Moreover, similar to JT-VAE , NeVAE utilizes Bayesian optimization over the continuous latent space to discover molecules with desirable properties.\n\\begin{table*}[ht!]\n\\begin{center}\n\\small\n\\resizebox{\\columnwidth}{!}{\n \\begin{tabular}{||l l l l l l l||}\n \\hline\n Method & Type & Input & \\makecell[l]{Generation\\\\ Strategy} & \\makecell[l]{Attention\\\\ Mechanism} & Features & \n\\makecell[l]{Conditional\\\\ Generation}\n\\\\ [0.5ex] \n \\hline\\hline\n  VGAE & AE/VAE & one single graph &All at Once& No & Node & No\\\\ \n \\hline\n GraphVAE  & VAE & dataset of graphs & All at Once& No & Node/Edge & Yes\\\\ \n \\hline\n MPGVAE  & VAE & dataset of graphs & All at Once& Yes & Node/Edge & Yes\\\\ \n \\hline\n RGVAE  & VAE & dataset of graphs & All at Once& No & Node/Edge & No\\\\\n \\hline\n Graphite   & AE/VAE & one single graph & All at Once& No & Node & No\\\\\n \\hline\n NED-VAE  & $\\beta$-VAE & dataset of graphs & All at Once& No & Node/Edge & No\\\\ \n \\hline\n DGVAE  & VAE & one single graph & All at Once & No & Node& No\\\\ \n \\hline\n JT-VAE  & VAE & dataset of graphs & Substructure-Based& No & Node/Edge & No\\\\\n \\hline\n HierVAE  & VAE & dataset of graphs & Substructure-Based& Yes & Node/Edge & Yes\\\\\n \\hline\n MHG-VAE  & VAE & dataset of graphs & Substructure-Based& No & Node/Edge & No\\\\\n \\hline\n MoleculeChef  & WAE & dataset of graphs & Substructure-Based& No & Node/Edge & No\\\\\n\\hline\n CGVAE & VAE & dataset of graphs & Node-by-Node& No & Node/Edge & No\\\\\n   \\hline\n DEFactor  & AE & dataset of graphs & Node-by-Node& No & Node/Edge & Yes\\\\\n  \\hline\nNeVAE& VAE & dataset of graphs & Node-by-Node& No & Node/Edge & No \\\\ \n\\hline\nGraphVRNN & VAE & dataset of graphs & Node-by-Node & No & Node & No \\\\ \n\\hline\nLim et al. & VAE & dataset of graphs & Node-by-Node & No & Node/Edge & Yes \\\\ \n[1ex] \n \\hline\n\\end{tabular}\n}\n\\caption{\\label{tab:autoencoder} The Main Characteristics of Autoencoder-Based Deep Graph Generators}\n\\end{center}\n\\end{table*}\n\\par\nGraphVRNN  proposes a VAE-based extension to GraphRNN  to learn the joint probability distributions of graph structure as well as the underlying node attributes by rewriting the likelihood function in Eq. (\\ref{eq:2}) as follows:\n\\begin{equation}\np(S^\\pi, X^\\pi) = \\prod_{i=1}^{n+1} p(S_i^\\pi, X_i^\\pi|S_{<i}^\\pi, X_{<i}^\\pi),\n\\end{equation}\nwhere $X \\in \\mathbb{R}^{n\\times k}$ is the attribute matrix. Then, the authors adopt an autoregressive variational autoencoder to capture the latent factors over graphs with complicated structural dependencies by optimising the lower bound of the likelihood as follows:\n\\begin{equation}\\label{eq:GraphVRNN}\n\\begin{split}\n\\mathcal{L}_{\\theta,\\phi,\\psi}(S^\\pi, X^\\pi) &=\n\\sum_i \\mathop{\\mathbb{E}_{\\textbf{z}_i\\sim q_\\psi (.)}}[\\log p_\\theta(S_i^\\pi, X_i^\\pi|S_{<i}^\\pi, X_{<i}^\\pi, \\textbf{z}_{\\leq i})]\\\\ &\n -\\beta D_{KL}(q_\\psi(\\textbf{z}_i|S_{\\leq i}^\\pi, X_{\\leq i}^\\pi)||p_\\phi(\\textbf{z}_i|S_{<i}^\\pi, X_{<i}^\\pi)),\n\\end{split}\n\\end{equation}\nwhere $q_\\psi(\\textbf{z}_i|S_{\\leq i}^\\pi, X_{\\leq i}^\\pi)$ and $p_\\phi(\\textbf{z}_i|S_{<i}^\\pi, X_{<i}^\\pi)$ are the proposal and the prior distributions in conditional VAE (CVAE)  formulation, respectively, and $D_{KL}$ is the Kullback-Leibler (KL) divergence that is tuned by the $\\beta$ hyperparameter. An overview of the GraphVRNN framework is depicted in Figure \\ref{GraphVRNN}.\n\\par\nLim et al.  utilize a combination of VAE and DeepGMG  for generating molecular graphs with desired properties containing an arbitrary input scaffold in their structure. To this purpose, the encoder uses a variant of the interaction network ,  to obtain a representation vector $h_G$ for the input graph, which will be further used to parametrize a normal distribution to sample a latent vector $\\textbf{z}$. The decoder, on the other hand, takes a scaffold $S$ as input and extends it by making sequential decisions of node and edge additions in the same way as DeepGMG, except that it also incorporates the latent vector $\\textbf{z}$ in the graph propagation process so that updating node and edge features during the generation is directly affected by $\\textbf{z}$. Moreover, the model makes it possible to conduct the process towards generating molecules with desired properties by concatenating the corresponding condition vector $C$ with $\\textbf{z}$. After the training with the VAE objective finishes, one could give a scaffold $S$ as well as a condition vector $C$ concatenated with a $\\textbf{z}$ sampled from the standard normal distribution to the decoder in order to get a generated molecule with optimized properties.", "cites": [3984, 4003, 242, 211, 3990, 3993, 227, 3981, 1652, 3988, 3996, 222, 3975, 216, 3973, 3974, 236, 229, 3995], "cite_extract_rate": 0.76, "origin_cites_number": 25, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from multiple autoencoder-based graph generation papers, particularly highlighting how node-by-node generation is implemented in models like CGVAE, DEFactor, and NeVAE. It offers a structured comparison through the table and includes some critical details, such as the use of valency masking and Bayesian optimization for chemical validity. While it identifies common patterns in generation strategies and embedding approaches, it lacks deeper evaluation of trade-offs or limitations between the methods."}}
{"id": "81107b44-cd24-49f4-be17-5a2445c81eb0", "title": "RL-Based Deep Graph Generators", "level": "section", "subsections": [], "parent_id": "bab571dd-55e6-4f4b-ac15-157a4152a738", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "RL-Based Deep Graph Generators"]], "content": "\\label{sec:rl}\nThis section provides a detailed review of generative approaches utilizing reinforcement learning algorithms to induce desired properties in the generated graphs. Table \\ref{tab:rl} gives a comparison among these methods from multiple aspects. \\par\nGCPN  proposes a stepwise approach for molecular graph generation, formulating the problem as a Markov Decision Process to train an RL agent in a chemistry-aware environment. Thus, at each generation step $t$, the method first takes the intermediate graph $G_t$ and the set of scaffolds $C$ as input and computes the state $s_t$ by applying a GCN variant that supports multiple edge types. It then samples an action $a_t$ from the policy $\\pi_\\theta$ based on the obtained node embeddings, which can be either to add a new scaffold subgraph or connect two nodes already in the graph. Next, the action will be further processed by the state transition dynamics, and if it violates chemical rules, it will be rejected so that the state stays unchanged. After that, GCPN utilizes two types of rewards to guide the RL agent, namely, intermediate and final rewards, where the former consists of a stepwise validity reward that encourages the generation process to obey chemical valency rules, and an adversarial reward, which employs the GAN framework  to ensure similarity between real molecules and those to be generated. On the other hand, the final reward includes a domain-specific reward for molecular property optimization and a similar adversarial reward. Finally, the authors adopt Proximal Policy Optimization (PPO)  to optimize the policy network parameters. An overview of the method is depicted in Figure \\ref{gcpn}, where each row corresponds to one step in the generation process. \n\\begin{figure*}[t!]\n\\centering\n\\includegraphics[width=0.85\\textwidth]{figures/GCPN}\n\\vspace*{-0.2cm}\n\\centering\\caption{An overview of GCPN  (reprinted with permission).}\n\\label{gcpn}\n\\end{figure*}\n\\par\nSubsequently, several methods propose extensions to GCPN. For example, Shi et al.  utilize a combination of general semantic features extracted from the SMILES representations of molecules and their graph representations in order to form more comprehensive states during the generation process. To this end, the authors propose an architecture consisting of a SMILES encoder and an action generator. First, the encoder obtains context vector $z$ from an input SMILES string, which will be further processed by two attention mechanisms, namely action-attention and graph-attention, to get the enhanced context vector $\\tilde{z}$. Then, the model concatenates the current graph state $s_t$ with $\\tilde{z}$ to pass a heterogeneous state into the action generator that has the same generation mechanism as GCPN, except that it does not involve adversarial rewards. Furthermore, the model is trained in two stages. The supervised learning stage learns an initialization for the model parameters to alleviate the instability of an RL agent training by minimizing the following objective function:\n\\begin{equation}\nJ =-\\frac{1}{M}\\sum_{m=1}^M \\log \\frac{1}{N} \\sum_{n=1}^N\\sum_{t}\\log P(a_t|\\tilde{z}, s_t)+D_{KL}(P_z||P_0),\n\\end{equation}\nwhere $M$ is the number of molecules in the training dataset, $N$ denodes the number of sampled trajectories for generating each molecule, and $P_z$ and $P_0$ are the distribution of the learned context vector and a prior distribution, respectively. Afterwards, the reinforcement learning stage further optimizes the process towards generating molecular graphs with desired properties. Figure \\ref{fig:Shi} provides an overview of this framework.\\par\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.7\\textwidth]{figures/Shi}\n\\vspace*{-1cm}\n\\begin{center}\n\\caption{An overview of the framework proposed by Shi et al. . In the supervised learning phase, only the part in the gray box is trained. On the other hand, the whole architecture is involved in the reinforcement learning stage.}\n\\label{fig:Shi}\n\\end{center}\n\\end{figure}\nKarimi et al.  propose another extension to GCPN for drug-combination design, which is a key part of combination therapy. To this end, the authors first develop Hierarchical Variational Graph Auto-Encoders (HVGAE) to embed prior knowledge such as gene-gene, gene-disease, and disease-disease networks to acquire more accurate disease representations. Then, they formulate the problem as generating a set of graphs $\\mathcal{G}=\\{G^{(k)}\\}_{k=1}^K$  conditioned on the learned disease representations by employing a similar generation strategy as GCPN, with the difference that in addition to chemical validity and adversarial rewards, they design a reward to encourage generating disease-specific drug combinations.\\par\nMore recently, DeepGraphMolGen  combines GCPN with a molecular property prediction network implemented as a GCN followed by a feedforward layer, which provides GCPN with an extra chemical reward to tilt the process towards generating molecules with additional property, i.e., binding potency to dopamine transporters.\n\\begin{table*}[ht!]\n\\begin{center}\n\\small\n \\begin{tabular}{||m{0.15\\textwidth} l l l l m{0.12\\textwidth} m{0.23\\textwidth}||}\n \\hline\n Method & \\makecell[l]{Generation \\\\ Strategy} & \\makecell[l]{Attention\\\\ Mechanism}& Features & \\makecell[l]{Conditional\\\\ Generation} & Action & Reward\\\\ [0.5ex] \n \\hline\\hline\n GCPN  & Sequential & No & Node/Edge & No & Link prediction & Domain-specific + GAN\\\\ \n \\hline\n Shi et al. & Sequential & Yes & Node/Edge & No & Link prediction & Domain-specific\\\\ \n \\hline\n Karimi et al.  & Sequential & Yes & Node/Edge & Yes & Link prediction & Domain-specific + GAN + A reward for drug combinations\\\\ \n \\hline\n DeepGraphMolGen & Sequential & No & Node/Edge & No & Link prediction & Domain-specific + GAN + Property reward (by the property prediction network)\\\\ \n \\hline\n GraphOpt  & Sequential & No & Node/Edge & No & Link prediction & Learning a reward function via inverse reinforcement learning\\\\ \n \\hline\n MNCE-RL  & Sequential & No & Node/Edge & No & Production rule selection & Domain-specific + A reward regarding the number of generation steps\\\\ \n \\hline\n GEGL  & Sequential & No & Node/Edge & No & Generating a molecule & Domain-specific\\\\ \n[1ex] \n \\hline\n\\end{tabular}\n\\caption{\\label{tab:rl} The Main Characteristics of RL-Based Deep Graph Generators}\n\\end{center}\n\\end{table*}\n\\par\nBesides GCPN and its subsequent approaches, there exist other RL-based methods that generate graph structures by taking different strategies. GraphOpt  models graph formation via a Markov Decision Process, aiming to learn both a graph construction procedure $\\Pi$ and a  usually unknown latent objective function $\\mathcal{F}:G\\rightarrow \\mathbb{R}$ that reflects the underlying graph formation mechanism. Therefore, inspired by , the authors formulate the following objective:\n\\begin{equation}\n\\begin{split}\n\\Pi^* &= \\mathop{\\text{argmin}}_{\\Pi} \\mathop{\\max}_{\\mathcal{F}}[\\mathcal{F}(G)-\\mathcal{F}(\\Pi(V))],\\\\\n\\mathcal{F}_{opt} &= \\mathop{\\text{argmax}}_{\\mathcal{F}} [\\mathcal{F}(G)-\\mathcal{F}(\\Pi^*(V))],\n\\end{split}\n\\end{equation}\nwhere $\\mathcal{F}_{opt}$ assigns the highest score to the observed graphs compared to all other ones, and optimization over $\\mathcal{F}$ is, in fact, a search for the reward function via inverse reinforcement learning (IRL) . In other words, GraphOpt learns a reward function, which is in contrast to most of the RL frameworks that utilize an existing one. The optimal construction procedure, on the other hand, tries to construct a graph $G' = \\Pi^*(V)$ in a sequential link formation process given node-set $V$, which is expected to be the most similar graph to the observed one using $\\mathcal{F}$ as the similarity measure. To this end, the authors propose a continuous latent action space to infer a link formation action $a_t$ at each time step $t$ by first sampling two vectors $a^{(1)}$ and $a^{(2)}$ from a normal distribution parametrized based on the current graph state $s_t$, which is computed by a GNN . They then choose two graph nodes with the most similar embeddings to the obtained vectors to construct an edge.\\par \nMNCE-RL  proposes a graph convolutional policy network with a novel GCN architecture for generating molecules with optimized properties, which, similar to MHG-VAE, utilizes grammar rules to guarantee the validity of molecules. To this end, the authors first extend the NCE graph grammar  to make it applicable for generating molecules. They then infer the production rules of the grammar from a set of input molecules. Next, the RL-based generation process starts whose action space consists of the set of legal production rules, and at each step, the policy samples a rule based on the node features obtained by applying the proposed GCN on the intermediate graph. A domain-specific reward guides the process towards generating desirable molecules. Moreover, MNCE-RL assigns a negative reward when the number of steps exceeds a threshold to avoid prolonging the generating process. \\par\nGEGL  proposes to incline a deep neural network called neural apprentice policy towards generating molecules with desired properties. In this respect, the apprentice policy first generates a set of molecules by a SMILES-based LSTM and stores them into a fixed-size max-reward priority queue $\\mathcal{Q}$. Then, a genetic expert policy utilizes the content of $\\mathcal{Q}$ as seed molecules and applies two genetic operators, namely, the graph-based mutation and crossover , to them and stores the generated molecules in another priority queue denoted by $\\mathcal{Q}_{ex}$. After generating each sample, both $\\mathcal{Q}$ and $\\mathcal{Q}_{ex}$ are updated so that they always contain molecules with the highest rewards. Next, the apprentice policy updates its model's parameters by learning to imitate the molecules stored in $\\mathcal{Q}\\cup\\mathcal{Q}_{ex}$, and the whole procedure repeats iteratively. This way, the expert policy guides the apprentice policy to generate molecules with preferred properties.", "cites": [3974, 264, 3971, 3998, 4004, 3980, 3977, 2219], "cite_extract_rate": 0.5333333333333333, "origin_cites_number": 15, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple RL-based graph generation papers, particularly focusing on how they extend GCPN and differ in reward mechanisms and generation strategies. It provides a comparative table to summarize key features. However, it lacks deeper critical analysis of limitations or trade-offs between methods, and while it identifies some patterns (e.g., use of domain-specific rewards), it does not generalize to a meta-level framework or insight."}}
{"id": "302dd969-b32a-4095-82da-6251d85cb9b7", "title": "Random Walk-Based Methods", "level": "subsection", "subsections": [], "parent_id": "ec591b09-d8a8-426d-8472-4197b6521c2b", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Adversarial Deep Graph Generators"], ["subsection", "Random Walk-Based Methods"]], "content": "A series of works focus on generating random walks instead of the entire graph, as graph random walks are invariant under node reordering. In this respect, NetGAN  introduces the first implicit generative model for graphs that learns the distribution of biased random walks over a single graph using the WGAN framework . In particular, NetGAN first samples a collection of random walks using the biased second-order strategy  to prepare the models training data. Then, the generator learns to sequentially generate random walks node-by-node using the LSTM  architecture, which is initialized by a latent vector $\\textbf{z}$ sampled from a standard normal distribution. Meanwhile, the discriminator decides whether a random walk is real or not after processing its entire node sequence by another LSTM. After training finishes, the authors construct the adjacency matrix of a new graph using multiple generated random walks. Further to this, a number of generative approaches have been proposed inspired by the idea of NetGAN or extending it. For example, STGGAN  adopts a similar generating scheme for spatial-temporal graphs.\\par\nMMGAN  generalizes NetGAN to capture higher-order connectivity patterns by introducing multiple types of random walks, each biased towards different motif structures. To simplify the process, MMGAN focuses only on 3-node motifs and proposes an architecture consisting of three GANs, namely, NetGAN that considers pairwise relationships, and two other motif-based GANs. The random walks generated by each of the three GANs are then combined to construct the output graph.\\par\n{\\large S}HADOW{\\large C}AST  proposes another extension to NetGAN in order to make the generation process controllable, which can be considered as a step towards generating graphs with more explainable properties. To this end, the authors first define a graph called $shadow$ with the same structure as the original one but with different node labels so that these node-level properties can control the generation process. Then, they expand the architecture of NetGAN by adding a sequence-to-sequence model called shadow caster, which is implemented by an LSTM . Specifically, the shadow caster takes in sampled walks from the shadow network and generates synthetic shadow walks of preferred distribution to control the generation process. Next, these model-generated shadow walks are fed into both generator and discriminator as conditions, which are finally trained using the conditional GAN  framework.", "cites": [529, 7009, 1010], "cite_extract_rate": 0.375, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of random walk-based adversarial graph generation methods, particularly focusing on NetGAN and its extensions (MMGAN, SHADOWCAST). It integrates information from the cited papers to explain the general idea and variations of the approach, showing some synthesis. However, it lacks critical evaluation of the methods' strengths, weaknesses, or comparative performance. The abstraction level is limited, as it primarily describes specific techniques without identifying broader patterns or principles in adversarial graph generation."}}
{"id": "0338a992-7715-47a4-a857-8f433f93abf9", "title": "Graph-Based Methods", "level": "subsection", "subsections": ["94895d9e-d012-4398-85ea-5616b749d900", "e55da7b3-c6d7-4391-968d-7ba7ead301cf"], "parent_id": "ec591b09-d8a8-426d-8472-4197b6521c2b", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Adversarial Deep Graph Generators"], ["subsection", "Graph-Based Methods"]], "content": "\\begin{table*}[ht!]\n\\begin{center}\n\\small\n \\begin{tabular}{||l l l l l l||}\n \\hline\n Method & Input & \\makecell[l]{Generation\\\\ Strategy}& \\makecell[l]{Attention\\\\ Mechanism}& Features& \\makecell[l]{Conditional\\\\ Generation}\\\\ [0.5ex] \n \\hline\\hline\n NetGAN  & one single graph & Sequential & No & - & No\\\\ \n \\hline\n MMGAN  & one single graph & Sequential & No & - & No\\\\\n \\hline\n {\\large S}HADOW{\\large C}AST  & one single graph & Sequential & No & Node & Yes\\\\\n \\hline\n MolGAN  & dataset of graphs & All at Once & No & Node/Edge & No\\\\ \n \\hline\n CONDGEN  & dataset of graphs & All at Once & No & - & Yes\\\\\n \\hline\n TSGG-GAN  & dataset of graphs & All at Once & No & Node & Yes\\\\\n \\hline\n VJTNN + GAN  & dataset of graphs & Sequential & Yes & Node/Edge & No\\\\\n \\hline\nMol-CycleGAN  & dataset of graphs & Sequential & No & Node/Edge & No\\\\\n \\hline\n Misc-GAN  & one single graph & Not mentioned & No & - & No\\\\\n \\hline\n\\end{tabular}\n\\caption{\\label{tab:adversarial} The Main Characteristics of Adversarial Deep Graph Generators}\n\\end{center}\n\\end{table*}\nUnlike random walk-based approaches, most of the existing adversarial graph generators deal with the entire graph. Here, we study these methods in two categories.", "cites": [7158, 7214, 7009], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 9, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a tabular comparison of adversarial deep graph generation methods, listing their characteristics such as input type, generation strategy, and whether they support conditional generation. While it organizes the information from the cited papers in a structured and consistent manner, it lacks deeper synthesis or analysis of the relationships between methods. There is little discussion of broader patterns or critical evaluation of their strengths and limitations."}}
{"id": "94895d9e-d012-4398-85ea-5616b749d900", "title": "General Graph-Based Adversarial DGGs", "level": "subsubsection", "subsections": [], "parent_id": "0338a992-7715-47a4-a857-8f433f93abf9", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Adversarial Deep Graph Generators"], ["subsection", "Graph-Based Methods"], ["subsubsection", "General Graph-Based Adversarial DGGs"]], "content": "MolGAN  proposes the first implicit generative model for small molecular graphs. In this respect, its generator first takes a latent vector \\textbf{z} sampled from $\\mathcal{N}(0, I)$. Then, it outputs a probabilistic graph all at once using an MLP in a way similar to GraphVAE , which, as a consequence, limits the model to generate graphs of a predefined maximum size. However, in contrast to GraphVAE, MolGAN does not need to perform an expensive graph matching algorithm, as it makes the model likelihood-free using the GAN framework. Next, a permutation-invariant discriminator tries to distinguish between generated graphs and real ones using a combination of the Relational-GCN  and an MLP. The authors train the discriminator using the WGAN  objective, while they combine a reinforcement learning objective with that of the WGAN to train the generator, aiming at inclining the process towards generating molecules with desired chemical properties. More precisely, the authors employ a deterministic policy gradient algorithm, namely, DDPG , to maximize the reward, which is approximated by a reward network with the same architecture as the discriminator.  The overall architecture of MolGAN is shown in Figure \\ref{fig:MolGAN}.\\par \nLGGAN  adopts a similar generator to that of MolGAN. However, its discriminator uses JK-Net  to compute graph embeddings and outputs both the graph label and the probability of the graph being real. Moreover, to incorporate the class information, the authors utilize the AC-GAN  framework.\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.75\\textwidth]{figures/MolGAN}\n\\vspace*{-1cm}\n\\begin{center}\n\\caption{An overview of MolGAN  (reprinted with permission).}\n\\label{fig:MolGAN}\n\\end{center}\n\\end{figure}\\par\nCONDGEN  proposes a model of graph variational generative adversarial nets for conditional structure generation. It addresses both the challenges of permutation-invariance and context-structure conditioning indicated by the information of attributes or labels in the networks. The method first applies the trick of latent space conjugation to the base VGAE  model in order to convert its node-level encoding into a permutation-invariant graph-level one that allows learning from a dataset of graphs with variable sizes, which is a notable improvement over the VGAE. More precisely, $\\mu$ and $\\sigma$ in Eq. (\\ref{VGAE-1}) are replaced by the following parameters:\n\\begin{equation}\nq(\\textbf{z}_{i}|X,A)=\\mathcal{N}(\\bar{\\textbf{z}}|\\bar{\\mu},\\text{diag}(\\bar{\\sigma}^2)),\n\\end{equation}\nwhere $\\bar{\\mu} =\\frac{1}{n} \\sum_{i=1}^n g_\\mu (X,A)_i$ and $\\bar{\\sigma}^2 =\\frac{1}{n^2} \\sum_{i=1}^n g_\\sigma (X,A)_i^2$. However, the process is still not completely permutation-invariant because the reconstruction loss of the VGAE is computed between the generated adjacency matrix $A'$ and the original matrix $A$, which may be under different node permutations. Therefore, the authors propose a GCN-based discriminator to enforce the structural similarity between the generated and the true adjacency matrices and learn its parameters by optimizing the GAN objective. Thus, the encodings $\\text{GCN}_D(A)$ and $\\text{GCN}_D(A')$ computed by the discriminator, become permutation-invariant and the reconstruction loss can be computed as $||\\text{GCN}_D(A) - \\text{GCN}_D(A')||^2_2$. Moreover, according to , the authors use the concatenation of condition vector $ C $ and latent variable \\textbf{Z} to enable conditional structure generation. Then, motivated by CycleGAN , they further enforce mapping consistency between the graph context and the structure spaces by sharing the parameters in the two GCN networks, namely, the GCNs in the graph encoder and the discriminator.\n\\par\nMore recently, TSGG-GAN  adopts a time series conditioned generative model that aims to generate a graph given an input multivariate time series, where each time series acts as context information associated with one of the graph nodes. This is particularly the case when it is straightforward to obtain node-level information, while the underlying network is totally unknown. To this end, using SRU  to extract the information of the time series followed by an MLP, the generator generates the entire graph all at once. At the same time, the discriminator takes a pair of a multivariate time series and a graph as inputs, which are then processed using SRU and GCN, respectively. Thereafter, the discriminator utilizes Neural Tensor Networks (NTN)  to measure the similarity between the time series and the graph and decides whether the graph is real or not.", "cites": [7214, 259, 529, 236, 7022, 229, 4005, 1001, 4006], "cite_extract_rate": 0.6, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key components from multiple adversarial graph generation papers, highlighting how they build upon or differ from prior models like GraphVAE. It offers some critical evaluation, such as noting limitations in permutation-invariance and how certain techniques address them. While it identifies some broader patterns (e.g., the use of GANs to avoid graph matching), the abstraction remains somewhat constrained to the specific models discussed."}}
{"id": "e55da7b3-c6d7-4391-968d-7ba7ead301cf", "title": "Graph-to-Graph Translators", "level": "subsubsection", "subsections": [], "parent_id": "0338a992-7715-47a4-a857-8f433f93abf9", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Adversarial Deep Graph Generators"], ["subsection", "Graph-Based Methods"], ["subsubsection", "Graph-to-Graph Translators"]], "content": "In addition to the aforementioned methods, there exist other approaches trying to generate a new graph based on an initial one. VJTNN  proposes a graph-to-graph translation model that learns a mapping from a source molecular graph $X$ to a target graph $Y$ with enhanced chemical properties by utilizing a similar encoder-decoder architecture as JT-VAE  whose tree decoding process is further enriched by adding an attention mechanism. More specifically, VJTNN augments the basic encoder-decoder model with latent code \\textbf{z} derived based on the embeddings of both source and target graphs and minimizes the conditional VAE loss function to learn the mapping $F : (X, \\textbf{z}) \\rightarrow Y$. The authors then propose an adversarial variation called VJTNN  + GAN to force generated graphs to follow the distribution of the target ones, which is trained using the WGAN framework .\\par\nMol-CycleGAN  establishes structural similarity between the source and target molecular graphs by adopting a CycleGAN-based  approach. To this end, the method first computes the latent space embeddings for $X$ and $Y$ using JT-VAE  and then learns the transformation $F : X \\rightarrow Y$ ( and its reverse, i.e., $G : Y \\rightarrow X$) in that space. Mol-CycleGAN also introduces the discriminator $D_X$ (and $D_Y$) to decide whether a sample is from the distribution of $X$ (or $Y$) or it is generated by $G$ (or $F$). The model parameters are trained by optimizing the following loss function:\n\\begin{equation}\n\\begin{split}\n\\mathcal{L}(F, G, D_X, D_Y)=&\\mathcal{L}_{GAN}(F, D_Y, X, Y)+\\mathcal{L}_{GAN}(G, D_X, Y, X)\\\\\n&+\\lambda_1\\mathcal{L}_{cyc}(F, G) + \\lambda_2\\mathcal{L}_{identity}(F, G),\n\\end{split}\n\\end{equation}\nwhere the authors utilize the adversarial loss of LS-GAN  and the similar $\\mathcal{L}_{cyc}(F, G)$ and $\\mathcal{L}_{identity}(F, G)$ as CycleGAN, where the former reduces the space of mapping functions and acts as a regularizer, while the latter makes the generated molecule not to be structurally far away from the original one. After the training finishes, Mol-CycleGAN takes a molecule $X$ as input and calculates its embedding by applying the encoder of the JT-VAE. Then, $F(X)$ computes an embedding corresponding to a molecule with desired properties that is also structurally similar to $X$. Finally, the model generates the optimized molecular graph $Y$ using the JT-VAE's decoder.\\par\nMisc-GAN  proposes another translation model inspired by CycleGAN  to learn a mapping function $F$ from a source graph $G_s$ to its corresponding target graph $G_t$ while preserving the hierarchical graph structures (i.e., the community structures) in the target graph in different levels of granularity. The model training consists of three stages: First, it constructs coarser graphs in $ L $ granularity levels based on an input target graph $G_t$. Then, at each level $l$, it trains an independent CycleGAN-based generative model from $G_s$ to $G_t^{(l)}$. Finally, all generated graphs $\\tilde{G}_t^{(l)}$ are aggregated together to form the reconstructed target graph $\\tilde{G}_t$. The framework is trained by minimizing the following loss function:\n\\begin{equation}\n\\mathcal{L} = \\mathcal{L}_{ms} + \\mathcal{L}_F + \\mathcal{L}_G + \\mathcal{L}_{cyc},\n\\end{equation}\nwhere $\\mathcal{L}_{ms}$ is the multi-scale reconstruction loss between the target graph $G_t$ and the generated graph $\\tilde{G}_t$, $\\mathcal{L}_F$ is the forward adversarial loss for learning a mapping from the source to the target graph, $\\mathcal{L}_G$ is the backward adversarial loss to learn the reverse mapping, and $\\mathcal{L}_{cyc}$ is the cycle consistency loss .", "cites": [7158, 63, 7022, 1652], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes two adversarial graph-to-graph translation approaches (VJTNN + GAN and Mol-CycleGAN) by connecting their shared encoder-decoder and CycleGAN-based architectures to the broader theme of structural similarity and property optimization. It provides some critical insights, such as the role of cycle consistency and identity losses in maintaining graph structure, but does not deeply evaluate or critique their limitations. The abstraction is moderate, as it highlights the importance of preserving structural properties across translations and the use of adversarial training for distribution alignment, though it stops short of proposing a unifying framework."}}
{"id": "d450729c-c84e-45d9-9617-3641e5036f26", "title": "Flow-based Deep Graph Generators", "level": "section", "subsections": [], "parent_id": "bab571dd-55e6-4f4b-ac15-157a4152a738", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Flow-based Deep Graph Generators"]], "content": "\\label{sec:flow}\n\\begin{table*}[ht!]\n\\begin{center}\n\\small\n \\begin{tabular}{||l l l l l l||}\n \\hline\n Method & Category & \\makecell[l]{Generation\\\\ Strategy} & \\makecell[l]{Attention\\\\ Mechanism} & Features & \\makecell[l]{Conditional\\\\ Generation}\\\\ [0.5ex] \n \\hline\\hline\n GNF  & Autoencoder-based & All at Once & Yes& Node/Edge& No\\\\ \n\\hline\nGraphNVP  & - & All at Once & No& Node/Edge& No\\\\ \n\\hline\nGraphAF  & Autoregressive & Node-by-node & No& Node/Edge& No\\\\ \n\\hline\nGrAD  & Autoregressive & Block of nodes & Yes& -& No\\\\ \n[1ex] \n \\hline\n\\end{tabular}\n\\caption{\\label{tab:flow-based} The Main Characteristics of Flow-Based Deep Graph Generators}\n\\end{center}\n\\end{table*}\nIn addition to the methods we have discussed so far, a line of research has recently emerged, which employs flow-based approaches in the field of graph generation. For example, GNF  develops a generative model of graphs by combining normalizing flows with a graph auto-encoder. More specifically, the authors first train a permutation invariant graph auto-encoder that encodes an input graph to a set of node features $X\\in \\mathbb{R}^{n\\times k}$ using a standard GNN. Then, a simple decoder outputs a probabilistic adjacency matrix $\\hat{A}$, in which edge probability between two arbitrary nodes $i$ and $j$ with embedding vectors $x_i$ and $x_j$ is computed as follows:\n\\begin{equation}\n\\hat{A}_{ij}=\\frac{1}{1+\\exp(C(||x_i-x_j||^2_2-1))},\n\\end{equation}\nwhere C is a temperature hyperparameter. After the auto-encoder training completes, the encoder is employed to compute node features X to be used as training input for the GNF. Then, the GNF, which is based on non-volume preserving flows , learns a mapping from the complicated graph distribution into a latent distribution that is well modelled as a Gaussian. At inference time, GNF generates node features by first sampling $Z \\sim \\mathcal{N} (0, I)$ from the latent space followed by applying the inverse mapping, $X = f^{-1}(Z)$ which is then fed into the decoder to get the predicted adjacency matrix as illustrated in Figure \\ref{fig:GNF}. GraphNVP  takes a similar approach, but rather than pretraining an auto-encoder to get continuous node features, the authors propose to perform Dequantization ,  by adding uniform noise to the discrete adjacency tensor as well as the node label matrix. More precisely, GraphNVP proposes a two-step generation scheme by learning two latent representations for each graph, one for the adjacency tensor and the other for node labels. \n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.55\\textwidth]{figures/GNF}\n\\vspace*{-1cm}\n\\begin{center}\n\\caption{The framework of GNF  for the graph generation (reprinted with permission).}\n\\label{fig:GNF}\n\\end{center}\n\\end{figure}\n\\par\nIn addition to the flow-based methods we have studied so far that generate the whole graph in one step, there are other approaches adopting autoregressive generation strategies. For example, GraphAF  proposes to generate molecular graphs by combining the advantages of both autoregressive and flow-based models. The method first converts a molecular graph structure $G = (A, C)$, where both node type matrix $C$ and the adjacency matrix $A$ are discrete, into continuous data $G' = (A', C')$ using Dequantization technique ,  in order to make the data usable for a flow-based model. Then, conditional distributions for the $i$-th generation step are defined according to Autoregressive Flows (AF)  as follows:\n\\begin{equation}\\label{GraphA-1}\n\\begin{split}\n&p(C'_i|G_i) =\\mathcal{N}(\\mu^C_i, (\\alpha_i^C)^2)\\\\\n&p(A'_{ij}|G_i, C_i, A_{i, 1:j-1}) =\\mathcal{N}(\\mu^A_{ij}, (\\alpha_{ij}^A)^2),\n\\end{split}\n\\end{equation}\nwhere $G_i$ is the current sub-graph, $\\mu^C_i$, $\\alpha_i^C$, and $\\mu^A_{ij}$, $\\alpha_{ij}^A$ are the means and standard deviations of Gaussian distributions, which are computed by different neural networks based on node embeddings of the sub-graph generated so far. To calculate the exact likelihood, an invertible mapping from the molecule structures $G' = (A', C')$ to latent Gaussian space \\textbf{z} is defined as:\n\\begin{equation}\nz_i=(C'_i-\\mu^C_i)\\odot\\frac{1}{\\alpha_i^C}  ,\\ z_{ij}= (A'_{ij}-\\mu^A_{ij})\\odot\\frac{1}{\\alpha_{ij}^A},\n\\end{equation}\nwhere $\\frac{1}{\\alpha_i^C}$ and $\\frac{1}{\\alpha_{ij}^A}$ denote element-wise reciprocals of $\\alpha_i^C$ and $\\alpha_{ij}^A$, respectively and $\\odot$ is the element-wise multiplication. At inference time, GraphAF just samples random variables $z_i$ and $z_{ij}$ from the latent Gaussian space and converts them to the molecule structures as in Eq. (\\ref{GraphA-2}) to generate new graphs in an autoregressive manner:\n\\begin{equation}\\label{GraphA-2}\nC'_i=z_i\\odot \\alpha_i^C+\\mu^C_i\\ ,\\ A'_{ij}=z_{ij}\\odot \\alpha_{ij}^A+\\mu^A_{ij}.\n\\end{equation}\nGraphAF further proposes a valency-based rejection sampling similar to MolecularRNN  to guarantee the validity of generated molecules. The authors also propose to fine-tune the generation process with reinforcement learning to generate molecules with optimized properties.\n\\par\nMore recently, GrAD  proposes another autoregressive flow-based approach for graph generation, which can also be considered as a variant of GRAN . In particular, its training consists of two stages. Firstly, for generating each new block of $B$ nodes in the $t$-th step, the model samples latent codes $H_{b_t}\\in \\mathbb{R}^{B\\times k}$ to initialize the corresponding node representations. Then, different from GRAN's formulation in Eq. (\\ref{GRAN-GNN}), the node features get updated using graph attention layers almost according to , except that self-attention weights are calculated only based on each node's neighborhood to inject structural information of the currently generated graph into the updating process. After node representations are obtained, the model follows a similar generation strategy as GRAN and jointly optimizes the generator's parameters and the distribution of latent codes. In the second training stage, GrAD trains a flow-based reversible model to map samples from the optimized distribution of latent codes to a simple Gaussian distribution. Therefore, at the inference time, one can first sample a batch of $B$ vectors $Z\\in \\mathbb{R}^{B\\times k}$ from a Gaussian base distribution and apply the inverse mapping $H_{b_t}=f^{-1}(Z)$ to obtain initial node representations and then go through the generation process.\\par\nTable \\ref{tab:flow-based} summarizes the main characteristics of the flow-based DGGs.", "cites": [3284, 3991, 7763, 4008, 3983, 3997, 4000, 8709, 4007, 38], "cite_extract_rate": 1.0, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple flow-based graph generation papers, including GNF, GraphNVP, and GraphAF, and connects them with related concepts like dequantization and autoregressive flows. It integrates these ideas to form a coherent narrative about the flow-based category, though the critical evaluation is limited to brief mentions of techniques like rejection sampling. The abstraction is moderate, with some recognition of patterns such as the use of invertible mappings and latent representations."}}
{"id": "3ed9337d-a321-4a13-9b79-65cd7343f747", "title": "Molecular Graph Generation", "level": "subsection", "subsections": [], "parent_id": "117439f4-e4a0-47bc-aab6-5d03dcf6610b", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Applications"], ["subsection", "Molecular Graph Generation"]], "content": "The molecule generation approaches aim to downsize the high-dimensional chemical space to expedite drug design and material discovery. Since molecules can be considered as graphs, where the atoms form the graph's node-set and the chemical bonds determine how those nodes connect, the most widely explored application of modern deep graph generative methods is generating molecular structures, a problem that has been previously mostly formulated as producing SMILES strings. Using graph generators instead of SMILES based models results in generating more valid intermediate substructures compared to mostly meaningless partially generated substrings. It also allows better capturing the similarities between molecules as molecules with similar structures can have totally different SMILES encodings.\\par \nDeep molecular graph generation approaches proposed so far utilize various frameworks, from VAEs and  GANs to RL-based , autoregressive  and flow-based frameworks . They also adopt different generation strategies at varying granularity levels. For example, some of them generate the whole graph all at once , while others add one atom at a time  or use valid chemical substructures as their building blocks .\\par \nMoreover, in molecular graph generation, two challenges must be taken into account. First, the generated molecules must satisfy the explicitly specified validity constraints, i.e., an atom's chemical bonds should not exceed its valence. The graph generator models proposed so far address the issue by employing different mechanisms, including introducing structural penalties during the training , adopting valency-based rejection sampling at the inference time , utilizing a grammar-based approach , adding regularization terms to the objective function, using valid chemical substructures as building blocks, and employing valency masking mechanism. The second challenge to be considered is that new molecular structures should obey some desired properties. This problem has also been addressed by taking various strategies including minimizing a distance  or utilizing Bayesian optimization  in some continuous latent space, maximizing a domain-specific reward in RL-based approaches , or performing the generation given an input molecule with desired properties and try to preserve those properties in the target molecule .", "cites": [3979, 3984, 8335, 3971, 3990, 8709, 3993, 4010, 227, 7160, 1652, 258, 222, 7158, 264, 3991, 3975, 4009, 3973, 3974, 7214, 3987, 236, 3983, 4000], "cite_extract_rate": 0.9259259259259259, "origin_cites_number": 27, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes a wide range of molecular graph generation techniques, connecting ideas across multiple papers to present a coherent narrative on frameworks and strategies. It abstracts key challenges (validity and property optimization) and discusses various mechanisms to address them, offering a high-level understanding. While it does not deeply critique specific papers, it identifies limitations and discusses common approaches, indicating a balanced analytical perspective."}}
{"id": "9f56295d-adee-459b-a500-693817ea82f7", "title": "Non-Molecular Graph Generation", "level": "subsection", "subsections": [], "parent_id": "117439f4-e4a0-47bc-aab6-5d03dcf6610b", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Applications"], ["subsection", "Non-Molecular Graph Generation"]], "content": "Although the most remarkable application of modern graph generation approaches explored so far is generating molecular structures, several other non-application-specific approaches have been proposed  working on more general datasets. While these methods' ultimate goal is to be used on real-world applications such as generating social network graphs, most of them suffer from scalability issues. Thus, they are currently being applied to synthetic or relatively small real datasets. Despite the steps taken towards making these models more scalable , it should be specifically considered as future work.", "cites": [8710, 7763, 3988, 3997, 3996, 3976], "cite_extract_rate": 0.75, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the cited papers by identifying a common theme of scalability challenges in non-molecular graph generation, and ties together different methods (e.g., autoregressive, RNN-based, and auto-decoder approaches) under this theme. It provides a critical perspective by pointing out the current limitations of these methods in handling large-scale real-world graphs. However, it stops short of offering deeper comparative analysis or meta-level insights that would elevate it to a high insight level."}}
{"id": "b50016b5-c407-400c-ab93-8b44f7d17b10", "title": "Language-Based Graph Generation", "level": "subsubsection", "subsections": [], "parent_id": "b7af38ed-4ce0-4703-84b9-4a9be3654d1c", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Applications"], ["subsection", "Future Applications"], ["subsubsection", "Language-Based Graph Generation"]], "content": "In natural language processing, several approaches have been proposed to extract rich graph-structured information from textual data. They include methods aiming to extract AMRs (Abstract Meaning Representations) , semantic graphs, semantic dependency graphs, and even those that transform one graph into another based on some input sentences. However, most existing methods often propose some domain-specific procedures to produce these graph-structured knowledge representations, which, as a result, are not capable enough to consider various effective factors. Therefore, as a future orientation, the community can solve such problems with a conditional generative approach, making it possible to generate graphs from their corresponding distribution given the specified textual input.", "cites": [4012, 1171, 276, 1122, 4011], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview by identifying a common limitation in existing methodsreliance on domain-specific proceduresand proposes a conditional generative approach as a future direction. It synthesizes information from multiple papers related to semantic parsing and AMR generation, though it could offer deeper integration or a more novel framework. The critique is constructive but not highly nuanced, and the abstraction generalizes the problem to a broader trend in the field without reaching a meta-level synthesis."}}
{"id": "b033e0f6-252c-492f-86a4-b8baf5b12de1", "title": "Scene Graph Generation", "level": "subsubsection", "subsections": [], "parent_id": "b7af38ed-4ce0-4703-84b9-4a9be3654d1c", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Applications"], ["subsection", "Future Applications"], ["subsubsection", "Scene Graph Generation"]], "content": "Scene graphs are structured representations of images providing higher-level knowledge for scene understanding, where the objects in each image form the node-set of the corresponding scene graph, and the relationships between objects determine how the nodes connect. As this class of graphs has a wide range of applications, including image captioning, visual question answering, image retrieval, and image generation , scene graph generation becomes a line of research in recent years. \\par\nMost scene graph generation methods first detect objects from an input image using object detection models like Faster R-CNN  to form the set of graph nodes. Meanwhile, the relationships can be extracted either jointly with the objects  or after all the objects are detected . Among these approaches, some generate scene graphs solely based on input images , while others benefit from additional text input , or some self-generated extra information . Moreover, as the number of objects increases, some models  propose to first generate multiple subgraphs and then aggregate them to construct the complete scene graph to address the scalability issue.\\par \nHere, we have briefly reviewed and categorized some of the scene graph generation methods. However, they are more of a relational information extractor from images rather than graph generators, which estimate the underlying data distribution. Therefore, it can be explored in the future.", "cites": [4014, 4013, 209, 4017, 4015, 8714, 271, 284, 4016, 267], "cite_extract_rate": 0.9090909090909091, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple cited papers by grouping them into general categories such as object detection, relationship extraction, and scalability strategies. It provides a coherent overview of scene graph generation approaches. While it offers some abstraction by highlighting common patterns (e.g., subgraph aggregation for scalability), the critical analysis is limited to pointing out that existing methods are more relational extractors than true graph generators, without deeper evaluation of strengths or weaknesses. Overall, the section is analytical but lacks a more comprehensive evaluative or meta-level perspective."}}
{"id": "1625c316-8ad1-42e8-8bce-bd0c9724c8cd", "title": "Datasets", "level": "subsection", "subsections": [], "parent_id": "a7e6dbb8-00b3-4051-85b0-306047f7dab6", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Implementations"], ["subsection", "Datasets"]], "content": "There are many datasets for learning on graphs that have been investigated by previous studies, including  and . However, none of these studies has thoroughly and exclusively examined and categorized the datasets used in graph generation approaches. Here, we have summarized the most prominent ones in three general categories according to the main graph generation applications, as shown in Table \\ref{tab:Dataset}.\\\\\n\\begin{table}[ht!]\n\\small\n\\begin{tabular}{||l|m{0.19\\textwidth}|m{0.4\\textwidth}||}\n\\hline\nCategory                                                                                 & Dataset         & Citation \\\\ \\hline \\hline\n\\multirow{18}{*}{\\begin{tabular}[c]{@{}l@{}}Chemical\\\\ \\&\\\\ Bioinformatics\\end{tabular}} & ZINC            &   \n       \\\\ \\cline{2-3} \n                                                                                         & QM9             &          \\\\ \\cline{2-3} \n                                                                                         & CEPDB           &          \\\\ \\cline{2-3} \n                                                                                         & Polymer         &          \\\\ \\cline{2-3} \n                                                                                         & USPTO           &          \\\\ \\cline{2-3} \n                                                                                         & GuacaMol        &        \\\\ \\cline{2-3} \n                                                                                         & ChEMBL          &          \\\\ \\cline{2-3} \n                                                                                         & Protein         &         \\\\ \\cline{2-3} \n                                                                                         & MOSES           &          \\\\ \\cline{2-3} \n                                                                                         & NCI-H23         &          \\\\ \\cline{2-3} \n                                                                                         & Yeast           &          \\\\ \\cline{2-3} \n                                                                                         & MOLT-4          &         \\\\ \\cline{2-3} \n                                                                                         & MCF-7           &          \\\\ \\cline{2-3} \n                                                                                         & Enzymes         &         \\\\ \\cline{2-3} \n                                                                                         & PPI             &          \\\\ \\hline\n\\multirow{5}{*}{Social}                                                                  & Cora            &          \\\\ \\cline{2-3} \n                                                                                         & Citeseer        &          \\\\ \\cline{2-3} \n                                                                                         & Pubmed          &        \\\\ \\cline{2-3} \n                                                                                         & DBLP            &          \\\\ \\hline\n\\multirow{11}{*}{Synthetic}                                                               & Barabasi-Albert &       \\\\ \\cline{2-3} \n                                                                                         & Erdos-Renyi     &          \\\\ \\cline{2-3} \n                                                                                         & Watts-Strogatz  &          \\\\ \\cline{2-3} \n                                                                                         & Community       &          \\\\ \\cline{2-3} \n                                                                                         & Grid            &         \\\\ \\cline{2-3} \n                                                                                         & Lobster         &         \\\\ \\cline{2-3} \n                                                                                         & Cycles          &          \\\\ \\cline{2-3} \n                                                                                         & Ego             &          \\\\ \\hline\n\\end{tabular}\n\\caption{\\label{tab:Dataset} Summary of the Commonly Used Datasets}\n\\end{table}", "cites": [3979, 3984, 7009, 3971, 8710, 3990, 8709, 3993, 227, 3981, 1652, 7763, 3988, 553, 3997, 3972, 3996, 3980, 222, 7158, 264, 2787, 3991, 3998, 3975, 3973, 3974, 7214, 3987, 236, 229, 3983, 4000, 3976, 3995], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 45, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a list of datasets used in graph generation and categorizes them into chemical & bioinformatics, social, and synthetic, but it lacks synthesis of insights from the cited papers. There is minimal critical analysis or evaluation of the datasets strengths, weaknesses, or suitability for different graph generation approaches. The section does not abstract broader trends or principles in dataset usage across the cited works, focusing instead on a basic enumeration of sources."}}
{"id": "991dea34-c0eb-4ab9-b86e-ed9895c339a9", "title": "Scalability", "level": "subsection", "subsections": [], "parent_id": "069195c9-7623-4eb5-bf36-250e92ef911b", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Future Directions"], ["subsection", "Scalability"]], "content": "Most of the proposed graph generation methods are only applicable to small graphs with a maximum of a few tens of nodes. Therefore, designing molecular graphs, which are mostly small in size, is the most prominent application of DGGs so far. Although some initial steps  have been taken towards scalability of generator models, much more effort is necessary to make them applicable in a wider range of real-world applications, such as social network modeling.", "cites": [3997, 3996, 3976], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from three papers that address scalability in graph generation, noting a common issue of high computational complexity and the need for more efficient methods. It provides a general critique of existing models, such as their inefficiency for large graphs, but lacks in-depth comparative analysis or detailed evaluation of the cited works. The section abstracts the problem of scalability and suggests a broader need for improvement, though it does not offer a meta-level framework or novel synthesis of the ideas."}}
{"id": "630c8d76-400d-4e33-94ea-2550985539d1", "title": "node ordering", "level": "subsection", "subsections": [], "parent_id": "069195c9-7623-4eb5-bf36-250e92ef911b", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Future Directions"], ["subsection", "node ordering"]], "content": "Each graph with $n$ nodes can be represented under $n!$ different node orderings. Therefore, it becomes intractable for likelihood-based DGGs to calculate the exact likelihood as the graph size increases. To address this issue, some approximate approaches such as using approximate graph matching algorithms  or maximizing a lower bound of the likelihood by only considering subsets of node orderings (i.e., fixed , uniform random , BFS , or a family of canonical orderings ), have been proposed. However, it is still necessary to provide more effective solutions for the node ordering problem, as a result of which, the generators can generate larger samples with higher quality.", "cites": [236, 3997, 3996, 3976, 222], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the node ordering problem across several cited papers, connecting the general challenge with specific approaches like fixed, random, BFS, and canonical orderings. It provides some critical analysis by pointing out the inefficiency of current methods for large graphs and the intractability of exact likelihood calculation. However, it stops short of offering a deeper comparative or evaluative framework, and while it abstracts the issue to a broader challenge in DGGs, it does not yet reach a meta-level analysis."}}
{"id": "c21bcb60-bdc1-4b4e-a37c-4b6ac278338f", "title": "Interpretability", "level": "subsection", "subsections": [], "parent_id": "069195c9-7623-4eb5-bf36-250e92ef911b", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Future Directions"], ["subsection", "Interpretability"]], "content": "As mentioned earlier, DGGs are utilized in critical applications such as designing drug molecules, which directly affects public health. Therefore, the more transparent the generation procedure, the better control is exercised on the desirability of generated samples, which prevents additional trials and errors by limiting the number of candidate solutions. Hence, it is of great importance to make graph generation methods more interpretable. As of now, deep generative methods in areas such as image and text  have slowly moved towards being more interpretable. However, only a few attempts  have recently been made in graph generation, making model interpretability a notable future research prospect.", "cites": [1993, 4021, 3981, 4002, 4018, 4019, 4020], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section introduces the importance of interpretability in deep graph generators (DGGs), drawing on cited papers to highlight related work in interpretable generative models for text and graphs. It synthesizes ideas across these works to build a case for interpretability as a future direction. However, it offers limited critical evaluation of the cited papers and does not deeply compare their approaches or limitations. The abstraction is moderate, as it generalizes the idea of interpretability but stops short of providing a comprehensive framework or meta-level analysis."}}
{"id": "afa81139-75d9-487e-a7c3-4705c8673fc0", "title": "Conditional Graph Generation", "level": "subsection", "subsections": [], "parent_id": "069195c9-7623-4eb5-bf36-250e92ef911b", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Future Directions"], ["subsection", "Conditional Graph Generation"]], "content": "When generating new graphs, in most cases, one aims to discover structures with desired characteristics. While conditional generation is relatively well investigated in image  and text  domains, it is comparably less explored in the field of graph generation. For example, in molecular graph generation, the generated molecules must satisfy some validity constraints or hold desired chemical properties. However, only a limited number of proposed methods adopt a conditional approach by whether incorporating conditional codes into the generation process , enforcing the existence of favourable substructures in the output graph  or performing the generation conditioned on an input molecule to ensure the structural similarity . Meanwhile, the majority of methods do not formulate the problem as conditional generation and address the issue by employing other techniques like property optimization in some latent continuous space  or injecting validity constraints, whether at the training  or the inference time . Nevertheless, this issue has been even less studied in the non-molecular graph generation models, and only a few of them have partially addressed the problem . Therefore, focusing more on conditional graph generation problems, especially those that have not yet been explored, such as class conditioned generation, is an important future research direction.", "cites": [3984, 507, 1001, 3990, 8709, 3993, 3116, 227, 1652, 1995, 3975, 3974, 1996, 8715, 236, 3983, 7159], "cite_extract_rate": 0.85, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple papers by contrasting how conditional generation is approached in image/text domains versus graph generation, and by grouping techniques into categories such as substructure enforcement and property conditioning. It critically points out the underdevelopment of conditional methods in non-molecular graph generation and highlights limitations in current approaches. The analysis abstracts beyond specific papers to identify broader trends and unexplored areas like class-conditional generation, offering a forward-looking perspective."}}
{"id": "9cf43d93-0085-46a8-a3b4-e05267162ac1", "title": "Acronym", "level": "section", "subsections": [], "parent_id": "bab571dd-55e6-4f4b-ac15-157a4152a738", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Acronym"]], "content": "Table \\ref{tab:acronyms} summarizes the acronyms and nomenclature used in this survey.\n\\begin{table}[ht!]\n\\small\n\\begin{tabular}{||m{0.18\\textwidth}|m{0.5\\textwidth}|l||}\n\\hline\nAcronym      & Model Name                                   & Reference \\\\ \\hline \\hline\nDGG          & Deep Graph Generator                    &           \\\\ \\hline\nRNN          & Recurrent Neural Network                    &           \\\\ \\hline\nLSTM          & Long Short-Term Memory                    &           \\\\ \\hline\nGRU          & Gated Recurrent Unit                    &           \\\\ \\hline\nVAE          & Variational Autoencoder                     &\\\\ \\hline\nGAN          & Generative Adversarial Network               &\\\\ \\hline\nGNN          & Graph Neural Network               &\\\\ \\hline\nGCN          & Graph Convolutional Network               &\\\\ \\hline\nREIN        &       Recurrent Edge Inference Network                                      &\\\\ \\hline\nDeepNC        &       Deep Generative Network Completion                          &\\\\ \\hline\nGRAN         & Graph Recurrent Attention Network            &\\\\ \\hline\nGRAM         & Graph Generative Model with Graph Attention Mechanism                    &\\\\ \\hline\nAGE          & Attention-Based Graph Evolution              &\\\\ \\hline\nDeepGMG      &          Deep Generative Models of Graphs                                    &\\\\ \\hline\nDeepGG      &          Deep Graph Generators          &\\\\ \\hline\nBiGG         & BIg Graph Generation                         &\\\\ \\hline\nVGAE         & Variational Graph Autoencoder               &\\\\ \\hline\nMPGVAE       & Message Passing Graph VAE                    & \\\\ \\hline\nNED-VAE      & Node-Edge Disentangled VAE                   &\\\\ \\hline\nDGVAE      & Dirichlet Graph VAE                   &\\\\ \\hline\nJT-VAE       & Junction Tree VAE                            &\\\\ \\hline\nHierVAE      & Hierarchical VAE                             &\\\\ \\hline\nMHG-VAE      & Molecular Hypergraph Grammar VAE             &\\\\ \\hline\nCGVAE        & Constrained Graph VAE                        &\\\\ \\hline\nDEFactor     & Differentiable Edge\nFactorization-based Probabilistic Graph\nGeneration      &\\\\ \\hline\nGraphVRNN    & Graph Variational RNN   &\\\\ \\hline\nGCPN         & Graph Convolutional Policy Network           &\\\\ \\hline\nMNCE-RL         & Molecular Neighborhood-Controlled Embedding RL               &\\\\ \\hline\nGEGL         & Genetic Expert-Guided Learning               &\\\\ \\hline\nMMGAN        & Multi-MotifGAN                               &\\\\ \\hline\nMolGAN       &    Molecular GAN &\\\\ \\hline\nLGGAN       &    Labeled Graph GAN &\\\\ \\hline\nTSGG-GAN     & Time Series Conditioned Graph Generation-GAN &\\\\ \\hline\nVJTNN     & Variational Junction Tree Encoder-Decoder &\\\\ \\hline\nMisc-GAN     & Multi-Scale GAN                              &\\\\ \\hline\nGNF      & Graph Normalizing Flow       &\\\\ \\hline\nGrAD         & Graph Auto-Decoder                           &\\\\ \\hline\n\\end{tabular}\n\\caption{\\label{tab:acronyms} Acronyms with their extended names}\n\\end{table}", "cites": [3971, 3993, 3981, 1652, 3988, 7763, 3997, 222, 4001, 264, 3991, 3998, 3975, 5680, 3973, 3974, 243, 7214, 3987, 229, 3976, 3995], "cite_extract_rate": 0.6285714285714286, "origin_cites_number": 35, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.0, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section merely presents a list of acronyms and their corresponding model names without synthesizing, critically analyzing, or abstracting the information from the cited papers. There is no integration of ideas, evaluation of strengths/limitations, or identification of broader trends or principles in the field of deep graph generation."}}
{"id": "907e3557-aa22-4161-982e-66059012efd8", "title": "Source Codes", "level": "section", "subsections": [], "parent_id": "bab571dd-55e6-4f4b-ac15-157a4152a738", "prefix_titles": [["title", "Deep Graph Generators: A Survey"], ["section", "Source Codes"]], "content": "Table \\ref{tab:codes} summarizes the set of publicly available source codes for deep learning-based graph generation approaches discussed in the survey.\n\\begin{table*}\n\\begin{center}\n\\small\n\\begin{tabular}[t]{||l|lm{0.33\\textwidth}ll||}\n\\hline\nCategory                         & Method        & URL & Language/Framework & O.A.\\\\ \\hline \\hline\n\\multirow{13}{*}{Autoregressive} & MolMP         & \\url{https://github.com/kevinid/molecule_generator}  &   Python/MXNet     & Yes            \\\\ \\cline{2-5} \n                                 & MolRNN        &   \\url{https://github.com/kevinid/molecule_generator}  &  Python/MXNet   & Yes               \\\\ \\cline{2-5} \n                                 & GraphRNN    &   \\url{https://github.com/JiaxuanYou/graph-generation}  &  Python/PyTorch & Yes                 \\\\ \\cline{2-5} \n                                                                  & DeepNC    &   \\url{https://github.com/congasix/DeepNC}  &  Python/TensorFlow & Yes                 \\\\ \\cline{2-5} \n                                & Bacciu et al.       &  \\url{https://github.com/marcopodda/grapher}   &         Python/PyTorch & Yes          \\\\ \\cline{2-5} \n                                 & GraphGen      &  \\url{https://github.com/idea-iitd/graphgen}   &         Python/PyTorch & Yes          \\\\ \\cline{2-5} \n                                                                  & GRAN          &  \\url{https://github.com/lrjconan/GRAN}   &    Python/PyTorch     & Yes             \\\\ \\cline{2-5} \n                                 & DeepGMG      &  \\url{https://github.com/JiaxuanYou/graph-generation/blob/master/main_DeepGMG.py}   &   Python/PyTorch      & No           \\\\ \\cline{2-5}\n                                 & BiGG          &   \\url{https://github.com/google-research/google-research/tree/master/bigg}  &         Python/PyTorch      & Yes    \\\\ \\hline\n\\multirow{19}{*}{Autoencoder-Based}    & VGAE          &   \\url{https://github.com/tkipf/gae}  &    Python/TensorFlow      & Yes          \\\\ \\cline{2-5} \n                                 & GraphVAE     &  \\url{https://github.com/JiaxuanYou/graph-generation/tree/master/baselines/graphvae}   &     Python/PyTorch     & No          \\\\ \\cline{2-5}\n                        & Graphite      &  \\url{https://github.com/ermongroup/graphite}   &     Python/TensorFlow     & Yes      \\\\ \\cline{2-5} \n                                 & NED-VAE       &  \\url{https://github.com/xguo7/NED-VAE}   &          & Yes      \n                        \\\\ \\cline{2-5} \n                        & DGVAE         &  \\url{https://github.com/xiyou3368/DGVAE}   &   Python/TensorFlow       & Yes         \\\\ \\cline{2-5} \n                                 & JT-VAE        &  \\url{https://github.com/wengong-jin/icml18-jtnn}   &   Python/PyTorch       & Yes         \\\\ \\cline{2-5} \n                                 & HierVAE       &  \\url{https://github.com/wengong-jin/hgraph2graph}   &    Python/PyTorch      & Yes          \\\\ \n                                 \\cline{2-5} \n                                 & MHG-VAE       &   \\url{https://github.com/ibm-research-tokyo/graph_grammar}  &    Python/PyTorch      & Yes                 \n                                 \\\\ \\cline{2-5} \n                                 & MoleculeChef  &   \\url{https://github.com/john-bradshaw/molecule-chef}  &     Python/PyTorch     & Yes          \n                                 \\\\\n                                 \\cline{2-5} \n                                 & CGVAE         &   \\url{https://github.com/microsoft/constrained-graph-variational-autoencoder}  &     Python/TensorFlow     & Yes          \\\\ \\cline{2-5}\n                                 & NeVAE         &  \\url{https://github.com/Networks-Learning/nevae}   &    Python/TensorFlow      & Yes         \\\\ \\cline{2-5} \n                                                                  & Lim et al.    &   \\url{https://\ngithub.com/jaechanglim/GGM}  &       & Yes             \\\\  \\hline\n\\multirow{7}{*}{RL-Based}        & GCPN           &  \\url{https://github.com/bowenliu16/rl_graph_generation}   &    Python/TensorFlow      & Yes          \\\\ \\cline{2-5} \n                                 & Karimi et al.     &  \\url{https://github.com/Shen-Lab/Drug-Combo-Generator}   &      Python/TensorFlow    & Yes          \\\\ \\cline{2-5} \n                                 & DeepGraphMolGen  &  \\url{https://github.com/dbkgroup/prop_gen}   &    Python/PyTorch      & Yes          \\\\\\cline{2-5} \n                                 & MNCE-RL   &  \\url{https://github.com/Zoesgithub/MNCE-RL}   &    Python/PyTorch      & Yes          \\\\\\cline{2-5} \n                                 & GEGL     &  \\url{https://github.com/sungsoo-ahn/genetic-expert-guided-learning}   &      Python/PyTorch    & Yes          \\\\  \\hline\n\\multirow{7}{*}{Adversarial}    \n                                 & NetGAN        &   \\url{https://github.com/danielzuegner/netgan}  &    Python/TensorFlow      & Yes         \\\\\\cline{2-5} \n                                 & MolGAN        &  \\url{https://github.com/nicola-decao/MolGAN}   &    Python/TensorFlow      & Yes          \\\\ \\cline{2-5} \n                                 & CONDGEN      & \\url{https://github.com/KelestZ/CondGen}    &    Python/PyTorch      & Yes          \\\\ \\cline{2-5} \n                                 & VJTNN + GAN       & \\url{https://github.com/wengong-jin/iclr19-graph2graph}    &    Python/PyTorch      & Yes          \\\\ \\cline{2-5} \n                                 & Mol-CycleGAN  &   \\url{https://github.com/ardigen/mol-cycle-gan}  &       Python/Keras   & Yes          \\\\ \\cline{2-5} \n                                 & Misc-GAN      &   \\url{https://github.com/Leo02016/Miscgan}  &   Python/TensorFlow, Matlab       & Yes          \\\\  \n                                 \\hline\n\\multirow{5}{*}{Flow-Based}       & GNF           &   \\url{https://github.com/jliu/graph-normalizing-flows}  &     Python/TensorFlow     & Yes                 \\\\ \\cline{2-5}\n& GraphNVP            &   \\url{https://github.com/Kaushalya/graph-nvp}  &     Python/Chainer-Chemistry     & Yes                 \\\\ \\cline{2-5}\n                                 & GraphAF       &  \\url{https://github.com/DeepGraphLearning/GraphAF}   &          Python/PyTorch      & Yes        \\\\ \n                                 \\hline\n\\end{tabular}\n\\caption{\\label{tab:codes} A set of publicly available source codes. O.A. = Original Authors}\n\\end{center}\n\\end{table*}\n\\vspace{12pt}\n\\begin{flushleft}\n{\n\\justify\n\\bibliographystyle{unsrt}\n\\bibliography{IEEEabrv,references}\n}\n\\end{flushleft}\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/Faezeh_Faez.png}}]{Faezeh Faez} received her B.Sc. and the M.Sc. degrees in Software Engineering from Sharif University of Technology, Tehran, Iran. She is currently a Ph.D. candidate in Artificial Intelligence in the Department of Computer Engineering at Sharif University of Technology. Her current research interests include machine learning, deep learning, and deep graph generative models.\n\\end{IEEEbiography}\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/Ms._Ommi.jpeg}}]{Yassaman Ommi} is currently a B.Sc. student in computer science at Amirkabir University of Technology (Tehran Polytechnic), Tehran, Iran. Her current research interests include graph-based deep learning, pattern recognition, and complex networks.\n\\end{IEEEbiography}\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/Dr._Soleimani.JPG}}]{Mahdieh Soleymani Baghshah} received the B.Sc., M.Sc., and Ph.D. degrees from the Department of Computer Engineering, Sharif University of Technology, Iran, in 2003, 2005, and 2010, respectively. She is an assistant professor with the Computer Engineering Department, Sharif University of Technology, Tehran, Iran. Her research interests include machine learning and deep learning.\n\\end{IEEEbiography}\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/Dr._Rabiee.png}}]{Hamid R. Rabiee} (SM07) received his BS and MS degrees (with Great Distinction) in Electrical Engineering from CSULB, Long Beach, CA (1987, 1989), his EEE degree in Electrical and Computer Engineering from USC, Los Angeles, CA (1993), and his Ph.D. in Electrical and Computer Engineering from Purdue University, West Lafayette, IN, in 1996. From 1993 to 1996 he was a Member of Technical Staff at AT\\&T Bell Laboratories. From 1996 to 1999 he worked as a Senior Software Engineer at Intel Corporation. He was also with PSU, OGI and OSU universities as an adjunct professor of Electrical and Computer Engineering from 1996-2000. Since September 2000, he has joined Sharif University of Technology, Tehran, Iran. He was also a visiting professor at the Imperial College of London for the 2017-2018 academic year. He is the founder of Sharif University Advanced Information and Communication Technology Research Institute (AICT), ICT Innovation Center, Advanced Technologies Incubator (SATI), Digital Media Laboratory (DML), Mobile Value Added Services Laboratory (VASL), Bioinformatics and Computational Biology Laboratory (BCB) and Cognitive Neuroengineering Research Center. He is also a consultant and member of AI in Health Expert Group at WHO. He has been the founder of many successful High-Tech start-up companies in the field of ICT as an entrepreneur. He is currently a Professor of Computer Engineering at Sharif University of Technology, and Director of AICT, DML, and VASL. He has received numerous awards and honors for his Industrial, scientific and academic contributions, and holds three patents. His research interests include statistical machine learning, Bayesian statistics, data analytics and complex networks with applications in social networks, multimedia systems, cloud and IoT privacy, bioinformatics, and brain networks.\n\\end{IEEEbiography}\n\\EOD\n\\end{document}", "cites": [3979, 3984, 7009, 3971, 8710, 3990, 3993, 3981, 1652, 3997, 3996, 3972, 222, 4001, 7158, 264, 3991, 3998, 3975, 3974, 7214, 236, 229, 3983, 3976, 3977, 4000], "cite_extract_rate": 0.7941176470588235, "origin_cites_number": 34, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a list of publicly available source codes for various deep graph generation methods without synthesizing insights or connecting ideas across the cited works. It lacks critical analysis of the implementations or their limitations and offers no abstraction or broader patterns related to code availability, framework usage, or reproducibility."}}
