{"id": "18ea1659-9afd-4cf0-b7ab-6d8859dc5af6", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "039eb0dd-a257-4d85-a714-ad5fc65c6f0b", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Introduction"]], "content": "Within the last decade enormous advances on deep neural networks (DNNs) have been realized, encouraging their adaptation in a variety of research fields, where complex systems have to be modeled or understood, as for example earth observation, medical image analysis or robotics. Although DNNs have become attractive in high risk fields such as medical image analysis  or autonomous vehicle control , their deployment in mission- and safety-critical real world applications remains limited. The main factors responsible for this limitation are\n\\begin{itemize}\n  \\setlength\\itemsep{0.5em}\n    \\item the lack of expressiveness and transparency of a deep neural network's inference model, which makes it difficult to trust their outcomes ,\n    \\item the inability to distinguish between in-domain and out-of-domain samples  and the sensitivity to domain shifts ,\n    \\item the inability to provide reliable uncertainty estimates for a deep neural network's decision  and frequently occurring overconfident predictions ,\n    \\item the sensitivity to adversarial attacks that make deep neural networks vulnerable for sabotage .\n\\end{itemize}\nThese factors are mainly based on an uncertainty already included in the data (data uncertainty) or a lack of knowledge of the neural network (model uncertainty). To overcome these limitations, it is essential to provide uncertainty estimates, such that uncertain predictions can be ignored or passed to human experts . Providing uncertainty estimates is not only important for a safe decision-making in high-risks fields, but also crucial in fields where the data sources are highly inhomogeneous and labeled data is rare, such as in remote sensing . Also for fields where uncertainties form a crucial part of the learning techniques, such as for active learning  or reinforcement learning , uncertainty estimates are highly important.\nIn recent years, researchers have shown an increased interest in estimating uncertainty in DNNs . The most common way to estimate the uncertainty on a prediction (the predictive uncertainty) is based on separately modelling the uncertainty caused by the model (epistemic or model uncertainty) and the uncertainty caused by the data (aleatoric or data uncertainty). While the first one is reducible by improving the model which is learned by the DNN, the latter one is not reducible. The most important approaches for modeling this separation are Bayesian inference , ensemble approaches , test time data augmentation approaches , or single deterministic networks containing explicit components to represent the model and the data uncertainty . Estimating the predictive uncertainty is not sufficient for safe decision-making. Furthermore, it is crucial to assure that the uncertainty estimates are reliable. To this end, the calibration property (the degree of reliability) of DNNs has been investigated and re-calibration methods have been proposed  to obtain reliable (well-calibrated) uncertainty estimates.\nThere are several works that give an introduction and overview of uncertainty in statistical modelling. Ghanem et al.  published a handbook about uncertainty quantification, which includes a detailed and broad description of different concepts of uncertainty quantification, but without explicitly focusing on the application of neural networks. The theses of Gal  and Kendall  contain a good overview about Bayesian neural networks, especially with focus on the Monte Carlo (MC) Dropout approach and its application in computer vision tasks. The thesis of Malinin  also contains a very good introduction and additional insights into Prior networks. Wang et al. contributed two surveys on Bayesian deep learning . They introduced a general framework and the conceptual description of the Bayesian neural networks (BNNs), followed by  an updated presentation of Bayesian approaches for uncertainty quantification in neural networks with special focus on recommender systems, topic models, and control. In  an evaluation of uncertainty quantification in deep learning is given by presenting and comparing the uncertainty quantification based on the softmax output, the ensemble of networks, Bayesian neural networks, and autoencoders on the MNIST data set. Regarding the practicability of uncertainty quantification approaches for real life mission- and safety-critical applications, Gustafsson et al.  introduced a framework to test the robustness required in real-world computer vision applications and delivered a comparison of two popular approaches, namely MC Dropout and Ensemble methods.\nHÃ¼llermeier et al.  presented the concepts of aleatoric and epistemic uncertainty in neural networks and discussed different concepts to model and quantify them. In contrast to this, Abdar et al.  presented an overview on uncertainty quantification methodologies in neural networks and provide an extensive list of references for different application fields and a discussion of open challenges.\\\\\nIn this work, we present an extensive overview over all concepts that have to be taken into account when working with uncertainty in neural networks while keeping the applicability on real world applications in mind. Our goal is to provide the reader with a clear thread from the sources of uncertainty to applications, where uncertainty estimations are needed. Furthermore, we point out the limitations of current approaches and discuss further challenges to be tackled in the future. For that, we provide a broad introduction and comparison of different approaches and fundamental concepts. The survey is mainly designed for people already familiar with deep learning concepts and who are planning to incorporate uncertainty estimation into their predictions. But also for people already familiar with the topic, this review provides a useful overview of the whole concept of uncertainty in neural networks and their applications in different fields.\\\\\nIn summary, we comprehensively discuss\n\\begin{itemize}\n  \\setlength\\itemsep{0.5em}\n    \\item Sources and types of uncertainty (Section \\ref{sec:uncertainty_types_and_sources}),\n    \\item Recent studies and approaches for estimating uncertainty in DNNs (Section \\ref{sec:uncertainty_quantification_methods}),\n    \\item Uncertainty measures and methods for assessing the quality and impact of uncertainty estimates (Section \\ref{sec:uncertainty_measures}),\n    \\item Recent studies and approaches for calibrating DNNs (Section \\ref{sec:calibration}),\n    \\item An overview over frequently used evaluation data sets, available benchmarks and implementations\\footnote{The list of available implementations can be found in Section \\ref{sec:data_sets_and_baselines} as well as within an additional GitHub repository under \\href{https://github.com/JakobCode/UncertaintyInNeuralNetworks\\_Resources}{github.com/JakobCode/UncertaintyInNeuralNetworks\\_Resources}.} (Section \\ref{sec:data_sets_and_baselines}),\n    \\item An overview over real-world applications using uncertainty estimates (Section \\ref{sec:application_fields}),\n    \\item A discussion on current challenges and further directions of research in the future (Section \\ref{sec:con}).\n\\end{itemize}\nIn general, the principles and methods for estimating uncertainty and calibrating DNNs can be applied to all regression, classification, and segmentation problems, if not stated differently. In order to get a deeper dive into explicit applications of the methods, we refer to the section on applications and to further readings in the referenced literature.", "cites": [4601, 4598, 4606, 4612, 4617, 8806, 7755, 4596, 4590, 1828, 3289, 2441, 4594, 759, 8807, 4610, 4600, 4607, 1044, 3288, 4616, 4603, 4593, 4025, 4597, 4599, 4615, 4592, 4605, 4609, 8805, 8633, 4602, 4595, 4591, 4608, 4614, 4611, 3234, 4604, 4613], "cite_extract_rate": 0.7068965517241379, "origin_cites_number": 58, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes a wide range of uncertainty estimation approaches and connects them to broader themes such as model calibration, application domains, and types of uncertainty. It demonstrates abstraction by discussing overarching principles like aleatoric and epistemic uncertainty, and shows some critical analysis by highlighting limitations such as computational costs and the need for re-calibration. While not deeply critiquing individual papers, it provides a coherent and insightful analytical overview."}}
{"id": "b9af7cc5-b075-4669-8b41-eed3b73255d2", "title": "Deep Neural Network Design and Training", "level": "subsection", "subsections": [], "parent_id": "695bfab1-de93-493c-976a-2aa81c0f0d91", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty in Deep Neural Networks"], ["subsection", "Deep Neural Network Design and Training"]], "content": "The design of a DNN covers the explicit modeling of the neural network and its stochastic training process. The assumptions on the problem structure induced by the design and training of the neural network are called inductive bias . We summarize all decisions of the modeler on the network's structure (e.g. the number of parameters, the layers, the activation functions, etc.) and training process (e.g. optimization algorithm, regularization, augmentation, etc.) in a structure configuration $s$.\n    The defined network structure gives the third factor of uncertainty in a neural network's predictions:\n    \\vspace{0.8em}\n    \\begin{mdframed}\n        \\textbf{Factor III: Errors in the Model Structure}\\\\\n        The structure of a neural network has a direct effect on its performance and therefore also on the uncertainty of its prediction. For instance, the number of parameters affects the memorization capacity, which can lead to under- or over-fitting on the training data. Regarding uncertainty in neural networks, it is known that deeper networks tend to be overconfident in their softmax output, meaning that they predict too much probability on the class with highest probability score .\n    \\end{mdframed}\n    \\vspace{0.8em}\n    For a given network structure $s$ and a training data set $\\D$, the training of a neural network is a stochastic process and therefore the resulting neural network $f_\\theta$ is based on a random variable,\n    \\begin{align}\n        \\theta\\vert D, s \\sim p_{\\theta|D,s}.\n    \\end{align}\n    The process is stochastic due to random decisions as the order of the data, random initialization or random regularization as augmentation or dropout. The loss landscape of a neural network is highly non-linear and the randomness in the training process in general leads to different local optima $\\theta^*$ resulting in different models . Also, parameters as batch size, learning rate, and the number of training epochs affect the training and result in different models. Depending on the underlying task these models can significantly differ in their predictions for single samples, even leading to a difference in the overall model performance. This sensitivity to the training process directly leads to the fourth factor for uncertainties in neural network predictions: \n    \\vspace{0.8em}\n    \\begin{mdframed}\n    \\textbf{Factor IV: Errors in the Training Procedure} \\\\\n    The training process of a neural network includes many parameters that have to be defined (batch size, optimizer, learning rate, stopping criteria, regularization, etc.) and also stochastic decisions within the training process (batch generation and weight initialization) take place. All these decisions affect the local optima and it is therefore very unlikely that two training processes deliver the same model parameterization. A training data set that suffers from imbalance or low coverage of single regions in the data distribution also introduces uncertainties on the network's learned parameters, as already described in the data acquisition. This might be softened by applying augmentation to increase the variety or by balancing the impact of single classes or regions on the loss function.\n    \\end{mdframed}\n    Since the training process is based on the given training data set $\\D$, errors in the data acquisition process (e.g. label noise) can result in errors in the  training  process.", "cites": [208, 759, 3288], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview by framing DNN design and training as key sources of uncertainty, synthesizing concepts such as inductive bias and stochastic training from the cited works. It integrates these into a broader discussion of model and training-induced uncertainties. While it offers some critical insight into how design choices impact calibration and uncertainty, it stops short of deep comparative or evaluative analysis of the cited papers' methodologies or limitations."}}
{"id": "ae1d7700-2122-494c-9572-78ac85447ec9", "title": "Predictive Uncertainty Model", "level": "subsection", "subsections": ["897710f8-9cf6-4254-9319-d5f427b2ded5", "cf1e789d-36cb-4385-a33d-3ae68f07aef2"], "parent_id": "695bfab1-de93-493c-976a-2aa81c0f0d91", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty in Deep Neural Networks"], ["subsection", "Predictive Uncertainty Model"]], "content": "As a modeller, one is mainly interested in the uncertainty that is propagated onto a prediction $y^*$, the so-called \\textit{predictive uncertainty}. Within the data acquisition model, the probability distribution for a prediction $y^*$ based on some sample $x^*$ is given by\n\\begin{equation}\\label{eq:real_world_distribution}\n    p(y^*|x^*) = \\int_\\Omega p(y^*|\\omega)p(\\omega|x^*)d\\omega\n\\end{equation}\nand a maximum a posteriori (MAP) estimation is given by\n\\begin{equation}\\label{eq:real_world_max_likelihood}\n    y^* = \\arg \\max_y p(y | x^*)~.\n\\end{equation}\nSince the modeling is based on the unavailable latent variable $\\omega$, one takes an approximative representation based on a sampled training data set $\\mathcal{D}=\\{x_i,y_i\\}_{i=1}^N$ containing $N$ samples and corresponding targets. The distribution and MAP estimator in \\eqref{eq:real_world_distribution} and \\eqref{eq:real_world_max_likelihood} for a new sample $x^*$ are then predicted based on the known examples by\n\\begin{equation}\\label{eq:data_set_distribution}\n    p(y^*\\vert x^*) = \\int_D p(y^*\\vert \\D,x^*)\n\\end{equation}\nand\n\\begin{equation}\\label{eq:data_set_max_likelihood}\n    y^* = \\arg \\max_y p(y | \\D,x^*)~.\n\\end{equation}\nIn general, the distribution given in \\eqref{eq:data_set_distribution} is unknown and can only be estimated based on the given data in $D$. For this estimation, neural networks form a very powerful tool for many tasks and applications. \n\\par\nThe prediction of a neural network is subject to both model-dependent and input data-dependent errors, and therefore the predictive uncertainty associated with $y^*$ is in general separated into \\textit{data uncertainty} (also statistical or aleatoric uncertainty ) and \\textit{model uncertainty} (also systemic or epistemic uncertainty ). Depending on the underlying approach, an additional explicit modeling of \\textit{distributional uncertainty}  is used to model the uncertainty, which is caused by examples from a region not covered by the training data.", "cites": [8633, 4604], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from the cited papers, such as the distinction between data and model uncertainty, and introduces the idea of distributional uncertainty. While it provides a coherent analytical framework for understanding predictive uncertainty, it lacks in-depth critical evaluation of the methods or limitations of the cited works. It abstracts the concept of uncertainty into general categories but stops short of offering a meta-level synthesis or novel theoretical insight."}}
{"id": "897710f8-9cf6-4254-9319-d5f427b2ded5", "title": "Model- and Data Uncertainty", "level": "subsubsection", "subsections": [], "parent_id": "ae1d7700-2122-494c-9572-78ac85447ec9", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty in Deep Neural Networks"], ["subsection", "Predictive Uncertainty Model"], ["subsubsection", "Model- and Data Uncertainty"]], "content": "The model uncertainty covers the uncertainty that is caused by shortcomings in the model, either by errors in the training procedure, an insufficient model structure, or lack of knowledge due to unknown samples or a bad coverage of the training data set. \nIn contrast to this, data uncertainty is related to uncertainty that directly stems from the data. Data uncertainty is caused by information loss when representing the real world within a data sample and represents the distribution stated in \\eqref{eq:real_world_distribution}. For example, in regression tasks noise in the input and target measurements causes data uncertainty that the network cannot learn to correct. In classification tasks, samples which do not contain enough information in order to identify one class with 100\\% certainty cause data uncertainty on the prediction. The information loss is a result of the measurement system, e.g. by representing real world information by image pixels with a specific resolution, or by errors in the labelling process.  \\\\\nConsidering the five presented factors for uncertainties on a neural network's prediction, model uncertainty covers Factors I, III, IV, and V and data uncertainty is related to Factor II. While model uncertainty can be (theoretically) reduced by improving the architecture, the learning process, or the training data set, the data uncertainties cannot be explained away . Therefore, DNNs that are capable of handling uncertain inputs and that are able to remove or quantify the model uncertainty and give a correct prediction of the data uncertainty are of paramount importance for a variety of real world mission- and safety-critical applications.\n\\begin{figure*}\n\t\\centering\n\t\\begin{tikzpicture}\n\t\t\\node[inner sep=0pt] at (0,0)\n\t\t{\\includegraphics[clip, trim=7.5cm 2.8cm 10.5cm 1.2cm, width=.316\\textwidth]{figures/data_regression.pdf}};\n\t\t\\node[inner sep=0pt] at (6,0)\n\t\t{\\includegraphics[clip, trim=7.5cm 2.8cm 10.5cm 1.2cm, width=.316\\textwidth]{figures/model_regression.pdf}};\n\t\t\\node[inner sep=0pt] at (12,0)\n\t\t{\\includegraphics[clip, trim=7.5cm 2.8cm 10.5cm 1.2cm, width=.316\\textwidth]{figures/distributional_regression.pdf}};\n\t\t\\node[inner sep=0pt]  at (0,-5.5)\n\t\t{\\includegraphics[clip, trim=7.5cm 2.8cm 10.5cm 1.2cm, width=.316\\textwidth]{figures/data_classification.pdf}};\n\t\t\\node[inner sep=0pt] at (6,-5.5)\n\t\t{\\includegraphics[clip, trim=7.8cm 2.8cm 10.2cm 1.2cm, width=.316\\textwidth]{figures/model_classification.pdf}};\n\t\t\\node[inner sep=0pt] at (12,-5.5)\n\t\t{\\includegraphics[clip, trim=7.5cm 2.8cm 10.5cm 1.2cm, width=.316\\textwidth]{figures/distributional_classification.pdf}};\n\t\t\\draw (-2.75,-2.75) -- (15,-2.75);\n\t\t\\draw (3,3) -- (3,-8.5);\n\t\t\\draw (9,3) -- (9,-8.5);\n\t\\end{tikzpicture}\n\t\\caption{Visualization of the data, the model, and the distributional uncertainty for classification and regression models.}\n\t\\label{fig:uncertainty_types}\n\\end{figure*}\nThe Bayesian framework offers a practical tool to reason about uncertainty in deep learning . In Bayesian modeling, the model uncertainty is formalized as a probability distribution over\nthe model parameters $\\theta$, while the data uncertainty is formalized as a probability distribution over the model outputs $y^*$, given a parameterized model $f_\\theta$. The distribution over a prediction $y^*$, the predictive distribution, is then given by \n\\begin{align}\\label{eq:predictive_uncertainty_intro} \n    \t    p(y^*|x^*, D)&=\\int\\underbrace{p(y^*\\vert x^*,\\theta)}_{\\text{Data}}\\underbrace{p(\\theta\\vert D)}_{\\text{Model}}d\\theta~.\n\\end{align}\nThe term $p(\\theta | D)$ is referenced as posterior distribution on the model parameters and describes the uncertainty on the model parameters given a training data set $D$. The posterior distribution is in general not tractable. While ensemble approaches seek to approximate it by learning several different parameter settings and averaging over the resulting models , Bayesian inference reformulates it using Bayes Theorem  \n\\begin{equation}\\label{eq:bayes_posterior_intro}\n    p(\\theta|D) = \\frac{p(D|\\theta)p(\\theta)}{p(D)}~.\n\\end{equation}\nThe term $p(\\theta)$ is called the prior distribution on the model parameters, since it does not take any information but the general knowledge on $\\theta$ into account. The term $p(D\\vert\\theta)$ represents the likelihood that the data in $D$ is a realization of the distribution predicted by a model parameterized with $\\theta$. Many loss functions are motivated by or can be related to the likelihood function. Loss functions that seek to maximize the log-likelihood (for an assumed distribution) are for example the cross-entropy or the mean squared error . \nEven with the reformulation given in \\eqref{eq:bayes_posterior_intro}, the predictive distribution given in \\eqref{eq:predictive_uncertainty_intro} is still intractable. To overcome this, several different ways to approximate the predictive distribution were proposed. A broad overview on the different concepts and some specific approaches is presented in Section \\ref{sec:uncertainty_quantification_methods}.", "cites": [4618, 3288], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes concepts from both cited papers (Bayesian CNNs and Deep Ensembles) by connecting them under a Bayesian framework for uncertainty estimation. It abstracts the distinction between model and data uncertainty, framing them within the predictive distribution equation. While it does not deeply critique the methods, it identifies limitations such as the intractability of the posterior and the computational cost of Bayesian approaches, showing a level of critical awareness."}}
{"id": "cf1e789d-36cb-4385-a33d-3ae68f07aef2", "title": "Distributional Uncertainty", "level": "subsubsection", "subsections": [], "parent_id": "ae1d7700-2122-494c-9572-78ac85447ec9", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty in Deep Neural Networks"], ["subsection", "Predictive Uncertainty Model"], ["subsubsection", "Distributional Uncertainty"]], "content": "Depending on the approaches that are used to quantify the uncertainty in $y^*$, the formulation of the predictive distribution might be further separated into data, distributional, and model parts :\n\\begin{equation}\\label{eq:predictive_uncertainty_split_2_intro}\np(y^*|x^*, D)=\\int\\int \\underbrace{p(y\\vert \\mu)}_{\\text{Data}}\\underbrace{p(\\mu\\vert x^*,\\theta)}_{\\text{Distributional}}\\underbrace{p(\\theta\\vert D)}_{\\text{Model}}d\\mu d\\theta~.\n\\end{equation}\nThe distributional part in \\eqref{eq:predictive_uncertainty_split_2_intro} represents the uncertainty on the actual network output, e.g. for classification tasks this might be a Dirichlet distribution, which is a distribution over the categorical distribution given by the softmax output.\nModeled this way, distributional uncertainty refers to uncertainty that is caused by a change in the input-data distribution, while model uncertainty refers to uncertainty that is caused by the process of building and training the DNN. As modeled in \\eqref{eq:predictive_uncertainty_split_2_intro}, the model uncertainty affects the estimation of the distributional uncertainty, which affects the estimation of the data uncertainty.\nWhile most methods presented in this paper only distinguish between model and data uncertainty, approaches specialized on out-of-distribution detection often explicitly aim at representing the distributional uncertainty . A more detailed presentation of different approaches for quantifying uncertainties in neural networks is given in Section \\ref{sec:uncertainty_quantification_methods}. In Section \\ref{sec:uncertainty_measures}, different measures for measuring the different types of uncertainty are presented.", "cites": [8633], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a conceptual synthesis of uncertainty decomposition by referencing a mathematical formulation and connecting it to specific uncertainty types. It abstracts the idea of distributional uncertainty as a distinct source, contrasting it with model and data uncertainties. While it hints at the importance of distinguishing uncertainty sources, it lacks deeper critical analysis of the cited approaches or their limitations."}}
{"id": "8f3aceee-639b-4bb8-b299-e1edd450ebe7", "title": "Uncertainty Classification", "level": "subsection", "subsections": [], "parent_id": "695bfab1-de93-493c-976a-2aa81c0f0d91", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty in Deep Neural Networks"], ["subsection", "Uncertainty Classification"]], "content": "On the basis of the input data domain, the predictive uncertainty can also be classified into three main classes: \n\\begin{itemize}\n  \\setlength\\itemsep{0.5em}\n    \\item \\textit{In-domain uncertainty} \\\\\n    In-domain uncertainty represents the uncertainty related to an input drawn from a data distribution assumed to be equal to the training data distribution. The in-domain uncertainty stems from the inability of the deep neural network to explain an in-domain sample due to lack of in-domain knowledge. From a modeler point of view, in-domain uncertainty is caused by design errors (model uncertainty) and the complexity of the problem at hand (data uncertainty). Depending on the source of the in-domain uncertainty, it might be reduced by increasing the quality of the training data (set) or the training process .   \n    \\item \\textit{Domain-shift uncertainty}  \\\\\n    Domain-shift uncertainty denotes the uncertainty related to an input drawn from a shifted version of the training distribution. The distribution shift results from insufficient coverage by the training data and the variability inherent to real world situations. A domain-shift might increase the uncertainty due to the inability of the DNN to explain the domain shift sample on the basis of the seen samples at training time. Some errors causing domain shift uncertainty can be modeled and can therefore be reduced. For example, occluded samples can be learned by the deep neural network to reduce domain shift uncertainty caused by occlusions . However, it is difficult if not impossible to model all errors causing domain shift uncertainty, e.g., motion noise . From a modeler point of view, domain-shift uncertainty is caused by external or environmental factors but can be reduced by covering the shifted domain in the training data set. \n    \\item \\textit{Out-of-domain uncertainty} \\\\\n    Out-of-domain uncertainty represents the uncertainty related to an input drawn from the subspace of unknown data. The distribution of unknown data is different and far from the training distribution. While a DNN can extract in-domain knowledge from domain-shift samples, it cannot extract in-domain knowledge from out-of-domain samples. For example, when domain-shift uncertainty describes phenomena like a blurred picture of a dog, out-of-domain uncertainty describes the case when a network that learned to classify cats and dogs is asked to predict a bird.\n    The out-of-domain uncertainty stems from the inability of the DNN to explain an out-of-domain sample due to its lack of out-of-domain knowledge. From a modeler point of view, out-of-domain uncertainty is caused by input samples, where the network is not meant to give a prediction for or by insufficient training data.\n\\end{itemize}\nSince the model uncertainty captures what the DNN does not know due to lack of in-domain or out-of-domain knowledge, it captures all, in-domain, domain-shift, and out-of-domain uncertainties. In contrast, the data uncertainty captures in-domain uncertainty that is caused by the nature of the data the network is trained on, as for example overlapping samples and systematic label noise. \n\\begin{figure*}[t]\n\\resizebox{\\textwidth}{!}{\n   \\includegraphics{figures/Overview_fertig_cropped.pdf}}\n    \\caption{The illustration shows the different steps of a neural network pipeline, based on the earth observation example of land cover classification (here settlement and forest) based on optical images. The different factors that affect the predictive uncertainty are highlighted in the boxes. Factor I is shown as changing environments by cloud covered trees, different types and colors of trees. Factor II is shown by insufficient measurements, that can not directly be used to separate between settlement and forest and by label noise. In practice, the resolution of such images can be low and which would also be part of Factor II. Factor III and Factor IV represent the uncertainties caused by the network structure and the stochastic training process, respectively. Factor V in contrast is represented by feeding the trained network with unknown types of images, namely cows and pigs.} \n\\label{fig:uncertainty_sources}\n\\end{figure*}", "cites": [4616, 3251, 8319, 8808, 3299, 4604, 1624], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic classification of uncertainty types (in-domain, domain-shift, out-of-domain) and relates them to cited works, but lacks deeper synthesis of ideas across papers. There is minimal critical analysis of the cited methods or their limitations, and the abstraction remains at a surface level, offering no new conceptual frameworks or meta-level insights."}}
{"id": "33e781ef-a127-43ed-8702-88e09976b718", "title": "Uncertainty Estimation", "level": "section", "subsections": ["51765ff7-d94c-4419-adeb-5f1d5e3b561a", "65bd2854-4c7c-4495-b6b5-340505392445", "4fdfdf37-e014-4b52-b80a-5b1aa7684c7d", "781f73a5-8ad5-4b3b-aeac-5bca88d02c49", "ad900f38-8b0a-4a6b-ab56-457ba95c6ae7"], "parent_id": "039eb0dd-a257-4d85-a714-ad5fc65c6f0b", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Estimation"]], "content": "\\label{sec:uncertainty_quantification_methods}\nAs described in Section \\ref{sec:uncertainty_types_and_sources}, several factors may cause model and data uncertainty and affect a DNN's prediction. This variety of sources of uncertainty makes the complete exclusion of uncertainties in a neural network impossible for almost all applications. Especially in practical applications employing real world data, the training data is only a subset of all possible input data, which means that a miss-match between the DNN domain and the unknown actual data domain is often unavoidable. However, an exact representation of the uncertainty of a DNN prediction is also not possible to compute, since the different uncertainties can in general not be modeled accurately and are most often even unknown.\\\\\nTherefore, methods for estimating uncertainty in a DNN prediction is a popular and vital field of research. The data uncertainty part is normally represented in the prediction, e.g. in the softmax output of a classification network or in the explicit prediction of a standard deviation in a regression network . In contrast to this, several different approaches which model the model uncertainty and seek to separate it from the data uncertainty in order to receive an accurate representation of the data uncertainty were introduced . \\\\\nIn general, the methods for estimating the uncertainty can be split in four different types based on the number (single or multiple) and the nature (deterministic or stochastic) of the used DNNs. \n\\begin{itemize}\n  \\setlength\\itemsep{0.5em}\n    \\item \\textbf{Single deterministic methods} give the prediction based on one single forward pass within a deterministic network. The uncertainty quantification is either derived by using additional (external) methods or is directly predicted by the network. \n    \\item \\textbf{Bayesian methods} cover all kinds of stochastic DNNs, i.e. DNNs where two forward passes of the same sample generally lead to different results. \n    \\item \\textbf{Ensemble methods} combine the predictions of several different deterministic networks at inference. \n    \\item \\textbf{Test-time augmentation methods} give the prediction based on one single deterministic network but augment the input data at test-time in order to generate several predictions that are used to evaluate the certainty of the prediction.\n\\end{itemize}\n\\input{figures/methods_diagram}\n\\input{tables/methods_overview}\n\\input{figures/UncertaintyMethods}\nIn the following, the main ideas and further extensions of the four types are presented and their main properties are discussed. In Figure \\ref{fig:methods_diagram}, an overview of the different types and methods is given. In Figure \\ref{fig:methods_overview}, the different underlying principles that are used to differentiate between the different types of methods are presented. \nTable \\ref{tab:types_compare} summarizes the main properties of the methods presented in this work, such as complexity, computational effort, memory consumption, flexibility, and others. \n\\input{tables/table_deterministic}", "cites": [8633, 3288], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section introduces a categorization of uncertainty estimation methods but primarily describes the four types without deeply integrating or synthesizing the cited papers into a broader narrative. It does make a basic distinction between data and model uncertainty, which connects to the cited works, but lacks critical evaluation of the methods or deeper abstraction to overarching principles. The inclusion of figures and tables suggests a structured presentation, but the analysis remains at a high-level overview."}}
{"id": "51765ff7-d94c-4419-adeb-5f1d5e3b561a", "title": "Single Deterministic Methods", "level": "subsection", "subsections": ["4bcab73f-de19-4117-aec4-349b0c596eac", "030cf20c-f0d2-4fb7-a22f-4ddb1e9417c8", "dc290065-ab0a-4da2-9665-8ecf9cd04ce4"], "parent_id": "33e781ef-a127-43ed-8702-88e09976b718", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Estimation"], ["subsection", "Single Deterministic Methods"]], "content": "}\\label{sec:deterministic_methods}\nFor deterministic neural networks the parameters are deterministic and each repetition of a forward pass delivers the same result. With single deterministic network methods for uncertainty quantification, we summarize all approaches where the uncertainty on a prediction $y^*$ is computed based on one single forward pass within a deterministic network. In the literature, several such approaches can be found. They can be roughly categorized into approaches where one single network is explicitly modeled and trained in order to quantify uncertainties  and approaches that use additional components in order to give an uncertainty estimate on the prediction of a network . While for the first type, the uncertainty quantification affects the training procedure and the predictions of the network, the latter type is in general applied on already trained networks. Since trained networks are not modified by such methods, they have no effect on the network's predictions. In the following, we call these two types \\textit{internal} and \\textit{external} uncertainty quantification approaches.", "cites": [4617, 4592, 4622, 4621, 8633, 4620, 4613, 4619], "cite_extract_rate": 0.8888888888888888, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic descriptive overview of single deterministic methods for uncertainty quantification, introducing the distinction between internal and external approaches. While it cites multiple papers, it does not deeply synthesize their ideas or compare them meaningfully. It offers limited critical analysis and abstraction, focusing mainly on categorization without broader conceptual insights."}}
{"id": "4bcab73f-de19-4117-aec4-349b0c596eac", "title": "Internal Uncertainty Quantification Approaches", "level": "subsubsection", "subsections": [], "parent_id": "51765ff7-d94c-4419-adeb-5f1d5e3b561a", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Estimation"], ["subsection", "Single Deterministic Methods"], ["subsubsection", "Internal Uncertainty Quantification Approaches"]], "content": "Many of the internal uncertainty quantification approaches followed the idea of predicting the parameters of a distribution over the predictions instead of a direct pointwise maximum-a-posteriori estimation. Often, the loss function of such networks takes the expected divergence between the true distribution and the predicted distribution into account as e.g., in . The distribution over the outputs can be interpreted as a quantification of the model uncertainty (see Section \\ref{sec:uncertainty_types_and_sources}), trying to emulate the behavior of a Bayesian modeling of the network parameters . The prediction is then given as the expected value of the predicted distribution. \nFor classification tasks, the output in general represents class probabilities. These probabilities are a result of applying the softmax function \n\\begin{align}\n\\begin{split}\n&\\text{softmax}:\\mathbb{R}^K\\rightarrow\\left\\{z\\in  \\mathbb{R}^K\\vert z_i \\geq 0, \\sum_{k=1}^K z_k =1\\right\\} \\\\\n&\\text{softmax}(z)_j = \\frac{\\exp(z_j)}{\\sum_{k=1}^K\\exp(z_k)}\n\\end{split}\n\\end{align}\nfor multiclass settings and the sigmoid function \n\\begin{align}\n\\begin{split}\n&\\text{sigmoid}:\\mathbb{R}\\rightarrow[0,1] \\\\\n&\\text{sigmoid}(z) = \\frac{1}{1+\\exp(-z)}\n\\end{split}\n\\end{align}\nfor binary classification tasks on the logits $z$. These probabilities can be already interpreted as a prediction of the data uncertainty. However, it is widely discussed that neural networks are often over-confident and the softmax output is often poorly calibrated, leading to inaccurate uncertainty estimates . Furthermore, the softmax output cannot be associated with model uncertainty. But without explicitly taking the model uncertainty into account, out-of-distribution samples could lead to outputs that certify a false confidence. For example, a network trained on cats and dogs will very likely not result in 50\\% dog and 50\\% cat when it is fed with the image of a bird. This is, because the network extracts features from the image and even though the features do not fit to the cat class, they might fit even less to the dog class. As a result, the network puts more probability on cat. Furthermore, it was shown that the combination of rectified linear unit (ReLu) networks and the softmax output leads to settings where the network becomes more and more confident as the distance between an out-of-distribution sample and the learned training set becomes larger .\nFigure \\ref{fig:softmax_probability} shows an example where the rotation of a digit from MNIST leads to false predictions with high softmax values. \n\\input{figures/rotating_mnist}\nThis phenomenon is described and further investigated by Hein et al.  who proposed a method to avoid this behaviour, based on enforcing a uniform predictive distribution far away from the training data.\nSeveral other classification approaches  followed a similar idea of taking the logit magnitude into account, but make use of the Dirichlet distribution. The Dirichlet distribution is the conjugate prior of the categorical distribution and hence can be interpreted as a distribution over categorical distributions. The density of the Dirichlet distribution is defined by \n\\begin{equation*}\n    \\text{Dir}\\left(\\mu\\vert\\alpha\\right) = \\frac{\\Gamma(\\alpha_0)}{\\prod_{c=1}^{K}\\Gamma(\\alpha_c)}\\prod_{c=1}^{K}\\mu_c^{\\alpha_c-1}, \n    \\quad \\alpha_c > 0,~\\alpha_0=\\sum_{c=1}^K\\alpha_c \\quad,\n\\end{equation*}\nwhere $\\Gamma$ is the gamma function, $\\alpha_1, ..., \\alpha_K$ are called the concentration parameters, and the scalar $\\alpha_0$ is the precision of the distribution. In practice, the concentrations $\\alpha_1,...,\\alpha_K$ are derived by applying a strictly positive transformation, as for example the exponential function, to the logit values. As visualized in Figure \\ref{fig:simplex_uncertainty}, a higher concentration value leads to a sharper Dirichlet distribution. \\\\\nThe set of all class probabilities of a categorical distribution over $k$ classes is equivalent to a $k-1$-dimensional standard or probability simplex. Each node of this simplex represents a probability vector with the full probability mass on one class and each convex combination of the nodes represents a categorical distribution with the probability mass distributed over multiple classes. Malinin et al.  argued that a high model uncertainty should lead to a lower precision value and therefore to a flat distribution over the whole simplex, since the network is not familiar with the data. In contrast to this, data uncertainty should be represented by a sharper but also centered distribution, since the network can handle the data, but cannot give a clear class preference. In Figure \\ref{fig:simplex_uncertainty} the different desired behaviors are shown. \n\\input{figures/solution_simplex}\nThe Dirichlet distribution is utilized in several approaches as \\textit{Dirichlet Prior Networks}  and \\textit{Evidential Neural Networks} . Both of these network types output the parameters of a Dirichlet distribution from which the categorical distribution describing the class probabilities can be derived. The general idea of prior networks  is already described above and is visualized in Figure \\ref{fig:simplex_uncertainty}. \nPrior networks are trained in a multi task way with the goal of minimizing the expected Kullback-Leibler (KL) divergence between the predictions of in-distribution data and a sharp Dirichlet distribution and between a flat Dirichlet distribution and the predictions of out-of-distribution data . Besides the main motivation of a better separation between in-distribution and OOD samples, these approaches also improve the separation between the confidence of correct and incorrect predictions, as was shown by . As a follow up,  discussed that for the case that the data uncertainty is high, the forward definition of the KL-divergence can lead to an undesirable multi-model target distribution. In order to avoid this, they reformulated the loss using the reverse KL-divergence. The experiments showed improved results in the uncertainty estimation as well as for the adversarial robustness. \nZhao et al.  extended the Dirichlet network approach by a new loss function that aims at minimizing an upper bound on the expected error based on the $\\mathcal{L}_\\infty$-norm, i.e. optimizing an expected worst-case upper bound.  argued that using a mixture of Dirichlet distributions gives much more flexibility in approximating the posterior distribution. Therefore, an approach where the network predicts the parameters for a mixture of $K$ Dirichlet distributions was suggested. For this, the network logits represent the parameters for $M$ Dirichlet distributions and additionally $M$ weights $\\omega_i, i=1,..,M$ with the constraint $\\sum_{i=1}^M\\omega_i=1$ are optimized. Nandy et al.  analytically showed that for in-domain samples with high data uncertainty, the Dirichlet distribution predicted for a false prediction is often flatter than for a correct prediction. They argued that this makes it harder to differentiate between in- and out-of-distribution predictions and suggested a regularization term towards maximizing the gap between in- and out-of-distribution samples. \\\\\nEvidential neural networks  also optimize the parameterization of a single Dirichlet network. The loss formulation is derived by using subjective logic and interpret the logits as multinomial opinions or beliefs, as introduced in Evidence or Dempster-Shafer theory . Evidential neural networks set the total amount of evidence in relation with the number of classes and conclude a value of uncertainty from this, i.e. receiving an additional \"I don't know class\". The loss is formulated as expected value of a basic loss, as for example categorical cross entropy, with respect to a Dirichlet distribution parameterized by the logits. Additionally, a regularization term is added, encouraging the network to not consider features that provide evidence for multiple classes at the same time, as for example a circle is for 6 and 8. Due to this, the networks do not differentiate between data uncertainty and model uncertainty, but learn whether they can give a certain prediction or not.   extended this idea by differentiating between acuity and dissonance in the collected evidence in order to better separate in- and out-of-distribution samples. For that, two explicit data sets containing overlapping classes and out-of-distribution samples are needed to learn a regularization term. Amini et al.  transferred the idea of evidential neural networks from classification tasks to regression tasks by learning the parameters of an evidential normal inverse gamma distribution over an underlying Normal distribution. Charpentier et al.  avoided the need of OOD data for the training process by using normalizing flows to learn a distribution over a latent space for each class. A new input sample is projected onto this latent space and a Dirichlet distribution is parameterized based on the class wise densities of the received latent point. \nBeside the Dirichlet distribution based approaches described above, several other internal approaches exist. In , a relatively simple approach based on small pertubations on the training input data and the temperature scaling calibration is presented leading to an efficient differentiation of in- and out-of-distritbuion samples. Mo$\\dot{\\text{z}}$ejko et al.  made use of the inhibited softmax function. It contains an artificial and constant logit that makes the absolute magnitude of the single logits more determining in the softmax output. Van Amersfoort et al.  showed that Radial Basis Function (RBF) networks can be used to achieve competitive results in accuracy and very good results regarding uncertainty estimation. RBF networks learn a linear transformation on the logits and classify inputs based on the distance between the transformed logits and the learned class centroids. In , a scaled exponentiated $L_2$ distance was used. The data uncertainty can be directly derived from the distances between the centroids. By including penalties on the Jacobian matrix in the loss function, the network was trained to be more sensitive to changes in the input space. As a result, the method reached good performance on out-of-distribution detection. In several tests, the approach was compared to a five members deep ensemble  and it was shown that this single network approach performs at least equivalently well on detecting out-of-distribution samples and improves the true-positive rate. \\\\\nFor regression tasks, Oala et al.  introduced an uncertainty score based on the lower and an upper bound output of an interval neural network. The interval neural network has the same structure as the underlying deterministic neural network and is initialized with the deterministic network's weights. In contrast to Gaussian representations of uncertainty given by a standard deviation, this approach can give non-symmetric values of uncertainty. Furthermore, the approach is found to be more robust in the presence of noise. Tagasovska and Lopez-Paz  presented an approach to estimate data and model uncertainty. A simultaneous quantile regression loss function was introduced in order to generate well-calibrated prediction intervals for the data uncertainty.  The model uncertainty is quantified based on a mapping from the training data to zero, based on so called Orthonormal Certificates. The aim was that out-of-distribution samples, where the model is uncertain, are mapped to a non-zero value and thus can be recognized. Kawashima et al.  introduced a method which computes virtual residuals in the training samples of a regression task based on a cross-validation like pre-training step. With original training data expanded by the information of these residuals, the actual predictor is trained to give a prediction and a value of certainty. The experiments indicated that the virtual residuals represent a promising tool in order to avoid overconfident network predictions.", "cites": [4627, 4626, 3289, 4624, 8807, 4625, 4623, 4619, 3288, 4622, 8633, 8639, 1624, 3251, 4611, 4613], "cite_extract_rate": 0.7272727272727273, "origin_cites_number": 22, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple Dirichlet-based and softmax-related uncertainty estimation methods, connecting them through a coherent narrative on data and model uncertainty. It provides critical evaluation by highlighting issues like overconfidence and false certainty in out-of-distribution samples. The discussion abstracts these methods into a conceptual framework involving the probability simplex and divergences, showing meta-level understanding of the field."}}
{"id": "030cf20c-f0d2-4fb7-a22f-4ddb1e9417c8", "title": "External Uncertainty Quantification Approaches", "level": "subsubsection", "subsections": [], "parent_id": "51765ff7-d94c-4419-adeb-5f1d5e3b561a", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Estimation"], ["subsection", "Single Deterministic Methods"], ["subsubsection", "External Uncertainty Quantification Approaches"]], "content": "~\\\\\nExternal uncertainty quantification approaches do not affect the models' predictions, since the evaluation of the uncertainty is separated from the underlying prediction task. Furthermore, several external approaches can be applied to already trained networks at the same time without affecting each other. Raghu et al.  argued that when both tasks, the prediction and the uncertainty quantification, are done by one single method, the uncertainty estimation is biased by the actual prediction task. Therefore, they recommended a \"direct uncertainty prediction\" and suggested to train two neural networks, one for the actual prediction task and a second one for the prediction of the uncertainty on the first network's predictions. Similarly, Ramalho and Miranda  introduced an additional neural network for uncertainty estimation. But in contrast to , the representation space of the training data is considered and the density around a given test sample is evaluated. The additional neural network uses this training data density in order to predict whether the main network's estimate is expected to be correct or false. Hsu et al.  detected out-of-distribution examples in classification tasks at test-time by predicting total probabilities for each class, additional to the categorical distribution given by the softmax output. The class wise total probability is predicted by applying the sigmoid function to the network's logits. Based on these total probabilities, OOD examples can be identified as those with low class probabilities for all classes.  \\\\\nIn contrast to this, Oberdiek et al.  took the sensitivity of the model, i.e. the model's slope, into account by using gradient metrics for the uncertainty quantification in classification tasks. Lee et al.  applied a similar idea but made use of back propagated gradients. In their work they presented state of the art results on out-of-distribution and corrupted input detection.", "cites": [4617, 4592, 7130, 4620, 4621], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from the cited papers by distinguishing between different strategies for external uncertainty quantification and identifying their shared goal of decoupling uncertainty estimation from prediction. It provides some critical analysis by pointing out the issue of bias in single-task models and contrasting different methods (e.g., using representation density vs. gradient sensitivity). However, deeper evaluation of limitations or trade-offs between approaches is limited, and the generalization to broader principles is moderate rather than highly abstract."}}
{"id": "dc290065-ab0a-4da2-9665-8ecf9cd04ce4", "title": "Summing Up Single Deterministic Methods", "level": "subsubsection", "subsections": [], "parent_id": "51765ff7-d94c-4419-adeb-5f1d5e3b561a", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Estimation"], ["subsection", "Single Deterministic Methods"], ["subsubsection", "Summing Up Single Deterministic Methods"]], "content": "Compared to many other principles, single deterministic methods are computational efficient in training and evaluation. For training, only one network has to be trained and often the approaches can even be applied on pre-trained networks. Depending on the actual approach, only a single or at most two forward passes have to be fulfilled for evaluation. The underlying networks could contain more complex loss functions, which slows down the training process  or external components that have to be trained and evaluated additionally . But in general, this is still more efficient than the number of predictions needed for ensembles based methods (Section \\ref{sec:ensemble.methods}), Bayesian methods (Section \\ref{sec:bayesian_methods}), and test-time data augmentation methods (Section \\ref{sec:test_time_augmentation}).\nA drawback of single deterministic neural network approaches is the fact that they rely on a single opinion and can therefore become very sensitive to the underlying network architecture, training procedure, and the training data.\n\\input{tables/table_bayesian_methods}", "cites": [4617, 4613], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a brief comparative overview of single deterministic methods, highlighting their computational efficiency and limitations. It cites two relevant papers but does not deeply synthesize their contributions or connect them to broader themes. The critical analysis is limited but includes an evaluation of methodological drawbacks such as sensitivity to architecture and data."}}
{"id": "65bd2854-4c7c-4495-b6b5-340505392445", "title": "Bayesian Neural Networks", "level": "subsection", "subsections": ["78daf31e-4286-4b9d-b08a-bb3e05166e37", "e6bfbade-59c4-4a36-9057-34e98611718d", "5640575a-f2c5-44fa-8507-32c600eb808c", "3aa6b1db-f185-4451-a4c0-99067177b029"], "parent_id": "33e781ef-a127-43ed-8702-88e09976b718", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Estimation"], ["subsection", "Bayesian Neural Networks"]], "content": "}\\label{sec:bayesian_methods}\nBayesian Neural Networks (BNNs)  have the ability to combine the scalability, expressiveness, and predictive performance of neural networks with the Bayesian learning as opposed to learning via the maximum likelihood principles. This is achieved by inferring the probability distribution over the network parameters $\\theta=(w_1,...,w_K)$. More specifically, given a training input-target pair $(x, y)$ the posterior distribution over the space of parameters $p(\\theta|x,y)$ is modelled by assuming a prior distribution over the parameters $p(\\theta)$ and applying Bayes theorem:\n\\begin{equation}\n\\label{1.b.eq1}\np(\\theta\\vert x,y) = \\frac{p(y\\vert x,\\theta)p(\\theta)}{p(y|x)} \\propto p(y\\vert x,\\theta)p(\\theta).\n\\end{equation}\nHere, the normalization constant in \\eqref{1.b.eq1} is called the model evidence $p(y|x)$ which is defined as\n\\begin{equation}\np(y | x) = \\int p(y\\vert x, \\theta)p(\\theta)d\\theta.\n\\end{equation}\nOnce the posterior distribution over the weights have been estimated, the prediction of an output $y^*$ for a new input data $x^*$ can be obtained by Bayesian Model Averaging or Full Bayesian Analysis that involves marginalizing the likelihood $p(y|x,\\theta)$ with the posterior distribution:\n\\begin{equation}\n\\label{eq:bayesian_marginalization}\np(y^*|x^*, x, y) = \\int p(y^*|x^*,\\theta) p(\\theta|x,y)d\\theta.\n\\end{equation}\nThis Bayesian way of prediction is a direct application of the law of total probability and endows the ability to compute the principled predictive uncertainty. The integral of \\eqref{eq:bayesian_marginalization} is intractable for the most common prior posterior pairs and approximation techniques are therefore typically applied. The most widespread approximation, the \\textit{Monte Carlo Approximation}, follows the law of large numbers and approximates the expected value by the mean of $N$ deterministic networks, $f_{\\theta_1}, ...,f_{\\theta_N}$, parameterized by $N$ samples, $\\theta_1, \\theta_2, ..., \\theta_N$, from the posterior distribution of the weights, i.e.\n\\begin{equation}\n   \ty^*\\quad \\approx \\quad \\frac{1}{N}\\sum_{i=1}^{N} y_i^* \\quad = \\quad \\frac{1}{N}\\sum_{i=1}^{N} f_{\\theta_i}(x^*).\n\\end{equation} \nWilson and Izmailov  argue that a key advantage of BNNs lie in this marginalization step, which particularly can improve both the accuracy and calibration of modern deep neural networks. We note that the use-cases of BNNs are not limited for uncertainty estimation but open up the possibility to bridge the powerful Bayesian tool-boxes within deep learning. Notable examples include Bayesian model selection , model compression , active learning , continual learning , theoretic advances in Bayesian learning  and beyond.\\\\\nWhile the formulation is rather simple, there exist several challenges. For example, no closed form solution exists for the posterior inference as conjugate priors do not typically exist for complex models such as neural networks . Hence, approximate Bayesian inference techniques are often needed to compute the posterior probabilities. Yet, directly using approximate Bayesian inference techniques have been proven to be difficult as the size of the data and number of parameters are too large for the use-cases of deep neural networks. In other words, the integrals of above equations are not computationally tractable as the size of the data and number of parameters grows. Moreover, specifying a meaningful prior for deep neural networks is another challenge that is less understood.\nIn this survey, we classify the BNNs into three different types based on how the posterior distribution is inferred to approximate Bayesian inference:\n\\begin{itemize}\n  \\setlength\\itemsep{0.5em}\n    \\item \\textit{Variational inference} \\\\\n    Variational inference approaches approximate the (in general intractable) posterior distribution by optimizing over a family of tractable distributions. \n    \\item \\textit{Sampling approaches} \\\\\n    Sampling approaches deliver a representation of the target random variable from which realizations can be sampled. Such methods are based on Markov Chain Monte Carlo and further extensions. \n    \\item \\textit{Laplace approximation}  \\\\\n    Laplace approximation simplifies the target distribution by approximating the log-posterior distribution and then, based on this approximation, deriving a normal distribution over the network weights.\n\\end{itemize}\nWhile limiting our scope to these three categories, we also acknowledge several advances in related domains of BNN research. Some examples are (i) approximate inference techniques such as alpha divergence , expectation propagation , assumed density filtering  etc, (ii) probabilistic programming to exploit modern Graphical Processing Units (GPUs) , (iii) different types of priors , (iv) advancements in theoretical understandings of BNNs , (iv) uncertainty propagation techniques to speed up the marginalization procedures  and (v) computations of aleatoric uncertainty .\\\\", "cites": [4629, 4638, 4630, 4639, 4631, 4636, 4632, 8811, 4634, 8813, 1044, 8812, 4597, 4628, 4637, 8809, 4633, 4641, 4640, 4635, 8810], "cite_extract_rate": 0.5, "origin_cites_number": 42, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes information from multiple papers to present a structured classification of Bayesian Neural Network approaches. It offers critical analysis by acknowledging limitations such as intractability and challenges in prior specification. The abstraction level is strong, as it frames Bayesian methods within a broader theoretical and practical context, highlighting their potential beyond uncertainty estimation."}}
{"id": "78daf31e-4286-4b9d-b08a-bb3e05166e37", "title": "Variational Inference", "level": "subsubsection", "subsections": [], "parent_id": "65bd2854-4c7c-4495-b6b5-340505392445", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Estimation"], ["subsection", "Bayesian Neural Networks"], ["subsubsection", "Variational Inference"]], "content": "The goal of variational inference is to infer the posterior probabilities $p(\\theta|x,y)$ using a pre-specified family of distributions $q(\\theta)$. Here, these so-called variational family $q(\\theta)$ is defined as a parametric distribution. An example is the Multivariate Normal distribution where its parameters are the mean and the covariance matrix. The main idea of variational inference is to find the settings of these parameters that make $q(\\theta)$ to be close to the posterior of interest $p(\\theta|x,y)$. This measure of closeness between the probability distributions are given by the Kullback-Leibler (KL) divergence\n\\begin{equation}\\label{eq:kl}\n    \\text{KL}(q||p) = \\mathbb{E}_q\\left [ \\text{log} \\frac{q(\\theta)}{p(\\theta|x,y)} \\right ].\n\\end{equation}\nDue to the posterior $p(\\theta\\vert x, y)$ the KL-divergence in \\eqref{eq:kl} can not be minimized directly. Instead, the evidence lower bound (ELBO), a function that is equal to the KL divergence up to a constant, is optimized. For a given prior distribution on the parameters $p(\\theta)$, the ELBO is given by\n\\begin{equation}\n     L = \\mathbb{E}_q\\left[\\log\\frac{p(y|x,\\theta)}{q(\\theta)}\\right]\n\\end{equation}\nand for the KL divergence \n\\begin{equation}\n    \\text{KL}(q||p) = -L + \\log p(y\\vert x)\n\\end{equation}\nholds.\nVariational methods for BNNs have been pioneered by Hinton and Van Camp  where the authors derived a diagonal Gaussian approximation to the posterior distribution of neural networks (couched in information theory - a minimum description length). Another notable extension in 1990s has been proposed by Barber and Bishop , in which the full covariance matrix was chosen as the variational family, and the authors demonstrated how the ELBO can be optimized for neural networks. Several modern approaches can be viewed as extensions of these early works  with a focus on how to scale the variational inference to modern neural networks. \\\\\nAn evident direction with the current methods are the use of stochastic variational inference (or Monte-Carlo variational inference), where the optimization of ELBO is performed using mini-batch of data. One of the first connections to stochastic variational inference has been proposed by Graves et al.  with Gaussian priors. In 2015, Blundell et al.  introduced Bayes By Backprop, a further extension of stochastic variational inference   to non-Gaussian priors and demonstrated how the stochastic gradients can be made unbiased. Notable, Kingma et al.  introduced the local reparameterization trick to reduce the variance of the stochastic gradients. One of the key concepts is to reformulate the loss function of neural network as the ELBO. As a result the intractable posterior distribution is indirectly optimized and variational inference is compatible to back-propagation with certain modifications to the training procedure. These extensions widely focus on the fragility of stochastic variational inference that arises due to sensitivity to initialization, prior definition and variance of the gradients. These limitations have been addressed recently by Wu et al. , where a hierarchical prior was used and the moments of the variational distribution are approximated deterministically. \\\\\nAbove works commonly assumed mean-field approximations as the variational family, neglecting the correlations between the parameters. In order to make more expressive variational distributions feasible for deep neural networks, several works proposed to infer using the matrix normal distribution  or more expressive variants  where the covariance matrix is decomposed into the Kronecker products of smaller matrices or in a low rank form plus a positive diagonal matrix. A notable contribution towards expressive posterior distributions has been the use of normalizing flows  - a hierarchical probability distribution where a sequence of invertible transformations are applied so that a simple initial density function is transformed into a more complex distribution. Interestingly, Farquhar et al.  argue that mean-field approximation is not a restrictive assumption, and the layer-wise weight correlations may not be as important as capturing the depth-wise correlations. While the claim of Farquhar et al.  may remain to be an open question, the mean-field approximations have an advantage on smaller computational complexities . For example, Osawa et al.  demonstrated that variational inference can be scaled up to ImageNet size data-sets and architectures using multiple GPUs and proposed practical tricks such as data augmentation, momentum initialization and learning rate scheduling. \\\\\nOne of the successes in variational methods have been accomplished by casting existing stochastic elements of deep learning as variational inference. A widely known example is Monte Carlo Dropout (MC Dropout) where the dropout layers are formulated as Bernoulli distributed random variables, and training a neural network with dropout layers can be approximated as performing variational inference . A main advantage of MC dropout is that the predictive uncertainty can be computed by activating dropout not only during training, but also at test time. In this way, once the neural network is trained with dropout layers, the implementation efforts can be kept minimum and the practitioners do not need expert knowledge to reason about uncertainty - certain criteria that the authors are attributing to its success  . The practical values of this method has been demonstrated also in several works  and resulted in different extensions (evaluating the usage of different dropout masks for example for convolutional layers  or by changing the representations of the predictive uncertainty into model and data uncertainties ). Approaches that build upon the similar idea but randomly drop incoming activations of a node, instead of dropping an activation for all following nodes, were also proposed within the literature  and called drop connect. This was found to be more robust on the uncertainty representation, even though it was shown that a combination of both can lead to higher accuracy and robustness in the test predictions . Lastly, connections of variation inference to Adam , RMS Prop  and batch normalization  have been further suggested in the literature.\\\\", "cites": [4601, 8815, 4648, 4649, 4646, 4647, 4643, 4618, 3329, 4644, 8814, 4025, 4642, 4595, 4645, 8816, 3278], "cite_extract_rate": 0.6071428571428571, "origin_cites_number": 28, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.3, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a coherent synthesis of variational inference in Bayesian neural networks, linking foundational concepts to modern advancements. It critically discusses limitations such as fragility in optimization and trade-offs in computational complexity. The abstraction is strong, with general observations on trends like the shift from mean-field to structured posteriors and connections between variational inference and optimization techniques."}}
{"id": "e6bfbade-59c4-4a36-9057-34e98611718d", "title": "Sampling Methods", "level": "subsubsection", "subsections": [], "parent_id": "65bd2854-4c7c-4495-b6b5-340505392445", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Estimation"], ["subsection", "Bayesian Neural Networks"], ["subsubsection", "Sampling Methods"]], "content": "Sampling methods, or also often called Monte Carlo methods, are another family of Bayesian inference algorithms that represent uncertainty without a parametric model. Specifically, sampling methods use a set of hypotheses (or samples) drawn from the distribution and offer an advantage that the representation itself is not restricted by the type of distribution (e.g. can be multi-modal or non-Gaussian) - hence probability distributions are obtained non-parametrically. Popular algorithms within this domain are particle filtering, rejection sampling, importance sampling and Markov Chain Monte Carlo sampling (MCMC) . \\\\ \nIn case of neural networks, MCMC is often used since alternatives such as rejection and importance sampling are known to be inefficient for such high dimensional problems. The main idea of MCMC is to sample from arbitrary distributions by transition in state space where this transition is governed by a record of the current state and the proposal distribution that aims to estimate the target distribution (e.g. the true posterior). To explain this, let us start defining the Markov Chain: a Markov Chain is a distribution over random variables $x_1, \\cdots, x_T$ which follows the state transition rule:\n\\begin{align}\np(x_1,\\cdots, x_T) = p(x_1)\\prod_{t=2}^{T}p(x_t|x_{t-1}),\n\\end{align}\ni.e. the next state only depends on the current state and not on any other former state. \nIn order to draw samples from the true posterior, MCMC sampling methods first generate samples in an iterative and the Markov Chain fashion. Then, at each iteration, the algorithm decides to either accept or reject the samples where the probability of acceptance is determined by certain rules. In this way, as more and more samples are produced, their values can approximate the desired distribution. \\\\\nHamiltonian Monte Carlo or Hybrid Monte Carlo (HMC)  is an important variant of MCMC sampling method (pioneered by Neals  for neural networks), and is often known to be the gold standards of Bayesian inference . The algorithm works as follows: (i) start by initializing a set of parameters $\\theta$ (either randomly or in a user-specific manner). Then, for a given number of total iterations, (ii) instead of a random walk, a momentum vector - an auxiliary variable $\\rho$ is sampled, and the current value of parameters $\\theta$ is updated via the Hamiltonian dynamics:\n\\begin{align}\nH(\\rho, \\theta) = -\\text{log} p(\\rho, \\theta) = -\\text{log} p(\\rho|\\theta) - \\text{log} p(\\theta).\n\\end{align}\nDefining the potential energy ($V(\\theta) = - log p(\\theta)$ and the kinetic energy $T(\\rho|\\theta) = -\\text{log} p(\\rho|\\theta)$, the update steps via Hamilton's equations are governed by,\n\\begin{align}\n\\frac{d\\theta}{dt} &= \\frac{\\partial H}{\\partial \\rho} = \\frac{\\partial T}{\\partial \\rho} \\ \\text{and}\\\\\n\\frac{d\\rho}{dt} &= - \\frac{\\partial H}{\\partial \\theta} = -\\frac{\\partial T}{\\partial \\theta} - \\frac{\\partial V}{\\partial \\theta}.\n\\end{align}\nThe so-called leapfrog integrator is used as a solver . (iii) For each step, a Metropolis acceptance criterion is applied to either reject or accept the samples (similar to MCMC). Unfortunately, HMC requires the processing of the entire data-set per iteration, which is computationally too expensive when the data-set size grows to million to even billions. Hence, many modern algorithms focus on how to perform the computations in a mini-batch fashion stochastically. In this context, for the first time, Welling and Teh  proposed to combine Stochastic Gradient Descent (SGD) with Langevin dynamics (a form of MCMC ) in order to obtain a scalable approximation to MCMC algorithm based on mini-batch SGD . The work demonstrated that performing Bayesian inference on Deep Neural Networks can be as simple as running a noisy SGD. This method does not include the momentum term of HMC via using the first order Langevin dynamics and opened up a new research area on Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC).  \\\\\nConsequently, several extensions are available which include the use of 2nd order information such as preconditioning and optimizing with the Fisher Information Matrix (FIM) , the Hessian , adapting preconditioning diagonal matrix , generating samples from non-isotropic target densities using Fisher scoring , and samplers in the Riemannian manifold  using the first order Langevin dynamics and Levy diffusion noise and momentum . Within these methods, the so-called parameter dependent diffusion matrices are incorporated with an intention to offset the stochastic perturbation of the gradient. To do so, the \"thermostat\" ideas  are proposed so that a prescribed constant temperature distribution is maintained with the parameter dependent noise. Ahn et al.  devised a distributed computing system for SG-MCMC to exploit the modern computing routines, while Wang et al.  showed that Generative Adversarial Models (GANs) can be used to distill the samples for improved memory efficiency, instead of distillation for enhancing the run-time capabilities of computing predictive uncertainty . Lastly, other recent trends are techniques that reduce the variance  and bias  arising from stochastic gradients. \\\\\nConcurrently, there have been solid advances in theory of SG-MCMC methods and their applications in practice. Sato and Nakagawa , for the first time, showed that the SGLD algorithm with constant step size weakly converges; Chen et al.  showed that faster convergence rates and more accurate invariant measures can be observed for SG-MCMCs with higher order integrators rather than a 1st order Euler integrator while Teh et al.  studied the consistency and fluctuation properties of the SGLD. As a result, verifiable conditions obeying a central limit theorem for which the algorithm is consistent, and how its asymptotic bias-variance decomposition depends on step-size sequences have been discovered. A more detailed review of the SG-MCMC with a focus on supporting theoretical results can be found in Nemeth and Fearnhead . Practically, SG-MCMC techniques have been applied to shape classification and uncertainty quantification , empirically study and validate the effects of tempered posteriors (or called cold-posteriors)  and train a deep neural network in order to generalize and avoid over-fitting .\\\\", "cites": [4636, 4652, 8819, 4650, 8826, 8818, 8825, 8822, 4657, 4655, 8823, 4654, 4651, 3301, 8817, 4653, 166, 8824, 8821, 4656, 8820], "cite_extract_rate": 0.5121951219512195, "origin_cites_number": 41, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key sampling methods for Bayesian inference in neural networks, particularly emphasizing MCMC and its variants like HMC and SG-MCMC. It identifies limitations of certain methods (e.g., computational cost) and highlights theoretical advancements (e.g., convergence properties). The abstraction is strong, connecting ideas to broader principles like scalability, geometry adaptation, and noise control."}}
{"id": "5640575a-f2c5-44fa-8507-32c600eb808c", "title": "Laplace Approximation", "level": "subsubsection", "subsections": [], "parent_id": "65bd2854-4c7c-4495-b6b5-340505392445", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Estimation"], ["subsection", "Bayesian Neural Networks"], ["subsubsection", "Laplace Approximation"]], "content": "The goal of the Laplace Approximation is to estimate the posterior distribution over the parameters of neural networks $p(\\theta\\mid x,y)$ around a local mode of the loss surface with a Multivariate Normal distribution. The Laplace Approximation to the posterior can be obtained by taking the second-order Taylor series expansion of the log posterior over the weights around the MAP estimate $\\hat \\theta$ given some data $(x, y)$. If we assume a Gaussian prior with a scalar precision value $\\tau>0$, then this corresponds to the commonly used $L_2$-regularization, and the Taylor series expansion results in\n\\begin{align*}\n\\log p(\\theta\\mid x,y) & \\approx \\log p(\\hat \\theta \\mid x,y) \\\\ & + \\frac{1}{2}(\\theta -\\hat \\theta)^T(H + \\tau I)(\\theta -\\hat \\theta),\n\\end{align*}\nwhere the first-order term vanishes because the gradient of the log posterior $\\delta\\theta=\\nabla \\log p(\\theta \\mid x,y)$ is zero at the maximum $\\hat \\theta$. Taking the exponential on both sides and approximating integrals by reverse engineering densities, the weight posterior is approximately a Gaussian with the mean $\\hat \\theta$ and the covariance matrix $(H+\\tau I)^{-1}$ where $H$ is the Hessian of $\\log p(\\theta\\mid x,y) $. This means that the model uncertainty is represented by the Hessian $H$ resulting in a Multivariate Normal distribution:\n\\begin{align}\n    p(\\theta\\mid x,y) \\sim \\mathcal{N}\\left(\\hat \\theta, (H+\\tau I)^{-1}\\right).\n\\end{align}\nIn contrast to the two other methods described, the Laplace approximation can be applied on already trained networks, and is generally applicable when using standard loss functions such as MSE or cross entropy and piece-wise linear activations (e.g RELU). Mackay  and Denker et al.  have pioneered the Laplace approximation for neural networks in 1990s, and several modern methods provide an extension to deep neural networks . \\\\\nThe core of the Laplace Approximation is the estimation of the Hessian. Unfortunately, due to the enormous number of parameters in modern neural networks, the Hessian matrices cannot be computed in a feasible way as opposed to relative smaller networks in Mackay  and Denker et al. . Consequently, several different ways for approximating $H$ have been proposed in the literature. A brief review is as follows. Instead of diagonal approximations  (e.g. , ), several researchers have been focusing on including the off-diagonal elements (e.g. ,  and ). Amongst them, layer-wise Kronecker Factor approximation of , ,  and  have demonstrated a notable scalability . A recent extension can be found in  where the authors propose to re-scale the eigen-values of the Kronecker factored matrices so that the diagonal variance in its eigenbasis is accurate. The work presents an interesting idea as one can prove that in terms of a Frobenius norm, the proposed approximation is more accurate than that of . However, as this approximation is harmed by inaccurate estimates of eigenvectors, Lee et al.  proposed to further correct the diagonal elements in the parameter space. \\\\\nExisting works obtain Laplace Approximation using various approximation of the Hessian in the line of fidelity-complexity trade-offs. For several works, an approximation using the diagonal of the Fisher information matrix or Gauss Newton matrix, leading to independently distributed model weights, have been utilized in order to prune weights  or perform continual learning in order to avoid catastrophic forgetting . In Ritter et al. , the Kronecker factorization of the approximate block-diagonal Hessian  have been applied to obtain scalable Laplace Approximation for neural networks. With this, the weights among different layers are still assumed to be independently distributed, but not the correlations within the same layer. Recently, building upon the current understandings of neural network's loss landscape that many eigenvalues of the Hessian tend to be zero,  developed a low rank approximation that leads to sparse representations of the layers' co-variance matrices. Furthermore, Lee et al.  demonstrated that the Laplace Approximation can be scaled to ImageNet size data-sets and architectures, and further showed that with the proposed sparsification technique, the memory complexity of modelling correlations can be made similar to the diagonal approximation. Lastly, Kristiadi et al.  proposed a simple procedure to compute the last-layer Gaussian approximation (neglecting the model uncertainty in all other layers of neural networks), and showed that even such a minimalist solution can mitigate overconfidence predictions of ReLU networks.\\\\\nRecent efforts have extended the Laplace Approximation beyond the Hessian approximation. To tackle the widely known assumption that the Laplace Approximation is for the bell shaped true posterior and thus resulting in under-fitting behavior , Humt et al.  proposed to use Bayesian Optimization and showed that hyperparameters of the Laplace Approximation can be efficiently optimized with increaed calibration performance. Another work in this domain is by Kristiadi et al. , who proposed uncertainty units - a new type of hidden units that changes the geometry of the loss landscape so that more accurate inference is possible. While Shinde et al.  demonstrated practical effectiveness of the Laplace Approximation to the autonomous driving applications, Feng et al.  showed the possibility to (i) incorporate contextual information and (ii) domain adaptation in a semi-supervised manner within the context of image classification. This is achieved by designing unary potentials within a Conditional Random Field. Several real-time methods also exist that do not require multiple forward passes to compute the predictive uncertainty. So-called linearized Laplace Approximation has been proposed in  using the ideas of Mackay  and have been extended with Laplace bridge for classification . Within this framework, Daxberger et al.  proposed inferring the sub-networks to increase the expressivity of covariance propagation while remaining computationally tractable.\\\\", "cites": [4664, 153, 4663, 4660, 8828, 3300, 1562, 4667, 8827, 4659, 4666, 4661, 4665, 4658, 4662], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 27, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple works on Laplace Approximation, connecting ideas such as Hessian estimation, Kronecker factorization, and sparsification techniques. It critically discusses limitations (e.g., scalability, eigenvector inaccuracies) and highlights methodological trade-offs (fidelity vs. complexity). The abstraction level is strong as it identifies overarching trends in Hessian approximation methods and their implications for model uncertainty and calibration."}}
{"id": "3aa6b1db-f185-4451-a4c0-99067177b029", "title": "Sum Up Bayesian Methods", "level": "subsubsection", "subsections": [], "parent_id": "65bd2854-4c7c-4495-b6b5-340505392445", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Estimation"], ["subsection", "Bayesian Neural Networks"], ["subsubsection", "Sum Up Bayesian Methods"]], "content": "Bayesian methods for deep learning have emerged as a strong research domain by combining principled Bayesian learning for deep neural networks. A review of current BNNs has been provided with a focus on mostly, how the posterior $p(\\theta\\vert x,y)$ is inferred. As an observation, many of the recent breakthroughs have been achieved by performing approximate Bayesian inference in a mini-batch fashion (stochastically) or investigating relatively simple but scalable techniques such as MC-dropout or Laplace Approximation. As a result, several works demonstrated that the posterior inference in large scale settings are now possible , and the field has several practical approximation tools to compute more expressive and accurate posteriors since the revival of BNNs beyond the pioneers . There are also emerging challenges on new frontiers beyond accurate inference techniques. Some examples are: (i) how to specify meaningful priors? , (ii) how to efficiently marginalize over the parameters for fast predictive uncertainty?  (iii) infrastructures such as new benchmarks, evaluation protocols and software tools  , and (iv) towards better understandings on the current methodologies and their potential applications .", "cites": [4629, 4638, 4630, 4664, 8829, 3302, 4668, 4669, 4661, 3301, 4665, 4640, 3278, 4656], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 21, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key trends in Bayesian neural networks, particularly around scalable approximate inference techniques like MC-dropout and Laplace Approximation. It abstracts these methods into broader themes such as posterior inference, prior specification, and uncertainty estimation infrastructure. While it does not provide a deeply critical evaluation of each approach, it does highlight limitations and open challenges, contributing to a nuanced analytical overview."}}
{"id": "7e3b6412-5190-4394-be53-d12c07b2aac7", "title": "Principles of Ensemble Methods", "level": "subsubsection", "subsections": [], "parent_id": "4fdfdf37-e014-4b52-b80a-5b1aa7684c7d", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Estimation"], ["subsection", "Ensemble Methods"], ["subsubsection", "Principles of Ensemble Methods"]], "content": "Ensembles derive a prediction based on the predictions received from multiple so-called ensemble members.\nThey target at a better generalization by making use of synergy effects among the different models, arguing that a group of decision makers tend to make better decisions than a single decision maker . For an ensemble $f:X \\rightarrow Y$ with members $f_i:X \\rightarrow Y$ for $i \\in {1,2,...,M}$, this could be for example implemented by simply averaging over the members' predictions, \n\\begin{equation*}\n    f(x):=\\frac{1}{M} \\sum_{i=1}^M f_i(x)~.\n\\end{equation*}\nBased on this intuitive idea, several works applying ensemble methods to different kinds of practical tasks and approaches, as for example bioinformatics , remote sensing , or reinforcement learning  can be found in the literature. Besides the improvement in the accuracy, ensembles give an intuitive way of representing the model uncertainty on a prediction by evaluating the variety among the member's predictions. \\\\\nCompared to Bayesian and single deterministic network approaches, ensemble methods have two major differences. First, the general idea behind ensembles is relatively clear and there are not many groundbreaking differences in the application of different types of ensemble methods and their application in different fields. Hence, this section focuses on different strategies to train an ensemble and some variations that target on making ensemble methods more efficient. Second, ensemble methods were originally not introduced to explicitly handle and quantify uncertainties in neural networks. Although the derivation of uncertainty from ensemble predictions is obvious, since they actually aim at reducing the model uncertainty, ensembles were first introduced and discussed in order to improve the accuracy on a prediction . Therefore, many works on ensemble methods do not explicitly take the uncertainty into account. Notwithstanding this, ensembles have been found to be well suited for uncertainty estimations in neural networks .\\\\", "cites": [4670, 3288], "cite_extract_rate": 0.18181818181818182, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear explanation of ensemble methods and their application in uncertainty estimation, drawing on the cited papers to highlight their use in different domains and their connection to reducing model uncertainty. It offers some critical perspective by noting that ensembles were not originally designed for uncertainty quantification and that many works do not explicitly consider it. However, it lacks deeper comparative or novel synthesis of the cited works and remains at a moderate level of abstraction."}}
{"id": "37e3936a-4b5d-49cd-811b-c73e9b28a890", "title": "Single- and Multi-Mode Evaluation", "level": "subsubsection", "subsections": [], "parent_id": "4fdfdf37-e014-4b52-b80a-5b1aa7684c7d", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Estimation"], ["subsection", "Ensemble Methods"], ["subsubsection", "Single- and Multi-Mode Evaluation"]], "content": "One main point where ensemble methods differ from the other methods presented in this paper is the number of local optima that are considered, i.e. the differentiation into \\textit{single-mode} and \\textit{multi-mode} evaluation. \\\\\nIn order to create synergies and marginalise false predictions of single members, the members of an ensemble have to behave differently in case of an uncertain outcome. The mapping defined by a neural network is highly non-linear and hence the optimized loss function contains many local optima to which a training algorithm could converge to. Deterministic neural networks converge to one single local optimum in the solution space . Other approaches, e.g. BNNs, still converge to one single optimum, but additionally take the uncertainty on this local optimum into account . This means, that neighbouring points within a certain region around the solution also affect the loss and also influence the prediction of a test sample. Since these methods focus on single regions, the evaluation is called \\textit{single-mode} evaluation. In contrast to this, ensemble methods consist of several networks, which should converge to different local optima. This leads to a so called multi-mode evaluation . \n\\input{figures/loss_landscape}\nIn Figure \\ref{fig:loss_landscape}, the considered parameters of a single-mode deterministic, single-mode probabilistic (Bayesian) and multi-mode ensemble approach are visualized. The goal of multi-mode evaluation is that different local optima could lead to models with different strengths and weaknesses in the predictions such that a combination of several such models brings synergy effects improving the overall performance. \\\\", "cites": [4671], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a clear analytical distinction between single-mode and multi-mode evaluation by synthesizing the core idea from the cited paper on deep ensembles and integrating it with concepts from Bayesian neural networks. It constructs a conceptual framework that highlights the role of multiple local optima in ensemble methods. However, it offers limited critical evaluation of the cited works and could benefit from a more nuanced discussion of their limitations or trade-offs."}}
{"id": "9ce35270-ac08-482d-92ab-e17c1f1db144", "title": "Bringing Variety into Ensembles", "level": "subsubsection", "subsections": [], "parent_id": "4fdfdf37-e014-4b52-b80a-5b1aa7684c7d", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Estimation"], ["subsection", "Ensemble Methods"], ["subsubsection", "Bringing Variety into Ensembles"]], "content": "One of the most crucial points when applying ensemble methods is to maximize the variety in the behaviour among the single networks . In order to increase the variety, several different approaches can be applied:\n\\begin{itemize}\n  \\setlength\\itemsep{0.5em}\n    \\item \\textbf{Random Initialization and Data Shuffle} \\\\\n    Due to the very non-linear loss landscape, different initializations of a neural network lead in general to different training results. Since the training is realized on mini-batches, the order of the training data points also affects the final result. \n    \\item \\textbf{Bagging and Boosting}\\\\\n    Bagging (\\textbf{B}ootstrap \\textbf{agg}regat\\textbf{ing}) and Boosting are two strategies that vary the distribution of the used training data sets by sampling new sets of training samples from the original set. Bagging is sampling from the training data uniformly and with replacement . Thanks to the replacement process, ensemble members can see single samples several times in the training set while missing some other training samples. For boosting, the members are trained one after another and the probability of sampling a sample for the next training set is based on the performance of the already trained ensemble . \n    \\item \\textbf{Data Augmentation}\\\\\n    Augmenting the input data randomly for each ensemble member leads to models trained on different data points and therefore in general to a larger variety among the different members. \n    \\item \\textbf{Ensemble of different Network Architecture}\\\\\n    The combination of different network architectures leads to different loss landscapes and can therefore also increase the diversity in the resulting predictions .\n\\end{itemize}\nIn several works, it has been shown that the variety induced by random initialization works sufficiently and that bagging could even lead to a weaker performance . Livieris et al.  evaluated different bagging and boosting strategies for ensembles of weight constrained neural networks. Interestingly, it is found that bagging performs better for a small number of ensemble members while boosting performs better for a large number.\nNanni et al.  evaluated ensembles based on different types of image augmentation for bioimage classification tasks and compared those to each other. Guo and Gould  used augmentation methods within in an ensemble approach for object detection. Both works stated that the ensemble approach using augmentations improves the resulting accuracy. In contrast to this,  stated with respect to uncertainty quantification that image augmentation can harm the calibration of an ensemble and post-processing calibration methods have to be slightly adapted when using ensemble methods. Other ways of inducing variety for specific tasks have been also introduced. For instance, in , the members are trained with different attention masks in order to focus on different parts of the input data. Other approaches focused on the training process and introduced learning rate schedulers that are designed to discover several local optima within one training process . Following, an ensemble can be built based on local optima found within one single training run. It is important to note that if not explicitly stated, the works and approaches presented so far targeted on improvements in the predictive accuracy and did not explicitly consider uncertainty quantification. \\\\", "cites": [4678, 4677, 4672, 4676, 4673, 4675, 4674, 3288], "cite_extract_rate": 0.6153846153846154, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers on strategies to introduce variety in ensembles, such as random initialization, bagging, boosting, data augmentation, and attention-based methods. It provides some critical insights, such as the trade-off between accuracy and calibration when combining data augmentation with ensembles. While it identifies broader patterns (e.g., the general importance of diversity), it does not fully abstract these into a meta-level theoretical framework."}}
{"id": "c3b26604-2508-4eee-b0cc-935596bd13d0", "title": "Ensemble Methods and Uncertainty Quantification", "level": "subsubsection", "subsections": [], "parent_id": "4fdfdf37-e014-4b52-b80a-5b1aa7684c7d", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Estimation"], ["subsection", "Ensemble Methods"], ["subsubsection", "Ensemble Methods and Uncertainty Quantification"]], "content": "Besides the improvement in the accuracy, ensembles are widely used for modelling uncertainty on predictions of complex models, as for example in climate prediction . Accordingly, ensembles are also used for quantifying the uncertainty on a deep neural network's prediction, and over the last years they became more and more popular for such tasks . Lakshminarayanan et al.  are often referenced as a base work on uncertainty estimations derived from ensembles of neural networks and as a reference for the competitiveness of deep ensembles. They introduced an ensemble training pipeline to quantify predictive uncertainty within DNNs. In order to handle data and model uncertainty, the member networks are designed with two heads, representing the prediction and a predicted value of data uncertainty on the prediction. The approach is evaluated with respect to accuracy, calibration, and out-of-distribution detection for classification and regression tasks. In all tests, the method performs at least equally well as the BNN approaches used for comparison, namely Monte Carlo Dropout and Probabilistic Backpropagation. Lakshminarayanan et al.  also showed that shuffling the training data and a random initialization of the training process induces a sufficient variety in the models in order to predict the uncertainty for the given architectures and data sets. Furthermore, bagging is even found to worsen the predictive uncertainty estimation, extending the findings of Lee et al. , who found bagging to worsen the predictive accuracy of ensemble methods on the investigated tasks. Gustafsson et al.  introduced a framework for the comparison of uncertainty quantification methods with a specific focus on real life applications. Based on this framework, they compared ensembles and Monte Carlo dropout and found ensembles to be more reliable and applicable to real life applications. These findings endorse the results reported by Beluch et al.  who found ensemble methods to deliver more accurate and better calibrated predictions on active learning tasks than Monte Carlo Dropout. Ovadia et al.  evaluated different uncertainty quantification methods based on test sets affected by distribution shifts. The excessive evaluation contains a variety of model types and data modalities. As a take away, the authors stated that already for a relatively small ensemble size of five, deep ensembles seem to perform best and are more robust to data set shifts than the compared methods. Vyas et al.  presented an ensemble method for the improved detection of out-of-distribution samples. For each member, a subset of the training data is considered as out-of-distribution. For the training process, a loss, seeking a minimum margin greater zero between the average entropy of the in-domain and the out-of-distribution subsets is introduced and leads to a significant improvement in the out-of-distribution detection. \\\\", "cites": [4616, 3293, 4600, 4675, 3288], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "comparative", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively synthesizes several ensemble-based uncertainty quantification papers, highlighting their methods and common conclusions about the performance of deep ensembles. It includes critical evaluations by contrasting ensemble methods with Bayesian approaches and noting limitations such as the negative impact of bagging. While it identifies trends in ensemble effectiveness, it stops short of providing a meta-level abstraction or a novel conceptual framework."}}
{"id": "7c4dbab1-06f4-4752-955d-84b7181f67aa", "title": "Making Ensemble Methods more Efficient", "level": "subsubsection", "subsections": [], "parent_id": "4fdfdf37-e014-4b52-b80a-5b1aa7684c7d", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Estimation"], ["subsection", "Ensemble Methods"], ["subsubsection", "Making Ensemble Methods more Efficient"]], "content": "Compared to single model methods, ensemble methods come along with a significantly increased computational effort and memory consumption . When deploying an ensemble for a real life application the available memory and computational power are often limited. Such limitations could easily become a bottleneck  and could become critical for applications with limited reaction time. Reducing the number of models leads to less memory and computational power consumption.\n\\textit{Pruning approaches} reduce the complexity of ensembles by pruning over the members and reducing the redundancy among them. For that, several approaches based on different diversity measures are developed to remove single members without strongly affecting the performance . \nDistillation is another approach where the number of networks is reduced to one single model. It is the procedure of teaching a single network to represent the knowledge of a group of neural networks . First works on the distillation of neural networks were motivated by restrictions when deploying large scale classification problems . The original classification problem is separated into several sub-problems focusing on single blocks of classes that are difficult to differentiate. Several smaller trainer networks are trained on the sub-problems and then teach one student network to separate all classes at the same time. In contrast to this, \\textit{Ensemble distillation approaches} capture the behaviour of an ensemble by one single network. First works on ensemble distillation used the average of the softmax outputs of the ensemble members in order to teach a student network the derived predictive uncertainty . Englesson and Azizpour  justify the resulting predictive distributions of this approach and additionally cover the handling of out-of-distribution samples. When averaging over the members' outputs, the model uncertainty, which is represented in the variety of ensemble outputs, gets lost. To overcome this drawback, researchers applied the idea of learning higher order distributions, i.e. distributions over a distribution, instead of directly predicting the output . The members are then distillated based on the divergence from the average distribution. The idea is closely related to the prior networks  and the evidential neural networks , which are described in Section \\ref{sec:deterministic_methods}.  modelled ensemble members and the distilled network as prior networks predicting the parameters of a Dirichlet distribution. The distillation then seeks to minimize the KL divergence between the averaged Dirichlet distributions of the ensemble members and the output of the distilled network. Lindqvist et al.  generalized this idea to any other parameterizable distribution. With that, the method is also applicable to regression problems, for example by predicting a mean and standard deviation to describe a normal distribution. Within several tests, the distillation models generated by these approaches are able to distinguish between data uncertainty and model uncertainty. Although distillation methods cannot completely capture the behaviour of an underlying ensemble, it has been shown that they are capable of delivering good and for some experiments even comparable results . \\\\\nOther approaches, as \\textit{sub-ensembles}  and \\textit{batch-ensembles}  seek to reduce the computation effort and memory consumption by sharing parts among the single members. It is important to note that the possibility of using different model architectures for the ensemble members could get lost when parts of the ensembles are shared. Also, the training of the models cannot be run in a completely independent manner. Therefore, the actual time needed for training does not necessarily decrease in the same way as the computational effort does. \\\\\nSub-ensembles  divide a neural network architecture into two sub-networks. The trunk network for the extraction of general information from the input data, and the task network that uses these information to fulfill the actual task. In order to train a sub-ensemble, first, the weights of each member's trunk network are fixed based on the resulting parameters of one single model's training process. Following, the parameters of each ensemble members' task network are trained independently from the other members. As a result, the members are built with a common trunk and an individual task sub-network. Since the training and the evaluation of the trunk network have to be done only once, the number of computations needed for training and testing decreases by the factor $\\frac{M \\cdot N_{\\text{task}}  + N_{\\text{trunk}}}{M\\cdot N}$, where $N_{\\text{task}}$, $N_{\\text{trunk}}$, and $N$ stand for the number of variables in the task networks, the trunk network, and the complete network. Valdenegro-Toro  further underlined the usage of a shared trunk network by arguing that the trunk network is in general computational more costly than the task network. \nIn contrast to this, batch-ensembles  connect the member networks with each other at every layer. The ensemble members' weights are described as a Hadamard product of one shared weight matrix $W \\in \\R^{n\\times m}$ and $M$ individual rank one matrices $F_i \\in \\R^{n \\times m}$, each linked with one of the $M$ ensemble members. The rank one matrices can be written as a multiplication $F_i=r_is_i^\\text{T}$ of two vectors $s\\in \\R^{n}$ and $r\\in \\R^{m}$ and hence the matrix $F_i$ can be described by $n+m$ parameters. With this approach, each additional ensemble member increases the number of parameters only by the factor $\\frac{n+m}{M\\cdot(n+m)+n\\cdot m} + 1$ instead of $\\frac{M+1}{M}=1 + \\frac{1}{M}$. On the one hand, with this approach, the members are not independent anymore such that all the members have to be trained in parallel. On the other hand, the authors also showed that the parallelization can be realized similar to the optimization on mini-batches and on a single unit.\\\\", "cites": [6989, 8806, 4590, 4680, 681, 4679, 8633, 4594, 4613], "cite_extract_rate": 0.6, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple papers to present a coherent narrative on efficiency improvements in ensemble methods, particularly distillation and sub-ensembles. It critically evaluates trade-offs, such as the loss of model uncertainty during distillation and the non-independent training of batch-ensembles. It also abstracts broader concepts like parameter sharing and higher-order distribution learning to highlight generalizable patterns across methods."}}
{"id": "2cc00bbc-fc33-44ef-a304-012b3a168687", "title": "Sum Up Ensemble Methods", "level": "subsubsection", "subsections": [], "parent_id": "4fdfdf37-e014-4b52-b80a-5b1aa7684c7d", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Estimation"], ["subsection", "Ensemble Methods"], ["subsubsection", "Sum Up Ensemble Methods"]], "content": "Ensemble methods are very easy to apply, since no complex implementation or major modification of the standard deterministic model have to be realized. Furthermore, ensemble members are trained independently from each other, which makes the training easily parallelizable. Also, trained ensembles can be extended easily, but the needed memory and the computational effort increases linearly with the number of members for training and evaluation. The main challenge when working with ensemble methods is the need of introducing diversity among the ensemble members. For accuracy, uncertainty quantification, and out-of-distribution detection, random initialization, data shuffling, and augmentations have been found to be sufficient for many applications and tasks . Since these methods may be applied anyway, they do not need much additional effort. The independence of the single ensemble members leads to a linear increase in the required memory and computation power with each additional member. This holds for the training as well as for testing. This limits the deployment of ensemble methods in many practical applications where the computation power or memory is limited, the application is time-critical, or very large networks with high inference time are included .\\\\\nMany aspects of ensemble approaches are only investigated with respect to the performance on the predictive accuracy but do not take predictive uncertainty into account. This also holds for the comparison of different training strategies for a broad range of problems and data sets. \nEspecially since the overconfidence from single members can be transferred to the whole ensemble, strategies that encourage the members to deliver different false predictions instead of all delivering the same false prediction should be further investigated. For a better understanding of ensemble behavior, further evaluations of the loss landscape, as done by Fort et al. , could offer interesting insights.", "cites": [4671, 8806, 3288], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes insights from multiple papers, highlighting the ease of use, parallelizability, and limitations of ensemble methods. It provides critical analysis by pointing out that many studies focus only on predictive accuracy and not on uncertainty, and suggests that overconfidence can propagate in ensembles. The section also abstracts key challenges such as diversity and computational cost, offering broader implications for the deployment and evaluation of ensemble techniques."}}
{"id": "781f73a5-8ad5-4b3b-aeac-5bca88d02c49", "title": "Test Time Augmentation", "level": "subsection", "subsections": [], "parent_id": "33e781ef-a127-43ed-8702-88e09976b718", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Estimation"], ["subsection", "Test Time Augmentation"]], "content": "}\\label{sec:test_time_augmentation}\nInspired by ensemble methods and adversarial examples , the test time data augmentation is one of the simpler predictive uncertainty estimation techniques. The basic method is to create multiple test samples from each test sample by applying data augmentation techniques on it and then test all those samples to compute a predictive distribution in order to measure uncertainty. The idea behind this method is that the augmented test samples allow the exploration of different views and is therefore capable of capturing the uncertainty. Mostly, this technique of test time data augmentations has been used in medical image processing . One of the reasons for this is that the field of medical image processing already makes heavy use of data augmentations while using deep learning , so it is quite easy to just apply those same augmentations during test time to calculate the uncertainties. Another reason is that collecting medical images is costly, thus forcing practitioners to rely on data augmentation techniques. \nMoshkov et al.  used the test time augmentation technique for cell segmentation tasks. For that, they created multiple variations of the test data before feeding it to a trained UNet or Mask R-CNN architecture. Following, they used a majority voting to create the final output segmentation mask and discuss the policies of applying different augmentation techniques and how they affect the final predictive results of the deep networks.\\\\\nOverall, test time augmentation is an easy method for estimating uncertainties because it keeps the underlying model unchanged, requires no additional data, and is simple to put into practice with off-the-shelf libraries. Nonetheless, it needs to be kept in mind that during applying this technique, one should only apply valid augmentations to the data, meaning that the augmentations should not generate data from outside the target distribution. According to , test time augmentation can change many correct predictions into incorrect predictions (and vice versa) due to many factors such as the nature of the problem at hand, the size of training data, the deep neural network architecture, and the type of augmentation. To limit the impact of these factors, Shanmugam et al.  proposed a learning-based method for test time augmentation that takes these factors into consideration. In particular, the proposed method learns a function that aggregates the predictions from each augmentation of a test sample. Similar to , Molchanov et al.  proposed a method, named âgreedy Policy Searchâ, for constructing a test-time augmentation policy by choosing augmentations to be include in a fixed-length policy. Similarly, Kim et al.  proposed a method for learning a loss predictor from the training data for instance-aware test-time augmentation selection. The predictor selects test-time augmentations with the lowest predicted loss for a given sample.\nAlthough learnable test time augmentation techniques  help to select valid augmentations, one of the major open question is to find out the effect on uncertainty due to different kinds of augmentations. It can for example happen that a simple augmentation like reflection is not able to capture much of the uncertainty while some domain specialized stretching and shearing captures more uncertainty. It is also important to find out how many augmentations are needed to correctly quantify uncertainties in a given task. This is particularly important in applications like earth observation, where inference might be needed on global scale with limited resources.", "cites": [4682, 4683, 825, 4681], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple papers to present a coherent narrative on test time augmentation as a method for uncertainty estimation. It critically discusses limitations, such as the potential for correct predictions to become incorrect, and compares different approaches like greedy policy search and loss predictor methods. The section abstracts beyond specific papers by identifying general principles and open questions about the nature and effectiveness of augmentations in capturing uncertainty."}}
{"id": "ad900f38-8b0a-4a6b-ab56-457ba95c6ae7", "title": "Neural Network Uncertainty Quantification Approaches for Real Life Applications", "level": "subsection", "subsections": [], "parent_id": "33e781ef-a127-43ed-8702-88e09976b718", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Estimation"], ["subsection", "Neural Network Uncertainty Quantification Approaches for Real Life Applications"]], "content": "In order to use the presented methods on real life tasks, several different considerations have to be taken into account. The memory and computational power is often restricted while many real world tasks my be time-critical . An overview over the main properties is given in Table \\ref{tab:types_compare}. \\\\\nThe presented applications all come along with advantages and disadvantages, depending on the properties a user is interested in. While ensemble methods and test-time augmentation methods are relatively easy to apply, Bayesian approaches deliver a clear description of the uncertainty on the models parameters and also deliver a deeper theoretical basis. The computational effort and memory consumption is a common restriction on real life applications, where single deterministic network approaches perform best, but distillation of ensembles or efficient Bayesian methods can also be taken into consideration. Within the different types of Bayesian approaches, the performance, the computational effort, and the implementation effort still vary strongly. Laplace approximations are relatively easy to apply and compared to sampling approaches much less computational effort is needed. Furthermore, there often already exist pretrained networks for an application. In this case, Laplace Approximation and external deterministic single network approaches can in general be applied to already trained networks. \\\\~\\\\\nAnother important aspect that has to be taken into account for uncertainty quantification in real life applications is the source and type of uncertainty. For real life applications, out-of-distribution detection forms the maybe most important challenge in order to avoid unexpected decisions of the network and to be aware of adversarial attacks. Especially since many motivations of uncertainty quantification are given by risk minimization, methods that deliver risk averse predictions are an important field to evaluate.\nMany works already demonstrated the capability of detecting out-of-distribution samples on several tasks and built a strong fundamental tool set for the deployment in real life applications . However, in real life, the tasks are much more difficult than finding out-of-distribution samples among data sets (e.g., MNIST or CIFAR data sets etc.) and the main challenge lies in comparing such approaches on several real-world data sets against each other. The work of Gustafsson et al.  forms a first important step towards an evaluation of methods that better suits the demands in real life applications. Interestingly, they found for their tests ensembles to outperform the considered Bayesian approaches. This indicates, that the multi-mode evaluation given by ensembles is a powerful property for real life applications. Nevertheless Bayesian approaches have delivered strong results as well and furthermore come along with a strong theoretical foundation . As a way to go, the combination of efficient ensemble strategies and Bayesian approaches could combine the variability in the model parameters while still considering several modes for a prediction. \nAlso, single deterministic approaches as the prior networks  deliver comparable results while consuming significantly less computation power. However, this efficiency often comes along with the problem that separated sets of in- and out-of-distribution samples have to be available for the training process . \nIn general, the development of new problem and loss formulations as for example given in  leads to a better understanding and description of the underlying problem and forms an important field of research.", "cites": [4664, 3293, 8807, 4665, 3294, 4600, 3232, 8633, 1044, 4613], "cite_extract_rate": 0.7692307692307693, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes and integrates ideas from multiple cited papers, connecting Bayesian methods, ensembles, and single deterministic networks within the context of real-world application constraints. It also engages in critical analysis by comparing the strengths and weaknesses of these approaches and identifying open challenges. The abstraction level is strong, as it generalizes observations about uncertainty sources and method properties, offering insights that go beyond individual papers."}}
{"id": "07c7f2da-ef2c-475e-823a-a86caf9c3b67", "title": "Uncertainty Measures and Quality", "level": "section", "subsections": ["09365f96-9cba-420c-afc8-a3c8d6b4d1b6", "d7ef6da1-d088-4266-8f76-706da1adb552", "52da755e-6d99-4ad1-8f6b-287af0fc5c38"], "parent_id": "039eb0dd-a257-4d85-a714-ad5fc65c6f0b", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Measures and Quality"]], "content": "\\label{sec:uncertainty_measures}\nIn Section \\ref{sec:uncertainty_quantification_methods}, we presented different methods for modeling and predicting different types of uncertainty in neural networks. In order to evaluate these approaches, measures have to be applied on the derived uncertainties. In the following, we present different measures for quantifying the different predicted types of uncertainty.\nIn general, the correctness and trustworthiness of these uncertainties is not automatically given. In fact, there are several reasons why evaluating the quality of the uncertainty estimates is a challenging task.\n\\begin{itemize}\n  \\setlength\\itemsep{0.5em}\n    \\item First, the quality of the uncertainty estimation depends on the underlying method for estimating uncertainty. This is exemplified in the work undertaken by Yao et al. , which shows that different approximates of Bayesian inference (e.g. Gaussian and Laplace approximates) result in different qualities of uncertainty estimates.\n    \\item Second, there is a lack of ground truth uncertainty estimates  and defining ground truth uncertainty estimates is challenging. For instance, if we define the ground truth uncertainty as the uncertainty across human subjects, we still have to answer questions as \"How many subjects do we need?\" or \"How to choose the subjects?\".\n    \\item Third, there is a lack of a unified quantitative evaluation metric . To be more specific, the uncertainty is defined differently in different machine learning tasks such as classification, segmentation, and regression. For instance, prediction intervals or standard deviations are used to represent uncertainty in regression tasks, while entropy (and other related measures) are used to capture uncertainty in classification and segmentation tasks.\n\\end{itemize}", "cites": [4684, 4685, 3288], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates key ideas from the cited papers to highlight challenges in evaluating uncertainty, particularly the method dependency and lack of ground truth. While it makes some connections across sources, it does not fully synthesize them into a novel framework. The critical analysis is present in pointing out limitations like the misleading nature of metrics and the absence of a unified evaluation system, though it stops short of deeper, nuanced critique."}}
{"id": "09365f96-9cba-420c-afc8-a3c8d6b4d1b6", "title": "Evaluating Uncertainty in Classification Tasks", "level": "subsection", "subsections": ["1b5886d5-9af7-4c82-b2f2-48edadb5f482", "d93d8bab-5114-41f4-a244-1861fac352b3", "0c83491b-d4ea-49e8-af4d-01614b6386be", "9c2e0b54-8727-4734-918a-7309443b8949"], "parent_id": "07c7f2da-ef2c-475e-823a-a86caf9c3b67", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Measures and Quality"], ["subsection", "Evaluating Uncertainty in Classification Tasks"]], "content": "For classification tasks, the network's softmax output already represents a measure of confidence. But since the raw softmax output is neither very reliable  nor can it represent all sources of uncertainty , further approaches and corresponding measures were developed.", "cites": [4593, 1624], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section briefly introduces the concept of using softmax outputs for confidence and acknowledges their limitations, referencing two papers that explore uncertainty measures for classification. It shows some analytical insight by pointing out that raw softmax outputs are insufficient and that alternative measures exist, but it lacks deeper synthesis or comparison across the cited works. The abstraction is limited, as it does not move beyond the specific examples to present broader principles or frameworks."}}
{"id": "d93d8bab-5114-41f4-a244-1861fac352b3", "title": "Measuring Model Uncertainty in Classification Tasks", "level": "subsubsection", "subsections": [], "parent_id": "09365f96-9cba-420c-afc8-a3c8d6b4d1b6", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Measures and Quality"], ["subsection", "Evaluating Uncertainty in Classification Tasks"], ["subsubsection", "Measuring Model Uncertainty in Classification Tasks"]], "content": "~\\\\\nAs already discussed in Section \\ref{sec:uncertainty_quantification_methods}, a single softmax prediction is not a very reliable way for uncertainty quantification since it is often badly calibrated  and does not have any information about the certainty the model itself has on this specific output . An (approximated) posterior distribution $p(\\theta \\vert D)$ on the learned model parameters can help to receive better uncertainty estimates. With such a posterior distribution, the softmax output itself becomes a random variable and one can evaluate its variation, i.e. uncertainty. For simplicity, we denote $p(y\\vert \\theta, x)$ also as $p$ and it will be clear from context whether $p$ depends on $\\theta$ or not. The most common measures for this are the mutual information (MI), the expected Kullback-Leibler Divergence (EKL), and the predictive variance. Basically, all these measures compute the expected divergence between the (stochastic) softmax output and the expected softmax output\n\\begin{align}\n    \\hat{p} = \\mathbb{E}_{\\theta\\sim p(\\theta\\vert D)}\\left[p(y\\vert x, \\theta\\right]~.\n\\end{align}\nThe MI uses the entropy to measure the mutual dependence between two variables. In the described case, the difference between the information given in the expected softmax output and the expected information in the softmax output is compared, i.e.\n\\begin{align}\\label{eq:MI}\n    \\text{MI}\\left(\\theta, y \\vert x, D\\right) = \\text{H}\\left[\\hat{p}\\right] - \\mathbb{E}_{\\theta\\sim p(\\theta\\vert D)}\\text{H}\\left[p(y \\vert x, \\theta )\\right]~.\n\\end{align}\nSmith and Gal  pointed out that the MI is minimal when the knowledge about model parameters does not increase the information in the final prediction. Therefore, the MI can be interpreted as a measure of model uncertainty. \\\\\nThe Kullback-Leibler divergence measures the divergence between two given probability distributions. The EKL can be used to measure the (expected) divergence among the possible softmax outputs,\n\\begin{align}\\label{eq:EKL}\n    \\mathbb{E}_{\\theta\\sim p(\\theta \\vert D)}\\left[KL(\\hat{p}~||~p)\\right] =\\mathbb{E}_{\\theta\\sim p(\\theta \\vert D)}\\left[\\sum_{i=1}^K \\hat{p}_i \\log\\left(\\frac{\\hat{p}_i}{p_i}\\right)\\right]~,\n\\end{align}\nwhich can also be interpreted as a measure of uncertainty on the model's output and therefore represents the model uncertainty. \\\\\nThe predictive variance evaluates the variance on the (random) softmax outpus, i.e.\n\\begin{align}\\label{eq:pred_sigma}\n    \\sigma(p) &= \\mathbb{E}_{\\theta\\sim p(\\theta\\vert D)} \\left[\\left(p - \\hat{p}  \\right)^2\\right]~.\n\\end{align}\nAs described in Section \\ref{sec:uncertainty_quantification_methods}, an analytically described posterior distribution $p(\\theta\\vert D)$ is only given for a subset of the Bayesian methods. And even for an analytically described distribution, the propagation of the parameter uncertainty into the prediction is in almost all cases intractable and has to be approximated for example with Monte Carlo approximation. Similarly, ensemble methods collect predictions from $M$ neural networks, and test-time data augmentation approaches receive $M$ predictions from $M$ different augmentations applied to the original input sample. For all these cases, we receive a set of $M$ samples, $\\left\\{p^i\\right\\}_{i=1}^M$, which can be used to approximate the intractable or even undefined underlying distribution. With these approximations, the measures defined in \\eqref{eq:MI}, \\eqref{eq:EKL}, and \\eqref{eq:pred_sigma} can be applied straight forward and only the expectation has to be replaced by average sums. For example, the expected softmax output becomes \n\\begin{align*}\n    \\hat{p} \\approx \\frac{1}{M}\\sum_{i=1}^M p^i~.\n\\end{align*}\nFor the expectations given in \\eqref{eq:MI}, \\eqref{eq:EKL}, and \\eqref{eq:pred_sigma}, the expectation is approximated similarly.", "cites": [4593], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a clear analytical explanation of model uncertainty measures such as MI, EKL, and predictive variance in classification tasks, linking them to the posterior distribution of model parameters. It integrates the cited paper by mentioning the effectiveness of MI for uncertainty estimation, showing some synthesis and abstraction. However, it lacks deeper critical evaluation of the strengths and limitations of these methods, relying mainly on definitions and basic interpretations."}}
{"id": "0c83491b-d4ea-49e8-af4d-01614b6386be", "title": "Measuring Distributional Uncertainty in Classification Tasks", "level": "subsubsection", "subsections": [], "parent_id": "09365f96-9cba-420c-afc8-a3c8d6b4d1b6", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Measures and Quality"], ["subsection", "Evaluating Uncertainty in Classification Tasks"], ["subsubsection", "Measuring Distributional Uncertainty in Classification Tasks"]], "content": "~\\\\\nAlthough these uncertainty measures are widely used to capture the variability among several predictions derived from Bayesian neural networks , ensemble methods , or test-time data augmentation methods , they cannot capture distributional shifts in the input data or out-of-distribution examples, which could lead to a biased inference process and a falsely stated confidence. If all predictors attribute a high probability mass to the same (false) class label, this induces a low variability among the estimates. Hence, the network seams to be certain about its prediction, while the uncertainty in the prediction itself (given by the softmax probabilities) is also evaluated to be low. To tackle this issue, several approaches described in Section \\ref{sec:uncertainty_quantification_methods} take the magnitude of the logits into account, since a larger logit indicates larger evidence for the corresponding class . Thus, the methods either interpret the total sum of the (exponentials of) the logits as precision value of a Dirichlet distribution (see description of Dirichlet Priors in Section \\ref{sec:deterministic_methods}) , or as a collection of evidence that is compared to a defined constant . One can also derive a total class probability for each class individually by applying the sigmoid function to each logit . Based on the class-wise total probabilities, OOD samples might easier be detected, since all classes can have low probability at the same time. Other methods deliver an explicit measure how well new data samples suit into the training data distribution. Based on this, they also give a measure that a sample will be predicted correctly .", "cites": [4592, 7130, 8639, 4619, 8633, 4613, 3288], "cite_extract_rate": 0.7, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key ideas from multiple papers, particularly those related to Dirichlet distributions, evidence-based uncertainty, and out-of-distribution detection. It moves beyond simple descriptions to explain how different methods address distributional uncertainty and highlights the limitations of traditional softmax outputs. The abstraction is strong, as it generalizes the role of logit magnitudes and alternative output interpretations in capturing uncertainty."}}
{"id": "9c2e0b54-8727-4734-918a-7309443b8949", "title": "Performance Measure on Complete Data Set", "level": "subsubsection", "subsections": [], "parent_id": "09365f96-9cba-420c-afc8-a3c8d6b4d1b6", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Measures and Quality"], ["subsection", "Evaluating Uncertainty in Classification Tasks"], ["subsubsection", "Performance Measure on Complete Data Set"]], "content": "~\\\\\nWhile the measures described above measure the performance of individual predictions, others evaluate the usage of these measures on a set of samples. Measures of uncertainty can be used to separate between correctly and falsely classified samples or between in-domain and out-of-distribution samples . For that, the samples are split into two sets, for example in-domain and out-of-distribution or correctly classified and falsely classified. The two most common approaches are the \\textit{Receiver Operating Characteristic} (ROC) curve and the \\textit{Precision-Recall} (PR) curve. Both methods generate curves based on different thresholds of the underlying measure. For each considered threshold, the ROC curve plots the true positive rate against the false positive rate\\footnote{The true positive rate is the number of samples, which are correctly predicted as positive divided by the total number of true samples. The false positive rate is the number of samples falsely predicted as positive divided by the total number of negative samples (see also )}, and the PR curve plots the precision against the recall\\footnote{The precision is equal to the number of samples that are correctly classified as positive, divided by the total number of positive samples. The recall is equal to the number of samples correctly predicted as positive divided by the total number of positive samples (see also )}. While the ROC and PR curves give a visual idea of how well the underlying measures are suited to separate the two considered test cases, they do not give a qualitative measure. To reach this, the area under the curve (AUC) can be evaluated. Roughly speaking, the AUC gives a probability value that a randomly chosen positive sample leads to a higher measure than a randomly chosen negative example. For example, the maximum softmax values measure ranks of correctly classified examples higher than falsely classified examples. Hendrycks and Gimpel  showed for several application fields that correct predictions have in general a higher predicted certainty in the softmax value than false predictions. Especially for the evaluation of in-domain and out-of-distribution examples, the \\textit{Area Under Receiver Operating Curve} (AUROC) and the \\textit{Area Under Precision Recall Curce} (AUPRC) are commonly used . The clear weakness of these evaluations is the fact that the performance is evaluated and the optimal threshold is computed based on a given test data set. A distribution shift from the test set distribution can ruin the whole performance and make the derived thresholds impractical.", "cites": [8633, 8639, 1624], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers by discussing how uncertainty measures can be evaluated on full data sets, particularly for distinguishing in-domain and out-of-distribution samples. It abstracts the use of ROC and PR curves into general evaluation strategies and highlights a shared limitation regarding distribution shifts. However, the critical analysis is somewhat limited to a general observation about threshold dependence without deeper comparative or methodological critique."}}
{"id": "34d11d6d-7a6b-48a6-83c2-934650e857f9", "title": "Measuring Data Uncertainty in Regression Predictions", "level": "subsubsection", "subsections": [], "parent_id": "d7ef6da1-d088-4266-8f76-706da1adb552", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Measures and Quality"], ["subsection", "Evaluating Uncertainty in Regression Tasks"], ["subsubsection", "Measuring Data Uncertainty in Regression Predictions"]], "content": "In contrast to classification tasks, where the network typically outputs a probability distritution over the possible classes, regression tasks only predict a pointwise estimation without any hint of data uncertainty. As already described in Section \\ref{sec:uncertainty_quantification_methods}, a common approach to overcome this is to let the network predict the parameters of a probability distribution, for example a mean vector and a standard deviation for a normally distributed uncertainty . Doing so, a measure of data uncertainty is directly given. The prediction of the standard deviation allows an analytical description that the (unknown) true value is within a specific region. The interval that covers the true value with a probability of $\\alpha$ (under the assumption that the predicted distribution is correct) is given by  \n\\begin{equation}\n    \\left[\\hat{y}-\\frac{1}{2}\\Phi^{-1}(\\alpha)\\cdot\\sigma;\\quad \\hat{y}+\\frac{1}{2}\\Phi^{-1}(\\alpha)\\cdot\\sigma\\right]\n\\end{equation}\nwhere $\\Phi^{-1}$ is the quantile function, the inverse of the cumulative probability function. For a given probability value $\\alpha$ the quantile function gives a boundary, such that $100\\cdot\\alpha\\%$ of a standard normal distribution's probability mass is on values smaller than $\\Phi^{-1}(\\alpha)$. Quantiles assume some probability distribution and interpret the given prediction as the expected value of the distribution. \\\\\nIn contrast to this, other approaches  directly predict a so called prediction interval (PI) \n\\begin{align}\n    PI(x) = \\left[B_l, B_u\\right]\n\\end{align}\nin which the prediction is assumed to lay. Such intervals induce an uncertainty as a uniform distribution without giving a concrete prediction. The certainty of such approaches can, as the name indicates, be directly measured by the size of the predicted interval. The \\textit{Mean Prediction Interval Width} (MPIW) can be used to evaluate the average certainty of the model . In order to evaluate the correctness of the predicted intervals the \\textit{Prediction Interval Coverage Probability} (PICP) can be applied . The PCIP represents the percentage of test predictions that fall into a prediction interval and is defined as \n\\begin{equation}\\label{eq:picp}\n    \\text{PICP}=\\frac{c}{n}~,\n\\end{equation}\nwhere $n$ is the total number of predictions and $c$ the number of ground truth values that are actually captured by the predicted intervals.", "cites": [8830, 4686, 3288], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear analytical overview of methods for measuring data uncertainty in regression, integrating concepts from multiple papers. It synthesizes the idea of prediction intervals and compares them to probabilistic outputs, though the critique of the cited works is limited. The abstraction is reasonable, as it identifies general principles such as correctness and tightness of intervals."}}
{"id": "52da755e-6d99-4ad1-8f6b-287af0fc5c38", "title": "Evaluating Uncertainty in Segmentation Tasks", "level": "subsection", "subsections": [], "parent_id": "07c7f2da-ef2c-475e-823a-a86caf9c3b67", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Uncertainty Measures and Quality"], ["subsection", "Evaluating Uncertainty in Segmentation Tasks"]], "content": "The evaluation of uncertainties in segmentation tasks is very similar to the evaluation for classification problems. The uncertainty is estimated in segmentation tasks using approximates of Bayesian inference  or test-time data augmentation techniques . In the context of segmentation, the uncertainty in pixel wise segmentation is measured using confidence intervals , the predictive variance , the predictive entropy  or the mutual information . The uncertainty in structure (volume) estimation is obtained by averaging over all pixel-wise uncertainty estimates . The quality of volume uncertainties is assessed by evaluating the coefficient of variation, the average Dice score or the intersection over union . These metrics measure the agreement in area overlap between multiple estimates in a pair-wise fashion.\nIdeally, a false segmentation should result in an increase in pixel-wise and structure uncertainty. To evaluate whether this is the case, Nair et al.  evaluated the pixel-level true positive rate and false detection rate as well as the ROC curves for the retained pixels at different uncertainty thresholds. Similar to , McClure et al.  also analyzed the area under the ROC curve.", "cites": [4606, 4688, 4687, 8831, 4603], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic descriptive overview of uncertainty evaluation methods in segmentation tasks, mentioning common metrics and referencing relevant papers. While it connects some ideas (e.g., pixel-wise and structure uncertainty), it lacks deeper synthesis or a unifying framework. There is minimal critical analysis or abstraction beyond the cited works, with no clear evaluation of strengths, weaknesses, or broader implications of the approaches."}}
{"id": "71582559-f2d3-45a1-a741-a40d9e53b709", "title": "Calibration", "level": "section", "subsections": ["c8944f89-810f-4c7d-ab40-2e0f83834278", "e0dba29a-a672-47b8-bc13-8cb4402e7555"], "parent_id": "039eb0dd-a257-4d85-a714-ad5fc65c6f0b", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Calibration"]], "content": "\\label{sec:calibration}\nA predictor is called well-calibrated if the derived predictive confidence represents a good approximation of the actual probability of correctness . Therefore, in order to make use of uncertainty quantification methods, one has to be sure that the network is well calibrated. Formally, for classification tasks a neural network $f_\\theta$ is calibrated  if it holds that \n\\begin{align}\\label{eq:calibration_classification}\n    \\forall p \\in [0,1]:\\quad \\sum_{i=1}^N \\sum_{k=1}^K\\frac{y_{i,k}\\cdot\\mathbb{I}\\{f_\\theta(x_i)_k=p\\}}{\\mathbb{I}\\{f_\\theta(x_i)_k=p\\}} \\xrightarrow[]{N \\to \\infty} p~.\n\\end{align}\nHere, $\\mathbb{I}\\{\\cdot\\}$ is the indicator function that is either 1 if the condition is true or 0 if it is false and $y_{i,k}$ is the $k$-th entry in the one-hot encoded groundtruth vector of a training sample $(x_i,y_i)$. This formulation means that for example $30\\%$ of all predictions with a predictive confidence of $70\\%$ should actually be false.\nFor regression tasks the calibration can be defined such that predicted confidence intervals should match the confidence intervals empirically computed from the data set , i.e.\n\\begin{equation}\\label{eq:calibration_regression}\n\\forall p \\in [0,1]:\\quad \\sum_{i=1}^N\\frac{\\mathbb{I}\\left\\{y_i\\in \\text{conf}_{p}(f_\\theta(x_i))\\right\\}}{N} \\xrightarrow[]{N \\to \\infty} p,\n\\end{equation}\nwhere $\\text{conf}_p$ is the confidence interval that covers $p$ percent of a distribution. \nA DNN is called under-confident if the left hand side of \\eqref{eq:calibration_classification} and \\eqref{eq:calibration_regression} are larger than $p$. Equivalently, it is under-confident if the terms are smaller than $p$. The calibration property of a DNN can be visualized using a \\textit{reliability diagram}, as shown in Figure \\ref{fig:reliability_diagram}.\nIn general, calibration errors are caused by factors related to model uncertainty . This is intuitively clear, since as discussed in Section \\ref{sec:uncertainty_types_and_sources}, data uncertainty represents the underlying uncertainty that an input $x$ and a target $y$ represent the same real world information. Following, correctly predicted data uncertainty would lead to a perfectly calibrated neural network.\nIn practice, several works pointed out that deeper networks tend to be more overconfident than shallower ones . \\\\\nSeveral methods for uncertainty estimation presented in Section \\ref{sec:uncertainty_quantification_methods} also improve the networks calibration . This is clear, since these methods quantify model and data uncertainty separately and aim at reducing the model uncertainty on the predictions.\nBesides the methods that improve the calibration by reducing the model uncertainty, a large and growing body of literature has investigated methods for explicitly reducing calibration errors. These methods are presented in the following, followed by measures to quantify the calibration error. It is important to note that these methods do not reduce the model uncertainty, but propagate the model uncertainty onto the representation of the data uncertainty. For example, if a binary classifier is overfitted and predicts all samples of a test set as class A with probability 1, while half of the test samples are actually class B, the recalibration methods might map the network output to 0.5 in order to have a reliable confidence. This probability of 0.5 is not equivalent to the data uncertainty but represents the model uncertainty propagated onto the predicted data uncertainty.\n\\begin{figure*}[t]\n    \\begin{center}\n        \\begin{subfigure}{0.25\\textwidth}\\includegraphics[width=\\textwidth]{figures/overconfident.png}\\subcaption[]{Underconfidence}\\end{subfigure}\n        \\begin{subfigure}{0.25\\textwidth}\\includegraphics[width=\\textwidth]{figures/underconfident.png}\\subcaption[]{Overconfidence}\\end{subfigure}\n        \\begin{subfigure}{0.25\\textwidth}\\includegraphics[width=\\textwidth]{figures/calibrated.png}\\subcaption[]{Calibrated classifier}\\end{subfigure}\n    \\end{center}\n    \\caption{(a) Reliability diagram showing an overconfident classifier: The bin-wise accuracy is smaller than the corresponding confidence. (b) Reliability diagram of an underconfident classifier: The bin-wise accuracy is larger than the corresponding confidence. (c) Reliability diagram of a well calibrated classifier: The confidence fits the actual accuracy for the single bins.}\n    \\label{fig:reliability_diagram}\n\\end{figure*}", "cites": [759, 4689, 4690, 3288], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from the cited papers, particularly integrating ideas on calibration errors, the influence of network depth, and methods for recalibrating predictions. It provides an analytical perspective by linking calibration to model and data uncertainty, and by discussing how recalibration techniques propagate model uncertainty. While it offers some level of abstraction by generalizing calibration behaviors and their implications, it lacks deeper comparative or evaluative analysis of the methods, limiting its critical depth."}}
{"id": "c8944f89-810f-4c7d-ab40-2e0f83834278", "title": "Calibration Methods", "level": "subsection", "subsections": ["886e481a-f19e-4a18-8e0f-c24754d408ef", "14644a47-0f49-48f7-a0d2-b62942954202", "d193f79c-349d-447b-816c-8fa8897f56dd"], "parent_id": "71582559-f2d3-45a1-a741-a40d9e53b709", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Calibration"], ["subsection", "Calibration Methods"]], "content": "Calibration methods can be classified into three main groups according to the step when they are applied: \n\\begin{itemize}\n  \\setlength\\itemsep{0.5em}\n    \\item \\textit{Regularization methods applied during the training phase}  \\\\\n    These methods modify the objective, optimization and/or regularization procedure in order to build DNNs that are inherently calibrated.\n    \\item \\textit{Post-processing methods applied after the training process of the DNN} \\\\  \n     These methods require a held-out calibration data set to adjust the prediction scores for recalibration. They only work under the assumption that the distribution of the left-out validation set is equivalent to the distribution, on which inference is done.\n     Hence, also the size of the validation data set can influence the calibration result. \n    \\item \\textit{Neural network uncertainty estimation methods}\\\\\n    Approaches, as presented in Section \\ref{sec:uncertainty_quantification_methods}, that reduce the amount of model uncertainty on a neural network's confidence prediction, also lead to a better calibrated predictor. This is because the remaining predicted data uncertainty better represents the actual uncertainty on the prediction. Such methods are based for example on Bayesian methods  or deep ensembles .\n\\end{itemize}\nIn the following, we present the three types of calibration methods in more detail. \n\\begin{figure*}[h]\n    \\centering\n    \\resizebox{\\textwidth}{!}{\n        \\begin{tikzpicture}[framed, every annotation/.style = {draw,\n            fill = white, font = \\large}]\n        \\path[mindmap,concept color=black!40,text=black,\n        every node/.style={concept,circular drop shadow},\n        root/.style    = {concept color=black!40,\n            font=\\Large\\bfseries,text width=10em},\n        level 1 concept/.append style={font=\\normalsize\\bfseries, clockwise from=-15,\n            sibling angle=75,text width=7em,\n            level distance=15em,inner sep=0pt},\n        level 2 concept/.append style={font=\\small\\bfseries,level distance=9em},\n        ]\n        node[root] {Neural Network Calibration Methods} [clockwise from=-15]\n        child[concept color=orange] {\n            node[concept] {Regularization Methods}\n            [clockwise from=90]\n            child { node[concept] (Objective Function Modification)\n                {Objective Function Modification\\textsuperscript{11}}}\n            child { node[concept] (Data Augmentation)\n                {Data Augmentation\\textsuperscript{10}} }\n            child { node[concept] (Label Smoothing)\n                {Label Smoothing\\textsuperscript{9}}}\n            child { node[concept] (Exposure to OOD Examples)\n                {Exposure to OOD examples\\textsuperscript{8}}}\n        }\n        child[concept color=green!40!black] {\n            node[concept] {Uncertainty Estimation Approaches}\n            [clockwise from=-60]\n            child { node[concept] (Bayesian Neural Networks)\n                {Bayesian Neural Networks\\textsuperscript{6}}}\n            child { node[concept] (Ensemble of Neural Networks)\n                {Ensemble of Neural Networks\\textsuperscript{5}}}\n        }\n        child[concept color=blue!60] {\n            node {Post-Processing Methods} [clockwise from=-90]\n            child { node (temeprature_scaling){Temperature Scaling\\textsuperscript{4}} }\n            child { node (histogram_binning){Histogram Binning\\textsuperscript{3}} }\n            child { node (gaussian_process) {Gaussian Processes\\textsuperscript{2}} }\n            child { node (ensemble_of_post_processing_models) {Ensemble of Post-Processing Models\\textsuperscript{1}} }\n        };    \n        \\node[draw,text width=18cm, align=left] at (0,-10.5){\n            \\textsuperscript{1}~\n            ~~~\\textsuperscript{2}~\n            ~~~\\textsuperscript{3}~\n            ~~~\\textsuperscript{4}\n            ~~~\\textsuperscript{5}~\n            ~~~\\textsuperscript{6}~\n            \\textsuperscript{8}~\n            ~~~\\textsuperscript{9}~\n            ~~~\\textsuperscript{10}~\n            ~~~\\textsuperscript{11}~\n        };\n        \\end{tikzpicture}\n    }\n    \\caption{Visualization of the different types of uncertainty calibration methods presented in this paper.}\n    \\label{fig:calibration_diagram}\n\\end{figure*}", "cites": [4612, 3300, 4141, 8832, 759, 301, 4607, 3254, 4691, 3288, 4597, 4693, 4676, 4694, 4695, 3251, 4692, 3234, 4696], "cite_extract_rate": 0.8636363636363636, "origin_cites_number": 22, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section organizes calibration methods into three categories and cites relevant papers, providing a basic level of synthesis by associating each method with its respective group. However, it lacks deeper critical analysis or comparison of the cited works and does not abstract beyond the surface-level categorization. The figure and listing of methods are informative but primarily descriptive."}}
{"id": "886e481a-f19e-4a18-8e0f-c24754d408ef", "title": "Regularization Methods", "level": "subsubsection", "subsections": [], "parent_id": "c8944f89-810f-4c7d-ab40-2e0f83834278", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Calibration"], ["subsection", "Calibration Methods"], ["subsubsection", "Regularization Methods"]], "content": "Regularization methods for calibrating confidences manipulate the training of DNNs by modifying the objective function or by augmenting the training data set. The goal and idea of regularization methods is very similar to the methods presented in Section \\ref{sec:deterministic_methods} where the methods mainly quantify model and data uncertainty separately within a single forward pass. However, the methods in Section \\ref{sec:deterministic_methods} quantify the model and data uncertainty, while these calibration methods are regularized in order to minimize the model uncertainty. Following, at inference, the model uncertainty cannot be obtained anymore. This is the main motivation for us to separate the approaches presented below from the approaches presented in Section \\ref{sec:deterministic_methods}. \\\\\nOne popular regularization based calibration method is label smoothing . For label smoothing, the labels of the training examples are modified by taking a small portion $\\alpha$ of the true class' probability mass and assign it uniformly to the false classes. For hard, non-smoothed labels, the optimum cannot be reached in practice, as the gradient of the output with respect to the logit vector $z$,\n\\begin{align}\n    \\begin{split}\n    \\nabla_z \\text{CE}(y, \\hat y(z)) &= \\text{softmax}(z) - y \\\\\n    &= \\frac{\\exp(z)}{\\sum_{i=1}^K \\exp(z_i)}-y~,\n    \\end{split}\n\\end{align}\ncan only converge to zero with increasing distance between the true and false classes' logits. As a result, the logits of the correct class are much larger than the logits for the incorrect classes and the logits of the incorrect classes can be very different to each other. Label-smoothing avoids this and while it generally leads to a higher training loss, the calibration error decreases and the accuracy often increases as well . \\\\\nSeo et al.  extended the idea of label smoothing and directly aimed at reducing the model uncertainty. For this, they sampled $T$ forward passes of a stochastic neural network already at training time. Based on the $T$ forward passes of a training sample $(x_i,y_i)$, a normalized model variance $\\alpha_i$ is derived as the mean of the Bhattacharyya coefficients  between the $T$ individual predictions $\\hat y_1,...,\\hat y_T$ and the average prediction $\\bar y = \\frac{1}{T}\\sum_{t=1}^T\\hat y_t$,\n\\begin{align}\n    \\begin{split}\n        \\alpha_i &= \\frac{1}{T}\\sum_{t=1}^T BC(\\bar y_i, \\hat y_{i,t})  \\\\\n        &=\\frac{1}{T}\\sum_{t=1}^T \\sum_{k=1}^K \\sqrt{\\bar y_{i,k} \\cdot \\hat y_{i,t,k}}~.\n    \\end{split}\n\\end{align}\nBased on this $\\alpha_i$, Seo et al.  introduced the variance-weighted confidence-integrated loss function that is a convex combination of two contradictive loss functions,\n\\begin{align}\n    \\begin{split}\n        L^{\\text{VWCI}}(\\theta)=-\\sum_{i=1}^N(1-\\alpha_i)L_{\\text{GT}}^{(i)}(\\theta) + \\alpha_i L_{\\text{U}}^{(i)}(\\theta)~,\n    \\end{split}\n\\end{align}\nwhere $L_\\text{GT}^{(i)}$ is the mean cross-entropy computed for the training sample $x_i$ with given ground-truth $y_i$. $L_\\text{U}$ represents the mean KL-divergence between a uniform target probability vector and the computed prediction. The adaptive smoothing parameter ${\\alpha}_i$ pushes predictions of training samples with high model uncertainty (given by high variances) towards a uniform distribution while increasing the prediction scores of samples with low model uncertainty. As a result, variances in the predictions of a single sample are reduced and the network can then be applied with a single forward pass at inference.\nPereyra et al.  combated the overconfidence issue by adding the negative entropy to the standard loss function and therefore a penalty that increases with the network's predicted confidence. This results in the entropy-based objective function $L^H$, which is defined as\n\\begin{equation}\nL^H(\\theta) = -\\frac{1}{N} \\sum_{i=1}^{N} y_i \\log \\hat{y}_i - \\alpha_i H(\\hat{y}_i)~,\n\\end{equation}\nwhere $H(\\hat{y}_i)$ is the entropy of the output and $\\alpha_i$ a parameter that controls the strength of the entropy-based confidence penalty. The parameter $\\alpha_i$ is computed equivalently as for the VWCI loss. \nInstead of regularizing the training process by modifying the objective function, Thulasidasan et al.  regularized it by using a data-agnostic data augmentation technique named mixup . In mixup training, the network is not only trained on the training data, but also on virtual training samples $(\\tilde x, \\tilde y)$ generated by a convex combination of two random training pairs $(x_i,y_i)$ and $(x_j,y_j)$, i.e. \n\\begin{equation}\n\\tilde{x} = \\lambda x_i + (1 - \\lambda) x_j\n\\end{equation}\n\\begin{equation}\n\\tilde{y} = \\lambda y_i + (1 - \\lambda) y_j~.\n\\end{equation}\nAccording to , the label smoothing resulting from mixup training can be viewed as a form of entropy-based regularization resulting in inherent calibration of networks trained with mixup. Maro$\\tilde{\\text{n}}$as et al.  see mixup training among the most popular data augmentation regularization techniques due to its ability to improve the calibration as well as the accuracy. However, they argued that in mixup training the data uncertainty in mixed inputs affects the calibration and therefore mixup does not necessarily improve the calibration. They also underlined this claim empirically. Similarly, Rahaman and Thiery  experimentally showed that the distributional-shift induced by data augmentation techniques such as mixup training can negatively affect the confidence calibration. Based on this observation, Maro$\\tilde{\\text{n}}$as et al.  proposed a new objective function that explicitly takes the calibration performance on the unmixed input samples into account. Inspired by the expected calibration error (ECE, see Section \\ref{sec:calibration_quality}) Naeini et al.  measured the calibration performance on the unmixed samples for each batch $b$ by the differentiable squared differences between the batch accuracy and the mean confidence on the batch samples. The total loss is given as a weighted combination of the original loss on mixed and unmixed samples and the calibration measure evaluated only on the unmixed samples: \n\\begin{equation}\nL^{ECE}(\\theta) = \\frac{1}{B} \\sum_{b \\in B} L^b(\\theta) + \\beta ECE_b~,\n\\end{equation}\nwhere $L^b(\\theta)$ is the original unregularized loss using training and mixed samples included in batch $b$ and $\\beta$ is a hyperparameter controlling the relative importance given to the batchwise expected calibration error $ECE_b$. By adding the batchwise calibration error for each batch $b \\in B$ to the standard loss function, the miscalibration induced by mixup training is regularized. \nIn the context of data augmentation, Patel et al.  improved the calibration of uncertainty estimates by using on-manifold data augmentation. While mixup training combines training samples, on-manifold adversarial training generate out-of-domain samples using adversarial attack. They experimentally showed that on-manifold adversarial training outperforms mixup training in improving the calibration. Similar to , Hendrycks et al.  showed that exposing classifiers to out-of-distribution examples at training can help to improve the calibration.", "cites": [4141, 301, 4693, 4676, 3254, 7191, 4691], "cite_extract_rate": 0.6363636363636364, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes multiple calibration methods, particularly focusing on regularization, and connects label smoothing, entropy-based penalties, and mixup training to broader principles of model calibration and uncertainty reduction. It critically evaluates the effectiveness and limitations of these techniques, such as how data augmentation can both help and hinder calibration. While it identifies patterns across approaches, it remains grounded in specific methods rather than offering fully meta-level abstractions."}}
{"id": "14644a47-0f49-48f7-a0d2-b62942954202", "title": "Post-Processing Methods", "level": "subsubsection", "subsections": [], "parent_id": "c8944f89-810f-4c7d-ab40-2e0f83834278", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Calibration"], ["subsection", "Calibration Methods"], ["subsubsection", "Post-Processing Methods"]], "content": "Post-processing (or post-hoc) methods are applied after the training process and aim at learning a re-calibration function. For this, a subset of the training data is held-out during the training process and used as a calibration set. The re-calibration function is applied to the network's outputs (e.g. the logit vector) and yields an improved calibration learned on the left-out calibration set. Zhang et al.  discussed three requirements that should be satisfied by post-hoc calibration methods. They should \n\\begin{enumerate}\n    \\item preserve the accuracy, i.e. should not affect the predictors performance.\n    \\item be data efficient, i.e. only a small fraction of the training data set should be left out for the calibration.\n    \\item be able to approximate the correct re-calibration map as long as there is enough data available for calibration. \n\\end{enumerate}\nFurthermore, they pointed out that none of the existing approaches fulfills all three requirements. \\\\\nFor classification tasks, the most basic but still very efficient way of post-hoc calibration is temperature scaling . For temperature scaling, the temperature $T>0$ of the softmax function\n\\begin{equation}\\label{eq:temp_scaling}\n    \\text{softmax}(z_i) = \\frac{\\exp^{z_i/T}}{\\sum_{j=1}^K\\exp^{z_j/T}}~,\n\\end{equation}\nis optimized. For $T=1$ the function remains the regular softmax function. For $T>1$ the output changes such that its entropy increases, i.e. the predicted confidence decreases. For $T\\in(0,1)$ the entropy decreases and following, the predicted confidence increases. As already mentioned above, a perfect calibrated neural network outputs MAP estimates. Since the learned transformation can only affect the uncertainty, the log-likelihood based losses as cross-entropy do not have to be replaced by a special calibration loss. While the data efficiency and the preservation of the accuracy is given, the expressiveness of basic temperature scaling is limited . To overcome this, Zhang et al.  investigated an ensemble of several temperature scaling models. Doing so, they achieved better calibrated predictions, while preserving the classification accuracy and improving the data efficiency and the expressive power. \nKull et al.  were motivated by non-neural network calibration methods, where the calibration is performed class-wise as a one-vs-all binary calibration. They showed that this approach can be interpreted as learning a linear transformation of the predicted log-likelihoods followed by a softmax function. This again is equivalent to train a dense layer on the log-probabilities and hence the method is also very easy to implement and apply. Obviously, the original predictions are not guaranteed to be preserved. \nAnalogous to temperature scaling for classification networks, Levi et al.  introduced standard deviation scaling (std-scaling) for regression networks. As the name indicates, the method is trained to rescale the predicted standard deviations of a given network. Equivalently to the motivation of optimizing temperature scaling with the cross-entropy loss, std-scaling can be trained using the Gaussian log-likelihood function as loss, which is in general also used for the training of regression networks, which also give a prediction for the data uncertainty.\nWenger et al.  proposed a Gaussian process (GP) based method, which can be used to calibrate any multi-class classifier that outputs confidence values and presented their methodology by calibrating neural networks. The main idea behind their work is to learn the calibration map by a Gaussian process that is trained on the networks confidence predictions and the corresponding ground-truths in the left out calibration set. For this approach, the preservation of the predictions is also not assured.", "cites": [759, 4612, 4607, 4697], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes information from multiple papers, connecting post-processing methods across classification and regression tasks. It integrates key concepts like temperature scaling, standard deviation scaling, and GP-based calibration, and evaluates their strengths and limitations. While it identifies some critical aspects, such as the trade-off between expressiveness and data efficiency, the analysis remains focused on method-level descriptions rather than deeper theoretical or meta-level insights."}}
{"id": "d193f79c-349d-447b-816c-8fa8897f56dd", "title": "Calibration with Uncertainty Estimation Approaches", "level": "subsubsection", "subsections": [], "parent_id": "c8944f89-810f-4c7d-ab40-2e0f83834278", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Calibration"], ["subsection", "Calibration Methods"], ["subsubsection", "Calibration with Uncertainty Estimation Approaches"]], "content": "As already discussed above, removing the model uncertainty and receiving an accurate estimation of the data uncertainty leads to a well calibrated predictor. Following several works based on deep ensembles  and BNNs,   also compared their performance to other methods based on the resulting calibration. Lakshminarayanan et al.  and Mehrtash et al.  reported an improved calibration by applying deep ensembles compared to single networks. However, Rahaman and Thiery showed that for specific configurations as the usage of mixup-regularization, deep ensembles can even increase the calibration error. On the other side they showed that applying temperature scaling on the averaged predictions can give a significant improvement on the calibration. \\\\\nFor the Bayesian approaches,  showed that restricting the Bayesian approximation to the weights of the last fully connected layer of a DNN is already enough to improve the calibration significantly. Zhang et al.  and Laves et al.  showed that confidence estimates computed with MC dropout can be poorly calibrated. To overcome this, Zhang et al.  proposed structured dropout, which consists of dropping channel, blocks or layers, to promote model diversity and reduce calibration errors. \\\\", "cites": [3300, 8832, 4658, 4692, 4676, 4695, 4694, 3288], "cite_extract_rate": 1.0, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a critical comparison of calibration methods, particularly deep ensembles and Bayesian approaches, integrating findings from multiple papers to highlight both successes and failures. While it synthesizes some key insights, such as the impact of mixup regularization and structured dropout, it does not fully abstract these into a broader theoretical framework. The analysis is nuanced but remains focused on method-specific evaluations rather than proposing a novel conceptual synthesis."}}
{"id": "e0dba29a-a672-47b8-bc13-8cb4402e7555", "title": "Evaluating Calibration Quality", "level": "subsection", "subsections": [], "parent_id": "71582559-f2d3-45a1-a741-a40d9e53b709", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Calibration"], ["subsection", "Evaluating Calibration Quality"]], "content": "\\label{sec:calibration_quality}\nEvaluating calibration consists of measuring the statistical consistency between the predictive distributions and the observations . \nFor classification tasks, several calibration measures are based on binning. For that, the predictions are ordered by the predicted confidence $\\hat p_i$ and grouped into $M$ bins $b_1,...,b_M$. Following, the calibration of the single bins is evaluated by setting the average bin confidence \n\\begin{equation}\\label{eq:conf}\n    \\text{conf}(b_m)=\\frac{1}{\\vert b_m \\vert} \\sum_{s\\in b_m}\\hat{p}_s\n\\end{equation}\n in relation to the average bin accuracy \n \\begin{equation}\\label{eq:acc}\n    \\text{acc}(b_m) = \\frac{1}{\\vert b_m \\vert} \\sum_{s \\in b_m} \\mathbbm{1}(\\hat{y}_s=y_s)~,\n\\end{equation}\nwhere  $\\hat{y}_s$, $y_s$ and $\\hat{p}_s$ refer to the predicted and true class label of a sample $s$. As noted in , confidences are well-calibrated when for each bin $\\text{acc}(b_m)=\\text{conf}(b_m)$.\nFor a visual evaluation of a model's calibration, the reliability diagram introduced by  is widely used. For a reliability diagram, the $\\text{conf}(b_m)$ is plotted against $\\text{acc}(b_m)$. For a well-calibrated model, the plot should be close to the diagonal, as visualized in Figure \\ref{fig:reliability_diagram}. The basic reliability diagram visualization does not distinguish between different classes. In order to do so and hence to improve the interpretability of the calibration error, Vaicenavicius et al.  used an alternative visualization named multidimensional reliability diagram. \nFor a quantitative evaluation of a model's calibration, different calibration measures can be considered. \\\\\nThe \\textit{Expected Calibration Error} (ECE) is a widely used binning based calibration measure . For the ECE, $M$ equally-spaced bins $b_1,...,b_M$ are considered, where $b_m$ denotes the set of indices of samples whose confidences fall into the interval $I_m=]\\frac{m-1}{M},\\frac{m}{M}]$. The ECE is then computed as the weighted average of the bin-wise calibration errors, i.e.\n\\begin{equation}\\label{eq:ece}\n    \\text{ECE} = \\sum_{m=1}^{M}\\frac{\\vert b_m \\vert}{N}\\vert \\text{acc}(b_m)-\\text{conf}(b_m)\\vert~.\n\\end{equation}\nFor the ECE, only the predicted confidence score (top-label) is considered. In contrast to this, the \\textit{Static Calibration Error} (SCE)  considers the predictions of all classes (all-labels). For each class, the SCE computes the calibration error within the bins and then averages across all the bins, i.e.\n\\begin{equation}\\label{eq:sce}\n   \\text{SCE} = \\frac{1}{K} \\sum_{k=1}^{K} \\sum_{m=1}^{M} \\frac{\\vert b_{m_k} \\vert}{N} \\vert \\text{conf}(b_{m_k})-\\text{acc}(b_{m_k}) \\vert~.\n\\end{equation}\nHere $conf(b_{m_k})$ and $acc(b_{m_k})$ are the confidence and accuracy of bin $b_m$ for class label $k$, respectively. Nixon et al.  empirically showed that all-labels calibration measures such as the SCE are more effective in assessing the calibration error than the top-label calibration measures as the ECE.\nIn contrast to the ECE and SCE, which group predictions into $M$ equally-spaced bins (what in general leads to different numbers of evaluation samples per bin), the adaptive calibration error  adaptively groups predictions into $R$ bins with different width but equal number of predictions. With this adaptive bin size, the \\textit{adaptive Expected Calibration Error} (aECE)\n\\begin{equation}\\label{eq:a_ece}\n    \\text{aECE} = \\frac{1}{R}\\sum_{r=1}^{R} \\vert \\text{conf}(b_r) - \\text{acc}(b_r) \\vert~,\n\\end{equation}\nand the \\textit{adaptive Static Calibration Error} (aSCE)\n\\begin{equation}\\label{eq:a_sce}\n    \\text{aSCE} = \\frac{1}{K R} \\sum_{k=1}^{K} \\sum_{r=1}^{R} \\vert \\text{conf}(b_{r_k})-\\text{acc}(b_{r_k}) \\vert\n\\end{equation}\nare defined as extensions of the ECE and the SCE.\\\\\nAs has been empirically shown in  and , the adaptive binning calibration measures $\\text{aECE}$ and $\\text{aSCE}$ are more robust to the number of bins than the corresponding equal-width binning calibration measures $\\text{ECE}$ and $\\text{SCE}$.\nIt is important to make clear that in a multi-class setting, the calibration measures can suffer from imbalance in the test data. Even when then calibration is computed classwise, the computed errors are weighted by the number of samples in the classes. Following, larger classes can shadow the bad calibration on small classes, comparable to accuracy values in classification tasks .", "cites": [8832, 759, 4693, 4607, 3254, 4699, 4698, 7838, 4694], "cite_extract_rate": 0.75, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.7, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a strong analytical overview of calibration evaluation methods, integrating concepts from multiple cited works to explain different metrics (ECE, SCE, aECE, aSCE) and their strengths and weaknesses. It highlights the transition from top-label to all-label calibration measures and discusses the impact of data imbalance on evaluation. The section shows a good level of synthesis and abstraction, while offering some critical insights into the limitations of metrics like ECE."}}
{"id": "2dbc07cb-8a4d-4558-872e-32e013a2582a", "title": "Data Sets and Baselines", "level": "section", "subsections": [], "parent_id": "039eb0dd-a257-4d85-a714-ad5fc65c6f0b", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Data Sets and Baselines"]], "content": "\\label{sec:data_sets_and_baselines} \n\\begin{table*}[ht!]\n\t\\centering\n\t\\caption{Overview of frequently compared benchmark approaches, tasks and their data sets among existing works organized according to the taxonomy of this paper.}\n\t\\begin{tabular}{>{\\raggedright\\arraybackslash}p{3.1cm}p{4.5cm}p{3.5cm}p{4cm}}\n\t\t& \\textbf{Tasks}\n\t\t& \\textbf{Task index: Data sets}\n\t\t& \\textbf{Baselines}\n\t\t\\\\ \\hline\n\t\t\\addlinespace[2ex]\n\t\t\\textbf{Bayesian Neural Networks}  \n\t\t& 1. Regression\\textsuperscript{1,3,4,7,8,11,12,15-17}\n\t\t\\newline 2. Calibration\\textsuperscript{6,10,13,14}\n\t\t\\newline 3. OOD Detection\\textsuperscript{4,6,8,10,12,13}\n\t\t\\newline 4. Adversarial Attacks\\textsuperscript{4,8,12}\n\t\t\\newline 5. Active Learning\\textsuperscript{7,12,14} \n\t\t\\newline 6. Continual Learning\\textsuperscript{10}\n\t\t\\newline 7. Reinforcement Learning (Intrinsic Motivation, Contextual Bandits)\\textsuperscript{1,14,15}\n\t\t& 1: UCI\n\t\t\\newline~~2,~~3,~~4:~~(not)MNIST,\n\t\t\\newline~~CIFAR10/100,~~SHVN,\n\t\t\\newline~~ImageNet\n\t\t\\newline 5: UCI\n\t\t\\newline 6: Permuted MNIST\n\t\t& Softmax\\textsuperscript{44},\n\t\t\\newline MCdropout\\textsuperscript{\\hyperlink{gal2016dropout}{1}}, ~~DeepEnsemble\\textsuperscript{\\hyperlink{2}{2}},\n\t\t\\newline BBB\\textsuperscript{3},~~NormalizingFlow\\textsuperscript{4},\n\t\t\\newline PBP\\textsuperscript{7},~~SWAG\\textsuperscript{6},\n\t\t\\newline KFAC\\textsuperscript{8},~~DVI\\textsuperscript{11}\n\t\t\\newline HMC\\textsuperscript{9}, ~~\n\t\t\\newline VOGN\\textsuperscript{10},~~INF\\textsuperscript{12}\n\t\t\\\\ \n\t\t\\addlinespace[2ex]\n\t\t\\multicolumn{4}{p\\linewidth}{\n\t\t\t\t\t\\textsuperscript{\\hypertarget{gal2016dropout}{1}}~\n\t\t\t\t\t\\textsuperscript{3}~\n\t\t\t\t\t\\textsuperscript{4}~\n\t\t\t\t\t\\textsuperscript{5}~\n\t\t\t\t\t\\textsuperscript{6}~\n\t\t\t\t\t\\textsuperscript{7}~\n\t\t\t\t\t\\textsuperscript{8}~\n\t\t\t\t\t\\textsuperscript{9}~\n\t\t\t\t\t\\textsuperscript{10}~\n\t\t\t\t\t\\textsuperscript{11}~\n\t\t\t\t\t\\textsuperscript{12}~\n\t\t\t\t\t\\textsuperscript{13}~\n\t\t\t\t\t\\textsuperscript{14}~\n\t\t\t\t\t\\textsuperscript{15}~\n\t\t\t\t\t\\textsuperscript{16}~\n\t\t\t\t\t\\textsuperscript{17}~\n\t\t\t\t}\n\t\t\t\t\\\\\n\t\t\\addlinespace[2ex]\n\t\t\\hline\n\t\t\\addlinespace[2ex]\n\t\t\\textbf{Ensembles}\n\t\t& 1. Regression\\textsuperscript{2,20}\n\t\t\\newline 2. Calibration\\textsuperscript{2,20,24-32}\n\t\t\\newline 3. OOD Detection\\textsuperscript{2,20,25,27-30,32-34}\n\t\t\\newline 4. Active Learning\\textsuperscript{31}\n\t\t& 1: Toy\\textsuperscript{7}, UCI\n\t\t\\newline 2, 3: Toy, (not)MNIST, SVHN, LSUN, CIFAR10/100,\n\t\t\\newline (Tiny)ImageNet, Diabetic~~Retinopathy\n\t\t\\newline 4: MNIST\n\t\t& Softmax\\textsuperscript{44},~~MFVI\\textsuperscript{5},~~SGLD\\textsuperscript{55},\n\t\t\\newline MCdropout\\textsuperscript{\\hyperlink{1}{1}},~~DeepEnsemble\\textsuperscript{2},\n\t\t\\newline BBB\\textsuperscript{3},~~PBP\\textsuperscript{7},~~NormalizingFlow\\textsuperscript{4},\n\t\t\\newline TemperatureScaling\\textsuperscript{38,54}\n\t\t\\\\ \n\t\t\\addlinespace[2ex]\n\t\t\\multicolumn{4}{p\\linewidth}{\n\t\t\t\\textsuperscript{\\hypertarget{2}{2}}~\n\t\t\t\\textsuperscript{20}~\n\t\t\t\\textsuperscript{24}~\n\t\t\t\\textsuperscript{25}~\n\t\t\t\\textsuperscript{26}~\n\t\t\t\\textsuperscript{27}~\n\t\t\t\\textsuperscript{28}~\n\t\t\t\\textsuperscript{29}~\n\t\t\t\\textsuperscript{30}~\n\t\t\t\\textsuperscript{31}~\n\t\t\t\\textsuperscript{32}~\n\t\t\t\\textsuperscript{33}~\n\t\t\t\\textsuperscript{34}~\n\t\t}\n\t\t\\\\%\\hline\n\t\t\\addlinespace[2ex]\n\t\t\\hline\n\t\t\\addlinespace[2ex]\n\t\t\\textbf{Single Deterministic Models}\n\t\t& 1. Regression\\textsuperscript{21-23}\n\t\t\\newline 2. Calibration\\textsuperscript{21-23,39,40,41,43}\n\t\t\\newline 3. OOD Detection\\textsuperscript{21,22,38,39,41-53}\n\t\t\\newline 4. Adversarial Attacks\\textsuperscript{21,41,48}\n\t\t& 1. Toy\\textsuperscript{7}, UCI, NYU Depth\n\t\t\\newline~~2,~~3:~~(E/Fashion/not)MNIST,\n\t\t\\newline~~Toy,~~CIFAR10/100,~~SVHN,\n\t\t\\newline~~LSUN,~~(Tiny)ImageNet,~~IMDB,\n\t\t\\newline~~Diabetic~~Retinopathy,~~Omniglot\n\t\t\\newline 4: MNIST, CIFAR10, NYU Depth, Omniglot\n\t\t& Softmax\\textsuperscript{44},~~GAN\\textsuperscript{27},~~Dirichlet\\textsuperscript{48},~~BBB\\textsuperscript{3},\n\t\t\\newline~~MCdropout\\textsuperscript{\\hyperlink{1}{1}},~~DeepEnsemble\\textsuperscript{\\hyperlink{2}{2},57},\n\t\t\\newline~~Mahalanobis\\textsuperscript{56},~~TemperatureScaling\\textsuperscript{38,54}\n\t\t\\newline~~NormalizingFlow\\textsuperscript{4}\n\t\t\\\\%\\hline\n\t\t\\addlinespace[2ex]\n\t\t\\multicolumn{4}{p\\linewidth}{\n\t\t\t\\textsuperscript{21}~\n\t\t\t\\textsuperscript{22}~\n\t\t\t\\textsuperscript{23}~\n\t\t\t\\textsuperscript{38}\n\t\t\t\\textsuperscript{39}\n\t\t\t\\textsuperscript{40}\n\t\t\t\\textsuperscript{41}\n\t\t\t\\textsuperscript{42}\n\t\t\t\\textsuperscript{43}\n\t\t\t\\textsuperscript{44}\n\t\t\t\\textsuperscript{45}\n\t\t\t\\textsuperscript{46}\n\t\t\t\\textsuperscript{47}\n\t\t\t\\textsuperscript{48}\n\t\t\t\\textsuperscript{49}\n\t\t\t\\textsuperscript{50}\n\t\t\t\\textsuperscript{51}\n\t\t\t\\textsuperscript{52}\n\t\t\t\\textsuperscript{53}\n\t\t}\n\t\t\\\\\n\t\t\\addlinespace[2ex]\n\t\t\\hline\n\t\t\\addlinespace[2ex]\n\t\t\\textbf{Test-Time Data Augmentation}\n\t\t& 1. Semantic Segmentation\\textsuperscript{36, 37}\n\t\t\\newline 2. Calibration\\textsuperscript{35}\n\t\t\\newline 3. OOD Detection\\textsuperscript{35-37}\n\t\t& 1, 2, 3: Medical data, Diabetic Retinopathy\n\t\t& Softmax\\textsuperscript{44},~~MCdropout\\textsuperscript{\\hyperlink{1}{1}}\n\t\t\\\\%\\hline\n\t\t\\addlinespace[2ex]\n\t\t\\multicolumn{4}{p\\linewidth}{\n\t\t\t\\textsuperscript{35}~\n\t\t\t\\textsuperscript{36}~\n\t\t\t\\textsuperscript{37}~\n\t\t}\n\t\t\\\\\n\t\t\\addlinespace[2ex]\n\t\t\\hline\t\t\n\t\t\\\\\n\t\t\\multicolumn{4}{p\\linewidth}{\n\t\t\t\\textsuperscript{54}\n\t\t\t\\textsuperscript{55}\n\t\t\t\\textsuperscript{56}\n\t\t\t\\textsuperscript{57}\n\t\t}\n\t\\end{tabular}\n\t\\label{tab:datasets_baselines}\n\\end{table*}\nIn this section, we collect commonly used tasks and data sets for evaluating uncertainty estimation among existing works. Besides, a variety of baseline approaches commonly used as comparison against the methods proposed by the researchers are also presented. By providing a review on the relevant information of these experiments, we hope that both researchers and practitioners can benefit from it. While the former can gain a basic understanding of recent benchmarks tasks, data sets and baselines so that they can design appropriate experiments to validate their ideas more efficiently, the latter might use the provided information to select more relevant approaches to start based on a concise overview on the tasks and data sets on which the approach has been validated.\nIn the following, we will introduce the data sets and baselines summarized in table \\ref{tab:datasets_baselines} according to the taxonomy used throughout this review.\nThe structure of the table is designed to organize the main contents of this section concisely, hoping to provide a clear overview of the relevant works. We group the approaches of each category into one of four blocks and extract the most commonly used tasks, data sets and provided baselines for each column respectively. The corresponding literature is listed at the bottom of each block to facilitate lookup. Note that we focus on methodological comparison here, but not the choice of architecture for different methods which has an impact on performance as well. Due to the space limitation and visual density, we only show the most important elements (task, data set, baselines) ranked according to the frequency of use in the literature we have researched.\nThe main results are as follows. One of the most frequent tasks for evaluating uncertainty estimation methods are regression tasks, where samples close and far away from the training distribution are studied. Furthermore, the calibration of uncertainty estimates in the case of classification problems is very often investigated. \nFurther noteworthy tasks are out-of-distribution (OOD) detection and robustness against adversarial attacks. In the medical domain, calibration of semantic segmentation results is the predominant use case.\nThe choice of data sets is mostly consistent among all reviewed works. For regression, toy data sets are employed for visualization of uncertainty intervals while the UCI data sets are studied in light of (negative) log-likelihood comparison. The most common data sets for calibration and OOD detection are MNIST, CIFAR10 and 100 as well as SVHN while ImageNet and its tiny variant are also studied frequently. These form distinct pairs when OOD detection is studied where models trained on CIFAR variants are evaluated on SVHN and visa versa while MNIST is paired with variants of itself like notMNIST and FashionMNIST. Classification data sets are also commonly distorted and corrupted to study the effects on calibration, blurring the line between OOD detection and adversarial attacks.\nFinally, the most commonly used baselines by far are Monte Carlo (MC) Dropout and deep ensembles while the softmax output of deterministic models is almost always employed as a kind of surrogate baseline. It is interesting to note that inside each approach--BNNs, Ensembles, Single Deterministic Models and Input Augmentation--some baselines are preferred over others.\nBNNs are most frequently compared against variational inference methods like Bayes' by Backprop (BBB) or Probabilistic Backpropagation (PBP) while for Single Deterministic Models it is more common to compare them against distance-based methods in the case of OOD detection.\nOverall, BNN methods show a more diverse set of tasks considered while being less frequently evaluated on large data sets like ImageNet.\nTo further facilitate access for practitioners, we provide web-links to the authors' official implementations (marked by a star) of all common baselines as identified in the baselines column. Where no official implementation is provided, we instead link to the highest ranked implementations found on \\href{https://github.com/}{GitHub} at the time of this survey. The list can be also found within \\href{https://github.com/JakobCode/UncertaintyInNeuralNetworks\\_Resources}{our GitHub repository on available implementations}\\footnote{\\href{https://github.com/JakobCode/UncertaintyInNeuralNetworks\\_Resources}{https://github.com/JakobCode/UncertaintyInNeuralNetworks\\_Resources}}. The relevant baselines are Softmax\\textsuperscript{*} (\\href{https://github.com/hendrycks/error-detection}{TensorFlow}, \\href{https://github.com/hendrycks/outlier-exposure}{PyTorch}), MCdropout (\\href{https://github.com/yaringal/DropoutUncertaintyExps}{TensorFlow\\textsuperscript{*}}; PyTorch: \\href{https://github.com/cpark321/uncertainty-deep-learning}{1}, \\href{https://github.com/JavierAntoran/Bayesian-Neural-Networks}{2}), DeepEnsembles (TensorFlow: \\href{https://github.com/vvanirudh/deep-ensembles-uncertainty}{1}, \\href{https://github.com/Kyushik/Predictive-Uncertainty-Estimation-using-Deep-Ensemble}{2}, \\href{https://github.com/axelbrando/Mixture-Density-Networks-for-distribution-and-uncertainty-estimation}{3}; PyTorch: \\href{https://github.com/bayesgroup/pytorch-ensembles}{1}, \\href{https://github.com/cpark321/uncertainty-deep-learning}{2}), BBB (PyTorch: \\href{https://github.com/ThirstyScholar/bayes-by-backprop}{1}, \\href{https://github.com/nitarshan/bayes-by-backprop}{2}, \\href{https://github.com/cpark321/uncertainty-deep-learning}{3}, \\href{https://github.com/JavierAntoran/Bayesian-Neural-Networks}{4}), NormalizingFlow (\\href{https://github.com/AMLab-Amsterdam/MNF_VBNN}{TensorFlow}, \\href{https://github.com/janosh/torch-mnf}{PyTorch}), \\href{https://github.com/HIPS/Probabilistic-Backpropagation}{PBP}, SWAG (\\href{https://github.com/wjmaddox/swa_gaussian}{1\\textsuperscript{*}}, \\href{https://github.com/bayesgroup/pytorch-ensembles}{2}), KFAC (PyTorch: \\href{https://github.com/DLR-RM/curvature}{1}, \\href{https://github.com/bayesgroup/pytorch-ensembles}{2}, \\href{https://github.com/JavierAntoran/Bayesian-Neural-Networks}{3}; \\href{https://github.com/tensorflow/kfac}{TensorFlow}), DVI (\\href{https://github.com/Microsoft/deterministic-variational-inference}{TensorFlow\\textsuperscript{*}}, \\href{https://github.com/markovalexander/DVI}{PyTorch}), \\href{https://github.com/JavierAntoran/Bayesian-Neural-Networks}{HMC}, \\href{https://github.com/team-approx-bayes/dl-with-bayes}{VOGN\\textsuperscript{*}}, \\href{https://github.com/DLR-RM/curvature}{INF\\textsuperscript{*}}, \\href{https://github.com/ctallec/pyvarinf}{MFVI}, \\href{https://github.com/JavierAntoran/Bayesian-Neural-Networks}{SGLD}, TemperatureScaling (\\href{https://github.com/gpleiss/temperature_scaling}{1\\textsuperscript{*}}, \\href{https://github.com/facebookresearch/odin}{2}, \\href{https://github.com/cpark321/uncertainty-deep-learning}{3}), \\href{https://github.com/KaosEngineer/PriorNetworks}{GAN\\textsuperscript{*}}, \\href{https://github.com/dougbrion/pytorch-classification-uncertainty}{Dirichlet} and \\href{https://github.com/pokaxpoka/deep_Mahalanobis_detector}{Mahalanobis\\textsuperscript{*}}.", "cites": [4630, 4664, 4617, 4627, 4590, 4648, 4700, 4649, 4679, 4626, 3289, 4594, 4620, 4621, 3293, 759, 3302, 4625, 3288, 8812, 4616, 4619, 4025, 4642, 4592, 4680, 4676, 4674, 8639, 1624, 8830, 4678, 4683, 3267, 3251, 4692, 3278, 4613], "cite_extract_rate": 0.7307692307692307, "origin_cites_number": 52, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section is primarily descriptive, presenting a table of datasets, tasks, and baseline methods used in uncertainty estimation. While it organizes information by categories and tasks, it lacks deeper synthesis of ideas across papers, critical evaluation of approaches, and abstraction to broader principles. The narrative is minimal and focuses on listing rather than analyzing or connecting the cited works in a meaningful way."}}
{"id": "77b5da91-1f19-4c15-8640-b5faf69d58e8", "title": "Active Learning", "level": "subsubsection", "subsections": ["f7e0d8b1-9d2d-49ec-9731-26873e781591"], "parent_id": "56922a8f-84a8-4d62-aa6d-86c2e59e8e40", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Applications of Uncertainty Estimates"], ["subsubsection", "Active Learning"]], "content": "The process of collecting labeled data for supervised training of a DNN can be laborious, time-consuming, and costly. To reduce the annotation effort, the active learning framework shown in Figure \\ref{fig:active_learning} trains the DNN sequentially on different labelled data sets increasing in size over time . In particular, given a small labelled data set and a large unlabeled data set, a deep neural network trained in the setting of active learning learns from the small labeled data set and decides based on the acquisition function, which samples to select from the pool of unlabeled data. The selected data are added to the training data set and a new DNN is trained on the updated training data set. This process is then repeated with the training set increasing in size over time. Uncertainty sampling is one most popular criteria used in acquisition functions  where predictive uncertainty determines which training samples have the highest uncertainty and should be labelled next. Uncertainty based active learning strategies for deep learning applications have been successfully used in several works . \n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.38\\textwidth]{figures/active_learning.pdf}\n    \\caption{The active learning framework: The acquisition function evaluates the uncertainties on the network's test predictions in order to select unlabelled data. The selected data are labelled and added to the pool of labelled data, which is used to train and improve the performance of the predictor.}\n    \\label{fig:active_learning}\n\\end{figure}", "cites": [4599, 4610, 4701, 8805, 1044], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a general description of active learning and the role of uncertainty sampling, citing relevant papers. While it mentions the use of uncertainty estimates in acquisition functions, it does not deeply synthesize the cited works or highlight their specific contributions or differences. The analysis remains at a surface level without evaluating limitations or identifying broader patterns or principles across the cited methods."}}
{"id": "f7e0d8b1-9d2d-49ec-9731-26873e781591", "title": "Reinforcement Learning", "level": "subsubsection", "subsections": [], "parent_id": "77b5da91-1f19-4c15-8640-b5faf69d58e8", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Applications of Uncertainty Estimates"], ["subsubsection", "Active Learning"], ["subsubsection", "Reinforcement Learning"]], "content": "The general framework of deep reinforcement learning is shown in Figure \\ref{fig:reinforcement_learning}. In the context of reinforcement learning, uncertainty estimates can be used to solve the exploration-exploitation dilemma. It says that uncertainty estimates can be used to effectively balance the exploration of unknown environments with the exploitation of existing knowledge extracted from known environments. For example, if a robot interacts with an unknown environment, the robot can safely avoid catastrophic failures by reasoning about its uncertainty. To estimate the uncertainty in this framework, Huang et al.  used an ensemble of bootstrapped models (models trained on different data sets sampled with replacement from the original data set), while Gal and Ghahramani  approximated Bayesian inference via dropout sampling. Inspired by  and , Kahn et al.  and LÃ¶tjens et al.  used a mixture of deep Bayesian networks performing dropout sampling on an ensemble of bootstrapped models. For further reading, Ghavamzadeh et al.  presented a survey of Bayesian reinforcement learning. \n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.38\\textwidth]{figures/reinforcement_learning.pdf}\n    \\caption{The reinforcement learning framework: The agent interacts with the environment by executing a specific action influencing the next state of the agent. The agent observes a reward representing the cost associated with the executed action. The agent chooses actions based on a policy learned by a deep neural network. However, the predicted uncertainty associated with the action predicted by the deep neural network can help the agent to decide weather to execute the predicted action or not.}\n    \\label{fig:reinforcement_learning}\n\\end{figure}", "cites": [4596, 4614, 1343], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of how uncertainty estimates are used in reinforcement learning, mentioning specific methods like ensemble of bootstrapped models and dropout-based Bayesian inference. However, it lacks deeper synthesis of the cited papers, does not critically evaluate their strengths or weaknesses, and offers minimal abstraction or generalization beyond individual approaches."}}
{"id": "e330bdbb-a5a2-4a38-b6ec-85fed5ce7a5a", "title": "Medical Analysis", "level": "subsubsection", "subsections": [], "parent_id": "9b15acc6-a237-457d-988d-2dbb131f41f4", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Applications of Uncertainty Estimates"], ["subsection", "Uncertainty in Real-World Applications"], ["subsubsection", "Medical Analysis"]], "content": "Since the size, shape, and location of many diseases vary largely across patients, the estimation of the predictive uncertainty is crucial in analyzing medical images in applications such as lesion detection , lung node segmentation , brain tumor segmentation , parasite segmentation in images of liver stage malaria , recognition of abnormalities on chest radiographs , and age estimation . Here, uncertainty estimates in particular improve the interpretability of decisions of DNNs . They are essential to understand the reliability of segmentation results, to detect false segmented areas and to guide human experts in the task of refinement . Well-calibrated and reliable uncertainty estimates allow clinical experts to properly judge whether an automated diagnosis can be trusted . Uncertainty was estimated in medical image segmentation based on Monte Carlo dropout , spike-and-slab dropout , and spatial dropout . Wang et al.  used test time data augmentation to estimate the data-dependent uncertainty in medical image segmentation.", "cites": [4703, 4591, 4603, 4702, 4687, 4688, 4683, 8831, 4606], "cite_extract_rate": 0.6, "origin_cites_number": 15, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "low", "analysis": "The section provides a factual overview of medical applications where uncertainty estimation is used, citing specific papers for each application. However, it lacks deep synthesis of ideas or a broader framework to connect these applications. There is minimal critical analysis or evaluation of the cited methods, and the level of abstraction remains low, focusing mainly on concrete examples rather than overarching principles."}}
{"id": "d9cfe77c-99d1-4e5a-8976-ae60d7f37ffd", "title": "Robotics", "level": "subsubsection", "subsections": [], "parent_id": "9b15acc6-a237-457d-988d-2dbb131f41f4", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Applications of Uncertainty Estimates"], ["subsection", "Uncertainty in Real-World Applications"], ["subsubsection", "Robotics"]], "content": "Robots are active agents that perceive, decide, plan, and act in the real-world â all based on their incomplete knowledge about the world. As a result, mistakes of the robots not only cause failures of their own mission, but can endanger human lives, e.g. in case of surgical robotics, self-driving cars, space robotics, etc. Hence, the robotics application of deep learning poses unique research challenges that significantly differ from those often addressed in computer vision and other off-line settings . For example, the assumption that the testing condition come from the same distribution as training is often invalid in many settings of robotics, resulting in deterioration of the performance of DNNs in uncontrolled and detrimental conditions. This raises the questions how we can quantify the uncertainty in a DNNâs predictions in order to avoid catastrophic failures. Answering such questions are important in robotics, as it might be a lofty goal to expect data-driven approaches (in many aspects from control to perception) to always be accurate. Instead, reasoning about uncertainty can help in leveraging the recent advances in deep learning for robotics. \nReasoning about uncertainties and the use of probabilistic representations, as oppose to relying on a single, most-likely estimate, have been central to many domains of robotics research, even before the advent of deep learning . In robot perception, several uncertainty-aware methods have been proposed in the past, starting from localization methods  to simultaneous localization and mapping (SLAM) frameworks . As a result, many probabilistic methods such as factor graphs  are now the work-horse of advanced consumer products such as robotic vacuum cleaners and unmanned aerial vehicles. In case of planning and control, estimation problems are widely treated as Bayesian sequential learning problems, and sequential decision making frameworks such as POMDPs  assume a probabilistic treatment of the underlying planning problems. With probabilistic representations, many reinforcement learning algorithms are backed up by stability guarantees for safe interactions in the real-world . Lastly, there have been also several advances starting from reasoning (semantics  to joint reasoning with geometry), embodiment (e.g. active perception ) to learning (e.g. active learning  and identifying unknown objects ). \nSimilarly, with the advent of deep learning, many researchers proposed new methods to quantify the uncertainty in deep learning as well as on how to further exploit such information. As oppose to many generic approaches, we summarize task-specific methods and their application in practice as followings. Notably,  proposed to perform novelty detection using auto-encoders, where the reconstructed outputs of auto-encoders was used to decide how much one can trust the networkâs predictions. Peretroukhin et al.  developed a SO(3) representation and uncertainty estimation framework for the problem of rotational learning problems with uncertainty.  demonstrated uncertainty-aware, real world application of a reinforcement learning algorithm for robotics, while  proposed to leverage spatial information, on top of MC-dropout.  developed deep learning based localization systems along with uncertainty estimates. Other approaches also learn on the robots' past experiences of failures or detect inconsistencies of the predictors . In summary, the robotics community has been both, the users and the developers of the uncertainty estimation frameworks targeted to a specific problems. \nYet, robotics pose several unique challenges to uncertainty estimation methods for DNNs. These are for example, (i) how to limit the computational burden and build real-time capable methods that can be executed on the robots with limited computational capacities (e.g. aerial, space robots, etc); (ii) how to leverage spatial and temporal information, as robots sense sequentially instead of having a batch of training data for uncertainty estimates; (iii) whether robots can select the most uncertainty samples and update its learner online; (iv) Whether robots can purposefully manipulate the scene when uncertain. Most of these challenges arise due to the properties of robots that they are physically situated systems.", "cites": [4711, 4710, 4704, 3881, 8828, 8833, 4705, 4706, 4596, 4712, 4707, 4708, 4709, 4614, 8400], "cite_extract_rate": 0.40540540540540543, "origin_cites_number": 37, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple robotics-related papers to construct a coherent narrative on the importance and application of uncertainty estimation in robotic systems. It critically evaluates the unique challenges in the robotics domain and connects specific methods to broader themes like real-time computation and spatial-temporal reasoning. The abstraction is strong, as it generalizes across methods and identifies overarching issues such as safety, embodiment, and online learning in robotics."}}
{"id": "e98c01dd-e9df-4538-8b11-ca24c3cd697b", "title": "Earth Observation(EO)", "level": "subsubsection", "subsections": [], "parent_id": "9b15acc6-a237-457d-988d-2dbb131f41f4", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Applications of Uncertainty Estimates"], ["subsection", "Uncertainty in Real-World Applications"], ["subsubsection", "Earth Observation(EO)"]], "content": "Earth Observation (EO) systems are increasingly used to make critical decisions related to urban planning , resource management , disaster response , and many more. Right now, there are hundreds of EO satellites in space, owned by different space agencies and private companies. Figure \\ref{fig:ESA} shows the satellites owned by the European Space Agency (ESA). Like in many other domains, deep learning has shown great initial success in the field of EO over the past few years . These early successes consisted of taking the latest developments of deep learning in computer vision and applying them to small curated earth observation data sets . At the same time, the underlying data is very challenging. Even though the amount of data is huge, so is the variability in the data. This variability is caused by different sensor types, spatial changes (e.g. different regions and resolutions), and temporal changes (e.g. changing light conditions, weather conditions, seasons). Besides the challenge of efficient uncertainty quantification methods for such large amounts of data, several other challenges that can be tackled with uncertainty quantification exist in the field of EO. All in all, the sensitivity of many EO applications together with the nature of EO systems and the challenging EO data make the quantification of uncertainties very important in this field. Despite hundreds of publications in the last years on DL for EO, the range of literature on measuring uncertainties of these systems is relatively small. \nFurthermore, due to the large variation in the data, a data sample received at test time is often not covered by the training data distribution. For example while preparing training data for a local climate zone classification, the human experts might be presented only with images where there is no obstruction and structures are clearly visible. When a model which is trained on this data set is deployed in real world, it might see the images with clouds obstructing the structures or snow giving them a completely different look. Also, the classes in EO data can have a very wide distribution. For example, there are millions of types of houses in the world and no training data can contain the examples for all of them. The question is where the OOD detector will draw the line and declare the following houses as OOD. Hence, OOD detection is important in earth observation and uncertainty measurements play an important part in this . \nAnother common task in EO, where uncertainties can play an important role, is the data fusion. Optical images normally contain only a few channels like RGB. In contrast to this, EO data can contain optical images with up to hundreds of channels, and a variety of different sensors with different spatial, temporal, and semantic properties. Fusing the information from these different sources and channels propagates the uncertainties from different sources onto the prediction. The challenge lies in developing methods that do not only quantify uncertainties but also the amount of contribution from different channels individually and which learn to focus on the trustworthy data source for a given sample .\nUnlike normal computer vision scenarios where the image acquisition equipment is quite near to the subject, the EO satellites are hundreds of kilometers away from the subject. The sensitivity of sensors, the atmospheric absorption properties, and surface reflectance properties all contribute to uncertainties in the acquired data. Integrating the knowledge of physical EO systems, which also contain information about uncertainty models in those systems, is another major open issue. However, for several applications in EO, measuring uncertainties is not only something good to have but rather an important requirement of the field. E.g., the geo-variables derived from EO data may be assimilated into process models (ocean, hydrological, weather, climate, etc) and the assimilation requires the probability distribution of the estimated variables.\n\\begin{figure*}[t]\n\\resizebox{\\textwidth}{!}{\n   \\includegraphics{figures/ESA_satellites_cropped.jpg}}\n    \\caption{European Space Agency (ESA) Developed Earth Observation Missions .}\n    \\label{fig:ESA} \n\\end{figure*}", "cites": [4609], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a coherent narrative on the importance of uncertainty in Earth Observation applications, integrating the cited paper's focus on out-of-distribution detection. It connects this idea to broader challenges in the field, such as data variability and sensor sensitivity. While it does offer some critical analysis by pointing out the limited literature on uncertainty in EO, it could more deeply evaluate the cited approaches or contrast them with others."}}
{"id": "964544a8-0c56-446c-8edc-dc3bb28f1f78", "title": "Conclusion - How well do the current uncertainty quantification methods work for real world applications?", "level": "subsection", "subsections": [], "parent_id": "4fde619f-e57d-42f5-974b-6ca984c28e03", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Conclusion and Outlook"], ["subsection", "Conclusion - How well do the current uncertainty quantification methods work for real world applications?"]], "content": "\\label{ssec:conclusion}\nEven though many advances on uncertainty quantification in neural networks have been made over the last years, their adoption in practical mission- and safety-critical applications is still limited. There are several reasons for this, which are discussed one-by-one as follows:\n\\begin{itemize}\n  \\setlength\\itemsep{0.5em}\n    \\item \\textbf{Missing Validation of Existing Methods over Real-World Problems}~\\\\\n    Although DNNs have become the defacto standard in solving numerous computer vision and medical image processing tasks, the majority of existing models are not able to appropriately quantify uncertainty that is inherent to their inferences particularly in real world applications. This is primarily because the baseline models are mostly developed using standard data sets such as Cifar10/100, ImageNet, or well known regression data sets that are specific to a particular use case and are therefore not readily applicable to complex real-world environments, as for example low resolutional satellite data or other data sources affected by noise. Although many researchers from other fields apply uncertainty quantification in their field , a broad and structured evaluation of existing methods based on different real world applications is not available yet. Works like  already built first steps towards a real life evaluation.\n    \\item \\textbf{Lack of Standardized Evaluation Protocol}~\\\\\n    Existing methods for evaluating the estimated uncertainty are better suited to compare uncertainty quantification methods based on measurable quantities such as the calibration  or the performance on out-of-distribution detection . As described in Section \\ref{sec:data_sets_and_baselines}, these tests are performed on standardized sets within the machine learning community. Furthermore, the details of these experiments might differ in the experimental setting from paper to paper . However, a clear standardized protocol of tests that should be performed on uncertainty quantification methods is still not available. For researchers from other domains it is difficult to directly find state of the art methods for the field they are interested in, not to speak of the hard decision on which sub-field of uncertainty quantification to focus. This makes the direct comparison of the latest approaches difficult and also limits the acceptance and adoption of current existing methods for uncertainty quantification.\n    \\item \\textbf{Inability to Evaluate Uncertainty Associated to a Single Decision}~\\\\ \n    Existing measures for evaluating the estimated uncertainty (e.g., the expected calibration error) are based on the whole testing data set. This means, that equivalent to classification tasks on unbalanced data sets, the uncertainty associated with single samples or small groups of samples may potentially get biased towards the performance on the rest of the data set. But for practical applications, assessing the reliability of a predicted confidence would give much more possibilities than an aggregated reliability based on some testing data, which are independent from the current situation . Especially for mission- and safety-critical applications, pointwise evaluation measures could be of paramount importance and hence such evaluation approaches are very desirable. \n    \\item \\textbf{Lack of Ground Truth Uncertainties}~\\\\\n    Current methods are empirically evaluated and the performance is underlined by reasonable and explainable values of uncertainty. A ground truth uncertainty that could be used for validation is in general not available.\n    Additionally, even though existing methods are calibrated on given data sets, one cannot simply transfer these results to any other data set since one has to be aware of shifts in the data distribution and that many fields can only cover a tiny portion of the actual data environment. In application fields as EO, the preparation of a huge amount of training data is hard and expensive and hence synthetic data can be used to train a model. For this artificial data, artificial uncertainties in labels and data should be taken into account to receive a better understanding of the uncertainty quantification performance. The gap between the real and synthetic data, or estimated and real uncertainty further limits the adoption of currently existing methods for uncertainty quantification.\n    \\item \\textbf{Explainability Issue:} ~\\\\\n    Existing methods of neural network uncertainty quantification deliver predictions of certainty without any clue about what causes possible uncertainties. Even though those certainty values often look \\textit{reasonable} to a human observer, one does not know whether the uncertainties are actually predicted based on the same observations the human observer made. But without being sure about the reasons and motivations of single uncertainty estimations, a proper transfer from one data set to another, and even only a domain shift, are much harder to realize with a guaranteed performance. Regarding safety critical real life applications, the lack of explainability makes the application of the available methods significantly harder. Besides the explainability of neural networks decisions, existing methods for uncertainty quantification are not well understood on a higher level. For instance, explaining the behavior of single deterministic approaches, ensembles or Bayesian methods is a current direction of research and remains difficult to grasp in every detail . It is, however, crucial to understand how those methods operate and capture uncertainty to identify pathways for refinement, detect and characterize uncertainty, failures and important shortcomings .\n\\end{itemize}", "cites": [4671, 4600, 7839, 8633, 4595, 4668], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes insights from multiple cited papers to construct a coherent narrative about the challenges of applying uncertainty quantification methods in real-world settings. It critically evaluates the limitations of current approaches, such as lack of standardized protocols, inability to assess pointwise uncertainty, and explainability issues. The analysis generalizes these findings to highlight broader systemic issues in the field that hinder practical adoption."}}
{"id": "556a7e6a-39ab-43ff-8b98-4411e6832e1c", "title": "Outlook", "level": "subsection", "subsections": [], "parent_id": "4fde619f-e57d-42f5-974b-6ca984c28e03", "prefix_titles": [["title", "A Survey of Uncertainty in Deep Neural Networks"], ["section", "Conclusion and Outlook"], ["subsection", "Outlook"]], "content": "\\begin{itemize}\n    \\item \\textbf{Generic Evaluation Framework}\\\\\n    As already discussed above, there are still problems regarding the evaluation of uncertainty methods, as the lack of 'ground truth' uncertainties, the inability to test on single instances, and standardized benchmarking protocols, etc. To cope with such issues, the provision of an evaluation protocol containing various concrete baseline data sets and evaluation metrics that cover all types of uncertainty would undoubtedly help to boost research in uncertainty quantification. Also, the evaluation with regard to risk-averse and worst case scenarios should be considered there. This means, that uncertainty predictions with a very high predicted uncertainty should never fail, as for example for a prediction of a red or green traffic light. Such a general protocol would enable researchers to easily compare different types of methods against an established benchmark as well as on real world data sets. The adoption of such a standard evaluation protocol should be encouraged by conferences and journals.\n    \\item \\textbf{Expert \\& Systematic Comparison of Baselines}~\\\\\n    A broad and structured comparison of existing methods for uncertainty estimation on real world applications is not available yet. An evaluation on real world data is even not standard in current machine learning research papers. As a result, given a specific application, it remains unclear which method for uncertainty estimation performs best and whether the latest methods outperform older methods also on real world examples. This is also partly caused by the fact, that researchers from other domains that use uncertainty quantification methods, in general present successful applications of single approaches on a specific problem or a data set by hand. Considering this, there are several points that could be adopted for a better comparison within the different research domains. For instance, domain experts should also compare different approaches against each other and present the weaknesses of single approaches in this domain. Similarly, for a better comparison among several domains, a collection of all the works in the different real world domains could be collected and exchanged on a central platform. Such a platform might also help machine learning researchers in providing an additional source of challenges in the real world and would pave way to broadly highlight weaknesses in the current state of the art approaches. Google's repository on baselines in uncertainties in neural networks \\footnote{\\href{https://github.com/google/uncertainty-baselines}{https://github.com/google/uncertainty-baselines}} could be such a platform and a step towards achieving this goal. \n    \\item \\textbf{Uncertainty Ground Truths} \\\\\n    It remains difficult to validate existing methods due to the lack of uncertainty ground truths. An actual uncertainty ground truth on which methods can be compared in an ImageNet like manner would make the evaluation of predictions on single samples possible. To reach this, the evaluation of the data generation process and occurring sources of uncertainty, as for example the labeling process, might be investigated in more detail.\n    \\item \\textbf{Explainability and Physical Models} \\\\\n    Knowing the actual reasons for a false high certainty or a low certainty makes it much easier to engineer the methods for real life applications, which again increases the trust of people into such methods. Recently, AntorÃ¡n et al.  claimed to have published the first work on explainable uncertainty estimation. Uncertainty estimations, in general, form an important step towards explainable artificial intelligence. Explainable uncertainty estimations would give an even deeper understanding of the decision process of a neural network, which, in practical deployment of DNNs, shall incorporate the desired ability to be risk averse while staying applicable in real world (especially safety critical applications). Also, the possibility of improving explainability with physically based arguments offers great potential. While DNNs are very flexible and efficient, they do not directly embed the domain specific expert knowledge that is mostly available and can often be described by mathematical or physical models, as for example earth system science problems . Such physic guided models offer a variety of possibilities to include explicit knowledge as well as practical uncertainty representations into a deep learning framework . \n    \\end{itemize}\n\\bibliography{main_bib}\n\\end{document}", "cites": [4714, 4713, 7839], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section analytically addresses key challenges in uncertainty estimation, integrating insights from cited works to propose a structured evaluation framework, systematic baseline comparison, and the need for uncertainty ground truths. It critically evaluates the limitations of current practices and abstracts broader research needs, particularly in explainability and integration with domain knowledge."}}
