{"id": "3b626ccf-78b0-4e5d-a303-dac86144c819", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "6a75a2f0-f319-4440-b296-cb2eb0722a2e", "prefix_titles": [["title", "Translation Quality Assessment: A Brief Survey on Manual and Automatic Methods"], ["section", "Introduction"]], "content": "Machine translation (MT) research, starting from the 1950s  , has been one of the main research topics in computational linguistics (CL) and natural language processing (NLP), and has influenced and been influenced by several other language processing tasks such as parsing and language modeling. Starting from rule-based methods to example-based, and then statistical methods , to the current paradigm of neural network structures , MT quality continue to improve. However, as MT and translation quality assessment (TQA) researchers report,  MT outputs are still far from reaching human parity . MT quality assessment is thus still an important task to facilitate MT research itself, and also for downstream applications. TQA remains a challenging and difficult task because of the richness, variety, and ambiguity phenomena of natural language itself, e.g. the same concept can be expressed in different word structures and patterns in different languages, even inside one language .\nIn this work, we introduce  human judgement and evaluation (HJE) criteria that have been used in standard international shared tasks and more broadly, such as NIST , WMT , and IWSLT . We then introduce  automated TQA methods, including the automatic evaluation metrics that were proposed inside these shared tasks and beyond.\nRegarding Human Assessment (HA) methods, we categorise them into traditional and advanced sets, with the first set including intelligibility, fidelity, fluency, adequacy, and comprehension, and the second set including task-oriented, extended criteria, utilizing post-editing, segment ranking, crowd source intelligence (direct assessment), and revisiting traditional criteria. \nRegarding automated TQA methods, we classify these into three categories including simple n-gram based word surface matching, deeper linguistic feature integration such as syntax and semantics, and deep learning (DL) models, with the first two regarded as traditional and the last one regarded as advanced due to the recent appearance of DL models for NLP. We further divide each of these three categories into sub-branches, each with a different focus. Of course, this classification does not have clear boundaries. For instance some automated metrics are involved in both n-gram word surface similarity and linguistic features. This paper differs from the existing works  by introducing recent developments in MT evaluation measures, the different classifications from manual to automatic evaluation methodologies, the introduction of more recently developed quality estimation (QE) tasks, and its concise presentation of these concepts.\nWe hope that our work will shed light and offer a useful guide for both MT researchers and researchers in other relevant NLP disciplines, from the similarity and evaluation point of view, to find useful quality assessment methods, either from the manual or automated perspective, inspired from this work. This might include, for instance, natural language generation , natural language understanding ,  and automatic summarization .\nThe rest of the paper is organized as follows: Sections 2 and 3 present human assessment and automated assessment methods respectively;\nSection 4 presents some discussions and perspectives;  Section 5 summarizes our conclusions and future work.  We also list some further relevant readings in the appendices, such as evaluating methods of TQA itself, MT QE, and mathematical formulas.\\footnote{This work is based on an earlier preprint edition }", "cites": [6987, 1551, 38, 303], "cite_extract_rate": 0.1, "origin_cites_number": 40, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The introduction section primarily provides a descriptive overview of TQA methods, listing categories and briefly mentioning cited papers without deeply integrating or connecting their ideas. It lacks critical analysis of the cited works and offers only minimal abstraction by touching on classifications and broader NLP tasks, but does not identify overarching principles or trends."}}
{"id": "318fd55b-90fd-49cb-9a5c-39102a55f40f", "title": "\\emph{Semantic Similarity", "level": "subsubsection", "subsections": [], "parent_id": "d713479f-7f15-44e5-bf5f-52ad1cd69bd4", "prefix_titles": [["title", "Translation Quality Assessment: A Brief Survey on Manual and Automatic Methods"], ["section", "Automated Assessment Methods"], ["subsection", "Deeper Linguistic Features"], ["subsubsection", "\\emph{Semantic Similarity"]], "content": "}\nAs a contrast to  syntactic information, which captures  overall grammaticality or sentence structure similarity, the semantic similarity of  automatic translations and the source sentences (or references) can be measured by  employing semantic features.\nTo capture the semantic equivalence of sentences or text fragments, \\textbf{named entity} knowledge is taken from the literature on named-entity recognition, which aims to identify and classify atomic elements in a text into different entity categories . The most commonly used entity categories include the names of persons, locations, organizations and time . In the MEDAR2011 evaluation campaign, one baseline system based on Moses  utilized an Open NLP toolkit to perform named entity detection, in addition to other packages. The low performances from the perspective of named entities causes a drop in fluency and adequacy. In the quality estimation of the MT task in WMT 2012,  introduced features including named entity, in addition to discriminative word lexicon, neural networks, back off behavior   and edit distance. Experiments on individual features showed that, from the perspective of the increasing the correlation score with human judgments, the named entity feature contributed the most to the overall performance, in comparisons to the impacts of other features.\n\\textbf{Multi-word Expressions} (MWEs) set obstacles for MT models due to their complexity in presentation as well as idiomaticity . To investigate the effect of MWEs in MT evaluation (MTE), \\newcite{mwe4mte2015} focused on the \\textit{compositionality} of noun compounds. They identify the \\textbf{noun compounds} first from the system outputs and reference with Stanford parser. The matching scores of the system outputs and reference sentences are then recalculated, adding up to the Tesla metric, by considering the predicated compositionality of identified noun compound phrases. Our own recent work in this area  provides an extensive investigation into various MT errors caused by MWEs.\n\\textbf{Synonyms} are words with the same or close meanings. One of the most widely used synonym databases in the NLP literature is  WordNet , which is an English lexical database grouping English words into sets of synonyms. WordNet classifies  words mainly into four kinds of POS categories; Noun, Verb, Adjective, and Adverb, without prepositions, determiners, etc. Synonymous words or phrases are organized using the unit of synsets. Each synset is a hierarchical structure with the words at different levels according to their semantic relations. \n\\textbf{Textual entailment} is usually used as a directive relation between text fragments. If the truth of one text fragment TA follows another text fragment TB, then there is a directional relation between TA and TB (TB $\\Rightarrow$ TA). Instead of the pure logical or mathematical entailment, textual entailment in natural language processing (NLP) is usually performed with a relaxed or loose definition . For instance, according to text fragment TB, if it can be inferred that the text fragment TA is \\textit{most likely} to be true then the relationship TB $\\Rightarrow$ TA is also established. Since the relation is directive, it means that the inverse inference (TA $\\Rightarrow$ TB) is not ensured to be true . \\newcite{castillo-estrella-2012-semantic}  present a new approach for MT evaluation based on the task of ``Semantic Textual Similarity\". This problem is addressed using a textual entailment engine based on WordNet semantic features. \n\\textbf{Paraphrase} is to restate the meaning of a passage of text but utilizing other words, which can be seen as bidirectional textual entailment . Instead of the literal translation, word by word and line by line used by meta-phrases, a paraphrase represents a dynamic equivalent. Further knowledge of paraphrases from the aspect of linguistics is introduced in the works by . \\newcite{SnoverDorrSchwartzMicciulla2006} describe a new evaluation metric TER-Plus (TERp). Sequences of words in the reference are considered to be paraphrases of a sequence of words in the hypothesis if that phrase pair occurs in the TERp phrase table. \n\\textbf{Semantic roles} are employed by researchers as linguistic features in  MT evaluation. To utilize  semantic roles,  sentences are usually first shallow parsed and entity tagged. Then the semantic roles are used to specify the arguments and adjuncts that occur in both the candidate translation and reference translation. For instance, the semantic roles introduced by \\newcite{GimenezMarquez2007,GimenezMarquez2008} include causative agent, adverbial adjunct, directional adjunct, negation marker, and predication adjunct, etc. In a further development, \\newcite{LoWu2011a,LoWu2011b} presented the MEANT metric designed to capture the predicate-argument relations as structural relations in semantic frames, which are not reflected in the flat semantic role label features in the work of \\newcite{GimenezMarquez2007}. Furthermore, instead of using uniform weights, \\newcite{LoTurmuluruWu2012} weight the different types of semantic roles as empirically determined by their relative importance to the adequate preservation of meaning. Generally, semantic roles account for the semantic structure of a segment and have proved effective in assessing adequacy of translation.\n\\textbf{Language models} are also utilized by MT evaluation researchers. A statistical language model usually assigns a probability to a sequence of words by means of a probability distribution. \\newcite{GamonAueSmets2005} propose the LM-SVM, language model, and support vector machine methods investigating the possibility of evaluating MT quality and fluency in the absence of reference translations. They evaluate the performance of the system when used as a classifier for identifying highly dis-fluent and ill-formed sentences.\nGenerally, the linguistic features mentioned above, including both syntactic and semantic features, are combined in two ways, either by following a machine learning approach , or trying to combine a wide variety of metrics in a more simple and straightforward way, such as .", "cites": [8759, 3145, 8760], "cite_extract_rate": 0.13636363636363635, "origin_cites_number": 22, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual overview of semantic features used in TQA, such as named entities, MWEs, synonyms, textual entailment, paraphrase, and semantic roles. While it connects these features to their application in MT evaluation, the synthesis is limited to listing their usage across different works. There is little critical analysis or identification of broader patterns or limitations in the cited papers."}}
{"id": "97d72fde-8d63-4c85-976d-be6d9a7b7fc8", "title": "Conclusions and Future Work", "level": "section", "subsections": [], "parent_id": "6a75a2f0-f319-4440-b296-cb2eb0722a2e", "prefix_titles": [["title", "Translation Quality Assessment: A Brief Survey on Manual and Automatic Methods"], ["section", "Conclusions and Future Work"]], "content": "In this paper we have presented a survey of the state-of-the-art in translation quality assessment methodologies from the viewpoints of both manual judgements and automated methods. This work differs from conventional MT evaluation review work by its concise structure and inclusion of some recently published work and references.\nDue to space limitations, in the main content, we focused on conventional human assessment methods and automated evaluation metrics with reliance on reference translations. However, we also list some interesting and related work in the appendices, such as the quality estimation in MT when the reference translation is not presented during the estimation, and the evaluating methodology for TQA methods themselves.\nHowever, this arrangement does not affect the overall understanding of this paper as a self contained overview. \nWe believe this work can help both MT and NLP researchers and practitioners in identifying appropriate quality assessment methods for their work. We also expect this work might shed some light on  evaluation methodologies in other NLP tasks, due to the similarities they share, such as text summarization , natural language understanding , \nnatural language generation , as well as programming language (code) generation .\n\\section*{Acknowledgments}\nWe appreciate the comments from Derek F. Wong, editing help from Ying Shi (Angela), and the anonymous reviewers for their valuable reviews and feedback.\nThe ADAPT Centre for Digital Content Technology is funded under the SFI Research Centres Programme (Grant 13/RC/2106) and is co-funded under the European Regional Development Fund. The input of Alan Smeaton is part-funded by Science Foundation Ireland \nunder grant number SFI/12/RC/2289 (Insight Centre). \n\\bibliographystyle{acl_natbib}\n\\bibliography{nodalida2021}\n\\section*{Appendices}\n\\input{sec-Evaluate-TQA}\n\\subsection*{Appendix B: MT QE}\nIn past years, some MT evaluation methods that do not use manually created gold reference translations were proposed. These are referred to as ``Quality Estimation (QE)\". Some of the related works have already been introduced in previous sections. \nThe most recent quality estimation tasks can be found at WMT12 to WMT20  . These defined a novel evaluation metric that provides some advantages over the traditional ranking metrics. The DeltaAvg metric assumes that the reference test set has a number associated with each entry that represents its extrinsic value. Given these values, their metric does not need an explicit reference ranking, the way that Spearman ranking correlation does. The goal of the DeltaAvg metric is to measure how valuable a proposed ranking is according to the extrinsic values associated with the test entries.\n\\begin{equation} \nDeltaAvg_v[n]= \\frac{\\sum\\limits_{k=1}^{n-1}V(S_{1,k})}{n-1}-V(S)\n\\end{equation}\nFor scoring, two task evaluation metrics were used that have traditionally been used for measuring performance in regression tasks: Mean Absolute Error (MAE) as a primary metric, and Root of Mean Squared Error (RMSE) as a secondary metric. For a given test set S with entries $s_{i},1\\leqslant{i}\\leqslant|S|$, $H(s_i)$ is the proposed score for entry $s_i$ (hypothesis), and $V(s_i)$ is the reference value for entry $s_i$ (gold-standard value).\n\\begin{eqnarray} \n\\text{MAE}&=&\\frac{\\sum_{i-1}^{N}|H(s_i)-V(s_i)|}{N}\\\\\n\\text{RMSE}&=&\\sqrt{\\frac{\\sum_{i-1}^{N}(H(s_i)-V(s_i))^2}{N}}\n\\end{eqnarray}\nwhere $N=|S|$. Both these metrics are non-parametric, automatic and deterministic (and therefore consistent), and extrinsically interpretable.\nSome further readings on MT QE are the comparison between MT evaluation and QE \\newcite{SPECIA2010MTE_vs_QE} and the QE framework model QuEst ;\nthe weakly supervised approaches for quality estimation and the limitations analysis of QE Supervised Systems , and unsupervised QE models ; the recent shared tasks on QE .\nIn very recent years, the two shared tasks, i.e. MT quality estimation and traditional MT evaluation metrics, have tried to integrate into each other and benefit from both knowledge. For instance, in WMT2019 shared task, there were 10 reference-less evaluation metrics which were used for the QE task, ``QE as a Metric\", as well .\n\\subsection*{Appendix C: Mathematical Formulas}\nSome mathematical formulas that are related to aforementioned metrics: \nSection 2.1.2 -  Fluency / Adequacy / Comprehension:\n\\begin{eqnarray}\n\\text{Comprehension}=\\frac{\\#\\text{Cottect}}{6} \\\\\n\\text{Fluency}=\\frac{\\frac{\\text{Judgment point}-1}{\\text{S}-1}}{\\#\\text{Sentences in passage}} \\\\\n\\text{Adequacy}=\\frac{\\frac{\\text{Judgment point}-1}{\\text{S}-1}}{\\#\\text{Fragments in passage}} \n\\end{eqnarray}\n\\noindent\nSection 3.1.1 - Editing Distance:\n\\begin{comment}\n\\end{comment}\n\\begin{eqnarray}\n\\text{WER}=\\frac{\\text{substitution+insertion+deletion}}{\\text{reference}_{\\text{length}}}.\n\\end{eqnarray}\n\\begin{small}\n\\begin{equation}\n\\text{PER}\n=1-\\frac{\\text{correct}-\\max(0,\\text{output}_{\\text{length}}-\\text{reference}_{\\text{length}})}{\\text{reference}_{\\text{length}}}.\n\\end{equation}\n\\end{small}\n\\begin{equation} \n\\text{TER}=\\frac{\\#\\text{of edit}}{\\#\\text{of average reference words}}\n\\end{equation}\n\\noindent\nSection 3.1.2 - Precision and Recall:\n\\begin{align}\n\\text{BLEU}&=\\text{BP}\\times\\exp\\sum_{n=1}^{N}\\lambda_{n}\\log{\\text{Precision}_{\\text{n}}},\\\\\n\\text{BP}&=\n\\begin{cases}\n1 & \\text{if } c>r,\\\\\ne^{1-\\frac{r}{c}} & \\text{if } c<=r.\n\\end{cases}\n\\end{align}\n\\noindent\nwhere $c$ is the total length of candidate translation, and $r$ refers to the sum of effective reference sentence length in the corpus. Bellow is from NIST metric, then F-measure, METEOR and LEPOR:\n\\begin{eqnarray} \n\\text{Info}=\\log_{2}\\big(\\frac{\\#\\text{occurrence of}~w_1,\\cdots,w_{n-1}}{\\#\\text{occurrence of}~ w_1,\\cdots,w_n}\\big)\n\\end{eqnarray}\n\\begin{eqnarray}\nF_{\\beta}=(1+\\beta^{2})\\frac{PR}{R+\\beta^{2}P}\n\\end{eqnarray}\n\\begin{align} \n\\text{Penalty}&=LP0.5 \\times ( \\frac{\\#\\text{chunks}}{\\#\\text{matched unigrams}})^3\\\\\n\\text{MEREOR}&=\\frac{10PR}{R+9P}\\times (1-\\text{Penalty})\n\\end{align}\n\\begin{align}\n\\text{LEPOR}&=LP \\times NPosPenal \\times Harmonic(\\alpha R, \\beta P)\n\\end{align}\n\\begin{gather*} \n\\text{\\textit{h}LEPOR} = {Harmonic(w_{LP}LP},\n\\\\ w_{NPosPenal}NPosPenal, w_{HPR}HPR)\n\\end{gather*}\n\\begin{gather*} \n\\text{\\textit{n}LEPOR} = LP \\times NPosPenal \\\\\n\\times exp(\\sum_{n=1}^{N}w_nlogHPR) \\end{gather*}\n\\noindent\nwhere, in our own metric LEPOR and its variations, \\textit{n}LEPOR (\\textit{n}-gram \\textit{precision} and \\textit{recall} LEPOR) and \\textit{h}LEPOR (\\textit{harmonic} LEPOR), \\textit{P} and \\textit{R} are for precision and recall, \\textit{LP} for length penalty, \\textit{NPosPenal} for n-gram position difference penalty, and \\textit{HPR} for harmonic mean of precision and recall, respectively .\n\\end{document}", "cites": [4273], "cite_extract_rate": 0.05263157894736842, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides some analytical insights by discussing the integration of QE and traditional MT evaluation, and by introducing metrics like DeltaAvg, MAE, and RMSE. However, the synthesis of the cited papers is limited, as it primarily presents formulas and brief mentions of QE frameworks without deeper integration. The critical analysis is minimal, focusing more on descriptions than on evaluating the strengths and weaknesses of the approaches."}}
