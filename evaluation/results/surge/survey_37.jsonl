{"id": "bbc69146-7807-4176-b27a-91dd4786d761", "title": "Introduction", "level": "section", "subsections": ["adc8afbe-0b38-4244-888c-6cd00af6b5d8"], "parent_id": "f7c843a6-4822-4ee2-abf6-9ebd0e0b9e9d", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Introduction"]], "content": "\\label{sec:intro}\nLanguage is situational. Every utterance fits in a specific time, place, and scenario, conveys specific characteristics of the speaker, and typically has a well-defined intent. \nFor example, someone who is uncertain is more likely to use tag questions (e.g., “This is true, isn’t it?”) than declarative sentences (e.g., “This is definitely true.”). Similarly, a professional setting is more likely to include formal statements (e.g., “Please consider taking a seat.”) as compared to an informal situation (e.g., “Come and sit!”). For artificial intelligence systems to accurately understand and generate language, it is necessary to model language with style/attribute,\\footnote{Note that we interchangeably use the terms \\textit{style} and \\textit{attribute} in this survey. \\textit{Attribute} is a broader terminology that can include content preferences, e.g., sentiment, topic, and so on. This survey uses \\textit{style} in the same broad way, following the common practice in recent papers (see Section~\\ref{sec:define}).} which goes beyond merely verbalizing the semantics in a non-stylized way.\nThe values of the attributes can be drawn from a wide range of choices depending on pragmatics, such as the extent of formality, politeness, simplicity, personality, emotion,\npartner effect (e.g., reader awareness), genre of writing (e.g., fiction or non-fiction), and so on. \nThe goal of text style transfer (TST) is  to automatically control the style attributes of text while preserving the content. TST has a wide range of applications, as outlined by  and . The style of language is crucial because it makes natural language processing more user-centered. TST has many immediate applications. For instance, one such application is intelligent bots for which users prefer distinct and consistent persona (e.g., empathetic) instead of emotionless or inconsistent persona.\nAnother application is the development of intelligent writing assistants; for example, non-expert writers often need to polish their writings to better fit their purpose, e.g., more professional, polite, objective, humorous, or other advanced writing requirements, which may take years of experience to master. Other applications include automatic text simplification (where the target style is “simple”), debiasing online text (where the target style is “objective”), fighting against offensive language (where the target style is “non-offensive”), and so on.\nTo formally define text style transfer, let us denote the target utterance as $\\bm{x}'$ and the target discourse style attribute as $a'$. TST aims to model $p(\\bm{x}'|a,\\bm{x})$, where $\\bm{x}$ is a given text carrying a source attribute value $a$. Consider the previous example of text expressed by two different extents of formality:\n\\begin{table}[ht]\n    \\begin{tabular}{llll}\n    Source sentence $\\bm{x}$: & ``\\textit{Come and sit!}'' & Source attribute $a$: & \\textit{Informal}\n    \\\\\n    Target sentence $\\bm{x}'$: & ``\\textit{Please consider taking a seat.}'' & Target attribute $a'$: & \\textit{Formal}\n    \\\\\n    \\end{tabular}\n\\end{table}\nIn this case, a TST model should be able to modify the formality and generate the formal sentence $\\bm{x}'=$``\\textit{Please consider taking a seat.}'' given the informal input $\\bm{x}=$``\\textit{Come and sit!}''. Note that the key difference of TST from another NLP task, style-conditioned language modeling, is that the latter is conditioned on only a style token, whereas TST takes as input both the target style attribute $a'$ and a source sentence $\\bm{x}$ that constrains the content.\nCrucial to the definition of style transfer is the distinction of ``style'' and ``content,'' for which there are two common practices. The first one is by linguistic definition, where non-functional linguistic features are classified into the style (e.g., formality), and the semantics are classified into the content. In contrast, the second practice is data-driven, -- given two corpora (e.g., a positive review set and a negative review set), the invariance between the two corpora is the content, whereas the variance is the style (e.g., sentiment, topic) . \nDriven by the growing needs for TST, active research in this field has emerged, from the traditional linguistic approaches, to the more recent neural network-based approaches. Traditional approaches rely on term replacement and templates. For example, early work in NLG for weather forecasts builds domain-specific templates to \\myRed{express different types of weather with different levels of uncertainty for different users }. Research that more explicitly focuses on TST starts from the frame language-based systems , and schema-based NLG systems  which generate text with pragmatic constraints such as formality under small-scale well-defined schema. Most of this earlier work required domain-specific templates, hand-featured phrase sets that express a certain attribute (e.g., friendly), and sometimes a look-up table of expressions with the same meaning but multiple different attributes .\nWith the success of deep learning in the last decade, a variety of neural methods have been recently proposed for TST. If parallel data are provided, standard sequence-to-sequence models are often directly applied~ (see Section~\\ref{sec:sup}). However, most use cases do not have parallel data, so TST on non-parallel corpora has become a prolific research area (see Section~\\ref{sec:unsup}).\nThe first line of approaches \\textit{disentangle} text into its content and attribute in the latent space, and apply generative modeling .\nThis trend was then joined by another distinctive line of approach, \\textit{prototype editing}  which extracts a sentence template and its attribute markers to generate the text. Another paradigm soon followed, i.e., \\textit{pseudo-parallel corpus construction} to train the model as if in a supervised way with the pseudo-parallel data . These three directions, (1) disentanglement, (2) prototype editing, and (3) pseudo-parallel corpus construction, are further advanced with the emergence of Transformer-based models .\nGiven the advances in TST methodologies, it now starts to expand its impact to downstream applications, such as persona-based dialog generation , stylistic summarization , stylized language modeling to imitate specific authors , online text debiasing , simile generation , and many others.\n\\begin{table}[ht]\n    \\small\n    \\caption{Overview of the survey.}\n    \\label{tab:overview}\n    \\centering\n    \\setlength\\tabcolsep{0pt}\n    \\setlength\\extrarowheight{2pt}\n    \\begin{tabular*}{\\columnwidth}{p{0.13\\columnwidth}@{\\extracolsep{\\fill}}p{0.08\\columnwidth}@{\\extracolsep{\\fill}}p{0.12\\columnwidth}@{\\extracolsep{\\fill}}p{0.21\\columnwidth}@{\\extracolsep{\\fill}}p{0.21\\columnwidth}}\n    \\toprule\n    \\multicolumn{1}{c}{\\textbf{Motivation}} & \\multicolumn{2}{c}{\\textbf{Data}} & \\multicolumn{1}{c}{\\textbf{Method}} & \\multicolumn{1}{c}{\\textbf{Extended Applications}} \\\\ \n    \\cline{1-1} \n    \\cline{2-3}\n    \\cline{4-4}\n    \\cline{5-5}\n    \\begin{minipage}[t]{1.3\\linewidth}\n    \\begingroup\n    \\fontsize{7.5pt}{9pt}\\selectfont\n    \\begin{itemize}[nosep, wide=0pt, leftmargin=*, after=\\strut]\n        \\item Artistic writing\n        \\item Communication\n        \\item Mitigating social issues\n    \\end{itemize}\n    \\endgroup\n    \\end{minipage}\n    &\n    \\begin{minipage}[t]{1.5\\linewidth}\n    \\begingroup\n    \\fontsize{7.5pt}{9pt}\\selectfont\n    \\textbf{Tasks}\n    \\begin{itemize}[nosep, wide=0pt, leftmargin=*, after=\\strut]\n        \\item Formality\n        \\item Politeness\n        \\item Gender\n        \\item Humor\n        \\item Romance\n        \\item Biasedness\n    \\end{itemize}\n    \\endgroup\n    \\end{minipage}\n    \\begin{minipage}[t]{4\\linewidth}\n    \\begingroup\n    \\fontsize{7.5pt}{9pt}\\selectfont\n    \\textbf{Key Properties}\n    \\begin{itemize}[nosep, wide=0pt, leftmargin=*, after=\\strut]\n        \\item Parallel vs. non-parallel\n        \\item Uni- vs. bi-directional\n        \\item Dataset size\n        \\item Large vs. small word overlap\n    \\end{itemize}\n    \\endgroup\n    \\end{minipage}\n    &\n    \\begin{minipage}[t]{1.5\\linewidth}\n    \\begingroup\n    \\fontsize{7.5pt}{9pt}\\selectfont\n    { \\quad } \n    \\begin{itemize}[nosep, wide=0pt, leftmargin=*, after=\\strut]\n        \\item Toxicity\n        \\item Authorship\n        \\item Simplicity\n        \\item Sentiment\n        \\item Topic\n        \\item Political slant\n    \\end{itemize}\n    \\endgroup\n    \\end{minipage}\n    &\n    \\begin{minipage}[t]{1.5\\linewidth}\n    \\begingroup\n    \\fontsize{7.5pt}{9pt}\\selectfont\n    \\textbf{On Parallel Data}\n    \\begin{itemize}[nosep, wide=0pt, leftmargin=*, after=\\strut]\n        \\item Multi-tasking\n        \\item Inference techniques\n        \\item Data augmentation\n    \\end{itemize}\n    \\endgroup\n    \\begingroup\n    \\fontsize{7.5pt}{9pt}\\selectfont\n    { \\quad }\n    \\newline\n    \\textbf{On Non-Parallel Data}\n    \\begin{itemize}[nosep, wide=0pt, leftmargin=*, after=\\strut]\n        \\item Disentanglement\n        \\item Prototype editing\n        \\item Pseudo data construction\n    \\end{itemize}\n    \\endgroup\n    \\end{minipage}\n    & \n    \\begin{minipage}[t]{1.2\\linewidth}\n    \\begingroup\n    \\fontsize{7.5pt}{9pt}\\selectfont\n    \\textbf{Helping Other NLP Tasks}\n    \\endgroup\n    \\begingroup\n    \\fontsize{7.5pt}{9pt}\\selectfont\n    \\begin{itemize}[nosep, wide=0pt, leftmargin=*, after=\\strut]\n        \\item Paraphrasing\n        \\item Data augmentation\n        \\item Adversarial robustness\n        \\item Persona-consistent dialog\n        \\item \\myRed{Anonymization}\n        \\item Summarization\n        \\item Style-specific MT\n    \\end{itemize}\n    \\endgroup\n    \\end{minipage}\n    \\\\\n    \\bottomrule\n\\end{tabular*}\n\\end{table}", "cites": [5667, 5669, 7976, 5665, 1996, 5668, 5666, 8939, 1109, 5664], "cite_extract_rate": 0.3793103448275862, "origin_cites_number": 29, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The introduction section effectively synthesizes multiple cited papers by discussing historical and modern approaches to TST, highlighting how neural methods have evolved from traditional ones. It identifies broader patterns such as the shift from template-based systems to deep learning and introduces key paradigms like disentanglement and pseudo-parallel corpus construction. The section also provides some critical insights, such as the challenge of content preservation in unsupervised approaches, though deeper evaluations are reserved for later sections."}}
{"id": "23cc9d8c-2914-4c68-81cc-b1924db5c876", "title": "\\myRed{Data-Driven Definition of Style as the Scope of this Survey.", "level": "paragraph", "subsections": [], "parent_id": "909efbe6-b7e5-4ff1-9372-9f9e1e2c4de9", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "What Is Text Style Transfer?"], ["subsection", "How to Define Style?"], ["paragraph", "\\myRed{Linguistic Definition of Style."], ["paragraph", "\\myRed{Data-Driven Definition of Style as the Scope of this Survey."]], "content": "}\\label{sec:assumptions}\nThis survey aims to provide an overview of existing neural text style transfer approaches. To be concise, we will limit the scope to the most common settings of existing literature. Specifically, most deep learning work on TST adopts a \\textbf{data-driven definition of style}, and the scope of this survey covers the styles in currently available TST datasets.\nThe data-driven definition of style is different from the linguistic or rule-based definition of style, which theoretically constrains what constitutes a style and what not, such as a style guide \\citep[e.g.,][]{american1983publication} that requires that formal text not include any contraction, e.g., ``isn't.'' \\myRed{The distinction of the two defintions of style is shown in Figure~\\ref{fig:defi}.}\n\\myRed{With the rise of deep learning methods of TST, the data-driven definition of style extends the linguistic style to a broader concept -- the general attributes in text. It regards ``style'' as the attributes that vary across datasets, as opposed to the characteristics that stay invariant . The reason is that deep learning models, which are the focus of this survey, need large corpora to learn the style from, but not all styles have well-matched large corpora. \nTherefore, apart from the very few manually-annotated\ndatasets with linguistic style definitions, such as\nformality  and humor \\& romance ,\nmany recent dataset collection works automatically look for meta-information to link a corpus to a certain attribute. A typical example is the widely used Yelp review dataset , where reviews with low ratings are put into the negative corpus, and reviews with high ratings are put into the positive corpus, although the negative vs. positive opinion is not a style that belongs to the linguistic definition, but more of a content-related attribute.}\nMost methods mentioned in this survey can be applied to scenarios that follow this data-driven definition of style. As a double-sided sword, the prerequisite for most methods is that there \\textit{exist} style-specific corpora for each style of interest, either parallel or non-parallel. Note that there can be future works that do not take such an assumption, which will be discussed in Section~\\ref{sec:loosening}.", "cites": [5668, 5667], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information by contrasting the data-driven and linguistic definitions of style, using cited papers to support its discussion of how deep learning approaches have broadened the concept of style. It provides a critical perspective by highlighting the limitations of relying on manually-annotated datasets and the pragmatic shift in defining style based on available corpora. The abstraction is evident as it generalizes the trend in TST research towards data-driven style definitions and discusses the implications for methodology and future work."}}
{"id": "e3e4546e-1b53-42c3-a94f-a4d706cf7b2d", "title": "Existing Subtasks with Datasets", "level": "subsection", "subsections": ["88034508-b7ba-41df-8201-061a92011c03"], "parent_id": "9564d037-b609-4c09-9a15-436074150f0e", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "What Is Text Style Transfer?"], ["subsection", "Existing Subtasks with Datasets"]], "content": "\\label{sec:subtasks}\n\\begin{table*}[h]\n\\small\n\\caption{List of common subtasks of TST and their corresponding attribute values and datasets. For datasets with multiple attribute-specific corpora, we report their sizes by the number of sentences of the smallest of all corpora. We also report whether the dataset is parallel (\\textit{Pa?}).}\n  \\label{tab:task_type}\n  \\centering\n  \\begingroup\n  \\fontsize{7.5pt}{9pt}\\selectfont\n    \\begin{tabular}{p{1.4cm} p{3cm} p{5.8cm} >{\\centering\\arraybackslash}p{0.6cm} >{\\centering\\arraybackslash}p{0.2cm}}\n    \\toprule\n     \\multicolumn{1}{l}{\\textbf{Task}} & \\multicolumn{1}{l}{\\textbf{Attribute Values}} & \\multicolumn{1}{l}{\\textbf{Datasets}} &  \\multicolumn{1}{c}{\\textbf{Size}} & \\multicolumn{1}{l}{\\textbf{Pa?}} \\\\\n    \\midrule\n    \\multicolumn{3}{l}{\\textbf{\\textit{Style Features}}} \\\\\n    \\multirow{2}{*}{Formality} & \\multirow{2}{*}{Informal$\\leftrightarrow$Formal} & GYAFC\\footnote{GYAFC data: \\url{https://github.com/raosudha89/GYAFC-corpus}}  & 50K & {\\cmark}\n    \\\\ \n    &  & XFORMAL\\footnote{GYAFC data: \\url{https://github.com/Elbria/xformal-FoST}}  & 1K & {\\cmark}\n    \\\\ \\hline\n    Politeness & Impolite$\\rightarrow$Polite & Politeness\\footnote{Politeness data: \\url{https://github.com/tag-and-generate/politeness-dataset}}  & 1M & \\xmark\n    \\\\ \\hline\n    Gender & Masculine$\\leftrightarrow$Feminine & Yelp Gender\\footnote{The Yelp Gender dataset is from the Yelp Challenge \\url{https://www.yelp.com/dataset} and its preprocessing needs to follow .}  & 2.5M & \\xmark\n    \\\\ \\hline\n    Humor\\&\\newline Romance & Factual$\\leftrightarrow$Humorous$\\leftrightarrow$\\newline Romantic & FlickrStyle\\footnote{FlickrStyle data: \\url{https://github.com/lijuncen/Sentiment-and-Style-Transfer/tree/master/data/imagecaption}}  & 5K & \\cmark\n    \\\\ \\hline\n    Biasedness & Biased$\\rightarrow$Neutral & Wiki Neutrality\\footnote{Wiki Neutrality data: \\url{http://bit.ly/bias-corpus}}  & 181K & \\cmark \\\\ \\hline\n    \\multirow{3}{*}{Toxicity} & \\multirow{3}{*}{Offensive$\\rightarrow$Non-offensive} & Twitter~ & 58K & \\multirow{3}{*}{\\xmark}\n    \\\\\n    & & Reddit  & 224K\n    \\\\\n    & & Reddit Politics & 350K \\\\ \\hline\n    \\multirow{2}{*}{Authorship} & Shakespearean$\\leftrightarrow$Modern & Shakespeare  & 18K & \\multirow{2}{*} {\\cmark} \\\\\n    & Different Bible translators & Bible\\footnote{Bible data: \\url{https://github.com/keithecarlson/StyleTransferBibleData}}  & 28M &\n    \\\\ \\hline\n    \\multirow{4}{*}{Simplicity} & \\multirow{4}{*}{Complicated$\\rightarrow$Simple} & PWKP  & 108K & \\cmark \\\\\n    & & Expert  & 2.2K & \\cmark \\\\\n    & & MIMIC-III\\footnote{MIMIC-III data: Request access at \\url{https://mimic.physionet.org/gettingstarted/access/} and follow the preprocessing of }  & 59K & \\xmark \\\\\n    & & MSD\\footnote{MSD data: \\url{https://srhthu.github.io/expertise-style-transfer/}}  & 114K & \\cmark \\\\ \\hline\n    \\multirow{2}{*}{Engagingness} & \\multirow{2}{*}{Plain$\\rightarrow$Attractive} & Math\\footnote{Math data: \\url{https://gitlab.cs.washington.edu/kedzior/Rewriter/}}  & $<$1K & {\\cmark} \\\\\n    & & TitleStylist\\footnote{TitleStylist data: \\url{https://github.com/jind11/TitleStylist}}  & 146K & \\xmark\n    \\\\ \\hline\n    \\multicolumn{4}{l}{\\textbf{\\textit{Content Preferences}}} \\\\\n    \\multirow{2}{*}{Sentiment} & \\multirow{2}{*}{Positive$\\leftrightarrow$Negative} & Yelp\\footnote{Yelp data: \\url{https://github.com/shentianxiao/language-style-transfer}}  & 250K & \\multirow{2}{*}{\\xmark} \\\\\n    & & Amazon\\footnote{Amazon data: \\url{https://github.com/lijuncen/Sentiment-and-Style-Transfer/tree/master/data/amazon}}  & 277K & \\\\ \\hline\n    Topic & Entertainment$\\leftrightarrow$Politics & Yahoo! Answers\\footnote{Yahoo! Answers data: \\url{https://webscope.sandbox.yahoo.com/catalog.php?datatype=l&did=11}}  & 153K & \\xmark \\\\ \\hline\n    Politics & Democratic$\\leftrightarrow$Republican & Political\\footnote{Political data: \\url{https://nlp.stanford.edu/robvoigt/rtgender/}}  & 540K & \\xmark \\\\\n    \\bottomrule\n    \\end{tabular}\n  \\endgroup\n\\end{table*}\nWe list the common subtasks and corresponding datasets for neural TST in Table~\\ref{tab:task_type}. The attributes of interest vary from style features (e.g., formality and politeness) to content preferences (e.g., sentiment and topics). Each task of which will be elaborated below.", "cites": [5669, 8752, 5674, 7976, 7978, 7977, 5670, 5668, 5672, 5666, 5671, 5673], "cite_extract_rate": 0.6, "origin_cites_number": 20, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily presents a descriptive overview of TST subtasks and their corresponding datasets, with limited synthesis of ideas across the cited works. It mentions whether datasets are parallel or not but does not connect these observations to broader methodological trends or evaluate the implications. There is little critical analysis or abstraction beyond listing factual information."}}
{"id": "88034508-b7ba-41df-8201-061a92011c03", "title": "Formality.", "level": "paragraph", "subsections": ["48782e6d-8732-4b21-8ea8-80f7d88580a5", "4e5e116c-708d-4837-98ce-85bf6127d72d", "4d190c58-3eb8-4e37-9a94-9ff782a80773", "9adfa333-613e-487a-901f-57213a2d4bf4", "6787d314-03c9-44a5-b178-d4919cf33a7c", "5e928461-320e-4180-9a80-7b8d400ea10f", "6e020cd3-0401-4c1c-b255-5c80e3f30b17", "adba2757-3ac6-4894-a006-3f567891495c", "4fab0691-06d3-41fc-8bb5-a44a3c694768", "26f5ec52-4736-440c-bcf5-87e6a1707de7", "98c340bd-32ed-403c-bde3-b82f798dda91"], "parent_id": "e3e4546e-1b53-42c3-a94f-a4d706cf7b2d", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "What Is Text Style Transfer?"], ["subsection", "Existing Subtasks with Datasets"], ["paragraph", "Formality."]], "content": "Adjusting the extent of formality in text was first proposed by . It is one of the most distinctive stylistic aspects that can be observed through many linguistic phenomena, such as more full names (e.g., ``television'') instead of abbreviations (e.g., ``TV''), and more nouns (e.g., ``solicitation'') instead of verbs (e.g., ``request''). \nThe formality dataset, Grammarly’s Yahoo Answers Formality Corpus (GYAFC) ,\ncontains 50K formal-informal pairs retrieved by first getting 50K informal sentences from the Yahoo Answers corpus, and then recruiting crowdsource workers to rewrite them in a formal way. \\myRed{ extend the formality dataset to a multilingual version with three more languages, Brazilian Portuguese, French, and Italian.}", "cites": [8752], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the formality subtask in text style transfer and introduces the GYAFC dataset. However, it lacks synthesis by not connecting this information to broader themes or methodologies in the field. There is no critical evaluation of the dataset or the cited work, and the abstraction is minimal, offering no general insights into trends or principles of formality transfer."}}
{"id": "48782e6d-8732-4b21-8ea8-80f7d88580a5", "title": "Politeness.", "level": "paragraph", "subsections": [], "parent_id": "88034508-b7ba-41df-8201-061a92011c03", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "What Is Text Style Transfer?"], ["subsection", "Existing Subtasks with Datasets"], ["paragraph", "Formality."], ["paragraph", "Politeness."]], "content": "Politeness transfer  aims to control the politeness in text. For example, ``Could you please send me the\ndata?'' is a more polite expression than ``send\nme the data!''.  compiled a dataset of 1.39 million automatically labeled instances from the raw Enron corpus . As politeness is culture-dependent, this dataset mainly focuses on politeness in North American English.", "cites": [7977], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the politeness transfer task and cites one paper, but it lacks synthesis of ideas from multiple sources and deeper analysis of the cited work. It does not compare different approaches or highlight broader patterns or limitations in the field."}}
{"id": "4e5e116c-708d-4837-98ce-85bf6127d72d", "title": "Gender.", "level": "paragraph", "subsections": [], "parent_id": "88034508-b7ba-41df-8201-061a92011c03", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "What Is Text Style Transfer?"], ["subsection", "Existing Subtasks with Datasets"], ["paragraph", "Formality."], ["paragraph", "Gender."]], "content": "Linguistic phenomena related to gender is a heated research area\n. The gender-related TST dataset is proposed by  who compiled 2.5M reviews from Yelp Dataset Challenge that are labeled with the gender of the user.", "cites": [5669], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.0, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides minimal synthesis, critical analysis, or abstraction. It only briefly mentions the existence of a gender-related TST dataset and cites one paper without elaborating on its methodology, contributions, or limitations. There is no integration with other subtasks or broader discussion of gender as a stylistic attribute in TST."}}
{"id": "4d190c58-3eb8-4e37-9a94-9ff782a80773", "title": "Humor\\&Romance.", "level": "paragraph", "subsections": [], "parent_id": "88034508-b7ba-41df-8201-061a92011c03", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "What Is Text Style Transfer?"], ["subsection", "Existing Subtasks with Datasets"], ["paragraph", "Formality."], ["paragraph", "Humor\\&Romance."]], "content": "Humor and romance are some artistic attributes that can provide readers with joy.  first propose to borrow the FlickrStyle stylized caption dataset  from the computer vision domain. In the FlickrStyle image caption dataset, each image has three captions, with a factual, a humorous, and a romantic style, respectively. By keeping only the captions of the three styles,  created a subset of the FlickrStyle dataset of 5K parallel (factual, humorous, romantic) triplets.", "cites": [5665], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the Humor & Romance subtask in text style transfer, citing only one paper and mentioning the dataset used. It lacks synthesis of ideas from multiple sources, does not offer critical evaluation of the method or dataset, and does not abstract or generalize the findings into broader patterns or principles. The narrative remains at a surface level."}}
{"id": "9adfa333-613e-487a-901f-57213a2d4bf4", "title": "Biasedness.", "level": "paragraph", "subsections": [], "parent_id": "88034508-b7ba-41df-8201-061a92011c03", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "What Is Text Style Transfer?"], ["subsection", "Existing Subtasks with Datasets"], ["paragraph", "Formality."], ["paragraph", "Biasedness."]], "content": "Wiki Neutrality Corpus  is the first corpus of biased and neutralized sentence pairs. It is collected from Wikipedia revisions that adjusted the tone of existing sentences to a more neutral voice. The types of bias in the biased corpus include framing bias, epistemological bias, and demographic bias.", "cites": [5666], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a minimal description of the Wiki Neutrality Corpus and its sources, with only a brief mention of the types of bias addressed. It lacks synthesis of ideas from the cited paper and does not compare or evaluate the methodology or limitations. There is no abstraction or generalization to broader patterns or principles in text style transfer."}}
{"id": "5e928461-320e-4180-9a80-7b8d400ea10f", "title": "Authorship.", "level": "paragraph", "subsections": [], "parent_id": "88034508-b7ba-41df-8201-061a92011c03", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "What Is Text Style Transfer?"], ["subsection", "Existing Subtasks with Datasets"], ["paragraph", "Formality."], ["paragraph", "Authorship."]], "content": "Changing the tone of the author is an artistic use of text style transfer.  created an aligned corpus of 18K pairs of Shakespearean English and its modern English translation.  collected 28M parallel data from English versions of the Bible by different translators.", "cites": [5671], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides minimal synthesis and largely describes two datasets related to authorship style transfer without contextualizing them within broader trends or comparing them. There is no critical analysis of the methods, limitations, or implications of the cited work. It lacks abstraction and fails to generalize or identify patterns beyond the specific examples."}}
{"id": "6e020cd3-0401-4c1c-b255-5c80e3f30b17", "title": "Simplicity.", "level": "paragraph", "subsections": [], "parent_id": "88034508-b7ba-41df-8201-061a92011c03", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "What Is Text Style Transfer?"], ["subsection", "Existing Subtasks with Datasets"], ["paragraph", "Formality."], ["paragraph", "Simplicity."]], "content": "Another important use of TST is to lower the language barrier for readers, such as translating\nlegalese, medical jargon, or other professional text into simple English, to avoid discrepancies between expert wordings and laymen's understanding . Common tasks include converting standard English Wikipedia into Simple Wikipedia whose dataset contains 108K samples . Another task is to simplify medical descriptions to patient-friendly text, including a dataset with 2.2K samples , another non-parallel dataset with 59K free-text discharge summaries compiled from MIMIC-III , and a more recent parallel dataset with 114K samples compiled from from the health reference Merck Manuals (MSD) where discussions on each medical topic has one version for professionals, and the other for consumers .", "cites": [5670, 5674], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides factual descriptions of datasets and tasks related to simplifying text for laypeople, but it lacks synthesis of ideas from the cited papers. It mentions parallel and non-parallel datasets without analyzing their implications or comparing the approaches from the cited works. There is minimal critical evaluation or abstraction to broader patterns or principles."}}
{"id": "adba2757-3ac6-4894-a006-3f567891495c", "title": "Sentiment.", "level": "paragraph", "subsections": [], "parent_id": "88034508-b7ba-41df-8201-061a92011c03", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "What Is Text Style Transfer?"], ["subsection", "Existing Subtasks with Datasets"], ["paragraph", "Formality."], ["paragraph", "Sentiment."]], "content": "Sentiment modification is the most popular task in previous work on TST. It aims to change the sentiment polarity in reviews, for example from a negative review to a positive review, or vice versa. There is also work on  transferring sentiments on fine-grained review ratings, e.g., 1-5 scores. Commonly used datasets include Yelp reviews , and Amazon product reviews .", "cites": [5668], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal synthesis of the cited papers, merely mentioning that sentiment modification is a common TST task and citing one relevant paper without elaborating on its contributions. It lacks critical evaluation or comparison of methods, and does not abstract beyond specific examples to highlight broader trends or principles in sentiment-based style transfer."}}
{"id": "4fab0691-06d3-41fc-8bb5-a44a3c694768", "title": "Topic.", "level": "paragraph", "subsections": [], "parent_id": "88034508-b7ba-41df-8201-061a92011c03", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "What Is Text Style Transfer?"], ["subsection", "Existing Subtasks with Datasets"], ["paragraph", "Formality."], ["paragraph", "Topic."]], "content": "There are a few works that cover topic transfer. For example,  form a two-topic corpus by compiling Yahoo! Answers under two topics, entertainment and politics, respectively. There is also a recent dataset with 21 text styles such as Sciences, Sport, Politics, and others .", "cites": [5672, 5675], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a minimal description of works related to topic transfer, merely listing the existence of datasets and approaches without connecting ideas or forming a broader narrative. There is no critical evaluation or abstraction of patterns, and the synthesis is limited to basic mention of papers without meaningful integration."}}
{"id": "26f5ec52-4736-440c-bcf5-87e6a1707de7", "title": "Political Slant.", "level": "paragraph", "subsections": [], "parent_id": "88034508-b7ba-41df-8201-061a92011c03", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "What Is Text Style Transfer?"], ["subsection", "Existing Subtasks with Datasets"], ["paragraph", "Formality."], ["paragraph", "Political Slant."]], "content": "Political slant transfer proposed by  aims to transfer the political view in text. For example, a republican's comment can be ``defund all illegal immigrants,'' while democrats are more likely to support humanistic actions towards immigrants. The political slant dataset  is collected from comments on Facebook posts of the United States Senate and House members. The dataset uses top-level comments directly responding to the posts of a democratic or republican Congressperson. There are 540K training, 4K development, and 56K test instances in the dataset.", "cites": [5669], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a basic description of the political slant subtask in text style transfer, including an example and dataset statistics, but does not integrate or synthesize multiple sources. It fails to offer critical analysis of the cited work or broader patterns in the literature, remaining at a factual and surface-level description."}}
{"id": "98c340bd-32ed-403c-bde3-b82f798dda91", "title": "Combined Attributes.", "level": "paragraph", "subsections": [], "parent_id": "88034508-b7ba-41df-8201-061a92011c03", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "What Is Text Style Transfer?"], ["subsection", "Existing Subtasks with Datasets"], ["paragraph", "Formality."], ["paragraph", "Combined Attributes."]], "content": " propose a more challenging setting of text attribute transfer -- multi-attribute transfer. For example, the source sentence can be a positive review on an Asian restaurant written by a male reviewer, and the target sentence is a negative review on an American restaurant written by a female.\nEach of their datasets has 1-3 independent categories of attributes. Their first dataset is FYelp, which is compiled from the Yelp Dataset Challenge, labeled with sentiment (positive or negative),  gender (male or female), and eatery category (American, Asian, bar, dessert, or Mexican).\nTheir second dataset, Amazon, which is based on the Amazon product review dataset , contains the following attributes: sentiment (positive or negative), and product category (book, clothing, electronics, movies, or music). Their third dataset, Social Media Content dataset, collected from internal Facebook data which is private, contains gender (male or female), age group (18-24 or 65+), and writer-annotated feeling (relaxed or annoyed).", "cites": [5665], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes datasets and their attributes without synthesizing methods or findings across the cited works. It lacks critical analysis or evaluation of the limitations and strengths of the approaches. There is minimal abstraction or generalization to broader trends or principles in multi-attribute text style transfer."}}
{"id": "45352081-ea6b-4604-826b-eeb429b50d6b", "title": "BLEU with Gold References.", "level": "paragraph", "subsections": ["1ccd9099-7667-47bb-95f4-fa726b6185da", "5885e1d6-9b13-4435-8d55-7ae27025c2c7", "147fc50b-930f-4109-91b0-1309acbe4845", "dff9d6e8-6a76-4265-88c4-f3b30c000dc7", "e4528a03-9229-491b-8545-fb5fe896dfda"], "parent_id": "c32e0277-b7ef-47ca-a851-1436fcc283ad", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "How to Evaluate Style Transfer?"], ["subsection", "Automatic Evaluation"], ["paragraph", "BLEU with Gold References."]], "content": "Similar to many text generation tasks, text style transfer also has human-written references on several datasets (e.g., Yelp, Captions, etc.), so it is common to use the BLEU score  between the gold references and model outputs. Using BLEU to evaluate TST models has been seen across pre-deep learning works  and deep learning approaches .\nThere are three problems with using BLEU between the gold references and model outputs:\n\\begin{enumerate}[align=left, \nleftmargin=*,\nlabel=Problem \\arabic*), ref= \\arabic*]\n    \\item It mainly evaluates content and simply copying the input can result in high BLEU scores \\label{list:prob1}\n    \\item BLEU is shown to have low correlation with human evaluation \\label{list:prob2}\n    \\item Some datasets do not have human-written references \\label{list:prob3} \n\\end{enumerate}\nProblem~\\ref{list:prob1}: Different from machine translation, where using BLEU only is sufficient, TST has to consider the caveat that simply copying the input sentence can achieve high BLEU scores with the gold references \\myRed{on many datasets (e.g., $\\sim$40 on Yelp, $\\sim$20 on Humor\\&Romance, $\\sim$50 for informal-to-formal style transfer, and $\\sim$30 for formal-to-informal style transfer). It is because most text rewrites have a large extent of n-gram overlap with the source sentence. In contrast, machine translation does not have this concern, because the vocabulary of its input and output are different, and copying the input sequence does not give high BLEU scores.} \nA possible fix to consider is to combine BLEU with PINC  as in paraphrasing . By using PINC and BLEU as a 2-dimensional metric, we can minimize the n-gram overlap with the source sentence but maximize the n-gram overlap with the reference sentences.\nProblem~\\ref{list:prob2}\\&\\ref{list:prob3}: Other problems include insufficient correlation of BLEU with human evaluations (e.g., $\\leq$0.30 w.r.t. human-rated grammaticality shown in  and $\\leq$0.45 w.r.t. human evaluations shown in ), and the unavailability of human-written references for some datasets (e.g., gender and political datasets , and the politeness dataset ). A commonly used fix is to make the evaluation more fine-grained using three different independent aspects, namely transferred style strength, semantic preservation, and fluency, which will be detailed below.", "cites": [8940, 5665, 5676, 5669, 8939, 7977], "cite_extract_rate": 0.6, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates multiple papers to discuss the limitations and potential fixes for using BLEU in evaluating text style transfer. It synthesizes the problem of content preservation and the lack of correlation with human evaluation. However, the analysis is somewhat generic and lacks deeper comparison or a novel framework to connect the cited works cohesively."}}
{"id": "1ccd9099-7667-47bb-95f4-fa726b6185da", "title": "Transferred Style Strength.", "level": "paragraph", "subsections": [], "parent_id": "45352081-ea6b-4604-826b-eeb429b50d6b", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "How to Evaluate Style Transfer?"], ["subsection", "Automatic Evaluation"], ["paragraph", "BLEU with Gold References."], ["paragraph", "Transferred Style Strength."]], "content": "To automatically evaluate the transferred style strength, most works separately train a style classifier to distinguish the attributes .\\footnote{Note that this style classifier usually report 80+\\% or 90+\\% accuracy, and we will discuss the the problem of false positives and false negatives in last paragraph of automatic evaluation.} This classifier is used to judge whether each sample generated by the model conforms to the target attribute. The transferred style strength is calculated as $\\frac{\\text{\\# test samples correctly classified}}{\\text{\\# all test samples}}$.  shows that the attribute classifier correlates well with human evaluation on some datasets (e.g., Yelp and Captions), but has almost no correlation with others (e.g., Amazon). The reason is that some product genres has a dominant number of positive or negative reviews.", "cites": [5665, 5669, 1996, 5668], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the concept of using style classifiers to evaluate transferred style strength across multiple papers, creating a coherent narrative around their usage. It provides a critical note by mentioning the high accuracy of these classifiers and alludes to potential issues like false positives and negatives. While it identifies a pattern of correlation with human evaluation on certain datasets, the abstraction and deeper critical analysis remain limited, focusing more on summarizing and highlighting dataset-specific challenges."}}
{"id": "5885e1d6-9b13-4435-8d55-7ae27025c2c7", "title": "Semantic Preservation.", "level": "paragraph", "subsections": [], "parent_id": "45352081-ea6b-4604-826b-eeb429b50d6b", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "How to Evaluate Style Transfer?"], ["subsection", "Automatic Evaluation"], ["paragraph", "BLEU with Gold References."], ["paragraph", "Semantic Preservation."]], "content": "Many metrics can be applied to measure the similarity between the input and output sentence pairs, including BLEU~, ROUGE~, METEOR~, chrF~, Word Mover Distance (WMD)~. Recently, some additional deep-learning-based metrics are proposed, such as cosine similarity based on sentence embeddings~, and BERTScore~. There are also evaluation metrics that are specific for TST such as the Part-of-Speech distance~. Another newly proposed metric is to first delete all attribute-related expressions in the text, and then apply the above similarity evaluations .\nAmong all the metrics,  showed that METEOR and WMD have better correlation with human evaluation than BLEU, although, in practice, BLEU is the most widely used metric to evaluate the semantic similarity between the source sentence and style-transferred output~.", "cites": [5677, 8940, 7977], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section lists several evaluation metrics used in text style transfer but does not deeply synthesize the insights or connect the cited papers to a broader narrative. It includes a brief comparison (e.g., METEOR and WMD vs. BLEU) but lacks critical evaluation of their strengths or limitations. The abstraction is minimal, focusing largely on individual metrics rather than identifying overarching principles or trends."}}
{"id": "147fc50b-930f-4109-91b0-1309acbe4845", "title": "Fluency.", "level": "paragraph", "subsections": [], "parent_id": "45352081-ea6b-4604-826b-eeb429b50d6b", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "How to Evaluate Style Transfer?"], ["subsection", "Automatic Evaluation"], ["paragraph", "BLEU with Gold References."], ["paragraph", "Fluency."]], "content": "Fluency is a basic requirement for natural language outputs. To automate this evaluation, perplexity is calculated via a language model (LM) pretrained on the training data of all attributes~. However, the effectiveness of perplexity remains debateable, as  showed its high correlation with human ratings of fluency, whereas  suggested no significant\ncorrelation between perplexity and human scores. We note that perplexity by LM can suffer from the following undesired properties:\n\\begin{enumerate}\n    \\item Biased towards shorter sentences than longer sentences.\n    \\item For the same meaning, less frequent words will have worse perplexity (e.g., agreeable) than frequent words (e.g., good).\n    \\item A sentence's own perplexity will change if the sentence prior to it changes. \n    \\item LMs are not good enough yet.\n    \\item LMs do not necessarily handle well the domain shift between their training corpus and the style-transferred text.\n    \\myRed{\\item Perplexity scores produced by LMs are sensitive to the training corpora, LM architecture and configuration, as well as optimization configuration. Therefore, different models' outputs must be evaluated by exactly the same LM for fair comparison, which adds more difficulty to benchmarking. }\n\\end{enumerate}\nSuch properties will bias against certain models, which is not desired for an evaluation metric. \nAs a potential remedy, future researchers can try grammaticality checker to score the generated text.", "cites": [5677], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the use of perplexity as a fluency metric in text style transfer, citing relevant work and pointing out its limitations. While it integrates ideas from at least one cited paper to form a critique, the synthesis is limited and does not build a novel framework. It identifies several problematic properties of perplexity and suggests a potential remedy, showing some critical depth but not a comprehensive comparative analysis."}}
{"id": "dff9d6e8-6a76-4265-88c4-f3b30c000dc7", "title": "Task-Specific Criteria.", "level": "paragraph", "subsections": [], "parent_id": "45352081-ea6b-4604-826b-eeb429b50d6b", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "How to Evaluate Style Transfer?"], ["subsection", "Automatic Evaluation"], ["paragraph", "BLEU with Gold References."], ["paragraph", "Task-Specific Criteria."]], "content": "As TST can serve as a component for other downstream applications, some task-specific criteria are also proposed to evaluate the quality of generated text. For example,  evaluated the effect of their tailored text on reducing smokers' intent to smoke through clinical trials.  applied TST to generate eye-catchy headlines so they have an attractive score, and future works in this direction can also test the click-through rates.  evaluated how the generated text as augmented data can improve the downstream attribute classification accuracy.", "cites": [1996, 7976], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions a few task-specific evaluation methods from cited works but does not synthesize or connect these ideas into a broader framework. It lacks critical evaluation of the methods or their limitations and offers minimal abstraction beyond the examples provided. The content remains largely descriptive of individual applications without deeper analysis."}}
{"id": "e4528a03-9229-491b-8545-fb5fe896dfda", "title": "Tips for Automatic Metrics.", "level": "paragraph", "subsections": [], "parent_id": "45352081-ea6b-4604-826b-eeb429b50d6b", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "How to Evaluate Style Transfer?"], ["subsection", "Automatic Evaluation"], ["paragraph", "BLEU with Gold References."], ["paragraph", "Tips for Automatic Metrics."]], "content": "For the evaluation metrics that rely on the pretrained models, namely the style classifier and LM, we need to beware of the following:\n\\begin{enumerate}\n    \\item The pretrained models for automatic evaluation should be separate from the proposed TST model\n    \\item Machine learning models can be imperfect, so we should be aware of the potential false positives and false negatives\n    \\item The pretrained models are imperfect in the sense that they will favor towards a certain type of methods\n\\end{enumerate}\nFor the first point, it is important to not use the same style classifier or LM in the proposed TST approach, otherwise it can overfit or hack the metrics.\nFor the second point, we need to understand what can be the false positives and false negatives of the generated outputs. An illustrative example is that if the style classifier only reports 80+\\% performance (e.g., on the gender dataset  and Amazon dataset ), even perfect style rewrites can only score 80+\\%, but maybe an imperfect model can score 90\\% because it can resemble the imperfect style classification model more and makes advantage of the \\textit{false positives}.\nOther reasons for false positives can be adversarial attacks.  showed that merely paraphrasing by synonyms can drop the performance of high-accuracy classification models from TextCNN  to BERT  by 90+\\%. Therefore, higher scores by the style classifier does not necessarily indicate more successful transfer.\nMoreover, the style classifier can produce \\textit{false negatives} if there is a distribution shift between the training data and style-transferred outputs. For example, in the training corpus, a product may appear often with the \\textit{positive} attribute, and in the style-transferred outputs, this product co-occurs with the opposite, \\textit{negative} attribute. Such false negatives are observed on the Amazon product review dataset . On the other hand, the biases of the LM correlate with sentence length, synonym replacement, and prior context. \nThe third point is a direct result implied by the second point, so in practice, we need to keep in mind and check whether the proposed model takes advantage of the evaluation metrics or makes improvements that are generalizable.", "cites": [1159, 5665, 5669, 7], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section provides analytical insights by critically examining the use of automatic evaluation metrics in text style transfer. It synthesizes ideas from multiple cited works to highlight limitations and biases of style classifiers and language models. The discussion abstracts from specific papers to broader concerns like overfitting, adversarial effects, and generalizability of evaluation methodologies."}}
{"id": "9af8395a-76e3-42b5-91e3-a86f3f0122d3", "title": "Human Evaluation", "level": "subsection", "subsections": ["65b4662c-ef51-43d0-8365-e1a25d80177b"], "parent_id": "26300ce8-5fa2-4dc5-a127-885725f5f853", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "How to Evaluate Style Transfer?"], ["subsection", "Human Evaluation"]], "content": "Compared to the pros and cons of the automatic evaluation metrics mentioned above, human evaluation stands out for its flexibility and comprehensiveness. For example, when asking humans to evaluate the fluency, we do not need to worry for the bias towards shorter sentences as in the LM. We can also design criteria that are not computationally easy such as comparing and ranking the outputs of multiple models. There are several ways to conduct human evaluation. In terms of evaluation types, there are pointwise scoring, namely asking humans to provide absolute scores of the model outputs, and pairwise comparison, namely asking humans to judge which of the two outputs is better, or providing a ranking for multiple outputs.\nIn terms of the criteria, humans can provide overall evaluation, or separate scores for transferred style strength, semantic preservation, and fluency.\nHowever, the well-known limitations of human evaluation are cost and  irreproducibility.\nPerforming human evaluations can be time consuming, which may result in significant time and financial costs. Moreover, the human evaluation results in two studies are often not directly comparable, because human evaluation results tend to be subjective and not easily irreproducible . Moreover, some styles are very difficult to evaluate without expertise and extensive reading experience.\nAs a remedy, we encourage future researchers to report inter-rater agreement scores such as the Cohen's kappa  and Krippendorff's alpha . \\myRed{ also recommends to standardize and describe evaluation protocols (e.g., linguistic background of the annotators, compensation, detailed annotation instructions for each evaluation aspect), and release annotations.}", "cites": [5672, 8752], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of human evaluation in text style transfer, highlighting its strengths and limitations. It synthesizes general perspectives on human evaluation rather than deeply integrating insights from the cited papers. The critical analysis addresses well-known issues like cost, subjectivity, and irreproducibility, while also suggesting remedies like inter-rater agreement and standardized protocols. However, it does not extensively generalize or present novel frameworks, keeping the abstraction level moderate."}}
{"id": "65b4662c-ef51-43d0-8365-e1a25d80177b", "title": "Tips for Human Evaluation.", "level": "paragraph", "subsections": [], "parent_id": "9af8395a-76e3-42b5-91e3-a86f3f0122d3", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "How to Evaluate Style Transfer?"], ["subsection", "Human Evaluation"], ["paragraph", "Tips for Human Evaluation."]], "content": "As common practice, most works use 100 outputs for each style transfer direction (e.g., 100 outputs for formal$\\rightarrow$informal, and 100 outputs for informal$\\rightarrow$formal), and two human annotators for each task~.", "cites": [5665, 5668], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a very brief, descriptive statement about common human evaluation practices in the field but does not synthesize information from the cited papers. It lacks critical analysis of the methods or limitations and fails to generalize or abstract broader patterns or principles from the referenced works."}}
{"id": "d2fa079e-140d-4dea-b5f9-65ae89b6a576", "title": "Suggested Evaluation Settings for Future Work", "level": "subsection", "subsections": [], "parent_id": "26300ce8-5fa2-4dc5-a127-885725f5f853", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "How to Evaluate Style Transfer?"], ["subsection", "Suggested Evaluation Settings for Future Work"]], "content": "Currently, the experiments of various TST works do not adopt the same setting, making it difficult to do head-to-head comparison among the empirical results of multiple works. Although it is reasonable to customize the experimental settings according to the needs of a certain work, it is suggested to at least use the standard setting in at least one of the many reported experiments, to make it easy to compare with previous and future works. For example, at least (1) experiment on at least one commonly used dataset, (2) list up-to-date best-performing previous models as baselines, (3) report on a superset of the most commonly used metrics, and (4) release system outputs.\nFor (1), we suggest that future works use at least one of the most commonly used benchmark datasets, such as the Yelp data prepreocessed by  and its five human references provided by , Amazon data preprocessed by , and formality data provided by .\nFor (2), we suggest that future works actively check the latest style transfer papers curated at \\url{https://github.com/fuzhenxin/Style-Transfer-in-Text} and our repository \\url{https://github.com/zhijing-jin/Text_Style_Transfer_Survey}, and compare with the state-of-the-art performances instead of older ones. We also call for more reproducibility in the community, including source codes and evaluation codes, because, for example, there are several different scripts to evaluate the BLEU scores.\nFor (3), since no single evaluation metric is perfect and comprehensive enough for TST, it is strongly suggested to use both human and automatic evaluation on three criteria. In evaluation, apart from customized use of metrics, we suggest that most future works to include at least the following evaluation practices:\n\\begin{itemize}[nolistsep]\n    \\item Human evaluation: rate at least two state-of-the-art models according to the curated paper lists\n    \\item Automatic evaluation: at least report the BLEU score with all available references if there exist human-written references (e.g., the five references for the Yelp dataset provided by ), and BLEU with the input only when there are no human-written references.\n\\end{itemize}\n\\myRed{For (4), it will also be very helpful to provide system outputs for each TST paper, so that future works can better reproduce both human and automatic evaluation results. Note that releasing system outputs can help future works' comparison of automatic evaluation results, because there can be different scripts to evaluate the BLEU scores, as well as different style classifiers and LM. It will be a great addition to the TST community if future work can establish an online leaderboard, let existing works upload their output files, and automatically evaluate the model outputs using a standard set of automatic evaluation scripts.}", "cites": [5665, 5668, 8939], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a structured and analytical discussion on evaluation practices in TST, drawing from the cited papers to highlight common challenges and propose unified standards. It synthesizes the need for standardized datasets, metrics, and reproducibility across multiple works. The call for system output sharing and leaderboard creation reflects a forward-looking, community-driven abstraction of the field's needs."}}
{"id": "3a4496e9-393e-48c2-92bf-c412c0a4d926", "title": "Methods on Parallel Data", "level": "section", "subsections": ["9cb69da4-1cb4-4b5a-8faa-0c6110f08429"], "parent_id": "f7c843a6-4822-4ee2-abf6-9ebd0e0b9e9d", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Methods on Parallel Data"]], "content": "\\label{sec:sup}\nOver the last several years, various methods have been proposed for text style transfer. In general, they can be categorized based on whether the dataset has parallel text with different styles or several non-parallel mono-style corpora. The rightmost column ``Pa?'' in Table~\\ref{tab:task_type} shows whether there exist parallel data for each TST subtask. \nIn this section, we will cover TST methods on parallel datasets, and in Section~\\ref{sec:unsup} we will detail the approaches on non-parallel datasets.\nTo ease the understanding for the readers, we will in most cases explain TST on one attribute between two values, such as transferring the formality between informal and formal tones, which can potentially be extended to multiple attributes.\nMost methods adopt the standard neural sequence-to-sequence (seq2seq) model with the encoder-decoder architecture, which was initially developed for neural machine translation (NMT)~ and extensively seen on text generation tasks such as summarization  and many others . The encoder-decoder seq2seq model can be implemented by either LSTM as in ~ or Transformer~ as in~. Copy mechanism~ is also added to better handle stretches of text which should not be changed (e.g., some proper nouns and rare words)~. Based on this architecture, recent works have developed multiple directions of improvement: \\textbf{multi-tasking}, \\textbf{inference techniques}, and \\textbf{data augmentation}, which will be introduced below.", "cites": [8941, 2401, 303, 5678, 5679, 168, 461, 7096, 7186, 38], "cite_extract_rate": 0.8461538461538461, "origin_cites_number": 13, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic overview of methods using parallel data for text style transfer, primarily describing common architectural approaches such as encoder-decoder and attention mechanisms. While it mentions some innovations like copy mechanisms and multi-tasking, it lacks in-depth integration of ideas across papers and does not critically evaluate their strengths or weaknesses. The generalization is minimal, focusing more on specific model types than on broader patterns or theoretical implications."}}
{"id": "9cb69da4-1cb4-4b5a-8faa-0c6110f08429", "title": "Multi-Tasking.", "level": "paragraph", "subsections": ["4c9e86fc-eded-4133-abb9-9aa96c1bf569"], "parent_id": "3a4496e9-393e-48c2-92bf-c412c0a4d926", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Methods on Parallel Data"], ["paragraph", "Multi-Tasking."]], "content": "In addition to the seq2seq learning on paired attributed-text,  propose adding three other loss functions: (1) classifier-guided loss, which is calculated using a well-trained attribute classifier and encourages the model to generate sentences conforming to the target attribute, (2) self-reconstruction loss, which encourages the seq2seq model to reconstruct the input itself by specifying the desired style the same as the input style, and (3) cycle loss, which first transfers the input sentence to the target attribute and then transfers the output back to its original attribute. Each of the three losses can gain performance improvement of 1-5 BLEU points with the human references . Another type of multi-tasking is to jointly learn TST and machine translation from French to English, which improves the performance by 1 BLEU score with human-written references . \nSpecific for formality transfer,  multi-task TST and grammar error correction (GEC) so that knowledge from GEC data can be transferred to the informal-to-formal style transfer task. \nApart from the additional loss designs, using the pretrained language model GPT-2~ can lead to improvement by at least 7 BLEU scores with human references .", "cites": [8941], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of methods involving multi-tasking in text style transfer using parallel data, but it primarily describes different loss functions and applications without deeper connections or a unifying framework. There is minimal critical analysis of the cited works, and no broader abstraction or identification of overarching principles. The content remains at the level of summarizing specific techniques and their performance gains."}}
{"id": "4c9e86fc-eded-4133-abb9-9aa96c1bf569", "title": "Inference Techniques.", "level": "paragraph", "subsections": ["1f53d85b-c62d-44ff-8c51-928a3f326df6"], "parent_id": "9cb69da4-1cb4-4b5a-8faa-0c6110f08429", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Methods on Parallel Data"], ["paragraph", "Multi-Tasking."], ["paragraph", "Inference Techniques."]], "content": "To avoid the model copying too many parts of the input sentence and not performing sufficient edits to flip the attribute,  first identify words in the source sentence requiring replacement, and then change the words by negative lexically constrained decoding~ that avoids naive copying. Since this method only changes the beam search process for model inference, it can be applied to any text style transfer model without model re-training.", "cites": [4988, 1632], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a very brief and minimal description of one inference technique without clearly linking it to the cited papers. It does not synthesize or integrate the ideas from the two cited works, nor does it offer any critical evaluation or comparison. The abstraction level is low as it only mentions a specific method without identifying broader trends or principles."}}
{"id": "8fcbaf38-1da2-4fc3-aab5-dfa87cd8e17c", "title": "Disentanglement", "level": "subsection", "subsections": ["77c2ffdd-3a25-43d1-952b-dd6720518906", "d5f75676-be07-42a0-b8bc-b5723fff95ec", "6f3df2e5-e8a8-4593-8d2b-0cea241b111b"], "parent_id": "ce88f28e-bd59-4a88-94f3-60cfb919da0e", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Methods on Non-Parallel Data"], ["subsection", "Disentanglement"]], "content": "\\label{sec:disentanglement}\nDisentanglement-based models usually perform the following three actions:\n\\begin{itemize}\n    \\item Encode the text $\\bm{x}$ with attribute $a$ into a latent representation $\\bm{z}$ (i.e., $\\bm{x} \\rightarrow \\bm{z}$) \\label{item:disent_encode}\n    \\item Manipulate the latent representation $\\bm{z}$ to remove the source attribute (i.e., $\\bm{z} \\rightarrow \\bm{z}'$) \\label{item:disent_manipulate}\n    \\item Decode into text $\\bm{x}'$ with the target attribute $a'$ (i.e., $\\bm{z}' \\rightarrow \\bm{x}'$)\n    \\label{item:disent_decode}\n\\end{itemize}\nTo build such models, the common workflow in disentanglement papers consists of the following three steps:\n\\begin{enumerate}[align=left, leftmargin=*,label=Step \\arabic*), ref=\\arabic*]\n    \\item Select a model as the backbone for the encoder-decoder learning (Section~\\ref{sec:enc_dec}) \\label{item:select_enc_dec}\n    \\item Select a manipulation method of the latent representation (Section~\\ref{sec:manipulation}) \\label{item:select_mani}\n    \\item For the manipulation method chosen above, select (multiple) appropriate loss functions (Section~\\ref{sec:losses}) \\label{item:select_loss}\n\\end{enumerate}\nThe organization of this section starts with Section~\\ref{sec:enc_dec} which introduces the encoder-decoder training objectives that is used for Step~\\ref{item:select_enc_dec}. Next, Section~\\ref{sec:manipulation} overviews three main approaches to manipulate the latent representation for Step~\\ref{item:select_mani}, and Section~\\ref{sec:losses} goes through a plethora of training objectives for Step~\\ref{item:select_loss}.\nTable~\\ref{table:disentanglement} provides an overview of existing models and their corresponding configurations. To give a rough idea of the effectiveness of each model, we show their performance on the Yelp dataset.\n\\begin{table*}[ht]\n\\caption{Summary of existing disentanglement-based methods and the setting they adopted, with a reference of their performance on the Yelp dataset. For the settings, we include the encoder-decoder training method (Enc-Dec) in Section~\\ref{sec:enc_dec}, the disentanglement method (Disen.) in Section~\\ref{sec:manipulation}, and the loss types used to control style (Style Control) and content (Content Control) in Section~\\ref{sec:losses}. For the model performance, we report automatic evaluation scores including BLEU with the one human reference (BL-Ref) provided by , accuracy (Acc.), BLEU with the input (BL-Inp) and perplexity (PPL). $^*$ marks numbers reported by . \\myRed{Readers can refer to  for more complete performance results on Yelp.}}\n\\label{table:disentanglement}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{lcccc|cccc}\n\\toprule\n & \\multicolumn{4}{c}{\\textbf{Settings}}                     & \\multicolumn{3}{|c}{\\textbf{Performance on Yelp}} \\\\\n & \\textbf{Enc-Dec} & \\textbf{Disen.} & \\textbf{Style Control} & \\textbf{Content Control} & \\textbf{BL-Ref} & \\textbf{Acc. (\\%)} & \\textbf{BL-Inp} & \\textbf{PPL$\\downarrow$} \\\\ \\midrule\n & VAE & LRE & -- & -- & -- & -- & -- & -- \\\\\n & VAE & ACC & ACO & -- & 22.3 & 86.7 & 58.4 & -- \\\\\n & AE\\&GAN & ACC & AdvR$\\parallel$AdvO & -- & 7.8 & 73.9 & 20.7 & 72$^*$\\\\\n & AE & ACC & AdvR & -- & 12.9 & 46.9 & 40.1 & 166.5$^*$\\\\\n & AE & ACC & ACO & -- & 6.8 & 87.2 & -- & 32.8$^*$\\\\\n & GAN & ACC & AdvR & -- & -- & 73.4 & 31.2 & 29.7 \\\\\n & AE & ACC & LMO & -- & -- & 91.2 & 57.8 & 47.0\\&60.9 \\\\\n & AE & ACC & AdvO & Cycle & -- & 90.5 & -- & 133 \\\\\n & AE & ACC & AdvO & Noun & 24.9 & 92.7 & 63.3 & -- \\\\\n & VAE & LRE & -- & -- & -- & 88.3 & -- & -- \\\\\n & AE & LRS & ACR\\&AdvR & -- & -- & -- & -- & -- \\\\\n & AE\\&VAE & LRS & ACR\\&AdvR & BoW\\&AdvBoW & -- & 93.4 & -- & -- \\\\\n & VAE & LRS & ACR\\&AdvR & BoW\\&AdvBoW & -- & -- & -- & -- \\\\\n & AE & ACC & ACO & Cycle & 20.3 & 87.7 & 54.9 & 73 \\\\\n & AE & LRE & -- & -- & 24.6 & 95.4 & -- & 46.2 \\\\\n & GAN & ACC & ACO\\&AdvR & -- & -- & 95.5 & 53.3 & -- \\\\\n & VAE & LRE & -- & -- & 18.8 & 92.3 & -- & 18.3 \\\\\n & VAE & ACC & ACO & Cycle & 26.0 & 90.8 & -- & 109 \\\\\n & AE & LRE & -- & -- & -- & -- & -- & -- \\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table*}", "cites": [8940, 5669, 5680, 1996, 5677, 7976, 5665, 5668, 4964, 5681], "cite_extract_rate": 0.631578947368421, "origin_cites_number": 19, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a structured overview of disentanglement-based methods but primarily functions as a descriptive summary of steps and models. It lists configurations and performance metrics on the Yelp dataset without substantial integration of ideas across papers or deeper analysis of their relative strengths and weaknesses. The abstraction is limited to the three-step workflow, lacking broader conceptual insights or evaluation of trends."}}
{"id": "dfbe6ab4-ed7a-477c-8221-a5af8988ae13", "title": "Auto-Encoder (AE).", "level": "paragraph", "subsections": [], "parent_id": "77c2ffdd-3a25-43d1-952b-dd6720518906", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Methods on Non-Parallel Data"], ["subsection", "Disentanglement"], ["subsubsection", "Encoder-Decoder Training Method"], ["paragraph", "Auto-Encoder (AE)."]], "content": "Auto-encoding  is a commonly used method to learn the latent representation $\\bm{z}$, which first encodes the input sentence $\\bm{x}$ into a latent vector $\\bm{z}$ and then reconstructs a sentence as similar to the input sentence as possible.\nAE is used in many TST works \\cite[e.g.,][]{shen2017style,Hu2017TowardCG,fu2018style,Zhao2018AdversariallyRA,prabhumoye-etal-2018-style,yang2018unsupervised}. \nTo avoid auto-encoding from blindly copying all the elements from the input,  adopt denoising auto-encoding (DAE)  to replace AE in NLP tasks. Specifically, DAE first passes the input sentence $\\bm{x}$ through a noise model to randomly drop, shuffle, or mask some words, and then reconstructs the original sentence from this corrupted sentence. This idea is used in later TST works, e.g., . \\myRed{As pre-trained models became prevalent in recent years, the DAE training method has increased in popularity relative to its counterparts such as GAN and VAE, because pre-training over large corpora can grant models better performance in terms of semantic preservation and fluency~.}", "cites": [7976, 7979, 2981], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the role of auto-encoders and denoising auto-encoders in text style transfer, integrating ideas from multiple cited papers. It also highlights a trend in the adoption of DAE due to the rise of pre-trained models, showing some analytical depth. However, it lacks deeper critical evaluation of the methods or broader theoretical abstraction, limiting its overall insight level."}}
{"id": "42c2e9da-1ff7-44cd-a3fd-a821a4a11bae", "title": "Variational Auto-Encoder (VAE).", "level": "paragraph", "subsections": [], "parent_id": "77c2ffdd-3a25-43d1-952b-dd6720518906", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Methods on Non-Parallel Data"], ["subsection", "Disentanglement"], ["subsubsection", "Encoder-Decoder Training Method"], ["paragraph", "Variational Auto-Encoder (VAE)."]], "content": "Instead of reconstructing data based on the deterministic latent representations by AE, a variational auto-encoder (VAE)~ reconstructs data based on the sampled latent vector from its posterior, and use the regularization by Kullback–Leibler divergence. VAE is also commonly used in TST works .\nThe VAE loss is formulated as\n\\begin{equation}\n\\begin{split}\n     \\mathcal{L}_{\\mathrm{VAE}}(\\bm{\\theta}_{\\mathrm{E}},\\bm{\\theta}_{\\mathrm{G}})\n     = -\\mathbb{E}_{q_{\\mathrm{E}}(\\bm{z}|\\bm{x})}\\log p_{\\mathrm{G}}(\\bm{x}|\\bm{z}) \n     + \\lambda \\mathrm{KL}\\Big{[}q_{\\mathrm{E}}(\\bm{z}|\\bm{x})||p(\\bm{z})\\Big{]},\n\\end{split}\n\\end{equation}\nwhere $\\lambda$ is the hyper-parameter to balance the reconstruction loss and the KL term, $p(\\bm{z})$ is the prior drawn from the standard normal distribution of $\\mathcal{N}(\\bm{0}, \\bm{I})$, and $q_{\\mathrm{E}}(\\bm{z}|\\bm{x})$ is the posterior in the form of $\\mathcal{N}(\\bm{\\mu}, \\bm{\\sigma})$, where $\\bm{\\mu}$ and $\\bm{\\sigma}$ are predicted by the encoder.\n\\begin{figure*}[h]\n     \\centering\n     \\begin{subfigure}[b]{0.33\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{figs/disentanglement1}\n         \\caption{Latent Representation Editing}\n         \\label{figure:disentanglement-type-1}\n     \\end{subfigure}\n     \\hfill\n     \\begin{subfigure}[b]{0.32\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{figs/disentanglement2}\n         \\caption{Attribute Code Control}\n         \\label{figure:disentanglement-type-2}\n     \\end{subfigure}\n     \\hfill\n     \\begin{subfigure}[b]{0.33\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{figs/disentanglement3}\n         \\caption{Latent Representation Splitting}\n         \\label{figure:disentanglement-type-3}\n     \\end{subfigure}\n        \\caption{Three methods to manipulate the latent space based on disentanglement for text style transfer.}\n        \\label{figure:disentanglement-three-types}\n\\end{figure*}", "cites": [2073, 5680, 1996, 5672], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of VAEs and their use in text style transfer but lacks deeper synthesis of the cited papers. It mentions the loss function and the role of latent variables without connecting these ideas across the referenced works or evaluating their effectiveness. There is minimal critical or abstract analysis, focusing instead on surface-level explanation."}}
{"id": "b5957f5e-847d-44f7-a13c-a9ef45308044", "title": "Generative Adversarial Networks (GANs).", "level": "paragraph", "subsections": [], "parent_id": "77c2ffdd-3a25-43d1-952b-dd6720518906", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Methods on Non-Parallel Data"], ["subsection", "Disentanglement"], ["subsubsection", "Encoder-Decoder Training Method"], ["paragraph", "Generative Adversarial Networks (GANs)."]], "content": "GANs~ can also be applied to TST . The way GANs work is to first approximate the samples drawn from a true distribution $\\bm{z}$ by employing a noise sample $\\bm{s}$ and a generator function $G$ to produce $\\widehat{\\bm{z}} = G(\\bm{s})$. Next, a critic/discriminator $f_c(\\bm{z})$ is used to distinguish real data and generated samples. The critic is trained to distinguish the real samples from generated samples, and the generator is trained to fool the critic. Formally, the training process is expressed as a min-max game played among the encoder $E$, generator $G$, and the critic $f_c$:\n\\begin{equation}\n    \\max_{c}\\min_{E,G}\\mathcal{L}_{\\mathrm{GAN}}\n    = -\\mathbb{E}_{p(\\bm{z})}\\log p_{\\mathrm{G}}(\\bm{x}|\\bm{z}) +\\mathbb{E}_{p(\\bm{z})}f_c(\\bm{z})-\\mathbb{E}_{p(\\widehat{\\bm{z}})}f_c(\\widehat{\\bm{z}}).\n\\end{equation}", "cites": [5668, 4964], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of how GANs are used in text style transfer but fails to meaningfully synthesize the two cited papers. It does not connect their ideas or highlight how they contribute uniquely to non-parallel TST. There is little critical analysis or identification of broader patterns or limitations in the use of GANs for this task."}}
{"id": "d5f75676-be07-42a0-b8bc-b5723fff95ec", "title": "Latent Representation Manipulation", "level": "subsubsection", "subsections": [], "parent_id": "8fcbaf38-1da2-4fc3-aab5-dfa87cd8e17c", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Methods on Non-Parallel Data"], ["subsection", "Disentanglement"], ["subsubsection", "Latent Representation Manipulation"]], "content": "\\label{sec:manipulation}\nBased on the general encoder and decoder training method, the core element of disentanglement is the manipulation of latent representation $\\bm{z}$. Figure \\ref{figure:disentanglement-three-types} illustrates three main methods: latent representation editing, attribute code control, and latent representation splitting. In addition, the ``Disen.'' column of Table~\\ref{table:disentanglement} shows the type of latent representation manipulation for each work in disentanglement.\nThe first approach, \\textit{Latent Representation Editing} (LRE), shown in Figure~\\ref{figure:disentanglement-type-1}, is achieved by ensuring two properties of the latent representation $\\bm{z}$. The first property is that $\\bm{z}$ should be able to serve as the latent representation for auto-encoding, namely aligning $f_c(\\bm{z})$ with the input $\\bm{x}$, where $\\bm{z} \\overset{\\Delta}{=} E(\\bm{x})$.\nThe second property is that $\\bm{z}$ should be learned such that it incorporates the new attribute value of interest $a'$. To achieve this, the common practice is to first learn an attribute classifier $f_c$, e.g., a multilayer perceptron (MLP) that takes the latent representation $\\bm{z}$ as input, and then iteratively update $\\bm{z}$ within the constrained space by the first property and in the same time maximize the prediction confidence score regarding $a'$ by this attribute classifier \n~. An alternative way to achieve the second property is to multi-task by another auto-encoding task on the corpus with the attribute $a'$ and share most layers of the transformer except the query transformation and layer normalization layers .\nThe second approach, \\textit{Attribute Code Control} (ACC), as shown in Figure~\\ref{figure:disentanglement-type-2}, first enforces the latent representation $\\bm{z}$ of the sentence $\\bm{x}$ to contain all information except its attribute value $a$ via adversarial learning, and then the transferred output is decoded based on the combination of $\\bm{z}$ and a structured attribute code $\\bm{a}$ corresponding to the attribute value $a$. During the decoding process, the attribute code vector $\\bm{a}$ controls the attribute of generated text by acting as either the initial state~ or the embedding~. \nThe third approach, \\textit{Latent Representation Splitting} (LRS), as illustrated in Figure~\\ref{figure:disentanglement-type-3}, first disentangles the input text into two parts: the latent attribute representation $\\bm{a}$, and semantic representation $\\bm{z}$ that captures attribute-independent information. We then replace the source attribute $\\bm{a}$ with the target attribute $\\bm{a}'$, and the final transferred text is generated using the combination of $\\bm{z}$ and ${a}'$ .", "cites": [7976, 5680, 1996, 5668], "cite_extract_rate": 0.5, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear, factual description of three approaches to latent representation manipulation in text style transfer. It integrates concepts from multiple cited papers by grouping them into these categories, but lacks deeper comparative analysis or evaluation of their strengths and weaknesses. Some abstraction is present in identifying common methodological patterns, but the insights remain at a general level without proposing a novel framework or critical reflection on limitations."}}
{"id": "be85cca7-17e2-494f-816f-fff135d3cc82", "title": "Attribute Classifier on Outputs (ACO).", "level": "paragraph", "subsections": [], "parent_id": "6f3df2e5-e8a8-4593-8d2b-0cea241b111b", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Methods on Non-Parallel Data"], ["subsection", "Disentanglement"], ["subsubsection", "Training Objectives"], ["paragraph", "Attribute Classifier on Outputs (ACO)."]], "content": "ACO aims to make sentences generated by the generator ${G}$ carry the target attribute $a'$ according to a pre-trained attribute classifier $f_c$~. The generator $G$ takes as input the learned attribute vector $\\widehat{\\bm{a}'}$, which can be either an attribute code vector trained from scratch (as in the ACC approach) or the attribute representation disentangled from text (by the LRS approach). We denote the generation process to obtain the transferred sentence $\\widehat{\\bm{x}'}$ as $\\widehat{\\bm{x}'} \\overset{\\Delta}{=} G(E(\\bm{x}); \\bm{a}')$. Correspondingly, ACO minimizes the following learning objective:\n\\begin{equation}\n    \\mathcal{L}_{\\mathrm{ACO}}(\\bm{\\theta}_{\\mathrm{G}}, \\bm{a}')=-\\mathbb{E}_{p(\\bm{x})}\\log f_c(\\bm{x}')\n    ~.\n    \\label{eq:aco}\n\\end{equation}\nIn training, ACO can be trained in two ways: either a normal loss function trained by Gumbel-softmax distribution to approximate the discrete training~,\nor a negative reward for reinforcement learning by policy gradient training  as in .", "cites": [5682, 5672, 782, 5669, 1996], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the ACO method and references relevant papers, but it lacks substantial synthesis, critical evaluation, or abstraction. It mentions how ACO can be trained using Gumbel-softmax or reinforcement learning, but does not compare these approaches or discuss their relative strengths and weaknesses. The narrative remains at a surface level without deeper insights or broader conceptual framing."}}
{"id": "c4b02c65-4c9c-48f1-afad-e36dd6d94f2a", "title": "Attribute Classifier on Representations (ACR).", "level": "paragraph", "subsections": [], "parent_id": "6f3df2e5-e8a8-4593-8d2b-0cea241b111b", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Methods on Non-Parallel Data"], ["subsection", "Disentanglement"], ["subsubsection", "Training Objectives"], ["paragraph", "Attribute Classifier on Representations (ACR)."]], "content": "Different from the previous ACO objective whose training signal is from the the output sentence $\\widehat{\\bm{x}'}$, ACR directly enforces the disentangled attribute representation $\\bm{a}$ to be correctly classified by the attribute classifier,\nby the following objective :\n\\begin{equation}\n    \\mathcal{L}_{\\mathrm{ACR}}(\\bm{\\theta}_{\\mathrm{E}},\\bm{\\theta}_{f_c})=-\\mathbb{E}_{p(\\bm{a})}\\log f_c(\\bm{a})~.\n\\end{equation}", "cites": [5680], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a minimal description of the ACR training objective and directly cites one paper, but does not synthesize or connect it with other methods or ideas in the field. There is no critical evaluation or comparison of ACR with other objectives, and the discussion lacks abstraction or identification of broader patterns or principles in disentanglement techniques."}}
{"id": "fbc07551-66b1-44ed-92e7-08814ecfddc1", "title": "Adversarial Learning on Representations (AdvR).", "level": "paragraph", "subsections": [], "parent_id": "6f3df2e5-e8a8-4593-8d2b-0cea241b111b", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Methods on Non-Parallel Data"], ["subsection", "Disentanglement"], ["subsubsection", "Training Objectives"], ["paragraph", "Adversarial Learning on Representations (AdvR)."]], "content": "As the previous ACR explicitly requires the latent $\\bm{a}$ to be classified by $f_c$, AdvR trains from another perspective -- enforcing that no attribute-related information is contained in $\\bm{z}$ . Note that by combining ACR and AdvR, we can make attribute information captured \\textit{fully} and \\textit{exclusively} in $\\bm{a}$.\nTo achieve AdvR, the encoder $E$ is trained to generate the latent representation $\\bm{z} \\overset{\\Delta}{=} E(\\bm{x})$ so that $\\bm{z}$ cannot be discriminated by the attribute classifier $f_c$, which is expressed by the following learning objective:\n\\begin{equation}\n    \\max_{E} \\min_{f_c} \\mathcal{L}_{\\mathrm{AdvR}}(\\bm{\\theta}_{\\mathrm{E}},\\bm{\\theta}_{f_c})=-\\mathbb{E}_{p(\\bm{x})}\\log f_c(E(\\bm{x}))\n    ~.\n\\end{equation}\nSince AdvR can be imbalanced if the number of samples of each attribute value differs largely, an extension of AdvR is to treat different attribute values with equal weight :\n\\begin{equation}\n\\begin{split}\n    \\max_{E} \\min_{f_c} \\mathcal{L}_{\\mathrm{AAE}}(\\bm{\\theta}_{\\mathrm{E}},\\bm{\\theta}_{f_c})&=- \\mathbb{E}_{p(\\bm{x})}\\Big{[}\\log f_c(E(\\bm{x}))\\Big{]} \\\\\n    &- \\mathbb{E}_{p(\\bm{x}')}\\Big{[}\\log(1-f_c(E(\\bm{x}')))\\Big{]}~.\n\\end{split}\n\\label{equation:alignment-auto-encoder}\n\\end{equation}\nNote that $p(\\bm{x})$ is the distribution of sentences of one attribute, and $p(\\bm{x}')$ is the distribution of sentences of the other attribute.", "cites": [5680, 4964, 5668, 5672], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section insightfully synthesizes the key concept of adversarial learning on representations by connecting it to prior approaches like ACR. It abstracts the idea by formulating general learning objectives and explaining their purpose in disentangling latent variables. While it briefly mentions an extension to address class imbalance, it does not deeply critique or compare these approaches to others, limiting the critical depth."}}
{"id": "185fb8ff-c3a7-4169-a48f-320e6f45547e", "title": "Adversarial Learning on Outputs (AdvO).", "level": "paragraph", "subsections": [], "parent_id": "6f3df2e5-e8a8-4593-8d2b-0cea241b111b", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Methods on Non-Parallel Data"], ["subsection", "Disentanglement"], ["subsubsection", "Training Objectives"], ["paragraph", "Adversarial Learning on Outputs (AdvO)."]], "content": "Apart from AdvR that adversarially learn the latent representations, we can also use AdvO to perform adversarial training on the outputs, to make them undistinguishable from the real data . Specifically, for each attribute $a_i$, we train a classifier $f_c^{(i)}$ to distinguish between true $\\bm{x}_i$ from the mono-style corpus of attribute $a_i$, and the generated sentence $\\widehat{\\bm{x}_i} \\overset{\\Delta}{=} G(E(\\bm{x}_k); \\bm{a}_i)$, where $k \\neq i$, which aims to have the attribute $a_i$. The loss function is\n\\begin{equation}\n\\begin{split}\n    \\max_{E,G} \\min_{f_c^{(i)}} \\mathcal{L}_{\\mathrm{AdvO}}^{(i)}(\\bm{\\theta}_{\\mathrm{E}},\\bm{\\theta}_{\\mathrm{G}},\\bm{\\theta}_{f_c^{(i)}}) \n    & = -\\mathbb{E}_{p(\\bm{x}_i)}\\Big{[}\\log f_c^{(i)}(\\bm{x}_i)\\Big{]} \\\\\n    & - \\mathbb{E}_{p(\\bm{x}_k)}\\Big{[}\\log(1-f_c^{(i)}(G(E(\\bm{x}_k); a_i)))\\Big{]}\n    ~.\n\\end{split}\n\\label{equation:cross-alignment-auto-encoder}\n\\end{equation}\nIn the training process, usually we first optimize all attribute classifiers $f_c^{(i)}$, and then train the encoder, generator, and the attribute classifiers together by optimizing the sum the all AdvO training losses:\n\\begin{equation}\n    \\max_{E,G} \\sum_i^{|\\mathbb{A}|} \\min_{f_c^{(i)}} \\mathcal{L}_{\\mathrm{AdvO}}^{(i)}(\\bm{\\theta}_{\\mathrm{E}},\\bm{\\theta}_{\\mathrm{G}},\\bm{\\theta}_{f_c^{(i)}})~.\n\\end{equation}\nNote that in order to propagate the gradients, it is feasible to use the sequence of hidden states in the generator instead of discrete text for $G(E(\\bm{x}_k); a_i)$ .", "cites": [8940, 5681, 5668], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a clear description of the AdvO approach and formulates the relevant loss functions, referencing three papers to ground the explanation. However, it does not deeply synthesize the ideas or compare them across works. The discussion is largely method-centric and lacks critical evaluation or abstraction into broader principles."}}
{"id": "ac8bd751-ac75-4e30-ae47-0627164dc918", "title": "Language Modeling on Outputs (LMO).", "level": "paragraph", "subsections": [], "parent_id": "6f3df2e5-e8a8-4593-8d2b-0cea241b111b", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Methods on Non-Parallel Data"], ["subsection", "Disentanglement"], ["subsubsection", "Training Objectives"], ["paragraph", "Language Modeling on Outputs (LMO)."]], "content": "The above AdvO learns classifiers to distinguish between true samples and generated samples. Such discriminative classification can be alternatively achieved by generative language modeling, namely $\\mathrm{LM}_i$ for each mono-style corpus with the attribute $a_i$~. Specifically, the training objective for each attribute is \n\\begin{equation}\n\\begin{split}\n    \\mathcal{L}_{\\mathrm{LMO}}^{(i)}(\\bm{\\theta}_{\\mathrm{E}},\\bm{\\theta}_{\\mathrm{G}},\\bm{\\theta}_{\\mathrm{LM}_i})\n    =-\\mathbb{E}_{p(\\bm{x_i})}\\Big{[}\\log p_{\\mathrm{LM}_i}(\\bm{x}_i)\\Big{]} \n    +\\gamma \\mathbb{E}_{p(\\bm{z}_k)}\\Big{[}\\log p_{\\mathrm{LM}_i}(G(E(\\bm{x}_k); a_i))\\Big{]}~,\n\\end{split}\n\\end{equation}\nwhere $\\gamma$ is a hyperparameter to weight the two terms.\nThe total training objective sums over the losses of all attributes:\n\\begin{equation}\n    \\max_{E,G} \\sum_i^{|\\mathbb{A}|} \\min_{\\mathrm{LM}^{(i)}} \\mathcal{L}_{\\mathrm{LMO}}^{(i)}(\\bm{\\theta}_{\\mathrm{E}},\\bm{\\theta}_{\\mathrm{G}},\\bm{\\theta}_{\\mathrm{LM}_i})~.\n\\end{equation}\n\\subsubsubsection{Content-Oriented Losses} \n\\label{sec:aim2}\nThe style-oriented losses introduced above ensures the attribute information to be contained in $\\bm{a}$, but not necessarily putting constraints on the style-independent semantics $\\bm{z}$.\nTo learn the attribute-independent information fully and exclusively in $\\bm{z}$, the following \\textit{content-oriented losses} are proposed:", "cites": [5677], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the Language Modeling on Outputs (LMO) training objective and cites one paper to support it, but does not synthesize or integrate insights across multiple works. It lacks critical analysis of the method's strengths or limitations and remains focused on a specific formulation without abstracting broader patterns or principles. The narrative is minimal and primarily serves to present the technique rather than offering deep analytical or comparative insights."}}
{"id": "51164293-86b7-45f0-ac05-e9e2249ea04b", "title": "Cycle Reconstruction (Cycle).", "level": "paragraph", "subsections": [], "parent_id": "6f3df2e5-e8a8-4593-8d2b-0cea241b111b", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Methods on Non-Parallel Data"], ["subsection", "Disentanglement"], ["subsubsection", "Training Objectives"], ["paragraph", "Cycle Reconstruction (Cycle)."]], "content": "The cycle reconstruction loss\n~ first encodes a sentence $\\bm{x}$ to its latent representation $\\bm{z} \\overset{\\Delta}{=} E(\\bm{x})$, and then feed $\\bm{z}$ to the generator $G$ to obtain the generated sentence $G(\\bm{z})$. Since the alignment of the input and the generated sentence is to preserve attribute-independent semantic information, the generator can be conditioned on any attribute, namely $\\bm{a}$ or $\\bm{a}'$. The cycle loss constrains the output $\\widehat{\\bm{x}'}$ to align with the input $\\bm{x}$ (and, similarly, the output $\\widehat{\\bm{x}}$ to align with the input $\\bm{x}'$) so that the content information can be preserved:\n\\begin{equation}\n   \\mathcal{L}_{\\mathrm{Cycle}}(\\bm{\\theta}_{E},\\bm{\\theta}_{G})=- \\mathbb{E}_{p(\\bm{x})}\\Big{[}\\log p_{\\mathrm{G}}(\\bm{x}|E(\\bm{x}))\\Big{]}\n   -\\mathbb{E}_{p(\\bm{x}')}\\Big{[}\\log p_{\\mathrm{G}}(\\bm{x}'|E(\\bm{x}'))\\Big{]}\n   ~. \\label{eq:cycle_loss}\n\\end{equation}\nOne way to train the above cycle loss is by reinforcement learning as done by  who use the loss function as a negative for content preservation.", "cites": [5682, 5672, 1996, 5673, 5681], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes the concept of cycle reconstruction across multiple papers, particularly connecting it to the goal of content preservation in non-parallel text style transfer. It provides a mathematical formulation of the loss function and briefly mentions its use in reinforcement learning. However, the critical analysis is limited—there is no discussion of the method's shortcomings or comparative strengths and weaknesses. The abstraction is modest, as it generalizes the idea of cycle consistency but does not place it within a broader conceptual or theoretical framework."}}
{"id": "532346dc-e031-4d5a-882f-3c7fc64d4ad2", "title": "Bag-of-Words Overlap (BoW).", "level": "paragraph", "subsections": [], "parent_id": "6f3df2e5-e8a8-4593-8d2b-0cea241b111b", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Methods on Non-Parallel Data"], ["subsection", "Disentanglement"], ["subsubsection", "Training Objectives"], ["paragraph", "Bag-of-Words Overlap (BoW)."]], "content": "To approximately measure content preservation, bag-of-words (BoW) features are used by .\nTo focus on content information only,  exclude\nstopwords and style-specific words.\nLet us denote the vocabulary set as $\\mathbb{V}$. We first predict the distribution of BoW features $q_{\\mathrm{BoW}}(\\bm{z})$ of the latent representation $\\bm{z}$ using softmax on the $1 \\times |\\mathbb{V}|$ BoW features. We then calculate the cross entropy loss of this BoW distribution $q_{\\mathrm{BoW}}(\\bm{z})$ against the ground-truth BoW distribution $p_{\\mathrm{BoW}}(\\bm{x})$ in the input sentence $\\bm{x}$.\nThe BoW loss is formulated as follows:\n\\begin{equation}\n\\label{equation:BoW}\n    \\mathcal{L}_{\\mathrm{BoW}}(\\bm{\\theta}_{\\mathrm{E}},\\bm{\\theta}_{q_{\\mathrm{BoW}}})=-p_{\\mathrm{BoW}}(\\bm{x}) \\log q_{\\mathrm{BoW}}(\\bm{z})\n    ~.\n\\end{equation}", "cites": [5680], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section describes the use of bag-of-words overlap as a training objective for non-parallel text style transfer but does not synthesize ideas from multiple papers, nor does it critically evaluate the approach or identify broader patterns. It primarily presents a methodological definition without contextualizing it within the field or contrasting it with other techniques."}}
{"id": "b0b97f14-bfb9-4ae1-a9e9-43fb273ce6a0", "title": "Adversarial BoW Overlap (AdvBoW).", "level": "paragraph", "subsections": [], "parent_id": "6f3df2e5-e8a8-4593-8d2b-0cea241b111b", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Methods on Non-Parallel Data"], ["subsection", "Disentanglement"], ["subsubsection", "Training Objectives"], ["paragraph", "Adversarial BoW Overlap (AdvBoW)."]], "content": "BoW ensures the content to be \\textit{fully} captured in $\\bm{z}$. As a further step, we want to ensure that the content information is exclusively captured in $\\bm{z}$, namely not contained in $\\bm{a}$ at all, via the following AdvBow loss on $\\bm{a}$ .\nWhen disentangling $\\bm{z}$ and $\\bm{a}$ in the LRS framework, \nwe train an adversarial classifier $q_{\\mathrm{BoW}}(\\bm{a})$ to predict the BoW features given $\\bm{a}$ by aligning it with the ground-truth BoW distribution $p_{\\mathrm{BoW}}(\\bm{x})$, namely minimizing\n\\begin{equation}\n\\label{equation:AdvBoW}\n    \\mathcal{L}_{\\mathrm{AdvBoW}}(\\bm{\\theta}_{\\mathrm{E}},\\bm{\\theta}_{q_{\\mathrm{BoW}}})=-p_{\\mathrm{BoW}}(\\bm{x}) \\log q_{\\mathrm{BoW}}(\\bm{z})\n    ~.\n\\end{equation}\nThe final min-max objective is\n\\begin{equation}\n    \\max_{E} \\min_{q_{\\mathrm{BoW}}} \\mathcal{L}_{\\mathrm{AdvBoW}}(\\bm{\\theta}_{\\mathrm{E}},\\bm{\\theta}_{q_{\\mathrm{BoW}}}).\n\\end{equation}", "cites": [5680], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a minimal description of the AdvBoW method and its training objective, primarily paraphrasing the mathematical formulation. It cites one paper but does not integrate or connect it with other related works in the field. There is no critical evaluation or abstraction into broader concepts or trends."}}
{"id": "ae1778b4-7a5a-4e06-86a9-b6ed202db7fa", "title": "Other Losses/Rewards.", "level": "paragraph", "subsections": [], "parent_id": "6f3df2e5-e8a8-4593-8d2b-0cea241b111b", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Methods on Non-Parallel Data"], ["subsection", "Disentanglement"], ["subsubsection", "Training Objectives"], ["paragraph", "Other Losses/Rewards."]], "content": "There are also other losses/rewards in recent work such as the noun overlap loss (Noun) , as well as rewards for semantics and fluency~. We do not discuss them in much detail because they do not directly operate on the disentanglement of latent representations.", "cites": [8940, 5673, 5682, 5683], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal description of additional losses and rewards in non-parallel text style transfer but fails to synthesize or integrate the cited papers meaningfully. It lacks critical analysis and does not generalize to broader patterns or principles, merely stating that these losses are not discussed in detail because they do not directly operate on disentanglement."}}
{"id": "424e222f-f444-45ad-9493-b5b1d5c3028b", "title": "Prototype Editing", "level": "subsection", "subsections": ["d79149f2-9b32-429d-8972-cc191a7b6c32", "5b970262-ac41-45a9-a244-1b35ad99bc64", "2ef91b1b-6b3e-4ba2-af8d-334de1b73235"], "parent_id": "ce88f28e-bd59-4a88-94f3-60cfb919da0e", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Methods on Non-Parallel Data"], ["subsection", "Prototype Editing"]], "content": "\\label{sec:proto_nlg}\nDespite a plethora of models that use end-to-end training of neural networks, the prototype-based text editing approach still attracts lots of attention, since the proposal of a pipeline method called {\\it delete, retrieve}, and {\\it generate} . \nPrototype editing is reminiscent of early word replacement methods used for TST, such as synonym matching using a style dictionary , WordNet , hand-crafted rules , or using hypernyms and definitions to replace the style-carrying words .\nFeaturing more controllability and interpretability, prototype editing builds an explicit pipeline for text style transfer from $\\bm{x}$ with attribute $a$ to its counterpart $\\bm{x}'$ with attribute ${a}'$:\n\\begin{enumerate}[align=left, \nleftmargin=*,\nlabel=Step \\arabic*), ref=\\arabic*]\n    \\item Detect attribute markers of $a$ in the input sentence $\\bm{x}$, and delete them, resulting in a content-only sentence (Section~\\ref{sec:attr_marker_detect}); \\label{item:proto_marker}\n    \\item Retrieve candidate attribute markers carrying the desired attribute ${a}'$ (Section~\\ref{sec:tar_attr_retr}); \\label{item:proto_template}\n    \\item Infill the sentence by adding new attribute markers and make sure the generated sentence is fluent (Section~\\ref{sec:gen_from_proto}). \\label{item:proto_infill}\n\\end{enumerate}", "cites": [5665], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a clear analytical structure for the prototype editing approach by breaking it into three steps and linking it to earlier word replacement methods. It synthesizes the key idea from the cited paper (Delete, Retrieve, Generate) and places it in the context of previous techniques. While it introduces a conceptual framework, it does not offer a deep critical evaluation of the cited work or compare multiple approaches in detail."}}
{"id": "d79149f2-9b32-429d-8972-cc191a7b6c32", "title": "Attribute Marker Detection", "level": "subsubsection", "subsections": [], "parent_id": "424e222f-f444-45ad-9493-b5b1d5c3028b", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Methods on Non-Parallel Data"], ["subsection", "Prototype Editing"], ["subsubsection", "Attribute Marker Detection"]], "content": "\\label{sec:attr_marker_detect}\nExtracting attribute markers is a non-trivial NLP task. Traditional ways to do it involve first using tagging, parsing and morphological analysis to select features, and then filtering by mutual information and Chi-square testing. In recent deep learning pipelines,\nthere are three major types of approaches to identify attribute markers: frequency-ratio methods, attention-based methods, and fusion methods.\n{\\bf Frequency-ratio methods} calculate some statistics for each n-gram in the corpora.\nFor example,  detect the attribute markers by calculating its relative frequency of co-occurrence with attribute $a$ versus ${a}'$, and those with frequencies higher than a threshold are considered the markers of $a$. Using a similar approach,  first calculate the ratio of mean TF-IDF between the two attribute corpora for each n-gram, then normalize this ratio across all possible n-grams, and finally mark those n-grams with a normalized ratio $p$ higher than a pre-set threshold as attribute markers.\n{\\bf Attention-based methods} train an attribute classifier using the attention mechanism , and consider words with attention weights higher than average as markers .\nFor the architecture of the classifier,  use LSTM, and  use a BERT classifier, \\myRed{where the BERT classifier has shown higher detection accuracy for the attribute markers.}\n{\\bf Fusion methods} combine the advantages of the above two methods. For example,  prioritize the attribute markers predicted by frequency-ratio methods, and use attention-based methods as an auxiliary back up. One use case is when\nfrequency-ratio methods fail to identify any attribute markers in a given sentence, they will use the attention-based methods as a secondary choice to generate attribute markers.\nAnother case is to reduce false positives. To reduce the number of attribute markers that are wrongly recognized,  set a threshold to filter out\nlow-quality attribute markers by frequency-ratio methods, and in cases where all attribute markers are deleted, they use the markers predicted by attention-based methods.\nThere are still remaining limitations of the previous methods, such as\nimperfect accuracy of the attribute classifier, and unclear relation between attribute and attention scores. Hence,  propose word importance scoring, similar to what is used by  for adversarial paraphrasing, to measure how important a token is to the attribute by the difference in the attribute probability of the original sentence and that after deleting a token.", "cites": [5665, 5673, 7980, 5684, 7977, 168], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple approaches for attribute marker detection (frequency-ratio, attention-based, and fusion methods) and integrates insights from several cited papers. It provides a critical evaluation of the limitations of these methods and suggests a novel alternative (word importance scoring). The abstraction level is strong, as it generalizes patterns and proposes a higher-level understanding of how attribute markers are identified in text style transfer systems."}}
{"id": "5b970262-ac41-45a9-a244-1b35ad99bc64", "title": "Target Attribute Retriever", "level": "subsubsection", "subsections": [], "parent_id": "424e222f-f444-45ad-9493-b5b1d5c3028b", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Methods on Non-Parallel Data"], ["subsection", "Prototype Editing"], ["subsubsection", "Target Attribute Retriever"]], "content": "\\label{sec:tar_attr_retr}\nAfter deleting the attribute markers $\\mathrm{Marker}_a(\\bm{x})$ of the sentence $\\bm{x}$ with attribute $a$, we need to find a counterpart attribute marker $\\mathrm{Marker}_{a'}(\\bm{x}')$ from another sentence $\\bm{x}'$ carrying a different attribute ${a}'$. Denote the sentence template with all attribute markers deleted as $\\mathrm{Template}(\\bm{x}) \\overset{\\Delta}{=} \\bm{x} \\backslash \\mathrm{Marker}_a(\\bm{x})$. Similarly, the template of the sentence $\\bm{x}'$ is $\\mathrm{Template}(\\bm{x}') \\overset{\\Delta}{=} \\bm{x}' \\backslash \\mathrm{Marker}_{a'}(\\bm{x}')$.\nA common approach is to find the counterpart attribute marker by its context, because the templates of the original attribute and its counter attribute marker should be similar. Specifically, we first match a template $\\mathrm{Template}(\\bm{x})$ with the most similar template $\\mathrm{Template}(\\bm{x}')$ in the opposite attribute corpus, and then identify the attribute markers $\\mathrm{Marker}_a(\\bm{x})$ and $\\mathrm{Marker}_{a'}(\\bm{x}')$ as counterparts of each other.\nTo match templates with their counterparts, most previous works find the nearest neighbors by the cosine similarity of sentence embeddings. Commonly used sentence embeddings include TF-IDF as used in , averaged GloVe embedding distance used in , and Universal Sentence Encoder  used in . Apart from sentence embeddings,  use Part-of-Speech templates to match several candidates in the opposite corpus, and conduct an exhaustive search to fill parts of the candidate sentences into the masked positions of the original attribute markers.", "cites": [5665], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual description of the Target Attribute Retriever method within the Prototype Editing framework, primarily summarizing how template matching is performed using sentence embeddings and POS-based approaches. It integrates a few methods but does so in a limited way without deep comparative or critical analysis. There is minimal abstraction beyond the specific techniques mentioned, preventing the identification of broader principles or trends."}}
{"id": "2ef91b1b-6b3e-4ba2-af8d-334de1b73235", "title": "Generation from Prototypes", "level": "subsubsection", "subsections": [], "parent_id": "424e222f-f444-45ad-9493-b5b1d5c3028b", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Methods on Non-Parallel Data"], ["subsection", "Prototype Editing"], ["subsubsection", "Generation from Prototypes"]], "content": "\\label{sec:gen_from_proto}\n and  feed the content-only sentence template and new attribute markers into a pretrained language model that rearranges them into a natural sentence.\nThis infilling process can naturally be achieved by a masked language model (MLM)~. For example,  use MLM of the template conditioned on the target attribute, and this MLM is trained on an additional attribute classification loss using the model output and a fixed pre-trained attribute classifier.\nSince these generation practices are complicated,\n propose a simpler way. They skip Step 2 that explicitly retrieves attribute candidates, and, instead, directly learn a generation model that only takes attribute-masked sentences as inputs. This generation model is trained on data where the attribute-carrying sentences $\\bm{x}$ are paired with their templates $\\mathrm{Template}(\\bm{x})$.\nTraining on the pairs of $( \\mathrm{Template}(\\bm{x}), \\bm{x})$ constructed in the above way can make the model learn how to fill the masked sentence template with the target attribute $a$.", "cites": [5665, 7980, 1963, 7977], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the key idea of using masked language models for style transfer from non-parallel data, drawing on multiple papers to present a coherent narrative of the 'Prototype Editing' approach. It abstracts the general pattern of using attribute-masked templates and pre-trained models for infilling, but lacks deeper critical evaluation of the methods' limitations or trade-offs. The analysis is primarily conceptual, but not yet at the level of offering a novel framework or deep critique."}}
{"id": "742b79c3-a7af-48eb-891a-c2a4fc695060", "title": "Retrieval-Based Corpora Construction.", "level": "paragraph", "subsections": ["a954a2d2-92c0-426c-824e-dd9fec9b7b96"], "parent_id": "3dac7492-5a7e-429a-9651-bd01d7ba1692", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Methods on Non-Parallel Data"], ["subsection", "Pseudo-Parallel Corpus Construction"], ["paragraph", "Retrieval-Based Corpora Construction."]], "content": "One common way to construct pseudo-parallel data is through retrieval, namely extracting aligned sentence pairs from two mono-style corpora.  empirically observe that semantically similar sentences in the two mono-style corpora tend to be the attribute-transferred counterparts of each other. Hence, they construct the initial pseudo corpora by matching sentence pairs in the two attributed corpora according to the cosine similarity of pretrained sentence embeddings.\nFormally, for each sentence $\\bm{x}$, its pseudo counterpart $\\widehat{\\bm{x}'}$ is its most similar sentence in the other attribute corpus $\\bm{X}'$, namely $\\widehat{\\bm{x}'}=\\argmax_{\\bm{x}'\\in \\bm{X}'} \\mathrm{Similarity}(\\bm{x}, \\bm{x}')$. This approach is extended by  who use large-scale hierarchical alignment to extract pseudo-parallel style transfer pairs. \nSuch retrieval-based pseudo-parallel data construction is also useful for machine translation .", "cites": [5685, 8939], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly describes retrieval-based pseudo-parallel corpus construction by referencing two papers, but it lacks in-depth synthesis of their ideas. It provides minimal critical analysis and does not compare or contrast the approaches. The abstraction is limited, as it does not generalize beyond the specific methods described."}}
{"id": "a954a2d2-92c0-426c-824e-dd9fec9b7b96", "title": "Generation-Based Corpora Construction.", "level": "paragraph", "subsections": [], "parent_id": "742b79c3-a7af-48eb-891a-c2a4fc695060", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Methods on Non-Parallel Data"], ["subsection", "Pseudo-Parallel Corpus Construction"], ["paragraph", "Retrieval-Based Corpora Construction."], ["paragraph", "Generation-Based Corpora Construction."]], "content": "Another way is through generation, such as iterative back-translation (IBT) . IBT is a widely used method in machine translation  which adopts an iterative process to generate pseudo-parallel corpora. \nBefore starting the iterative process, IBT needs to first initialize two style transfer models, $M_{a \\rightarrow a'}$ which transfers from the attribute $a$ to the other attribute $a'$ and $M_{a' \\rightarrow a}$ which transfers from $a'$ to $a$. \nThen, in each iteration, it executes the following steps:\n\\begin{enumerate}[align=left, \nleftmargin=*,\nlabel=Step \\arabic*), ref=\\arabic*]\n    \\item Use the models to generate pseudo-parallel corpora. Specifically, $M_{a \\rightarrow a'}(\\bm{x})$ generates pseudo pairs $(\\bm{x}, \\widehat{\\bm{x}'})$  for all $\\bm{x} \\in \\bm{X}$,\n    and $M_{a' \\rightarrow a}(\\bm{x}')$ generates pairs of $(\\widehat{\\bm{x}}, \\bm{x}')$ for all $\\bm{x}' \\in \\bm{X}'$;\n    \\label{list:ibt1}\n    \\item Re-train these two style transfer models on the datasets generated by \\ref{list:ibt1}, i.e., re-train $M_{a \\rightarrow a'}(\\bm{x})$ on $(  \\widehat{\\bm{x}}, \\bm{x}')$ pairs and $M_{a' \\rightarrow a}(\\bm{x}')$ on $(\\widehat{\\bm{x}'}, \\bm{x})$ pairs.\n    \\label{list:ibt2}\n\\end{enumerate}\nFor Step~\\ref{list:ibt1}, in order to generate the initial pseudo-parallel corpora, a simple baseline is to randomly initialize the two models $M_{a \\rightarrow a'}$ and\n$M_{a' \\rightarrow a}$, and use them to translate the attribute of each sentence in $\\bm{x} \\in \\bm{X}$ and $\\bm{x}' \\in \\bm{X}'$. However, this simple initialization is subject to randomness and may not bootstrap well. Another way adopted by  borrows the idea from unsupervised machine translation  that first learns an unsupervised word-to-word translation table between attribute $a$ and $a'$, and uses it to generate an initial\npseudo-parallel corpora. Based on such initial corpora, they train initial style transfer models and bootstrap the IBT process.\nAnother model, Iterative Matching and Translation (IMaT) , does not learn the word translation table, and instead trains the initial style transfer models on a retrieval-based pseudo-parallel corpora introduced in the \\textit{retrieval-based corpora construction} above.\nFor Step~\\ref{list:ibt2}, during the iterative process, it is possible to encounter divergence, as there is no constraint to ensure that each iteration will produce better pseudo-parallel corpora than the previous iteration.\nOne way to enhance the convergence of IBT is to add additional losses. For example,  use the attribute classification loss ACO, as in Eq.~\\eqref{eq:aco}, to check whether the generated sentence by back-translation fits the desired attribute according to a pre-trained style classifier.\nAlternatively, IMaT  uses a checking mechanism instead of additional losses. At the end of each iteration, IMaT looks at all candidate pseudo-pairs of an original sentence, and uses Word Mover Distance  to select the sentence that has the desired attribute and is the closest to the original sentence.", "cites": [8868, 5005, 7877, 5669, 8939, 7876], "cite_extract_rate": 0.75, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers to explain the IBT and IMaT methods, connecting their use of pseudo-parallel data construction and iterative training. It provides critical insights by discussing potential issues such as divergence in IBT and contrasting techniques like using additional losses versus a checking mechanism. The section also abstracts some broader patterns in non-parallel data generation strategies, though it remains grounded in specific techniques rather than offering a more generalized meta-framework."}}
{"id": "da130ecc-10bb-415d-8452-5c2875b9d1e3", "title": "More Styles.", "level": "paragraph", "subsections": ["962660f0-ddaf-4bbf-b04d-5d9fb69d3776", "113f63cb-1184-42d3-9bb9-63050eec3fdd"], "parent_id": "df922ef8-ba88-4cdb-94c8-f5b65c32f735", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Research Agenda"], ["subsection", "Expanding the Scope of Styles"], ["paragraph", "More Styles."]], "content": "Extending the list of styles for TST is one popular research direction. Existing research originally focused on styles such as simplification , formality , and sentiment transfer , while the recent two years have seen a richer set of styles such as politeness  , biasedness  ,   medical text simplification  , and so on.\nSuch extension of styles is driven by the advancement of TST methods, and also various downstream needs, such as persona-based dialog generation, customized text rewriting applications, and moderation of online text.\nApart from the styles that have been researched as listed in Table~\\ref{tab:task_type}, there are also many other new styles that can be interesting to conduct new research on, including but not limited to the following:\n\\begin{itemize}\n    \\item Factual-to-empathetic transfer, to improve counseling dialogs (after the first version of this survey in 2020, we gladly found that this direction has now a preliminary exploration by );\n    \\item Non-native-to-native transfer (i.e., reformulating grammatical error correction with TST);\n    \\item Sentence disambiguation, to resolve nuance in text.\n\\end{itemize}", "cites": [5666, 7977, 5668, 5674], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates ideas from multiple papers to highlight the trend of expanding TST to new styles, linking methodological advancements to application-driven motivations. It provides a coherent narrative but lacks in-depth comparative or critical analysis of the methods. Some abstraction is present in identifying broader application areas, though the section remains focused on listing potential styles and their relevance."}}
{"id": "962660f0-ddaf-4bbf-b04d-5d9fb69d3776", "title": "More Difficult Forms of Style.", "level": "paragraph", "subsections": [], "parent_id": "da130ecc-10bb-415d-8452-5c2875b9d1e3", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Research Agenda"], ["subsection", "Expanding the Scope of Styles"], ["paragraph", "More Styles."], ["paragraph", "More Difficult Forms of Style."]], "content": "Another direction is to explore more complicated forms of styles. As covered by this survey, the early work on deep learning-based TST explores relatively simple styles, such as  verb tenses  and positive-vs-negative Yelp reviews . In these tasks, each data point is one sentence with a clear, categorized style, and the entire dataset is in the same domain. Moreover, the existing datasets can decouple style and style-independent contents relatively well.\nWe propose that TST can potentially be extended into the following settings:\n\\begin{itemize}\n    \\item Aspect-based style transfer (e.g., transferring the sentiment on one aspect but not the other aspects on aspect-based sentiment analysis data)\n    \\item Authorship transfer (which has tightly coupled style and content)\n    \\item Document-level style transfer (which includes discourse planning)\n    \\item Domain adaptive style transfer (which is preceded by )\n\\end{itemize}", "cites": [5674, 1996, 5668], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes information by identifying a trend in early TST research focusing on simple, categorized styles and suggests new, more complex settings for future work. It abstracts from specific papers to highlight broader directions in style complexity. While it mentions limitations (e.g., tight coupling of style and content in authorship transfer), it does not deeply compare or evaluate the cited works, keeping the critical analysis at a moderate level."}}
{"id": "2a40da5d-9228-4102-a717-72a6b4e1567b", "title": "Challenges for Disentanglement.", "level": "paragraph", "subsections": [], "parent_id": "a8adb54e-c778-441c-84b1-46894141518d", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Research Agenda"], ["subsection", "Improving the Methodology on Non-Parallel Data"], ["subsubsection", "Understanding the Strengths and Limitations of Existing Methods"], ["paragraph", "Challenges for Disentanglement."]], "content": "Theoretically, although disentanglement is impossible without inductive biases or other forms of supervision , disentanglement is achievable with some weak signals, such as only knowing how many factors have changed, but not which ones . \nIn practice, some big challenges for disentanglement-based methods include, for example, the difficulty to train deep text generative models such as VAEs and GANs. Also, it is not easy to represent all styles as latent code. Moreover, if targeting multiple styles, the computational complexity linearly increases with the number of styles to model.", "cites": [5686, 4423], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes theoretical and practical insights from the two cited papers to highlight the challenges of disentanglement in non-parallel text style transfer. It abstracts the key ideas of weak supervision and inductive biases into a broader discussion of methodological limitations and complexity. While it offers critical analysis of the practical difficulties, it could benefit from deeper comparative evaluation of the cited papers' approaches."}}
{"id": "ecb65d63-dd79-47dc-b0f4-d440e874c442", "title": "Understanding the Evolution from Traditional NLG to Deep Learning Methods", "level": "subsubsection", "subsections": ["2ac1230f-bbc0-40fe-8cfc-7d9995c088a6", "e37055e6-3421-4028-b9cc-9c3403b5550f", "8c61c62c-6c7d-435b-bbc9-625ec1ce4319"], "parent_id": "6f01b5ce-1b57-41cd-a513-7209559d83ad", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Research Agenda"], ["subsection", "Improving the Methodology on Non-Parallel Data"], ["subsubsection", "Understanding the Evolution from Traditional NLG to Deep Learning Methods"]], "content": "Despite the exciting methodological revolution led by deep learning recently, we are also interested in the merging point of traditional computational linguistics and the deep learning techniques . Specific to the context of TST, we will introduce the traditional NLG framework, and its impact on the current TST approaches, especially the prototype editing method.", "cites": [5687], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions the traditional NLG framework and its influence on current TST methods, citing one paper that discusses the evolution of neural networks in NLP. However, it lacks deeper synthesis of ideas, critical evaluation of the cited work, and generalization to broader trends or principles. The narrative remains superficial and does not offer a novel or insightful perspective on the evolution from traditional to deep learning methods."}}
{"id": "8c61c62c-6c7d-435b-bbc9-625ec1ce4319", "title": "Meeting Point of Traditional and New Methods.", "level": "paragraph", "subsections": [], "parent_id": "ecb65d63-dd79-47dc-b0f4-d440e874c442", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Research Agenda"], ["subsection", "Improving the Methodology on Non-Parallel Data"], ["subsubsection", "Understanding the Evolution from Traditional NLG to Deep Learning Methods"], ["paragraph", "Meeting Point of Traditional and New Methods."]], "content": "Viewing prototype-based editing as a merging point where traditional, controllable framework meets deep learning models, we can see that it takes advantage of the powerful deep learning models and the interpretable pipeline of the traditional NLG.\nThere are several advantages in merging the traditional NLG with the deep learning models. First, sentence planning-like steps make the generated contents more controllable. For example, the template of the original sentence is saved, and the counterpart attributes can also be explicitly retrieved, as a preparation for the final rewriting. Such a controllable, white-box approach can be easy to tune, debug, and improve. The accuracy of attribute marker extraction, for example, is constantly improving across literature  and different ways to extract attribute markers can be easily fused .\nSecond, sentence planning-like steps ensure the truthfulness of information. As most content words are kept and no additional information is hallucinated by the black-box neural networks, we can better ensure that the information of the attribute-transferred output is consistent with the original input.", "cites": [5665, 7980], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the two cited papers by highlighting a common methodology of merging traditional NLG with deep learning, and abstracts this into a general framework of controllability and truthfulness. It also provides a critical perspective by contrasting the benefits of such hybrid approaches against black-box neural networks, though it could offer more in-depth evaluation of limitations or trade-offs."}}
{"id": "13124705-d762-46be-8db7-c3d1b8deade9", "title": "Machine Translation.", "level": "paragraph", "subsections": [], "parent_id": "3528137f-b0b0-4b4d-a0ec-53a065859380", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Research Agenda"], ["subsection", "Improving the Methodology on Non-Parallel Data"], ["subsubsection", "Inspiration from Tasks with Similar Nature"], ["paragraph", "Machine Translation."]], "content": "The problem settings of machine translation and text style transfer share much in common: the source and target language in machine translation is analogous to the original and desired attribute, $a$ and $a'$, respectively. The major difference is that in NMT, the source and target corpora are in completely different languages, which have almost disjoint word vocabulary, whereas in text style transfer, the input and output are in the same language, and the model is usually encouraged to copy most content words from input such as the BoW loss introduced in Section~\\ref{sec:aim2}. Some TST works have been inspired by MT, such as the pseudo-parallel construction , and in the future there may be more interesting intersections.\n\\myRed{", "cites": [5685, 5669], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear analytical link between machine translation and text style transfer by identifying shared problem settings and key differences, such as language versus style transformation. It synthesizes the two cited papers by highlighting how pseudo-parallel data construction from comparable corpora has been applied to TST. However, it lacks deeper critical evaluation of the methods or limitations and offers only modest abstraction by pointing out general parallels rather than deriving broader principles."}}
{"id": "dbf18154-fa0a-4d02-a6da-aa825b573331", "title": "Data-to-Text Generation.", "level": "paragraph", "subsections": [], "parent_id": "3528137f-b0b0-4b4d-a0ec-53a065859380", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Research Agenda"], ["subsection", "Improving the Methodology on Non-Parallel Data"], ["subsubsection", "Inspiration from Tasks with Similar Nature"], ["paragraph", "Data-to-Text Generation."]], "content": "Data-to-text generation is another potential domain that can draw inspiration from and to TST. The data-to-text generation task is to generate textual descriptions from structured data such as tables , meaning representations , or Resource Description Framework (RDF) triples .\nWith the recent rise of pretrained seq2seq models for transfer learning , it is common to formulate data-to-text as a seq2seq task by serializing the structured data into a sequence . Then data-to-text generation can be seen as a special form of TST from structured information to text. This potential connection has not yet been investigated but worth exploring.}", "cites": [2345, 7533, 9, 7981, 5688, 2033], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section briefly synthesizes the data-to-text generation task and its relation to TST by referencing several papers, but the integration is limited and does not form a deep or novel narrative. It does not critically evaluate the cited works or their limitations, and while it identifies a potential connection between data-to-text and TST, it stops short of abstracting broader principles or frameworks."}}
{"id": "86cfb727-423a-4b52-adb9-85d25cf3c125", "title": "Neural Style Transfer.", "level": "paragraph", "subsections": [], "parent_id": "3528137f-b0b0-4b4d-a0ec-53a065859380", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Research Agenda"], ["subsection", "Improving the Methodology on Non-Parallel Data"], ["subsubsection", "Inspiration from Tasks with Similar Nature"], ["paragraph", "Neural Style Transfer."]], "content": "Neural style transfer first originates in image style transfer , and its disentanglement ideas inspired some early TST researcg . The difference between image style transfer and TST is that, for images, it is feasible to disentangle the explicit representation of the image texture as the gram matrix of image neural feature vectors, but for text, styles do not have such an explicit representation, but more abstract attributes. Besides this difference, many other aspects of style transfer research can have shared nature. Note that there are style transfer works across different modalities, including images , text, voice , handwriting , and videos .\nMany new advances in one style transfer field can inspire another style transfer field. For example, image style transfer has been used as a way for data augmentation  and adversarial attack , but TST has not yet been applied for such usage.", "cites": [5693, 5691, 3992, 7982, 7022, 5689, 5692, 5690, 7983, 5668], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.3, "critical": 2.3, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple style transfer works across different modalities, such as image, voice, and video, and connects them to the TST context by highlighting shared challenges and potential cross-modal inspirations. It abstracts the idea of 'disentanglement' and the lack of explicit style representation in text, offering a broader conceptual perspective. However, the critical analysis is limited to pointing out that TST has not yet been used for data augmentation or adversarial attacks, without deeper evaluation or comparison of the cited methods."}}
{"id": "6282bf57-fa85-4b47-acee-336e0417bf72", "title": "Style-Conditioned Language Modeling.", "level": "paragraph", "subsections": [], "parent_id": "3528137f-b0b0-4b4d-a0ec-53a065859380", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Research Agenda"], ["subsection", "Improving the Methodology on Non-Parallel Data"], ["subsubsection", "Inspiration from Tasks with Similar Nature"], ["paragraph", "Style-Conditioned Language Modeling."]], "content": "Different from language modeling that learns how to generate general natural language text, conditional language modeling learns how to generate text given a condition, such as some context, or a control code . Recent advances of conditional language models  also include text generation conditioned on a style token, such as positive or negative. \nPossible conditions include author style , speaker identity, persona and emotion , genre, attributes derived from text, topics, and sentiment .\nThey are currently limited to a small set of pre-defined ``condition'' tokens and can only generate from scratch a sentence, but not yet able to be conditioned on an original sentence for style rewriting. The interesting finding in this research direction is that it can make good use of a pretrained LM and just do some light-weight inference techniques to generate style-conditioned text, so perhaps such approaches can inspire future TST methods and reduce the carbon footprints of training TST models from scratch.", "cites": [1995, 3135, 1997, 5694], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from four different papers, drawing a general narrative about the use of style tokens and conditioning in language modeling. It provides a basic comparison to traditional language models and highlights a key limitation—lack of support for conditioning on an original sentence for rewriting. While it identifies a general trend (using pre-trained LMs for controllable generation), the abstraction remains at a moderate level, and the critical analysis is limited to pointing out current constraints without deeper evaluation of trade-offs or effectiveness."}}
{"id": "9bd338ac-fe5b-46d0-a222-c4b6d188a3b6", "title": "Counterfactual Story Rewriting.", "level": "paragraph", "subsections": [], "parent_id": "3528137f-b0b0-4b4d-a0ec-53a065859380", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Research Agenda"], ["subsection", "Improving the Methodology on Non-Parallel Data"], ["subsubsection", "Inspiration from Tasks with Similar Nature"], ["paragraph", "Counterfactual Story Rewriting."]], "content": "Counterfactual story rewriting aims to learn a new event sequence in the presence of a perturbation of a previous event (i.e., counterfactual condition) .  propose the first dataset, each sample of which takes an originally five-sentence story, and changes the event in the second sentence to a new, counterfactual event. The task is to generate the last three sentences of the story based on the newly altered second sentence that initiates the story. The criteria of the counterfactual story rewriting include relevance with the first two sentences, and minimal edits from the original story ending. This line of research is relatively difficult to directly apply to TST, because its motivation and dataset nature is different from the general text style transfer, and more importantly, this task is not conditioned on a predefined categorized style token, but the free-form textual story beginning.", "cites": [5695], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview by discussing the nature and goals of counterfactual story rewriting and how it relates to TST. It integrates the key elements of the cited paper but does not deeply connect this task to other methodologies or broader themes. The critical analysis is present in identifying the limitations of applying this task directly to TST, but it is relatively brief and could be more developed. The abstraction is limited to a high-level comparison of task structures rather than identifying overarching principles."}}
{"id": "b3a4a6f6-e160-4533-bade-953adc9c2b9e", "title": "Contrastive Text Generation.", "level": "paragraph", "subsections": [], "parent_id": "3528137f-b0b0-4b4d-a0ec-53a065859380", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Research Agenda"], ["subsection", "Improving the Methodology on Non-Parallel Data"], ["subsubsection", "Inspiration from Tasks with Similar Nature"], ["paragraph", "Contrastive Text Generation."]], "content": "As neural network-based NLP models more easily learn spurious statistical correlations in the data rather than achieve robust understanding , there are recent works to construct auxillary datasets composed of near-misses of the original data. For example,  ask crowdsource workers to rewrite the input of the task with minimal changes but matching a different target label. To alleviate expensive human labor,  develop an automatic text editing approach to generate contrast set for aspect-based sentiment analysis. The difference between contrastive text generation and text style transfer is that the former does not require content preservation but mainly aims to construct a slightly textually different input that can result in a change of the ground-truth output, to test the model robustness. So the two tasks are not completely the same, although they have some intersections that might inspire future work, such as aspect-based style transfer suggested in Section~\\ref{sec:expanding_scope_of_style}.", "cites": [5696, 4235], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes two papers by highlighting their shared focus on generating perturbed text to test model robustness, but the integration is limited and does not offer a novel framework. It includes a basic comparison between contrastive text generation and text style transfer, identifying a key difference in content preservation. The abstraction is moderate, as it identifies a general pattern of using near-miss examples to improve evaluation but does not extend this to a broader theoretical or methodological insight."}}
{"id": "5d2de73c-f0ff-4a71-bec6-cc786bbc7d6c", "title": "Prototype-Based Text Editing.", "level": "paragraph", "subsections": [], "parent_id": "3528137f-b0b0-4b4d-a0ec-53a065859380", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Research Agenda"], ["subsection", "Improving the Methodology on Non-Parallel Data"], ["subsubsection", "Inspiration from Tasks with Similar Nature"], ["paragraph", "Prototype-Based Text Editing."]], "content": "Prototype editing is not unique in TST, but also widely used in other NLP tasks. Knowing the new advances in prototype editing for other tasks can potentially inspire new method innovations in TST.  first proposes the protype editing approach to improve LM by first sampling a lexically similar sentence prototype and then editing it using variational encoder and decoders. This prototype-and-then-edit approach can also be seen in summarization , machine translation , conversation generation , code generation , and question answering . As an extension to the retrieve and edit steps,  use an ensemble approach to retrieve a set of relevant prototypes, edit, and finally rerank to pick the best output for machine translation. Such extension can also be potentially applied to text style transfer.", "cites": [5697, 5000, 5699, 7984, 7225, 5698, 3130], "cite_extract_rate": 0.5833333333333334, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers on prototype-based text editing and connects them to the broader context of text style transfer, suggesting that this approach is not unique to TST but has potential for adaptation. It identifies a common methodological pattern (retrieve and edit) across different NLP tasks, offering some abstraction. However, it lacks deeper critical analysis of the methods' strengths and weaknesses and does not fully explore how these techniques might specifically benefit or fail in TST."}}
{"id": "3bd49843-4131-4dac-9ea5-6db09c11389d", "title": "Linguistic Styles with No Matched Data.", "level": "paragraph", "subsections": ["a7794825-b931-49f1-b5fb-68088a01845e"], "parent_id": "543b4b52-be1e-4b95-b66f-84d20f67036e", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Research Agenda"], ["subsection", "Loosening the Style-Specific Dataset Assumptions"], ["paragraph", "Linguistic Styles with No Matched Data."]], "content": "Since there are various concerns raised by the data-driven definition of style as described in Section~\\ref{sec:define}, a potentially good research direction is to bring back the linguistic definition of style, and thus remove some of the concerns associated with large datasets. Several methods can be a potential fit for this: prompt design  that passes a prompt to GPT  to obtain a style-transferred text; style-specific template design; or use templates to first generate synthetic data and make models learn from the synthetic data. Prompt design is not yet investigated as a direction for TST research, but it is an interesting direction to explore.}\n\\myRed{", "cites": [679, 8752, 7985, 1591], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section introduces the idea of leveraging prompt-based methods to address the limitations of data-driven style definitions, drawing from the broader literature on prompting and few-shot learning. While it connects these methods to the TST task, the synthesis is limited and the discussion lacks deeper integration of the cited papers. It identifies a potential research direction but does not critically evaluate the cited methods or their limitations in the context of TST."}}
{"id": "a7794825-b931-49f1-b5fb-68088a01845e", "title": "Distinguishing Styles from a Mixed Corpus.", "level": "paragraph", "subsections": [], "parent_id": "3bd49843-4131-4dac-9ea5-6db09c11389d", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Research Agenda"], ["subsection", "Loosening the Style-Specific Dataset Assumptions"], ["paragraph", "Linguistic Styles with No Matched Data."], ["paragraph", "Distinguishing Styles from a Mixed Corpus."]], "content": "It might also be possible to distinguish styles direction from a mixed corpus with no style labels.\nFor example,  learn a style vector space from text;}  use unsupervised representation learning to separate the style and contents from a mixed corpus of unspecified styles;  use cycle training with a conditional variational auto-encoder to unsupervisedly learn to express the same semantics through different styles. Theoretically, although disentanglement is impossible without inductive biases or other forms of supervision , disentanglement is achievable with some weak signals, such as only knowing how many factors have changed, but not which ones . A more advanced direction can be emergent styles , since styles can be evolving, for example across dialog turns.", "cites": [5686, 5700, 8752, 4423, 5667], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates concepts from several cited papers, particularly on unsupervised disentanglement and VAE-based methods, to build a narrative around learning to distinguish styles from a mixed corpus. It shows a moderate level of critical analysis by acknowledging theoretical limitations of disentanglement without supervision and referencing weak signal approaches. The section also abstracts slightly by discussing broader ideas like 'emergent styles' and long-term dynamics, but it remains largely focused on summarizing and connecting existing methods rather than offering a novel synthesis or deep critique."}}
{"id": "6863339c-7803-48a6-9478-7ea0e48c37b0", "title": "Improving Evaluation Metrics", "level": "subsection", "subsections": [], "parent_id": "99c8f4e3-6c98-40c3-b7ed-fcac390565de", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Research Agenda"], ["subsection", "Improving Evaluation Metrics"]], "content": "\\label{sec:improving_eval}\nThere has been a lot of attention to the problems of evaluation metrics of TST and potential improvements . \nRecently,  has proposed a new framework which is a live environment to evaluate NLG in a principled and reproducible manner.\nApart from the existing scoring methods, future works can also make use of linguistic rules such as a checklist to evaluate what capabilities the TST model has achieved. For example, there can be a checklist for formality transfer according to existing style guidelines such as the APA style guide . Such a checklist-based evaluation can make the performance of black-box deep learning models more interpretable, and also allow for more insightful error analysis.", "cites": [7127, 7986, 5701, 5702], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes ideas from multiple papers, particularly highlighting the limitations of current evaluation metrics and suggesting improvements such as checklist-based evaluations. It also critically identifies issues with the lack of consensus and reproducibility in TST evaluations. However, the integration is not particularly novel, and the abstraction remains moderate, focusing more on practical suggestions than deep meta-level insights."}}
{"id": "498570a1-dfa4-4bde-a3ff-2fed27edabb3", "title": "Paraphrase Generation.", "level": "paragraph", "subsections": ["8c0e6372-7c2c-49e1-931f-068f376b00ce", "2ccea4e4-4bd6-42be-b1b3-8553e576683e"], "parent_id": "96cac565-fcdf-4909-a54f-ea22f72da01b", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Expanding the Impact of TST"], ["subsection", "Connecting TST to More NLP Tasks"], ["paragraph", "Paraphrase Generation."]], "content": "Paraphrase generation is to express the same information in alternative ways . The nature of paraphrasing shares a lot in common with TST, which is to transfer the style of text while preserving the content. \\myRed{One of the common ways of paraphrasing is syntactic variation, such as ``X wrote Y.'', ``Y was written by X.'' and ``X is the writer of Y.'' . Besides syntactic variation, it also makes sense to include stylistic variation as a form of paraphrases, which means that the linguistic style transfer (not the content preference transfer in Table~\\ref{tab:task_type}) can be regarded as a subset of paraphrasing. The caution here is that if the paraphrasing is for a downstream task, researchers should first check if the downstream task is compatible with the used styles. For example, dialog generation may be sensitive to all linguistic styles, whereas summarization can allow linguistic style-varied paraphrases in the dataset.}\nThere are three implications of this connection of TST and paraphrase generation. First, many trained TST models can be borrowed for paraphrasing, such as formality transfer and simplification. A second connection is that the method innovations proposed in the two fields can inspire each other. For example,  formulate style transfer as a paraphrasing task. Thirdly, the evaluation metrics of the two tasks can also inspire each other. For example,  associate the semantic similarity metrics for two tasks.", "cites": [8759, 5703], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes information from two key papers, highlighting the conceptual overlap between TST and paraphrase generation. It provides a nuanced discussion by distinguishing between syntactic and stylistic variations and critically addressing compatibility issues for downstream tasks. The abstraction is strong as it generalizes the relationship between TST and paraphrasing, identifying broader implications for methodology and evaluation."}}
{"id": "8c0e6372-7c2c-49e1-931f-068f376b00ce", "title": "Data Augmentation.", "level": "paragraph", "subsections": [], "parent_id": "498570a1-dfa4-4bde-a3ff-2fed27edabb3", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Expanding the Impact of TST"], ["subsection", "Connecting TST to More NLP Tasks"], ["paragraph", "Paraphrase Generation."], ["paragraph", "Data Augmentation."]], "content": "Data augmentation generates text similar to the existing training data so that the model can have larger training data. TST is a good method for data augmentation because TST can produce text with different styles but the same meaning. Image style transfer has already been used for data augmentation , so it can be interesting to see future works to also apply text style transfer for data augmentation.", "cites": [5691], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a brief, descriptive overview of how TST can be used for data augmentation, with a single citation to a related paper. It lacks synthesis of multiple sources, critical evaluation of the cited work, and abstract-level insights. The discussion is minimal and does not explore broader implications, trends, or limitations."}}
{"id": "2ccea4e4-4bd6-42be-b1b3-8553e576683e", "title": "Adversarial Robustness Probing.", "level": "paragraph", "subsections": [], "parent_id": "498570a1-dfa4-4bde-a3ff-2fed27edabb3", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Expanding the Impact of TST"], ["subsection", "Connecting TST to More NLP Tasks"], ["paragraph", "Paraphrase Generation."], ["paragraph", "Adversarial Robustness Probing."]], "content": "Another use of style transferred text is adversarial robustness probing. For example, styles that are task-agnostic can be used for general adversarial attack (e.g., politeness transfer to probe sentiment classification robustness) , while the styles that can change the task output can be used to construct contrast sets (e.g., sentiment transfer to probe sentiment classification robustness) .\n applies image style transfer to adversarial attack, and future research can also explore the use of TST in the two ways suggested above.", "cites": [5692], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces the concept of using text style transfer for adversarial robustness probing, citing one paper on image style transfer and suggesting two applications. However, it lacks synthesis of multiple sources, critical evaluation of the cited work, and broader abstraction. The content remains at a surface level without deep analysis or identification of overarching trends."}}
{"id": "82130eaa-a122-446c-b9f7-8d77799514a3", "title": "Persona-Consistent Dialog Generation.", "level": "paragraph", "subsections": ["562c7bb3-cff5-47d6-82aa-32aff86144f6", "7f4cf141-b8c8-452d-a25d-5de5c9b16f73", "60448eb0-62b4-47f4-9046-d135e699120f"], "parent_id": "90c6478a-32a5-458f-b655-1e3cd90e0c33", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Expanding the Impact of TST"], ["subsection", "Connecting TST to More Specialized Applications"], ["paragraph", "Persona-Consistent Dialog Generation."]], "content": "A useful downstream application of TST is persona-consistent dialog generation . Since conversational agents directly interact with users, there is a strong demand for human-like dialog generation. Previously, this is done by encoding speaker traits into a vector and the conversation is then conditioned on this vector . As future work, text style transfer can also be used as part of the pipeline of persona-based dialog generation, where the persona can be categorized into distinctive style types, and then the generated text can be post-processed by a style transfer model.", "cites": [2021, 5694], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section makes a general connection between TST and persona-consistent dialog generation, citing two relevant papers. It offers a brief analytical perspective by suggesting that TST could be integrated into the dialog generation pipeline for post-processing. However, the synthesis is limited to two papers and lacks deeper comparative or critical analysis of their strengths and weaknesses."}}
{"id": "562c7bb3-cff5-47d6-82aa-32aff86144f6", "title": "Attractive Headline Generation.", "level": "paragraph", "subsections": [], "parent_id": "82130eaa-a122-446c-b9f7-8d77799514a3", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Expanding the Impact of TST"], ["subsection", "Connecting TST to More Specialized Applications"], ["paragraph", "Persona-Consistent Dialog Generation."], ["paragraph", "Attractive Headline Generation."]], "content": "In journalism writing, it is crucial to generate engaging headlines.   first use TST to generate eye-catchy headlines with three different styles, humorous, romantic, and clickbaity styles.  follow this direction and propose a disentanglement-based model to generate attractive headlines for Chinese news.", "cites": [7976], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a minimal synthesis of information, only mentioning two works briefly without connecting them to broader themes or each other. There is no critical evaluation or comparison of the approaches used in these papers, nor does it generalize beyond the specific examples to identify patterns or principles in TST applications for headline generation. The content remains at a surface level summary."}}
{"id": "5940a89c-60cc-4cae-a474-132664e29c6d", "title": "Beneficial.", "level": "paragraph", "subsections": [], "parent_id": "ddbd1d5f-e304-409f-927c-6a1ea1a52102", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Expanding the Impact of TST"], ["subsection", "Considering Ethical Impacts of TST"], ["subsubsection", "Social Impact of TST Applications"], ["paragraph", "Beneficial."]], "content": "An important direction of NLP for social good is to fight against abusive online text. Text style transfer can serve as a very helpful tool as it can be used to transfer malicious text to normal language. Shades of abusive language include hate speech, offensive language, sexist and racist language, aggression, profanity, cyberbullying, harassment, trolling, and toxic language . There are also other negative text such as propaganda , and others. It is widely known that malicious text is harmful to people. For example, research shows that cyberbullying victims tend to have more stress and suicidal ideation , and also detachment from family and offline victimization . There are more and more efforts put into combating toxic language, such as 30K content moderators that Facebook and Instagram employ   . Therefore, the automatic malicious-to-normal language transfer can be a helpful intelligent assistant to address such needs. Apart from purifying malicious text on social media, it can also be used on social chatbots to make sure there are no bad contents in language they generate~.", "cites": [7987, 2409], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic overview of how text style transfer can be used to combat abusive language, citing two papers for context. However, it does not deeply synthesize these sources or connect them to form a broader narrative. There is little critical evaluation or comparison of the cited works, and the section lacks abstraction or meta-level insights, focusing instead on general, surface-level benefits."}}
{"id": "d418cc29-c70f-4cdb-9efd-e3b673e271a4", "title": "Double-Sided Sword.", "level": "paragraph", "subsections": [], "parent_id": "ddbd1d5f-e304-409f-927c-6a1ea1a52102", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Expanding the Impact of TST"], ["subsection", "Considering Ethical Impacts of TST"], ["subsubsection", "Social Impact of TST Applications"], ["paragraph", "Double-Sided Sword."]], "content": "Besides positive and neutral applications, there are, unfortunately, several text style transfer tasks that are double-sided swords.\nFor example, one of the most popular TST tasks, sentiment modification, although it can be used to change intelligent assistants or robots from a negative to positive mood (which is unlikely to harm any parties), the vast majority of papers applies this technology to manipulate the polarity of reviews, such as Yelp  and Amazon reviews .\nThis leads to a setting where a negative restaurant review is changed to a positive comment, or vice versa, with debatable ethics. Such a technique can be used\nas a cheating method for the commercial body to polish its reviews, or harm the reputation of their competitors. Once this technology is used, it will automatically manipulate the online text to contain polarity that the model owner desires. Hence, we suggest the research community raise serious concern against the review sentiment modification task.\nAnother task, political slant transfer, may induce concerns within some specific context. For example, social bots (i.e., autonomous bots on social media, such as Twitter bots and Facebook bots) are a big problem in the U.S., even playing a significant role in the 2016 United States presidential election .\nIt is reported that at least 400,000 bots were responsible for about 19\\% of the total Tweets. Social bots usually target to advocate certain ideas, supporting campaigns, or aggregating other sources either by acting as a \"follower\" and/or gathering followers itself. So the political slant transfer task, which transfers the tone and content between republican comments and democratic ones, are highly sensitive and may face the risk of being used on social bots to manipulate political views of the mass.\nSome more arguable ones are male-to-female tone transfer, which can be potentially used for identity deception. The cheater can create an online account and pretend to be an attractive young lady. There is also the reversed direction (female-to-male tone transfer) which can be used for applications  such as authorship obfuscation  which anonymizes the author attributes by hiding the gender of a female author by re-synthesizing the text to use male.", "cites": [5704, 5668], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section integrates the cited papers by connecting sentiment modification and political slant transfer to broader ethical and societal concerns, showing some synthesis. It critically highlights the potential misuse of TST in review manipulation and political discourse, but does so more by emphasizing implications than by deeply analyzing the limitations or comparing approaches in the literature. The abstraction is limited, as it focuses on specific examples without generalizing to a meta-level understanding of ethical risks in TST."}}
{"id": "524ca115-64cf-4495-a881-bfad9f50d6fe", "title": "Data Privacy Issues for TST", "level": "subsubsection", "subsections": [], "parent_id": "68a59cf7-c777-420e-ab7d-d39fc4071f30", "prefix_titles": [["title", "Deep Learning for Text Style Transfer: \\\\ A Survey"], ["section", "Expanding the Impact of TST"], ["subsection", "Considering Ethical Impacts of TST"], ["subsubsection", "Data Privacy Issues for TST"]], "content": "Another ethical concern is the use of data in the research practice. Researchers should not overmine user data, such as the demographic identities. Such data privacy widely exists in the data science community as a whole, and there have been many ethical discussions .\nAlthough the TST task needs data containing \\textit{some} attributes along with the text content. While it is acceptable to use ratings of reviews that are classified as positive or negative, but the user attributes are sensitive, including the gender of the user's account , and age . \\myRed{The collection and potential use of such sensitive user attributes can have implications that need to be carefully considered.}", "cites": [5669, 5705], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions ethical concerns around data privacy in text style transfer and highlights the sensitivity of user attributes like gender and age. However, it does not effectively synthesize or integrate the cited papers to support its claims, nor does it provide a critical analysis of the research practices or limitations. The abstraction is minimal, focusing only on general ethical considerations without identifying broader patterns or principles in the field."}}
