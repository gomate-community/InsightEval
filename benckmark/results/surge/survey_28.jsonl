{"id": "355a5220-6797-48ae-9ddc-681e46e0768e", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "ca169d78-5c5d-483c-9bd5-7ee46d40e336", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Introduction"]], "content": "\\label{sec-introduction}\nCommunities have been investigated as early as the 1920s in sociology and social anthropology . However, it is only after the 21st century that advanced scientific tools were primarily developed upon real-world data . Since 2002 , Girvan and Newman opened a new direction with graph partition. In the past ten years, researchers from computer science have extensively studied community detection  by utilizing network topological structures  and semantic information  for both static and dynamic networks  and small and large networks . Graph-based approaches are developed increasingly to detect communities in environments with complex data structures . Network dynamics and community impacts can be analyzed in details within community detection, such as rumor spread, virus outbreak, and tumour evolution.\n\\begin{figure}[!t]\n\\centering\n\\subfigure[graph]{\\includegraphics[width=0.19\\textwidth]{figure/osn.pdf}} \\hfill\n\\subfigure[communities]{\\includegraphics[width=0.27\\textwidth]{figure/osnc.pdf}} \n\\caption{\\footnotesize (a) An illustration of a graph where nodes denote users in a social network. (b) An illustration of two communities ($C_1$ and $C_2$) based on the prediction of users' occupations. The detection utilizes users' closeness in online activities (topology) and account profiles (attributes). }\n\\label{fig-osn}\n\\end{figure}\nCommunity detection is a research area with increasing practical significance. As the saying goes, \\textit{Birds of a feather flock together} . Based on the theory of \\textit{Six Degrees of Separation}, any person in the world can know anyone else through six acquaintances . Indeed, our world is a vast network formed by a series of communities. For example, in social networks (Fig. \\ref{fig-osn}), platform sponsors promote products to targeted users in detected communities   . Community detection in citation networks  determines the importance, interconnectedness, evolution of research topics and identifies research trends. In metabolic networks  and Protein-Protein Interaction (PPI) networks , community detection reveals the complexities of metabolisms and proteins with similar biological functions. Similarly, community detection in brain networks  reflects the functional and anatomical segregation of brain regions.\nMany traditional techniques, such as spectral clustering  and statistical inference , are applied to small networks and simple cases. However, real-world networks in rich nonlinear information make traditional models less applicable to practical applications, including complex topology and high dimensional features. Their computational costs are expensive. The powerful techniques of \\textbf{deep learning} offer flexible solutions with good community detection performance to: (1) learn nonlinear network properties, such as relations denoting edges between nodes, (2) represent lower-dimensional network embeddings preserving the complicated network structure, and (3) achieve better community detection from various information. Therefore, deep learning for community detection is a new trend that demands a timely comprehensive survey\\footnote{This paper is an extended vision of our published survey  in IJCAI-20, which is the first published work on the review of community detection approaches with deep learning.}. \nTo the best of our knowledge, this paper is the first comprehensive survey focusing on deep learning contributions in community detection. In discovering inherent patterns and functions , existing surveys mainly focused on community detection on \\textit{specific techniques} , \\textit{different network types} , \\textit{community types} , and \\textit{application scenarios} . The surveys on \\textit{specific techniques} are summarized but not limited to partial detection based on probabilistic graphical models , Label Propagation Algorithm (LPAs) , and evolutionary computation for single- and multi-objective optimizations . In terms of different \\textit{network types}, researchers provide overviews on dynamic networks , directed networks  and multi-layer networks . Moreover, detection techniques are reviewed over disjoint and overlapping  \\textit{community types}. Regarding \\textit{application scenarios}, the focus has been on techniques on social networks .\nObserving the past, current and future trends, this paper aims to support researchers and practitioners to understand the community detection field with respect to:\n\\begin{itemize}\n\\item \\textbf{Systematic Taxonomy and Comprehensive Review.} We propose a new systematic taxonomy for this survey (see Fig. \\ref{fig-taxonomy}). For each category, we review, summarize and compare the representative works. We also briefly introduce community detection applications in the real world. These scenarios provide horizons for future community detection research and practices.\n\\item \\textbf{Abundant Resources and High-impact References.} The survey collects open resources, including benchmark data sets, evaluation metrics and technique implementations. Publications in the latest high-impact international conferences and high-quality peer-reviewed journals cover data mining, artificial intelligence, machine learning and knowledge discovery.\n\\item \\textbf{Future Directions.} As deep learning is a new research area, we discuss current limitations, critical challenges and open opportunities for future directions.\n\\end{itemize}\nThe rest of the article is organized as follows: Section \\ref{sec-preliminaries} defines essential notations, concepts, input and output of deep learning approaches. Section \\ref{sec-traditional-alg} overviews community detection development. Section \\ref{sec-categorization} introduces a deep learning taxonomy. Sections \\ref{sec-model-cn}--\\ref{sec-model-filtering} summarize comprehensive reviews on each category in the taxonomy. Section \\ref{sec-experimentation} and Section \\ref{sec-application} organize the popular implementation resources and real-world applications. Lastly, Section \\ref{sec-future} discusses current challenges, suggests future research directions before the conclusion in Section \\ref{sec-conclusion}. The supporting materials can be found in APPENDIX \\ref{sec-stdlcd} (core techniques of reviewed literature summarized in tables), APPENDIX \\ref{sec-ddds}--\\ref{sec-appcode} (resource descriptions of data sets, evaluation metrics and implementation projects) and APPENDIX \\ref{sec-abbr} (abbreviations).", "cites": [6530, 6525, 6531, 1671, 9039, 6527, 1712, 6526, 8483, 9036, 9034, 9150, 9033, 9037, 9035, 9038, 6528, 6529], "cite_extract_rate": 0.46153846153846156, "origin_cites_number": 39, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a broad analytical overview by situating deep learning in community detection within the context of traditional methods and highlighting its advantages. It synthesizes key ideas from multiple surveys and applications, showing some integration of concepts like dynamic networks, node attributes, and overlapping communities. However, the critical analysis is limited to general statements about traditional methods and the potential of deep learning, without detailed evaluation of cited works."}}
{"id": "cde0d638-88e1-45a1-9a8c-b74f35a31e85", "title": "A Development of Community Detection", "level": "section", "subsections": [], "parent_id": "ca169d78-5c5d-483c-9bd5-7ee46d40e336", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "A Development of Community Detection"]], "content": "\\label{sec-traditional-alg}\nCommunity detection has been significant in network analysis and data mining. Fig. \\ref{fig-timeline} illustrates its development from traditional to deep learning in a timeline. Their respective categories are summarized in Fig. \\ref{fig-taxonomy} in the left and right parts, respectively. Two categories of methods suggest the development changes. Traditional methods mainly explore communities from network structures. We briefly review these seven categories of methods in this section. Deep learning uncovers deep network information and models complex relationships from high-dimensional data to lower-dimensional vectors. These will be reviewed in the following sections.\n\\vspace{2mm}\n\\noindent\n\\textbf{Graph Partition.} These methods, well known as graph clustering , are employed in deep learning models. They partition a network into communities with a given number $K$. Kernighan-Lin  is a representative heuristic algorithm. It initially divides a network into two arbitrary subgraphs and optimizes on nodes. Spectral bisection  is another representative method applying spectrum Laplacian matrix.\n\\vspace{2mm}\n\\noindent\n\\textbf{Statistical Inference.} Stochastic Block Model (SBM)  is a widely applied generative model by assigning nodes into communities and controlling their probabilities of likelihood. The variants include Degree-Corrected SBM (DCSBM)  and Mixed Membership SBM (MMB) .\n\\vspace{2mm}\n\\noindent\n\\textbf{Hierarchical Clustering.} This group of methods discover hierarchical community structures (\\textit{i.e.}, dendrogram) in three ways: divisive, agglomerative and hybrid. The Girvan-Newman (GN) algorithm finds community structure in a divisive way by successively removing edges such that a new community occurs . Fast Modularity (Fast\\textit{Q}) , an agglomerative algorithm, gradually merges nodes, each of which is initially regarded as a community. Community Detection Algorithm based on Structural Similarity (CDASS)  jointly applies divisive and agglomerative strategies in a hybrid way. \n\\vspace{2mm}\n\\noindent\n\\textbf{Dynamical Methods.} Random walks are utilized to detect communities dynamically. For example, the random walk in WalkTrap  calculates node distances and the probability of community membership. Information Mapping (InfoMap)  applies the minimal-length encoding. Label Propagation Algorithm (LPA)  identifies diffusion communities through an information propagation mechanism. \n\\vspace{2mm}\n\\noindent\n\\textbf{Spectral Clustering.} The network spectra reflects the community structure. Spectral clustering  partitions the network on the normalized Laplacian matrix and the regularized adjacency matrix, and fits SBM in the pseudo-likelihood algorithm. On the spectra of normalized Laplacian matrices, Siemon \\textit{et al.} integrated communities in macroscopic and microscopic neural brain networks to obtain clusters.\n\\vspace{2mm}\n\\noindent\n\\textbf{Density-based Algorithms.} Significant clustering algorithms include Density-Based Spatial Clustering of Applications with Noise (DBSCAN) , Structural Clustering Algorithm for Networks (SCAN)  and Locating Structural Centers for Community Detection (LCCD) . They identify communities, hubs and outliers by measuring entities' density. \n\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=0.9\\textwidth]{figure/cnn.pdf}\n\\caption{\\footnotesize A general framework for CNN-based community detection with details in Section \\ref{sec-model-cnn}. As Convolutional Neural Network (CNN) input, the graph is preprocessed into the image data on nodes or edges. The $d$-dimensional latent features are convolutionally mapped within multiple CNN hidden layers and a final fully connected layer outputs representations of each node or each edge for classifications. Focusing on nodes, the flow \\ding{172} predicts community labels in $k$ classes that nodes with same labels are clustered into a community. Focusing on edges, the flow \\ding{173} predicts edge labels in two classes, \\textit{i.e.}, inner and inter. The preliminary communities are formed by removing inter-community edges and optimized by merging with a measurement such as Modularity $Q$.} \\label{fig-cnn}\n\\end{figure*}\n\\vspace{2mm}\n\\noindent\n\\textbf{Optimizations.} Community detection generally maximizes the likelihood. Modularity (\\textit{Q})  is the most classic optimization function following its variant Fast\\textit{Q} . Louvain  is another well-known optimization algorithm that employs the node-moving strategy on optimized modularity. Moreover, the extensions of greedy optimizations include simulated annealing , extremal optimization  and spectral optimization . Effective in local and global searching , the evolutionary optimizations consist of single and multiple objectives. For example, Multi-Agent Genetic Algorithm (MAGA-Net)  applies a single modularity function. Combo  mixes Normalized Mutual Information (NMI)  and Conductance (CON) . \\textit{Q}, NMI and CON estimate the network partition quality with details in APPENDIX \\ref{sec-EM}.\n\\vspace{2mm}\n\\noindent\n\\textbf{Why community detection needs deep learning?} Capturing information from connections in a straightforward way may lead to sub-optimal community detection results. Deep learning models  bring the following additional advantages to community detection. Thus, deep learning-based community detection has been a new emerging branch. Its general framework learns lower-dimensional vectors from high-dimensional data of complex structural relationships . It, therefore, enables knowledge discoveries via the state-of-the-art machine learning and data mining techniques. The framework with representations can further embed non-structural features, such as node attributes, to increase the knowledge of community memberships . Besides, groups of information from nodes , edges , neighborhoods  or multi-graphs  can be jointly recognized with special attentions in the deep learning process, leading to effective community detection results. With the deep learning ability to big data, more and more large-scale , high-sparse , complex structural , dynamic  networks in real-world scenarios can be explored. Along with the achievements in a relatively short time (in Fig. \\ref{fig-timeline}), the development of community detection within the deep learning domain keeps facing a series of challenges. In this paper, we review accomplishments in Sections \\ref{sec-model-cn}--\\ref{sec-application}, and point out opportunities and challenges in Section \\ref{sec-future}.", "cites": [6530, 6534, 6533, 9039, 8488, 8483, 6532, 9150, 8487, 8564, 9037, 9116, 9042, 166, 9043, 9040, 9041, 1430], "cite_extract_rate": 0.4090909090909091, "origin_cites_number": 44, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a factual summary of traditional community detection methods and briefly introduces the motivation for deep learning. It cites multiple papers but does not deeply integrate or synthesize their findings into a cohesive narrative. Limited critical evaluation of the cited works is present, and while it touches on general categories, it lacks meta-level abstraction or identification of overarching principles."}}
{"id": "afcc3bd7-a99a-4b39-826e-c7d51c2ce4ca", "title": "GCN-based Community Detection", "level": "subsection", "subsections": [], "parent_id": "53179a86-3ef2-45d1-a6a6-5ebd937b3b6c", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Convolutional Network-based Community Detection"], ["subsection", "GCN-based Community Detection"]], "content": "\\label{sec-model-gcn}\nGCNs aggregate node neighborhood's information in deep graph convolutional layers to globally capture complex features for community detection (Fig. \\ref{fig-gcn}). There are two classes of community detection methods based on GCNs: (1) supervised/semi-supervised community classification, and (2) community clustering with unsupervised network representation. Community classification methods are limited by a lack of labels in the real world. In comparison, network representations are more flexible to cluster communities through techniques such as matrix reconstructions and objective optimizations. TABLE \\ref{table-gcn} in APPENDIX \\ref{sec-stdlcd} compares the techniques.\nGCNs employ a few traditional community detection methods as deep graph operators, such as SBMs for statistical inference, Laplacian matrix for spectrum analysis and probabilistic graphical models for belief propagation. For example, Line Graph Neural Network (LGNN)  is a supervised community detection model, which improves SBMs with better community detection performance and reduces the computational cost. Integrating the non-backtracking operator with belief propagation's message-passing rules, LGNN learns node represented features in directed networks. The softmax function identifies conditional probabilities that a node $v_i$ belongs to the community $C_k$ ($o_{i,k} = p(y_i=c_k|\\Theta,\\mathcal{G}$), and minimizes the cross-entropy loss over all possible permutations $S_{\\mathcal{C}}$ of community labels:\n\\begin{equation}\n\\small\n\\mathcal{L}(\\Theta)=\\min_{\\pi \\in S_{\\mathcal{C}}}{-\\sum\\nolimits_i\\log{o_{i, \\pi(y_{i})}}}.\n\\end{equation}\nSince GCN is not initially designed for the community detection task, community structures are not the focus in learning node embeddings. To meet this gap, a semi-supervised GCN community detection model, named MRFasGCN, characterizes hidden communities through extending the network-specific Markov Random Field as a new convolutional layer (eMRF) which makes MRFasGCN community oriented and performs a smooth refinement to the coarse results from GCN. To enable unsupervised community detection, the GCN-based framework, denoted as SGCN , designs a local label sampling model to locate the structural centers for community detection. By integrating the label sampling model with GCN, SGCN encodes both network topology and node attributes in the training of each node's community membership without any prior label information. \nIn terms of a probabilistic inference framework, detecting overlapping communities can be solved by a generative model inferring the community affiliations of nodes. For example, Neural Overlapping Community Detection (NOCD)  combines the Bernoulliâ€“Poisson (BP) probabilistic model and a two-layer GCN to learn community affiliation vectors by minimizing BP's negative log-likelihood. By setting a threshold to keep recognizing and removing weak affiliations, final communities are obtained.\nSpectral GCNs represent all latent features from the node's neighborhood. The features of neighboring nodes converge to the same values by repeatedly operating Laplacian smoothing in deep GCN layers. However, these models lead to an over-smoothing problem in community detection. To reduce the negative impact, Graph Convolutional Ladder-shape Networks (GCLN)  is designed as a new GCN architecture for unsupervised community detection ($k$-means), which is based on the U-Net in the CNN field. A contracting path and an expanding path are built symmetrically in GCLN. The contextual features captured from the contracting path fuse with the localized information learned in the expanding path. The layer-wise propagation follows Eq. (\\ref{equ-gcn}).\nSince different types of connections are generally treated as plain edges, GCNs represent each type of connection and aggregate them, leading to redundant representations. Independence Promoted Graph Disentangled Network (IPGDN)  distinguishes the neighborhood into different parts and automatically discovers the nuances of a graph's independent latent features to reduce the difficulty in detecting communities. IPGDN is supported by Hilbert-Schmidt Independence Criterion (HSIC) regularization  in neighborhood routings.\nFor attributed graphs, community detection by GCNs relies on both structural information and attributed features, where neighboring nodes and the nodes with similar features are likely to cluster in the same community. Therefore, graph convolutions multiply the above two graph signals and need to filter out high-frequency noises smoothly. To this end, a low-pass filter is embedded in Adaptive Graph Convolution (AGC)  with spectral clustering through:\n\\begin{equation}\n\\small\np(\\lambda_q) = (1 - \\frac{1}{2} \\lambda_q)^k,\n\\end{equation}\nwhere the frequency response function of $\\mathcal{G}$ denotes $p(\\Lambda)=\\text{diag}\\left(p\\left(\\lambda_1\\right),\\cdots,p\\left(\\lambda_n\\right)\\right)$ decreasing and nonnegative on all eigenvalues. AGC convolutionally selects suitable neighborhood hop sizes in $k$ and represents graph features by the $k$-order graph convolution as:\n\\begin{equation}\n\\small\n\\bar{\\bm{X}} = (I - \\frac{1}{2}\\bm{L}_s)^k\\bm{X}, \n\\end{equation}\nwhere $\\bm{L}_s$ denotes the symmetrically normalized graph Laplacian on $\\lambda_q$. The filter smooths the node embedding $\\bar{\\bm{X}}$ adjusting to $k$ that higher $k$ leads to better filtering performance.\nAdaptive Graph Encoder (AGE)  is another smoothing filter model scalable to community detection. AGE adaptively performs node similarity measurement ($\\bm{S} = \\left[s_{ij}\\right]$) and $t$-stacked Laplacian smoothing filters ($\\bar{\\bm{X}} = (\\bm{I} - \\gamma \\bm{L})^t \\bm{X}$):\n\\begin{equation}\n\\small\n\\mathcal{L} = \\sum\\nolimits_{(v_i,v_j)\\in V'} -s'_{ij}\\log(s_{ij}) - (1-s'_{ij})\\log(1 - s_{ij}),  \n\\end{equation}\nwhere $V'$ denotes balanced training set over positive (similar) and negative (dissimilar) samples, $s'_{ij}$ is the ranked binary similarity label on node pairs $(v_i,v_j)$.\nA few works make significant contributions to GCN's filters. For example, Graph Convolutional Neural Networks with Cayley Polynomials (CayleyNets) , in a spectral graph convolution architecture, proposes an effective Cayley filter for high-order approximation on community detection. It specializes in narrow-band filtering since low frequencies contain extensive community information for community detection-aimed representations. Cooperating with the Cayley filter, CayleyNets involves a mean pooling in spectral convolutional layers and a semi-supervised softmax classifier on nodes for community membership prediction. \nTo capture the global cluster structure for community detection, the Spectral Embedding Network for attributed graph clustering (SENet)  introduces the loss of spectral clustering into a three-layer GCN's output layer by minimizing: \n\\begin{equation}\n\\small\n\\mathcal{L} = -\\operatorname{tr}((\\bm{H}^{(3)})^{\\top} \\bm{D}^{-\\frac{1}{2}} \\bm{K} \\bm{D}^{-\\frac{1}{2}} \\bm{H}^{(3)}), \n\\end{equation}\nwhere $\\bm{K}$ is a kernel matrix encoding typologies and attributes. \nCommunity Deep Graph Infomax (CommDGI)  jointly optimizes graph representations and clustering through Mutual Information (MI) on nodes and communities, and measures graph modularity for maximization. It applies contrastive training to obtain better representations, $k$-means for node clustering and targets cluster centers. Zhao \\textit{et al.}  proposed a graph debiased contrastive learning which simultaneously performs representations and clustering so that the clustering results and discriminative representations are both improved.\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=0.48\\textwidth]{figure/gat.pdf}\n\\caption{\\footnotesize A general framework for GAT-based community detection with details in Section \\ref{sec-model-gat}. Graph Attention Network (GAT) assigns attention coefficients $\\bm{\\alpha}^{(l)}_{ij}$: \\{green, blue, purple\\} in each neighborhood $N(v_i)$ in $l$-th hidden layer. The represented vector $\\bm{h}'_i$ aggregates available information: \\ding{172} differently colored edges between the same node pairs in multiplex networks, or \\ding{173} metapaths or context paths in a heterogeneous network. The GAT embeddings $\\bm{H}$ are analyzed to cluster communities.} \\label{fig-gat}\n\\end{figure}", "cites": [6536, 6535, 6534, 220, 9044, 6537], "cite_extract_rate": 0.5, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.3, "critical": 3.8, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple GCN-based community detection approaches, integrating their design choices, objectives, and innovations into a coherent narrative. It critically evaluates limitations such as over-smoothing, entanglement of filters, and the lack of focus on community structures in standard GCNs. The section also abstracts key principles like the role of spectral filters, the importance of local/structural information fusion, and the impact of probabilistic modeling on overlapping community detection."}}
{"id": "21a97351-bc5f-490d-b0fc-f5e42ec1d359", "title": "Graph Attention Network-based Community Detection", "level": "section", "subsections": [], "parent_id": "ca169d78-5c5d-483c-9bd5-7ee46d40e336", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Graph Attention Network-based Community Detection"]], "content": "\\label{sec-model-gat}\nGraph Attention Networks (GATs)  aggregate nodes' features in neighborhoods by trainable weights with attentions:\n\\begin{equation}\n\\small\n\\bm{h}_{i}^{(l+1)}=\\sigma\\left(\\sum\\nolimits_{j \\in N(v_i)} \\alpha_{i j}^{(l+1)} \\bm{W}^{(l+1)} \\bm{h}_{j}^{(l)}\\right),\n\\end{equation}\nwhere $\\bm{h}_{i}^{(l)}$ represents the node $v_i$'s representation in the $l$-th layer ($\\bm{h}_{i}^{(0)} = \\bm{x}_i$) and $\\alpha_{i j}^{(l)}$ is the attention coefficient between $v_i$ and $v_j \\in N(v_i)$. In community detection (Fig. \\ref{fig-gat}), the attention mechanism contributes to adaptively learning the importance of each node in a neighborhood. Correlations between similar nodes are computed close to the ground truth of community membership, which is inherited in GAT's representations. These attentions filter spatial relations or integrate metapaths. Therefore, communities in attributed, multiplex, heterogeneous networks can be easily detected (TABLE \\ref{table-gat}).\nThe relations need special attention in deep community detection models. For example, citations and co-subject relationships are both significant in clustering papers into research topics. The multiplex network is constructed by multiple relation types reflected on edges (\\textit{i.e.}, $E^{(r)}$). Each type of edges is grouped in a layer $r$. Thus, GATs can pay attentions to relation types. Deep Graph Infomax for attributed Multiplex network embedding (DMGI)  independently embeds each relation type and computes network embeddings in maximizing the globally shared features to detect communities. Its contrastive learning conducts between the original network and a corrupted network on each layer through the discriminator. A consensus regularization is subsequently applied with the attention coefficient to integrate final embeddings. The benefit of applying the attention mechanism is to preprocess the less significant relations by weakening their coefficient, especially when various types of relations are presented.\nDespite the extrinsic supervision signal sharing globally in multiple relations, DMGI cannot capture the intrinsic signal into node embeddings where node attributes are considered. To fill the gap, High-order Deep Multiplex Infomax (HDMI)  is developed to capture both signals and their interactions. HDMI further designs a fusion module for the node embedding combination over multiplex network layers based on the semantic attention  on community memberships. \nHeterogeneous Information Network (HIN) involves diverse types of nodes $\\mathcal{V}$ and edges $\\mathcal{E}$. Deep community detection over heterogeneous networks represents heterogeneous typologies and attributes $\\mathcal{X}$. Metapath Aggregated Graph Neural Network (MAGNN)  offers a superior community detection solution by multi-informative semantic metapaths which distinguish heterogeneous structures in graph attention layers. MAGNN generates node attributes from semantic information. Since heterogeneous nodes and edges exist in intra- and inter-metapaths, MAGNN utilizes the attention mechanism in both of their embeddings by aggregating semantic variances over nodes and metapaths. Thus, MAGNN reduces the high heterogeneity and embeds richer topological and semantic information to enhance community detection results. Moreover, HeCo  embeds network schema and metapaths from HINs. A modified contrastive learning is employed to guide two parts of embeddings. Sequentially, the attention mechanism acts on node importance, network heterogeneity and semantic features, where community detection benefits.\nThe above two works both use metapaths to facilitate community detection, but defining meaningful metapaths requires lots of domain knowledge. Thus, Context Path-based Graph Neural Network (CP-GNN)  is built to learn node embeddings by context paths where attention mechanisms are exploited to discriminate the importance of different relationships. The non-predefined context paths are significant in capturing the high-order relationship for community detection. \n\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=0.7\\textwidth]{figure/gan.pdf}\n\\caption{\\footnotesize A general framework for GAN-based community detection with details in Section \\ref{sec-model-gan}. Generative Adversarial Network (GAN) produces fake samples $\\phi_{g}(\\bm{z})$ by the generator $\\phi_g$ to fool the discriminator $\\phi_d$. GAN's real samples can be node embeddings $\\bm{H}_v$, local topology (\\textit{e.g.,} triplet, clique) or communities. Thus, real and fake samples competitively finetune community features: The flow \\ding{172} obtains community membership via clique-level GAN. The flow \\ding{173} detects communities based on competitive node-level representations from $\\bm{H}_v$ or triplets. The flow \\ding{174} directly discriminates communities through the discriminator.} \n\\label{fig-gan}\n\\end{figure*}", "cites": [6538, 6539, 8564, 180, 7583, 1430], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes information from multiple papers by connecting GAT-based approaches for community detection in heterogeneous and multiplex networks. It provides a coherent narrative by discussing how attention mechanisms are used to learn node importance, integrate metapaths, and manage heterogeneity. The section also identifies limitations, such as the need for domain knowledge in metapath design, and introduces solutions like CP-GNN, showing a critical and analytical perspective. It abstracts beyond specific methods to highlight the broader role of attention in enhancing community detection through richer topological and semantic integration."}}
{"id": "6fca7cce-e604-4c3a-acef-9448984100de", "title": "Generative Adversarial Network-based Community Detection", "level": "section", "subsections": [], "parent_id": "ca169d78-5c5d-483c-9bd5-7ee46d40e336", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Generative Adversarial Network-based Community Detection"]], "content": "\\label{sec-model-gan}\nAdversarial training is effective in generative models and improves discriminative ability. However, it needs to solve the overfitting problem when applied to community detection (see Fig. \\ref{fig-gan} and TABLE \\ref{table-gan}). Generative Adversarial Networks (GANs)  competitively trains a generator $\\phi_g$ and a discriminator $\\phi_d$ in the adversarial framework. $\\phi_d(\\bm{x})$ represents the probability of input data, while $\\phi_g(\\bm{z})$ learns the generator's distribution $p_g$ via input noise variables $p_{\\bm{z}}(\\bm{z})$. The generator fools the discriminator by generating fake samples. Its objective function is defined as: \n\\begin{equation}\n\\small\n\\begin{aligned}\n\\min_{\\phi_g} \\max_{\\phi_d} & \\mathbb{E}_{\\bm{x} \\sim p_{data}(\\bm{x})} [\\log \\phi_d(\\bm{x})] \\\\\n& +\\mathbb{E}_{\\bm{z} \\sim p_{\\bm{z} }(\\bm{z} )} [\\log (1-\\phi_d(\\phi_g(\\bm{z} )))]. \n\\end{aligned}\n\\end{equation}\nSeed Expansion with generative Adversarial Learning (SEAL)  generates seed-aware communities from selected seed nodes by Graph Pointer Network with incremental updates (iGPN). It consists of four components in the community level, \\textit{i.e.}, generator, discriminator, seed selector and locator. The discriminator adopts Graph Isomorphism Networks (GINs) to modify generated communities with the ground truth of community labels. The locator is designed to provide regularization signals to the generator, so that irrelevant nodes in community detection can be eliminated.\nTo imbalanced communities, Dual-Regularized Graph Convolutional Networks (DR-GCN)  utilizes a conditional GAN into the dual-regularized GCN model, \\textit{i.e.}, a latent distribution alignment regularization and a class-conditioned adversarial regularization. The first\nregularization $\\mathcal{L}=(1-\\alpha)\\mathcal{L}_{gcn}+\\alpha \\mathcal{L}_{dist}$ balances the communities by minimizing the Kullback-Leibler (KL) divergence between majority and minority community classes $\\mathcal{L}_{dist}$ guided with a standard GCN training $\\mathcal{L}_{gcn}$. The second regularization is designed to distinguish communities on labeled node representations:\n\\begin{equation}\n\\small\n\\begin{aligned}\n\\min_{\\phi_{g}, \\mathcal{L}} &\\max_{\\phi_{d}} \\mathcal{L}(\\phi_{d}, \\phi_{g}) = \\mathbb{E}_{v_{i} \\sim p_{data}(v_{i})} \\log \\phi_{d}(v_{i} |y_{i}) \\\\\n& +\\mathbb{E}_{\\bm{z} \\sim p_{\\bm{z}}(\\bm{z})} [\\log(1-\\phi_{d}(\\phi_{g}(\\bm{z} |y_{i}))) + \\mathcal{L}_{reg}], \n\\end{aligned}\n\\end{equation}\nwhere $\\mathcal{L}_{reg} = \\sum_{u \\in N(v_{i})} \\| \\bm{h}_{v'_{i}} - \\bm{h}_{u} \\|_2$ forces the generated fake node $v'_{i}$ to reconstruct the respective neighborhood relations (as $u \\sim v_{i}$), $y_{i}$ is the ground truth label of node $v_{i}$. \nInstead of generating only one kind of fake samples, Jointly Adversarial Network Embedding (JANE)  employs two network information of topology and node attributes to capture semantic variations from adversarial groups of real and fake samples. Specifically, JANE represents embeddings $\\bm{H}$ through a multi-head self-attention encoder $\\phi_e$, where Gaussian noises are added for fake features for competition over the generator $\\phi_g$ and the discriminator $\\phi_d$:\n\\begin{equation}\n\\small\n\\begin{aligned}\n\\min_{\\phi_g,\\phi_e} \\max_{\\phi_d} & {\\mathcal{L} (\\phi_d, \\phi_e, \\phi_g)}\n= \\\\\n&\\mathbb{E}_{(\\bm{a}, \\bm{x}) \\sim p_{\\bm{AX}}}[\\underbrace{\\mathbb{E}_{\\bm{h} \\sim p_{\\phi_e}(\\cdot |\\bm{a}, \\bm{x})}[\\log \\phi_d(\\bm{h}, \\bm{a}, \\bm{x})]}_{\\log \\phi_d(\\phi_e(\\bm{a}, \\bm{x}), \\bm{a}, \\bm{x})}] \\\\\n+& \\mathbb{E}_{\\bm{h} \\sim p_{\\bm{H}}}[\\underbrace{\\mathbb{E}_{(\\bm{a}, \\bm{x}) \\sim p_{\\phi_g}(\\cdot |\\bm{h})}[\\log (1-\\phi_d(\\bm{h}, \\bm{a}, \\bm{x}))]}_{\\log (1-\\phi_d(\\bm{h}, \\phi_g(\\bm{h})))}], \n\\end{aligned}\n\\end{equation}\nwhere $p_{\\bm{AX}}$ denotes the joint distribution of the topology $\\bm{A}$ ($\\bm{a} \\subseteq \\bm{A}$) and sampled node attributes $\\bm{X}$ ($\\bm{x} \\subseteq \\bm{X}$).\n\\begin{figure*}[!t]\n\\centering \\footnotesize\n\\subfigure[\\footnotesize A general framework of AE-based community detection]{\\includegraphics[width=0.8\\textwidth]{figure/AE-a-framework.pdf}}\n\\subfigure[\\footnotesize static unattributed graph]{\\includegraphics[width=0.3\\textwidth]{figure/AE-b-unattributed_static.pdf}} \n\\subfigure[\\footnotesize static attributed graph]{\\includegraphics[width=0.27\\textwidth]{figure/AE-c-attributed.pdf}} \n\\subfigure[\\footnotesize dynamic graph]{\\includegraphics[width=0.26\\textwidth]{figure/AE-d-dynamic.pdf}} \n\\subfigure[\\footnotesize cross-domain graph]{\\includegraphics[width=0.2\\textwidth]{figure/AE-e-cross-domain.pdf}} \n\\subfigure[\\footnotesize heterogeneous graph]{\\includegraphics[width=0.33\\textwidth]{figure/AE-f-heterogeneous.pdf}}\n\\subfigure[\\footnotesize attributed multi-view graph]{\\includegraphics[width=0.41\\textwidth]{figure/AE-g-multi_view.pdf}} \n\\caption{\\footnotesize A general framework for AE-based community detection consists of (a) the framework of the input layer, AE and clustering layers and the output layer and (b)-(g) AE-based community detection processes for each type of graph: (a) The input can be one of the graphs. They have topology and attributes information which are represented by the adjacency matrix $\\bm{A}$ (\\ding{172}-\\ding{174}, \\ding{177}), node attribute vectors $\\bm{X}$ (\\ding{173}, \\ding{177}), node similarity matrix $\\bm{S}$ (\\ding{175}) and metapaths $\\Phi$ (\\ding{176}). More than one AE can be included that each AE embeds a part of input. Two optional work flows representively output node embeddings $\\bm{H}_v$ and community membership matrix $\\bm{H}_c$ for community detection results (\\textit{i.e.}, the AE output layer). (b) Inputting typologies $\\bm{A}$, the AE outputs node embeddings $\\bm{H}_{v}$ by minimizing the reconstruction loss $\\mathcal{L}_{re}$ (solid arrows) or outputs the community membership matrix $\\bm{H}_{c}$ by further minimizing the clustering loss over $\\bm{P}$ and $\\bm{Q}$ distributions which measures node's probability in a community (dashed arrows). The community prior information for $\\mathcal{L}(\\bm{O},\\bm{H})$ (dashed arrows) or the sparse penalty for $\\Omega(\\rho\\|\\sum{\\bm{h}_{i}/n})$ (dashed arrows) optionally guides AE learning upon hidden layer embeddings $\\bm{h}_{i} \\in \\bm{H}$. (c) Aiming at $\\bm{H}_{v}$ or $\\bm{H}_{c}$, typologies $\\bm{A}$ and node attributes $\\bm{X}$ are respectively represented by AE1 and AE2 (solid arrows). Their embeddings $\\bm{H}_{\\bm{A}}$ and $\\bm{H}_{\\bm{X}}$ are aggregated by minimizing the consistency loss. Otherwise (dashed arrows), an AE embeds all input over $(\\bm{A},\\bm{X})$. (d) The $t$-th snapshot $\\bm{A}_{(t)}$ in the dynamic graph is represented for the node embeddings $\\bm{H}_{(t)}$. The temporal smoothness regularization optimizes $\\bm{H}_{(t)}$ over the previous snapshot's embeddings. $\\bm{O}$ is optionally employed. (e) Two AEs respectively represent similarity matrices from the source and target domains into $\\bm{H}_{s}$ and $\\bm{H}_{t}$ where $\\bm{H}_{s}$ guides AE2's training and node embeddings of the target graph $\\bm{H}_{t}$ is outputted. (f) Each metapath is represented in an AE with the input feature vectors $\\{\\bm{x}_{i\\Phi}\\}$ denoting the meta proximity $\\bm{p}_{i} \\in \\bm{P}$ between node $v_{i}$ in the metapath $\\Phi$. All metapaths' embeddings are aggregated for $\\bm{H}_{v}$ in the fully connected layer in these stacked AEs. (g) Attributes in each view are input with $A$ into an AE where node embeddings in different views are optimized by the clustering loss.}\n\\label{fig-stacked-ae}\n\\end{figure*}\nThe proximity can capture underlying relationships within communities. However, sparsely connected networks in the real world do not provide enough edges. Attributes in networks cannot be measured by proximity. To address these limitations, Proximity Generative Adversarial Network (ProGAN)  encodes each node's proximity from a set of instantiated triplets, so that community relationships are discovered and preserved in a low-dimensional space.\nCommunity Detection with Generative Adversarial Nets (CommunityGAN)  is proposed for overlapping communities, which obtains node representation by assigning each node-community pair a nonnegative factor. Its objective function optimizes through a motif-level generator $\\phi_g(\\cdot|v_i;\\Theta_g)$ and discriminator $\\phi_d(\\cdot,\\Theta_d)$: \n\\begin{equation}\n\\small\n\\begin{aligned}\n\\min_{\\Theta_g} \\max_{\\Theta_d} & {\\mathcal{L}(\\phi_g, \\phi_d)} =\n\\sum_i (\\mathbb{E}_{C' \\sim p_{true} (\\cdot |v_{i})} \\left[ \\log \\phi_d (C'; \\Theta_d) \\right] \\\\\n& + \\mathbb{E}_{V' \\sim \\phi_g(V' |v_i ; \\Theta_g)} \\left[ \\log (1-\\phi_d(V'; \\Theta_d) \\right] ), \n\\end{aligned}\n\\end{equation}\nwhere $\\Theta_g$ and $\\Theta_d$ unifies all nonnegative representation vector of node $v_i$ in the generator and the discriminator, $V'\\subseteq V$ denotes a node subset, $C'$ represents motifs (\\textit{i.e.}, cliques), and conditional probability $p_{true}(C'| v_i)$ describes the preference distribution of $C'$ covering $v_i$ over all other motifs $C'\\in \\mathcal{C'}$. \nTo capture the global structural information, Community-Aware Network Embedding (CANE)  integrates a community detection model, \\textit{i.e.} Latent Dirichlet Allocation (LDA), into the adversarial node representation process: \n\\begin{equation}\n\\small\n\\begin{aligned}\n\\min_{\\Theta_g} & \\max_{\\Theta_d}  {\\mathcal{L}(\\phi_g, \\phi_d)} = \\\\\n\\sum_i & (\\mathbb{E}_{v \\sim p_{true} (v |v_{i})} \\left[ \\log \\phi_{d} (v, v_{i}; \\Theta_d) + P_{\\phi}(v |v_{i}) \\right] \\\\\n& + \\mathbb{E}_{v \\sim \\phi_{g}(v |v_{i} ; \\Theta_g)} \\left[ \\log (1-\\phi_{d}(v, v_{i}; \\Theta_d) - P_{\\phi}(v |v_{i}) \\right] ), \n\\end{aligned}\n\\end{equation}\nwhere $P_{\\phi}(v |v_{i})$ indicates the community similarity between a node $v_{i}$ and the node sample $v$, $\\phi_{g}(v |v_{i} ; \\Theta_g)$ outputs the node connectivity distribution. In turn, the representations characterize the community property.\nNetwork Embedding and overlapping Community detection with Adversarial learning (ACNE)  further generates negative communities as fake samples to learn distinguished community representation. ACNE utilizes walking strategy with perception to assist sampling of overlapping communities for nodes and exploits the discriminator to jointly map the node and community membership embeddings, so as to preserve the correlation of node-community in representations.", "cites": [8252], "cite_extract_rate": 0.125, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section describes several GAN-based community detection methods (SEAL, DR-GCN, JANE, and ProGAN) with technical formulations, but lacks a deeper synthesis of their contributions or connections between them. It briefly mentions limitations like overfitting and imbalance but does not critically evaluate the trade-offs or effectiveness of the proposed solutions. The abstraction remains limited to the specific models, without identifying overarching principles or patterns in GAN-based approaches."}}
{"id": "f92b593a-590a-473e-b8c3-679f6d000e8a", "title": "Stacked AE-based Community Detection", "level": "subsection", "subsections": [], "parent_id": "f7678c3c-af53-43f0-901b-6e1d476063be", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Autoencoder-based Community Detection"], ["subsection", "Stacked AE-based Community Detection"]], "content": "\\label{sec-stacked-ae}\nCompared to a single AE, a group of stacked AEs developed in deep hidden layers can better embed high-dimensional community features with representing each encoder in the stack individually representing one type of input data. In this architecture, stacked AEs represent multi-level and dynamic information to flexibly support wide community detection implementations .\nSemi-supervised Nonlinear Reconstruction Algorithm with DNN (semi-DRN)  designs a stacked AE into a community detection model, where a modularity matrix learns the nonlinear node representations in AEs and $k$-means obtains final community structure. Given an edge between nodes $v_i$ and $v_j$ in the adjacency matrix $\\bm{A}=[a_{ij}]$, the modularity  $b_{ij} = a_{ij} - \\frac{k_i k_j}{2m}$ in the modularity matrix $\\bm{B}$ is optimized for maximization. Encoding the node pairwise similarities (community membership) based on node representations, a pairwise constraint matrix $\\bm{O} = [o_{i,j}\\in \\{0,1\\}]$ is, meanwhile, defined to provide a prior knowledge that nodes $v_i$ and $v_j$ belong to the same community ($o_{i,j} = 1$) or not ($o_{i,j} = 0$). Thus, semi-DRN is optimized by minimizing the loss function:\n\\begin{equation}\n\\small\n\\mathcal{L} = \\mathcal{L}(\\bm{B}, \\bm{\\hat{B}}) + \\lambda \\mathcal{L}(\\bm{O},\\bm{H}), \n\\end{equation}\nwhere $\\bm{\\hat{B}}$ represents the reconstructed $\\bm{B}$ over stacked representations by a series of AEs, $\\lambda$ denotes an adjusting weight between the AE reconstruction loss $\\mathcal{L}(\\bm{B}, \\bm{\\hat{B}})$ and the pairwise constraints $\\mathcal{L}(\\bm{O},\\bm{H})$, and $\\mathcal{L}(\\bm{O},\\bm{H})$ measures each pair of community membership $o_{ij}$ and latent representations $(\\bm{h}_i,\\bm{h}_j)$ within stacked AEs. \nSimilarly, Deep Network Embedding with Structural Balance Preservation (DNE-SBP)  incorporates the adjusting weight on pairwise constraints for signed networks so that the stacked AE clusters the closest nodes distinguished by positive and negative connections. Unified Weight-free Multi-component Network Embedding (UWMNE) and its variant with Local Enhancement (WMCNE-LE)  preserve community properties from network topology and semantic information and integrate the diverse information in deep AE from a local network structure perspective.\nTo discover $t$ time-varying community structure, Semi-supervised Evolutionary Autoencoder (sE-Autoencoder)  is developed within an evolutionary clustering framework, assuming community structures at previous time steps successively guide the detection at the current time step. To this end, sE-Autoencoder adds a temporal smoothness regularization $\\mathcal{L}(\\bm{H}_{(t)}, \\bm{H}_{(t-1)})$ for minimization  in:\n\\begin{equation}\n\\small\n\\mathcal{L} = \\mathcal{L}(\\bm{S}_{(t)}, \\bm{\\hat{S}}_{(t)})+ \\lambda \\mathcal{L}(\\bm{O}, \\bm{H}_{(t)}) + (1-\\lambda) \\mathcal{L}(\\bm{H}_{(t)}, \\bm{H}_{(t-1)}),\n\\end{equation}\nwhere reconstruction error $\\mathcal{L}(\\bm{S}_{(t)}, \\bm{\\hat{S}}_{(t)})$ minimizes the loss between the similarity matrix $\\bm{S}_{(t)}$ and reconstructed matrix $\\bm{\\hat{S}}_{(t)}$ at the time step $t$, and the parameter $\\lambda$ controls the node pairwise constraint $\\mathcal{L}(\\bm{O},\\bm{H}_{(t)})$ and the temporal smoothness regularization over time $t$-th graph representation $\\bm{H}_{(t)}$.\nTo attributed networks, Deep Attributed Network Embedding (DANE)  develops a two-branch AE framework: one branch maps the highly nonlinear network structure to the low-dimensional feature space, and the other collaboratively learns node attributes. As similar nodes are more likely to be clustered in the same community, DANE measures these similarities by a series of proximity regarding network topological and attribute information in the representation learning, where optimizations are applied on reconstruction losses at first-order proximity $\\mathcal{L}_f$, higher-order proximity $\\mathcal{L}_h$ and semantic proximity $\\mathcal{L}_s$, and a negative log likelihood control $\\mathcal{L}_c$ for a consistent and complementary representation.\nTo solve the problem of shifted distributions, imbalanced features or a lack of data, Transfer Learning-inspired Community Detection with Deep Transitive Autoencoder (Transfer-CDDTA)  uses the transfer learning framework and modifies it to the unsupervised community detection task. On the source and target domains, Transfer-CDDTA balances stacked AEs via KL divergence while learning node embeddings. Aiming to map community information into one smooth feature space, CDDTA separates the input adjacency matrix $\\bm{A}$ into a source domain $s$ and a target domain $t$ by similarity matrices $\\bm{S}_s$ and $\\bm{S}_t$ to keep node pairwise similarity values for each stacked AE. Transfer-CDDTA then incorporates domain-independent features into the following minimization: \n\\begin{equation}\n\\small\n\\mathcal{L} = \\mathcal{L}_s(\\bm{S}_s, \\bm{\\hat{S}}_s) + \\mathcal{L}_t(\\bm{S}_t, \\bm{\\hat{S}}_t) + \\alpha KL(\\bm{H}_s,\\bm{H}_t)+\\beta\\mathcal{L}(\\Theta; \\gamma),\n\\end{equation}\nwhere $\\alpha$, $\\beta$, $\\gamma$ are trade-off parameters, $\\mathcal{L}_s$ and $\\mathcal{L}_t$ denote reconstruction losses of source and target domains, $KL$ smooths the KL divergence on encoded features ($\\bm{H}_s$, $\\bm{H}_t$) across two domains, and $\\mathcal{L}(\\Theta)$ is a regularization term on trainable variables to reduce overfitting in the optimization. \nDeep alIgned autoencoder-based eMbEdding (DIME)  stacks AEs for the multiple aligned structures in heterogeneous social networks. It employs metapaths to represent different relations (heterogeneous links denoting $\\mathcal{A}_{ij}$ which represents anchor links between multiple aligned network $\\mathcal{G}_i$ and $\\mathcal{G}_j$) and various attribute information $\\mathcal{X}=\\{\\bm{X}_i\\}$. Correspondingly, a set of meta proximity measurements are developed for each meta path and embed the close nodes to a close area in the low-dimensional latent feature space. The relatively close region reflects communities going to be detected.", "cites": [8451, 6532], "cite_extract_rate": 0.2222222222222222, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates several deep autoencoder-based methods for community detection and links them through a common framework, showing how they use stacked AEs to encode community structures. It provides a coherent narrative by highlighting the use of pairwise constraints and temporal smoothing. However, it lacks deeper critical evaluation of the methods' limitations or trade-offs and does not elevate the discussion to a more meta-level abstraction or comparative framework."}}
{"id": "53adba29-dca8-44b7-bf5a-62e9305c7887", "title": "Denoising AE-based Community Detection", "level": "subsection", "subsections": [], "parent_id": "f7678c3c-af53-43f0-901b-6e1d476063be", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Autoencoder-based Community Detection"], ["subsection", "Denoising AE-based Community Detection"]], "content": "\\label{sec-denoise-ae}\nThe denoising process, subtracting noises within DNN layers, can effectively enhance models' robustness. Denoising AEs  minimize the reconstruction loss between the input $\\bm{x}$ and AE features upon corrupt $\\widetilde{\\bm{x}}$:\n\\begin{equation}\n\\small\n\\mathcal{L}(\\bm{x}, \\phi_r(\\phi_e(\\widetilde{\\bm{x}}))). \n\\label{eq-ae-denonising}\n\\end{equation}\nDeep Neural Networks for Graph Representation (DNGR)  applies stacked denoising encoder to increase the robustness of capturing the local structural information when detecting communities. Specifically, it generates a probabilistic co-occurrence matrix and a shifted positive pointwise MI matrix by randomly walking over communities. A Deep Neural network-based Clustering-oriented network  network embedding (DNC)  is the extension of DNGR which joint learns node embeddings and cluster assignments.\nTo corrupted node attributes, Graph Clustering with dynamic Embedding (GRACE)  focuses on the dynamically changing inter-community activities via a self-training clustering guided by the influence propagation within neighborhoods.\nMarginalized Graph AutoEncoder (MGAE)  denoises both graph attributes and structures to improve community detection through the marginalization process. It obtains the corrupted attributes $\\widetilde{\\bm{X}}$ in $\\eta$ times by randomly removing some attributes. The objective function in MGAE training is: \n\\begin{equation}\n\\small\n\\mathcal{L} = \\frac{1}{\\eta}\\sum\\nolimits_{i=1}^{\\eta}\\|\\bm{X}-\\widetilde{\\bm{D}}^{-\\frac{1}{2}} \\widetilde{\\bm{A}} \\widetilde{\\bm{D}}^{-\\frac{1}{2}} \\widetilde{\\bm{X}} \\bm{W}\\|_2+\\lambda \\mathcal{L}(\\bm{W}), \n\\end{equation}\nwhere $\\mathcal{L}(\\bm{W})$ denotes a regularization term on AE's parameters $\\bm{W}$ with a coeffcient $\\lambda$.", "cites": [6533], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of denoising autoencoder-based community detection methods, mentioning key approaches such as DNGR, DNC, GRACE, and MGAE. It shows minimal synthesis by briefly connecting these methods under the theme of denoising but lacks a deeper integration into a coherent framework. There is little to no critical analysis or evaluation of limitations, and abstraction is limited to reiterating specific techniques rather than revealing overarching principles or trends."}}
{"id": "5af7250c-ed0b-4641-b270-0ffd8a8e2f75", "title": "Graph Convolutional AE-based Community Detection", "level": "subsection", "subsections": [], "parent_id": "f7678c3c-af53-43f0-901b-6e1d476063be", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Autoencoder-based Community Detection"], ["subsection", "Graph Convolutional AE-based Community Detection"]], "content": "\\label{sec-gcn-ae}\nIt is a great success to introduce GCNs into AEs: GCNs provide by neighborhood aggregation over community representations, and AEs alleviate the over-smoothing issue in GCNs to clarify community boundaries. For example, Structural Deep Clustering Network (SDCN)  designs a delivery operator to connect AE and GCN in DNN layers for the structure-aware representations. When SDCN integrates the structural information into deep clustering, it updates communities by applying a dual self-supervised optimization in backpropagation.\nGCN-based approach for Unsupervised Community Detection (GUCD)  employs the semi-supervised MRF  as the encoder in the convolutional layer and proposes a community-centric dual decoder to identify communities in attributed networks. The dual decoder reconstructs the network topology and node attributes. \nOne2Multi Graph Autoencoder for Multi-view Graph Clustering (O2MAC)  deals with the multi-view graph by dividing it into multiple one-view graphs and assigning each with an AE, named One2Multi. In the encoder, a GCN is applied to embed a set of view-separated graphs. Decoders respectively for one-view graphs select the most informative graph to represent the multi-view graph. O2MAC significantly captures shared features among multi-view graphs and improves clustering results through a self-training optimization.", "cites": [6540], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from multiple papers by highlighting how GCNs are integrated with AEs to improve community detection, particularly in handling structure-aware representations and multi-view graphs. It provides a coherent narrative by identifying common strategies such as dual decoders and self-training optimization. While it includes some analytical elements, such as the role of AE in mitigating over-smoothing, it lacks deeper critical evaluation or comparison of limitations among the approaches."}}
{"id": "8964ba9d-2242-4352-9768-0c6750ed12bb", "title": "Graph Attention AE-based Community Detection", "level": "subsection", "subsections": [], "parent_id": "f7678c3c-af53-43f0-901b-6e1d476063be", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Autoencoder-based Community Detection"], ["subsection", "Graph Attention AE-based Community Detection"]], "content": "\\label{sec-gat-ae}\nInstead of integrating GCNs, this community detection category applies GATs into AEs, in which a GAT is employed as the encoder to rank the importance of nodes within a neighborhood. For example, Deep Attentional Embedded Graph Clustering (DAEGC)  exploits high-order neighbors to cluster communities under self-training. Graph Embedding Clustering with Cluster-Specificity Distribution (GEC-CSD)  utilizes graph attention AE as a generator to learn the distinguished community representations in the framework of adversarial learning integrating self-training, where discriminator can make sure the diversity of cluster distributions.\nConsidering multi-view attributes in networks, Multi-View Attribute Graph Convolution Networks (MAGCN)  designs a two-pathway encoder: the first pathway encodes with a multi-view attribute GAT capable of denoising, and the second pathway develops a encoder to obtain consistent embeddings over multi-view attributes. Thus, noises and distribution variances are removed for community detection. Self-supervised Graph Convolutional network for Multi-view Clustering (SGCMC)  is developed from MAGCN through sharing a coefficient matrix in different views. \nDeep Multi-Graph Clustering (DMGC)  introduces AEs to represent each graph with an attention coefficient that node embeddings of multiple graphs cluster cross-graph centroids to obtain communities on Cauthy distribution.", "cites": [9045], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section primarily describes various autoencoder-based community detection methods incorporating graph attention mechanisms, such as DAEGC, GEC-CSD, MAGCN, SGCMC, and DMGC. It provides a basic synthesis by grouping these methods under the GAT-AE category and mentioning their shared features like self-training and adversarial learning. However, it lacks deeper critical evaluation or abstraction into broader principles, offering more of a factual summary than a nuanced analysis."}}
{"id": "3357802f-2052-4b34-84bc-f5cb0f4bec9a", "title": "VAE-based Community Detection", "level": "subsection", "subsections": [], "parent_id": "f7678c3c-af53-43f0-901b-6e1d476063be", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Autoencoder-based Community Detection"], ["subsection", "VAE-based Community Detection"]], "content": "\\label{sec-model-vae}\nVariational AutoEncoder (VAE) is an extension of AEs based on variational inference (\\textit{e.g.}, mean and covariance of features) . It is first introduced into the graph learning field by Variational Graph AutoEncoder (VGAE)  assuming prior distribution and applying GCN as the encoder, through which the learned representation $\\bm{Z}$ is a latent distribution. Community detection based on VAEs is activated by algorithms such as SBM, to fast inference community memberships in node representations . The inference process considers the uncertainty of the network , \\textit{e.g.}, community contradiction between neighbors of boundary nodes connecting multiple communities. VAEs are also required to handle sparsity in detecting communities. Meanwhile, VAEs easily incorporate deeper nonlinear relationship information. For example, Triad (Variational) Graph Autoencoder (TGA/TVGA)  replaces VAE/VGAE's decoder with a new triad decoder, which describes a real-world existing triadic closure property in communities.\nVariational Graph Embedding and Clustering with Laplacian Eigenmaps (VGECLE)  divides the graph representation into mean and covariance while generatively detecting communities, indicating each node's uncertainty of implicit relationships to its actual geographic position. With a Mixture-of-Gaussian prior and a Teacher-Student (T-S) as regularization, VGECLE aims to let the node $v_i$ (student) learn a distribution close to its neighbors' (teacher).\nDeep Generative Latent Feature Relational Model (DGLFRM)  and Ladder Gamma Variational Autoencoder for Graphs (LGVG)  further capture the community membership strength on each node. DGLFRM models the sparse node embeddings by a Beta-Bernoulli process which can detect overlapping communities and infer the number of communities. LGVG is devised to learn the multi-layered and gamma-distributed embeddings so that it detects communities from fine-grained and coarse-grained granularities.\nTo capture the higher-order structural features of communities, Variational Graph Autoencoder for Community Detection (VGAECD)  employs a Gaussian Mixture Model (GMM) to generalize the network generation process via bringing in a parameter of community assignments. VGAECD defines the joint probability of generative process, it can be detailed as: \\begin{equation}\n\\small\np(\\bm{a}, \\bm{z}, c) = p(\\bm{a} |\\bm{z}) p(\\bm{z} |c) p(c),\n\\end{equation} \nwhere $p(c) = \\pi_{c}$ denotes the prior probability of community $C_c$, $p(\\bm{z} |c)$ is calculated from the Gaussain distribution corresponding to community $C_c$ with the mean $\\mu_c$ and the standard deviation ${\\sigma}_c$ from a GCN layer (\\textit{i.e.}, $\\bm{z} \\sim \\mathcal{N}(\\mu_c, {\\sigma}_c^2 \\bm{I})$), and $\\bm{a}$ with the posterior (reconstruction) probability $p(\\bm{a} |\\bm{z})$ is sampled from the multivariate Bernoulli distribution parametrized by $\\mu_x$ that is learned from the decoder on $\\bm{z}$. The variational distribution can be obtained by the overall loss function as: \n\\begin{equation}\n\\small\n\\mathcal{L}_{\\mathrm{ELBO}}(\\bm{x}) = \\mathbb{E}_{q(\\bm{z} | \\bm{x}, \\bm{a})}[\\log p(\\bm{x},\\bm{a} | \\bm{z})] - KL[q(c | \\bm{x}) \\| p(c | \\bm{z})],\n\\end{equation}\nwhere the first term calculates reconstruction loss, and $KL$ divergence minimizes the clustering loss. The evidence lower bound (ELBO) is maximized to optimize the function. \nThe training progress of VGAECD reveals a favoured treatment for minimizing the reconstruction loss over the community loss, which leads to a sub-optimal community detection result. To resolve it, Optimizing Variational Graph Autoencoder for Community Detection (VGAECD-OPT)  proposes a dual optimization to learn the community-aware latent representation, the ELBO maximization becomes:\n\\begin{equation}\n\\small\n\\mathcal{L}_{\\mathrm{ELBO}}(\\bm{a}) = \\mathbb{E}_{q(\\bm{z}, c | \\bm{a})}[\\log p(\\bm{a} | \\bm{z})] - KL[q(\\bm{z}, c | \\bm{a}) \\| p(\\bm{z}, c)].\n\\end{equation}\nTo the sparsity and noises in the real world, Adversarially Regularized Variational Graph AutoEncoder (ARVGA)  embeds the latent representations $\\bm{Z}$ based on the prior distribution $p_{z}$ under the adversarial AE framework. $\\bm{Z}$ is represented by a variational graph encoder aiming at a robust community detection. Meanwhile, a special discriminator $\\phi_d$ is designed to decide adversarially on $p_{z}$ guided embeddings $\\bm{Z}$ and encodings on real samples $\\phi_g(\\bm{X}, \\bm{A})$:\n\\begin{equation}\n\\small\n\\begin{aligned}\n\\min_{\\Theta_g}\\max_{\\Theta_d} & \\mathbb{E}_{\\bm{z} \\sim p_{z}}[\\log \\phi_d(\\bm{Z})]\\\\\n& +\\mathbb{E}_{\\bm{x} \\sim p(\\bm{x})}[\\log (1-\\phi_d(\\phi_g(\\bm{X}, \\bm{A})))].\n\\end{aligned} \n\\end{equation}", "cites": [6541, 229, 232, 5680, 6542], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple VAE-based community detection approaches, linking concepts such as generative processes, latent distributions, and decoders to form a coherent narrative. It demonstrates critical analysis by pointing out limitations, such as the sub-optimal community detection in VGAECD due to the imbalance between reconstruction and community loss. The discussion abstracts over specific models to highlight broader trends, such as the handling of uncertainty, overlapping communities, and adversarial regularization for robustness."}}
{"id": "7d19d960-ac8c-41ed-bc4f-758fba8f7e5d", "title": "Deep Sparse Filtering-based Community Detection", "level": "section", "subsections": [], "parent_id": "ca169d78-5c5d-483c-9bd5-7ee46d40e336", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Deep Sparse Filtering-based Community Detection"]], "content": "\\label{sec-model-filtering} \nSparse Filtering (SF) is a two-layer learning model capable of handling high-dimensional graph data. The high sparse input $\\bm{A}$ with many $0$ elements is represented into lower-dimensional feature vectors $\\bm{h}_i$ with non-zero values. To explore deeper information such as community membership, Deep SF (DSF) stacks multiple hidden layers to finetune hyperparameters $\\Theta$ and extensively smooth data distributions $Pr(\\bm{h}_i)$.\nAs a representative method, Community Discovery based on Deep Sparse Filtering (DSFCD)  is developed on three phases: network representation, community feature mapping and community discovery. The network representation phase performs on the adjacency matrix $\\bm{A}$, modularity matrix $\\bm{B}$ and two similarity matrices $\\bm{S}$ and $\\bm{S}'$, respectively. The best representation is selected to input into the DSF for community feature mapping represented on each node $\\bm{h}_i$. Meanwhile, $\\bm{h}_i$ preserves the node similarity in the original network $\\bm{A}$ and latent community membership features. The node similarity is modeled in the loss function:\n\\begin{equation} \n\\small\n\\mathcal{L} = \\sum\\nolimits_i \\| \\bm{h}_{i}\\|_1 + \\lambda \\sum\\nolimits_i KL(\\bm{h}_i, \\bm{h}^*_j),\n\\end{equation}\nwhere $\\|\\cdot\\|_1$ is the $L_1$ norm penalty to optimize sparseness, $\\bm{h}^*_j$ denotes the representation of the most similar node to $v_{i}$ by measuring distances through KL divergence. When the learning process is optimized on the minimized loss, similar nodes are clustered into communities. The DSF architecture is investigated to be significant in experiments on real-world data sets. DSFCD discovers communities in higher accuracy than SF.\n\\begin{table*}[!t]\n\\centering \\footnotesize\n\\renewcommand\\arraystretch{1}\n\\setlength{\\tabcolsep}{1.5mm}\n\\caption{Summary of commonly used evaluation metrics in community detection.}\n\\begin{tabular}{lccl} \n\\toprule[1pt]\n\\textbf{Metrics} & \\textbf{Overlap} & \\textbf{Ground Truth} & \\textbf{Publications} \\\\ \\midrule\n\\rowcolor{gray!30} \n&  &  & , \\\\\n\\rowcolor{gray!30} \n\\multirow{-2}{*}{ACC} & \\multirow{-2}{*}{$\\checkmark$} & \\multirow{-2}{*}{Yes} &  \\\\\n& &  & , \\\\\n\\multirow{-2}{*}{NMI} & \\multirow{-2}{*}{$\\times$} & Yes &  \\\\\nOverlapping-NMI & $\\checkmark$ &  &  \\\\\n\\rowcolor{gray!30} \nPrecision & $\\checkmark$ & Yes &  \\\\\nRecall & $\\checkmark$ & Yes &  \\\\\n\\rowcolor{gray!30} \nF1-Score & $\\checkmark$ & Yes &   \\\\\nARI & $\\checkmark$ & Yes &  \\\\\n\\rowcolor{gray!30} \n\\textit{Q} & $\\times$ &  &  \\\\ \n\\rowcolor{gray!30}\nExtended-\\textit{Q} & $\\checkmark$ & \\multirow{-2}{*}{No} &  \\\\\nJaccard & $\\checkmark$ & Yes &  \\\\\n\\rowcolor{gray!30} \nCON & $\\checkmark$ & No &  \\\\\nTPR & $\\checkmark$ & No  &  \\\\ \n\\bottomrule[1pt]\n\\multicolumn{4}{l}{\\makecell[l]{``Overlap'' indicates whether the metric can evaluate the overlapping community that is defined in Section \\ref{sec-preliminaries}. \\\\\n``Ground Truth'' indicates whether the actual community structure of the network is required. }}\\\\\n\\end{tabular}\n\\label{tab-metrics}\n\\end{table*}\n\\begin{table*}[!t]\n\\centering \\footnotesize\n\\renewcommand\\arraystretch{1}\n\\setlength{\\tabcolsep}{5mm}\n\\caption{Summary of open-source implementations.}\n\\begin{tabular}{ccll}\n\\toprule[1 pt]\n\\textbf{Category} & \\textbf{Tool} & \\textbf{Method} & \\textbf{URL} \\\\ \\midrule\n\\rowcolor{gray!30} \n\\cellcolor{white!100} & \\cellcolor{white!100} & AGE & \\url{https://github.com/thunlp/AGE} \\\\\n\\cellcolor{white!100} & \\cellcolor{white!100} & LGNN & \\url{https://github.com/zhengdao-chen/GNN4CD} \\\\\n\\rowcolor{gray!30} \n\\cellcolor{white!100} & \\cellcolor{white!100} \\multirow{-3}{*}{PyTorch} & NOCD & \\url{https://github.com/shchur/overlapping-community-detection} \\\\ \\cmidrule{2-4}\n\\cellcolor{white!100} & \\cellcolor{white!100} & AGC & \\url{https://github.com/karenlatong/AGC-master} \\\\\n\\rowcolor{gray!30} \n\\cellcolor{white!100} \\multirow{-5}{*}{GCN} & \\cellcolor{white!100} \\multirow{-2}{*}{TensorFlow} & CayleyNet &  \\url{https://github.com/amoliu/CayleyNet} \\\\ \\midrule\n\\cellcolor{white!100} & \\cellcolor{white!100} & CP-GNN  & \\url{https://github.com/RManLuo/CP-GNN} \\\\\n\\rowcolor{gray!30} \n\\cellcolor{white!100} & \\cellcolor{white!100} & DMGI & \\url{https://github.com/pcy1302/DMGI} \\\\\n\\cellcolor{white!100} & \\cellcolor{white!100} &  MAGNN & \\url{https://github.com/cynricfu/MAGNN} \\\\ \n\\rowcolor{gray!30} \n\\cellcolor{white!100} & \\cellcolor{white!100} & HDMI  & \\url{https://github.com/baoyujing/HDMI} \\\\\n\\cellcolor{white!100} \\multirow{-5}{*}{GAT} & \\cellcolor{white!100} \\multirow{-5}{*}{PyTorch} &\nHeCo  & \\url{https://github.com/liun-online/HeCo} \\\\\n\\midrule\n\\rowcolor{gray!30} \n\\cellcolor{white!100} & \\cellcolor{white!100} PyTorch & SEAL & \\url{https://github.com/yzhang1918/kdd2020seal} \\\\ \\cmidrule{2-4}\n\\cellcolor{white!100} \\multirow{-2}{*}{GAN} & \\cellcolor{white!100} TensorFlow & CommunityGAN & \\url{https://github.com/SamJia/CommunityGAN} \\\\ \\midrule\n\\rowcolor{gray!30} \n\\cellcolor{white!100} & \\cellcolor{white!100} & DANE & \\url{https://github.com/gaoghc/DANE} \\\\\n\\cellcolor{white!100} & \\cellcolor{white!100} \\multirow{-2}{*}{TensorFlow} & DIME & \\url{http://www.ifmlab.org/files/code/Aligned-Autoencoder.zip} \\\\ \\cmidrule{2-4}\n\\rowcolor{gray!30} \n\\cellcolor{white!100} & \\cellcolor{white!100} &  & \\href{https://github.com/shenxiaocam/Deep-network-embedding-for-graph-representation-learning-in-signed-networks}{https://github.com/shenxiaocam/Deep-network-embedding-for-} \\\\\n\\rowcolor{gray!30} \n\\cellcolor{white!100} & \\cellcolor{white!100} & \\multirow{-2}{*}{DNE-SBP} & \\href{https://github.com/shenxiaocam/Deep-network-embedding-for-graph-representation-learning-in-signed-networks}{graph-representation-learning-in-signed-networks} \\\\\n\\cellcolor{white!100} \\multirow{-5}{*}{Stacked AE} & \\cellcolor{white!100} \\multirow{-3}{*}{Matlab} & semi-DRN & \\url{http://yangliang.github.io/code/DC.zip} \\\\ \\midrule\n\\rowcolor{gray!30} \n\\cellcolor{white!100} Sparse AE & \\cellcolor{white!100} PyTorch & GraphEncoder & \\url{https://github.com/zepx/graphencoder} \\\\ \\midrule\n\\cellcolor{white!100} & \\cellcolor{white!100} Keras & DNGR & \\url{https://github.com/MdAsifKhan/DNGR-Keras} \\\\ \\cmidrule{2-4}\n\\rowcolor{gray!30} \n\\cellcolor{white!100} \\multirow{-2}{*}{\\makecell{Denoising\\\\AE}} & \\cellcolor{white!100} Matlab & MGAE & \\url{https://github.com/FakeTibbers/MGAE} \\\\ \\midrule\n\\cellcolor{white!100} & \\cellcolor{white!100} PyTorch & SDCN & \\url{https://github.com/bdy9527/SDCN} \\\\ \\cmidrule{2-4}\n\\rowcolor{gray!30} \n\\cellcolor{white!100} \\multirow{-2}{*}{\\makecell{Graph Convolutional\\\\AE}} & \\cellcolor{white!100} TensorFlow & O2MAC & \\url{https://github.com/songzuolong/WWW2020-O2MAC} \\\\ \\midrule\n\\cellcolor{white!100} Graph Attention & \\cellcolor{white!100} & DMGC & \\url{https://github.com/flyingdoog/DMGC} \\\\\n\\rowcolor{gray!30} \n\\cellcolor{white!100} AE & \\cellcolor{white!100} \\multirow{-2}{*}{TensorFlow} & SGCMC & \\url{https://github.com/xdweixia/SGCMC} \\\\\n\\midrule\n\\cellcolor{white!100} Variational & \\cellcolor{white!100} & ARGA/ARVGA & \\url{https://github.com/Ruiqi-Hu/ARGA} \\\\\n\\rowcolor{gray!30} \n\\cellcolor{white!100} AE & \\cellcolor{white!100} \\multirow{-2}{*}{TensorFlow} & DGLFRM & \\url{https://github.com/nikhil-dce/SBM-meet-GNN} \\\\\n\\bottomrule[1 pt]\n\\end{tabular}\n\\label{table-code}\n\\end{table*}", "cites": [6536, 6534, 6533, 6540, 6535, 7583, 9045, 9044, 6532, 6538, 220, 8564, 8252, 232, 6537, 6541, 1430, 6542], "cite_extract_rate": 0.29508196721311475, "origin_cites_number": 61, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of Deep Sparse Filtering-based community detection, particularly DSFCD, with some synthesis of the methodâ€™s phases and mathematical formulation. However, it lacks in-depth integration of cited papers and does not critically evaluate their limitations or contributions. The abstraction level is limited to method-level descriptions rather than broader patterns or principles in deep learning for community detection."}}
{"id": "6240ad5b-e538-43f1-be5c-7b164331a486", "title": "Data Sets", "level": "subsection", "subsections": [], "parent_id": "3e3360f7-fab9-4d5a-aba9-e2384614fc86", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Published Resources"], ["subsection", "Data Sets"]], "content": "\\label{sec-datasets}\nBoth real-world data sets and synthetic data sets are popularly published. Real-world data sets in community detection experiments are collected from real-world applications, which test performances of proposed methods from the real applicable aspect. Synthetic data sets are generated by specific models on manually designed rules, and particular functions can be tested by these data sets. The state-of-the-art popular real-world data sets can be categorized into citation/co-authorship networks, social networks (online and offline), webpage networks and product co-purchasing networks. The typical data sets covering various network shapes (\\textit{i.e.}, unattributed, attributed, multi-view, signed, heterogeneous and dynamic) are summarized in TABLE \\ref{table-rwdatasets}. The related description is detailed in APPENDIX \\ref{sec-ddds} (Real-world Data Sets). Girvan-Newman (GN) networks  and Lancichinettiâ€“Fortunatoâ€“Radicchi (LFR) networks  are two widely applied synthetic benchmarks, described in APPENDIX \\ref{sec-ddds} (Synthetic Benchmark Data Sets).", "cites": [8483, 8484], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a basic overview of real-world and synthetic data sets used in community detection, with minimal synthesis of the cited papers. It lists data types and refers to appendices for details without connecting the cited works to broader themes or trends. There is no critical evaluation or deep abstraction, as the focus remains primarily on factual description."}}
{"id": "5edfba2b-b98c-4fef-84a2-3f8beb52c691", "title": "Practical Applications", "level": "section", "subsections": [], "parent_id": "ca169d78-5c5d-483c-9bd5-7ee46d40e336", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Practical Applications"]], "content": "\\label{sec-application} \nCommunity detection has many applications across different tasks and domains, as summarized in Fig. \\ref{fig-application}. We now detail some typical applications in the following areas.\n\\vspace{2mm}\n\\noindent\n\\textbf{Recommendation Systems.} Community structure plays a vital role for graph-based recommendation systems , as the community members may have similar interests and preferences. By detecting relations between nodes (\\textit{i.e.}, users--users, items--tems, users--items), models such as CayleyNets  and UWMNE/WMCNE-LE  produce high-quality recommendations.\n\\vspace{2mm}\n\\noindent\n\\textbf{Biochemistry.} In this field, nodes represent proteins or atoms in compound and molecule graphs, while edges denote their interactions. Community detection can identify complexes of new proteins  and chemical compounds which are functional in regional organs (\\textit{e.g.}, in brain ) or pathogenic factor of a disease (\\textit{e.g.}, community-based lung cancer detection ). To various tumor types on genomic data sets, the previous study  shows relevances between communities' survival rates and distributions of tumor types over communities.\n\\vspace{2mm}\n\\noindent\n\\textbf{Online Social Networks.} \nAnalyzing online social activities can identify online communities and correlate them in the real world. The practice on online social networks  such as Twitter, LinkedIn and Facebook reveals similar interests among online users that individual preferences can be automatically provided. Meanwhile, community detection can be used for online privacy  and to identify \\textbf{criminals} based on online social behaviors , who support and diffuse criminal ideas and even who may practice terrorism activities .\n\\vspace{2mm}\n\\noindent\n\\textbf{Community Deception.} To hide from the community detection, community deception  covers a group of users in social networks such as Facebook. Deception is either harmful to virtual communities or harmless that provides justifiable benefits. From community-based structural entropy, Residual Entropy Minimization (REM) effectively nullify community detection algorithms . A systematic evaluation  on community detection robustness to deception is carried out in large networks.\n\\vspace{2mm}\n\\noindent\n\\textbf{Community Search.} Community search aims to queue up a series of nodes dependent on communities . For example, a user (node) search on interests (communities) after another user (node). To this end, communities are formed temporally based on the user's interest. There are several practices applied in this scenario. Local community search  assumes one query node at a time and expands the search space around it. The strategy is attempted repeatedly until the community finds all its members. Attributed Truss Communities (ATC)  interconnects communities on query nodes with similar query node attributes.\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=0.48\\textwidth]{figure/application}\n\\caption{\\footnotesize Practical applications of community detection.}\n\\label{fig-application}\n\\end{figure}", "cites": [8483, 220, 6544, 6543], "cite_extract_rate": 0.21052631578947367, "origin_cites_number": 19, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of community detection applications in various domains, referencing specific papers to support examples. It integrates information from cited works to highlight different use cases but lacks in-depth comparative or evaluative analysis. Some general patterns are identified (e.g., deception, search), yet the discussion remains at the level of individual applications without deeper abstraction or synthesis into a novel framework."}}
{"id": "fcb4fd16-9c00-46e0-ac37-4aa38eb59859", "title": "Community Embedding", "level": "subsection", "subsections": [], "parent_id": "2a034c24-34f6-44f9-b1ca-c3ccc5da62d7", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Future Directions"], ["subsection", "Community Embedding"]], "content": "Node embedding methods traditionally preserve nodes' neighborhood information, in which structural information of communities are ignored . To this end, designing community-aware learning processes to characterize community information can improve the accuracy of community detection .\n\\vspace{2mm}\n\\noindent\n\\textbf{Opportunities:} To date, few works integrate the community embedding into a deep learning model, so more efforts are desired for this promising area. In general, as community embedding that learns representations on each community increases computational cost, future work needs to develop fast computation-aimed algorithms. Furthermore, the optimization mechanism to hyperparameters in the community embedding needs a new design so that deep community detection can meet the expectation.", "cites": [218], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section identifies an emerging research areaâ€”community embeddingâ€”and highlights its potential for improving community detection. It synthesizes the idea from DeepWalk and points out limitations such as computational cost and hyperparameter optimization. However, the analysis remains somewhat general and lacks deeper connections to multiple works or a nuanced critique of existing methods."}}
{"id": "4bd1f908-439a-4412-96bc-7be8c660b6f1", "title": "Heterogeneous Networks", "level": "subsection", "subsections": [], "parent_id": "2a034c24-34f6-44f9-b1ca-c3ccc5da62d7", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Future Directions"], ["subsection", "Heterogeneous Networks"]], "content": "For an accurate depiction of reality, networks are required to involve heterogeneous information  that characterizes relationships between different types of entities, such as the role-play relation between actors and movies. Since community detection methods designed for homogeneous networks cannot be directly employed, due to lack of capacity to model the complex structural and semantic information, community detection research on heterogeneous networks is challenging. \n\\vspace{2mm}\n\\noindent\n\\textbf{Opportunities:} Metapath is a promising research effort to deal with diverse semantic information, which describes a composite relation between the node types involved. It allows deep models to represent nodes by aggregating information from the first-order structure with different metapaths and measuring node similarity to cluster communities . However, the selection of most meaningful metapaths remains an open problem. The future efforts should focus on a flexible schema for metapath selection and novel models that can exploit various types of relationships.", "cites": [1430, 1670, 248], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes the cited papers by highlighting the role of metapath-based approaches in modeling heterogeneous networks and connects them to broader challenges in community detection. It provides a critical perspective by identifying the open problem of metapath selection and suggests directions for improvement. The abstraction is strong, as it moves beyond individual methods to discuss general challenges and opportunities in handling semantic and structural diversity."}}
{"id": "6c5be384-6be4-45a2-bb5e-03514800aa41", "title": "Network Heterophily", "level": "subsection", "subsections": [], "parent_id": "2a034c24-34f6-44f9-b1ca-c3ccc5da62d7", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Future Directions"], ["subsection", "Network Heterophily"]], "content": "Network heterophily  can be interpreted as a phenomenon that connected nodes belong to different communities or characterized by dissimilar features. For example, fraudsters intentionally make a connection with normal users to hide from being discovered. For community detection, boundary nodes connected across communities comply with this property. It is significant to capture network heterophily providing valuable information on community division.\n\\vspace{2mm}\n\\noindent\n\\textbf{Opportunities:} Since most methods heavily rely on homophily, they assume connected nodes share more similarities and are more likely from the same community. Deep learning methods that exploit network heterophily are expected for better community detection performance.", "cites": [6545], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section introduces network heterophily and its relevance to community detection, referencing one key paper to highlight the limitations of current deep learning methods relying on homophily. While it provides a clear analytical direction by pointing out the potential of heterophily-aware models, it lacks deeper synthesis of multiple sources and broader comparative or evaluative analysis. The abstraction is moderate as it identifies the importance of heterophily as a general concept in the context of community detection."}}
{"id": "1ccc366b-1ab1-4727-934c-83b43aaf2628", "title": "Topologically Incomplete Networks", "level": "subsection", "subsections": [], "parent_id": "2a034c24-34f6-44f9-b1ca-c3ccc5da62d7", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Future Directions"], ["subsection", "Topologically Incomplete Networks"]], "content": "Relationships in real-world scenarios are not always available, which leads to incomplete network topology and isolated subgraphs . For example, protein-protein interaction (PPI) networks are usually incomplete since monitoring all protein-protein interactions are expensive . Deriving meaningful knowledge of communities from limited topology information has been crucial to this case.\n\\vspace{2mm}\n\\noindent\n\\textbf{Opportunities:} The requirement of complete network topology reduces the applicability of community detection methods, especially those based on neighborhood aggregations. To this end, deep learning methods should be further developed with an information recovery mechanism to achieve accurate community detection on TINs.", "cites": [9046], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section briefly addresses the challenge of topologically incomplete networks, citing a relevant paper on PPI networks, but it does not deeply synthesize or connect multiple sources. It offers a critical perspective by pointing out the limitation of current methods requiring complete topology, but lacks detailed evaluation or comparison of existing approaches. The abstraction is limited to a general problem statement and a suggestion for future research."}}
{"id": "ef16ad2f-ac9b-4a80-a0a7-e79b616afbae", "title": "Cross-domain Networks", "level": "subsection", "subsections": [], "parent_id": "2a034c24-34f6-44f9-b1ca-c3ccc5da62d7", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Future Directions"], ["subsection", "Cross-domain Networks"]], "content": "In the real world, there are many similar networks such as Facebook and Twitter. Each network has its independent domain. Borrowing knowledge from an information-rich domain benefits the network learning in the target domain . Therefore, community detection is encouraged to develop its own deep learning models to improve detection results .\n\\vspace{2mm}\n\\noindent\n\\textbf{Opportunities:} Through community adaptations, the latent representation transferring from the source to the target domain faces the following challenges: (1) a lack of explicit community structure, (2) node label shortages, (3) missing a community's ground truth, (4) poor representation performance caused by the inferior network structure, and (5) deep learning with small-scale networks. The future methods aim to measure cross-domain coefficiency, distribution shifts, and control computational costs.", "cites": [6546], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions the concept of cross-domain community detection and references a single paper, but it lacks substantial synthesis of ideas from multiple sources. It identifies some challenges but does not critically analyze or compare existing works in depth. The abstraction is limited to a few general challenges without revealing broader patterns or principles."}}
{"id": "3d8da433-ccb8-4e57-a0c2-8698b5afa1e7", "title": "Signed Networks", "level": "subsection", "subsections": [], "parent_id": "2a034c24-34f6-44f9-b1ca-c3ccc5da62d7", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Future Directions"], ["subsection", "Signed Networks"]], "content": "It has been increasingly noticed that not all occurring connections get entities close. Generally, friendship indicates the positive sentiment (\\textit{e.g.}, like and support), while foes express the negative attitude (\\textit{e.g.}, dislike). These distinctions are reflected on network edges, called signed edges and signed networks . As impacts of positive and negative ties are different, existing community detection methods working on unsigned networks are not applicable to signed networks. \n\\vspace{2mm}\n\\noindent\n\\textbf{Opportunities:} To conduct community detection in signed network, the main challenges lie in adapting negative ties. Deep learning techniques should be exploited to represent both ties. Negative ties offer distinguishing community knowledge in learning signed network embeddings . Future works are expected to cope with signed edges and further identify positive/negative information automatically on edges.", "cites": [6532], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of the key idea from the cited paper regarding the need for specialized methods in signed networks. It identifies the distinct nature of signed networks and highlights the potential of deep learning for embedding both positive and negative ties. However, it lacks deeper critical analysis or nuanced evaluation of existing approaches and primarily focuses on forward-looking suggestions without extensive comparative or meta-level insights."}}
{"id": "a813a846-cd6b-40c4-9f63-62ed1c11417e", "title": "Summarized Techniques of Deep Learning-based Community Detection Methods", "level": "section", "subsections": [], "parent_id": "ca169d78-5c5d-483c-9bd5-7ee46d40e336", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Summarized Techniques of Deep Learning-based Community Detection Methods"]], "content": "\\label{sec-stdlcd}\nTABLE \\ref{table-cnn}--\\ref{table-autoencoder} summarize the reviewed literature in sections \\ref{sec-model-cnn} (CNN-based Community Detection), \\ref{sec-model-gcn} (GCN-based Community Detection), \\ref{sec-model-gat} (GAT-based Community Detecion), \\ref{sec-model-gan} (GAN-based Community Detection) and \\ref{sec-model-autoencoder} (AE-based Community Detection), respectively. Regarding the ``Input'', different methods require different inputs, such as network topology $\\bm{A}$, node attributes $\\bm{X}$ and other preprocessed data. Regarding the availability of ground truth labels, methods vary in supervised, semi-supervised and unsupervised ``Learning''. Meanwhile, methods that are able to output overlapping communities are marked by ``$\\checkmark$'' in the ``Overlap'' column. Methods that can detect overlapping communities are also suitable to disjoint community detection. To other special requirements summarized in the tables, please refer to the corresponding sections.\n\\begin{table*}[h]\n\\centering \n\\footnotesize\n\\renewcommand\\arraystretch{1.1}\n\\setlength{\\tabcolsep}{4.5mm}\n\\caption{Summary of CNN-based community detection methods.}\n\\rowcolors{2}{white}{gray!30}\n\\begin{tabular}{lcccccc}\n\\toprule[1 pt]\n\\textbf{Method} & \\textbf{Input} & \\textbf{Learning} &  \\textbf{Preprocess} & \\textbf{Co-technique} & \\textbf{Overlap} & \\textbf{Network} \\\\ \n\\midrule\nXin \\textit{et al.}  & $\\bm{A}$ & Supervise & Node to image & -- & $\\times$ & TINs \\\\ \nSparseConv  & $\\bm{A}$ & Supervise & Node to image & Sparse matrix & $\\times$ & Sparse network \\\\\nSparseConv2D  & $\\bm{A}$ & Semi-supervise & Node to image & Sparse matrix & $\\times$ & Sparse network \\\\ \nComNet-R  & $\\bm{A}$ & Supervise & Edge to image & Local modularity & $\\times$ & Large-scale network \\\\\n\\bottomrule[1 pt]\n\\end{tabular}\n\\label{table-cnn}\n\\end{table*}\n\\begin{table*}[h]\n\\centering \n\\footnotesize\n\\renewcommand\\arraystretch{1.1}\n\\setlength{\\tabcolsep}{2.5mm}\n\\caption{Summary of GCN-based community detection methods.}\n\\rowcolors{2}{white}{gray!30}\n\\begin{tabular}{lcccccc}\n\\toprule[1 pt]\n\\textbf{Method} & \\textbf{Input} & \\textbf{Learning} & \\textbf{Convolution} & \\textbf{Clustering} & \\textbf{Co-technique} & \\textbf{Overlap} \\\\ \n\\midrule\nLGNN & $\\bm{A},\\bm{X}$ & Supervise & First-order + Line graph & -- & Edge features & $\\checkmark$ \\\\ \nMRFasGCN & $\\bm{A},\\bm{X}$ & Semi-supervise & First-order + Mean Field Approximate & -- & eMRF & $\\times$  \\\\\nSGCN & $\\bm{A},\\bm{X}$ & Unsupervise & First-order & -- & Label sampling & $\\times$ \\\\\nNOCD & $\\bm{A},\\bm{X}$ & Unsupervise & First-order & -- & Bernoulliâ€“Poisson & $\\checkmark$ \\\\  \nGCLN & $\\bm{A},\\bm{X}$ & Unsupervise & First-order & $k$-means & U-Net architecture & $\\times$ \\\\  \nIPGDN & $\\bm{A},\\bm{X}$ & Unsupervise & \\makecell[c]{First-order + Disentangled\\\\ representation} & $k$-means & HSIC as regularizer & $\\times$ \\\\\nAGC & $\\bm{A}, \\bm{X}$ & Unsupervise & $k$-order + Laplacian smoothing filter & Spectral Clustering & -- & $\\times$ \\\\  \nAGE & $\\bm{A},\\bm{X}$ & Unsupervise & Laplacian smoothing filter & Spectral Clustering & Adaptive learning & $\\times$ \\\\\nCayleyNet & $\\bm{A},\\bm{X}$ & Semi-supervise & Laplacian smoothing filter & -- & Cayley polynomial & $\\times$ \\\\ \nSENet & $\\bm{A},\\bm{X}$ & Unsupervise & Third-order + Spectral clustering loss & $k$-means & Kernel matrix learning & $\\times$ \\\\\nCommDGI & $\\bm{A},\\bm{X}$ & Unsupervise & First-order + Sampling & -- & Joint optimization & $\\times$ \\\\\nZhao \\textit{et al.} & $\\bm{A},\\bm{X}$ & Unsupervise & First-order + Sampling & -- & Joint optimization & $\\times$ \\\\\n\\bottomrule[1 pt]\n\\end{tabular}\n\\label{table-gcn}\n\\end{table*}\n\\begin{table*}[h]\n\\centering \\footnotesize\n\\renewcommand\\arraystretch{1.1}\n\\setlength{\\tabcolsep}{2.1mm}\n\\caption{Summary of GAT-based community detection methods.}\n\\rowcolors{2}{white}{gray!30}\n\\begin{tabular}{lcccccccc}\n\\toprule[1 pt]\n\\textbf{Method} & \\textbf{Input} & \\textbf{Metapath} & \\textbf{Learning} & \\textbf{Attention Mechanism} & \\textbf{Co-technique} & \\textbf{Clustering} & \\textbf{Overlap} & \\textbf{Network} \\\\ \\midrule\nDMGI & $V,E^{(r)},\\bm{X}$ & $\\times$ & Unsupervise &  & Contrastive learning & $k$-means & $\\times$ & Multiplex \\\\\nHDMI & $V,E^{(r)},\\bm{X}$ & $\\times$ & Unsupervise &  & MI & $k$-means & $\\times$ & Multiplex \\\\\nMAGNN & $\\mathcal{V},\\mathcal{E},\\mathcal{X}$ & $\\checkmark$ & Unsupervise &  & -- & $k$-means & $\\times$ & Heterogeneous\\\\\nHeCo & $\\mathcal{V},\\mathcal{E}$ & $\\checkmark$ & Unsupervise &  & Contrastive learning & $k$-means & $\\times$ & Heterogeneous \\\\ \nCP-GNN & $\\mathcal{V},\\mathcal{E}$ & $\\times$ & Unsupervise &  & -- & $k$-means & $\\times$ & Heterogeneous \\\\ \n\\bottomrule[1 pt]\n\\end{tabular}\n\\label{table-gat}\n\\end{table*}\n\\begin{table*}[h]\n\\centering \\footnotesize\n\\renewcommand\\arraystretch{1.1}\n\\setlength{\\tabcolsep}{3.5mm}\n\\caption{Summary of GAN-based community detection methods.} \n\\begin{tabular}{lccccccc}\n\\toprule[1 pt]\n\\textbf{Method} & \\textbf{Input} & \\textbf{Learning} & \\textbf{Generator} & \\textbf{Discriminator} & \\textbf{Generated Samples} & \\textbf{Clustering} & \\textbf{Overlap} \\\\ \n\\midrule\n\\rowcolor{gray!30}\nSEAL & $\\bm{A},\\bm{X}$ & Semi-supervise & iGPN & GINs & Communities & -- & $\\checkmark$ \\\\\nDR-GCN & $\\bm{A},\\bm{X}$ & Semi-supervise & MLP & MLP & Embeddings & $k$-means & $\\times$ \\\\\n\\rowcolor{gray!30}\n&  &  &  &  & Topology, attributes, &  &  \\\\\n\\rowcolor{gray!30}\n\\multirow{-2}{*}{JANE} & \\multirow{-2}{*}{$\\bm{A},\\bm{X}$} & \\multirow{-2}{*}{Unsupervise} & \\multirow{-2}{*}{Various} & \\multirow{-2}{*}{MLP} & embeddings & \\multirow{-2}{*}{--} & \\multirow{-2}{*}{$\\times$} \\\\\nProGAN & $\\bm{A},\\bm{X}$ & Unsupervise & MLP & MLP & Triplets & $k$-means & $\\times$ \\\\\n\\rowcolor{gray!30}\nCommunityGAN & $\\bm{A}$ &  Unsupervise & AGM & AGM & Motifs & -- & $\\checkmark$ \\\\\nCANE & $\\bm{A}$ & Unsupervise & Softmax & MLP & Node pairs & $k$-means & $\\times$ \\\\\n\\rowcolor{gray!30}\nACNE & $\\bm{A}$ & Unsupervise & Softmax & MLP & Nodes, Communities & -- & $\\checkmark$ \\\\\n\\bottomrule[1 pt]\n\\end{tabular}\n\\label{table-gan}\n\\end{table*}\n\\begin{table*}[h] \n\\centering \\footnotesize\n\\renewcommand\\arraystretch{1.1}\n\\setlength{\\tabcolsep}{1mm}\n\\caption{Summary of AE-based community detection methods.} \n\\begin{tabular}{clcccccc}\n\\toprule[1 pt]\n\\textbf{Category} & \\textbf{Method} & \\textbf{Input} & \\textbf{Learning} & \\textbf{Encoder} & \\textbf{Decoder} & \\textbf{Loss} & \\textbf{Overlap} \\\\ \\midrule\n\\rowcolor{gray!30}\n\\cellcolor{white} & semi-DNR & $\\bm{B}$ & Semi-supervise & MLP & MLP & reconstruction+pairwise & $\\times$ \\\\\n\\cellcolor{white} & DNE-SBP & $\\bm{A}(+,-)$ & Semi-supervise & MLP & MLP & reconstruction+regularization+pairwise & $\\times$ \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white} & UWMNE/WMCNE-LE & $\\bm{B},\\bm{X}$ & Unsupervise & MLP & MLP & reconstruction+pairwise & $\\times$ \\\\\n\\cellcolor{white} & sE-Autoencoder & $\\{\\bm{A}_t\\}$ & Semi-supervise & MLP & MLP & reconstruction+regularization+pairwise & $\\times$ \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white} & DANE & $\\bm{A}, \\bm{X}$ & Unsupervise & MLP & MLP & reconstruction+proximity & $\\times$ \\\\\n\\cellcolor{white} & Transfer-CDDTA & $\\bm{S}_s,\\bm{S}_t$ & Unsupervise & MLP & MLP & reconstruction+regularization+proximity & $\\times$ \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white} &  & $\\mathcal{V}$, $\\mathcal{E}$, &  &  &  & reconstruction+regularization &  \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white} \\multirow{-10}{*}{\\makecell[c]{Stacked\\\\AE}} & \\multirow{-2}{*}{DIME } & $\\mathcal{X}, \\{\\mathcal{A}_{ij}\\}$ & \\multirow{-2}{*}{Unsupervise} & \\multirow{-2}{*}{MLP} & \\multirow{-2}{*}{MLP} & +information fusion  & \\multirow{-2}{*}{$\\times$} \\\\ \n\\midrule  \n\\cellcolor{white} & GraphEncoder & $\\bm{A},\\bm{D}, \\bm{S}$ & Unsupervise & MLP & MLP & reconstruction+regularization+sparsity & $\\times$ \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white} & WCD & $\\bm{S}$ & Unsupervise & MLP & MLP & reconstruction+sparsity & $\\times$ \\\\\n\\cellcolor{white} & DFuzzy & $\\bm{A}$ & Unsupervise & MLP & MLP & reconstruction+sparsity & $\\checkmark$ \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white} \\multirow{-5}{*}{\\makecell[c]{Sparse\\\\AE}} & CDMEC & $\\bm{S}_s,\\bm{S}_t$ & Unsupervise & MLP & MLP & reconstruction+sparsity & $\\times$ \\\\ \\midrule\n\\cellcolor{white} & DNGR & $\\bm{A}$ & Unsupervise & MLP & MLP & reconstruction & $\\times$ \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white} & DNC & $\\bm{A}$ & Unsupervise & MLP & MLP & reconstruction+clustering & $\\times$ \\\\\n\\cellcolor{white} & GRACE & $\\bm{A},\\bm{X}$ & Unsupervise & MLP & MLP & reconstruction+clustering & $\\times$ \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white} \\multirow{-4}{*}{\\makecell[c]{Denoising\\\\AE}} & MGAE & $\\bm{A},\\bm{X}$ & Unsupervise & GCN & GCN & reconstruction+regularization & $\\times$ \\\\ \\midrule  \n\\cellcolor{white} & GUCD & $\\bm{A},\\bm{X}$ & Unsupervise & MRFasGCN & MLP & reconstruction+pairwise & $\\times$ \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white} & SDCN & $\\bm{A},\\bm{X}$ & Unsupervise & GCN+DNN & DNN & reconstruction+clustering & $\\times$ \\\\\n\\cellcolor{white} \\multirow{-3}{*}{\\makecell[c]{Graph\\\\Convolutional\\\\ AE}} & O2MAC & $\\{\\bm{A}\\},\\bm{X}$ & Unsupervise & GCN & Inner Product & reconstruction+clustering & $\\times$ \\\\ \\midrule\n\\rowcolor{gray!30}\n\\cellcolor{white} & DAEGC & $\\bm{A},\\bm{X}$ & Unsupervise & GAT & Inner Product & reconstruction+clustering & $\\times$ \\\\\n\\cellcolor{white} &  &  &  &  & Inner Product & reconstruction+regularization & \\\\\n\\cellcolor{white} & \\multirow{-2}{*}{GEC-CSD} & \\multirow{-2}{*}{$\\bm{A},\\bm{X}$} & \\multirow{-2}{*}{Unsupervise} & \\multirow{-2}{*}{GAT} &  +GAT & +clustering+adversarial & \\multirow{-2}{*}{$\\times$} \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white} &  &  &  &  & Inner Product & reconstruction+clustering & \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white} & \\multirow{-2}{*}{MAGCN} & \\multirow{-2}{*}{$\\bm{A},\\{\\bm{X}\\}$} & \\multirow{-2}{*}{Unsupervise} & \\multirow{-2}{*}{GAT+MLP} & +GCN & +consistency & \\multirow{-2}{*}{$\\times$} \\\\\n\\cellcolor{white} &  &  &  &  & & reconstruction+regularization & \\\\\n\\cellcolor{white} & \\multirow{-2}{*}{SGCMC} & \\multirow{-2}{*}{$\\bm{A},\\{\\bm{X}\\}$} & \\multirow{-2}{*}{Unsupervise} & \\multirow{-2}{*}{GAT} & \\multirow{-2}{*}{GAT} & +clustering+consistency & \\multirow{-2}{*}{$\\times$} \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white} &  &  &  &  &  & reconstruction+regularization & \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white} \\multirow{-10}{*}{\\makecell[c]{Graph\\\\Attention\\\\AE}} & \\multirow{-2}{*}{DMGC} & \\multirow{-2}{*}{$\\{A\\}$} & \\multirow{-2}{*}{Unsupervise} & \\multirow{-2}{*}{MLP} & \\multirow{-2}{*}{MLP} & +proximity+clustering & \\multirow{-2}{*}{$\\times$} \\\\ \\midrule\n\\cellcolor{white} & TGA/TVGA & $\\bm{A}$,$\\bm{X}$ & Unsupervise & GCN & Triad & reconstruction & $\\times$ \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white} & VGECLE & $\\bm{A}$ & Semi-supervise & DNN & DNN & reconstruction+pairwise & $\\times$ \\\\\n\\cellcolor{white} & DGLFRM & $\\bm{A},\\bm{X}$ & Semi-supervise & GCN & DNN & reconstruction+regularization & $\\checkmark$ \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white} & LGVG & $\\bm{A},\\bm{X}$ & Semi-supervise & GCN & DNN & reconstruction+regularization & $\\checkmark$ \\\\\n\\cellcolor{white} & VGAECD & $\\bm{A},\\bm{X}$ & Unsupervise & GCN & Inner Product & reconstruction+clustering & $\\times$ \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white} & VGAECD-OPT & $\\bm{A},\\bm{X}$ & Unsupervise & GCN & Inner Product & reconstruction+clustering & $\\times$ \\\\\n\\cellcolor{white} \\multirow{-8}{*}{\\makecell[c]{Variational \\\\AE}} & ARGA/ARVGA & $\\bm{A},\\bm{X}$ & Unsupervise & GCN & Inner Product & reconstruction & $\\times$ \\\\ \n\\bottomrule[1 pt]\n\\end{tabular}\n\\label{table-autoencoder}\n\\end{table*}", "cites": [6536, 6534, 6539, 6533, 168, 6540, 6535, 7583, 9045, 9044, 6532, 38, 6538, 220, 8564, 8252, 232, 6537, 6541, 1430, 6542], "cite_extract_rate": 0.3442622950819672, "origin_cites_number": 61, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive summary of deep learning-based community detection methods, primarily through structured tables. While it categorizes methods by their type (CNN, GCN, GAT, GAN, AE) and highlights basic attributes like input, learning paradigm, and overlap capability, it does not synthesize ideas across papers or present a deeper narrative. There is minimal critical evaluation or abstraction of broader principles, as the focus remains on factual presentation rather than analysis or insight."}}
{"id": "271fc3d9-9dda-4bea-9956-138b655c1856", "title": "Real-world Data Sets", "level": "subsection", "subsections": [], "parent_id": "71e712a0-3355-42da-8f49-3fbc32a0fb35", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Detailed description: Data Sets"], ["subsection", "Real-world Data Sets"]], "content": "\\textbf{Citation/Co-authorship Networks.}\n\\textit{Citeseer}, \\textit{Cora} and \\textit{Pubmed}\\footnote{\\url{https://linqs.soe.ucsc.edu/data}} are the most popular group of paper citation networks used in community detection experiments as attributed networks, where nodes represent publications and links mean citations. The nodes are described by binary word vectors. Topics are class labels. \n\\textit{DBLP}\\footnote{\\url{http://snap.stanford.edu/data/com-DBLP.html}} is a co-authorship network from computer science bibliography website. A ground truth community labels that the authors who published papers on one journal or conference. In complicated scenarios, the heterogeneous \\textit{DBLP} can be extracted with considering diverse information such as co-authorship, citation.\n\\textit{ACM}\\footnote{\\url{http://dl.acm.org/}} is a paper network with two views, one view describes whether two papers are written by the same author, the other view depicts if the subjects of two papers are the same. The nodes (\\textit{i.e.} papers) are described by the bag-of-words of the keywords. The heterogeneous \\textit{ACM} can be constructed as well.\n\\textbf{Online Social Networks.}\nIn most of online social network data sets, nodes represent users, edges depict relationship between them such as friendships or followings. In the \\textit{YouTube}\\footnote{\\url{http://snap.stanford.edu/data/com-Youtube.html}} dataset, social network is formed by users having friendship, each user-created group for video sharing is a community. \\textit{LiveJournal}\\footnote{\\url{http://snap.stanford.edu/data/soc-LiveJournal1.html}} is a friendship social network collected from a online blog community where users state friendship to each other and define groups that can be ground truth communities. \\textit{PolBlogs}\\footnote{\\url{http://www-personal.umich.edu/~mejn/netdata/}\\label{newman}} is a social blog network of weblogs on US politics, where nodes are blogs and hyperlinks between them extract to edges. \nSome data sets also collect node attributes. In the \\textit{Facebook}\\footnote{\\url{http://snap.stanford.edu/data/ego-Facebook.html}} and \\textit{Twitter}\\footnote{\\url{http://snap.stanford.edu/data/ego-Twitter.html}} data sets, for example, attributes are described by user profiles while nodes and edges represent users and their connections, respectively. The heterogeneous \\textit{Twitter} extracts different types information from users, tweets, and locations. \\textit{Flickr} provides an online platform for users to share photos, the network is formed by users following each other, attributes are image tags, and community labels represent different interest groups. \\textit{Blogcatalog} is another blogger community, where attributes are extracted from users' blogs, community labels denote various topics. \\textit{Gplus}\\footnote{\\url{http://snap.stanford.edu/data/ego-Gplus.html}} is the ego network constructed by users sharing their \\textit{circles} in Google+, in which links exist if users follow each other and their profiles are extracted as attributes. Furthermore, there are three signed online social networks, \\textit{Slashdot} is extracted from the technology news site Slashdot where users form the relationships of friends or foes which represented by positive or negative links. \\textit{Epinions} is a \\textit{who trust whom} network constructed from the Epinions site\\footnote{\\url{http://www.epinions.com/}}, where the positive and negative edges reflects that users trust or distrust each other. In the signed \\textit{Wiki} data set, the network describes election of Wikipedia admins among users, edges indicate that the users vote \\textit{for} (positive) or \\textit{against} (negative) to the other. Considering heterogeneous information, \\textit{Last.fm} as a music website\\footnote{\\url{https://www.last.fm/}} constructs the network of users, artists and artist tags by tracking user listening information from various sources. \\textit{Foursquare} is another heterogeneous social network, which is formed by various kinds of location-related services. \n\\textbf{Traditional Social Networks.}\nThe data sets of \\textit{Karate}, \\textit{Dolphin} and \\textit{Football} are the most widely-used social networks in community detection. \\textit{Karate}\\textsuperscript{\\ref{newman}} describes friendships between 34 members of Zacharyâ€™s karate club. In \\textit{Dolphin}\\textsuperscript{\\ref{newman}} social network, nodes and edges represent dolphins and tighter connection between two dolphins, respectively. In \\textit{Football}\\textsuperscript{\\ref{newman}} data set, nodes represent different football teams while edges indicate the matches between them. Another traditional social network data sets include: the \\textit{Friendship6} and \\textit{Friendship7} data sets that construct high school friendship networks, and \\textit{Email}\\footnote{\\url{https://snap.stanford.edu/data/email-Eu-core.html}} data set describing email communication of the EU research institution. Considering the evolution of networks, \\textit{Cellphone Calls}\\footnote{\\url{http://www.cs.umd.edu/hcil/VASTchallenge08/download/Download.htm}} consists of a set of cell phone call records over ten days, where nodes are cell phones and edges represent phone calls. \\textit{Rados}\\footnote{\\url{http://networkrepository.com/ia-radoslaw-email.php}} and \\textit{Enron Email}\\footnote{\\url{http://www.cs.cmu.edu/\\%7Eenron/}} are popular email data sets from employees' communication over a period of time, where senders or recipients are represented by nodes and edges exist if two individuals emailed. Another three dynamic netowrks are \\textit{High School}, \\textit{Hospital} \nand \\textit{Hypertext}\\footnote{\\url{http://www.sociopatterns.org/datasets}}. \\textit{High School} forms the temporal friendship network of contacts between students in different classes of a high school within one week. \\textit{Hospital} constructs the time-varing network of contacts between patients, patients and health-care workers and among the workers in a hospital during a few days. \\textit{Hypertext} extracts a social network of conference attendees during the ACM Hypertext 2009 conference.\n\\begin{table*}[!t]\n\\centering \\footnotesize\n\\renewcommand\\arraystretch{1}\n\\setlength{\\tabcolsep}{1mm}\n\\caption{Summary of real-world benchmark data sets.} \n\\begin{tabular}{clrrrrl}\n\\toprule[1 pt]\n\\textbf{Network} & \\textbf{Data Set} & \\textbf{Number of} & \\textbf{Number of}  & \\textbf{Number of} & \\textbf{Given Number of} &  \\\\\n\\textbf{Category} & \\textbf{Name} & \\textbf{Nodes} & \\textbf{Edges}  & \\textbf{Attributes} & \\textbf{Communities} & \\textbf{Publications} \\\\ \\midrule\n\\rowcolor{gray!30} \n\\cellcolor{white!100} & Karate  & 34 & 78 & 0 & 2 &  \\\\\n\\cellcolor{white!100} & Dolphins  &  62 & 159 &  0 & 2 &  \\\\\n\\rowcolor{gray!30} \n\\cellcolor{white!100} & Friendship6  & 69 & 220 & 0 & 6 &  \\\\\n\\cellcolor{white!100} & Friendship7  & 69 & 220 & 0 & 7 &  \\\\\n\\rowcolor{gray!30} \n\\cellcolor{white!100} & Polbooks  &  105 & 441 &  0 & 3 &  \\\\\n\\cellcolor{white!100} &  &  &  &  &  & , \\\\\n\\cellcolor{white!100} & \\multirow{-2}{*}{Football } & \\multirow{-2}{*}{115} & \\multirow{-2}{*}{613} & \\multirow{-2}{*}{0} & \\multirow{-2}{*}{12} &  \\\\\n\\rowcolor{gray!30} \n\\cellcolor{white!100} & Email & 1,005 & 25,571 & 0 & 42 &  \\\\\n\\cellcolor{white!100} & Polblogs  & 1,490 & 16,718 & 0 & 2 &  \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white!100} & Amazon  & 334,863 & 925,872 &  0 & 75,149 &  \\\\\n\\cellcolor{white!100} & DBLP  & 317,080 & 1,049,866 &  0 & 13,477 &  \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white!100} & YouTube  & 1,134,890 & 2,987,624 & 0 & 8,385 &  \\\\\n\\cellcolor{white!100} \\multirow{-11}{*}{\\makecell[c]{Unattributed\\\\Network}} & LiveJournal  & 3,997,962 & 34,681,189 &  0 & 287,512 &  \\\\ \\midrule \n\\rowcolor{gray!30} \n\\cellcolor{white!100} & Texas  & 187 & 328 &  1,703 & 5 &  \\\\\n\\cellcolor{white!100} & Cornell   & 195 & 304 & 1,703 & 5 &  \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white!100} & Washington  & 230 & 446 &  1,703 & 5 &  \\\\\n\\cellcolor{white!100} & Wisconsin  & 265 & 530 &  1,703 & 5 &  \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white!100} & Wiki  & 2,405 & 17,981 &  4,973 & 19 &  \\\\\n\\cellcolor{white!100} &  &  &  &  &  & ,\\\\\n\\cellcolor{white!100} &  &  &  &  &  & ,\\\\\n\\cellcolor{white!100} & \\multirow{-3}{*}{Cora } & \\multirow{-3}{*}{2,708} & \\multirow{-3}{*}{5,429} & \\multirow{-3}{*}{1,433} & \\multirow{-3}{*}{7} &   \\\\\n\\rowcolor{gray!30} \n\\cellcolor{white!100} &  &  &  &  &  & , \\\\\n\\rowcolor{gray!30} \n\\cellcolor{white!100} &  &  &  &  &  & , \\\\\n\\rowcolor{gray!30} \n\\cellcolor{white!100} & \\multirow{-3}{*}{Citeseer } & \\multirow{-3}{*}{3,312} & \\multirow{-3}{*}{4,715} & \\multirow{-3}{*}{3,703} & \\multirow{-3}{*}{6} &  \\\\\n\\cellcolor{white!100} & UAI2010  & 3,363 & 45,006 &  4,972 & 19 &  \\\\\n\\rowcolor{gray!30} \n\\cellcolor{white!100} & Facebook  & 4,039 & 88,234 & 1,283 & 193 &  \\\\\n\\cellcolor{white!100} & Blogcatalog  & 5,196 & 171,743 & 8,189 & 6 &  \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white!100} & Flickr  & 7,564 & 239,365 & 12,047 & 9 &  \\\\ \n\\cellcolor{white!100} & DBLP & 17,725 & 52,890 & 6,974 & 4 &  \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white!100} &  &  &  &  &  & ,\\\\\n\\rowcolor{gray!30}\n\\cellcolor{white!100} &  &  &  &  &  & ,\\\\\n\\rowcolor{gray!30}\n\\cellcolor{white!100} & \\multirow{-3}{*}{PubMed } & \\multirow{-3}{*}{19,717} & \\multirow{-3}{*}{44,338} & \\multirow{-3}{*}{500} & \\multirow{-2}{*}{3} &  \\\\ \n\\cellcolor{white!100} & Twitter  & 81,306 & 1,768,149 & 216,839 & 4,065 &  \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white!100} \\multirow{-21}{*}{\\makecell[c]{Attributed \\\\Network}} & GPlus  & 107,614 & 13,673,453 & 15,907 & 468 &  \\\\ \\midrule \n& & & \\textbf{View1} ~~~~\\textbf{View2} & & & \\\\ \\cmidrule{4-4}\n\\rowcolor{gray!30} \n\\cellcolor{white!100} & ACM  & 3,025 & 29,281  2,210,761 & 1,830 & 3 &  \\\\ \n\\cellcolor{white!100} \\multirow{-2}{*}{\\makecell[c]{Multi-view \\\\Network}} & IMDB  & 4,780 & 98,010 ~~~ 21,018 & 1,232 & 3 &  \\\\ \\midrule \n&  &  & \\textbf{Edges} (+) &\\textbf{Edges} (-) &  &  \\\\ \\cmidrule{4-5}\n\\rowcolor{gray!30}\n\\cellcolor{white!100} & Epinions  & 7,000 & 404,006 & 47,143 & -- &  \\\\\n\\cellcolor{white!100} &  Slashdot  & 7,000 & 181,354 & 56,675 & -- &  \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white!100} \\multirow{-3}{*}{\\makecell[c]{Signed \\\\Network}} & Wiki  & 7,118 & 81,318 & 22,357 & -- &  \\\\ \\midrule\n&  &  &  & \\textbf{Metapath} &  &  \\\\ \\cmidrule{5-5}\n\\rowcolor{gray!30}\n\\cellcolor{white!100} &  ACM & 8,916 & 12,769 & \\checkmark & -- &  \\\\\n\\cellcolor{white!100} &  IMDB & 11,616 & 17,106 & \\checkmark & -- &  \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white!100} & Last.fm & 20,612 & 128,804 & \\checkmark & -- &  \\\\\n\\cellcolor{white!100} & DBLP & 26,128 & 119,783 & \\checkmark & -- &  \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white!100} & Foursquare & 93,069 & 174,484 & $\\times$ & -- &  \\\\\n\\cellcolor{white!100} \\multirow{-7}{*}{\\makecell[c]{Heterogeneous \\\\Network}} &  Twitter &  9,793,112 & 10,271,142 & $\\times$ & -- &  \\\\ \\midrule\n&  &  &  & \\textbf{Time steps} &  &  \\\\ \\cmidrule{5-5}\n\\rowcolor{gray!30}\n\\cellcolor{white!100} & Hospital  & 75 & 32,424 & 9 & -- &  \\\\\n\\cellcolor{white!100} &  Hypertext  & 113 & 20,818 & 5 & -- &  \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white!100} & Enron Mail  & 151 & 33,124 & 12 & -- &  \\\\\n\\cellcolor{white!100} & Rados  & 167 & 82,927 & 10 & -- &  \\\\\n\\rowcolor{gray!30}\n\\cellcolor{white!100} & High School  & 327 & 188,508 & 9 & -- &  \\\\\n\\cellcolor{white!100}\\multirow{-6}{*}{\\makecell[c]{Dynamic \\\\Network}} & Cellphone Calls  & 400 & 9,834 & 10 & -- &   \\\\\n\\bottomrule[1 pt]\n\\end{tabular}\n\\label{table-rwdatasets}\n\\end{table*}\n\\textbf{Webpage Networks.} The webpage networks connect between one type of world wide web resources. \\textit{IMDB}\\footnote{\\url{https://www.imdb.com/}} is a website of movies, containing information like casts, storyline and user reviews. Heterogeneous \\textit{IMDB} is constructed by representing nodes through movies, actors and directors. Multi-view \\textit{IMDB} exploits co-actor relationship and co-director relationship. The bag-of-words of plots are extracted as attributes, community can be labeled by movie's genre. \\textit{Wiki}\\footnote{\\url{https://linqs.soe.ucsc.edu/data}} is a webpage network where a node represents the certain webpage, edge means that hyperlink connects any two webpages, and attributes from weighted word vectors. Another wiki data set is \\textit{UAI2010} which extracts related article information from English-Wikipedia pages. \\textit{WebKB}\\footnote{\\url{https://linqs-data.soe.ucsc.edu/public/lbc/WebKB.tgz}} is the webpage data set which contains 4 sub-datasets extracted from four different universities, \\textit{i.e.}, \\textit{Cornell}, \\textit{Texas}, \\textit{Washington} and \\textit{Wisconsin}.\n\\textbf{Product Co-purchasing Networks.}\nThe \\textit{Amazon}\\footnote{\\url{http://snap.stanford.edu/data/\\#amazon}} data set is provided by Amazon website to analyze the co-purchased products where nodes represent products and edges link any two products which are co-purchased frequently. The ground truth communities are labeled by product categories. \\textit{Polbooks}\\textsuperscript{\\ref{newman}} describes the purchase of political books during the time of US presidential election in 2004. Nodes donate books, edges link any two frequently co-purchased books by the same buyers, and the community labels reflect buyers' political attitude.", "cites": [6533, 9047, 248, 9045, 6532, 9048, 9049, 6542, 6535, 8483, 8451, 6541, 6536, 6540, 553, 6538, 9033, 8564, 8252, 232, 1430, 6534, 7583, 9044, 6547, 6537], "cite_extract_rate": 0.32098765432098764, "origin_cites_number": 81, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of real-world datasets used in community detection, listing their characteristics and sources. It makes minimal effort to synthesize or integrate insights from the cited papers, merely mentioning them in the context of dataset descriptions. There is no critical evaluation of the datasets or the cited methods, nor any abstraction to broader patterns or principles in the field."}}
{"id": "b6e369d2-dfb3-4ed2-a208-ee0235b6fa35", "title": "Synthetic Benchmark Data Sets", "level": "subsection", "subsections": [], "parent_id": "71e712a0-3355-42da-8f49-3fbc32a0fb35", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Detailed description: Data Sets"], ["subsection", "Synthetic Benchmark Data Sets"]], "content": "\\textbf{Girvan-Newman (GN) Networks}:\nThe classic GN benchmark network contains 128 nodes that partitioned into 4 communities with 32 nodes in each community. Every node has a fixed average degree $k_{in}$ and connects a pre-defined number of nodes in another community $k_{out}$. For example, $k_{in}+k_{out}=16$. A parameter $\\mu$ controls the ratio of neighbors in other communities for each node.\n\\textbf{Lancichinettiâ€“Fortunatoâ€“Radicchi (LFR) Networks} : The LFR benchmark data set simulates the degree of nodes in a real-world network and the scale-free nature of the network for more flexible networks. The community validation is more challenging, and the results are more convincing. The LFR generation tool controls synthetic network topology via a set of parameters, including network size $n$, the average $k$ and maximum $Maxk$ degree, the minimum $Minc$ and maximum $Maxc$ community size and the mixing parameter $\\mu$. The node degree and community size are governed by power laws distributions with exponents of $\\gamma$ and $\\beta$, respectively.\nLFR is the most common simulation benchmark in community detection research, the extended LFR benchmark can generate overlapping communities in directed and weighted networks .", "cites": [8483, 8484], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a factual description of two synthetic benchmark datasets (GN and LFR) and their key parameters, drawing from the cited papers. It integrates some core concepts from both works but remains largely descriptive without offering deeper comparative analysis or critical evaluation of their strengths and weaknesses. Some abstraction is attempted by highlighting the increasing realism and flexibility of synthetic data, but broader patterns or principles are not fully explored."}}
{"id": "a067c3d1-eafb-4f6c-b982-ad6c516355d9", "title": "Detailed DESCRIPTION: Evaluation Metrics", "level": "section", "subsections": [], "parent_id": "ca169d78-5c5d-483c-9bd5-7ee46d40e336", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Detailed DESCRIPTION: Evaluation Metrics"]], "content": "\\label{sec-EM}\n\\noindent\n\\textbf{Normalized Mutual Information (NMI)}: The metric provides a standard evaluation on community detection outputs $\\mathcal{C}$ and the ground truth $\\mathcal{C^{*}}$. It reports the mutual parts in a normalized score:\n\\begin{equation}\n\\small\n\\operatorname{NMI}(\\mathcal{C}, \\mathcal{C^{*}})=\\frac{-2 \\sum_{i=1}^{K} \\sum_{j=1}^{K^{*}} n_{i j} \\log \\frac{n_{i j} n}{n_{i} n_{j}}}{\\sum_{i=1}^{K} n_{i} \\log \\frac{n_{i}}{n}+\\sum_{j=1}^{K^{*}} n_{j} \\log \\frac{n_{j}}{n}},\n\\end{equation}\nwhere $K$ and $K^{*}$ are the number of detected and ground truth communities, respectively, and $n$ is the number of nodes. $n_{ij}$ denotes the number of nodes appearing in both the $i$-th detected community $C_{i}$ and the $j$-th ground truth community $C_{j}^{*}$. $n_{i}$ and $n_{j}$ sum up the number of nodes in $C_{i}$ and $C_{j}^{*}$, respectively. NMI can evaluate overlapping communities by its variant: Normalized Mutual Information for Overlapping Community (Overlapping-NMI).\n\\vspace{2mm}\n\\noindent\t\n\\textbf{Accuracy (ACC)}: ACC evaluates the community membership on each node as follows:\n\\begin{equation}\n\\small\n\\operatorname{ACC}\\left(\\mathcal{C}, \\mathcal{C^{*}} \\right) = \\frac{1}{n}\\sum_{i=1}^{n}\\delta(y_{i},y_{i}^{*}), \n\\end{equation}\nwhere $y_{i}$ and $y_{i}^{*}$ denote the $i$-th node's community label in community detection results and the ground truth, respectively. The Kronecker delta function $\\delta = 1$ if $y_{i}=y_{i}^{*}$, otherwise $0$.\n\\vspace{2mm}\n\\noindent\n\\textbf{Precision}: In the community detection evaluation, precision describes the percentage of clustered nodes in each detected community existing in its ground truth:\n\\begin{equation}\n\\small\n\\operatorname{Precision}(C_{k}, C^*_{k})= \\frac{|C_k \\cap C^*_k|} {|C_k|}, \n\\end{equation}\nwhere $C_k$ and $C_k^*$ denote the $k$-th detected community and groud truth community, respectively.\n\\vspace{2mm}\n\\noindent\n\\textbf{Recall}: Accordingly, recall measures the percentage of ground truth nodes of each community discovered in the detected community by counting the $k$-th community as below: \n\\begin{equation}\n\\small\n\\operatorname{Recall}(C_{k}, C^*_{k}) = \\frac{|C_k \\cap C^*_k|} {|C_k^*|}. \n\\end{equation}\n\\vspace{2mm}\n\\noindent\n\\textbf{F1-score}: The F1-score balances Precision and Recall: \n\\begin{equation}\n\\small\n\\begin{aligned}\n\\operatorname{F1-score}&\\left(C_{k}, C^*_{k}\\right)=\\\\\n& 2 \\cdot \\frac{\\operatorname{Precision}\\left(C_{k}, C^*_{k}\\right) \\cdot \\operatorname{Recall}\\left(C_{k}, C^*_{k}\\right)}{\\operatorname{Precision}\\left(C_{k}, C^*_{k}\\right)+\\operatorname{Recall}\\left(C_{k}, C^*_{k}\\right)}\n\\end{aligned}.\n\\end{equation}\nOver the whole set of communities, the F1-score evaluates the true/false positive/negative node clustering results in detail:\n\\begin{equation}\n\\small\n\\begin{aligned}\n\\operatorname{F1-score}&\\left(\\mathcal{C}, \\mathcal{C}^*\\right)= \\\\\n&\\sum\\nolimits_{C_{k}\\in \\mathcal{C}} \\frac{|C_{k}|}{\\sum_{C_{k}\\in \\mathcal{C}} |C_{k}|} \\max\\limits_{C_{k}^{*}\\in \\mathcal{C}^{*}} \\operatorname{F1-score}\\left(C_{k}, C_{k}^{*} \\right)\n\\end{aligned}. \n\\end{equation}\n\\vspace{2mm}\n\\noindent\n\\textbf{Adjusted Rand Index (ARI)}: ARI is a variation of \\textit{Rand Index (RI)} that measures the percentage of correctly divided nodes based on true/false positive/negative:\n\\begin{equation}\n\\small\n\\begin{aligned}\n\\operatorname{ARI}&(\\mathcal{C}, \\mathcal{C}^*) = \\\\ &\\frac{\\sum_{ij}\\tbinom{n_{ij}}{2}-\\left[\\sum_i \\tbinom{n_{i}}{2}\\sum_{j} \\tbinom{n_{j}}{2}\\right]/\\tbinom{n}{2}}{\\frac{1}{2}\\left[\\sum_{i} \\tbinom{n_{i}}{2}+\\sum_{j} \\tbinom{n_{j}}{2}\\right]-\\left[\\sum_{i} \\tbinom{n_{i}}{2}\\sum_{j} \\tbinom{n_{j}}{2}\\right]/\\tbinom{n}{2}}\n\\end{aligned}, \n\\end{equation}\nwhere $n$ is the number of nodes, \n$n_{i}$ is the number of nodes in the $i$-th detected community $C_{i}$ while $n_{j}$ denotes the number of nodes in the $j$-th ground truth community $C_{j}^{*}$. $n_{ij}$ indicates the number of nodes simultaneously appearing in $C_{i}$ and $C_{j}^{*}$.\n\\vspace{2mm}\n\\noindent\t\n\\textbf{Modularity (\\textit{Q})}: \\textit{Q} is widely used to evaluate the strength of detected communities without ground truth. It compares with a null model which is a random graph with an equivalent degree distribution as the original graph:  \n\\begin{equation}\n\\small\nQ = \\sum_{k} (e_{kk} - a_{k}^{2}), \n\\label{eq-modularity}\n\\end{equation}\nwhere $e_{kk}$ is the ratio of edges linking nodes within the $k$-th community $C_{k}$ to the total edges in network, $a_{k}$ is the proportion of edges that associate with nodes in community $C_{k}$ to the number of total edges. The extended modularity  evaluates overlapping community detection performances.  \n\\vspace{2mm}\n\\noindent\n\\textbf{Jaccard}: Jaccard measures whether the $k$-th detected community $C_{k}$ is similar to its ground truth $C_{k}^{*}$ as follows: \n\\begin{equation}\n\\small\n\\operatorname{JC}(C_{k}, C_{k}^{*})=\\frac{|C_{k}\\cap C_{k}^{*}|}{|C_{k} \\cup C_{k}^{*}|}. \n\\end{equation}\nFor the whole set of communities, Jaccard is computed as: \n\\begin{equation}\n\\small\n\\begin{aligned}\n\\operatorname{JC}\\left(\\mathcal{C}, \\mathcal{C}^*\\right)= &\\sum_{C_{k}\\in \\mathcal{C}}\\frac{\\max_{C_{k}^{*}\\in \\mathcal{C}^{*}}\\operatorname{JC}(C_{k}, C_{k}^{*})}{2|\\mathcal{C}|}\\\\\n&+\\sum_{C_{k}^{*}\\in \\mathcal{C}^{*}} \\frac{\\max_{C_{k}\\in \\mathcal{C}}\\operatorname{JC}(C_{k}, C_{k}^{*})}{2|\\mathcal{C^*}|}\n\\end{aligned}. \n\\end{equation}\n\\vspace{2mm}\n\\noindent\n\\textbf{Conductance (CON)}: CON measures the separability of the $k$-th detected community through the fraction of total edge numbers of $C_{k}$ linking to outside communities:\n\\begin{equation}\n\\small\n\\operatorname{CON}(C_{k})=\\frac{m_{k}^{out}}{2 m_{k}^{in} + m_{k}^{out}},\n\\end{equation}\nwhere $m_{k}^{in}$ indicates the number of internal edges in community $C_{k}$, and $m_{k}^{out}$ is the number of external edges on the cross-community boundary of $C_{k}$.\n\\vspace{2mm}\n\\noindent\n\\textbf{Triangle Participation Ratio (TPR)}: TPR indicates the community density through measuring the fraction of triads within a detected community $C_{k}$. It is defined as follows:\n\\begin{equation}\n\\small\n\\begin{array}{r}\n\\operatorname{TPR}(C_{k})=\\mid\\left\\{v_{i}:v_{i}\\in C_{k},\\left\\{\\left(v_{j}, v_{w}\\right):v_{j}, v_{w} \\in C_{k},  \\right.\\right. \\\\\n\\left.\\left.\\left(v_{i}, v_{j}\\right),\\left(v_{j}, v_{w}\\right),\\left(v_{i}, v_{w}\\right) \\in E\\right\\} \\neq \\emptyset\\right\\}| / |C_{k}|\n\\end{array}.\n\\end{equation}", "cites": [9050, 9051], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a clear, factual description of various evaluation metrics for community detection. It includes mathematical formulations for NMI, ACC, Precision, Recall, F1-score, ARI, Modularity, Jaccard, and TPR, and briefly connects NMI to overlapping community detection via the cited paper. However, there is minimal critical evaluation of these metrics, no comparative discussion of their strengths and weaknesses, and little abstraction to broader principles or trends in evaluation methodology."}}
{"id": "327f8dbf-80d1-48d9-a0d7-2e81c9f306fb", "title": "Detailed description: Open-source Implementations ", "level": "section", "subsections": [], "parent_id": "ca169d78-5c5d-483c-9bd5-7ee46d40e336", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Detailed description: Open-source Implementations "]], "content": "\\label{sec-appcode}\n\\textbf{Under PyTorch and GCN:} The implementation of LGNN for community detection in graphs runs 5-class distortive SBMs-based GNN and GCN-based LGNN, respectively. The main algorithm and other utilities of NOCD implement overlapping community detection. AGE implements adaptive graph encoder for attributed graph embedding and tests node clustering experiment results. \\textbf{Under PyTorch and GAT:} DMGI and HDMI examine graph clustering performance on unsupervised attributed multiplex network embedding. The implementation of MAGNN provides instructions to detect node clusters in heterogeneous graphs via metapath aggregated GNN. HeCo testifies the model's performance of node clustering on heterogeneous networks with providing four preprocessed data sets. CP-GNN implements experiments of community detection on four real-world heterogeneous graphs. SEAL  runs algorithms on Python and Shell validating on 50 communities. \\textbf{Under PyTorch and AEs:}  GraphEncoder implements for graph clustering on deep representations. Under the DNN architecture, SDCN cluster the graph on AE embeddings.\nUnder \\textbf{TensorFlow} and GCN:  AGC clusters attributed graph via adaptive graph convolution and intra-cluster distance computation. CayleyNet presents an implementation of community detection for a variety of different networks, including a sparse filter and rational spectral filters with Jacobi. There is an implementation on the CommunityGAN  method under TensorFlow and GAN. DIME and DANE are stacked AE-based implementations applying TensorFlow. On heterogeneous networks: Foursquare and Twitter, DIME examines community detection results and hyperparameters. For graph convolutional AE-based community detection tasks, TensorFlow is employed by O2MAC and SGCMC over multi-view graphs, while DMGC via attentive cross-graph association for deep multi-graph clustering. Moreover, DGLFRM and ARGA/ARVGA apply TensorFlow to VAE-based community detection. Other Python packages, such as \\textbf{Keras}, are employed in our summarized implementations, \\textit{i.e.}, DNGR.\nDespite the popularity of Python, \\textbf{Matlab} is another popular tool to implement deep community detection methods. semi-DRN implements community detection via stacked AE. DNE-SBP  inputs signed adjacency matrix to detect $k$ network communities upon node representations. MGAE makes use of marginalized graph autoencoder to cluster graph.\n\\clearpage\n\\onecolumn", "cites": [6536, 6534, 6540, 6535, 7583, 9044, 6532, 6538, 220, 8564, 8252, 232, 6541, 1430], "cite_extract_rate": 0.56, "origin_cites_number": 25, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily lists open-source implementations of deep learning-based community detection methods without providing a coherent synthesis or critical evaluation. It categorizes works based on frameworks (e.g., PyTorch, TensorFlow) and model types (e.g., GCN, GAT, AEs), but does not compare or contrast these implementations or highlight broader trends or principles in the field."}}
{"id": "8c0eded0-b664-420b-8080-96a484c1420e", "title": "Abbreviations", "level": "section", "subsections": [], "parent_id": "ca169d78-5c5d-483c-9bd5-7ee46d40e336", "prefix_titles": [["title", "A Comprehensive Survey on Community Detection with Deep Learning"], ["section", "Abbreviations"]], "content": "\\label{sec-abbr}\n\\begin{table*}[h]\n\\centering \\footnotesize\n\\renewcommand\\arraystretch{1.2}\n\\caption{Abbreviations in this survey.}\n\\begin{tabular}{llll}\n\\toprule[1pt]\n\\textbf{Abbr.} & \\textbf{Full Name} & \\textbf{Ref.} & \\textbf{Paper Title} \\\\ \\midrule\n\\rowcolor{gray!30}\nACC & Accuracy & & \\\\\n& Network Embedding and overlapping Community & & Self-Training Enhanced: Network embedding and overlapping  \\\\ \n\\multirow{-2}{*}{ACNE} & detection with Adversarial learning & \\multirow{-2}{*}{} & community detection with adversarial learning \\\\\n\\rowcolor{gray!30}\nAE & AutoEncoder &  &  \\\\\nAGC & Adaptive Graph Convolution &  & Attributed graph clustering via adaptive graph convolution \\\\\n\\rowcolor{gray!30}\nAGE & Adaptive Graph Encoder &  & Adaptive graph encoder for attributed graph embedding \\\\\nARGA & Adversarial Regularized Graph AutoEncoder &  & Adversarially regularized graph autoencoder for graph embedding \\\\ \n\\rowcolor{gray!30}\nARI & Adjusted Rand Index &  &  \\\\\n& Adversarially Regularized Variational Graph & & \\\\\n\\multirow{-2}{*}{ARVGA} & AutoEncoder & \\multirow{-2}{*}{} & \\multirow{-2}{*}{Adversarially regularized graph autoencoder for graph embedding} \\\\\n\\rowcolor{gray!30}\nATC & Attributed Truss Communities &  & Attribute-driven community search \\\\ \\midrule\nBP & Bernoulli--Poisson & & \\\\ \\midrule\n\\rowcolor{gray!30}\n& & & CANE: community-aware network embedding via adversarial \\\\\n\\rowcolor{gray!30}\n\\multirow{-2}{*}{CANE} & \\multirow{-2}{*}{Community-Aware Network Embedding} & \\multirow{-2}{*}{} & training \\\\\n& Graph Convolutional Neural Networks with & & CayleyNets: Graph convolutional neural networks with complex \\\\ \n\\multirow{-2}{*}{CayleyNets} & Cayley Polynomials & \\multirow{-2}{*}{} & rational spectral filters \\\\ \n\\rowcolor{gray!30}\n& Community Detection Algorithm based on  &  & Community detection in complex networks using structural \\\\\n\\rowcolor{gray!30}\n\\multirow{-2}{*}{CDASS} & Structural Similarity & \\multirow{-2}{*}{} & similarity \\\\\n& (Stacked Autoencoder-Based) Community &  & Stacked autoencoder-based community detection method via  \\\\\n\\multirow{-2}{*}{CDMEC} & Detection Method via Ensemble Clustering & \\multirow{-2}{*}{} & an ensemble clustering framework \\\\\n\\rowcolor{gray!30}\nCNN & Convolutional Neural Network &  &  \\\\\nCommDGI & Community Deep Graph Infomax &  & CommDGI: Community detection oriented deep graph infomax \\\\\n\\rowcolor{gray!30}\n& Community Detection with Generative & & CommunityGAN: Community detection with generative \\\\\n\\rowcolor{gray!30}\n\\multirow{-2}{*}{CommunityGAN} & Adversarial Nets & \\multirow{-2}{*}{} & adversarial nets \\\\\n&  &  & Edge classification based on Convolutional Neural Networks  \\\\\n\\multirow{-2}{*}{ComNet-R} & \\multirow{-2}{*}{Community Network Local Modularity R} & \\multirow{-2}{*}{} & for community detection in complex network \\\\\n\\rowcolor{gray!30}\nCON & Conductance &  &  \\\\ \n& & & Detecting communities from heterogeneous graphs: A Context  \\\\\n\\multirow{-2}{*}{CP-GNN} & \\multirow{-2}{*}{Context Path-based Graph Neural Network} & \\multirow{-2}{*}{} & path-based graph neural network model \\\\\n\\midrule\n\\rowcolor{gray!30}\nDAEGC & Deep Attentional Embedded Graph Clustering &  & Attributed graph clustering: A deep attentional embedding approach \\\\ \nDANE & Deep Attributed Network Embedding &  & Deep attributed network embedding \\\\\n\\rowcolor{gray!30}\n& Deep Autoencoder-like Nonnegative & & Deep autoencoder-like nonnegative matrix factorization for \\\\\n\\rowcolor{gray!30}\n\\multirow{-2}{*}{DANMF} & Matrix Fatorization & \\multirow{-2}{*}{} & community detection \\\\\n& Density-Based Spatial Clustering of &  & A density-based algorithm for discovering clusters in large \\\\\n\\multirow{-2}{*}{DBSCAN} & Applications with Noise & \\multirow{-2}{*}{} & spatial databases with noise \\\\\n\\rowcolor{gray!30}\nDCSBM & Degree-Corrected Stochastic Block Model &  & Stochastic blockmodels and community structure in networks \\\\\n& Deep Learning-based Fuzzy &  & DFuzzy: A deep learning-based fuzzy clustering model for large \\\\\n\\multirow{-2}{*}{DFuzzy} & Clustering Model & \\multirow{-2}{*}{} & graphs \\\\\n\\rowcolor{gray!30}\n& Deep Generative Latent Feature Relational &  &  \\\\ \n\\rowcolor{gray!30}\n\\multirow{-2}{*}{DGLFRM} & Model & \\multirow{-2}{*}{} & \\multirow{-2}{*}{Stochastic blockmodels meet graph neural networks} \\\\ \n&  &  & BL-MNE: Emerging heterogeneous social network embedding \\\\\n\\multirow{-2}{*}{DIME} & \\multirow{-2}{*}{Deep Aligned Autoencoder-based Embedding} & \\multirow{-2}{*}{} & through broad learning with aligned autoencoder \\\\\n\\rowcolor{gray!30}\nDMGC & Deep Multi-Graph Clustering &  & Deep multi-graph clustering via attentive cross-graph association \\\\\n& Deep Graph Infomax for attributed &  &  \\\\ \n\\multirow{-2}{*}{DMGI} & Multiplex network embedding & \\multirow{-2}{*}{} & \\multirow{-2}{*}{Unsupervised attributed multiplex network embedding} \\\\\n\\rowcolor{gray!30}\n& Deep Neural network-based Clustering-oriented & & DNC: A deep neural network-based clustering-oriented network \\\\\n\\rowcolor{gray!30}\n\\multirow{-2}{*}{DNC} & network embedding & \\multirow{-2}{*}{} & embedding algorithm \\\\\n& Deep Network Embedding with &  & Deep network embedding for graph representation learning \\\\\n\\multirow{-2}{*}{DNE-SBP} & Structural Balance Preservation & \\multirow{-2}{*}{} & in signed networks \\\\\n\\rowcolor{gray!30}\nDNGR & Deep Neural Networks for Graph Representation &  & Deep neural networks for learning graph representations \\\\\nDNMF & Deep Nonnegative Matrix Factorization & & \\\\\n\\bottomrule[1pt]\n\\end{tabular} \\label{tab-abbr1}\n\\end{table*}\n\\begin{table*}[!t]\n\\centering \\footnotesize\n\\renewcommand\\arraystretch{1.2}\n\\caption{Abbreviations in this survey (continue-1).}\n\\begin{tabular}{llll}\n\\toprule[1pt]\n\\textbf{Abbr.} & \\textbf{Full Name} & \\textbf{Ref.} & \\textbf{Paper Title} \\\\ \\midrule\n\\rowcolor{gray!30}\nDNN & Deep Neural Network & & \\\\\n& Dual-Regularized Graph Convolutional &  &  \\\\\n\\multirow{-2}{*}{DR-GCN} & Networks & \\multirow{-2}{*}{} & \\multirow{-2}{*}{Multi-class imbalanced graph convolutional network learning} \\\\\n\\rowcolor{gray!30}\nDSF & Deep Sparse Filtering & & \\\\\n& Community Discovery based on Deep &  &  \\\\\n\\multirow{-2}{*}{DSFCD} & Sparse Filtering & \\multirow{-2}{*}{} & \\multirow{-2}{*}{Community discovery in networks with deep sparse filtering} \\\\ \\midrule\n\\rowcolor{gray!30}\nELBO & Evidence Lower Bound & &  \\\\\nEM & Expectation-Maximization algorithm  & & \\\\\n\\rowcolor{gray!30}\n&  &  & Graph convolutional networks meet Markov random fields: \\\\ \n\\rowcolor{gray!30}\n\\multirow{-2}{*}{eMRF} & \\multirow{-2}{*}{extended Network Markov Random Fields} & \\multirow{-2}{*}{} & Semi-supervised community detection in attribute networks \\\\ \\midrule\nFast\\textit{Q} & Fast Modularity &  &  \\\\ \\midrule\n\\rowcolor{gray!30}\nGAN & Generative Adversarial Network &  &  \\\\\nGAT & Graph Attention Network &  &  \\\\\n\\rowcolor{gray!30}\nGCLN & Graph Convolutional Ladder-shape Networks &  & Going deep: Graph convolutional ladder-shape networks \\\\\nGCN & Graph Convolutional Network &  &  \\\\\n\\rowcolor{gray!30}\n& Graph embedding clustering with & & Graph embedding clustering: Graph attention auto-encoder with  \\\\\n\\rowcolor{gray!30}\n\\multirow{-2}{*}{GEC-CSD} & cluster-specificity distribution & \\multirow{-2}{*}{} & cluster-specificity distribution \\\\\nGIN & Graph Isomorphism Network & &  \\\\\n\\rowcolor{gray!30}\nGMM & Gaussian Mixture Model  & &  \\\\\nGN & Girvan-Newman networks &  &  \\\\\n\\rowcolor{gray!30}\nGRACE & Graph Clustering with Dynamic Embedding &  & Graph clustering with dynamic embedding \\\\\nGraphEncoder & Autoencoder-based Graph Clustering Model &  & Learning deep representations for graph clustering \\\\\n\\rowcolor{gray!30}\n& GCN-based approach for Unsupervised &  & Community-centric graph convolutional network for unsupervised \\\\\n\\rowcolor{gray!30}\n\\multirow{-2}{*}{GUCD} & Community Detection & \\multirow{-2}{*}{} &  community detection \\\\ \\midrule\nHDMI & High-order Deep Multiplex Infomax &  & HDMI: High-order deep multiplex infomax \\\\\n\\rowcolor{gray!30}\n& Co-contrastive learning mechanism for & & Self-supervised heterogeneous graph neural network with co-contrastive \\\\\n\\rowcolor{gray!30}\n\\multirow{-2}{*}{HeCo} & Heterogeneous graph neural networks & \\multirow{-2}{*}{} &  learning \\\\\nHIN & Heterogeneous Information Network  & & \\\\\n\\rowcolor{gray!30}\nHSIC & Hilbert-Schmidt Independence Criterion &  & Measuring statistical dependence with Hilbert-Schmidt norms \\\\ \\midrule\nInfoMap & Information Mapping &  & Maps of random walks on complex networks reveal community structure \\\\\n\\rowcolor{gray!30}\n& Graph Pointer Network with incremental &  & SEAL: Learning heuristics for community detection with generative \\\\\n\\rowcolor{gray!30}\n\\multirow{-2}{*}{iGPN} & updates & \\multirow{-2}{*}{} & adversarial networks \\\\\n& Independence Promoted Graph Disentangled &  &  \\\\ \n\\multirow{-2}{*}{IPGDN} & Network & \\multirow{-2}{*}{} & \\multirow{-2}{*}{Independence promoted graph disentangled networks} \\\\ \\midrule\n\\rowcolor{gray!30}\nJANE & Jointly Adversarial Network Embedding &  & JANE: Jointly adversarial network embedding \\\\ \\midrule\nKL & Kullback-Leibler divergence & &  \\\\ \\midrule\n\\rowcolor{gray!30}\n& Locating Structural Centers for Community &  & Locating structural centers: A density-based clustering method for \\\\\n\\rowcolor{gray!30}\n\\multirow{-2}{*}{LCCD} & Detection & \\multirow{-2}{*}{} & community detection \\\\\nLDA & Latent Dirichlet Allocation & & \\\\\n\\rowcolor{gray!30}\nLFR & Lancichinettiâ€“Fortunatoâ€“Radicchi networks &  &  \\\\ \nLGNN & Line Graph Neural Network &  & Supervised community detection with line graph neural networks \\\\\n\\rowcolor{gray!30}\n& Ladder Gamma Variational Autoencoder for &  &  \\\\\n\\rowcolor{gray!30}\n\\multirow{-2}{*}{LGVG} & Graphs & \\multirow{-2}{*}{} & \\multirow{-2}{*}{Graph representation learning via ladder gamma variational autoencoders} \\\\\nLPA & Label Propagation Algorithm & & \\\\\n\\midrule\n\\rowcolor{gray!30}\n&  &  & A multi-agent genetic algorithm for community detection in complex \\\\\n\\rowcolor{gray!30}\n\\multirow{-2}{*}{MAGA-Net} & \\multirow{-2}{*}{Multi-Agent Genetic Algorithm} & \\multirow{-2}{*}{} &  networks \\\\\n& Multi-View Attribute Graph Convolution &  &  \\\\\n\\multirow{-2}{*}{MAGCN} & Networks & \\multirow{-2}{*}{} & \\multirow{-2}{*}{Multi-view attribute graph convolution networks for clustering} \\\\\n\\rowcolor{gray!30}\n&  &  & MAGNN: Metapath aggregated graph neural network for heterogeneous \\\\\n\\rowcolor{gray!30}\n\\multirow{-2}{*}{MAGNN} & \\multirow{-2}{*}{Metapath Aggregated Graph Neural Network} & \\multirow{-2}{*}{} &  graph embedding \\\\\n& Modularized Deep NonNegative Matrix &  & Community detection based on modularized deep nonnegative matrix \\\\\n\\multirow{-2}{*}{MDNMF} & Factorization & \\multirow{-2}{*}{} &  factorization \\\\\n\\rowcolor{gray!30}\nMGAE & Marginalized Graph AutoEncoder &  & MGAE: Marginalized graph autoencoder for graph clustering \\\\\n\\bottomrule[1pt]\n\\end{tabular}\\label{tab-abbr2}\n\\end{table*}\n\\begin{table*}[!t]\n\\centering \\footnotesize\n\\renewcommand\\arraystretch{1.2}\n\\caption{Abbreviations in this survey (continue-2).}\n\\begin{tabular}{llll}\n\\toprule[1pt]\n\\textbf{Abbr.} & \\textbf{Full Name} & \\textbf{Ref.} & \\textbf{Paper Title} \\\\ \\midrule\n\\rowcolor{gray!30}\nMI & Mutual Information &  &  \\\\\nMMB & Mixed Membership Stochastic Blockmodel &  & Mixed membership stochastic blockmodels \\\\ \n\\rowcolor{gray!30}\n& Markov Random Fields as a convolutional &  & Graph convolutional networks meet Markov random fields: \\\\\n\\rowcolor{gray!30}\n\\multirow{-2}{*}{MRFasGCN} & layer in Graph Convolutional Networks & \\multirow{-2}{*}{} & Semi-supervised community detection in attribute networks \\\\  \\midrule\nNMF & Nonnegative Matrix Factorization &  &  \\\\\n\\rowcolor{gray!30}\nNMI & Normalized Mutual Information&  &  \\\\\nNOCD & Neural Overlapping Community Detection &  & Overlapping community detection with graph neural networks \\\\ \\midrule\n\\rowcolor{gray!30}\nOne2Multi & One-view to Multi-view &  & One2multi graph autoencoder for multi-view graph clustering \\\\\n& One2Multi Graph Autoencoder for Multi-view &  & \\\\ \n\\multirow{-2}{*}{O2MAC} & Graph Clustering & \\multirow{-2}{*}{} & \\multirow{-2}{*}{One2multi graph autoencoder for multi-view graph clustering} \\\\ \\midrule\n\\rowcolor{gray!30}\nPPI & Protein-Protein Interaction & &  \\\\ \n&  &  & Progan: Network embedding via proximity generative \\\\\n\\multirow{-2}{*}{ProGAN} & \\multirow{-2}{*}{Proximity Generative Adversarial Network} & \\multirow{-2}{*}{} & adversarial network \\\\ \\midrule\n\\rowcolor{gray!30}\n\\textit{Q} & Modularity &  &  \\\\ \\midrule\nSBM & Stochastic Block Model &  &  \\\\\n\\rowcolor{gray!30}\nSCAN & Structural Clustering Algorithm for Networks &  & SCAN: A structural clustering algorithm for networks \\\\\nSDCN & Structural Deep Clustering Network &  & Structural deep clustering network \\\\\n\\rowcolor{gray!30}\n& Seed Expansion with generative Adversarial &  & SEAL: Learning heuristics for community detection with \\\\\n\\rowcolor{gray!30}\n\\multirow{-2}{*}{SEAL} & Learning & \\multirow{-2}{*}{} & generative adversarial networks \\\\\n& Semi-supervised Nonlinear Reconstruction &  &  \\\\ \n\\multirow{-2}{*}{semi-DRN} & Algorithm with Deep Nerual Network & \\multirow{-2}{*}{} & \\multirow{-2}{*}{Modularity based community detection with deep learning} \\\\ \n\\rowcolor{gray!30}\nsE-Autoencoder & Semi-supervised Evolutionary Autoencoder &  & An evolutionary autoencoder for dynamic community detection \\\\\nSENet & Spectral Embedding Network &  & Spectral embedding network for attributed graph clustering \\\\\n\\rowcolor{gray!30}\nSF & Sparse Filtering &  &  \\\\\nSGC &  Simplification of GCN &  & Simplifying graph convolutional networks \\\\\n\\rowcolor{gray!30}\n& Self-supervised Graph Convolutional network &  &  \\\\ \n\\rowcolor{gray!30}\n\\multirow{-2}{*}{SGCMC} & for Multi-view Clustering & \\multirow{-2}{*}{} & \\multirow{-2}{*}{Self-supervised graph convolutional network for multi-view clustering} \\\\ \n& A framework of local label sampling model & & Unsupervised learning for community detection in attributed \\\\ \n\\multirow{-2}{*}{SGCN} & and GCN model & \\multirow{-2}{*}{} & networks based on graph convolutional network \\\\\n\\rowcolor{gray!30}\nSGVB & Stochastic Gradient Variational Bayes & & \\\\\nSparseConv & Sparse Matrix Convolution &  & A deep learning based community detection approach \\\\ \\midrule\n\\rowcolor{gray!30}\nREM & Residual Entropy Minimization & & \\\\ \\midrule\nTGA/TVGA & Triad (Variational) Graph Autoencoder &  & Effective decoding in graph auto-encoder using triadic closure \\\\\n\\rowcolor{gray!30}\nTIN & Topologically Incomplete Networks&  &  \\\\\n& Transfer Learning-inspired Community & & High-performance community detection in social networks using \\\\\n\\multirow{-2}{*}{Transfer-CDDTA} & Detection with Deep Transitive Autoencoder & \\multirow{-2}{*}{} &  a deep transitive autoencoder \\\\\n\\rowcolor{gray!30}\nTPR & Triangle Participation Ratio & & \\\\ \\midrule\n& Unified Weight-free Multi-component & &  \\\\ \n\\multirow{-2}{*}{UWMNE} & Network Embedding & \\multirow{-2}{*}{} & \\multirow{-2}{*}{Integrative network embedding via deep joint reconstruction} \\\\ \\midrule\n\\rowcolor{gray!30}\nVAE & Variational AutoEncoder &  & Auto-encoding variational bayes \\\\\nVGAE & Variational Graph AutoEncoder &  & Variational graph auto-encoders \\\\\n\\rowcolor{gray!30}\n& Variational Graph Autoencoder for &  &  \\\\ \n\\rowcolor{gray!30}\n\\multirow{-2}{*}{VGAECD} & Community Detection & \\multirow{-2}{*}{} & \\multirow{-2}{*}{Learning community structure with variational autoencoder} \\\\ \n& Optimizing Variational Graph Autoencoder &  &  \\\\\n\\multirow{-2}{*}{VGAECD-OPT} & for Community Detection & \\multirow{-2}{*}{} & \\multirow{-2}{*}{Optimizing variational graph autoencoder for community detection} \\\\\n\\rowcolor{gray!30}\n& Variational Graph Embedding and Clustering &  &  \\\\\n\\rowcolor{gray!30}\n\\multirow{-2}{*}{VGECLE} & with Laplacian Eigenmaps & \\multirow{-2}{*}{} & \\multirow{-2}{*}{Variational graph embedding and clustering with Laplacian eigenmaps} \\\\ \\midrule\nWalkTrap & WalkTrap  &  & Computing communities in large networks using random walks \\\\\n\\rowcolor{gray!30}\n& & & A weighted network community detection algorithm based on deep \\\\\n\\rowcolor{gray!30}\n\\multirow{-2}{*}{WCD} & \\multirow{-2}{*}{Weighted Community Detection} & \\multirow{-2}{*}{} & learning \\\\\n& Weight-free Multi-Component Network &  &  \\\\\n\\multirow{-2}{*}{WMCNE-LE} & Embedding with Local Enhancement & \\multirow{-2}{*}{} & \\multirow{-2}{*}{Integrative network embedding via deep joint reconstruction} \\\\\n\\bottomrule[1pt]\n\\end{tabular}\\label{tab-abbr3}\n\\end{table*}\n\\end{document}", "cites": [6536, 6534, 6533, 2618, 6540, 6535, 7583, 9045, 9044, 6532, 6538, 220, 8564, 8252, 232, 9037, 6537, 5680, 6541, 9041, 9040, 229, 1430, 6542], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 72, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.2, "abstraction": 1.3}, "insight_level": "low", "analysis": "The section primarily serves as a list of abbreviations used in the survey, with minimal synthesis or integration of the cited papers. It lacks critical analysis or comparison of the methods, and offers no abstraction or generalization of broader trends or principles in deep learning for community detection."}}
