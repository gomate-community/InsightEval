{"id": "bdd927ec-b6c9-483f-bda4-225373bcb48c", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "bfdf8d73-ba63-4f59-a95e-9ffd530bd403", "prefix_titles": [["title", "Image Data Augmentation for Deep Learning: A Survey"], ["section", "Introduction"]], "content": "Deep learning has made incredible progress in many fields, including computer vision(CV)~, recommender system (RS)~, natural language processing (NLP)~  and so on. The development of these research field \nis mainly affected by the following three aspects: the progress of deep network architectures, great computing power, and access to big data.\t  \nFirstly, the scale of the network architectures is often proportional to its generalization ability, such as 152-layers ResNet~, which, compared with shallow network, can gain significant accuracy form the increased depth.\nSecondly, the development of computing power has a significant impact on deep learning. With stronger computing power, it is possible to design models with deeper architecture.\nFinally, sufficient open datasets like Imagenet~, MS-COCO~ and PASCAL VOC~ are crucial to the development of deep learning models. \nHowever, we observe some imbalance among the developments of these three perspectives.\nWhile various network architectures for different CV tasks have been proposed and the computation power of  graphics processing unit (GPU) have rapidly been increasing, fewer attention has been paid to using data augmentation methods to generate qualified training data. The core idea of data augmentation is to improve the sufficiency and diversity of training data by generating synthetic dataset. The augmented data can be regarded as being extracted from a distribution that is close to the real one. \nThen,  the augmented dataset can represent more comprehensive characteristics. But some research challenges remain in image data augmentation methods. \nFirst, image data augmentation techniques can be applied into various CV tasks, such as, object detection~, semantic segmentation~ and image classification~.\t \nBut the challenge is that data augmentation methods are tasks-independent. Because the operations are performed on the image data and labels at the same time, and the label types are different under different tasks, the data augmentation methods for object detection task can not be directly applied to semantic segmentation task. This results in inefficiency and low scalability.\nSecond, there is no theoretical research on data augmentation. For example, there is no quantitative standard on the size of sufficient training datasets. The size of generated training data is usually designed according to personal experience and extensive experiments. \nIn addition, paradox may exist when the size of the original dataset is so small. We will face the challenge of how to generate qualified data based on very little data.\nTo the best of our knowledge, works related to image data augmentation research did not review image data augmentation methods in terms of CV tasks.\nOne work~ explores and compares multiple solutions to the problem of data augmentation in image classification, but it only relates to image classification task and experiments with only traditional transformations and GANs.\n reviews existing face data augmentation works from perspectives of the transformation types and deep learning. However, the survey is  aimed at the face recognition tasks only.\nOne work mainly focuses on different data augmentation techniques based on data warping and oversampling~. However, it does not provide a systematic review of different approaches.\nAnother work closely related to ours is~ which present some existing methods and promising developments of data augmentation. However, it does not provide an evaluation on the effectiveness of data augmentation for various actual tasks and lacks some newly proposed methods, such as, CutMix~, AugMix~, GridMask~, etc.\nIn this paper, we aim to fill the aforementioned gaps by summarizing existing novel image data augmentation methods. To this end, we propose a taxonomy of image data augmentation methods, as illustrated in Fig.~\\ref{taxnomy}. Based on this taxonomy, we systematically review the data augmentation techniques from the perspectives of common CV tasks, including object detection, semantic segmentation and image classification. \nFurthermore, we also conduct experiments from the perspectives of these three CV tasks. \nBased on experiment results, we compare the performance of different kinds of data augmentation methods and their combinations on various deep learning models with open image datasets. \nWe will also discuss future directions for image data augmentation research. \n\\begin{figure}[]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{./pictures/Taxnomy2}\n\t\\caption{A taxonomy of image data augmentation methods.}\n\t\\label{taxnomy}\n\\end{figure}\nThe reminder of this paper is organized as follows. We present the basic data augmentation methods first, such as traditional image manipulation, image erasing based methods and image mix based methods.\nThen we discuss some advanced techniques, including auto augment based methods, feature augmentation techniques, and deep generative models. \nTo evaluate the effect of various kinds of data augmentation methods, we conduct experiments in three typical CV tasks with various common public image datasets.\nFinally, we highlight some promising directions for future research.", "cites": [4453, 8490, 7764, 97, 895, 5547, 107, 5546, 486, 3291, 1534], "cite_extract_rate": 0.6470588235294118, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.8, "abstraction": 3.3}, "insight_level": "medium", "analysis": "The section provides an analytical overview of image data augmentation in deep learning by identifying key challenges such as task independence and lack of theoretical grounding. It synthesizes prior surveys and research to highlight gaps in the literature, particularly the absence of a comprehensive, task-oriented review. While it introduces a taxonomy to organize methods, it does not fully connect the cited papers into a novel or deeply integrative framework."}}
{"id": "5370f2bd-e0f8-4472-a55f-ff2b787f6342", "title": "Image Erasing", "level": "subsection", "subsections": [], "parent_id": "f70f803b-14c7-44be-a9f4-fc3140fe3310", "prefix_titles": [["title", "Image Data Augmentation for Deep Learning: A Survey"], ["section", "Basic Data Augmentation Methods"], ["subsection", "Image Erasing"]], "content": "Image augmentation approaches based on image erasing typically delete one or more sub-regions in the image. The main idea is to replace the pixel values of these sub-regions with constant values or random values.  \nIn~, authors considered a simple regularization technique of randomly masking out square regions of input during training convolutional neural networks (CNNs), which is known as cutout. This method is capable of improving the robustness and overall performance of CNNs.\n proposed Hide-and-Seek (HaS) to hide patches in a training image randomly, which can force the network to seek other relevant content while the most discriminative content is hidden.\n~ proposed random erasing, which selects a rectangle region in an image randomly and replaces its pixels with random values. This method is simple, but makes significant improvements.\nRecently, in~, authors analyzed the requirement of information dropping and then proposed a structured method, GridMask, which is also based on the deletion of regions in the input images. Unlike Cutout and HaS, GridMask neither removes a continuous region nor randomly selects squares, the deleted regions are a set of spatially uniformly distributed squares, which can be controlled in terms of density and size. Furthermore, to balance the object occlusion and information retention, FenceMask~ was proposed, which is based on the simulation of object occlusion strategy.", "cites": [8319, 5549, 5548, 5547, 5550], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from five papers by connecting the theme of image erasing and its variations, such as Cutout, Hide-and-Seek, Random Erasing, GridMask, and FenceMask. It offers a coherent narrative by highlighting how each method introduces erasing as a form of regularization or robustness training. While it does provide some critical insights, such as noting the structured nature of GridMask and its advantages over random erasing, deeper comparative evaluation or limitations of these methods are not extensively discussed. The abstraction is moderate, as it identifies a broader strategy of information removal but does not fully elevate it to a meta-level principle."}}
{"id": "0c73cfd7-b7a1-4468-bea6-e2aa10421644", "title": "Image Mix", "level": "subsection", "subsections": [], "parent_id": "f70f803b-14c7-44be-a9f4-fc3140fe3310", "prefix_titles": [["title", "Image Data Augmentation for Deep Learning: A Survey"], ["section", "Basic Data Augmentation Methods"], ["subsection", "Image Mix"]], "content": "Image mix data augmentation has received increasing attention in recent years.\nThese methods are mainly completed by mixing two or more images or sub-regions of images into one.\nIn~, authors enlarge the dataset by synthesizing every new image with two images randomly selected in the training set, known as pairing samples. The synthesis method used is to average the intensity of two images on each pixel.\n~ discusses a more general synthesis method, Mixup. Mixup, which is not just average the intensity of two images, conducts convex combinations sample pairs and their labels. Therefore, Mixup establishes a linear relationship between data augmentation and the supervision signal and can regularize the neural network to favor simple linear behavior in-between training samples. \nSimilar with pairing samples and Mixup, ~ proposes the CutMix. Instead of simply removing pixels or mixing images from training set. CutMix replaces the removed regions with a patch from another image and can generate more natural images compared to Mixup. \n~ proposes Fmix that uses random binary masks obtained by applying a threshold to low-frequency images sampled from Fourier space.\nFmix can take on a wide range of shapes of random masks and can improve performance over Mixup and CutMix. Instead of mixing multiple samples, AugMix~ first mixes multiple augmentation operations into three augmentation chains and then mixes together the results of several augmentation chains in convex combinations. Therefore, the whole process is typically mixing the results generated by the same image in different augmentation pipelines. In ManifoldMix~, authors improve the hidden representations and decision boundaries of neural networks at multiple layers by mixing hidden representations rather than input samples.", "cites": [107, 7191, 858, 3291], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively synthesizes several key image mix augmentation methods, connecting them through a coherent narrative about their evolution and approaches to mixing data. It provides a critical analysis by highlighting differences in their strategies (e.g., averaging intensity vs. replacing regions vs. mixing in Fourier space) and noting that CutMix generates more natural images than Mixup. While it identifies some general trends in the use of convex combinations and regularization, it does not fully abstract to a meta-level principle but offers useful comparative insights."}}
{"id": "12d728d7-dbce-4f94-b2f3-fd436de666e4", "title": "Auto Augment", "level": "subsection", "subsections": [], "parent_id": "c85d1254-ed93-43f5-a6a8-11d462f65311", "prefix_titles": [["title", "Image Data Augmentation for Deep Learning: A Survey"], ["section", "Advanced Approaches"], ["subsection", "Auto Augment"]], "content": "Instead of manually designing data augmentation methods, researchers try to automatically search augmentation approaches to obtain improved performance. Auto augment has been the frontier of deep learning research and been extensively studied.\nAuto augment is based on the fact that different data have different characteristics, so different data augmentation methods have different benefits. Automatic searching for augmentation methods can bring more benefits than manual design.\n describes a simple procedure called AutoAugment to automatically search for improved data augmentation policies. Specifically, AutoAugment consists of two parts: search algorithm and search space. The search algorithm is designed to find the best policy regarding highest validation accuracy. The search space contains many policies which details various augmentation operations and magnitudes with which the operations are applied. \nHowever, a key challenge of auto augmentation methods is to choose an effective augmentation policy from a large search space of candidate operations. \nThe search algorithm usually uses Reinforcement Learning~, which brings high time cost.\nTherefore, to reduce the time cost of AutoAugment, ~ proposes Fast AutoAugment that finds effective augmentation policies via a more efficient search strategy based on density matching. In comparison to AutoAugment, this method can speed up the search time. Meanwhile, ~ proposes Population Based Augmentation (PBA) to reduce the time cost of AutoAugment which generates nonstationary augmentation policy schedules instead of a fixed augmentation policy. PBA can match the performance of AutoAugment on multiple datasets with less computation time.\nRecently, ~ proposed RandAugment surpassing all previous automated augmentation techniques, including AutoAugment and PBA.\nRandAugment dramatically reduces the search space for data augmentation by removing a separate search, which is computationally expensive. In addition, RandAugment further improves the performance of AutoAugment and PBA. \nHowever, data augmentation might introduce noisy augmented examples and bring negative influence on inference. Therefore,  ~ proposed KeepAugment to use the saliency map to detect important regions on the original images and then preserve these informative regions during augmentation. KeepAugment automatically improve the data augmentation schemes, such as AutoAugment. In~, authors observed that the augmentation operations in the later training period are more influential and proposed Augmentation-wise Weight Sharing strategy. Compared with AutoAugment, this work improves efficiency significantly and make it affordable to directly search on large scale datasets. \nUnlike auto-augmentation methods searching strategies in an offline manner,  ~ formulates the augmentation policy as a parameterized probability distribution and the parameters can be optimized jointly with network parameters, known as OHL-Auto-Aug.", "cites": [5552, 5551, 7954, 8928], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of Auto Augment and related methods, integrating multiple approaches such as PBA, RandAugment, and OHL-Auto-Aug. It connects ideas across papers by discussing common themes like computational efficiency and policy optimization. While it does identify some limitations (e.g., time cost of AutoAugment, noise in augmented examples), the analysis remains somewhat surface-level and does not offer deep or novel synthesis of these methods."}}
{"id": "b9673310-1933-4313-9b5e-4a766a070942", "title": "Feature Augmentation", "level": "subsection", "subsections": [], "parent_id": "c85d1254-ed93-43f5-a6a8-11d462f65311", "prefix_titles": [["title", "Image Data Augmentation for Deep Learning: A Survey"], ["section", "Advanced Approaches"], ["subsection", "Feature Augmentation"]], "content": "Rather than conduct augmentation only in the input space, feature augmentation performs the transformation in a learned feature space. In~, authors claimed that when traversing along the manifold it is more likely to encounter realistic samples in feature space than compared to input space. Therefore, various augmentation methods by manipulating the vector representation of data within a learned feature space are investigated, which includes adding noise, nearest neighbor interpolation and extrapolation.\nRecently, ~ proposed FeatMatch which is a novel learned feature-based refinement and augmentation method to produce a varied set of complex transformations. Moreover, FeatMatch can utilize information from both within-class and across-class prototypical representations.\nMore recently, authors in~ proposed an implicit data augmentation method, known as Moment Exchange, by encouraging the models to utilize the moment information of latent features. Specifically, the moments of the learned features of one training image are replaced by those of another.", "cites": [7955, 5554, 5553], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the concept of feature augmentation across three papers, connecting them under the idea of manipulating learned representations instead of input images. It abstracts the general approach but does not deeply analyze limitations or compare these methods in detail. The critical evaluation is limited, offering only basic observations about their mechanisms."}}
{"id": "29d4f5a8-e097-414c-92b2-d6c9bc531502", "title": "Deep Generative Models ", "level": "subsection", "subsections": [], "parent_id": "c85d1254-ed93-43f5-a6a8-11d462f65311", "prefix_titles": [["title", "Image Data Augmentation for Deep Learning: A Survey"], ["section", "Advanced Approaches"], ["subsection", "Deep Generative Models "]], "content": "The ultimate goal of data augmentation is to draw samples from the distribution, which represent the generating mechanism of dataset.\nHence, the data distribution we generate data from should not be different with the original one. This is the core idea of deep generative models. Among all the deep generative models methods, generative adversarial networks (GANs)~ are very representative methods. On the one hand, the generator can help generate new images. On the other hand, the discriminator ensures that the gap between the newly generated images and the original images is not too large. Although GAN has indeed been a powerful technique to perform unsupervised generation to augment data~, how to generate high-quality data and evaluate them still remains a challenging problems. In this subsection, we would like to introduce some image data augmentation techniques based on GAN.\nIn~, based on conditional adversarial networks~, authors proposed Pix2Pix to learn the mapping from the input images to output images. However, to train Pix2Pix, a large amount of paired data is needed. It is challenging to collect the paired data. Therefore, in ~, unlike Pix2Pix, a CycleGAN model is proposed to learn the translation on an image from the source domain $X$ to a target domain $Y$ in the absence of paired samples.\nAs the number of source and target domains increases, CycleGAN has to train models for each paired domain separately. For instance, if the task is to do transformation among $n$ domains, we need to train $n \\times (n-1)$ models between every two domains. To deal with this issue, ~ proposed StarGAN to improve the scalability and robustness in handling more than two domains. Generally, StarGAN builds only one model to perform image-to-image translation among multiply domains. In the generation phase, we just need to provide the generator with the source image and an attribute label which indicates the target domain. However, StarGAN takes the domain label as an additional input and learns a deterministic mapping per each domain, which may result in the same output per each domain given an input image. To address this problem, ~ proposed StarGAN $v2$, which is a scalable approach that can generate diverse images across multiple domains. \nIn this work, researchers define the domain and style of images as visually distinct category groups and the specific appearance of each image, respectively.\nFor example, the dog can be used as a domain, but there are many kinds of dogs, such as Labrador and Husky. Therefore, the specific dog breed can be viewed as the style of an image. \nIn this way, StarGAN $v2$ can translate an image of one domain to diverse images of a target domain, and support multiple domains.", "cites": [7022, 7764, 6996, 529, 896], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers on GAN-based data augmentation, connecting the evolution from Pix2Pix to CycleGAN and finally to StarGAN v2. It provides a critical perspective by highlighting limitations such as the need for paired data and lack of diversity in earlier models. The abstraction is strong as it introduces the concept of domain and style to generalize the framework beyond individual techniques."}}
{"id": "2952386a-75f7-483d-aafa-a86ad0914b69", "title": "Semantic Segmentation", "level": "subsection", "subsections": [], "parent_id": "e4daa695-5294-4f1b-b7af-01025271e626", "prefix_titles": [["title", "Image Data Augmentation for Deep Learning: A Survey"], ["section", "Evaluation"], ["subsection", "Semantic Segmentation"]], "content": "\\label{segmentation}\nIn this subsection, we conduct experiments of semantic segmentation on PASCAL VOC dataset. In table~\\ref{semantic_segmentation}, we report the performance improvement on Intersection over Union(IoU) metric with several semantic segmentation models: deeplabv3+~, PSPNet~, GCNet~, and ISANet~. \nIn our experiment, we apply different data augmentation methods based on our taxonomy in Fig~\\ref{taxnomy}. Specifically, the applied image manipulation methods include flipping, scaling ratio, rotation, noise injection, cropping, translation and sharpening.\nThe applied image erasing methods include random erasing, GridMask, FenceMask, Cutout, and HaS.\nThe applied image mix methods include mosaic, Mixup, CutMix, and Fmix. \nTable~\\ref{semantic_segmentation} presents the mean IoU  with and without data augmentation on semantic segmentation models. We observe that data augmentation methods bring IoU improvements for all models.\n\\begin{table}[h]\n\t\\huge\n\t\\resizebox{.95\\columnwidth}{!}{\n\t\t\\renewcommand\\arraystretch{1.5}{\n\t\t\\begin{tabular}{llll}\n\t\t\t\\toprule[2pt]\n\t\t\tModel   & {\\begin{tabular}[c]{@{}l@{}}w/o aug\\\\ (\\%)\\end{tabular}} &{\\begin{tabular}[c]{@{}l@{}}w/ aug\\\\ (\\%)\\end{tabular}}& {\\begin{tabular}[c]{@{}l@{}}Improvement\\\\ (\\%)\\end{tabular}} \\\\ \\midrule\n\t\t\tDeepLabV3+ &75.32\\% &75.75\\% &0.53\\% \\\\\n\t\t\tPSPNet     &73.38\\% &74.33\\% &0.95\\% \\\\\n\t\t\tGCNet      &71.86\\% &72.93\\% &1.07\\% \\\\ \n\t\t\tISANet\t   &71.65\\% &74.26\\% &2.61\\% \\\\\n\t\t\t\\bottomrule[1.5pt]\n\t\\end{tabular}}}\n\t\\caption{Semantic segmentation improvement from data augmentation based on IoU and accuracy.}\n\t\\label{semantic_segmentation}\n\\end{table}", "cites": [4858, 2575], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section presents a comparison of data augmentation effects on semantic segmentation models, including results from specific models like ISANet and GCNet. However, it lacks synthesis of the underlying ideas from the cited papers and does not critically analyze or evaluate the methodologies or limitations of the approaches. The abstraction level is minimal, as the section focuses primarily on reporting performance metrics without identifying broader trends or principles."}}
{"id": "3b4450a3-e5e8-4124-8855-de021e25805a", "title": "Image Classification", "level": "subsection", "subsections": [], "parent_id": "e4daa695-5294-4f1b-b7af-01025271e626", "prefix_titles": [["title", "Image Data Augmentation for Deep Learning: A Survey"], ["section", "Evaluation"], ["subsection", "Image Classification"]], "content": "In this experiment, we compare the classification accuracy with and without augmentation on several widely used image classification techniques, including  Wide-ResNet~, DenseNet~, and Shake ResNet~. These models are evaluated on several public image classification datasets, including CIFAR-10~, CIFAR-100~ and SVHN~.  Moreover, the data augmentation methods applied are the same with those in~\\ref{segmentation}, which includes several image manipulation methods, image erasing methods and image mix methods.\nTable~\\ref{classify_res} summarizes the image classification results with and without data augmentation. It can be observed that data augmentation leads to average accuracy improvement(AAI).\n\\begin{table}[ht]\n \\LARGE\n\t\t\\resizebox{.95\\columnwidth}{!}{\n\t\t\t \\renewcommand\\arraystretch{1.5}{\n\t\t\t\\begin{tabular}{lllll}\n\t\t\t\t\\toprule[1.5pt]\n\t\t\t\tDataset  & Model & {\\begin{tabular}[c]{@{}l@{}}w/o aug\\\\ (\\%)\\end{tabular}} &{\\begin{tabular}[c]{@{}l@{}}w/ aug\\\\ (\\%)\\end{tabular}}& {\\begin{tabular}[c]{@{}l@{}}AAI\\\\ (\\%)\\end{tabular}} \\\\ \\midrule\n\t\t\t\t& DenseNet & 94.15 & 94.59 & 0.44  \\\\  \n\t\t\t\tCIFAR-10 & Wide-ResNet & 93.34 & 94.67 &  1.33    \\\\ \n\t\t\t\t& Shake-ResNet &93.7 & 94.84 & 1.11    \\\\ \\midrule \n\t\t\t\t& DenseNet & 74.98  & 75.93  & 0.95   \\\\  \n\t\t\t\tCIFAR-100 & Wide-ResNet & 74.46  &  76.52  & 2.06    \\\\ \n\t\t\t\t& Shake-ResNet & 73.96   & 76.76  &  2.80  \\\\  \\midrule\n\t\t\t\t& DenseNet & 97.91  & 97.98  & 0.07   \\\\  \n\t\t\t\tSVHN & Wide-ResNet & 98.23  &  98.31  & 0.80    \\\\ \n\t\t\t\t& Shake-ResNet & 98.37  & 98.40  & 0.30     \\\\  \n\t\t\t\t\\bottomrule[1.5pt]\n\t\t\\end{tabular}}}\n\t\t\\caption{Image classification accuracy improvement from data augmentation on CIFAR-10, CIFAR-100, and SVHN.}\n\t\t\\label{classify_res}\n\t\\end{table}", "cites": [5555, 96, 7956], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a descriptive overview of image classification experiments using data augmentation with specific models and datasets. It lacks synthesis of broader themes from the cited papers and does not critically evaluate their strengths or limitations. There is minimal abstraction beyond the specific results presented."}}
{"id": "843b00cc-d28d-4a56-9517-378690917438", "title": "Discussion for Future Directions", "level": "section", "subsections": [], "parent_id": "bfdf8d73-ba63-4f59-a95e-9ffd530bd403", "prefix_titles": [["title", "Image Data Augmentation for Deep Learning: A Survey"], ["section", "Discussion for Future Directions"]], "content": "Despite extensive efforts on image data augmentation research to bring performance improvement on deep learning models, several open problems remain yet to completely to solve, which are summarized as follows.\\\\\n\t\\textbf{Theoretical Research on Data Augmentation.} There is a lack of theoretical research on data augmentation. Data augmentation is more regarded as an auxiliary tool to improve the performance. Specifically, some methods can improve the accuracy, but we do not fully understand the reasons behind, such as pairing samples and mixup. To human eyes, the augmented data with pairing samples and mixup are visually meaningless. Furthermore, there is no theory on the size of sufficient training datasets.  The size of the dataset suitable for tasks and models is usually designed based on personal experience and through extensive experiments. For example, researchers determine the size of datasets according to the specific models, training objectives, and the difficulty of data collection.  \t\n\tRigorous and thorough interpretability can not only explain why some augmentation techniques are useful but also can help guide the process of choosing or designing the most applicable and effective methods to enlarge our datasets. Thus, a critical future perspective is to develop theoretical support for data augmentation.\\\\\n\t\\textbf{The Evaluation of Data Augmentation Methods.} The quantity and diversity of training data are of great importance to model's generalization ability. However, since there is no unified metrics,\thow to evaluate the synthesized image quality is an open problem~. At this stage, researchers evaluate the quality of the synthetic data in several following ways. \n\tFirst, the synthetic data are usually evaluated by human eyes, which is time-consuming, labor-intensive, and subjective. Amazon Mechanical Turk (AMT) is often used to evaluate the realism of outputs. AMT evaluates the quality and authenticity of the generated images by asking participants to vote for various images synthesized with different methods. \n\tSecond, some studies combine the evaluation with specific tasks, which is to evaluate data augmentation methods according to their effect on the tasks metrics with and without data augmentation, such as classification tasks with classification accuracy and semantic segmentation with IOU of masks.\n\tHowever, there is no evaluation index only for the synthetic data itself. \n\tGenerally, the evaluation metrics are based on diversity of individual data and consistency of overall data distribution regardless of what task is. Data quality analysis can help design evaluation metrics .\\\\\n\t\\textbf{Class Imbalance.} class imbalance or very few data can  severely skew the data distribution~. This situation occurs since the learning process is often biased toward the majority class examples, so that minority ones are not well modeled.\n\t Synthetic minority of oversampling technique (SMOTE)~ is to oversample the minority class. However, the oversampling is repeating drawing from the current data set.\n\tThis may saturate the minority class and cause overfitting. Ultimately, we expect generated data can simulate distribution similar with training data while diversity never losses.\\\\\n\t \\textbf{The Number of Generated Data.} \tAn interesting point for data augmentation is that the increase in the amount of training data is not exactly proportional to the increase in the performance. When a certain amount of data is reached, continue to increase the data without improving the effect. This may partly because, despite the increase in the number of data, the diversity of data remains unchanged.\n\t Thus, how much data should be generated is good enough to improve the model performance remains to be further explored.\\\\\n\t\\textbf{The Selection and Combination of Data Augmentation.} Since various data augmentation can be combined together to generate new image data, the selection and combination of data augmentation techniques are critical.  Image recognition experiment shows that results from~ combined methods are often better than single method. Therefore, how to choose and combine methods is a key point when performing data augmentation.\n\tHowever, from our evaluation, the methods applicable for different datasets and tasks are not the same.\n\t Therefore, the set of augmentation methods must be carefully designed, implemented, and tested for every new task and dataset.", "cites": [117], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a coherent analytical overview of key challenges in data augmentation, including theoretical gaps, evaluation issues, class imbalance, and method selection. It integrates some cited concepts, like SMOTE, but lacks deeper synthesis across multiple papers. The section identifies limitations and open problems, showing a critical perspective, though it could offer more nuanced comparisons and evaluations. It abstracts from specific methods to broader themes like diversity, realism, and performance trade-offs."}}
