{"id": "0f099f18-e4c4-412a-82fe-c0c62d3104eb", "title": "Introduction", "level": "section", "subsections": ["14feeb0c-118f-418e-a254-a2a30068fd1e"], "parent_id": "1ea135c8-d69d-401d-8fff-aca1042236fb", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Introduction"]], "content": "Semantic segmentation has recently become one of the fundamental problems, and accordingly, a hot topic for the fields of computer vision and machine learning. Assigning a separate class label to each pixel of an image is one of the important steps in building complex robotic systems such as driverless cars/drones, human-friendly robots, robot-assisted surgery, and intelligent military systems. Thus, it is no wonder that in addition to scientific institutions, industry-leading companies studying artificial intelligence are now summarily confronting this problem. \nThe simplest problem definition for semantic segmentation is pixel-wise labelling. Because the problem is defined at the pixel level, finding only class labels that the scene includes is considered insufficient, but localising labels at the original image pixel resolution is also a fundamental goal. Depending on the context, class labels may change. For example, in a driverless car, the pixel labels may be \\emph{human, road} and \\emph{car}  whereas for a medical system , they could be \\emph{cancer cells, muscle tissue, aorta wall} etc.  \nThe recent increase in interest in this topic has been undeniably caused by the extraordinary success seen with convolutional neural networks  (CNN) that have been brought to semantic segmentation. Understanding a scene at the semantic level has long been one of the main topics of computer vision, but it is only now that we have seen actual solutions to the problem.\nIn this paper, our primary motivation is to focus on the recent scientific developments in semantic segmentation, specifically on the evolution of deep learning-based methods using 2D images. The reason we narrowed down our survey to techniques that utilise only 2D visible imagery is that, in our opinion, the scale of the problem in the literature is so vast and widespread that it would be impractical to analyse and categorise all semantic segmentation modalities (such as 3D point clouds, hyper-spectral data, MRI, CT\\footnote{We consider MRI and CT essentially as 3D volume data. Although individual MRI/CT slices are 2D, when doing semantic segmentation on these types of data, neighbourhood information in all three dimensions are utilised. For this reason, medical applications are excluded from this survey.} etc.) found in journal articles to any degree of detail. In addition to analysing the techniques which make semantic segmentation possible and accurate, we also examine the most popular image sets created for this problem. Additionally, we review the performance measures used for evaluating the success of semantic segmentation. Most importantly, we propose a taxonomy of methods, together with a technical evolution of them, which we believe is novel in the sense that it provides insight to the existing deficiencies and suggests future directions for the field.\nThe remainder of the paper is organised as follows: in the following subsection, we refer to other survey studies on the subject and underline our contribution. Section 2 presents information about the different image sets, the challenges, and how to measure the performance of semantic segmentation. Starting with Section 3, we chronologically scrutinise semantic segmentation methods under three main titles, hence in three separate sections. Section 3 covers the methods of pre- and early deep convolutional neural networks era. Section 4 provides details on the fully convolutional neural networks, which we consider a milestone for the semantic segmentation literature. Section 5 covers the state-of-the-art methods on the problem and provides details on both the architectural details and the success of these methods. Before finally concluding the paper in Section 7, Section 6 provides a future scope and potential directions for the field.", "cites": [1725], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a general overview of semantic segmentation and its applications, integrating context from the cited paper on automated driving. It begins to synthesize information by connecting the role of CNNs with the broader problem of semantic understanding in computer vision. While it introduces a taxonomy and hints at critical evaluation by highlighting methodological limitations, it does not deeply compare or critique the cited works. The abstraction is moderate, as it outlines broader trends and challenges in the field, particularly in the choice of 2D imagery over other modalities."}}
{"id": "14feeb0c-118f-418e-a254-a2a30068fd1e", "title": "Surveys on Semantic Segmentation", "level": "subsection", "subsections": [], "parent_id": "0f099f18-e4c4-412a-82fe-c0c62d3104eb", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Introduction"], ["subsection", "Surveys on Semantic Segmentation"]], "content": "Very recently, driven by both academia and industry, the rapid increase of interest in semantic segmentation has inevitably led to a number of survey studies being published .\nSome of these surveys focus on a specific problem, such as comparing semantic segmentation approaches for horizon/skyline detection , whilst others deal with relatively broader problems related to industrial challenges, such as semantic segmentation for driverless cars   or medical systems . These studies are useful if working on the same specific problem, but they lack an overarching vision that may `technically' contribute to the future directions of the field.\nAnother group  of survey studies on semantic segmentation have provided a general overview of the subject, but they lack the necessary depth of analysis regarding deep learning-based methods. Whilst semantic segmentation was studied for two decades prior to deep learning, actual contribution to the field has only been achieved very recently, particularly following a revolutionary paper on fully convolutional networks (FCN)  (which has also been thoroughly analysed in this paper). It could be said that most state-of-the-art studies are in fact extensions of that same  study. For this reason, without scrupulous analysis of FCNs and the direction of the subsequent papers, survey studies will lack the necessary academic rigour in examining semantic segmentation using deep learning.\nThere are recent reviews of deep semantic segmentation, for example by  and , which provide a comprehensive survey on the subject. These survey studies cover almost all the popular semantic segmentation image sets and methods, and for all modalities such as 2D, RGB, 2.5D, RGB-D, and 3D data. Although they are inclusive in the sense that most related material on deep semantic segmentation is included, the categorisation of the methods is coarse, since the surveys attempt to cover almost everything umbrellaed under the topic of semantic segmentation literature. \nA detailed categorisation of the subject was provided in . Although this survey provides important details on the subcategories that cover almost all approaches in the field, discussions on how the proposed techniques are chronologically correlated are left out of their scope. Recent deep learning studies on semantic segmentation follow a number of fundamental directions and labour with tackling the varied corresponding issues. In this survey paper, we define and describe these new challenges, and present the chronological evolution of the techniques of all the studies within this proposed context. We believe our attempt to understand the evolution of semantic segmentation architectures is the main contribution of the paper. We provide a table of these related methods, and explain them briefly one after another in chronological order, with their metric performance and computational efficiency. This way, we believe that readers will better understand the evolution, current state-of-the-art, as well as the future directions seen for 2D semantic segmentation.", "cites": [1728, 1726, 976, 8490, 1727, 1729, 1725, 810], "cite_extract_rate": 0.8, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes information from multiple survey papers, identifying common themes and shortcomings such as lack of depth, chronological correlation, or overarching vision. It provides critical analysis by pointing out limitations in existing surveys and positioning its own contribution within this context. The discussion abstracts beyond individual papers to define broader challenges and trends in deep learning-based semantic segmentation, particularly emphasizing the evolution of methods post-FCN."}}
{"id": "4eb62e15-9432-4e24-b9a3-5e206a5b328a", "title": "General Purpose Semantic Segmentation Image Sets", "level": "subsubsection", "subsections": [], "parent_id": "5553cbf6-1d47-4afe-a8b9-1457c090e311", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Image Sets, Challenges and Performance Evaluation"], ["subsection", "Image Sets and Challenges"], ["subsubsection", "General Purpose Semantic Segmentation Image Sets"]], "content": "\\begin{itemize}\n\\item PASCAL Visual Object Classes (VOC) : This image set includes image annotations not only for semantic segmentation, but for also classification, detection, action classification, and person layout tasks. The image set and annotations are regularly updated and the leaderboard of the challenge is public\\footnote{\\url{ http://host.robots.ox.ac.uk:8080/leaderboard/main\\_bootstrap.php}} (with more than 140 submissions just for the segmentation challenge alone). It is the most popular among the semantic segmentation challenges and is still active following its initial release in 2005. The PASCAL VOC semantic segmentation challenge image set includes 20 foreground object classes and one background class. The original data consisted of 1,464 images for the purposes of training, plus 1,449 images for validation. The 1,456 test images are kept private for the challenge. The image set includes all types of indoor and outdoor images and is generic across all categories.\nThe PASCAL VOC image set has a number of extension image sets, the most popular among these are PASCAL Context  and PASCAL Parts . The first  is a set of additional annotations for PASCAL VOC 2010, which goes beyond the original PASCAL semantic segmentation task by providing annotations for the whole scene. The statistics section contains a full list of more than 400 labels (compared to the original 21 labels). The second  is also a set of additional annotations for PASCAL VOC 2010. It provides segmentation masks for each body part of the object, such as the separately labelled limbs and body of an animal. For these extensions, the training and validation set contains 10,103 images, while the test set contains 9,637 images. There are other extensions to PASCAL VOC using other functional annotations such as the Semantic Parts (PASParts)  image set and the Semantic Boundaries Dataset (SBD) . For example, PASParts  additionally provides `instance’ labels such as two instances of an object within an image are labelled separately, rather than using a single class label. However, unlike the former two additional extensions , these further extensions  have proven less popular as their challenges have attracted much less attention in state-of-the-art semantic segmentation studies; thus, their leaderboards are less crowded. In Figure \\ref{PascalParts}, a sample object, parts and instance segmentation are depicted.\n\\begin{figure*}[t]\n\\centering\n\\includegraphics*[clip=false,width=1\\textwidth]{figures/Fig1.png}\n\\caption{A sample image and its annotation for object, instance and parts segmentations separately, from left to right.}\n \\label{PascalParts}\n\\end{figure*} \n\\item Common Objects in Context (COCO) : With 200K labelled images, 1.5 million object instances, and 80 object categories, COCO is a very large scale object detection, semantic segmentation, and captioning image set, including almost every possible types of scene. COCO provides challenges not only at the instance-level and pixel-level (which they refer to as \\emph{stuff}) semantic segmentation, but also introduces a novel task, namely that of \\emph{panoptic} segmentation , which aims at unifying instance-level and pixel-level segmentation tasks. Their leaderboards\\footnote{\\url{http://cocodataset.org}}  are relatively less crowded because of the scale of the data. On the other hand, for the same reason, their challenges are assessed only by the most ambitious scientific and industrial groups, and thus are considered as the state-of-the-art in their leaderboards. Due to its extensive volume, most studies partially use this image set to pre-train or fine-tune their model, before submitting to other challenges such as PASCAL VOC 2012. \n\\item ADE20K dataset : ADE20K contains more than 20K scene-centric images with objects and object parts annotations. Similarly to PASCAL VOC, there is a public leaderboard\\footnote{\\url{http://sceneparsing.csail.mit.edu/}} and the benchmark is divided into 20K images for training, 2K images for validation, and another batch of held-out images for testing. The samples in the dataset have varying resolutions (average image size being 1.3M pixels), which can be up to 2400$\\times$1800 pixels. There are total of150 semantic categories included for evaluation.\n\\item Other General Purpose Semantic Segmentation Image Sets: Although less popular than either PASCAL VOC or COCO, there are also some other image sets in the same domain. Introduced by , YouTube-Objects is a set of low-resolution (480$\\times$360) video clips with more than 10k pixel-wise annotated frames. \nSimilarly, SIFT-flow  is another low-resolution (256$\\times$256) semantic segmentation image set with 33 class labels for a total of 2,688 images. These and other relatively primitive image sets have been mostly abandoned in the semantic segmentation literature due to their limited resolution and low volume.\n\\end{itemize}", "cites": [486, 1728, 1732, 1730, 1731], "cite_extract_rate": 0.6, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual summary of several semantic segmentation datasets and their characteristics, including PASCAL VOC, COCO, and ADE20K. It integrates some information from the cited papers, such as the goals and tasks of the datasets, but lacks deeper synthesis or analysis of the relationships between these datasets or their impact on the field. There is minimal critical evaluation or abstraction to broader trends in the literature."}}
{"id": "6fde11ca-1b1d-49b1-8c49-f2bf6e69390f", "title": "Urban Street Semantic Segmentation Image Sets", "level": "subsubsection", "subsections": [], "parent_id": "5553cbf6-1d47-4afe-a8b9-1457c090e311", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Image Sets, Challenges and Performance Evaluation"], ["subsection", "Image Sets and Challenges"], ["subsubsection", "Urban Street Semantic Segmentation Image Sets"]], "content": "\\begin{itemize}\n\\item Cityscapes : This is a largescale image set with a focus on the semantic understanding of urban street scenes. It contains annotations for high-resolution images from 50 different cities, taken at different hours of the day and from all seasons of the year, and also with varying backgrounds and scene layouts. The annotations are carried out at two quality levels: fine for 5,000 images and course for 20,000 images. There are 30 different class labels, some of which also have instance annotations (vehicles, people, riders etc.). Consequently, there are two challenges with separate public leaderboards\\footnote{\\url{https://www.cityscapes-dataset.com/benchmarks/}}: one for pixel-level semantic segmentation, and a second for instance-level semantic segmentation. There are more than 100 entries to the challenge, making it the most popular regarding semantic segmentation of urban street scenes.\n\\item Other Urban Street Semantic Segmentation Image Sets: There are a number of alternative image sets for urban street semantic segmentation, such as Cam-Vid , KITTI , SYNTHIA , and IDD . These are generally overshadowed by the Cityscapes image set  for several reasons. Principally, their scale is relatively low. Only the SYNTHIA image set  can be considered as largescale (with more than 13k annotated images); however, it is an artificially generated image set, and this is considered a major limitation for security-critical systems like driverless cars.\n\\end{itemize}", "cites": [1733], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of the Cityscapes dataset and its challenges, as well as a brief listing of other urban street image sets. It integrates some information from the cited paper but does so in a limited and descriptive manner without deeper connections or analysis. The section lacks critical evaluation or abstraction to broader trends in the field."}}
{"id": "8a457c68-568c-4588-accf-9bac8b3fc0dc", "title": "Small-scale and Imbalanced Image Sets", "level": "subsubsection", "subsections": [], "parent_id": "5553cbf6-1d47-4afe-a8b9-1457c090e311", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Image Sets, Challenges and Performance Evaluation"], ["subsection", "Image Sets and Challenges"], ["subsubsection", "Small-scale and Imbalanced Image Sets"]], "content": "In addition to the aforementioned large-scale image sets of different categories, there are several image sets with insufficient scale or strong imbalance such that, when applied to deep learning-based semantic segmentation models, high-level segmentation accuracies cannot be directly obtained. Most public challenges on semantic segmentation include sets of this nature such as the DSTL or RIT-18 , just to name a few. Because of the overwhelming numbers of these types of sets, we chose to include only the details of the large-scale sets that attract the utmost attention from the field. \nNonetheless, being able to train a model that performs well on small-scale or imbalanced data is a correlated problem to ours. Besides conventional deep learning techniques such as transfer learning or data augmentation; the problem of insufficient or imbalanced data can be attacked by using specially designed deep learning architectures such as some optimized convolution layer types (, etc.) and others that we cover in this survey paper. What is more, there are recent studies that focus on the specific problem of utilizing insufficient sets for the problem of deep learning-based semantic segmentation . Although we acknowledge this problem as fundamental for the semantic segmentation field, we leave the discussions on techniques to handle small-scale or imbalanced sets for semantic segmentation, beyond the scope of this survey paper.", "cites": [509, 1734], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions the problem of small-scale and imbalanced image sets but does not deeply synthesize or integrate the cited papers. It references two papers but does not elaborate on how they contribute to the discussion or connect their ideas. The critical and abstract dimensions are similarly underdeveloped, as it lacks evaluation of the approaches and fails to generalize or identify broader patterns or principles in the field."}}
{"id": "e2ee2643-5821-4de5-8c1b-c00fd3023aca", "title": "Accuracy", "level": "subsubsection", "subsections": [], "parent_id": "7e7a3568-4f20-471e-ba07-d512cd6c93e4", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Image Sets, Challenges and Performance Evaluation"], ["subsection", "Performance Evaluation"], ["subsubsection", "Accuracy"]], "content": "Measuring the performance of segmentation can be complicated, mainly because there are two distinct values to measure. The first is classification, which is simply determining the pixel-wise class labels; and the second is localisation, or finding the correct set of pixels that enclose the object. Different metrics can be found in the literature to measure one or both of these values. The following is a brief explanation of the principal measures most commonly used in evaluating semantic segmentation performance. \n\\begin{itemize}\n    \\item\\emph{ROC-AUC}: ROC stands for the Receiver-Operator Characteristic curve, which summarises the trade-off between true positive rate and false-positive rate for a predictive model using different probability thresholds; whereas AUC stands for the area under this curve, which is 1 at maximum. This tool is useful in interpreting binary classification problems and is appropriate when observations are balanced between classes. However, since most semantic segmentation image sets  are not balanced between the classes, this metric is no longer used by the most popular challenges.\n    \\item\\emph{Pixel Accuracy}: Also known as \\emph{global accuracy} , pixel accuracy (PA) is a very simple metric which calculates the ratio between the amount of properly classified pixels and their total number. Mean pixel accuracy (mPA), is a version of this metric which computes the ratio of correct pixels on a per-class basis. mPA is also referred to as \\emph{class average accuracy} . \n    \\begin{equation}\\label{pa}\n        PA=\\frac{\\sum_{j=1}^{k}{n_{jj}}}{\\sum_{j=1}^{k}{t_{j}}}, \\qquad mPA=\\frac{1}{k}\\sum_{j=1}^{k}\\frac{n_{jj}}{t_{j}}\n    \\end{equation}     \n    where $n_{jj}$ is the total number of pixels both classified and labelled as class \\emph{j}. In other words, $n_{jj}$ corresponds to the total number of \\textit{True Positives} for class \\emph{j}. $t_{j}$  is the total number of pixels labelled as class \\emph{j}.\n    \\item\\emph{Intersection over Union} (IoU): Also known as the Jaccard Index, IoU is a statistic used for comparing the similarity and diversity of sample sets. In semantics segmentation, it is the ratio of the intersection of the pixel-wise classification results with the ground truth, to their union.\n    \\begin{equation}\\label{iu}\n    IoU=\\frac{\\sum_{j=1}^{k}{n_{jj}}}{\\sum_{j=1}^{k}({n_{ij}+n_{ji}+n_{jj}})}, \\qquad i \\neq j\n    \\end{equation}\n    where, $n_{ij}$ is the number of pixels which are labelled as class \\emph{i}, but classified as class \\emph{j}. In other words, they are  \\textit{False Positives} (false alarms) for class \\emph{j}. Similarly, ${n_{ji}}$, the total number of pixels labelled as class \\emph{j}, but classified as class \\emph{i} are the \\textit{False Negatives} (misses) for class \\emph{j}.\n    Two extended versions of IoU are also widely in use:\n    $\\circ$\\emph{ Mean Intersection over Union} (mIoU): mIoU is the class-averaged IoU, as in (\\ref{miou}).\n    \\begin{equation}\\label{miu}\n        mIoU=\\frac{1}{k}\\sum_{j=1}^{k}\\frac{n_{jj}}{n_{ij}+n_{ji}+n_{jj}}, \\qquad i \\neq j\n    \\label{miou}    \n    \\end{equation}\n    $\\circ$\\emph{ Frequency-weighted IoU} (FwIoU): This is an improved version of MIoU that weighs each class importance depending on appearance frequency by using $t_{j}$ (the total number of pixels labelled as class \\emph{j}, as also defined in  (\\ref{pa})). The formula of FwIoU is given in (\\ref{fiu}):\n    \\begin{equation}\\label{fiu}\n        FwIoU=\\frac{1}{\\sum_{j=1}^{k}t_{j}}\\sum_{j=1}^{k}{t_{j}\\frac{n_{jj}}{n_{ij}+n_{ji}+n_{jj}}}, \\qquad i \\neq j\n    \\end{equation}\n     IoU and its extensions, compute the ratio of true positives (hits) to the sum of false positives (false alarms), false negatives (misses) and true positives (hits). Thereby, the IoU measure is more informative when compared to pixel accuracy simply because it takes false alarms into consideration, whereas PA does not. However, since false alarms and misses are summed up in the denominator, the significance between them is not measured by this metric, which is considered its primary drawback. In addition, IoU only measures the number of pixels correctly labelled without considering how accurate the segmentation boundaries are.\n     \\item\\emph{Precision-Recall Curve (PRC)-based metrics}: Precision (ratio of hits over a summation of hits and false alarms) and recall (ratio of hits over a summation of hits and misses) are the two axes of the PRC used to depict the trade-off between precision and recall, under a varying threshold for the task of binary classification. PRC is very similar to ROC. However, PRC is more powerful in discriminating the effects between the false positives (alarms) and false negatives (misses). That is predominantly why PRC-based metrics are commonly used for evaluating the performance of semantic segmentation. The formula for Precision (also called Specificity) and Recall (also called Sensitivity) for a given class \\emph{j}, are provided in (\\ref{prerec}):\n    \\begin{equation}\\label{prerec}\n       Prec.=\\frac{n{_{jj}}}{n_{ij}+n_{jj}}, \\quad\n        Recall=\\frac{n_{jj}}{n_{ji}+n_{jj}},  i \\neq j\n    \\end{equation}\n    There are three main PRC-based metrics:\n   $\\circ$\\emph{ F}$_{score}$:  Also known as the ‘\\emph{dice coefficient}’, this measure is the harmonic mean of the precision and recall for a given threshold. It is a normalised measure of similarity, and ranges between 0 and 1 (Please see (\\ref{f})).\n    \\begin{equation}\\label{f}\n        F_{score}=2 \\times \\frac{Precision\\times Recall}{Precision+Recall}\n    \\end{equation}\n    $\\circ$\\emph{ PRC-AuC}: This is similar to the ROC-AUC metric. It is simply the area under the PRC. This metric refers to information about the precision-recall trade-off for different thresholds, but not the \\emph{shape} of the PR curve.\n    $\\circ$\\emph{ Average Precision} (AP): This metric is a single value that summarises both the shape and the AUC of PRC. In order to calculate AP, using the PRC, for uniformly sampled recall values (e.g., 0.0, 0.1, 0.2, ..., 1.0), precision values are recorded. The average of these precision values is referred to as the average precision. This is the most commonly used single value metric for semantic segmentation. Similarly, mean average precision (mAP) is the mean of the AP values, calculated on a per-class basis.\n       \\item\\emph{Hausdorff Distance} (HD): Hausdorff Distance is used incorporating the longest distance between classified and labelled pixels as an indicator of the largest segmentation error , with the aim of tracking the performance of a semantic segmentation model. The unidirectional HDs as $hd(X,Y)$ and $hd(Y,X)$ are presented in (\\ref{hdu1}) and (\\ref{hdu2}), respectively.\n    \\begin{equation}\\label{hdu1}\n    hd\\left ( X, Y \\right )= \\max_{{x\\epsilon X}}\\min_{{y\\epsilon Y}}\\left \\| x-y \\right \\|_{2},\n    \\end{equation}\n    \\begin{equation}\\label{hdu2}\n    hd\\left ( Y, X \\right )= \\max_{{y\\epsilon Y}}\\min_{{x\\epsilon X}}\\left \\| x-y \\right \\|_{2}.\n    \\end{equation}\n  where, $X$ and $Y$ are the pixel sets. The $x$ is the pixel in the segmented counter $X$ and $y$ is the pixel in the target counter $Y$ . The bidirectional HD between these sets is shown in (\\ref{hdb}), where the Euclidean distance is employed for (\\ref{hdu1}), (\\ref{hdu2}) and (\\ref{hdb}).\n    \\begin{equation}\\label{hdb}\n    HD\\left ( X,Y \\right )= max\\left ( hd\\left ( X,Y \\right ),hd\\left ( Y,X \\right ) \\right ).\n    \\end{equation}\n\\end{itemize}\nIoU and its variants, along with AP, are the most commonly used accuracy evaluation metrics in the most popular semantic segmentation challenges .", "cites": [1735, 486, 7501, 1732, 1733, 1736, 1731], "cite_extract_rate": 0.6363636363636364, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a structured analytical overview of accuracy metrics for semantic segmentation, integrating concepts from the cited papers by explaining their relevance (e.g., HD in medical image segmentation from paper 1735). It offers comparisons between metrics like PA, IoU, and AP, highlighting their strengths and limitations. While it generalizes patterns (e.g., imbalance in image sets affecting ROC-AUC), the critique of individual papers is limited, and the synthesis remains focused on technical details rather than deeper conceptual integration."}}
{"id": "51517df9-2061-423a-b542-fcd5b7bd4352", "title": "Computational Complexity", "level": "subsubsection", "subsections": [], "parent_id": "7e7a3568-4f20-471e-ba07-d512cd6c93e4", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Image Sets, Challenges and Performance Evaluation"], ["subsection", "Performance Evaluation"], ["subsubsection", "Computational Complexity"]], "content": "The burden of computation is evaluated using two main metrics: how fast the algorithm completes, and how much computational memory is demanded. \n\\begin{itemize}\n\\item\\emph{Execution time}: This is measured as the whole processing time, starting from the instant a single image is introduced to the system/algorithm right through until the pixel-wise semantic segmentation results are obtained. The performance of this metric significantly depends on the hardware utilised. Thus, for an algorithm, any execution time metric should be accompanied by a thorough description of the hardware used. There are notations such as Big-O, which provide a complexity measure independent of the implementation domain. However, these notations are highly theoretical and are predominantly not preferred for extremely complex algorithms such as deep semantic segmentation as they are simple and largely inaccurate.\nFor a deep learning-based algorithm, the offline (i.e., training) and online (i.e., testing) operation may last for considerably different time intervals. Technically, the execution time refers only to the online operation or, academically speaking, the test duration for a single image. Although this metric is extremely important for industrial applications, academic studies refrain from publishing exact execution times, and none of the aforementioned challenges was found to have provided this metric. A recent study,  provided a 2D histogram of Accuracy (MIoU\\%) vs frames-per-second, in which some of the state-of-the-art methods with open source codes (including their proposed structure, namely image cascade network – ICNet), were benchmarked using the Cityscapes   image set.\n\\item\\emph{Memory Usage}: Memory usage is specifically important when semantic segmentation is utilised in limited performance devices such as smartphones, digital cameras, or when the requirements of the system are extremely restrictive. The prime examples of these would be military systems or security-critical systems such as self-driving cars. \nThe usage of memory for a complex algorithm like semantic segmentation may change drastically during operation. That is why a common metric for this purpose is \\emph{peak memory usage}, which is simply the maximum memory required for the entire segmentation operation for a single image. The metric may apply to computer (data) memory or GPU memory depending on the hardware design.\nAlthough critical for industrial applications, this metric is not usually made available for any of the aforementioned challenges.\n\\end{itemize}\nComputational efficiency is a very important aspect of any algorithm that is to be implemented on a real system. A comparative assessment of the speed and capacity of various semantic segmentation algorithms is a challenging task. Although most state-of-the-art algorithms are available with open-source codes, benchmarking all of them, with their optimal hyper-parameters, seems implausible. \nFor this purpose, we provide an inductive way of comparing the computational efficiencies of methods in the following sections. In Table \\ref{MethodsTable}, we categorise methods into mainly four levels of computational efficiency and discuss our categorisation related to the architectural design of a given method. This table also provides a chronological evolution of the semantic segmentation methods in the literature.", "cites": [1733], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of computational complexity in semantic segmentation, discussing key metrics like execution time and memory usage. It synthesizes concepts from the cited work (e.g., Cityscapes benchmark) to highlight how these metrics are evaluated in practice. While it identifies the lack of standardization and availability of such metrics in academic studies, the analysis remains somewhat general and could benefit from deeper cross-paper comparisons and more structured abstraction of underlying principles."}}
{"id": "085f3dfd-8d14-4669-8d3e-ab87b2ddc4fa", "title": "Before Fully Convolutional Networks", "level": "section", "subsections": ["9b3cff5a-4f9e-4315-a55c-3cf708aadcfd", "ffef2800-e682-4cee-9f8a-34513a7ec32c"], "parent_id": "1ea135c8-d69d-401d-8fff-aca1042236fb", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Before Fully Convolutional Networks"]], "content": "As mentioned in the Introduction, the utilisation of FCNs is a breaking point for semantic segmentation literature. Efforts on semantic segmentation literature prior to FCNs  can be analysed in two separate branches, as pre-deep learning and early deep learning approaches. In this section, we briefly discuss both sets of approaches.", "cites": [810], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides minimal synthesis and no meaningful integration of cited papers beyond a brief mention of FCNs as a breaking point. It does not compare or contrast pre-deep learning and early deep learning approaches in any depth, nor does it offer critical evaluation or abstract insights into broader trends or principles in the field."}}
{"id": "9b3cff5a-4f9e-4315-a55c-3cf708aadcfd", "title": "Pre-Deep Learning Approaches", "level": "subsection", "subsections": ["3eabd60a-239f-4226-bbde-0b39185bcfff"], "parent_id": "085f3dfd-8d14-4669-8d3e-ab87b2ddc4fa", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Before Fully Convolutional Networks"], ["subsection", "Pre-Deep Learning Approaches"]], "content": "The differentiating factor between conventional image segmentation and semantic segmentation is the utilisation of semantic features in the process. Conventional methods for image segmentation such as thresholding, clustering, and region growing, etc. (\\emph{please see}  \\emph{for a survey on conventional image segmentation techniques}) utilise handcrafted low-level features (i.e., edges, blobs) to locate object boundaries in images. Thus, in situations where the semantic information of an image is necessary for pixel-wise segmentation, such as in similar objects occluding each other, these methods usually return a poor performance.\nRegarding semantic segmentation efforts prior to DCNNs becoming popular, a wide variety of approaches  utilised graphical models, such as Markov Random Fields (MRF), Conditional Random Fields (CRF) or forest-based (or sometimes referred to as ‘holistic’) methods, in order to find scene labels at the pixel level. The main idea was to find an inference by observing the dependencies between neighbouring pixels. In other words, these methods modelled the semantics of the image as a kind of prior information among adjacent pixels. Thanks to deep learning, today we know that image semantics require abstract exploitation of largescale data. Initially, graph-based approaches were thought to have this potential. The so-called ‘super-pixelisation’, which is usually the term applied in these studies, was a process of modelling abstract regions. However, a practical and feasible implementation for largescale data processing was never achieved for these methods, while it was accomplished for DCNNs, first by  and then in many other studies.\nAnother group of studies, sometimes referred to as the ‘Layered models’ , used a composition of pre-trained and separate object detectors so as to extract the semantic information from the image. Because the individual object detectors failed to classify regions properly, or because the methods were limited by the finite number of object classes provided by the ‘hand-selected’ bank of detectors in general, their performance was seen as relatively low compared to today’s state-of-the-art methods.\nAlthough the aforementioned methods of the pre-deep learning era are no longer preferred as segmentation methods, some of the graphical models, especially CRFs, are currently being utilised by the state-of-the-art methods as post-processing (refinement) layers, with the purpose of improving the semantic segmentation performance, the details of which are discussed in the following section.\n\\begin{figure*}[t]\n\\centering\n\\begin{subfigure}{4.8cm}\t\n    \\centering\n\t\\includegraphics*[trim=0 0 0 0,width=1.0\\textwidth]{figures/Figure2a.png}\n\t\\caption{Input Image}\t\n\\end{subfigure}\n\\begin{subfigure}{4.8cm}\n    \\centering\n\t\\includegraphics*[trim=0 0 0 0,width=1.0\\textwidth]{figures/Figure2b.png}\n\t\\caption{Segmented Image}\t\n\\end{subfigure}\n\\begin{subfigure}{4.8cm}\n    \\centering\n\t\\includegraphics*[trim=0 0 0 0,width=1.0\\textwidth]{figures/Figure2c.png}\n\t\\caption{Refined Result}\t \n\\end{subfigure}\n\\caption{Effect of using graphical model-based refinement on segmentation results.}\n \\label{CRFRefinement}\n\\end{figure*}", "cites": [810, 1737], "cite_extract_rate": 0.1111111111111111, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of pre-deep learning approaches by categorizing them into graphical models and layered models, and it connects these ideas to modern practices, such as the use of CRFs as refinement layers. It integrates two cited papers to highlight the transition from graph-based models to deep learning, though the synthesis is somewhat limited by the lack of additional comparative references. Critical analysis is present in pointing out limitations of early methods, but a deeper, more nuanced critique is missing."}}
{"id": "3eabd60a-239f-4226-bbde-0b39185bcfff", "title": "Refinement Methods", "level": "subsubsection", "subsections": [], "parent_id": "9b3cff5a-4f9e-4315-a55c-3cf708aadcfd", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Before Fully Convolutional Networks"], ["subsection", "Pre-Deep Learning Approaches"], ["subsubsection", "Refinement Methods"]], "content": "Deep neural networks are powerful in extracting abstract local features. However, they lack the capability to utilise global context information, and accordingly cannot model interactions between adjacent pixel predictions . On the other hand, the popular segmentation methods of the pre-deep learning era, the graphical models, are highly suited to this sort of task. That is why they are currently being used as a refinement layer on many DCNN-based semantic segmentation architectures.\nAs also mentioned in the previous section, the idea behind using graphical models for segmentation is finding an inference by observing the low-level relations between neighbouring pixels. In Figure \\ref{CRFRefinement}, the effect of using a graphical model-based refinement on segmentation results can be seen. The classifier (see Figure \\ref{CRFRefinement}.b) cannot correctly segment pixels where different class labels are adjacent. In this example, a CRF-based refinement  is applied to improve the pixel-wise segmentation results. CRF-based methods are widely used for the refinement of deep semantic segmentation methods, although some alternative graphical model-based refinement methods also exist in the literature . \nCRFs  are a type of discriminative undirected probabilistic graphical model. They are used to encode known relationships between observations and to construct consistent interpretations. Their usage as a refinement layer comes from the fact that, unlike a discrete classifier, which does not consider the similarity of adjacent pixels, a CRF can utilise this information. The main advantage of CRFs over other graphical models (such as Hidden Markov Models) is their conditional nature and their ability to avoid the problem of label bias . Even though a considerable number of methods (see Table \\ref{MethodsTable}) utilise CRFs for refinement, these models started to lose popularity in relatively recent approaches because they are notoriously slow and very difficult to optimise .", "cites": [1739, 1738, 1737], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key ideas from the cited papers by connecting the use of CRFs and MRFs as refinement methods in pre-deep learning and early deep learning approaches. It critically evaluates CRFs by highlighting their advantages (e.g., avoiding label bias) and limitations (e.g., computational slowness). The abstraction level is strong, as the section generalizes the role of graphical models in refining segmentation outputs and explains their relevance to broader challenges like global context and pixel interaction modeling."}}
{"id": "ffef2800-e682-4cee-9f8a-34513a7ec32c", "title": "Early Deep Learning Approaches", "level": "subsection", "subsections": [], "parent_id": "085f3dfd-8d14-4669-8d3e-ab87b2ddc4fa", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Before Fully Convolutional Networks"], ["subsection", "Early Deep Learning Approaches"]], "content": "\\label{pre-early-deep}\nBefore FCNs first appeared in 2014\\footnote{FCN  ] was officially published in 2017. However, the same group first shared the idea online as pre-printed literature in 2014  .}, the initial few years of deep convolutional networks saw a growing interest in the idea of utilising the newly discovered deep features for semantic segmentation . The very first approaches, which were published prior to the proposal of a Rectified Linear Unit (ReLU) layer , used activation functions such as \\emph{tanh}  (or similar continuous functions), which can be computationally difficult to differentiate. Thus, training such systems were not considered to be computation-friendly, or even feasible for largescale data.\nHowever, the first mature approaches were just simple attempts to convert classification networks such Alex-Net and VGG to segmentation networks by fine-tuning the fully connected layers . They suffered from the overfitting and time-consuming nature of their fully connected layers in the training phase. Moreover, the CNNs used were not sufficiently deep so as to create abstract features, which would relate to the semantics of the image.\nThere were a few early deep learning studies in which the researchers declined to use fully connected layers for their decision-making. However, they utilised different structures such as a recurrent architecture  or using labelling from a family of separately computed segmentations . By proposing alternative solutions to fully connected layers, these early studies showed the first traces of the necessity for a structure like the FCN, and unsurprisingly they were succeeded by .\nSince their segmentation results were deemed to be unsatisfactory, these studies generally utilised a refinement process, either as a post-processing layer or as an alternative architecture to fully connected decision layers . Refinement methods varied, such as Markov random fields , nearest neighbour-based approach , the use of a calibration layer , using super-pixels , or a recurrent network of plain CNNs . Refinement layers, as discussed in the previous section, are still being utilised by post-FCN methods, with the purpose of increasing the pixel-wise labelling performance around regions where class intersections occur.", "cites": [1740, 810], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates the cited works into a narrative about early deep learning methods for semantic segmentation, highlighting limitations of fully connected layers and early solutions like recurrent architectures and nearest neighbor-based approaches. It provides some critical evaluation of these methods, particularly their computational inefficiency and unsatisfactory results, while also identifying the broader trend of moving away from fully connected layers toward more spatially aware structures. While not forming a novel framework, it successfully generalizes early challenges and solutions, showing a clear progression toward the FCN era."}}
{"id": "486bc016-7734-4261-b9f7-ad3f68d1c163", "title": "Fully Convolutional Networks for Semantic Segmentation", "level": "section", "subsections": [], "parent_id": "1ea135c8-d69d-401d-8fff-aca1042236fb", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Fully Convolutional Networks for Semantic Segmentation"]], "content": "In , the idea of dismantling fully connected layers from deep CNNs (DCNN) was proposed, and to imply this idea, the proposed architecture was named as ‘Fully Convolutional Networks’ (see Figure 3). The main objective was to create semantic segmentation networks by adapting classification networks such as AlexNet , VGG  , and GoogLeNet  into fully convolutional networks, and then transferring their learnt representations by fine-tuning. The most widely used architectures obtained from the study  are known as ‘FCN-32s’, ‘FCN16s’, and ‘FCN8s’, which are all transfer-learnt using the VGG architecture .\nFCN architecture was considered revolutionary in many aspects. First of all, since FCNs did not include fully connected layers, inference per image was seen to be considerably faster. This was mainly because convolutional layers when compared to fully connected layers, had a marginal number of weights. Second, and maybe more significant, the structure allowed segmentation maps to be generated for images of any resolution. In order to achieve this, FCNs used deconvolutional layers that can upsample coarse deep convolutional layer outputs to dense pixels of any desired resolution. Finally, and most importantly, they proposed the skip architecture for DCNNs.\n\\begin{figure}[t]\n\\centering\n\\includegraphics*[trim=20 0 80 10,clip=false,width=0.98\\textwidth]{figures/Fig_FCN_Skip_Connections.pdf}\n\\caption{Fully convolutional networks (FCNs) are trained end-to-end and are designed to\nmake dense predictions for per-pixel tasks like semantic segmentation. FCNs consist of no fully connected layers .}\n \\label{FCNs}\n\\end{figure} \nSkip architectures (or connections) provide links between nonadjacent layers in DCNNs. Simply by summing or concatenating outputs of unconnected layers, these connections enable information to flow, which would otherwise be lost because of an architectural choice such as max-pooling layers or dropouts. The most common practice is to use skip connections preceding a max-pooling layer, which downsamples layer output by choosing the maximum value in a specific region. Pooling layers helps the architecture create feature hierarchies, but also causes loss of localised information which could be valuable for semantic segmentation, especially at object borders. Skip connections preserve and forward this information to deeper layers by way of bypassing the pooling layers. Actually, the usage of skip connections in   was perceived as being considerably primitive. The ‘FCN-8s’ and ‘FCN-16s’ networks included these skip connections at different layers. Denser skip connections for the same architecture, namely ‘FCN-4s’ and ‘FCN-2s’, were also utilised for various applications . This idea eventually evolved into the encoder-decoder structures  for semantic segmentation, which are presented in the following section.", "cites": [1741, 97, 7501, 810, 305], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes key concepts from multiple papers, particularly linking the introduction of FCNs to their influence on encoder-decoder structures and skip connections. It provides a coherent narrative on the evolution of FCN variants and their technical implications. However, the critical analysis is limited—while it mentions that skip connections in early FCNs were 'primitive,' it does not deeply evaluate the trade-offs or shortcomings of these methods. The abstraction level is moderate, as it identifies the broader impact of skip connections and FCN structures on semantic segmentation, but does not reach a meta-level analysis of design principles."}}
{"id": "a810e757-8e18-4cb5-b9eb-b720ad7146bd", "title": "Post-FCN Approaches", "level": "section", "subsections": ["8056ef15-b70f-46d2-aa67-e56388a68b74", "b43e8d58-b826-4b10-90a7-9a74f992259e", "34b80f3b-2d89-4524-9834-3bce20359d42", "5c2bb78c-80a6-489a-b0eb-e5a37bb4c679"], "parent_id": "1ea135c8-d69d-401d-8fff-aca1042236fb", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Post-FCN Approaches"]], "content": "Almost all subsequent approaches on semantic segmentation followed the idea of FCNs; thus it would not be wrong to state that decision-making with fully-connected layers effectively ceased to exist\\footnote{Many methods utilise fully connected layers such as RCNN , which are discussed in the following sections. However, this and other similar methods that include fully connected layers have mostly been succeeded by fully convolutional versions for the sake of computational efficiency.} following the appearance of FCNs to the issue of semantic segmentation.\nOn the other hand, the idea of FCNs also created new opportunities to further improve deep semantic segmentation architectures. Generally speaking, the main drawbacks of FCNs can be summarised as inefficient loss of label localisation within the feature hierarchy, inability to process global context knowledge, and the lack of a mechanism for multiscale processing. Thus, most subsequent studies have been principally aimed at solving these issues through the proposal of various architectures or techniques. For the remainder of this paper, we analyse these issues under the title, ‘fine-grained localisation’. Consequently, before presenting a list of the post-FCN state-of-the-art methods, we focus on this categorisation of techniques and examine different approaches that aim at solving these main issues. In the following, we also discuss scale invariance in the semantic segmentation context and finish with object detection-based approaches, which are a new breed of solution that aim at resolving the semantic segmentation problem simultaneously with detecting object instances.\n\\begin{figure}[ht]\n\\centering\n\\begin{subfigure}{8cm}\t\n\t\\includegraphics*[clip=false,trim=23 20 25 10,width=1\\textwidth]{figures/Fig_Localizatino_a.pdf}\n\t\\caption{Encoder-Decoder Architecture.}\t\n\\end{subfigure}\n\\begin{subfigure}{8cm}\t\n\t\\includegraphics*[trim=0 0 25 -10,clip=false,width=1\\textwidth]{figures/SPPLayers.pdf}\n\t\\caption{Spatial-Pyramid Pooling Layer}\t\n\\end{subfigure}\n\\begin{subfigure}{8cm}\t\n\t\\includegraphics*[trim=-10 50 0 -30,clip=false,width=1\\textwidth]{figures/Fig_Dilated_Conv.png}\n\t\\caption{Regular vs. Dilated Convolutions}\t\n\\end{subfigure}\n\\begin{subfigure}{8cm}\t\n\t\\includegraphics*[trim=0 0 0 40,clip=true,width=1\\textwidth]{figures/Fig_DeepLab3.png}\n\t\\caption{DeepLabv3+ Architecture}\t\n\\end{subfigure}\n\\begin{subfigure}{8cm}\t\n\t\\includegraphics*[trim=0 310 0 0,clip=false,width=1\\textwidth]{figures/Fig_DlinkNet.png}\n\t\\caption{DlinkNet Architecture}\t\n\\end{subfigure}\n\\caption{Different architectures for fine-grained pixel-wise label localisation.}\n\\label{Localization}\n\\end{figure}", "cites": [8429], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section identifies the general influence of FCNs and outlines the main challenges they introduced, which shows a moderate level of abstraction. However, it lacks detailed synthesis of the cited papers and provides minimal critical evaluation of the methods discussed. The inclusion of figures helps illustrate different approaches but does not replace in-depth analysis or comparative evaluation."}}
{"id": "35163cd9-a439-440c-8c93-c9c656c80cab", "title": "Encoder-Decoder Architecture", "level": "subsubsection", "subsections": [], "parent_id": "8056ef15-b70f-46d2-aa67-e56388a68b74", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Post-FCN Approaches"], ["subsection", "Techniques for Fine-grained Localisation"], ["subsubsection", "Encoder-Decoder Architecture"]], "content": "The so-called Encoder-Decoder (ED) architectures (also known as the U-nets, referring to the pioneering study of ) are comprised of two parts. Encoder gradually reduces the spatial dimension with pooling layers, whilst decoder gradually recovers the object details and spatial dimension. Each feature map of the decoder part only directly receives the information from the feature map at the same level of the encoder part using skip connections, thus EDs can create abstract hierarchical features with fine localisation (see Figure \\ref{Localization}.a). U-Net  and Seg-Net  are very well-known examples. In this architecture, the strongly correlated semantic information, which is provided by the adjacent lower-resolution feature map of the encoder part, has to pass through additional intermediate layers in order to reach the same decoder layer. This usually results in a level of information decay. However, U-Net architectures have proven very useful for the segmentation of different applications, such as medical images , street view images , satellite images , just to name a few. Although earlier ED architectures were designed for object segmentation tasks only, there are also modified versions such as ``TernausNetV2'' , that provide instance segmentation capability with minor architectural changes.", "cites": [1741, 7501, 1742, 1728], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the concept of encoder-decoder architectures using multiple cited papers, highlighting their structure and purpose, and shows how they address fine-grained localization. It also introduces a limitation (information decay) and mentions a modified version (TernausNetV2) that extends the architecture for instance segmentation. While it offers some analytical perspective, it does not deeply compare or critique the different implementations or generalize extensively to broader principles."}}
{"id": "840460fa-3679-40ac-baf4-ed5a3c96c169", "title": "Spatial Pyramid Pooling", "level": "subsubsection", "subsections": [], "parent_id": "8056ef15-b70f-46d2-aa67-e56388a68b74", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Post-FCN Approaches"], ["subsection", "Techniques for Fine-grained Localisation"], ["subsubsection", "Spatial Pyramid Pooling"]], "content": "The idea of constructing a fixed-sized spatial pyramid was first proposed by , in order to prevent a Bag-of-Words system losing spatial relations among features. Later, the approach was adopted to CNNs by , in that, regardless of the input size, a spatial pyramid representation of deep features could be created in a Spatial Pyramid Pooling Network (SPP-Net). The most important contribution of the SPP-Net was that it allowed inputs of different sizes to be fed into CNNs. Images of different sizes fed into convolutional layers inevitably create different-sized feature maps. However, if a pooling layer, just prior to a decision layer, has stride values proportional to the input size, the feature map created by that layer would be fixed (see Figure \\ref{Localization}.b). By , a modified version, namely Pyramid Attention Network (PAN) was additionally proposed. The idea of PAN was combining an SPP layer with global pooling to learn a better feature representation.\nThere is a common misconception that SPP-Net structure carries an inherent scale-invariance property, which is incorrect. SPP-Net allows the efficient training of images at different scales (or resolutions) by allowing different input sizes to a CNN. However, the trained CNN with SPP is scale-invariant if, and only if, the training set includes images with different scales. This fact is also true for a CNN without SPP layers.\nHowever, similar to the original idea proposed in , the SPP layer in a CNN constructs relations among the features of different hierarchies. Thus, it is quite similar to skip connections in ED structures, which also allow information flow between feature hierarchies.\nThe most common utilisation of an SPP layer for semantic segmentation is proposed in , such that the SPP layer is appended to the last convolutional layer and fed to the pixel-wise classifier.", "cites": [509, 1743], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates the core concepts from both the SPP-Net and PAN papers, showing how the spatial pyramid pooling idea evolved and was adapted for semantic segmentation. It also corrects a common misconception about scale invariance, demonstrating some critical analysis. However, the synthesis remains limited to the two papers and lacks deeper comparative or meta-level insights that would elevate the abstraction and critical evaluation further."}}
{"id": "d5e4d293-145c-4e6d-b9e4-ea7e84ccbde4", "title": "Feature Concatenation", "level": "subsubsection", "subsections": [], "parent_id": "8056ef15-b70f-46d2-aa67-e56388a68b74", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Post-FCN Approaches"], ["subsection", "Techniques for Fine-grained Localisation"], ["subsubsection", "Feature Concatenation"]], "content": "This idea is based on fusing features extracted from different sources. For example, in  the so-called ‘DeepMask’ network utilises skip connections in a feed-forward manner, so that an architecture partially similar to both SPP layer and ED is obtained. The same group extends this idea with a top-down refinement approach of the feed-forward module and propose the so-called ‘SharpMask’ network , which has proven to be more efficient and accurate in segmentation performance. Another approach from this category is the so-called ‘ParseNet’ , which fuses CNN features with external global features from previous layers in order to provide context knowledge. Another approach by  is to fuse the ``stage features'' (i.e. deep encoder activations) with ``refinement path features'' (an idea similar to skip connections), using a convolutional (Feature Adaptive Fusion FAF) block.  Although a novel idea in principle, feature fusion approaches (including SPP) create hybrid structures, therefore they are relatively difficult to train.", "cites": [1744, 8491], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly describes a few approaches that use feature concatenation for fine-grained localisation but lacks deep synthesis, as it does not fully integrate the ideas from the cited papers into a broader framework. It provides minimal critical evaluation of the methods and no clear abstraction or identification of overarching principles in the field."}}
{"id": "5a2d7d3b-6ed9-406f-99b0-8211a126106e", "title": "Dilated Convolution", "level": "subsubsection", "subsections": [], "parent_id": "8056ef15-b70f-46d2-aa67-e56388a68b74", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Post-FCN Approaches"], ["subsection", "Techniques for Fine-grained Localisation"], ["subsubsection", "Dilated Convolution"]], "content": "The idea of dilated (atrous) convolutions is actually quite simple: with contiguous convolutional filters, an effective receptive field of units can only grow linearly with layers; whereas with dilated convolution, which has gaps in the filter (see Figure \\ref{Localization}.c), the effective receptive field would grow much more quickly . Thus, with no pooling or subsampling, a rectangular prism of convolutional layers is created. Dilated convolution is a very effective and powerful method for the detailed preservation of feature map resolutions. The negative aspect of the technique, compared to other techniques, concerns its higher demand for GPU storage and computation, since the feature map resolutions do not shrink within the feature hierarchy .", "cites": [97], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of dilated convolutions and their benefits for preserving feature map resolution, but it does not effectively synthesize information from the cited paper (Deep Residual Learning for Image Recognition). There is no critical evaluation of the technique or its limitations beyond a superficial note on GPU demands. The abstraction is minimal, as it does not generalize the concept to broader trends or principles in semantic segmentation."}}
{"id": "4f7624ce-cdb7-4ecc-ada8-bc7b705fe0e0", "title": "Conditional Random Fields", "level": "subsubsection", "subsections": [], "parent_id": "8056ef15-b70f-46d2-aa67-e56388a68b74", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Post-FCN Approaches"], ["subsection", "Techniques for Fine-grained Localisation"], ["subsubsection", "Conditional Random Fields"]], "content": "As also discussed in Section 3.1.1, CNNs naturally lack mechanisms to specifically ‘focus’ on regions where class intersections occur. Around these regions, graphical models are used to find inference by observing low-level relations between neighbouring feature maps of CNN layers. Consequently, graphical models, mainly CRFs, are utilised as refinement layers in deep semantic segmentation architectures. As in , CRFs connect low-level interactions with output from multiclass interactions, and in this way, global context knowledge is constructed.\nAs a refinement layer, various methods exist that employ CRFs to DCNNs, such as the Convolutional CRFs , the Dense CRF , and CRN-as-RNN . In general, CRFs help build context knowledge and thus a finer level of localisation in class labels.", "cites": [1739, 1737, 1745], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from three papers, connecting the role of CRFs as refinement layers that address CNN limitations in fine-grained localization. It abstracts the function of CRFs beyond individual methods, highlighting their contribution to building context knowledge and improving label localization. However, while it mentions the use of CRFs and some forms of integration with DCNNs, it does not deeply critique their limitations or compare them in detail with alternative refinement techniques."}}
{"id": "390bb8b3-5f84-4c7b-bd9a-5dc971c729e7", "title": "Recurrent Approaches", "level": "subsubsection", "subsections": [], "parent_id": "8056ef15-b70f-46d2-aa67-e56388a68b74", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Post-FCN Approaches"], ["subsection", "Techniques for Fine-grained Localisation"], ["subsubsection", "Recurrent Approaches"]], "content": "The ability of Recurrent Neural Networks (RNNs) to handle sequential information can help improve segmentation accuracy. For example,  used Conv-LSTM layers to improve their semantic segmentation results in image sequences. However, there are also methods that use recurrent structures on still images. For example, the Graph LSTM network  is a generalization of LSTM from sequential data or multidimensional data to general graph-structured data for semantic segmentation on 2D still images. Graph-RNN  is another example of a similar approach in which an LSTM-based network is used to fuse a deep encoder output with the original image in order to obtain a finer pixel-level segmentation. Likewise, in , the researchers utilised LSTM-chains in order to intertwine multiple scales, resulting in pixel-wise segmentation improvements. There are also hybrid approaches where CNNs and RNNs are fused. A good example of this is the so-called ReSeg model , in which the input image is fed to a VGG-like CNN encoder, and is then processed afterwards by recurrent layers (namely the ReNet architecture) in order to better localise the pixel labels. Another similar approach is the DAG-RNN , which utilize a DAG-structured CNN+RNN network, and models long-range semantic dependencies among image units. To the best of our knowledge, no purely recurrent structures for semantic segmentation exist, mainly because semantic segmentation requires a preliminary CNN-based feature encoding scheme.\nThere is currently an increasing trend in one specific type of RNN, namely ‘recurrent attention modules’. In these modules, attention  is technically fused in the RNN, providing a focus on certain regions of the input when predicting a certain part of the output sequence. Consequently, they are also being utilised in semantic segmentation .\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{12cm}\t\n\t\\includegraphics*[trim=0 0 0 0,clip=false,width=1\\textwidth]{figures/Fig_MaskRCNN.pdf}\n\t\\caption{Regions with CNN features-based (Mask-RCNN) architecture}\t\n\\end{subfigure}\n\\begin{subfigure}{12cm}\t\n\t\\includegraphics*[trim=0 0 0 0,clip=false,width=1\\textwidth]{figures/Fig_YOLACT.png}\n\t\\caption{Fully Convolutional Object Detector-based (YOLO)-based architecture}\t\n\\end{subfigure}\n\\caption{Different architectures for object detection-based semantic segmentation methods}\n\\label{ObjectBased}\n\\end{figure}", "cites": [7063, 38, 8492, 1748, 6977, 1746, 1747], "cite_extract_rate": 0.875, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of recurrent approaches in semantic segmentation, mentioning several papers and their methods but without substantial synthesis or critical evaluation. It lacks in-depth comparison or meta-level abstraction, and while it notes the absence of purely recurrent models for segmentation, it does not elaborate on the implications or evaluate the effectiveness of RNN-based components in the cited works."}}
{"id": "b43e8d58-b826-4b10-90a7-9a74f992259e", "title": "Scale-Invariance", "level": "subsection", "subsections": [], "parent_id": "a810e757-8e18-4cb5-b9eb-b720ad7146bd", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Post-FCN Approaches"], ["subsection", "Scale-Invariance"]], "content": "Scale Invariance is, by definition, the ability of a method to process a given input, independent of the relative scale (i.e. the scale of an object to its scene) or image resolution. Although it is extremely crucial for certain applications, this ability is usually overlooked or is confused with a method’s ability to include multiscale information. A method may use multiscale information to improve its pixel-wise segmentation ability, but can still be dependent on scale or resolution. That is why we find it necessary to discuss this issue under a different title and to provide information on the techniques that provide scale and/or resolution invariance.\nIn computer vision, any method can become scale invariant if trained with multiple scales of the training set. Some semantic segmentation methods such as   utilise this strategy. However, these methods do not possess an inherent scale-invariance property, which is usually obtained by normalisation with a global scale factor (such as in SIFT by ). This approach is not usually preferred in the literature on semantic segmentation. The image sets that exist in semantic segmentation literature are extremely large in size. Thus, the methods are trained to memorise that training set, because in principle, overfitting a largescale training set is actually tantamount to solving the entire problem space.", "cites": [1223, 37, 1749], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section attempts to synthesize the concept of scale invariance by differentiating it from multiscale information, referencing relevant papers. It identifies a conceptual gap in the field by noting that many methods do not inherently possess scale-invariance. However, the critical analysis is limited and lacks deeper evaluation or comparison of the cited works. The section abstracts to a moderate level by highlighting general trends in training and method design but stops short of a comprehensive meta-level framework."}}
{"id": "34b80f3b-2d89-4524-9834-3bce20359d42", "title": "Object Detection-based Methods", "level": "subsection", "subsections": [], "parent_id": "a810e757-8e18-4cb5-b9eb-b720ad7146bd", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Post-FCN Approaches"], ["subsection", "Object Detection-based Methods"]], "content": "There has been a recent growing trend in computer vision, which aims at specifically resolving the problem of object detection, that is, establishing a bounding box around all objects within an image. Given that the image may or may not contain any number of objects, the architectures utilised to tackle such a problem differ to the existing fully-connected/convolutional classification or segmentation models.\n\\begin{figure*}[t]\n\\centering\n\\begin{subfigure}{3.2cm}\t\n\t\\includegraphics*[trim=0 0 0 0,clip=false,width=1\\textwidth]{figures/Fig6a_original.jpg}\n\t\\caption{\\scriptsize image}\t\n\\end{subfigure}\n\\begin{subfigure}{3.2cm}\t\n\t\\includegraphics*[trim=0 0 0 0,clip=false,width=1\\textwidth]{figures/Fig6b_groundtruth.png}\n\t\\caption{\\scriptsize{reference}}\t\n\\end{subfigure}\n\\begin{subfigure}{3.2cm}\t\n\t\\includegraphics*[trim=0 0 0 0,clip=false,width=1\\textwidth]{figures/Fig6c_fcn32s.png}\n\t\\caption{\\scriptsize{FCN-32S}}\t\n\\end{subfigure}\n\\begin{subfigure}{3.2cm}\t\n\t\\includegraphics*[trim=0 0 0 0,clip=false,width=1\\textwidth]{figures/Fig6d_fcn8s.png}\n\t\\caption{\\scriptsize FCN-8S}\t\n\\end{subfigure}\n\\begin{subfigure}{3.2cm}\t\n\t\\includegraphics*[trim=0 0 0 0,clip=false,width=1\\textwidth]{figures/Fig6i_CMSA.png}\n\t\\caption{\\scriptsize CMSA}\t\n\\end{subfigure}\n\\begin{subfigure}{3.2cm}\t\n\t\\includegraphics*[trim=0 0 0 0,clip=false,width=1\\textwidth]{figures/Fig6e_DeepLabV1.png}\n\t\\caption{\\scriptsize{DeepLabv1}}\t\n\\end{subfigure}\n\\begin{subfigure}{3.2cm}\t\n\t\\includegraphics*[trim=0 0 0 0,clip=false,width=1\\textwidth]{figures/Fig6f_CRFasRNN.png}\n\t\\caption{\\scriptsize{CRF-as-RNN}}\t\n\\end{subfigure}\n\\begin{subfigure}{3.2cm}\t\n\t\\includegraphics*[trim=0 0 0 0,clip=false,width=1\\textwidth]{figures/Fig6g_DeepLabV2.png}\n\t\\caption{\\scriptsize{DeepLab.v2}}\t\n\\end{subfigure}\n\\begin{subfigure}{3.2cm}\t\n\t\\includegraphics*[trim=0 0 0 0,clip=false,width=1\\textwidth]{figures/Fig6h_DeepLabV2_wCRF.png}\n\t\\caption{\\scriptsize{DeepLab.v2+CRF}}\t\n\\end{subfigure}\n\\begin{subfigure}{3.2cm}\t\n\t\\includegraphics*[trim=0 0 0 0,clip=false,width=1\\textwidth]{figures/Fig6j_PAN.png}\n\t\\caption{\\scriptsize{PAN}}\t\n\\end{subfigure}\n\\caption{(a) A sample image from the PASCAL VOC validation set, (b) its semantic segmentation ground truth, and results obtained from different studies are depicted: c) FCN-32S , d) FCN-8S , e) CMSA , f) DeepLab-v1 , g) CRF-as-RNN , h) DeepLab-v2 , i) DeepLab-v2 with CRF refinement , j) PAN .}\n\\label{SampleResults}\n\\end{figure*}\nThe pioneering study that represents this idea is the renowned ‘Regions with CNN features’ (RCNN) network . Standard CNNs with fully convolutional and fully connected layers lack the ability to provide varying length output, which is a major flaw for an object detection algorithm that aims to detect an unknown number of objects within an image. The simplest way to resolve this problem is to take different regions of interest from the image, and then to employ a CNN in order to detect objects within each region separately. This region selection architecture is called the ‘Region Proposal Network’ (RPN) and is the fundamental structure used to construct the RCNN network (see Figure \\ref{ObjectBased}.a). Improved versions of RCNN, namely ‘Fast-RCNN’  and ‘Faster-RCNN’  were subsequently also proposed by the same research group. Because these networks allow for the separate detection of all objects within the image, the idea was easily implemented for instance segmentation, as the ‘Mask-RCNN’ .\nThe basic structure of RCNNs included the RPN, which is the combination of CNN layers and a fully connected structure in order to decide the object categories and bounding box positions. As discussed within the previous sections of this paper, due to their cumbersome structure, fully connected layers were largely abandoned with FCNs. RCNNs shared a similar fate when the ‘You-Only-Look-Once’ (YOLO) by  and ‘Single Shot Detector’ (SSD) by  were proposed. YOLO utilises a single convolutional network that predicts the bounding boxes and the class probabilities for these boxes. It consists of no fully connected layers, and consequently provides real-time performance. SSD proposed a similar idea, in which bounding boxes were predicted after multiple convolutional layers. Since each convolutional layer operates at a different scale, the architecture is able to detect objects of various scales. Whilst slower than YOLO, it is still considered to be faster then RCNNs. This new breed of object detection techniques was immediately applied to semantic segmentation. Similar to MaskRCNN, ‘Mask-YOLO’  and ‘YOLACT’  architectures were implementations of these object detectors to the problem of instance segmentation (see Figure \\ref{ObjectBased}b). Similar to YOLACT, some other methods also achieve fast, real-time instance segmentation such as; ESE-Seg , SOLO, SOLOv2, DeepSnake , and CenterPoly.\nLocating objects within an image prior to segmenting them at the pixel level is both intuitive and natural, due to the fact that it is effectively how the human brain supposedly accomplishes this task .  In addition to these ``two-stage (detection+segmentation) methods, there are some recent studies that aim at utilizing the segmentation task to be incorporated into one-stage bounding-box detectors and result in a simple yet efficient instance segmentation framework . However, the latest trend is to use global-area-based methods by generating intermediate FCN feature maps and then assembling these basis features to obtain final masks .  \nIn recent years, a trend of alleviating the demand for pixel-wise labels is realized mainly by employing bounding boxes, and by expanding from semantic segmentation to instance segmentation applications. In both semantic segmentation and instance segmentation methods, the category of each pixel is recognized, and the only difference is that instance segmentation also differentiates object occurrences of the same category. Therefore, weakly-supervised instance segmentation (WSIS) methods are also utilized for instance segmentation. The supervision of WSIS methods can use different annotation types for training, which are usually in the form of either bounding boxes  or image-level labels . Hence, employing object detection-based methods for semantic segmentation is an area significantly prone to further development in near future by the time this manuscript is prepared. \n\\begin{table*}[p]\n\\centering\n\\begin{tabular}{|p{2.5cm}|p{9.3cm}|p{2.9cm}|>{\\centering\\arraybackslash}p{0.8cm}|}\n\\hline\nMethod & Method Summary & Rankings & Eff.\\\\\\hline \\hline\nMultiScale-Net.\\newline\\scriptsize & \\emph{Multiscale convolutional network fused parallel with a segmentation framework (either superpixel or CRF-based). Relatively lower computational efficiency due to a CRF block.} & \\scriptsize68.7\\% mPA @SIFTflow & $\\star$$\\hspace{0.1cm}$$\\star$\\\\\\hline\nRecurrent CNN\\newline\\scriptsize & \\emph{Recurrent architecture constructed by using different instances of a CNN, in which each network instance is fed with previous label predictions (obtained from the previous instance). Heavy computational load when multiple instances (3 in their best performing experiments) are fed.}  & \\scriptsize77.7\\% mPA @SIFTflow  & $\\star$ \\\\\\hline\nFCN\\newline \\newline  &  \\emph{Fully convolutional encoder structure (i.e., no fully connected layers) with skip connections that fuse multiscale activations at the final decision layer. Relative fast due to no fully connected layers or a refinement block.} & \\scriptsize85.2\\% mPA @SIFTflow\\newline\\scriptsize62.2\\% mIoU @PASCAL 2012\\newline65.3\\% mIoU @CitySca. (w/o course)\\newline39.3\\% mIoU @ADE20K   & $\\star$ $\\star$ $\\star$ \\\\\\hline\nDeepLab.v1\\newline  & \\emph{CNN with dilated convolutions, succeeded by a fully-connected (i.e. Dense) CRF.} Fast and optimized computation leads to near real-time performance. & \\scriptsize66.4\\% mIoU @PASCAL 2012 & $\\star$ $\\star$ $\\star$\\\\\\hline\nCMSA\\newline \\newline  & \\emph{Layers of a pyramidal input are fed to separate FCNs for different scales in parallel. These multiscale FCNs are also connected in series to provide pixel-wise category, depth and normal output, simultaneously. Relatively lower computational efficiency due to progressive processing of sequence of different scales}. & \\scriptsize83.8\\% mPA @SIFTflow\\newline\\scriptsize62.6\\% mIoU @PASCAL 2012 & $\\star$$\\hspace{0.1cm}$$\\star$ \\\\\\hline\nUNet\\newline  & \\emph{Encoder/decoder structure with skip connections that connect same levels of ED and final input-sized classification layer. Efficient computation load due to no fully connected layers or a refinement block.} & \\scriptsize72.7\\% mIoU @PASCAL 2012 (\\emph{tested by )}& $\\star$ $\\star$ $\\star$\\\\\\hline\nSegNet\\newline\\newline  & \\emph{Encoder/decoder structure (similar to UNet) with skip connections that transmit only pooling indices (unlike U-Net, for which skip connections concatenate same-level activations.  Efficient computation load due to no fully connected layers or a refinement block).} & \\scriptsize59.9\\% mIoU @PASCAL 2012\\newline79.2\\% mIoU @CitySca. (w/o course) & $\\star$ $\\star$ $\\star$\\\\\\hline\nDeconvNet\\newline\\newline  & \\emph{Encoder/decoder structure (namely ‘the Conv./Deconv. Network’) without skip connections. The encoder (convolutional) part of the network is transferred from the VGG-VD-16L. Efficient computation load due to no fully connected layers or a refinement block.} . & \\scriptsize74.8\\% mIoU @PASCAL 2012 & $\\star$ $\\star$ $\\star$ \\\\\\hline\nMSCG \\newline \\newline& \\emph{Multiscale context aggregation using only a rectangular prism of dilated convolutional layers, without pooling or subsampling layers, to perform pixel-wise labelling. Efficient computation load due to no fully connected layers or a refinement block.} & \\scriptsize67.6\\% mIoU @PASCAL 2012\\newline\\scriptsize67.1\\% mIoU @CitySca. (w/o course) &  $\\star$ $\\star$ $\\star$ \\\\\\hline\nCRF-as-RNN\\newline  \\newline  & \\emph{Fully convolutional CNN (i.e., FCN) followed by a CRF-as-RNN layer, in which an iterative CRF algorithm is formulated as an RNN. Because of the RNN block, computational efficiency is limited.}   & \\scriptsize65.2\\% mIoU @PASCAL 2012\\newline\\scriptsize62.5\\% mIoU @CitySca. (w/o course) &  $\\star$$\\hspace{0.1cm}$$\\star$ \\\\\\hline\nFeatMap-Net.\\newline \\newline  & \\emph{Layers of a pyramidal input fed to parallel multiscale feature maps (i.e., CNNS), and later fused in an upsample/concatenation (i.e. pyramid pooling) layer  to provide the final feature map to a Dense CRF Layer. Well-planned but loaded architecture leads to moderate computational efficiency.}  & \\scriptsize88.1\\% mPA @SIFTflow\\newline\\scriptsize75.3\\% mIoU @PASCAL 2012 & $\\star$$\\hspace{0.1cm}$$\\star$\\\\\\hline \nGraph LSTM\\newline \\newline  & \\emph{Generalization of LSTM from sequential data to general graph-structured data for semantic segmentation on 2D still images, mostly people/parts. Graph-LSTM processing considerably limits computation efficiency.} & \\scriptsize60.2\\% mIoU @PASCAL Person/Parts 2010 & $\\star$\\\\\\hline\nDAG-RNN\\newline \\newline  & \\emph{DAG-structured CNN+RNN network that models long-range semantic dependencies among image units. Due to chain structured sequential processing of pixels with a recurrent model, the computational efficiency is considerably limited.} & \\scriptsize85.3\\% mPA @SIFTflow & $\\star$\\\\\\hline\n\\end{tabular}\n\\end{table*}\n\\begin{table*}[p]\n\\centering\n\\begin{tabular}{|p{2.5cm}|p{9.3cm}|p{2.9cm}|>{\\centering\\arraybackslash}p{0.8cm}|}\n\\hline\ncont'd. &  &  & \\\\\\hline \\hline\nDeepLab.v2\\newline   \\newline  & \\emph{Improved version of DeepLab.v1, with additional ‘dilated (atrous) spatial pyramid pooling’ (ASPP) layer. Similar computational performance to DeepLab.v1.} & \\scriptsize79.7\\% mIoU @PASCAL 2012\\newline\\scriptsize70.4\\% mIoU @CitySca. (w/o course) & $\\star$ $\\star$ $\\star$ \\\\\\hline\nPSPNet\\newline \\newline   & \\emph{CNN followed by a pyramid pooling layer similar to , but without a fully connected decision layer. Hence, computational performance closer to FCN .} & \\scriptsize85.5\\% mIoU @PASCAL 2012\\newline81.2\\% mIoU @CitySca. (w. course)\\newline55.4\\% mIoU @ADE20K & $\\star$ $\\star$ $\\star$ \\\\\\hline\nDeepLab.v3\\newline & \\emph{Improved version of DeepLab.v2, with optimisation of ASPP layer hyperparameters and without a Dense CRF layer, for faster operation.}  & \\scriptsize85.7\\% mIoU @PASCAL 2012\\newline81.3\\% mIoU @CitySca. (w. course) & $\\star$ $\\star$ $\\star$ \\\\\\hline\nDIS\\newline \\newline  & \\emph{One network predicts labelmaps/tags, while another performs semantic segmentation using these predictions. Both networks use ResNet101  for preliminary feature extraction. They declare similar computational efficiency to DeepLabv2 } &  \\scriptsize41.7\\% mIoU @COCO\\newline\\scriptsize86.8\\% mIoU @PASCAL 2012&  $\\star$ $\\star$ $\\star$ \\\\\\hline\nMask-RCNN\\newline \\newline   & \\emph{Object Detector Fast-RCNN followed by ROI-pooling and Convolutional layers, applied to instance segmentation, with near real-time performance (see Figure \\ref{ObjectBased}.a).}  & \\scriptsize37.1\\% mIoU @COCO \\emph{tested by}  & $\\star$ $\\star$ $\\star$ \\\\\\hline\nGCN\\newline \\newline   & \\emph{Fed by an initial ResNet-based  encoder, GCN uses large kernels to fuse high- and low-level features in a multiscale manner, followed by a convolutional Border Refinement (BR) module. Its fully convolutional architectue allows near real-time performance. }  & \\scriptsize83.6\\% mIoU @PASCAL 2012\\newline76.9\\% mIoU @CitySca. (w/o course) & $\\star$ $\\star$ $\\star$\\\\\\hline\nSDN\\newline  \\newline  & \\emph{UNET architecture that consists of multiple shallow deconvolutional networks, called SDN units, stacked one by one to integrate contextual information and guarantee fine recovery of localised information. Computational efficiency similar to UNET-like architectures.} & \\scriptsize83.5\\% mIoU @PASCAL 2012 & $\\star$ $\\star$ $\\star$ \\\\\\hline\nDFN\\newline \\newline  &  \\emph{Consists of two sub-networks: Smooth Net (SN) and Border Net (BN). SN utilises an attention module and handles global context, whereas BN employs a refinement block to handle borders. Limited computational efficiency due to an attention block}. & \\scriptsize86.2\\% mIoU @PASCAL 2012\\newline\\scriptsize80.3\\% mIoU @CitySca. (w.course) & $\\star$$\\hspace{0.1cm}$$\\star$ \\\\\\hline\nMSCI\\newline \\newline   &\\emph{Aggregates features from different scales via connections between Long Short-term Memory (LSTM) chains. Limited computational efficiency due to multiple RNN blocks (i.e. LSTMs).}  & \\scriptsize88.0\\% mIoU @PASCAL 2012 & $\\star$$\\hspace{0.1cm}$$\\star$ \\\\\\hline\nDeepLab.v3+\\newline  \\newline   &  \\emph{Improved version of DeepLab.v3, using special encoder-decoder structure with dilated convolutions (with no Dense CRF employed for faster operation).} & \\scriptsize87.3\\% mIoU @PASCAL 2012\\newline82.1\\% mIoU @CitySca. (w. course) & $\\star$ $\\star$ $\\star$\\\\\\hline\nHPN\\newline  \\newline  & \\emph{Followed by a convolutional ‘Appearance Feature Encoder’, a ‘Contextual Feature Encoder’ consisting of LSTMs generates super-pixel features fed to a Softmax-based classification layer. Limited computational efficiency due to multiple LSTMs.} & \\scriptsize85.8\\% mIoU @PASCAL 2012\\newline\\scriptsize92.3\\% mPA @SIFTflow & $\\star$$\\hspace{0.1cm}$$\\star$ \\\\\\hline\nEncNet\\newline   \\newline  & \\emph{Fully connected structure to extract context is fed by dense feature maps (obtained from ResNet ) and followed by a convolutional prediction layer. Fully connected layers within their ``Context Encoding Module'' limits computational performance.}  & \\scriptsize85.9\\% mIoU @PASCAL 2012\\newline55.7\\% mIoU @ADE20K & $\\star$$\\hspace{0.1cm}$$\\star$ \\\\\\hline\nPSANet\\newline &  \\emph{A convolutional point-wise spatial attention (PSA) module is attached to o pretrained convolutional encoder, so that pixels are interconnected through a self-adaptively learnt attention map to provide global context. Additional PSA module limits computational efficieny compared to fully convolutional architectures (e.g. FCN).} & \\scriptsize85.7\\% mIoU @PASCAL 2012\\newline81.4\\% mIoU @CitySca. (w. course) & $\\star$$\\hspace{0.1cm}$$\\star$ \\\\\\hline\n\\end{tabular}\n\\end{table*}\n\\begin{table*}[p]\n\\centering\n\\begin{tabular}{|p{2.5cm}|p{9.3cm}|p{2.9cm}|p{0.8cm}|}\n\\hline\ncont'd. &  &  & \\\\\\hline \\hline\nPAN\\newline &  \\emph{SPP layer with global pooling architecture. Similar architecture and thus, computational efficiency with PSPNet .} & \\scriptsize84.0\\% mIoU @PASCAL 2012 (\\emph{taken from the paper, not listed in the leaderboard}) & $\\star$ $\\star$ $\\star$ \\\\\\hline\nExFuse\\newline  \\newline   &  \\emph{Improved version of GCN  for feature fusing which introduces more semantic information into low-level features and more spatial details into high-level features, by additional skip connections. Computational performance comparable to GCN.} & \\scriptsize87.9\\% mIoU @PASCAL 2012 & $\\star$ $\\star$ $\\star$ \\\\\\hline\nEMANet152\\newline  \\newline  &  \\emph{Novel attention module between two CNN structures converts input feature maps to output feature maps, thus providing global context. Computationally more efficient compared to other attention governing architectures (e.g. PSANet).} & \\scriptsize88.2\\% mIoU @PASCAL 2012\\newline39.9\\% mIoU @COCO & $\\star$ $\\star$ $\\star$ \\\\\\hline\nKSAC\\newline  \\newline  &  \\emph{Allows branches of different receptive fields to share the same kernel to facilitate communication among branches and perform feature augmentation inside the network. The idea is similar to ASPP layer of DeepLabv3 , hence similar computational performance.} &  \\scriptsize88.1\\% mIoU @PASCAL 2012 & $\\star$ $\\star$ $\\star$ \\\\\\hline\nCFNet\\newline  \\newline  & \\emph{Using a distribution of co-occurrent features for a given target in an image, a fine-grained spatial invariant representation is learnt and the CFNet is constructed. Similar architecture to PSANet , hence similar (and limited) computational performance due to fully connected layers.} & \\scriptsize87.2\\% mIoU @PASCAL 2012 & $\\star$$\\hspace{0.1cm}$$\\star$ \\\\\\hline\nYOLACT\\newline  & \\emph{Object Detector YOLO followed by Class Probability and Convolutional layers, applied to instance segmentation (see Figure \\ref{ObjectBased}.b), with \\underline{real-time} semantic segmentation performance}.  & \\scriptsize72.3\\% mAP$_{50}$ @PASCAL SBD\\newline\\scriptsize31.2\\% mAP @COCO & $\\star$ $\\star$ $\\star$\\newline$\\star$ \\\\\\hline\nESE-Seg\\newline  & \\emph{ESE-Seg is an object detection-based approach that uses explicit shape encoding by explicitly decoding the multiple object shapes with tensor operations in real-time.}  & \\scriptsize69.3\\% mAP$_{50}$ @PASCAL SBD\\newline\\scriptsize21.6\\% mAP @COCO & $\\star$ $\\star$ $\\star$\\newline$\\star$ \\\\\\hline\nSOLO\\newline  \\newline  & \\emph{The central idea of SOLO framework is to reformulate the instance segmentation as two simultaneous problems: category prediction and instance mask generation, using a single convolutional backbone. The model can run in real-time with proper parameter tuning.} & \\scriptsize37.8\\% mAP @COCO & $\\star$ $\\star$ $\\star$\\\\\\hline\nEfficientNet-L2 + NASFPN + Noisy Student\\newline  & \\emph{The study aims at understaing the effect of pre- and self training and apply this to semantic segmentation problem. For their experiment, they utilize a neural architecture search (NAS) strategy \nwith EfficientNet-L2  as the backbone architecture. The model is the leader of PASCAL VOC 2012 challenge by the time this manuscript was written.} & \\scriptsize90.5\\% mIoU @PASCAL 2012 &  $\\star$ $\\star$ $\\star$ \\\\\\hline\nDCNAS\\newline  \\newline  & \\emph{Neural Architecture Search applied to MobileNetV3 , a densely connected search space for semantic segmentation. Although computational performance is not explicitly indicated, the resulting architecture possibly provides U-Net like computational efficiency for model inference. } & \\scriptsize86.9\\% mIoU @PASCAL 2012 (\\emph{taken from the paper, not listed in the leaderboard})\\newline83.6\\% mIoU @CitySca. (w. course)& $\\star$ $\\star$ $\\star$\\\\\\hline\nSOLOv2\\newline  \\newline  & \\emph{Updated, real-time version of SOLO , empowered by an efficient and holistic instance mask representation scheme, which dynamically segments each instance in the image, without resorting to bounding\nbox detection.} & \\scriptsize37.1\\% mAP @COCO& $\\star$ $\\star$ $\\star$\\newline$\\star$\\\\\\hline\n\\end{tabular}\n\\label{MethodsTable}\n\\end{table*}\n\\begin{table*}[h]\n\\centering\n\\begin{tabular}{|p{2.5cm}|p{9.3cm}|p{2.9cm}|p{0.8cm}|}\n\\hline\ncont'd. &  &  & \\\\\\hline \\hline\nDeep Snake\\newline  \\newline  & \\emph{Deep Snake is a fully convolutional architecture with a contour-based approach for real-time instance segmentation.} & \\scriptsize62.1\\% mAP$_{50}$ @PASCAL SBD\\newline\\scriptsize30.3\\% mAP @COCO & $\\star$ $\\star$ $\\star$\\newline$\\star$\\\\\\hline\nBlendMask\\newline  \\newline  & \\emph{Using both top-down and\nbottom-up instance segmentation approaches, BlendMask learns attention maps for each instance using a single\nconvolution layer.} & \\scriptsize37.1\\% mAP @COCO & $\\star$ $\\star$ $\\star$\\newline$\\star$\\\\\\hline\nSwiftNetRN18-Pyr\\newline  \\newline  & \\emph{Based on shared pyramidal representation and fusion of heterogeneous features, SwiftNetRN18-Pry fuses hybrid representation within a ladder-style decoder. Provides beyond real-time performance with modest accuracy.} & \\scriptsize35.0\\% mIoU @ADE20K & $\\star$ $\\star$ $\\star$\\newline$\\star$\\\\\\hline\nBOXInst\\newline  \\newline  & \\emph{Achieves mask-level instance segmentation with only bounding-box\nannotations for training. Core idea is to redesign the loss of learning masks in instance segmentation} & \\scriptsize61.4\\% mAP$_{50}$ @PASCAL SBD\\newline\\scriptsize31.6\\% mAP @COCO & $\\star$ $\\star$ $\\star$\\\\\\hline\n\\end{tabular}\n\\caption{State-of-the-art semantic segmentation methods, showing the method name and reference, brief summary, problem type targeted, and refinement model (if any).}\n\\label{MethodsTable}\n\\end{table*}", "cites": [1755, 1757, 1760, 802, 1759, 1761, 1751, 508, 7502, 850, 6977, 209, 520, 1741, 8492, 1764, 1769, 206, 97, 7504, 7501, 810, 1749, 1223, 7063, 1754, 1750, 1752, 1756, 1770, 737, 37, 1758, 7503, 1763, 1762, 1743, 7505, 7064, 857, 1745, 1753, 509, 1768, 1765, 1766, 1767], "cite_extract_rate": 0.746268656716418, "origin_cites_number": 67, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of object detection-based methods in semantic segmentation, synthesizing key concepts from multiple papers such as R-CNN, YOLO, SSD, and their adaptations to instance segmentation. It discusses architectural trends and trade-offs (e.g., two-stage vs. one-stage methods) and highlights recent developments in weak supervision and real-time performance. While it integrates ideas effectively and identifies broader patterns, it lacks deeper critical evaluation of specific limitations and could offer more nuanced comparative insights."}}
{"id": "5c2bb78c-80a6-489a-b0eb-e5a37bb4c679", "title": "Evolution of Methods", "level": "subsection", "subsections": [], "parent_id": "a810e757-8e18-4cb5-b9eb-b720ad7146bd", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Post-FCN Approaches"], ["subsection", "Evolution of Methods"]], "content": "In Table \\ref{MethodsTable}, we present several semantic segmentation methods, each with a brief summary, explaining the fundamental idea that represents the proposed solutions, their position in available leaderboards, and a categorical level of the method's computational efficiency. The intention is for readers to gain a better evolutionary understanding of the methods and architectures in this field, and a clearer conception of how the field may subsequently progress in the future. Regarding the brief summaries of the listed methods, please refer to the categorisations provided earlier in this section.\nTable \\ref{MethodsTable} includes 34 methods spanning an eight-year period, starting with early deep learning approaches through to the most recent state-of-the-art techniques. Most of the listed studies have been quite successful and have significantly high rankings in the previously mentioned leaderboards. Whilst there are many other methods, we believe this list to be a clear depiction of the advances in deep learning-based semantic segmentation approaches. In Figure \\ref{SampleResults}, a sample image from the PASCAL VOC validation set, its semantic segmentation ground truth and results obtained from some of the listed studies are depicted. Figure \\ref{SampleResults} clearly shows the gradually growing success of different methods starting with the pioneering FCN architectures to more advanced architectures such as DeepLab  or CRF-as-RNN .\nJudging by the picture it portrays, the deep evolution of the literature clearly reveals a number of important implications. First, graphical model-based refinement modules are being abandoned due to their slow nature. A good example of this trend would be the evolution of DeepLab from  to  (see Table \\ref{MethodsTable}). Notably, no significant study published in 2019 and 2020 employed a CRF-based or similar module to refine their segmentation results. Second, most studies published in the past two years show no significant leap in performance rates. For this reason, researchers have tended to focus on experimental solutions such as object detection-based or Neural Architecture Search (NAS)-based approaches. Some of these very recent group of studies  focus on (NAS)-based techniques, instead of hand-crafted architectures. EfficientNet-NAS  belongs to this category and is the leading study in PASCAL VOC 2012 semantic segmentation challenge at the time the paper was prepared. We believe that the field will witness an increasing interest in NAS-based methods in the near future. In general, considering all studies of the post-FCN era, the main challenge of the field still remains to be \\emph{efficiently} integrating (i.e. in  real-time) global context to localisation information, which still does not appear to have an off-the-shelf solution, although there are some promising techniques, such as YOLACT .\nIn Table \\ref{MethodsTable}, the right-most column represents a categorical level of computational efficiency. \nWe use a four-level categorisation (one star to four stars) to indicate the computational efficiency of each listed method. For any assigned level of the computational efficiency of a method, we explain our reasoning in the table with solid arguments. For example, one of the four-star methods in Table \\ref{MethodsTable} is ``YOLACT'' by , which claims to provide real-time performance (i.e. $>$30fps) on both PASCAL VOC 2012 and COCO image sets.", "cites": [7505, 1741, 1745, 1768, 7502, 1763], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key trends in the post-FCN era by connecting ideas from multiple cited papers, such as the shift away from CRF-based refinement and the rise of NAS-based methods. It offers critical insights by evaluating the performance stagnation in recent years and highlighting the limitations of CRF-based modules. The abstraction level is strong, as it generalizes these observations into broader patterns and future research directions, such as the ongoing challenge of integrating global context with localization."}}
{"id": "be8f087e-949b-4d20-87cb-2330fbe327d3", "title": "Weakly-Supervised Semantic Segmentation (WSSS)", "level": "subsection", "subsections": [], "parent_id": "511d59eb-d8b1-4b46-87e9-ff1fa938af77", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Future Scope and Potential Research Directions"], ["subsection", "Weakly-Supervised Semantic Segmentation (WSSS)"]], "content": "Over the last few years, there has been an increasing research effort directed towards the approaches that are alternative to pixel-level annotations such as; unsupervised, semi-supervised  and weakly-supervised methods. Recent studies show that, WSSS methods usually perform better than the other schemes  where annotations are in the form of image-level labels , video-level labels , scribbles , points , and bounding boxes . In case of image-level labels, class activation maps (CAMs)  are used to localize the small discriminative regions which are not suitable particularly for the large-scale objects, but can be utilized as initial seeds (pseudo-masks) .", "cites": [1778, 1771, 1751, 1779, 7065, 1777, 1780, 6978, 1776, 737, 1772, 1774, 1775, 1773], "cite_extract_rate": 0.6086956521739131, "origin_cites_number": 23, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a general overview of weakly-supervised semantic segmentation and integrates multiple papers that focus on pseudo-masks, constraints, and alternative forms of weak supervision. It begins to synthesize the common themes (e.g., use of CAMs, pseudo-label generation) across works, but the critical analysis is limited—there is little evaluation of trade-offs or comparative assessment between methods. Some abstraction is evident in grouping different forms of weak supervision, but deeper meta-level insights are not fully developed."}}
{"id": "27fe3c08-c148-46da-9550-40e08accd4e9", "title": "Zero-/Few-Shot Learning", "level": "subsection", "subsections": [], "parent_id": "511d59eb-d8b1-4b46-87e9-ff1fa938af77", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Future Scope and Potential Research Directions"], ["subsection", "Zero-/Few-Shot Learning"]], "content": "Motivated by humans' ability to recognize new concepts in a scene by using only a few visual samples, zero-shot and/or few-shot learning methods have been introduced. Few-shot semantic segmentation (FS3) methods  has been proposed to recognize objects from unseen classes by utilizing few annotated examples; however, these methods are limited to handling a single unseen class only. Zero-shot semantic segmentation (ZS3) methods have been developed recently to generate visual features by exploiting word embedding vectors in the case of zero training samples . However, the major drawback of ZS3 methods is their insufficient prediction ability to distinguish between the seen and the unseen classes even if both are included in a scene. This disadvantage is usually overcome by generalized ZS3 (GZS3), which recognizes both seen and unseen classes simultaneously. GZS3 studies mainly rely on generative-based methods. Feature extractor training is realized without considering semantic features in GZS3 adopted with generative approaches so that the bias is introduced towards the seen classes. Therefore, GZS3 methods result in performance reduction on unseen classes . Much of the recent work on ZS3 has involved such as; exploiting joint embedding space to alleviate the seen bias problem , analyzing different domain performances , and incorporating spatial information .", "cites": [8493, 1781, 1776, 1782, 1783], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple cited papers to create a coherent narrative about the challenges and approaches in zero-/few-shot semantic segmentation. It critically discusses limitations such as the inability of FS3 to handle multiple classes and the seen-class bias in GZS3 methods. While it identifies broader patterns like the use of joint embedding spaces and generative approaches, it does not yet reach a meta-level of abstraction or propose a novel framework."}}
{"id": "9962178a-f02b-44d6-b98a-ef8f2f5f18f2", "title": "Domain Adaptation", "level": "subsection", "subsections": [], "parent_id": "511d59eb-d8b1-4b46-87e9-ff1fa938af77", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Future Scope and Potential Research Directions"], ["subsection", "Domain Adaptation"]], "content": "Recent studies also rely on the use of synthetic large-scale image sets such as GTA5  and SYNTHIA  because of their capability to cope with laborious pixel-level annotations. Although these rich-labeled synthetic images have the advantage of reducing the labeling cost, they also bring about domain shift while training with unlabeled real images. Therefore, applying domain adaptation for aligning the synthetic and the real image sets is of much importance . Unsupervised domain adaptation (UDA) methods are widely employed in semantic segmentation .", "cites": [7066, 1790, 8496, 1787, 8494, 1788, 1786, 1789, 1785, 1784, 8495], "cite_extract_rate": 0.6875, "origin_cites_number": 16, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions domain adaptation and lists several relevant papers but fails to synthesize their contributions into a cohesive narrative. It describes the general problem and the use of unsupervised domain adaptation without critically evaluating the approaches or their limitations. The discussion remains at a surface level, focusing primarily on the role of synthetic datasets and the need for domain alignment."}}
{"id": "763bf75f-b1d6-40b7-8dca-a0c747e711f3", "title": "Real-Time Processing", "level": "subsection", "subsections": [], "parent_id": "511d59eb-d8b1-4b46-87e9-ff1fa938af77", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Future Scope and Potential Research Directions"], ["subsection", "Real-Time Processing"]], "content": "Adopting compact and shallow model architectures  and restricting the input to be low-resolution  are brand new innovations proposed very recently to overcome the computational burden of large-scale semantic segmentation. To choose a real-time semantic segmentation strategy, all aspects of an application should be considered, as all of these strategies somehow correlate with decreasing the model’s discriminative ability and losing information of object boundaries or small objects to some extent. Some other strategies have also been proposed for the retrieval of rich contextual information in real-time applications including attention mechanisms , depth-wise separable convolutions , pyramid fusion , grouped convolutions  and neural architecture search , pipeline parallelism .", "cites": [1793, 871, 7292, 505, 1792, 1794, 1791, 1795, 850, 687], "cite_extract_rate": 0.625, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates recent papers on real-time semantic segmentation by highlighting common strategies (e.g., compact models, downsampling, attention mechanisms) and identifying the trade-offs between speed and performance. While it synthesizes multiple approaches into a general narrative, it lacks in-depth critical evaluation of specific limitations or comparative analysis of methods. The abstraction level is moderate, as it generalizes patterns like the use of feature aggregation and efficient convolutions but stops short of proposing a meta-level framework."}}
{"id": "48b1b56f-3ed3-4918-bd61-df9ea4de0d3d", "title": "Contextual Information", "level": "subsection", "subsections": [], "parent_id": "511d59eb-d8b1-4b46-87e9-ff1fa938af77", "prefix_titles": [["title", "A Survey on Deep Learning-based Architectures\\\\for Semantic Segmentation on 2D images"], ["section", "Future Scope and Potential Research Directions"], ["subsection", "Contextual Information"]], "content": "Contextual information aggregation with the purpose of augmenting pixel representations in semantic segmentation architectures is another promising research direction in recent years. In this aspect, mining contextual information , exploring context information on spatial and channel dimensions , focusing on object based contextual representations  and capturing the global contextual information for fine-resolution remote sensing imagery  are some of the recent studies. Alternative methods of reducing dense pixel-level annotations in semantic segmentation have been described which are based on using pixel-wise contrastive loss .", "cites": [8497, 1796, 1797], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers by highlighting different aspects of contextual information in semantic segmentation, such as spatial and channel context, object-based representations, and global context for remote sensing. However, it lacks a deeper critical evaluation of these methods and does not fully abstract overarching principles or frameworks, instead focusing on general descriptions of the approaches and their purposes."}}
