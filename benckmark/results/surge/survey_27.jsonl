{"id": "36136bf4-f7b9-452f-91ec-425579bfc421", "title": "Introduction", "level": "section", "subsections": ["79f455a5-bda9-4c48-b5ea-5e918a847dc8", "d2517d00-8cd9-42bb-ade7-06abb21e8e9f", "d0f49c84-7d54-46d5-bd16-4226835dc127"], "parent_id": "c99b7cd0-bfbd-4d1c-9bd1-123b6e7e8e3e", "prefix_titles": [["title", "Automatic Speech Recognition And Limited Vocabulary: A Survey"], ["section", "Introduction"]], "content": "\\label{sec:introduction}\nAutomatic speech recognition (ASR) is the process and the related technology applied to convert a speech signal into the matching sequence of words or other linguistic entities using algorithms implemented in computing devices   . ASR has become an exciting field for many researchers. Presently, users prefer to use devices such as computers, smartphones, or any other connected device through speech. \nAutomatic speech recognition can technically be defined as a power density spread over a time-frequency domain . Current speech processing techniques (encompassing speech synthesis, speech processing, speaker identification or verification) pave the way to create human-to-machine voice interfaces. ASR can be applied in several applications including  voice services , program control and data entry , avionics , disabled assistance , amongst others.\nAlthough ASR can be advantageous in easing human-to-machine communication; in many cases, it is goes beyond helpful and becomes absolutely necessary. For example, low-literacy levels and the extinction of under-resourced languages are ideal candidates for ASR.. In fact, the high penetration of communication tools such as smartphones in the developing world  and their increasing presence in rural areas  provides an unprecedented opportunity to develop a voice-based application that can help to mitigate the low literacy levels in those areas. Smartphones offer many advantages over a PC-based interface, such as high mobility and portability, easy recharge of their batteries, and conventional embedded features such as microphones and speakers.", "cites": [6128], "cite_extract_rate": 0.1111111111111111, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The introduction provides a general overview of ASR and its applications, mentioning under-resourced languages and low literacy as areas where ASR is beneficial. It includes a citation related to ICT penetration in rural Cameroon but does not synthesize or connect it to ASR in any meaningful way. The section lacks critical evaluation or abstraction beyond the cited paper, remaining largely descriptive."}}
{"id": "79f455a5-bda9-4c48-b5ea-5e918a847dc8", "title": "Motivation", "level": "subsection", "subsections": [], "parent_id": "36136bf4-f7b9-452f-91ec-425579bfc421", "prefix_titles": [["title", "Automatic Speech Recognition And Limited Vocabulary: A Survey"], ["section", "Introduction"], ["subsection", "Motivation"]], "content": "In regions with low literacy levels, people are used to speaking local languages that are often considered as under-resourced languages because of the lack or insufficiency of formal written grammar and vocabulary. Since people do not know how to read or to write well-resourced languages (such as English or French), the development of ASR systems for under-resourced languages appears as an appealing solution to overcome this limitation. However, due to the complexity of the task, limited vocabulary must be considered. \nThis paper focuses on limited vocabulary in ASR to allow researchers who wish to work on under-resourced languages to have an overview on how to develop a speech recognition system for limited vocabulary. In contrast to limited vocabulary systems, large vocabulary continuous speech recognition (LVCSR) systems are usually trained on thousands of hours of speech and billions of words of text . The development of large vocabulary systems is complex since the larger the vocabulary, the harder the manipulation of learning algorithms, with more rules needed to build the dataset. LVCSR systems can be very efficient when they are applied on similar domains to those on which they were trained . However, they are not robust enough to handle mismatched training and test conditions as the context may not be well handled. In fact, most of the input can be silence or contain background noise, which can be mistaken for speech; this increases the false positive rate . Thus, LVCSR systems are not suitable for transfer learning targeting small or limited vocabulary.", "cites": [7139], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.8}, "insight_level": "medium", "analysis": "The section synthesizes the cited paper by integrating its discussion of limited-vocabulary speech recognition and the need for specialized datasets. It provides a basic comparison between limited vocabulary and LVCSR systems, pointing out limitations of the latter in handling mismatched conditions and noise. While it moves beyond mere description, it lacks deeper critical evaluation and more generalized principles, offering mainly contextual insights rather than a novel framework or meta-level analysis."}}
{"id": "b0c3adb8-18c7-473c-9187-b21c698ceed9", "title": "Architecture", "level": "subsection", "subsections": [], "parent_id": "370f0378-9711-44ec-91c9-bf276c4678b8", "prefix_titles": [["title", "Automatic Speech Recognition And Limited Vocabulary: A Survey"], ["section", "Automatic speech recognition "], ["subsection", "Architecture"]], "content": "Audio signals need to be digitised before the recognition process starts. The digitisation of the signal requires the selection of an appropriate sampling frequency to catch the high-pitched voices . In general, all ASR systems have the same architecture whether the vocabulary is limited, medium, large, or very large. This architecture can be modified or supplemented according to the recognition to be performed. ASR is usually composed of five typical components:\n\\begin{enumerate}\n    \\item Feature extraction;\n    \\item Acoustic Model;\n    \\item Language Model;\n    \\item Pronunciation Model; and\n    \\item Decoder.\n\\end{enumerate}\nIn the architecture illustrated in Figure~\\ref{fig3}, the speech signal is received and then features are extracted. The obtained parameters are passed to the decoder, which uses the language, the acoustic, and the pronunciation models for learning. \n\\begin{figure*}\n\\centering\n\\includegraphics[width=16cm]{Fig3.png}\n\\caption{Architecture of an ASR (adapted from .}\\label{fig3}\n\\end{figure*}", "cites": [6993], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the ASR architecture with a list of components, but it integrates only one cited paper in a superficial manner without connecting it to broader themes. There is minimal critical analysis or abstraction beyond the cited work, and the narrative remains largely factual and system-focused."}}
{"id": "f5d81643-43ab-4980-9694-ee1bbd40a1d8", "title": "Feature extraction", "level": "subsection", "subsections": [], "parent_id": "370f0378-9711-44ec-91c9-bf276c4678b8", "prefix_titles": [["title", "Automatic Speech Recognition And Limited Vocabulary: A Survey"], ["section", "Automatic speech recognition "], ["subsection", "Feature extraction"]], "content": "Feature extraction is the first step for an ASR system. It converts the waveform speech signal to a set of feature vectors with the aim of having high discrimination between phonemes . The feature extraction performs all the required measurements on the selected segment that will be used to make a decision . The measured features may be used to update long-term statistical measures to facilitate the adaptation of the process to varying environmental conditions (mainly the background) . Feature extraction will determine the voice areas in the recording to be written out and extract sequences of acoustic parameters from them. There are many techniques for feature extraction, as reported in  , including:\n\\begin{itemize}\n    \\item Linear Predictive coding (LPC) : LPC is a technique for signal source modelling in speech signal processing. \n    \\item RelAtive SpecTral (RASTA) filtering : RASTA is designed to decrease the impact of noise as well as heighten speech. This technique is widely used for noisy speech.\n    \\item Linear Discriminant Analysis (LDA) and Probabilistic LDA : This technique uses the state-dependent variables of Hidden Markov Model-based (HMM) on i-vector extraction. The i-vector is a low dimensional vector with a fixed length that contains relevant information.\n    \\item Mel-frequency cepstrum (MFCCs): It is the most commonly used technique, with a frameshift and length usually between 20 and 32 ms, using 1024 frequency bins, 26 mel channels and between 10 and 40 cepstral coefficients with cepstral mean normalisation . This technique has low complexity and a high ACC of recognition. Mel-Frequency Cepstrum Coefficient (MFCC) is the usual method for character extraction in most papers tackling the design of speech recognition systems for limited vocabulary . The public sphinx base library provides an implementation of this method that can be used directly, as was done in . Figure~\\ref{fig4} provides a brief description of the MFCC method, encompassing six steps as described below. \n\\end{itemize}\n\\begin{enumerate}\n    \\item Framing and windowing: For the acoustic parameters to be stable, the speech signal must be examined over a sufficiently short period. This step aims to cut windows of 20 to 30 ms. The process is illustrated in  Figure~\\ref{fig9}.\n    \\item Hamming window: The Hamming window is used to reduce the spectral distortion of the signal. This is in contrast to the Rectangular window, which is simple but can cause problems since it slices the signal boundaries abruptly; the Hamming window reduces the signal values toward zero at the boundaries. This can help avoid discontinuities. This is expressed mathematically as follows: \n    \\begin{equation}\n        y(n)=x(n)\\times w(n),~0\\leq n\\leq N-1,\n    \\end{equation}\n    where $n$ is the number of windows, $N$ is the number of samples in each frame, $y(n)$ the output signal, $x(n)$ the input signal and $w(n)$ the Hamming windows defined as:\n    \\begin{equation}\n        w(n) = 0.54 -0.46\\cos\\left(\\frac{2 \\pi n}{N} \\right).\n    \\end{equation}\n    \\item\tFast Fourier Transformation (FFT) does the conversion of time domain windows into the frequency domain by discretising and interpolating the window onto a regular grid before the Fourier transform is applied. The FFT decreases the computation requirements compared to the discrete Fourier transform. The latter is defined as: \n\\begin{equation}\n    S(k)=\\sum_{n=0}^{N}y(n)\\text{e}^{-2\\pi K\\frac{n}{N}},~0\\leq k\\leq N,\n\\end{equation}\nwhere $N$ is the number of windows, $k$ is the index of the coefficient and $K$ is the number of coefficients. \n\\item\tMel Filter Wrapping banks allow reproduction of the selectivity of the human auditory system by providing a coefficient that gives the energy of the signal: \n\\begin{equation}\n    X(m)=\\sum_{k=0}^{N-1}|S(k)|W(k,m),~1\\leq m \\leq M, ~M<N,\n\\end{equation}\nwhere $m$ is the index of the filter, $M$ is the number of filters and $W$ is the weight function with inputs. The $k^{th}$ energy spectrum bin contributing to the $m^{th}$ output band.\n\\item Log allows one to obtain the logarithmic spectrum of Mel and to compress the sum $X(m)$ using:\n\\begin{equation}\n    X^\\prime(m)=\\ln(X(m)).\n\\end{equation}\n\\item\tDiscrete Cosine Transform (DCT) reduces the influence of low-energy components. MFCC coefficients are obtained by the discrete cosine transform given by:\n\\begin{equation}\n    c(k)=\\sum_{m=0}^{N-1}a_m \\cos\\bigg[\\frac{\\pi}{M}(m+\\frac{1}{2})k\\bigg]X^\\prime(m),~0\\leq k\\leq K,\n\\end{equation}\n where $a_0=\\frac{1}{\\sqrt{M}}, a_m=\\sqrt{\\frac{2}{M}}$.\n\\end{enumerate}\nAfter this last step, the coefficients are returned as outputs.\n\\begin{figure*}\n\\centering\n\\includegraphics[width=16cm]{Fig4.png}\n\\caption{Block diagram of MFCC feature extraction (adapted from .}\\label{fig4}\n\\end{figure*}\n\\begin{figure*}\n\\centering\n\\includegraphics[width=8cm]{Fig9.png}\n\\caption{Illustration of Framing and Windowing process with a Frame Shift of 10ms, a Frame size of 25ms and two representations: Hamming window and Rectangular window. }\\label{fig9}\n\\end{figure*}", "cites": [8657], "cite_extract_rate": 0.07142857142857142, "origin_cites_number": 14, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the process of feature extraction in ASR systems, especially focusing on MFCC, but lacks synthesis of ideas from the cited paper (Paper 1). It does not connect the topic of limited vocabulary ASR to the broader context of edge computing or machine learning constraints mentioned in the cited work. Additionally, there is no critical evaluation or comparison of techniques, nor abstraction to broader principles or patterns."}}
{"id": "8b56f1d6-63ba-46db-9117-47fef2224eba", "title": "Machine Learning", "level": "subsection", "subsections": [], "parent_id": "1ab9c8b8-974e-44cb-b924-621885e70500", "prefix_titles": [["title", "Automatic Speech Recognition And Limited Vocabulary: A Survey"], ["section", "Automatic speech recognition approaches"], ["subsection", "Machine Learning"]], "content": "Machine learning (ML) is an artificial intelligence technique that refers to systems that can learn by themselves . ML implies teaching a computer to recognise patterns in contrast to the traditional approach, which consists of programming a computer with specific rules. The teaching is done through a training process that involves feeding large amounts of audio data to the algorithm and allowing it to learn from data and detect patterns that can later be used to achieve some tasks . ML techniques can be grouped into four categories :\n\\begin{itemize}\n    \\item Supervised Learning consists of inferring a classification or regression from labelled training data, to develop predictive models. Classification techniques predict discrete responses while regression techniques predict continuous responses.\n    \\item Unsupervised Learning consists of drawing inferences from datasets composed of unlabelled input data. The most common technique in this category is clustering, which is used for exploratory data analysis in order to find hidden patterns or groupings in data.\n    \\item Semi-supervised Learning. The training of the system in this learning technique makes use of both labelled and unlabelled data. This type of training is adequate when it is very expensive to obtain labelled data.\n    \\item Active Learning. This approach is used when there is a lot of unlabelled data, but labelling is expensive. The algorithm interactively queries the user to label data.\n\\end{itemize}\nThe different ML steps for speech recognition are provided in Figure~\\ref{fig5} and are detailed as follows:\n\\begin{figure*}\n\\centering\n\\includegraphics[width=16cm]{Fig5.png}\n\\caption{ML technique}\\label{fig5}\n\\end{figure*}\n\\begin{enumerate}\n    \\item The first step is to select and prepare a training dataset composed of audios (from words or sentences) that have been acquired through microphones. This data will be used to feed the ML model during the learning process so that it can determine texts corresponding to audio inputs. Data must be meticulously prepared, organised and cleaned with the aim to mitigate bias during the training process. \n    \\item The second step is to perform a pre-processing on the input data. This pre-processing includes the reduction of the noise in the audio and the enhancement of data.\n    \\item The third step consists of choosing a parametric class where the model will be searched.  This is a fine-tuned process that is run on the training dataset. For speech recognition using limited vocabulary, some algorithms such as the Maximum Likelihood Linear Regression algorithm  can be used to train the AM, and the Viterbi algorithm  for decoding. The type of algorithm to use depends on the type of problem to be solved.\n    \\item The fourth step is the training of the algorithm. This is an iterative process. After running the algorithm, the results are compared with the expected ones. The weights and biases are eventually tuned  via the back propagation optimisation, to increase the accuracy of the algorithm. This process is repeated until a certain criterion is met and the resulting trained model is saved for further analysis with the test data.\n    \\item The fifth and final step is to use and improve the model. The model is then used on new data. \n\\end{enumerate}\nDifferent ML methods  have been used for acoustic modelling in speech recognition systems . The evaluation, decoding and training of HMMs are done by ML forward-backward , Viterbi  and Baum-Welch algorithms , respectively. In their work, Padmanabhan et al.  review these methods.", "cites": [8657], "cite_extract_rate": 0.1111111111111111, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of machine learning techniques and their application to ASR, but it lacks deep synthesis of the cited paper, critical evaluation of the methods or their limitations, and abstraction to broader patterns or principles. It primarily summarizes ML concepts and mentions a single review without integrating or analyzing it meaningfully."}}
{"id": "81a932eb-3009-4dfd-9cb9-d68a25645161", "title": " Deep Learning", "level": "subsection", "subsections": [], "parent_id": "1ab9c8b8-974e-44cb-b924-621885e70500", "prefix_titles": [["title", "Automatic Speech Recognition And Limited Vocabulary: A Survey"], ["section", "Automatic speech recognition approaches"], ["subsection", " Deep Learning"]], "content": "Deep Learning (DL) is a set of algorithms in ML. It uses model architectures made up of multiple non-linear transformations (neural networks) to model high-level abstractions in data . Deep Neural Networks (DNNs) work well for ASR when compared with Gaussian Mixture Model-based HMMs (GMM-HMM) systems, and they even outperform the latter in certain tasks . DL employs the Convolutional Neural Network (CNN) approach which owns the ability to automatically learn the invariant features to distinguish and classify the audio .\nBy learning multiple levels of data representations, DL can derive higher-level features from lower-level ones to form a hierarchy. For instance, in a speech classification task, the DL model can take phoneme values in the input layer and assign labels to the word in the sentence in the output layer. Between these two layers, there are a set of hidden layers that build successive higher-order features that are less sensitive to conditions, such as noise in the userâ€™s environment .\nDL can be implemented using various tools. However, Tensor Flow seems to be one of the best application methods currently available .\nFigure~\\ref{fig6} gives the steps of DL in ASR. Data augmentation helps to improve the performance of the model by generalising better and thereby reducing overfitting . Data augmentation creates a rich, diverse set of data from a small amount of data. Data augmentation can be applied as a pre-processing step before training the model or later, directly in real-time. Different augmentation policies can be applied to audio data such as Time warping, Frequency masking, and Time masking. Recently, a new augmentation method called SpecAugment has been proposed by Park et al. in  for the ASR system. They combined the warping of the features and the masking of blocks of frequency channels, as well as the blocks of time steps. To ease the augmentation process, a recent free MATLAB  Toolbox called Audiogmenter\\footnote{https://github.com/LorisNanni/Audiogmenter.}  has been proposed . \nThe feature extraction process aims to remove the non-dominant features and therefore reducing the training time while mitigating the complexity of the developed models.\n\\begin{figure*}\n\\centering\n\\includegraphics[width=16cm]{Fig6.png}\n\\caption{Steps of DL}\\label{fig6}\n\\end{figure*}", "cites": [6129, 8979, 7950], "cite_extract_rate": 0.375, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of deep learning in ASR, integrating some concepts from the cited papers like SpecAugment and the Audiogmenter toolbox. However, it lacks deeper synthesis of ideas across sources and does not critically evaluate the methods or identify broader trends. The abstraction is limited to general statements about feature extraction and data augmentation."}}
{"id": "c95fa329-c368-49f1-ab18-0b8e51ce5fb9", "title": "Construction", "level": "subsection", "subsections": [], "parent_id": "2db36247-cd4e-4d7f-8146-c9a46276d43e", "prefix_titles": [["title", "Automatic Speech Recognition And Limited Vocabulary: A Survey"], ["section", "Datasets management"], ["subsection", "Construction"]], "content": "Speech recognition research has traditionally required the resources of large organisations, such as universities or corporations . Microphones can save data coming from multiple sources; this permits the collection of enough data for recognition. Although this approach remains the most frequent use case, it is subject to some challenges including speaker localisation, speech enhancement, and ASR in distant-microphone scenarios .\nDatasets with limited vocabulary may be  a subset of larger speech datasets . Datasets are used to train and test ASR engines. It is important to have a multi-speakers database . To avoid inconsistencies between datasets, the collection is done during a short period (the same day if possible) . For the best results, the corpus of acoustic data used for learning must be performed in a good quality recording studio, but it does not have to occur in a professional studio. For instance, authors in used a Handy Recorder with four channels (H4n) that is  affordable\\footnote{https://www.amazon.com/Zoom-Four-Channel-Handy-Audio-Recorder/dp/B07X8CJBW9}. The creation of an audio dataset goes through four steps as shown in Figure~\\ref{fig7}. These steps are: requirement definition, corpus creation, voice recording, and labelling of the voice database.\n\\begin{figure*}\n\\centering\n\\includegraphics[width=16cm]{Fig7.png}\n\\caption{Construction of dataset for limited vocabulary}\\label{fig7}\n\\end{figure*}\n\\begin{enumerate}[label=\\alph*.)]\n  \\item Requirement definition:\nThe first step for the construction of a dataset for limited vocabulary consists of selecting a suitable environment, which can be a room with closed doors to reduce noise. The use of studio recorded audio is unrealistic, as these audios are free of background noise and recorded with high-quality microphones and are in a formal setting. Good ASR models should work equally well in moderated noisy environments with natural voices. For this, the audio recordings should be made in a simple environment with closed doors.\n\tFor good recognition, audio recordings should be made with several people (female and male) having different tones. The data should have a fixed and short duration to facilitate the training of the learning model and the evaluation process .\n\\item Corpus creation: \nAfter requirement definition, words or sentences of the limited vocabulary are chosen. The choice is made according to the needs, and it is limited to the context in which speech recognition will be performed. Only necessary data should be selected. The maximum size of the corpus for limited vocabulary is generally around a thousand of words.\n\\item Voice recording:\nSounds are recorded using a microphone that matches the desired conditions. It should minimise differences between training conditions and test conditions. Speech recordings are generally performed in an anechoic room and are usually digitised at 20 kHz using 16 bits  or at 8 kHz  or 16 kHz . The waveform audio file format container with file extension .wav is generally used . The WAV formats encoded to Pulse-Code Modulation (PCM) allow one to obtain an uncompressed and high-fidelity digital sound. Since these formats are easy to process in the pre-processing phase of speech recognition and for further processing, it is necessary to convert the audio files obtained after the recording (for instance OGG, WMA, MID, etc.) into WAV format.\n\\item \tLabelling of the voice database:\nMost ML models are done in a supervised approach. For supervised learning to work, a set of labelled data from which the model can learn to make correct decisions is required. The labelling step aims to identify raw data (text and audio files) and to add informative labels in order to specify the context so that an ML  model can learn from it. Each recorded file is marked by sub-words or phonemes. Labelling indicates which phonemes or words were spoken in the audio recording. To label the data, humans make judgments about some aspects of the unlabelled audio file. For example, one might ask to label all audios containing a given word or phoneme.\n\tThe ML model uses human-supplied labels to learn. The resulting trained model is used to make predictions on new data. Some software such as Audio Labeller\\footnote{https://www.mathworks.com/help/audio/ref/audiolabeler-app.html}  allows one to define reference labels for audio datasets. Audio Labeler also allows one to visualise these labels interactively.\n\\end{enumerate}\nThe minimal structure of a dataset for limited vocabulary takes into account elements such as the path to the audio file, the text corresponding to the audio file, the gender of the speaker, the age and the language used if there are many languages in the dataset\\footnote{https://huggingface.co/datasets/timit\\_asr }. An excerpt of such a dataset for ASR using limited vocabulary is given in Table \\ref{tab3}.\n\\begin{table*}\n\\caption{Structure of a multilingual dataset for limited vocabulary}\\label{tab3}\n\\centering\n\\begin{tabular}{ p{3cm} p{3cm} p{3cm} p{3cm} p{3cm} }\n \\hline\n Audio Wav file&\tText&\tAge&\tSex\t&Accent (Language)\\\\\n \\hline\nC:/dataset/audio$_1.wav$&\tOne&\t23\t&M&\tEnglish\\\\\nC:/dataset/audio$_2.wav$&\tTwo&\t20&\tF&\tEnglish\\\\\n$\\cdots$&$\\cdots$&$\\cdots$&$\\cdots$&$\\cdots$\\\\\nC:/dataset/audio$_n.wav$&\tDix\t&35\t&M&\tFrench\\\\\n\\hline\n\\end{tabular}\n\\end{table*}\nIf there are several speakers, then the speaker's index will be one key element in Table \\ref{tab3}. Some researchers also take into account the emotion of the speaker, mentioning if he is neutral, happy, angry, surprised or sad .", "cites": [7139, 8980, 6131, 6130], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 14, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a clear and structured description of the dataset construction process for limited vocabulary ASR, including steps like requirement definition, corpus creation, voice recording, and labelling. However, it offers minimal synthesis of the cited papers, only briefly referencing them without connecting their insights to broader themes. There is little to no critical analysis or abstraction to identify overarching principles or trends in the field."}}
{"id": "3c2b6af0-b36d-448b-bce2-cf48b3d0d2fb", "title": " Languages and Datasets", "level": "subsection", "subsections": [], "parent_id": "2db36247-cd4e-4d7f-8146-c9a46276d43e", "prefix_titles": [["title", "Automatic Speech Recognition And Limited Vocabulary: A Survey"], ["section", "Datasets management"], ["subsection", " Languages and Datasets"]], "content": "In the literature, several works have developed datasets such as VoiceHome2  that developed a French corpus for distant microphone speech processing in domestic environments. Also regarding French, the work in  proposes a multi-source data selection for the training of LMs dedicated to the transcription of broadcast news and TV shows.\nAn important work focusing on the English language is the reduced voice command database . It has been created from a worldwide cloud speech database and in combination with training, testing and real-time recognition algorithms based on artificial intelligence and DL neural networks. Another important English dataset for digits from 0 to 9, and 10 short phrases is the AV Digits Database . In a survey, 53 participants consisting of 41 males and 12 females were asked to read digits in English in random order five times. In another study, 33 agents were asked to record sentences, each sentence was repeated five times in 3 different modes: neutral, whisper and silent speech. A  larger dataset is the Isolet dataset proposed in . This dataset contains  150 voices divided into five groups of 30 people. In this dataset, each speaker pronounces  each letter of the English alphabet twice, which provides a set of 52 training examples for each speaker.\nApart from French and English, other languages, such as Sorani Kurdish have been tackled. BD-4SK-ASR (Basic Dataset for Sorani Kurdish Automatic Speech Recognition) is an experimental dataset which is used in the first attempt in developing an ASR system for Sorani Kurdish .\nA very large project run by Mozilla is the Common Voice Project initiated with the aim of producing an open-source database for ASR . The Mozilla Common voice dataset created in 2017 is intended for developers of language processing tools. In November 2020, more than 60 languages were represented on the platform. These languages include French, English, Chinese, Danish and Norwegian.\nIt is very important that speech recognition systems be tested for efficiency, regardless of the language.", "cites": [6132, 6133, 3626], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.2, "critical": 1.5, "abstraction": 1.8}, "insight_level": "low", "analysis": "The section provides a factual list of datasets for various languages, including French, English, and Sorani Kurdish, but lacks meaningful synthesis of their contributions or relationships. There is minimal critical evaluation of the works, such as their limitations or effectiveness for limited vocabulary ASR. The content remains at a concrete level without abstracting broader trends or principles in dataset management for under-resourced languages."}}
{"id": "e136a512-9399-4309-8e08-b6210980212f", "title": "Open-source code systems", "level": "subsection", "subsections": [], "parent_id": "f13a2853-cabf-4c00-aa17-606cca4b52a6", "prefix_titles": [["title", "Automatic Speech Recognition And Limited Vocabulary: A Survey"], ["section", "Speech recognition frameworks or Toolkits"], ["subsection", "Open-source code systems"]], "content": "A plethora of open-source frameworks, engines, or toolkits for ASR systems are proposed in the literature. The following is a non-exhaustive list of main works or projects. Most of them can easily handle small and large vocabulary.  \nThe HTK\\footnote{https://htk.eng.cam.ac.uk/} is one of the first speech recognition software developed since the late 1980s. It has been available for the research community since 2000 and is maintained at the Speech Vision and Robotics Group\\footnote{http://mi.eng.cam.ac.uk} of the Cambridge University Engineering Department (CUED) . It provides recipes to build baseline systems with HMM. HTK is considered a very simple and effective tool for research . It can build a noise-robust ASR system in a moderated noisy level environment, especially for small vocabulary systems. It is a practical solution to develop fast and accurate Small Vocabulary Automatic Speech Recognition (SVASR) . Several works use this toolkit .\nOne of the most popular toolkits is the CMU Sphinx\\footnote{https://cmusphinx.github.io/ }, designed for both mobile and server applications. CMU Sphinx is in fact a set of libraries and tools that can be used to develop speech-enabled applications. It is developed at Carnegie Mellon University in the late 1980s . Several versions have been released including Sphinx 1 to 4, and PocketSphinx for hand-held devices . CMU Sphinx is currently attracting the attention of the research community. It offers the possibility to build new LMs using its language Modelling Tool\\footnote{http://www.speech.cs.cmu.edu/tools/lmtool-new.html }.\nA former toolkit that may no longer be available is the Rapid Language Adaptation Toolkit (RLAT) introduced in  and used in . The website for the project is, unfortunately, no longer available. \nKaldi is an extendable and modular toolkit for ASR . The large community behind the project provides numerous third-party modules that can be used for several tasks. It supports DNN and offers excellent documentation on its website\\footnote{https://kaldi-asr.org/}. Several works have been based on this toolkit including . \nMicrosoft also proposes an open-source Cognitive Toolkit\\footnote{https://docs.microsoft.com/en-us/cognitive-toolkit/} (CNTK). It is used to create DNNs that power many Microsoft services and products. It enables researchers and data scientists to easily code such neural networks at the right abstraction level, and to efficiently train and test them on production-scale data . Although it is still used, it is a deprecated framework.\nJulius\\footnote{https://julius.osdn.jp/en\\_index.php} is software normally designed for LVCSR. It is based on word $N-gram$ and context-dependent HMM. It is used in several works such as . \nSimon toolkit\\footnote{https://simon.kde.org/ } is a general public license speech recognition framework developed in C++. It is designed to be as flexible as possible and it works with any language or dialect. Simon makes use of KDE libraries, CMU SPHINX or Julius together with HTK and it runs on Windows and Linux.\nPraat\\footnote{https://www.fon.hum.uva.nl/praat/} is a framework that enables speech analysis, synthesis, and manipulation. In addition, it allows speech labelling and segmentation. It has been used in .\nMozilla Common Voice\\footnote{https://commonvoice.mozilla.org/ \n} is a free speech recognition software for developers that can be integrated into projects. It works with DNN technology and targets several languages . Besides Common Voice, Mozilla has also developed DeepSpeech\\footnote{https://github.com/mozilla/DeepSpeech}, an open-source Speech-To-Text engine. It makes use of a model trained by the ML techniques proposed in . \nAnother larger project is the OpenSMILE (open-source Speech and Music Interpretation by Large-space Extraction) project\\footnote{https://www.audeering.com/opensmile/} that is completely free to use for research purposes. It received a lot of attention from the research community and claims more than 150,000 downloads. \nA recent model called Jasper (Just Another Speech Recognizer) has been introduced in 2019 . It can be used with the OpenSeq2Seq\\footnote{ttps://nvidia.github.io/OpenSeq2Seq/html/index.html} TensorFlow-based toolkit. OpenSeq2Seq enables, among others, speech recognition, speech commands and speech synthesis. It is used in recent works such as .\nOther recent engines for ASR have been released such as Fairseq\\footnote{https://github.com/pytorch/fairseq} and Wav2Letter++\\footnote{https://github.com/facebookresearch/wav2letter} (both developed by Facebook), Athena\\footnote{https://github.com/athena-team/athena}, ESPnet\\footnote{https://espnet.github.io/espnet/}, and Vosk\\footnote{https://alphacephei.com/vosk/} which is an offline ASR toolkit. Table \\ref{tab5} provides a summary of some open-source speech recognition toolkits.\n\\begin{table*}\n\\caption{A non-exhaustive list of open-source speech recognition frameworks/toolkits}\\label{tab5}\n\\centering\n\\begin{tabular}{ p{3cm} p{2cm} p{3cm} p{4cm} p{4cm}}\n \\hline\nToolkit / $1^{st}$ Release&Programming language&License&Trained models&\nApplied technology\\\\\n \\hline\nPraat, 1991& C/C++, Objective C& GNU General Public License&-&Neural networks\\\\\nHTK, 2000&C/C++&HTK Specific License&English&HMM, DNN\\\\\nCMU Sphinx, 2001&Java&BSD License&English plus 10 other languages&HMM\\\\\nSimon, 2008& C++&GNU General Public License&-&HMM\\\\\nKaldi, 2009&C++&Apache&Subset of English& DNN \\\\\nOpenSmile, 2010&C++&Free for non-commercial use&-&Machine/DL\\\\\nWav2Letter, 2016&Torch (Lua)&BSD License&English& Neural Networks\\\\\nMicrosoft CNTK, 2016&C++&BSD License&-& DNN\\\\\nMozilla Common Voice, 2017&C, Python, Java&Public domain CC-O&English, French, German, Chinese, $\\cdots$ & DNN\\\\\nOpenSeq2Seq, 2018&Python&Apache&-& DNN\\\\\nFairseq, 2019&Python&BSD Licence&English, Chinese, German, and French&Neural Networks\\\\\n \\hline\n\\end{tabular}\n\\end{table*}\nSeveral works have been performed for limited vocabularies in ASR using the above toolkits. We make a summary of these works in the following section.", "cites": [6135, 3626, 859, 8981, 6134, 6136], "cite_extract_rate": 0.2222222222222222, "origin_cites_number": 27, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily functions as a descriptive overview of various open-source ASR toolkits, listing their features, technologies, and languages supported. While it attempts to group toolkits by their technological approaches (e.g., HMM vs. DNN), it lacks deeper synthesis of ideas or trends across the cited works. There is minimal critical evaluation or abstraction beyond the tools themselves, and the cited papers are referenced only in a general way, without integration into a broader analytical narrative."}}
{"id": "ce3eb1c6-1db6-4069-8e42-89b1ab57b71b", "title": "Selected works on ASR using limited vocabulary", "level": "subsection", "subsections": [], "parent_id": "86191902-4e9e-4008-97e8-ccff3cf50de7", "prefix_titles": [["title", "Automatic Speech Recognition And Limited Vocabulary: A Survey"], ["section", "Summary of related work on ASR using limited vocabulary"], ["subsection", "Selected works on ASR using limited vocabulary"]], "content": "ASR using limited vocabulary has attracted the attention of researchers. The literature proposed systems for European languages such as English , Estonian , Norwegian , Romanian , Macedonian , as well as Asian languages including Chinese , Hindi , Indonesian , Arabic , Bangla  , Malayalam , Tamil , Marathi , Punjabi  and Urdu . Table \\ref{tab6} provides a list of selected works done on ASR using limited vocabulary base. \n\\end{multicols}\n\\begin{landscape}\n\\begin{table*}\n\\caption{Summary of works on ASR using limited vocabulary.}\\label{tab6}\n\\centering\n\\begin{tabular}{ p{5cm} p{3cm} p{4cm} p{2cm} p{2cm} p{2cm} p{4cm}}\n \\hline\nPapers, Year&Language&Toolkit/Model&Noisy&Acc (\\%)&\\# Speakers&Dataset \\\\\n \\hline\n&English&HTK&Yes& -&10&780 utterances\\\\\n &Romanian&LGB and LVQ3&Yes&95.33&30&10 Digits\\\\\n&Estonian&HTK&No& 97&60&400 utterances of numbers\\\\\n &Tamil& Feedforward neural networks&Yes&81&10 children&20 phonemes\\\\\n &Malayalam&From scratch using ANN&Yes&\n89&-&5 words\\\\\n &Arabic&HTK&Yes&81.79&59&944 sentences\\\\\n&Bangla&HTK&Yes&90-95&100&10 digits\\\\\n&Yoruba/ Hebrew&HTK&No&100&3&50 words/ short phrases\\\\\n&Urdu&CMU Sphinx&Yes&60&10&52 words\\\\\n&Arabic&HTK&No&97.99&13&33 words\\\\\n &Malayalam&From scratch: DWT and ANN&Yes&80&1&250 words\\\\\n&English&CMU Sphinx&No&71.73&-&11 words\\\\\n&Chinese&From scratch and HMM-based&Yes&89.6&8&640 speech samples\\\\\n&Wolof&HTK&Yes&81.9&25&Digits\\\\\n&Hindi&Microsoft Speech Server&Yes&90&24&79 words\\\\\n&Macedonian&HTK&Yes&94.52&30&188 words\\\\\n&Chinese&PocketSphinx (CMU)&Yes&90&1&10 voice commands\\\\\n&English&Kaldi&Yes&46.4&20&256 words\\\\\n&Marathi&HTK&No&80-90&5&620 words\\\\\n&Romanian&SRILM toolkit  and ProtoLOGOS &No&59.57&30&762 phrases\\\\\n&English&Kaldi&Yes&98&-&62 keywords\\\\\n&Norwegian&CMU Sphinx&Yes&58.31&24&10 sentences\\\\\n&Indonesian&HTK&Yes&100&1&Digits\\\\\n&Amazigh&CMU Sphinx&No&92.22&30&13 commands\\\\\n&Punjabi&CMU Sphinx&No&69.6&50&91 words\\\\\n&Dutch&PPVT&Yes&75.97&132&384 words\\\\\n&Chinese&Keras/Tensorflow&Yes&97.5&-&877 words\\\\\n&English&Google's Speech-to-Text API&Yes&95.2&432&300 shorts phrases\\\\\n&Spanish&Kaldi&Yes&55.78&33&164 words\\\\\n&English&Labview and NI my RIO 1900&Yes&96.375&3&10 words\\\\\n \\hline\n\\end{tabular}\n\\end{table*}\n\\end{landscape}\n\\begin{multicols}{2}", "cites": [6994, 6134], "cite_extract_rate": 0.06666666666666667, "origin_cites_number": 30, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section primarily presents a list of selected works on ASR using limited vocabulary with language, toolkit, and performance details, but it does not synthesize findings or connect ideas across papers. There is minimal critical analysis or identification of broader patterns, making it largely descriptive in nature."}}
{"id": "ab436628-e6d5-445c-8955-a4bc19da6b52", "title": "Future directions", "level": "section", "subsections": [], "parent_id": "c99b7cd0-bfbd-4d1c-9bd1-123b6e7e8e3e", "prefix_titles": [["title", "Automatic Speech Recognition And Limited Vocabulary: A Survey"], ["section", "Future directions"]], "content": "The World has more than 7000 languages according to the Ethnologue website\\footnote{https://www.ethnologue.com}, with the majority being under-resourced and even endangered. ASR systems offer an unprecedented opportunity to sustain such under-resourced languages and to fight the extinction of endangered ones. We should differentiate two types of under-resourced languages: those with an acknowledged written form and those without. In the first case, new datasets should be created and require new approaches for data recording and labelling, especially when there are not enough native speakers during the creation process. This is a challenge, especially with tonal languages. In fact, most languages in the developing world and especially in Sub-Saharan Africa are tonal . In a  recent survey on ASR for tonal languages , only two African languages were reported. In addition, some languages (especially dialects) share commonalities. Therefore, ASR systems with limited vocabulary targeting multiple similar languages can be designed. \nIn the second case, when the language that does not have an acknowledged written form, a new approach should be designed. Normally, an ASR system aims to transcribe a speech into a text.  In ASR using a limited vocabulary, the transcript text is usually a command or a short answer that can be used by an application or system to perform an action. In this scenario, the system can be a combination of a Speech-to-Speech Translation (S2ST) and a Speech Recognition. First, the speech in a non-written language is directly translated into a speech in a well-resourced language such as English, then the transcript in a well-resourced language is retrieved (generated) and sent to the application. The Direct S2ST model has been developed in , translating Spanish into English without passing through text. Their dataset is a subset of the Fisher dataset and is composed of parallel utterance pairs. The construction of datasets for ASR systems using limited vocabulary of nonwritten languages can also be based on the same principle.\nRegardless of whether the language is written or not, more noise resistant models should be developed, because the available data for under-resourced languages could be of low quality.\nThe limited computing resources and poor internet connectivity in some regions can prevent the use of ASR systems. There is currently a shift of ASR models from the cloud to the edge. It is performed by reducing the size of models and making models fast enough so that they can be executed on typical mobile devices. The latest optimisation techniques to achieve this, such as pruning, efficient Recurrent Neural Network variants and quantisation, are presented in . Although they provide remarkable results, such as reducing the size by 8.5x and increasing the speed by 4.5x, there is still a need to develop light and offline models that can be deployed on low-resource devices, such as off-the-shelf smartphones or raspberry Pi/Arduino modules.", "cites": [6138, 6137], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the two cited papers into a broader discussion about ASR for under-resourced languages, especially in terms of data construction and model optimization for edge deployment. It provides a critical perspective by highlighting limitations such as lack of coverage for African tonal languages and the need for further development of light models. The section abstracts beyond the specific papers to present overarching challenges and potential frameworks for addressing language preservation and technological accessibility."}}
