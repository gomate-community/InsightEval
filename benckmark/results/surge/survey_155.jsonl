{"id": "5572583f-5cd2-408e-8762-33546c0f8567", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "bcddf019-4576-40c9-aa47-8a01af6db525", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Introduction"]], "content": "\\label{Introduction}\nIn the fields of Artificial Intelligence (AI), algorithmic fairness, and (big) data ethics, the term \\textit{bias} usually refers to the case in which AI-powered decisions show prejudice against individuals or groups of people defined based on protected attributes like gender or race . Instances of this prejudice have caused discrimination in many fields, including recidivism scoring , online advertisement , facial recognition , and credit scoring . \nDefining the concepts of bias and fairness in mathematical terms is not a trivial task.  provided a survey on more than 20 different measures of algorithmic fairness, many of which are incompatible with each other.\nThis incompatibility - the so-called impossibility theorem  - forces scientists and practitioners to choose the measures they use based on their personal beliefs or other constraints (e.g., business models) on what has to be considered fair for the particular problem/domain.\nWhile algorithms may also be responsible for the amplification of pre-existing biases in the training data , the quality of the data itself contributes significantly to the development of discriminatory AI applications.  identified two ways in which bias is encoded in the data: correlations and causal influences among the protected attributes and other features; and the lack of representation of protected groups in the data. They also noted that biases manifest in ways that are specific to the data type. \nIn this work, we focused on how biases can be encoded in the data (e.g., via spurious correlations, causal relationship among the variables, and unrepresentative data samples) and, in particular, in \\emph{visual} data (i.e., images and videos), which comprises one of the most popular and complex data types. Visual data encapsulates many features that require human experience and context to interpret. These include the human subjects, how they are depicted and their reciprocal position in the image frame, implicit references to culture-specific notions and background knowledge, etc. Even the colouring scheme can convey different messages. Thus, making sense of visual content remains a very complex task, and understanding bias in visual data is even harder.\nComputer vision (CV), the primary domain that enables computers to gain high-level understanding from visual data, is heavily dominated nowadays by deep learning (DL) methods  that allowed for outstanding performance in tasks like object detection, image classification and image segmentation. DL methods, however, rely heavily on data, and the results are as good and ``fair'' as the data used for their training. CV has recently drawn attention for its ethical implications when deployed in several settings, ranging from targeted advertising to law enforcement. There has been mounting evidence that deploying CV systems without a comprehensive ethical assessment may result in major discrimination against protected groups. For instance, facial recognition technologies , gender classification algorithms , and autonomous driving systems  have been all shown to exhibit discriminatory behaviour. \nWhile bias in AI systems is a well-studied field, the research in biased CV is more limited despite the abundance of visual data produced nowadays and their widespread use in the ML community. Moreover, to the best of our knowledge, there is no comprehensive survey on bias in visual datasets ( represents a seminal work in the field, but it is limited to object detection datasets). Hence, the contributions of the present work are: i) to explore and discuss the different types of bias that arise from the collection of visual data; ii) to systematically review the works that aim at addressing and measuring bias in visual datasets; iii) to discuss some attempts to compile bias-aware datasets. We believe this work to be a useful tool for helping scientists and practitioners to both develop new bias-discovery methods and collect data in ways as less biased as possible. To the latter end, we propose a checklist that can be used to spot the different types of bias that might enter the data during the collection process (Table \\ref{tab:checklist}). \nThe structure of the survey is as follows.\nFirst, we describe in detail the different types of bias that might affect visual datasets (Section \\ref{sec:CVBias}), provide concrete examples of CV applications that are affected by those biases, and a description of how they manifest in the life cycle of visual content. Second, we systematically review the methods for bias discovery in visual content proposed in the literature (Section \\ref{sec:Discovery}). Third, in Section \\ref{sec:BiasFree}, we discuss the weaknesses and strengths of some bias-aware visual benchmark datasets. Finally, in Section \\ref{sec:Conclusions}, we conclude and outline some possible future research direction.", "cites": [4925, 3900, 166, 4927, 4928, 4926, 3112, 8704], "cite_extract_rate": 0.5333333333333333, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several cited papers to establish a context for bias in AI and visual datasets, connecting ideas about fairness, ethical implications, and bias sources. It provides some critical commentary, such as the impossibility theorem and limitations in current bias-aware datasets. However, the analysis remains largely conceptual without deep evaluation of individual papers or a novel framework. The section also begins to abstract concepts like the role of data quality and the challenges of fairness definitions in visual contexts."}}
{"id": "14f73cd8-032b-4c90-9a94-12b5e00bde6f", "title": "Manifestation of Bias in Visual Data", "level": "section", "subsections": ["bc6387a1-c15b-432b-ab1e-ba11dc653120", "f0935b5e-10a1-465f-b2fa-9243e1256906", "cc7f1542-b1f4-49ce-b76d-ed7378840e5e", "5f99812f-d488-4665-86e8-676bc5bf9fb0"], "parent_id": "bcddf019-4576-40c9-aa47-8a01af6db525", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Manifestation of Bias in Visual Data"]], "content": "\\label{sec:CVBias}\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width = 430pt]{Images/biases.pdf}\n    \\caption{Examples of selection, framing and label bias. On the right, a list of applications that can be affected by each type of bias.}\n   \\label{fig:biases}\n\\end{figure*}\nIn this section, we describe in detail the types of bias that pertain to the \\emph{capture} and \\emph{collection} of visual data (Figure \\ref{fig:biases}), namely: selection bias (Section~\\ref{sec:selectionbias}), framing bias (Section~\\ref{sec:framingbias}) and label bias (Section~\\ref{sec:labelbias}). Furthermore, we describe (Section \\ref{subsec:LifeCycle}) how they manifest within the life cycle of visual content, from capture to the deployment of CV algorithms. Note that a comprehensive analysis of historical discrimination and algorithmic bias is beyond the scope of this work. The interested reader can refer to  for a survey on methods for auditing algorithmic bias both in CV and other AI-related areas.\nOur categorisation builds on the scheme by  who organised the types of visual bias into four different categories: \\emph{selection bias}, \\emph{capture bias} (which we collapse into the more general concept of \\emph{framing bias}), \\emph{label bias}, and \\emph{negative set bias}. The latter arises when the labelling does not reflect entirely the population of the negative class (say \\emph{non-white} in a binary feature [white people/non-white people]). We consider negative class bias as an instance of selection and label bias. \nEven though our categorisation appears on the surface to be similar to the one by , their analysis focused on datasets for object detection. Instead, we contextualise bias in a more general setting and we also focus on discrimination against protected groups\\footnote{Note that, while this is the focus of our survey, we also take into account cases in which bias does not necessarily affect people, e.g., in object detection.}. Since selection, framing and label bias manifest in many different ways, we also go further by describing a sub-categorisation of these three types of bias (Table \\ref{tab:sub_biases}) including several biases commonly encountered in Statistics, Health studies, or Psychology and adapting them to the context of visual data. While in the following we describe selection, framing, and label bias in general terms, we also provide references in Table \\ref{tab:sub_biases} for the interested reader who might want to delve further into their different manifestations.", "cites": [4929], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates a strong analytical approach by building upon prior categorizations of bias and adapting them to the context of visual data. It integrates concepts from different fields like Statistics and Psychology, offering a meta-level abstraction. While it does not deeply critique specific cited works, it identifies broader patterns and situates the biases within the life cycle of visual content, providing a coherent and insightful narrative."}}
{"id": "25a00b63-08ca-4162-addc-4f5e78eaccbe", "title": "Affected applications", "level": "paragraph", "subsections": [], "parent_id": "5f15f2b0-1124-4b2f-82ed-7fa98108423a", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Manifestation of Bias in Visual Data"], ["subsection", "Selection Bias"], ["paragraph", "Definition"], ["paragraph", "Affected applications"]], "content": "In summer 2020, the New York Times published the story of a Black American individual wrongfully arrested due to an error made by a facial recognition algorithm . While we do not know whether bias in the data caused this exact case, we know that selection bias can lead to different error rates in face recognition. Hence, such technology would require much more care, especially in high-impact applications. \nAutonomous driving systems are also likely affected by selection bias as it is very challenging to collect a dataset that describes every possible scene and situation a car might face. The Berkeley Driving Dataset  for example, contains driving scenes from only four cities in the US; an autonomous car trained on such a dataset will likely under-perform in other cities with different visual characteristics. The effect of selection bias on autonomous driving becomes particularly risky when it affects pedestrian recognition algorithms.  studied the impact of under-representation of darker-skinned people on the predictive inequity of pedestrian recognition systems. They found evidence that the effect of this selection bias is two-fold: first, such imbalances ``beget less statistical certainty\" making the process of recognition more difficult; second, standard loss functions tend to prioritise the more represented groups and hence some kind of mitigation measures are needed in the training procedure .\nMoreover,  explained that part of the collection of benchmark face datasets is often done using facial detection algorithms. Therefore, every systematic bias in the training of those tools propagates to other datasets. This is a clear example of \\textit{algorithmic bias turning into selection bias} (see \\emph{automation bias}, Table \\ref{tab:sub_biases}), as described at the end of Section \\ref{subsec:LifeCycle}. Furthermore, image search engines can contribute to the creation of selection biases (see \\emph{availability bias}, Table \\ref{tab:sub_biases}) as well due to systematic biases in their retrieval algorithms; see,  for a study on gender bias in Google's image search engine.", "cites": [4928], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from cited papers to illustrate how selection bias affects specific applications like facial recognition and autonomous driving, connecting the issue to real-world consequences. It provides some critical analysis by pointing out the limitations of current datasets and the risks associated with biased algorithms, but does not deeply evaluate or contrast the methods. The section generalizes to a degree by discussing broader implications (e.g., automation bias, availability bias), but the abstraction remains at a moderate level, focusing on examples rather than overarching principles."}}
{"id": "5a3c72f2-3ea9-43c1-964d-63e77b0ec98f", "title": "Remarks", "level": "paragraph", "subsections": [], "parent_id": "5f15f2b0-1124-4b2f-82ed-7fa98108423a", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Manifestation of Bias in Visual Data"], ["subsection", "Selection Bias"], ["paragraph", "Definition"], ["paragraph", "Remarks"]], "content": "Finally,  pointed out that, while class imbalances have undoubtedly significant impact on some facial recognition algorithms, they do not explain every disparity in the performance of algorithms. For instance, they suggested that a group of subjects might be more challenging to recognise, even with balanced training data, if it is associated with higher variance (for example, due to hairstyle or make-up). The reader can also refer to  for an analysis of the effect of these kinds of attributes on facial recognition technology.", "cites": [4930], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides an analytical perspective by discussing limitations of class imbalances in explaining performance disparities in facial recognition, citing and extending insights from one key paper. It synthesizes the idea that other factors like variance in attributes may contribute to bias, and abstracts slightly by suggesting broader implications for understanding bias in visual data. However, it lacks deeper integration of multiple sources or more nuanced critique."}}
{"id": "60d92a5e-47ff-431e-b78f-b1046a05eebe", "title": "Definition", "level": "paragraph", "subsections": ["6c677191-a7b9-4b14-aa26-e84a1fe12181", "ab3d0da9-c847-49f5-9c05-acde08d49349", "e5c97031-4a5d-45f8-86dc-4fc34012e99a"], "parent_id": "cc7f1542-b1f4-49ce-b76d-ed7378840e5e", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Manifestation of Bias in Visual Data"], ["subsection", "Label Bias"], ["paragraph", "Definition"]], "content": "For supervised learning, labelled data are required. The quality of the labels\\footnote{Note that by label we mean any tabular information attached to the image data (object classes, measures, protected attributes, etc.)} is of paramount importance for learning and comprises a tedious task due to the complexity and volume of today's datasets.  define label bias as ``\\emph{the bias that arises when the labels differ systematically from the ground truth}''. For instance,  found some evidence that the face recognition dataset Labelled Faces in the Wild (LFW, see  for the original release and  for the attributes) presents a very low label accuracy against human annotation. \nFurthermore,  highlight bias as a result of the labelling process itself for reasons like ``semantic categories are often poorly defined, and different annotators may assign different labels to the same type of object\". Torralba and Efros' work mainly focused on object detection tasks, and hence by \\emph{different label assignment} they refer, e.g., to ``grass'' labelled as ``lawn'' or ``picture'' as ``painting''. Nevertheless, these problems arise especially when dealing with human-related features such as race or gender.\\\\\n\\noindent\n\\begin{minipage}[t]{0.45\\textwidth}\n    \\fbox{\n    \\parbox{\\textwidth}{\n\\emph{We define \\emph{label bias} as any errors in the labelling of visual data, with respect to some ground truth, or the use of poorly defined or inappropriate semantic categories}.}}\n\\end{minipage} \\\\", "cites": [4926, 8739], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes concepts from two cited papers to define label bias, integrating both technical and practical perspectives. It abstracts the issue by distinguishing between label inaccuracies and inappropriate semantic categories, extending beyond specific examples. While it offers some critical discussion about the limitations of existing datasets, it lacks deeper comparative analysis or a novel framework."}}
{"id": "6c677191-a7b9-4b14-aa26-e84a1fe12181", "title": "Description", "level": "paragraph", "subsections": [], "parent_id": "60d92a5e-47ff-431e-b78f-b1046a05eebe", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Manifestation of Bias in Visual Data"], ["subsection", "Label Bias"], ["paragraph", "Definition"], ["paragraph", "Description"]], "content": "As already mentioned, a significant source of labelling bias is the \\emph{poor definition of semantic categories}. Race is a particularly clear example of this: according to  ``The obvious biological differences among humans allow one to make educated guesses about an unknown person’s ancestry, but agreeing on a catalogue of human races has so far proved impossible\". Given such an impossibility, racial categorisation in visual datasets must come at best from subjects' own race perception or, even worse, from the stereotypical bias of annotators. From a CV standpoint then, it would be probably more accurate to use actual visual attributes, if strictly necessary, such as \\emph{Fitzpatrick skin type}  or \\emph{skin reflectance}  rather than a fuzzy category such as race. Note that, while skin tone can be a more objective trait, it still does not entirely reflect human diversity.\nSimilarly, the binary categorisation of gender has been criticised. The reader can refer to  to explore the challenges that the use of ill-defined categories poses to algorithmic fairness from both the ontological and operational points of view.\nMoreover,  argued that different annotators can come up with different labels for the same object. While this mainly applies to the labelling of object detection datasets rather than face datasets, where labels are usually binary or discrete, it gives us an important input about bias in CV in general: annotators' biases and preconceptions are reflected in the datasets.\nWe can view what has been described so far also as a problem of operationalisation of what  called unobservable theoretical constructs (see \\emph{measurement bias}, Table \\ref{tab:sub_biases}). They proposed a framework that serves as guideline for the mindful use of fuzzy semantic categories and answers the following questions on the validity of  operationalisations (or measurements) of a construct: ``Does the operationalization capture all relevant aspects of the construct purported to be measured? Do the measurements look plausible? Do they correlate with other measurements of the same construct? Or do they vary in ways that suggest that the operationalization may be inadvertently capturing aspects of other constructs? Are the measurements predictive of measurements of any relevant observable properties (and other unobservable theoretical constructs) thought to be related to the construct, but not incorporated into the operationalization? Do the measurements support known hypotheses about the construct? What are the consequences of using the measurements[?]\" \nA concrete case of the use of a fuzzy semantic category is provided in , where the creation of a face dataset for facial beauty is described. Since beauty and attractiveness are prototypical examples of subjective characteristics, it is obvious that attempts of constructing such datasets will be filled with the personal preconceptions of the participants who labelled the images (see \\emph{observer bias}, Table \\ref{tab:sub_biases}).", "cites": [4932, 4931, 4933, 4934], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers to build a coherent narrative around label bias, especially in relation to race and gender. It critically engages with the conceptual and operational challenges in defining such categories, and abstracts these issues to broader concerns about how biases in human perception affect CV datasets."}}
{"id": "ab3d0da9-c847-49f5-9c05-acde08d49349", "title": "Affected applications", "level": "paragraph", "subsections": [], "parent_id": "60d92a5e-47ff-431e-b78f-b1046a05eebe", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Manifestation of Bias in Visual Data"], ["subsection", "Label Bias"], ["paragraph", "Definition"], ["paragraph", "Affected applications"]], "content": "Since deep learning boosted the popularity of CV, a modern form of physiognomy has gained a certain momentum. Recently, several studies appeared claiming to be able to classify images according to the criminal attitude of the subjects\\footnote{See, for instance, the retracted article: Hashemi and Hall \\url{https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0282-4}. Last visited 30.03.2022.} or sexual orientation\\footnote{For example, Kosinski and Wang  \\url{https://www.gsb.stanford.edu/faculty-research/publications/deep-neural-networks-are-more-accurate-humans-detecting-sexual}. Last visited 30.03.2022.}. A commercial tool has also been released to detect terrorists and paedophiles. While ``doomed to fail\" for a series of technical reasons well explained by , these applications rely on a precise ideology that  called \\emph{computational empiricism}: an epistemological paradigm that claims, despite any scientific evidence, that the true nature of humans can be measured and unveiled by algorithms. The reader can also refer to the famous blog post ``Physiognomy's New Clothes\"\\footnote{B. Ag\\\"uera y Arcas, M. Mitchell and A. Todorov, \\textit{Physiognomy’s New Clothes}, May 2017. \\url{https://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a}. Last visited 30.03.2022.} for an introduction to the problem.", "cites": [4935, 6990], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates the cited papers by connecting their claims about facial profiling to the broader concept of computational empiricism. It critically evaluates the ideological underpinnings of these applications, rather than merely summarizing them, and points out their technical shortcomings. However, the analysis remains focused on a narrow set of examples without fully abstracting to broader patterns or proposing a new conceptual framework."}}
{"id": "e5c97031-4a5d-45f8-86dc-4fc34012e99a", "title": "Remarks", "level": "paragraph", "subsections": [], "parent_id": "60d92a5e-47ff-431e-b78f-b1046a05eebe", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Manifestation of Bias in Visual Data"], ["subsection", "Label Bias"], ["paragraph", "Definition"], ["paragraph", "Remarks"]], "content": "Just as selection bias, label bias can lead to a vicious cycle: a classification algorithm trained on biased labels will most likely reinforce the original bias when used to label newly collected data (see \\emph{automation bias}, Table \\ref{tab:sub_biases}). \n\\begin{table*}\n    \\centering\n    \\small\n    \\begin{adjustbox}{angle=0}\n    \\begin{tabular}{|l|p{75mm}|c|c|c|}\n    \\cline{3-5}\n    \\multicolumn{2}{c|}{}&\\multirow{4}{*}{\\rotatebox{270}{\\textbf{Selection}}}&\\multirow{4}{*}{\\rotatebox{270}{\\textbf{Framing}}}&\\multirow{4}{*}{\\rotatebox{270}{\\textbf{Label}}}\\\\\n    \\multicolumn{2}{c|}{}&&&\\\\\n    \\multicolumn{2}{c|}{}&&&\\\\\n    \\multicolumn{2}{c|}{}&&&\\\\\n    \\cline{1-2} \n    \\textbf{Name} & \\textbf{Description} &  &  &   \\\\\n    \\hline\n    Sampling bias$^*$ & Bias that arises from the sampling of the visual data. It includes class imbalance. & $\\bullet$ &  &   \\\\\n    Negative set bias  & When a negative class (say \\texttt{non-white} in a \\texttt{white/non-white} categorisation) is not representative enough. & $\\bullet$  &  & $\\bullet$  \\\\\n    Availability bias$^\\dagger$ & Distortion arising from the use of the most readily available data (e.g., using search engines).& $\\bullet$ &  & \\\\ \n    Platform bias & Bias that arises as a result of a data collection being carried out on a specific digital platform (e.g., Twitter, Instagram, etc.). & $\\bullet$ &  &  \\\\\n    Volunteer bias$^\\dagger$ & When data is collected in a controlled setting instead of being collected in-the-wild, volunteers that participate in the data collection procedure may differ from the general population. & $\\bullet$ &  &   \\\\\n    Crawling bias & Bias that arises as a result of the crawling algorithm/system used to collect images from the Web or with the use of an API (e.g., the keywords used to query an API, the seed websites used in a crawler). & $\\bullet$ & $\\bullet$ &  \\\\\n    Spurious correlation & Presence of spurious correlations in the dataset that falsely associate a certain group of subjects with any other features. & $\\bullet$ & $\\bullet$ &   \\\\\n    Exclusion bias$^*$ & Bias that arise when the data collection excludes partly or completely a certain group of people. & $\\bullet$ & $\\bullet$ &   \\\\\n    Chronological bias$^\\dagger$ & Distortion due to temporal changes in the visual world the data is supposed to represent. & $\\bullet$  & $\\bullet$ & $\\bullet$ \\\\\n    Geographical bias & Bias due to the geographic provenance of the visual content or of the photographer/video maker (e.g., brides and grooms depicted only in western clothes). & $\\bullet$  & $\\bullet$ &   \\\\\n    Capture bias  & Bias that arise from the way a picture or video is captured (e.g., objects always in the centre ,exposure, etc.). &  & $\\bullet$  &  \\\\\n    Apprehension bias$^\\dagger$ & Different behaviour of the subjects when they are aware of being photographed/filmed (e.g., smiling).  &  & $\\bullet$  & \\\\\n    Contextual bias  & Association between a group of subjects and a specific visual context (e.g., women and men respectively in household and working contexts) &  & $\\bullet$ &   \\\\ \n    Stereotyping$^\\mathsection$ & When a group is depicted according to stereotypes (e.g., female nurses vs. male surgeons).  &  & $\\bullet$  &   \\\\\n    Measurement bias  & Every distortion generated by the operationalisation of an unobservable theoretical construct (e.g., race operationalised as a measure of skin colour). &  &  & $\\bullet$   \\\\ \n    Observer bias$^\\dagger$ & Bias due to the way a annotator records the information. &  &  & $\\bullet$   \\\\\n    Perception bias$^\\dagger$ & When data is labelled according to the possibly flawed perception of a annotator (e.g., perceived gender or race) or when the annotation protocol is not specific enough or is misinterpreted. &  &  &  $\\bullet$  \\\\ \n    Automation bias$^\\mathsection$ & Bias that arises when the labelling/data selection process relies excessively on (biased) automated systems. & $\\bullet$ &  &  $\\bullet$  \\\\\n    \\hline\n    \\end{tabular}\n    \\end{adjustbox}\n    \\caption{A finer-grained categorisation of biases described in Section \\ref{sec:CVBias}. The bullets represent which types of bias they are a subcategory of (Note that there are overlaps as different kind of bias can co-occur). The definitions are adapted from: ($\\dagger$) \\url{https://catalogofbias.org/biases/}; (*) \\url{https://en.wikipedia.org/wiki/Sampling_bias}; ($\\mathsection$) \\url{https://en.wikipedia.org/wiki/List_of_cognitive_biases}; and .\n    }\n    \\label{tab:sub_biases}\n\\end{table*}", "cites": [4932, 4936], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides some synthesis by linking label bias to automation bias and referencing a table that categorizes various sub-biases. However, it does not deeply integrate the cited papers' theoretical or methodological contributions into a unified narrative. Critical analysis is limited to identifying how biased labels may reinforce existing biases, but it does not evaluate the approaches or limitations from the cited works. The abstraction level is moderate, as it introduces a categorization framework and discusses broader concepts like spurious correlations and generalization risks."}}
{"id": "a814cf95-a301-4cde-aa1f-4109b6d8c96c", "title": "1. Real world", "level": "paragraph", "subsections": ["4aea13a2-6553-48b3-a292-ac7c2f3d3aef", "8ebd2c0a-c4c4-444f-a1bd-1326395ba86d", "cf032f9e-4953-4734-8270-e892d6764fc5", "41bdf8ce-1f00-4b49-a357-b2db60efb89e", "e9694545-aeb5-4ea8-9fd0-edf68ee0ef00"], "parent_id": "5f99812f-d488-4665-86e8-676bc5bf9fb0", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Manifestation of Bias in Visual Data"], ["subsection", "Media Bias Life Cycle"], ["paragraph", "1. Real world"]], "content": "The journey of visual content alongside bias starts even before the content is generated. Inequalities undeniably shape our world, and this is reflected in the generation of data in general and of visual content in particular. For example,  found out that the dataset MS-COCO , a large-scale object detection, segmentation, and captioning dataset which is used as a benchmark in CV competitions, was more likely to associate kitchen objects with women. While both image capturing and dataset collection come at a later stage in the life cycle described in Figure \\ref{fig:LifeCycle}, it is clear that in this instance, such bias has roots in the gender division between productive and reproductive/care labour.\nNevertheless, as shown in the following paragraphs, each step of the life cycle of visual content can reproduce or amplify historical discrimination as well as insert new biases.", "cites": [486], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates a cited paper to illustrate how bias in visual datasets, such as MS-COCO, can originate from real-world inequalities, showing some synthesis. It offers a critical point by noting that such bias is rooted in societal labor divisions, but lacks deeper comparative or evaluative analysis of the paper’s methodology or limitations. The discussion begins to abstract the issue to a broader societal context, identifying a pattern in how real-world structures influence data."}}
{"id": "41bdf8ce-1f00-4b49-a357-b2db60efb89e", "title": "5. Data collection", "level": "paragraph", "subsections": [], "parent_id": "a814cf95-a301-4cde-aa1f-4109b6d8c96c", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Manifestation of Bias in Visual Data"], ["subsection", "Media Bias Life Cycle"], ["paragraph", "1. Real world"], ["paragraph", "5. Data collection"]], "content": "The part of the life cycle of visual content that is more relevant to our discussion is the collection of visual datasets. Here we encounter selection bias once again, as the data collection process can exclude or under-represent certain groups from appearing in the dataset, as well as \\emph{label bias} as great effort is usually expended to collect data along with additional information in the form of annotations. As we discussed in Section \\ref{sec:labelbias}, this process is prone to errors, mislabelling, and explicit discrimination (see  for an analysis of power dynamics in the labelling process of visual data). Furthermore, benchmark datasets have gained incredible importance in the CV field. On the one hand, they allowed significant measurable progress in the field. On the other hand, they represent only a small portion of the visual world (see  for a critique of the large-scale collection of visual data \\emph{in the wild}).", "cites": [4938, 4937], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes two cited papers to discuss issues in the data collection phase of visual datasets, linking them to broader concepts like selection bias and label bias. While it provides some critical perspective on the limitations of benchmark datasets, the critique is not deeply nuanced. It begins to generalize about the challenges in data collection, but the analysis remains somewhat constrained to the specific examples and does not fully elevate to overarching principles."}}
{"id": "e9694545-aeb5-4ea8-9fd0-edf68ee0ef00", "title": "6. Algorithms", "level": "paragraph", "subsections": [], "parent_id": "a814cf95-a301-4cde-aa1f-4109b6d8c96c", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Manifestation of Bias in Visual Data"], ["subsection", "Media Bias Life Cycle"], ["paragraph", "1. Real world"], ["paragraph", "6. Algorithms"]], "content": "Finally, there is the actual step of model training. Fairness and accountability of algorithms is a pressing issue as algorithm-powered systems are used pervasively in applications and services impacting a growing number of citizens . Important questions arise for the AI and CV communities on how to implement fairness in algorithms such as: What legal frameworks should governments put into place? What are the strategies to mitigate bias or make algorithms explainable? \nNevertheless, the journey of visual content does not end with the training of models. \nIndeed, there are several ways in which biased algorithms can generate vicious feedback loops as described in the remarks of Section \\ref{sec:labelbias}.\nMoreover, the recent explosion in popularity of generative models, namely Generative Adversarial Networks (GANs, ), has made the process of media creation very easy and fast (see  for a survey on AI-generated visual content). Such AI-generated media can then be reinserted in the content life cycle via the Web and present their own ethical issues (see  for a discussion on the socio-technical aspects and ambivalence of the Web of humans and AIs).", "cites": [4929, 4940, 4939], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section attempts to synthesize the role of algorithms in perpetuating bias and mentions feedback loops and generative models, drawing on cited papers for context. However, it does not deeply integrate or connect the ideas from the three papers into a cohesive narrative. The critical analysis is limited, as it raises questions without evaluating existing approaches or their shortcomings. Some abstraction is present, particularly in framing ethical issues and feedback loops, but broader patterns or principles are not clearly identified."}}
{"id": "f2b72fe9-0ce9-4dcb-bde4-82dfdae69bcf", "title": "Bias Discovery and Quantification in Visual Datasets", "level": "section", "subsections": ["36c012fa-4436-4116-8277-ac64b563bff2", "920fda46-f6b5-4b16-a421-faaf243af6cd", "872e15f1-aa9b-4253-a4da-2b737de6f3da", "bf512b8a-ed82-4af6-b451-ad1cebee16b6", "4137cf4b-134c-45cb-8a26-0d72b21ecf7c"], "parent_id": "bcddf019-4576-40c9-aa47-8a01af6db525", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Bias Discovery and Quantification in Visual Datasets"]], "content": "\\label{sec:Discovery}\n\\begin{table*}\n    \\centering\n    \\small\n    \\begin{adjustbox}{angle=0}\n    \\begin{tabular}{|l|l|}\n    \\hline\n    \\textbf{Symbol} & \\textbf{Description}\\\\\n    \\hline\n    $\\mathcal{D}$ & Cursive capital letters denotes visual datasets\\\\\n    $f_\\mathcal{D}(\\cdot)$ & A model $f$ trained on the dataset $\\mathcal{D}$ \\\\  \n     $\\mathbf{w}$ & Bold letters denotes vectors\\\\\n    $AP$ & Average Precision-Recall (AP) score\\\\\n    $AP_\\mathcal{B}(f_\\mathcal{D})$ & AP score of the model $f_{\\mathcal{D}}$ when tested on the dataset $\\mathcal{B}$\\\\\n    $||\\cdot ||_2$ & L2-norm\\\\\n    $|\\mathcal{D} |$ & The number of elements in the dataset $\\mathcal{D}$\\\\\n    $\\sigma(\\cdot)$ & Sigmoid function \\\\\n    $\\mathbf{1}[\\cdot]$  & Indicator function\\\\\n    $\\mathbb{P}(\\cdot)$ & Probability\\\\\n    $\\mathbb{P}(\\cdot|\\cdot)$ & Conditional probability\\\\\n    $H(\\cdot)$ & Shannon entropy\\\\\n    $H(\\cdot|\\cdot)$ & Conditional Shannon entropy\\\\\n    $I(\\cdot, \\cdot)$ & Mutual information\\\\\n    $D(\\cdot)$ & Simpson $D$ score\\\\\n    $ln(\\cdot)$ & Natural logarithm\\\\\n    $mean(\\cdot)$ & Arithmetic mean\\\\\n    \\hline\n    \\end{tabular}\n    \\end{adjustbox}\n    \\caption{Brief summary of the notation used in Section \\ref{sec:Discovery}.}\n    \\label{tab:notation}\n\\end{table*}\nThis section aims to understand how researchers have tackled the problem of discovery and quantification of bias in visual datasets since high-quality visual datasets are a critical ingredient towards fair and trustworthy CV systems . \nTo this end, we performed a systematic survey of papers addressing the following problem: \\textit{Given a visual dataset $\\mathcal{D}$, is it possible to discover/quantify what types of bias it exhibits (of those described in Section \\ref{sec:CVBias})?} In particular, we focus on the methodologies and measures used in the bias discovery process, of which we are going to outline the pros and cons. Furthermore, we will try to define open issues and possible future directions for research in this field. Note that this problem is critically different from both the problem of finding out whether an algorithm discriminates against a protected group and the problem of mitigating such bias. \nIn order to systematise this review, we proceed in the following way to collect the relevant material: \nfirst, we outlined a set of keywords to be used in three different scientific databases (DBLP, arXiv and Google Scholar) \nand we selected only the material relevant to our research question following a protocol described in the following paragraph; second, we summarised the results of our review and outlined the pros and cons of different methods in Section \\ref{sec:prosCons}. Our review methodology was inspired by the works of , and . Our protocol also resembles the one described in \\cite[Section 5.1.1]{kitchenham04}.\nGiven our problem, we identify the following relevant keywords: \\emph{bias}, \\emph{image}, \\emph{dataset}, \\emph{fairness}. For each of them, we also defined a set of synonyms and antonyms (see Table \\ref{tab:keywords}). Note that we include the words ``face\" and ``facial\" among the synonyms of the word image. This is mainly motivated by the fact that in the title and the abstract of the influential work of  there are no occurrences of the word ``image\". Instead, we find many occurrences of the expression ``facial analysis dataset\". Since facial analysis is an important case study for detecting bias in visual content, it makes sense to include it explicitly in our search. The search queries have been composed of all possible combinations of the different synonyms (antonyms) of the above keywords (for example, ``image dataset bias\", ``visual dataset discrimination\", or ``image collection fairness\"). \n\\begin{table*}\n\\centering\n\\begin{tabular}{ |l|l|l| }\n\\cline{2-3}\n\\multicolumn{1}{c|}{} & \\textbf{Synonyms} & \\textbf{Antonyms} \\\\\n\\hline\n \\textbf{bias} & discrimination & fair, unbiased \\\\ \n \\textbf{image} & visual, face, facial &  \\\\  \n \\textbf{dataset} & collection &   \\\\\n \\hline\n\\end{tabular}\n\\caption{Set of keywords and relative synonyms and antonyms.}\n\\label{tab:keywords}\n\\end{table*}\nThis process resulted, after manual filtering\\footnote{The manual filtering consisted in keeping only those papers that describe a method or a measure for discovering and quantifying bias in visual datasets. In some cases, we kept also works developing methods for algorithmic bias mitigation but that could be used to discover bias in the dataset as well (see ).}, in 17  relevant papers.\nWe expanded the list with 6 articles by looking at papers citing the retrieved works (via Google Scholar) and their ``related work\" sections. We also added to the list the work of  on causal signals in images that was not retrieved by the protocol described above.\nIn the following, we review all these 24 papers, dividing them into four categories according to the strategies they use to discover bias: \n\\begin{itemize}\n\\item \\textbf{Reduction to tabular data:} these rely on the attributes and labels attached to or extracted from the visual data and try to measure bias as if it were a tabular dataset.\n\\item \\textbf{Biased image representations:} these rely on lower-dimensional representations of the data to discover bias.\n\\item  \\textbf{Cross-dataset bias detection:} these  assess bias by comparing different datasets, trying to discover some sort of ``signature'' due to the data collection process.\n\\item \\textbf{Other methods:} Different methods that could not fit any of the above categories.\n\\end{itemize}\n\\begin{table*}\n    \\centering\n    \\small\n    \\begin{adjustbox}{angle=90}\n    \\begin{tabular}{|l|l|l|l|c|c|c|l|}\n    \\cline{5-7}\n    \\multicolumn{4}{c|}{}&\\multirow{4}{*}{\\rotatebox{270}{\\textbf{Selection}}}&\\multirow{4}{*}{\\rotatebox{270}{\\textbf{Framing}}}&\\multirow{4}{*}{\\rotatebox{270}{\\textbf{Label}}}&\\multicolumn{1}{}{}\\\\\n    \\multicolumn{3}{}{}&&&&\\\\\n    \\multicolumn{3}{}{}&&&&\\\\\n    \\multicolumn{3}{}{}&&&&\\\\\n    \\cline{2-4} \\cline{8-8}\n    \\multicolumn{1}{c|}{}& \\textbf{No.} & \\textbf{Paper} & \\textbf{Year} &  &  &  & \\textbf{Type of measures/methods}  \\\\\n    \\hline\n    \\multirow{13}{*}{\\textbf{Reduction to tabular data}}& 1&  & 2019 & $\\bullet$ &  $\\bullet$ & & Count; Demographic parity\\\\\n    & 2&  & 2020 &$\\bullet$ & $\\bullet$ & $\\bullet$ & Count; Demographic parity\\\\\n    & 3&  & 2017 & $\\bullet$& $\\bullet$& & Demographic parity\\\\\n    & 4&  & 2017 & $\\bullet$& & & Count\\\\\n    & 5&  & 2018 &$\\bullet$ & & & Count\\\\\n    & 6&  & 2019 &$\\bullet$ & & & Entropy-based; Information theoretical\\\\\n    & 7&  & 2018 &$\\bullet$ & $\\bullet$ & & Entropy-based\\\\\n    & 8&  & 2019 & $\\bullet$& $\\bullet$ & & Information theoretical\\\\\n    & 9&  & 2019 & $\\bullet$ & $\\bullet$ & & Dataset leakage\\\\\n    & 10&  & 2021 & $\\bullet$ & & & Causality\\\\\n    & 11&  & 2019 & &$\\bullet$ & & 4 different measures\\\\\n    & 12&  & 2020 &$\\bullet$ &$\\bullet$ &$\\bullet$ & 13 different measures\\\\\n    \\hline\n    \\multirow{4}{*}{\\textbf{Biased image representation}}& 13&  & 2021 &$\\bullet$ & & & Distance-based\\\\\n    & 14&  & 2021 & &$\\bullet$ & & Distance-based\\\\\n    & 15&  & 2020 & &$\\bullet$ & & Interventions\\\\\n    \\hline\n    \\multirow{5}{*}{\\textbf{Cross-dataset bias detection}}& 16&  & 2011 & $\\bullet$ & $\\bullet$ & & Cross-dataset generalisation\\\\\n    & 17&  & 2015 & $\\bullet$ & $\\bullet$ & & Cross-dataset generalisation\\\\\n    & 18&  & 2012 & $\\bullet$ & $\\bullet$& & Modelling bias\\\\\n    & 19& & 2019 & $\\bullet$ & & & Nearest neighbour in a latent space\\\\\n    \\hline\n    \\multirow{4}{*}{\\textbf{Other}}& 20&  & 2015 & $\\bullet$ & $\\bullet$ & & Model-based\\\\\n    & 21&  & 2019 & & $\\bullet$ & & Model-based\\\\\n    & 22&  & 2020 & $\\bullet$ & $\\bullet$ & & Modelling bias\\\\\n    & 23&  & 2017 &$\\bullet$ & & & Causality\\\\\n    & 24&  & 2020 &$\\bullet$ & $\\bullet$& & Crowd-sourcing\\\\\n    \\hline\n    \\end{tabular}\n    \\end{adjustbox}\n    \\caption{Summary of the collected material.}\n    \\label{tab:summary}\n\\end{table*}", "cites": [4947, 8860, 4948, 4942, 4945, 4941, 4943, 6990, 7872, 4949, 4944, 4946], "cite_extract_rate": 0.46153846153846156, "origin_cites_number": 26, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive summary of the collected papers and organizes them into four categories. However, it lacks deeper synthesis of ideas across the works, critical evaluation of their methodologies, and abstraction to broader principles or trends. The narrative remains largely centered on listing and categorizing rather than offering insightful analysis."}}
{"id": "d5106c45-9874-462a-83a0-1bd6912ac6a4", "title": "Count/demographic parity", "level": "paragraph", "subsections": ["3a35934b-b133-4e57-bc56-2bd808c63f57", "1bb84031-790b-49ab-958f-0b8bd20a40ce"], "parent_id": "36c012fa-4436-4116-8277-ac64b563bff2", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Bias Discovery and Quantification in Visual Datasets"], ["subsection", "Reduction to Tabular Data"], ["paragraph", "Count/demographic parity"]], "content": " proposed a simple method for auditing ImageNet  with respect to gender and age biases. They applied a face detection algorithm to two different subsets of ImageNet, the training set of ILSVRC  and the \\textit{person} category of ImageNet . After that, they applied an age recognition model and a gender recognition model to them. They then computed the dataset distribution across the age and gender categories, finding out a prevalence of men (58.48\\%) and a minimal amount of people (1.71\\%) in the $>60$ age group. Computing the percentage of men and women across every category allowed them to identify the highest class imbalances in the two subsets of ImageNet. \nFor example, in the ILSVRC subset, the 89.33\\% of the images in category \\textit{bulletproof vest} were labelled as men and the 82.81\\% of the images in category \\textit{lipstick} were labelled as women. Therefore, this method does not only give information on the selection bias but also on the \\emph{framing} of the protected attributes, given a suitable labelling of the dataset. The authors noted that this method relies on the assumption that the gender and age recognition models involved are not biased. As the authors pointed out, such an assumption is violated by the gender recognition model, and therefore, analysis cannot be totally reliable. \n performed another analysis of the \\textit{person} category of ImageNet , trying to address both selection and label bias. They addressed label bias by asking annotators to find out, first, whether the labels could be offensive or sensitive (e.g., sexual/racial slur), and, second, to point out whether some of the labels were not referring to visual categories (e.g., it is difficult to understand whether an image depicts a \\textit{philanthropist}). They removed such categories and continued their analysis by asking annotators to further label images according to some categories of interest (gender, age, and skin colour) to understand whether the remaining data were balanced with respect to those categories and then to address selection bias. This demographic analysis showed that women and dark-skinned people were both under-represented in the remaining non-offensive/sensitive and visual categories. Moreover, despite the overall under-representation, some categories were found to align with stereotypes (e.g., the 66.4\\% of people in the category \\emph{rapper} were dark-skinned). Hence, they also potentially addressed some framing bias. The annotation process was validated by measuring the annotators' agreement on a small controlled set of images. \n measured the correlation between a protected attribute and the occurrences of various objects/actions.\nThey assumed to have a dataset labelled with a protected attribute $G$ with values $\\{g_1,...,g_n\\}$ and an attribute $O=o$ describing the occurrences of said objects or actions in the images (for instance, $G$ could be the gender and $O$ a cooking tool or an outdoor sport scene). The bias score of the dataset with respect to a protected attribute value $G = g$ and the scene/object $O=o$ is defined as the percentage of $g$ co-occurrences with $o$: \n\\begin{equation}\n    b(o,g) = \\frac{c(o,g)}{\\sum_{x\\in \\{g_1,...,g_n\\}} c(o,x)}\n\\end{equation}\nwhere $c(o,x)$ counts the co-occurrences of the object/scene $o$ and the protected attribute's value $x$. If $b(o,g_i)>\\frac{1}{n}$, where $n$ is the number of values that the protected attribute can assume, it means that attribute $G=g$ is positively correlated with the object/action $O=o$.\n used instead a simple count to assess geographic bias in ImageNet  and Open Images . They found out, for example, that the great majority of images of which the geographic provenance is known comes from the USA or Western European countries, resulting in highly imbalanced data. Such a geography-based analysis has been expanded by . While having balanced data is important for many applications, a mere count is usually not enough to assess every type of bias, as a balanced dataset could still contain spurious correlations, framing bias, label bias, etc. \n constructed a benchmark dataset for gender classification. For testing discrimination in gender classification models, their dataset is balanced according to the distribution of both gender and Fitzpatrick skin type as they noticed that the error rates of classification models tended to be higher at the intersection of those categories (e.g., black women) because of the use of imbalanced training data. Hence, while they quantify bias by simply counting the instances with certain protected attributes, the novelty of their work is that they took into account multiple protected attributes at a time.", "cites": [7872, 895], "cite_extract_rate": 0.2222222222222222, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple works effectively by connecting methods for bias discovery in ImageNet and other datasets, integrating ideas such as count-based analysis, framing bias, and geographic bias into a cohesive narrative. It includes critical evaluation, especially in noting the limitations of model-based assumptions and the insufficiency of simple counting for comprehensive bias assessment. While it identifies some general patterns, such as the importance of multiple protected attributes, it does not rise to the level of meta-level abstraction or a novel theoretical framework."}}
{"id": "3a35934b-b133-4e57-bc56-2bd808c63f57", "title": "Information theoretical", "level": "paragraph", "subsections": [], "parent_id": "d5106c45-9874-462a-83a0-1bd6912ac6a4", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Bias Discovery and Quantification in Visual Datasets"], ["subsection", "Reduction to Tabular Data"], ["paragraph", "Count/demographic parity"], ["paragraph", "Information theoretical"]], "content": " introduced four measures to construct a balanced dataset of faces. Two measures of diversity: Shannon entropy $H(X)=-\\sum_{i=1}^n\\mathbb{P}(X=x_i)\\cdot ln(\\mathbb{P}(X=x_i))$ and Simpson index $D(X)=\\frac{1}{\\sum_{i=1}^n\\mathbb{P}(X=x_i)^2}$ where $\\mathbb{P}(X=x_i)$ is the probability of an image having value $x_i$ for the attribute $X\\in\\{x_i\\}_{i=1}^n$; and two measures of evenness: ($E_{Shannon}=\\frac{H(X)}{ln(n)}$ and $E_{Simpson}=\\frac{D(X)}{n}$). Such measures have been applied to a set of facial attributes, ranging from craniofacial distances to gender and skin colour, computed both via automated systems and with the help of human annotators.\n also proposed to use (conditional) Shannon entropy for discovering \\emph{framing bias} in emotion recognition datasets. Using a pre-trained model, they computed the dataset's top occurring objects/scenes. They computed the conditional entropy of each object across the positive and negative set of the emotions to see whether some objects/scenes were more likely to be related to a certain emotion. For example, they found out that objects like balloons or candy stores are only present in the negative-set of \\emph{sadness} in Deep Emotion . Given an object $c$ and an emotion  $E=e\\in\\{0,1\\}$ (where $e=1$ represents, for instance, \\textit{sadness} and $e=0$ represents the negative-set \\textit{non-sadness}) they computed: \n\\begin{equation}\n    H(E|X = c) = -\\sum_{e\\in\\{0,1\\}}p_{ec}\\cdot ln(p_{ec}) \n\\end{equation} \nwhere $$p_{ec} = \\mathbb{P}(E=e|X=c)$$\nWhen the conditional entropy of an object is zero, it means that such an object is associated only with the emotion $E$ or, on the contrary, is never associated with it (it only appears in the negative set). This may be considered a type of framing bias. \n introduced another definition of bias inspired by information theory. They wanted to develop a method for training classification models that do not overfit due to dataset biases. In doing so, they give a precise definition of bias: a dataset contains bias when $I(X, Y) \\gg 0$ where $X$ is the protected attribute, $Y$ is the output variable and $I(X, Y):= H(X)-H(X|Y)$ is the mutual information between those random variables. Kim et al. proposed to minimise such mutual information during training so that the model forgets the biases and generalises well. Note that if $Y$ is any feature in the dataset, this measure can be used to quantify bias in the dataset instead of the output of a model.", "cites": [4941, 1017, 4943, 4944], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information theoretical approaches from multiple papers to explain how diversity and evenness measures, as well as mutual information, are used for bias discovery and mitigation. It abstracts these methods into broader principles of entropy and information theory for quantifying bias. While it provides some analytical perspective, it could have offered a deeper comparative critique of the methods' effectiveness or limitations."}}
{"id": "1bb84031-790b-49ab-958f-0b8bd20a40ce", "title": "Other", "level": "paragraph", "subsections": [], "parent_id": "d5106c45-9874-462a-83a0-1bd6912ac6a4", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Bias Discovery and Quantification in Visual Datasets"], ["subsection", "Reduction to Tabular Data"], ["paragraph", "Count/demographic parity"], ["paragraph", "Other"]], "content": " defined \\textit{dataset leakage} to measure the possibility of a protected attribute to be retrieved using only the information about non-protected ones. Given a dataset $\\mathcal{D}=\\{(x_i,y_i,g_i)\\}$ where $x_i$ is an image, $y_i$ a non-protected attribute and $g_i$ is a protected attribute, the attribute $y_i$ is said to leak information about $g_i$ if there exists a function $f(\\cdot)$, called \\textit{attacker}, such that $f(y_i)\\approx g_i$. Operationally, the attacker $f(\\cdot)$ is a classifier trained on $\\{y_i, g_i\\}$. The dataset leakage is measured as follows: \n\\begin{equation}\n    \\lambda_\\mathcal{D}=\\frac{1}{|\\mathcal{D}|}\\sum_{i=1}^{|\\mathcal{D}|}\\mathbf{1}[f(y_i)=g_i]\n\\end{equation}\n explicitly used causality for studying spurious correlations in neuroimaging datasets. Given variables $X$ and $Y$ they wanted to test whether $X$ causes $Y$ or there exists a confounder variable $Z$ instead. Since those two hypotheses imply two different factorisations of the distribution $\\mathbb{P}(X|Y)$, the factorisation with a lower Kolmogorov complexity is the one that identifies the true causal structure. Kolmogorov complexity is approximated by Minimum Description Length (MDL). \n proposed 8 different measures for identifying gender framing bias in movies. The following measures are computed for a movie for each gender, therefore the means are computed across every frame in which an actor of a certain gender appears. The measures are the following: Emotional Diversity $H(X) = -\\sum_{i=1}^s\\mathbb{P}(X=x_i)\\cdot ln(\\mathbb{P}(X=x_i))$ where $\\mathbb{P}(X=x_i)$ is the probability that a character expresses a certain emotion $x_i$ and the sum is computed on the range of the different emotions shown by characters of a certain gender (the list of emotions was: anger, disgust, contempt, fear, happiness, neutral, sadness, and surprise); Spatial Staticity exploits ideas from time-series analysis to measure how static a character is during the movie, it is defined as $mean(PSD(p(t))$ where PSD is the power spectral density of the time-series of the position on the x-axis (resp. y-axis) of the character (the higher the value the less animated is the character); Spatial occupancy $mean(\\sqrt{A})$ where $A$ is the area of the face of the character; Temporal occupancy $\\frac{N}{N_{total}}$ where $N$ is the number of frames in which the character appears and $N_{total}$ is the total number of frames; Mean age computed over each frame and character; Intellectual Image $mean(G)$ where $G$ is the presence of glasses (this seems a debatable choice as it might itself suffer from some label bias); Emphasis on appearance $mean(E)$ where $E$ is the light exposure of faces again calculated for each frame; finally, the type and frequency of surrounding objects is analysed. The attributes involved in the computation of these measures are mainly extracted using Microsoft Face API, which  demonstrated to be biased against black women. \n proposed REVISE, a comprehensive tool for bias discovery in image datasets. The authors defined three sets of metrics. The first set contains metrics based solely on the use of bounding boxes of objects (Note that a \\textit{person} is considered an \\textit{object} as it can be classified by an objected detection model); the second set contains metrics that exploit also protected attribute labels; the third set uses additional unstructured information such as text when the protected attribute is not provided explicitly. REVISE implements 13 different metrics, some of which are very similar to what we described in the previous paragraphs. We are going to describe two of the most relevant: i) scene diversity $H(S)$ where $S$ is the scene attribute computed applying a pre-trained scene recognition algorithm ; and ii) appearance differences, computed by extracting features using a feature extractor of images with a same object/scene but a different protected attribute value and then training an SVM classifier on the extracted features to see whether it could learn different representations of subjects with different protected attribute values in the same context.", "cites": [7872, 4949], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes concepts from two distinct papers, integrating them under the theme of bias discovery and quantification in visual datasets. It provides a general explanation of how each method operates and relates to the broader topic. However, it lacks deeper critical analysis or comparative evaluation, and while some patterns (e.g., use of classifiers, metrics based on attributes) are noted, the abstraction remains limited to surface-level generalization."}}
{"id": "e708eb45-6e6c-4769-937f-93efc69f6d93", "title": "Distance-based", "level": "paragraph", "subsections": ["c281b821-281a-45e8-b9b6-c9710e929885"], "parent_id": "920fda46-f6b5-4b16-a421-faaf243af6cd", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Bias Discovery and Quantification in Visual Datasets"], ["subsection", "Biased Image Representations"], ["paragraph", "Distance-based"]], "content": " proposed a simple measure of diversity for testing their face dataset. They studied the distribution of pairwise L1-distances calculated after embedding the images in a lower-dimensional space using a neural network pre-trained on a different benchmark face dataset. If such distribution is skewed towards high pairwise distances, it means that the data show high diversity. Nonetheless, such an analysis is heavily influenced by the embedding. For instance, faces in a ``white-oriented'' dataset were also well separated in the embedding space, probably because the neural network used for the embedding had also been trained using a similarly biased dataset. \n developed a method for addressing human-like biases in the latent image representation of unsupervised generative models inspired by bias detection methods in Natural Language Processing . They discovered that the biases found in the latent space of two big models trained on ImageNet  match with human-like biases. The authors measured bias by looking at associations between semantic concepts (for example, man-career and woman-family) by measuring the cosine similarity among vectors in the latent space computed by applying the models to controlled samples of images that resemble those visual concepts. More precisely, given a model $f$ that maps images into a vector space $\\mathbb{R}^d$, two sets of images $J$ and $K$ (e.g., photos of men and women, respectively), and two sets $A$ and $B$ of images representing the concepts we want to measure the association with (e.g., photos of people at work and photos of people in familiar settings), we can measure the association of $J$ with $A$ and $K$ with $B$ in the following way:\n\\begin{equation}\n    s(J,K,A,B) = \\sum_{j\\in J}s(j,A,B) - \\sum_{k\\in K}s(k,A,B)\n\\end{equation}\nwhere\n\\begin{equation}\n\\begin{aligned}\ns(w,A,B) = & mean_{a\\in A}cos(f(w),f(a)) -\\\\ \n& - mean_{b\\in B}cos(f(w),f(b))\n\\end{aligned}\n\\end{equation} \nAlternatively, we can measure the size of the association via the Cohen's $d$:\n\\begin{equation}\n    d = \\frac{mean_{j\\in J}s(j,A,B) - mean_{k\\in K}s(k,A,B)}{std_{w\\in J\\cup K}s(w,A,B)}\n\\end{equation}\nThe authors found out that the representations they analysed contained several biased associations that resemble human cognitive biases (e.g., the association between flower/insects with pleasant/unpleasant respectively, male/female with career/family or white/black with tools/weapons). Nevertheless, it is not clear how many of those associations were present in the original training data and to what degree this is the responsibility of the CV models used for computing the representations. While this method for measuring biased associations is model agnostic, in the sense that it can be applied to any possible model, its results heavily depend on the learned representations and the employed models.", "cites": [895, 3110, 4945], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes concepts from multiple papers, integrating ideas about bias in image representations with methodologies from NLP. It critically evaluates the limitations of embedding-based approaches and raises important questions about the source of biases in representations. The discussion abstracts beyond individual studies to highlight general dependencies of bias on model and data choices."}}
{"id": "c281b821-281a-45e8-b9b6-c9710e929885", "title": "Other", "level": "paragraph", "subsections": [], "parent_id": "e708eb45-6e6c-4769-937f-93efc69f6d93", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Bias Discovery and Quantification in Visual Datasets"], ["subsection", "Biased Image Representations"], ["paragraph", "Distance-based"], ["paragraph", "Other"]], "content": " developed a method for assessing \\textit{algorithmic} biases in image classifiers via computing the causal relationship between the protected attribute and the output of a classifier. In doing so, they developed a method for intervening on the image attributes: they assumed to have a generator, such as a Generative Adversarial Network , that generates images from latent vectors. They also assumed to have learned hyper-planes in the latent space that separate different attributes. Sampling points along the direction orthogonal to an attribute's hyper-plane gives a modified version of the original image with respect to that attribute. The authors noted that such interventions are not entirely disentangled: for example, adding long hair to the images of white males also adds the beard, and changing the gender attributes to the images of black males adds earrings. This is probably due to datasets bias which is then detected as a side effect of the transformation described above. Note that, since such transformation is the result of geometric operations, it means that the bias is encoded in the geometry of the latent space. It would be interesting to study to what extent these manipulations of the latent space can be used as a bias exploration tool.", "cites": [4942], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of a method for detecting algorithmic bias through causal interventions in latent space, as described in the cited paper. It integrates the paper's core idea and highlights a limitation (incomplete disentanglement of attributes), indicating some level of critical evaluation. However, it does not extensively compare with other methods or synthesize a broader framework, limiting its abstraction and synthesis potential."}}
{"id": "872e15f1-aa9b-4253-a4da-2b737de6f3da", "title": "Cross-dataset Bias Detection", "level": "subsection", "subsections": ["2b619bc7-55a3-4b52-9765-d2c68e0823e9"], "parent_id": "f2b72fe9-0ce9-4dcb-bde4-82dfdae69bcf", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Bias Discovery and Quantification in Visual Datasets"], ["subsection", "Cross-dataset Bias Detection"]], "content": "\\label{subsec:cross-dataset}\nMethods in this category derive from the realisation that the issue of generalisation in CV might be due to bias. Researchers in the field of object detection are usually able to tell with fair accuracy which famous benchmark dataset an image comes from . This means that each dataset carries some sort of ``signature\" (bias) that makes the provenance of its images easily distinguishable and affects the ability of the models to generalise well. The methods described in the following aim to detect such signatures by comparing different datasets. Note that the works of  could also fit Section \\ref{subsec:LatentRep}.", "cites": [4946], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a minimal synthesis of the cited paper, only briefly referencing its relevance to dataset bias. It lacks critical evaluation of the methods or limitations of the work and does not generalize to broader patterns or principles in cross-dataset bias detection. The narrative remains descriptive and does not offer deeper analytical or comparative insights."}}
{"id": "2b619bc7-55a3-4b52-9765-d2c68e0823e9", "title": "Cross-dataset generalisation", "level": "paragraph", "subsections": ["a03fcedf-629e-4d9c-80da-200a977afa80"], "parent_id": "872e15f1-aa9b-4253-a4da-2b737de6f3da", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Bias Discovery and Quantification in Visual Datasets"], ["subsection", "Cross-dataset Bias Detection"], ["paragraph", "Cross-dataset generalisation"]], "content": " tested bias in object detection datasets by answering the following question: ``How well does a typical object detector trained on one dataset\ngeneralize when tested on a representative set of other datasets, compared with its performance on the native test set?\" The assumption here is that if the performance on the native test set is much higher it means the datasets exhibit some \\emph{bias} that is learned by the object detector $f_{\\mathcal{D}}$. Hence, let us consider a dataset of interest $\\mathcal{D}$ and other $n$ benchmarks $\\{\\mathcal{B}_i\\}$. They compare performance by computing:\n\\begin{equation}\n\\frac{1}{n}\\sum_{i=1}^n\n\\frac{AP_{\\mathcal{B}_i}(f_\\mathcal{D})}{AP_{\\mathcal{D}}(f_\\mathcal{D})}\n\\end{equation}\nThe closer to 1 this score is, the more $f_\\mathcal{D}$ generalises well. Note that if this score is low, we can say that the $f_\\mathcal{D}$ does not generalise well and thus the dataset $\\mathcal{D}$ is probably biased, while if the score is close to 1 we can say that the datasets share a similar representation of the visual world. Furthermore, the authors also proposed a test, which they presented as a toy experiment but which has been utilised in many other studies, called \\textit{Name the dataset}. They trained a model to recognise the source dataset of a particular image: the greater the accuracy of this model, the more distinguishable and the more biased the datasets are. The methods described above have also been used by . \n investigated the possibility of using CNN feature descriptors to address dataset bias. In particular, they replicated the experiments by  and  (see next paragraph) using DeCAF features . Furthermore, they slightly changed the measure used by Torralba and Efros for the evaluation of cross-generalisation: \n\\begin{equation}\n    CD = \\sigma(\\frac{1}{100}(AP_\\mathcal{D}(f_\\mathcal{D}) - \\frac{1}{n}\\sum_{i=1}^{n} AP_{\\mathcal{B}_i}(f_\\mathcal{D})))\n\\end{equation}\nwhere $\\sigma()$ is the sigmoid function.", "cites": [4946, 4943, 2292], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers by connecting the concept of cross-dataset generalization with methods for detecting and quantifying bias. It includes a critical evaluation by identifying the implications of low generalization scores and the role of distinguishability in bias detection. However, it could have offered deeper abstraction by contextualizing these methods within a broader theoretical framework of bias in visual datasets."}}
{"id": "cabe6fbe-7d79-4598-81d2-47361c707085", "title": "Model-based", "level": "paragraph", "subsections": ["3f3263d7-defc-40c0-ac2d-3469240c9cb6"], "parent_id": "bf512b8a-ed82-4af6-b451-ad1cebee16b6", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Bias Discovery and Quantification in Visual Datasets"], ["subsection", "Other Methods"], ["paragraph", "Model-based"]], "content": " proposed a simple method for addressing bias in object recognition datasets. They cropped a small central sub-image from each image in the original dataset. These cropped pictures were so small that humans could not recognise the objects in the pictures. Hence, if the attained performance of a model trained on such images was better than pure chance, the data contained distinguishable features spuriously correlated with the object categories. Then the data showed some kind of \\emph{selection} or \\emph{framing} (\\emph{capture}) bias.\n studied the political framing bias of images by scraping images from online news outlets. Their idea was to train a semi-supervised tool for classifying images according to their political orientation. They labelled images according to the political orientation of the source. They also used information regarding the articles hosting the images by feeding the network with both the image and the document embedding of the article. Note that the proposed architecture incorporates textual information at training time but allows them to classify images without any additional information at testing time. Thus, this model can be used to understand if a specific image or a collection of images is biased towards a particular political faction. Moreover, the visual explanation of such a model could give semantic information on the political framing of the dataset. \n proposed to use an ensemble classification algorithm for mitigating bias. The idea is to train a low-capacity network (i.e., with a low number of parameters) together with a high-capacity one (i.e., with more than double the parameters) so that the former learns spurious correlations while the latter learns to classify the data in an unbiased way. While this difference in capacity and the ensemble training encourages the two models to learn different patterns, there is still the possibility for the high-capacity model to learn simpler patterns and hence bias. To avoid this, the two networks are trained to result in conditionally independent outputs. This will be an incentive for the ensemble to isolate simple and complex patterns. While the authors use this method with the only purpose of mitigating algorithmic bias, studying the lower-capacity model can give information on spurious correlations in the training dataset, highlighting selection/framing bias.\n proposed a method to discover ``causal signals\" among objects appearing in image datasets. In particular, given a set of images $\\mathcal{D} = \\{d_i\\}$, they score the presence of certain objects/attributes $A$, $B$ by using an object detector or a feature extractor, respectively. Then they apply a Neural Causation Coefficient (NCC, ) network, a neural network that takes a bag of samples $\\{(a_i, b_i)_{i=1}^m\\}$ as input and returns a score $s\\in [0,1]$ where $s=0$ means that $A$ causes $B$ and $s=1$ means instead that $B$ causes $A$. Note that, while not every causal relationship is a source of bias, some might be. Moreover, such causal relationships can be thought of as a by-product of the selection of the subject. Hence this method can detect selection bias.", "cites": [4947, 8861, 8860, 4948], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple model-based methods for bias discovery, connecting them under the broader theme of detecting spurious correlations and selection/framing biases. It provides some critical analysis by pointing out limitations (e.g., the high-capacity model might still learn simpler patterns and bias). The section also abstracts from specific techniques to highlight how they can be used for broader bias identification, such as political framing or causal relationships."}}
{"id": "f4aed088-6c82-4b4e-9738-0dcc24ec69b6", "title": "Bias-aware Visual Data Collection", "level": "section", "subsections": ["5bd8f232-72c9-4209-8f79-0f616a02c507", "7e2dc6e0-6618-4336-87ff-eac4143a060b"], "parent_id": "bcddf019-4576-40c9-aa47-8a01af6db525", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Bias-aware Visual Data Collection"]], "content": "\\label{sec:BiasFree}\n\\begin{table*}\n    \\centering\n    \\small\n    \\begin{adjustbox}{angle=90}\n    \\begin{tabular}{|p{35mm}|l|l|p{28mm}|l|p{35mm}|c|c|c|}\n        \\cline{7-9}\n        \\multicolumn{6}{c|}{}&\\multirow{4}{*}{\\rotatebox{270}{\\textbf{Selection}}}&\\multirow{4}{*}{\\rotatebox{270}{\\textbf{Framing}}}&\\multirow{4}{*}{\\rotatebox{270}{\\textbf{Label}}}\\\\\n        \\multicolumn{6}{c|}{}& & &\\\\\n        \\multicolumn{6}{c|}{}& & &\\\\\n        \\multicolumn{6}{c|}{}& & &\\\\\n        \\cline{1-6} \n        \\textbf{Paper} & \\textbf{Year} & \\textbf{Size} & \\textbf{Protected attributes} & \\multicolumn1{p{15mm}|}{\\textbf{Pre-existing Content}} &  \\multicolumn1{p{30mm}|}{\\textbf{Labelling Process}}& & &\\\\\n        \\hline\n         & 2018 & 1.2K images & Binary gender; skin tone & Yes & Human annotators; experts; additional information & $\\bullet$ & & $\\bullet$\\\\\n         & 2021 & 108.5K images &  Age; binary gender; race & Yes & Crowd workers & $\\bullet$ & &\\\\\n         & 2019 & 970K images & Age; Binary gender; skin tone & Yes &  Machine annotators; crowd workers& $\\bullet$ & $\\bullet$ &\\\\\n         & 2020 &\\multicolumn1{p{18mm}|}{41K images; 44K videos} & Age; binary gender & Yes & Human annotators; machine annotators & $\\bullet$ & $\\bullet$ &\\\\\n          & 2019 & 50K images & - & No & - & &$\\bullet$ &\\\\\n         (Inclusive Benchmark)& 2020 & 12K images & Binary gender; race & Yes & Additional information & $\\bullet$ & &\\\\\n         (Non-binary Gender Benchmark) & 2020 & 2K images & Non-binary gender; race & Yes & Additional information & $\\bullet$ & &$\\bullet$\\\\\n          & 2021 & 45.1K videos & Age; Non-binary gender; skin tone & No & Human annotators; self-provided labels & $\\bullet$ & $\\bullet$ & $\\bullet$\\\\\n         \\hline\n    \\end{tabular}\n    \\end{adjustbox}\n    \\caption{Summary of the datasets described in Section \\ref{sec:BiasFree}.}\n    \\label{tab:datasets}\n\\end{table*}\nIn the following, we are going to describe some attempts to construct datasets where the existence of bias was taken into account during the dataset creation process. These datasets were constructed for specific purposes and were probably not thought of as universally bias-free datasets. Nonetheless, analysing which biases have been removed and which have been not might be helpful to understand the general challenge of bias in visual datasets. We summarise the content of this section in Table \\ref{tab:datasets}. Furthermore, we propose a checklist for helping the collection of bias-aware visual datasets (Table \\ref{tab:checklist}). Note that, while the previous section systematically reviewed the literature on bias discovery and quantification in visual data, this section is meant as a case study. Therefore, it is more limited in its scope.", "cites": [4950, 4941, 8862], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section presents a summary of bias-aware visual datasets in a tabular format but lacks meaningful synthesis of the ideas across papers. It describes the datasets' attributes and sources without connecting them to broader themes or evaluating their effectiveness. The absence of critical discussion or abstraction limits its insight quality to a basic descriptive level."}}
{"id": "5bd8f232-72c9-4209-8f79-0f616a02c507", "title": "Datasets", "level": "subsection", "subsections": [], "parent_id": "f4aed088-6c82-4b4e-9738-0dcc24ec69b6", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Bias-aware Visual Data Collection"], ["subsection", "Datasets"]], "content": "\\label{subsec:bias-aware_datasets}\n released the Pilot Parliaments Benchmark (PPB) dataset. PPB is a \\emph{face dataset} constructed by collecting the photos of members of six different national parliaments. The authors aimed to collect balanced data regarding the gender of the subjects and their skin colour. To do so, they selected three nations from African countries (Rwanda, Senegal, and South Africa) and three from European countries (Iceland, Finland, and Sweden) according to the gender parity rank\\footnote{\\url{https://data.ipu.org/women-ranking?month=6&year=2021}. Last visited 23.03.2022.} among their Members of Parliament (MP). The data have been labelled by three annotators (including the authors) according to (binary) gender appearance and the Fitzpatrick skin type (ranging from I to IV, these labels are used by dermatologist as a gold standard for skin classification). The definitive skin labels were provided by a board-certified dermatologist, while the definitive gender labels were also determined based on the title, prefix, or the name of the parliamentarians (note that using names as a proxy of gender can cause label bias ). While the data collection process described above resulted in a much more balanced dataset compared to other famous benchmarks (Adience ; IJB-A ), it is still not free from possible biases. For example, the selection process targets a small number of African and northern European countries to ensure gender and skin tone balance. Nevertheless, it completely excludes, for instance, Asian and South American countries. Moreover, MPs are likely to be middle-aged people, which could also exclude young and older people from the selection. On the framing bias side, different countries might have different standards/dress codes for their MPs' official portraits, which could turn into some biases as well. \n collected a \\emph{face dataset} emphasising, in particular, the balance in terms of age, gender, and race. They relied on a crowdsourcing workflow for annotating the images. In particular, they asked three different workers to label the images according to gender, age group, and race. They kept the label if there was a 2/3 accordance. Otherwise, they would have proposed the image to other three workers and discarded it if again it resulted in three different judgements. We can spot two sources of label and selection bias, respectively: first, we cannot be sure that the workers are able to determine the three labels homogeneously across every sub-group \\footnote{see the interesting Twitter thread from Sacha Costanza-Chock (@schock), \\url{https://twitter.com/schock/status/1346478831255789569} (Last visited 23.03.2022) for an on-point discussion about this issue}; second, discarding the photos the workers cannot agree on might result in the missed selection of a certain group of people whose characteristics are difficult to determine for the workers. Finally, the taxonomy of races used by the authors (White, Black, Indian, East Asian, Southeast Asian, Middle East, and Latino) already introduces a form of label bias. While it is derived from the taxonomy commonly used by the US Census Bureau and might be descriptive of the composition of the US population, it hardly captures the complexity of human diversity.\nDiversity in Face (DiF)  and KANFaces  are two \\emph{face datasets} that try to address the issue of bias by ensuring as much diversity as possible using the diversity measures proposed by  that we reviewed in Section \\ref{sec:Discovery}. The attributes they control the diversity for are: age, gender, skin tone, a set of craniofacial ratios, and pose. The authors also took into account a metric of illumination. By collecting diverse data, the authors try to avoid selection bias (in the case of age, gender, and skin tone) and some framing bias (in the case of pose and illumination).\n made an attempt to avoid framing bias in large-scale \\emph{object detection datasets}. In particular, they added controls for object rotations, viewpoints and background by asking crowd workers to photograph objects in their homes in a natural setting according to the instructions given by the authors. While this resulted in a much more diverse dataset (the authors used ImageNet  as a comparison), because of the above controls, the objects appear only in indoor context, are rarely occluded, and are often centre-aligned. Thus, it seems that specific framing biases have been avoided, while the collection procedure has introduced others. Also, the authors removed some classes from the dataset for reasons that range from privacy concerns (e.g., ``people'') or because they were not easy to move and photograph in different settings (e.g., ``beds''). In principle, this might generate some selection bias (more specifically, negative class bias) since the absence of those objects could make the negative classes less representative.    \n collected two benchmark datasets: the Inclusive Benchmark Database (IBD), and Non-binary Gender Benchmark Database (NGBD)\\footnote{While collecting inclusive data is a noble purpose, the use the authors made of their dataset (extending gender classification algorithms to non-binary genders) should be strongly discouraged as it might be used to target already marginalised groups of people.}. IBD contains 12,000 images of 168 different subjects, 21 of whom identify as LGBTQ. The geographic provenance of the subjects in the dataset is balanced. NGBD contains 2,000 images of 67 unique subjects. The subjects are public figures whose gender identity is known. Thus, the database contains multiple gender identities (namely: \\textit{non-binary}, \\textit{genderfluid}, \\textit{genderqueer}, \\textit{gender non-conforming}, \\textit{agender}, \\textit{gender neutral}, \\textit{gender-less}, \\textit{third gender}, and \\textit{queer}). The authors themselves identify two major risks of label bias: first, ``Gender identity has its multifaceted aspects that a simple label could not categorize\"  (the authors identify the problem of modelling gender as a continuum as a direction for future work); and, second, ``Gender is a complex socio-cultural construct and an internal identity that is not necessarily tied to physical appearances.\" . \n proposed the Casual Conversations Dataset for evaluating the performance of CV models across different demographic categories. Their dataset is composed of 3,011 subjects and contains over 45,000 videos, with an average of 15 videos per person. The videos were recorded in multiple US states with a diverse set of adults in various age, gender and apparent skin tone groups. This work represents probably the greatest effort to build a balanced dataset addressing both selection and framing bias (in the form of the illumination of videos). Nevertheless, some forms of imbalance remain. For example, most videos present bright lighting conditions; and most subjects are labelled as either male or female (with just the 0.1\\% of the participants that identify as ``Others\" and 2.1\\% whose gender is unknown). The label bias that the use of categories such as gender, age, and race could create is overcome by asking the participants their age and gender and using the Fitzpatrick Skin Type instead of race. Nonetheless, the authors declare that there are ``videos in which two subjects are present simultaneously\" but that they provide only one set of labels which might also be a form of label bias. Last, we note that the subjects are all from the US, which represents a serious selection bias as the US population hardly represents the whole of humankind.", "cites": [4950, 4941, 8862, 895, 4951], "cite_extract_rate": 0.5454545454545454, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 4.2, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes multiple datasets and their approaches to bias-aware data collection, connecting ideas like label bias, selection bias, and framing bias across the cited works. It critically evaluates the limitations and trade-offs of each dataset, such as exclusion of certain regions or reliance on problematic categorization. The analysis generalizes to highlight broader issues in diversity and fairness in visual datasets, though the abstraction remains somewhat tied to specific examples rather than fully conceptual frameworks."}}
{"id": "7e2dc6e0-6618-4336-87ff-eac4143a060b", "title": "Discussion", "level": "subsection", "subsections": [], "parent_id": "f4aed088-6c82-4b4e-9738-0dcc24ec69b6", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Bias-aware Visual Data Collection"], ["subsection", "Discussion"]], "content": "\\label{subsec:discussion_checklist}\nAs highlighted by the cases analysed in the previous section, dealing with bias in visual data is not a trivial task. In particular, collecting bias-aware visual datasets might be incredibly challenging. Thus, we propose a checklist (Table \\ref{tab:checklist}) to help scientists and practitioners spot and make explicit possible biases in the data they collect or use. Furthermore, we add a brief discussion on pre-processing mitigation strategies. Note that bias mitigation is an area on its own and would require an entire review to be adequately discussed. Therefore, we will just provide the interested reader with some hints and references.\nOur checklist is inspired by previous work on documentation and reflexive data practices , but adds several questions specific to selection, framing, and label bias because they have their own peculiarities and must be analysed separately. We start with a general set of questions on the dataset purpose and the collection procedure. Then, we continue with questions specific to selection, framing, and label bias. In particular, we ask whether the selection of subjects generates any imbalance or lack of diversity, whether there are spurious correlations or harmful framing, whether fuzzy categories are used for labelling, and whether the labelling process contributes to inserting biases. \n\\begin{table*}[h!]\n    \\centering\n    \\small\n    \\begin{adjustbox}{angle=0}\n    \\begin{tabular}{|l|p{100mm}|}\n    \\cline{2-2}\n    \\multicolumn{1}{c|}{}& \\textbf{Description}\\\\\n    \\hline\n    \\multirow{3}{*}{\\textbf{General}}& What are the purposes the data is collected for?  \\\\\n    & Are there uses of the data that should be discouraged because of possible biases? \\\\\n    & What kind of bias can be inserted by the way the collection process is designed? \\\\\n    \\hline\n    \\multirow{7}{*}{\\textbf{Selection bias}}& Do we need balanced data or statistically representative data?\\\\\n    & Are the negative sets representative enough?\\\\\n    & Is the dataset representative enough?\\\\\n    & Is there any group of subjects that is systematically excluded from the data? \\\\\n    & Do the data come from or depict a specific geographical area? \\\\\n    & Does the selection of the subjects create any spurious associations?\\\\\n    & Will the data remain representative for a long time?\\\\\n    \\hline\n    \\multirow{6}{*}{\\textbf{Framing bias}}& Are there any spurious correlation that can contribute to framing different subjects in different ways?\\\\\n    & Is there any biases due to the way images/videos are captured?\\\\\n    & Did the capture induce some behaviour in the subjects (e.g. smiling when photographed)?\\\\\n    & Are there any images that can possibly convey different messages depending on the viewer?\\\\\n    & Are subjects of a certain group depicted in a particular context more often than others?\\\\\n    & Do the data agree with harmful stereotypes?\\\\\n    \\hline\n    \\multirow{5}{*}{\\textbf{Label bias}}& If the labelling process relies on machines: have their biases been taken into account?\\\\\n    & If the labelling process relies on human annotators: is there an adequate and diverse pool of annotators? Have their possible biases been taken into account?\\\\\n    & If the labelling process relies on crowd sourcing: are there any biases due to the workers' access to crowd sourcing platforms? \\\\\n    & Do we use fuzzy labels? (e.g., race or gender)\\\\\n    & Do we operationalise any unobservable theoretical constructs/use proxy variables? \\\\\n    \\hline\n    \\end{tabular}\n    \\end{adjustbox}\n    \\caption{Checklist for bias-aware visual data collection}\n    \\label{tab:checklist}\n\\end{table*}\nWe want to stress that careful data collection and curation are probably the most effective mitigation strategies (see, for example, the mitigation insights described in ) for all the three types of bias presented in \\ref{sec:CVBias}. However, selection bias seems to be the easiest to mitigate using standard pre-processing techniques such as re-sampling or re-weighting. Note that several image datasets have a long-tail distribution of objects, and hence pre-processing mitigation techniques have to take that into account (see ), especially when many objects can co-occur in the same image. When the label bias is generated by poor operationalisation of unobservable theoretical construct , the entire labelling must be reconsidered. Otherwise, when bias is caused by the presence of synonyms or incorrect labelling, an effective data cleaning could help the mitigation. When it appears in the form of capture bias, framing bias can be mitigated by either re-sampling techniques that ensure diversity (in pose and lighting, for example) or via data augmentation. When the framing bias relates to the messages carried by the images, automatic pre-processing strategies are probably less effective as the way an image is interpreted and the message it conveys are not universal, which gives rise to the need for careful human inspection and analysis.", "cites": [7872, 4932, 3922], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes insights from the cited papers by integrating concepts such as reflexive data practices, operationalisation of unobservable constructs, and standardized dataset documentation to create a unified bias-aware data collection checklist. It demonstrates critical analysis by acknowledging the limitations of pre-processing techniques and the need for human intervention in framing bias. The section abstracts beyond individual studies to propose a generalizable framework for identifying and mitigating multiple types of bias in visual datasets."}}
{"id": "cd22f193-1cde-496f-93b1-2ba50635e45c", "title": "Conclusions and Research Outlook", "level": "section", "subsections": ["f44bf476-1b81-49b0-8887-b32b499a842d"], "parent_id": "bcddf019-4576-40c9-aa47-8a01af6db525", "prefix_titles": [["title", "A survey on bias in visual datasets"], ["section", "Conclusions and Research Outlook"]], "content": "\\label{sec:Conclusions}\nThe aim of this survey was threefold: first, to provide a description of different types of bias and to illustrate the processes through which they affect CV applications and visual datasets; second, to perform a systematic review of methods for bias discovery in visual datasets; and, third, to describe existing attempts to collect visual datasets in a bias-aware manner. We showed how the problem of bias is pervasive in CV; It accompanies the whole life cycle of visual content, involves several actors, and re-enters the life cycle through biased CV algorithms. One of our major contributions has been to provide a detailed description of the different manifestations (selection, framing, and label) of bias in visual data, along with several examples. We also went further by providing a sub-categorisation (Table \\ref{tab:sub_biases}) which also includes several categories of bias that are commonly described in Statistics, Health studies, or Psychology adapting them to the context of visual data. \nThe systematic review in Section \\ref{sec:Discovery} allowed us to draw some consideration on the state of the art in bias discovery methods for visual data and to outline some possible future streams of research. The vast majority of them used the strategy of reducing the problem to tabular data (Section \\ref{subsec:Reduction}). While it seems a natural option as it allows leveraging the wealth of techniques from the Fair Accountable and Transparent Machine Learning (FATML) literature, using different representations of visual data would certainly open new unexplored research directions. For example, representing an image as a scene graph , would allow the use of bias detection techniques used for rankings and graphical data . Moreover, since scene graphs can be thought of as small-scale knowledge graphs, this representation would allow both the integration of additional knowledge (e.g., from Wikipedia) and the use of methods for bias detection in knowledge graph embeddings .\n showed that a latent representation of data encapsulates bias in its geometry. This was also noted by  in their work on bias in word embeddings. Nonetheless, the study of this relationship between the geometry of latent spaces and bias is usually limited to variations of cosine similarity. Hence, a promising research direction would be to exploit more sophisticated tools for studying such relationships, e.g., Topological Data Analysis . \nSome of the most influential works on bias detection in images, e.g., , focused on object detection datasets. In such datasets, the long-tail distribution of objects is very common and evaluating bias in these datasets is difficult because the long tail distracts from the focal points. Moreover, the co-occurrence of different objects has obvious implications for the effectiveness of mitigation methods  for imbalanced data (e.g., oversampling, under-sampling), and therefore it should be studied more deeply. \nMost of the reviewed papers focus on images. Only few of them study bias in videos. This opens the road to many possibilities (for example, applying methods from time-series and multimodal analysis to the bias detection domain). Furthermore, we noted that selection and framing bias are the two types of bias that are more commonly taken into account. Nevertheless, label bias is often very concerning (Section \\ref{sec:labelbias}) and hence more research is needed to fill this gap. \nSimilarly, for obvious simplicity reasons, most related works concentrate on a single protected attribute (an exception to this is the work of ). Hence, multi-attribute fairness studies would enrich the literature in this respect. We also noted that most works we reviewed in Section \\ref{sec:BiasFree} focus on facial data. This is probably due to the concerning applications of face detection/recognition algorithms. Therefore, there is room for improving datasets and establishing data collection practices in other fields, including medical imaging, self-driving cars, etc. \nWe finally reviewed several attempts to collect bias-aware data in Section \\ref{sec:BiasFree} and concluded that there is no such thing as bias-free data. Hence, it is of utmost importance, along with the development of reliable bias discovery tools, that researchers and practitioners become aware of the biases of the datasets they collect and make them explicit in a standardised way (see, for example, ). In Table \\ref{tab:checklist}, we outline a checklist for the collection of visual data. We believe that having such a guide will help practitioners and scientists spot possible causes of bias, collect data that is as less biased as possible, and be aware of such biases during their analysis.", "cites": [3922, 3112, 4942], "cite_extract_rate": 0.25, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.2}, "insight_level": "high", "analysis": "The section effectively synthesizes ideas from multiple papers to present a coherent narrative on the limitations and future directions of bias discovery in visual datasets. It critically evaluates current methods, identifies research gaps (e.g., multi-attribute fairness, label bias), and proposes novel approaches like using scene graphs and topological data analysis. The section abstracts beyond specific works by highlighting overarching patterns and proposing a standardized, bias-aware data collection framework."}}
