{"id": "6517c947-ebd3-4d63-8315-8313fd7643c4", "title": "Two Sea Changes in NLP", "level": "section", "subsections": [], "parent_id": "eef570d4-0122-4f1a-8609-023f350f068a", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Two Sea Changes in NLP"]], "content": "\\label{sec:four-paradigms}\n\\term{Fully supervised learning}, where a task-specific model is trained solely on a dataset of input-output examples for the target task, has long played a central role in many machine learning tasks , and natural language processing (NLP) was no exception.\nBecause such fully supervised datasets are ever-insufficient for learning high-quality models, early NLP models relied heavily on \\term{feature engineering} (Tab.~\\ref{table:intro} a.; e.g.~), where NLP researchers or engineers used their domain knowledge to define and extract salient features from raw data and provide models with the appropriate inductive bias to learn from this limited data.\nWith the advent of neural network models for NLP, salient features were learned jointly with the training of the model itself , and hence focus shifted to \\term{architecture engineering}, where inductive bias was rather provided through the design of a suitable network architecture conducive to learning such features (Tab.~\\ref{table:intro} b.; e.g.~).\n\\footnote{\nEven during this stage, there was some use of pre-trained models exemplified by word2vec~ and GloVe~, but they were used for only a limited portion of the final model parameters.\n}\nHowever, from 2017-2019 there was a sea change in the learning of NLP models, and this fully supervised paradigm is now playing an ever-shrinking role.\nSpecifically, the standard shifted to the \\term{pre-train and fine-tune} paradigm (Tab.~\\ref{table:intro} c.; e.g.~).\nIn this paradigm, a model with a fixed\\footnote{This paradigm is less conducive to architectural exploration because (i) unsupervised pre-training allows models to learn with fewer structural priors, and\n(ii) as pre-training of models is time-consuming, experimenting with structural variants is costly.} architecture is \\term{pre-trained} as a language model (LM), predicting the probability of observed textual data. Because the raw textual data necessary to train LMs is available in abundance, these LMs can be trained on large datasets, in the process learning robust general-purpose features of the language it is modeling.\nThe above pre-trained LM will be then adapted to different downstream tasks by introducing additional parameters and \\term{fine-tuning} them using task-specific objective functions.\nWithin this paradigm, the focus turned mainly to \\term{objective engineering}, designing the training objectives used at both the pre-training and fine-tuning stages. For example,  show that introducing a loss function of predicting salient sentences from a document will lead to a better pre-trained model for text summarization.\nNotably, the main body of the pre-trained LM is generally (but not always; ) fine-tuned as well to make it more suitable for solving the downstream task.\nNow, as of this writing in 2021, we are in the middle of a second sea change, in which the ``pre-train, fine-tune'' procedure is replaced by one in which we dub ``\\term{pre-train, prompt, and predict}''.\nIn this paradigm, instead of adapting pre-trained LMs to downstream tasks via objective engineering, downstream tasks are reformulated to look more like those solved during the original LM training with the help of a textual \\term{prompt}.\nFor example, when recognizing the emotion of a social media post, ``I missed the bus today.'', we may continue with a prompt ``I felt so \\underline{\\hspace*{0.5cm}}'', and ask the LM to fill the blank with an emotion-bearing word.\nOr if we choose the prompt ``English: I missed the bus today. French: \\underline{\\hspace*{0.5cm}}''), an LM may be able to fill in the blank with a French translation.\nIn this way, by selecting the appropriate prompts we can manipulate the model behavior so that the pre-trained LM itself can be used to \\term{predict} the desired output, sometimes even without any additional task-specific training (Tab.~\\ref{table:intro} d.; e.g.~).\nThe advantage of this method is that, given a suite of appropriate prompts, a single LM trained in an entirely unsupervised fashion can be used to solve a great number of tasks . \nHowever, as with most conceptually enticing prospects, there is a catch -- this method introduces the necessity for \\term{prompt engineering}, finding the most appropriate prompt to allow a LM to solve the task at hand.\nThis survey attempts to organize the current state of knowledge in this rapidly developing field by providing an overview and formal definition of prompting methods (\\S\\ref{sec:2-formal-description}), and an overview of the pre-trained language models that use these prompts (\\S\\ref{sec:lm}).\nThis is followed by in-depth discussion of prompting methods, from basics such as prompt engineering (\\S\\ref{sec:4-prompt-template-engineering}) and answer engineering (\\S\\ref{sec:5-prompt-answer-engineering}) to more advanced concepts such as multi-prompt learning methods (\\S\\ref{sec:6-multi-prompt-learning}) and prompt-aware training methods (\\S\\ref{sec:tuning}).\nWe then organize the various applications to which prompt-based learning methods have been applied, and discuss how they interact with the choice of prompting method (\\S\\ref{sec:applications}).\nFinally, we attempt to situate the current state of prompting methods in the research ecosystem, making connections to other research fields (\\S\\ref{sec:related}), suggesting some current challenging problems that may be ripe for further research (\\S\\ref{sec:challenges}), and performing a meta-analysis of current research trends (\\S\\ref{sec:meta-analysis}).\nFinally, in order to help beginners who are interested in this field  learn more effectively, we highlight some systematic  resources about prompt learning (as well as pre-training) provided both within this survey and on companion websites:\n\\begin{itemize*}\n    \\item \\includegraphics[scale=0.01]{fig/IntroFig/logo.png}: A \n    \\href{http://pretrain.nlpedia.ai/}{website } of prompt-based learning that contains: frequent updates to this survey, related slides, etc.\n    \\item Fig.\\ref{fig:typo-prompt}: A typology of important concepts for prompt-based learning.\n    \\item Tab.\\ref{tab:papers-part1}: A systematic and comprehensive comparison among different prompting methods.  \n    \\item Tab.\\ref{tab:task-prompt}: An organization of commonly-used prompts.\n    \\item Tab.\\ref{tab:timeline}: A timeline of prompt-based research works.\n    \\item Tab.\\ref{tab:pretrained-aspect}: A systematic and comprehensive comparison among different pre-trained LMs.    \n\\end{itemize*}\n\\begin{table*}[!t]\n\\centering\n\\footnotesize\n\\setlength\\tabcolsep{15pt}\n\\renewcommand{\\arraystretch}{1.3}\n\\begin{tabular}{llc}\n\\toprule\n\\textbf{Paradigm}                                                    & \\textbf{Engineering}                   & \\textbf{Task Relation}           \\\\\n\\midrule\n\\multirow{4}{4cm}{a. Fully Supervised Learning (Non-Neural Network) }                                   & \\multirow{4}{4.5cm}{Features\\hspace{3cm}  (e.g.~word identity, part-of-speech, sentence length)}      & \\hspace{-1.8cm}\\multirow{4}{*}{\\parbox[c]{0em}{\\includegraphics[width=0.7in]{fig/IntroFig/stage2-taskrel.pdf}}}  \\\\\n\\\\\n\\\\\n                                       &                               &                                     \\\\\n\\midrule\n\\multirow{4}{4cm}{b. Fully Supervised Learning (Neural Network)} & \\multirow{4}{4.5cm}{Architecture \\hspace{2cm} (e.g.~convolutional, recurrent, self-attentional)} & \\hspace{-1.8cm}\\multirow{4}{*}{\\parbox[c]{0em}{\\includegraphics[width=0.7in]{fig/IntroFig/stage2-taskrel.pdf}}} \\\\\n\\\\\n                                                            &                               &                    \\\\\n                                                            &                               &                               \\\\\n\\midrule\n\\multirow{4}{4cm}{c. Pre-train, Fine-tune}              & \\multirow{4}{4.5cm}{Objective \\hspace{2cm} (e.g.~masked language modeling, next sentence prediction)}    & \\hspace{-1.8cm}\\multirow{4}{*}{\\parbox[c]{0em}{\\includegraphics[width=0.7in]{fig/IntroFig/stage3-taskrel.pdf}}}  \\\\\n\\\\\n                                                            &                               &                               \\\\\n                                                            &                               &                                     \\\\\n\\midrule\n\\multirow{4}{*}{d. Pre-train, Prompt, Predict}                 & \\multirow{4}{4.5cm}{Prompt (e.g.~cloze, prefix)}       & \\hspace{-1.8cm}\\multirow{4}{*}{\\parbox[c]{0em}{\\includegraphics[width=0.7in]{fig/IntroFig/stage4-taskrel.pdf}}}  \\\\\n\\\\\n                                                            &                               &                                      \\\\\n                                                            &                               &                                     \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Four paradigms in NLP. The ``\\textbf{engineering}'' column represents the type of engineering to be done to build strong systems. The ``\\textbf{task relation}'' column, shows the relationship between language models (LM) and other NLP tasks (CLS: classification, TAG: sequence tagging, GEN: text generation). \\includegraphics[scale=0.3]{fig/IntroFig/lm-square.pdf}: fully unsupervised training.\n\\includegraphics[scale=0.3]{fig/IntroFig/task-square.pdf}: fully supervised training.\n\\includegraphics[scale=0.3]{fig/IntroFig/overlap.pdf}: Supervised training combined with unsupervised training.\n\\includegraphics[scale=0.3]{fig/IntroFig/prompt-text.pdf} indicates a textual prompt. \nDashed lines suggest that different tasks can be connected by sharing parameters of pre-trained models. ``LM$\\rightarrow$Task'' represents \\textit{adapting LMs (objectives) to downstream tasks} while ``Task$\\rightarrow$LM'' denotes \\textit{adapting downstream tasks (formulations) to LMs}.} \n\\label{table:intro}\n\\end{table*}", "cites": [8565, 39, 7165, 1684, 679, 4191, 38, 168, 1569, 7225, 318, 8744, 1575, 50, 11, 8745, 9, 1159], "cite_extract_rate": 0.7241379310344828, "origin_cites_number": 29, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.5, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes ideas from multiple papers to present a coherent narrative of paradigm shifts in NLP, particularly from fully supervised learning to pre-train, fine-tune, and finally pre-train, prompt, and predict. It abstracts the evolution into a conceptual framework and identifies the broader implications of these changes, such as the shift from feature to architecture to objective engineering. Critical analysis is present but limited, mainly highlighting limitations like the need for prompt engineering and pretrain-finetune discrepancies."}}
{"id": "9db833e2-29aa-4c8b-881d-4f8b6fb32a58", "title": "Prompt Addition", "level": "subsubsection", "subsections": [], "parent_id": "43bb1294-41f5-4ad6-826a-465d704f9f34", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "A Formal Description of Prompting"], ["subsection", "Prompting Basics"], ["subsubsection", "Prompt Addition"]], "content": "In this step a \\term{prompting function} $f_{\\text{prompt}}(\\cdot)$ is applied to modify the input text $\\bm{x}$ into a \\term{prompt} $\\bm{x}' = f_{\\text{prompt}}(\\bm{x})$.\nIn the majority of previous work~, this function consists of a two step process:\n\\begin{enumerate}\n    \\item Apply a \\term{template}, which is a textual string that has two slots: an \\term{input slot} \\texttt{[X]} for input $\\bm{x}$ and an \\term{answer slot} \\texttt{[Z]} for an intermediate generated \\term{answer} text $\\bm{z}$ that will later be mapped into $\\bm{y}$.\n    \\item Fill slot \\texttt{[X]} with the input text $\\bm{x}$.\n\\end{enumerate}\nIn the case of sentiment analysis where $\\bm{x}=$``I love this movie.'', the template may take a form such as ``\\texttt{[X]} Overall, it was a \\texttt{[Z]} movie.''.\nThen, $\\bm{x}'$ would become ``I love this movie. Overall it was a \\texttt{[Z]} movie.'' given the previous example.\nIn the case of machine translation, the template may take a form such as ``Finnish: \\texttt{[X]} English: \\texttt{[Z]}'', where the text of the input and answer are connected together with headers indicating the language.\nWe show more examples in Tab.~\\ref{tab:def}\n\\begin{table*}[!t]\n\\centering\n\\footnotesize\n\\renewcommand{\\arraystretch}{1}\n\\setlength\\tabcolsep{6pt}\n\\begin{tabular}{lllll}\n\\toprule\n\\bf{Type}                                      & \\bf{Task}                            & \\bf{Input (\\texttt{[X]})}                                 & \\bf{Template}                                                        & \\bf{Answer (\\texttt{[Z]})}                     \\\\\n\\midrule\n\\multirow{9}{*}{Text CLS}      & \\multirow{3}{*}{Sentiment}        & \\multirow{3}{*}{I love this movie.}       & \\multirow{3}{*}{\\texttt{[X]} The movie is \\texttt{[Z]}.}                 & great                     \\\\\n                                          &                                   &                                           &                                                                  & fantastic                 \\\\\n                                          &                                   &                                           &                                                                  & ...                       \\\\\n                                           \\cmidrule{2-5}\n                                          & \\multirow{3}{*}{Topics}           & \\multirow{3}{*}{He prompted the LM.}  & \\multirow{3}{*}{\\texttt{[X]} The text is about \\texttt{[Z]}.}        & sports                    \\\\\n                                          &                                   &                                           &                                                                  & science                   \\\\\n                                          &                                   &                                           &                                                                  & ...                       \\\\\n                                          \\cmidrule{2-5}\n                                          & \\multirow{3}{*}{Intention}        & \\multirow{3}{*}{What is taxi fare to Denver?} & \\multirow{3}{*}{\\texttt{[X]} The question is about \\texttt{[Z]}.}        & quantity                  \\\\\n                                          &                                   &                                           &                                                                  & city                      \\\\\n                                          &                                   &                                           &                                                                  & ...                       \\\\\n                                          \\midrule\n\\multirow{3}{*}{Text-span CLS} & \\multirow{3}{2cm}{Aspect Sentiment} & \\multirow{3}{*}{Poor service but good food.}         &                                                                  & Bad                       \\\\\n                                          &                                   &                                           & \\texttt{[X]} What about service? \\texttt{[Z]}.               & Terrible                  \\\\\n                                          &                                   &                                           &                                                           & ...                       \\\\\n                                          \\midrule\n\\multirow{3}{*}{Text-pair CLS} & \\multirow{3}{*}{NLI}              & \\texttt{[X1]}: An old man with ...                   & \\multirow{3}{*}{ \\texttt{[X1]}? \\texttt{[Z]}, \\texttt{[X2]}} & Yes                       \\\\\n                                          &                                   & \\texttt{[X2]}: A man walks ...                       &                                                                  & No                     \\\\\n                                          &                                   &                                           &                                                                  & ...                       \\\\\n                                          \\midrule\n\\multirow{3}{*}{Tagging}        &   & \\texttt{[X1]}: Mike went to Paris.   &    & organization  \\\\\n                                          & NER  & \\texttt{[X2]}: Paris &  \\texttt{[X1]}\\texttt{[X2]} is a \\texttt{[Z]} entity.                                                                 &           location                \\\\\n                                          &                           &                                           &                                                                  &      ...                     \\\\\n                                          \\midrule\n\\multirow{6}{*}{Text Generation}          & \\multirow{3}{*}{Summarization}    & \\multirow{3}{*}{Las Vegas police ...}     & \\multirow{3}{*}{\\texttt{[X]} TL;DR: \\texttt{[Z]}}                       & The victim ... \\\\\n                                          &                                   &                                           &                                                                  & A woman ...    \\\\\n                                          &                                   &                                           &                                                                  & ...                       \\\\\n                                          \\cmidrule{2-5}\n                                          & \\multirow{3}{*}{Translation}               & \\multirow{3}{*}{Je vous aime.}            & \\multirow{3}{*}{French: \\texttt{[X]} English: \\texttt{[Z]}}             & I love you.               \\\\\n                                          &                                   &                                           &                                                                  & I fancy you.     \\\\\n                                          &                                   &                                           &                                                                  & ...  \\\\\n                                          \\bottomrule\n\\end{tabular}\n\\caption{\\label{tab:def}Examples of \\term{input}, \\term{template}, and \\term{answer} for different tasks. In the \\textbf{Type} column, ``CLS\" is an abbreviation for ``classification\". In the \\textbf{Task} column, ``NLI\" and ``NER'' are abbreviations for ``natural language inference\"~ and ``named entity recognition\"~ respectively.}\n\\end{table*}\nNotably, \n(1) the prompts above will have an empty slot to fill in for $\\bm{z}$, either in the middle of the prompt or at the end. \nIn the following text, we will refer to the first variety of prompt with a slot to fill in the middle of the text as a \\term{cloze prompt}, and the second variety of prompt where the input text comes entirely before $\\bm{z}$ as a \\term{prefix prompt}.\n(2) In many cases these template words are not necessarily composed of natural language tokens; they could be virtual words (e.g.~represented by numeric ids) which would be embedded in a continuous space later, and some prompting methods even generate continuous vectors directly (more in \\S\\ref{sec:4-continuous}). (3) The number of \\texttt{[X]} slots and the number of \\texttt{[Z]} slots can be flexibly changed for the need of tasks at hand.", "cites": [4192, 4193, 2653, 2466], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily provides a descriptive overview of prompt addition by explaining the process and offering examples from different NLP tasks. It references several papers but does so in a way that lacks deep synthesis or integration of ideas. While it introduces terms like 'cloze prompt' and 'prefix prompt,' there is limited critical evaluation or abstraction beyond the specific examples."}}
{"id": "3bff6053-d166-4fd2-b609-09b47f152422", "title": "Pre-trained Language Models", "level": "section", "subsections": ["7de1b798-7578-48f1-ab66-e3e01f12f2bb", "44406b4f-e2ee-4b0f-a819-d5cb388f6d36", "6541edf8-dae9-4f53-828b-8a674545831d", "dfc90948-588a-40d8-91b9-ab807691b1b0"], "parent_id": "eef570d4-0122-4f1a-8609-023f350f068a", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Pre-trained Language Models"]], "content": "\\label{sec:lm}\nGiven the large impact that pre-trained LMs have had on NLP in the pre-train and fine-tune paradigm, there are already a number of high-quality surveys that interested readers where interested readers can learn more .\nNonetheless, in this chapter we present a systematic view of various pre-trained LMs which (i) organizes them along various axes in a more systematic way, (ii) particularly focuses on aspects salient to prompting methods.\nBelow, we will detail them through the lens of \\textit{main training objective}, \\textit{type of text noising}, \\textit{auxiliary training objective}, \\textit{attention mask}, \\textit{typical architecture}, and \\textit{preferred application scenarios}.\nWe describe each of these objectives below, and also summarize a number of pre-trained LMs along each of these axes in Tab.~\\ref{tab:pretrained-aspect} in the appendix.", "cites": [1445, 4194, 7518, 9], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple pre-trained language model surveys and integrates it into a structured framework focusing on key aspects relevant to prompting. It organizes models along several axes, providing a systematic lens that goes beyond individual papers. While it offers some abstraction and insightful categorization, it lacks deeper critical evaluation of the cited works and their limitations."}}
{"id": "45002357-6086-45d5-93a4-ba14b108c5f7", "title": "Full Text Reconstruction (FTR)", "level": "paragraph", "subsections": [], "parent_id": "6c3a397b-2be5-4a47-af6b-9ac504fb2533", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Pre-trained Language Models"], ["subsection", "Training Objectives"], ["paragraph", "Standard Language Model (SLM)"], ["paragraph", "Full Text Reconstruction (FTR)"]], "content": "These objectives reconstruct the text by calculating the loss over the \\emph{entirety} of the input texts whether it has been noised or not~.", "cites": [1569], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.0, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a minimal and purely descriptive account of the Full Text Reconstruction (FTR) training objective, merely paraphrasing its function without synthesizing it with other objectives or concepts from the cited paper or elsewhere. It lacks critical evaluation, comparison, or abstraction, offering no deeper insight or broader context into the significance or limitations of FTR."}}
{"id": "254aff29-483d-4762-b112-c04a0933d84b", "title": "Masking", "level": "paragraph", "subsections": ["26477ea1-6b8b-4e46-8c6b-1694e66cec83", "e09e5c8b-40eb-4f13-9d90-e1a086ab4e94", "ee7178c5-1725-494e-ad8a-4d12195e5848"], "parent_id": "44406b4f-e2ee-4b0f-a819-d5cb388f6d36", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Pre-trained Language Models"], ["subsection", "Noising Functions"], ["paragraph", "Masking"]], "content": "(e.g.~)  The text will be masked in different levels, replacing a token or multi-token span with a special token such as \\texttt{[MASK]}.\nNotably, masking can either be random from some distribution or specifically designed to introduce prior knowledge, such as the above-mentioned example of masking entities to encourage the model to be good at predicting entities.", "cites": [4195], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of masking as a noising function but lacks deeper synthesis of the cited paper or broader context. It mentions a single paper briefly and only refers to one example (masking entities), without comparing different masking approaches or critically evaluating their effectiveness. The abstraction is limited to a general definition of masking without identifying overarching principles or trends."}}
{"id": "26477ea1-6b8b-4e46-8c6b-1694e66cec83", "title": "Replacement", "level": "paragraph", "subsections": [], "parent_id": "254aff29-483d-4762-b112-c04a0933d84b", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Pre-trained Language Models"], ["subsection", "Noising Functions"], ["paragraph", "Masking"], ["paragraph", "Replacement"]], "content": "(e.g.~) \nReplacement is similar to masking, except that the token or multi-token span is not replaced with a \\texttt{[MASK]} but rather another token or piece of information (e.g., an image region~).", "cites": [9], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal and largely descriptive explanation of the 'Replacement' noising function, with little synthesis of ideas from the cited paper. It does not compare this approach to others or critically analyze its strengths or limitations. The abstraction is also limited, as the section does not generalize the concept to broader patterns or principles in prompting methods."}}
{"id": "e09e5c8b-40eb-4f13-9d90-e1a086ab4e94", "title": "Deletion", "level": "paragraph", "subsections": [], "parent_id": "254aff29-483d-4762-b112-c04a0933d84b", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Pre-trained Language Models"], ["subsection", "Noising Functions"], ["paragraph", "Masking"], ["paragraph", "Deletion"]], "content": "(e.g.~) \nTokens or multi-token spans will be deleted from a text without the addition of \\texttt{[MASK]} or any other token. This operation is usually used together with the FTR loss.", "cites": [1569], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section on Deletion is extremely brief and lacks depth in integrating or synthesizing information from the cited paper (SuperGLUE). It only provides a minimal description of the deletion operation without explaining its relevance, implications, or how it connects to broader themes in pre-training. There is no critical evaluation or abstraction beyond the surface-level description."}}
{"id": "ee7178c5-1725-494e-ad8a-4d12195e5848", "title": "Permutation", "level": "paragraph", "subsections": [], "parent_id": "254aff29-483d-4762-b112-c04a0933d84b", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Pre-trained Language Models"], ["subsection", "Noising Functions"], ["paragraph", "Masking"], ["paragraph", "Permutation"]], "content": "(e.g.~)\nThe text is first divided into different spans (tokens, sub-sentential spans, or sentences), and then these spans are be permuted into a new text.", "cites": [2489], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section is purely descriptive and lacks deeper synthesis or abstraction. It briefly mentions the permutation method and cites one paper (mBART) without elaborating on its relevance or how permutation fits into broader prompting strategies. There is no critical evaluation or comparison with other methods, and no attempt is made to generalize or identify overarching principles."}}
{"id": "414960e5-6c2b-42c1-b574-851fa424af5d", "title": "", "level": "paragraph", "subsections": [], "parent_id": "89b91f88-1f30-44fd-adf4-e1040773ae8b", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Pre-trained Language Models"], ["subsection", "Directionality of Representations"], ["paragraph", "Left-to-Right"], ["paragraph", ""]], "content": "In addition to the two most common directionalities above, it is also possible to mix the two strategies together in a single model , or perform conditioning of the representations in a randomly permuted order , although these strategies are less widely used.\nNotably, when implementing these strategies within a neural model, this conditioning is generally implemented through \\term{attention masking}, which masks out the values in an attentional model , such as the popular Transformer architecture~.\nSome examples of such attention masks are shown in Figure~\\ref{fig:attn_mask}.\n\\begin{figure}[h]\n    \\centering\n    \\subfloat[Full.]{\n    \\includegraphics[height=0.17\\linewidth]{fig/attn_mask/fullly-visible.pdf}\n    }         \\hspace{3.2em}\n    \\subfloat[Diagonal.]{  \n    \\includegraphics[height=0.17\\linewidth]{fig/attn_mask/causal.pdf}\n    }        \\hspace{3.2em}\n    \\subfloat[Mixture.]{ \n    \\includegraphics[height=0.17\\linewidth]{fig/attn_mask/causal-prefix.pdf}\n        } \n    \\caption{Three popular attention mask patterns, where the subscript $t$ indicates the $t$-th timestep. \n    A shaded box at $(i, j)$ indicates that the attention mechanism is allowed to attend to the input element $i$ at output time step $j$. A white box indicates that the attention mechanism is not allowed to attend to the corresponding $i$ and $j$ combination.}\n    \\label{fig:attn_mask}\n\\end{figure}", "cites": [50, 4195, 11, 168, 38], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of different attention masking strategies used in pre-trained language models, referencing several relevant papers. It integrates the concept of attention masking across multiple works to explain how models condition their representations, but it does not offer a deep synthesis of the underlying ideas or evaluate the strengths and weaknesses of the approaches. The abstraction is limited to general patterns in model design without higher-level conceptual insights."}}
{"id": "d7088549-fdaf-427b-97a9-0aebb33087bd", "title": "Left-to-Right Language Model", "level": "subsubsection", "subsections": [], "parent_id": "dfc90948-588a-40d8-91b9-ab807691b1b0", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Pre-trained Language Models"], ["subsection", "Typical Pre-training Methods"], ["subsubsection", "Left-to-Right Language Model"]], "content": "Left-to-right LMs (L2R LMs), a variety of \\term{auto-regressive LM}, predict the upcoming words or assign a probability $P(\\bm{x})$ to a sequence of words $\\bm{x} = x_1, \\cdots, x_n$ .\nThe probability is commonly broken down using the chain rule in a left-to-right fashion: $P(\\bm{x}) = P(x_1)  \\times \\cdots P(x_n|x_1\\cdots x_{n-1})$.\\footnote{Similarly, a right-to-left LM can predict preceding words based on the future context, such as $P(x_i|x_{i+1},\\cdots,x_n)$.}\n\\begin{myboxi}[Example \\& Applicable Scenario]\nLeft-to-right LMs have been standard since their proposal by Markov in 1913 , and have been used continuously since then in both count-based  and neural forms .\nRepresentative examples of modern pre-trained left-to-right LMs include GPT-3 , \nand GPT-Neo .\nL2R pre-trained LMs are also the popular backbone that many prompting methods adopt~\n. \nOne practical reason for this is that many such models are large (PanGu-$\\alpha$~, Ernie-3~) and ponderous to train, or not even available publicly.\nThus using these models in the pre-train and fine-tune regimen is often not possible.\n\\end{myboxi}\n\\begin{figure*}[!t]\n    \\centering\n    \\subfloat[Left-to-right LM.]{\n    \\includegraphics[height=0.18\\linewidth]{fig/plm/l2r.pdf}\n    }         \\hspace{1.3em}\n    \\subfloat[Masked LM.]{  \n    \\includegraphics[height=0.18\\linewidth]{fig/plm/mlm.pdf}\n    }        \\hspace{1.3em}\n    \\subfloat[Prefix LM.]{ \n    \\includegraphics[height=0.18\\linewidth]{fig/plm/plm.pdf}\n        }  \\hspace{1.3em}\n    \\subfloat[Encoder-Decoder.]{\n    \\includegraphics[height=0.18\\linewidth]{fig/plm/seq2seq.pdf}\n    }      \n    \\caption{Typical paradigms of pre-trained LMs.}\n    \\label{fig:four-architecture}\n\\end{figure*}\n\\begin{table*}[!htbp]\n  \\centering \\footnotesize\n    \\begin{tabular}{lcccccll}\n    \\toprule\n    \\multirow{2}[2]{*}{\\textbf{LMs}} & \\multicolumn{3}{c}{$\\bm{x}$} & \\multicolumn{3}{c}{$\\bm{y}$}  & \\multirow{2}[2]{*}{\\textbf{Application}}\\\\\n    \\cmidrule(lr){2-4} \\cmidrule(lr){5-7}\n          & \\textbf{Mask} & \\textbf{Noise} & \\textbf{Main Obj.} & \\textbf{Mask} & \\textbf{Noise} & \\textbf{Main Obj.} \\\\\n    \\midrule\n    L2R   &  Diagonal  & None  & SLM   & -  & -  & - & NLU \\& NLG\\\\\n    Mask  & Full & Mask  & CTR   & -     & -     & -  & NLU\\\\\n    Prefix& Full & Any   & CTR   & Diagonal  & None  & SLM & NLU \\& NLG \\\\\n    En-De & Full & Any   & None$\\dag$  & Diagonal  & None  & FTR/CRT & NLU \\& NLG \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Typical architectures for pre-trained LMs. $\\bm{x}$ and $\\bm{y}$ represent text to be encoded and decoded, respectively. \n    \\textbf{SLM}: Standard language model.\n    \\textbf{CTR}: Corrupted text reconstruction.\n    \\textbf{FTR}: Full text reconstruction.\n    $\\dag$: Encoder-decoder architectures usually apply objective functions to the decoder only.\n    }\n      \\label{tab:four-architecture}\n\\end{table*}", "cites": [8746, 679], "cite_extract_rate": 0.18181818181818182, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a basic description of left-to-right language models, including their mathematical formulation and modern examples such as GPT-3. It integrates some information from the cited papers by mentioning their historical and practical relevance but lacks deeper synthesis across sources. There is minimal critical evaluation or abstraction beyond the specific models discussed."}}
{"id": "6d250a25-76b5-4fc3-a5a0-cbce66905c17", "title": "Masked Language Models", "level": "subsubsection", "subsections": [], "parent_id": "dfc90948-588a-40d8-91b9-ab807691b1b0", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Pre-trained Language Models"], ["subsection", "Typical Pre-training Methods"], ["subsubsection", "Masked Language Models"]], "content": "While autoregressive language models provide a powerful tool for modeling the probability of text, they also have disadvantages such as requiring representations be calculated from left-to-right.\nWhen the focus is shifted to generating the optimal representations for down-stream tasks such as classification, many other options become possible, and often preferable.\nOne popular bidirectional objective function used widely in representation learning is the \\term{masked language model}~(MLM; ), which aims to\npredict masked text pieces based on surrounded context. For example, $P(x_i|x_1,\\ldots,x_{i-1},x_{i+1},\\ldots,x_n)$ represents the probability of the word $x_i$ given the surrounding context.\n\\begin{myboxi}[Example \\& Applicable Scenario]\nRepresentative pre-trained models using MLMs include: BERT~, ERNIE~ and many variants.\nIn prompting methods, MLMs are generally most suitable for natural language understanding or analysis tasks (e.g., text classification, natural language inference \n, and extractive question answering).\nThese tasks are often relatively easy to be reformulated into cloze problems, which are consistent with the training objectives of the MLM.\nAdditionally, MLMs have been a pre-trained model of choice when exploring methods that combine prompting with fine-tuning, elaborated further in \\S\\ref{sec:tuning}.\n\\end{myboxi}", "cites": [4195, 3124, 2475], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of masked language models (MLMs) and their role in pre-trained language models, mentioning representative models and tasks where they excel. It integrates some information from the cited ERNIE papers to illustrate entity-level and phrase-level masking strategies but does not synthesize these into a broader conceptual framework. There is limited critical analysis or abstraction beyond the specific methods described."}}
{"id": "6103a771-d7c9-4463-8cc8-61555e1ab261", "title": "Encoder-decoder", "level": "paragraph", "subsections": [], "parent_id": "841865c7-2457-465c-81ae-6d99fd8576be", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Pre-trained Language Models"], ["subsection", "Typical Pre-training Methods"], ["subsubsection", "Prefix and Encoder-Decoder"], ["paragraph", "Encoder-decoder"]], "content": "The encoder-decoder model is a model that uses a left-to-right LM to decode $\\bm{y}$ conditioned on a \\textit{separate} encoder for text $\\bm{x}$ with a fully-connected mask; the parameters of the encoder and decoder are not shared.\nSimilarly to the prefix LM, diverse types of noising can be applied to the input $\\bm{x}$.\n\\begin{myboxi}[Example \\& Applicable Scenario]\nPrefix LMs have been explored in UniLM 1-2  and ERNIE-M~ while encoder-decoder models are widely used in pre-trained models such as T5~, BART~, MASS~ and their variants.\nPre-trained models with prefix LMs and encoder-decoder paradigms can be naturally used to text generation tasks with  or without  prompting using input texts.\nHowever, recent studies reveal that other non-generation tasks, such as information extraction , question answering  \n, and text generation evaluation   can be reformulated a generation problems by providing appropriate prompts.\nTherefore, prompting methods (i) broaden the applicability of these generation-oriented pre-trained models. For example, pre-trained models like BART are less used in NER while prompting methods make BART applicable,\nand (ii) breaks the difficulty of unified modelling among different tasks~. \n\\end{myboxi}", "cites": [4196, 4197, 7096, 4198, 50, 4199, 9, 4195, 1569], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of encoder-decoder models and their pre-training methods, integrating examples from several key papers (e.g., T5, BART, MASS) and linking them to the broader theme of prompting methods. It synthesizes the role of these models in both generation and non-generation tasks through the lens of prompting, but the critical analysis is limited to high-level statements without in-depth evaluation of trade-offs or limitations of the cited works. It identifies some abstraction by highlighting how prompting extends the utility of generation models beyond their original design."}}
{"id": "a79b1ca5-b6c7-463b-9f2d-82a6c332eb40", "title": "Prompt Shape", "level": "subsection", "subsections": [], "parent_id": "4bcf6fe3-ff16-403c-8dd7-c4e8b56b0836", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Prompt Engineering"], ["subsection", "Prompt Shape"]], "content": "As noted above, there are two main varieties of prompts: \\term{cloze prompts} , which fill in the blanks of a textual string, and \\term{prefix prompts} , which continue a string prefix.\nWhich one is chosen will depend both on the task and the model that is being used to solve the task.\nIn general, for tasks regarding generation, or tasks being solved using a standard auto-regressive LM, prefix prompts tend to be more conducive, as they mesh well with the left-to-right nature of the model. For tasks that are solved using masked LMs, cloze prompts are a good fit, as they very closely match the form of the pre-training task.\nFull text reconstruction models are more versatile, and can be used with either cloze or prefix prompts.\nFinally, for some tasks regarding multiple inputs such as \\term{text pair classification}, prompt templates must contain space for two inputs, \\texttt{[X1]} and \\texttt{[X2]}, or more.", "cites": [7225, 8536, 1591, 4198], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic description of prompt shapes (cloze vs. prefix) and their suitability for different model types and tasks. It references cited papers to support these distinctions but does not deeply synthesize or compare their contributions. There is limited critical evaluation or abstraction to broader principles."}}
{"id": "76b34b2d-4112-47fa-a802-5c938c4fa9b9", "title": "Manual Template Engineering", "level": "subsection", "subsections": [], "parent_id": "4bcf6fe3-ff16-403c-8dd7-c4e8b56b0836", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Prompt Engineering"], ["subsection", "Manual Template Engineering"]], "content": "Perhaps the most natural way to create prompts is to manually create intuitive templates based on human introspection.\nFor example, the seminal LAMA dataset  provides manually created cloze templates to probe knowledge in LMs.\n create manually crafted prefix prompts to handle a wide variety of tasks, including question answering, translation, and probing tasks for common sense reasoning.\n use pre-defined templates in a few-shot learning setting on text classification and conditional text generation tasks.", "cites": [679, 7786, 8745, 7225, 2466], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of manual template engineering and lists several papers that use this approach. However, it lacks synthesis by not connecting or contrasting these examples in a meaningful way. There is no critical evaluation of the methods or their limitations, and no abstraction to broader principles or patterns in the field."}}
{"id": "6c52b65c-258e-492f-aa47-5817653e1b98", "title": "Automated Template Learning", "level": "subsection", "subsections": ["4da546c7-04d1-4e57-b7a3-9d8884da84b7", "bedab224-d7f8-4e40-8609-91f15b62618f"], "parent_id": "4bcf6fe3-ff16-403c-8dd7-c4e8b56b0836", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Prompt Engineering"], ["subsection", "Automated Template Learning"]], "content": "While the strategy of manually crafting templates is intuitive and does allow solving various tasks with some degree of accuracy, there are also several issues with this approach: (1) creating and experimenting with these prompts is an art that takes time and experience, particularly for some complicated tasks such as semantic parsing ;\n(2) even experienced prompt designers may fail to manually discover optimal prompts .\nTo address these problems, a number of methods have been proposed to automate the template design process.\nIn particular, the automatically induced prompts can be further separated into \\term{discrete prompts}, where the prompt is an actual text string, and \\term{continuous prompts}, where the prompt is instead described directly in the embedding space of the underlying LM.\nOne other orthogonal design consideration is whether the prompting function $f_{\\text{prompt}}(\\bm{x})$ is \\term{static}, using essentially the same prompt template for each input, or \\term{dynamic}, generating a custom template for each input.\nBoth static and dynamic strategies have been used for different varieties of discrete and continuous prompts, as we will mention below.", "cites": [8747], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section introduces the concept of automated template learning in prompting, highlighting the limitations of manual approaches and categorizing automated methods into discrete and continuous prompts. It also mentions static vs. dynamic prompting functions, offering a basic abstraction of design choices. However, it cites only one paper and does not synthesize insights across multiple sources or provide deep critical analysis of the limitations and strengths of the cited work."}}
{"id": "6f5ef46e-76e6-4679-a795-6b0c0373dc11", "title": "D2: Prompt Paraphrasing", "level": "paragraph", "subsections": [], "parent_id": "4da546c7-04d1-4e57-b7a3-9d8884da84b7", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Prompt Engineering"], ["subsection", "Automated Template Learning"], ["subsubsection", "Discrete Prompts"], ["paragraph", "D2: Prompt Paraphrasing"]], "content": "Paraphrasing-based approaches take in an existing seed prompt (e.g.~manually constructed or mined), and paraphrases it into a set of other candidate prompts, then selects the one that achieves the highest training accuracy on the target task.\nThis paraphrasing can be done in a number of ways, including using round-trip translation of the prompt into another language then back , using replacement of phrases from a thesaurus , or using a neural prompt rewriter specifically optimized to improve accuracy of systems using the prompt .\nNotably,  perform paraphrasing \\emph{after} the input $\\bm{x}$ is input into the prompt template, allowing a different paraphrase to be generated for each individual input.", "cites": [4199], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of paraphrasing-based prompting approaches, mentioning techniques like round-trip translation and thesaurus-based phrase replacement. It integrates one cited paper (BARTScore) but does so in a limited way without connecting it to broader themes or contrasting it with other methods. There is no critical evaluation of the approaches or their limitations, and the level of abstraction remains minimal, focusing on specific methods rather than overarching principles."}}
{"id": "d0d53c5f-6015-48b0-bae6-fab9f019032c", "title": "D3: Gradient-based Search", "level": "paragraph", "subsections": [], "parent_id": "4da546c7-04d1-4e57-b7a3-9d8884da84b7", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Prompt Engineering"], ["subsection", "Automated Template Learning"], ["subsubsection", "Discrete Prompts"], ["paragraph", "D3: Gradient-based Search"]], "content": " applied a gradient-based search over actual tokens to find short sequences that can trigger the underlying pre-trained LM to generate the desired target prediction. This search is done in an iterative fashion, stepping through tokens in the prompt \n.\nBuilt upon this method,  automatically search for template tokens using downstream application training samples and demonstrates strong performance in prompting scenarios.", "cites": [7787, 7586], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section briefly describes the gradient-based search method as applied in two cited papers but does not integrate or synthesize their contributions into a broader conceptual framework. It lacks critical evaluation of the approaches and does not abstract key principles or trends from the discussed methods."}}
{"id": "4dd60bbe-b5f8-4652-a239-8e4da791f92e", "title": "D4: Prompt Generation", "level": "paragraph", "subsections": [], "parent_id": "4da546c7-04d1-4e57-b7a3-9d8884da84b7", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Prompt Engineering"], ["subsection", "Automated Template Learning"], ["subsubsection", "Discrete Prompts"], ["paragraph", "D4: Prompt Generation"]], "content": "Other works treat the generation of prompts as a text generation task and use standard natural language generation models to perform this task.\nFor example,  introduce the seq2seq pre-trained model T5 into the template search process. Since T5 has been pre-trained on a task of filling in missing spans, they use T5 to generate template tokens by (1) specifying the position to insert template tokens within a template\\footnote{The number of template tokens do not need to be pre-specified since T5 can decode multiple tokens at a masked position.} (2) provide training samples for T5 to decode template tokens.\n propose a domain adaptation algorithm that trains T5 to generate unique domain relevant features (DRFs; a set of keywords that characterize domain information) for each input. Then those DRFs can be concatenated with the input to form a template and be further used by downstream tasks.", "cites": [8744], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section describes how T5 is used for prompt generation and outlines the methods proposed in the cited work, but it does so primarily through a factual summary without deeper analysis or comparison. While there is a slight attempt to synthesize the general idea of using pre-trained generation models for prompting, it lacks critical evaluation of the approaches or abstract generalization to broader principles in prompting methods."}}
{"id": "67e7ab9e-74a1-457f-8ec1-815f8b7d1478", "title": "D5: Prompt Scoring", "level": "paragraph", "subsections": [], "parent_id": "4da546c7-04d1-4e57-b7a3-9d8884da84b7", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Prompt Engineering"], ["subsection", "Automated Template Learning"], ["subsubsection", "Discrete Prompts"], ["paragraph", "D5: Prompt Scoring"]], "content": " investigate the task of knowledge base completion and design a template for an input (head-relation-tail triple) using LMs.\nThey first hand-craft a set of templates as potential candidates, and fill the input and answer slots to form a filled prompt.\nThey then use a unidirectional LM to score those filled prompts, selecting the one with the highest LM probability.\nThis will result in custom template for each individual input.", "cites": [4200], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a straightforward description of the Prompt Scoring method, focusing on how a unidirectional LM is used to select the best template for knowledge base completion. It integrates a single paper to explain the general approach, but lacks connections to other works or broader trends in automated template learning. There is minimal critical analysis or abstraction to higher-level concepts."}}
{"id": "d542f58f-c39c-44b8-84cb-4524c4c91718", "title": "C1: {Prefix Tuning", "level": "paragraph", "subsections": [], "parent_id": "bedab224-d7f8-4e40-8609-91f15b62618f", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Prompt Engineering"], ["subsection", "Automated Template Learning"], ["subsubsection", "Continuous Prompts"], ["paragraph", "C1: {Prefix Tuning"]], "content": "} Prefix Tuning  is a method that prepends a sequence of continuous task-specific vectors to the input, while keeping the LM parameters frozen. Mathematically, this consists of optimizing over the following log-likelihood objective given a trainable prefix matrix $M_{\\phi}$ and a fixed pre-trained LM parameterized by $\\theta$.\n\\begin{equation}\n\\label{eq:prefix-tuning}\n    \\max_{\\phi} \\log P(\\bm{y}|\\bm{x}; \\theta; \\phi) = \\max_{\\phi} \\sum_{y_i} \\log P(y_i | h_{< i}; \\theta; \\phi) \n\\end{equation}\nIn Eq.~\\ref{eq:prefix-tuning}, $h_{<i} = [h_{<i}^{(1)}; \\cdots; h_{<i}^{(n)}]$ is the concatenation of all neural network layers at time step $i$. It is copied from $M_{\\phi}$ directly if the corresponding time step is within the prefix ($h_i$ is $M_{\\phi}[i]$), otherwise it is computed using the pre-trained LM. \nExperimentally,  observe that such continuous prefix-based learning is more sensitive to different initialization in low-data settings than the use of discrete prompts with real words. \nSimilarly,  prepend the input sequence with special tokens to form a template and tune the embeddings of these tokens directly. Compared to 's method, this adds fewer parameters as it doesn't introduce additional tunable parameters within each network layer.\n train a vision encoder that encodes an image into a sequence of embeddings that can be used to prompt a frozen auto-regressive LM to generate the appropriate caption. They show that the resulting model can perform few-shot learning for vision-language tasks such as visual question answering etc. Different from the above two works, the prefix used in  is sample-dependent, namely a representation of input images, instead of a task embedding.", "cites": [8536, 3003, 1591], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the concept of Prefix Tuning from Paper 3 and connects it with related ideas from Papers 1 and 2, such as the use of continuous embeddings and the comparison with discrete prompt methods. It provides some critical insights, such as the sensitivity of continuous prefixes to initialization in low-data settings and the parameter efficiency of different approaches. However, it does not abstract these ideas into broader theoretical principles or offer a deeper meta-level understanding of the field."}}
{"id": "3014f4bf-72d4-4626-bada-53aa63bd0b32", "title": "C2: Tuning Initialized with Discrete Prompts", "level": "paragraph", "subsections": [], "parent_id": "bedab224-d7f8-4e40-8609-91f15b62618f", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Prompt Engineering"], ["subsection", "Automated Template Learning"], ["subsubsection", "Continuous Prompts"], ["paragraph", "C2: Tuning Initialized with Discrete Prompts"]], "content": "There are also methods that initialize the search for a continuous prompt using a prompt that has already been created or discovered using discrete prompt search methods.\nFor example,  first define a template using a discrete search method such as \\textsc{AutoPrompt}~'s, initialize virtual tokens based on this discovered prompt, then fine-tune the embeddings to increase task accuracy.\nThis work found that initializing with manual templates can provide a better starting point for the search process.\n propose to learn a mixture of soft templates for each input where the weights and parameters for each template are jointly learned using training samples.\nThe initial set of templates they use are either manually crafted ones or those obtained using the  ``prompt mining'' method.\nSimilarly,  introduce the use of a continuous template whose shape follows a manual prompt template.", "cites": [7787, 7788, 4202, 4201], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of methods that use discrete prompts to initialize continuous prompt tuning, referencing several papers. However, it lacks deep synthesis of ideas, critical evaluation of strengths and limitations, and abstract generalization. The narrative is primarily a summary of methods without significant insight or comparative analysis."}}
{"id": "afb33abe-23bf-485f-81f2-f9252f7433a8", "title": "C3: Hard-Soft Prompt Hybrid Tuning", "level": "paragraph", "subsections": [], "parent_id": "bedab224-d7f8-4e40-8609-91f15b62618f", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Prompt Engineering"], ["subsection", "Automated Template Learning"], ["subsubsection", "Continuous Prompts"], ["paragraph", "C3: Hard-Soft Prompt Hybrid Tuning"]], "content": "Instead of using a purely learnable prompt template, these methods insert some tunable embeddings into a hard prompt template.\n propose ``P-tuning\", where continuous prompts are learned by inserting trainable variables into the embedded input.\nTo account for interaction between prompt tokens, they represent prompt embeddings as the output of a BiLSTM .\nP-tuning also introduces the use of task-related anchor tokens (such as ``capital\" in relation extraction) \nwithin the template for further improvement. These anchor tokens are not tuned during training.\n propose prompt tuning with rules (PTR), which uses manually crafted sub-templates to compose a complete template using logic rules. To enhance the representation ability of the resulting template, they also insert several virtual tokens whose embeddings can be tuned together with the pre-trained LMs parameters using training samples. The template tokens in \\textsc{PTR} contain both actual tokens and virtual tokens. Experiment results demonstrate the effectiveness of this prompt design method in relation classification tasks.", "cites": [4203, 7789], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the key ideas from both P-Tuning and PTR, integrating their approaches to explain how continuous and discrete prompts can be combined. While it offers a general explanation of the hybrid method, it primarily focuses on describing the techniques rather than evaluating their strengths and limitations in depth. The abstraction is moderate, as it identifies the use of both virtual and anchor tokens as a broader design pattern but stops short of presenting a meta-level framework."}}
{"id": "e7c1b8ba-aa00-4218-ad17-85432d9bb2c7", "title": "Answer Shape", "level": "subsection", "subsections": [], "parent_id": "feb14fcb-3310-4466-a79d-5f4c62dcf931", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Answer Engineering"], ["subsection", "Answer Shape"]], "content": "The shape of an answer characterizes its granularity.\nSome common choices include:\n\\begin{itemize}\n    \\item \\textbf{Tokens:} One of the tokens in the pre-trained LM's vocabulary, or a subset of the vocabulary.\n    \\item \\textbf{Span:} A short multi-token span. These are usually used together with cloze prompts.\n    \\item \\textbf{Sentence:} A sentence or document. These are commonly used with prefix prompts.\n\\end{itemize}\nIn practice, how to choose the shape of acceptable answers depends on the task we want to perform. \nToken or text-span  answer spaces are widely used in classification tasks (e.g.~sentiment classification; ), but also other tasks such as relation extraction  or named entity recognition .\nLonger phrasal or sentential answers are often used in language generation tasks , but also used in other tasks such as multiple-choice question answering (where the scores of multiple phrases are compared against each-other; ).", "cites": [4198, 7225, 8463], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section describes the concept of answer shape in prompting methods but lacks in-depth synthesis of the cited papers. It lists different types of answer shapes (tokens, span, sentence) and briefly mentions their use in tasks without connecting the cited works coherently. There is minimal critical analysis or abstraction to broader principles, making the section primarily descriptive with low insight quality."}}
{"id": "748ad265-ac37-4f3e-83b3-c5809798cfb5", "title": "Unconstrained Spaces", "level": "paragraph", "subsections": [], "parent_id": "fcd256f9-6093-4041-b9b1-77b6379aabc7", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Answer Engineering"], ["subsection", "Answer Space Design Methods"], ["subsubsection", "Manual Design"], ["paragraph", "Unconstrained Spaces"]], "content": "In many cases, the answer space $\\mathcal{Z}$ is the space of all tokens , fixed-length spans , or token sequences .\nIn these cases, it is most common to directly map answer $\\bm{z}$ to the final output $\\bm{y}$ using the identity mapping.", "cites": [7225], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic definition of unconstrained answer spaces in prompting methods but does not synthesize or integrate the cited paper meaningfully. It mentions the identity mapping without elaborating on how the cited work informs or supports this concept. There is little critical evaluation or abstraction beyond the specific technical description."}}
{"id": "d1cfe903-9475-4f3a-b08b-2597bc018eaf", "title": "Constrained Spaces", "level": "paragraph", "subsections": [], "parent_id": "fcd256f9-6093-4041-b9b1-77b6379aabc7", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Answer Engineering"], ["subsection", "Answer Space Design Methods"], ["subsubsection", "Manual Design"], ["paragraph", "Constrained Spaces"]], "content": "However, there are also cases where the space of possible outputs is constrained.\nThis is often performed for tasks with a limited label space such as text classification or entity recognition, or multiple-choice question answering.\nTo give some examples,  manually design lists of words relating to relevant topics (``health'', ``finance'', ``politics'',\n``sports'', etc.), emotions (``anger'', ``joy'', ``sadness'',\n``fear'', etc.), or other aspects of the input text to be classified.\n manually design lists such as ``person'', ``location'', etc. for NER tasks.\nIn these cases, it is necessary to have a mapping between the answer $\\mathcal{Z}$ and the underlying class $\\mathcal{Y}$.\nWith regards to multiple-choice question answering, it is common to use an LM to calculate the probability of an output among multiple choices, with  being an early example.", "cites": [8463, 4198], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of constrained output spaces in prompting methods, citing two papers as examples but not integrating or synthesizing their contributions into a broader framework. It lacks critical evaluation of the methods or limitations, and while it mentions general categories (e.g., topic, emotion), it does not abstract these into overarching principles or trends in the field."}}
{"id": "126cf15f-8186-4426-9182-e98b7d78b7bc", "title": "Prune-then-Search", "level": "paragraph", "subsections": [], "parent_id": "aa14ebc4-4412-48a7-958b-5c880f55f6c6", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Answer Engineering"], ["subsection", "Answer Space Design Methods"], ["subsubsection", "Discrete Answer Search"], ["paragraph", "Prune-then-Search"]], "content": "In these methods, first, an initial pruned answer space of several plausible answers $\\mathcal{Z}'$ is generated, and then an algorithm further searches over this pruned space to select a final set of answers.\nNote that in some of the papers introduced below, they define a function from label $\\bm{y}$ to a single answer token $\\bm{z}$, which is often called a \\term{verbalizer} .\n find tokens containing at least two alphabetic characters that are frequent in a large unlabeled dataset.\nIn the search step, they iteratively compute a word's suitability as a representative answer $\\bm{z}$ for a label $\\bm{y}$ by maximizing the likelihood of the label over training data.\n  learn a logistic classifier using the contextualized representation of the \\texttt{[Z]} token as input. In the search step, they select the top-$k$ tokens that achieve the highest probability score using the learned logistic classifier in the first step. Those selected tokens will form the answer.\n first construct a pruned search space $\\mathcal{Z}'$ by selecting top-$k$ vocabulary words based on their generation probability at the \\texttt{[Z]} position determined by training samples. Then the search space is further pruned down by only selecting a subset of words within $\\mathcal{Z}'$ based on their zero-shot accuracy on the training samples. (2) In the search step, they fine-tune the LM with fixed templates together with every answer mapping using training data and select the best label word as the answer based on the accuracy on the development set.", "cites": [7787, 2466, 8744], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of the 'Prune-then-Search' approach, summarizing the methods used in the cited papers. However, it lacks significant synthesis of ideas across the works and does not present a novel framework or deep comparative analysis. Critical evaluation of limitations or trade-offs is also minimal, and the abstraction remains at a surface level without identifying broader principles or patterns."}}
{"id": "c83f0e79-178b-415b-a636-f9b542b4c44c", "title": "Continuous Answer Search", "level": "subsubsection", "subsections": [], "parent_id": "894e3d7b-a100-4e52-9e1a-fe7cfe035b9c", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Answer Engineering"], ["subsection", "Answer Space Design Methods"], ["subsubsection", "Continuous Answer Search"]], "content": "Very few works explore the possibility of using soft answer tokens which can be optimized through gradient descent.\n assign a virtual token for each class label and optimize the token embedding for each class together with prompt token embeddings. Since the answer tokens are optimized directly in the embedding space, they do not make use of the embeddings learned by the LM and instead learn an embedding from scratch for each label.", "cites": [4201], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of continuous answer search but lacks synthesis of multiple sources, critical evaluation of the method's strengths or weaknesses, and broader abstraction. It references a single paper and only paraphrases its approach without comparing it to other methods or identifying underlying principles."}}
{"id": "46727054-6a76-4060-bfc1-4ab56f9ac0b3", "title": "Uniform averaging", "level": "paragraph", "subsections": ["9c833c54-6448-4367-8381-f13260c0307c", "32ca405a-6ab9-4254-b601-dc6385305c0e", "a639f8b3-e510-4817-9499-60f4e779388c", "30eb8376-cefc-4557-9bf4-a7b4ad11a23f"], "parent_id": "8f1b8b75-f731-49f2-ac21-e9ad46aae97c", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Multi-Prompt Learning"], ["subsection", "Prompt Ensembling"], ["paragraph", "Uniform averaging"]], "content": "The most intuitive way to combine the predictions when using multiple prompts is to take the average of probabilities from different prompts. \nConcretely, this indicates that $P(\\bm{z}|\\bm{x}) \\coloneqq \\frac{1}{K} \\sum_{i}^K P(\\bm{z}|f_{\\text{prompt},i}(\\bm{x}))$ where $f_{\\text{prompt},i}(\\cdot)$ is the $i$th prompt in the prompt ensemble.\n first filter their prompts by selecting $K$ prompts that achieve the highest accuracy on the training set, and then use the average log probabilities obtained from the top $K$ prompts to calculate the probability for a single token at \\texttt{[Z]} position when performing factual probing tasks.\n also try a simple average when using an ensemble model to annotate an unlabeled dataset. \nWhen performing text generation evaluation,  formulates this task as a text generation problem and take the average of the final generation scores obtained using different prompts.", "cites": [4199, 2466], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of uniform averaging in prompt ensembling by referencing how it is applied in different works, but it lacks deeper synthesis of the underlying ideas. It does not critically assess the method or its limitations, nor does it abstract the concept into broader patterns or principles. Instead, it remains at a surface level by listing examples of its usage."}}
{"id": "9c833c54-6448-4367-8381-f13260c0307c", "title": "Weighted averaging", "level": "paragraph", "subsections": [], "parent_id": "46727054-6a76-4060-bfc1-4ab56f9ac0b3", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Multi-Prompt Learning"], ["subsection", "Prompt Ensembling"], ["paragraph", "Uniform averaging"], ["paragraph", "Weighted averaging"]], "content": "Simple uniform averaging of results from multiple prompts is easy to implement, but can also be suboptimal given that some prompts are more performant than others.\nTo account for this, some works also explore to use of weighted averages for prompt ensembling where each prompt is associated with a weight.\nThe weights are typically pre-specified based on prompt performance or optimized using a training set.\nFor example,  learn the weight for each prompt by maximizing the probability of the target output over training data.\n use the same approach except that the weight for each prompt is optimized together with soft prompt parameters.\nBesides,  also introduce a data-dependent weighting strategy where the probability of the input appearing in that prompt is considered in weighting different prompts as well.  set the weight for each prompt proportional to the accuracy on the training set before training.", "cites": [4202, 8745, 2466], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a basic analytical discussion of weighted averaging in prompt ensembling, connecting different methods across cited papers by highlighting variations in how weights are determined (pre-specified, optimized, or data-dependent). It identifies a limitation of uniform averaging and presents a progression in techniques to address it. While it offers some integration and analysis, it lacks deeper critique and a more generalized framework."}}
{"id": "32ca405a-6ab9-4254-b601-dc6385305c0e", "title": "Majority voting", "level": "paragraph", "subsections": [], "parent_id": "46727054-6a76-4060-bfc1-4ab56f9ac0b3", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Multi-Prompt Learning"], ["subsection", "Prompt Ensembling"], ["paragraph", "Uniform averaging"], ["paragraph", "Majority voting"]], "content": "For classification tasks, majority voting can also be used to combine the results from different prompts .", "cites": [4201, 8536], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a very brief and superficial mention of majority voting as a method for combining results from different prompts in classification tasks, without elaborating on its mechanisms or implications. It cites two papers but does not synthesize their contributions or connect them to the concept of majority voting. There is no critical evaluation or abstraction to broader principles."}}
{"id": "a639f8b3-e510-4817-9499-60f4e779388c", "title": "Knowledge distillation", "level": "paragraph", "subsections": [], "parent_id": "46727054-6a76-4060-bfc1-4ab56f9ac0b3", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Multi-Prompt Learning"], ["subsection", "Prompt Ensembling"], ["paragraph", "Uniform averaging"], ["paragraph", "Knowledge distillation"]], "content": "An ensemble of deep learning models can typically improve the performance, and this superior performance can be distilled into a single model using knowledge distillation . To incorporate this idea,  train a separate model for each manually-created template-answer pair, and use the ensemble of them to annotate an unlabeled dataset. Then the final model is trained to distill the knowledge from the annotated dataset.  use a similar ensemble method on their automatically generated templates.", "cites": [7786, 8745, 2466, 8744], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section briefly describes the use of knowledge distillation in the context of prompt ensembling, referencing multiple papers. However, it lacks deep synthesis by not clearly connecting the different approaches or abstracting broader principles. There is minimal critical analysis of the methods or their limitations, and the narrative remains at a descriptive level."}}
{"id": "30eb8376-cefc-4557-9bf4-a7b4ad11a23f", "title": "Prompt ensembling for text generation", "level": "paragraph", "subsections": [], "parent_id": "46727054-6a76-4060-bfc1-4ab56f9ac0b3", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Multi-Prompt Learning"], ["subsection", "Prompt Ensembling"], ["paragraph", "Uniform averaging"], ["paragraph", "Prompt ensembling for text generation"]], "content": "There is relatively little work on prompt ensembling for generation tasks (i.e. tasks where the answers is a string of tokens instead of a single one).\nA simple way to perform ensembling in this case is to use standard methods that generate the output based on the ensembled probability of the next word in the answer sequence $P(z_t|\\bm{x},z_{<t}) \\coloneqq \\frac{1}{K} \\sum_{i}^K P(z_t|f_{\\text{prompt},i}(\\bm{x}),z_{<t})$.\nIn contrast,  train a separate model for each prompt $f_{\\text{prompt},i}(\\bm{x})$, and thus storing each of these fine-tuned LMs in memory is infeasible.\nInstead, they first decode generations using each model and then score each generation by averaging their generation probability across all models.", "cites": [7786], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section introduces the concept of prompt ensembling for text generation and briefly explains a uniform averaging method, citing one relevant paper. While it provides a functional explanation of the approach, the synthesis is limited to a single paper, and it does not connect the idea to broader trends or other prompting techniques. There is minimal critical analysis or abstraction, offering mostly a procedural description with slight analytical insight."}}
{"id": "c059e02a-a745-4a3e-a88f-ac8cfa361f6b", "title": "Prompt Augmentation", "level": "subsection", "subsections": ["c68fe2b9-84cf-4608-883f-86c98e3885de"], "parent_id": "1cd618a3-dfb4-4343-aa7c-ce9d90e45ec2", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Multi-Prompt Learning"], ["subsection", "Prompt Augmentation"]], "content": "\\label{sec:prompt-augmentation}\n\\term{Prompt augmentation}, also sometimes called \\term{demonstration learning}~, provides a few additional \\emph{answered prompts} that can be used to demonstrate how the LM should provide the answer to the actual prompt instantiated with the input $\\bm{x}$. \nFor example, instead of just providing a prompt of ``China's capital is \\texttt{[Z]} .'', the prompt can be prefaced by a few examples such as ``Great Britain's capital is London . Japan's capital is Tokyo . China's capital is \\texttt{[Z]} .'' Another example of performing addition of two numbers can be found in Fig.~\\ref{fig:multi-prompt-example}-(b).\nThese few-shot demonstrations take advantage of the ability of strong language models to learn repetitive patterns . \nAlthough the idea of prompt augmentation is simple, there are several aspects that make it challenging:\n(1) \\emph{Sample Selection:} how to choose the most effective examples? (2) \\emph{Sample Ordering:} How to order the chosen examples with the prompt?", "cites": [679, 8744], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of prompt augmentation and its purpose, drawing from two cited papers. It introduces the concept with simple examples but does not deeply synthesize or integrate ideas from the sources. There is no critical evaluation or comparison of the approaches, and the abstraction remains limited to surface-level observations without identifying broader principles or frameworks."}}
{"id": "c68fe2b9-84cf-4608-883f-86c98e3885de", "title": "Sample Selection", "level": "paragraph", "subsections": ["d50ec143-7378-4943-a8d1-573d3f0691bc"], "parent_id": "c059e02a-a745-4a3e-a88f-ac8cfa361f6b", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Multi-Prompt Learning"], ["subsection", "Prompt Augmentation"], ["paragraph", "Sample Selection"]], "content": "Researchers have found that the choice of examples used in this few-shot scenario can result in very different performance, ranging from near state-of-the-art accuracy on some tasks to near random guess . \nTo address this issue,  utilize sentence embeddings to sample examples that are close to the input in this embedding space.\nTo measure the generalization capability of pre-trained LMs to perform new tasks based on instructions,  provide both positive samples and negative samples that highlight things to avoid.", "cites": [8470, 2189, 8744], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from the three cited papers by highlighting the importance of sample selection in few-shot learning and the strategies used (e.g., sentence embeddings, positive/negative examples). It provides some level of analysis by pointing out the performance variability due to example choice, but lacks deeper comparison or critique of these methods. The abstraction is moderate as it generalizes the issue of sample quality and influence on model performance."}}
{"id": "d50ec143-7378-4943-a8d1-573d3f0691bc", "title": "Sample Ordering", "level": "paragraph", "subsections": [], "parent_id": "c68fe2b9-84cf-4608-883f-86c98e3885de", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Multi-Prompt Learning"], ["subsection", "Prompt Augmentation"], ["paragraph", "Sample Selection"], ["paragraph", "Sample Ordering"]], "content": " found that the order of answered prompts provided to the model plays an important role in model performance, and propose entropy-based methods to score different candidate permutations. \n search for a good permutation of training examples as augmented prompts and learn a separator token between the prompts for further gains in performance.\nPrompt augmentation is closely related to retrieval-based methods that provide more textual context to the model to improve performance , a method which has also been shown to be effective in prompt-based learning . However, the key difference lies in the fact that prompt augmentation also leverages the template and answer, while larger context learning does not.", "cites": [4202, 8470, 4204], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from three papers on the impact of sample ordering in prompt-based learning, connecting their findings about the importance of prompt permutation and entropy-based scoring. It offers some analytical perspective by distinguishing prompt augmentation from retrieval-based methods, but the critical evaluation is limited and the abstraction to broader principles is modest."}}
{"id": "0083cedc-b450-47a1-81f6-5b38de36941c", "title": "Prompt Composition", "level": "subsection", "subsections": [], "parent_id": "1cd618a3-dfb4-4343-aa7c-ce9d90e45ec2", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Multi-Prompt Learning"], ["subsection", "Prompt Composition"]], "content": "\\label{sec:6-prompt-composition}\nFor those composable tasks, which can be composed based on more fundamental subtasks,\nwe can also perform \\term{prompt composition}, using multiple sub-prompts, each for one subtask, and then defining a composite prompt based on those sub-prompts. This process is illustrated in Fig.~\\ref{fig:multi-prompt-example}-(c). For example, in the relation extraction task, which aims to extract the relation of two entities, we can break down the task into several subtasks including identifying the characteristics of entities and classifying the relationships between entities. Based on this intuition,  first use multiple manually created sub-prompts for entity recognition and relation classification and then compose them into a complete prompt based on logic rules for relation extraction.", "cites": [4203], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section introduces the concept of prompt composition and provides a simple example using relation extraction, referencing Paper 1 for context. It synthesizes the idea of breaking down tasks into subtasks and using sub-prompts, but lacks deeper integration with other works or a broader framework. There is minimal critical evaluation of the cited paper, and the abstraction level remains limited to task-specific examples without generalizing to overarching principles."}}
{"id": "5165a467-96bd-4114-95cb-4c692f925e14", "title": "Prompt Decomposition", "level": "subsection", "subsections": [], "parent_id": "1cd618a3-dfb4-4343-aa7c-ce9d90e45ec2", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Multi-Prompt Learning"], ["subsection", "Prompt Decomposition"]], "content": "For tasks where multiple predictions should be performed for one sample (e.g., sequence labeling), directly defining a holistic prompt with regards to the entire input text $\\bm{x}$ is challenging. One intuitive method to address this problem is to break down the holistic prompt into different sub-prompts, and then answer each sub-prompt separately.\nFig.\\ref{fig:multi-prompt-example}-(d) illustrates this idea with an example from the named entity recognition task, which aims to identify all named entities in an input sentence.\nIn this case, the input will first be converted into a set of text spans, and the model can then be prompted to predict the entity type (including ``Not an Entity'') for each span.\nIt is not easy to predict all the span types at the same time due to the large number of spans, so different prompts for each span can be created and predicted separately. This sort of \\term{prompt decomposition} for named entity recognition has been explored by  where they apply the approach we discussed here.", "cites": [4198], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section introduces the concept of prompt decomposition, primarily describing its application in named entity recognition. It integrates one cited paper but lacks a broader synthesis of multiple sources or a novel framework. The analysis is minimal, with no critique or evaluation of the approach, and it remains concrete without abstracting to overarching principles or patterns."}}
{"id": "346168dd-1169-402b-a8dc-79caa097cac8", "title": "Training Settings", "level": "subsection", "subsections": [], "parent_id": "2fefdea1-9dcf-4cea-9da4-b6cefa9782c6", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Training Strategies for Prompting Methods"], ["subsection", "Training Settings"]], "content": "In many cases, prompting methods can be used without \\emph{any} explicit training of the LM for the down-stream task, simply taking an LM that has been trained to predict the probability of text $P(\\bm{x})$ and applying it as-is to fill the cloze or prefix prompts defined to specify the task.\nThis is traditionally called the \\term{zero-shot} setting, as there is zero training data for the task of interest.\nHowever, there are also methods that use training data to train the model in concert with prompting methods.\nThese consist of either \\term{full-data learning}, where a reasonably large number of training examples are used to train the model, or \\term{few-shot learning} where a very small number of examples are used to train the model.\nPrompting methods are particularly useful in the latter case, as there are generally not enough training examples to fully specify the desired behavior, and thus using a prompt to push the model in the right direction is particularly effective.\nOne thing to note is that for many of the prompt engineering methods described in \\S\\ref{sec:4-prompt-template-engineering}, although annotated training samples are not explicitly used in the training of the downstream task model, they \\emph{are} often used in the construction or validation of the prompts that the downstream task will use.\nAs noted by , this is arguably not true zero-shot learning with respect to the downstream task.", "cites": [4205], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the concept of zero-shot, few-shot, and full-data learning in the context of prompting methods, connecting them to the broader framework of pre-trained language models. It critically notes that some methods labeled as 'zero-shot' may still rely on annotated data for prompt validation. The abstraction is reasonable, as it identifies the role of training data and prompts in influencing model performance, though it could provide deeper meta-level insights into the implications of these training settings."}}
{"id": "5935d0cc-18d0-454c-8933-54c9f87c1d2e", "title": "Promptless Fine-tuning", "level": "subsubsection", "subsections": [], "parent_id": "8a2e4e04-1dfd-407a-a6cf-49c3f64bfdd3", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Training Strategies for Prompting Methods"], ["subsection", "Parameter Update Methods"], ["subsubsection", "Promptless Fine-tuning"]], "content": "\\label{sec:model-fine-tuning}\nAs mentioned in the introduction, the \\term{pre-train and fine-tune} strategy has been widely used in NLP since before the popularization of prompting methods.\nHere we refer to pre-training and fine-tuning \\emph{without} prompts as \\term{promptless fine-tuning}, to contrast with the prompt-based learning methods introduced in the following sections.\nIn this strategy, given a dataset of a task, all (or some ) of the parameters of the pre-trained LM will be updated via gradients induced from downstream training samples. \nTypical examples of pre-trained models tuned in this way include BERT~\\citenumber{devlin-etal-2019-bert} and RoBERTa~\\citenumber{Liu2019RoBERTaAR}.\nThis is a simple, powerful, and widely-used method, but it may overfit or not learn stably on small datasets~.\nModels are also prone to \\term{catastrophic forgetting}, where the LM loses its ability to do things that it was able to do before fine-tuning .\n\\begin{myboxnote}[]\n\\begin{itemize*}\n    \\item \\textbf{Advantages}: Simplicity, no need for prompt design. Tuning all the LM parameters allows the model to fit to larger training datasets.\n    \\item \\textbf{Disadvantages}: LMs may overfit or not learn stably on smaller datasets.\n\\end{itemize*}\n\\end{myboxnote}", "cites": [8329, 7210, 38], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of promptless fine-tuning and its characteristics, such as simplicity and the risk of overfitting on small datasets. It cites several papers but does not synthesize their findings or connect them into a broader narrative. There is minimal critical analysis or abstraction beyond the specific methods described."}}
{"id": "819e6a92-5f6d-4645-864f-df177ae0f605", "title": "Tuning-free Prompting", "level": "subsubsection", "subsections": [], "parent_id": "8a2e4e04-1dfd-407a-a6cf-49c3f64bfdd3", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Training Strategies for Prompting Methods"], ["subsection", "Parameter Update Methods"], ["subsubsection", "Tuning-free Prompting"]], "content": "\\label{sec:tuning-free-prompting}\n\\term{Tuning-free prompting} directly generates the answers without changing the parameters of the pre-trained LMs based only on a prompt, as described in the simplest incarnation of prompting in \\S\\ref{sec:2-formal-description}.\nThese can be optionally augmenting input with answered prompts as described in \\S\\ref{sec:prompt-augmentation}, and this combination of tuning-free prompting and prompt augmentation is also referred to as \\term{in-context learning} .\nTypical examples of tuning-free prompting include LAMA \\citenumber{petroni-etal-2019-language}  and GPT-3~\\citenumber{brown2020language}.\n\\begin{myboxnote}[]\n\\begin{itemize*}\n    \\item \\textbf{Advantages}: Efficiency, there is no parameter update process. No catastrophic forgetting, as LM parameters remain fixed. Applicable in zero-shot  settings.\n    \\item \\textbf{Disadvantages}: Because prompts are the only method that provide the task specification, heavy engineering is necessary to achieve high accuracy. In particular in the in-context learning setting, providing many answered prompts can be slow at test time, and thus cannot easily use large training datasets.\n\\end{itemize*}\n\\end{myboxnote}", "cites": [679], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of tuning-free prompting, referencing LAMA and GPT-3 as examples, and mentions in-context learning as a related concept. It lists advantages and disadvantages but does not critically analyze the cited papers or synthesize insights across them. The content remains at a surface level without deeper abstraction or integration."}}
{"id": "5cb6f5b2-322e-456d-9772-a8011ca0e50a", "title": "Fixed-prompt LM Tuning", "level": "subsubsection", "subsections": [], "parent_id": "8a2e4e04-1dfd-407a-a6cf-49c3f64bfdd3", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Training Strategies for Prompting Methods"], ["subsection", "Parameter Update Methods"], ["subsubsection", "Fixed-prompt LM Tuning"]], "content": "\\label{sec:prompt-fix-fine-tuning}\n\\term{Fixed-prompt LM tuning} tunes the parameters of the LM, as in the standard pre-train and fine-tune paradigm, but additionally uses prompts with fixed parameters to specify the model behavior.\nThis potentially leads to improvements, particularly in few-shot scenarios.\nThe most natural way to do so is to provide a discrete textual template that is applied to every training and test example.\nTypical examples include PET-TC \\citenumber{schick2021exploiting}, PET-Gen \\citenumber{schick2020fewshot}, LM-BFF \\citenumber{gao2021making}.\n more recently observe that the prompt engineering can be reduced by allowing for a combination of answer engineering and partial LM fine-tuning.\nFor example, they define a very simple template, \\textit{null prompt}, where the input and mask are directly concatenated ``\\texttt{[X][Z]}'' without any template words, and find this achieves competitive accuracy.\n\\begin{myboxnote}[]\n\\begin{itemize*}\n    \\item \\textbf{Advantages}: Prompt or answer engineering more completely specify the task, allowing for more efficient learning, particularly in few-shot scenarios.\n    \\item \\textbf{Disadvantages}: Prompt or answer engineering are still required, although perhaps not as much as without prompting. LMs fine-tuned on one downstream task may not be effective on another one.\n\\end{itemize*}\n\\end{myboxnote}", "cites": [4191], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the concept of fixed-prompt LM tuning by connecting it to both traditional fine-tuning and prompting techniques, showing how it bridges these ideas. It provides a basic level of critical analysis by acknowledging the reduced need for prompt engineering and the limitations of task-specific fine-tuning. While it identifies a broader pattern in reducing prompt engineering through LM fine-tuning, it stops short of deeper abstraction or a novel unifying framework."}}
{"id": "2b5c6d46-2a5a-4133-886f-685feb5d69af", "title": "Factual Probing", "level": "paragraph", "subsections": ["538b4b13-5e25-41aa-8860-e424c32f985b"], "parent_id": "6f4038b2-8198-4897-825f-20ed5bed4032", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Applications"], ["subsection", "Knowledge Probing"], ["paragraph", "Factual Probing"]], "content": "\\term{Factual probing} (a.k.a.~fact retrieval) is one of the earliest scenarios with respect to which prompting methods were applied. The motivation of exploring this task is to quantify how much factual knowledge the pre-trained LM's internal representations bear. In this task, parameters of pre-trained models are usually fixed, and knowledge is retrieved by transforming the original input into a cloze prompt as defined in \\S\\ref{sec:2-basics}, which can be manually crafted or automatically discovered. Relevant datasets including \\texttt{LAMA}  and \\texttt{X-FACTR} . Since the answers are pre-defined, fact retrieval only focuses on finding effective templates and analyzing the results of different models using these templates. Both discrete template search  and continuous template learning  have been explored within this context, as well as prompt ensemble learning .", "cites": [4205, 7225, 4202, 7788, 7787, 7789], "cite_extract_rate": 0.8, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic overview of factual probing, mentioning relevant datasets and methods like discrete and continuous template search. However, it lacks deeper synthesis of ideas across the cited works and does not offer critical analysis or comparison. It remains largely descriptive, summarizing key components without exploring broader implications or identifying trends."}}
{"id": "538b4b13-5e25-41aa-8860-e424c32f985b", "title": "Linguistic Probing", "level": "paragraph", "subsections": [], "parent_id": "2b5c6d46-2a5a-4133-886f-685feb5d69af", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Applications"], ["subsection", "Knowledge Probing"], ["paragraph", "Factual Probing"], ["paragraph", "Linguistic Probing"]], "content": "Besides factual knowledge, large-scale pre-training also allows LMs to handle linguistic phenomena such as analogies , negations , semantic role sensitivity , semantic similarity , cant understanding , and rare word understanding . The above knowledge can also be elicited by presenting \\term{linguistic probing} tasks in the form of natural language sentences that are to be completed by the LM.", "cites": [679], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal description of linguistic probing tasks and the capabilities of LMs without substantial synthesis of the cited paper or additional sources. There is no critical evaluation or comparative discussion of the methods, and no broader patterns or principles are identified. The content remains at a surface level, summarizing concepts rather than offering deep insights."}}
{"id": "4d972898-4ca5-452e-b8aa-e3df4c3c1467", "title": "Classification-based Tasks", "level": "subsection", "subsections": ["a541263b-0e7f-4ac0-a104-c51368258129"], "parent_id": "f8808003-ed28-4ad0-92e8-54ac434d4691", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Applications"], ["subsection", "Classification-based Tasks"]], "content": "Prompt-based learning has been widely explored in classification-based tasks where prompt templates can be constructed relatively easily, such as text classification  and natural language inference .\nThe key to prompting for classification-based tasks is reformulating it as an appropriate prompt.\nFor example,  use a prompt such as ``the topic of this document is \\texttt{[Z]}.'', which is then fed into mask pre-trained LMs for slot filling.", "cites": [2466, 8463], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of how prompt-based learning is applied to classification tasks, mentioning two papers and a general example. It lacks synthesis of ideas across the cited works and offers minimal critical evaluation or abstraction beyond the specific examples given."}}
{"id": "a541263b-0e7f-4ac0-a104-c51368258129", "title": "Text Classification", "level": "paragraph", "subsections": ["24b63ce6-9b58-47f5-87aa-948eb570befa"], "parent_id": "4d972898-4ca5-452e-b8aa-e3df4c3c1467", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Applications"], ["subsection", "Classification-based Tasks"], ["paragraph", "Text Classification"]], "content": "For \\term{text classification} tasks, most previous work has used cloze prompts, and both prompt engineering  and answer engineering  have been explored extensively. \nMost existing works explore the efficacy of prompt learning for text classification in the context of \\term{few-shot} setting with ``\\term{fixed-prompt LM Tuning}'' strategies (defined in \\S\\ref{sec:prompt-fix-fine-tuning}).", "cites": [2466, 4201, 8536, 8744], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of text classification in the context of prompt-based learning, referencing relevant papers but not synthesizing or connecting their contributions in a deeper or cohesive narrative. It lacks critical evaluation of the approaches and does not abstract beyond specific methods to highlight broader trends or principles."}}
{"id": "24b63ce6-9b58-47f5-87aa-948eb570befa", "title": "Natural Language Inference (NLI)", "level": "paragraph", "subsections": [], "parent_id": "a541263b-0e7f-4ac0-a104-c51368258129", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Applications"], ["subsection", "Classification-based Tasks"], ["paragraph", "Text Classification"], ["paragraph", "Natural Language Inference (NLI)"]], "content": "NLI aims to predict the relationship (e.g., \\texttt{entailment}) of two given sentences. Similar to text classification tasks, for \\term{natural language inference} tasks, cloze prompts are commonly used~. Regarding prompt engineering, researchers mainly focus on the template search in the few-shot learning setting and the answer space $\\mathcal{Z}$ is usually manually pre-selected from the vocabulary.\n\\begin{table*}[!ht]\n\\centering\n\\scriptsize\n\\renewcommand\\arraystretch{1.1}\n\\setlength\\tabcolsep{4.5pt}\n\\begin{tabular}{lllllcllclll}\n\\toprule\n                                      &                                     &                                     &                                    & \\multicolumn{3}{c}{\\textbf{Prompt Engineering}}                                                 & \\multicolumn{3}{c}{\\textbf{Answer Engineering}}                                                    &                                   &                                         \\\\\n                                      \\cmidrule(lr){5-7} \\cmidrule(lr){8-10}\n\\multirow{-2}{*}{\\textbf{Work}}       & \\multirow{-2}{*}{\\textbf{Task}}     & \\multirow{-2}{*}{\\textbf{PLM}}      & \\multirow{-2}{*}{\\textbf{Setting}} & \\textbf{Shape}             & \\textbf{Man}                             & \\textbf{Auto}          & \\textbf{Shape}                 & \\textbf{Man}                             & \\textbf{Auto}       & \\multirow{-2}{*}{\\textbf{Tuning}} & \\multirow{-2}{*}{\\textbf{Mul-Pr}} \\\\\n\\midrule\nLMComm \\citenumber{DBLP:journals/corr/abs-1806-02847}                                & CR                                  & L2R                                & Zero                               & Clo                        & \\Checkmark                   & -                      & Sp                             & \\Checkmark                   & -                   & TFP                               & -                                       \\\\\n\\midrule\n                                      & CR,QA                              &                                     &                                    &                            &                                             &                        &                                &                                             &                     &                                   &                                         \\\\\n\\multirow{-2}{*}{GPT-2 \\citenumber{Radford2019LanguageMA}}               & \\cellcolor[HTML]{FFFFFF}SUM,MT     & \\multirow{-2}{*}{GPT-2}             & \\multirow{-2}{*}{Zero,Few}        & \\multirow{-2}{*}{Clo,Pre} & \\multirow{-2}{*}{\\Checkmark} & \\multirow{-2}{*}{-}    & \\multirow{-2}{*}{Tok,Sp,Sen} & \\multirow{-2}{*}{\\Checkmark} & \\multirow{-2}{*}{-} & \\multirow{-2}{*}{TFP}             & \\multirow{-2}{*}{PA}                    \\\\\n\\midrule\nWNLaMPro \\citenumber{DBLP:conf/aaai/SchickS20}                              & LCP                                 & BERT                                & Zero                               & Clo                        & \\Checkmark                   & -                      & Tok                            & \\Checkmark                   & -                   & TFP                               & -                                       \\\\\n\\midrule\nLMDiagnose \\citenumber{DBLP:journals/tacl/Ettinger20}                           & CR,LCP                             & BERT                                & Zero                               & Clo                        & \\Checkmark                   & -                      & Tok                            & \\Checkmark                   & -                   & TFP                               & -                                       \\\\\n\\midrule\nAdvTrigger \\citenumber{DBLP:conf/emnlp/WallaceFKGS19}                            & GCG                                 & GPT-2                               & Full                               & Pre                        & -                                           & Disc                   & Sen                            & \\Checkmark                   & -                   & TFP                               & -                                       \\\\\n\\midrule\nCohRank \\citenumber{DBLP:conf/emnlp/DavisonFR19}                              & CKM                                 & BERT                                & Zero                               & Clo                        & \\Checkmark                   & -                      & Tok,Sp                        & \\Checkmark                   & -                   & TFP                               & -                                       \\\\\n\\midrule\n                                      &                                     & Conv,Trans                         &                                    &                            &                                             &                        &                                &                                             &                     &                                   &                                         \\\\\n\\multirow{-2}{*}{LAMA \\citenumber{petroni-etal-2019-language}}                & \\multirow{-2}{*}{FP}                & \\cellcolor[HTML]{FFFFFF}ELMo,BERT  & \\multirow{-2}{*}{Zero}             & \\multirow{-2}{*}{Clo}      & \\multirow{-2}{*}{\\Checkmark} & \\multirow{-2}{*}{-}    & \\multirow{-2}{*}{Tok}          & \\multirow{-2}{*}{\\Checkmark} & \\multirow{-2}{*}{-} & \\multirow{-2}{*}{TFP}             & \\multirow{-2}{*}{-}                     \\\\\n\\midrule\nCTRL \\citenumber{DBLP:journals/corr/abs-1909-05858}                                 & GCG                                 & CTRL                                & Full                               & Pre                        & \\Checkmark                   & -                      & Sen                            & \\Checkmark                   & -                   & LMT                               & -                                       \\\\\n\\midrule\n                                      & TC,SUM                             &                                     &                                    &                            &                                             &                        &                                &                                             &                     &                                   &                                         \\\\\n\\multirow{-2}{*}{T5 \\citenumber{JMLR:v21:20-074}}                  & \\cellcolor[HTML]{FFFFFF}QA,MT      & \\multirow{-2}{*}{T5}                & \\multirow{-2}{*}{Full}             & \\multirow{-2}{*}{Pre}      & \\multirow{-2}{*}{\\Checkmark} & \\multirow{-2}{*}{-}    & \\multirow{-2}{*}{Tok,Sp,Sen} & \\multirow{-2}{*}{\\Checkmark} & \\multirow{-2}{*}{-} & \\multirow{-2}{*}{LMT}             & \\multirow{-2}{*}{-}                     \\\\\n\\midrule\n                                      &                                     & Trans,ELMo                         &                                    &                            &                                             &                        &                                &                                             &                     &                                   &                                         \\\\\n\\multirow{-2}{*}{Neg \\& Mis \\citenumber{DBLP:conf/acl/KassnerS20}}      & \\multirow{-2}{*}{FP}                & \\cellcolor[HTML]{FFFFFF}BERT        & \\multirow{-2}{*}{Zero}             & \\multirow{-2}{*}{Clo}      & \\multirow{-2}{*}{\\Checkmark} & \\multirow{-2}{*}{-}    & \\multirow{-2}{*}{Tok}          & \\multirow{-2}{*}{\\Checkmark} & \\multirow{-2}{*}{-} & \\multirow{-2}{*}{TFP}             & \\multirow{-2}{*}{-}                     \\\\\n\\midrule\nLPAQA \\citenumber{jiang-etal-2020-know}                                & FP                                  & BERT,ERNIE                         & Full                               & Clo                        & \\Checkmark                   & Disc                   & Tok                            & \\Checkmark                   & -                   & TFP                               & PE                                      \\\\\n\\midrule\nZSC \\citenumber{DBLP:journals/corr/abs-1912-10165}                            & TC                                  & GPT-2                               & Full                               & Pre                        & \\Checkmark                   & -                      & Tok,Sp                        & \\Checkmark                   & -                   & LMT                               & -                                       \\\\\n\\midrule\nPET-TC \\citenumber{schick2021exploiting}                               & TC                                  & RoBERTa,XLM-R                      & Few                                & Pre                        & \\Checkmark                   & -                      & Tok                            & \\Checkmark                   & Disc                   & LMT                               & PE                                      \\\\\n\\midrule\nContxFP \\citenumber{Petroni2020HowCA}                               & FP                                  & BERT,RoBERTa                       & Zero                               & Clo                        & \\Checkmark                   & Disc                   & Tok                            & \\Checkmark                   & -                   & TFP                               & -                                       \\\\\n\\midrule\nUnifiedQA \\citenumber{khashabi-etal-2020-unifiedqa} &\tQA &\tT5,BART &\tFull &\tPrefix &\t\\Checkmark &\t- &\tTok,Sp,Sen &\t\\Checkmark &\t- &\tLMT\t& - \\\\\n\\midrule\nRAG \\citenumber{DBLP:conf/nips/LewisPPPKGKLYR020} & QA,GCG,TC & BART & Full & Pre & - & Disc & Tok,Sp,Sen & \\Checkmark & - & LMPT & PE \\\\\n\\midrule\n                                      & QA,MT,GCG                         &                                     &                                    &                            &                                             &                        &                                &                                             &                     &                                   &                                         \\\\\n                                      & \\cellcolor[HTML]{FFFFFF}CR,TC,LCP &                                     &                                    &                            &                                             &                        &                                &                                             &                     &                                   &                                         \\\\\n\\multirow{-3}{*}{GPT-3 \\citenumber{brown2020language}}               & MR,SR,AR                          & \\multirow{-3}{*}{GPT-3}             & \\multirow{-3}{*}{Zero,Few}        & \\multirow{-3}{*}{Clo,Pre} & \\multirow{-3}{*}{\\Checkmark} & \\multirow{-3}{*}{-}    & \\multirow{-3}{*}{Tok,Sp,Sen} & \\multirow{-3}{*}{\\Checkmark} & \\multirow{-3}{*}{-} & \\multirow{-3}{*}{TFP}             & \\multirow{-3}{*}{PA}                    \\\\\n\\midrule\nCommS2S \\citenumber{yang-etal-2020-designing}                               & CR                                  & T5                                  & Full                               & Pre                        & \\Checkmark                   & -                      & Tok                            & \\Checkmark                   & -                   & LMT                               & -                                       \\\\\n\\midrule\nPET-SGLUE \\citenumber{schick2021its}                        & TC                                  & ALBERT                              & Few                                & Clo                        & \\Checkmark                   & -                      & Tok,Sp                        & \\Checkmark                   & -                   & LMT                               & PE                                      \\\\\n\\midrule\n                                      &                                     & GPT-1,GPT-2                        &                                    &                            &                                             &                        & \\multicolumn{3}{l}{}                                                                               &                                   &                                         \\\\\n\\multirow{-2}{*}{ToxicityPrompts \\citenumber{gehman2020realtoxicityprompts}} & \\multirow{-2}{*}{GCG}               & \\cellcolor[HTML]{FFFFFF}GPT-3,CTRL & \\multirow{-2}{*}{Zero}             & \\multirow{-2}{*}{Pre}      & \\multirow{-2}{*}{\\Checkmark} & \\multirow{-2}{*}{-}    & \\multicolumn{3}{l}{\\multirow{-2}{*}{N/A}}                                                          & \\multirow{-2}{*}{TFP}             & \\multirow{-2}{*}{-}                     \\\\\n\\midrule\nWhyLM \\citenumber{DBLP:conf/iclr/SaunshiMA21}                                & Theory                              & GPT-2                               & Full                               & Pre                        & \\Checkmark                   & -                      & Tok                            & \\Checkmark                   & -                   & PT                               & -                                       \\\\\n\\midrule\n                                      &                                     & mBERT,BERT                         &                                    &                            &                                             &                        &                                &                                             &                     &                                   &                                         \\\\\n\\multirow{-2}{*}{X-FACTR \\citenumber{jiang-etal-2020-x}}             & \\multirow{-2}{*}{FP}                & \\cellcolor[HTML]{FFFFFF}XLM,XLM-R  & \\multirow{-2}{*}{Zero}             & \\multirow{-2}{*}{Clo}      & \\multirow{-2}{*}{\\Checkmark} & \\multirow{-2}{*}{-}    & \\multirow{-2}{*}{Tok,Sp}      & \\multirow{-2}{*}{\\Checkmark} & \\multirow{-2}{*}{-} & \\multirow{-2}{*}{TFP}             & \\multirow{-2}{*}{-}                     \\\\\n\\midrule\nPetal \\citenumber{DBLP:conf/coling/SchickSS20}                                & TC                                  & RoBERTa                             & Few                                & Clo                        & \\Checkmark                   & -                      & Tok                            & -                                           & Disc                & LMT                               & PE                                      \\\\\n\\midrule\nAutoPrompt \\citenumber{autoprompt:emnlp20}                           & TC,FP,IE                          & BERT,RoBERTa                       & Full                               & Clo                        & -                                           & Disc                   & Tok                            & -                                           & Disc                & TFP                               & -                                       \\\\\n\\midrule\nCTRLsum \\citenumber{DBLP:journals/corr/abs-2012-04281}                              & SUM                                 & BART                                & Full                               & Pre                        & \\Checkmark                   & -                      & Sen                            & \\Checkmark                   & -                   & LMT                               & -                                       \\\\\n\\midrule\nPET-Gen \\citenumber{schick2020fewshot}                              & SUM                                 & PEGASUS                             & Few                                & Pre                        & \\Checkmark                   & -                      & Sen                            & \\Checkmark                   & -                   & LMT                               & PE                                      \\\\\n\\midrule\nLM-BFF \\citenumber{gao2021making}                               & TC                                  & RoBERTa                             & Few                                & Clo                        & -                                           & Disc                   & Tok                            & -                                           & Disc                & LMT                               & PE,PA                                  \\\\\n\\midrule\nWARP \\citenumber{Hambardzumyan2021WARPWA}                              & TC                                  & RoBERTa                             & Few,Full                               & Clo,Pre                   & \\Checkmark                   & Cont                   & Tok                            & \\Checkmark                   & Cont                & PT                               & PE                                      \\\\\n\\midrule\nPrefix-Tuning \\citenumber{li2021prefix}                        & D2T,SUM                            & GPT-2,BART                         & Full                               & Pre                        & -                                           & Cont                   & Sen                            & \\Checkmark                   & -                   & PT                               & -                                       \\\\\n\\midrule\nKATE \\citenumber{liu2021makes}                                 & TC,D2T,QA                         & GPT-3                               & Few                                & Pre                        & \\Checkmark                   & -                      & Tok,Sp,Sen                   & \\Checkmark                   & -                   & TFP                               & PA                                      \\\\\n\\midrule\n                                      & MT,MR                              &                                     &                                    &                            &                                             &                        &                                &                                             &                     &                                   &                                         \\\\\n\\multirow{-2}{*}{PromptProg \\citenumber{10.1145/3411763.3451760}}          & \\cellcolor[HTML]{FFFFFF}AR,QA      & \\multirow{-2}{*}{GPT-3}             & \\multirow{-2}{*}{Zero,Few}        & \\multirow{-2}{*}{Pre}      & \\multirow{-2}{*}{\\Checkmark} & \\multirow{-2}{*}{-}    & \\multirow{-2}{*}{Tok,Sp,Sen} & \\multirow{-2}{*}{\\Checkmark} & \\multirow{-2}{*}{-} & \\multirow{-2}{*}{TFP}             & \\multirow{-2}{*}{PA}                    \\\\\n\\midrule\nContxCalibrate \\citenumber{zhao2021calibrate}                       & TC,FP,IE                          & GPT-2,GPT-3                        & Few                                & Pre                        & \\Checkmark                   & -                      & Tok,Sp                        & \\Checkmark                   & -                   & TFP                               & PA                                      \\\\\n\\midrule\nPADA \\citenumber{bendavid2021pada}                                  & TC,TAG                             & T5                                  & Full                               & Pre                        & -                                           & Disc                   & \\multicolumn{3}{l}{N/A}                                                                            & LMPT                                & -                                       \\\\\n\\midrule\nSD \\citenumber{schick2021selfdiagnosis}                                   & GCG                                 & GPT-2                               & Zero                               & Pre                        & \\Checkmark                   & -                      & \\multicolumn{3}{l}{N/A}                                                                            & TFP                               & -                                       \\\\\n\\midrule\nBERTese \\citenumber{haviv-etal-2021-bertese}                              & FP                                  & BERT                                & Full                               & Clo                        & \\Checkmark                   & Disc                   & Tok                            & \\Checkmark                   & -                   & TFP                               & -                                       \\\\\n\\midrule\nPrompt2Data \\citenumber{scao2021many} & TC                              & RoBERTa                             & Full                               & Clo                        & \\Checkmark                   & -                      & Tok,Sp                        & \\Checkmark                   & -                   & LMT                               & -                                       \\\\\n\\midrule\n                                      &                                     & GPT-2,BERT                         &                                    &                            &                                             &                        &                                &                                             &                     &                                   &                                         \\\\\n\\multirow{-2}{*}{P-Tuning \\citenumber{liu2021ptuning}}            & \\multirow{-2}{*}{FP,TC}            & \\cellcolor[HTML]{FFFFFF}ALBERT      & \\multirow{-2}{*}{Few,Full}        & \\multirow{-2}{*}{Clo,Pre} & \\multirow{-2}{*}{\\Checkmark} & \\multirow{-2}{*}{Cont} & \\multirow{-2}{*}{Tok,Sp}      & \\multirow{-2}{*}{\\Checkmark} & \\multirow{-2}{*}{-} & \\multirow{-2}{*}{TFP,LMPT}         & \\multirow{-2}{*}{-}                     \\\\\n\\midrule\nGLM \\citenumber{du2021nlp}                                  & TC                                  & GLM                                 & Full                               & Clo                        & \\Checkmark                   & -                      & Tok,Sp                        & \\Checkmark                   & -                   & LMT                               & -                                       \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\label{tab:papers-part1}An organization of works on prompting (Part 1). See the caption of Tab.~\\ref{tab:papers-part2} for a detailed description for all the abbreviations used in this table.}\n\\end{table*}\n\\begin{table*}[!htbp]\n\\centering\n\\scriptsize\n\\renewcommand\\arraystretch{1.1}\n\\setlength\\tabcolsep{4.1pt}\n\\begin{tabular}{lllllcllclll}\n\\toprule\n                                      &                                     &                                     &                                    & \\multicolumn{3}{c}{\\textbf{Prompt Engineering}}                                                 & \\multicolumn{3}{c}{\\textbf{Answer Engineering}}                                                    &                                   &                                         \\\\\n                                                                            \\cmidrule(lr){5-7} \\cmidrule(lr){8-10}\n\\multirow{-2}{*}{\\textbf{Work}}       & \\multirow{-2}{*}{\\textbf{Task}}     & \\multirow{-2}{*}{\\textbf{PLM}}      & \\multirow{-2}{*}{\\textbf{Setting}} & \\textbf{Shape}             & \\textbf{Man}                             & \\textbf{Auto}          & \\textbf{Shape}                 & \\textbf{Man}                             & \\textbf{Auto}       & \\multirow{-2}{*}{\\textbf{Tuning}} & \\multirow{-2}{*}{\\textbf{Mul-Pr}} \\\\\n\\midrule\nADAPET \\citenumber{tam2021improving}                               & TC                                  & ALBERT                              & Few                                & Clo                        & \\Checkmark                   & -                      & Tok,Sp                        & \\Checkmark                   & -                   & LMT                               & -                                       \\\\\n\\midrule\nMeta \\citenumber{zhong2021meta}                                 & TC                                  & T5                                  & Full                               & Pre                        & \\Checkmark                   & -                      & Tok                            & \\Checkmark                   & -                   & LMT                               & -                                       \\\\\n\\midrule\nOptiPrompt \\citenumber{zhong2021optiprompt}                            & FP                                  & BERT                                & Full                               & Clo                        & \\Checkmark                   & Cont                   & Tok                            & \\Checkmark                   & -                   & PT                               & -                                       \\\\\n\\midrule\n                                      &                                     & BERT,BART                          &                                    &                            &                                             &                        &                                &                                             &                     &                                   &                                         \\\\\n\\multirow{-2}{*}{Soft \\citenumber{qin-eisner-2021-learning}}                & \\multirow{-2}{*}{FP}                & RoBERTa                             & \\multirow{-2}{*}{Full}             & \\multirow{-2}{*}{Clo}      & \\multirow{-2}{*}{\\Checkmark} & \\multirow{-2}{*}{Cont} & \\multirow{-2}{*}{Tok}          & \\multirow{-2}{*}{\\Checkmark} & \\multirow{-2}{*}{-} & \\multirow{-2}{*}{PT}             & \\multirow{-2}{*}{PE}                    \\\\\n\\midrule\nDINO \\citenumber{schick2021generating}                              & GCG                                 & GPT-2                               & Zero                               & Pre                        & \\Checkmark                   & -                      & \\multicolumn{3}{l}{N/A}                                                                            & TFP                               & -                                       \\\\\n\\midrule\nAdaPrompt \\citenumber{DBLP:journals/corr/abs-2104-07650}                            & IE                                  & BERT                                & Few,Full                          & Clo                        & \\Checkmark                   & -                      & Tok                            & -                   & Disc                   & LMT                               & -                                       \\\\\n\\midrule\nPMI$_\\text{DC}$ \\citenumber{holtzman2021surface}                           & GCG,QA,TC                         & GPT-2,GPT-3                        & Zero                               & Pre                        & \\Checkmark                   & -                      & Tok,Sp,Sen                   & \\Checkmark                   & -                   & TFP                               & -                                       \\\\\n\\midrule\nPrompt-Tuning \\citenumber{lester2021power}                          & TC                                  & T5                                  & Full                               & Pre                        & -                                           & Cont                   & Tok,Sp                        & \\Checkmark                   & -                   & PT                               & PE                                      \\\\\n\\midrule\nNatural-Instr \\citenumber{DBLP:journals/corr/abs-2104-08773}                  & GCG                                 & GPT-3,BART                         & Few,Full                          & Pre                        & \\Checkmark                   & -                      & Tok,Sp,Sen                   & \\Checkmark                   & -                   & TFP,LMT                          & PA                                      \\\\\n\\midrule\nOrderEntropy \\citenumber{Lu2021FantasticallyOP}                         & TC                                  & GPT-2,GPT-3                        & Few                                & Pre                        & \\Checkmark                   & -                      & Tok                            & \\Checkmark                   & -                   & TFP                               & PA                                      \\\\\n\\midrule\nFewshotSemp \\citenumber{Shin2021ConstrainedLM}                           & SEMP                                & GPT-3                               & Few                                & Pre                        & \\Checkmark                   & -                      & Sen                            & \\Checkmark                   & -                   & TFP                               & PA                                      \\\\\n\\midrule\n& QA,CR,TC & & & & & & & & & & \\\\\n\\multirow{-2}{*}{PanGu-$\\alpha$ \\citenumber{zeng2021pangualpha}} & SUM,GCG &\t\\multirow{-2}{*}{PanGu-$\\alpha$} & \\multirow{-2}{*}{Zero,Few}\t &\t\\multirow{-2}{*}{Clo,Pre} &\t\\multirow{-2}{*}{\\Checkmark}\t& \\multirow{-2}{*}{-}\t & \\multirow{-2}{*}{Tok,Sp,Sen} &\\multirow{-2}{*}{\\Checkmark} & \\multirow{-2}{*}{-} &\t\\multirow{-2}{*}{TFP} &\t\\multirow{-2}{*}{PA} \\\\\n\\midrule\n                                      &                                     & GPT-2,GPT-3                        &                                    &                            &                                             &                        &                                &                                             &                     &                                   &                                         \\\\\n\\multirow{-2}{*}{TrueFewshot \\citenumber{perez2021true}}         & \\multirow{-2}{*}{TC,FP}            & \\cellcolor[HTML]{FFFFFF}ALBERT      & \\multirow{-2}{*}{Few}              & \\multirow{-2}{*}{Clo,Pre} & \\multirow{-2}{*}{\\Checkmark} & \\multirow{-2}{*}{Disc} & \\multirow{-2}{*}{Tok,Sp}      & \\multirow{-2}{*}{\\Checkmark} & \\multirow{-2}{*}{-} & \\multirow{-2}{*}{TFP,LMT}        & \\multirow{-2}{*}{-}                     \\\\\n\\midrule\nPTR \\citenumber{han2021ptr}                                   & IE                                  & RoBERTa                             & Full                               & Clo                        & \\Checkmark                   & Cont                   & Tok,Sp                        & \\Checkmark                   & -                   & LMPT                                & PC                                      \\\\\n\\midrule\nTemplateNER \\citenumber{cui2021templatebased}                           & TAG                                 & BART                                & Few,Full                          & Clo,Pre                   & \\Checkmark                   & -                      & Tok                            & \\Checkmark                   & -                   & LMT                               & PD                                      \\\\\n\\midrule\nPERO \\citenumber{kumar2021reordering}                                  & TC,FP                              & BERT,RoBERTa                      & Few                                & Pre                        & \\Checkmark                   & -                      & Tok                            & \\Checkmark                   & -                   & TFP                               & PA                                      \\\\\n\\midrule\nPromptAnalysis \\citenumber{wei2021pretrained}                        & Theory                                  & BERT                                & Full                               & Clo                        & -                                           & Cont                   & \\multicolumn{3}{l}{N/A}                                                                            & PT                               & -                                       \\\\\n\\midrule\n& QA,MR,SUM & & & & & & & & & & \\\\\n\\multirow{-2}{*}{CPM-2 \\citenumber{DBLP:journals/corr/abs-2106-10715}} &\nTC,GCG,MT & \n\\multirow{-2}{*}{CPM-2} &\n\\multirow{-2}{*}{Full} & \n\\multirow{-2}{*}{Pre} &\n\\multirow{-2}{*}{-} &\n\\multirow{-2}{*}{Cont} & \n\\multirow{-2}{*}{Tok,Sp,Sent} &\n\\multirow{-2}{*}{\\Checkmark} &\n\\multirow{-2}{*}{-} & \n\\multirow{-2}{*}{PT,LMPT} & \n\\multirow{-2}{*}{-} \\\\\n\\midrule\nBARTScore \\citenumber{yuan2021bartscore}                             & EVALG                               & BART                                & Zero                               & Pre                        & \\Checkmark                   & Disc                   & Sen                            & \\Checkmark                   & -                   & TFP                               & PE                                      \\\\\n\\midrule\nNullPrompt \\citenumber{logan2021cutting}                           & TC                                  & RoBERTa,ALBERT                     & Few                                & Pre                        & \\Checkmark                   & -                      & Tok                            & \\Checkmark                   & -                   & LMPT                                & -                                       \\\\\n\\midrule\nFrozen \\citenumber{DBLP:journals/corr/abs-2106-13884}                               & VQA,VFP,MG                        & GPT-like                            & Full                               & Pre                        & -                                           & Cont                   & Sp (Visual)                    & \\Checkmark                   & -                   & PT                              & PA       \\\\\n\\midrule\n& TC,LCP,NLI & & & & & & & & & & \\\\\n& CR,QA,SUM & & & & & & & & & & \\\\\n\\multirow{-3}{*}{ERNIE-B3 \\citenumber{DBLP:journals/corr/abs-2107-02137}} & GCG & \\multirow{-3}{*}{ERNIE-B3} &\t\\multirow{-3}{*}{Zero} &\t\\multirow{-3}{*}{Clo,Pre} &\t\\multirow{-3}{*}{\\Checkmark} &\t\\multirow{-3}{*}{-} &\t\\multirow{-3}{*}{Tok,Sp,Sen} &\t\\multirow{-3}{*}{\\Checkmark} &\t\\multirow{-3}{*}{-} &\t\\multirow{-3}{*}{TFP} & \\multirow{-3}{*}{-}\\\\\n\\midrule\n& & & Zero,Few & & & & & & & & \\\\\n\\multirow{-2}{*}{Codex \\citenumber{chen2021evaluating}} &\t\\multirow{-2}{*}{CodeGen} &\t\\multirow{-2}{*}{GPT} &\tFull &\t\\multirow{-2}{*}{Pre} &\t\\multirow{-2}{*}{\\Checkmark }&\t\\multirow{-2}{*}{-} &\t\\multirow{-2}{*}{Span} &\t\\multirow{-2}{*}{\\Checkmark} &\t\\multirow{-2}{*}{Disc} &\t\\multirow{-2}{*}{TFP,LMT}\t & \\multirow{-2}{*}{PA} \\\\\n\\midrule\n& & & Zero,Few & & & & & & & & \\\\\n\\multirow{-2}{*}{HTLM \\citenumber{aghajanyan2021htlm}} & \n\\multirow{-2}{*}{TC,SUM} &\n\\multirow{-2}{*}{BART} & \nFull &\n\\multirow{-2}{*}{Clo} &\n\\multirow{-2}{*}{\\Checkmark} &\n\\multirow{-2}{*}{Disc} &\n\\multirow{-2}{*}{Tok,Sp,Sen} &\n\\multirow{-2}{*}{\\Checkmark} &\n\\multirow{-2}{*}{-} &\n\\multirow{-2}{*}{LMT} &\n\\multirow{-2}{*}{PA} \\\\\n\\midrule\nFLEX \\citenumber{DBLP:journals/corr/abs-2107-07170} & TC & T5 & Zero,Few & Pre & \\Checkmark & - & Tok,Sp & \\Checkmark & - & LMT & - \\\\ \n\\bottomrule\n\\end{tabular}\n\\caption{\\label{tab:papers-part2}An organization of works on prompting (Part 2). The \\textbf{Task} column lists the tasks that are performed in corresponding papers. We use the following abbreviations. \\textbf{CR}: Commonsense Reasoning. \\textbf{QA}: Question Answering. \\textbf{SUM}: Summarization. \\textbf{MT}: Machine Translation. \\textbf{LCP}: Linguistic Capacity Probing. \\textbf{GCG}: General Conditional Generation. \\textbf{CKM}: Commonsense Knowledge Mining. \\textbf{FP}: Fact Probing. \\textbf{TC}: Text Classification. \\textbf{MR}: Mathematical Reasoning. \\textbf{SR}: Symbolic Reasoning. \\textbf{AR}: Analogical Reasoning. \\textbf{Theory}: Theoretical Analysis. \\textbf{IE}: Information Extraction. \\textbf{D2T}: Data-to-text. \\textbf{TAG}: Sequence Tagging. \\textbf{SEMP}: Semantic Parsing. \\textbf{EVALG}: Evaluation of Text Generation. \\textbf{VQA}: Visual Question Answering. \\textbf{VFP}: Visual Fact Probing. \\textbf{MG}: Multimodal Grounding. \\textbf{CodeGen}: Code generation. The \\textbf{PLM} column lists all the pre-trained LMs that have been used in corresponding papers for downstream tasks. \\textbf{GPT-like} is an autoregressive language model which makes small modifications to the original GPT-2 architecture. For other pre-trained LMs, please refer to \\S\\ref{sec:lm} for more information. \\textbf{Setting} column lists the settings for prompt-based learning, can be zero-shot learning (\\textbf{Zero}), few-shot learning (\\textbf{Few}), fully supervised learning (\\textbf{Full}). Under \\textbf{Prompt Engineering}, \\textbf{Shape} denotes the shape of the template (\\textbf{Clo} for cloze and \\textbf{Pre} for prefix), \\textbf{Man} denotes whether human effort is needed, \\textbf{Auto} denotes data-driven search methods (\\textbf{Disc} for discrete search, \\textbf{Cont} for continuous search). Under \\textbf{Answer Engineering}, \\textbf{Shape} indicates the shape of the answer (\\textbf{Tok} for token-level, \\textbf{Sp} for span-level, \\textbf{Sen} for sentence- or document-level), and \\textbf{Man} and \\textbf{Auto} are the same as above. The \\textbf{Tuning} column lists tuning strategies (\\S\\ref{sec:tuning}). \\textbf{TFP}: Tuning-free Prompting. \\textbf{LMT}: Fixed-prompt LM Tuning. \\textbf{PT}: Fixed-LM Prompt Tuning. \\textbf{LMPT}: LM+Prompt Tuning. The \\textbf{Mul-Pr} column lists multi-prompt learning methods. \\textbf{PA}: Prompt Augmentation.  \\textbf{PE}: Prompt Ensembling. \\textbf{PC}: Prompt Composition. \\textbf{PD}: Prompt Decomposition.} \n\\end{table*}\n\\clearpage", "cites": [2466], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of how prompt-based methods are applied to NLI tasks, listing several works with their respective settings, prompt and answer engineering approaches. However, it lacks synthesis by not connecting the cited works into a broader narrative, offers minimal critical analysis of their strengths or limitations, and provides little abstraction beyond the specific systems mentioned."}}
{"id": "f7c2c125-c639-479f-aac7-f5de1b7ebfd5", "title": "Relation Extraction", "level": "paragraph", "subsections": ["a72f67ce-ec13-4f05-9f9f-5e3fad4a76d0", "adc5c6a6-d5d7-4d82-9ef8-f5298254c391"], "parent_id": "0e8fead6-1f4e-44a8-ac3d-dc3f195f7182", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Applications"], ["subsection", "Information Extraction"], ["paragraph", "Relation Extraction"]], "content": "\\term{Relation extraction} is a task of predicting the relation between two entities in a sentence.\n first explored the application of \\term{fixed-prompt LM Tuning} in relation extraction and discuss two major challenges that hinder the direct inheritance of prompting methodology from classification tasks: (1) The larger label space (e.g.~80 in relation extraction v.s 2 in binary sentiment classification) results in more difficulty in answer engineering. (2) In relation extraction, different tokens in the input sentence may be more or less important (e.g.~entity mentions are more likely to participate in a relation), which, however, can not be easily reflected in the prompt templates for classification since the original prompt template regards each word equally. To address the above problems,  propose an adaptive answer selection method to address the issue (1) and task-oriented prompt template construction for the issue (2), where they use special markers (e.g.~\\texttt{[E]}) to\nhighlight the entity mentions in the template. Similarly,  incorporate entity type information via multiple prompt composition techniques (illustrated in Fig.~\\ref{fig:multi-prompt-example}).", "cites": [4203], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of relation extraction in the context of prompting methods, highlighting key challenges and how specific papers address them. It synthesizes the core issues (label space and token importance) and links them to proposed solutions like adaptive answer selection and task-oriented prompt templates. While it offers some abstraction by identifying general patterns in the adaptation of prompting for relation extraction, it lacks deeper critical evaluation or a novel framework that would elevate the insight level to high."}}
{"id": "a72f67ce-ec13-4f05-9f9f-5e3fad4a76d0", "title": "Semantic Parsing", "level": "paragraph", "subsections": [], "parent_id": "f7c2c125-c639-479f-aac7-f5de1b7ebfd5", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Applications"], ["subsection", "Information Extraction"], ["paragraph", "Relation Extraction"], ["paragraph", "Semantic Parsing"]], "content": "\\term{Semantic parsing} is a task of generating a structured meaning representation given a natural language input.\n explore the task of few-shot semantic parsing using LMs by (1) framing the semantic parsing task as a paraphrasing task  and (2) constraining the decoding process by only allowing output valid according to a grammar. They experiment with the \\term{in-context learning} setting described in \\S\\ref{sec:tuning-free-prompting}, choosing answered prompts that are semantically close to a given test example (determined by the conditional generation probability of generating a test sample given another training example). The results demonstrate the effectiveness of the paraphrasing reformulation for semantic parsing tasks using pre-trained LMs.", "cites": [318, 8747], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from Paper 2 to explain how semantic parsing can be approached using pre-trained language models with paraphrasing and constrained decoding. It connects these methods to the broader prompting framework discussed earlier in the survey. However, the critical analysis is limited, as it primarily summarizes the results without evaluating their implications or limitations. The abstraction is moderate, identifying the general use of paraphrasing and decoding constraints in few-shot settings."}}
{"id": "adc5c6a6-d5d7-4d82-9ef8-f5298254c391", "title": "Named Entity Recognition", "level": "paragraph", "subsections": [], "parent_id": "f7c2c125-c639-479f-aac7-f5de1b7ebfd5", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Applications"], ["subsection", "Information Extraction"], ["paragraph", "Relation Extraction"], ["paragraph", "Named Entity Recognition"]], "content": "\\term{Named entity recognition} (NER) is a task of identifying named entities (e.g., person name, location) in a given sentence.\nThe difficulty of prompt-based learning's application to tagging tasks, exemplified as NER, is that, unlike classification, (1) each unit to be predicted is a token or span instead of the whole input text, (2) there is a latent relationship between the token labels in the sample context.\nOverall, the application of prompt-based learning in tagging task has not been fully explored.\n recently propose a template-based NER model using BART, which enumerates text spans and considers the generation probability of each type within manually crafted templates. For example, given an input ``Mike went to New York yesterday\", to determine what type of entity ``Mike\" is, they use the template ``Mike is a \\texttt{[Z]} entity\", and the answer space $\\mathcal{Z}$ consists of values such as ``person'' or ``organization''.", "cites": [4198], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of prompt-based approaches to Named Entity Recognition, focusing on a single paper that uses BART with manually crafted templates. It lacks synthesis of broader trends or connections to other works, and offers little critical analysis or abstraction beyond the specific method described."}}
{"id": "ae967515-5552-47c3-a36a-7465a564eb60", "title": "``Reasoning'' in NLP", "level": "subsection", "subsections": ["90a01848-cefd-41a9-9cb1-738e1a8ce897"], "parent_id": "f8808003-ed28-4ad0-92e8-54ac434d4691", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Applications"], ["subsection", "``Reasoning'' in NLP"]], "content": "There is still a debate\\footnote{e.g.~\\url{https://medium.com/reconstruct-inc/the-golden-age-of-computer-vision-338da3e471d1}} about if deep neural networks are capable of performing ``reasoning'' or just memorizing patterns based on large training data~.\nAs such, there have been a number of attempts to probe models' reasoning ability by defining benchmark tasks that span different scenarios.\nWe detail below how prompting methods have been used in these tasks.", "cites": [4115], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly introduces the concept of reasoning in NLP and mentions a single cited paper without integrating it into a broader discussion or narrative. It lacks critical evaluation of the paper or comparison with other works, and does not abstract or generalize the findings into a meta-level understanding of prompting methods in reasoning tasks."}}
{"id": "90a01848-cefd-41a9-9cb1-738e1a8ce897", "title": "Commonsense Reasoning", "level": "paragraph", "subsections": ["0777a298-f500-403d-9782-27dc5f72b61e"], "parent_id": "ae967515-5552-47c3-a36a-7465a564eb60", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Applications"], ["subsection", "``Reasoning'' in NLP"], ["paragraph", "Commonsense Reasoning"]], "content": "There are a number of benchmark datasets testing commonsense reasoning in NLP systems~.\nSome commonly attempted tasks involve solving Winograd Schemas , which require the model to identify the antecedent of an ambiguous pronoun within context, or involve completing a sentence given multiple choices. For the former, an example could be ``The trophy doesnt fit into the brown suitcase because it is too large.\" And the task for the model is to infer whether ``it\" refers to the trophy or the ``suitcase\". By replacing ``it\" with its potential candidates in the original sentences and calculating the probability of the different choices, pre-trained LMs can perform quite well by choosing the choice that achieves the highest probability . For the latter, an example could be ``Eleanor offered to fix her visitor some coffee. Then she realized she didnt have a clean \\texttt{[Z]}.\". The candidate choices are ``cup\", ``bowl\" and ``spoon\". The task for the pre-trained LM is to choose the one from the three candidates that most conforms to common sense. For these kinds of tasks, we can also score the generation probability of each candidate and choose the one with the highest probability .", "cites": [4192, 4206], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of commonsense reasoning tasks, such as Winograd Schemas and sentence completion, and explains how pre-trained language models can be used to solve them. However, it lacks synthesis of multiple cited papers, offers minimal critical evaluation beyond stating effectiveness, and does not generalize to broader principles or frameworks in prompting methods."}}
{"id": "0777a298-f500-403d-9782-27dc5f72b61e", "title": "Mathematical Reasoning", "level": "paragraph", "subsections": [], "parent_id": "90a01848-cefd-41a9-9cb1-738e1a8ce897", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Applications"], ["subsection", "``Reasoning'' in NLP"], ["paragraph", "Commonsense Reasoning"], ["paragraph", "Mathematical Reasoning"]], "content": "Mathematical reasoning is the ability to solve mathematical problems, e.g. arithmetic addition, function evaluation. Within the context of pre-trained LMs, researchers have found that pre-trained embeddings and LMs can perform simple operations such as addition and subtraction when the number of digits is small, but fail when the numbers are larger .  explore more complex mathematical (e.g.~$f(\\bm{x}) = \\bm{x} * \\bm{x}$, what is $f(f(3))$?) reasoning problems and improve LM performance through serializing reasoning for the question.", "cites": [4192, 679, 8748], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a minimal synthesis of the cited papers, merely stating that LMs struggle with larger numbers and that some approaches improve performance via serializing reasoning. There is little critical evaluation of the cited works, and no clear abstraction to broader principles in mathematical reasoning or prompting techniques."}}
{"id": "f6336ace-6e6d-4c6b-ab74-92959c3ad960", "title": "Question Answering", "level": "subsection", "subsections": [], "parent_id": "f8808003-ed28-4ad0-92e8-54ac434d4691", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Applications"], ["subsection", "Question Answering"]], "content": "Question answering (QA) aims to answer a given input question, often based on a context document.\nQA can take a variety of formats, such as extractive QA (which identifies content from the context document containing the answer; e.g. SQuAD ), multiple-choice QA (where the model has to pick from several choices; e.g. RACE ), and free-form QA (where the model can return an arbitrary textual string as a response; e.g. NarrativeQA~).\nGenerally, these different formats have been handled using different modeling frameworks. \nOne benefit of solving QA problems with LMs, potentially using prompting methods, is that different formats of QA tasks can be solved within the same framework.\nFor example,  reformulate many QA tasks as a text generation problem by fine-tuning seq2seq-based pre-trained models (e.g.~T5) and appropriate prompts from the context and questions.  take a closer look at such prompt-based QA systems using sequence to sequence pre-trained models (T5, BART, GPT2) and observe that probabilities from these pre-trained models on QA tasks are not very predictive of whether the model is correct or not.", "cites": [4192, 4198], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of how prompting methods are applied to QA tasks and mentions the use of sequence-to-sequence models. However, it lacks deeper synthesis of the cited works, does not clearly compare approaches, and offers minimal abstraction or meta-level insight. The critical analysis is limited to a single observation about predictive probabilities without broader evaluation."}}
{"id": "07aa9582-e545-477a-92ab-7aa15a3d4b24", "title": "Text Generation", "level": "subsection", "subsections": [], "parent_id": "f8808003-ed28-4ad0-92e8-54ac434d4691", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Applications"], ["subsection", "Text Generation"]], "content": "Text generation is a family of tasks that involve generating text, usually conditioned on some other piece of information.\nPrompting methods can be easily applied to these tasks by using \\term{prefix prompts} together with autoregressive pre-trained LMs.\n demonstrated impressive ability of such models to perform generation tasks such as text summarization and machine translation using prompts such as ``{translate to\nfrench}, \\texttt{[X]}, \\texttt{[Z]}''.  perform \\term{in-context learning} (\\S\\ref{sec:tuning-free-prompting}) for text generation, creating a prompt with manual templates and augmenting the input with multiple \\term{answered prompts}.  explore \\term{fixed-prompt LM tuning} (\\S\\ref{sec:prompt-fix-fine-tuning}) for few-shot text summarization with manually crafted templates.\n investigate \\term{fixed-LM prompt tuning} (\\S\\ref{sec:prompt-only-tuning}) for text summarization and data-to-text generation in few-shot settings, where learnable prefix tokens are prepended to the input while parameters in pre-trained models are kept frozen.\n explored the \\term{prompt+LM tuning}  strategy (\\S\\ref{sec:prompt-fine-tuning}) on text summarization task, where learnable prefix prompts are used and initialized by different types of guidance signals, which can then be updated together with parameters of pre-trained LMs.", "cites": [679, 7786, 1591], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the application of various prompting methods to text generation tasks, with brief mentions of different strategies from the cited papers. It integrates the methods in a task-specific context but lacks deeper synthesis or comparison of how these approaches relate to each other. There is little critical evaluation or abstraction to broader principles in prompting for generation."}}
{"id": "c66d3e36-6a69-4771-8681-9dbcd1a1db68", "title": "Automatic Evaluation of Text Generation", "level": "subsection", "subsections": [], "parent_id": "f8808003-ed28-4ad0-92e8-54ac434d4691", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Applications"], ["subsection", "Automatic Evaluation of Text Generation"]], "content": " have demonstrated that prompt learning can be used for automated evaluation of generated texts.\nSpecifically, they conceptualize the evaluation of generated text as a text generation problem, modeled using a pre-trained sequence-to-sequence, and then use \\term{prefix prompts} that bring\nthe evaluation task closer to the pre-training task.\nThey experimentally find that simply adding the phrase ``such as'' to the translated text when using pre-trained models can lead to a significant improvement in correlation on German-English\nmachine translation (MT) evaluation.", "cites": [4199], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal synthesis of the cited paper, merely paraphrasing its approach without connecting it to broader trends or other methods in prompting. It lacks critical analysis, such as evaluating the effectiveness or limitations of the method. Additionally, it offers little abstraction, remaining focused on a specific example without identifying general principles or patterns in the use of prompting for evaluation."}}
{"id": "f61d289d-2eef-4d9a-a0cf-4125bf400721", "title": "Multi-modal Learning", "level": "subsection", "subsections": [], "parent_id": "f8808003-ed28-4ad0-92e8-54ac434d4691", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Applications"], ["subsection", "Multi-modal Learning"]], "content": " shift the application of prompt learning from text-based NLP to the \\term{multi-modal} setting (vision and language).\n Generally, they adopt the \\term{fixed-LM prompt tuning} strategy together with \\term{prompt augmentation} techniques.\n They specifically represent each image\nas a sequence of continuous embeddings, and a pre-trained LM whose parameters are frozen is prompted with this prefix to generate texts such as image captions. \nEmpirical results show few-shot learning ability: with the help of a few demonstrations (answered prompts), system can rapidly learn words for new objects and novel visual categories.", "cites": [3003], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from the cited paper to explain how prompt learning is extended to multi-modal settings. It introduces key concepts such as fixed-LM prompt tuning and prompt augmentation, showing some level of abstraction by highlighting the generalization of few-shot learning capabilities. However, it lacks deeper critical evaluation or comparison of methods and focuses mainly on summarizing the approach and empirical results."}}
{"id": "2695b92c-4f85-4b7a-8390-609cf12d8fa2", "title": "Debiasing", "level": "paragraph", "subsections": [], "parent_id": "4ecff355-e8ee-42b9-83ff-7830a9ba9358", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Applications"], ["subsection", "Meta-Applications"], ["paragraph", "Domain Adaptation"], ["paragraph", "Debiasing"]], "content": " found that LMs can perform self-diagnosis and self-debiasing based on biased or debiased instructions. For example, to self-diagnosis whether the generated text contains violent information, we can use the following template ``The following text contains violence. \\texttt{[X]}\\texttt{[Z]}\". Then we fill \\texttt{[X]} with the input text and look at the generation probability at \\texttt{[Z]}, if the probability of ``Yes\" is greater than ``No\", then we would assume the given text contains violence, and vice versa. To perform debiasing when generating text, we first compute the probability of the next word $P(x_t|\\bm{x}_{<t}; \\theta)$ given the original input. Then we compute the probability of next word $P(x_t | [\\bm{x}_{<t};\\bm{x}_{\\text{diagnosis}}];\\theta)$ by appending self-diagnosis textual input to the original input as mentioned above. These two probability distributions for the next token can be combined to suppress the undesired attribute.", "cites": [9101], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the concept of self-diagnosis and self-debiasing from the cited paper and explains it through a concrete example, showing reasonable integration. It provides an analytical explanation of how debiasing can be operationalized, but lacks deeper critical evaluation of the method's limitations or trade-offs. The abstraction is modest, as it generalizes the approach but does not place it within a broader conceptual framework of debiasing in NLP."}}
{"id": "91e8c09e-a7d3-4ff9-948e-3075e34c7f63", "title": "Dataset Construction", "level": "paragraph", "subsections": [], "parent_id": "4ecff355-e8ee-42b9-83ff-7830a9ba9358", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Applications"], ["subsection", "Meta-Applications"], ["paragraph", "Domain Adaptation"], ["paragraph", "Dataset Construction"]], "content": " propose to use pre-trained LMs to generate datasets given certain instructions. As an example, suppose we have an unlabeled dataset in which each sample is a sentence. If we want to construct a dataset containing pairs of semantically similar sentences, then we can use the following template for each input sentence: ``Write two sentences that mean the same thing. \\texttt{[X]}\\texttt{[Z]}\" and attempt to generate a sentence that shares the same meaning as the input sentence.", "cites": [9140], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic descriptive example of using pre-trained language models for dataset construction, drawing from one cited paper. It lacks synthesis of multiple sources, critical evaluation of methods or limitations, and abstraction to broader principles or trends. The explanation is concrete and limited in analytical depth."}}
{"id": "378a8314-912e-450c-b832-b5de56375d6a", "title": "Few-shot Learning", "level": "paragraph", "subsections": ["10117453-5239-4fed-865c-4bba6487a9fe", "aada0734-d382-4eca-9dc6-45e8525cccae", "af3ed926-3894-47f4-9e2f-44bb16cc4d32", "b0c77b75-0188-48c9-90e7-690c9b669115", "a3a73f26-da6e-4c17-99de-1e25ac1b21b4", "c0a441dc-ced4-46c8-895a-20008b7fbfd1"], "parent_id": "c56a1962-f178-453d-b20c-ed15741cb96f", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Prompt-relevant Topics"], ["paragraph", "Ensemble Learning"], ["paragraph", "Few-shot Learning"]], "content": "\\term{Few-shot learning} aims to learn a machine learning system in the data-scarce scenarios with few training samples. \nThere are a wide variety of methods to achieve few-shot learning including model agnostic meta-learning~ (learning features rapidly adaptable to new tasks), embedding learning~ (embedding each sample in a lower-dimensional space where similar samples are close together), memory-based learning~ (representing each sample by a weighted average of contents from the memory) etc.~.\nPrompt augmentation can be regarded as another way to achieve few-shot learning (a.k.a.~priming-based few-shot learning ).\nCompared to previous methods, prompt augmentation directly prepends several labeled samples to the currently-processed sample elicit knowledge from pre-trained LMs even without any parameter tuning.", "cites": [677, 1695, 3624, 4207, 4204], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of few-shot learning by categorizing existing methods and positioning prompt augmentation as a distinct approach. It integrates ideas from multiple papers to form a coherent narrative but does so at a moderate level without a novel unifying framework. The critical analysis is limited, as it mainly contrasts prompt augmentation with other methods without in-depth evaluation of their strengths and weaknesses."}}
{"id": "10117453-5239-4fed-865c-4bba6487a9fe", "title": "Larger-context Learning", "level": "paragraph", "subsections": [], "parent_id": "378a8314-912e-450c-b832-b5de56375d6a", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Prompt-relevant Topics"], ["paragraph", "Ensemble Learning"], ["paragraph", "Few-shot Learning"], ["paragraph", "Larger-context Learning"]], "content": "\\term{Larger-context learning} aims to improve the system's performance by augmenting the input with additional contextual information, e.g.~retrieved from the training set  or external data sources .\nPrompt augmentation can be regarded as adding relevant labeled samples into the input, but a minor difference is in larger-context learning, the introduced context is not necessarily labeled data.", "cites": [7790], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a minimal description of the concept of larger-context learning and references a single paper (REALM) without elaborating on how it fits within the broader prompting paradigm. It lacks synthesis with other cited works, critical evaluation of the method, and higher-level abstraction or generalization of principles."}}
{"id": "aada0734-d382-4eca-9dc6-45e8525cccae", "title": "Query Reformulation", "level": "paragraph", "subsections": [], "parent_id": "378a8314-912e-450c-b832-b5de56375d6a", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Prompt-relevant Topics"], ["paragraph", "Ensemble Learning"], ["paragraph", "Few-shot Learning"], ["paragraph", "Query Reformulation"]], "content": "\\term{Query reformulation}  is commonly used in information retrieval  and question answering tasks , which aim to elicit more relevant texts (documents or answers) by expanding the input query with related query terms  or generating paraphrases.\nThere are several commonalities between prompt-based learning and query reformulation, for example (1) both aim to make better use of some existing knowledge bases by asking a right questions\n(2) the knowledge bases are usually a black-box, not available to the users, so researchers must learn how to probe it optimally based on solely questions.\nThere are also differences:\nthe knowledge base in traditional query reformulation problems is usually a search engine , or QA system . By contrast,  for prompt-based learning, we usually define this knowledge base as an LM, and need to find the appropriate query to elicit an appropriate answer from it.\n The input reformulation in prompt learning has changed the form of tasks. For example, an original text classification task has been converted into a cloze question problem, therefore bringing  additional complexity regarding how to (1) make an appropriate task formulation, and (2) change the modeling framework accordingly.\n These steps are not required in traditional query formulation.\nDespite these discrepancies, some methodologies from query reformulation research still can be borrowed for prompt learning, such as decomposing input query into multiple sub-queries , similar to prompt decomposition.", "cites": [4208, 8749], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section draws a conceptual link between query reformulation in information retrieval and prompt-based learning, highlighting both similarities and differences. It integrates the cited papers to explain how reformulation strategies can be adapted to prompting, but the analysis remains focused on general observations rather than deep synthesis or critique. The abstraction level is moderate as it identifies task reformulation as a broader theme."}}
{"id": "af3ed926-3894-47f4-9e2f-44bb16cc4d32", "title": "QA-based Task Formulation", "level": "paragraph", "subsections": [], "parent_id": "378a8314-912e-450c-b832-b5de56375d6a", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Prompt-relevant Topics"], ["paragraph", "Ensemble Learning"], ["paragraph", "Few-shot Learning"], ["paragraph", "QA-based Task Formulation"]], "content": "\\term{QA-based task formulation} aims to conceptualize different NLP tasks as a question-answering problem.\n are earlier works that attempt to unify multiple NLP tasks into a QA framework.\nLater, this idea has been further explored in information extraction  and text classification .\nThese methods are very similar to the prompting methods introduced here in that they use textual questions to specify which task is to be performed.\nHowever, one of the key points of prompting methods is how to better use the knowledge in pre-trained LMs, and these were not covered extensively on previous works advocating for QA formulations.", "cites": [4193, 2653, 4209], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the concept of QA-based task formulation by connecting earlier works in multitask learning and text classification to the broader theme of prompting methods. It identifies a similarity between QA formulations and prompting methods but also highlights a critical difference: the focus of prompting methods on leveraging pre-trained language models. The abstraction is moderate, as it generalizes the QA frameworks use in NLP tasks but does not provide a deep meta-level analysis."}}
{"id": "b0c77b75-0188-48c9-90e7-690c9b669115", "title": "Controlled Generation", "level": "paragraph", "subsections": [], "parent_id": "378a8314-912e-450c-b832-b5de56375d6a", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Prompt-relevant Topics"], ["paragraph", "Ensemble Learning"], ["paragraph", "Few-shot Learning"], ["paragraph", "Controlled Generation"]], "content": "\\term{Controlled generation} aims to incorporate various types of guidance beyond the input text into the generation model .\nSpecifically, the guidance signals could be \\emph{style tokens} , \\emph{length specifications} , \\emph{domain tags} , or any variety of other pieces of information used to control of the generated text.\nIt could also be \\textit{keywords} , \\textit{relation triples}  or even \\textit{highlighted phrases or sentences}  to plan the content of generated texts.\nIn a way, many of the prompting methods described here are a type of controllable generation, where the prompt is usually used to specify the \\emph{task itself}.\nThus, it is relatively easy to find commonalities between the two genres:\n(1) both add extra information to the input text for better generation, and these additional signals are (often) learnable parameters.\n(2) If ``controlled generation'' is equipped with seq2seq-based pre-trained models (e.g., BART), then it is can be regarded as prompt learning with input-dependent prompts and the \\term{prompt+LM fine-tuning} strategy (\\S\\ref{sec:prompt-fine-tuning}), e.g. \\textit{GSum} , where both the prompt's and pre-trained LM's parameters can be tuned.\nAlso, some clear discrepancies between controlled generation and prompt-based text generation are:\n(1) In controlled generation work, the control is generally performed over the style or content of the generations  while the underlying task remains the same. They don't necessarily require a pre-trained model.\nIn contrast, the main motivation for using prompts for text generation is to specify the task itself and better utilize the pre-trained model.\n(2) Moreover, most of the current work on prompt learning in text generation shares a dataset- or task-level prompt . Only very few works have explored input-dependent ones . However, this is a common setting and effective in the controlled text generation, which may provide valuable direction for the future work on prompt learning.", "cites": [4210, 3003, 1594, 1591, 2357, 4211], "cite_extract_rate": 0.5, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section thoughtfully synthesizes the concept of controlled generation with prompting methods, drawing connections and contrasts between the two. It offers a critical evaluation by highlighting differences in focus (style/content vs. task specification) and noting the underexplored area of input-dependent prompts in prompt learning. The section abstracts these ideas into a broader conceptual framework, identifying potential future directions based on these insights."}}
{"id": "a3a73f26-da6e-4c17-99de-1e25ac1b21b4", "title": "Supervised Attention", "level": "paragraph", "subsections": [], "parent_id": "378a8314-912e-450c-b832-b5de56375d6a", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Prompt-relevant Topics"], ["paragraph", "Ensemble Learning"], ["paragraph", "Few-shot Learning"], ["paragraph", "Supervised Attention"]], "content": "Knowing to pay attention to the important information is a key step when extracting useful information from objects such as long text sequences , images , or knowledge bases ).\n\\term{Supervised attention}   aims to provide explicit supervision over the attention of models based on the fact that completely data-driven attention can overfit to some artifacts .\nIn this respect, prompt learning and supervised attention share ideas that both aim to extract salient information with some clues, which need to be provided separately.\nTo solve this problem, supervised attention methods tried to use additional loss functions to learn to predict gold attention on a manually labeled corpus .\nResearch on prompt learning may also borrow ideas from this literature.", "cites": [4213, 8751, 4210, 4214, 4212, 8750], "cite_extract_rate": 0.5454545454545454, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the concept of supervised attention by drawing from multiple papers, connecting it with prompt learning as a shared goal of guiding model focus. It offers some abstraction by highlighting the broader idea of explicit attention supervision over data-driven mechanisms. However, the critical analysis is limited, as it does not deeply evaluate the strengths or weaknesses of the cited works or explore potential trade-offs between methods."}}
{"id": "c0a441dc-ced4-46c8-895a-20008b7fbfd1", "title": "Data Augmentation", "level": "paragraph", "subsections": [], "parent_id": "378a8314-912e-450c-b832-b5de56375d6a", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Prompt-relevant Topics"], ["paragraph", "Ensemble Learning"], ["paragraph", "Few-shot Learning"], ["paragraph", "Data Augmentation"]], "content": "Data augmentation is a technique that targets increasing the amount of data that can be used for training by making modifications to existing data .\nAs recently observed by , adding prompts can achieve a similar accuracy improvement to the addition of 100s of data points on average across classification tasks, which suggests that using prompts for a downstream task is similar to conducting data augmentation implicitly.", "cites": [4215, 8752], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical perspective by linking the concept of prompting to data augmentation, suggesting that prompts implicitly offer similar benefits as adding more data. It synthesizes the key idea from both cited papers but does not deeply connect or contrast them. Critical analysis is limited, as it mainly states an observation without evaluating the methodology or limitations of the cited works."}}
{"id": "61ac405a-2f6c-4b64-8a2c-7b6024fea567", "title": "Prompting with Structured Information", "level": "paragraph", "subsections": [], "parent_id": "acdb078b-967e-4887-8824-0e3260605e73", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Challenges"], ["subsection", "Prompt Design"], ["paragraph", "Tasks beyond  Classification and Generation"], ["paragraph", "Prompting with Structured Information"]], "content": "In many NLP tasks, the inputs are imbued with some variety of structure, such as tree, graph, table, or relational structures.\nHow to best express these structures in prompt or answer engineering is a major challenge.\nExisting works  make a step by making prompts with additional marks to encode lexical information, such as entity markings.\n present structured prompts based on hyper text markup language for more fine-grained web text generation. However, moving beyond this to more complicated varieties of structure is largely unexplored, and a potentially interesting research area.", "cites": [7791], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section briefly introduces the concept of structured information in prompting and cites one paper (HTLM) as an example of structured prompting using HTML. It mentions that moving beyond basic structured formats is underexplored, which indicates a small degree of synthesis and abstraction. However, it lacks deeper integration of ideas, critical evaluation of limitations, or comparison with other approaches, resulting in a moderate insight level."}}
{"id": "53fe97fa-36e9-4383-93d9-c5b6189530b3", "title": "Entanglement of Template and Answer", "level": "paragraph", "subsections": [], "parent_id": "acdb078b-967e-4887-8824-0e3260605e73", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Challenges"], ["subsection", "Prompt Design"], ["paragraph", "Tasks beyond  Classification and Generation"], ["paragraph", "Entanglement of Template and Answer"]], "content": "The performance of a model will depend on \\emph{both} the templates being used and the answer being considered.\nHow to simultaneously search or learn for the best combination of template and answer remains a challenging question.\nCurrent works typically select answers before select template , but  have demonstrated the initial potential of simultaneously learning both.", "cites": [7787, 4201, 8744], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a basic analytical discussion by highlighting the interplay between template and answer design in prompting. It synthesizes the key idea from the cited papersnamely, the importance of jointly optimizing these componentsbut does so in a somewhat superficial way without deeper comparative or evaluative analysis. It identifies a general challenge and points to the emerging direction of joint learning, offering a moderate level of abstraction by framing the issue as a broader research problem."}}
{"id": "4fd38e87-4279-4621-8bac-812a85686769", "title": "Many-class and Long-answer Classification Tasks", "level": "paragraph", "subsections": ["5a456bdf-53d3-4440-b5f5-2d853b5abc39"], "parent_id": "9b8d65c3-23b4-4f81-8524-7f107143784d", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Challenges"], ["subsection", "Answer Engineering"], ["paragraph", "Many-class and Long-answer Classification Tasks"]], "content": "For classification-based tasks, there are two main challenges for answer engineering: (a) When there are too many classes, how to select an appropriate answer space becomes a difficult combinatorial optimization problem. (b) When using multi-token answers, how to best decode multiple tokens using LMs remains unknown, although some multi-token decoding methods have been proposed .", "cites": [7225], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section highlights two key challenges in answer engineering for classification tasks but provides limited synthesis with the cited paper. It briefly mentions the relevance of multi-token decoding and combinatorial optimization without deeply integrating the findings of the cited work. The analysis is somewhat descriptive and lacks comprehensive comparison or abstraction to broader principles in the field."}}
{"id": "d0d3f3a2-2394-4cae-b26f-7054be7e16ca", "title": "Selection of Tuning Strategy", "level": "subsection", "subsections": [], "parent_id": "833ea6e6-7b2a-47a4-b80d-24d281fe18c0", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Challenges"], ["subsection", "Selection of Tuning Strategy"]], "content": "As discussed in \\S\\ref{sec:tuning}, there are a fairly wide variety of methods for tuning parameters of prompts, LMs, or both.\nHowever, given the nascent stage of this research field, we still lack a systematic understanding of the tradeoffs between these methods.\nThe field could benefit from systematic explorations such as those performed in the pre-train and fine-tune paradigm regarding the tradeoffs between these different strategies .", "cites": [38], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a brief analytical overview of the current state of tuning strategies in prompt-based learning, acknowledging the lack of a systematic understanding and suggesting the need for exploration similar to the pre-train and fine-tune paradigm. However, it only references one paper and does not deeply synthesize or compare multiple works. The abstraction is limited, as it points to a general need for more exploration without offering broader theoretical or conceptual insights."}}
{"id": "32272338-5214-4f32-98df-f62ec5ef19f4", "title": "Prompt Ensembling", "level": "paragraph", "subsections": ["cb99da65-a8f1-428c-8b08-8abc17bf1469", "10a9478d-bad7-4a1a-9550-ce7a35e58b16", "7b70576e-9d1f-40fc-a44a-fc2c62f6b240"], "parent_id": "4d8a31d2-c16a-4dde-8aa1-66129d10aa05", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Challenges"], ["subsection", "Multiple Prompt Learning"], ["paragraph", "Prompt Ensembling"]], "content": "In prompt ensembling methods, the space and time complexity increase as we consider more prompts.\nHow to distill the knowledge from different prompts remains underexplored.\n use an ensemble model to annotate a large dataset to distill the knowledge from multiple prompts. \nIn addition, how to select ensemble-worthy prompts is also under-explored.\nFor text generation tasks, the study of prompt ensemble learning has not been performed so far, probably because ensemble learning in text generation itself is relatively complicated. To remedy this problem, some recently proposed neural ensembling methods such as \\term{Refactor}  could be considered as a method for prompt ensembling in text generation tasks.", "cites": [8745, 7786, 2466], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.3, "critical": 3.0, "abstraction": 2.7}, "insight_level": "medium", "analysis": "The section introduces the concept of prompt ensembling and identifies key challenges such as increased complexity and lack of knowledge distillation strategies. While it references three papers, it does not deeply synthesize their contributions or connect them in a coherent framework. It points out under-explored areas and a possible solution (Refactor), showing some critical awareness of research gaps."}}
{"id": "cb99da65-a8f1-428c-8b08-8abc17bf1469", "title": "Prompt Composition and Decomposition", "level": "paragraph", "subsections": [], "parent_id": "32272338-5214-4f32-98df-f62ec5ef19f4", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Challenges"], ["subsection", "Multiple Prompt Learning"], ["paragraph", "Prompt Ensembling"], ["paragraph", "Prompt Composition and Decomposition"]], "content": "Both prompt composition and decomposition aim to break down the difficulty of a complicated task input by introducing multiple sub-prompts.\nIn practice, how to make a good choice between them is a crucial step.\nEmpirically, for those token~ or span~ prediction tasks (e.g., NER), prompt decomposition can be considered, while for those span relation prediction~ tasks (e.g., entity coreference), prompts composition would be a better choice.\nIn the future, the general idea of de-/composing can be explored in more scenarios.", "cites": [4192], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides some analytical insights by distinguishing between prompt composition and decomposition and associating them with specific task types (e.g., NER vs. entity coreference). However, it synthesizes only one cited paper and lacks deeper connections across multiple works. Critical analysis is limited to a brief mention of task suitability without evaluating limitations or trade-offs. The abstraction is modest, as it identifies a general idea of de-/composing prompts but stops short of a meta-level principle."}}
{"id": "10a9478d-bad7-4a1a-9550-ce7a35e58b16", "title": "Prompt Augmentation", "level": "paragraph", "subsections": [], "parent_id": "32272338-5214-4f32-98df-f62ec5ef19f4", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Challenges"], ["subsection", "Multiple Prompt Learning"], ["paragraph", "Prompt Ensembling"], ["paragraph", "Prompt Augmentation"]], "content": "Existing prompt augmentation methods are \nlimited by the input length, i.e., feeding too many demonstrations to input is infeasible.\nTherefore, how to select informative demonstrations, and order them in an appropriate is an interesting but challenging problem .", "cites": [4204], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 2.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces the concept of prompt augmentation and highlights a specific limitation related to input length, citing one relevant paper. While it identifies a challenge (selecting and ordering demonstrations), it lacks a deeper synthesis of the cited work with others, minimal comparative analysis, and does not abstract broader principles or trends in the literature."}}
{"id": "8e197055-34c9-4590-9c38-6c7ec8afd659", "title": "Theoretical and Empirical Analysis of Prompting", "level": "subsection", "subsections": [], "parent_id": "833ea6e6-7b2a-47a4-b80d-24d281fe18c0", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Challenges"], ["subsection", "Theoretical and Empirical Analysis of Prompting"]], "content": "Despite their success in many scenarios, theoretical analysis and guarantees for prompt-based learning are scarce.  showed that soft-prompt tuning can relax the non-degeneracy assumptions (the generation probability of each token is linearly independent) needed for downstream recovery (i.e. recover the ground-truth labels of the downstream task.), making it easier to extract task-specific information.  verified that text classification tasks can be reformulated as sentence completion tasks, thus making language modeling a meaningful pre-training task.  empirically show that prompting is often worth 100s of data points on average across classification tasks.", "cites": [8752, 4216, 4217], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview by connecting different theoretical perspectives on prompting, particularly its reformulation of classification as sentence completion. It integrates ideas from the cited papers but does not offer a novel synthesis or deep critique, focusing instead on summarizing key findings and their implications."}}
{"id": "faadecc8-ce4d-4abd-bc65-c22770cb5957", "title": "Transferability of Prompts", "level": "subsection", "subsections": [], "parent_id": "833ea6e6-7b2a-47a4-b80d-24d281fe18c0", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Challenges"], ["subsection", "Transferability of Prompts"]], "content": "Understanding the extent to which prompts are specific to the model and improving the transferability of prompts are also important topics.  show that prompts selected under tuned few-shot learning scenario (where one has a larger validation set to choose prompts) generalize well across models of similar sizes while prompts selected under true few-shot learning scenario (where one only has a few training samples) do not generalize as effectively as the former setting among models with similar sizes.\nThe transferability is poor when the model sizes are quite different in both scenarios.", "cites": [4205], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes a key finding from the cited paper regarding the generalizability of prompts under different few-shot learning scenarios. It provides a basic comparison between tuned and true few-shot settings but lacks deeper analysis or evaluation of the papers methodology or limitations. The abstraction is limited, as it only highlights a general trend without elevating it to a broader theoretical or conceptual level."}}
{"id": "c446f43a-a874-47d4-8285-eca11dad5c16", "title": "Calibration of Prompting Methods", "level": "subsection", "subsections": [], "parent_id": "833ea6e6-7b2a-47a4-b80d-24d281fe18c0", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Challenges"], ["subsection", "Calibration of Prompting Methods"]], "content": "Calibration   refers to the ability of a model to make good probabilistic predictions.\nWhen using the generation probability of the pre-trained LMs (e.g., BART) to predict the answer, we need to be careful since the probability distribution is typically not well calibrated. \n observed the probabilities of pre-trained models (e.g., BART, T5, GPT-2) on QA tasks are well calibrated.\n identify three pitfalls (majority label bias, recency bias and common token bias) that lead the pre-trained LMs to be biased toward certain answers when provided answered prompts. For example, if the final answered prompt has a positive label, then this will bias the model towards predicting positive words. To overcome those pitfalls,  first use context-free input (e.g. the prompt would be ``Input: Subpar acting. Sentiment: Negative\\textbackslash n Input: Beautiful film. Sentiment: Positive\\textbackslash n Input: N/A. Sentiment:\") to get the initial probability distribution $P_0$, then they use the real input (e.g. the prompt would be ``Input: Subpar acting. Sentiment: Negative\\textbackslash n Input: Beautiful film. Sentiment: Positive\\textbackslash n Input: Amazing. Sentiment:\") to get the probability distribution $P_1$. Finally, these two distributions can be used to get a calibrated generation probability distribution. However, this method has two drawbacks: (1) it comes with the overhead of finding proper context-free input (e.g.~whether to use ``N/A\" or ``None\") and (2) the probability distribution of the underlying pre-trained LM is still not calibrated.\nEven though we have a calibrated probability distribution, we also need to be careful when we assume a single gold answer for an input. This is because that all surface forms of a same object will compete for finite probability mass~. For example, if we consider the gold answer to be ``Whirlpool bath\", the generation probability of it will typically be low since the word ``Bathtub\" shares the same meaning and it will take over a large probability mass. To address this issue, we could either (i) perform answer engineering to construct a comprehensive gold answer set using paraphrasing methods (\\S\\ref{sec:discrete-answer-search}) or (ii) calibrate the probability of a word based on its prior likelihood within the context~.", "cites": [8753, 1594], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes insights from two papers, linking the issue of calibration in prompting methods with specific biases and the problem of surface form competition. It provides critical evaluation by highlighting the limitations of the proposed calibration method and offering alternative strategies. While the abstraction level is strong in identifying broader issues like model bias and probability mass competition, it does not fully transcend to a meta-level theoretical insight."}}
{"id": "02fb0a16-6bee-4779-bea4-e769ef57b94b", "title": "Meta Analysis", "level": "section", "subsections": ["da6f998d-a12e-46a3-82a7-8a48b6cb67c4", "b52fc976-bef2-4cf3-a61d-ea83efe3c587"], "parent_id": "eef570d4-0122-4f1a-8609-023f350f068a", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Meta Analysis"]], "content": "\\label{sec:meta-analysis}\n\\begin{table*}\n\\scriptsize\n\\renewcommand\\arraystretch{2}\\arrayrulecolor{black}\n\\captionsetup{singlelinecheck=false, font=black, labelfont=sc, labelsep=quad}\n\\caption{Timeline of prompt-based learning. The time for each paper is based on its first arXiv version (if exists) or estimated submission time. A web-version can refer to \\href{http://pretrain.nlpedia.ai/timeline.html}{NLPedia-Pretrain}. Works in \\textcolor{red}{red} consider natural language understanding (NLU) tasks; works in \\textcolor{blue}{blue} consider natural language generation (NLG) tasks; works in \\textcolor{tlgreen}{green} consider both NLU tasks and NLG tasks.}\\vskip -1.5ex\n\\begin{tabular}{@{\\,}r <{\\hskip 2pt} !{\\foo}\n>{\\raggedright\\arraybackslash}p{7cm}@{\\,}r <{\\hskip 2pt} !{\\foo} >{\\raggedright\\arraybackslash}p{5.5cm}}\n\\toprule\n\\addlinespace[1.5ex]\n2018.06.07 & \\textcolor{red}{LMComm}  & 2021.04.14 & \\textcolor{red}{Soft}  \\\\\n2019.02.14 & \\textcolor{tlgreen}{GPT-2}  & 2021.04.15 & \\textcolor{blue}{DINO}  \\\\\n2019.04.14 & \\textcolor{red}{WNLaMPro}  & 2021.04.15 & \\textcolor{red}{AdaPrompt}  \\\\ \n2019.07.31 & \\textcolor{red}{LMDiagnose}  & 2021.04.16 & \\textcolor{tlgreen}{PMI$_\\text{DC}$}  \\\\\n2019.08.20 & \\textcolor{blue}{AdvTrigger}  & 2021.04.18 & \\textcolor{red}{Prompt-Tuning}  \\\\\n2019.09.02 & \\textcolor{red}{CohRank}  & 2021.04.18 & \\textcolor{blue}{Natural-Instr}  \\\\\n2019.09.03 & \\textcolor{red}{LAMA}  & 2021.04.18 & \\textcolor{red}{OrderEntropy}  \\\\\n2019.09.11 & \\textcolor{blue}{CTRL}  & 2021.04.18 & \\textcolor{red}{FewshotSemp}  \\\\\n2019.10.23 & \\textcolor{tlgreen}{T5}  & 2021.04.26 & \\textcolor{tlgreen}{PanGu-$\\alpha$} \\\\\n2019.11.08 & \\textcolor{red}{Neg \\& Misprim}  & 2021.05.24 & \\textcolor{red}{TrueFewshot}  \\\\\n2019.11.28 & \\textcolor{red}{LPAQA}  & 2021.05.24 & \\textcolor{red}{PTR}  \\\\\n2019.12.10 & \\textcolor{red}{ZSC}  & 2021.06.03 & \\textcolor{red}{TemplateNER}  \\\\\n2020.01.21 & \\textcolor{red}{PET-TC}  & 2021.06.03 & \\textcolor{red}{PERO}  \\\\\n2020.03.10 & \\textcolor{red}{ContxFP}  & 2021.06.16 & \\textcolor{red}{PromptAnalysis}  \\\\\n2020.05.02 & \\textcolor{red}{UnifiedQA}  & 2021.06.20 & \\textcolor{tlgreen}{CPM-2}  \\\\\n2020.05.22 & \\textcolor{tlgreen}{RAG}  & 2021.06.21 & \\textcolor{red}{BARTScore}  \\\\\n2020.05.28 & \\textcolor{tlgreen}{GPT-3}   & 2021.06.24 & \\textcolor{red}{NullPrompt} \\\\\n2020.09.08  & \\textcolor{red}{CommS2S} & 2021.06.25 &  \\textcolor{blue}{Frozen}  \\\\\n2020.09.15 &  \\textcolor{red}{PET-SGLUE}  & 2021.07.05 & \\textcolor{red}{ERNIE-B3}  \\\\ \n2020.09.24 & \\textcolor{blue}{ToxicityPrompts}  & 2021.07.07 & \\textcolor{blue}{Codex}  \\\\\n2020.10.07 & \\textcolor{red}{WhyLM}  & 2021.07.14 & \\textcolor{tlgreen}{HTLM}  \\\\\n2020.10.13 & \\textcolor{red}{X-FACTR}  & 2021.07.15 & \\textcolor{red}{FLEX}  \\\\\n2020.10.26 & \\textcolor{red}{Petal}  \\\\\n2020.10.29 & \\textcolor{red}{AutoPrompt}  \\\\\n2020.12.08 & \\textcolor{blue}{CTRLsum}  \\\\\n2020.12.22 & \\textcolor{blue}{PET-Gen}  \\\\\n2020.12.31 & \\textcolor{red}{LM-BFF}  \\\\\n2021.01.01 & \\textcolor{red}{WARP}  \\\\\n2021.01.01 & \\textcolor{blue}{Prefix-Tuning}  \\\\\n2021.01.17 & \\textcolor{tlgreen}{KATE}  \\\\\n2021.02.15 & \\textcolor{tlgreen}{PromptProg}  \\\\\n2021.02.19 & \\textcolor{red}{ContxCalibrate}  \\\\\n2021.02.24 & \\textcolor{red}{PADA}  \\\\\n2021.02.27 & \\textcolor{blue}{SD}  \\\\\n2021.03.09 & \\textcolor{red}{BERTese}  \\\\\n2021.03.15 & \\textcolor{red}{Prompt2Data}  \\\\\n2021.03.18 & \\textcolor{red}{P-Tuning}  \\\\\n2021.03.18 & \\textcolor{red}{GLM}  \\\\\n2021.03.22 & \\textcolor{red}{ADAPET}  \\\\\n2021.04.10 & \\textcolor{red}{Meta}  \\\\\n2021.04.12 & \\textcolor{red}{OptiPrompt}  \\\\\n\\end{tabular} \\label{tab:timeline}\n\\end{table*}\nIn this section, we aim to give a quantitative birds-eye view of existing research on prompting methods by performing a meta analysis over existing research works along different dimensions.", "cites": [7792, 7465, 679, 7786, 7225, 2466, 4217, 4201, 4199, 2189, 4202, 9101, 8748, 4198, 7788, 8744, 9140, 8747, 4218, 4203, 7791, 4216, 8752, 4191, 8536, 8753, 4204, 1995, 1591, 4205, 8745, 4219, 4220, 4206, 7586, 2218, 9, 7787, 1594, 7789, 8470, 4200, 3003], "cite_extract_rate": 0.7666666666666667, "origin_cites_number": 60, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily presents a timeline of prompt-based learning research, listing papers with dates and task types (NLU, NLG). It lacks synthesis of ideas across papers, critical evaluation of their strengths or limitations, and abstraction to broader trends or principles. As such, it functions more as a descriptive overview than an insightful analysis."}}
{"id": "9a4e08c2-c665-40fe-8ecf-0aabb7871345", "title": "Year", "level": "paragraph", "subsections": ["ac977d00-d142-4a5e-aadd-6c85f8025253", "98fc39b0-7a6d-4f3f-a59a-b24a88fc4158", "3613012b-b092-479f-a04b-e7f74e26e4aa"], "parent_id": "b52fc976-bef2-4cf3-a61d-ea83efe3c587", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Meta Analysis"], ["subsection", "Trend Analysis"], ["paragraph", "Year"]], "content": "With the emergence of different kinds of pre-trained LMs, prompt-based learning has become a more and more active research field, as can be seen in Fig.~\\ref{fig:meta-analysis}-(a). We can see a huge surge in 2021, which is perhaps due to the prevalence of GPT-3 , which greatly increased the popularity of prompting in the few-shot multi-task setting.\n\\begin{figure*}[!ht]\n    \\centering\n    \\subfloat[Year.]{\n    \\includegraphics[height=0.185\\linewidth]{fig/meta_analysis/year.pdf}\n    }        \n    \\subfloat[Task.]{  \n    \\includegraphics[height=0.185\\linewidth]{fig/meta_analysis/task.pdf}\n    }        \n    \\subfloat[Automatic Search.]{  \n    \\includegraphics[height=0.185\\linewidth]{fig/meta_analysis/temp-ans.pdf}\n    }        \n    \\subfloat[Search Space.]{ \n    \\includegraphics[height=0.185\\linewidth]{fig/meta_analysis/temp-search-space.pdf}\n        } \n    \\caption{Meta-analyses over different dimensions. The statistics are based on the works in Tab.~\\ref{tab:papers-part1} and Tab.~\\ref{tab:papers-part2}. In (d), we use the following abbreviations. TC: text classification, FP: factual probing, GCG: general conditional generation, QA: question answering, CR: commonsense reasoning, SUM: summarization, O: others.}\n    \\label{fig:meta-analysis}\n\\end{figure*}", "cites": [679], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview by observing a surge in prompt-based learning research in 2021 and linking it to the influence of GPT-3. While it integrates the abstract idea of few-shot learning from the cited paper, it lacks deeper synthesis with other works or critical evaluation of GPT-3's role and limitations. It identifies a temporal trend and generalizes to the broader impact of pre-trained models, showing moderate abstraction."}}
{"id": "282657e3-3fb7-4b77-ae5f-59d9b663b01a", "title": "Auxiliary Objective", "level": "subsection", "subsections": [], "parent_id": "03634696-64fe-487e-9054-1ac68e6193d0", "prefix_titles": [["title", "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"], ["section", "Appendix on Pre-trained LMs"], ["subsection", "Auxiliary Objective"]], "content": "\\label{appendix:sec:auxiliary}\nIn this subsection, more auxiliary objectives for pre-training language models have been listed.\n\\begin{itemize*}\n    \\item \\textbf{Next Sentence Prediction (NSP)} : A binary classification loss predicting whether two segments appear consecutively within a larger document, or are random unrelated sentences.\n    \\item \\textbf{Sentence Order Prediction (SOP)} : A binary classification loss for predicting whether two sentences are in a natural or swapped order.\n    \\item \\textbf{Capital Word Prediction (CWP)} : A binary classification objective calculated over each word, predicting whether whether\n    each word is capitalized or not. \n    \\item \\textbf{Sentence Deshuffling (SDS)} : A multi-class classification task to reorganize permuted segments.\n    \\item \\textbf{Sentence distance prediction (SDP)}  : A three-class classification task, predicting the positional relationship between two sentences (adjacent in the same document, not adjacent but in the same document, in different documents).\n    \\item \\textbf{Masked Column Prediction (MCP)} : Given a table, recover the names and data types of masked columns.\n    \\item \\textbf{Linguistic-Visual Alignment (LVA)} : A binary classification to Predict whether the text content can be aligned to visual content.\n    \\item \\textbf{Image Region prediction (IRP)} : Given an image whose partial features are masked (zeroed out), predict the masked regions.\n    \\item \\textbf{Replaced Token Detection (RTD)} : A binary classification loss predicting whether each token in corrupted input was replaced by a generative sample or not.\n    \\item \\textbf{Discourse Relation Prediction (DRP)} : Predict the semantic or rhetorical relation between two sentences.\n    \\item \\textbf{Translation Language Modeling (TLM)} : Consider parallel sentences and mask words randomly in both source and target sentences.  \n    \\item \\textbf{Information Retrieval Relevance (IRR)} : Predict the information retrieval relevance of two sentences.\n    \\item \\textbf{Token-Passage Prediction (TPP)} : Identify the keywords of a passage appearing in the segment.\n    \\item \\textbf{Universal Knowledge-Text Prediction (UKTP)} : Incorporate knowledge into one pre-trained language model.\n    \\item \\textbf{Machine Translation (MT)}  : Translate a sentence from the source language into the target language.\n    \\item \\textbf{Translation Pair Span Corruption (TPSC)}  : Predict the masked spans from a translation pair.\n     \\item \\textbf{Translation Span Corruption (TSC)}  : Unlike TPSC, TSC only masks and predicts the spans in one language.\n     \\item \\textbf{Multilingual Replaced Token Detection (MRTD)} : Distinguish real input tokens from corrupted multilingual sentences by a Generative Adversarial Network, where both the generator and the discriminator are shared across languages.\n     \\item \\textbf{Translation Replaced Token Detection (TRTD)} : Distinguish the real tokens and masked tokens in the translation pair by the Generative Adversarial Network.\n     \\item \\textbf{Knowledge Embedding (KE)} : Encode entities and relations in knowledge graphs (KGs) as distributed representations\n    \\item \\textbf{Image-to-text transfer (ITT)} : Is similar to the image caption that generates a corresponding description for the input image.\n    \\item \\textbf{Multimodality-to-text transfer (MTT)} : Generate the target text based on both the visual information and the noised linguistic information.\n\\end{itemize*}", "cites": [8462, 1551, 1150, 1278, 2476, 8754, 4221, 4195], "cite_extract_rate": 0.6153846153846154, "origin_cites_number": 13, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic list of auxiliary objectives used in pre-training language models, each briefly defined and associated with a paper. However, it lacks synthesis by not connecting these objectives to broader themes or evaluating their effectiveness. There is minimal critical analysis or abstraction, as the section primarily serves as a descriptive catalog without deeper insights or comparisons."}}
