{"id": "cab8c0f2-c9d4-40ab-b09d-c705d9882dbe", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "e93cd0c5-bcfd-4f51-83f9-f1a68d346733", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Introduction"]], "content": "\\label{sec:introduction}\n\\IEEEPARstart{M}{achine} learning (ML) has achieved remarkable success in various areas, such as computer vision, natural language processing, and healthcare.\nThe goal of ML is to design a model that can learn general and predictive knowledge from training data, and then apply the model to new (test) data. \nTraditional ML models are trained based on the \\emph{i.i.d.} assumption that training and testing data are identically and independently distributed.\nHowever, this assumption does not always hold in reality.\nWhen the probability distributions of training data and testing data are different, the performance of ML models often deteriorates due to domain distribution gaps~. \nCollecting the data of all possible domains to train ML models is expensive and even prohibitively impossible. Therefore, enhancing the \\emph{generalization} ability of ML models is important in both industry and academic fields.\nThere are many generalization-related research topics such as domain adaptation, meta-learning, transfer learning, covariate shift, and so on.\nIn recent years, \\emph{Domain generalization (DG)} has received much attention.\nAs shown in \\figurename~\\ref{fig-dg-example}, the goal of \\dg is to learn a model from one or several different but related domains (\\ieno, diverse training datasets) that will generalize well on \\emph{unseen} testing domains.\nFor instance, given a training set consisting of images coming from sketches, cartoon images and paintings, domain generalization requires to train a good machine learning model that has minimum prediction error in classifying images coming from natural images or photos, which are clearly having distinguished distributions from the images in training set.\nOver the past years, \\dg has made significant progress in various areas such as computer vision and natural language processing.\nDespite the progress, there has not been a survey in this area that comprehensively introduces and summarizes its main ideas, learning algorithms and other related problems to provide research insights for the future.\nIn this paper, we present the first survey on \\dg to introduce its recent advances, with special focus on its formulations, theories, algorithms, research areas, datasets, applications, and future research directions.\nWe hope that this survey can provide a comprehensive review for interested researchers and inspire more research in this and related areas.\n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{figures/fig-dg.pdf}\n    \\caption{Examples from the dataset PACS~ for \\dg. The training set is composed of images belonging to domains of sketch, cartoon, and art paintings. DG aims to learn a generalized model that performs well on the unseen target domain of photos.}\n    \\label{fig-dg-example}\n    \\vspace{-.1in}\n\\end{figure}\nThere are several survey papers after the conference version of our paper, and they are significantly different from ours.\nConcurrently,  also wrote a survey on DG, while their focus is in computer vision field.\nA more recent survey paper is on out-of-distribution (OOD) generalization by .\nTheir work focused on causality and stable neural networks.\nA related survey paper~ is for OOD detection instead of building a working algorithm that can be applied to any unseen environments.\nThis paper is a heavily extended version of our previously accepted short paper at IJCAI-21 survey track (6 pages, included in the appendix file).\nCompared to the short paper, this version makes the following extensions:\n\\begin{itemize}\n    \\item We present the theory analysis on domain generalization and the related domain adaptation.\n    \\item We substantially extend the methodology by adding new categories: \\textit{e.g.,}  causality-inspired methods, generative modeling for feature disentanglement, invariant risk minimization, gradient operation-based methods, and other learning strategies to comprehensively summarize these DG methods.\n    \\item For all the categories, we broaden the analysis of methods by including more related algorithms, comparisons, and discussion. And we also include more recent papers (over 30\\% of new work).\n    \\item We extend the scope of datasets and applications, and we also explore evaluation standards to domain generalization. Finally, we build an open-sourced codebase for DG research named \\emph{DeepDG}\\footnote{\\url{https://github.com/jindongwang/transferlearning/tree/master/code/DeepDG}} and conduct some analysis of the results on public datasets.\n\\end{itemize}\nThis paper is organized as follows.\nWe formulate the problem of domain generalization and discuss its relationship with existing research areas in Section~\\ref{sec-back-related}.\nSection~\\ref{sec-theory} presents the related theories in \\dg.\nIn Section~\\ref{sec-method}, we describe some representative DG methods in detail.\nIn Section~\\ref{sec-research-area}, we show some new DG research areas extended from the traditional setting.\nSection~\\ref{sec-app} presents the applications and Section~\\ref{sec-dataset} introduces the benchmark datasets for DG.\nWe summarize the insights from existing work and present some possible future directions in Section~\\ref{sec-diss}.\nFinally, we conclude this paper in Section~\\ref{sec-con}.", "cites": [7611, 2684], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The introduction section provides a general overview of domain generalization and briefly mentions related survey papers. While it does reference two cited works, the integration is minimal and primarily serves to highlight differences in focus or scope rather than synthesizing ideas across them. There is no critical evaluation of the cited works, and the section lacks abstraction or identification of broader principles or trends."}}
{"id": "80388c53-2ae9-4231-9c79-14d33986d120", "title": "Formalization of Domain Generalization", "level": "subsection", "subsections": [], "parent_id": "0084c652-4e7b-4ed9-b07c-cd56a528b63a", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Background"], ["subsection", "Formalization of Domain Generalization"]], "content": "In this section, we introduce the notations and definitions used in this paper.\n\\begin{definition}[Domain]\nLet $\\mathcal{X}$ denote a nonempty input space and $\\mathcal{Y}$ an output space.\nA domain is composed of data that are sampled from a distribution. We denote it as $\\mathcal{S} = \\{(\\mathbf{x}_i, y_i)\\}^n_{i=1} \\sim P_{XY}$, where $\\mathbf{x} \\in \\mathcal{X} \\subset \\mathbb{R}^d$, $y \\in \\mathcal{Y} \\subset \\mathbb{R}$ denotes the label, and $P_{XY}$ denotes the joint distribution of the input sample and output label. $X$ and $Y$ denote the corresponding random variables.\n\\end{definition}\n\\begin{definition}[Domain generalization]\n\\label{def-dg}\nAs shown in \\figurename~\\ref{fig-dg-dist}, in \\dg, we are given $M$ training (source) domains $\\mathcal{S}_{train}=\\{\\mathcal{S}^i \\mid i=1,\\cdots,M\\}$ where $\\mathcal{S}^i = \\{(\\mathbf{x}^i_j, y^i_j)\\}^{n_i}_{j=1}$ denotes the $i$-th domain.\nThe joint distributions between each pair of domains are different: $P^i_{XY} \\ne P^j_{XY}, 1 \\le i \\ne j \\le M$.\nThe goal of \\dg is to learn a robust and generalizable predictive function $h: \\mathcal{X} \\to \\mathcal{Y}$ from the $M$ training domains to achieve a minimum prediction error on an \\emph{unseen} test domain $\\mathcal{S}_{test}$ (\\ieno, $\\mathcal{S}_{test}$ cannot be accessed in training and $P^{test}_{XY} \\ne P^i_{XY} \\text{ for }i \\in \\{1,\\cdots,M\\}$):\n\\begin{equation}\n    \\min_{h} \\, \\mathbb{E}_{(\\mathbf{x},y) \\in \\mathcal{S}_{test}} [ \\ell(h(\\mathbf{x}),y) ],\n\\end{equation}\nwhere $\\mathbb{E}$ is the expectation and $\\ell(\\cdot, \\cdot)$ is the loss function.\n\\end{definition}\n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=.48\\textwidth]{figures/fig-dg-dist.pdf}\n    \\caption{Illustration of \\dg. Adapted from .}\n    \\label{fig-dg-dist}\n\\end{figure}\nWe list the frequently used notations in \\tablename~\\ref{tb-notation}.\n\\begin{table}[htbp]\n\\caption{Notations used in this paper.}\n\\label{tb-notation}\n\\resizebox{.5\\textwidth}{!}{\n\\begin{tabular}{ll|ll}\n\\toprule\nNotation & Description & Notation & Description \\\\ \\hline\n$\\mathbf{x}, y$ & Instance/label & $\\ell(\\cdot, \\cdot)$ & Loss function \\\\\n$\\mathcal{X}, \\mathcal{Y}$ & Feature/label space & $h$ & Predictive function \\\\ \n$\\mathcal{S}$ & Domain & $g,f$ & Feature extractor/classifier \\\\ \n$P(\\cdot)$ & Distribution & $\\epsilon$ & Error (risk) \\\\ \n$\\mathbb{E}[\\cdot]$ & Expectation & $\\theta$ & Model parameter \\\\\n$M$ & Number of source domain & $n_i$ & Data size of source domain $i$ \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{table}", "cites": [8578], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a formal definition of domain generalization and related notations, but it does so primarily through description without synthesizing or contrasting multiple cited works. The only cited paper is briefly referenced for its framework, yet the section lacks critical analysis or deeper abstraction to broader principles in the field."}}
{"id": "0b607704-3202-40c4-a100-1b8da4b0b23d", "title": "Related Research Areas", "level": "subsection", "subsections": [], "parent_id": "0084c652-4e7b-4ed9-b07c-cd56a528b63a", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Background"], ["subsection", "Related Research Areas"]], "content": "There are several research fields closely related to domain generalization, including but not limited to: transfer learning, domain adaptation, multi-task learning, multiple domain learning, meta-learning, lifelong learning, and zero-shot learning. We summarize their differences with \\dg in \\tablename~\\ref{tb-related} and briefly describe them in the following.\\looseness=-1\n\\textbf{Multi-task learning}~ jointly optimizes models on several related tasks. By sharing representations between these tasks, we could enable the model to generalize better on the original task.  \nNote that multi-task learning does not aim to enhance the generalization to a new (unseen) task.\nParticularly, multi-domain learning is a kind of multi-task learning, which trains on multiple related domains to learn good models for each original domain~ instead of new test domains.\n\\textbf{Transfer learning}~ trains a model on a source task and aims to enhance the performance of the model on a different but related target domain/task.\nPretraining-finetuning is the commonly used strategy for transfer learning where the source and target domains have different tasks and target domain is accessed in training.\nIn DG, the target domain cannot be accessed and the training and test tasks are often the same while they have different distributions.\\looseness=-1\n\\begin{table*}[t!]\n\\centering\n\\caption{Comparison between \\dg and some related learning paradigms.}\n\\label{tb-related}\n\\begin{tabular}{llllc}\n\\toprule\nLearning paradigm & Training data & Test data & Condition & Test access\\\\ \\hline\nMulti-task learning & $\\mathcal{S}^1, \\cdots, \\mathcal{S}^n$ & $\\mathcal{S}^1, \\cdots, \\mathcal{S}^n$ & $\\mathcal{Y}^i \\ne \\mathcal{Y}^j, 1 \\le i \\ne j \\le n$ & $\\checkmark$\\\\ \nTransfer learning & $\\mathcal{S}^{src}$, $\\mathcal{S}^{tar}$ & $\\mathcal{S}^{tar}$ & $\\mathcal{Y}^{src} \\ne \\mathcal{Y}^{tar}$ &  $\\checkmark$ \\\\ \nDomain adaptation & $\\mathcal{S}^{src}, \\mathcal{S}^{tar}$ & $\\mathcal{S}^{tar}$ & $P(\\mathcal{X}^{src}) \\ne P(\\mathcal{X}^{tar})$ &  $\\checkmark$\\\\ \nMeta-learning & $\\mathcal{S}^1, \\cdots, \\mathcal{S}^n$ & $\\mathcal{S}^{n+1}$ & $\\mathcal{Y}^i \\ne \\mathcal{Y}^j, 1 \\le i \\ne j \\le n+1$ & $\\checkmark$ \\\\ \nLifelong learning & $\\mathcal{S}^1, \\cdots, \\mathcal{S}^n$ & $\\mathcal{S}^1, \\cdots, \\mathcal{S}^n$ &$\\mathcal{S}^i$ arrives sequentially &$\\checkmark$\\\\\nZero-shot learning & $\\mathcal{S}^1, \\cdots, \\mathcal{S}^n$ & $\\mathcal{S}^{n+1}$ & $\\mathcal{Y}^{n+1} \\ne \\mathcal{Y}^i, 1 \\le i \\le n$ & $\\times$ \\\\\nDomain generalization & $\\mathcal{S}^1, \\cdots, \\mathcal{S}^n$ & $\\mathcal{S}^{n+1}$ & $P(\\mathcal{S}^i) \\ne P(\\mathcal{S}^j), 1 \\le i \\ne j \\le n+1$ & $\\times$\\\\\n\\bottomrule\n\\end{tabular}\n\\vspace{-.1in}\n\\end{table*}\n\\textbf{Domain adaptation}~(DA)~ is also popular in recent years.\nDA aims to maximize the performance on a given target domain using existing training source domain(s).\nThe difference between DA and DG is that DA has access to the target domain data while DG cannot see them during training.\nThis makes DG more challenging than DA but more realistic and favorable in practical applications.\n\\textbf{Meta-learning}~ aims to learn the learning algorithm itself by learning from previous experience or tasks, i.e., learning-to-learn.\nWhile the learning tasks are different in meta-learning, the learning tasks are the same in domain generalization.\nMeta-learning is a general learning strategy that can be used for DG~ by simulating the meta-train and meta-test tasks in training domains to enhance the performance of DG.\n\\textbf{Lifelong Learning}~, or continual learning, cares about the learning ability among multiple sequential domains/tasks. It requires the model to continually learn over time by accommodating new knowledge\nwhile retaining previously learned experiences.\nThis is also different from DG since it can access the target domain in each time step, and it does not explicitly handle the different distributions across domains.\n\\textbf{Zero-shot learning}~ aims at learning models from seen classes and classify samples whose categories are unseen in training. \nIn contrast, domain generalization in general studies the problem where training and testing data are from the same classes but with different distributions.", "cites": [2685, 7612, 2469, 328, 7109, 8580, 8579, 2687, 2686], "cite_extract_rate": 0.5294117647058824, "origin_cites_number": 17, "insight_result": {"type": "comparative", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes information by comparing domain generalization with several related paradigms and using a table to highlight key differences. It integrates insights from the cited papers to establish a structured and coherent comparison. While it does not deeply critique individual papers, it provides a clear and analytical contrast of assumptions and goals, offering a higher-level abstraction of the broader landscape of domain-related learning methods."}}
{"id": "f76b8a85-d553-485e-b15c-5fae1812fb4e", "title": "Domain Adaptation", "level": "subsection", "subsections": [], "parent_id": "d95f76a3-df21-48c3-a0e3-508d0354defb", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Theory"], ["subsection", "Domain Adaptation"]], "content": "\\label{sec:thr-da}\nFor a binary classification problem, we denote the true labeling functions on the source domain as ${h^*}^s: \\mathcal{X} \\rightarrow [0,1]$\n\\footnote{When the output is in $(0,1)$, it means the probability of $y=1$.}\nand that on the target domain as ${h^*}^t$.\nLet $h: \\mathcal{X} \\rightarrow [0,1]$ be any classifier from a hypothesis space $\\clH$.\nThe classification difference on the source domain between two classifiers $h$ and $h'$ can then be measured by\n\\begin{align}\n    \\epsilon^s (h,h') = \\bbE_{\\bfxx \\sim P^s_X} [h(\\bfxx) \\neq h'(\\bfxx)] = \\bbE_{\\bfxx \\sim P^s_X} [|h(\\bfxx) - h'(\\bfxx)|],\n\\end{align}\nand similarly we can define $\\epsilon^t$ when taking $\\bfxx \\sim P^t_X$ in the expectation.\nWe define $\\epsilon^s(h) := \\epsilon^s(h, {h^*}^s)$ and $\\epsilon^t(h) := \\epsilon^t(h, {h^*}^t)$ as the risk of classifier $h$ on the source and target domains, respectively.\nThe goal of DG/DA is to minimize the target risk $\\epsilon^t(h)$, but it is not accessible since we do not have any information on ${h^*}^t$.\nSo people seek to bound the target risk $\\epsilon^t(h)$ using the tractable source risk $\\epsilon^s(h)$.\nBen-David et al.  (Thm.~1) give a bound relating the two risks:\n\\begin{align}\n  \\epsilon^t(h) \\le{} & \\epsilon^s(h) + 2 d_1(P^s_X, P^t_X) \\\\\n  & {}+ \\min_{P_X \\in \\{P^s_X, P^t_X\\}} \\bbE_{\\bfxx \\sim P_X} [| {h^*}^s(\\bfxx) - {h^*}^t(\\bfxx) |],\n  \\label{eqn:da-bound-tv}\n\\end{align}\nwhere $d_1(P^s_X, P^t_X) := \\sup_{\\clA \\in \\scX} |P^s_X[\\clA] - P^t_X[\\clA]|$ is the \\emph{total variation} between the two distributions, and $\\scX$ denotes the sigma-field on $\\clX$.\nThe second term on the r.h.s measures the difference of cross-domain distributions,\nand the third term represents the difference in the labeling functions\n(covariate shift is not \\emph{a priori} assumed).\nHowever, the total variation is a strong distance (i.e., it tends to be very large) that may loosen the bound~\\eqref{eqn:da-bound-tv}, and is hard to estimate using finite samples.\nTo address this,  developed another bound (, Thm.~2; , Thm.~1):\n\\begin{align}\n  \\epsilon^t(h) \\le{} & \\epsilon^s(h) + d_{\\clH\\Delta\\clH}(P^s_X, P^t_X) + \\lambda_\\clH,\n  \\label{eqn:da-bound-hdh}\n\\end{align}\nwhere the $\\clH\\Delta\\clH$-divergence is defined as $d_{\\clH\\Delta\\clH}(P^s_X, P^t_X) := \\sup_{h, h' \\in \\clH} |\\epsilon^s(h, h') - \\epsilon^t(h, h')|$, replacing the total variation $d_1$ to measure the distribution difference,\nand the ideal joint risk $\\lambda_\\clH := \\inf_{h \\in \\clH} \\left[ \\epsilon^s(h) + \\epsilon^t(h) \\right]$ measures the complexity of $\\clH$ for the prediction tasks on the two domains.\n$\\clH\\Delta\\clH$-divergence has a better finite-sample guarantee, leading to a non-asymptotic bound:\n\\begin{theorem}[Domain adaptation error bound (non-asymptotic)  (Thm.~2)]\n\\label{the-da-nonasymp}\nLet $d$ be the Vapnikâ€“Chervonenkis (VC) dimension~ of $\\clH$, and $\\clU^s$ and $\\clU^t$ be unlabeled samples of size $n$ from the two domains.\nThen for any $h \\in \\clH$ and $\\delta \\in (0,1)$, the following inequality holds with probability at least $1-\\delta$:\n\\begin{align}\n    \\epsilon^t(h) \\le{} & \\epsilon^s(h) + \\ddh_{\\clH\\Delta\\clH} (\\clU^s, \\clU^t) + \\lambda_\\clH \\\\\n    & {}+ 4 \\sqrt{\\frac{ 2d \\log (2n) + \\log (2/\\delta) }{n}},\n\\end{align}\nwhere $\\ddh_{\\clH\\Delta\\clH}(\\clU^s, \\clU^t)$ is the estimate of $d_{\\clH\\Delta\\clH}(P^s_X, P^t_X)$ on the two sets of finite data samples.\n\\end{theorem}\nIn the above bounds, the domain distribution difference $d(P^s_X, P^t_X)$ is not controllable, but one may learn a representation function $g: \\clX \\to \\clZ$ that maps the original input data $\\bfxx$ to some representation space $\\clZ$, so that the representation distributions of the two domains become closer.\nThis direction of DA is thus called DA based on domain-invariant representation (DA-DIR).\nThe theory of domain-invariant representations has since inspired many DA/DG methods, which can be seen in Section~\\ref{sec-repr}.", "cites": [2688], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a formal analytical treatment of domain adaptation, synthesizing theoretical bounds from the cited works and explaining their implications. It critically highlights limitations of total variation as a distance measure and introduces an alternative bound, which reflects a moderate level of critical engagement. The section abstracts from individual papers to discuss broader theoretical principles like domain-invariant representations, but it does not go beyond these to offer a novel or meta-level synthesis of ideas."}}
{"id": "414be1e7-c19c-48bc-8fbc-da92d2f401c0", "title": "Average risk estimation error bound", "level": "subsubsection", "subsections": [], "parent_id": "dffc7599-8cfc-4450-abd1-2bb88ae20413", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Theory"], ["subsection", "Domain Generalization"], ["subsubsection", "Average risk estimation error bound"]], "content": "The first line of \\dg theory considers the case where the target domain is totally unknown (not even unsupervised data), and measures the average risk over all possible target domains.\nAssume that all possible target distributions follow an underlying hyper-distribution $\\clP$ on $(\\bfxx,y)$ distributions: $P^t_{XY} \\sim \\clP$,\nand that the source distributions also follow the same hyper-distribution: $P^1_{XY}, \\cdots, P^M_{XY} \\sim \\clP$.\nFor generalization to any possible target domain, the classifier to be learned in this case also includes the domain information $P_X$ into its input, so prediction is in the form $y = h(P_X, \\bfxx)$ on the domain with distribution $P_{XY}$.\nFor such a classifier $h$, its average risk over all possible target domains is then given by:\n\\begin{align}\n    \\clE(h) := \\bbE_{P_{XY} \\sim \\clP} \\bbE_{(\\bfxx,y) \\sim P_{XY}} [\\ell(h(P_X, \\bfxx), y)],\n    \\label{eqn:dg-avg-risk-exact}\n\\end{align}\nwhere $\\ell$ is a loss function on $\\clY$.\nExactly evaluating the expectations is impossible, but we can estimate it using finite domains/distributions following $\\clP$, and finite $(\\bfxx,y)$ samples following each distribution.\nAs we have assumed $P^1_{XY}, \\cdots, P^M_{XY} \\sim \\clP$, the source domains and supervised data could serve for this estimation:\n\\begin{align}\n    \\clEh(h) := \\frac{1}{M} \\sum_{i=1}^M \\frac{1}{n^i} \\sum_{j=1}^{n^i} \\ell(h(\\clU^i, \\bfxx^i_j), y^i_j),\n    \\label{eqn:dg-avg-risk-est}\n\\end{align}\nwhere we use the supervised dataset $\\clU^i := \\{\\bfxx^i_j \\mid (\\bfxx^i_j, y^i_j) \\in \\clS^i\\}$ from domain $i$ as an empirical estimation for $P^i_X$.\nThe first problem to consider is how well such an estimate approximates the target $\\clE(h)$.\nThis can be measured by the largest difference between $\\clE(h)$ and $\\clEh(h)$ on some space of $h$.\nTo our knowledge, this is first analyzed by , where the space of $h$ is taken as a reproducing kernel Hilbert space (RKHS).\nHowever, different from common treatment, the classifier $h$ here also depends on the distribution $P_X$, so the kernel defining the RKHS should be in the form $\\kkb((P_X^1, \\bfxx_1), (P_X^2, \\bfxx_2))$.\n construct such a kernel using kernels $k_X, k'_X$ on $\\clX$ and kernel $\\kappa$ on the RKHS $\\clH_{k'_X}$ of kernel $k'_X$:\n$\\kkb((P_X^1, \\bfxx_1), (P_X^2, \\bfxx_2)) := \\kappa(\\Psi_{k'_X} (P_X^1), \\Psi_{k'_X} (P_X^2)) k_X(\\bfxx_1, \\bfxx_2)$,\nwhere $\\Psi_{k'_X} (P_X) := \\bbE_{\\bfxx \\sim P_X} [k'_X(\\bfxx, \\cdot)] \\in \\clH_{k'_X}$ is the kernel embedding of distribution $P_X$ via kernel $k'$.\nThe result is given in the following theorem, which gives a bound on the largest average risk estimation error within an origin-centered closed ball $\\clB_{\\clH_\\kkb} (r)$ of radius $r$ in the RKHS $\\clH_\\kkb$ of kernel $\\kkb$,\nin a slightly simplified case where $n^1 = \\cdots = n^M =: n$.\n\\begin{theorem}[Average risk estimation error bound for binary classification~]\n    Assume that the loss function $\\ell$ is $L_\\ell$-Lipschitz in its first argument and is bounded by $B_\\ell$.\n    Assume also that the kernels $k_X, k'_X$ and $\\kappa$ are bounded by $B_k^2, B_{k'}^2 \\ge 1$ and $B_\\kappa^2$, respectively, and the canonical feature map $\\Phi_\\kappa: v \\in \\clH_{k'_X} \\mapsto \\kappa(v, \\cdot) \\in \\clH_\\kappa$ of $\\kappa$ is $L_\\kappa$-H\\\"older of order $\\alpha \\in (0,1]$ on the closed ball $\\clB_{\\clH_{k'_X}} (B_{k'})$ \\footnote{\n        This means that for any $u,v \\in \\clB_{\\clH_{k'_X}} (\\clB_{\\clH_{k'}})$, it holds that $\\Vert \\Phi_\\kappa(u) - \\Phi_\\kappa(v) \\Vert \\le L_\\kappa \\Vert u - v \\Vert^\\alpha$, where the norms are of the respective RKHSs.}.\n    Then for any $r > 0$ and $\\delta \\in (0,1)$, with probability at least $1-\\delta$, it holds that:\n    \\begin{align}\n        \\sup_{h \\in \\clB_{\\clH_\\kkb} (r)} \\left| \\clEh(h) - \\clE(h) \\right|\n        \\le{} & C \\bigg( B_\\ell \\sqrt{- M^{-1} \\log \\delta} \\\\\n        + r B_k L_\\ell \\Big( B_{k'} L_\\kappa \\big( & n^{-1} \\log (M/\\delta) \\big)^{\\alpha/2} + B_\\kappa / \\sqrt{M} \\Big) \\bigg),\n    \\end{align}\n    where $C$ is a constant.\n\\end{theorem}\nThe bound becomes larger in general if $(M, n)$ is replaced with $(1, Mn)$.\nIt indicates that using domain-wise datasets is better than just pooling them into one mixed dataset, so the domain information plays a role.\nThis result is later extended in , and  give a bound for multi-class classification in a similar form.\n\\begin{figure*}[t!]\n\t\\centering\n\t\\resizebox{\\textwidth}{!}{\n\t\\input{figures/fig_tree}\n\t}\n\t\\caption{Taxonomy of domain generalization methods.}\n\t\\label{fig-main}\n\\end{figure*}", "cites": [2689, 8578, 2690], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section synthesizes theoretical concepts from multiple papers by formalizing the idea of average risk estimation in domain generalization and integrating the use of kernel embeddings. It critically evaluates the trade-offs between domain-wise datasets and mixed datasets, highlighting the importance of domain structure. The abstraction is strong, as it formulates a general theoretical framework applicable across different methods and extends the analysis to multi-class classification."}}
{"id": "8237c4af-eec4-4047-9385-c3cfab062f49", "title": "Generalization risk bound", "level": "subsubsection", "subsections": [], "parent_id": "dffc7599-8cfc-4450-abd1-2bb88ae20413", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Theory"], ["subsection", "Domain Generalization"], ["subsubsection", "Generalization risk bound"]], "content": "Another line of DG theory considers the risk on a specific target domain, under the assumption of covariate shift (i.e., the labeling function $h^*$ or $P_{Y|X}$ is the same over all domains).\nThis measurement is similar to what is considered in domain adaptation theory in Section~\\ref{sec:thr-da}, so we adopt the same definition for the source risks $\\epsilon^1, \\cdots, \\epsilon^M$ and the target risk $\\epsilon^t$.\nWith the covariate shift assumption, each domain is characterized by the distribution on $\\clX$.\n then consider approximating the target domain distribution $P^t_X$ within the convex hull of source domain distributions: $\\Lambda := \\{\\sum_{i=1}^M \\pi_i P^i_X \\mid \\pi \\in \\Delta_M\\}$, where $\\Delta_M$ is the $(M-1)$-dimensional simplex so that each $\\pi$ represents a normalized mixing weights.\nSimilar to the domain adaptation case, distribution difference is measured by the $\\clH$-divergence to include the influence of the classifier class.\n\\begin{theorem}[Domain generalization error bound~]\n    Let $\\gamma := \\min_{\\pi \\in \\Delta_M} d_\\clH (P^t_X, \\sum_{i=1}^M \\pi_i P^i_X)$ with minimizer $\\pi^*$\n    \\footnote{The original presentation does not mention that the $\\pi$ is the minimizer, but the proof indicates so.}\n    be the distance of $P^t_X$ from the convex hull $\\Lambda$, and $P^*_X := \\sum_{i=1}^M \\pi^*_i P^i_X$ be the best approximator within $\\Lambda$.\n    Let $\\rho := \\sup_{P'_X, P''_X \\in \\Lambda} d_\\clH (P'_X, P''_X)$ be the diameter of $\\Lambda$.\n    Then it holds that\n    \\begin{align}\n        \\epsilon^t(h) \\le \\sum_{i=1}^M \\pi^*_i \\epsilon^i(h) + \\frac{\\gamma + \\rho}{2} + \\lambda_{\\clH, (P^t_X, P^*_X)},\n    \\end{align}\n    where $\\lambda_{\\clH, (P^t_X, P^*_X)}$ is the ideal joint risk across the target domain and the domain with the best approximator distribution $P^*_X$.\n\\end{theorem}\nThe result can be seen as the generalization of domain adaptation bounds in Section~\\ref{sec:thr-da} when there are multiple source domains.\nAgain similar to the domain adaptation case, this bound motivates \\dg methods based on domain invariant representation, which simultaneously minimize the risks over all source domains corresponding to the first term of the bound, as well as the representation distribution differences among source and target domains in the hope to reduce $\\gamma$ and $\\rho$ on the representation space.\nTo sum up, the theory of generalization is an active research area and other researchers also derived different DG theory bounds using informativeness~ and adversarial training~.\n\\iffalse\nThe work of  presented an adversarial learning-based DG theory:\n\\begin{theorem}[Domain generalization error bound~]\n\\label{thm-dg-all}\nLet $\\mathcal{X}$ be a space and $\\mathcal{H}$ be a class of hypothesis corresponding to this space. We use $\\mathcal{Q}$ to denote the unseen target domain and $\\{\\mathcal{P}_i\\}_{i=1}^{M}$ denotes the $M$ training source domains. Further, let $\\{\\psi_i\\}_{i=1}^{M}$ denote the convex combination of the $M$ sources and $\\sum_{i} \\psi_i = 1$. Let the object $\\mathcal{O}$ be a set of distributions such that for every distribution $\\mathcal{S} \\in \\mathcal{O}$ the following holds:\n\\begin{align}\n    \\sum_i \\psi_i d_{\\mathcal{H} \\Delta \\mathcal{H}}(\\mathcal{P}_i, \\mathcal{S}) \\le \\max_{i,j} d_{\\mathcal{H} \\Delta \\mathcal{H}}(\\mathcal{P}_i, \\mathcal{P}_j).\n\\end{align}\nThen, the following inequality holds for any $h \\in \\mathcal{H}$:\n\\begin{equation}\n\\begin{split}\n    \\epsilon_{\\mathcal{Q}}(h) \\le \\lambda_\\psi + \\sum_{i} \\psi_i \\epsilon_{\\mathcal{P}_i}(h) &+ \\frac{1}{2} \\min_{\\mathcal{S} \\in \\mathcal{O}} d_{\\mathcal{H} \\Delta \\mathcal{H}} (\\mathcal{S}, \\mathcal{Q})\\\\\n    &+ \\frac{1}{2} \\max_{i,j} d_{\\mathcal{H} \\Delta \\mathcal{H}} (\\mathcal{P}_i, \\mathcal{P}_j),\n\\end{split}\n\\end{equation}\nwhere $\\lambda_\\psi = \\sum_i \\psi_i \\lambda_i$ and each $\\lambda_i$ is the error of an ideal joint hypothesis for $\\mathcal{Q}$ and $\\mathcal{P}_i$.\n\\end{theorem}\nWe can see that this theorem is consistent with the DA theory.\nThe above theorem indicates that the generalization error bound for \\dg relies on four components: the ideal joint error for each source-target pair, the ERM on all source domains, the diverse joint distributions, and the source-source divergence maximization.\nThe third term generally inspires the data manipulation methods (Section~\\ref{sec-method-data}) where some works tried to generate distributions that are near the unseen target distribution $\\mathcal{Q}$.\nThe last term corresponds to the reduction of source-source divergence which is similar to existing DA methods. More methods can be found in Section~\\ref{sec-repr}.\n\\fi", "cites": [2689, 2692, 2691], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes theoretical concepts from multiple cited papers, integrating domain adaptation and domain generalization into a coherent narrative about risk bounds. It abstracts key components of the bounds (e.g., ideal joint error, source-source divergence) and links them to broader algorithmic strategies like domain-invariant representation. However, it lacks deeper critical evaluation of the cited theories, such as their limitations or assumptions, and does not offer a novel unifying framework."}}
{"id": "cf59e4a6-0568-4d09-9703-07b90490d958", "title": "Data Manipulation", "level": "subsection", "subsections": ["7c6c1bf3-eada-45f1-a375-c0915a702d43", "8b4b7767-46e2-40b4-8b5b-b81f0a7c3b81"], "parent_id": "48a808ea-1890-418b-ade1-822dc777b008", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Methodology"], ["subsection", "Data Manipulation"]], "content": "\\label{sec-method-data}\nWe are always hungry for more training data in machine learning (ML).\nThe generalization performance of a ML model often relies on the quantity and diversity of the training data.\nGiven a limited set of training data, data manipulation is one of the cheapest and simplest way to generate samples so as to enhance the generalization capability of the model.\nThe main objective for data manipulation-based DG is to increase the diversity of existing training data using different data manipulation methods. At the same time, the data quantity is also increased.\nAlthough the theoretical insight for why data augmentation or generation techniques can enhance the generalization ability of a model, experiments by  showed that the model tend to make predictions for both OOD and in-distribution samples based on trivial syntactic heuristics for NLP tasks.\nWe formulate the general learning objective of data manipulation-based DG as:\n\\begin{equation}\n    \\label{eq-augmentation-all}\n    \\min_{h} \\, \\mathbb{E}_{\\mathbf{x},y} [ \\ell(h(\\mathbf{x}),y) ] + \\mathbb{E}_{\\mathbf{x}^\\prime,y} [ \\ell(h(\\mathbf{x}^\\prime),y) ],\n\\end{equation}\nwhere $\\mathbf{x}^\\prime = \\mathcal{M}(\\mathbf{x})$ denotes the manipulated data using a function $\\mathcal{M}(\\cdot)$.\nBased on the difference on this function, we further categorize existing work into two types: \\emph{data augmentation} and \\emph{data generation}.", "cites": [2693], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of the concept of data manipulation in domain generalization and integrates one cited paper to highlight an issue with syntactic heuristics in NLP tasks. While it formulates a general learning objective, it lacks deeper connections to other relevant works and does not offer a comprehensive or novel framework. The critical analysis is minimal, and the abstraction shows some effort to generalize the approach but remains limited in scope."}}
{"id": "4d268815-6da3-4a54-812e-2f25ca28e58b", "title": "Domain randomization", "level": "paragraph", "subsections": [], "parent_id": "7c6c1bf3-eada-45f1-a375-c0915a702d43", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Methodology"], ["subsection", "Data Manipulation"], ["subsubsection", "Data augmentation-based DG"], ["paragraph", "Domain randomization"]], "content": "}\nOther than typical augmentation, domain randomization is an effective technique for data augmentation.\nIt is commonly done by generating new data that can simulate complex environments based on the limited training samples.\nHere, the $\\mathcal{M}(\\cdot)$ function is implemented as several manual transformations (commonly used in image data) such as: altering the location and texture of objects, changing the number and shape of objects, modifying the illumination and camera view, and adding different types of random noise to the data.\n first used this method to generate more training data from the simulated environment for generalization in the real environment.\nSimilar techniques were also used in  to strengthen the generalization capability of the models.\n further took into account the structure\nof the scene when randomly placing objects for data generation, which enables the neural network to learn to utilize context when detecting objects.\n proposed to not only augment features, but also labels.\nIt is easy to see that by randomization, the diversity of samples can be increased.\nBut randomization is often random, indicating that there could be some useless randomizations that could be further removed to improve the efficiency of the model.", "cites": [2698, 2697, 2695, 2696, 2694], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of domain randomization and mentions several papers, but does not deeply synthesize or integrate their contributions into a cohesive framework. It lacks critical evaluation of the methods or comparative analysis between them. Some general patterns are mentioned, such as the inclusion of scene structure and label augmentation, but these are not abstracted into broader principles or theoretical insights."}}
{"id": "8ce80dba-2e06-4b6b-be69-d61ba4bf91f8", "title": "Adversarial data augmentation", "level": "paragraph", "subsections": [], "parent_id": "7c6c1bf3-eada-45f1-a375-c0915a702d43", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Methodology"], ["subsection", "Data Manipulation"], ["subsubsection", "Data augmentation-based DG"], ["paragraph", "Adversarial data augmentation"]], "content": "}\nAdversarial data augmentation aims to guide the augmentation to optimize the generalization capability, by enhancing the diversity of data while assuring their reliability. \n used a Bayesian network to model dependence between label, domain and input instance, and proposed CrossGrad, a cautious data augmentation strategy that perturbs the input along the direction of greatest domain change while changing the class label as little as possible. \n proposed an iterative procedure that augments the source dataset with examples from a fictitious target domain that is ``hard\" under the current model, where adversarial examples are appended at each iteration to enable adaptive data augmentation. \n adversarially trained a transformation network for data augmentation instead of directly updating the inputs by gradient ascent while they adopted the regularization of weak and strong augmentation in .\nAdversarial data augmentation often has certain optimization goals that can be used by the network.\nHowever, its optimization process often involves adversarial training, thus is difficult.", "cites": [196, 2701, 2699, 2702, 2700], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of adversarial data augmentation in domain generalization but lacks deeper synthesis of the cited papers. It mentions different approaches without clearly connecting their underlying principles or evaluating their strengths and weaknesses. There is minimal abstraction or identification of broader trends or patterns in the field."}}
{"id": "8b4b7767-46e2-40b4-8b5b-b81f0a7c3b81", "title": "Data generation-based DG", "level": "subsubsection", "subsections": [], "parent_id": "cf59e4a6-0568-4d09-9703-07b90490d958", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Methodology"], ["subsection", "Data Manipulation"], ["subsubsection", "Data generation-based DG"]], "content": "Data generation is also a popular technique to generate diverse and rich data to boost the generalization capability of a model.\nHere, the function $\\mathcal{M}(\\cdot)$ can be implemented using some generative models such as Variational Auto-encoder (VAE)~, and Generative Adversarial Networks (GAN)~.\nIn addition, it can also be implemented using the Mixup~ strategy.\n used ComboGAN~ to generate new data and then applied domain discrepancy measure such as MMD~ to minimize the distribution divergence between real and generated images to help learn general representations.\n leveraged adversarial training to create ``fictitious\" yet ``challenging\" populations, where a Wasserstein Auto-Encoder (WAE)~ was used to help generate samples that preserve the semantic and have large domain transportation.\n generated novel distributions under semantic consistency and then maximized the difference between source and the novel distributions.\n introduced a simple transformation based on image stylization to explore cross-source variability for better generalization, where AdaIN  was employed to achieve fast stylization to arbitrary styles.\nDifferent from others,  used adversarial training to generate \\emph{domains} instead of samples.\nThese methods are more complex since different generative models are involved and we should pay attention to the model capacity and computing cost.\nIn addition to the above generative models, Mixup~ is also a popular technique for data generation.\nMixup generates new data by performing linear interpolation between any two instances and between their labels with a weight sampled from a Beta distribution, which does not require to train generative models.\nRecently, there are several methods using Mixup for DG, by either performing Mixup in the original space~ to generate new samples; or in the feature space~ which does not explicitly generate raw training samples.\nThese methods achieved promising performance on popular benchmarks while remaining conceptually and computationally simple.", "cites": [154, 7191, 2708, 7217, 2707, 7111, 2710, 2706, 7613, 2709, 2712, 7110, 2704, 5680, 2705, 2711, 2703], "cite_extract_rate": 0.9444444444444444, "origin_cites_number": 18, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 1.8, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of data generation-based DG methods, listing the generative models and strategies (e.g., GAN, VAE, Mixup) used in cited works. It offers minimal synthesis, merely connecting these methods by the common theme of data generation. There is little critical evaluation or identification of broader principles or limitations in these approaches."}}
{"id": "626bfc61-61e4-4bea-ac22-15fc285af8ec", "title": "Representation Learning", "level": "subsection", "subsections": ["e56e6ae8-5c5a-4b37-bc97-ea7f31e81e20", "997cdaa1-ad9d-4625-a0f9-f2b3f7e71b26"], "parent_id": "48a808ea-1890-418b-ade1-822dc777b008", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Methodology"], ["subsection", "Representation Learning"]], "content": "\\label{sec-repr}\nRepresentation learning has always been the focus of machine learning for decades~ and is also one of the keys to the success of \\dg. \nWe decompose the prediction function $h$ as $h = f \\circ g$, where $g$ is a representation learning function and $f$ is the classifier function. The goal of representation learning can be formulated as:\n\\begin{equation}\n    \\label{eq-repre-all}\n    \\min_{f,g} \\, \\mathbb{E}_{\\mathbf{x},y}  \\ell(f(g(\\mathbf{x})),y)  + \\lambda \\ell_{\\operatorname{reg}},\n\\end{equation}\nwhere $\\ell_{\\operatorname{reg}}$ denotes some regularization term and $\\lambda$ is the tradeoff parameter.\nMany methods are designed to better learn the feature extraction function $g$ with corresponding $\\ell_{\\operatorname{reg}}$. In this section, we categorize the existing literature on representation learning into two main categories based on different learning principles: \\emph{domain-invariant representation learning} and \\emph{feature disentanglement}.", "cites": [318], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a general analytical overview of representation learning in the context of domain generalization, introducing a decomposition of the prediction function and a unified formulation with a regularization term. It integrates a cited paper by referencing the importance of disentangling explanatory factors in data, but it lacks deeper synthesis across multiple sources or critical evaluation of the cited work's strengths and limitations. The abstraction level is moderate, as it identifies two main categories of representation learning methods."}}
{"id": "e27dd658-87bb-4f7c-9f5e-274cd6b7ee13", "title": "Kernel-based methods", "level": "paragraph", "subsections": [], "parent_id": "e56e6ae8-5c5a-4b37-bc97-ea7f31e81e20", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Methodology"], ["subsection", "Representation Learning"], ["subsubsection", "Domain-invariant representation-based DG"], ["paragraph", "Kernel-based methods"]], "content": "}\nKernel-based method is one of the most classical learning paradigms in machine learning.\nKernel-based machine learning relies on the kernel function to transform the original data into a high-dimensional feature space without ever computing the coordinates of the data in that space, but by simply computing the inner products between the samples of all pairs in the feature space.\nOne of the most representative kernel-based methods is Support Vector Machine (SVM)~.\nFor \\dg, there are plenty of algorithms based on kernel methods, where the representation learning function $g$ is implemented as some feature map $\\phi(\\cdot)$ which is easily computed using kernel function $k(\\cdot, \\cdot)$ such as RBF kernel and Laplacian kernel.\n first used kernel method for \\dg  and extended it in . They adopted the positive semi-definite kernel learning to learn a domain-invariant kernel from the training data.\n adapted transfer component analysis (TCA)~ to bridge the multi-domain distance to be closer for DG.\nSimilar to the core idea of TCA, Domain-Invariant Component Analysis (DICA)~ is one of the classic methods using kernel for DG.\nThe goal of DICA is to find a feature transformation kernel $k(\\cdot, \\cdot)$ that minimizes the distribution discrepancy between all data in feature space.\n adopted a similar method as DICA and further added attribute regularization.\nIn contrast to DICA which deals with the marginal distribution,  learned a feature representation which has domain-invariant class conditional distribution.\nScatter component analysis (SCA)~ adopted Fisher's discriminant analysis to minimize the discrepancy of representations from the same class and the same domain, and maximize the discrepancy of representations from the different classes and different domains.\n proposed an Elliptical Summary Randomisation (ESRand) that comprises of a randomised kernel and elliptical data summarization.\nESRand projected each domain into an ellipse to represent the domain information and then used some similarity metric to compute the distance.\n proposed multi-domain discriminant analysis to perform class-wise kernel learning for DG, which is more fine-grained.\nTo sum up, this category of methods is often highly related to other categories to act as their divergence measures or theoretical support.", "cites": [8578, 2690, 2713, 2714, 8581], "cite_extract_rate": 0.5454545454545454, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of kernel-based methods in domain generalization, listing several approaches and their goals without deep comparison or critique. It integrates some ideas across the cited papers, particularly around the use of kernel functions for domain-invariant representations, but lacks a nuanced analysis of their relative strengths, weaknesses, or theoretical implications. There is minimal abstraction beyond the specific methods discussed."}}
{"id": "12944aaf-2fa5-4305-8f9a-f9b341c4ac87", "title": "Domain adversarial learning", "level": "paragraph", "subsections": [], "parent_id": "e56e6ae8-5c5a-4b37-bc97-ea7f31e81e20", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Methodology"], ["subsection", "Representation Learning"], ["subsubsection", "Domain-invariant representation-based DG"], ["paragraph", "Domain adversarial learning"]], "content": "}\nDomain-adversarial training is widely used for learning domain-invariant features.\n and  proposed Domain-adversarial neural network (DANN) for domain adaptation, which adversarially trains the generator and discriminator. The discriminator is trained to distinguish the domains while the generator is trained to fool the discriminator to learn domain invariant feature representations. \n adopted such idea for DG.\n used adversarial training by gradually reducing the domain discrepancy in a manifold space.\n proposed a conditional invariant adversarial network (CIAN) to learn class-wise adversarial networks for DG.\nSimilar ideas were also used in .\n used single-side adversarial learning and asymmetric triplet loss to make sure only the real faces from different domains were indistinguishable, but not for the fake ones. After that, the extracted features of fake faces are more dispersed than before in the feature space and those of real ones are more aggregated, leading to a better generalized class boundary for unseen domains.\nIn addition to adversarial domain classification,  introduced additional entropy regularization by minimizing the KL divergence between the conditional distributions of different training domains to push the network to learn domain-invariant features. \nSome other GAN-based methods ~ were also proposed with theoretically guaranteed generalization bound.", "cites": [2692, 2718, 2717, 2715, 2719, 2716], "cite_extract_rate": 0.46153846153846156, "origin_cites_number": 13, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of domain adversarial learning methods in DG, listing various papers and their approaches without synthesizing them into a broader framework. There is minimal critical analysis of the methods or limitations, and abstraction is limited to reiterating the adversarial training concept rather than identifying overarching principles or trends."}}
{"id": "78952e7d-2db7-4226-884b-e50e0dec3e0b", "title": "Explicit feature alignment", "level": "paragraph", "subsections": [], "parent_id": "e56e6ae8-5c5a-4b37-bc97-ea7f31e81e20", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Methodology"], ["subsection", "Representation Learning"], ["subsubsection", "Domain-invariant representation-based DG"], ["paragraph", "Explicit feature alignment"]], "content": "}\nThis line of works aligns the features across source domains to learn domain-invariant representations through explicit feature distribution alignment , or feature normalization .\n introduced a cross-domain contrastive loss for representation learning, where mapped domains are semantically aligned and yet maximally separated.\nSome methods explicitly minimized the feature distribution divergence by minimizing the maximum mean discrepancy (MMD) , second order correlation , both mean and variance (moment matching) , Wasserstein distance , \\etcno, of domains for either \\da or \\dg.\n aligned the marginal distribution of different source domains via optimal transport by minimizing the Wasserstein distance to achieve domain-invariant feature space.\nMoreover, there are some works that used feature normalization techniques to enhance domain generalization capability .\n introduced Instance Normalization (IN) layers to CNNs to improve the generalization capability of models. IN has been extensively investigated in the field of image style transfer , where the style of an image is reflected by the IN parameters, \\ieno, mean and variance of each feature channel. Thus, IN layers  could be used to eliminate instance-specific style discrepancy to enhance generalization . However, IN is task agnostic and may remove some discriminative information. In IBNNet, IN and Batch Normalization (BN) are utilized in parallel to preserve some discriminative information . In , BN layers are replaced by Batch-Instance Normalization (BIN) layers, which adaptively balance BN and IN for each channel by selectively using BN and IN.\n proposed a Style Normalization and Restitution (SNR) module to simultaneously ensure both high generalization and discrimination capability of the networks. After the style normalization by IN, a restitution step is performed to distill task-relevant discriminative features from the residual (\\ieno, the difference between the original feature and the style normalized feature) and add them back to the network to ensure high discrimination. The idea of restitution is extended to other alignment-based method to restorate helpful discriminative information dropped by alignment .\nRecently,  applied IN to unsupervised DG where there are no labels in the training domains to acquire invariant and transferable features.\nA combination of different normalization techniques is presented in  to show that adaptively learning the normalization technique can improve DG.\nThis category of methods is more flexible and can be applied to other kind of categories.", "cites": [2724, 2722, 154, 2735, 2727, 2732, 2734, 156, 2721, 2730, 2726, 2729, 2725, 2731, 2728, 2720, 2733, 2723], "cite_extract_rate": 0.75, "origin_cites_number": 24, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively synthesizes a range of methods centered around explicit feature alignment, connecting normalization techniques (e.g., IN, BN, BIN) with distribution alignment (e.g., MMD, CORAL, Wasserstein). It shows critical analysis by highlighting limitations such as task agnosticism and the potential loss of discriminative information due to normalization. While it identifies patterns in the use of normalization and alignment, it does not fully abstract to a higher-level theoretical understanding but provides meaningful insights into methodological trade-offs."}}
{"id": "2c750554-ebe1-4a6c-8f5a-0e80f13d5d3b", "title": "Invariant risk minimization (IRM)", "level": "paragraph", "subsections": [], "parent_id": "e56e6ae8-5c5a-4b37-bc97-ea7f31e81e20", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Methodology"], ["subsection", "Representation Learning"], ["subsubsection", "Domain-invariant representation-based DG"], ["paragraph", "Invariant risk minimization (IRM)"]], "content": "}\n\\newcommand{\\clG}{\\mathcal{G}}\n considered another perspective on the domain-invariance of representation for domain generalization.\nThey did not seek to match the representation distribution of all domains, but to enforce the optimal classifier on top of the representation space to be the same across all domains.\nThe intuition is that the ideal representation for prediction is the \\emph{cause} of $y$, and the causal mechanism should not be affected by other factors/mechanisms, thus is domain-invariant.\nFormally, IRM can be formulated as:\n\\begin{align}\n    \\min_{\\substack{g \\in \\clG, \\\\\n        f \\in \\bigcap_{i=1}^M \\arg\\min_{f' \\in \\clF} \\epsilon^i (f' \\circ g)}}\n    \\sum_{i=1}^M \\epsilon^i (f \\circ g)\n\\end{align}\nfor some function classes $\\clF$ of $g$ and $\\clF$ of $f$.\nThe constraint for $f$ embodies the desideratum that all domains share the same representation-level classifier, and the objective function encourages $f$ and $g$ to achieve a low source domain risk.\nHowever, this problem is hard to solve as it involves an inner-level optimization problem in its constraint.\nThe authors then develop a surrogate problem to learn the feature extractor $g$ that is much more practical:\n\\begin{align}\n    \\min_{g \\in \\clG} \\sum_{i=1}^M \\epsilon^i (g) + \\lambda \\left\\| \\left. \\nabla_f \\epsilon^i (f \\circ g) \\right|_{f=1} \\right\\|^2,\n\\end{align}\nwhere a dummy representation-level classifier $f = 1$ is considered, and the gradient norm term measures the optimality of this classifier.\nThe work also presents a generalization theory under a perhaps strong linear assumption, that for plenty enough source domains, the ground-truth invariant classifier can be identified.\nIRM has gain notable visibility recently.\nThere are some further theoretical analyses on the success~ and failure cases of IRM~,\nand IRM has been extended to other tasks including text classification~ and reinforcement learning~.\nThe idea to pursue the invariance of optimal representation-level classifier is also extended.\n promote this invariance by minimizing the extrapolated risk among source domains, which essentially minimizes the variance of source-domain risks.\n aim to learn such a representation in a self-supervised setup, where the second domain is constructed by data augmentation showing various semantic-irrelevant variations.\nRecently,  found the invariance of $f$ alone is not sufficient. They found IRM still fails if $g$ captures ``fully informative invariant features'', which makes $y$ independent of $x$ on all domains. This is particularly the case for classification (vs. regression) tasks. An information bottleneck regularization is hence introduced to maintain only partially informative features.", "cites": [2740, 2736, 2737, 2739, 195, 8562, 2738], "cite_extract_rate": 0.875, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a coherent synthesis of the IRM framework and related works, connecting the theoretical motivation to practical formulations and extensions. It critically evaluates the limitations of IRM and highlights subsequent improvements, such as the introduction of information bottleneck regularization. The narrative abstracts key principles, such as the importance of invariance in representation and the insufficiency of invariance alone for classification tasks."}}
{"id": "2ab928a0-284f-49a1-91be-e4dec76a8b7a", "title": "Multi-component analysis", "level": "paragraph", "subsections": [], "parent_id": "997cdaa1-ad9d-4625-a0f9-f2b3f7e71b26", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Methodology"], ["subsection", "Representation Learning"], ["subsubsection", "Feature disentanglement-based DG"], ["paragraph", "Multi-component analysis"]], "content": "}\nIn multi-component analysis, the domain-shared and domain-specific features are in general extracted using the domain-shared and domain-specific network parameters.\nThe method of UndoBias~ started from a SVM model to maximize interval classification on all training data for \\dg.\nThey represented the parameters of the $i$-th domain as $\\mathbf{w}_i=\\mathbf{w}_0+\\Delta_i$, where $\\mathbf{w}_0$ denotes the domain-shared parameters and $\\Delta_i$ denotes the domain-specific parameters.\nSome other methods extented the idea of UndoBias from different aspects.\n proposed to use multi-view learning for \\dg. They proposed Multi-view DG (MVDG) to learn the combination of exemplar SVMs under different views for robust generalization.\n designed domain-specific networks for each domain and one shared domain-invariant network for all domains to learn disentangled representations, where low-rank reconstruction is adopted to align two types of networks in structured low-rank fashion.\n extended the idea of UndoBias into the neural network context and developed a low-rank parameterized CNN model for end-to-end training.\n learned disentangled representations through manually comparing the attention heat maps for certain areas from different domains.\nThere are also other works that adopt multi-component analysis for disentanglement~.\nIn general, multi-component analysis can be implemented in different architectures and remains effective for representation disentanglement.", "cites": [2744, 2741, 2746, 2684, 2745, 2743, 2742], "cite_extract_rate": 0.5, "origin_cites_number": 14, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of methods that use multi-component analysis for domain generalization, mentioning a few approaches and their general strategies. However, it lacks deeper integration of the underlying concepts and does not present a novel framework. It offers minimal critical evaluation of the cited papers and does not highlight strengths or limitations. The abstraction level is low, focusing mainly on concrete methods rather than overarching principles or trends."}}
{"id": "958524db-df54-4f38-8dc4-11ca6a253b55", "title": "Generative modeling", "level": "paragraph", "subsections": [], "parent_id": "997cdaa1-ad9d-4625-a0f9-f2b3f7e71b26", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Methodology"], ["subsection", "Representation Learning"], ["subsubsection", "Feature disentanglement-based DG"], ["paragraph", "Generative modeling"]], "content": "}\nGenerative models can be used for disentanglement from the perspective of data generation process.\nThis kind of methods tries to formulate the generative mechanism of the samples from the domain-level, sample-level, and label-level.\nSome works further disentangle the input into class-irrelevant features, which contain the information related to specific instance~.\nThe Domain-invariant variational autoencoder (DIVA)~ disentangled the features into domain information, category information, and other information, which is learned in the VAE framework.\n disentangled the fine-grained domain information and category information that are learned in VAEs.\n also used VAE for disentanglement, where they proposed a Unified Feature Disentanglement Network (UFDN) that treated both data domains and image attributes of interest as latent factors to be disentangled.\nSimilarly,  disentangled the semantic and variational part of the samples.\nSimilar spirits also include .\n proposed to disentangle the style and other information using generative models that their method is both for \\da and \\dg.\nGenerative models can not only improve OOD performance, but can also be used for generation tasks, which we believe is useful for many potential applications.", "cites": [2749, 2704, 2747, 2751, 2750, 2748], "cite_extract_rate": 0.75, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 1.8, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of generative modeling approaches for feature disentanglement in domain generalization, listing several methods and their goals without substantial synthesis or critical evaluation. While it mentions common themes such as using VAEs to separate domain, class, and residual features, it lacks a structured comparison or deeper abstraction to broader principles. The writing also contains incomplete sentences and missing citations, which detract from its analytical depth."}}
{"id": "2c4a28d3-c558-475c-b044-b894476bef33", "title": "Causality-inspired methods", "level": "paragraph", "subsections": [], "parent_id": "997cdaa1-ad9d-4625-a0f9-f2b3f7e71b26", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Methodology"], ["subsection", "Representation Learning"], ["subsubsection", "Feature disentanglement-based DG"], ["paragraph", "Causality-inspired methods"]], "content": "}\nCausality is a finer description of variable relations beyond statistics (joint distribution). Causal relation gives information of how the system will behave under intervention, so it is naturally suitable for transfer learning tasks, since domain shift can be seen as an intervention. Particularly, under the causal consideration, the desired representation is the true cause of the label (e.g., object shape), so that prediction will not be affected by intervention on correlated but semantically irrelevant features (e.g., background, color, style). There are a number of works~ that exploited causality for domain adaptation.\nFor domain generalization,  reweighted input samples in a way to make the weighted correlation reflect causal effect.  took Fourier features as the causing factors of images, and enforce the independence among these features. Using the additional data of object identity (it is a more detailed label than the class label),  enforced the conditional independence of the representation from domain index given the same object. When such object label is unavailable,  further learned an object feature based on labels in a separate stage. For single-source domain generalization,  used data augmentation to present information of the causal factor. The augmentation operation is seen as producing outcomes under intervention on irrelevant features, which is implemented based on specific domain knowledge.\nThere are also generative methods under the causal consideration.  explicitly modeled a manipulation variable that causes domain shift, which may be unobserved.\n leveraged causal invariance for single-source generalization, i.e., the invariance of the process of generating $(x,y)$ data based on the factors, which is explained more general than inference invariance that existing methods implicitly rely on. The two factors are allowed correlated which is more realistic. They theoretically prove the identifiability of the causal factor is possible and the identification benefits generalization.  extended the method and theory to multiple source domains. With more informative data, the irrelevant factor is also identifiable.", "cites": [2756, 2753, 2754, 2757, 2752, 8562, 8582, 7112, 2755, 8583], "cite_extract_rate": 0.7692307692307693, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple causality-inspired methods from the cited papers, connecting ideas such as causal factors, independence constraints, and modeling domain shifts as interventions. It provides a coherent narrative by abstracting these approaches into a causal framework for domain generalization. While it includes some critical discussion, such as the importance of identifiability and the realism of allowing correlated factors, it could offer more explicit comparisons or limitations for deeper analysis."}}
{"id": "adcdcb59-2df4-433f-9221-644ec47d7864", "title": "Ensemble learning-based DG", "level": "subsubsection", "subsections": [], "parent_id": "ce90feea-3834-4cf8-8fff-8632dbb6b58b", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Methodology"], ["subsection", "Learning Strategy"], ["subsubsection", "Ensemble learning-based DG"]], "content": "Ensemble learning usually combines multiple models, such as classifiers or experts, to enhance the power of models.\nFor \\dg, ensemble learning exploits the relationship between multiple source domains by using specific network architecture designs and training strategies to improve generalization.\nThey assume that any sample can be regarded as an integrated sample of the multiple source domains, so the overall prediction result can be seen as the superposition of the multiple domain networks.\n proposed to use learnable weights for aggregating the predictions from different source specific classifiers, where a domain predictor is used to predict the probability that a sample belongs to each domain (weights). \n maintained domain-dependent batch normalization (BN) statistics and BN parameters for different source domains while all the other parameters were shared. In inference, the final prediction was a linear combination of the domain-dependent models with the combination weights inferred by measuring the distances between the instance normalization statistics of the test sample and the accumulated population statistics of each domain. \nThe work of  proposed domain-specific layers of different source domains and learning the linear aggregation of these layers to represent a test sample.\n proposed Domain Adaptive Ensemble Learning (DAEL), where a DAEL model is composed of a CNN feature extractor shared across domains and multiple domain-specific classifier heads. Each classifier is an expert to its own domain and a non-expert to others. DAEL aims to learn these experts collaboratively, by teaching the non-experts with the expert so as to encourage the ensemble to learn how to handle data from unseen domains.\nThere are also other works~.\nEnsemble learning remains a powerful tool for DG since ensemble allows more diversity of models and features.\nHowever, one drawback of ensemble learning-based DG is maybe its computational resources as we need more space and computations for training and saving different models.", "cites": [2685, 7113, 2759, 8584, 2758], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key ideas from multiple papers, highlighting common themes like domain-specific components and aggregation strategies. It abstracts the concept of ensemble learning in DG to emphasize model diversity and adaptability. While it includes a general critique of computational costs, a more in-depth comparison or limitation-based analysis of the individual approaches could further elevate the critical dimension."}}
{"id": "f370a79b-97b1-4d00-a608-c6b46ad6848e", "title": "Meta-learning-based DG", "level": "subsubsection", "subsections": [], "parent_id": "ce90feea-3834-4cf8-8fff-8632dbb6b58b", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Methodology"], ["subsection", "Learning Strategy"], ["subsubsection", "Meta-learning-based DG"]], "content": "The key idea of meta-learning is to learn a general model from multiple tasks by either optimization-based methods~, metric-based learning~, or model-based methods~.\nThe idea of meta-learning has been exploited for \\dg. \nThey divide the data form multi-source domains into meta-train and meta-test sets to simulate domain shift.\nDenote $\\theta$ the model parameters to be learned, meta-learning can be formulated as:\n\\begin{equation}\n    \\label{eq-meta}\n    \\begin{split}\n    \\theta^\\ast &= \\operatorname{Learn}(\\mathcal{S}_{mte}; \\phi^\\ast)\\\\ &= \\operatorname{Learn}(\\mathcal{S}_{mte}; \\operatorname{MetaLearn}(\\mathcal{S}_{mtrn})),\n    \\end{split}\n\\end{equation}\nwhere $\\phi^\\ast = \\operatorname{MetaLearn}(\\mathcal{S}_{mtrn})$ denotes the meta-learned parameters from the meta-train set $\\mathcal{S}_{mtrn}$ which is then used to learn the model parameters $\\theta^\\ast$ on the meta-test set $\\mathcal{S}_{mte}$.\nThe two functions $\\operatorname{Learn}(\\cdot)$ and $\\operatorname{MetaLearn}(\\cdot)$ are to be designed and implemented by different meta-learning algorithms, which corresponds to a bi-level optimization problem.\nThe gradient update can be formulated as:\n\\begin{equation}\n    \\theta = \\theta - \\alpha \\frac{\\partial (\\ell(\\mathcal{S}_{mte};\\theta) + \\beta \\ell(\\mathcal{S}_{mtrn};\\phi))}{\\partial \\theta},\n\\end{equation}\nwhere $\\eta$ and $\\beta$ are learning rates for outer and inner loops, respectively.\n proposed  Model-agnostic meta-learning (MAML). Inspired by MAML,  proposed MLDG (meta-learning for \\dg) to use the meta-learning strategy for DG.\nMLDG splits the data from the source domains into meta-train and meta-test to simulate the domain shift situation to learn general representations.\n proposed to learn a meta regularizer (MetaReg) for the classifier.  proposed feature-critic training for the feature extractor by designing a meta optimizer.\n used the similar idea of MLDG and additionally introduced two complementary losses to explicitly regularize the semantic structure of feature space.\n proposed an extended version of information bottleneck named Meta Variational Information Bottleneck (MetaVIB).\nThey regularize the Kullbackâ€“Leibler (KL) divergence between distributions of latent encoding of the samples that have the same category from different domains and learn to generate weights by using stochastic neural networks.\nRecently, some works also adopted meta-learning for semi-supervised DG or discriminative DG~.\nMeta-learning is widely adopted in DG research and it can be incorporated into several paradigms such as disentanglement~.\nMeta-learning performs well on massive domains since meta-learning can seek transferable knowledge from multiple tasks.", "cites": [1695, 2760, 8580, 8579, 2762, 8757, 2761, 2745, 2763, 2686], "cite_extract_rate": 0.7692307692307693, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple meta-learning-based DG approaches and connects them under a common framework, showing a reasonable understanding of the shared principles (e.g., domain shift simulation, bi-level optimization). However, the critical evaluation is limited, as it does not deeply assess the strengths, weaknesses, or trade-offs of these methods. It offers some abstraction by identifying overarching strategies like domain-invariant representation learning and probabilistic modeling, but the insights remain focused on methodological patterns rather than deeper theoretical or practical implications."}}
{"id": "fb03502d-a406-47f0-b9db-65a87049657c", "title": "Gradient operation-based DG", "level": "subsubsection", "subsections": [], "parent_id": "ce90feea-3834-4cf8-8fff-8632dbb6b58b", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Methodology"], ["subsection", "Learning Strategy"], ["subsubsection", "Gradient operation-based DG"]], "content": "Other than meta-learning and ensemble learning, several recent works consider using gradient information to force the network to learn generalized representations.\n proposed a self-challenging training algorithm that aims to learn general representations by manipulating gradients. \nThey iteratively discarded the dominant features activated on the training data, and forced the network to activate remaining features that correlate with labels. In this way, network can be forced to learn from more bad cases which will improve generalization ability.\n proposed a gradient-matching scheme, where their assumption is that the gradient direction of two domains should be the same to enhance common representation learning.\nTo this end, they proposed to maximize the gradient inner product (GIP) to align the gradient direction\nacross domains. With this operation, the network can find weights such that the input-output correspondence is as close as possible\nacross domains. GIP can be formulated as:\n\\begin{equation}\n    \\mathcal{L}=\\mathcal{L}_{\\text {cls }}\\left(\\mathcal{S}_{t rain} ; \\theta\\right)-\\lambda \\frac{2}{M(M-1)} \\sum_{i,j}^{i \\neq j} G_{i} \\cdot G_{j},\n\\end{equation}\nwhere $G_i$ and $G_j$ are gradient for two domains that can be calulated as $G=\\mathbb{E} \\frac{\\partial \\ell(x, y; \\theta)}{\\partial \\theta}$.\nThe gradient-invariance was achieved by adding CORAL~ loss between gradients in , while  maximized the neuron coverage of\nDNN with gradient similarity regularization between the\noriginal and the augmented samples.\nAdditionally,  designed a knowledge distillation approach for based on gradient filtering.", "cites": [7114, 2734, 2764, 2765, 8585, 7614], "cite_extract_rate": 1.0, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple gradient-based domain generalization techniques, connecting ideas such as gradient discarding, gradient matching, and gradient regularization to show a common goal of improving generalization. It offers a coherent narrative by grouping these works under a shared theme. While it does provide some level of abstraction by highlighting gradient alignment as a unifying concept, it lacks deeper critical evaluation or comparison of the approaches' strengths, weaknesses, and empirical effectiveness. The analysis remains mostly descriptive with limited critique."}}
{"id": "3aa562e5-e067-4d11-9f3a-cf8c5e25c952", "title": "Distributionally robust optimization-based DG", "level": "subsubsection", "subsections": [], "parent_id": "ce90feea-3834-4cf8-8fff-8632dbb6b58b", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Methodology"], ["subsection", "Learning Strategy"], ["subsubsection", "Distributionally robust optimization-based DG"]], "content": "The goal of distributionally robust optimization (DRO)~ is to learn a model at worst-case distribution scenario to hope it can generalize well to the test data, which shares similar goal as DG.\nTo optimize the worst-case distribution scenario,  proposed a GroupDRO algorithm that requires explicit group annotation of the samples.\nSuch annotation was later narrowed down to a small fraction of validation set in , where they formulated a two-stage weighting framework.\nOther researchers reduced the variance of training domain risks by risk extrapolation (VRex)~ or reducing class-conditioned Wasserstein DRO~.\nRecently,  proposed the setting of subpopulation shift where they also applied DRO to this problem.\nParticularly,  proposed AdaRNN, a similar algorithm to the spirit of DRO that did not require explicit group annotation; instead, they learned the worst-case distribution scenario by solving an optimization problem.\nTo summarize, DRO focuses on the optimization process that can also be leveraged in DG research.", "cites": [2768, 2767, 2766, 195, 2769, 7615], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several DRO-based DG approaches, connecting their goals and strategies to form a coherent narrative around robust optimization. It highlights key developments such as the shift from explicit group annotations to more adaptive methods. While it includes some critical analysis (e.g., limitations of requiring group annotations), deeper evaluation or comparison of strengths and weaknesses is limited, keeping the abstraction at a moderate level."}}
{"id": "cb34ea1d-e0b2-413f-8bb0-a183ef7ae1e5", "title": "Self-supervised learning-based DG", "level": "subsubsection", "subsections": [], "parent_id": "ce90feea-3834-4cf8-8fff-8632dbb6b58b", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Methodology"], ["subsection", "Learning Strategy"], ["subsubsection", "Self-supervised learning-based DG"]], "content": "Self-supervised learning (SSL) is a recently popular learning paradigm that builds self-supervised tasks from the large-scale unlabeled data~.\nInspired by this,  introduced a self-supervision task of solving jigsaw puzzles to learn generalized representations.\nApart from introducing new pretext tasks, contrastive learning is another popular paradigm of self-supervised learning, which was adopted in several recent works~.\nThe core of contrastive learning is to perform unsupervised learning between positive and negative pairs.\nNote that self-supervised learning is a general paradigm that can be applied to any existing DG methods, especially unsupervised DG where there are no labels in training domains~.\nAnother possible application of SSL-based DG is the pretraining of multi-domain data that trains powerful pretraining models while also handling domain shifts.\nHowever, a possible limitation of SSL-based DG maybe its computational efficiency and requirement of computing resources.", "cites": [2770, 128, 322, 2771, 8586], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic overview of self-supervised learning (SSL) in the context of domain generalization, mentioning a few approaches and their general goals. While it attempts to connect SSL to DG and identifies its potential application in unsupervised DG and pretraining, it lacks a deeper synthesis of the cited works into a cohesive framework. The analysis is minimal, with only a brief mention of computational limitations without elaborating on their significance or comparing approaches. Some patterns are identified, such as the use of pretext tasks and contrastive learning, but the discussion remains largely at the level of individual methods rather than offering higher-level insights."}}
{"id": "8fef70a7-65fe-41b4-a324-4da5cfcf8b34", "title": "Other learning strategy for DG", "level": "subsubsection", "subsections": [], "parent_id": "ce90feea-3834-4cf8-8fff-8632dbb6b58b", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Methodology"], ["subsection", "Learning Strategy"], ["subsubsection", "Other learning strategy for DG"]], "content": "There are some other learning strategies for \\dg.\nFor instance, metric learning was adopted in  to explore better pair-wise distance for DG.\n used random forest to improve the generalization ability of convolutional neural networks (CNN). They sampled the triplets based on the probability mass function of the split results given by random forest, which is used for updating the CNN parameters by triplet loss.\nOther works~ adopted model calibration for DG, where they argued that the calibrated performance has a close relationship with OOD performance.\n followed the lottery ticket hyphothesis to design network substructures for DG, while  focused on the shape-invariant features.\nAdditionally,  observed the flat minima is important to DG and they designed a simple stochastic weight averaging densely method to find the flat minima.\nSince DG is a general learning problem, there will be more works that uses other strategy in the future.", "cites": [7115, 2772, 2774, 2776, 2775, 2773], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a brief overview of different learning strategies for domain generalization but lacks deep synthesis or critical evaluation. It lists methods and their general ideas without establishing strong connections between them or analyzing their strengths and weaknesses. Some abstraction is attempted, but the discussion remains superficial and does not offer a novel or unifying perspective."}}
{"id": "405d9cac-bccb-4e2e-b4a5-ebaacadd0a56", "title": "Single-source Domain Generalization", "level": "subsection", "subsections": [], "parent_id": "52a80918-a2fa-4678-a45e-1411b6499857", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Other Domain Generalization Research Areas"], ["subsection", "Single-source Domain Generalization"]], "content": "Setting $M=1$ in Def.~\\ref{def-dg} gives single-source DG.\nCompared to traditional DG ($M>1$), single-source DG becomes more challenging since there are less diversity in training domains.\nThus, the key to this problem is to generate novel domains using data generation techniques to increase the diversity and informativeness of training data.\nSeveral methods designed different generation strategies~ for single-source DG in computer vision tasks.\nA recent work~ studied this setting in time series data where there is usually one unified dataset by using min-max optimization.\nWe expect more application areas can benefit from single-source DG.", "cites": [2756, 2729, 2710, 2719, 2777, 2704, 7615, 8586], "cite_extract_rate": 0.8, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of single-source domain generalization and briefly mentions several methods, but it lacks meaningful synthesis or comparison of the cited works. It does not evaluate their strengths and weaknesses or connect them to a broader conceptual framework. The content remains largely surface-level and descriptive."}}
{"id": "008cfae4-52d4-4ad5-a04a-dd68eca8ee07", "title": "Semi-supervised Domain Generalization", "level": "subsection", "subsections": [], "parent_id": "52a80918-a2fa-4678-a45e-1411b6499857", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Other Domain Generalization Research Areas"], ["subsection", "Semi-supervised Domain Generalization"]], "content": "Compared to traditional DG, semi-supervised DG does not require the full labels of training domains.\nIt is common to apply existing semi-supervised learning algorithms such as FixMatch~ and FlexMatch~ to learn pseudo labels for the unlabeled samples.\nFor instance, two recent works adopted the consistency regularization in semi-supervised learning~ for semi-supervised DG.\nIt can be seen that this setting is more general than traditional DG and we expect there will be more works in this area.", "cites": [2699, 8587, 7192], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a brief, factual overview of semi-supervised domain generalization but lacks meaningful synthesis or comparison across the cited works. It mentions methods like FixMatch and FlexMatch without evaluating their strengths or weaknesses. Additionally, it does not abstract broader patterns or principles from the discussed approaches."}}
{"id": "edffeacc-1721-4d2e-a017-21a754d348b9", "title": "Federated Learning with Domain Generalization", "level": "subsection", "subsections": [], "parent_id": "52a80918-a2fa-4678-a45e-1411b6499857", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Other Domain Generalization Research Areas"], ["subsection", "Federated Learning with Domain Generalization"]], "content": "Privacy and security of machine learning is becoming increasingly critical~.\n firstly studied this problem and showed that if the features are stable, then the model is more robust to membership inference attack.\nIn federated DG~, models do not access the raw training data; instead, they aggregate the parameters from different clients.\nUnder this circumstance, the key is to design better aggregation scheme through generalization techniques~.\nFederated DG is more important in healthcare~. On the other hand, decentralized training is another possible solution~. However, similar privacy risks emerge when there is a need to update the model. Thus, we hope there could be more research.", "cites": [2780, 2779, 671, 2778], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of federated learning with domain generalization and mentions some related papers, but it lacks a deep synthesis of their contributions. It does not critically evaluate the methods or identify meaningful trends or limitations. The abstraction is minimal, with the content remaining close to the surface-level descriptions of individual papers."}}
{"id": "25f8c054-1495-4046-8388-f2c86c369e55", "title": "Other DG Settings", "level": "subsection", "subsections": [], "parent_id": "52a80918-a2fa-4678-a45e-1411b6499857", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Other Domain Generalization Research Areas"], ["subsection", "Other DG Settings"]], "content": "There are also other settings in DG, such as open \\dg~ and unsupervised \\dg~.\nOpen DG shares the similar setting of universal domain adaptation where the training and test label spaces are not the same.\nUnsupervised DG assumes that all labels in the training domains are not accessible.\nAs the environment gets more general and challenging, there will be other DG research areas aiming at solving certain limitations.", "cites": [2711], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a minimal description of other DG settings, such as Open DG and Unsupervised DG, with limited synthesis of the cited paper and no comparative or evaluative analysis. It lacks deeper abstraction or integration of broader patterns, offering only basic definitions and a hint at future research directions."}}
{"id": "9df407ef-782a-4041-ba2f-cc9cac5ea90a", "title": "Applications", "level": "section", "subsections": [], "parent_id": "e93cd0c5-bcfd-4f51-83f9-f1a68d346733", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Applications"]], "content": "\\label{sec-app}\nIn this section, we discuss the popular tasks/applications for \\dg (ref. \\figurename~\\ref{fig-app}).\nHigh generalization ability is desired in various vision tasks. Many works investigate DG on classification. Some works also study DG for semantic segmentation~, action recognition~, face anti-spoofing~, person Re-ID~, street view recognition~, video understanding~, and image compression~.\nMedical analysis~ is one of the important application areas for DG due to its nature of data scarcity and existence of domain gaps, with the tasks of tissue segmentation~, Parkinson's disease recognition~, activity recognition~, chest X-ray recognition~, and EEG-based seizure detection~. \nApart from those areas, DG is also useful in reinforcement learning of robot control~ to generalize to an unseen environment.\nSome work used DG to recognize speech utterance~, fault diagnosis~, physics~, brain-computer interface~.\n\\begin{figure}[t!]\n\t\\centering\n\t\\resizebox{.5\\textwidth}{!}{\n\t\\input{figures/fig-application}\n\t}\n\t\\caption{Several applications of \\dg.}\n\t\\label{fig-app}\n\\end{figure}\nIn natural language processing, it is also common that training data comes from different domains with different distributions and DG techniques are helpful. \nSome work used \\dg for sentiment classification on the Amazon Review dataset~.\nOthers used DG for semantic parsing~, web page classification~. For instance, if we are given natural language data from multiple domains and want to learn a generalized model that predicts well on any new domain, we can use \\dg to acquire domain-invariant representations.\nMoreover, DG techniques favor broad prospects in some applications, such as financial analysis, weather prediction, and logistics.\nFor instance,  tried to adopt DG to time series modeling. They first propose the temporal covariate shift problem that widely exists in time series data, then, they proposed an RNN-based model to solve this problem to align the hidden representations between any pair of training data that are from different domains.\nTheir algorithm, the so-called AdaRNN, was applied to stock price prediction, weather prediction, and electric power consumption.\nAnother example is , where they applied domain generalization to sensor-based human activity recognition.\nIn their application, the activity data from different persons are from different distributions, resulting in severe model collapse when applied to new users.\nTo resolve such problem, they developed a variational autoencoder-based network to learn the domain-invariant and domain-specific modules, thus achieving the disentanglement.\nIn the future, we hope there can be more DG applications in other areas to tackle with the distributional shift that widely exists in different applications.\nAnother important problem is the evaluation of DG algorithms without accessing the test distribution in reality. While we can use the test data for evaluation in research, we simply cannot do it for real applications. In this case, one possible approach would be performing meta-train and meta-test split for the original data for multiple times. In each time, one split can be regarded as the unseen test data while the other as the training data. We can call it the meta-cross-validation for DG in reality. At the same time, we also hope there could be more evaluation metrics. For more evaluation in research, please refer to the next section.\n\\begin{table*}[t!]\n  \\centering\n  \\caption{Eighteen popular datasets for \\dg. The last ten datasets are from WILDS~.}\n  \\label{tb-dataset}\n    \\begin{tabular}{lrrrll}\n    \\toprule\n    Dataset & \\#Domain & \\#Class & \\#Sample & Description & Reference \\\\ \\hline\n    Office-Caltech & 4 & 10 & 2,533  & Caltech, Amazon, Webcam, DSLR &  \\\\\n    Office-31 & 3 & 31 & 4,110  & Amazon, Webcam, DSLR &  \\\\\n    PACS  & 4     & 7     & 9,991  & Art, Cartoon, Photos, Sketches &  \\\\\n    VLCS  & 4     & 5     & 10,729  & Caltech101, LabelMe, SUN09, VOC2007 &  \\\\\n    Office-Home & 4     & 65    & 15,588  & Art, Clipart, Product, Real &  \\\\\n    Terra Incognita & 4     & 10    & 24,788  & Wild animal images taken at locations L100, L38, L43, L46 &  \\\\\n    Rotated MNIST & 6     & 10    & 70,000  & Digits rotated from $0^\\circ$ to $90^\\circ$ with an interval of $15^\\circ$ &  \\\\\n    DomainNet & 6     & 345   & 586,575 & Clipart, Infograph, Painting, Quickdraw, Real, Sketch &  \\\\\n    iWildCam2020-wilds  & 323     & 182                                                     & 203,029 & Species classification across different   camera traps          &                          \\\\\n    Camelyon17-wilds    & 5       & 2                                                       & 45,000  & Tumor identification across five different hospitals            &                         \\\\\n    RxRx1-wilds         & 51      & 1,139                                                   & 84,898  & Genetic perturbation classification across experimental batches &                            \\\\\n    OGB-MolPCBA         & 120,084 & 128                                                     & 400,000 & Molecular property prediction across different scaffolds        &                                 \\\\\n    GlobalWheat-wilds   & 47      & bounding boxes     & 6,515   & Wheat head detection across regions of the world                &           \\\\\n    CivilComments-wilds & -       & 2                                                       & 450,000 & Toxicity classification across demographic identities           &                          \\\\\n    FMoW-wilds          & 80      & 62                                                      & 118,886 & Land use classification across different regions and years      &                     \\\\\n    PovertyMap-wilds    & 46      & real value & 19,669  & Poverty mapping across different countries                      &                               \\\\\n    Amazon-wilds        & 3920    & 5                                                       & 539,502 & Sentiment classification across different users                 &                           \\\\\n    Py150-wilds         & 8,421   & next token                                              & 150,000 & Code completion across different codebases                      & \\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{table*}", "cites": [2708, 2718, 2724, 2715, 2786, 2704, 2781, 2716, 1567, 2755, 2760, 2785, 2743, 2782, 7615, 2686, 8589, 2707, 2787, 2690, 8588, 2733, 2784, 2761, 2684, 2702, 2769, 2783], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 49, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a broad list of DG applications across vision, medical analysis, NLP, and other domains, citing relevant papers. However, it lacks synthesis of core ideas or themes across these works, and offers minimal critical analysis or abstraction beyond describing specific uses of DG techniques. The narrative is largely descriptive, with little insight into overarching principles or comparative evaluations of approaches."}}
{"id": "2de4b50f-870d-4b34-9a29-436561c22cf8", "title": "Datasets", "level": "subsection", "subsections": [], "parent_id": "cb4e6b83-9dd6-40ba-9d7c-af8cab7c735d", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Datasets, Evaluation, and Benchmark"], ["subsection", "Datasets"]], "content": "\\tablename~\\ref{tb-dataset} offers an overview of several popular datasets.\nAmong them, PACS~, VLCS~, and Office-Home~ are three most popular datasets.\nFor large-scale evaluation, DomainNet~ and Wilds~ (\\emph{i.e.,} a collection of datasets in \\tablename~\\ref{tb-dataset} with `-wilds') are becoming popular.\nBesides the datasets mentioned above, there exists some other datasets for \\dg with different tasks.\nThe Graft-versus-host disease (GvHD) dataset~ is also popular and is used to test several methods~ for the flow cytometry problem.\nThis dataset was collected from 30 patients with sample sizes ranging from 1,000 to 10,000. This is a time series classification dataset. \nSome works~ applied \\dg to semantic segmentation, where CityScape~ and GTA5~ datasets were adopted as benchmark datasets.\nSome works applied DG to object detection, using the datasets of Cityscapes~, GTA5~, Synthia~ for investigation~.\nSome other works used public datasets or RandPerson~ for person re-identification . Some works~ used the OpenAI Gym~ as the testbed to evaluate the performance of algorithms in reinforcement learning problems such as Cart-Pole and mountain car.\nIn addition to these widely used datasets, there are also other datasets used in existing literature.\nThe Parkinsonâ€™s telemonitoring dataset~ is popular for predicting the clinicianâ€™s motor and total UPDRS scoring of Parkinsonâ€™s disease symptoms from voice measures.\nSome methods~ used the data from several people as the training domains to learn models that generalize to unseen subjects.\nIt is worth noting that the datasets of domain generalization have some overlaps with domain adaptation.\nFor instance, Office-31, Office-Caltech, Office-Home, and DomainNet are also widely used benchmarks for \\da.\nTherefore, most domain adaptation datasets can be used for \\dg benchmark in addition to those we discussed here.\nFor example, Amazon Review dataset~ is widely used in \\da. It has four different domains on product review (DVDs, Kitchen appliance, Electronics and Books), which can also be used for \\dg.", "cites": [2718, 1790, 2727, 2785, 2732, 8578, 2726, 1733, 1346, 2686, 2707, 2690, 2733, 2684, 2769], "cite_extract_rate": 0.64, "origin_cites_number": 25, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of various datasets used in domain generalization, mentioning their application areas and some properties. It synthesizes basic information from the cited papers, such as the use of DomainNet and WILDS for large-scale evaluation, but does not deeply connect ideas across sources or build a comprehensive narrative. There is minimal critical analysis or identification of broader trends or limitations in the datasets or approaches."}}
{"id": "70d17a07-0f02-4664-aef0-a26e64d45355", "title": "Evaluation", "level": "subsection", "subsections": [], "parent_id": "cb4e6b83-9dd6-40ba-9d7c-af8cab7c735d", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Datasets, Evaluation, and Benchmark"], ["subsection", "Evaluation"]], "content": "To test \\dg algorithm on a test domain, three strategies are proposed~, namely, \\emph{Test-domain validation set}, \\emph{Leave-one-domain-out cross-validation}, and \\emph{Training-domain validation set}. \nTest-domain validation set utilizes parts of the target domain as validations.\nAlthough it can obtain the best performance in most circumstances for that validation and testing share the same distribution, there is often no access to targets when training, which means it cannot be adopted in real applications.\nLeave-one-domain-out is another strategy to choose the final model when training data contains multiple sources.\nIt leaves one training source as the validation while treating the others as the training part.\nObviously, when only a single source exists in the training data, it is no longer applicable.\nIn addition, due to different distributions among sources and targets, final results rely heavily on the selections of validation, which makes final results unstable.\nThe most common strategy for \\dg is Training-domain validation set which is used in most existing work.\nIn this strategy, each source is split into two parts, the training part and the validation part.\nAll training parts are combined for training while all validation parts are combined for selecting the best model.\nSince there still exists divergences between the combined validation and the real unseen targets, this simple and most popular strategy cannot achieve the best performance for some time.\nWe need to mention that there may exist other evaluation protocols for DG such as  since designing effective evaluation protocols is often consistent with the OOD performance.\nCurrently, most of the works adopted the train-domain validation strategy which may not always generate good performance since the distribution of validation set is not the same as the new training data. On the other hand, using accuracy alone may not be sufficient to valid the model performance. We are looking forward to new evaluation metrics that can truly reflect the properties test distributions as much as possible in order to obtain better results.\\looseness=-1", "cites": [2691, 2788], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of evaluation strategies in domain generalization by discussing their pros and cons. It integrates insights from the cited papers to highlight the limitations of existing methods, such as the mismatch between validation and test distributions. While it offers a coherent discussion and identifies broader patterns, it does not propose a novel framework or provide deep theoretical synthesis."}}
{"id": "a0ea5fba-ff98-41df-b891-9f4a6422c8ea", "title": "Benchmark", "level": "subsection", "subsections": [], "parent_id": "cb4e6b83-9dd6-40ba-9d7c-af8cab7c735d", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Datasets, Evaluation, and Benchmark"], ["subsection", "Benchmark"]], "content": "To test the performance of DG algorithms in a unified codebase, in this paper, we develop a new codebase for DG, named \\emph{DeepDG}~.\nCompared to the existing DomainBed~, DeepDG simplifies the data loading and model selection process, while also makes it possible to run all experiments in a single machine.\nDeepDG splits the whole process into a data preparation part, a model part, a core algorithm part, a program entry, and some other auxiliary functions.\nEach part can be freely modified by users without affecting other parts.\nUsers can add their own algorithms or datasets to DeepDG and compare with some state-of-the-art methods fairly.\nThe current public version of DeepDG is only for image classification and we offer supports for Office-31, PACS, VLCS, and Office-Home datasets.\nCurrently, nine state-of-the-art methods are implemented under the same environment, and it covers all three groups, including Data manipulation (Mixup~), Representation learning (DDC~, DANN~, CORAL~), and Learning strategy (MLDG~, RSC~, GroupDRO~, ANDMask~).\nWe conduct some experiments on the two most popular image classification datasets, PACS and Office-Home, with DeepDG and \\tablename~\\ref{tab:my-table-deepdg} shows the results.\nResNet-18 is used as the base feature network. \nTraining-domain validation set is used for selecting final models, and $20\\%$ of sources are for validation while the others are for training.\nFrom \\tablename~\\ref{tab:my-table-deepdg}, we observe more insightful conclusions.\n(1) The baseline method, ERM, has achieved acceptable results on both datasets. Some methods, such as DANN and ANDMask, even have worse performance.\n(2) Simple data augmentation method, Mixup, cannot obtain remarkable results. \n(3) CORAL has slight improvements on both datasets compared to ERM, which is consistent with results offered by DomainBed~.\n(4) RSC, a learning strategy, achieves the best performance on both datasets, but the improvements are unremarkable compared to ERM.\nThe results indicate the benefits of domain generalization in different tasks.\n\\input{tables/table_results}\n\\centering\n\\vspace{-.1in}\n\\caption{Summary of existing literature for \\dg.}\n\\label{tb-all}\n\\vspace{-.1in}\n\\resizebox{.75\\textwidth}{!}{\n\\begin{tabular}{lll}\n\\toprule\n\\textbf{Literature} & \\textbf{Category} & \\textbf{Application} \\\\ \\hline\n & Augmentation & Car detection \\\\\n & Augmentation, meta-learning & Image classification \\\\\n & Augmentation & Image classification \\\\\n & Augmentation & Reinforcement learning \\\\\n & Augmentation & Reinforcement learning \\\\\nCrossGrad~ & Augmentation, adversarial & Font/Image/speech recognition \\\\\n & Augmentation & Reinforcement learning \\\\\n & Augmentation & Image classification \\\\\n & Augmentation & Image classification \\\\\nDomainMix~ & Augmentation & Person Re-ID \\\\\nMixup~ & Augmentation & Image classification \\\\\n & Augmentation & Image classification, semantic segmentation \\\\\nMixStyle~ & Augmentation & Image classification, Person Re-ID, RL \\\\\nADAGE~ & Generation & Image classification \\\\\n & Generation & Image classification, hand pose recognition \\\\\nDDG~ & Generation & Image classification \\\\\n & Generation & Image classification \\\\\nUNVP~ & Generation & Image classification \\\\\nDeGIA~ & Generation & Image classification \\\\\nDDAIG~ & Generation & Image classification \\\\\nL2A-OT~ & Generation & Image classification \\\\\nM-ADA~ & Generation, disentangle & Image classification, semantic segmentation \\\\\nModel-based DG~ & Generation & Image classification \\\\\n\\hline\n & Kernel & Flow cytometry \\\\\nMTL~ & Kernel & Parkinson's disease, satellite image, flow cytometry \\\\\n & Kernel & Satellite image \\\\\nESRand~ & Kernel & Activity recognition \\\\\nKDICA~ & Kernel & Image classification \\\\\nSCA~ & Kernel & Image classification \\\\\nTCA~ & Kernel & Landmine detection, Parkinson's disease \\\\\nMDA~ & Kernel & Image classification \\\\\nCIDDG~ & Kernel & Image classification \\\\\nDICA~ & Kernel & Flow Cytometry, Parkinsonâ€™s disease \\\\\n & Explicit feature align & EEG seizure \\\\\n & Explicit feature align & Image classification, physics \\\\\nMTAE~ & Explicit feature align & Image classification \\\\\n & Explicit feature align & Brain computer interface \\\\\nSNR~ & Explicit feature align & Person Re-ID, Image classification, semantic segmentation, object detection \\\\\nFAR~ & Explicit feature align & Image classification \\\\\n & Explicit feature align & Fault diagnosis \\\\\nDSDGN~ & Explicit feature align & Fault diagnosis \\\\\n & Explicit feature align & Water object detection \\\\\nMatchDG~ & Explicit feature align & Image classification \\\\\n & Explicit feature align & Image classification \\\\\nCCSA~ & Explicit feature align & Image classification \\\\\nBNE~ & Explicit feature align & Image classification \\\\\nDSON~ & Explicit feature align & Image classification \\\\\nDFDG~ & Explicit feature align & Image classification \\\\\nDDGFD~ & Explicit feature align & Fault diagnosis \\\\\n & Explicit feature align & Image classification \\\\ \n & Adversarial & Image classification \\\\\nDANN~ & Adversarial & Image classification \\\\\nDLOW~ & Adversarial & Semantic segmentation \\\\\nSSDG~ & Adversarial & Face anti-spooï¬ng \\\\\nCIAN~ & Adversarial & Image classification \\\\\nMMD-AAE~ & Adversarial & Image classification, action recognition \\\\\nCAADG~ & Adversarial & Image classification \\\\\nMADDG~ & Adversarial & Face anti-spooï¬ng \\\\\nDANNCE~ & Adversarial & Image classification \\\\\n & Adversarial & Twitter sentence classification \\\\\n & Adversarial & Image classification \\\\\nIRM~ & Invariant risk minimization & Image classification \\\\\nDomain2Vec~ & Disentangle, multi-component & Image classification \\\\\n & Disentangle, multi-component & Image classification \\\\\nUndoBias~ & Disentangle, multi-component & Image classification \\\\\n & Disentangle, multi-component & Image classification \\\\\nLRE~ & Disentangle, multi-component & Image classification, action recognition \\\\\nMVDG~ & Disentangle, multi-component & Image/video classification \\\\\nCSD~ & Disentangle, multi-component & Image/speech utterance classification \\\\\nLRE-SVM~ & Disentangle, multi-component & Image classification \\\\\nDIVA~ & Disentangle, generative model & Medical imaging \\\\\nCSG~ & Disentangle, generative model & Image classification \\\\\nUFDN~ & Disentangle, generative model & Image classification \\\\\nDAL~ & Disentangle, generative model & Image classification \\\\\n\\hline\nD-SAM~ & Ensemble & Image classification \\\\\n & Ensemble & Human detection \\\\\n & Ensemble & Image classification \\\\\nDAEL~ & Ensemble & Image classification \\\\\n & Ensemble & Place categorization\\\\\nMetaReg~ & Meta-learning & Image/sentiment classification \\\\\nDADG~ & Meta-learning, adversarial & Image classification \\\\\nMASF~ & Meta-learning & Image classification, tissue Segmentation \\\\\nMetaVIB~ & Meta-learning, feature align. & Image classification \\\\\n & Meta-learning & Web page classification \\\\\nMLDG~ & Meta-learning & Image classification, reinforcement learning \\\\\nEpi-FCR~ & Meta-learning & Image classification, action recognition \\\\\nFC~ & Meta-learning & Image classification \\\\\nDGSML~ & Meta-learning, feature align & Image classification \\\\\n & Meta-learning & Semantic parsing \\\\\nM3L~ & Meta-learning & Person Re-ID \\\\\nJigen~ & Self-supervision & Image classification \\\\\nRSC~ & Self-challenging & Image classification \\\\\nCCFN~ & Random forest & Image classification \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{table*}", "cites": [2715, 2790, 2727, 2760, 2697, 2732, 7616, 7191, 2698, 2690, 2731, 2794, 2689, 2717, 2758, 8584, 2750, 7116, 2708, 2695, 2716, 2796, 8581, 2685, 2706, 2696, 2705, 2791, 8589, 2741, 8578, 2719, 2728, 196, 2692, 7110, 2700, 2792, 2707, 2725, 2762, 2793, 2763, 2684, 2761, 7614, 2789, 2738, 2718, 2724, 2713, 2704, 2755, 2694, 2743, 2751, 2686, 128, 2795, 8580, 2703, 8588, 2788, 2714, 2759, 2746, 8579, 2702], "cite_extract_rate": 0.6831683168316832, "origin_cites_number": 101, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section is primarily descriptive, focusing on presenting a benchmark codebase and summarizing results from various DG papers. It lists methods and their performance but offers minimal synthesis across works, limited critical evaluation of their strengths and weaknesses, and little abstraction to highlight broader trends or principles in domain generalization."}}
{"id": "3c0ed520-78a5-4bad-b96d-303363b3d305", "title": "Summary of Existing Literature", "level": "subsection", "subsections": [], "parent_id": "44bc842e-8826-4148-b30b-c1daa1edaf8d", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Discussion"], ["subsection", "Summary of Existing Literature"]], "content": "The quantity and diversity of training data are critical to a model's generalization ability.\nMany methods aim to enrich the training data with the data manipulation methods to achieve good performance.\nHowever, one issue of the data manipulation methods is that there is a lack of theoretical guarantee of the unbound risk of generalization.\nTherefore, it is important to develop theories for the manipulation-based methods which could further guide the data generation designs without violating ethical standards.\nCompared to data manipulation, representation learning has theoretical support in general .\nKernel-based methods are widely used in traditional methods while deep learning-based methods play a leading role in recent years.\nWhile domain adversarial training often achieves better performance in domain adaptation, in DG, we did not see significant results improvements from these adversarial methods. We think this is probably because the task is relatively easy.\nFor the explicit distribution matching, more and more works tend to match the joint distributions rather than just match the marginal~ or conditional~ distributions.\nThus, it is more feasible to perform dynamic distribution matching~.\nBoth disentanglement and IRM methods have good motivations for generalization, while more efficient training strategy can be developed.\nThere are several studies  that pointed out merely learning domain-invariant features are insufficient and representation smoothness should also be considered.\nFor learning strategy, there is a trend that many works used meta-learning for DG, where it requires to design better optimization strategies to utilize the rich information of different domains.\nIn addition to deep networks, there are also some work~ that used random forest for DG, and we hope more diverse methods will come.", "cites": [2735, 8578, 2797, 2723], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section shows reasonable synthesis by connecting ideas from multiple cited works, especially in discussing data manipulation, distribution matching, and representation learning. It provides some critical points, such as the lack of theoretical guarantees for data manipulation and the limited success of adversarial methods in DG, but these critiques remain relatively high-level without deeper evaluation. The section abstracts key trends (e.g., moving from marginal to joint distribution matching, the role of meta-learning), offering a moderate level of insight into broader directions in DG research."}}
{"id": "0d30a7a4-39cf-46bb-8e85-d0e7d0d7bd4d", "title": "Continuous \\dg", "level": "subsubsection", "subsections": [], "parent_id": "ef0437ac-e054-49e1-8c48-fbf1fe31ea0f", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Discussion"], ["subsection", "Future Research Challenges"], ["subsubsection", "Continuous \\dg"]], "content": "For many real applications, a system consumes streaming data with non-stationary statistics. \nIn this case, it is of great importance to perform continuous \\dg that efficiently updates DG models to overcome catastrophic forgetting and adapts to new data.\nWhile there are some \\da methods focusing on continuous learning~, there are only very few investigations on continuous DG  whenever this is favorable in real scenarios.", "cites": [2798, 2799], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section introduces the concept of continuous domain generalization and references two relevant papers, but the synthesis remains minimal and the discussion lacks depth. It briefly mentions the importance of adapting to non-stationary data and catastrophic forgetting but does not meaningfully integrate the cited works into a cohesive narrative. Some abstract patterns are identified, such as the need for adaptation across continuously indexed domains, but the critique and comparative depth are limited."}}
{"id": "2213a085-0604-48ff-ba14-6a4fd86e5d8d", "title": "Domain generalization to novel categories", "level": "subsubsection", "subsections": [], "parent_id": "ef0437ac-e054-49e1-8c48-fbf1fe31ea0f", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Discussion"], ["subsection", "Future Research Challenges"], ["subsubsection", "Domain generalization to novel categories"]], "content": "The existing DG algorithms usually assume the label space for different domains are the same.\nA more practical and general setting is to support the generalization on new categories, \\textit{i.e.}, both domain and task generalization.\nThis is conceptually similar to the goal of meta-learning and zero-shot learning.\nSome work~ proposed zero-shot DG and we expect more work to come in this area. There are some prior work  that tried to use the boundary-based learning paradigms or consistency regularization to solve this problem, which are good approaches that future work might build methods upon them.", "cites": [2711, 7116, 2800], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from three papers to introduce the concept of domain generalization to novel categories, linking it to meta-learning and zero-shot learning. It provides some abstraction by framing the problem in a broader context but lacks deeper critical analysis or a comparative framework to highlight strengths and weaknesses of the approaches. The narrative is coherent but remains at a relatively high-level overview without delving into nuanced evaluation."}}
{"id": "d208e8a4-3305-4ea0-a6cc-55cdd5230a81", "title": "Interpretable domain generalization", "level": "subsubsection", "subsections": [], "parent_id": "ef0437ac-e054-49e1-8c48-fbf1fe31ea0f", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Discussion"], ["subsection", "Future Research Challenges"], ["subsubsection", "Interpretable domain generalization"]], "content": "Disentanglement-based DG methods decompose a feature into domain-invariant/shared and domain-specific parts, which provide some interpretation to DG.\nFor other categories of methods, there is still a lack of deep understanding of the semantics or characteristics of learned features in DG models.\nFor example, how to relate the results of the approach with the input\nfeature space. How close are current methods to provide this level of interpretability?\nCausality~ may be one promising tool to understand \\dg networks and provide interpretations.", "cites": [2753], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a brief analytical discussion on the interpretability of domain generalization methods, connecting disentanglement approaches with the potential of causal reasoning. It integrates the cited paper to some extent but lacks deeper synthesis or comparison with other works. The critique is limited, and while it identifies a broader trend in the field, the abstraction remains modest without fully articulating overarching principles."}}
{"id": "67cdab3c-bfd7-4b0f-a28f-8045d0d4ffeb", "title": "Large-scale pre-training/self-learning and DG", "level": "subsubsection", "subsections": [], "parent_id": "ef0437ac-e054-49e1-8c48-fbf1fe31ea0f", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Discussion"], ["subsection", "Future Research Challenges"], ["subsubsection", "Large-scale pre-training/self-learning and DG"]], "content": "In recent years, we have witnessed the rapid development of large-scale pre-training/self-learning, such as BERT~, GPT-3~, and Wav2vec~.\nPre-training on large-scale dataset and then finetuning the model to downstream tasks could improve its performance, where pre-training is beneficial to learn general representations. Therefore, how to design useful and efficient DG methods to help large-scale pre-training/self-learning is worth investigating.", "cites": [679, 7, 864], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section briefly mentions three pre-training/self-learning models (BERT, GPT-3, wav2vec 2.0) but does not synthesize their contributions in depth or connect their ideas coherently. It lacks critical evaluation or comparison of these works and merely states a general observation about the potential of DG to aid large-scale pre-training, without identifying broader patterns or principles."}}
{"id": "179cbc51-d427-4ae4-b68f-675aff3ff0cb", "title": "Test-time Generalization", "level": "subsubsection", "subsections": [], "parent_id": "ef0437ac-e054-49e1-8c48-fbf1fe31ea0f", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Discussion"], ["subsection", "Future Research Challenges"], ["subsubsection", "Test-time Generalization"]], "content": "While DG focuses on the training phase, we can also request test-time generalization in inference phase.\nThis further bridges \\da and \\dg since we can also use the inference unlabeled data for adaptation.\nVery few recent works~ paid attention to this setting.\nCompared to traditional DG, test-time generalization will allow more flexibility in inference time, while it requires less computation and more efficiency as there are often limited resources in inference-end devices.", "cites": [2801], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces test-time generalization and connects it to domain adaptation and domain generalization but lacks deeper synthesis of the cited paper. It does not elaborate on how the method from the cited paper contributes to the broader understanding of test-time generalization. The analysis is minimal, and the section does not provide a comparative or abstract view of the concept or its implications."}}
{"id": "c58a2462-2599-40a6-863a-95234177c2e8", "title": "Performance evaluation for DG", "level": "subsubsection", "subsections": [], "parent_id": "ef0437ac-e054-49e1-8c48-fbf1fe31ea0f", "prefix_titles": [["title", "Generalizing to Unseen Domains: A Survey on Domain Generalization"], ["section", "Discussion"], ["subsection", "Future Research Challenges"], ["subsubsection", "Performance evaluation for DG"]], "content": "The recent work~ pointed out that on several datasets, the performance of some DG methods is almost the same as the baseline (i.e., ERM).\nWe do not take it as the full evidence that DG is not useful in real applications. Instead, we argue that this might be due to the inappropriate evaluation schemes in use today, or the domain gaps not being so large.\nIn more realistic situations such as person ReID where there are obvious domain gaps~, the improvement of DG is dramatic.\nTherefore, we stay positive about the value of DG and hope researchers can also find more suitable settings and datasets for the study.", "cites": [2727, 2788], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical perspective by addressing the evaluation challenges in domain generalization, integrating insights from two cited papers to question the perceived effectiveness of DG methods. It connects their findings to suggest that current evaluation schemes may be inadequate, and it moves beyond simple description by offering a critical stance on the issue. However, it does not fully synthesize a novel framework nor does it generalize deeply to broader principles of evaluation methodology."}}
