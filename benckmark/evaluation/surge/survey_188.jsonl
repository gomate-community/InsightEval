{"id": "847f0401-f427-4d1e-aab1-fb948a23b449", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "de20a706-7b82-4ffa-aeec-10b931cc4cdb", "prefix_titles": [["title", "A Survey on Multi-modal Summarization"], ["section", "Introduction"]], "content": "\\label{sec:intro}\nEveryday, the Internet is flooded with tons of new information coming from multiple sources. Due to the technological advancements, people can now share information in multiple formats with various modes of communication to be used at their disposal. This alarmingly increasing amount of content on the Internet makes it difficult for the users to receive useful information from the torrent of sources, necessitating research on the task of multi-modal summarization (MMS). Various studies have shown that including multi-modal data as input can indeed help improve the summary quality .  claimed that on an average having a pictorial summary can improve the user satisfaction by 12.4\\% over a plain text summary. The fact that nearly every content sharing platform has a provision to accompany an opinion or fact in multiple media forms, and every mobile phone has the feature to deliver that kind of facility are indicative of the superiority of a multi-modal means of communication in terms of ease in conveying and understanding information.\nInformation in the form of multi-modal inputs has been leveraged in many tasks other than summarization including multi-modal machine translation , multi-modal movement prediction , multi-modal question answering , multi-modal lexico-semantic classification , multi-modal keyword extraction , product classification in e-commerce , multi-modal interactive artificial intelligence frameworks , multi-modal emoji prediction , multi-modal frame identification , multi-modal financial risk forecasting ,  multi-modal sentiment analysis ,  multi-modal named identity recognition , multi-modal video description generation , multi-modal product title compression  and multi-modal biometric authentication . The shear number of application possibilities for multi-modal information processing and retrieval tasks are quite impressive. Research on multi-modality can also be utilized in other closely related research problems like image-captioning , image-to-image translation , seismic pavement testing , aesthetic assessment , and visual question-answering .\nText summarization is one of the oldest problems in the fields of natural language processing (NLP) and information retrieval (IR), that has attracted various researchers due to its challenging nature and potential for many applications. Research on text summarization can be traced back to more than six decades in the past . The NLP and IR community have tackled research in text summarization for multiple applications by developing myriad of techniques and model architectures . As an extension to this, the problem of multi-modal summarization adds another angle by incorporating visual and aural aspects into the mix, making the task more challenging and interesting to tackle. This extension of incorporating multiple modalities into a summarization problem expands the breadth of the problem, leading to wider application range for the task. \nIn recent years, multi-modal summarization has experienced many new developments, including release of new datasets, advancements in techniques to tackle the MMS task, as well as proposals of more appropriate evaluation metrics. The idea of multi-modal summarization is a rather flexible one, embracing a broad range of possibilities for the input and output modalities, and also making it difficult to apprehend existing works on the MMS task with knowledge of uni-modal summarization techniques alone. This necessitates a survey on multi-modal summarization.\nThe MMS task, just like any uni-modal summarization task, is a demanding one, and existence of multiple correct solutions makes it very challenging. Humans creating a multi-modal summary have to use their prior understanding and external knowledge to produce the content. Establishing computer systems to mimic this behaviour becomes difficult given their inherent lack of human perception and knowledge, making the problem of automatic multi-modal summarization a non-trivial but interesting task.\nAlthough quite a few survey papers were written for uni-modal summarization tasks including surveys on text summarization  and video summarization , and a few survey papers covering multi-modal research . However, to the best of our knowledge, we are the first to present a survey on multi-modal summarization.  The closest work to ours is the work on multi-dimensional summarization by , who proposes the method for  summarization of things in cyber-physical society through a multi-dimensional lens of semantic computing. However, our survey is distinct from that work as  focuses on how understanding human behaviour, psychology, and advances in cognitive sciences can help to improve the current summarization systems in the emerging cyber-physical society while in this manuscript we mostly focus on the direct applications and techniques adopted by the research community to tackle the MMS task.\nThrough this manuscript, we unify and systematize the information presented in related works, including the datasets, methodology, and evaluation techniques. With this survey, we aim to assist researchers familiarize with various techniques and resources available to proceed with research in the area of multi-modal summarization.\nThe rest of the paper is structured as follows: We formally define the MMS task in Section \\ref{sec:mms}. In Section \\ref{sec:seg}, we provide an extensive organization of existing works. In Section \\ref{sec:method}, we give an overview about the techniques used for the task of MMS. In Section \\ref{sec:data} we introduce the datasets available for the MMS task and evaluation techniques devised for the evaluation of multi-modal summaries, respectively. We discuss about possibilities of future work in Section \\ref{sec:future} and conclude our paper in Section \\ref{sec:conc}.", "cites": [763, 1261, 7050, 764, 1262, 1263, 1260, 1259, 7186, 1264, 1257, 8433, 8432, 1030, 1265, 1258], "cite_extract_rate": 0.23529411764705882, "origin_cites_number": 68, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The introduction synthesizes the context and relevance of multi-modal summarization by listing related tasks and citing a few multi-modal papers, but it does so in a largely descriptive and non-integrated manner. There is minimal critical analysis of the cited works, and abstraction is limited to general observations about the application landscape without deeper theoretical or conceptual insights."}}
{"id": "e2a672cc-79c9-440c-be30-2f753de66abc", "title": "Multi-modal Summarization task", "level": "section", "subsections": [], "parent_id": "de20a706-7b82-4ffa-aeec-10b931cc4cdb", "prefix_titles": [["title", "A Survey on Multi-modal Summarization"], ["section", "Multi-modal Summarization task"]], "content": "\\label{sec:mms}\nIn this section we formally define what classifies as a multi-modal summarization task. Before formalizing the multi-modal summarization we broadly define the term summarization\\footnote{In this paper, summarization stands for automatic summarization unless specified otherwise.}. According to Wikipedia\\footnote{\\url{https://en.wikipedia.org/wiki/Automatic_summarization}}, automatic summarization is ``\\textit{the process of shortening a set of data computationally, to create an abstract that represents the most important or relevant information within the original content}.'' Formally, summarization is the process of obtaining the set $X_{sum} = f(D)$ such that $length(X_{sum}) \\leq length(D)$, where $X_{sum}$ is the output summary, $D$ is the input data, and function $f(.)$ is the summarization function. \nThe multi-modal summarization task can be defined as a summarization task that takes more than one mode of information representation (termed as modality) as input, and depends on information sharing across different modalities to generate the final summary. Mathematically speaking, when the input dataset $D$ can be broken down into several partially disjoint sets of different modality information $\\{M_1 \\bigcup M_2 \\bigcup ... \\bigcup M_n\\}$, where $n \\geq 2$ and $\\exists$ several pairs of $(M_i, M_j)$ for $(i,j) \\in \\{1,..,n\\}$ such that the shared latent information between $(M_i, M_j)$ is not $\\varnothing$ , then the task of obtaining the set $X_{sum} = f(D)$ is known as multi-modal summarization\\footnote{The reason for restricting $n \\geq 2$ for the task definition is limitation of current techniques, that are unable to successfully generate modalities other than text for multi-modal summarization. Even though there have been some recent breakthroughs in text-to-image generation (like Open AI's DALL-E ), and text-to-speech synthesis (like Google's Duplex ); they still lack the level of integrity and robustness to be used in a real-world application like MMS.}. If $n' \\geq 2$ for $X_{sum} = \\{M'_1 \\bigcup M'_2 \\bigcup ... \\bigcup M'_{n'}\\}$, then the output summary is multi-modal; otherwise, the output is a uni-modal summary. \nIn this survey, we mainly focus on recent works that have natural language as the \\textit{central modality}\\footnote{We believe that the MMS models that have video as the \\textit{central modality} tend to be closely related to the task of video summarization.}, where a \\textit{central modality} (or \\textit{key modality}) is selected according to the intuition: \\textit{''For any information processing task in multi-modal scenarios, including content summarization, amongst all the modalities, there is often a preferable mode of representation based on the significance and ability to fulfill the task''} . Other modalities that aid the central modality to convey information are termed as \\textit{adjacent modalities}.\n\\noindent\\textbf{Various aspects of multi-modal summarization:} Literature has explored the MMS task for myriad of reasons and motives, and doing so, has lead to different challenges and variants of the task. Some of the most prominent and interesting ones are discussed below: \n\\begin{itemize}\n    \\item \\textbf{Combined complementary-supplementary multi-modal summarization task (CCS-MMS) :}  proposed the CCS-MMS task of generating a multi-modal summary that considers text as the central modality, and images, audio and videos as the adjacent modality. The task is to generate the multi-modal summary such that it consists of both supplementary and complementary enhancements, which are defined as follows:\n    \\begin{itemize}\n        \\item \\textbf{Supplementary enhancement:} When the adjacent modalities reinforce the facts and ideas presented in the central modality, the adjacent modalities are termed as \\textit{supplementary enhancements}.\n        \\item \\textbf{Complementary enhancement:} When the adjacent modalities complete the information by providing additional but relevant information that is not covered in the central modality, the adjacent modalities are termed as \\textit{complementary enhancements}.\n    \\end{itemize}\n    \\item \\textbf{Summarization objectives:} We can distinguish prior work based on summarization objectives they have used. For instance,  uses weighted sum of three sub-modular objective functions to create an extractive text summarization system that is guided by multi-modal inputs. The chosen submodular functions are - salience of input text, image information covered by text summary, and non-redundancy in input text.  uses a single objective function for an ILP setup, that is the weighted average of uni-modal salience, and cross-modal correspondence.  proposes two different sets of multi-modal objectives for the task of extractive multi-modal summary generation - a) summarization-based objective, and b) clustering-based objectives. For summarization-based objectives, they use the following three objectives - i) Salience(txt) / Redundancy(txt), ii) Salience(img) / Redundancy(img), and iii) cross-modal correspondence; while for clustering-based objectives, they use PBM , a popular cluster validity index (a function of cluster compactness and separation) to evaluate the uni-modal clusters of image and text, giving the following set of objectives - i) PBM(txt), PBM(img), and cross-modal correspondence. Almost all the neural networks based multi-modal summarization frameworks  on the other hand use the standard negative log-likelihood function over the output vocabulary as the training objective. Some works also use textual and visual coverage loss to prevent over-attending the input as well .\n    \\item \\textbf{Multi-modal social media event summarization:} Various works have been conducted on the social media data that consists of opinions and experiences of a diverse set of population.  proposes the problem of summarizing asynchronous information from multiple social media platforms like Twitter, Instagram, and Flickr to generate a summary of event that is widely covered by users of these platforms extensively.  propose multi-modal summarization of trending topics in microblogs. They use Sina Weibo\\footnote{\\url{http://www.weibo.com/}} microblogs for the experimentation, which is a very popular microblogging platform in China.  uses the Weibo platform information to summarize disaster events like train crash and earthquakes. \n\\end{itemize}", "cites": [1267, 1266, 7339, 1268], "cite_extract_rate": 0.26666666666666666, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key concepts from multiple cited papers to define and categorize the MMS task, particularly distinguishing between supplementary and complementary enhancements. It provides critical insights by evaluating the limitations of current techniques in handling multi-modal outputs beyond text and highlighting the varying objectives used in different frameworks. The section abstracts the task into a structured definition and broader conceptual framework, such as the central vs. adjacent modality distinction."}}
{"id": "50ef98c8-c302-40a5-8941-31d89235c55c", "title": "On the basis of encoding the input", "level": "subsection", "subsections": ["ca0d2416-acf5-4bd6-83f7-4de8e66d47fe"], "parent_id": "57c925ba-556b-4d51-b1cf-6926cf3ae82e", "prefix_titles": [["title", "A Survey on Multi-modal Summarization"], ["section", "Organization of existing work"], ["subsection", "On the basis of encoding the input"]], "content": "A multi-modal summarization task is highly driven by the kind of input it is given. Due to this dependency on diverse input modalities, the feature extraction and encoding strategies also differ for different multi-modal summarization systems. Existing works can be distinguished from others on the basis of the type of input and its encoding strategy in the following categories: \n\\vspace{1.5mm}\n\\noindent\\textbf{Multi-modal Diversity (MMD):} Different combinations of input (text, image,video \\& audio) involve different preprocessing and encoding strategies. We can classify the existing works depending on the combination of modalities in which the input is represented. Various combinations within the input modalities like \\textit{text-image} , \\textit{text-video} ,  \\textit{audio-video}\\footnote{Note that \\textit{audio-video} and \\textit{text-audio-video} works are grouped together since in most of the existing works, automatic speech transcription is performed to obtain the textual modality part of data in the pre-processing step.} , and \\textit{text-image-audio-video}  have been explored in the literature of MMS. The different feature extraction strategies for individual modalities are described in Section \\ref{subsec:preprocess}.\n\\vspace{1.5mm}\n\\noindent\\textbf{Input Text Multiplicity (ITM):} Since a major focus of this survey is on MMS tasks with text as the \\textit{central modality}, the number of text documents in input can also be one way of categorizing the related works. Depending upon whether the textual input is single-document  or multi-document , the input preprocessing and the overall summarization strategies might differ. Having multiple documents makes the task a lot more challenging, since the degree of redundant information in input becomes a lot more prominent, making the data somewhat more noisy .\n\\vspace{1.5mm}\n\\noindent\\textbf{Multi-modal Synchronization (MMSy)\\footnote{Note that the term synchronization is mostly used when there is a continuous media in consideration.}:} Synchronization refers to the interaction of two or more things at the same time or rate. For multi-modal summarization, having a synchronized input indicates that the multiple modalities have a coordination in terms of information flow, making them convey information in unison. We then classify input as synchronous  and asynchronous .\n\\vspace{1.5mm}\n\\noindent\\textbf{Domain Specificity (DS):} Domain can be defined as the specific area of cognition that is covered by any data, and depending upon the extent of domain coverage, we can classify works as \\textit{domain-specific} or \\textit{generic}.\nThe approach to summarize a \\textit{domain-specific} input can differ from the \\textit{generic} input greatly, since feature extraction in the former can be very particular in nature while not so in the latter, impacting the overall techniques immensely. Most of the news summarization tasks  are \\textit{generic} in nature, since news covers information about almost all the domains; whereas movie summarization , sports event summarization for tennis  and soccer , meetings recording summarization , tutorial summarization , social media event summarization  are examples of \\textit{domain specific} tasks.\n\\begin{figure}[t]\n\\includegraphics[width=\\textwidth]{survey.pdf}\\centering\n\\caption{Visual representation of proposed taxonomy. The dark orange nodes coming out of the root (in yellow) represents the segregation based on input, output and adopted methodology, while the light orange nodes following them represent the respective characteristics of the research work on which the works can be distinguished. The teal colored rectangles in the leaf denote the various categories of each such characteristic.} \\label{fig1}\n\\end{figure}", "cites": [1270, 1271, 1269, 1268], "cite_extract_rate": 0.2222222222222222, "origin_cites_number": 18, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of how multi-modal summarization works can be categorized based on input characteristics. It introduces four categories (MMD, ITM, MMSy, DS) and briefly links them to cited papers, but the integration is minimal and lacks deeper synthesis. There is little critical evaluation of the cited works or identification of broader patterns, keeping the insight level at medium."}}
{"id": "ca0d2416-acf5-4bd6-83f7-4de8e66d47fe", "title": "Feature Extraction Strategies", "level": "subsubsection", "subsections": [], "parent_id": "50ef98c8-c302-40a5-8941-31d89235c55c", "prefix_titles": [["title", "A Survey on Multi-modal Summarization"], ["section", "Organization of existing work"], ["subsection", "On the basis of encoding the input"], ["subsubsection", "Feature Extraction Strategies"]], "content": "\\label{subsec:preprocess}\nIn a multi-modal setting, pre-processing \\& feature extraction becomes vital step, since it involves extracting features from different modalities. Each input modality has been dealt with using modal-specific feature-extraction techniques. Even though some works tend to learn the semantic representation of data using their own proposed models, nearly all follow the same steps for feature extraction. Since the related works have different sets of input modalities, we describe feature extraction techniques for each modality individually.\n\\vspace{1.5mm}\n\\noindent\\textbf{Text:} Traditionally, before the era of deep learning, Term Frequency-Document Inverse Frequency (TF-IDF)  was used to identify relevant text segments . Due to significant advancements in feature extraction, almost all the MMS tasks in the past five years either use pre-trained embeddings like word2vec  or Glove . These pre-trained embeddings utilize the fact that the semantic information of a word is related to its contextual neighbors for training. Some works also train similar embeddings on their own datasets  (refer to \\textit{Feature Extraction} in Section \\ref{subsec:neural}). Some works also adopt different pre-processing steps, depending upon the task specifications. For example,  applied a normalizer to handle the concept of expressive lengthening dealing with microblog datasets. Even though current MMS systems have not yet adopted them, it is worth mentioning Transformer-based word representations  like BERT, etc. that have achieved state-of-the-art performance in the vast majority of NLP and vision tasks. This achievement can be credited to their fast training due to parallelization, and ability to pre-train the language models on unlabelled corpora. We even have multi-lingual embeddings like LabSE , and multi-modal text-image embeddings like UNITER , VilBERT , VisualBERT , Pixel-BERT , etc.\n\\begin{table}[tp]  \\centering\n\\scriptsize\n\\caption{\\textbf{Comprehensive list of works that use specific pre-trained deep learning frameworks used to generate image embeddings.}}\\label{tab:img}\n\\renewcommand{\\arraystretch}{2}\n\\begin{tabular}{|p{0.2\\textwidth}|p{0.6\\textwidth}|}\n\\hline\n\\textbf{Pre-trained network} & \\textbf{Works using this framework}\\\\\n\\hline\nVGGNet  & , , , , , , , , , \n\\\\\\hline\nResNet  & , ,  \\\\\\hline\nGoogleNet  &  \\\\\\hline\n\\end{tabular}\n\\end{table}\n\\vspace{1.5mm}\n\\noindent\\textbf{Images:} Images, unlike text, are non-sequential and have a two-dimensional contextual span. Convolutional neural network (CNN) based deep neural network models have proven to be very promising in feature extraction tasks, but training these models requires large datasets, making it difficult to train features on MMS datasets. Hence, most of the existing works use pre-trained networks (e.g., ResNet , VGGNet , GoogleNet ) trained on large image classification datasets like ImageNet . The technique of extracting local features (containing information about a confined patch of image) along with global features has shown promise in the MMS task as well . A detailed list of frameworks that use pre-trained deep learning networks can be found in Table \\ref{tab:img}.  uses Speeded-Up Robust Features (SURF) for each image, following a bag-of-word approach to creating a visual vocabulary.  handle images by first extracting the Scale Invariant Feature Transform (SIFT) features. These SIFT features are fed to a hierarchical quantization module  to obtain a 10,000-dimensional bag of the visual histogram. Having been inspired by the success of self-attention and Transformers  in effectively  modeling textual sequences, researchers in computer vision have adopted the techniques like self-attention, unsupervised pre-training, parallelizability of transformer architecture, etc. to better model the image representations\\footnote{The readers are encouraged to read the extensive survey provided by .}. In order to adopt the self-attention layer dedicated to text sequences,  proposed a framework that restricts the self-attention to the local neighborhoods, thus significantly increasing the size of images that the model can process, despite maintaining larger receptive fields per layer than a CNN framework.  illustrated that usage of self-attention in conjunction with CNNs is not required, and a pure transformer applied to the sequence of image patches can also perform well on image classification tasks.  developed and optimized deep image transformer frameworks that do not saturate early with more depth.\nTo the best of our knowledge, none of the existing multi-modal summarization works use image transformers to encode the images. Since these large-scale models have a lot more capability to store more learned patterns from large-scale datasets due to the huge parameter space, they are bound to improve the overall summarization process by aiding in better image understanding.\n\\vspace{1.5mm}\n\\noindent\\textbf{Audio and video:} Audio and video are usually present together as a single synchronized continuous media, and hence we discuss the pre-processing techniques used to extract features from them simultaneously. Continuous media has been processed in many diverse ways. Since audios and videos are susceptible to noise, it becomes of utmost importance to detect relevant segments before proceeding to the training phase\\footnote{Note that some deep neural models like  or  prefer to encode individual frames using CNNs, and then use trainable RNNs to encode temporal information in videos. This CNN-RNN framework is not part of pre-processing, but instead, it belongs to the main model since these layers are also affected during training.}. While some works have adopted a naïve sliding window approach, making equal length cuts and further experimenting on these segments , quite a few have done a modal conversion, changing the information media using automatic speech transcription to generate speech transcriptions and extracting key-frames from video using techniques like boundary shot-detection . Some works have also taken into account the nature of the dataset, and performed semantic segmentation, getting better segment slices. For example,  worked on a tennis dataset and used the information that the umpire requires the audience to remain quiet during the match point, performing segmentation consisting of a segment that begins with low audio activity followed by high audio energy levels as a result of the cheering and the commentary. If the audio and video are converted into another modality, then their pre-processing follows the same procedure as the new modalities, whereas, in the case of segmentation, various metrics like acoustic confidence, audio magnitude, sound localization for audio, motion detection, and Spatio-temporal features driven by intensity, color, and orientation for video have been explored to determine the salience and relevance of segments depending upon the task at hand .\n\\vspace{1.5mm}\n\\noindent\\textbf{Cross-modal correspondence:} Although the majority of works train their own shared embedding space for multiple modalities using the information from the target datasets , quite a few works  also tend to use pre-trained neural network models  trained on the image-caption datasets like Pascal1k , Flickr8k , Flick30k  etc. to leverage the information overlap amongst different modalities. This becomes a necessity for small datasets that are mostly used for extractive summarization. However even these pre-trained models cannot process raw data, and hence the text and image inputs are first pre-processed to desired embedding formats and then are fed to these models with pre-trained weights. For example,   required a 6,000-dimensional sentence vector and 4,096-dimensional image vector generated by applying Principal Component Analysis (PCA)  to the 18,000-dimensional output from the Hybrid Gaussian Laplacian mixture model (HGLMM)  and extracting the weights from the final fully connected layer, $fc7$, from VGGNet , respectively. In recent years, various Transformer-based  models have also been developed to correlate semantic information across textual and visual modalities. These BERT  inspired models include ViLBERT , VisualBERT , VideoBERT , VLP , Pixel-BERT  etc. to name a few. There has also been some video-text representation learning like  and  that can be used to summarize multi-modal content with continuous modalities. However, none of the recent works on multi-modal summarization has utilized these transformer-based techniques in their system pipelines.\n\\vspace{1.5mm}\n\\noindent\\textbf{Domain specific techniques:} Most of the systems proposed to solve the problem of multi-modal summarization are generic and can be adapted to other domains and problem statements as well. But, there do exist some works that benefit from the external knowledge of particular domains and problem settings to create better-performing systems. For instance,  utilizes the fact that in tennis, the umpire always requires spectators to be silent before a serve, until the end of the point. The authors also pointed out that the end of the point is usually marked by a loud cheer from the supporters of the players in the audience. They used this fact to perform smooth segmentation of tennis clips using audio energy levels to indicate the start and end positions of a segment. Similar to this,  utilized atomic events in a game of soccer like a pass, goal, dribble, etc. to segment the video, which is later connected together to generate the summary. Other than sports, such domain-specific solutions have also been adopted in other domains. For example, , when summarizing meeting recordings of a conference room, seeks out some visual activity like ``someone entering the room'' or ``someone standing up to write something on a whiteboard'' to detect some event likely to contain relevant information. In a different domain setting, people have benefited from other data pre-processing strategies, for instance,  extracts various key aspects of products like ``environmentally friendly refrigerators'' or ``energy efficient freezers'' to generate a captivating summary for Chinese e-commerce products.", "cites": [1275, 8434, 305, 1281, 1276, 1684, 1271, 1274, 1273, 1278, 1272, 38, 1280, 7040, 97, 768, 1268, 1270, 1277, 732, 1279], "cite_extract_rate": 0.44680851063829785, "origin_cites_number": 47, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes feature extraction strategies across modalities and connects them with broader trends in deep learning, particularly in the use of pre-trained models and self-attention. It includes critical observations, such as the underutilization of image transformers in MMS and the limitations of CNN-based approaches with small datasets. The abstraction level is moderate, identifying patterns like the importance of pre-processing and the shift toward transformer-based architectures in vision."}}
{"id": "f6fca4d8-6752-473d-8e05-1e3b8528b3ab", "title": "On the basis of method", "level": "subsection", "subsections": [], "parent_id": "57c925ba-556b-4d51-b1cf-6926cf3ae82e", "prefix_titles": [["title", "A Survey on Multi-modal Summarization"], ["section", "Organization of existing work"], ["subsection", "On the basis of method"]], "content": "A lot of various approaches have been developed to solve the MMS task, and we can organize the existing works on the basis of proposed methodologies as follows:\n\\vspace{1.5mm}\n\\noindent\\textbf{Learning process (LP):} A lot of work has been done in both supervised learning  and unsupervised learning . It can be observed that a large fraction of supervised techniques adopt deep neural networks to tackle the problem , whereas in unsupervised techniques a large diversity of techniques have been adopted including deep neural networks , integer linear programming , differential evolution , submodular optimization  etc.\n\\vspace{1.5mm}\n\\noindent\\textbf{Handling of continuous media (HCM):} We can also distinguish between works depending upon how the proposed models handle continuous media (audio and video in this case). There are three broad distinctions possible, a) \\textit{extracting-information}, where the model extracts information from continuous media to get a discrete representation , b) \\textit{semantic-segmentation}, where a logical technique is proposed to slice out the continuous media , and c) \\textit{sliding window}, when a naïve fixed window based modeling is performed .\n\\vspace{1.5mm}\n\\noindent\\textbf{Notion of importance (NI):} One of the most significant distinctions would be the notion of importance used to generate the final summary. A diverse set of objectives ranging from interestingness , redundancy , cluster validity index , acoustic energy / visual illumination , and social popularity  have been explored in attempt to solve the MMS task.\n\\vspace{1.5mm}\n\\noindent\\textbf{Cross-modal information exchange (CIE):} The most important part of a MMS model is the ability to extract and share information across multiple modalities. Most of the works either adopt a proximity-based approach , a pre-trained model on image-caption pairs based corpora for information overlap , or learn the semantic overlap over uni-modal embeddings .\n\\vspace{1.5mm}\n\\noindent\\textbf{Algorithms (A):} The algorithm for the multimodal summarization task varies from traditional multiobjective optimization strategies to modern deep learning-based approaches. We can classify the existing works based on the different algorithms as Neural models (NN), Integer Linear Programming based models (ILP), Submodular Optimization-based models (SO), Nature-Inspired Algorithm based models (NIA), Graph-based models (G), and other algorithms (Oth). We have discussed these different methods in detail in Section \\ref{sub:main-model}. The other algorithms comprise different clustering-based, LDA  - based and audio-video analysis-based techniques which were earlier used for performing multimodal summarization.", "cites": [1268], "cite_extract_rate": 0.058823529411764705, "origin_cites_number": 17, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section categorizes existing MMS approaches based on method, but it primarily describes these categories without effectively connecting or synthesizing insights from multiple papers. It lacks critical evaluation of the cited methods and only briefly mentions a few techniques. While it introduces a classification framework, the abstraction remains limited to listing rather than offering meta-level insights or deeper trends."}}
{"id": "45471543-fca5-4226-8d1d-4973a0f1b505", "title": "On the basis of decoding the output", "level": "subsection", "subsections": [], "parent_id": "57c925ba-556b-4d51-b1cf-6926cf3ae82e", "prefix_titles": [["title", "A Survey on Multi-modal Summarization"], ["section", "Organization of existing work"], ["subsection", "On the basis of decoding the output"]], "content": "The summarization objective decides the desired type of output. For different summarization objectives, the type of output and decoding method vary. Depending on the type of output and the decoding method, we can categorize the existing works on the following basis: \n\\vspace{1.5mm}\n\\noindent\\textbf{Content intensity (CI):} The degree to which an output summary elaborates on a concept can hugely impact the overall modeling. The output summary can either be \\textit{informative}, having detailed information about the input topic , or \\textit{indicative}, only hinting at the most relevant information .\n\\vspace{1.5mm}\n\\noindent\\textbf{Text Summarization Type (TST):} The most widely discussed distinction for text summarization works is the distinction of \\textit{extractive} vs \\textit{abstractive}. Abstractive summarization systems generally use a beam search or greedy search mechanism for decoding the output summary. While extractive systems during decoding use some scoring mechanism to identify the salient, non-redundant, and readable elements from the input for the final output. Depending on the nature of an output text summary, we can also classify the works in MMS tasks (containing text in the output) into \\textit{extractive MMS}  and \\textit{abstractive MMS} \\footnote{Note that other modalities beside text have been so far subject to only extractive approaches in MMS researches.}.\n\\vspace{1.5mm}\n\\noindent\\textbf{Multi-modal expressivity (MME):} Whether the output is \\textit{uni-modal} (comprising of one modality)  or \\textit{multi-modal} (comprising of multiple modalities)  is a major classification for the existing work. Mostly the systems producing multimodal output involve some post-processing steps for selecting the final output elements from the non-central modalities. \n\\vspace{1.5mm}\n\\noindent\\textbf{Central modality (CM):} Based on \\textit{central-modality} (defined in Section \\ref{sec:mms}), existing works can also be distinguished depending on the base modality around which the final output, as well as the model, are formulated. A large portion of the prior work adopts either a text-centric approach  or a video-centric\\footnote{Here audio is assumed to be a part of video since in all the existing works video and audio are synchronous to each other.} approach . Few of the decoding methods followed popularly in neural models have been discussed in detail in Section \\ref{subsec:neural}.\n\\begin{table}\n \\setlength\\extrarowheight{2pt}\n \\centering\n \\tiny\n \\caption{\\textbf{Comprehensive study of existing work using the proposed taxonomy (refer to Section \\ref{sec:seg}).}}\n \\label{tab:categ}\n \\vspace{-3mm}\n \\rotatebox{90}{\n \\begin{tabular}{|  l*{30}{|c}|}\n \\hline\n   & \\multicolumn{10}{c|}{\\textbf{Input Based}} & \\multicolumn{8}{c|}{\\textbf{Output Based}} & \\multicolumn{12}{c|}{\\textbf{Method Based}} \\\\ \\cline{2-31}\n  \\multirow{2}{*}{\\textbf{\\hspace{10mm} Papers}} & \\multicolumn{4}{c|}{\\textbf{MMD}} & \\multicolumn{2}{c|}{\\textbf{ITM}} & \\multicolumn{2}{c|}{\\textbf{MMSy}} & \\multicolumn{2}{c|}{\\textbf{DS}} & \\multicolumn{2}{c|}{\\textbf{CI}} & \\multicolumn{2}{c|}{\\textbf{TST}} & \\multicolumn{2}{c|}{\\textbf{MME}} & \\multicolumn{2}{c|}{\\textbf{CM}} & \\multicolumn{2}{c|}{\\textbf{LP}} & \\multicolumn{3}{c|}{\\textbf{HCM}} & \\multicolumn{3}{c|}{\\textbf{NI}} & \\multicolumn{3}{c|}{\\textbf{CIE}} & \\multicolumn{1}{c|}\n  {\\textbf{A}} \\\\ \\cline{2-31}\n   & \\rotatebox{270}{Text\\phantom{.}} & \\rotatebox{270}{Images\\phantom{.}} & \\rotatebox{270}{Audio\\phantom{.}} &  \\rotatebox{270}{Video\\phantom{.}} & \\rotatebox{270}{Single-doc\\phantom{.}} & \\rotatebox{270}{Multi-doc\\phantom{.}} & \\rotatebox{270}{Sync\\phantom{.}} &   \\rotatebox{270}{Async\\phantom{.}} & \\rotatebox{270}{Domain Specific\\phantom{.}} & \\rotatebox{270}{Generic\\phantom{.}} & \\rotatebox{270}{Informative\\phantom{.}} & \\rotatebox{270}{Indicative\\phantom{.}} & \\rotatebox{270}{Abstractive\\phantom{.}} & \\rotatebox{270}{Extractive\\phantom{.}} & \\rotatebox{270}{Uni-modal\\phantom{.}} & \\rotatebox{270}{Multi-modal\\phantom{.}} & \\rotatebox{270}{Text\\phantom{.}} & \\rotatebox{270}{Video\\phantom{.}} & \\rotatebox{270}{Unsupervised\\phantom{.}} & \\rotatebox{270}{Supervised\\phantom{.}} & \\rotatebox{270}{Extracting info\\phantom{.}} & \\rotatebox{270}{Semantic seg\\phantom{.}} & \\rotatebox{270}{Sliding window\\phantom{.}} & \\rotatebox{270}{Redundancy based\\phantom{.}} & \\rotatebox{270}{Interestingness\\phantom{.}} & \\rotatebox{270}{Other\\phantom{.}} & \\rotatebox{270}{Image Caption\\phantom{.}} & \\rotatebox{270}{Proximity\\phantom{.}} & \\rotatebox{270}{Uni-modal embedding\\phantom{.}} & \\rotatebox{270}{Algorithms\\phantom{.}}\\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \n  Oth \\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \n  Oth\\\\ \\hline\n    & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & \n  Oth\\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \n  Oth\\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \n  SO, G \\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \n  NN \\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \n  NN \\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \n  NN \\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & NN \\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & NN \\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & NN\\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}}  & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & ILP \\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}}  & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & &  & NIA\\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}}  & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & NIA \\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & NIA\\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & Oth\\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & SO\\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & & & Oth\\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & G\\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & & & & Oth\\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & NN\\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & Oth\\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & & & Oth\\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & NN\\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & NN\\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & NN\\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & SO, G\\\\ \\hline\n   & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & \\textcolor{applegreen}{\\textbf{\\checkmark}} & & NN\\\\ \\hline\n \\end{tabular}\n}\n\\end{table}", "cites": [1271, 1267, 1268, 1270], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 28, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates analytical insight by organizing MMS research based on the decoding method and introducing a taxonomy that generalizes across cited works. It abstracts the concept of summarization into categories such as content intensity, summarization type, and multi-modal expressivity. While it offers some critical observations (e.g., that other modalities are limited to extractive approaches), deeper evaluations or limitations of these methods are not extensively discussed."}}
{"id": "3ae659f3-ad34-4a31-bba9-a03978d1dac6", "title": "Neural Models", "level": "subsubsection", "subsections": [], "parent_id": "7013dadb-301e-427d-ae12-6cf38e4e66ec", "prefix_titles": [["title", "A Survey on Multi-modal Summarization"], ["section", "Overview of Methods"], ["subsection", "Main Model"], ["subsubsection", "Neural Models"]], "content": "\\label{subsec:neural}\nA few extractive summarization models  and almost all of the abstractive text summarization based MMS architectures  use Neural Networks (NN) in one form or another. Obtaining annotated dataset with sufficient instances to train these supervised techniques is the most difficult step for any Deep learning based MMS framework. The existing datasets satisfying these conditions belong to news domain, and have \\textit{text-image} type input (refer to datasets \\#4, \\#5, \\#6, \\#7, \\#19 in Table \\ref{tab:datasets}) or \\textit{text-audio-video} type input (refer to datasets \\#17, \\#18 in Table \\ref{tab:datasets}). All these frameworks utilize the effectiveness of seq2seq RNN models for language processing and generation, and encoding temporal aspects in videos;  CNN networks are also adopted to encode discrete visual information in form of images  and video frames . All the neural models have an encoder-decoder architecture at their heart, having three key elements: 1) a \\textit{feature extraction module (encoder)}, 2) a \\textit{summary generation module (decoder)}, and 3) a \\textit{multi-modal fusion module}. Fig. \\ref{fig:neural} describes a generic neural model to generate \\textit{text-image}\\footnote{We formulate \\textit{text-image} summaries in our generic model since the existing neural models only output text  or text-image  output.} summaries for multi-modal input. \n\\begin{figure}[t]\n\\includegraphics[width=\\textwidth]{generic_dl_model.pdf}\\centering\n\\caption{A generic framework portraying existing neural models.} \\label{fig:neural}\n\\end{figure}\n\\vspace{1.5mm}\n\\noindent\\underline{Feature Extraction (Encoder):} \\textit{Encoder} is a generic term that entails both textual encoders as well as visual encoders. Various \\textit{encoders} have been explored to encode contextual information in textual modality, ranging from \\textit{sentence-level encoders}  to \\textit{hierarchical document level encoders}  with \\textit{Long Short Term Memory (LSTM) units}  or \\textit{Gated Recurrent Units (GRU)}  as the underlying RNN architecture. Most of the visual encoders do not train the parameter weights from scratch, but rather prefer to use CNN based pre-trained embeddings (refer to Section \\ref{subsec:preprocess}). However, notably, in order to capture the contextual information of images,  used a bi-directional GRU unit to encode information from multiple images (encoded using VGGNet ) into one context vector, which is a unique approach for discrete image inputs. However, this RNN-CNN based encoding strategy is very standard approach adopted to  encoding video input.  and  in their respective works use pre-trained CNNs to encode individual frames, and then feed them as input to randomly initialized bi-directional RNNs to capture the temporal dependencies across these frames.  and  use ResNeXt-101 3D Convolutional Neural Network  trained to recognize 400 diverse human actions on the Kinetics dataset  to tackle the problem of generating text summaries for tutorial videos from How2 dataset .\n\\vspace{1.5mm}\n\\noindent\\underline{Multi-modal fusion strategies:} A lot of fusion techniques have been developed in the field of MMS. However, most of the works that take text-image based inputs focus on \\textit{multi-modal attention} to facilitate a smooth information flow across the two modalities. Attention strategies has proven to be a very useful technique to help discard noise and focus on relevant information . The attention mechanism has been adopted by all the neural models that attempt to solve the MMS task. It has been applied to modal-specific information (\\textit{uni-modal attention}), as well as at the information sharing step in form of \\textit{multi-modal attention} to determine the degree of involvement of a specific modality for each input individually.  proposed the hierarchical multi-modal attention for the first time to solve the task of multi-modal summarization of long sentences. The attention module comprises of individual text and image attention layers, followed by a subsequent layer of modality attention layer. Although multi-modal attention has shown great promise in text-image summarization tasks, it itself is not sufficient for text-video-audio summarization tasks . Hence, to overcome this weakness,  proposed \\textit{bi-hop attention} as an extension of bi-linear attention , and  developed a novel \\textit{conditional self-attention mechanism} module to capture local semantic information of video conditioned on the input text information. Both of these techniques were backed empirically, and established state-of-the-art in their respective problems.\n\\vspace{1.5mm}\n\\noindent\\underline{Decoder:} Depending on the encoding strategy used, the textual decoders also vary from plain \\textit{unidirectional RNN}  generating a word at a time to \\textit{hierarchical RNN decoders}  performing this step in multiple levels of granularity. Although a vast majority of neural models focus only on generating textual summary using multi-modal information as input , some work also output images as an supplement to the generated summary ; reinforcing the textual information and improving the user experience. These works either use a post-processing strategy to select the image(s) to become a part of final multi-modal summary , or they incorporate this functionality in their proposed model . All the three frameworks that have implicit text-image summary generation characteristic adapt the final loss to be a weighted average of text generation loss together with the image selection loss.  treats the image selection as a classification task and adopts cross-entropy loss to train the image selector.  also treats the image selection process as a classification problem, and adopts an unsupervised learning technique that uses RL methods . The proposed technique uses representativeness and diversity as the two reward functions for the RL learning.  perceives the proposes a \\textit{cover frame selector}\\footnote{Model proposed by  only selects one image per input, chosen from the video frames.} that selects one image based on the hierarchical CNN-RNN based video encoding conditioned on article semantics using a \\textit{conditional self-attention module}.  uses pairwise hinge loss to measure the loss during the model training.\nAlthough the encoder-decoder model acts as the basic skeleton for the neural models solving MMS task, a lot of variations have been made, depending upon the input and output specifics.  proposes a visual coverage mechanism to mitigate the repetition of visual information. \n uses two image filters, namely \\textit{image attention filter} and \\textit{image context filter} to avoid noise introduction, filtering out useful information.  proposes a multi-modal objective function, that generates multi-modal summary at the end of this step, avoiding any statistical post-processing step for image selection.  utilizes the fact that audio and video are synchronous, and audio can easily be converted to textual format, utilizing these speech transcriptions as the the bridge across the asynchronous modalities of text and video. They also formulate various fusion techniques including \\textit{early fusion} (concatenation of multi-modal embeddings), \\textit{tensor fusion} , and \\textit{late fusion}  to enhance the information representation in the latent space.", "cites": [1284, 1286, 1285, 1032, 1271, 305, 38, 1267, 1283, 1282, 243, 1270], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 21, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes well by integrating various neural models and fusion strategies from multiple papers into a coherent overview of MMS methods. It shows some critical analysis by discussing limitations of attention mechanisms and mentioning alternative approaches like RL and hinge loss. It abstracts to a moderate extent by identifying patterns in encoder-decoder architecture and attention-based fusion but lacks a deeper meta-level analysis."}}
{"id": "59a42624-a26f-451e-a0af-cd2fbc235710", "title": "Nature Inspired Algorithms", "level": "subsubsection", "subsections": [], "parent_id": "7013dadb-301e-427d-ae12-6cf38e4e66ec", "prefix_titles": [["title", "A Survey on Multi-modal Summarization"], ["section", "Overview of Methods"], ["subsection", "Main Model"], ["subsubsection", "Nature Inspired Algorithms"]], "content": "Genetic algorithms  and other nature inspired meta-heuristic optimization algorithms like the Grey Wolf Optimizer  and Water Cycle algorithm  have shown great promise for extractive text summarization .   has illustrated that such algorithms can also be useful in multi-modal scenarios by experimenting with a multi-objective setting using differential evolution as the underlying guidance strategy. For the multi-objective optimization setup, the authors have proposed two different sets of objectives: one redundancy based (including uni-modal salience, redundancy and cross-modal correspondence) and one using cluster validity indices (PBM index  was used in this case). Both of these settings have performed better than the baselines. The optimization setup outputs the top most suitable sentences and images, which follow similar post-processing procedure as .  on the other hand used Grey Wolf Optimizer  based multi-objective optimization strategy to obtain the combined complementary-supplementary multi-modal summaries. The proposed approach was split into two key steps: a) global coverage text format (GCTF) - obtaining extractive text summaries using Grey Wolf Optimizer over all the input modalities in a clustering setup, b) visual enhanced text summaries (VETS) - using one-shot population based strategy to enhance the obtained text summaries with visual modalities to obtain the complementary and supplementary enhancements in a data-driven manner. The overall pipeline adopted similar pre-processing and post-processing steps as .", "cites": [1268], "cite_extract_rate": 0.125, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes two different approaches using nature-inspired algorithms for multi-modal summarization, connecting them through their use of multi-objective optimization and similar pre/post-processing. It provides an analytical overview by describing the objectives and steps involved in each method, but lacks deeper critical evaluation of their limitations or comparative strengths. Some abstraction is attempted by grouping the approaches under a common theme, but broader patterns or principles in the use of these algorithms are not clearly articulated."}}
{"id": "62f706e4-1b45-47d4-9d6e-89df14829fc4", "title": "Graph based Models", "level": "subsubsection", "subsections": [], "parent_id": "7013dadb-301e-427d-ae12-6cf38e4e66ec", "prefix_titles": [["title", "A Survey on Multi-modal Summarization"], ["section", "Overview of Methods"], ["subsection", "Main Model"], ["subsubsection", "Graph based Models"]], "content": "Graph based techniques have been widely adopted in extractive text summarization frameworks . These techniques involve graph formulation of text documents where nodes are represented by document sentences and the edge weights are formulated using similarity across two sentences. Extending this idea to a multi-modal set up, Modani et al.  proposed a graph based approach to generate text-image summaries. A graph was constructed using \\textit{content segments} (representing either sentences or images) as the nodes, and each node is given a weight depending on its information content. For sentences, this weight is computed as the sum of \\#nouns, \\#adverbs, \\#adjectives, \\#verbs, and half the \\#pronouns, while an images node's weight is given by the average similarity score with all other image segments. An edge weight for an edge connecting two sentences is computed as the cosine similarity of sentence embeddings (evaluated using auto-encoders), edge weight connecting two images is computed as the cosine similarity of image embeddings (evaluated using VGGNet ) and the edge weight connecting a sentence and an image is computed as the cosine similarity of sentence embedding and image embedding projected in a shared vector space (using Deep Fragment embeddings ). After graph construction, an iterative greedy strategy  is adopted to select appropriate content segments and generate the \\textit{text-image summary}. \nLi et al.  also use a graph based technique to evaluate the salience of text to generate an extractive text summary using multi-modal input (containing text documents, images, videos). A guided LexRank  was proposed to evaluate the salience score of the text unit (comprising of document sentences and speech transcriptions). The guidance strategy proposed by Li et al.  had bidirectional connections for sentences belonging to documents, but only unidirectional connections were made for speech transcriptions with only outward edges to follow on their assumption that speech transcriptions might not always be grammatically correct, and hence should only be used for guidance and not for summary generation. This textual score was then used as a submodular function for the final model (refer to Sec \\ref{subsec:submod}).", "cites": [1280, 305, 8435], "cite_extract_rate": 0.375, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview by connecting graph-based methods from both text and multi-modal domains, showing how Modani et al. and Li et al. adapt these techniques for multi-modal summarization. It integrates key components like node weighting and edge similarity computation, but lacks a deeper critical evaluation of the methods' strengths, limitations, or comparative effectiveness. Some level of abstraction is achieved by discussing general principles of graph construction for multi-modal data."}}
{"id": "38c6076c-0af6-47fe-9908-7c9d2ca81628", "title": "Post-processing", "level": "subsection", "subsections": [], "parent_id": "da99a035-2438-4b24-8b5e-87248532999a", "prefix_titles": [["title", "A Survey on Multi-modal Summarization"], ["section", "Overview of Methods"], ["subsection", "Post-processing"]], "content": "\\label{sec:post-proc}\nMost of the existing works are not capable of generating multi-modal summaries\\footnote{Although, all the surveyed methods are \"multi-modal summarization\" approaches, i.e. they all summarize multi-modal information, however, most of them summarize it to generate uni-modal outputs.}. The systems that do generate multi-modal summaries either have an inbuilt system capable to generating multi-modal output (mainly by generating text using seq2seq mechanisms and selecting relevant images)  or they adopt some post-processing steps to obtain the visual and vocal supplements of the generated textual summaries . Neural network models that use multi-modal attention mechanisms to determine the relevance of modality for each input case have been used for selecting the most suitable image . More precisely, the visual coverage scores (after the last decoding step), i.e. the summation of attention values while generating the text summary, are used to determine the most relevant images. Depending upon the needs of the task, a single image  as well as multiple images  can be extracted to supplement the text. \n\\begin{figure}[!ht]\n  \\centering\n  \\subfloat[Input image distribution]{\\includegraphics[width=0.5\\textwidth]{dat_in_img.pdf}}\n  \\subfloat[Input audio/video distribution]{\\includegraphics[width=0.5\\textwidth]{dat_in_vid.pdf}}\n  \\\\\n  \\subfloat[Language distribution in datasets.]{\\includegraphics[width=0.5\\textwidth]{dat_lan.pdf}}\n  \\subfloat[Abstractive/Extractive text output]{\\includegraphics[width=0.5\\textwidth]{dat_out_txt.pdf}}\n  \\\\\n  \\caption{Dataset statistics.} \\label{fig:data}\n\\end{figure}\n proposes a text-image-video summary generation task, which as the name suggests, outputs all possible modalities in the final summary. Having extracted most important sentences and images (containing video key-frames as well) using the ILP framework, the images are separated from the key-frames, and are supplemented with other images from the input set that have a moderate similarity, with a pre-determined threshold and upper bound to avoid noisy and redundant information. Cosine similarity of global image features is used as the underlying similarity matrix in this case. A weighted average of verbal scores and visual scores is used to determine the most suitable video for the multi-modal summary. \\textit{verbal score} is defined as the information overlap between speech transcriptions and generated text summary, while the \\textit{visual score} is defined as the information overlap between the key-frames of a video with the generated image summary.", "cites": [1271], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of post-processing methods in multi-modal summarization, mainly paraphrasing the approach from the cited paper. It integrates some concepts like attention mechanisms and similarity-based selection but lacks a deeper synthesis of ideas from multiple sources. There is minimal critical analysis or identification of broader patterns or principles in the field."}}
{"id": "a55206a7-b7b1-446f-878b-d99901b3829b", "title": "Datasets and Evaluation Techniques", "level": "section", "subsections": ["b762874e-e27a-48e4-99b1-20fc854e5fdb", "db0803f9-e7b2-491f-8995-8cb0b8c1d8b2"], "parent_id": "de20a706-7b82-4ffa-aeec-10b931cc4cdb", "prefix_titles": [["title", "A Survey on Multi-modal Summarization"], ["section", "Datasets and Evaluation Techniques"]], "content": "\\label{sec:data}\nDue to the flexible nature of the MMS task, with a large variety of input-output modalities, the MMS task does not have a standard dataset used as a common evaluation benchmark for all approaches to this date. Nonetheless, we have collected information about datasets used in the previous works, and a comprehensive study of 20 datasets can be found at Table \\ref{tab:datasets}. \nIt was found that out of these 21 datasets, 12 datasets are of news-related origin , and including the dataset on video tutorials by , there are 13 datasets that are domain-independent, thus suitable to test out domain generic models. 6 out of the 21 datasets produce text-only summaries using multi-modal input; out of these six datasets, 2 datasets' output comprises of extracted text summaries  and 4 datasets' output contains abstractive summaries . One the other hand, there are 8 datasets that output text-image summaries, which can further be divided into 6 extractive text-image summary generation datasets  and 2 abstractive text-image summary generation datasets . Datasets \\#19 () and \\#20 () are the only two datasets that comprise of text, image, audio and video in the output. However, these datasets are small, and thus limited to extractive summarization techniques. Meanwhile, dataset \\#20 () is the only existing dataset that comprises of both complementary and supplementary enhancements in the multi-modal summary (refer to Section \\ref{sec:mms} for the definition). Out of the 21 datasets, 17 datasets contain text in the multi-modal summary, 11 contain images as well, 3 comprises solely of audio-video outputs , and 1 dataset has a fixed template\\footnote{ focusing on summarizing tennis matches, and thus the output has a fixed template comprising of three different summarization tasks: a) summarization of entire tournament, b) summarization of a match and c) summarization of a tennis player.} as output . Of these 17 text-containing datasets, 10 datasets contain extractive text summaries  and the rest 7 datasets contain abstractive summaries . It is interesting to note that 5 out of these 7 abstractive datasets belong to the news-domain , while the other two focus on e-commerce product summarization  and tutorial summarization .Out of the $21$ datasets only $4$  of them are publicly available.\n\\begin{figure}[ht] \\ContinuedFloat\n  \\subfloat[Single vs Multiple document distribution for datasets]{\\includegraphics[width=0.5\\textwidth]{dat_doc.pdf}}\n  \\subfloat[Domain distribution for datasets]{\\includegraphics[width=0.5\\textwidth]{dat_dom.pdf}}\n  \\\\\n  \\subfloat[Input modality distribution]{\\includegraphics[width=0.5\\textwidth]{dat_in.pdf}}\n  \\subfloat[Output modality distribution]{\\includegraphics[width=0.5\\textwidth]{dat_out.pdf}}\n  \\caption{Dataset statistics (cont.).}\n\\end{figure}\nDepending on the input, we can also divide the 20 datasets based on the presence/absence of video in the input. There are 10 datasets that contain videos, whereas the rest 11 mostly work with text-image inputs. Due to the nature of this survey (the main focus on text modality), all 21 datasets in consideration contain text as input. A majority of these text sources are single documents , but there are 6 datasets that have multiple documents in the input . ,  and  however do not contain text documents, but the speech transcriptions from corresponding audio inputs. While most of these datasets comprise of multi-sentence summaries generated from input documents,  contains a single sentence as the source as well as the reference summary. Most of these datasets use English-based text and audio, but there are 3 datasets that contain Chinese text . \nThere are some datasets that have inputs other than text, image, audio and video. For instance,  and  contain temporal information about the dataset for the task of multi-modal timelines generation.  also utilize user information including demographics like gender, birthday, user profile (short biography), and other information including user name, nickname, number of followers, number of microblogs posted, profile registration time, and user's level of interests in different topics for generating summaries of an event based on social  media content. Detailed plots for selected statistics on the datasets covered in this study can be found at Figure \\ref{fig:data}.\n\\begin{table}[ht]\n\\centering\n\\tiny\n\\caption{\\textbf{A study on datasets available for multi-modal summarization.} `T' stands for English text, `TC' stands for Chinese text, `TF' stands for text (template filling), `TE' stands for text (extractive), `TA' stands for text (abstractive), `I' stands for images, `V' stands for video, `A' stands for audio, `U' signifies user information, and `TM' denotes existence of temporal information about the data such as publication date. The `\\textbf{*}' denotes publicly available datasets and the `-' denotes the unavailability of details.}\\label{tab:datasets}\n\\renewcommand{\\arraystretch}{2}\n\\begin{tabular}{|l|p{0.06\\textwidth}|p{0.08\\textwidth}| p{0.09\\textwidth}| p{0.3\\textwidth}|p{0.08\\textwidth}|}\n\\hline\n\\textbf{ID \\& Paper} &\n\\textbf{Used In Paper} &\n\\textbf{Input Modalities} & \\textbf{Output Modalities} & \\textbf{Data Statistics} & \\textbf{Domain} \\\\\n\\hline\n\\#1:  (2018) &  & T, I & TA & 66,000 triplets (sentence, image and summary) & News \\\\\\hline\n\\#2: (2018)\\textbf{*}  & & T, I & TA, I & 313k documents,  2.0m images & News \\\\\\hline\n\\#3:  (2018) &  & T, I & TA, I & 219k documents & News \\\\\\hline\n\\#4:  (2013) &  & T, I & TE, I & 8 topics (each containing 150+ documents) & News \\\\\\hline\n\\#5:  (2013) &  & TC, I & TE, I & 10 topics (127k microblogs and 48k images) & Social Media \\\\\\hline\n\\#6:  (2014) &  & TC, I & TE, I & 20 topics (310k documents, 114k images) & Social Media \\\\\\hline\n\\#7:  (2020) &  & TC, I & TA & 1,375,453 instances from home appliances, clothing, and cases \\& bags categories & E-commerce \\\\\\hline\n\\#8:  (2018) &  & T, A & TE & - & News \\\\\\hline\n\\#9:  (2018) &  & T, I, TM & TE, I & 6 topics & Social Media \\\\\\hline\n\\#10:  (2012) &  & T, I, TM & TE, I & 4 topics (6k documents, 2k images) & News \\\\\\hline\n\\#11:  (2019) &  & T, I, U & TE, I & 12 topics (9.1m documents,  2.2m users, 15m images) & News (disasters) \\\\\\hline\n\\#12:  (2011) &  & T, A, V & TF & 66 hrs video (33 matches), 1,250 articles related to Australian Open 2010 tennis tournament & Sports (Tennis) \\\\\\hline\n\\#13:  (2018)\\textbf{*} &  & T, A, V & TA & 2,000 hrs video & Multiple domains \\\\\\hline\n\\#14:  (2020) &  & T, A, V & TA & 1970 articles from Daily Mail (avg. video length 81.96 secs), and 203 articles from CNN (avg. video length 368.19 secs) & News \\\\\\hline\n\\#15:  (2020) &  & T, A, V & TA, I & 184,920  articles (Weibo) with avg. video duration 1 min, avg. article length 96.84 words, avg. summary length 11.19 words & News \\\\\\hline\n\\#16:   (2019) & & T, A, V & A, V & 20 complete soccer games from 2017-2018 season of French Ligue 1 & Sports (Soccer / Football) \\\\\\hline\n\\#17:  (2009) &  & T, A, V & A, V & 3 movie segments (5-7 min each) & Movies \\\\\\hline\n\\#18:  (2013) &  & T, A, V & A, V & 7 half hour segments of movies & Movies \\\\\\hline \n\\#19:  (2020) &  & T, I, A, V & TE, I, A, V & 25 topics (500 documents, 151 images, 139 videos)  & News \\\\\\hline\n\\#20:  (2021)\\textbf{*} &  & T, I, A, V & TE, I, A, V & 25 topics (contains complementary and supplementary multi-modal references) & News \\\\\\hline\n\\#21:  (2017)\\textbf{*} & & T, TC, I, A, V & TE & 25 documents in English, 25 documents in Chinese & News \\\\\\hline\n\\end{tabular}\n\\end{table} \nThese datasets span a wide variety of domains, including sports like tennis  and football , movies , social media-based information , e-commerce . In the coming days, we are likely bound to see more large-scale domain specific datasets to advance this field. \nAlthough there have been a lot of innovative attempts in solving the MMS task, the same does not go for the evaluation techniques used to showcase the quality of generated summaries. Most of the existing works use uni-modal evaluation metrics, including ROUGE scores  to evaluate the text summaries, accuracy and precision-recall based metrics to evaluate the image and video parts of generated summaries. A few works have also reported \\textit{True Positives} and \\textit{False Positives} as well . The best way to evaluate the quality of a summary is to perform extensive human evaluations. Various techniques have been used to get the best user performance evaluations including the quiz method ,\nand user-satisfaction test . These manual evaluation techniques are mainly of two kinds: a) simple scoring of summary quality based on input , and b) answering \nthe questions based on input to quantify the information retention of input data instance . However, one major issue with these manual evaluations is that they cannot be conducted for the entire dataset, and are hence performed on a subset of the test dataset. There are a lot of uncertainties involving this subset, as well as the mental conditions of the human evaluators while performing these quality checks. Hence it can be unreliable to compare two results of separate human evaluation experiments, even for the same task.", "cites": [1285, 1271, 1268, 1270, 1266], "cite_extract_rate": 0.17857142857142858, "origin_cites_number": 28, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a detailed listing of datasets and their attributes, but lacks synthesis across the cited works to form a broader narrative. It offers minimal critical analysis, merely stating facts like limited public availability and dataset sizes without evaluating their implications. While some patterns (e.g., news vs. e-commerce domains) are noted, the generalization remains shallow and descriptive in nature."}}
{"id": "b762874e-e27a-48e4-99b1-20fc854e5fdb", "title": "Text Summary Evaluation Techniques", "level": "subsection", "subsections": [], "parent_id": "a55206a7-b7b1-446f-878b-d99901b3829b", "prefix_titles": [["title", "A Survey on Multi-modal Summarization"], ["section", "Datasets and Evaluation Techniques"], ["subsection", "Text Summary Evaluation Techniques"]], "content": "Since the scope of this work is mostly limited to text-centric MMS techniques,\nit is important to discuss evaluation of text summaries separately and in tandem to other modalities. Even though quite a few MMS works generate uni-modal text summaries from multi-modal inputs , they still use very basic string based n-gram overlap metrics like ROUGE  to conduct the evaluation. Through this survey, we want to direct the researchers to not just focus on ROUGE, but also look at other aspects of text summarization as well. For instance,  proposes four key-characteristics that an ideal summary must have:\n\\begin{enumerate}\n    \\item \\textbf{Coherence} the quality of smooth transition between different summary sentences such that sentences are not completely unrelated or completely same.\n    \\item \\textbf{Consistency} the factual correctness of summary with respect to input document.\n    \\item \\textbf{Fluency} the grammatical correctness and readability of sentences.\n    \\item \\textbf{Relevance} the ability of a summary to capture important and relevant information from the input document.\n\\end{enumerate}\n also illustrated how ROUGE is not capable of gauging the quality of generated summaries by doing an n-gram overlap with human written reference summaries. There are other metrics out there that use more advanced strategies to do the evaluation, such as n-gram based metric like WIDAR , embedding-based metrics like ROUGE-WE , MoverScore  and Sentence Mover Similarity (SMS)  or neural model-based metrics like BERTScore , SUPERT , BIANC  and S$^3$ . These evaluation metrics have been proven to be empirically better at captivating the above-mentioned characteristics of a summary , and hence upcoming research works should also report performance on some of these metrics along with ROUGE to have more accurate analysis of the generated summaries.", "cites": [1287, 7340], "cite_extract_rate": 0.18181818181818182, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates the key characteristics of a good summary from one source and uses it to contextualize the limitations of ROUGE, while also mentioning alternative metrics. It shows some synthesis by linking these ideas to the evaluation of multi-modal summarization. However, the critical analysis is limited to general statements about ROUGE’s shortcomings, and the abstraction is moderate with identification of broader evaluation characteristics but no deeper meta-level insights."}}
{"id": "db0803f9-e7b2-491f-8995-8cb0b8c1d8b2", "title": "Multi-modal Summary Evaluation Techniques", "level": "subsection", "subsections": [], "parent_id": "a55206a7-b7b1-446f-878b-d99901b3829b", "prefix_titles": [["title", "A Survey on Multi-modal Summarization"], ["section", "Datasets and Evaluation Techniques"], ["subsection", "Multi-modal Summary Evaluation Techniques"]], "content": "In an attempt to evaluate the multi-modal summaries,  proposes a multi-modal automatic evaluation (MMAE) technique that jointly considers uni-modal salience and cross-modal relevance. In their case, the final summary comprises of text and images, and the final objective function is formulated as a mapping of three objective functions: 1) salience of text, 2) salience of images, and 3) text-image relevance. This mapping function is learnt using supervised techniques (Linear Regression, Logistic Regression and Multi-layer Perceptron in their case) to minimize training loss with human judgement scores. Although the metric seems promising, there are a lot of conditions that must be met in order to perform the evaluation. \nThe MMAE metric does not effectively evaluate the information integrity\\footnote{Information integrity is the dependability or trustworthiness of information. In context of a multi-modal summary evaluation task, it refers to the ability to make a judgement that is unbiased towards any modality (i.e. an ideal evaluation metric does not give higher importance to information from one modality (e.g. text) over other modality (e.g. images)).} of a multi-modal summary, since it uses uni-modal salience scores as a feature in the overall judgement making process, leading to a cognitive bias.   improves upon this by proposing an evaluation metric based on joint multi-modal representation (termed as MMAE++), projecting the generated summaries and the ground truth summaries into a joint semantic space. In contrast to other multi-modal evaluation metrics, they attempt to look at the multi-modal summaries as a whole entity, than a combination of piece-wise significant elements. A neural network based model is used to train this joint representation. Images of two image caption pairs are swapped to obtain two image-text pairs that are semantically close to each other to obtain the training data for joint representation automatically. The evaluation model is trained using a multi-modal attention mechanism  to fuse the text and image vectors, using max-margin loss as loss function.\n propose a novel evaluation technique termed by them as \\textit{Multimedia Summary Quality (MuSQ)}. Just like other multi-modal summarization metrics described above, \\textit{MuSQ} is also limited to text-image summaries. However, unlike the majority of previous evaluation metrics for multi-modal summarization or document summarization techniques , \\textit{MuSQ} does not require a ground truth to evaluate the quality of generated summary. \\textit{MuSQ} is a naive coverage based evaluation metric denoted as $\\mu_M$, and is defined as:\n\\begin{gather}\n    \\mu_M = \\mu_T + \\mu_I + \\sigma_{T,I}\\\\\n    \\mu_T = \\sum_{v\\in T} R_v * max_{u\\in S}\\{Sim(u,v)\\}\\\\\n    \\mu_I = \\sum_{w\\in V} \\hat{R_w} * max_{x\\in I}\\{Sim(w,x)\\}\\\\\n    \\sigma_{T,I} = \\sum_{v\\in S}\\sum_{w\\in I} \\{Sim(w,x) * R_v * \\hat{R_w}\\}\n\\end{gather}\nwhere $\\mu_T$ denotes the degree of coverage of input text document $T$ by text summary $S$, $\\mu_I$ denotes the degree of coverage of input images $V$ by the image summary $I$. $\\sigma_{T,I}$ measures the cohesion across the text sentences and images of final multi-modal summary. $R_v$ and $\\hat{R_w}$ are respectively the individual reward values for each input sentence and input image that denote the extent of information content in each content fragment (a text sentence or an image).\n\\begin{table}[ht]\\centering\n\\scriptsize\n\\caption{\\textbf{Comparative study of evaluation techniques for evaluation techniques on multi-modal summarization.}}\\label{tab:evaluation}\n\\renewcommand{\\arraystretch}{2}\n\\begin{threeparttable}\n\\begin{tabular}{|p{0.3\\textwidth}| p{0.6\\textwidth}|}\n\\hline\n\\textbf{Metric name \\& corresponding paper} & \\textbf{Pros \\& Cons} \\\\\n\\hline\n\\multirow{5}{0.2\\textwidth}{Multi-modal Automatic Evaluation (MMAE). } & \\textbf{Advantages} \\\\\n& - MMAE shows high correlation with human judgement scores. \\\\\n& \\textbf{Disadvantages} \\\\\n& - Requires a substantial manually annotated dataset. \\\\\n& - Might perform ambiguously$^\\ddagger$ for evaluation of other new domains. \\\\\n\\hline\n\\multirow{5}{0.2\\textwidth}{MMAE++. } & \\textbf{Advantages} \\\\\n& - Utilizes joint multi-modal representation of sentence-image pairs to better improve the correlation scores over MMAE metric . \\\\\n& \\textbf{Disadvantages} \\\\\n& - Requires a substantial manually annotated dataset. \\\\\n& - Might perform ambiguously\\tnote{1} for evaluation of other new domains. \\\\\n\\hline\n\\multirow{5}{0.2\\textwidth}{Multimedia Summary Quality (MuSQ). } & \\textbf{Advantages} \\\\\n& - Does not require manually created gold summaries. \\\\\n& \\textbf{Disadvantages} \\\\\n& - The technique is very naive, and only considers coverage of input information and text-image cohesiveness. \\\\\n& - The metric output is not normalized. Hence the evaluation scores are highly sensitive to the cardinality of input text sentences and input images. \\\\\n\\hline\n\\end{tabular}\n\\begin{tablenotes}\n    \\item[1] Here \"might perform ambiguously\" refers to the fact that since model-based metrics are biased towards the training data, it is hard to determine how well would they perform on unseen domains. For instance, if the model is trained on news summarization dataset, and the task is to evaluate medical report summaries, then the model performance cannot be determined without further experiments.\n\\end{tablenotes}\n\\end{threeparttable}\n\\end{table}\nTo sum up, only a handful of works have focused on the evaluation of multi-modal summaries. Even the proposed evaluation metrics have a lot of drawbacks. The evaluation metrics proposed by  and  require a large human evaluation score-based training data to learn the parameter weights. Since these metrics are highly dependent on the human-annotated dataset, the quality of this dataset can compromise the evaluation process if the training dataset is restrictive in domain coverage or is of poor quality. It also becomes difficult to generalize these metrics since they depend on the domain of training data. The evaluation technique proposed by , although independent from gold summaries, is too naive, and has its own drawbacks. The evaluation metric is not normalized, and hence shows great variation when comparing the results of two input data instances with different sizes. \nOverall, the discussed strategies have their own pros and cons; however, there is a great scope for future improvement in the area of `evaluation techniques for multi-modal summaries' (refer to Section \\ref{sec:future}).\n\\begin{table}[tp]  \\centering\n\\caption{\\textbf{Results of different methods for text and image output modalities.} This study is limited to works that contain text in the generated multi-modal summary\\textsuperscript{$\\dagger$}. Note that the comparison should be done with care as most of the proposed approaches use different datasets (the ``Dataset No.'' column corresponds to the ID column in Table \\ref{tab:datasets}). Column `ME' indicates presence/absence of manual evaluation in the corresponding work. Here 'N.A' or Not Available is used to denote the unavailablity of images in the output or unavailabilty of scores for an evaluation metric. '(ABS)' denotes abstractive summarization type and '(EXT)' denotes extractive summarization type. \\newline \\small\\textsuperscript{$\\dagger$ For population based techniques , the best score across multiple solutions were reported in this work.}}\\label{tab:results}\n\\renewcommand{\\arraystretch}{2}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{|l|l|l|c|c|c|c|c|c|c|c|} \\hline\n\\multirow{2}{*}{\\textbf{Paper}} & \\multirow{2}{*}{\\textbf{Dataset No.}} & \\multirow{2}{*}{\\textbf{Domain}} & \\multicolumn{4}{c|}{\\textbf{Text score (ROUGE)}} & \\multicolumn{3}{c|}{\\textbf{Image score}} & \\multirow{2}{*}{\\textbf{ME}} \\\\ \\cline{4-10}\n &  & & \\textbf{R-1} & \\textbf{R-2} & \\textbf{R-L} & \\textbf{R-SU4} & \\textbf{Precision} & \\textbf{Recall} & \\textbf{MAP} & \\\\ \\hline\n\\multirow{2}{*}{ (EXT)} &  (English) & News & 0.442 & 0.133 & N.A & 0.187 & N.A & N.A & N.A & \\checkmark \\\\\n &  (Chinese) & & 0.414 & 0.125 & N.A & 0.173 & N.A & N.A & N.A & \\checkmark \\\\ \\hline\n (ABS) &  & News & 0.472 & 0.248 & 0.444 & N.A & N.A & N.A & N.A & \\\\ \\hline\n (ABS) &  & News & 0.408 & 0.1827 & 0.377 & N.A & 0.624 & N.A & N.A & \\checkmark \\\\\n (ABS) & & & 0.411 & 0.183 & 0.378 & N.A & 0.654 & N.A & N.A & \\checkmark \\\\ \\hline\n (EXT) &  & News & 0.271 & 0.125 & 0.156 & N.A & N.A & N.A & N.A & \\\\ \\hline\n (ABS) &  & News & 0.326 & 0.120 & 0.238 & N.A & N.A & 0.4978 & N.A & \\\\ \\hline\n (ABS) & & Multi-domain & N.A & N.A & 0.549 & N.A & N.A & N.A & N.A & \\checkmark \\\\ \\hline\n (EXT) &  & News & 0.260 & 0.074 & 0.226 & N.A & 0.599 & 0.38 & N.A & \\\\\n (EXT) &  & & 0.420 & 0.167 & 0.390 & N.A & 0.767 & 0.982 & N.A & \\\\ \\hline\n (EXT) & & News & 0.556 & 0.256 & 0.473 & N.A & 0.620 & 0.720 & N.A & \\checkmark \\\\ \\hline\n (EXT) & & News & 0.369 & 0.097 & N.A & N.A & N.A & N.A & N.A & \\\\ \\hline\n (EXT) & & Social Media & 0.507 & 0.303 & N.A & 0.232 & N.A & N.A & N.A & \\\\ \\hline\n (EXT) & & News & 0.442 & 0.109 & 0.320 & N.A & N.A & N.A & N.A & \\\\ \\hline\n\\multirow{2}{*}{ (EXT)} &  (social trends)& Social Media & 0.504 & 0.307 & N.A & 0.235 & N.A & N.A & N.A & \\\\\n &  (product events) & & 0.478 & 0.279 & N.A & 0.187 & N.A & N.A & N.A & \\\\ \\hline\n\\multirow{2}{*}{ (EXT)} &  (DailyMail) & News & 0.417 & 0.186 & 0.317 & N.A & N.A & N.A & N.A & \\checkmark \\\\\n &  (CNN) & & 0.278 & 0.088 & 0.187 & N.A & N.A & N.A & N.A & \\checkmark \\\\\\hline\n (ABS) & & News & 0.251 & 0.096 & 0.232 & N.A & N.A & N.A & 0.654 & \\checkmark\\\\\\hline\n\\multirow{3}{*}{ (ABS)} &  (Home Appliances)& E-commerce & 0.344 & 0.125 & 0.224 & N.A & N.A & N.A & N.A & \\checkmark \\\\\n &  (Clothing) & & 0.319 & 0.111 & 0.215 & N.A & N.A & N.A & N.A & \\checkmark \\\\\n &  (Cases \\& Bags) & & 0.338 & 0.125 & 0.224 & N.A & N.A & N.A & N.A & \\checkmark \\\\\\hline\n\\end{tabular}}\n\\end{table}", "cites": [1285, 1271, 1268, 1270], "cite_extract_rate": 0.2, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 4.2, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section goes beyond merely describing the evaluation techniques by critically analyzing their limitations, such as cognitive bias and lack of normalization. It synthesizes the key contributions of the cited works, comparing MMAE, MMAE++, and MuSQ in a structured manner, and highlights the trade-offs between requiring gold summaries and being too simplistic. It also generalizes these insights to identify the broader challenge of domain dependency and the need for future improvements in multi-modal summary evaluation."}}
{"id": "7bb2a9de-ee7f-4f08-8c4b-4c0f149e59db", "title": "Results and Discussion", "level": "section", "subsections": [], "parent_id": "de20a706-7b82-4ffa-aeec-10b931cc4cdb", "prefix_titles": [["title", "A Survey on Multi-modal Summarization"], ["section", "Results and Discussion"]], "content": "Since the MMS task is quite broad, covering multiple sub-problem statements, it is difficult to compare models due to the lack of a standard evaluation metric (refer to Section \\ref{sec:data}). We are then restricted to presenting the results using uni-modal evaluation techniques like ROUGE scores  for text summaries, and precision-recall scores for image summaries. In Section \\ref{sec:seg}, we have described the diversity of works done so far, with some working on timeline generation , while others working on generic news summarization ; making it difficult to conduct a fair comparison of different architectures\\footnote{Note that we only display the results that have text as the \\textit{central modality} (refer to Section \\ref{sec:mms}).}. Even comparing two models that have a very similar settings like  and   (both are trained on large scale abstractive news summarization datasets), is not adequate because datasets \\#5 and \\#7 have different sizes of training data (refer to Table \\ref{tab:datasets}). Other such example is of  and , both these works intake text-video inputs; however  is trained on English dataset with ~2k instances, and  is trained on Chinese dataset with ~1,84k instances (refer to Table \\ref{tab:datasets}). Nonetheless, we attempted to give the readers an overview of the potential of existing architectures. There are a few observations that can be made even with these constraints. We can observe that the abstractive summarization models go neck-to-neck with extractive summarization models, even though extractive summarization models have an advantage of keeping the basic grammatical syntax intact, illustrating the advancement in neural summarization models in the MMS task. An extensive study can be found in Table \\ref{tab:results}.\nThere exist some works that share a common dataset to illustrate the efficacy of their proposed model architectures. For instance,  and  share a common dataset (dataset \\#2). Both the works produce competitive results, with  outperforming  by small difference in all modalities. It can also be observed from the results of  that the input language does not affect the quality of summary at all. Results for both English and Chinese datasets (refer to dataset \\#3 in Table \\ref{tab:datasets}) are close, and the difference can be accredited to non-overlapping content across the two datasets. We can also observe from the results by  that neural models require large datasets to perform better. The CNN part of dataset only comprises of 200 data instances, while the DailyMail part of dataset comprises of 1970 instances. The authors also suggest that the larger size of videos in CNN data leads to worse performance, even though the underlying learning strategies are the same. \nSome datasets are also an extension of existing ones; for instance, dataset \\#19 () was extended from dataset \\#21 *) by incorporating images and videos in the references, while dataset \\#20 () was extended from dataset \\#19 () by introducing complementary and supplementary enhancements for the multi-modal references. Therefore all four works share the same reference summaries. Hence even though the other modalities differ, the works can be partially compared with each other for the text modality. From this, it can be deduced that the two-step approach proposed by  that first generates the \\textit{Global Coverage Text Format summary (GCTF)} using grey-wolf optimizer on a multi-objective optimization setup, and then enhances this by using other modalities outperforms all the prior works; illustrating the power of population based techniques. The submodular optimization  is able to outperform the Genetic Algorithm technique , which is again a population based-technique by some margin, which we believe can be credited to both, the ability of sub-modular optimization as well as the trade-off for a the multi-modal summary generation framework. Since  only-generates text, while  generates a multi-modal output comprising of text, images, and videos; there might be some trade-off to improve quality of other modalities over text. Since  and  both present their works on the same dataset (dataset \\#19), and it is evident that the population-based genetic algorithm proposed in  produces better summaries as compared to the single point optimization strategy using integer linear programming proposed in , both in terms of text as well as image output. For the video output\\footnote{Since dataset \\#19 () and \\#20 () are the only datasets which contain text and video in the output, we have reported the results in text instead of making another column in Table \\ref{tab:results}. It should also be noted that accuracy is used to evaluate the video summary, because both of these datasets restrict a single video in the output summary. Since dataset \\#20 is extended from dataset \\#19 they both share the same text and video outputs.}  and  performed equally well with an accuracy of 44\\%, while  was able to obtain video accuracy of 64\\% (in contrast to the average accuracy of 16\\% for random selection over 10 attempts).\nOut of the 17 works reported in Table \\ref{tab:results}, 8 have performed some sort of manual evaluation along with automatic evaluation to produce a clearer picture about the performance of various summarization strategies. Through these experiments, prior works have statistically shown how the presence of multi-modal information can not only aid the uni-modal summarization process, but improve the overall user experience.  has shown that an output containing text and images increases user satisfaction by 12.4\\% in juxtaposition to text summaries.  also illustrate that having visual cues in a text summary helps improve the overall satisfaction by 22\\%, makes the topic 19\\% more fascinating, and helps users understand the topic better by 14.5\\%.  also empirically justify through manual annotations that a multi-modal summary should have both complementary and supplementary enhancements to improve the user experience.", "cites": [1270, 1271, 1268], "cite_extract_rate": 0.23076923076923078, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple cited papers to highlight trends in multi-modal summarization, particularly in relation to datasets, evaluation metrics, and the performance of different optimization strategies. It includes critical comparisons of techniques, such as the advantages of population-based methods over single-point optimization. However, the abstraction level is limited, as it does not fully generalize beyond the described works to propose overarching principles or a new theoretical framework."}}
{"id": "c82531a0-ef65-4076-a25e-44000fc8b580", "title": "Scope of improvement", "level": "subsection", "subsections": [], "parent_id": "bfe0931d-c347-430e-942f-5b842220b6c0", "prefix_titles": [["title", "A Survey on Multi-modal Summarization"], ["section", "Future Work"], ["subsection", "Scope of improvement"]], "content": "\\noindent\\textbf{Better fusion of multi-modal information:} Almost all the works discussed in this survey adopt a late-joint representation approach, where uni-modal information is extracted beforehand, and the information-sharing across multiple modalities takes place at a later stage. These works either use a pre-trained model on image captions  or train the multi-modal correspondence in a naïve way, using a neural multi-modal attention mechanism. However  have proposed a multi-stage fusion approach with a fusion forget gate\nmodule for solving the task of multimodal summarization in videos. Their proposed approach tries to improve the interaction between multiple modalities to complete the missing information of each modality. Further they have also introduced a forget gate to suppress the flow of unecessary multimodal noise. Using this approach the model was able to outperform the  model  8.3 BLEU-4 points, 7.4 ROUGEL points and 3.9 METEOR points in the How2  dataset. Although these techniques are able to capture the essence of semantic overlap across modalities, there is still room for improvement in fusion modeling.\n\\vspace{1.5mm}\n\\noindent\\textbf{Better evaluation metrics (for multi-modal summaries):} Most of the existing works use uni-modal evaluation techniques like ROUGE scores  for text, and precision-recall based metrics for images and videos. The multi-modal evaluation metrics proposed by  and   have shown some promise, but they require a large set of human evaluation scores of generated summaries for training to determine the parameter values, making them unfit as a universal metric, especially when the summaries to be evaluated are from different domains than the data the models were trained on. These proposed evaluation metrics are also very specific as they work only for text-image based multi-modal summaries. Hence the community still lacks an evaluation metric that could judge the quality of a summary comprising of multiple summaries. Even the standard text summarization metrics have some inherent shortcomings, as illustrated by the survey performed by . They illustrated that even though these metrics are able to cover up basic concepts like informativeness, fluency, succinctness and factuality; they still lack other important aspects like usefulness as discovered by the survey conducted on users who frequently use automatic summarization mechanisms. In order to improve the overall user satisfaction, similar techniques should be incorporated for the evaluation of multi-modal summarization systems as well.\n\\vspace{1.5em}\n\\noindent\\textbf{More datasets} All the datasets proposed in the community till date are mostly centered towards the news domain, even though there are multiple potential applications in other domains like medical report summarization, tutorial summarization, simplification summarization, slogan generation etc. which could benefit from multi-modal information. There are also potential new research areas that can be explored, but due to the lack of dataset availability, the community is unable to pursue research in these fields. Some of these are: explainable MMS, sentiment lossless MMS, multi-lingual MMS, data-stream MMS, large-scale MMS of long documents etc.\n\\vspace{1.5mm}\n\\noindent\\textbf{Complementary and Supplementary MMS:}\nIt is well established fact that multi-modal systems improve the user experience and help paint a clearer picture of topics or events discussed in input documents . However, there does not exist any system that can generate the complementary and supplementary multi-modal summaries together. A large majority of research work today focus on developing supplementary multi-modal summaries . There also exists some works that generate complementary multi-modal summaries as well .   also illustrated how an ideal multi-modal summary should comprise of both complementary and supplementary enhancements.\nBut the concepts of complementary and supplementary enhancements should not be limited to visual modalities over textual central modality as proposed in . For instance, summarizing articles with user opinion from the comments section can be a great application\\footnote{This task can be considered multi-modal if we extend the notion of modality to something more generic; however, since the scope of this survey is limited to the distinction in modality being the form of representing information, we do not consider such works in great detail.}. Even though this is a text-only task, the concepts of complementary and supplementary enhancements can be extended to cover up comments that cover vivid perspectives, in both favor and against the information presented in the article. \nNo abstractive complementary-supplementary MMS framework or application has been proposed in the community so far, and hence the exploration potential in this is quite vast.", "cites": [1267, 1271, 305, 1285], "cite_extract_rate": 0.36363636363636365, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple papers to highlight trends in fusion strategies and evaluation metrics, while also identifying specific limitations and unmet needs in the field. It critically evaluates the shortcomings of existing approaches and proposes generalizable directions for improvement. The abstraction is strong as it moves beyond specific systems to discuss broader patterns and future opportunities in multi-modal summarization."}}
{"id": "dca6a78e-5890-48d6-9526-709a72f81f4d", "title": "New directions", "level": "subsection", "subsections": [], "parent_id": "bfe0931d-c347-430e-942f-5b842220b6c0", "prefix_titles": [["title", "A Survey on Multi-modal Summarization"], ["section", "Future Work"], ["subsection", "New directions"]], "content": "\\noindent\\textbf{Manually generated dataset for evaluation of MMS evaluation metrics :} There is a need of some human annotated datasets to evaluate the performance of existing and upcoming evaluation metrics. There have been some works in text summarization that can be used to draw out some parallels; for instance,  releases the SummEval dataset that gives out human annotation scores for 1600 article documents scored by 11 annotators in four key-characteristics of a summary - \\textit{consistency}, \\textit{coherence}, \\textit{fluency} and \\textit{relevance}. Similar work is also needed in the MMS, where, other than uni-modal aspects, the ability to judge the cross-modal information correspondence also should be taken into account.\n\\vspace{1.5mm}\n\\noindent\\textbf{Explainable and Controlled MMS:} \n showed that automated abstractive summarization models suffer from the problem of hallucinations and often generate fictional content. Explainable and Controlled MMS refers to the process of developing summarization systems where we do not  treat these automated systems as black boxes generating summaries; Rather we have the power to understand and control the output of these models so as to produce content of our desired type.\nEven though existing MMS summarization frameworks have shown substantial improvement in the recent few years, it is still a mystery how each modality is handled and understood to obtain the final summaries. This calls for more explainable systems that also output some meta-data in tandem with the summaries to better understand the functioning of these models. Attention mechanism  is one way to get better insights in the model working. In the context of text summarization,  proposed a select and generate strategy where elements are first extracted from a document based on informativeness, novelty, and relevance and then an abstractor generates an abstractive summary using the extracted elements. Their extractor module features an interaction matrix to explain the selection logic and by changing the thresholds of the model one can control the final summary quality.\nIn the multimodal context,  proposed DGExplain which exploits the cross-modal association between the news of multiple modalities and the user comments to detect misinformation. Explainable and controlled multimodal summarization systems can be built using this kind of explainable framework to detect and filter incorrect content and summarize the true facts.  proposed a multi-tasking approach to generate topic-aware multimodal summaries. Their proposed model aims to embed topic awareness in both the visual and textual outputs. Thus, these kinds of models are stepping stones towards developing systems that are able to control the information flow from different modalities in input and output. \n\\vspace{1.5mm}\n\\noindent\\textbf{Application-oriented MMS:}\nWe can use different MMS techniques to leverage the output of various tasks like product description generation, product review summarization, multi-modal microblog summarization, education material summarization, medical report summarization and simplification of any multi-modal content.\nFor each of these tasks, earlier text-only  or image-only  summarization methods were majorly used. However,  showed that the quality of e-commerce product descriptions could be improved by incorporating visual information and textual descriptions of a product during the summarization process.  utilized the visual features from the x-rays\nassociated with the radiology reports to improve the medical report summarization quality. \nDuring any natural disaster, people post relevant content on microblogging websites, which concerned authorities could use for rescue operations.  proposed a multi-modal approach to summarize these posts utilizing both the textual and visual aspects of the post to improve the summary quality. Recently, educational content has been multi-modal, comprising video, audio and text. We believe that educational material summary quality can be significantly enhanced if information from all of these modalities is utilized during the summarization process . All of these recent works highlight the ability of MMS to combine the knowledge from various modalities to produce superior-quality summaries. Hence making it a more robust choice over the traditional uni-modality-based methods for multiple applications in future. \n\\vspace{1.5mm}\n\\noindent\\textbf{Sentiment/Emotion Lossless MMS:} The point of a summary is to provide users with the information that they'd gain from reading the entire document; and an ideal summary would not only do that, but also elicit the same sentiments that the user would feel when reading the entire document.  proposes an extractive text summarization framework that attempts to retain the sentiment of input in the generated summary. This task would be very relevant in few domains like story summarization, novel summarization etc. where the users tend to empathize with the content in the summary.  proposed a transformer based architecture to perform aspect based multimodal sentiment analysis. In future we can combine ideas from aspect based summarization systems  and multimodal aspect based sentiment recognition frameworks to gnerate sentiment aware MMS. When working with multi-modal data, this becomes even more challenging and interesting since various additional flavors of sentiment can be obtained from different modalities; and in some cases some modalities can fill the lack of sentiment in others. For instance, in a news article covering an earthquake, the text tends to be objective and devoid of subjective and sentiment-bearing expressions in order to remain professional, but the images and videos are able to convey these sentiments and emotions conveniently. Hence we believe that this kind of multi-modal summarization would help move current systems one step further in an attempt to obtain ideal summaries\\footnote{Note that this problem would be mostly restrictive to single-document summarization tasks (with some exceptions), since multiple articles tend to cover different aspects of a topic, often leading to conflicting opinions, and hence conflicting sentiments and emotions. There the problem statement can be changed to providing an unbiased and sentiment-less summary to be faithful to the users.}. \n\\vspace{1.5mm}\n\\noindent\\textbf{Multi-lingual MMS:} Multi-modal information has proven to be useful for multi-modal neural machine translation tasks , and it has been a highly debated question whether language affects visual perception, a universal form of perception, shared by all individuals . The fact that this question remains open till this date speaks volumes about how multi-modal information can prove to be useful for multi-lingual summarization tasks, if harnessed properly.\n\\vspace{1.5mm}\n\\noindent\\textbf{Data-stream MMS:} Data-stream summarization, also known as \\textit{update summarization} or \\textit{online summarization} or \\textit{dynamic summarization} has been explored in great extent in the automatic text summarization community . Data-stream summarization is used in situations where the input information is not static, and accordingly the summarization system needs to dynamically keep the summary up-to-date with the latest information. It is a challenging problem as it requires the summary to retain the key-highlights from past events, while being consistent and fluent with the most recent events as well. Data stream summarization has been used for various applications like social media content summarization , review summarization , etc.\nWith the world moving towards multi-modal information representation, there is a need to make these models robust and adaptive to multi-modal information. A few of these are discussed in the `Application-oriented MMS' part of this section. \n\\vspace{1.5mm}\n\\noindent\\textbf{Query-based MMS:} A lot of work has been done in query based text summarization , but there is no existing research on query-based summarization in a multi-modal setting. Since it has been shown that visual content can help improve the quality of experience , we believe that query-based summarization setup, that has a user-interaction, could really be improved by introducing multi-modal form of information.\n\\vspace{1.5mm}\n\\noindent\\textbf{MMS at scale:} Although some work has been done on generic datasets in terms of domain coverage , most of the existing works have been performed in a protective environment with some pre-defined notions of input and output formats. In order to produce a large-scale ready-to-use MMS framework, a more generic setup is required, that has better generalization and high adaptive capabilities.\n\\vspace{1.5mm}\n\\noindent\\textbf{MMS with user interaction:} Inspired from query-chain summarization frameworks , there is a possibility of a multi-modal summarization based on user interaction, which could help improve the overall user satisfaction.", "cites": [1287, 1288, 1290, 168, 1268, 1289, 1266, 8436, 1291], "cite_extract_rate": 0.2647058823529412, "origin_cites_number": 34, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes insights from multiple papers to construct a coherent narrative on future research directions in MMS. It critically discusses the limitations of current systems, such as hallucinations in abstractive models and the need for explainability, while also proposing potential solutions like attention-based frameworks. Furthermore, it abstracts beyond individual studies to identify broader trends and challenges, such as sentiment retention, multilingual capabilities, and real-time data stream summarization."}}
