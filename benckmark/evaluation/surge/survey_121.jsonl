{"id": "255c2a97-23ca-4f9c-97e1-17bc8daf6522", "title": "Document Processing \\& Understanding", "level": "section", "subsections": [], "parent_id": "7dee3631-5c10-4ff0-83ae-c48619c9e5c2", "prefix_titles": [["title", "A Survey of Deep Learning Approaches for OCR and Document Understanding"], ["section", "Document Processing \\& Understanding"]], "content": "\\label{sec:document-processing-understanding}\nDocument processing historically involved handcrafted rule-based algorithms~, but with the widespread success of deep learning~, computer vision (CV) and natural language processing (NLP) based methods have come to the fore.\nAdvancements in object detection and image segmentation have led to systems that edge close to human performance on a variety of tasks~.\nAs a result, these methods have been applied to a variety of other domains including NLP and speech~.\nSince documents can be read and viewed as a visual information medium, many practitioners leverage computer vision techniques as well and use them for text detection and instance segmentation~. \nWe cover specific methods to do these in Sections~\\ref{sec:text-detection} and~\\ref{sec:instance-segmentation}.\nThe widespread success and popularity of large pretrained language models such as ELMo and BERT have caused document understanding to shift towards using deep learning based models~.\nThese models can be fine-tuned for a variety of tasks and have replaced word vectors as the de-facto standard for pretraining for natural language tasks.\nHowever, language models, both recurrent neural network based and transformer based~, struggle with long sequences~.\nGiven that texts can be very dense and long in business documents, model architecture modifications are necessary.\nThe most simple approach is to truncate documents into smaller sequences of 512 tokens such that pretrained language models can be used off-the-shelf~. Another approach that has gained traction recently is based on reducing the complexity of the self-attention component of transformer-based language models .\nAll effective, modern, end-to-end document understanding systems present in the literature integrate multiple deep neural network architectures for both reading and comprehending a document's content. Since documents are made for humans, not machines, practitioners must combine CV as well as NLP architectures into a unified solution. While specific use cases will dictate the exact techniques used, a full end-to-end system employs:\n\\begin{itemize}\n    \\item A computer-vision based document layout analysis module, which partitions each document page into distinct content regions. This model not only delineates between relevant and irrelevant regions, but also serves to categorize the type of content it identifies.\n    \\item An optical character recognition (OCR) model, whose purpose is to locate and faithfully transcribe all written text present in the document. Straddling the boundary between CV and NLP, OCR models may either use document layout analysis directly or solve the problem in an independent fashion.\n    \\item Information extraction models that use the output of OCR or document layout analysis to comprehend and identify relationships between the information that is being conveyed in the document. Usually specialized to a particular domain and task, these models provide the structure necessary to make a document machine readable, providing utility in document understanding.\n\\end{itemize}\nIn the following sections, we expand upon these concepts that constitute an end-to-end document understanding solution.", "cites": [795, 303, 206, 115, 800, 798, 8385, 38, 7, 791, 793, 797, 792, 7298, 796, 8384, 2401, 794, 790, 799], "cite_extract_rate": 0.7692307692307693, "origin_cites_number": 26, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key deep learning techniques (e.g., OCR, layout analysis, and transformers for long sequences) by linking them to broader document understanding tasks, drawing on multiple cited works. While it provides an analytical structure for end-to-end systems, the critical analysis is limited to stating general shortcomings (e.g., struggles with long sequences) without deep evaluation of specific paper contributions. It abstracts some patterns, such as the integration of CV and NLP, but does not present a novel framework."}}
{"id": "db6bda7f-5ad5-4c06-9040-d3c4c7598936", "title": "Text Detection", "level": "subsection", "subsections": ["54eb0570-92bb-4bac-b84d-741d65bfa806", "c239ee98-1a29-46e9-b939-21eaed9cfb9d"], "parent_id": "ab1c72d1-5fe4-43bf-8325-08c660ce0dd5", "prefix_titles": [["title", "A Survey of Deep Learning Approaches for OCR and Document Understanding"], ["section", "Optical Character Recognition"], ["subsection", "Text Detection"]], "content": "\\label{sec:text-detection}\nText detection is the task of finding text present in a page or image.\nThe input, an image, is often represented by a three dimensional tensor, $C \\times H \\times W$, where $C$ is the number of channels (often three, for red, green and blue), $H$ is the height, and $W$ is the width of the image.\nText detection is a challenging problem because text comes in a variety of shapes and orientations and can often be distorted.\nWe explore two common ways researchers pose the text detection problem: as an object detection task and as a instance segmentation task. A text detection model must either learn to output coordinates of bounding boxes around text (object detection), or a mask, where pixels with text are marked and pixels without are not (instance segmentation).\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{OCR.png}\n    \\caption{Here, we show the general OCR process. A document can take the left path and go through an object detection model, which outputs bounding boxes, and a transcription model that transcribes the text in each of those bounding boxes. If the document takes the middle path, the object passes through a generic text instance segmentation model that colors pixels black if they contain text and a text transcription model that transcribes the regions of text the instance segmentation model identifies. If the document takes the right path, the model goes through a character-specific instance segmentation model, which outputs which character a pixel corresponds to. All paths produce the same structured output. The document comes from FUNSD~.}\n    \\label{fig:ocr-figure}\n\\end{figure}", "cites": [801], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of text detection in OCR, mentioning two common approaches (object detection and instance segmentation) and including a figure that references the FUNSD dataset. However, it lacks synthesis of broader ideas across papers, critical evaluation of methods or limitations, and abstraction to general principles, focusing instead on surface-level definitions and tasks."}}
{"id": "54eb0570-92bb-4bac-b84d-741d65bfa806", "title": "Text Detection as Object Detection", "level": "subsubsection", "subsections": [], "parent_id": "db6bda7f-5ad5-4c06-9040-d3c4c7598936", "prefix_titles": [["title", "A Survey of Deep Learning Approaches for OCR and Document Understanding"], ["section", "Optical Character Recognition"], ["subsection", "Text Detection"], ["subsubsection", "Text Detection as Object Detection"]], "content": "\\label{sec:text-detection-object-detection}\nTraditionally, text detection revolved around hand-crafting features to detect characters~.\nAdvances in deep learning, especially in object detection and semantic segmentation, have led to a change in how text detection is tackled.\nUsing these well-performing object detectors from the traditional computer vision literature, such as the Single-Shot MultiBox Detector (SSD) and Faster R-CNN models~, practitioners build efficient text detectors.\nOne of the first papers applying a regression-based detector for text is TextBoxes~. \nThey added long default boxes that have large aspect ratios to SSD, in order to adapt the object detector to text.\nSeveral papers built on this work to make regression-based models resilient to orientations, like the Deep Matching Prior Network (DMPNet) and the Rotation-Sensitive Regression Detector (RRD)~.\nOther papers have a similar approach to the problem, but develop their own proposal network that is tuned towards text rather than towards natural images.\nFor instance,  combine convolutional networks with recurrent networks using a vertical anchor mechanism in their Connectionist Text Proposal Network to improve accuracy for horizontal text.\nObject detection models are generally evaluated via an intersection over union (IoU) metric and an F1 score.\nThe metric computes how much of a candidate bounding box overlaps with the ground truth bounding box (the intersection) divided by the total space occupied by both the candidate and ground truth bounding boxes (the union).\nNext, an IoU threshold $\\tau$ is chosen to determine which predicted boxes count as true positives (IoU $\\geq \\tau$).\nThe remainder are classified as false positives.\nAny box that the model fails to detect is classified as a false negative.\nUsing those definitions, an F1 score is computed to evaluate the object detection model.", "cites": [807, 209, 804, 803, 802, 806, 805], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual overview of how object detection techniques have been applied to text detection, listing relevant papers and their modifications. It shows minimal synthesis by grouping methods based on orientation adaptation but does not deeply connect or contrast the approaches. There is little critical evaluation of the methods or identification of broader trends beyond surface-level observations."}}
{"id": "c239ee98-1a29-46e9-b939-21eaed9cfb9d", "title": "Text Detection as Instance Segmentation", "level": "subsubsection", "subsections": [], "parent_id": "db6bda7f-5ad5-4c06-9040-d3c4c7598936", "prefix_titles": [["title", "A Survey of Deep Learning Approaches for OCR and Document Understanding"], ["section", "Optical Character Recognition"], ["subsection", "Text Detection"], ["subsubsection", "Text Detection as Instance Segmentation"]], "content": "\\label{sec:text-detection-instance-segmentation}\nText detection in documents has its own unique set of challenges: notably, the text is usually dense and documents contain a lot more text than what is usually present in natural images. To combat this density problem, text detection can be posed as an ultra-dense instance segmentation task.\nInstance segmentation is the task of classifying each pixel of an image as specific, pre-defined categories.\nSegmentation-based text detectors work at the pixel level to identify regions of text.\nThese per-pixel predictions are often used to estimate probabilities of text regions, characters, and their relationships among adjacent characters in a unified framework.\nPractitioners use popular segmentation methods like Fully Convolutional Networks (FCN) to detect text~, improving upon object detection models, especially when text is misaligned or distorted.\nSeveral papers build on this segmentation foundation to output word bounding areas by extracting bounding areas directly from the segmentation output~.\nTextSnake extends this further by predicting the text region, center line, direction of text, and candidate radius from an FCN~.\nThese features are then combined with a striding algorithm to extract the central axis points to reconstruct the text instance.", "cites": [791, 808, 810, 809], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of text detection as instance segmentation, referencing relevant papers like TextSnake and PixelLink. It integrates basic ideas from these works to explain how segmentation-based approaches differ from object detection, but lacks deeper comparative or critical analysis. Some generalization is attempted (e.g., mentioning FCN's flexibility), but the section remains focused on specific methodologies without identifying broader trends or principles."}}
{"id": "db0f4447-7e9b-466c-9b8c-6262fb55d937", "title": "Word-level versus character-level", "level": "subsection", "subsections": [], "parent_id": "ab1c72d1-5fe4-43bf-8325-08c660ce0dd5", "prefix_titles": [["title", "A Survey of Deep Learning Approaches for OCR and Document Understanding"], ["section", "Optical Character Recognition"], ["subsection", "Word-level versus character-level"]], "content": "\\label{sec:text-transcription}\nWhile most papers cited above try to directly detect words or even lines of words, some papers argue that character-level detection is an easier problem than general text detection because characters are less ambiguous than text lines or words.\nCRAFT uses an FCN model to output a two-dimensional Gaussian heatmap for each character~.\nCharacters that are close together are then grouped together in a rotated rectangle that has the smallest area possible to still encapsulate the set of characters.\nMore recently,  combine global, word-level, and character-level features obtained using Region Proposal Networks (RPN) to great success.\nMost of the models described above were mainly developed for text scene detection, but can be easily adapted to document text detection to handle difficult cases like distorted text.\nWe expect less distortion in documents than in natural images, but poorly scanned documents or documents with certain fonts could still pose these problems.", "cites": [811], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section briefly introduces the distinction between word-level and character-level OCR approaches, citing one relevant paper (CRAFT) and mentioning another method involving RPNs. However, it primarily summarizes individual techniques without deeply comparing them or offering a broader conceptual framework. There is minimal critical evaluation or abstraction beyond the cited works."}}
{"id": "fbc2aaa2-b236-4423-af03-87a3371531ce", "title": "Text Transcription", "level": "subsection", "subsections": [], "parent_id": "ab1c72d1-5fe4-43bf-8325-08c660ce0dd5", "prefix_titles": [["title", "A Survey of Deep Learning Approaches for OCR and Document Understanding"], ["section", "Optical Character Recognition"], ["subsection", "Text Transcription"]], "content": "\\label{sec:text-transcription}\nText transcription is the task of transcribing the text present in an image.\nThe input, an image, is often a crop corresponding to either a character, word, or sequence of words, and has dimension $C \\times H' \\times W'$.\nA text transcription model must learn to ingest this cropped image and output a sequence of tokens belonging to some pre-specified vocabulary $V$.\n$V$ often corresponds to a set of characters. For digit recognition for instance, this is the most intuitive approach~.\nOtherwise, $V$ can also correspond to a set of words, similarly to a word-level language modeling problem~. \nIn both cases, the problem can be framed as a multi-class classification problem with the number of classes equal to the size of the vocabulary $V$.\nWord-level text transcription models require more data as the number of classes in the multi-class classification problem is much larger than for character-level. On one hand, predicting words instead of characters decreases the probability of making small typos (like replacing an \"a\" by an \"o\" in a word like \"elephant\"). On the other, limiting oneself to a word-level vocabulary means that it is not possible to transcribe words which are not part of this vocabulary. This problem doesn't exist at the character-level, as the number of characters is limited. As long as we know the language of the document, it is straightforward to build a vocabulary which contains all the possible characters. Subword units are a viable alternative~, as they alleviate the issues present in both word and character level transcription. \nRecently the research community has moved towards using recurrent neural networks, specifically recurrent models with LSTM or GRU units on top of a convolutional image feature extractor~.\nTo transcribe a token, two different decoding mechanisms are often used.\nOne is standard greedy decoding or beam search using an attention-based sequence decoder with cross entropy loss~, exactly like decoding with a conditional language model.\nSometimes images are poorly oriented or misaligned, reducing the effectiveness of standard sequence attention.\nTo overcome this,~ uses attention alignment, encoding spatial information of characters directly, while~ use spatial attention mechanisms directly.\nThe second way in which transcription decoding is often done is with connectionist temporal classification (CTC) loss~, a common loss function in speech which models repeated characters in sequence outputs well.\nThe majority of text transcription models borrow from advances in sequence modeling for both text and speech and often can utilize these advancements well with only minor adjustments.\nAs a result, practitioners seldom directly tackle this aspect relative to the other components of the document understanding task.", "cites": [243, 813, 815, 814, 812, 168, 8384], "cite_extract_rate": 0.7, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of text transcription in OCR by connecting key concepts across cited papers, such as vocabulary design, decoding mechanisms (CTC vs. attention-based), and spatial alignment approaches. It integrates these ideas into a coherent discussion of trade-offs and trends in the field. However, while it identifies some limitations (e.g., vocabulary constraints for word-level models), it does not offer a comprehensive comparative evaluation or deeply nuanced critique of the approaches."}}
{"id": "7012ceef-6a64-491f-99cb-bbe0d0a38dd7", "title": "End-to-end models", "level": "subsection", "subsections": [], "parent_id": "ab1c72d1-5fe4-43bf-8325-08c660ce0dd5", "prefix_titles": [["title", "A Survey of Deep Learning Approaches for OCR and Document Understanding"], ["section", "Optical Character Recognition"], ["subsection", "End-to-end models"]], "content": "\\label{sec:combined-models}\nEnd-to-end approaches combine text detection and text transcription in order to improve both components jointly~.\nFor instance, if the text prediction has a very low probability, it means the detected box either did not capture the entire word or captured something that is not text.\nAn end-to-end approach may be very effective in this case.\nCombining these two methods is fairly common and both Fast Oriented Text Spotting (FOTS) and TextSpotter with Explicit Alignment and Attention sequentially combine these models to train end-to-end~.\nThese approaches use shared convolutions as features to both text detection and recognition, and implement methods for complex orientations of text.\n introduce TextDragon, an end-to-end model that performs well on distorted text by utilizing a differentiable region of interest slide operator, which specializes in correcting distortions in regions of interest. \nMask TextSpotter is another end-to-end model that combines region proposal networks for bounding boxes with text and character segmentation~.\nThese recent works show the power of end-to-end OCR solutions in reducing errors.\nYet, having separate text detection and text recognition models offers more flexibility. First, the two models can be trained separately. In the case where only a small dataset is available to train the whole OCR module, but a lot of text recognition data is easily accessible, it makes sense to leverage this big amount of data in the training of the recognition model. Moreover, with two separate models, it is easy to compute two separate sets of metrics and have a more complete understanding of where the bottleneck might be.\nHence, both two-model and end-to-end approaches are viable. Whether one is better than the other mainly depends on the data available and what one wants to achieve.", "cites": [818, 817, 816, 815], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple end-to-end OCR models (FOTS, TextSpotter, TextDragon, Mask TextSpotter) and connects their shared strategies, such as using convolutional features and handling complex text orientations. It also critically discusses the trade-off between end-to-end and two-model approaches, noting the benefits of flexibility and data utilization when models are separate. While it provides useful comparisons and general insights, it stops short of offering a deeper theoretical framework or identifying overarching principles beyond the specific models."}}
{"id": "e9b51816-66b8-415f-93f7-12c02ebbd0d5", "title": "Datasets for Text Detection \\& Transcription", "level": "subsection", "subsections": [], "parent_id": "ab1c72d1-5fe4-43bf-8325-08c660ce0dd5", "prefix_titles": [["title", "A Survey of Deep Learning Approaches for OCR and Document Understanding"], ["section", "Optical Character Recognition"], ["subsection", "Datasets for Text Detection \\& Transcription"]], "content": "\\label{sec:text-detection-datasets}\nMost of the literature revolves around scene text detection, rather than document text detection, and report results on those datasets.\nSome of the major ones are ICDAR~, Total-Text~, CTW1500~, and SynthText~.\n present FUNSD, a dataset for text detection, transcription, and document understanding with 199 fully annotated forms comprising of 31k word level bounding boxes. Another recent document understanding dataset comes from the ICDAR 2019 Robust Reading Challenge on Scanned Receipts OCR and Information Extraction (SROIE). It contains 1000 whole scanned receipt images, with line-level annotations for text detection/transcription, and labels for Key Information Extraction. The website contains a ranking of the solutions proposed to address this problem. As solutions are still posted after the end of the competition, it is a good way to keep track of the most recent methods.", "cites": [820, 821, 819, 801], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic descriptive overview of key datasets for text detection and transcription, mentioning their characteristics and contributions. It integrates the cited papers by categorizing them as either scene text or document text datasets but does not offer a deeper synthesis of their methodologies or how they influence the field. There is minimal critical analysis or abstraction to broader trends or principles."}}
{"id": "872cb3d9-2ab2-43da-9f85-2dd63e5d76ea", "title": "Document Layout Analysis", "level": "section", "subsections": ["09eca77f-2430-4aa1-a5bd-c41cc3f4ea98", "0df51f92-2d49-4e68-81c9-e79dcaaa3c41", "1e0244b9-1bab-424d-a81b-a4a89f4a6821"], "parent_id": "7dee3631-5c10-4ff0-83ae-c48619c9e5c2", "prefix_titles": [["title", "A Survey of Deep Learning Approaches for OCR and Document Understanding"], ["section", "Document Layout Analysis"]], "content": "\\label{sec:layout-analysis}\nDocument layout analysis is the process of locating and categorizing regions of interest on a picture or scanned image of a page.\nBroadly, most approaches can be distilled into page segmentation and logical structural analysis~.\nPage segmentation methods focus on appearance and use visual cues to partition pages into distinct regions; the most common are text, figures, images, and tables. \nIn contrast, logical structural analysis focuses on providing finer-grained semantic classifications for these regions, i.e. identifying a region of text that is a paragraph and distinguishing that from a caption or document title.\nResearch in methods for document layout analysis has a long history, both in academia and industry.~\\footnote{The first ISO standard that defined aspects of modern-day document layout analysis was drafted four decades ago: ISO 8613-1:1989}\nFrom the first pioneering heuristic approaches~, to multi-stage classical machine learning systems~, the evolution of document layout analysis methods is now dominated by end-to-end differentiable methods~.", "cites": [823, 822, 8386], "cite_extract_rate": 0.25, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 2.0, "abstraction": 2.3}, "insight_level": "low", "analysis": "The section provides a basic overview of document layout analysis, distinguishing between page segmentation and logical structural analysis. While it references three papers, it does so in a largely descriptive manner without synthesizing their contributions into a broader narrative or framework. There is minimal critical evaluation or abstraction to overarching principles."}}
{"id": "09eca77f-2430-4aa1-a5bd-c41cc3f4ea98", "title": "Instance Segmentation for Layout Analysis", "level": "subsection", "subsections": [], "parent_id": "872cb3d9-2ab2-43da-9f85-2dd63e5d76ea", "prefix_titles": [["title", "A Survey of Deep Learning Approaches for OCR and Document Understanding"], ["section", "Document Layout Analysis"], ["subsection", "Instance Segmentation for Layout Analysis"]], "content": "\\label{sec:instance-segmentation}\nWhen applied to the problem of layout analysis in business documents, instance segmentation methods predict per-pixel labels to categorize regions of interest.\nSuch methods are flexible and easily adapt to the courser-grained task of page segmentation or the more-specific task of logical structural analysis.\n\\begin{figure}[t]\n    \\label{fig:layout-analysis}\n    \\centering\n    \\includegraphics[width=\\textwidth]{layout-analysis-fig.png}\n    \\caption{A document is passed through a generic layout analysis model, resulting in a layout segmentation mask with the following classes: figure (green), figure caption (orange), heading (purple), paragraph (red), and algorithm (blue). The document has been reproduced with permission~.}\n\\end{figure}\nIn , the authors describe an end-to-end neural network that combines both text and visual features in a encoder-decoder architecture that also incorporates an unsupervised pretraining network.\nDuring inference, their approach uses a downsampling cascade of pooling layers to encode visual information, which is fed into a symmetrical upsampling cascade for decoding. \nAt each cascade level, the produced encoding is also directly passed into the respective decoding block, concatenating the down- and up-sampled representations.\nThis architecture ensures that visual feature information at different levels of resolution is considered during the encoding and decoding process~.\nFor the final decoding layer, localized text embeddings are supplied alongside the computed visual representation.\nThis U-Net inspired encoding-decoding architecture has been adopted for document layout analysis in several different approaches~.\nThe method in , and later extended by  via additional text embeddings, use convolution maxpooling layers with large filter sizes to feed the document image through a ResNet bottleneck~.\nThe representation is then processed by bilinear upsampling layers and smaller 1x1 and 3x3 convolution layers.\nBoth works are used to perform layout analysis on historical documents and newspapers from multiple European languages, respectively.\nIn , the authors combine the U-Net architecture pattern with trainable multiplication layers.\nThis layer type is specialized for extracting co-occurrence texture features from the network's convolution feature maps, which are effective for locating regions that have periodically repeating information, such as tables.", "cites": [823, 8387, 97, 824, 825], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of instance segmentation in layout analysis, particularly highlighting the U-Net architecture and its adaptations. It synthesizes key ideas from multiple papers, showing how visual and textual features are integrated and how different design choices (e.g., multiplication layers) improve performance. However, it lacks deeper critical evaluation of the methods' limitations and broader comparative analysis across approaches."}}
{"id": "0df51f92-2d49-4e68-81c9-e79dcaaa3c41", "title": "Addressing Data Scarcity and Alternative Approaches", "level": "subsection", "subsections": [], "parent_id": "872cb3d9-2ab2-43da-9f85-2dd63e5d76ea", "prefix_titles": [["title", "A Survey of Deep Learning Approaches for OCR and Document Understanding"], ["section", "Document Layout Analysis"], ["subsection", "Addressing Data Scarcity and Alternative Approaches"]], "content": "Obtaining high quality training data for layout analysis is a labor intensive task that requires both mechanical precision and an understanding of the document's contents.\nAs a consequence of the difficulties in layout annotation of documents from brand new domains, several approaches exist to either leverage structure in unlabeled data or use well-defined rule sets to generate synthetic labeled documents to further improve generalizability and performance of document layout analysis systems.\nMasked language models such as BERT and RoBERTa have shown effective empirical performance on many downstream NLP tasks~.\nInspired by the pretraining strategy in BERT and RoBERTa,  define a Masked Visual-Language Model, which randomly masks input tokens and uses the model to predict the masked tokens. Unlike BERT, their method provides the 2-D positional embedding of the token during this masked prediction task, which enables the model to combine both semantic and spatial relationships between textual elements. Mentioned earlier in section~\\ref{sec:instance-segmentation},  introduce an auxiliary document image reconstruction task in their broader instance segmentation-based network. During training, this auxiliary module uses a separate upsampling decoder that, without the aid of skip connections, predicts the original pixel values from the encoded representation.\nWhile pretraining lets practitioners gain more value from their unlabeled documents, this technique alone is not always sufficient to effectively surmount data scarcity concerns. Relying on the intuition that many business and academic documents have repeated patterns in both content as well as page-level organization, several approaches have emerged to manufacture synthetic, labeled data in order to provide data suitable for a pretraining-like routine .\nIn , the authors propose a three-stage method for synthesizing new labeled documents.  First, they generate the document by randomly choosing the a document background from a set of nearly 200 known document backgrounds. Second, they use a grid based layout method to define both individual document element content and their respective sizes. Third, their process introduces corruptions, such as Gaussian blur and random image crops. This modular, rule-based synthetic document generation approach creates a heterogeneous dataset to make pretraining of layout analysis models more robust.\nAlternatively, instead of defining rules to generate a heterogeneous set of documents, several synthesizing procedures take cues from data augmentation methods.\n and  describe general-purpose toolkits that use an existing set of labeled documents to introduce deformations and perturbations in source images.\nImportantly, such changes to the training data are balanced so as to preserve the original semantic content while still exposing the model training to realistic errors that it must account for during inference on unseen data.", "cites": [7, 8388, 826, 8386, 828, 827], "cite_extract_rate": 0.75, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several methods for addressing data scarcity in document layout analysis by connecting pretraining techniques (e.g., BERT, LayoutLM) with synthetic data generation (e.g., DocEmul). It provides a coherent narrative by highlighting how these approaches differ and complement each other. However, it lacks deeper critical evaluation of their relative strengths and weaknesses, and while some general patterns are identified, the abstraction remains at a mid-level rather than reaching a meta-level conceptualization."}}
{"id": "1e0244b9-1bab-424d-a81b-a4a89f4a6821", "title": "Datasets for Layout Analysis", "level": "subsection", "subsections": [], "parent_id": "872cb3d9-2ab2-43da-9f85-2dd63e5d76ea", "prefix_titles": [["title", "A Survey of Deep Learning Approaches for OCR and Document Understanding"], ["section", "Document Layout Analysis"], ["subsection", "Datasets for Layout Analysis"]], "content": "Recently, there has been a deluge of datasets specifically targeting the document layout analysis problem. The International Conference on Document Analysis and Recognition (ICDAR) has produced several datasets from their various annual competitions; the most recent from 2017 and 2019 provide gold-standard data for document layout analysis and other document processing tasks .\nOn the larger side, DocBank is a a collection of half a million document pages with token-level annotations suitable for training and evaluating document layout analysis systems . The authors constructed this dataset using weak supervision , matching data from the LaTeX source of known PDFs to form annotations. Similarly,  created PubLayNet by automatically matching XML content representations for over one million PDFs on PubMed Centralâ„¢, consiting of approximately 360 thousand document images. While not full document layout,  have created PubTabNet from PubMed Central as well. Their data consists of 568 thousand table images alongside an HTML representations of content.", "cites": [829, 831, 830, 828], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual summary of datasets relevant to document layout analysis, mentioning key contributions from the cited papers. However, it lacks deeper synthesis of ideas, critical evaluation of methodologies, or abstraction to broader trends, as it merely lists the datasets and their characteristics without contextualizing their strengths, weaknesses, or implications for the field."}}
{"id": "aa2dae4e-3f50-46e8-8455-72949ee2d942", "title": "2D Positional Embeddings", "level": "subsection", "subsections": [], "parent_id": "5c3c58c6-8219-43ce-81f3-0bc1c7ebaf0b", "prefix_titles": [["title", "A Survey of Deep Learning Approaches for OCR and Document Understanding"], ["section", "Information Extraction"], ["subsection", "2D Positional Embeddings"]], "content": "Multiple sequence tagging approaches have been proposed which augment current named entity recognition (NER) methods by embedding attributes of 2D bounding boxes and merging them with text embeddings to create models which are simultaneously aware of both context and spatial positioning when extracting information.\n embeds the pair of $x,y$ coordinates that define a bounding box using two different embedding tables and pretrain a masked language model (LM).\nDuring pretraining, text is randomly masked but the 2D positional embeddings are retained.\nThis model can then be fine-tuned on a downstream task.\nAlternatively, the bounding box coordinates can also be embedded using $sin$ and $cos$ functions like positional encoding methods~.\nOther features can also be embedded such as the line or sequence number~.\nIn this scenario, the document is preprocessed to assign a line number to each individual token. Each token is then ordered from left to right and given a sequential position. Finally, both the line and sequential positions are embedded.\nWhile these strategies have seen success, relying solely on the line number or bounding box coordinates can be misleading when the document has been scanned on an uneven surface, leading to curved text.\nAdditionally, bounding box based embeddings still miss critical visual information such as typographical emphases (bold, italics) and images such as logos.\nTo overcome these, a crop of the image corresponding to the token of interest can be embedded using a Faster R-CNN model to create token image embeddings which are combined with the 2D positional embeddings~.", "cites": [8388, 38, 832], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the concept of 2D positional embeddings by integrating methods from multiple papers, showing how they extend NER approaches for document understanding. It offers a critical perspective by pointing out limitations such as handling curved text and missing typographical cues. The discussion abstracts beyond individual papers to present a broader framework of spatial modeling in document information extraction."}}
{"id": "9c50ea32-b469-4b93-a5cd-7b6e8b439660", "title": "Image Embeddings", "level": "subsection", "subsections": [], "parent_id": "5c3c58c6-8219-43ce-81f3-0bc1c7ebaf0b", "prefix_titles": [["title", "A Survey of Deep Learning Approaches for OCR and Document Understanding"], ["section", "Information Extraction"], ["subsection", "Image Embeddings"]], "content": "Information extraction for documents can also be framed as a computer vision challenge wherein the goal of the model is to semantically segment information or regress bounding boxes over the areas of interest.\nThis strategy helps preserve the 2D layout of the document and allows models to take advantage of 2D correlations.\nWhile it is theoretically possible to learn strictly from the document image, directly embedding textual information into the image simplifies the task for models to understand the 2D textual relationships.\nIn these cases, an encoding function is applied onto a proposed textual level (i.e. character, token, word) to create individual embedding vectors.\nThese vectors are  transposed into each pixel that comprises the bounding box corresponding to the embedded text, ultimately creating an image of $W \\times H \\times D$ where $W$ is the width, $H$ is the height, and $D$ is the embedding dimension.\nProposed variants are listed as following:\n\\begin{enumerate}\n    \\item \\textit{CharGrid} embeds characters with a one-hot encoding into the image~ \n    \\item \\textit{WordGrid} embeds individual words using word2vec or FastText~ \n    \\item \\textit{BERTgrid} finetunes BERT on task-specific documents and is used obtain contextual wordpiece vectors~\n    \\item \\textit{C+BERTgrid}, combines context-specific and character vectors~\n\\end{enumerate}\nWhen comparing the grid methods, C+BERTgrid has shown the best performance, likely due to its contextualized word vectors combined with a degree of resiliency to OCR errors. \n proposes an alternative approach to directly apply text embeddings to the image.\nA grid is projected on top of the image and a mapping function assigns each token to a unique cell in the grid.\nModels then learn to assign each cell in the grid to a class.\nThis method significantly reduces the dimensionality due to its grid system, while still retaining the majority of the 2D spatial relationships.", "cites": [834, 796, 835, 833], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the concept of image embeddings across multiple papers, presenting a unified view of how textual information is encoded into visual grids. It critically evaluates the performance of different variants, highlighting strengths such as contextualization and OCR resilience. The abstraction is strong, as it generalizes the idea of 2D document representation and discusses how spatial and textual information can be jointly modeled."}}
{"id": "a3d3f144-355d-445f-9178-9818964739ce", "title": "Documents as Graphs", "level": "subsection", "subsections": [], "parent_id": "5c3c58c6-8219-43ce-81f3-0bc1c7ebaf0b", "prefix_titles": [["title", "A Survey of Deep Learning Approaches for OCR and Document Understanding"], ["section", "Information Extraction"], ["subsection", "Documents as Graphs"]], "content": "Unstructured text on documents can also be represented as graph networks, where the nodes in a graph represent different textual segments.\nTwo nodes are connected with an edge if they are cardinally adjacent to each other, allowing the relationship between words to be modeled directly~.\nAn encoder such as a BiLSTM encodes text segments into nodes~.\nEdges can be represented as a binary adjacency matrix or a richer matrix, encoding additional visual information such as the distance between segments or shape of the source and target nodes~.\nA graph convolutional network is then applied at different receptive fields in a similar fashion to dilated convolutions~ to ensure that both local and global information can be learned~.\nAfter this, the representation is passed to a sequence tagging decoder.\nDocuments can also be represented as a directed graph and a spatial dependency parser~. \nIn this representation, nodes are represented by textual segments, but field nodes denoting the node type are used to initialize each DAG.\nIn addition, two kinds of edges are defined:\n\\begin{enumerate}\n    \\item Edges that group together segments belonging to the same category (STORENAME $\\rightarrow$ Peet's $\\rightarrow$ Coffee; a field node followed by two nodes representing a store name)\n    \\item Edges that connect relationships between different groups (Peet's $\\rightarrow$ 94107; a zipcode).\n\\end{enumerate}\nA transformer with an additional 2D positional embedding is used to spatially encode the text.\nAfter this, the task becomes to predict the relationship matrix for each edge type.\nThis method can represent arbitrarily deep hierarchies and can be applied towards complicated document layouts.", "cites": [37, 8387, 832], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key concepts from multiple papers to present a coherent narrative on representing documents as graphs. It integrates ideas from spatial dependency parsing, multi-scale context aggregation, and directed acyclic graphs, offering a structured explanation. While it identifies the limitations of traditional sequence tagging, it could provide deeper critical evaluation of the methods and their trade-offs."}}
{"id": "8926c720-3179-44ea-97bb-88e166093c96", "title": "Tables", "level": "subsection", "subsections": [], "parent_id": "5c3c58c6-8219-43ce-81f3-0bc1c7ebaf0b", "prefix_titles": [["title", "A Survey of Deep Learning Approaches for OCR and Document Understanding"], ["section", "Information Extraction"], ["subsection", "Tables"]], "content": "\\label{sec:table-extraction}\nTabular data extraction remains a challenging aspect of information extraction due to their wide variety of formats and complex hierarchies. Table datasets typically have multiple tasks to perform~. The first task is table detection which involves localizing the bounding box containing the table(s) inside the document. The next task is table structure recognition, which requires extracting the row, column, and cell information into a common format. This can be taken one step further to table recognition, which requires understanding both the structural information as well as the content by classifying cells within the table itself~. As textual and visual features are equally important to properly extracting and understanding tables, many diverse methods have been proposed to perform this task.\nOne such proposal named TableSense performs both table detection and structure recognition~. \nTableSense uses a three stage approach: cell featurization, object detection with convolutional models, and an uncertainty-based active learning sampling mechanism. TableSense's proposed architecture for table detection performs significantly better than traditional methods in computer vision such as YOLO-v3 or Mask R-CNN~.\nSince this approach does not work well for general spreadsheets, ~ extend upon the previous work by using a multitask framework to jointly learn table regions, structural components of spreadsheets, and cell types.\nThey add an additional stage, which leverages language models to learn the semantic contents of table cells in order to flatten complex tables into a single standard format.\n propose TUTA, which focuses on understanding the content within tables after the structure has been determined. The authors present three new objectives for language model pretraining for table understanding by using tree-based transformers. The objectives introduced for pretraining are designed to help the model understand tables at the token, cell, and table level. The authors mask a proportion of tokens depending on the table cell for the model to predict, randomly mask particular cell headers for the model to predict the header string based on its location, and provide the table with context such as table titles or descriptions that may or may not be associated for the model to identify which contextual elements are positively associated with the table. The transformer architecture is modified to reduce distractions from attention by limiting the attention connections to items based on a cell's  hierarchical distance to another cell. Fine-tuning TUTA has demonstrated state of the art performance on multiple datasets for cell type classification.", "cites": [837, 830, 520, 836], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a structured overview of table extraction tasks and synthesizes key approaches from multiple papers, integrating them into a narrative about the progression from detection to content understanding. It introduces TUTA and explains its novel pretraining objectives, showing some abstraction of concepts. However, the critical evaluation is limited, as the section does not deeply critique the approaches or compare their trade-offs in detail."}}
