{"id": "7c97d96d-d866-49bc-a866-2dc3e0cf6313", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "c226fd34-5a4b-426b-a6d8-50438f783d8d", "prefix_titles": [["title", "Deep Learning for Micro-expression Recognition:  A Survey"], ["section", "Introduction"]], "content": "\\label{sec:introduction}}\n\\IEEEPARstart{F}{acial} \nexpression (FE) is one of the most powerful and universal means for human communication, which is highly associated with human mental states, attitudes, and intentions.\n Besides ordinary FEs (also known as macro-expressions) that we see daily, emotions can also be expressed in a special format of Micro-expressions (MEs) under certain conditions. MEs are FEs revealing people’s  hidden  feelings in high-stake situations when people try to conceal their true feelings . Different from macro-expressions, MEs are spontaneous, subtle, and rapid (1/25 to 1/3 second) facial movements reacting  to  emotional  stimulus . \nThe ME phenomenon was firstly discovered by Haggard and Isaacs  in 1966. Three years later, Ekman and Friesen also declared the finding of MEs  during examining psychiatric patient's videos for lie detection.\nIn the following years, Ekman~\\etal~ continued ME research and developed the Facial Action Coding System (FACS)  and Micro Expression Training Tool (METT) . Specifically,  FACS breaks down FEs into individual components of muscle movement, called Action Units (AUs) .\nAU analysis can effectively resolve the ambiguity issue to represent individual expression and increase Facial Expression Recognition (FER) performance . \nFig.~\\ref{fig:MicroMacro} shows the example of micro- and macro-expressions as well as activated AUs in each FE. On the other hand, METT is helpful for increasing people's emotional awareness. \\textcolor{black}{It can promote manual ME detection performance which provides a potential chance  to build reliable ME datasets.}  \n\\begin{figure}\n  \\centering\n\\includegraphics[width=\\columnwidth]{image/macro-micro.jpg}\n\\vspace{-4ex}\n\\caption{Examples of micro-expressions in CASME II  and macro-expressions in MMI , as well as the active AUs. The red arrow represents the muscle movement direction. AU4, AU6, AU7, AU9, AU12, AU15, and AU25 represent brow lowerer, cheek raise, lids tight, nose wrinkle, lip corner puller,  lip corner depressor, and lips part, respectively. }\n\\vspace{-3ex}\n\\label{fig:MicroMacro}\n\\end{figure}\n\\textcolor{black}{\nMER is the task of classifying ME clips into various emotion categories. In each ME clip, the frame starting facial movements is denoted as the onset frame, while the end frame is the offset frame. The frame with the largest intensity is the apex frame. Like FER, MER also classifies facial images/sequences into categories such as anger, surprise, and happiness. However, MER is more challenging as spontaneous MEs are involuntary, subtle, and fleeting. In addition, MEs can also be impacted by emotional context and cultural background . Therefore, it is difficult to collect and annotate ME data, leading to small-scale ME datasets and existing methods are incapable of dealing with subtleness and fleetness. }\n MER has drawn increasing interest recently due to its practical importance in  many human-computer interaction systems. The first spontaneous MER research can be traced to Pfister~\\etal's work~ which  utilized a Local Binary Pattern from Three Orthogonal Planes (LBP-TOP)~ on the first public spontaneous ME dataset: SMIC .\nFollowing the work of~, various approaches based on appearance and geometry features ~  were proposed for improving the performance of MER. \nIn recent years, with the advance of Deep Learning (DL) and its successful extensions on object detection , human tracking , image retrieval   and FER , researchers have started to exploit MER with DL. Although MER with DL becomes challenging because of the limited ME samples and low intensity, great progress on MER has been made through  designing effective shallow networks, exploring Generative Adversarial Net (GAN)~ and so on. Currently, DL-based MER has achieved the state-of-the-art performance. \n{\\textcolor{black}{In this survey, we review the research on MER by DL since 2016 when the DL technology was firstly adopted in MER. Due to the page limitation, the representative works published in well-known journals and conferences, such as IEEE TPAMI, IEEE TAC, IEEE TIP, and ACM MM are specifically discussed. The ordinary FER approaches and MER with traditional learning methods are not considered in this survey.} Although a few MER surveys have discussed the historical evolution and algorithmic pipelines for MER , they mainly focus on traditional methods and only introduce some recent DL approaches. The DL-based MER has not been discussed systematically and specifically. As far as we know, this is the first survey of the DL-based MER. Different from previous surveys, we analyze the strengths and shortcomings of dynamic network inputs which are important for MER based on DL. Furthermore, the network blocks, architectures, training strategies, and losses are discussed and summarized in detail and future research directions are identified. The goal of this survey is to provide a DL-based MER dictionary that can serve as a reference point for future MER research.\n\\begin{figure}\n  \\centering\n\\includegraphics[width=8.7cm]{image/micro_datasets.jpg}\n\\vspace{-1ex}\n\\caption{Examples of ME samples in ME datasets for MER.}\n\\vspace{-4ex}\n\\label{fig:microdatasets}\n\\end{figure}\n\\textcolor{black}{This paper is organized as follows: Section \\ref{sec:Datasets} introduces spontaneous ME datasets. Section \\ref{sec:taxonomy} presents the taxonomy we defined for MER based on DL. Section \\ref{sec:inputsall} discusses the various inputs for deep MER.  Section \\ref{sec:network} provides a detailed review of neural networks for MER.} \nThe evaluation matrix, protocol, and the performance of representative DL-based MER are described \nin Section \\ref{sec:experiments}.\nSection \\ref{sec:Challenge} summarizes current challenges and potential study directions.\nFinally,  Section 8 discusses the ethical considerations.\n\\begin{table*}[t]\n\\renewcommand{\\arraystretch}{1.3}\n\\centering\n\\small\n\\caption{Spontaneous Datasets for MER}\n\\label{dataset}\n\\scalebox{0.72}{ \n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}  \\bottomrule[1.5pt]\n Database    &Resolution&Facial size& Frame rate& Samples & subjects & Expression &AU&Apex&Eth&Env \\\\\\bottomrule[1.5pt] \n\\makecell[c]{SMIC \\\\ HS/NIR/VIS } & $640\\times480$ & $190\\times230$& 100/25/25& 164/71/71 &16/8/8& \\makecell[c]{ \\textit{Pos} (51) \\textit{Neg} (70) \\textit{Sur} (43) / \\textit{Pos} (28) \\textit{Neg} (23) \\textit{Sur} (20) /   \\\\  \\textit{Pos} (28) \\textit{Neg} (24) \\textit{Sur} (19)} &$\\circ$&$\\circ$&3&\\textit{L}\\\\ \n\\hline\nCASME   & \\makecell[c]{$640\\times480$\\\\ $1280\\times720$}   & $150\\times90$ &60&195&35&\\makecell[c]{ \\textit{Hap} (5) \\textit{Dis} (88) \\textit{Sad} (6) \\textit{Con} (3) \\textit{Fea} (2) \\\\ \\textit{Ten} (28) \\textit{Sur} (20) \\textit{Rep} (40)  }&$\\bullet$&$\\bullet$&1&\\textit{L}\\\\\\hline  \nCASME II & $640\\times480$ &$250\\times340$&200&247&35&\\ \\textit{Hap} (33) \\textit{Sur} (25) \\textit{Dis} (60)  \\textit{Rep} (27)  \\textit{Oth} (102)&$\\bullet$&$\\bullet$&1&\\textit{L} \\\\\\hline\nCAS(ME)$^2$ & $640\\times480$& -&30& \\makecell[c]{Macro 300  \\\\ Micro 57 }  &22&\\textit{Hap} (51) \\textit{Neg} (70) \\textit{Sur} (43) \\textit{Oth} (19)&$\\bullet$&$\\bullet$&1&\\textit{L}   \\\\\\hline \nSAMM~&$2040\\times1088$ & $400\\times400$ &200 &159&32&\\textit{Hap} (24) \\textit{Ang} (20) \\textit{Sur} (13) \\textit{Dis} (8) \\textit{Fea} (7) \\textit{Sad} (3) \\textit{Oth} (84)&$\\bullet$&$\\bullet$&13&\\textit{L}  \\\\\\hline\nMEVIEW & $720\\times1280$ &- &25&31&16 & \\textit{Hap} (6) \\textit{Ang} (2) \\textit{Sur} (9) \\textit{Dis} (1) \\textit{Fea} (3) \\textit{Unc} (13) Con(6)  &$\\bullet$&$\\circ$&-&\\textit{W}\\\\\\hline \nMMEW~& $1920\\times1080$&$400\\times400$&90&300&36 & \\textit{Hap} (36) \\textit{Ang} (8) \\textit{Sur} (80) \\textit{Dis} (72) \\textit{Fea} (16) \\textit{Sad} (13) \\textit{Oth} (102)  &$\\bullet$ &$\\bullet$&1&\\textit{L}\\\\\\hline \nComposite ME~& \\makecell[c]{$640\\times480$\\\\ $1280\\times720$ \\\\ $720\\times1280$} &\\makecell[c]{$150\\times90$\\\\$250\\times340$ \\\\ $400\\times400$}  &200&442&68&\\textit{Pos} (109), \\textit{Neg} (250), and \\textit{Sur} (83)&$\\circ \\bullet$&$\\circ \\bullet$&13&\\textit{L} \\\\\\hline\nCompound ME~&\\makecell[c]{$640\\times480$\\\\ $1280\\times720$ \\\\ $720\\times1280$} &$150\\times90$  &200&1050&90& \\makecell[c]{ \\textit{Neg} (233) \\textit{Pos} (82) \\textit{Sur} (70) \\\\ \\textit{PS} (74) \\textit{N S} (236) \\textit{PN} (197) \\textit{NN} (158)}&$\\circ \\bullet$&$\\circ \\bullet$&13&\\textit{L} \\\\ \\bottomrule[1.5pt]\n\\end{tabular}}\n{\\raggedright $^1$   Eth: Ethnicity;  Env : Environment. \\par\n$^2$ \\textit{Pos}: Positive; \\textit{Neg}: Negative; \\textit{Sur}: Surprise; \\textit{Hap}: Happiness; \\textit{Dis}: Disgust;  \\textit{Rep}: Repression;  \n\\textit{Ang}: Anger;  \\textit{Fea}: Fear; \\textit{Sad}: Sadness; \\textit{Con}: Contempt;\n\\textit{Unc}: Unclear; \n\\textit{Oth}: Others; \n\\textit{PS}: Positively surprise; \\textit{NS} Negatively surprise;  \\textit{PN}: Positively negative;  \\textit{NN}: Negatively negative;\n\\textit{L}:Laboratory; \\textit{W}:In the wild. \\par\n$^3$ $\\circ$ represents unlabeled; $\\bullet$  represents  labeled and - represents unknown  \\par\n }\n\\end{table*}", "cites": [1534, 8944, 5740, 5739, 5743, 5738, 5742, 5741], "cite_extract_rate": 0.21052631578947367, "origin_cites_number": 38, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes several prior surveys on micro-expression recognition and highlights how this survey differs by focusing specifically on deep learning methods. It shows a basic level of critical analysis by pointing out limitations of earlier surveys and the need for a more systematic review of DL-based MER. The section also begins to abstract key challenges (e.g., small-scale datasets, subtlety of MEs) but does not fully develop overarching principles or deep comparative insights."}}
{"id": "cdf6ce5b-82da-43d6-9b3f-a338b3d2bc7a", "title": "Datasets", "level": "section", "subsections": [], "parent_id": "c226fd34-5a4b-426b-a6d8-50438f783d8d", "prefix_titles": [["title", "Deep Learning for Micro-expression Recognition:  A Survey"], ["section", "Datasets"]], "content": "\\label{sec:Datasets}\nDifferent from macro-expressions which can be easily captured in our daily life, \nMEs are involuntary brief FEs, particularly occurring under high stake situations. Four early databases appeared continuously around 2010: Canal9~, York-DDT , Polikvsky’s database  and USF-HD . However, Canal9 and York-DDT are not aimed for ME research. Polikvsky’s database  and USF-HD include only posed MEs which are collected by asking participants to intentionally pose or\nmimic a micro movement. The posed expressions contradict with the spontaneous nature of MEs.  Currently, these databases are not used anymore for MER.  In the recent years, several spontaneous\nME databases were created, including: SMIC~ and its extended version SMIC-E, CASME , CASME II , CAS(ME)$^2$ , SAMM~, and micro-and-macro expression\nwarehouse (MMEW)~. In this survey, we focus on the spontaneous datasets. \n\\textcolor{black}{\nIn a general ME dataset collection procedure, participants are asked to keep a poker face while watching video clips to induce spontaneous MEs. The video clips are selected according to previous psychological studies, which can elicit strong emotions. Commonly, a high-speed camera is utilized to record facial videos. After one participant watched a video clip, he/she fills in a self-report questionnaire to report his/her true feelings about the video clip. As well, considering cultural backgrounds may have an impact on MEs , participants from different ethnicities could be recruited  for the potential study of cultural impact on MEs.}\n\\textcolor{black}{\nSince the MEs are subtle and rapid, annotators are usually trained with FACS and certified facial action unit coders are employed to detect the MEs in the facial videos. The FACS helps people look precisely at the facial movements to make ME detection reliable. Specifically, when the duration of the facial action unit is less than 0.5s, the clip is regarded as a ME clip. The MEs are annotated into discrete categories. In SMIC , the emotions are labeled as ‘positive’, ‘negative’, and ‘surprise’ according to the participants’ self-reports. However, mixed emotions may be induced while the participants watch one video clip. Annotations based on the general emotion reported after watching the video, which usually allows one emotion, are not accurate. To this end, several datasets, such as CASME  and CASME II , consider AUs, self-reports, and the watched video clips to label the MEs. When there are ambiguities and conflicts in the emotion annotation, the emotion is annotated as ‘others’. Furthermore, to alleviate annotation bias caused by an individual annotator, the ME annotations are always carried out through cross-validation by multiple annotators.  }\nThe specific details of datasets are introduced as followings:\n\\textbf{SMIC}   is consisted of three subsets: SMIC-HS, SMIC-VIS and SMIC-NIR.  SMIC-VIS and SMIC-NIR contain 71 samples recorded by normal speed cameras with 25 fps of visual (VIS) and near-inferred  light range (NIR), respectively. \n\\textbf{CASME}  contains spontaneous 159 ME clips from 19 subjects including frames from onset to offset.   The emotions were labeled partly based on AUs and also taking account of  participants’ self-reports and the content of the video episodes. Besides the onset and offset, the apex frames are also labeled. The shortcoming of CASME is the imbalanced sample distribution among classes. \n\\textbf{CASME II}   is an improved version of the CASME dataset. Samples in CASME II are increased to 247 MEs from 26  subjects and they are recorded by high-speed camera at 200 fps with face sizes cropped to $280 \\times 340$. Thus, it has a greater temporal and spatial resolution, compared with CASME. \n\\textbf{CAS(ME)$^2$}  consists of spontaneous macro- and micro-expressions elicited from 22 subjects. CAS(ME)$^2$ has samples with longer durations which makes it suitable for ME spotting.  Compared to the above datasets, the samples in CAS(ME)$^2$ were recorded with a relatively low frame rate in a relatively small number of ME samples, which makes it unsuitable for DL approaches.\n\\textbf{SAMM}  collects 159 ME samples from 32 participants.  The samples were collected by a gray-scale camera at 200 fps in controlled lighting conditions to prevent flickering. Unlike previous datasets that lack ethnic diversity, the participants are from 13 different ethnicities. \n\\textbf{MEVIEW}  is  in-the-wild ME dataset. The samples in MEVIEW are collected from poker games and TV interviews on the Internet.\n  In total, 31 videos from 16 individuals were annotated in the dataset and the average length of videos is three seconds.   \n\\textbf{MMEW}~ contains 300 ME and 900 macro-expression samples acted out by the same participants with a larger resolution ($1920\\times 1080$ pixels). MEs and macro-expressions in MMEW were annotated to the same  emotion classes. \n\\textbf{The composite dataset}~ is proposed by the 2nd Micro-Expression Grand Challenge (MEGC2019). The composite dataset merges samples from three spontaneous facial ME datasets: CASME II , SAMM , and  SMIC-HS .  This is to facilitate the evaluation of newly developed methods. As the annotations in the three datasets vary hugely, the composite dataset unifies emotion labels in all three datasets. The emotion labels are re-annotated as \\textit{positive}, \\textit{negative}, and \\textit{surprise}. \n\\textbf{The compound micro-expression dataset (CMED)}~ is constructed by combining MEs from the CASME, CASME II, CAS(ME)$^2$, SMIC-HS, and SAMM datasets. Specifically, the MEs are divided into basic and compound emotional categories, as shown in Table \\ref{dataset}. Psychological studies demonstrate that there are usually complex expressions in daily life. Multiple emotions co-exist in one FE, termed as ``compound expressions\" .   Compound expression analysis reflects more complex mental states and more abundant human facial emotions. \nThe specific comparisons of the ME datasets are shown in Table \\ref{dataset}  and example samples are\nshown in Figure \\ref{fig:microdatasets}. Although MEVIEW collects MEs in the wild, the number of ME samples is too small to learn robust ME features.  The state-of-the-art approaches are commonly tested on the SMIC-HS, CASME , CASME II , and SAMM databases. As some emotions are difficult to trigger, such as fear and contempt, these categories have only a few samples and are not enough for learning. In most practical experiments, only the emotion categories with more than 10 samples are considered. Recently, the composite dataset  is popular, because it can verify the generalization ability of the method on datasets with different natures. For further increasing the MER performance, MMEW collected micro- and macro-expressions from the same subjects which\nmay be helpful for further cross-modal research.\n\\begin{figure*}\n     \\centering\n     \\begin{subfigure}[b]{0.78\\textwidth}\n         \\begin{tikzpicture}[scale=0.64]\n\\tikzset{grow'=right,level distance=88pt}\n\\tikzset{execute at begin node=\\strut}\n\\tikzset{every tree node/.style={anchor=base west}}\n\\tikzset{edge from parent/.style=\n{draw,\nedge from parent path={(\\tikzparentnode.east)\n--+(+8pt,+0pt)|-\n(\\tikzchildnode)}}}\n\\Tree [.{\\makecell[c]{MER\\\\with DL}} [.Input  [.Pre-processing [.Face~detection:~\\textit{Adaboost~~CNN-based~} ] [.Face~registration:~\\textit{ASM~~AMM~CNN-based~}  ] [.Temporal~normalization:~\\textit{TIM~~CNN-based}  ]  [.Motion~magnification:~\\textit{EVM~~GLMM~{CNN-based}~} ] [.Regions~of~interest:~\\textit{Grid~~FACS~EyeMouth~~landmarks~~Learning-based~ } ]  [.Data~augmentation:~\\textit{Multi-ratio~~Temporal~{GAN}~} ]]\n[.Modality [.Static [.Apex~spotting:{\\makecell[l]{~\\textit{Optical~flow~~Feature~contrast~~Frequency~}\\\\~\\textit{CNN-based~}}} ]\n[.Apex~based~recognition:~\\textit{} ] ] [.Dynamic [.Sequence:~\\textit{} ] [.Frame~aggregation:~\\textit{Onset-Apex~~Snippets~~Selected~frames~}  ] [.Image~with~dynamic~information:~\\textit{Dynamic~image~~Active~image~} ] [.Optical~flow:{\\makecell[l]{~\\textit{~Lucas-Kanade~~Farnebäck~~TV-L1}~\\textit{FlowNet~}}} ] ] [.Combination:{\\makecell[l]{~\\textit{{Optical~flow+Apex}~~{Optical~flow+Sequence}~ {Optical~flow+Key~frames}   }\\\\~\\textit{{Optical~flow+Landmarks}}}} ] ] ]\n[.Network\n [.Block:{\\makecell[l]{~\\textit{~RES~Inception~~HyFeat ~Capsule~RCN ~{Attention}}~\\textit{{Graph}}\\\\~\\textit{{Transformer}} }} ]\n[.Architecture [.Single~stream:~\\textit{~2D~~3D~} ]\n[.Multiple~stream: [.Same~block:~\\textit{~Dual~~Triple~~Four~} ]  [.Different~blocks:~\\textit{~Dual~~Triplet~ } ]  [.Handcraft+CNN:~\\textit{~Dual~} ]   ]\n[.Cascade:~\\textit{CNN+RNN~~CNN+LSTM~~\\textcolor{black}{CNN+GCN} } ]\n[.Multi-task~learning:~\\textit{landmark~Gender~{AU}~Multi-binary-class} ]\n]\n[.Training~strategy:~{~\\textit{Finetune~Knowledge~distillation}~\\textit{Domain~adaption}} ]\n[.Loss:~\\textit{Cross-entropy~Metric~Margin~{Imbalance} }  ]\n] ]\n\\end{tikzpicture}\n         \\label{fig:demonstration}\n     \\end{subfigure}\n     \\hfill\n     \\begin{subfigure}[b]{0.16\\textwidth}\n \\includegraphics[width=\\textwidth]{image/surveyTonxmy.pdf}\n         \\label{fig:real}\n     \\end{subfigure}\n     \\hfill\n        \\caption{Taxonomy for MER based on deep learning. \\textcolor{black}{The studies cited on the branches are example approaches discussed in this paper. The future directions and corresponding approaches are shown on the right side. The future directions are annotated in brackets.}}\n        \\label{fig:Taxonomy}        \n\\end{figure*}", "cites": [5740, 5749, 681, 5745, 3797, 8946, 309, 5746, 7990, 7992, 5744, 7991, 3769, 5747, 1219, 1244, 1225, 1217, 5748, 8945], "cite_extract_rate": 0.20408163265306123, "origin_cites_number": 98, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a well-structured synthesis of ME datasets and their collection/annotation procedures, integrating insights from multiple papers to highlight key characteristics such as spontaneity, labeling methods, and modality differences. It includes critical analysis by identifying limitations such as dataset imbalance, small sample sizes, and the impact of posed vs. spontaneous expressions. While it offers some abstraction by grouping datasets into categories and discussing trends like cross-modal research, it does not fully generalize to a meta-level understanding of dataset challenges in MER."}}
{"id": "c65d6d2d-2870-43b8-b784-6bd064d6dee2", "title": "Face detection and registration", "level": "subsubsection", "subsections": [], "parent_id": "664b50e4-718f-464c-91cc-30ee0dcc18d7", "prefix_titles": [["title", "Deep Learning for Micro-expression Recognition:  A Survey"], ["section", "Inputs"], ["subsection", "Pre-processing"], ["subsubsection", "Face detection and registration"]], "content": "For processing MEs,  face detection which removes the background and gets the facial region is the first step.  One of the most widely used algorithms for face detection is Viola-Jones  based on a cascade of weak classifiers. However, this method can not deal with large pose variations and occlusions.  Matsugu~\\etal~  firstly adopted CNN network for face detection with a rule-based algorithm,  which is robust to translation, scale, and pose.  Recently, face detectors based on DL have been utilized in popular open source libraries, such as dlib and OpenCV. \nSince spontaneous MEs involve muscle movements of low intensity, even little pose variations and movements may heavily affect MER performance. \nTo this end, face registration is crucial for MER. It aligns the detected faces onto a reference face to handle varying head-pose issues for successful MER.\nCurrently,  one of the most used facial registration methods is the Active Shape Models (ASM)   encoding both geometry and intensity information.  Then, the Active Appearance Models (AAM)  is presented for matching any face with any expression rapidly. \nWith the fast development of DL, deep networks with cascaded regression  have become the state-of-the-art methods for face alignment due to their excellent performances.", "cites": [1225], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic description of face detection and registration methods, integrating a few key approaches and mentioning their relevance to MER. However, it lacks deeper synthesis of ideas, critical evaluation of methods, or abstraction to broader trends. The narrative is mostly factual and linear, with minimal comparative or analytical depth."}}
{"id": "6986f862-31af-4d1a-a129-73724bb91908", "title": "Motion magnification", "level": "subsubsection", "subsections": [], "parent_id": "664b50e4-718f-464c-91cc-30ee0dcc18d7", "prefix_titles": [["title", "Deep Learning for Micro-expression Recognition:  A Survey"], ["section", "Inputs"], ["subsection", "Pre-processing"], ["subsubsection", "Motion magnification"]], "content": "One challenge for MER is that the facial movements of MEs are too subtle to be distinguished. Therefore, motion magnification is important to enhance the ME intensity level. One of the commonly used  methods is  the Eulerian Video Magnification method (EVM)  . For MEs, the EVM is applied for facial motion magnification . EVM magnifies either motion or color content across two consecutive frames in videos. However, a larger motion amplification level leads to a larger scale of motion amplification, which causes bigger artifacts. Different from EVM considering local magnification, Global Lagrangian Motion Magnification (GLMM)  was proposed for consistently tracking and exaggerating the FEs and global displacements across a whole video. Furthermore, the learning-based motion magnification  was firstly used in ME magnification by Lei~\\etal~  through extracting shape representations from the intermediate layers of networks. Compared with the traditional methods, the shape representations from the intermediate layers introduce less noise.", "cites": [5745], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple methods, including Eulerian Video Magnification (EVM), Global Lagrangian Motion Magnification (GLMM), and learning-based approaches, connecting their roles in enhancing micro-expressions. It provides a basic comparison of local versus global motion magnification and introduces the learning-based method as an improvement over traditional ones. However, it lacks deeper critical evaluation of the limitations or trade-offs of each method and does not rise to the level of meta-level abstraction or a novel conceptual framework."}}
{"id": "cd301e4d-6c70-4e20-ba1a-fda7f7060d8b", "title": "Temporal Normalization (TN)", "level": "subsubsection", "subsections": [], "parent_id": "664b50e4-718f-464c-91cc-30ee0dcc18d7", "prefix_titles": [["title", "Deep Learning for Micro-expression Recognition:  A Survey"], ["section", "Inputs"], ["subsection", "Pre-processing"], ["subsubsection", "Temporal Normalization (TN)"]], "content": "Besides the low intensity,  the short and varied duration also increases the difficulty for robust MER. This problem is especially serious when the videos are filmed with relatively low frame rate. To solve this issue, the Temporal Interpolation Model (TIM) was introduced to interpolate all ME sequences into the same specified length based on path graph between the frames. There are three strengths of applying TIM: 1) up-sampling ME clips with too few frames; 2)  more stable features can be expected with a unified clip length; 3) extending ME clips to long sequences and sub-sampling to short clips for data augmentation. Additionally, CNN-based temporal interpolation   have been proposed to solve complex scenarios in reality. \n\\begin{figure}\n  \\centering\n\\includegraphics[width=\\columnwidth]{image/roi.jpg}\n\\caption{Examples of regions of interest. (a) Equal block; (b) FACS-based RoIs ; (c)  RoIs Masked eye and cheek ; (d)  Eye and mouth ; (e) Difference-based ME datasets~; (f) landmark-based local regions~.}\n\\label{fig:roi}\n\\end{figure}", "cites": [8946, 8945, 5744], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section introduces Temporal Normalization and briefly mentions TIM and CNN-based methods, but it lacks synthesis of the cited papers. It does not connect or compare these methods in a coherent framework. There is minimal critical analysis or abstraction to broader patterns in the field."}}
{"id": "2ca4d598-094a-4634-a09e-097bae0256d6", "title": "Regions of interest (RoIs)", "level": "subsubsection", "subsections": [], "parent_id": "664b50e4-718f-464c-91cc-30ee0dcc18d7", "prefix_titles": [["title", "Deep Learning for Micro-expression Recognition:  A Survey"], ["section", "Inputs"], ["subsection", "Pre-processing"], ["subsubsection", "Regions of interest (RoIs)"]], "content": "FEs are formulated by basic facial movements , which correspond to specific facial muscles and relate to different facial regions. In other words, not all facial regions contribute equally to FER. Especially for MEs, the MEs only trigger specific small regions, as MEs involve subtle facial movements.  Moreover, the empirical experience and quantitative analysis in~ found that the outliers such as eyeglass have a seriously negative impact on the performance of MER. Therefore, it is important to suppress the influence of outliers.\nSome studies alleviate the influence of regions without useful information by extracting features on the RoIs~. Several MER approaches  divided the entire face into several equal blocks for better describing local changes (see Fig.~\\ref{fig:roi} (a)).  Davison~\\etal~   selected RoIs from the\nface based on the FACS~, shown in  Fig.~\\ref{fig:roi} (b). In addition, to eliminate the noise caused by the eye blinking and motion-less regions, Le~\\etal   proposed to mask the eye and cheek regions for each image (see Fig.~\\ref{fig:roi} (c)). \nHowever, the motion of eyes has a big contribution to MER under certain situations, \\eg~lid tighten refers to negative emotion. In work ,  Liong~\\etal~ utilized the eyes and mouth regions for MER, as shown in Fig.~\\ref{fig:roi} (d). Besides, Xia~\\etal~ found that the regions around the eyes, nose, and mouth are mostly active for MEs and can be chosen as RoIs through analyzing difference heat maps of ME datasets, as shown in Fig.~\\ref{fig:roi} (e). Furthermore, Xie~\\etal~ and Li~\\etal~ proposed to extract features on small facial blocks located by facial landmarks  (see Fig.~\\ref{fig:roi} (f)). In this way, the dimension of learning space can be drastically reduced and helpful for deep model learning on small ME datasets.", "cites": [8946, 8945], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple cited papers effectively, connecting different ROI selection strategies (block-based, FACS-based, landmark-based) into a coherent discussion of how local facial regions impact MER performance. It also provides a critical perspective by highlighting both the benefits and limitations of focusing on specific regions, such as the potential negative impact of including eye regions. The section identifies patterns across different methods but stops short of offering a fully meta-level abstraction or a novel framework."}}
{"id": "bbc0988b-d8a6-4cca-9039-d9f6dfa8f3d4", "title": "Data augmentation", "level": "subsubsection", "subsections": [], "parent_id": "664b50e4-718f-464c-91cc-30ee0dcc18d7", "prefix_titles": [["title", "Deep Learning for Micro-expression Recognition:  A Survey"], ["section", "Inputs"], ["subsection", "Pre-processing"], ["subsubsection", "Data augmentation"]], "content": "The main challenge for MER with DL is the  small-scale ME datasets.  The current ME datasets are too limited to train a robust DL model from scratch, therefore data augmentation is necessary. The common way for data augmentation is random crop and rotation in terms of the spatial domain. Xia~\\etal~augmented MEs through magnifying MEs with multiple ratios . Fig.~\\ref{fig:Data_augmentation} (a) and (b) show the examples of magnified ME apex frames with different ratios on the basis of EVM  and learning-based magnification , respectively. \n Additionally, Generative Adversarial Network (GAN)  can augment data by producing synthetic images.  Xie~\\etal~ introduced the AU Intensity Controllable GAN (AU-ICGAN) to synthesize subtle MEs. As Fig.~\\ref{fig:Data_augmentation} (d) shows, the ME sequences with continuous AU intensity can be synthesized through~. Yu~\\etal~ proposed a Identity-aware and Capsule-Enhanced Generative Adversarial Network (ICE-GAN) to complete the ME synthesis and recognition tasks. ICE-GAN outperformed the winner of MEGC2019 by 7$\\%$, demonstrating the effectiveness of GAN for ME augmentation and recognition.  The synthesized images corresponding to different emotions are shown in Fig.~\\ref{fig:Data_augmentation} (c).\nBesides, Liong~\\etal~ utilized conditional GAN to  generate optical-flow images to improve the MER accuracy based on computed optical flow. For ME clips, sub-sampling MEs from extended ME sequences through TIM can augment ME sequences~. \n\\begin{figure}\n  \\centering\n\\includegraphics[width=\\columnwidth]{image/Data_augmentation.jpg}\n\\caption{Examples of magnified and synthesized MEs.}\n\\label{fig:Data_augmentation}\n\\end{figure}\n\\begin{figure*}\n  \\centering\n\\includegraphics[width=18.2cm]{image/optical.jpg}\n\\caption{Examples of various inputs.}\n\\label{fig:opticalflow}\n\\end{figure*}", "cites": [5745, 5750, 8945], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes data augmentation methods used in micro-expression recognition, citing specific papers for techniques like motion magnification and GAN-based synthesis. However, it lacks deeper synthesis by not connecting these methods to broader themes in deep learning or MER. There is little critical analysis or evaluation of the effectiveness, limitations, or trade-offs of the approaches, and no abstraction to general principles or trends."}}
{"id": "15c083ee-d732-48cb-8595-c68ef9a468b0", "title": "Dynamic image sequence", "level": "subsubsection", "subsections": [], "parent_id": "8beb7989-a233-427e-a164-20d22b35bac8", "prefix_titles": [["title", "Deep Learning for Micro-expression Recognition:  A Survey"], ["section", "Inputs"], ["subsection", "Input modality"], ["subsubsection", "Dynamic image sequence"]], "content": "As the facial movements are subtle in the spatial domain, while change fast in the temporal domain, the temporal dynamics along the video sequences are essential in improving the MER performance. In this subsection, we  describe the various dynamic inputs. \n\\textit{Sequence.} Most ME researches  utilize consecutive frames in video clips , as shown in~Fig. \\ref{fig:opticalflow} (b). \n With the success of 3D CNN  and Recurrent Neural Network (RNN)~ in video analysis ,  MER based on sequence  is developed that considers the spatial and temporal information simultaneously. However, the computation  cost is relatively high and the complex model tends to overfit the small-scale training data. \n\\textit{Frame aggregation.} \nMEs are mostly collected with a high-speed camera (\\eg~200 fps) to capture the rapid subtle changes. Liong~\\etal~discovered that there is redundant information in ME clips recorded with high-speed cameras~. The redundancy could decrease the performance of MER. The experimental results of~ demonstrate that the onset, apex, and offset frames can provide enough spatial and temporal information to ME classification.  Liong~\\etal~ extracted features on onset and apex frames for MER, as shown in~ Fig. \\ref{fig:opticalflow} (c). Furthermore, in order to avoid apex frame spotting, Liu~\\etal~ and Kumar~\\etal~ designed  simple strategies to select aggregated frames automatically.\n\\textit{Image with dynamic information.}\nImage with dynamic information~ is a standard image that holds the dynamics of an entire video sequence in a single instance. The dynamic image generated  by using the rank pooling algorithm has been successfully used in MER~ to summarize the subtle dynamics and appearance in an image.  Similar to dynamic images, active images  encapsulated the spatial and temporal information of a video sequence into a single instance through estimating and accumulating the change of each pixel component (See Fig.~\\ref{fig:opticalflow} (d)). \n\\begin{table*}\n\\renewcommand{\\arraystretch}{1.3}\n\\centering\n\\small\n\\caption{The comparisons of inputs for MER}\n\\scalebox{0.84}{ \n\\begin{tabular}{|l|l|l|l|l|}\n \\bottomrule[1.5pt]\n\\multicolumn{2}{|c|}{Input modality}& \\multicolumn{1}{c|}{Strength}&\\multicolumn{1}{c|}{Shortcoming} \\\\ \\bottomrule[1.5pt]\n\\multicolumn{2}{|c|}{Static} &Efficient; Take advantage of  massive facial images  & \\makecell[l]{Require magnification and apex detection \\\\  Without temporal information}\n\\\\ \\bottomrule[1.5pt]\n\\multirow{4}{*}{Dynamic}&Sequence&Process directly&Not efficient; Information  redundancy\\\\  \\cline{2-4}\n&Frame  aggregation&Efficiently leverage key temporal information & Require apex  detection\\\\\\cline{2-4}\n&Image with dynamic information& Efficiently embed  spatio-temporal information    & Require dynamic information computation\n\\\\  \\cline{2-4} \n&Optical flow&Remove identity to some degree; Movement considered & Optical flow computation is necessary\\\\ \\bottomrule[1.5pt]\n\\multicolumn{2}{|c|}{Combination}& Explore spatial and temporal information&High computation cost\\\\ \\bottomrule[1.5pt]\n\\end{tabular}\n}\n\\\\\n \\label{tab:input}\n\\end{table*}\n\\textit{Optical flow.} The motion between ME frames contributes important information for ME recognition.  Optical flow approximates the local image motion, which has been verified to be helpful for motion representation~. It specifies the magnitude and direction of pixel motion in a given sequence of images with a two-dimension vector field (horizontal and vertical optical flows).  In recent years, several novel methodologies\n have been presented to improve optical flow techniques~, such as Farnebäck’s~, Lucas-Kanade~,  TV-L1~, FlowNet~, as shown in Fig. \n\\ref{fig:opticalflow} (e). \n\\textcolor{black}{Currently, many MER approaches utilize optical flow to represent the micro-facial movement and reduce the identity characteristic~.  Researches~ indicated  that  optical  flow-based  methods  always  outperform appearance-based methods. To further capture the subtle facial changes, multiple works~ extracted features on computed optical flows on the onset and mid-frame/apex in horizontal and vertical directions separately. \n}", "cites": [5753, 7991, 5749, 5747, 5748, 8945, 3797, 5751, 5752], "cite_extract_rate": 0.24324324324324326, "origin_cites_number": 37, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple approaches for dynamic input in MER, including sequences, frame aggregation, and dynamic image representations. It integrates related ideas and provides some critical perspectives, such as the trade-off between efficiency and performance when using high-speed frames. However, the synthesis remains largely structured around categories rather than offering a novel framework, and the critical evaluation is limited to general shortcomings without deeper, nuanced critique of specific methods."}}
{"id": "e8aa1ac7-3121-4980-89e8-7e31f29173f4", "title": "Discussion", "level": "subsection", "subsections": [], "parent_id": "49a5a48a-ae08-446f-b482-df00bb880a39", "prefix_titles": [["title", "Deep Learning for Micro-expression Recognition:  A Survey"], ["section", "Inputs"], ["subsection", "Discussion"]], "content": "In summary, the input is one of the key components to guarantee robust MER. \nThe various ME inputs have different strengths and shortcomings. The comparisons of inputs are shown in Table \\ref{tab:input}.\nThe input pre-processing is the first step in the MER system. Besides common face pre-processing approaches (face detection and registration), motion magnification, RoIs, and TIM also play important roles for robust MER, due to the subtle and rapid characteristics of MEs. Current motion magnification approaches always introduce noises and artifacts. More effective  motion magnification approaches should be explored. Furthermore, considering the small-scale ME datasets are far from enough to train a robust deep model, data augmentation is necessary for MER. In the future, studying more robust GAN-based ME generation approaches is a promising research direction.  \nRegarding the static input, the apex-based MER can reduce computational complexity and take advantage of the massive FEs to resolve the small-dataset issue in some degree. But, magnification is necessary since all the temporal information is dropped in single apex-based methods and the motion intensity is still low in the apex frames. Moreover, as the apex label is absent in some ME datasets, the performance of apex-based MER severely relies on the apex detection algorithm. Currently, the apex frame detection in long videos is still challenging. The end-to-end framework for apex frame detection and MER needs to be further studied.\nCompared with the static image, the dynamic input is able to leverage spatial and temporal information for robust MER. The \\textcolor{black}{simplest} dynamic input is ME sequence which doesn't require extra operations.  However, there is redundancy in ME sequences, and the complexity of the deep model is relatively high and tends to overfit on small-scale ME datasets.  To solve the problem of redundancy, frame aggregation cascading multiple key frames is  utilized. \nBesides, the dynamic image improves the computation efficiency through embedding  the temporal and spatial information to a still image. It can simultaneously consider spatial and temporal information in one image without challenging apex frame detection. Furthermore, optical flow is widely used for MER as the optical flow describes the motions and removes the identity in some degree. However, most of the current optical flow-based MER methods are based on traditional optical flow, which is not end-to-end. In the future, more DL-based optical flow extraction can be further researched.  \nIn addition, combining various inputs is the inevitable trend to fully explore spatial and temporal information and leverage the merits of various inputs. Correspondingly, the combined inputs also inherit the shortcomings of the inputs. However, the multiple inputs could be\n complementary in some degree. So far, the method with various inputs has achieved the best performance. Considering the success of multiple inputs and limited ME samples, more combined modalities, such as optical flow, key frames, and landmarks can be promising research directions. \n\\begin{figure*}\n  \\centering\n\\includegraphics[width=18.5cm]{image/Special0network.jpg}\n\\caption{Special blocks: (a) Residual block   (b) Inception module; (c) RCN ;    (d) Spatial attention of CBAM  (e) Channel attention of CBAM ;  (f) Capsule module ;  (g) Spatio-temporal attention ;  (h) GCN .}\n\\label{fig:special}\n\\end{figure*}", "cites": [1210, 97, 5749, 8945, 306], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key input methods for MER by drawing on multiple cited papers, integrating insights on static vs. dynamic inputs and their respective challenges. It critically evaluates the shortcomings of current techniques (e.g., motion magnification noise, overfitting due to small datasets) and highlights promising future directions. The abstraction level is strong as it generalizes the role of different input types and suggests a broader trend toward combining modalities for better performance."}}
{"id": "5ed03eb7-f023-4f8b-9409-435f4cd4e6ef", "title": "Deep networks for MER", "level": "section", "subsections": ["513e7d8a-09ab-46fe-936c-726cca288d7d", "962a751c-f6a9-420b-b983-611e5b0592a7", "c5d71e79-b49f-4e53-8774-f8d15567adb5", "ac065c88-38ec-4ab2-8a85-a676f3b7b6eb", "a6b32ca4-196b-47d0-814e-f15c78b6c30e"], "parent_id": "c226fd34-5a4b-426b-a6d8-50438f783d8d", "prefix_titles": [["title", "Deep Learning for Micro-expression Recognition:  A Survey"], ["section", "Deep networks for MER"]], "content": "\\label{sec:network}\nConvolutional Neural Networks (CNNs) have shown excellent performances for various computer vision tasks, such as action recognition  and FER . In general, for image classification,  CNNs employ two dimensional convolutional kernels (denoted as 2D CNN) to leverage spatial context across the height and width of the images to make predictions. Compared with 2D CNN, \nCNNs with three-dimensional convolutional kernels (denoted as 3D CNN) are verified more effective for exploring spatio-temporal information of videos . 3D CNN can take advantage of spatio-temporal information  to improve the performance but comes with a computational cost because of the increased number of parameters. Moreover, the 3D CNN only can deal with videos with the fixed length due to the pre-defined kernels. Recurrent Neural Network (RNN)  \nwas proposed to process the time series data  with various duration. Furthermore,  Long Short-Term Memory (LSTM) was developed to settle the vanishing gradient problem that can be encountered when training RNNs. \nUnlike common video-based classification problems, for the recognition of subtle, fleeting, and involuntary MEs, \nvarious DL approaches have been proposed to boost MER performance. In this section, we introduced  the approaches in the view of special blocks, network architecture, training strategy, and loss.", "cites": [5739, 1282], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from both the general FER survey and 3D CNN research, connecting them to the unique challenges of MER. It provides a structured overview of different DL approaches, showing some analytical depth by discussing their strengths (e.g., 3D CNN for spatio-temporal info) and limitations (e.g., computational cost, fixed-length video constraints). However, it lacks deeper comparative analysis or a novel framework that would elevate it to high insight level."}}
{"id": "513e7d8a-09ab-46fe-936c-726cca288d7d", "title": "Network block", "level": "subsection", "subsections": [], "parent_id": "5ed03eb7-f023-4f8b-9409-435f4cd4e6ef", "prefix_titles": [["title", "Deep Learning for Micro-expression Recognition:  A Survey"], ["section", "Deep networks for MER"], ["subsection", "Network block"]], "content": "In terms of solving the two main ME challenges: overfitting on small-scale ME datasets and low intensity of MEs, various effective network blocks have been utilized and designed, such as ResNet family with residual modules , and Inception module . In this subsection, we introduce the special network blocks utilized for MER improvement.\n  For the challenge of small-scale datasets, recent researches~ demonstrate that  residual blocks with shortcut connections (shown in Fig.~\\ref{fig:special} (a)) achieves easy optimization and  reduces the effect of the vanishing gradient problem. Multiple MER works~ employed residual blocks for robust recognition on small-scale ME datasets.  Instead of directly applying the shortcut connection,~ further designed a convolutionable shortcut to learn the important residual information and AffectiveNet~ introduced an MFL module learning the low- and high-level feature parallelly to increase the discriminative capability between the inter and intra-class variations. \nSince the fully connected layer requires lots of parameters which makes it prone to extreme loss explosion and overfitting~,  the Inception module~  aggregates different sizes of filters to compute multi-scale spatial information and assembles $1\\times1\\times1$ convolutional filters to reduce the dimension and parameter, as shown in Fig.~\\ref{fig:special} (b). Multiple works~ utilized  the Inception module for efficient MER. Inspired by the Inception structure, a Hybrid Feature (HyFeat) block  was proposed to preserve the domain knowledge features for expressive regions of MEs and enrich features of edge variations through using different scaled convolutional filters.  \nFurthermore, considering the fact that CNN with more convolutional layers has stronger representation ability, but easy to overfit on small-scale datasets, paper  and~ introduced Recurrent Convolutional Network (RCN) which achieved a shallow architecture though recurrent connections, as shown in Fig.~\\ref{fig:special} (c). \nOn the other hand, MEs perform as the combination of multiple facial movements. The latent semantic information among subtle facial changes contributes important information for MER performance. Recent researches illustrate that the Graph Convolutional Network (GCN) is effective to model these semantic relationships and can be leveraged for face analysis tasks, as shown in Fig.~\\ref{fig:special} (h). Inspired by the successful application of GCN in FER,  developed the GCN for MER to further improve the performance by modeling the relationship between the local facial movements.  Lei~\\etal~ built graphs on the RoIs along facial landmarks contributing information to subtle MEs. The TCN residual blocks  and transformer  were applied for reasoning the relationships of RoIs. On the other hand, as the FE analysis can be benefited from the knowledge of AUs and FACS, the works  built graph on AU-level representations to boost the MER performance by inferring the AU relationship. \nBesides graph,   Capsule Neural Network (CapsNet)  was employed to  explore part-whole relationships on face to promote MER performance through better model hierarchical relationships by routing procedure~,  as shown in Fig.~\\ref{fig:special} (f). \nIn addition, since MEs have specific muscular activations on the face, MEs are related with local regional changes~. Therefore, it is crucial to highlight the representation on RoIs~. \nSeveral approaches  have shown the benefit of enhancing spatial encoding with attention module. \nExcept for spatial information, the temporal change also plays an important role for MER. As MEs have rapid changes, the frames have unequal contribution to MER. Wang~\\etal~ explored a global spatial and temporal attention module (GAM) based on the non-local network  to encode wider spatial and temporal information to capture local high-level semantic information, as shown in Fig. \\ref{fig:special} (g). \nMoreover,  Yao~\\etal~ learned the weights of each feature channel adaptively through adding squeezeand-and-excitation blocks.  Additionally, recent works  encoded the spatio-temporal and channel attention simultaneously to further boost the representational power of MEs. Specifically, \nCBAMNet  presented a convolutional block attention module (CBAM) cascading the spatial attention module (see Fig. \\ref{fig:special} (d)) and channel attention module (see Fig. \\ref{fig:special} (e)).  \n\\begin{figure*}\n  \\centering\n\\includegraphics[width=18cm]{image/structure.jpg}\n\\caption{(a) GAM based on single stream ;  (b)  CNN cascaded with LSTM ;\n(c) TSCNN~ based on three-stream network; \n(d) A dual-stream multi-task network incorporating gender detection  designed by GEME ; (e) CNN cascaded with GCNs on the basis of AU-feature graph   ; (f) MER based on knowledge distillation~;  and  (g)  The general concept of single-stream, multi-stream, cascaded networks, multi-task learning, and the training strategy of transferring knowledge. }\n\\label{fig:classical}\n\\end{figure*}\nIn summary, due to the special characteristics of MEs, many DL-based methods designed special blocks to extract discriminative ME representations from the latent semantic information. Recent MER researches indicate that attention and graph blocks are effective to model the semantic relationships. Current GCN-based MER are always based on the local facial regions and AU labels. In the future, more compact and concise representation, such as landmark location, can be further developed for efficient MER. \nMoreover, the transformer  has been verified effectively on modeling the relationship. For future MER research,  transformers can be further applied to model the relationships between facial landmarks, AUs, RoIs and frames to enhance ME representation.\nOn the other hand, other special blocks  targeted at learning discriminative ME features with less parameters to avoid overfitting. In the future, more efficient blocks should be studied to dig subtle ME movements on limited ME datasets.", "cites": [8948, 5755, 305, 7990, 5749, 5747, 5756, 502, 97, 7992, 8947, 8945, 5754, 1210, 306], "cite_extract_rate": 0.30612244897959184, "origin_cites_number": 49, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key network blocks used in deep MER, integrating insights from multiple papers to highlight how each component addresses challenges like overfitting and low-intensity MEs. It provides some critical analysis by pointing out limitations such as parameter explosion and the need for more efficient designs. Additionally, it abstracts patterns (e.g., use of attention and graph blocks for semantic modeling) and suggests future research directions, demonstrating a strong analytical and integrative perspective."}}
{"id": "0e9df517-45d7-4298-a51a-5b871236d23b", "title": "Single-stream networks", "level": "subsubsection", "subsections": [], "parent_id": "962a751c-f6a9-420b-b983-611e5b0592a7", "prefix_titles": [["title", "Deep Learning for Micro-expression Recognition:  A Survey"], ["section", "Deep networks for MER"], ["subsection", "Network architecture"], ["subsubsection", "Single-stream networks"]], "content": "Typical deep MER methods adopt single CNN with individual input . The apex frame, optical flow images and dynamic images are common inputs for single-stream 2D CNNs, while single-stream 3D CNNs extract the spatial and temporal features from ME sequences directly. Considering the limited ME samples are far from enough to train a robust deep network, multiple works designed single-stream shallow CNNs for MER .  Belaiche~\\etal~ achieved a shallow network through deleting multiple convolutional layers of the deep network Resnet.  Zhao~\\etal~  proposed a 6-layer CNN in which the input is followed by an $1\\times1$ convolutional layer to increase the non-linear representation. \nBesides designing shallow networks, many studies  fine-tuned deep networks pre-trained on large face datasets to avoid the overfitting problem. Li~\\etal~ firstly adopted the 16-layer VGG-FACE model pre-trained on VGG-FACE dataset  for MER. Following , the MER with  Resnet50, SEnet50 and VGG19 pre-trained on Imagenet was explored in . The results illustrate that VGG surpasses other architectures regarding the MER topic and is good at distinguishing the complex hidden information in data. \nAll of above works are based on 2D CNN with image input, while several works employed single 3D CNN to directly extract the spatial and temporal features from ME sequences. GAM , MERANet  and CBAMNet  combined attention modules to 3D CNN to enhance the representation in spatial and temporal dimensions.", "cites": [7991, 7990], "cite_extract_rate": 0.15384615384615385, "origin_cites_number": 13, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of single-stream networks in deep MER, mentioning both shallow and pre-trained models. It integrates a few sources and briefly connects them to the theme of addressing limited data, but lacks deeper synthesis, critical evaluation of the methods, and broader abstraction. The discussion remains surface-level without identifying overarching trends or principles."}}
{"id": "622e308d-85e0-4c64-b1a9-985584e9a1d7", "title": "Multi-stream network", "level": "subsubsection", "subsections": [], "parent_id": "962a751c-f6a9-420b-b983-611e5b0592a7", "prefix_titles": [["title", "Deep Learning for Micro-expression Recognition:  A Survey"], ["section", "Deep networks for MER"], ["subsection", "Network architecture"], ["subsubsection", "Multi-stream network"]], "content": "Single stream is a basic model structure and only extracts features from the single view of MEs. However, MEs have subtle movements and limited samples, the single view is not able to provide sufficient information. \nAs we discussed in Section \\ref{sec:inputs}, the various inputs from different views is able to effectively explore spatial and temporal information. Thus, the  multi-stream network is adopted in MER to learn features from multiple inputs. The multi-stream structure allows \nthe network extracting multi-view features through multi-path networks, as shown in Fig.~\\ref{fig:classical} (g). \nIn general, multi-stream networks can be classified to networks with the same blocks, different blocks and handcrafted features. \n\\emph{Multi-stream networks with the same blocks. }\nThe Optical Flow Features from Apex frame Network (OFF-ApexNet)   and  Dual-stream shallow network (DSSN)  built the dual-stream CNN for MER based on optical flow extracted from onset and apex. \nFurthermore, Liong~\\etal~ extended OFF-ApexNet to multiple streams with various optical flow components as input data. The   multi-stream CNN with optical flow  and Three-Stream CNN (TSCNN)~ designed three-stream CNN models for MER with three kinds of inputs (See Fig.~\\ref{fig:classical} (c)). Specifically, the former one utilized apex frame, optical flow and the apex frame masked by the optical flow threshold, while the latter approach employed the apex frames, optical flow between onset, apex, and offset frames to investigate the information of the static spatial, dynamic temporal and local information.\n In addition, She~\\etal~ proposed a four-stream model considering three RoIs and global regions as each stream to explore the local and global information.\n Besides multi-stream 2D CNNs, 3DFCNN~, SETFNet~ and  applied 3D flow-based CNNs for video-based MER consisting of multiple sub-streams to extract features from  frame sequences and optical flow, or RoIs. \n\\emph{Multi-stream networks with  different blocks.}  For enhancing the ME feature representation, some works  investigated the combination of different convolutions.  \nLiong~\\etal~ designed a Shallow Triple Stream Three-dimensional CNN (STSTNet)~  adopting multiple 2D CNN with different kernels.\nInstead of utilizing different kernels, AffectiveNet  constructed a four-path network with four different receptive fields (RF) to obtain multi-scale features \nfor better describing subtle MEs. \nOn the other hand, Landmark Relations with Graph\nAttention Convolutional Network (LR-GACNN)\n  and MER-GCN  built two-stream graph networks to explore relationships between landmark points and the local patches, and AUs and sequence, respectively.  Furthermore,  and  integrated  2D CNN and 3D CNN to extract spatio-temporal information. \n\\emph{Multi-stream networks with  handcrafted features.}\nSince the  subtle facial movements of MEs are highly related to face textures, the handcrafted features for low-level representation also plays an important role in MER. Multiple works  combined deep features and handcrafted features to leverage the low-level and high-level information for robust MER. Specifically, in the works  and , the CNN features on apex frame and LBP-TOP were concatenated to represent MEs.", "cites": [5750, 5748, 8947], "cite_extract_rate": 0.15789473684210525, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers to present a structured overview of multi-stream networks in MER, grouping them into categories based on architectural design. It identifies the rationale behind using multiple streams, such as capturing spatial, temporal, and local features, which shows a reasonable level of integration. However, the critical analysis is limited to stating that these approaches were designed to enhance feature representation without deeper evaluation of their strengths or weaknesses. The section offers some abstraction by categorizing the different types of multi-stream architectures and their purposes, but the insights remain somewhat surface-level."}}
{"id": "98643bdd-1474-4dfe-bb0f-4bdfcf17fea6", "title": "Cascaded network", "level": "subsubsection", "subsections": [], "parent_id": "962a751c-f6a9-420b-b983-611e5b0592a7", "prefix_titles": [["title", "Deep Learning for Micro-expression Recognition:  A Survey"], ["section", "Deep networks for MER"], ["subsection", "Network architecture"], ["subsubsection", "Cascaded network"]], "content": "Cascaded network combines various modules for different tasks sequentially to construct an effective network, as shown in Fig. \\ref{fig:classical} (g). Recent FE studies  demonstrate that learning a hierarchy of features gradually filters out the information unrelated to expressions. \nInspired by the FE studies , for further exploring the temporal information of MEs, Nistor~\\etal~  cascaded CNN and RNN to  extract features from individual frames of the sequence and capture the facial evolution during the sequence, respectively.  Furthermore, Bai~\\etal~ and Zhi~\\etal~ combined CNN with LSTMs in series to deal with ME samples with various duration directly, as shown in Fig.~\\ref{fig:classical} (b). Besides, in order to explore the AU semantics in MEs, Xie~\\etal~proposed an AU-assisted Graph Attention Convolutional Network (AU-GACN)  cascading 3D CNN and GCN to infer MEs based on AU features (see Fig.~\\ref{fig:classical} (f)). \nIn addition, multiple MER works combined  multi-stream and cascaded structure to further explore the multi-view series information. \nVGGFace2+LSTM ,  Temporal Facial Micro-Variation Network (TFMVN)  and MER with Ternary Attentions (MERTA)    developed three stream VGGNets followed by LSTMs to extract multi-view  spatio-temporal features. \nDifferent from above works,  Khor~\\etal~  proposed an Enriched Long-term Recurrent Convolutional Network (ELRCN)  adding one VGG+LSTM path with channel-wise stacking inputs for spatial enrichment. Besides, AT-Net  and SHCFNet  extracted spatial and temporal features by CNN and LSTM from the apex frame and optical-flow in parallel and concatenated them together to represent MEs.", "cites": [5739, 7992, 5746], "cite_extract_rate": 0.3, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of cascaded networks in MER, listing various approaches and their configurations. It synthesizes a few related papers by connecting the use of cascaded CNN and RNN/LSTM/GRU structures, and it mentions the goal of capturing temporal and spatial features. However, there is little critical evaluation of the strengths or weaknesses of these approaches, and the abstraction remains limited to surface-level patterns without deeper conceptual insights."}}
{"id": "e745fb61-73bd-4b5b-9c9b-dfc4481426a2", "title": "Multi-task network", "level": "subsubsection", "subsections": [], "parent_id": "962a751c-f6a9-420b-b983-611e5b0592a7", "prefix_titles": [["title", "Deep Learning for Micro-expression Recognition:  A Survey"], ["section", "Deep networks for MER"], ["subsection", "Network architecture"], ["subsubsection", "Multi-task network"]], "content": "Most existing works for MER focus on learning features that are sensitive to expressions. However, MEs in the real world are intertwined with various factors, such as subject identity and AUs. The approaches aiming at a single MER task are incapable of making full use of the information on face.  To address this issue, several multi-task learning-based MER approaches have been subsequently developed for better MER . Firstly, Li~\\etal~ developed a multi-task network combining facial landmarks detection and optical flow extraction to refine the optical flow features for MER with SVM. Following , several end-to-end deep multi-task networks leveraging different side tasks were proposed. \n GEnder-based ME recognition (GEME)  designed a dual-stream multi-task network incorporating gender detection task with MER (see Fig~\\ref{fig:classical} (d)), while feature refinement  and MER-auGCN   simultaneously detected AUs and MEs and further aggregated AU representation into ME representation. On the other hand, considering that a common feature representation can be learned from multiple tasks, Hu~\\etal~ formulated MER as a multi-task classification problem in which each category classification can be regarded as one-against-all pairwise classification problem.\nIn summary, the network architecture can be roughly divided into single-stream, multi-stream, cascaded networks, and multi-task learning, as shown in Fig. \\ref{fig:classical} (g). Single stream is the simple basic model architecture. However,  single-stream networks only consider the single view of MEs. To further leverage the ME information, the multi-stream network is proposed to learn features from multiple views for robust MER. Moreover, since learning  a  hierarchy  of  features  can  gradually  filter  out the information unrelated to expressions, the network cascades various modules, such as LSTMs and GCNs, sequentially  to  construct  an  effective  MER network. In the future, more effective modules should be combined in multi-stream and cascaded ways to further boost the MER performance. \nIn terms of the tasks, multiple task learning  can share knowledge  among tasks, introducing extra information and a low risk of overfitting in each task. Currently, most ME research only studied the contribution of landmarks detection, gender classification, and AU detection. Other tasks, such as face recognition and eye gaze tracking may also introduce useful  knowledge for MER. Exploring and taking advantage of  more  face related-tasks is a practical way to further improve MER performance.  \n\\vspace{-3ex}\n\\textcolor{black}{", "cites": [326], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple multi-task learning approaches for MER, connecting the use of side tasks like gender detection and AU detection to improve recognition. It provides a basic framework for categorizing network architectures and hints at broader patterns by suggesting the potential of incorporating additional face-related tasks. While it offers some critical perspective by pointing out the limitations of single-task approaches and the risk of overfitting, the analysis remains somewhat general and could benefit from deeper comparative or evaluative insights."}}
{"id": "c5d71e79-b49f-4e53-8774-f8d15567adb5", "title": "Training strategy", "level": "subsection", "subsections": [], "parent_id": "5ed03eb7-f023-4f8b-9409-435f4cd4e6ef", "prefix_titles": [["title", "Deep Learning for Micro-expression Recognition:  A Survey"], ["section", "Deep networks for MER"], ["subsection", "Training strategy"]], "content": "}\nAs we discussed before, DL-based MER suffers from a lack of adequate data. It is almost impossible to train a reliable deep model from scratch. \nCurrently,  there are large-scale FE datasets with labels. Leveraging the FE datasets by special training strategy, such as fine-tuning , knowledge distillation~, and domain adaptation , is a reasonable way to solve the problem of a small amount of data.   The knowledge of a pre-trained model for a related task can be transferred to MER to boost performance. The training strategy of transferring knowledge is shown in Fig~\\ref{fig:classical} (g). \nFine-tuning ME datasets on pre-trained models is widely used in MER  .\nPatel~\\etal~ provided two models pre-trained on ImageNet dataset and  FE datasets, respectively. The feature selection method  was also adopted to improve the model's performance. It was found that features captured from the FE datasets performed better in terms of accuracy, as it is more similar to the ME datasets than object/face datasets. \nBesides fine-tuning, another effective  transfer learning strategy is knowledge  distillation. Knowledge  distillation achieves  small and fast networks through leveraging information from pre-trained high-capacity networks.  Sun~\\etal~  utilized Fitnets  to  guide the shallow network learning for  MER by mimicking  the  intermediate  features  of  the  deep network pre-trained for macro-expression recognition and AU detection, as shown in Fig~\\ref{fig:classical} (e). However, the  appearances of MEs and  macro-expressions are different due to the different  intensity of  facial movements. Thus, mimicking the macro-expression representation directly is not reasonable.  Instead, SAAT   transferred attention on the style aggregated MEs generated by CycleGAN .\nIn addition,  domain adaptation methods can obtain domain  invariant representations by embedding domain adaptation in the pipeline of deep learning. In , , and ,  the gap between the MEs and macro-expressions was narrowed down by domain adaption based on adversarial learning strategy.\nIn general, fine-tuning is most widely used in MER. To further effectively transfer meaningful information from massive FEs, knowledge distillation and domain adaptation are also applied to MER by distilling knowledge and extracting domain invariant representations, respectively. However, the domain adaptation with adversarial learning increases the learning complexity. There are significant differences both spatially and temporally between macro-expressions and MEs, therefore, directly transferring the knowledge is not able to fully leverage the macro-expression information. \nConsidering that the facial muscle movements are consistent  between  MEs  and  macro-expressions, the attention and AUs can be further studied for transfer learning in future ME research. Moreover, semi-supervised and unsupervised learning could also be further developed to take advantage of unlabeled facial images.", "cites": [7991, 7022, 4501], "cite_extract_rate": 0.25, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to discuss training strategies for deep learning in MER, highlighting connections between fine-tuning, knowledge distillation, and domain adaptation. It offers a critical perspective by pointing out the limitations of transferring macro-expression knowledge directly to MEs and noting the increased complexity of adversarial domain adaptation. While it identifies broader patterns in the application of transfer learning, it does not fully abstract to a new theoretical framework or meta-level insights."}}
{"id": "ac065c88-38ec-4ab2-8a85-a676f3b7b6eb", "title": "Loss functions", "level": "subsection", "subsections": [], "parent_id": "5ed03eb7-f023-4f8b-9409-435f4cd4e6ef", "prefix_titles": [["title", "Deep Learning for Micro-expression Recognition:  A Survey"], ["section", "Deep networks for MER"], ["subsection", "Loss functions"]], "content": "Different from traditional methods, where the feature extraction and classification are independent, deep networks can perform end-to-end classification through loss functions by penalizing the deviation between the predicted and true labels during training. Most MER works directly applied the commonly used softmax cross-entropy loss .  The softmax loss is typically good at correctly classifying known categories. However, in practical classification tasks, the unknown samples need to be classified. Therefore, in order to obtain better-generalized ability, the inter-class difference and \nintra-class variation should be further optimized and reduced, respectively, especially for subtle and limited MEs. The metric learning techniques, such as contrastive loss  and triplet loss ,  was developed to ensure intra-class compactness and inter-class separability through measuring the relative distances between inputs. Xia~\\etal~ adopted    an adversarial learning approach and triplet loss with inequality regularization to converge the output of MicroNet efficiently.  However, metric learning loss usually requires effective sample mining strategies for robust recognition performance. Metric learning alone is not enough for learning a discriminative metric space for MEs. Intensive experiments demonstrate that importing a large margin  on softmax loss  can increase the inter-class difference.  Lalitha~\\etal~ and Li~\\etal~ combined softmax cross-entropy loss and center loss  to increase the compactness of intra-class variations  and separable inter-class differences through penalizing the distance between deep features and their corresponding class centers.\nSome special MEs are difficult to trigger, thus leading to data imbalance.\nMultiple MER works  utilized the Focal loss to overcome the imbalance challenge by adding a factor to put more focus on misclassified  and hard samples which are difficult to recognize. Moreover, MER-auGCN   designed an adaptive factor with the Focal loss to balance the proportion of the negative and positive samples in a given training batch. \nIn summary, MER suffers from  high intra-class variation, low inter-class differences, and imbalanced distribution because of the low intensity and spontaneous characteristics of MEs. Currently, most MER approaches are based on the basic softmax cross-entropy loss, but others utilized the triplet loss, center loss, or focal loss to encourage inter-class separability, intra-class compactness, and balanced learning. \nIn the future, exploring more effective loss functions  to learn discriminative representation for MEs can be a promising research direction.  Considering the low intensity of facial movements leading to low inter-class differences, \n better metric space  and larger margin loss for MER should be further studied. Recently, various methods  have been proposed for the classification of imbalanced long-tail distribution data. ME research can leverage the ideas for long-tail data to improve the MER performance.", "cites": [1244, 5757, 8945, 5758], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes information from multiple cited papers, connecting the use of different loss functions (softmax, triplet, center, focal) to address challenges specific to MER, such as subtle facial changes and class imbalance. It provides a critical perspective by pointing out limitations of individual loss functions and the need for effective sample mining. The section abstracts beyond individual methods to identify broader issues like inter-class separability and intra-class variation, and suggests future research directions informed by trends in imbalanced data classification."}}
{"id": "f6ca2d5f-77fa-4745-ad82-1f83af18d7c2", "title": "Evaluation matrix", "level": "subsection", "subsections": [], "parent_id": "b9f9fd43-4db4-477d-8819-c833117f84c7", "prefix_titles": [["title", "Deep Learning for Micro-expression Recognition:  A Survey"], ["section", "Experiments"], ["subsection", "Evaluation matrix"]], "content": "The common evaluation metrics for MER are \naccuracy and F1-score. In general, the accuracy metric measures the ratio of correct predictions over the total evaluated samples. However, the accuracy is susceptible to bias data. F1-score solves the bias problem by considering the total True Positives (TP), False Positives (FP) and False Negatives (FN) to reveal the true classification performance.\nFor the composited dataset which combines multiple datasets leading to severe data imbalance, Unweighted F1-score (UF1) and Unweighted Average Recall (UAR) are utilized to measure the performance of various methods.  UF1 is also known as macro-averaged F1-score which is determined by averaging the per-class F1-scores. UF1 provides equal emphasis on rare classes in imbalanced multi-class settings. UAR is defined as the average accuracy of each class divided by the number of classes without consideration of  samples per class. UAR can reduce the bias caused by class imbalance and is known as balanced accuracy. \n\\begin{table*}\n\\renewcommand{\\arraystretch}{1.3}\n\\centering \n\\small\n\\caption{MER on SMIC, CASME, CASME II, SAMM, and CMED datasets. }\n\\scalebox{0.70}{ \n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n \\bottomrule[1.5pt]\nDataset&Method&Year&Pre-p.&Input& Network architecture & Block &Pre-train & Protocol&Cate.& F1&ACC (\\%)\\\\\n \\bottomrule[1.5pt]\n\\multirow{8}{*}{SMIC}\n&TSCNN~& 2019&E, R & OF+Apex &3S-CNN&-& FER2013 &LOSO&3&0.7236&72.74\\\\\\cline{2-12}\n&DIKD~ &2020&-&Apex &CNN+KD+SVM&-& -&LOSO&3&0.71&76.06\n\\\\\\cline{2-12}\n&MTMNet  &2020 &- & Onset-Apex &\\textcolor{black}{2S-CNN}+DA+GAN&RES& \\makecell[c]{CK+ ,MMI , \\\\ Oulu-CASIA }&LOSO&3&0.744&76.0\\\\\\cline{2-12}\n&MiMaNet &2021&T& Apex+sequence &\\textcolor{black}{2S-CNN}+DA & RES& CK+ ,MMI&LOSO&3&0.778&78.6\\\\\\cline{2-12}\n& DSTAN&2021&T& OF+sequence &\\textcolor{black}{2S-CNN}+LSTM+SVM & Attention& -&LOSO&3&0.78&77\\\\\\cline{2-12}\n& AMAN&2022&E,T& sequence &\\textcolor{black}{CNN} & Attention& FER2013 &LOSO&3&0.77&79.87\n\\\\ \\bottomrule[1.5pt]\n\\multirow{5}{*}{CASME} & TSCNN~ &2019& E,R & OF+Apex &3S-CNN&-& FER2013 &LOSO&4&0.7270 & 73.88\n\\\\\\cline{2-12}\n&DIKD~ &2020&-&Apex &CNN+KD+SVM&RES& -&LOSO&4&0.77&81.80\n\\\\\\cline{2-12}\n&AffectiveNet \n&2020&E & DI &4S-CNN&MFL& -&LOSO&4&-&72.64\\\\\\cline{2-12}\n& DSTAN&2021&T& OF+sequence &\\textcolor{black}{2S-CNN}+LSTM+SVM & Attention& -&LOSO&4&0.75&78\\\\\n \\bottomrule[1.5pt]\n\\multirow{11}{*}{CASME II} \n&OFF-ApexNet  &2019 \n&-& OF &2S-CNN&-& -&LOSO&3&0.8697& 88.28\\\\\\cline{2-12}\n&TSCNN~ &2019&E, R & OF+Apex &3S-CNN&-& FER2013 &LOSO&5&0.807&80.97\\\\\\cline{2-12}\n&STSTNet~ &2019\n&E & OF &3S-3DCNN&-& -&LOSO&3& 0.8382& 86.86\\\\\\cline{2-12}\n&Graph-TCN~ & 2020 &L, R &Apex &TCN+GCN&Graph&- &LOSO&5&0.7246 & 73.98\\\\\\cline{2-12}\n&SMA-STN~ &2020 \n&-&\\textcolor{black}{Snippet}  &CNN&Attention&WIDER FACE&LOSO&5& 0.7946& 82.59\\\\\\cline{2-12}\n&GEME   &2021&- & DI &\\textcolor{black}{2S-CNN}+ML&RES& -&LOSO&5&0.7354&75.20\\\\\\cline{2-12}\n&MiMaNet &2021&T& Apex+sequence &\\textcolor{black}{2S-CNN}+DA & RES& CK+ ,MMI&LOSO&5&0.759&79.9\\\\\\cline{2-12}\n&LR-GACNN &2021&E& OF+Landmark &\\textcolor{black}{2S-GACNN} & Graph& -&LOSO&5&0.7090&81.30\\\\\\cline{2-12}\n& DSTAN&2021&T& OF+sequence &\\textcolor{black}{2S-CNN}+LSTM+SVM & Attention& -&LOSO&5&0.73&75\\\\\\cline{2-12}\n& AMAN&2022&E,T& sequence &\\textcolor{black}{CNN} & Attention& FER2013 &LOSO&5&0.71&75.40\\\\\\bottomrule[1.5pt]\n\\hline\n\\multirow{8}{*}{SAMM}\n&DIKD~ &2020&-&Apex &CNN+KD+SVM&-& -&LOSO&4&0.83&86.74\n\\\\\\cline{2-12}\n&SMA-STN~ &2020 \n&-&\\textcolor{black}{Snippet}  &CNN&-&WIDER FACE&LOSO&5& 0.7033& 77.20\\\\\\cline{2-12}\n&MTMNet  &2020 &- & Onset-Apex &\\textcolor{black}{2S-CNN}+GAN+DA&RES& \n\\makecell[c]{CK+ ,MMI , \\\\ Oulu-CASIA }&LOSO&5&0.736&74.1\\\\\\cline{2-12}\n&MiMaNet &2021&T& Apex+sequence &\\textcolor{black}{2S-CNN}+DA & RES& CK+ ,MMI&LOSO&5&0.764&76.7\\\\\\cline{2-12}\n&LR-GACNN &2021&E& OF+Landmark &\\textcolor{black}{2S-GACNN} & -& -&LOSO&5&0.8279&88.24\\\\\\cline{2-12}\n&GRAPH-AU  &2021&L& Apex &\\textcolor{black}{2S-CNN+GCN} & \\makecell[c]{Graph, \\\\ Transformer}& -&LOSO&5&0.7045&74.26\\\\\\cline{2-12}\n& AMAN&2022&E,T& sequence &\\textcolor{black}{CNN} & Attention& FER2013 &LOSO&5&0.67&68.85\\\\\n\\bottomrule[1.5pt]\nCMED& Shallow CNN &2020&E& OF &\\textcolor{black}{CNN} & -& -&LOSO&7&0.6353&66.06\\\\\\bottomrule[1.5pt]\n\\end{tabular}\n}\n{\\raggedright $^1$   Pre-p.: Pre-processing;  E:EVM; R: RoI; T: Temporal normalization ; L: Learning-based magnification. \\par \n$^2$ OF: Optical flow; DI: Dynamic image.\\par\n$^3$ nS-CNN: n-stream CNN; ML: Multi-task learning; DA: Domain adaption; KD: Knowledge distillation.\\par\n$^4$   Cate: Category; F1: F1-score; ACC: Accuracy; RES: Residual block.\\par\n }\n\\vspace{-2ex}\n \\label{tab:result_CASME}\n\\end{table*}", "cites": [7991, 5748, 8947, 1231, 8949], "cite_extract_rate": 0.25, "origin_cites_number": 20, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a tabular comparison of various deep learning methods for MER across datasets, using metrics like F1-score and accuracy. It synthesizes some information by discussing the use of UF1 and UAR for imbalanced datasets. However, it lacks in-depth critical evaluation of the cited works and does not abstract broader patterns or principles beyond listing results."}}
{"id": "79769ac9-9be3-4159-9caf-192b18dff050", "title": "Model evaluation protocols", "level": "subsection", "subsections": [], "parent_id": "b9f9fd43-4db4-477d-8819-c833117f84c7", "prefix_titles": [["title", "Deep Learning for Micro-expression Recognition:  A Survey"], ["section", "Experiments"], ["subsection", "Model evaluation protocols"]], "content": "Cross-validation is the widely utilized protocol for evaluating the MER performance. In cross-validation, the dataset is splitted into multiple folds and the training and testing were evaluated on different folds. It regards a fair verification and prevents overfitting on the small-scale ME datasets.  In the MER field, cross-validation includes leave-one-subject-out (LOSO), leave-one-video-out (LOVO), and K-Fold cross-validations.\nIn LOSO, every subject is taken as a test set in turn and the other subjects as the training data. This kind of subject-independent protocol can avoid subject bias and evaluate the generalization performance of various algorithms.  LOSO is the most popular cross-validation in MER. \nThe LOVO takes each sample as the validation unit which enables more training data and alleviates the overfitting to some degree.  However, it is not subject-independent, thus it can not well evaluate the generalization capability. Another problem is that the test number of LOVO is the sample size which may lead to huge time cost, not suitable for deep learning. \nFor K-fold cross-validation, the original samples are randomly partitioned into k equal-sized parts. Each part is taken as a test set in turn and the rest are the training data. Thus, the number of cross-validation tests is K. In practice,  the evaluation time can be greatly reduced by setting an appropriate K. The typical K values are 5 or 10.\nSince the MEs have small-scale datasets, the experiments on MER do not have reliable validation datasets. According to the released codes, some works  utilized the test datasets as the validation datasets directly and reserved the best epoch results on each fold as the final results. As the data is limited, even only two samples for some subjects, the final MER results will be greatly improved by regarding the test data as the validation data. \nAccording to , compared to the experiments based on the same epoch on all of the folds, the results can be increased by more than 10$\\%$ by testing on the test datasets. \nBut, in practice, the test data is unknown and it is not reasonable to reserve the best epoch results on each fold of the test data as the final results. \n\\begin{table*}\n\\renewcommand{\\arraystretch}{1.3}\n\\centering \n\\small\n\\caption{MER on the Composite dataset (MECG2019)}\n\\scalebox{0.80}{ \n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}\n \\bottomrule[1.5pt]\nMethod&Year&Pre-p.&Input& Network architecture& Block &Pre-train & Protocol&Cate.& UF1&UAR\\\\ \\bottomrule[1.5pt]\nNMER ~ &2019&E, R & OF &CNN+DA&-& -&LOSO&3&0.7885&0.7824   \\\\\\hline\nDual-Inception~ &2019&- &OF &2S-CNN&Inception &-&LOSO&3&0.7322& 0.7278\\\\\\hline\nICE-GAN~ &2020& GAN & Apex &CNN+GAN&Capsule &ImageNet &LOSO&3&0.845&0.841\\\\\\hline\nMTMNet  &2020 &- & Onset-Apex &\\textcolor{black}{2S-CNN}+GAN+DA  &RES& \\makecell[c]{CK+ ,MMI , \\\\ Oulu-CASIA }&LOSO&3&0.864&0.857\\\\\\hline\nFR  &2021&- & OF &\\textcolor{black}{2S-CNN}+ML & Inception & -&LOSO&3&0.7838&0.7832\\\\\\hline\nMiMaNet &2021&T& Apex+sequence &\\textcolor{black}{2S-CNN}+DA & RES& CK+ ,MMI&LOSO&3&0.883&0.876\\\\\\hline\nGRAPH-AU  &2021&L& Apex &\\textcolor{black}{2S-CNN+GCN} & \\makecell[c]{Graph, \\\\ Transformer }& -&LOSO&3&0.7914&0.7933\\\\\\hline\nBDCNN  &2022&L& OF &\\textcolor{black}{4S-CNN} & \\makecell[c]{-}&\n-&LOSO&3&0.8509&0.8500\n\\\\ \\bottomrule[1.5pt]\n\\end{tabular}\n}\n{\\raggedright $^1$   Pre-p.: Pre-processing;  E: EVM; R: RoI; T: Temporal normalization ; L: Learning-based magnification. \\par \n$^2$ OF: Optical flow; DI: Dynamic image.\\par\n$^3$  nS-CNN: n-stream CNN; ML: Multi-task learning; DA: Domain adaption; KD: Knowledge distillation\\par\n$^4$  Cate.: Category; RES: Residual block.\\par\n }\n \\label{tab:result_MER2019}\n\\end{table*}\nTables~\\ref{tab:result_CASME} and~\\ref{tab:result_MER2019} list the reported performance of representative recent work of DL-based MER on popular ME datasets. As we discussed before, the evaluation protocol is varying and the practical training rule of each paper is ambiguous, we can not directly make a conclusion that which method performs best for MER. But, from the experimental results, the general trends of MER can be found. \nFor the input, in general, the combined inputs can provide promising results on all of the datasets . This is because the different input modalities can contribute information from different views. On the basis of various input modalities, we can explore useful information on limited ME samples to the greatest extent. Since the combined inputs is a good choice for robust MER, the multi-stream network is recommended to learn effective representations from various inputs . In contrast to the combined inputs, the sole sequence performs worse , due to the limited information and redundancy. \nBesides, from Tables~\\ref{tab:result_CASME} and~\\ref{tab:result_MER2019}, it can be seen that the learning strategy including fine-tuning , domain adaptation  and knowledge distillation  can achieve state-of-the-art results on both the individual datasets and the composite dataset.  This could be explained that the limited ME sample is the main challenge for MER and leveraging other related data sources is a reasonable and effective solution. In the future, domain adaption and knowledge distillation should be further researched to boost MER performance. \nIn some latest works , the GCN becomes a mainstream choice for MER and shows promising performance. Currently, the spatio-temporal graph representation combined with GCNs obtains more attention in MER studies. The possible reason is that the landmark and AU information are helpful and effective for locating and representing the facial muscle movements. However, the small-sample ME datasets limit the ability of graph representation. The combination of transfer learning and graph should be a promising direction for future ME studies.", "cites": [7991], "cite_extract_rate": 0.05263157894736842, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by integrating evaluation protocols from various DL-based MER papers and explaining their implications. It offers critical analysis by pointing out limitations of certain protocols like LOVO and the questionable practice of using test data for validation. The abstraction is also strong, identifying general trends in input modalities, learning strategies, and the future potential of combining graph and transfer learning in the field."}}
{"id": "0a72e204-d8d4-4dbb-939e-1f979faadec6", "title": "AU analysis in MEs", "level": "subsection", "subsections": [], "parent_id": "ec410a7e-5af8-43d7-8be1-342d6d8baee1", "prefix_titles": [["title", "Deep Learning for Micro-expression Recognition:  A Survey"], ["section", "Challenges AND FUTURE DIRECTIONS"], ["subsection", "AU analysis in MEs"]], "content": "MEs reveal people's hidden emotions in high-stake situations~ and have various applications such as clinic diagnosis and national security. However, ME interpretation suffers ambiguities ~, \\eg, the inner brow raiser may refer to surprise or sad.  The FACS  has been verified to be effective for resolving the ambiguity issue. In FACS, action units (AUs) are defined as the basic facial movements, working as the building blocks to formulate multiple FEs~. \nFurthermore, the criteria for AU and FE correspondence is defined in FACS manual. Encoding AUs has been verified to  benefit the MER~ through embedding AU features. \nIn the future, the relationship between AUs and MEs can be further explored to improve MER.", "cites": [8946, 5759], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes the concept of AU analysis from the cited papers by linking FACS to the challenges of ME interpretation, but the integration is limited and lacks a deeper, novel framework. It briefly touches on the importance of AUs and their role in improving MER, but does not critically evaluate the cited works or identify limitations. The abstraction is minimal, focusing on a general observation rather than overarching principles or trends in AU analysis for MER."}}
{"id": "ec7e2b26-ea40-4fa1-90b1-0607cf649e53", "title": "Ethical considerations", "level": "section", "subsections": [], "parent_id": "c226fd34-5a4b-426b-a6d8-50438f783d8d", "prefix_titles": [["title", "Deep Learning for Micro-expression Recognition:  A Survey"], ["section", "Ethical considerations"]], "content": "As discussed above, MEs can help reveal people’s hidden feelings in high-stake situations and have practical applications in various fields, such as medical treatment and interrogations. MER, like many other computer vision and machine learning tasks,  could be misused, especially when used in surveillance with predatory data collection practices  Therefore, ethical issues should be considered.  \n }\n\\textcolor{black}{\nPrivacy and data protection is the primarily and frequently discussed ethical issue in machine learning. For MER, the critical privacy concern is the privacy of personal data. Currently, data protection laws are well established to regulate data privacy, for example, the EU General Data Protection Regulation (GDPR) . The legislation defined rules for the protection of personal data, including international data protection agreements, privacy shields, transfer of participant names, record data, etc. In the research community, consent forms concerning data collection, processing, and sharing need to be signed when collecting ME data. In practical applications, consent forms should also be considered to regulate the usage, as people’s faces are present in the recorded images/videos with sensitive and biometric information that may be misused beyond the intended purpose.  Pilot studies aim to remove sensitive information like identity while preserving facial properties , which could be further explored in MER.}\n\\textcolor{black}{\nMoreover, questions of reliability in MER systems are further pointed out together with privacy and data protection . Results of a deep learning-based MER system usually depend on the quality of training data, which are difficult to ascertain because of possible data biases. Transparency of data and models should be aware and well-studied.}", "cites": [5761, 5760], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from two cited papers to discuss privacy and reliability concerns in MER, showing a moderate level of integration. It introduces broader ethical themes like data protection and system reliability, but lacks deeper critical evaluation or comparison of the cited works. While it abstracts some concerns to a general level, the analysis remains somewhat surface-level without identifying major limitations or proposing a novel ethical framework."}}
