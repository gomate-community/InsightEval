{"id": "9ccbcfac-47e2-47c4-b6a6-daa7425f2e36", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "2f09dfe5-fc96-4d60-bf8a-870e95820098", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "Introduction"]], "content": "\\IEEEPARstart{O}{bject} detection~ is a fundamental and challenging task that aims to locate and classify object instances in an image. The object localization is to use a bounding box (an axis-aligned rectangle tightly bounding the object) to search for the spatial location and range of an object in an image as much as possible~. Object classification is to assess the presence of objects from a given set of object classes in an image. As one of the most fundamental tasks in computer vision, object detection is an indispensable technique for many high-level applications, \\eg, robot vision~, face recognition~, image retrieval~, augmented reality~, autonomous driving~, change detection~ and so on. With the development of convolutional neural networks (CNNs) in visual recognition~ and release of large scale dateset~, today's state-of-the-art object detector can achieve near-perfect performance under fully-supervised setting, \\ie, Fully-Supervised Object Detection (FSOD)~. Unfortunately, these fully-supervised object detection methods suffer from two inevitable limitations: 1) The large-scale instance annotations are difficult to obtain and labor-intensive. 2) When labeling these data, they may introduce annotation noises inadvertently.\nTo avoid the mentioned problems, the community starts to solve object detection in a weakly-supervised setting, \\ie, Weakly-Supervised Object Detection (WSOD). Different from the fully-supervised setting (cf. Fig.~\\ref{fsod_vs_wsod} (a)), WSOD aims to detect instances with only image-level labels (\\eg, categories of instances in the whole images). Meanwhile, WSOD can benefit from the large-scale datasets on the web, such as Facebook and Twitter. Another similar task is Weakly-Supervised Object Localization (WSOL), which only detects one instance in an image. Because WSOD and WSOL detect multiple instances and single instances respectively, we consider WSOL as a sub-task of WSOD. In the following paper, we use WSOD to represent both WSOD and WSOL. \n\\begin{figure}[t]\n  \\begin{center}\n      \\includegraphics[width=0.85\\linewidth]{./images/fsod_vs_wsod.pdf}\n  \\end{center}\n  \\caption{(a) Fully-Supervised Object Detection (FSOD) uses the \\emph{instance-level} annotations as supervision. (b) Weakly-Supervised Object Detection (WSOD) uses the \\emph{image-level} annotations as supervision.}\n  \\label{fsod_vs_wsod}\n\\end{figure}\n\\begin{figure*}[t]\n  \\begin{center}\n      \\includegraphics[width=0.9\\linewidth]{./images/sections_figure2.pdf}\n  \\end{center}\n  \\caption{The main content of this paper.}\n  \\label{sections_figure}\n\\end{figure*}\nIn this paper, we go over all typical WSOD methods and give a comprehensive survey (cf. Fig.~\\ref{sections_figure}) of recent advances in WSOD. Since the number of papers on WSOD is breathtaking, we sincerely apologize to those authors whose research on WSOD and other related fields are not included in this survey. In Section~\\ref{sec:wsod}, we introduce the background, main challenges, and basic framework. In Section~\\ref{sec:milestones}, according to the development timeline of WSOD, we introduce several modern classical methods in detail. Then, in-depth analyses are provided towards the all advanced techniques and tricks for the main challenges. In Section~\\ref{sec:datasets}, we demonstrate all prevailing benchmarks and standard evaluation metrics for WSOD. In Section~\\ref{sec:directions}, we briefly discuss the future directions.", "cites": [514, 799, 1534, 305, 206, 486, 2933, 802, 895, 2934, 97, 209, 1230, 8607, 520], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 21, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The introduction section primarily serves as a descriptive overview of the field of deep learning for object detection and localization, listing key concepts and mentioning several cited papers. However, it does not synthesize their contributions into a coherent narrative, nor does it offer critical evaluation or abstraction beyond the specific systems. It sets the context but lacks deeper insight or analysis."}}
{"id": "b6a2ddd6-3dbc-4d11-aa38-e3ad1025e91f", "title": "A Problem Definition", "level": "subsection", "subsections": [], "parent_id": "9e1fbaf4-764f-4115-b463-68f479f804c4", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "WSOD"], ["subsection", "A Problem Definition"]], "content": "\\blue{WSOD aims to classify and locate object instances using only image-level labels in the training phase. As shown in Fig.~\\ref{fsod_vs_wsod} (b), given an image with cat and dog, WSOD not only classifies the cat and dog but also locates their location using bounding boxes. Different from FSOD that can use instance-level annotations in the training phase shown in Fig.~\\ref{fsod_vs_wsod} (a), WSOD only accesses image-level labels. Because of this restriction, though hundreds of WSOD methods have been proposed, the performance gap between WSOD and FSOD is still large. For example, the mAP of state-of-the-art FSOD approach~ and WSOD approach~ is 86.9\\% and 54.9\\% on PASCAL VOC 2007 dataset~, respectively. Therefore, there are still many challenges in terms of the task of WSOD for researchers to solve, especially in the direction of improving the detection performance.}\n\\addtolength{\\tabcolsep}{-3pt}\n\\begin{table*}[t]\n  \\centering\n  \\caption{A summary of the state-of-the-art WSOD methods. For the proposals, SS represents selective search, EB represents edge boxes, and SW represents sliding window. The Challenges denotes the main contributions of corresponding papers.}\n      \\label{table08}\n      \\begin{tabular}{lcccccccc}\n      \\toprule\n      \\multirow{2}*{Approach}&\n      \\multirow{2}*{Year} & \n      \\multirow{2}*{Proposals} & \n      \\multicolumn{2}{c}{Network}& \n      \\multicolumn{3}{c}{Challenges} &\n      \\multirow{2}*{Code on Github} \\\\\n      \\cmidrule (r){4-5} \\cmidrule (r){6-8}\n      &&&MIL-based&CAM-based&Discriminative Region&Multiple Instances&Speed&\\\\\n      \\midrule\n      \\rowcolor{mygray}\n      WSDDN~&CVPR2016&EB&$\\surd$&&&&&hbilen/WSDDN\\\\\n      CAM~&CVPR2016&Heatmap&&$\\surd$&&&$\\surd$&zhoubolei/CAM\\\\\n      \\rowcolor{mygray}\n      WSLPDA~&CVPR2016&EB&$\\surd$&&$\\surd$&&&jbhuang0604/WSL\\\\\n      WELDON~&CVPR2016&SW&$\\surd$&&$\\surd$&&$\\surd$&\\\\\n      \\rowcolor{mygray}\n      ContextLocNet~&ECCV2016&SS&$\\surd$&&$\\surd$&&&vadimkantorov/contextlocnet\\\\\n      Grad-CAM~&ICCV2017&Heatmap&&$\\surd$&$\\surd$&&$\\surd$&ramprs/grad-cam\\\\\n      \\rowcolor{mygray}\n      OICR~&CVPR2017&SS&$\\surd$&&$\\surd$&&&ppengtang/oicr\\\\\n      WCCN~&CVPR2017&EB&$\\surd$&&$\\surd$&&&\\\\\n      \\rowcolor{mygray}\n      ST-WSL~&CVPR2017&EB&$\\surd$&&$\\surd$&$\\surd$&&\\\\\n      WILDCAT~&CVPR2017&Heatmap&&$\\surd$&$\\surd$&&$\\surd$&durandtibo/wildcat.pytorch\\\\\n      \\rowcolor{mygray}\n      SPN~&ICCV2017&SW&$\\surd$&&&&$\\surd$&ZhouYanzhao/SPN\\\\\n      TP-WSL~&ICCV2017&Heatmap&&$\\surd$&$\\surd$&&$\\surd$&\\\\\n      \\rowcolor{mygray}\n      PCL~&TPAMI2018&SS&$\\surd$&&$\\surd$&$\\surd$&&ppengtang/pcl.pytorch\\\\\n      GAL-fWSD~&CVPR2018&EB&$\\surd$&&&&$\\surd$&\\\\\n      \\rowcolor{mygray}\n      W2F~&CVPR2018&SS&$\\surd$&&$\\surd$&$\\surd$&$\\surd$&\\\\\n      ACoL~&CVPR2018&Heatmap&&$\\surd$&$\\surd$&&$\\surd$&xiaomengyc/ACoL\\\\\n      \\rowcolor{mygray}\n      ZLDN~&CVPR2018&EB&$\\surd$&&$\\surd$&&&\\\\\n      TS$^2$C~&ECCV2018&SS&$\\surd$&&$\\surd$&&&\\\\\n      \\rowcolor{mygray}\n      SPG~&ECCV2018&Heatmap&&$\\surd$&&&$\\surd$&xiaomengyc/SPG\\\\\n      WSRPN~&ECCV2018&EB&$\\surd$&&&&&\\\\\n      \\rowcolor{mygray}\n      C-MIL~&CVPR2019&SS&$\\surd$&&&&&WanFang13/C-MIL\\\\\n      WS-JDS~&CVPR2019&EB&$\\surd$&&$\\surd$&&&shenyunhang/WS-JDS\\\\\n      \\rowcolor{mygray}\n      ADL~&CVPR2019&Heatmap&&$\\surd$&&&$\\surd$&junsukchoe/ADL\\\\\n      Pred NET~&CVPR2019&SS&$\\surd$&&&&&\\\\\n      \\rowcolor{mygray}\n      WSOD2~&ICCV2019&SS&$\\surd$&&$\\surd$&&&researchmm/WSOD2\\\\\n      OAILWSD~&ICCV2019&SS&$\\surd$&&$\\surd$&&&\\\\\n      \\rowcolor{mygray}\n      TPWSD~&ICCV2019&SS&$\\surd$&&$\\surd$&&&\\\\\n      SDCN~&ICCV2019&SS&$\\surd$&&$\\surd$&&&\\\\\n      \\rowcolor{mygray}\n      C-MIDN~&ICCV2019&SS&$\\surd$&&$\\surd$&&&\\\\\n      DANet~&ICCV2019&Heatmap&&$\\surd$&&&$\\surd$&xuehaolan/DANet\\\\\n      \\rowcolor{mygray}\n      NL-CCAM~&WACV2020&Heatmap&&$\\surd$&$\\surd$&&$\\surd$&Yangseung/NL-CCAM\\\\\n      ICMWSD~&CVPR2020&SS&$\\surd$&&$\\surd$&&&\\\\\n      \\rowcolor{mygray}\n      EIL~&CVPR2020&Heatmap&&$\\surd$&$\\surd$&&$\\surd$&Wayne-Mai/EIL\\\\\n      SLV~&CVPR2020&SS&$\\surd$&&$\\surd$&&&\\\\\n      \\bottomrule\n      \\end{tabular}\n\\end{table*}\n\\addtolength{\\tabcolsep}{3pt}", "cites": [2282, 2291, 2306, 2936, 7089, 2294, 2287, 2259, 7090, 2275, 2296, 2283, 2286, 7637, 2937, 737, 2935, 2304, 2302, 2295, 2280, 2284, 2305], "cite_extract_rate": 0.6388888888888888, "origin_cites_number": 36, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a clear problem definition for WSOD and includes a descriptive table listing various methods with their attributes. However, it largely functions as a factual summary of the cited works without substantial synthesis of ideas or critical evaluation. There is minimal abstraction beyond specific methods, and the narrative remains focused on listing approaches rather than analyzing overarching trends or principles."}}
{"id": "cf380f17-be2b-4358-b139-2b6daf64a9e5", "title": "Main Challenges", "level": "subsection", "subsections": [], "parent_id": "9e1fbaf4-764f-4115-b463-68f479f804c4", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "WSOD"], ["subsection", "Main Challenges"]], "content": "\\label{sec:challenge}\nThe main challenges of WSOD come from two aspects: localization accuracy and speed. For localization accuracy, it consists of discriminative region problem and multiple instances with the same category problem. For speed, it is an important characteristic of real applications. In TABLE~\\ref{table08}, we summarize all typical WSOD methods and their contributions to these challenges.\n\\textbf{Discriminative Region Problem.} It is that detectors~ tend to focus on the most discriminative parts of the object. During training, there may exist more than one proposal around an object, and the most discriminative part region of the object is likely to have the highest score (\\eg, the region A is the most discriminative region in Fig.~\\ref{OICR_comparison} (left) and it has a higher score than that of other regions). If the model selects positive proposals only based on scores, it is easy to focus on the most discriminative part of the object rather than the whole object extent.\n\\begin{figure}[t]\n    \\begin{center}\n        \\includegraphics[width=0.85\\linewidth]{./images/OICR_comparison}\n    \\end{center}\n    \\caption{Detection results between model without classifier refinement (left) and model with classifier refinement (right). The figure comes from~.} \n    \\label{OICR_comparison}\n\\end{figure}\n\\textbf{Multiple Instance Problem.} It denotes that detectors~ are difficult to accurately recognize multiple instances when there may exist several objects with the same category in an image. Although there are multiple instances with the same category in an image, these detectors~ only select the highest score proposal of each category as the positive proposal and ignores other possible instance proposals.\n\\textbf{Speed Problem.} At present, the speed bottleneck of the WSOD approaches is mainly concentrated in proposal generation. Selective search (SS)~ and Edge boxes (EB)~ that are widely used in WSOD are too time-consuming.", "cites": [2259, 737, 2296], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section briefly describes the main challenges of WSOD (localization accuracy, multiple instances, and speed) and references three papers that have contributed to these challenges. However, it lacks deep synthesis of the cited works into a broader narrative, and the critical analysis is minimal, with no substantial evaluation of methods or identification of limitations. Some abstraction is attempted by categorizing issues, but it remains at a high-level overview without deeper conceptual generalization."}}
{"id": "f071d696-05af-451b-9c94-54e3300f6807", "title": "MIL-based Network", "level": "subsubsection", "subsections": [], "parent_id": "f5e50a1e-ba56-4da2-80ee-6c686c039fa9", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "WSOD"], ["subsection", "Basic WSOD Framework"], ["subsubsection", "MIL-based Network"]], "content": "\\label{sec:mil-based}\n\\blue{When the detection network predicts multiple instances in an image, it is considered a Multiple Instance Learning (MIL) problem~. Taking Fig.~\\ref{fsod_vs_wsod} (b) for example, an image is interpreted as a bag of proposals in the MIL problem. If the image is labeled cat, it means that at least one of the proposals tightly contains the cat instance. Otherwise, all of the regions do not contain the cat instance (likewise for dogs). The MIL-based network is based on the structure of WSDDN~ that consists of three components: proposal generator, backbone, and detection head. }\n\\textbf{Proposal Generator.} Numerous proposal generators are usually used in MIL-based networks. 1) \\textit{Selective search (SS)}~: it leverages the advantages of both exhaustive search and segmentation to generate initial proposals. 2) \\textit{Edge boxes (EB)}~: it uses object edges to generate proposals and is widely used in many approaches~. 3) \\textit{Sliding window (SW)}: it denotes that each point of the feature maps corresponds to one or more proposals in the relative position of the original image. And SW is faster than SS~ and EB~ in proposal generation. \n\\textbf{Backbone.} \\blue{With the development of CNNs and large scale datasets (\\eg, ImageNet~), the pretrained AlexNet~, VGG16~, GoogLeNet~, ResNet~, and SENet~ are prevailing feature representation networks for both classification and object detection.}\n\\textbf{Detection Head.} \\blue{It includes a classification stream and a localization stream. The classification stream predicts class scores for each proposal, and the localization stream predicts every proposal's existing probability score for each class. Then the two scores are aggregated to predict the confidence scores of an image as a whole, which are used to inject image-level supervision in learning.}\n\\blue{Given an image, we first feed it into the proposal generator and backbone to generate proposals and feature maps, respectively. Then, the feature maps and proposals are forwarded into a spatial pyramid pooling (SPP)~ layer to generate fixed-size regions. Finally, these regions are fed into the detection head to classify and localize object instances.}", "cites": [2259, 2282, 305, 2306, 97, 514, 2275, 509, 895], "cite_extract_rate": 0.5294117647058824, "origin_cites_number": 17, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic description of the MIL-based network framework, integrating a few key components from cited papers such as proposal generation, backbone CNNs, and detection heads. However, it lacks deeper synthesis or critical evaluation of the methods, and while it mentions general patterns (e.g., the role of SPP layers), it does not abstract these into broader principles or trends in the field."}}
{"id": "1ead24ef-1d81-47d2-b27d-95e21d77be3f", "title": "CAM-based Network", "level": "subsubsection", "subsections": [], "parent_id": "f5e50a1e-ba56-4da2-80ee-6c686c039fa9", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "WSOD"], ["subsection", "Basic WSOD Framework"], ["subsubsection", "CAM-based Network"]], "content": "\\label{sec:cam-based}\n\\blue{When the detection network only predicts a single instance in an image, it is considered an object localization problem. The CAM-based network is based on the structure of CAM~, which consists of three components: backbone, classifier, and class activation maps.}\n\\textbf{Backbone.} \\blue{It is similar to that of the MIL-based network introduced in Section~\\ref{sec:mil-based}.}\n\\textbf{Classifier.} \\blue{It is designed to classify the classes of an image, which includes a global average pooling (GAP) layer and a fully connected layer. }\n\\textbf{Class Activation Maps.} \\blue{It is responsible for locating object instances by using a simple segmentation technique. Because the class activation maps are produced by matrix multiplying the weight of the fully connected layer to the feature maps of the last convolutional layer, it spotlights the class-specific discriminative regions in every activation map. Therefore, it is easy to generate bounding boxes of every class by segmenting the activation map of the class. }\n\\blue{Given an image, we first feed it into the backbone to generate feature maps of this image. Then, the feature maps are forwarded into the classifier to classify the image's classes. Meanwhile, we matrix multiply the weight of the fully connected layer to the feature maps of the last convolutional layer to produce class activation maps. Finally, we segment the activation map of the highest probability class to yield bounding boxes for object localization.}", "cites": [737], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the CAM-based network, referencing one paper for the concept of using global average pooling for localization. However, it lacks synthesis of multiple sources, critical evaluation of the methods, and broader abstraction beyond the described components. The content is primarily a factual summary without deeper insights or comparative analysis."}}
{"id": "600b68e9-3062-4d34-b822-403fb271cf19", "title": "MIL-based Methods", "level": "subsection", "subsections": [], "parent_id": "57f26c22-b2e9-4b99-a5c9-6b9157c32986", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "Milestones of WSOD"], ["subsection", "MIL-based Methods"]], "content": "\\label{sec:mil-based methods}\n\\textbf{WSDDN.} \\blue{The biggest contribution of WSDDN~ is using two streams network, which aims to perform classification and localization respectively. WSDDN first uses a SPP~ on the top of the feature maps and generates a feature vector after two fully connected layer procedures. Next, the feature vector is fed into the classification stream and localization steam. Specifically, the classification stream is responsible for computing the class scores of each region, and the localization stream is designed to compute every region's existing probability for each class. Then, the matrix product of the class scores of each region and the existing probability for each class is considered as the final prediction scores. However, because of only accessing image-level labels in the training phase, the most discriminative part of the object will be paid more attention than the whole object instance in training. Due to the above limitation, WSDDN suffers from the discriminative region problem.}\n\\textbf{OICR.} \\blue{To alleviate the discriminative region problem, OICR~ uses WSDDN as its baseline and adds three instance classifier refinement procedures after the baseline. Every instance classifier refinement procedure, which consists of two fully connected layers, is designed to further predict the class scores for each proposal. Because the output of each instance classifier refinement procedure is the supervision of its latter refinement procedure, OICR can continue to learn so that larger area can have higher scores than WSDDN. Although the prediction of WSDDN may only focus on the discriminative part of the object, it will be refined after several instance classifier refinement procedures. }\n\\textbf{SDCN.} SDCN~ introduces a segmentation-detection collaborative mechanism. It consists of a detection branch and segmentation branch, which are responsible for detecting bounding boxes and generating segmentation masks respectively. In SDCN, the detection results will be converted to a heatmap by setting a classification score to all pixels within each proposal as the supervision mask of the segmentation branch. Meanwhile, the proposals of the highest overlap with the connected regions from the segmentation masks will be the pseudo ground-truth boxes of the detection branch. Both detection branch and segmentation branch are optimized alternatively and promoted each other, so SDCN achieves better detection performance than OICR.\n\\textbf{ICMWSD.} Different from SDCN which uses object detection with segmentation collaboration mechanism, ICMWSD~ addresses the problem of focusing on the most discriminative part of an object by leveraging context information. Firstly, ICMWSD obtains a dropped features by dropping the most discriminative parts. Then, maximizing the loss of the dropped features to force ICMWSD to look at the surrounding context regions.", "cites": [2259, 2937, 7090, 2296, 509], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key contributions of multiple MIL-based WSOD methods (WSDDN, OICR, SDCN, ICMWSD) and explains how each addresses the problem of focusing on discriminative parts. It provides a clear progression of ideas, showing how later methods refine earlier ones. Critical analysis is evident in pointing out the limitations (e.g., the discriminative region problem in WSDDN) and how subsequent approaches attempt to resolve them. While some abstraction is present in framing the evolution of ideas, broader meta-level insights or unifying principles are less emphasized."}}
{"id": "f31a26c2-85c7-4f0b-b802-55b4548075ab", "title": "CAM-based Methods", "level": "subsection", "subsections": [], "parent_id": "57f26c22-b2e9-4b99-a5c9-6b9157c32986", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "Milestones of WSOD"], ["subsection", "CAM-based Methods"]], "content": "\\textbf{CAM.} The biggest contribution of CAM~ is using class activation maps to predict instances. CAM firstly leverages a GAP layer on the last convolutional feature maps to generate a feature vector. \\blue{Then, the feature vector is fed into a classifier with a fully connected layer to generate prediction scores of an image.} Finally, CAM generates bounding boxes of each class by using a simple thresholding technique to segment the activation map of every class. \\blue{However, class activation maps of CAM spotlight the regions that are the most discriminative parts of the object, so CAM also faces the discriminative region problem as WSDDN.}\n\\textbf{WCCN.} To alleviate the discriminative region problem, WCCN~ proposes to use a cascaded network that has three cascade stages trained in an end-to-end pipeline. \\blue{The first stage is the CAM~ network that aims to generate class activation maps and initial proposals. The second stage is a segmentation network that uses the class activation maps to train object segmentation for refining object localization. The final stage is a MIL network that performs multiple instances of learning on proposals extracted in the second stage. Because the second and third stages refine object localization, WCCN alleviates the problem that tends to focus on the most discriminative part of the object.}\n\\textbf{ACoL.} To alleviate the discriminative region problem, ACoL~ introduces two parallel-classifiers for object localization using adversarial complementary learning. Specifically, it first leverages the first classifier to localize the most discriminative regions. Then, ACoL uses the masked feature maps by masking the most discriminative regions discovered in the first classifier as the input feature maps of the second classifier. This forces the second classifier to select the next discriminative regions. Finally, ACoL fuses the class activation maps of both classifiers to generate bounding boxes of every class by segmenting the activation map of the highest probability class.", "cites": [2282, 2280, 737], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the key ideas from the three cited papers, integrating them into a narrative about the evolution of CAM-based methods and their approaches to the discriminative region problem. It provides some critical analysis by noting the limitations of CAM and how subsequent methods like WCCN and ACoL address them. While it identifies a common issue across these methods, it does not elevate the discussion to a meta-level or propose a novel conceptual framework."}}
{"id": "cc91d332-8e08-4a84-9839-f3a6a101dc58", "title": "Context Modeling", "level": "subsection", "subsections": [], "parent_id": "e2bc9197-68e8-4c34-9892-4132e9e3f384", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "Specific Techniques for Discriminative Region Problem"], ["subsection", "Context Modeling"]], "content": "The context of one region is external information of this region, which can be obtained by masking the region of the feature maps with special numbers (\\eg, zero). There are two types of the strategy of using context modeling as follows.\n\\textbf{Strategy A.} It selects the regions that have a big gap between their scores and their contextual region's scores as positive proposals. For example, \\blue{WSLPDA~ first replaces the pixel values within one proposal with zero to obtain the contextual region. Then, WSLPDA compares the scores of proposals and their contextual region. If the gap between the two scores is large, it indicates that the proposal is likely positive. ContextLocNet~ subtracts the localization score of one proposal from the localization score of the external rectangle region of the proposal. Then, the subtraction is considered as the final localization score of the proposal. Similar to WSLPDA and ContextLocNet, TS$^2$C~ selects a positive proposal by comparing the mean objectness scores of the pixels in one proposal and its surrounding region. But to alleviate the impact of background pixels in the surrounding region, TS$^2$C computes the mean objectness scores only using pixels with large confidence values in the surrounding region.}\n\\textbf{Strategy B.} It selects positive proposals by leveraging the loss of context regions. For example, \\blue{OAILWSD~ believes that a proposal not tightly covers the object instance if the loss of the context feature maps of this proposal tends to decrease. Thus, OAILWSD first leverages the context classification loss to label regions. Then, it selects the top-scoring regions whose context class probabilities are low as positive proposals. ICMWSD~ first drops the most discriminative parts of the feature maps to obtain contextual feature maps. Then, it maximizes the loss of the contextual feature maps to force it focuses on the context regions.}", "cites": [7089, 7090, 2304, 2305], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes information from four different papers by categorizing their approaches into two distinct strategies (A and B) and explaining the common goal of improving detection via context modeling. It shows some critical analysis by noting how certain methods (e.g., TS2C) aim to alleviate the impact of background pixels. While it identifies patterns across the papers, it does not fully abstract these into overarching principles or frameworks."}}
{"id": "c90a1426-0fa6-428c-bef7-f7270e86482b", "title": "Inter-stream Self-training", "level": "subsubsection", "subsections": [], "parent_id": "e9087d2e-81e5-45fa-a18e-9311df94831e", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "Specific Techniques for Discriminative Region Problem"], ["subsection", "Self-training Algorithm"], ["subsubsection", "Inter-stream Self-training"]], "content": "OICR~ expects B, C, and D can inherit the class score of A to correctly localize objects in Fig.~\\ref{OICR_comparison} (right). So, OICR adds three refinement classifiers with two fully connected layers in WSDDN to address the issue shown in Fig.~\\ref{OICR_comparison} (left). Specifically, the supervision of the first refinement classifier is the output of WSDDN. As for other refinement classifiers, the supervision of the current refinement classifier is the output of its previous refinement classifier. Inspired by OICR, WSOD2~ consists of numerous classifiers. ICMWSD~ also inserts refinement streams in WSDDN, however, every refinement stream includes a classifier and a regressor. Besides, some approaches~ use OICR as their baseline.", "cites": [2305, 7090, 2296, 2287], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of inter-stream self-training techniques, referencing OICR, WSOD2, and ICMWSD, but does not deeply synthesize or connect the ideas across these papers. There is minimal critical analysis or evaluation of the methods' strengths and weaknesses, and no broader abstraction or meta-level insight into the underlying principles of inter-stream self-training."}}
{"id": "627794f6-b919-4f19-a73a-0af4d0885500", "title": "Inter-epoch Self-training", "level": "subsubsection", "subsections": [], "parent_id": "e9087d2e-81e5-45fa-a18e-9311df94831e", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "Specific Techniques for Discriminative Region Problem"], ["subsection", "Self-training Algorithm"], ["subsubsection", "Inter-epoch Self-training"]], "content": "\\blue{Self-Taught-WS~ uses relative improvement (RI) of the scores of each proposal of two adjacent epochs as a criterion for selecting the positive sample. Specifically, it chooses the proposals of the previous epoch whose intersection over union (IoU) $\\geq 0.5$ with the maximal RI proposal as the positive samples of the current epoch.}", "cites": [2275], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a factual description of the Inter-epoch Self-training method from the Self-Taught-WS paper, but it lacks synthesis with other works, critical evaluation, or abstraction to broader principles. It merely paraphrases the technique without contextualizing it within the broader WSOD literature or offering deeper insights."}}
{"id": "fe348b22-323b-4264-b4b5-36e4ad80a171", "title": "Cascaded Network", "level": "subsection", "subsections": [], "parent_id": "e2bc9197-68e8-4c34-9892-4132e9e3f384", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "Specific Techniques for Discriminative Region Problem"], ["subsection", "Cascaded Network"]], "content": "The cascaded network includes several stages and the supervision of the current stage is the output of the previous stage. \\blue{Such as WCCN~ and TS$^2$C~ consist of three stages. The first stage is the CAM module that is to generate initial proposals using class activation maps. The intermediate stage is the object segmentation module that is designed to refine initial proposals. The final stage is a multiple instance learning module that is responsible for detecting accurate objects.}", "cites": [2282, 2304], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of cascaded networks in the context of WSOD, mentioning two specific methods and their three-stage structure. It integrates some information from the cited papers but lacks deeper synthesis, critical evaluation, or abstraction. The discussion remains at a surface level without offering comparative insights or identifying broader trends."}}
{"id": "3b4da6c9-0073-45d7-89e5-d47c9711fbcb", "title": "Bounding Box Regression", "level": "subsection", "subsections": [], "parent_id": "e2bc9197-68e8-4c34-9892-4132e9e3f384", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "Specific Techniques for Discriminative Region Problem"], ["subsection", "Bounding Box Regression"]], "content": "Bounding box regression can improve object localization performance using instance-level annotations in the training phase, but the WSOD task only accesses image-level labels. To use bounding box regression for refining the initial proposals from SS~ or EB~, some approaches propose to yield high-quality pseudo ground-truth boxes as the supervision of bounding box regression.\nNow, numerous approaches~ include at least one of the bounding box regressors. The supervision of the regressor is the output of previous classifiers.", "cites": [2936, 7090, 2302, 2287], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly introduces the concept of bounding box regression in the context of WSOD but does so in a largely descriptive manner. It mentions that pseudo ground-truth boxes are used and that regressors are supervised by previous classifiers, yet it fails to synthesize deeper connections between the cited papers or critically evaluate their approaches. The content remains at a surface level without abstracting overarching patterns or principles."}}
{"id": "89989386-09fe-4531-8cec-bff75368388f", "title": "Discriminative Region Removal", "level": "subsection", "subsections": [], "parent_id": "e2bc9197-68e8-4c34-9892-4132e9e3f384", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "Specific Techniques for Discriminative Region Problem"], ["subsection", "Discriminative Region Removal"]], "content": "From Fig.~\\ref{OICR_comparison} (left), some researchers find that the highest score region only covers the most discriminative part of the object. To localize the whole object extent, masking the most discriminative part of the object is designed to force the detector to find the next discriminative region. \nTP-WSL~ is a two-phase learning network that detects the next discriminative regions by masking the most discriminative region. In the first phase, it yields class activation maps followed by masking the most discriminative region using a threshold among the activation map of the highest probability class. In the second phase, it multiplies the masked activation map by the feature maps of the second network to refine the feature maps for detecting the next discriminative regions.\n\\blue{Different from TP-WSL that has two backbones, ACoL~ consists of one shared backbone and two parallel-classifiers. The masked feature maps from the first classifier are fed into the second classifier to generate class activation maps. Finally, ACoL locates object instances in the fused activation maps by fusing the two-class activation maps of both classifiers. EIL~ proposes to share the weights of the two parallel-classifiers of ACoL, and it only segments the activation map of the highest probability class from the unmasked branch to yields object proposals. Comparing C-MIDN~ with ACoL, there are three differences. First, the detection network of C-MIDN is WSDDN~, but the detection network of ACoL is CAM~. Second, C-MIDN does not compute the loss of high overlap with the first detection module's top-scoring proposal in the second branch, but ACoL masks the first detection module's top-scoring proposal's region with zero in the second branch. Finally, C-MIDN chooses the top-scoring proposals of the second detection module and the top-scoring proposals of the first detection module with low overlap with selected proposals as positive proposals, but ACoL yields positive proposals by segmenting the fused class activation maps.}", "cites": [2259, 2280, 737, 2935], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear comparative overview of different approaches to discriminative region removal in weakly supervised object detection. It integrates key ideas from TP-WSL, ACoL, and C-MIDN, highlighting architectural and methodological differences. However, it does so in a mostly descriptive manner with limited critical evaluation or abstraction into broader principles."}}
{"id": "0144f94a-79d4-402a-a36c-b11a02b3ef8b", "title": "Incorporating Low-level Features", "level": "subsection", "subsections": [], "parent_id": "e2bc9197-68e8-4c34-9892-4132e9e3f384", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "Specific Techniques for Discriminative Region Problem"], ["subsection", "Incorporating Low-level Features"]], "content": "Low-level features usually retain richer object details, such as edges, corners, colors, pixels, and so on. We can obtain accurate object localization if making full use of these low-level features. For example, Grad-CAM~ leverages high-resolution Guided Backpropagation~ that highlights the image's details to create both high-resolution and class-discriminative visualizations. WSOD2~ first computes the score of a region proposal. Then, it selects the same region in low-level image features (\\eg, superpixels) and computes the score of this region. Finally, the product of the two scores is the final score of the region proposal.", "cites": [1812], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of two techniques (Grad-CAM and WSOD2) that incorporate low-level features for object localization. However, it lacks meaningful synthesis between the cited papers or broader trends in the field. There is no critical evaluation or comparison of the approaches, and the discussion remains at a concrete level without abstracting overarching principles or patterns."}}
{"id": "dadb3521-a095-4201-8ae2-60d36e7f5b37", "title": "Segmentation-detection Collaborative Mechanism", "level": "subsection", "subsections": [], "parent_id": "e2bc9197-68e8-4c34-9892-4132e9e3f384", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "Specific Techniques for Discriminative Region Problem"], ["subsection", "Segmentation-detection Collaborative Mechanism"]], "content": "Segmentation-detection collaborative mechanism includes a segmentation branch and a detection branch. The primary reasons for the collaborative mechanism are the following: 1) MIL (detection) can correctly distinguish an area as an object, but it is not good at detecting whether the area contains the entire object. 2) Segmentation can cover the entire object instance, but it cannot distinguish whether the area is a real object or not~. So, some models leverage deep cooperation between detection and segmentation by supervising each other to achieve accurate localization.\nWS-JDS~ first chooses the region proposals with top-scoring pixels generated by the semantic segmentation branch as the positive samples of the detection branch. Then, it sets the classification score to all pixels within each positive proposal of the detection branch as the supervision mask of the segmentation branch. \\blue{Similar to WS-JDS, SDCN~ also combines the detection branch with the segmentation branch which is introduced in Section~\\ref{sec:mil-based methods}.}", "cites": [2937], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of the cited papers by connecting the strengths and limitations of detection and segmentation in the context of weakly supervised object detection. It introduces a collaborative mechanism and briefly explains how two methods, WS-JDS and SDCN, utilize this idea. However, the critical analysis is limitedâ€”there is no in-depth evaluation of their effectiveness or limitations. The abstraction level is moderate, as it identifies a general trend of using segmentation and detection in tandem but does not offer a meta-level framework or principle."}}
{"id": "54ee603e-8a88-4025-9979-69342548efab", "title": "Transforming WSOD to FSOD", "level": "subsection", "subsections": [], "parent_id": "e2bc9197-68e8-4c34-9892-4132e9e3f384", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "Specific Techniques for Discriminative Region Problem"], ["subsection", "Transforming WSOD to FSOD"]], "content": "\\label{sec:wsod_fsod}\nTransforming WSOD to FSOD is another popular technique to achieve object detection using image-level labels, which is designed to train an FSOD model using the output of the WSOD model. The primary problem of transformation is to yield good pseudo ground-truth boxes from WSOD. There are several strategies to mine boxes as the pseudo ground-truth boxes. 1) \\textit{top score}: numerous approaches~ select top score detection boxes of WSOD as the pseudo ground-truth boxes. 2) \\textit{relative improvement (RI)}: ST-WSL~ selects the boxes with the maximal relative score improvement of two adjacent epochs as the pseudo ground-truth boxes. 3) \\textit{mergence}: W2F~ merges several small boxes into a big candidate box and uses these merged boxes as the pseudo ground-truth boxes for later training. SLV~ first merges the scores of several boxes to the pixels and then generates bounding boxes of each class by using a simple thresholding technique to segment the map of every class.\nIn addition, there are several FSOD models that have been used as follows: Fast R-CNN~, Faster R-CNN~, and SSD~. Numerous approaches~ use prediction boxes of WSOD as the pseudo ground-truth boxes to train Fast R-CNN. W2F~ uses prediction boxes of WSOD to train Faster R-CNN. GAL-fWSD~ uses prediction boxes of WSOD to train SSD.", "cites": [2936, 2937, 209, 2294, 2275, 2304, 2296, 802, 8429], "cite_extract_rate": 0.6923076923076923, "origin_cites_number": 13, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section describes several strategies for pseudo box generation and lists FSOD models used in the context of WSOD, but it largely summarizes these methods without connecting them into a broader conceptual framework. There is minimal critical evaluation or comparison of the approaches, and no attempt to generalize beyond the cited works to highlight overarching trends or principles."}}
{"id": "9e808fc8-51b0-437b-a79b-2beb0b2f1a82", "title": "Discussions", "level": "subsection", "subsections": [], "parent_id": "e2bc9197-68e8-4c34-9892-4132e9e3f384", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "Specific Techniques for Discriminative Region Problem"], ["subsection", "Discussions"]], "content": "\\blue{In the previous sections, we individually introduce several techniques that are commonly used to improve the detection performance by detailed listing numerous approaches. In this section, we will compare and discuss these techniques. }\n\\blue{Firstly, context modeling and discriminative region removal are two similar techniques. Context modeling is to calculate the scores of the proposal and its context region respectively. Then it chooses the positive proposal derives from the two scores. On the other hand, the discriminative region removal is to directly erases top-scoring regions by setting zero value in the feature maps of the first branch followed by feeding the erased feature maps into the second branch.}\n\\blue{Secondly, the self-training algorithm usually co-occurs with bounding box regression. Bounding box regression is responsible for refining the initial proposals from SS~ or EB~. And self-training algorithm is designed to refine the prediction result of the baseline. The core problem of both the self-training algorithm and bounding box regression is yielding good pseudo ground-truth boxes.}\n\\blue{Thirdly, the cascaded network and segmentation-detection collaborative mechanism are two similar techniques. They leverage the segmentation module to improve the performance of the object detection module. A cascaded network is a sequential structure that the previous module is responsible for training the latter module. However, the segmentation-detection collaborative mechanism is a circular structure that leverages deep cooperation between detection and segmentation supervising each other to achieve accurate localization.}\n\\blue{Finally, incorporating low-level features technique leverages the advantage of the high-resolution characteristics of low-level features to improve object localization. The key idea of transforming WSOD to FSOD technique is to make full use of the advantages of the network structure of the FSOD model (\\eg, Fast R-CNN~).}", "cites": [8429], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview by grouping and comparing different techniques, such as context modeling, discriminative region removal, self-training, and cascaded networks. It integrates ideas from cited works into a structured discussion, though it lacks in-depth critique or identification of overarching theoretical principles. The abstraction is moderate, as it highlights general strategies rather than just describing individual methods."}}
{"id": "74ea7ff3-69a1-4ddc-a744-5eb8ef38e5ce", "title": "Specific Techniques for Multiple Instance problem", "level": "section", "subsections": [], "parent_id": "2f09dfe5-fc96-4d60-bf8a-870e95820098", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "Specific Techniques for Multiple Instance problem"]], "content": "\\label{sec:tec_multiple}\n\\blue{In this section, we will introduce how to make full use of the spatial relationship of proposals to solve multiple instance problem introduced in Section~\\ref{sec:challenge}. Specifically, the two proposals that are far away from each other are likely to correspond to two object instances, while the two proposals with large overlap may correspond to the same object instance.}\n\\blue{ST-WSL~ leverages a graph to detect multiple instances with the same category in an image. It first chooses N top-scoring proposals of each positive class as the nodes of the graph. The edge between two nodes indicates a large overlap between them. Then it selects the greatest degree (number of connections to other nodes) nodes as positive proposals using Non-Maximum Suppression (NMS) algorithm ~. PCL~ introduces the proposal cluster to replace the proposal bag that includes all of the proposals of each category. PCL assigns the same label and spatially adjacent proposals to the same proposal cluster. If proposals do not overlap each other, they will be assigned in different proposal clusters. Then, PCL selects the highest score proposal from each proposal cluster as the positive proposal. W2F~ iteratively merges the highly overlapping proposals with top-scoring into big proposals. Finally, these big proposals are considered positive proposals.}", "cites": [2275, 2294], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of specific techniques used to address the multiple instance problem in weakly supervised object detection, referencing two key papers (PCL and W2F) and one unnamed method (ST-WSL). While it introduces a common theme of spatial relationships among proposals, it lacks deeper synthesis of ideas, does not critically evaluate the methods or their limitations, and offers minimal abstraction or generalization beyond the described techniques."}}
{"id": "80d0c9da-6233-48ab-bd98-ebb5dc2b3fd1", "title": "Specific Techniques for Speed Problem", "level": "section", "subsections": [], "parent_id": "2f09dfe5-fc96-4d60-bf8a-870e95820098", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "Specific Techniques for Speed Problem"]], "content": "In this section, we will introduce several advanced techniques for solving the speed problem introduced in Section~\\ref{sec:challenge}. The main reason for the slow speed is that the MIL-based method adopts SS~ or EB~ to generate a large number of initial proposals that most of which are negative.\nThe methods for improving speed can be broadly categorized into three groups: 1) \\textit{Transformation-based}~: these approaches use their prediction boxes as the pseudo ground-truth boxes to train Faster R-CNN~ or SSD~ and then use Faster R-CNN or SSD to infer images. 2) \\textit{Sliding-window-based}~: these approaches use the sliding window technique to quickly generate proposals by walking through every point on the feature map. 3) \\textit{Heatmap-based}~: these approaches segment the heatmap using a threshold to generate proposals to improve the speed of proposal generation.", "cites": [2291, 2280, 209, 737, 2284, 2935, 802, 2295, 2286], "cite_extract_rate": 0.5, "origin_cites_number": 18, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section lists three broad categories of speed-improvement techniques but merely assigns each category to certain papers without explicitly connecting or integrating them. There is minimal critical analysis of the approaches, and no overarching principles or meta-level insights are discussed, making the section largely descriptive."}}
{"id": "08e89b6d-afea-4b27-bd7a-52da1fd7729d", "title": "Easy-to-hard Strategy", "level": "subsection", "subsections": [], "parent_id": "ce7d5bf1-8d1b-41d8-aef3-9041d9f0eac1", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "Training Tricks"], ["subsection", "Easy-to-hard Strategy"]], "content": "Previous approaches~ use all of the images at once without a training sequence to train the detection model. The easy-to-hard strategy denotes that the model is trained by using the images with progressively increasing difficulty. In this way, the model can gain better detection results. For example, ZLDN~ first computes the difficulty scores of images. Then, all of the images are ranked in an ascending order based on the difficulty scores. Finally, ZLDN uses the images with increasing difficulty to progressively train themselves.", "cites": [2259, 2282, 2306, 737, 7089, 2935, 2296], "cite_extract_rate": 0.875, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic descriptive overview of the easy-to-hard strategy, mentioning one specific paper (ZLDN) in detail and listing others as 'previous approaches' or 'existing methods.' It lacks a deeper synthesis of how the cited papers relate to or influence one another, and offers minimal critical evaluation or abstraction beyond the specific examples."}}
{"id": "0adf3465-774a-41a4-ac25-00d063ca6407", "title": "Negative Evidence", "level": "subsection", "subsections": [], "parent_id": "ce7d5bf1-8d1b-41d8-aef3-9041d9f0eac1", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "Training Tricks"], ["subsection", "Negative Evidence"]], "content": "Negative evidence contains the low-scoring regions, activations, and activation maps. For example, WELDON~ uses the classification scores of the $k$ top-scoring proposals and the $m$ low-scoring proposals to generate the classification scores of the image by simply summing. WILDCAT~ leverages the $k^+$ highest probability activations and $k^-$ lowest probability activations of the activation map to generate the prediction score. NL-CCAM~ uses the lowest probability activation maps. Specifically, it first ranks all of the activation maps in a descending order based on the probability of every class. Then, it fuses these class activation maps using a specific combinational function into one map, which is segmented to predict object instances.", "cites": [2284], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of how three different papers (WELDON, WILDCAT, and NL-CCAM) use negative evidence, but it does so in a fragmented manner without clearly synthesizing their approaches into a broader narrative. There is minimal critical analysis or evaluation of their effectiveness or limitations. The section lacks abstraction and does not generalize the concept of negative evidence into broader principles or trends in the field."}}
{"id": "63c15766-8dd2-4067-a214-0290e264c274", "title": "Optimizing Smoothed Loss Functions", "level": "subsection", "subsections": [], "parent_id": "ce7d5bf1-8d1b-41d8-aef3-9041d9f0eac1", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "Training Tricks"], ["subsection", "Optimizing Smoothed Loss Functions"]], "content": "If the loss function of the model is non-convex, it tends to fall into sub-optimal and falsely localizes object parts while missing a full object extent during training~. So C-MIL~ replaces the non-convex loss function with a series of smoothed loss functions to alleviate the problem that the model tends to get stuck into local minima. At the beginning of training, C-MIL first performs the image classification task. During the training process, the loss function of C-MIL is slowly transformed from the convex image classification loss to the non-convex object detection loss function.", "cites": [2283], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes the concept of non-convex loss functions and introduces how C-MIL addresses this issue by using smoothed loss functions, showing basic integration of the cited paper. It provides some analytical insight into the problem of local minima in WSOD training. However, it lacks deeper critical evaluation or comparison with other methods and only slightly generalizes to broader optimization strategies in the field."}}
{"id": "0e25f43d-18cb-4250-9fdd-efdf225dd053", "title": "Datasets", "level": "subsection", "subsections": [], "parent_id": "1bfb95db-ecd4-4dab-be9d-ade21af94465", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "Datasets and Performance Evaluation"], ["subsection", "Datasets"]], "content": "Datasets play an important role in WSOD task. Most approaches of the WSOD use PASCAL VOC~, MSCOCO~, ILSVRC~, or CUB-200~ as training and test datasets.\n\\textbf{PASCAL VOC.} It includes 20 categories and tens of thousands of images with instance annotations. PASCAL VOC has several versions, such as PASCAL VOC 2007, 2010, and 2012. Specifically, PASCAL VOC 2007 consists of 2,501 training images, 2,510 validation images, and 4,092 test images. PASCAL VOC 2010 consists of 4,998 training images, 5,105 validation images, and 9,637 test images. PASCAL VOC 2012 consists of 5,717 training images, 5,823 validation images, and 10,991 test images.\n\\textbf{MSCOCO.} It is large-scale object detection, segmentation, and captioning dataset. MSCOCO has 80 object categories, 330K images ($>$200K labeled), 1.5 million object instances. In object detection, MSCOCO is as popular as PASCAL VOC datasets. Because MSCOCO has more images and categories than PASCAL VOC datasets, the difficulty of training on the MSCOCO dataset is higher than that of PASCAL VOC datasets.\n\\textbf{ILSVRC.} The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is a large-scale dataset. In ILSVRC, the model usually uses 200 fully labeled categories and 1,000 categories in object detection and object localization, respectively. ILSVRC has several versions, such as ILSVRC 2013, ILSVRC 2014, and ILSVRC 2016. Specifically, ILSVRC 2013 which is usually used in object detection has 12,125 images for training, 20,121 images for validation, and 40,152 images for testing. In addition, ILSVRC 2014 and 2016 inherit the ILSVRC 2012 dataset in object localization, which contains 1.2 million images of 1,000 categories in the training set. And ILSVRC 2012 dataset has 50,000 and 100,000 images with labels in the validation and test set, respectively.\n\\textbf{CUB-200.} Caltech-UCSD Birds 200 (CUB-200) contains 200 bird species which is a challenging image dataset. It focuses on the study of subordinate categorization. CUB-200-2011~ is an extended version of CUB-200, which adds many images for each category and labels new part localization annotations. CUB-200-2011 contains 5,994 images in the training set and 5,794 images in the test set.", "cites": [486, 895], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides factual descriptions of the datasets used in WSOD, including their sizes, categories, and splits. It cites two relevant papers but merely summarizes their contents without integrating or synthesizing insights across them. There is no critical evaluation of the datasets or their limitations, and no generalization to broader trends or principles in dataset usage for weakly supervised learning."}}
{"id": "80078116-a35b-49e5-88e4-dc487a92e339", "title": "Evaluation Metrics", "level": "subsection", "subsections": [], "parent_id": "1bfb95db-ecd4-4dab-be9d-ade21af94465", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "Datasets and Performance Evaluation"], ["subsection", "Evaluation Metrics"]], "content": "In the state-of-the-art WSOD approaches, there are three standard evaluation metrics: mAP, CorLoc, and top error.\n\\textbf{mAP (mean Average Precision).} Average Precision (AP) is usually used in image classification and object detection. It consists of precision and recall. If $tp$ denotes the number of the correct prediction samples among all of the positive samples, $fp$ denotes the number of the wrong prediction samples among all of the positive samples, and $fn$ denotes the number of the wrong prediction samples among all of the negative samples, precision and recall can be computed as\n\\begin{equation}\n    \\begin{aligned}\n    \\text{recall} & = tp / (tp + fn), \\\\\n    \\text{precision} &= tp / (tp + fp),\n    \\end{aligned}\n\\end{equation}\nwhere the correct prediction sample denotes IoU of the positive sample and its corresponding ground-truth box $\\geq$ 0.5. Meanwhile, the IoU is defined as\n\\begin{equation}\n    {\\rm IoU}(b, b^g) =  area(b \\cap b^g) / area(b \\cup b^g),\n\\end{equation}\nwhere $b$ denotes a prediction sample, $b^g$ denotes a corresponding ground-truth box, and $area$ denotes the region size of the intersection or union. The mAP is the mean of all of the class average precisions and is a final evaluation metric of performance on the test dataset.\n\\textbf{CorLoc (Correct Localization).} CorLoc denotes the percentage of images that exist at least one instance of the prediction boxes whose IoU $\\geq$ 50\\% with ground-truth boxes for every class in these images. CorLoc is a final evaluation metric of localization accuracy on the trainval dataset.\n\\textbf{Top Error.} \\blue{Top error consists of Top-1 classification error (1-err cls), Top-5 classification error (5-err cls), Top-1 localization error (1-err loc), and Top-5 localization error (5-err loc). Specifically, Top-1 classification error is equal to $1.0 - cls_1$, where $cls_1$ denotes the accuracy of the highest prediction score (likewise for Top-1 localization error). Top-5 classification error is equal to $1.0 - cls_5$, where $cls_5$ denotes that it counts as correct if one of the five predictions with the highest score is correct (likewise for Top-5 localization error). Numerous approaches~ use top error to evaluate the performance of the model.}", "cites": [2280, 737, 2284, 2295], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the three common evaluation metrics used in WSOD and WSOL without connecting them to the broader themes or insights from the cited papers. There is minimal synthesis or abstraction, and no critical analysis of the strengths, weaknesses, or comparative effectiveness of these metrics across the works. The content remains factual and lacks deeper analytical engagement."}}
{"id": "14b5b33f-b99a-4375-971a-7af70c7e8948", "title": "Experimental Results", "level": "subsection", "subsections": [], "parent_id": "1bfb95db-ecd4-4dab-be9d-ade21af94465", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "Datasets and Performance Evaluation"], ["subsection", "Experimental Results"]], "content": "\\noindent\\textbf{Results on Pascal VOC.} The results of state-of-the-art WSOD methods on datasets Pascal VOC 2007, 2010, and 2012 are shown in Table~\\ref{table_voc}. The WSOD methods with ``+FR\" denote that their initial predictions are fed into the Fast R-CNN~ and serve as pseudo ground-truth bounding box annotations, \\ie, these methods transform the WSOD into FSOD problems. From the results, we can observe the performance on all three Pascal VOC datasets have achieved unprecedented progress in recent few years (\\eg, mAP 52.1\\% in CVPR'20 vs. 29.1\\% in CVPR'16 on Pascal VOC 2012). Meanwhile, comparing the methods and their counterparts with Fast R-CNN (\\eg, OICR vs. OICR+FR), we can find the detection performance can be further improved by using this FSOD transforming strategy.\n\\textbf{Results on MSCOCO.} The results of state-of-the-art WSOD methods on dataset MSCOCO are shown in Table~\\ref{table_coco}. We only report the AP metric, and the AP$_{50}$ denotes that the IoU threshold is equal to $0.5$. Similarly, the performance on MSCOCO also doubled in the last few years (\\eg, AP$_{50}$ 11.5\\% vs. 24.8\\% in test set). Since MSCOCO contains more object categories than PASCAL VOC datasets, the results on MSCOCO are still far from satisfactory. However, the performance gains by transforming WSOD to FSOD is relative marginal (\\eg, 0.7\\% gains in AP for PCL model).\n\\textbf{Results on ILSVRC 2020 and CUB-200.} TABLE~\\ref{table_ILSVRC_CUB} summaries the object localization performance of state-of-the-art WSOD methods on these two datasets. Compared to the PASCAL VOC and MSCOCO, quite few WSOD methods have evaluated their performance on these two benchmarks. From TABLE~\\ref{table_ILSVRC_CUB}, we can find the performance gains are also significant (1-err cls 35.6\\% vs. 27.7\\% in ILSVRC 2012).\n\\addtolength{\\tabcolsep}{-4.5pt}  \n\\begin{table}[t]\n    \\centering\n    \\caption{The summary of detection results (mAp (\\%) and CorLoc (\\%)) of state-of-the-art WSOD methods on Pascal VOC 2007, 2010, and 2012 datasets. The FR means Fast R-CNN~.}\n    \\label{table_voc}\n        \\begin{tabular}{lcccccc}\n        \\toprule\n        \\multirow{2}*{Approach}&\n        \\multicolumn{2}{c}{2007}&\n        \\multicolumn{2}{c}{2010}&\n        \\multicolumn{2}{c}{2012}\\\\\n        \\cmidrule (r){2-3} \\cmidrule (r){4-5} \\cmidrule (r){6-7}\n        &mAP &CorLoc &mAP &CorLoc &mAP &CorLoc \\\\\n        \\midrule\n        WSDDN~$_{\\text{CVPR2016}}$&39.3&58.0&36.2&59.7&-&-\\\\\n        \\rowcolor{mygray}\n        WSLPDA~$_{\\text{CVPR2016}}$&39.5&52.4&30.7&-&29.1&-\\\\\n        ContextLocNet~$_{\\text{ECCV2016}}$&36.3&55.1&-&-&35.3&54.8\\\\\n        \\rowcolor{mygray}\n        OICR~$_{\\text{CVPR2017}}$&42.0&61.2&-&-&38.2&63.5\\\\\n        WCCN~$_{\\text{CVPR2017}}$&42.8&56.9&39.5&-&37.9&-\\\\\n        \\rowcolor{mygray}\n        ST-WSL~$_{\\text{CVPR2017}}$&41.7&56.1&-&-&39.0&58.8\\\\\n        SPN~$_{\\text{ICCV2017}}$&-&60.6&-&-&-&-\\\\\n        \\rowcolor{mygray}\n        TST~$_{\\text{ICCV2017}}$&34.5&60.8&-&-&-&-\\\\\n        PCL~$_{\\text{TPAMI2018}}$&45.8&63.0&-&-&41.6&65.0\\\\\n        \\rowcolor{mygray}\n        GAL-fWSD~$_{\\text{CVPR2018}}$&47.0&68.1&\\textbf{45.1}&\\textbf{68.3}&43.1&67.2\\\\\n        W2F~$_{\\text{CVPR2018}}$&52.4&70.3&-&-&47.8&69.4\\\\\n        \\rowcolor{mygray}\n        ZLDN~$_{\\text{CVPR2018}}$&47.6&61.2&-&-&42.9&61.5\\\\\n        MELM~$_{\\text{CVPR2018}}$&47.3&61.4&-&-&42.4&-\\\\\n        \\rowcolor{mygray}\n        TS$^2$C~$_{\\text{ECCV2018}}$&44.3&61.0&-&-&40.0&64.4\\\\\n        C-WSL~$_{\\text{ECCV2018}}$&45.6&63.3&-&-&43.0&64.9\\\\\n        \\rowcolor{mygray}\n        WSRPN~$_{\\text{ECCV2018}}$&47.9&66.9&-&-&43.4&67.2\\\\\n        C-MIL~$_{\\text{CVPR2019}}$&40.7&59.5&-&-&46.7&67.4\\\\\n        \\rowcolor{mygray}\n        WS-JDS~$_{\\text{CVPR2019}}$&45.6&64.5&39.9&63.1&39.1&63.5\\\\\n        Pred NET~$_{\\text{CVPR2019}}$&53.6&\\textbf{71.4}&-&-&49.5&70.2\\\\\n        \\rowcolor{mygray}\n        WSOD2~$_{\\text{ICCV2019}}$&53.6&69.5&-&-&47.2&\\textbf{71.9}\\\\\n        OAILWSD~$_{\\text{ICCV2019}}$&47.6&66.7&-&-&43.4&66.7\\\\\n        \\rowcolor{mygray}\n        TPWSD~$_{\\text{ICCV2019}}$&51.5&68.0&-&-&45.6&68.7\\\\\n        SDCN~$_{\\text{ICCV2019}}$&50.2&68.6&-&-&43.5&67.9\\\\\n        \\rowcolor{mygray}\n        C-MIDN~$_{\\text{ICCV2019}}$&52.6&68.7&-&-&50.2&71.2\\\\\n        ICMWSD~$_{\\text{CVPR2020}}$&\\textbf{54.9}&68.8&-&-&\\textbf{52.1}&70.9\\\\\n        \\rowcolor{mygray}\n        SLV~$_{\\text{CVPR2020}}$ &53.5&71.0&-&-&49.2&69.2\\\\\n        \\hline\n        OICR~+FR$_{\\text{CVPR2017}}$ &47.0&64.3&-&-&42.5&65.6\\\\\n        \\rowcolor{mygray}\n        PCL~+FR$_{\\text{TPAMI2018}}$ &48.8&66.6&-&-&44.2&68.0\\\\\n        MEFF~+FR$_{\\text{CVPR2018}}$ &51.2&-&-&-&-&-\\\\\n        \\rowcolor{mygray}\n        C-WSL~+FR$_{\\text{ECCV2018}}$ &47.8&65.6&-&-&45.4&66.9\\\\\n        WSRPN~+FR$_{\\text{ECCV2018}}$ &50.4&68.4&-&-&45.7&69.3\\\\   \n        \\rowcolor{mygray}   \n        WS-JDS~+FR$_{\\text{CVPR2019}}$ &52.5&68.6&\\textbf{45.7}&\\textbf{68.1}&46.1&69.5\\\\\n        SDCN~+FR$_{\\text{ICCV2019}}$ &53.7&\\textbf{72.5}&-&-&46.7&69.5\\\\\n        \\rowcolor{mygray}\n        C-MIDN~+FR$_{\\text{ICCV2019}}$ &53.6&71.9&-&-&\\textbf{50.3}&\\textbf{73.3}\\\\\n        SLV~+FR$_{\\text{CVPR2020}}$ &\\textbf{53.9}&72.0&-&-&-&-\\\\\n        \\bottomrule\n        \\end{tabular}\n\\end{table}\n\\addtolength{\\tabcolsep}{4.5pt} \n\\addtolength{\\tabcolsep}{-1.5pt} \n\\begin{table}[t]\n    \\centering\n    \\caption{Detetion results on MSCOCO dataset comes from~. These models use VGG16 as their convolutional neural network. There is no difference between AP and mAP under the MSCOCO context.}\n    \\label{table_coco}\n    \\scalebox{0.95}{\n        \\begin{tabular}{lccccc}\n            \\toprule\n            \\multirow{2}*{Approach}& \n            \\multirow{2}*{Year}&\n            \\multicolumn{2}{c}{Val}&\n            \\multicolumn{2}{c}{Test}\\\\\n            \\cmidrule (r){3-4} \\cmidrule (r){5-6}\n            &&AP&AP$_{50}$&AP&AP$_{50}$\\\\\n            \\midrule\n            WSDDN~&CVPR2016&-&-&-&11.5\\\\\n            \\rowcolor{mygray}\n            WCCN~&CVPR2017&-&-&-&12.3\\\\\n            PCL~&TRAMI2018&8.5&19.4&-&-\\\\\n            \\rowcolor{mygray}\n            C-MIDN~&ICCV2019&9.6&21.4&-&-\\\\\n            WSOD2~&ICCV2019&10.8&22.7&-&-\\\\\n            \\rowcolor{mygray}\n            ICMWSD~&CVPR2020&\\textbf{11.4}&\\textbf{24.3}&\\textbf{12.1}&\\textbf{24.8}\\\\\n            \\hline\n            Diba et al.~+SSD~&arXiv 2017&-&-&-&13.6\\\\ \n            \\rowcolor{mygray}\n            OICR~+Ens+FR~&CVPR2017&7.7&17.4&-&-\\\\\n            MEFF~+FR~&CVPR2018&8.9&19.3&-&-\\\\\n            \\rowcolor{mygray}\n            PCL~+Ens.+FR~&TPAMI2018&9.2&19.6&-&-\\\\\n            \\bottomrule\n        \\end{tabular}\n    }\n\\end{table}\n\\addtolength{\\tabcolsep}{1.5pt}\n\\begin{table*}[t]\n    \\centering\n    \\caption{Object localization performance on ILSVRC 2012 and CUB-200-2011 datasets.}\n    \\label{table_ILSVRC_CUB}\n        \\begin{tabular}{lccccccccc}\n        \\toprule\n        \\multirow{2}*{Approach}& \n        \\multirow{2}*{Year}&\n        \\multicolumn{4}{c}{ILSVRC 2012 (top error \\%)}&\n        \\multicolumn{4}{c}{CUB-200-2011 (top error \\%)}\\\\\n        \\cmidrule (r){3-6} \\cmidrule (r){7-10}\n        &&1-err cls&5-err cls&1-err loc&5-err loc&1-err cls&5-err cls&1-err loc&5-err loc\\\\\n        \\midrule\n        CAM~&CVPR2016&35.6&13.9&57.78&45.26&-&-&-&-\\\\\n        \\rowcolor{mygray}\n        ACoL~&CVPR2018&32.5&\\textbf{12.0}&54.17&36.66&-&-&54.08&39.05\\\\\n        SPG~&ECCV2018&-&-&51.4&\\textbf{35.05}&-&-&53.36&40.62\\\\\n        \\rowcolor{mygray}\n        DANet~&ICCV2019&32.5&\\textbf{12.0}&54.17&40.57&\\textbf{24.6}&\\textbf{7.7}&47.48&38.04\\\\\n        NL-CCAM~&WACV2020&\\textbf{27.7}&-&\\textbf{49.83}&39.31&26.6&-&47.6&\\textbf{34.97}\\\\\n        \\rowcolor{mygray}\n        EIL~&CVPR2020&29.73&-&53.19&-&25.23&-&\\textbf{42.54}&-\\\\\n        \\bottomrule\n        \\end{tabular}\n\\end{table*}\n\\begin{table*}[t]\n    \\centering\n    \\caption{Some techniques and tricks for improving detection results and the approaches that utilize\n    them. 1) Cont: Context modeling, 2) Self-t: Self-training algorithm, 3) Casc: Cascaded network, 4) BboxR: Bounding box regression, 5) DisRR: Discriminative region removal, 6) Low-l: Incorporating low-level features, 7) Seg-D: Segmentation-detection collaborative mechanism, 8) Trans: Transforming WSOD to FSOD, 9) E-t-h: Easy-to-hard strategy, 10) NegE: Negative evidence, 11) SmoL: Optimizing smoothed loss functions.}\n    \\label{table09}\n        \\begin{tabular}{lcccccccccccc}\n        \\toprule\n        \\multirow{2}*{Approach}&\n        \\multicolumn{8}{c}{Specific techniques for discriminative region problem} & \n        \\multicolumn{3}{c}{Training tricks}\\\\\n        \\cmidrule (r){2-9} \\cmidrule (r){10-12}\n        &Cont&Self-t&Casc&BboxR&DisRR&Low-l&Seg-D&Trans&E-t-h&NegE&SmoL\\\\\n        \\midrule\n        \\rowcolor{mygray}\n        WSDDN~&&&&&&&&&&&\\\\\n        CAM~&&&&&&&&&&&\\\\\n        \\rowcolor{mygray}\n        WSLPDA~&$\\surd$&&&&&&&&&&\\\\\n        WELDON~&&&&&&&&&&$\\surd$&\\\\\n        \\rowcolor{mygray}\n        ContextLocNet~&$\\surd$&&&&&&&&&&\\\\\n        Grad-CAM~&&&&&&$\\surd$&&&&&\\\\\n        \\rowcolor{mygray}\n        OICR~&&$\\surd$&&&&&&$\\surd$&&&\\\\\n        WCCN~&&&$\\surd$&&&&&&&&\\\\\n        \\rowcolor{mygray}\n        ST-WSL~&&$\\surd$&&&&&&$\\surd$&&&\\\\\n        WILDCAT~&&&&&&&&&&$\\surd$&\\\\\n        \\rowcolor{mygray}\n        SPN~&&&&&&&&&&&\\\\\n        TP-WSL~&&&&&$\\surd$&&&&&&\\\\ \n        \\rowcolor{mygray}\n        PCL~&&$\\surd$&&&&&&$\\surd$&&&\\\\\n        GAL-fWSD~&&&&&&&&$\\surd$&&&\\\\\n        \\rowcolor{mygray}\n        W2F~&&$\\surd$&&&&&&$\\surd$&&&\\\\\n        ACoL~&&&&&$\\surd$&&&&&&\\\\\n        \\rowcolor{mygray}\n        ZLDN~&&&&&&&&&$\\surd$&&\\\\\n        TS$^2$C~&$\\surd$&&$\\surd$&&&&&$\\surd$&&&\\\\\n        \\rowcolor{mygray}\n        SPG~&&&&&&&&&&&\\\\\n        WSRPN~&&&&&&&&&&&\\\\\n        \\rowcolor{mygray}\n        C-MIL~&&&&&&&&&&&$\\surd$\\\\\n        WS-JDS~&&&&&&&$\\surd$&$\\surd$&&&\\\\\n        \\rowcolor{mygray}\n        ADL~&&&&&&&&&&&\\\\\n        Pred NET~&&&&$\\surd$&&&&$\\surd$&&&\\\\\n        \\rowcolor{mygray}\n        WSOD2~&&$\\surd$&&$\\surd$&&$\\surd$&&&&&\\\\\n        OAILWSD~&$\\surd$&$\\surd$&&&&&&&&&\\\\\n        \\rowcolor{mygray}\n        TPWSD~&&$\\surd$&&$\\surd$&&&&&&&\\\\\n        SDCN~&&&&&&&$\\surd$&$\\surd$&&&\\\\\n        \\rowcolor{mygray}\n        C-MIDN~&&$\\surd$&&&$\\surd$&&&$\\surd$&&&\\\\\n        DANet~&&&&&&&&&&&\\\\\n        \\rowcolor{mygray}\n        NL-CCAM~&&&&&&&&&&$\\surd$&\\\\\n        ICMWSD~&$\\surd$&$\\surd$&&$\\surd$&&&&&&&\\\\\n        \\rowcolor{mygray}\n        EIL~&&&&&$\\surd$&&&&&&\\\\\n        SLV~&&$\\surd$&&$\\surd$&&&&$\\surd$&&&\\\\\n        \\bottomrule\n        \\end{tabular}\n\\end{table*}", "cites": [2282, 2289, 2291, 2306, 2936, 7089, 2294, 8429, 2287, 2259, 7090, 2275, 2296, 802, 2283, 2286, 2937, 737, 2935, 2304, 2302, 2295, 2290, 2280, 2284, 2276, 2305], "cite_extract_rate": 0.6585365853658537, "origin_cites_number": 41, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a comparative overview of experimental results across multiple WSOD benchmarks, integrating performance data from various cited papers. While it connects results under common evaluation frameworks and highlights trends (e.g., performance gains from using Fast R-CNN), it lacks deeper synthesis into a novel narrative. Critical analysis is limited to surface-level observations, such as marginal gains on MSCOCO, and abstraction is minimal, focusing mostly on concrete metrics rather than overarching principles."}}
{"id": "5ecf381a-e969-4630-938a-820602070f9b", "title": "Model Directions", "level": "subsection", "subsections": [], "parent_id": "5d4d4d9b-ac69-47c5-b147-5bcd28ddb450", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "Future Directions and Tasks"], ["subsection", "Model Directions"]], "content": "\\textbf{Better Initial Proposals.} The main proposal generators of the existing methods are selective search~, edge boxes~, heatmap, and sliding window. Selective search and edge boxes are too time-consuming and yield plenty of initial proposals that most of them are negative proposal. Segmenting heatmap tends to focus on the discriminative part of the object. The performance of the sliding window is strongly dependent on the size of objects. For example, if the size of the object instance is roughly fixed, then the sliding window works very well. Otherwise, it works badly. Because these generators have inherent disadvantages, we need to design a proposal generator that can yield fewer and more accurate initial proposals. The quality of the initial proposals directly affects the detection performance of the detector. So how to yield good initial proposals may be a new research direction.\n\\textbf{Better Positive Proposals.} Most WSOD methods select the proposals with the top score as positive proposals, which tend to focus on the most discriminative parts of the object rather than the whole object region. Because of the above problem, ST-WSL~ selects positive proposals according to the number of their surrounding proposals. And Self-Taught-WS~ selects positive proposals relying on the relative improvement (RI) of the scores of each proposal of two adjacent epochs. Besides, the key of self-training and cascaded network is to select accurate proposals as the pseudo ground-truth boxes for later training. Thus, how can we design a better algorithm that can accurately select positive proposals may be an important research direction.\n\\textbf{Lightweight Network.} Today's state-of-the-art object detectors~ leverage a very deep CNN to extract image feature maps and high-dimension fully connected layers to detect object instances that can achieve satisfactory detection performance. But the deep CNN and high-dimension fully connected layers rely on large memory and strong GPU computation power. Hence, a deep network is difficult to deploy on CPU devices (\\eg, mobile phones). With the popularity of mobile devices, lightweight network with few parameters has received more and more attention from researchers, such as Light-Head R-CNN~. Thus, designing a lightweight network in weakly supervised object detection may be a new research direction.", "cites": [97, 2938, 2275, 799], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from the cited papers by grouping them into research directions (initial and positive proposals, lightweight networks). It identifies limitations in current proposal generation methods and highlights techniques like ST-WSL and Self-Taught-WS that aim to improve proposal selection. While it provides a coherent narrative and some critical analysis of existing approaches, the insights remain largely focused on method-level observations without deeper theoretical abstraction or a novel synthesis of broader concepts."}}
{"id": "806eba73-1bd2-4931-b096-1b96d4f1feac", "title": "Application Directions", "level": "subsection", "subsections": [], "parent_id": "5d4d4d9b-ac69-47c5-b147-5bcd28ddb450", "prefix_titles": [["title", "Deep Learning for Weakly-Supervised Object Detection and Object Localization: A Survey"], ["section", "Future Directions and Tasks"], ["subsection", "Application Directions"]], "content": "\\textbf{Medical Imaging.} \\blue{With the development of deep learning, it has evolved into cross-learning with multiple disciplines, especially the medical field. Because of lacking brain's Magnetic Resonance Imaging (MRI) and X-rays images with sufficient labels, weakly-supervised brain lesion detection~ has received attention from researchers. The purpose of weakly-supervised brain lesion detection is to give the model the ability to accurately locate lesion region and classify lesion category that helps the doctor complete the diagnosis of the disease. Weakly-supervised lesion detection is not only applied in brain disease, but also other organ diseases, such as chest, abdomen, and pelvis. In addition to lesion detection, weakly-supervised learning is applied in disease prognosis~. During hospital visits, patients need to undergo a large number of tests to pinpoint their disease. These tests are generally presented to doctors and patients in the form of graphic reports. However, these numerous graphic reports lack correct labeling information. So, medical imaging may be another potential research direction in a weakly supervised setting.}\n\\textbf{3D Object Detection.} \\blue{In recent years, with the continuous improvement of the accuracy of object detection of images~, 3D object detection~ has received unprecedented attention. The purpose of 3D object detection is to detect object instances in the point cloud using 3D bounding boxes. Comparing with 2D object detection, 3D object detection tends to cost more computation and its supervision is more difficult to obtain and labor-intensive. Therefore, how to train light and accurate 3D detection models in the point cloud using simple labels may be a big challenge. Fortunately, weakly-supervised object detection is successfully applied in 2D object detection. According to the above analysis, we think that 3D weakly-supervised object detection that uses weak supervision(\\eg, 2D bounding boxes and text labels) to train object detection models in the 3D scene may be a hot research direction.}\n\\textbf{Video Object Detection.} \\blue{Video object detection~ is to classify and locate object instances in a piece of video. One of the solutions is to break the video into many frames and the detector achieves object detection in these frame images~. However, the detector will face one big problem that the quality of these frame images has deteriorated. To improve the performance of video object detection, expanding the training dataset is a good approach. Unfortunately, tagging object location and category in videos is much more difficult than in 2D images. Therefore, training video object detection in the weakly-supervised setting is necessary.}", "cites": [799, 2942, 206, 2939, 2933, 7407, 2320, 802, 2941, 7405, 209, 520, 2940], "cite_extract_rate": 0.65, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of potential application directions for weakly-supervised object detection, drawing on cited papers to highlight key challenges and opportunities in medical imaging, 3D detection, and video detection. It synthesizes information across domains, such as linking 2D detection methods to 3D and video settings. While it offers a coherent narrative and identifies broader patterns (e.g., the difficulty of obtaining precise supervision in different modalities), it lacks deeper comparative or critical evaluation of the cited works and does not propose a novel framework or synthesis."}}
