{"id": "24204e94-b539-4b86-bbee-4acd9e25a204", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "66ef2e50-c617-457c-84da-f8257ab93ea9", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Introduction"]], "content": "\\label{sec:intro}\nWith the scaling of model size and data size~, large language models (LLMs) demonstrate the in-context learning (ICL) ability, that is, learning from a few examples in the context. \nMany studies have shown that LLMs can perform a series of complex tasks through ICL, such as solving mathematical reasoning problems~. These strong abilities have been widely verified as emerging abilities for large language models~. \nThe key idea of in-context learning is to learn from analogy. Figure~\\ref{fig:icl} gives an example that describes how language models make decisions via ICL.\nFirst, ICL requires a few demonstration examples to form a prompt context. These examples are usually written in natural language templates. \nThen, ICL concatenates a query question and the piece of prompt context together to form the input,  which is then fed into the language model for prediction.\nDifferent from supervised learning, which requires a training stage that uses backward gradients to update model parameters, ICL does not perform parameter updates. The model is expected to learn the pattern hidden in the demonstration and accordingly make the right prediction. \n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.45\\textwidth]{fig/icl.pdf}\n    \\caption{Illustration of in-context learning. ICL requires a prompt context containing a few demonstration examples written in natural language templates. Taking this prompt and a query as the input, large language models are responsible for making predictions.}\n    \\label{fig:icl}\n\\end{figure}\n\\input{fig/taxonomy.tex}\nAs a new paradigm, ICL has multiple attractive advantages. \nFirst, since the demonstration is written in natural language, it provides an interpretable interface to communicate with LLMs~.\nThis paradigm makes it much easier to incorporate human knowledge into LLMs by changing the demonstration and templates~. \nSecond, in-context learning is similar to the decision process of human beings by learning from analogy~. \nThird, compared to supervised training, ICL is a training-free learning framework. \nThis could not only greatly reduce the computational costs for adapting the model to new tasks, but also make language-model-as-a-service~ possible and can be easily applied to large-scale real-world tasks.\nDespite being promising, there are also interesting questions and intriguing properties that require further investigation in ICL. \nAlthough a range of vanilla GPT models show excellent ICL capability, several studies have found that this capability can be significantly improved through adaptation during pretraining~.\nMoreover, the performance of ICL is sensitive to specific settings, including the prompt template, the selection and order of demonstration examples, and other factors~. Additionally, optimizing the conciseness of demonstration examples and improving the computational efficiency of ICL are critical areas of ongoing research~. \nFurthermore, despite preliminary explanations~, the underlying working mechanism of ICL remains unclear and requires further investigation.\nWith the rapid growth of studies in ICL, our survey aims to sensitize the community toward the current progress.\nIn the following sections, we delve into an in-depth discussion of related studies, and we summarize the key findings in Appendix~\\ref{app:takeaway}. \nWe highlight the challenges and potential directions and hope our work provide a useful roadmap for beginners interested in this area and shed light on future research.\nThe strong performance of ICL relies on two stages: (1) the training stage that cultivates the ICL ability of LLMs, and (2) the inference stage where LLMs predict according to task-specific demonstrations.\nIn terms of the training stage, LLMs are directly trained on language modeling objectives, such as left-to-right generation. Although the models are not specifically optimized for in-context learning, they still exhibit the ICL ability.\nExisting studies on ICL basically take a well-trained LLM as the backbone, and thus this survey will not cover the details of pretraining language models. \nTowards the inference stage, as the input and output labels are all represented in interpretable natural language templates, there are multiple directions for improving ICL performance. \nWe organize the current progress in ICL following the taxonomy above (as shown in Figure~\\ref{taxo_of_icl}). \nWith a formal definition of ICL~(\\S\\ref{sec:formulation}), \nwe provide a detailed discussion of the ICL-augmented pretraining and warmup approaches~(\\S\\ref{sec:training}) and the demonstration designing strategies (\\S\\ref{sec:prompt_tuning}). \\S\\ref{sec:analysis} provides in-depth discussions of current explorations on unveiling the secrets behind the ICL. \nWe further provide useful evaluation and resources~(\\S\\ref{sec:evaluation}), potential applications~(\\S\\ref{sec:application}) and potential challenges~(\\S\\ref{sec:challege_future}) in Appendix", "cites": [1578, 2189, 8470, 679, 3368, 7697, 8556, 1552, 1554, 3369, 2445, 9115], "cite_extract_rate": 0.631578947368421, "origin_cites_number": 19, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a basic descriptive overview of in-context learning, citing several relevant papers to support its points. While it mentions some findings from the literature, such as the importance of demonstration order and the role of model scale, it does not deeply synthesize or connect these ideas into a cohesive framework. There is limited critical evaluation of the cited works, and the abstraction is modest, focusing more on summarizing features and applications rather than uncovering deeper principles."}}
{"id": "c5b589e0-c439-4d4c-88cc-1b6df436c094", "title": "Definition and Formulation", "level": "section", "subsections": [], "parent_id": "66ef2e50-c617-457c-84da-f8257ab93ea9", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Definition and Formulation"]], "content": "\\label{sec:formulation}\nFollowing , we here provide a formal definition of in-context learning:\n\\begin{quote}\n\\textsl{In-context learning is a paradigm that allows language models to learn tasks given only a few examples in the form of demonstration.}\n\\end{quote}\nFormally, given a query input text $x$ and a set of candidate answers $Y = \\{y_1, \\ldots, y_m\\}$, a pretrained language model $\\mathcal{M}$ takes the candidate answer with the maximum score as the prediction,\\footnote{$Y$ could be class labels or a set of free-text phrases.} conditioned a demonstration set $C$.\n$C$ contains an optional task instruction $I$ and $k$ demonstration examples, thus $C = \\{ I, s(x_1, y_1), \\ldots, s(x_k, y_k) \\}$ or $C = \\{ s^{\\prime}(x_1, y_1, I), \\ldots, s^{\\prime}(x_k, y_k, I) \\}$, where $s^{\\prime}(x_i, y_i, I)$ is an example written in natural language according to the task.\nThe likelihood of a candidate answer $y_j$ comes from a scoring function $f$ on the whole input sequence:\n\\begin{equation}\n    P( y_j \\mid x) \\triangleq\n    f_\\mathcal{M} ( y_j,  C, x)\n\\end{equation}\nThe final predicted label $\\hat y$ is the candidate answer with the highest probability:\n\\begin{equation}\n    \\hat y = \\arg\\max_{y_j \\in Y } P(y_j \\mid x). \n\\end{equation}\nAccording to the definition, we can see that ICL differs from related concepts as follows: (1) \\textit{Prompt Learning}: prompts can be discrete templates or soft parameters that encourage the model to predict the desired output. ICL can be regarded as a subclass of prompt tuning where the demonstration examples are part of the prompt.  made a thorough survey on prompt learning, but ICL was not included in their study. (2) \\textit{Few-shot Learning}: few-shot learning is a general machine learning approach that involves adapting model parameters to perform a task with a limited number of supervised examples~. In contrast, ICL does not require parameter updates and is directly performed on pretrained LLMs.", "cites": [679], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a clear and formal definition of in-context learning and situates it within related paradigms like prompt learning and few-shot learning, showing some synthesis of ideas. It includes a critical comparison by highlighting how ICL differs from these concepts, particularly in not requiring parameter updates. While it introduces useful abstraction by framing ICL as a subclass of prompt tuning, it lacks deeper evaluation or nuanced critique of the cited works and does not fully integrate multiple sources into a novel framework."}}
{"id": "d04516c1-e5c9-4a71-b557-9810697071f4", "title": "Model Training", "level": "section", "subsections": ["803089ad-8508-470d-8df7-06e2822a82ca", "cfccaa5e-3b97-42f9-888b-ff671fa0d565"], "parent_id": "66ef2e50-c617-457c-84da-f8257ab93ea9", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Model Training"]], "content": "\\label{sec:training}\nAlthough LLMs have demonstrated promising ICL capability directly, many studies revealed that these ICL capabilities can be further enhanced through specialized training before inference~.", "cites": [7711, 7136], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a minimal description of how ICL capabilities can be enhanced through specialized training, citing two relevant papers. However, it lacks synthesis of the two papers' ideas, does not engage in critical evaluation of their methods or limitations, and offers no abstraction or broader conceptual framing. The narrative remains surface-level and descriptive."}}
{"id": "803089ad-8508-470d-8df7-06e2822a82ca", "title": "Pretraining", "level": "subsection", "subsections": [], "parent_id": "d04516c1-e5c9-4a71-b557-9810697071f4", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Model Training"], ["subsection", "Pretraining"]], "content": "\\label{sec:pretraining}\nOne straightforward direction to boost the ICL capability of LLMs is through pretraining or continual pretraining.\nFor instance,  and  proposed to reorganize pretraining corpora by aggregating related contexts, making models learn to reason across prior demonstrations. Differently,  introduced a meta-distillation pretraining process, which allows LLMs to reason with distilled demonstration vectors, thereby enhancing ICL efficiency without compromising its effectiveness.\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.92\\columnwidth]{fig/crop_train.pdf}\n    \\caption{Illustration of model training methods to enhance ICL capabilities through two different stages: pretraining and warmup.}\n    \\label{fig:train_method}\n\\end{figure}", "cites": [7136], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly describes two pretraining approaches from the cited papers but lacks integration or synthesis of ideas. It does not compare or critically assess the methods, nor does it abstract to broader principles or trends in pretraining for ICL. The content remains largely descriptive and does not offer deeper analytical insight."}}
{"id": "cfccaa5e-3b97-42f9-888b-ff671fa0d565", "title": "Warmup", "level": "subsection", "subsections": [], "parent_id": "d04516c1-e5c9-4a71-b557-9810697071f4", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Model Training"], ["subsection", "Warmup"]], "content": "\\label{sec:warmup}\nAnother way to enhance ICL ability is adding a continual training stage between pretraining and ICL inference, which we call model warmup for short. \nWarmup is an optional procedure for ICL, which adjusts LLMs before inference by modifying or adding parameters.\nAs most pretraining data are not tailored for ICL~, researchers have introduced various warmup strategies to bridge the gap between pretraining and ICL inference. Both  and  proposed to continually finetune LLMs on a broad range of tasks with multiple demonstration examples, which boosts ICL abilities.\nTo encourage the model to learn input-label mappings from the context,  proposed symbol tuning, which substitutes natural language labels (e.g., ``positive/negative sentiment'') with arbitrary symbols (e.g., ``foo/bar'').  proposed a self-supervised method to align raw text with ICL formats in downstream tasks. Besides, multiple studies have indicated the potential value of instructions~. Tuning the 137B LaMDA-PT~ on over 60 datasets verbalized via natural language instruction templates, FLAN~ improves the ability of LLMs to follow instructions, boosting both the zero-shot and few-shot ICL performance.  and  proposed to further scale up instruction tuning with more than 1000+ task instructions.", "cites": [8534, 1587, 7711, 8649, 1553, 2198, 7468], "cite_extract_rate": 0.875, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple warmup approaches for in-context learning effectively, connecting papers on instruction tuning, symbol tuning, and self-supervised training to show how they aim to bridge the pretraining-inference gap. While it offers a coherent narrative, it lacks deeper critical evaluation of trade-offs or limitations of each method. It provides some level of abstraction by identifying instruction tuning and symbol substitution as broader strategies but does not offer a novel or meta-level theoretical framework."}}
{"id": "bc0582f7-8484-43f1-8d1e-ca6b7d828fbb", "title": "Demonstration Organization", "level": "subsection", "subsections": ["fee666c2-68ae-4733-89a0-9f9017fc554d", "862f6284-d97d-4dbe-b05c-0f108c797835", "38ac0700-6366-4274-a02b-fc58997d5d1c"], "parent_id": "45f129c0-f2ad-411c-a646-2578b65d6743", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Prompt Designing"], ["subsection", "Demonstration Organization"]], "content": "\\label{sec:demonstration_org}\nMany studies have shown that the performance of ICL strongly relies on the demonstration surface, including the selection, formatting, and ordering of demonstration examples~. \nIn this subsection, we survey demonstration organization strategies and classify them into three categories, as shown in Table~\\ref{tab:promptmethods}.", "cites": [8470, 1594], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions the importance of demonstration organization in ICL and classifies strategies into three categories, but does not provide a detailed synthesis of the cited papers. It lacks critical evaluation or comparison of the approaches, and while it hints at broader patterns (e.g., order sensitivity), it does not abstract these into overarching principles or frameworks."}}
{"id": "732044bb-34fe-421d-b1c8-9440e96dc829", "title": "Unsupervised Method", "level": "paragraph", "subsections": [], "parent_id": "fee666c2-68ae-4733-89a0-9f9017fc554d", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Prompt Designing"], ["subsection", "Demonstration Organization"], ["subsubsection", "Demonstration Selection"], ["paragraph", "Unsupervised Method"]], "content": "A straightforward approach to selecting ICL examples is to choose the nearest neighbors of input instances based on their similarities~. Distance metrics, such as L2 distance or cosine similarity based on sentence embeddings, are commonly used for this purpose. For example,  proposed KATE, the first $k$NN-based unsupervised retriever for selecting in-context examples. Similarly, $k$-NN cross-lingual demonstrations can be retrieved for multi-lingual ICL to strengthen source-target language alignment~.  proposed to combine graphs and confidence scores to select diverse and representative examples. In addition to distance metrics, mutual information~ and perplexity~ have proven valuable for prompt selection without labeled examples or specific LLMs. Furthermore, using output scores \nof LLMs as unsupervised metrics has shown effectiveness in demonstration selection~. Particularly,  selected the best subset permutation of $k$NN examples based on the code length for data transmission to compress label $y$ given $x$ and $C$.  used infoscore, i.e., the average of $P(y|x_i,y_i,x)   P(y|x)$ for all $(x,y)$ pairs in a validation set with a diversity regularization.", "cites": [3374, 3370, 3373, 2189, 8650, 3372, 3371], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of unsupervised methods for demonstration selection in ICL, listing several approaches and their key components. While it mentions multiple papers and their contributions, it does not deeply connect or synthesize the ideas into a broader framework. There is minimal critical analysis or identification of overarching patterns, focusing instead on factual descriptions of the methods."}}
{"id": "a86b2ef8-d0e3-4802-9253-132694da9348", "title": "Supervised Method", "level": "paragraph", "subsections": [], "parent_id": "fee666c2-68ae-4733-89a0-9f9017fc554d", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Prompt Designing"], ["subsection", "Demonstration Organization"], ["subsubsection", "Demonstration Selection"], ["paragraph", "Supervised Method"]], "content": "Though off-the-shelf retrievers offer convenient services for extensive NLP tasks, they are heuristic and sub-optimal due to the lack of task-specific supervision. To address this issue, numerous supervised methods have been developed~. EPR~ introduced a two-stage method to train a dense retriever for demonstration selection. For a specific input, it first utilized unsupervised methods (e.g., BM25) to recall similar examples as candidates and then used this data to build a supervised dense retriever. Following EPR,  adopted a unified demonstration retriever to select demonstrations across different tasks. Unlike prior work that retrieves individual demonstrations,  proposed retrieving entire demonstration sets to model inter-relationships between examples. Additionally,  introduced AdaICL, a model-adaptive method that employs LLM to predict the unlabeled data set, generating an uncertainty score for each instance.\nBased on prompt tuning,  viewed LLMs as topic models that can infer concepts $\\theta$ from a few demonstrations and generate tokens based on these concepts. They represent latent concepts with task-related concept tokens, which are learned to maximize $P(y|x,\\theta)$. Demonstrations are selected based on their likelihood to infer the concept variable using $P(\\theta|x,y)$. Additionally, reinforcement learning was introduced by  for example selection. They formulated demonstration selection as a Markov decision process~ and selected demonstrations via Q-learning. The action is choosing an example, and the reward is defined as the accuracy of a labeled validation set. \n\\begin{table}[t]                \n\\centering      \n\\setlength{\\tabcolsep}{2pt}  \n{  \\fontsize{9pt}{11pt}\\selectfont          \n\\begin{tabular}{lccccccc}      \n\\toprule             \n\\bf Model & \\bf Method & \\bf SST5 &  \\bf SST2 & \\bf CQA  & \\bf SNLI & \\bf News & \\bf Avg \\\\       \n\\midrule             \n\\multirow{3}{*}{GPT2}          \n& topk & 40.1 & 74.9 & 30.2 & 39.7&62.7 & 49.5\\\\            \n& votek & 32.4 & 51.0 & 29.8 & 35.8& 25.5 & 34.9 \\\\            \n& mdl & \\textbf{43.3} & \\textbf{86.7} & \\textbf{32.7} & \\textbf{41.4}& \n\\textbf{68.0} & \\textbf{54.4}\\\\       \n\\midrule             \n\\multirow{3}{*}{GPT-J}              \n& topk & \\textbf{46.9} & 84.6 & 58.4 & \\textbf{60.7} & \\textbf{69.1} & \\textbf{63.9} \\\\            \n& votek & 33.8 & 87.3 & 63.4 & 43.1& 25.3 & 50.6\\\\            \n& mdl & 37.6 & \\textbf{87.9} & \\textbf{64.1} & 59.8  & 68.2 &63.5\\\\        \n\\midrule              \n\\multirow{3}{*}{Qwen2}        \n& topk & 54.1 & 83.3 & 76.3 & \\textbf{68.2} &64.9 &  \\textbf{69.4}\\\\            \n& votek & \\textbf{55.3} & \\textbf{86.9} & 76.1 &51.6& \\bf 65.3 & 67.0\\\\            \n& mdl & 54.6 & 86.1 & \\textbf{77.1} &65.0& 63.2 &69.2\\\\            \n\\midrule             \n\\multirow{3}{*}{Llama3}              \n& topk & 53.0 & \\textbf{90.3} & 76.1 & \\textbf{64.0} & 74.0 &  \\textbf{71.5}\\\\            \n& votek & 54.9 & 88.9 & 72.6 & 57.7& \\textbf{78.3} & 70.5\\\\            \n& mdl & \\textbf{54.4} & 89.1 & \\textbf{76.5} & 59.9 & 74.6 &70.9 \\\\               \n\\bottomrule                \n\\end{tabular}}                   \n\\caption{Fair comparison of demonstration selection methods. CQA and News are abbreviations of Commonsense QA and AG News, respectively. The best results are \\textbf{bolded}. Our experiments on topk~, votek~, mdl~ show that topk selects the most effective examples on average.}    \n\\label{tab:experiment_design_centered_model_names}        \n\\end{table} \nIn order to have a more intuitive comparison of the performance of several unsupervised methods, we select topk~, votek~, mdl~ \nto conduct experiments. The result is shown in Table 2. The details of the experiment can be found in Appendix \\ref{app:experiment}.", "cites": [3374, 3375, 3377, 2189, 3378, 3376], "cite_extract_rate": 0.6, "origin_cites_number": 10, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section synthesizes several methods for demonstration selection and presents them in a structured manner, but lacks a deeper integration into a unified narrative. It compares experimental results across models and methods, which indicates a comparative approach. However, the critique of the methods is minimal and the generalization to broader principles or theoretical frameworks is limited."}}
{"id": "862f6284-d97d-4dbe-b05c-0f108c797835", "title": "Demonstration Reformatting", "level": "subsubsection", "subsections": [], "parent_id": "bc0582f7-8484-43f1-8d1e-ca6b7d828fbb", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Prompt Designing"], ["subsection", "Demonstration Organization"], ["subsubsection", "Demonstration Reformatting"]], "content": "\\label{sec:reformatting}\nIn addition to directly selecting examples from training data, another research trend involves utilizing LLMs to reformat the representation of existing demonstrations~. For instance,  proposed generating demonstrations directly from LLMs to reduce the reliance on external demonstration data. Structured Prompting  proposed to encode demonstration examples separately with special positional embeddings, which are then provided to the test examples using a rescaled attention mechanism. Diverging from these methods, other approaches focus on modifying the latent representation of demonstrations~. Specifically,  developed In-Context Vectors (ICVs) derived from the latent embeddings of demonstration examples in LLMs. These ICVs are used during inference to adjust the latent states of the LLM, thereby enhancing the model's ability to follow the demonstrations more effectively.", "cites": [8651, 3379], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of demonstration reformatting techniques in ICL, mentioning different methods such as generating demonstrations from LLMs, structured prompting, and latent representation modification. However, it lacks synthesis by not clearly connecting or contrasting these methods, offers minimal critical evaluation of their strengths or limitations, and does not abstract to higher-level principles or patterns within the field."}}
{"id": "38ac0700-6366-4274-a02b-fc58997d5d1c", "title": "Demonstration Ordering", "level": "subsubsection", "subsections": [], "parent_id": "bc0582f7-8484-43f1-8d1e-ca6b7d828fbb", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Prompt Designing"], ["subsection", "Demonstration Organization"], ["subsubsection", "Demonstration Ordering"]], "content": "\\label{sec:order}\nOrdering the selected demonstration examples is also an important aspect of demonstration organization.  have proven that order sensitivity is a common problem and always exists for various models. To handle this problem, previous studies have proposed several training-free methods for sorting demonstration examples. Particularly,  arranged examples based on their proximity to the input, positioning the closest example as the rightmost demonstration.  introduced global and local entropy metrics, finding a positive correlation between these metrics and the ICL performance. Consequently, they utilized the entropy metric to determine the optimal demonstration ordering. Additionally, ICCL~ suggested ranking demonstrations from simple to complex, thereby gradually increasing the complexity of demonstration examples during the inference process.", "cites": [3369, 2189, 8470], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly describes different demonstration ordering methods from three papers but fails to provide a deeper synthesis or comparative analysis. It presents the methods in a list-like manner without evaluating their strengths, weaknesses, or implications. There is minimal abstraction, as it does not generalize these methods into broader principles or trends within ICL."}}
{"id": "3bdaadb6-7ca4-4ca2-8e38-e78de33e240f", "title": "Instruction Formatting", "level": "subsection", "subsections": [], "parent_id": "45f129c0-f2ad-411c-a646-2578b65d6743", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Prompt Designing"], ["subsection", "Instruction Formatting"]], "content": "\\label{sec:instruction}\nA common way to format demonstrations is concatenating examples $(x_1, y_1), \\ldots, (x_k, y_k)$ with a template $\\mathcal{T}$ directly. However, in some tasks that need complex reasoning (e.g., math word problems and commonsense reasoning), it is not easy to learn the mapping from $x_i$ to $y_i$ with only $k$ demonstrations. Although template engineering has been studied in prompting~,  some researchers aim to design a better format of demonstrations for ICL by describing tasks with the instruction $I$.  found that given several demonstration examples, LLMs can generate task instructions themselves. Considering the generation abilities of LLMs,  proposed an Automatic Prompt Engineer for automatic instruction generation and selection.\nTo further improve the quality of the automatically generated instructions, several strategies have proposed using LLMs to bootstrap off its own generations~. \nAdditionally, chain-of-thought (CoT)~ introduces intermediate reasoning steps between inputs and outputs to enhance problem-solving and comprehension. Recent advancements have also emphasized the process of enhancing step-by-step reasoning in models~.", "cites": [3380, 3381, 1578, 424, 3382, 3383, 3384, 2467], "cite_extract_rate": 0.8888888888888888, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several papers to highlight the importance of instruction formatting in improving in-context learning, particularly for complex reasoning tasks. It integrates ideas from automatic instruction generation (e.g., APE) and chain-of-thought prompting, but lacks deeper comparative or evaluative analysis of these methods. The discussion moves toward abstraction by addressing broader strategies like instruction induction and iterative prompting, but the analysis remains somewhat surface-level without a strong critical or meta-level synthesis."}}
{"id": "f8a812bb-8dc9-4815-84d6-18b13d203e1a", "title": "Scoring Function", "level": "subsection", "subsections": [], "parent_id": "45f129c0-f2ad-411c-a646-2578b65d6743", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Prompt Designing"], ["subsection", "Scoring Function"]], "content": "\\label{sec:scoring}\n\\input{tabs/score_func}\nThe scoring function determines how to transform the predictions of a language model into an estimation of the likelihood of a specific answer. The Direct method uses the conditional probability of candidate answers represented by tokens in the model's vocabulary . The answer with the highest probability is selected as the final answer, but this method restricts template design by requiring answer tokens to be at the end of input sequences.\nPerplexity (PPL) is another commonly used metric that computes the sentence perplexity of the entire input sequence \\( S_j = \\{ C, s(x, y_j, I) \\} \\), which includes tokens from demonstration examples \\( C \\), the input query \\( x \\), and the candidate label \\( y_j \\). PPL evaluates the probability of the sentence, eliminating token position limitations but requiring additional computation time.  proposed using channel models (Channel) to compute the conditional probability in reverse, estimating the likelihood of the input query given the label. This approach requires language models to generate every token in the input, potentially boosting performance under imbalanced training data. We summarize all three scoring functions in Table~\\ref{tab:score_func}.", "cites": [679, 3385], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of three scoring functions for in-context learning, drawing from two cited papers. It integrates the ideas to some extent by mentioning each method and its characteristics but lacks deeper synthesis, critical evaluation, or abstraction into broader principles. The analysis remains at a surface level without significant insight or comparative depth."}}
{"id": "55f7b1b0-ead8-405c-a87a-2eebb0d97e59", "title": "Analysis", "level": "section", "subsections": ["3427e8cb-a3e1-4bac-aca3-43f553eaaaa2", "b67a0787-b89f-4174-9d2f-f1c0b7ad9f5a"], "parent_id": "66ef2e50-c617-457c-84da-f8257ab93ea9", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Analysis"]], "content": "\\label{sec:analysis}\nTo understand ICL, recent studies attempt to investigate what influence ICL performance~ and why ICL works~. \nIn this section, we present a detailed elaboration of influencing factors~(\\S \\ref{sec:inf_factors}) and learning mechanisms~(\\S \\ref{sec:mech}) of ICL, as illustrated in Figure~\\ref{fig:factor}.\n\\begin{figure*}\n    \\centering\n    \\vspace{-1.0cm}\n    \\includegraphics[width=0.9\\textwidth]{fig/icl_ana.pdf}\n    \\caption{Summary of factors that have a relatively strong correlation to ICL performance and different perspectives to explain why ICL works.}\n    \\label{fig:factor}\n\\end{figure*}", "cites": [3386, 7712], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section introduces the Analysis part of the survey and references two key papers on ICL, but it does not deeply synthesize or integrate their findings into a broader narrative. While it hints at organizing factors and mechanisms, the content provided is minimal and lacks explicit connections between the cited works. There is no clear critical evaluation or abstraction beyond the papers, leaving the insight level at a moderate level."}}
{"id": "9175a2f0-51d6-4413-abde-252cc2e7cae3", "title": "Pretraining Stage", "level": "subsubsection", "subsections": [], "parent_id": "3427e8cb-a3e1-4bac-aca3-43f553eaaaa2", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Analysis"], ["subsection", "Influencing Factors"], ["subsubsection", "Pretraining Stage"]], "content": "\\label{sec:inf_factors_pre}\nWe first introduce factors that influence the pretraining stage. The diversity of pretraining corpora significantly impacts ICL performance~. \nIn particular,  found that the source domain is more important than the corpus size, suggesting that combining multiple corpora may lead to the emergence of ICL ability. Similarly,  empirically identified a task diversity threshold beyond which LLMs exhibit strong ICL capabilities in unseen tasks.\nAnother line of research investigates the impact of data distribution on ICL~. For instance,  demonstrated that ICL capability emerges when the training data exhibits specific distributional properties, such as burstiness, wherein items appear in clusters rather than being uniformly distributed over time.\nBeyond these works, several studies have investigated the impact of model architecture and training process on ICL performance~.  investigated the emergent abilities of many large-scale models on multiple tasks. They suggested that a pretrained model acquires some emergent ICL abilities when it reaches a large scale of pretraining steps or model parameters.  pointed out that the in-context samples should attend to each other during inference, indicating that current causal LLMs may lead to suboptimal ICL performance.", "cites": [3388, 3386, 679, 3387, 8556], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a reasonable synthesis of how different pretraining aspects influence ICL, grouping related findings on data diversity, burstiness, and model scale. It shows some critical evaluation by pointing out limitations, such as the suboptimal ICL performance of causal LLMs. However, it lacks deeper comparative analysis and does not fully abstract or generalize into a broader theoretical framework."}}
{"id": "5bc33e85-fc18-430c-a28b-9f08a9afcf64", "title": "Inference Stage", "level": "subsubsection", "subsections": [], "parent_id": "3427e8cb-a3e1-4bac-aca3-43f553eaaaa2", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Analysis"], ["subsection", "Influencing Factors"], ["subsubsection", "Inference Stage"]], "content": "\\label{sec:inf_factors_infe}\nDuring inference, there are also multiple properties of demonstration examples that influence ICL performance.  proved that input-label settings such as the pairing format, the exposure of label space, and the input distribution contribute substantially to ICL performance. \nHowever, contrary to the conclusion in  that\ninput-label mapping matters little to ICL, latter studies showed that the accurate mapping influence ICL performance significantly~.  further pointed that flipped or semantically-unrelated input-label mapping also can be learned.\nFrom the perspective of demonstration construction, recent literature focuses on the diversity and simplicity of demonstrations~, the order of samples~, and the similarity between demonstrations and queries~. For example,  found that demonstration samples with embeddings closer to those of the query samples typically yield better performance than those with more distant embeddings.\nNotably, despite efforts to refine demonstrations to optimize the performance, there still remain clear feature biases during ICL inference~. Overcoming strong prior biases and ensuring the model gives equal weight to all contextual information remain challenges~.", "cites": [3389, 3377, 2189, 8470, 7712, 3390, 427, 3391, 7137], "cite_extract_rate": 0.75, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers on how in-context learning is influenced by input-label mappings and demonstration construction, but the integration is somewhat fragmented and lacks a novel, unified framework. It includes some critical analysis, such as pointing out contradictions in findings about input-label mapping importance and highlighting remaining challenges like feature biases. It identifies broader patterns (e.g., diversity, order, and similarity of demonstrations) but does not offer deep abstraction or meta-level principles."}}
{"id": "603d2bd4-3e98-4a01-9bd8-b6aadc1927f2", "title": "Functional Modules", "level": "subsubsection", "subsections": [], "parent_id": "b67a0787-b89f-4174-9d2f-f1c0b7ad9f5a", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Analysis"], ["subsection", "Learning Mechanism"], ["subsubsection", "Functional Modules"]], "content": "\\label{sec:mech_fun}\nThe ICL capability is intimately connected to specific functional modules within Transformers.\nAs one of the core components, the attention module is a focal point in the study of ICL mechanism~. Particularly,  identified specific attention heads, referred to as ``induction heads'', that can replicate previous patterns for next-token prediction, thus progressively developing ICL capabilities.\nAdditionally,  focused on the information flow in Transformers and found that during the ICL process, demonstration label words serve as anchors, which aggregate and distribute key information for the final prediction.", "cites": [3393, 3392, 7713], "cite_extract_rate": 0.375, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview by highlighting key functional modules, particularly the attention mechanism and induction heads, and connects them to the ICL process. It integrates ideas from multiple papers to form a narrative about how these modules contribute to in-context learning. However, the synthesis is somewhat limited in depth, and there is little critical evaluation or abstraction into broader principles."}}
{"id": "a6fbde0e-09e4-47ee-819b-4e8355af9aaf", "title": "Bayesian View", "level": "paragraph", "subsections": [], "parent_id": "974284f2-bba9-44b8-96d0-90963643f1ad", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Analysis"], ["subsection", "Learning Mechanism"], ["subsubsection", "Theoretical Interpretation"], ["paragraph", "Bayesian View"]], "content": "In the Bayesian framework, ICL is explained as implicit Bayesian inference, where models perform ICL by identifying a shared latent concept among examples~. Additional perspectives suggest that LLMs encode the Bayesian Model Averaging algorithm via the attention mechanism~. As the number of in-context examples increases, implicit Bayesian inference becomes analogous to kernel regression~.", "cites": [3396, 3368, 3387, 3395, 3394], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates the Bayesian perspective on in-context learning across several papers, connecting ideas like implicit Bayesian inference, latent concept identification, and the role of attention mechanisms. While it provides a coherent analytical framework, it lacks deeper critical evaluation of the assumptions or limitations in the cited works and does not generalize to a broader theoretical or conceptual level."}}
{"id": "b4efe669-d087-41b2-9a4c-d1b8e85990ac", "title": "Gradient Descent View", "level": "paragraph", "subsections": [], "parent_id": "974284f2-bba9-44b8-96d0-90963643f1ad", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Analysis"], ["subsection", "Learning Mechanism"], ["subsubsection", "Theoretical Interpretation"], ["paragraph", "Gradient Descent View"]], "content": "Gradient descent offers another valuable lens for understanding ICL. ~ identified a dual form between Transformer attention and gradient descent, finding that GPT-based ICL behaves similarly to explicit fine-tuning from multiple perspectives. Other studies have attempted to establish connections between ICL and gradient descent in simplified regression settings~. For instance,  showed that linear attention-only Transformers with manually constructed parameters are closely related to models learned by gradient descent.  found that self-attention-only Transformers exhibit similarities with models trained via gradient descent. However, the simplified settings used in these studies have led to debates about the direct applicability of these connections in real-world contexts~.  argued that Transformers perform ICL on linear regression using higher-order optimization techniques rather than gradient descent.", "cites": [8653, 3393, 8652], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a moderate level of synthesis by grouping related theoretical perspectives under the 'Gradient Descent View' and showing how multiple studies connect ICL to gradient descent. It includes critical analysis by mentioning the limitations of simplified settings and a counterpoint from a cited paper. The abstraction is limited to identifying general similarities rather than deeper meta-principles."}}
{"id": "7efb0396-f265-4288-97b3-a8cba48ccb14", "title": "Other Views", "level": "paragraph", "subsections": [], "parent_id": "974284f2-bba9-44b8-96d0-90963643f1ad", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Analysis"], ["subsection", "Learning Mechanism"], ["subsubsection", "Theoretical Interpretation"], ["paragraph", "Other Views"]], "content": "Beyond connecting ICL with a single algorithm, researchers have analyzed it from various perspectives, including ability decoupling, algorithmic learning, and information theory.  decoupled ICL capabilities into task recognition ability and task learning ability, each manifesting under different conditions. Another typical theory abstracts ICL as an algorithmic learning problem~, where Transformers dynamically select algorithms, such as gradient descent and ridge regression, tailored to different ICL instances. Moreover,  utilized information theory to show an error bound for ICL under linguistically motivated assumptions, explaining how next-token prediction can bring about the ICL ability. \nThese analytical studies have taken an essential step to explain ICL. However, most of them focused on simple tasks and small models. Extending analysis on extensive tasks and large models may be the next step to be considered.", "cites": [3397, 3399, 7714, 3391, 3398], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes insights from multiple papers by grouping different theoretical perspectives (ability decoupling, algorithmic learning, information theory) under a unified theme of alternative views on ICL. It provides a critical observation by pointing out the limitations of current analyses to simple tasks and small models, suggesting a direction for future research. The abstraction is moderate, identifying thematic categories but not fully elevating the discussion to overarching principles."}}
{"id": "0a759d57-0691-46bd-b44b-b65962f3c7a4", "title": "Application", "level": "section", "subsections": [], "parent_id": "66ef2e50-c617-457c-84da-f8257ab93ea9", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Application"]], "content": "\\label{sec:application}\n\\label{app}\nGiven its user-friendly interface and lightweight prompting method, ICL has broad applications on traditional NLP tasks~.\nParticularly, by using demonstrations that explicitly guide the reasoning process, ICL manifests remarkable effects on tasks requiring complex reasoning~ and compositional generalization~. \nWe explore several emerging and prevalent applications of ICL, including data engineering, model augmentation, and knowledge updating.\n\\textbf{1) Data Engineering:} Unlike traditional methods such as human annotation and noisy automatic annotation, ICL generates relatively high-quality data at a lower cost, leading to improved performance.~.\n\\textbf{2) Model Augmentation:} The context-flexible nature of ICL shows promise in model augmentation. It can enhance retrieval-augmented methods by prepending grounding documents to the input~. Additionally, ICL for retrieval demonstrates potential in steering models toward safer outputs~.\n\\textbf{3) Knowledge Updating:} LLMs often contain outdated or incorrect knowledge~. ICL has demonstrated efficacy in revising such knowledge through carefully crafted demonstrations, yielding higher success rates compared to gradient-based methods~.\nAs mentioned above, ICL has yielded significant benefits on both traditional and emergent NLP applications. \nThe tremendous success of ICL in NLP has inspired researchers to explore its potential in various modalities beyond text (elaborated in Appendix~\\ref{app:vision}), including vision ~, vision-language~, as well as speech applications~.", "cites": [3401, 1578, 424, 3400, 7565, 8654, 3001, 9122, 3005, 3002, 3003, 7715, 3004, 1576], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 21, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of ICL applications, listing several areas such as data engineering, model augmentation, and knowledge updating. It integrates a few cited papers to support each application but does so in a surface-level manner without connecting broader trends or developing a unified framework. There is minimal critical evaluation of the works or their limitations, and the level of abstraction remains low, focusing on specific examples rather than general principles."}}
{"id": "64592261-2e34-4c89-82f0-e119f7459b09", "title": "Efficiency and Scalability", "level": "paragraph", "subsections": ["04759d55-93a4-4f25-82cf-dfb0d48ffd43"], "parent_id": "802fa11b-661a-42b8-8518-6da66bcd6b28", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Challenges and Future Directions"], ["paragraph", "Efficiency and Scalability"]], "content": "The use of demonstrations in ICL introduces two challenges: (1) higher computational costs with an increasing number of demonstrations (\\textit{efficiency}), and (2) fewer learnable samples due to the maximum input length of LLMs (\\textit{scalability}). Prior research has attempted to mitigate these issues by distilling lengthy demonstrations into compact vectors~ or expediting LLM inference times~. However, these methods often involve a trade-off in performance or necessitate access to model parameters, which is impractical for closed-source models like ChatGPT and Claude~. Thus, enhancing the scalability and efficiency of ICL with more demonstrations remains a significant challenge.", "cites": [3403, 3402], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes two cited papers to highlight common challenges in ICL efficiency and scalability, integrating their approaches (e.g., distilling demonstrations, sparsity for inference). It offers a critical perspective by pointing out the performance trade-offs and limitations of requiring access to model parameters. However, the abstraction is somewhat limited, as it does not move beyond identifying the general problem and its constraints to propose broader theoretical or architectural principles."}}
{"id": "04759d55-93a4-4f25-82cf-dfb0d48ffd43", "title": "Generalization", "level": "paragraph", "subsections": ["4c62fbf4-3315-4e53-a63b-a83cbb5b16c3"], "parent_id": "64592261-2e34-4c89-82f0-e119f7459b09", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Challenges and Future Directions"], ["paragraph", "Efficiency and Scalability"], ["paragraph", "Generalization"]], "content": "ICL heavily relies on high-quality demonstrations selected from annotated examples, which are often scarce in low-resource languages and tasks. This scarcity poses a challenge to the generalization ability of ICL~. Given that there is a substantial discrepancy in the availability of annotated high-resource data and low-resource data, the potential to leverage high-resource data to address low-resource tasks is highly appealing~.", "cites": [3372], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the generalization challenge in ICL, particularly in low-resource settings, and cites one paper to support this point. However, it lacks synthesis of ideas from multiple sources, deeper critical evaluation of the cited work, and abstraction to broader principles or frameworks."}}
{"id": "4c62fbf4-3315-4e53-a63b-a83cbb5b16c3", "title": "Long-context ICL", "level": "paragraph", "subsections": [], "parent_id": "04759d55-93a4-4f25-82cf-dfb0d48ffd43", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Challenges and Future Directions"], ["paragraph", "Efficiency and Scalability"], ["paragraph", "Generalization"], ["paragraph", "Long-context ICL"]], "content": "Recent advances in context-extended LLMs have spurred research into the impact of ICL when using an increasing number of demonstration examples~. However, researchers have found that increasing the number of demonstrations does not necessarily enhance performance and may even be detrimental. These performance declines indicate a need for further investigation. Additionally,  developed LongICLBench, which includes diverse extreme-label classification tasks, revealing further weaknesses of LLMs in comprehending extended demonstrations.", "cites": [7716, 3404], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes information from two papers, connecting the trend of using more demonstration examples in ICL with the observed limitations of LLMs in long-context settings. It provides a critical perspective by highlighting performance declines and the need for further investigation. However, the abstraction is limited to the specific context of long demonstrations and extreme-label classification without deeper generalization to broader principles or frameworks."}}
{"id": "1c2b901c-e1cc-4779-9e45-8ae68dd7893f", "title": "In-context Learning Beyond Text", "level": "subsection", "subsections": [], "parent_id": "2aa08bb9-e45f-4156-b22d-928383da2a66", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Takeaway"], ["subsection", "In-context Learning Beyond Text"]], "content": "The tremendous success of ICL in NLP has inspired researchers to explore in-context learning in different modalities beyond natural language with promising results.\n\\textbf{$\\Diamond$ Takeaway}: \n(1) Properly formatted data (e.g., interleaved image-text datasets for vision-language tasks) and architecture designs are key factors for activating the potential of in-context learning. Exploring it in a more complex structured space such as for graph data is challenging and promising~.\n(2) Findings in textual in-context learning demonstration design and selection cannot be trivially transferred to other modalities. Domain-specific investigation is required to fully leverage the potential of in-context learning in various modalities.", "cites": [7717], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides analytical insights by emphasizing the importance of modality-specific data formatting and architecture design, based on the cited paper on graph data. It also highlights the non-transferability of textual demonstration strategies to other modalities, suggesting the need for domain-specific investigation. While it integrates a single paper well, it lacks deeper synthesis across multiple works and more critical evaluation of limitations or trade-offs."}}
{"id": "634951dd-b69d-404b-ade3-4141eca1b76e", "title": "Experimental Detail", "level": "section", "subsections": [], "parent_id": "66ef2e50-c617-457c-84da-f8257ab93ea9", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Experimental Detail"]], "content": "\\label{app:experiment}\nIn the experiment, we utilize 8 demonstrations and test on gpt2~, gptj~, LLaMA3-8B-Instruct and Qwen2-7B-Instruct~. All experiments are executed on a single NVIDIA A100 (80G). For datasets we choose sst2~, sst5~, commonsense\\_qa~, ag\\_news~ and snli~. For the last two datasets, we only select 1000 data from the training set for retrieval and the first 1000 data from the test set for testing. During the inference phase, a PPL-based approach is employed. The entire code framework is built upon OpenICL~, for which we extend our gratitude to the authors.", "cites": [3405, 1096, 7718], "cite_extract_rate": 0.4, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of the experimental setup, including the models, datasets, and methods used. It cites relevant papers but does not synthesize their contributions or connect them to broader themes in ICL. There is no critical evaluation or abstraction of patterns or principles from the cited works."}}
{"id": "bd2bad96-bfd7-49ee-9cce-84cf58960935", "title": "Traditional Tasks", "level": "subsection", "subsections": [], "parent_id": "e33c6336-e49c-4261-adca-a52c2331dc95", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Evaluation and Resources"], ["subsection", "Traditional Tasks"]], "content": "As a general learning paradigm, ICL can be examined on various traditional datasets and benchmarks, e.g., SuperGLUE~, SQuAD~. \nImplementing ICL with 32 randomly sampled examples on SuperGLUE, ~ found that GPT-3 can achieve results comparable to state-of-the-art (SOTA) finetuning performance on COPA and ReCoRD, but still falls behind finetuning on most NLU tasks.\n~ showed the potential of scaling up the number of demonstration examples. However, the improvement brought by scaling is very limited. At present, compared to finetuning, there still remains some room for ICL to reach on traditional NLP tasks.\n\\input{tabs/tab_dataset.tex}", "cites": [1565, 679, 1569, 3379], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic comparison between in-context learning (ICL) and traditional fine-tuning methods on datasets like SuperGLUE and SQuAD, drawing from multiple papers. It connects the idea of ICL performance with the number of demonstration examples but stops short of offering a novel synthesis. There is some critical evaluation, such as noting the limited improvement from scaling examples and the gap between ICL and fine-tuning. However, the analysis remains surface-level and lacks abstraction or identification of overarching principles."}}
{"id": "15a026a5-d602-4a24-8aa8-2f69176da092", "title": "New Challenging Tasks", "level": "subsection", "subsections": [], "parent_id": "e33c6336-e49c-4261-adca-a52c2331dc95", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Evaluation and Resources"], ["subsection", "New Challenging Tasks"]], "content": "In the era of large language models with in-context learning capabilities, researchers are more interested in evaluating the intrinsic capabilities of large language models without downstream task finetuning~.\nTo explore the capability limitations of LLM on various tasks, \n~ proposed the BIG-Bench~, a large benchmark covering  \na large range of tasks, including linguistics, chemistry, biology, social behavior, and beyond. \nThe best models have already outperformed the average reported human-rater results on 65\\% of the BIG-Bench tasks through ICL~. To further explore tasks actually unsolvable by current language models,  proposed a more challenging ICL benchmark, BIG-Bench Hard (BBH). BBH includes 23 unsolved tasks, constructed by selecting challenging tasks where the state-of-art model performances are far below the human performances. Besides, researchers are searching for inverse scaling tasks,\\footnote{\\url{https://github.com/inverse-scaling/prize}} that is, tasks where model performance reduces when scaling up the model size. Such tasks also highlight potential issues with the current paradigm of ICL.\nTo further probe the model generalization ability, ~ proposed OPT-IML Bench, consisting of 2000 NLP tasks from 8 existing benchmarks, especially benchmark for ICL on held-out categories.\nSpecifically, a series of studies focus on exploring the reasoning ability of ICL.~ generated an example from a synthetic world model\nrepresented in first-order logic and parsed the ICL generations into symbolic proofs for formal analysis. They found that LLMs can make correct individual deduction steps via ICL.\n~ constructed the MGSM benchmark to evaluate the chain-of-thought reasoning abilities of LLMs in multilingual settings, finding that LLMs manifest complex reasoning across multiple languages.\nTo further probe more sophisticated planning and reasoning abilities of LLMs, ~ provided multiple test cases for evaluating various reasoning abilities on actions and change, where existing ICL methods on LLMs show poor performance.\nIn addition, ~ proposed a benchmark called SAMSum, which is a human-annotated dataset specifically designed for multi-turn dialogue summarization, to evaluate the quality of dialogue summaries generated by LLMs via ICL.", "cites": [2221, 2215, 7466, 1550, 2220, 5965], "cite_extract_rate": 0.75, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of various challenging benchmarks and tasks in in-context learning, mentioning multiple papers but not synthesizing their findings into a cohesive theme. It lists the goals and results of each benchmark without substantial comparison or deeper analysis of their implications. There is some attempt to categorize types of reasoning tasks (e.g., chain-of-thought, multilingual reasoning), but no meta-level abstraction or critical evaluation of the paradigm or methods is offered."}}
{"id": "0351607d-6a8b-444a-8c0d-074db1af9e50", "title": "Open-source Tools", "level": "subsection", "subsections": [], "parent_id": "e33c6336-e49c-4261-adca-a52c2331dc95", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "Evaluation and Resources"], ["subsection", "Open-source Tools"]], "content": "Noticing that ICL methods are often implemented differently and evaluated using different LLMs and tasks,  developed OpenICL, an open-source toolkit enabling flexible and unified ICL assessment. With its adaptable architecture, OpenICL facilitates the combination of distinct components and offers state-of-the-art retrieval and inference techniques to accelerate the integration of ICL into advanced research.", "cites": [7718], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly introduces OpenICL as an open-source toolkit for ICL assessment but lacks synthesis of ideas beyond this single paper. It does not compare it to other tools or provide a broader context, and no critical analysis or limitations of the tool are discussed. The description remains factual and does not abstract to general principles or evaluate its impact."}}
{"id": "4a7fc6b4-2d7c-41b4-8888-0d93b87f5fb8", "title": "Visual In-Context Learning", "level": "subsection", "subsections": [], "parent_id": "6f8ecda4-ab0a-4fa9-99a1-7bcb0f3e77fe", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "In-Context Learning Beyond Text"], ["subsection", "Visual In-Context Learning"]], "content": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.49\\textwidth]{fig/mm-case.pdf}\n    \\caption{Image-only and textual augmented prompting for visual in-context learning.}\n    \\label{fig:mm_case}\n\\end{figure}\nEmploying masked auto-encoders (MAE) for image patch infilling, the model trained by  generates consistent output images at inference, demonstrating robust ICL capabilities for tasks like image segmentation. This method is expanded in Painter , which incorporates multiple tasks to develop a generalist model with competitive performance. SegGPT  further builds on this by integrating diverse segmentation tasks and exploring ensemble techniques to enhance example quality. Additionally,  introduce the Prompt Diffusion model, the first diffusion-based model with ICL abilities, guided by an extra text prompt for more precise image generation, as illustrated in Figure~\\ref{fig:mm_case}.\nSimilar to ICL in NLP, the effectiveness of visual in-context learning greatly depends on the choice of demonstration images, as shown in research by  and . To optimize this,  examine two strategies: using an unsupervised retriever to select the nearest samples with an existing model, and a supervised approach to train a specialized retriever to boost ICL performance. These approaches improve results by ensuring semantic similarity and better alignment in viewpoint, background, and appearance. Beyond retrieval,  also investigate a prompt fusion technique to further enhance outcomes.", "cites": [3406, 3001, 3407, 9144], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several papers on visual in-context learning, connecting ideas such as MAE-based infilling, multi-task models, and diffusion approaches to build a coherent narrative on the topic. It highlights how demonstration image selection and prompt fusion affect performance, showing some abstraction by identifying broader patterns. While it provides some analysis of the approaches, it lacks deeper critical evaluation or identification of limitations in the cited works."}}
{"id": "0c600619-28cd-4f67-844f-b94c602c69e1", "title": "Multi-Modal In-Context Learning", "level": "subsection", "subsections": [], "parent_id": "6f8ecda4-ab0a-4fa9-99a1-7bcb0f3e77fe", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "In-Context Learning Beyond Text"], ["subsection", "Multi-Modal In-Context Learning"]], "content": "In the vision-language domain, a vision encoder paired with a frozen language model demonstrates multi-modal few-shot learning capabilities after training on image-caption datasets, as shown by the Frozen model . Extending this, Flamingo integrates a vision encoder with large language models (LLMs) for enhanced in-context learning across multi-modal tasks, leveraging large-scale web corpora . Similarly, Kosmos-1 exhibits zero-shot, few-shot, and multi-modal chain-of-thought prompting abilities . METALM introduces a semi-causal language modeling objective to achieve strong ICL performance across vision-language tasks . The ICL-D3IE approach employs a novel in-context learning framework that iteratively updates diverse demonstrationsincluding hard, layout-aware, and formatting demonstrations to train large language models (LLMs) for enhanced document information extraction (DIE). Recent advancements include creating instruction tuning datasets from existing vision-language tasks or with advanced LLMs like GPT-4, connecting LLMs with powerful vision foundational models like BLIP-2 for multi-modal learning .", "cites": [7565, 3003, 2243, 7564, 3015, 2237, 2229, 3408, 2238], "cite_extract_rate": 0.9, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of multi-modal in-context learning by listing several models and their features, but it does not deeply synthesize or connect these works into a broader narrative. There is limited critical analysis or comparison of methods, and no clear abstraction of overarching principles or trends in the field."}}
{"id": "2a212c7e-84d8-4ea3-a5e1-494fcfbb99c9", "title": "Speech In-Context Learning", "level": "subsection", "subsections": [], "parent_id": "6f8ecda4-ab0a-4fa9-99a1-7bcb0f3e77fe", "prefix_titles": [["title", "A Survey on In-context Learning"], ["section", "In-Context Learning Beyond Text"], ["subsection", "Speech In-Context Learning"]], "content": "In the speech area, ~ treated text-to-speech synthesis as a language modeling task. \nThey use audio codec codes as an intermediate representation and propose the first TTS framework with strong in-context learning capability. \nSubsequently, VALLE-X~ extend the idea to multi-lingual scenarios, demonstrating superior performance in zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks.\n\\end{document}", "cites": [3002, 3004], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a minimal synthesis by linking two papers in the context of extending in-context learning to speech, but it lacks deeper integration of ideas. It is primarily descriptive, summarizing each papers contributions without critical evaluation or comparison. No broader patterns or principles of in-context learning in speech are abstracted."}}
