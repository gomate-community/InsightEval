{"id": "47f469f1-f6b8-4bfb-97f2-f6ea2d185e1d", "title": "Introduction", "level": "section", "subsections": ["6d6db78a-7755-4a96-a963-7722f746e735", "dcbe059d-359d-4055-994f-945b8f30668c"], "parent_id": "ec29dc88-a786-4e46-84c8-1cb14580a6e7", "prefix_titles": [["title", "Word Embeddings: A Survey"], ["section", "Introduction"]], "content": "The task of representing words and documents is part and parcel of most, if not all, Natural Language Processing (NLP) tasks. In general, it has been found to be useful to represent them as vectors, which have an appealing, intuitive interpretation, can be the subject of useful operations (e.g. addition, subtraction, distance measures, etc) and lend themselves well to be used in many Machine Learning (ML) algorithms and strategies.\nThe Vector Space Model (VSM), generally attributed to Salton (\\citeyear{salton_1975}) and stemming from the Information Retrieval (IR) community, is arguably the most successful and influential model to encode words and documents as vectors.\nAnother very important part of natural language-based solutions is, of course, the study of language models. A language model is a statistical model of language usage. It focuses mainly on predicting the next word given a number of previous words. This is very useful, for instance, in speech recognition software, where one needs to correctly decide what is the word said by the speaker, even when signal quality is poor or there is a lot of background noise.\nThese two seemingly independent fields have arguably been brought together by recent research on Neural Network Language Models (NNLMs), with ) having developed the first\\footnote{They claim this idea has been put forward before (), but not used at scale.} large-scale language models based on neural nets. \nTheir idea was to reframe the problem as an unsupervised learning problem. A key feature of this solution is the way raw words vectors are first projected onto a so-called \\textit{embedding layer} before being fed into other layers of the network. Among other reasons, this was imagined to help ease the effect of the curse of dimensionality on language models, and help generalization ().\nWith time, such \\textbf{word embeddings} have emerged as a topic of research in and of themselves, with the realization that they can be used as standalone features in many NLP tasks () and the fact that they encode surprisingly accurate syntactic and semantic word relationships ().\nMore recently\\footnote{Their roots, however, date back at least two decades, with the work of .}, other ways of creating embeddings have surfaced, which rely not on neural networks and embedding layers but on leveraging word-context matrices to arrive at vector representations for words. Among the most influential models we can cite the GloVe model ().\nThese two types of model have something in common, namely their reliance on the assumption that words with similar contexts (other words) have the same meaning. This has been called the distributional hypothesis, and has been suggested some time ago by Harris (\\citeyear{harris_1954}), among others.\nThis brings us to the definition of \\textit{word embeddings} we will use in this article, as suggested by the literature (for instance, ), according to which word embeddings are \\textbf{dense, distributed, fixed-length word vectors, built using word co-occurrence statistics as per the distributional hypothesis}.\nEmbedding models derived from neural network language models have () been called \\textit{prediction-based} models, since they usually leverage language models, which predict the next word given its context. Other matrix-based models have been called \\textit{count-based} models, due to their taking into account global word-context co-occurrence counts to derive word embeddings. \\footnote{Note that a link between both types of models has been suggested by .} These are described next.\nThis survey is structured as follows: in section 2 we describe the origins of statistical language modelling. In section 3 we give an overview of word embeddings, generated both by so-called prediction-based models and by count-based methods. In Section 4 we conclude and in Section 5 we provide some pointers to promising further research topics.", "cites": [8369], "cite_extract_rate": 0.1, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The introduction synthesizes foundational concepts from the literature, including the distributional hypothesis, neural network language models, and count-based models like GloVe, but it does so in a largely descriptive manner without deep integration of the cited papers. There is minimal critical evaluation or comparison of methods, and while some abstraction is attempted (e.g., defining word embeddings as 'dense, distributed, fixed-length vectors'), the section remains focused on summarizing known ideas rather than offering novel insights."}}
{"id": "003c97a0-4027-44c4-ace2-53eff27802d0", "title": "Statistical Language Modelling", "level": "subsection", "subsections": [], "parent_id": "a4fed3e5-349d-4412-a8f3-477e813d9b20", "prefix_titles": [["title", "Word Embeddings: A Survey"], ["section", "Background: The Vector Space Model and Statistical Language Modelling"], ["subsection", "Statistical Language Modelling"]], "content": "Statistical language models are probabilistic models of the distribution of words in a language. For example, they can be used to calculate the likelihood of the next word given the words immediately preceding it (its \\textit{context}). One of their earliest uses has been in the field of speech recognition (), to aid in correctly recognizing words and phrases in sound signals that have been subjected to noise and/or faulty channels.\nIn the realm of textual data, such models are useful in a wide range of NLP tasks, as well as other related tasks, such as information retrieval.\nWhile a full probabilistic model containing the likelihood of every word given all possible word contexts that may arise in a language is clearly intractable, it has been empirically observed that satisfactory results are obtained using a context size as small as 3 words (). A simple mathematical formulation of such an \\textit{n-gram model} with window size equal to $T$ follows:\n\\[ P(w_1^T)= \\prod_{t=1}^{T} P(w_t | w_1^{t-1}), \\]\nwhere $w_t$ is the $t$-th word and $w_i^T$ refers to the sequence of words from $w_i$ to $w_T$, i.e. $(w_i, w_{i+1}, w_{i+2} ... w_T)$. $P(w_t|w_1^{t-1})$ refers to the fraction of times $w_t$ appears after the sequence $w_1^{t-1}$. Actual prediction of the next word given a context is done via maximum likelihood estimation (MLE), over all words in the vocabulary.\nSome problems reported with these models have been () the high dimensionality involved in calculating discrete joint distributions of words with vocabulary sizes in the order of 100,000 words and difficulties related to generalizing the model to word sequences not present in the training set.\nEarly attempts of mitigating these effects, particularly those related to generalization to unseen phrases, include the use of smoothing, e.g. pretending every new sequence has count one, rather than zero in the training set (this is referred to as \\textit{add-one} or \\textit{Laplace smoothing}. Also, \\textit{backing off} to increasingly shorter contexts when longer contexts aren't available (). Another strategy which reduces the number of calculations needed and helps with generalization is the clustering of words in so-called \\textit{classes} (cf. now famous Brown Clustering ).\nFinally, neural networks () and log-linear models () have also been used to train language models (giving rise to so-called \\textit{neural} language models), delivering better results, as measured by perplexity.", "cites": [1684, 7165, 8370], "cite_extract_rate": 0.3, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of statistical language modeling, including n-gram models, smoothing techniques, and the shift to neural models. While it mentions some approaches like Brown Clustering and neural language models, it does not deeply synthesize insights across the cited papers or offer a novel narrative. The critical evaluation is minimal, and the abstraction is limited to general mentions of problems and techniques without identifying broader principles or trends."}}
{"id": "bc6e9c55-7d96-4620-a8a7-2057060a43a2", "title": "Word Embeddings", "level": "section", "subsections": ["dd537414-ea52-447a-b64a-6815656f57d2", "614d6369-2709-4c64-9dfd-5a10a1b88ea7"], "parent_id": "ec29dc88-a786-4e46-84c8-1cb14580a6e7", "prefix_titles": [["title", "Word Embeddings: A Survey"], ["section", "Word Embeddings"]], "content": "As mentioned before, word embeddings are fixed-length vector representations for words. There are multiple ways to obtain such representations, and this section will explore various different approaches to training word embeddings, detailing and they work and where they differ from each other.\nWord embeddings are commonly () categorized into two types, depending upon the strategies used to induce them. Methods which leverage local data (e.g. a word's context) are called \\textbf{prediction-based} models, and are generally reminiscent of neural language models. On the other hand, methods that use global information, generally corpus-wide statistics such as word counts and frequencies are called \\textbf{count-based} models. We describe both types next.", "cites": [672], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of word embeddings and categorizes them into prediction-based and count-based models. While it mentions the cited paper in a general context, it does not synthesize multiple sources or connect their ideas into a broader narrative. It lacks critical evaluation and deeper abstraction, offering minimal insight beyond summarizing the classification of methods."}}
{"id": "dd537414-ea52-447a-b64a-6815656f57d2", "title": "Prediction-based Models", "level": "subsection", "subsections": [], "parent_id": "bc6e9c55-7d96-4620-a8a7-2057060a43a2", "prefix_titles": [["title", "Word Embeddings: A Survey"], ["section", "Word Embeddings"], ["subsection", "Prediction-based Models"]], "content": "The history of the development of prediction-based models for embeddings is deeply linked with that of neural language models (NNLMs), because that is how they were initially produced. As mentioned before, a word's \\textit{embedding} is just the projection of the raw word vector into the first layer of such models, the so-called \\textit{embedding layer}.\nThe history of NNLMs, which started with the first large neural language model (), is mostly one of gradual efficiency gains, occasional insights and trade-offs between complex models and simpler models, which can train on more data.\nMuch though early results (as measured by perplexity) clearly indicated that neural language models were indeed better at modelling language than their previous n-gram-based counterparts, long training times (sometimes upwards of days and weeks) are frequently cited among the major factors that hindered the development of such models.\nNot long after the seminal paper by Bengio et al. (\\citeyear{bengio_et_al_2003}), many contributions were made towards increasing efficiency and performance of these models. \n\\renewcommand{\\arraystretch}{1.3}\n\\begin{table*}[h]\n\\small\n\\begin{tabularx}{\\textwidth}{|L{15mm}|X|L{15mm}|X|}\n\\hline\n\\textbf{Article} & \\textbf{Overview of Strategy} & \\textbf{Architecture}  & \\textbf{Notes} \\\\ \\hline \nBengio et al. 2003 & Embeddings are derived as a by-product of training a neural network language model. & Neural Net & Commonly referred to as the first neural network language model. \\\\ \\hline\nBengio and Senecal 2003 & Makes improvements on the previous paper, by using a Monte Carlo method to estimate gradients, bypassing the calculation of costly partition functions. & Neural Net & Decreased training times by a factor of 19 with respect to Bengio et al. 2003. \\\\ \\hline\nMorin and Bengio 2005 & Full softmax prediction is replaced by a more efficient binary tree approach, where only binary decisions at each node leading to the target word are needed. & Neural Net, Hierarchical Softmax  & Report a speed up with respect to Bengio and Senecal 2003 (over three times as fast during training and 100 times as fast during testing), but at a slightly lower score (perplexity). \\\\ \\hline \nMnih and Hinton 2007 & Among other models, the log-bilinear model is introduced here. Log-bilinear models are neural networks with a single, linear, hidden layer (). & Log-linear Model & First appearance of the log-linear model, which is a simpler model, much faster and slightly outscores the model from Bengio et al. (2003). \\\\ \\hline\nMnih and Hinton 2008 & Authors train the log-bilinear model using hierarchical softmax, as suggested in Morin and Bengio (2005), but the word tree is learned rather than obtained from external sources. & Log-linear Model, Hierarchical Softmax  &  Reports being 200 times as fast as previous log-bilinear models.\\\\ \\hline\nCollobert and Weston 2008 & A multi-task neural net is trained using not only unsupervised data but also supervised data such as SRL and POS annotations. The model jointly optimizes all of those tasks, but the target was only to learn embeddings. & Deep Neural Net, Negative Sampling & First time a model was built primarily to output just embeddings. Semi-supervised model (language model + NLP tasks).\\\\ \\hline\nMikolov et al. 2013b & Introduces new two models, namely CBOW and SG. Both are log-linear models, using the two-step training procedure. CBOW predicts the target word given a context, SG predicts each context word given a target word. & Log-linear Model, Hierarchical Softmax & Trained on DistBelief, which is the precursor to TensorFlow (). \\\\ \\hline\nMikolov et al. 2013c & Improvements to CBOW and SG, including negative sampling instead of hierarchical softmax and subsampling of frequent words. & Log-linear Model, Negative Sampling  & SGNS (skip-gram with negative sampling), the best performing variant of Word2Vec, was introduced here.\\\\ \\hline\nBojanowski et al. 2016 & Embeddings are trained at the n-gram level, in order to help generalization for unseen data, especially for languages where morphology plays an important role. & Log-linear Model, Hierarchical Softmax & Reports better results than SGNS. Embeddings are also reported to be good for composition (into sentence, document embeddings). \\\\ \\hline\n\\end{tabularx}\n\\caption{Overview of strategies for building prediction-based models for embeddings.}\n\\end{table*}\nBengio and Sen√®cal (\\citeyear{bengio_and_senecal_2003}) identified that one of the main sources of computational cost was the \\textit{partition function} or \\textit{normalization factor} required by softmax output layers \\footnote{Softmax output layers are used when you train neural networks that need to predict multiple outputs, in this case the probability of each word in the vocabulary being the next word, given the context.}, such as those in neural network language models (NNLMs). Using a concept called \\textit{importance sampling} (), they managed to bypass calculation of the costly normalization factor, estimating instead gradients in the neural net using an auxiliary distribution (e.g. old n-gram language models) and sampling random examples from the vocabulary. They report gains of a factor of 19 in training time, with respect to the previous model, with similar scores (as measured by perplexity).\nA little bit later, Morin and Bengio\\footnote{To our knowledge, this is the first time the term \\textit{word embedding} was used in this context.} (\\citeyear{morin_and_bengio_2005}) have suggested yet another approach for speeding up training and testing times, using a Hierarchical Softmax layer. They realized that, if one arranged the output words in a hierarchical binary tree structure, one could use, as a proxy for calculating the full distribution for each word, the probability that, at each node leading to the word, the correct path is chosen. Since the height of a binary tree over a set \\(V\\) of words is \\(|V|/\\log(|V|)\\), this could yield exponential speedup. In practice, gains were less pronounced, but they still managed gains of a factor of 3 for training times and 100 for testing times, w.r.t. the model using importance sampling.\nMnih and Hinton (\\citeyear{mnih_and_hinton_2007}) were probably the first authors to suggest the Log-bilinear Model\\footnote{These are special cases of \\textit{log-linear} models. See Appendix A for more information.} (LBL), which has been very influential in later works as well.  \nAnother article by Mnih and Hinton (\\citeyear{mnih_and_hinton_2008}) can be seen as an extension of the LBL () model, using a slightly modified version of the hierarchical softmax scheme proposed by Morin and Bengio (\\citeyear{morin_and_bengio_2005}), yielding a so-called Hierarchical Log-bilinear Model (HLBL). Whereas Morin and Bengio (2005) used a pre-built word tree from WordNet, Mnih and Hinton (\\citeyear{mnih_and_hinton_2008}) learned such a tree specifically for the task at hand. In addition to other minor optimizations, they reports large gains over previous LBL models (200 times as fast) and conclude that using purpose-built word trees was key to such results.\nSomewhat parallel to the works just mentioned, Collobert and Weston (\\citeyear{collobert_and_weston_2008}) approached the problem from a slightly different angle; they were the first to design model with the specific intent of learning embeddings only. In previous models, embeddings were just treated as an interesting by product of the main task (usually language models). In addition to this, they also introduced two improvements worth mentioning: they used words' full contexts (before and after) to predict the centre word \\footnote{Previous models focused on building language models, so they just used the left context.}. Perhaps most importantly, they introduced a more clever way of leveraging unlabelled data for producing good embeddings: instead of training a language model (which is not the objective here), they expanded the dataset with \\textit{false} or \\textit{negative} examples \\footnote{I.e. sequences of words with the actual centre word replaced by a random word from the vocabulary.} and simply trained a model that could tell positive (actually occurring) from false examples.\\footnote{This has been () called \\textit{negative sampling} and speeds up training because one can avoid costly operations such as calculating cross-entropies and softmax terms.}\nHere we should mention two specific contributions by Mikolov et al. (\\citeyear{mikolov_et_al_2009,mikolov_et_al_2010}), which have been used in later models. In the first work, () a two-step method for bootstraping a NNLM was suggested, whereby a first model was trained using a single word as context. Then, the full model (with larger context) was trained, using as initial embeddings those found by the first step.\nIn (), the idea of using Recurrent Neural Networks (RNNs) to train language models is first suggested; the argument is that RNNs keep \\textit{state} in the hidden layers, helping the model remember arbitrarily long contexts, and one would not need to decide, beforehand, how many words to use as context in either side. \nIn \\citeyear{mnih_and_teh_2012} Mnih and Teh have suggested further efficiency gains to the training of NNLMs. By leveraging  Noise-contrastive Estimation (NCE). \\footnote{Not to be confused with Contrastive Divergence ().} NCE () is a way of estimating probability distributions by means of binary decisions over true/false examples.\\footnote{This is somewhat similar to negative sampling, as applied by . In fact, negative sampling can be seen as a simplified form of NCE, to be used in cases where you just want to train the model (i.e. obtain embeddings), rather than obtain the full probability distribution over the next word ()}. This has enabled the authors to further reduce training times for NNLMs. In addition to faster training times, they also report better perplexity score w.r.t. previous neural language models. \nIt could be said that, in 2013, with Mikolov et al. (\\citeyear{linguisticregularities,efficientestimation,distreprofwords}) the NLP community have again (the main other example being ) had its attention drawn to word embeddings as a topic worthy of research in and of itself. These authors analyzed the embeddings obtained with the training of a recurrent neural network model () with an eye to finding possible syntactic regularities possibly encoded in the vectors.\nPerhaps surprisingly, event for the authors themselves, they did find not only syntactic but also semantic regularities in the data. Many common relationships such as male-female, singular-plural, etc actually correspond to arithmetical operations one can perform on word vectors (see Figure \\ref{fig1} for an example).\n\\begin{figure}[!htb]\n\\begin{center}\n\\includegraphics[width=6cm]{pictures/word2vec2.png}\n\\caption{Projection of high dimensional word embeddings (obtained with an RNN language model) in 2D: high-level word embeddings encode multiple relationships between words; here shown: singular-plural (dotted line) and male-female (solid line) relationships. Adapted from .}\n\\vspace{-1.5em}\n\\label{fig1}\n\\end{center}\n\\end{figure}\nA little later, in \\citeyear{efficientestimation} and \\citeyear{distreprofwords}, Mikolov et al. have introduced two models for learning embeddings, namely the continuous bag-of-words (\\textbf{CBOW}) and skip-gram (\\textbf{SG}) models. Both of these models are log-linear models (as seen in previous works) and use the two-step procedure () for training. The main difference between CBOW and SG lies in the loss function used to update the model; while CBOW trains a model that aims to predict the centre word based upon its context, in SG the roles are reversed, and the centre word is, instead, used to predict each word appearing in its context.\n\\begin{table*}[h]\n\\small\n\\begin{tabularx}{\\textwidth}{|L{15mm}|X|L{60mm}|}\n\\hline\n\\textbf{Article} & \\textbf{Overview of Strategy} &  \\textbf{Notes} \\\\ \\hline \nDeerwester et al. 1990 & LSA is introduced. Singular value decomposition (SVD) is applied on a term-document matrix. &  Used mostly for IR, but can be used to build word embeddings. \\\\ \\hline\nLund and Burgess 1996 & The HAL method is introduced. Scan the whole corpus one word at a time, with a context window around the word to collect weighted word-word co-occurrence counts, building a word-word co-occurrence matrix. & Reported an optimal context size of 8. \\\\ \\hline\nRohde et al. 2006 & Authors introduce the COALS method, which is an improved version of HAL, using normalization procedures to stop very common terms from overly affecting co-occurrence counts.  & Optimal variant used SVD factorization. Reports gains over HAL (), LSA () and other methods. \\\\ \\hline\nDhillon et al. 2011 & LR-MVL is introduced. Uses CCA (Canonical Correlation Analysis) between left and right contexts to induce word embeddings. & Reports gains over C\\&W embeddings (), HLBL () and other methods, over many NLP tasks. \\\\ \\hline\nLebret and Collobert 2013 & Applied a modified version of Principal Component Analysis (called Hellinger PCA) to the word-context matrix. & Embeddings can be \\textit{tuned} before being used in actual NLP tasks. Also reports gains over C\\&W embeddings, HLBL and other methods, over many NLP tasks. \\\\ \\hline\nPennington et al. 2014 & Introduced \\textit{GloVe}, a log-linear model trained to encode semantic relationships between words as vector offsets in the learned vector space, using the insight that co-occurrence ratios, rather than raw counts, are the actual conveyors of word meaning. & Reports gains over all previous count-based models and also SGNS (), in multiple NLP tasks. \\\\ \\hline\n\\end{tabularx}\n\\caption{Overview of strategies for building count-based models for embeddings.}\n\\end{table*}\nThe first versions of CBOW and SG () use hierarchical softmax layers, while the variants\\footnote{These have been published under the popular \\textit{Word2Vec} toolkit (https://code.google.com/archive/p/word2vec/).} suggested in  use negative sampling instead. Furthermore, the variants introduced subsampling of frequent words, to reduce the amount of noise due to overly frequent words and accelerate training. These variants were shown to perform better, with faster training times.\nAmong the most recent contributions to prediction-based models for word embeddings one can cite the two articles ( and ) usually cited as the sources of the \\textit{FastText}\\footnote{https://research.fb.com/projects/fasttext/} toolkit, made available by Facebook, Inc. They have suggested an improvement over the skip-gram model from , whereby one learns not word embeddings, but \\textit{n-gram} embeddings (which can be composed to form words). The rationale behind this decision lies in the fact that languages that rely heavily on morphology and compositional word-building (such as Turkish, Finnish and other highly inflexional languages) have some information encoded in the word parts themselves, which can be used to help generalize to unseen words. They report better results w.r.t. SGNS (skip-gram variant with negative sampling) (), particularly in languages such as German, French and Spanish.\nA structured comparison of prediction-based models for building word embeddings can be seen on Table 1.", "cites": [1684, 8369, 7165, 673], "cite_extract_rate": 0.23529411764705882, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple cited papers into a coherent narrative, tracing the evolution of prediction-based models and connecting improvements in efficiency and performance over time. It includes critical insights by discussing trade-offs (e.g., speed vs. perplexity), and highlights key innovations such as hierarchical softmax and negative sampling. While it identifies patterns in model design and training strategies, the abstraction remains at a moderate level, focusing more on technical advancements than higher-level theoretical or conceptual frameworks."}}
{"id": "614d6369-2709-4c64-9dfd-5a10a1b88ea7", "title": "Count-based Models", "level": "subsection", "subsections": [], "parent_id": "bc6e9c55-7d96-4620-a8a7-2057060a43a2", "prefix_titles": [["title", "Word Embeddings: A Survey"], ["section", "Word Embeddings"], ["subsection", "Count-based Models"]], "content": "As mentioned before, count-based models are another way of producing word embeddings, not by training algorithms that predict the next word given its context (as is the case in language modelling) but by leveraging word-context co-occurrence counts globally in a corpus. These are very often represented () as word-context matrices.\nThe earliest relevant example of leveraging word-context matrices to produce word embeddings is, of course, \\textit{Latent Semantic Analysis} (LSA) () where SVD is applied to a term-document \\footnote{Term-document matrices are a subset of word-context matrices .} matrix. This solution was initially envisioned to help with information retrieval. While one is probably more interested in document vectors in IR, it's also possible to obtain word vectors this way; one just needs to look at the rows (rather than columns) of the factorized matrix.\nA little later,  have introduced the \\textit{Hyperspace Analogue to Language} (HAL). Their strategy can be described as follows: for each word in the vocabulary, analyze all \\textit{contexts} it appears in and calculate the co-occurrence count between the target word and each context word, inversely proportional to the distance from the context word to the target word. The authors report good results (as measured by analogy tasks), with an optimal context window size of 8.\nThe original HAL model did not apply any normalization to word co-occurrence counts found. Therefore, very common words like \\textit{the} contribute disproportionately to all words that co-occur with them.  have found this to be a problem, and introduced the \\textit{COALS} method, introducing normalization strategies to factor out such frequency differences in words. Instead of using raw counts, they suggest it's better to consider the \\textit{conditional} co-occurrence, i.e. how much more more likely a word $a$ is to co-occur with word $b$ than it is to co-occur with a random word from the vocabulary. They report better results than previous methods, using the SVD-factorized variant\\footnote{I.e., factorizing the co-occurrence matrix in order to reduce dimensions and improve results.}.\nA somewhat different alternative was proposed by , in which they introduce the \\textit{Low Rank Multi-View Learning} (LR-MVL) method. In short, it's an iterative algorithm where embeddings are derived by leveraging Canonical Correlation Analysis (CCA) () between the left and right contexts of a given word. One interesting feature of this model is that when embeddings are used for downstream NLP tasks, they are concatenated with embeddings for their context words too, yielding better results. Authors report gains over other matrix factorization methods, as well as neural embeddings, over many NLP tasks.\n have also contributed to count-based models by suggesting that a Hellinger PCA\\footnote{This amounts to minimizing the distance between principal components and actual data, but using the Hellinger distance instead of the more common Euclidean distance.} transformation be applied to the word-context matrix instead. Results are reported to be better than previous count-based models such as LR-MVL and neural embeddings, such as those by  and HLBL .\nThe last model we will cover in this section is the well-known \\textit{GloVe}\\footnote{https://nlp.stanford.edu/projects/glove/} by . This model starts at the insight that \\textit{ratios} of co-occurrences, rather than raw counts, encode actual semantic information about pair of words. This relationship is used to derive a suitable loss function for a log-linear model, which is then trained to maximize the similarity of every word pair, as measured by the ratios of co-occurrences mentioned earlier. Authors report better results than other count-based models, as well as prediction based models such as SGNS (), in tasks such as word analogy and NER (named entity recognition).\nA structured comparison of count-based models for building word embeddings can be seen on Table 2.", "cites": [1684, 8371, 7263], "cite_extract_rate": 0.2727272727272727, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates several count-based models into a coherent narrative, explaining their motivations, techniques, and reported results. It also highlights differences and improvements, such as the shift from raw co-occurrence counts to normalized or ratio-based approaches. While it includes some critical evaluation (e.g., issues with common words in HAL), the analysis is not deeply nuanced and could benefit from more systematic comparison or theoretical framing."}}
{"id": "87b76636-75ef-47b8-9cf3-e9129a6ba162", "title": "Adapting embeddings for task-specific work", "level": "subsection", "subsections": [], "parent_id": "8ff3fd81-eb5a-4aba-9898-0ca96249ab63", "prefix_titles": [["title", "Word Embeddings: A Survey"], ["section", "Further Work"], ["subsection", "Adapting embeddings for task-specific work"]], "content": "Works such as ,  and  have highlighted improved results for NLP tasks when embeddings are \\textit{tuned} for specific tasks.", "cites": [7263], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides minimal synthesis and lacks any meaningful integration or comparison of the cited works. It only mentions that certain papers have shown improved results when embeddings are tuned for specific tasks, without elaborating on how these approaches differ or align. There is no critical evaluation or abstraction beyond the specific papers, making the insight quality low."}}
{"id": "e935bc34-c9ef-4a49-b931-dfac572f5215", "title": "The link between prediction-based and count-based models", "level": "subsection", "subsections": [], "parent_id": "8ff3fd81-eb5a-4aba-9898-0ca96249ab63", "prefix_titles": [["title", "Word Embeddings: A Survey"], ["section", "Further Work"], ["subsection", "The link between prediction-based and count-based models"]], "content": "For example,  have suggested that the SGNS model () actually is equivalent to using a slightly modified word-context matrix, weighted using PMI (pointwise mutual information) statistics. Insight on what links the two models may yield more advances in both areas.", "cites": [1684], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a minimal analytical discussion by mentioning a conceptual link between prediction-based and count-based models as suggested by a cited paper. It lacks deeper synthesis of ideas, critical evaluation of the strengths or weaknesses of the models, and abstraction to broader patterns or principles. The insight is limited to a brief observation rather than a comprehensive or nuanced analysis."}}
{"id": "ddee35ff-0002-465e-bd99-2a8879aaf33f", "title": "Composing word embeddings for higher-level entities", "level": "subsection", "subsections": [], "parent_id": "8ff3fd81-eb5a-4aba-9898-0ca96249ab63", "prefix_titles": [["title", "Word Embeddings: A Survey"], ["section", "Further Work"], ["subsection", "Composing word embeddings for higher-level entities"]], "content": "While research on how to compose word vectors to represent higher-level entities such as sentences and documents is not altogether new (generally under the name of \\textit{distributional compositionality}), recent works have adapted solutions specifically for neural word embeddings: we can cite here \\textit{Paragraph2Vec} (), \\textit{Skip-Thought Vectors} by  and also \\textit{FastText} itself ( and ).\n\\appendix", "cites": [7264, 7265, 673, 8369], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal synthesis by listing a few methods for composing word embeddings but fails to connect their underlying ideas or principles. There is no critical evaluation of the cited works, and the content remains descriptive without abstracting broader trends or concepts. The lack of analysis or integration limits its insight quality."}}
