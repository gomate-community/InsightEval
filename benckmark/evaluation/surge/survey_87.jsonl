{"id": "3e165ffa-29cf-4605-ba88-0e6c84a411e5", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "48e1448c-68e1-4eb0-9088-8c4cfe65bb83", "prefix_titles": [["title", "How should my chatbot interact? A survey on social characteristics in human-chatbot interaction design"], ["section", "Introduction"]], "content": "\\label{sec:introduction}\nChatbots are computer programs that interact with users in natural language . The origin of the chatbot concept dates back to 1950 . ELIZA  and A.L.I.C.E.  are examples of early chatbot technologies, where the main goal was to mimic human conversations. Over the years, the chatbot concept has evolved. Today, chatbots can have distinct and diverse characteristics, which has resulted in several synonyms, such as multimodal agents, chatterbots, and conversational interfaces. In this survey, we use the term ``\\textit{chatbot}'' to refer to \\textit{a disembodied conversational agent that holds a natural language conversation via text-based environment to engage the user in either a general-purpose or task-oriented conversation.}\nChatbots are changing the patterns of interactions between humans and computers . Many instant messenger tools, such as Skype, Facebook Messenger, and Telegram provide platforms to develop and deploy chatbots, which either engage with users in general conversations or help them solve domain specific tasks . As messaging tools increasingly become platforms, traditional websites and apps are providing space for this new form of human-computer interaction (HCI) . For example, in the 2018 F8 Conference, Facebook announced that it had 300K active chatbots on Facebook Messenger . The BotList \\footnote{https://botlist.co/} website indexes thousands of chatbots for education, entertainment, games, health, productivity, travel, fun, and several other categories. The growth of chatbot technology is changing how companies engage with their customers , students engage with their learning groups , and patients self-monitor the progress of their treatment , among many other applications.\nHowever, chatbots still fail to meet users' expectations . While many studies on chatbot design focus on improving chatbots' functional performance and accuracy (see, e.g., ), the literature has consistently suggested that chatbots' interactional goals should also include social capabilities . According to the Media Equation theory , people naturally respond to social situations when interacting with computers . As chatbots are designed to interact with users in a way that mimics person-to-person conversations, new challenges in HCI arise . Neururer and colleagues (\\citeyear{neururer2018perceptions}) state that making a conversational agent acceptable to users is primarily a social, not only technical, problem. Studies on chatbots have shown that people prefer agents who: conform to gender stereotypes associated with tasks ; self-disclose and show reciprocity when recommending ; and demonstrate a positive attitude and mood . When chatbots do not meet these expectations, the user may experience frustration and dissatisfaction . In contrast, designing overly humanized agents results in uncanny feelings and increased expectations , which also negatively impacts the interaction. The challenge remains as to what social characteristics are relevant for improving chatbots' communication and social skills and in which domains they have shown to be beneficial.\nAlthough chatbots' social characteristics have been explored in the literature, this knowledge is spread across several domains in which chatbots have been studied, such as customer services, education, finances, and travel. In the HCI domain, some studies focus on investigating the social aspects of human-chatbot interactions (see, e.g., ). However, most studies focus on a single or small set of characteristics (e.g., ); in other studies, the social characteristics emerged as secondary, exploratory results (e.g., ). It has become difficult to find evidence regarding what characteristics are important for designing a particular chatbot, and what research opportunities exist in the field.\nWhilst the literature has extensively reviewed the technical aspects of chatbots design (e.g., ), a lack of studies brings together the social characteristics that influence the way users perceive and behave toward chatbots. To fill this gap, this survey compiles research initiatives for understanding the impact of chatbots' social characteristics on the interaction. We bring together literature that is spread across several research areas. From our analysis of 56 scientific studies, we derive a conceptual model of social characteristics, aiming to help researchers and designers identify the subset of characteristics that are relevant to their context and how adopting--or neglecting--a particular characteristic may influence the way humans perceive the chatbots. The research question that guided our investigation was: \\textbf{What chatbot social characteristics benefit human interaction and what are the challenges and strategies associated with them?}\nTo answer this question, we discuss why designing a chatbot with a particular characteristic can enrich the human-chatbot interaction. Our results provide insight into whether the characteristic is desirable for a particular chatbot, so that designers can make informed decisions by selecting the appropriate subset of characteristics, as well as inspire researchers' further investigations. In addition, we discuss the influence of humanness and the conversational context on users' perceptions as well as the interrelationship among the identified characteristics. We stated 22 propositions about how social characteristics may influence one another. In the next section, we present an overview of the studies included in this survey.", "cites": [6262], "cite_extract_rate": 0.02631578947368421, "origin_cites_number": 38, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The introduction synthesizes chatbot literature by connecting early chatbot examples to current platforms and user expectations, and it integrates references to explain the importance of social characteristics in HCI. It identifies gaps in existing research, particularly the lack of focus on social features. The section abstracts the concept of chatbots and their evolving roles, pointing toward broader implications for design and user experience, though it does not yet develop a full meta-framework."}}
{"id": "ad5096ab-28b5-4a2f-b84b-74bcbea02a02", "title": "Overview of the surveyed literature", "level": "section", "subsections": [], "parent_id": "48e1448c-68e1-4eb0-9088-8c4cfe65bb83", "prefix_titles": [["title", "How should my chatbot interact? A survey on social characteristics in human-chatbot interaction design"], ["section", "Overview of the surveyed literature"]], "content": "\\label{sec:overview}\nThe literature presents no coherent definition of chatbots; thus, to find relevant studies we used a search string that includes the synonyms \\textit{``chatbots, chatterbots, conversational agents, conversational interfaces, conversational systems, conversation systems, dialogue systems, digital assistants, intelligent assistants, conversational user interfaces, and conversational UI''}. We explicitly left out studies that relate to embodiment (\\textit{``ECA, multimodal, robots, eye-gaze, gesture''}), and speech input mode (\\textit{``speech-based, speech-recognition, voice-based''}). The search string did not include the term ``social bots'', because it refers to chatbots that produce content for social networks such as Twitter . We did not include ``personal assistants'' either, since this term consistently refers to commercially available, voice-based assistants such as Google Assistant, Amazon's Alexa, Apple's Siri, and Microsoft's Cortana.\nWe decided to not include terms that relate to social characteristics/traits, because most studies do not explicitly label their results as such. Additionally, to find studies from a variety of domains (not only computing), we used Google Scholar to search for papers. We started with a set of about one thousand papers. In the first round, we reduced this amount by about half based on the titles. We also removed papers for which full text was not available or written in a language other than English. We started the second round with 464 studies. In this round, we excluded short/position papers, theses, book chapters, and documents published in non-scientific venues. We also read the abstracts and removed all the papers that focused on technical aspects, rather than social ones. The third and last round started with 104 papers. After reading the full texts, we removed the papers that either did not highlight social characteristics or represented previous, ongoing research of another complete study from the same authors. The datasets with all analyzed studies (original list of results and exclusions per round) is available in .\nAfter filtering the search results, we had 56 remaining studies. Most of the selected studies are recent publications (less than 10 years old). The publication venues include the domains of human-computer interactions (25 papers), learning and education (8 papers), information and interactive systems (8 papers), virtual agents (5 papers), artificial intelligence (3 papers), and natural language processing (2 papers). We also found papers from health, literature \\& culture, computer systems, communication, and humanities (1 paper each). Most papers (59\\%) focus on task-oriented chatbots. General purpose chatbots reflect 33\\% of the surveyed studies. Most general purpose chatbots (16 out of 19) are designed to handle topic-unrestricted conversations. The most representative specific domain is education, with 9 papers, followed by customer services, with 5 papers. See the supplementary materials for the complete list of topics. \nMost surveyed studies adopted real chatbots (35 out of 56); among them, 18 studies analyze logs of conversations or users' perceptions of third-party chatbots such as Cleverbot (e.g., ), Talkbot (e.g., ), and Woebot . Nine studies introduce a self-developed architecture and/or dialogue management (e.g. ). In another nine studies, the chatbots were designed for research purposes using third-party platforms for chatbot development and deployment, such as IBM's Watson service (e.g., ) and Microsoft's Bot Framework  as well as pattern-matching packages such as Artificial Intelligence Markup Language (AIML) . When a chatbot was simulated (11 studies), Wizard of Oz (WoZ) is the most used technique (9 studies). In a WoZ study, participants believe to be interacting with a chatbot when, in fact, a person (or ``wizard'') pretends to be the automated system . Eight studies do not address a particular chatbot. See the supplementary materials for details.\nWe analyzed the papers by searching for chatbot behavior or attributed characteristics that influence the way users perceive it and behave toward it. Noticeably, the characteristics and categories are seldom explicitly highlighted to in the literature, so the conceptual model was derived using a qualitative coding process inspired by methods such as Grounded Theory  (open coding stage). For each study (\\textit{document}), we selected relevant statements from the paper (\\textit{quotes}) and labeled them as a characteristic (\\textit{code}). These steps were performed by one researcher (the first author). After coding all the studies, a second researcher (the second author) reviewed the produced set of characteristics and both researchers participated in discussion sessions to identify characteristics that could be merged, renamed, or removed. At the end, the characteristics were grouped into the categories, depending on whether the characteristic relates to the chatbot's virtual representation, conversational behavior, or social protocols. Finally, the quotes for each characteristic were labeled as references to benefits, challenges, or strategies.\nWe derived a total of 11 social characteristics, and grouped them into three categories, as depicted in Table \\ref{tab:conceptualmodel}: \\textbf{conversational intelligence}, \\textbf{social intelligence}, and \\textbf{personification}. The next section describes the derived conceptual model.\n\\begin{landscape}\n\\begin{table}[ht]\n\\scriptsize\n\\centering\n\\caption{Conceptual model of chatbots social characteristics. Eleven characteristics found in the literature were grouped into three main categories, depending on whether the social characteristic relates to the chatbot's conversational behavior, social protocols, or virtual representation. For each characteristic, we point out the benefits, challenges, and strategies of implementing the characteristic in chatbots as reported in the literature.}\n\\vspace{1mm}\n\\begin{tabular}{c|p{2.5cm}|p{6cm}|p{6cm}|p{6cm}|}\n\\cline{2-5}\n & \\textbf{\\shortstack{\\\\Social\\\\Characteristics}}& \\multicolumn{1}{c|}{\\textbf{Benefits}} & \\multicolumn{1}{c|}{\\textbf{Challenges}} & \\multicolumn{1}{c|}{\\textbf{Strategies}} \\\\ \\hline\n\\multicolumn{1}{|c|}{\\multirow{14}{*}{\\rotatebox{90}{\\textbf{\\shortstack{Conversational\\\\Intelligence}}}}} & \\multirow{6}{*}{\\textit{Proactivity}} & \\textbf{[B1]} to provide additional information & \\textbf{[C1]} timing and relevance & \\textbf{[S1]} to leverage conversational context \\\\\n\\multicolumn{1}{|l|}{} &  & \\textbf{[B2]} to inspire users and to keep the conversation alive & \\textbf{[C2]} privacy & \\textbf{[S2]} to select a topic randomly \\\\\n\\multicolumn{1}{|l|}{} &  & \\textbf{[B3]} to recover from a failure & \\textbf{[C3]} users' perception of being controlled &  \\\\\n\\multicolumn{1}{|l|}{} &  & \\textbf{[B4]} to improve conversation productivity &  &  \\\\\n\\multicolumn{1}{|l|}{} &  & \\textbf{[B5]} to guide and engage users &  &  \\\\ \\cline{2-5}\n\\multicolumn{1}{|l|}{} & \\multirow{4}{*}{\\textit{Conscientiousness}} & \\textbf{[B1]} to keep the conversation on track & \\textbf{[C1]} to handle task complexity & \\textbf{[S1]} conversational flow \\\\\n\\multicolumn{1}{|l|}{} &  & \\textbf{[B2]} to demonstrate understanding & \\textbf{[C2]} to harden the conversation & \\textbf{[S2]} visual elements \\\\\n\\multicolumn{1}{|l|}{} &  & \\textbf{[B3]} to hold a continuous conversation & \\textbf{[C3]} to keep the user aware of the chatbot's context & \\textbf{[S3]} confirmation messages \\\\ \\cline{2-5}\n\\multicolumn{1}{|l|}{} & \\multirow{4}{*}{\\textit{Communicability}} & \\textbf{[B1]} to unveil functionalities & \\textbf{[C1]} to provide business integration & \\textbf{[S1]} to clarify the purpose of the chatbot \\\\ \n\\multicolumn{1}{|l|}{} &  & \\textbf{[B2]} to manage the users' expectations & \\textbf{[C2]} to keep visual elements consistent with textual inputs & \\textbf{[S2]} to advertise the functionality and suggest the next step \\\\\n\\multicolumn{1}{|l|}{} &  &  &  & \\textbf{[S3]} to provide a help functionality \\\\\\hline\n\\multicolumn{1}{|l|}{\\multirow{19}{*}{\\rotatebox{90}{\\textbf{\\shortstack{Social\\\\Intelligence}}}} }& \\multirow{6}{*}{\\textit{Damage control}} & \\textbf{[B1]} to appropriately respond to harassment & \\textbf{[C1]} to deal with unfriendly users & \\textbf{[S1]} emotional reactions \\\\\n\\multicolumn{1}{|l|}{} &  & \\textbf{[B2]} to deal with testing & \\textbf{[C2]} to identify abusive utterances & \\textbf{[S2]} authoritative reactions \\\\\n\\multicolumn{1}{|l|}{} &  & \\textbf{[B3]} to deal with lack of knowledge & \\textbf{[C3]} to fit the response to the context & \\textbf{[S3]} to ignore the user's utterance and change the topic \\\\\n\\multicolumn{1}{|l|}{} &  &  &  & \\textbf{[S4]} \\textit{conscientiousness} and \\textit{communicability} \\\\\n\\multicolumn{1}{|l|}{} &  &  &  & \\textbf{[S5]} to predict users' satisfaction \\\\\\cline{2-5}\n\\multicolumn{1}{|l|}{} & \\multirow{2}{*}{\\textit{Thoroughness}} & \\textbf{[B1]} to increase human-likeness & \\textbf{[C1]} to decide how much to say & Not identified \\\\\n\\multicolumn{1}{|l|}{} &  & \\textbf{[B2]} to increase believability & \\textbf{[C2]} to be consistent &  \\\\ \\cline{2-5}\n\\multicolumn{1}{|l|}{} & \\multirow{2}{*}{\\textit{Manners}} & \\textbf{[B1]} to increase human-likeness & \\textbf{[C1]} to deal with face-threatening acts & \\textbf{[S1]} to engage in small talk \\\\\n\\multicolumn{1}{|l|}{} &  &  & \\textbf{[C2]} to end a conversation gracefully & \\textbf{[S2]} to adhere turn-taking protocols \\\\ \\cline{2-5}\n\\multicolumn{1}{|l|}{} & \\multirow{3}{*}{\\textit{Moral agency}} & \\textbf{[B1]} to avoid stereotyping & \\textbf{[C1]} to avoid alienation & Not identified \\\\\n\\multicolumn{1}{|l|}{} &  & \\textbf{[B2]} to enrich interpersonal relationships & \\textbf{[C2]} to build unbiased training data and algorithms &  \\\\ \\cline{2-5}\n\\multicolumn{1}{|l|}{} & \\multirow{3}{*}{\\textit{\\shortstack{Emotional\\\\intelligence}}} & \\textbf{[B1]} to enrich interpersonal relationships & \\textbf{[C1]} to regulate affective reactions & \\textbf{[S1]} to use social-emotional utterances \\\\\n\\multicolumn{1}{|l|}{} &  & \\textbf{[B2]} to increase engagement &  & \\textbf{[S2]} to manifest \\textit{conscientiousness} \\\\\n\\multicolumn{1}{|l|}{} &  & \\textbf{[B3]} to increase believability &  & \\textbf{[S3]} reciprocity and self-disclosure \\\\ \\cline{2-5}\n\\multicolumn{1}{|l|}{} & \\multirow{3}{*}{\\textit{Personalization}} & \\textbf{[B1]} to enrich interpersonal relationships & \\textbf{[C1]} privacy & \\textbf{[S1]} to learn from and about the user \\\\\n\\multicolumn{1}{|l|}{} &  & \\textbf{[B2]} to provide unique services &  & \\textbf{[S2]} to provide customizable agents \\\\\n\\multicolumn{1}{|l|}{} &  & \\textbf{[B3]} to reduce interactional breakdowns &  & \\textbf{[S3]} visual elements \\\\ \\hline\n\\multicolumn{1}{|l|}{\\multirow{5}{*}{\\rotatebox{90}{\\textbf{\\shortstack{Personifi-\\\\cation}}}} }& \\multirow{3}{*}{\\textit{Identity}} & \\textbf{[B1]} to increase engagement & \\textbf{[C1]} to avoid negative stereotypes & \\textbf{[S1]} to design and elaborate on a persona \\\\\n\\multicolumn{1}{|l|}{} &  & \\textbf{[B2]} to increase human-likeness & \\textbf{[C2]} to balance the \\textit{identity} and the technical capabilities &  \\\\\\cline{2-5}\n\\multicolumn{1}{|l|}{} & \\multirow{2}{*}{\\textit{Personality}} & \\textbf{[B1]} to increase believability & \\textbf{[C1]} to adapt humor to the users' culture & \\textbf{[S1]} to use appropriate language \\\\\n\\multicolumn{1}{|l|}{} &  & \\textbf{[B2]} to enrich interpersonal relationships & \\textbf{[C2]} to balance the \\textit{personality} traits & \\textbf{[S2]} to have a sense of humor \\\\\n\\hline\n\\end{tabular}\n\\label{tab:conceptualmodel}\n\\end{table}\n\\end{landscape}", "cites": [6263, 6262], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by grouping diverse studies into a conceptual model of 11 social characteristics. It shows critical engagement by discussing limitations such as the lack of explicit labeling of social traits and the challenges in distinguishing between task-oriented and general-purpose chatbots. The abstraction is evident in the creation of a structured framework (conversational intelligence, social intelligence, personification) that generalizes findings beyond individual papers."}}
{"id": "9ad3c98e-3389-49e5-a57e-7e261302fc58", "title": "Conversational Intelligence", "level": "subsection", "subsections": ["30b70a69-50cb-4695-9975-c797ccdef4e2", "8d7c33ab-b84e-4247-bfcd-ac939fd2d82f", "d1a8a8a3-347c-4cde-8b5a-76ed8060b75f"], "parent_id": "5950bb1f-b30c-410b-94c0-7eba5ce91d72", "prefix_titles": [["title", "How should my chatbot interact? A survey on social characteristics in human-chatbot interaction design"], ["section", "Chatbots Social Characteristics"], ["subsection", "Conversational Intelligence"]], "content": "\\label{sec:conversationalintelligence}\n\\textbf{Conversational intelligence} enables the chatbot to actively participate in the conversation and to demonstrate awareness of the topic discussed, the evolving conversational context, and the flow of the dialogue. Therefore, \\textbf{conversational Intelligence} refers to the ability of a chatbot to effectively converse beyond the technical capability of achieving a conversational goal . In this section, we discuss social characteristics related to \\textbf{conversational intelligence}, namely: \\textit{proactivity} (18 studies), \\textit{conscientiousness} (11 studies), and \\textit{communicability} (6 studies). Most of the studies rely on data that comes from the log of the conversations, interviews, and questionnaires. The questionnaires are mostly Likert-scales, and some of them include subjective feedback. Most studies analyzed the interaction with real chatbots, although Wizard of Oz (WoZ) settings are also common. Only two papers did not evaluate a particular type of interaction because they were based on a literature review  and surveys with chatbot users in general . Only five studies applied only quantitative methods, while seven focused on qualitative methods. The majority of the studies (15) applied mixed methods (both quantitative and qualitative). See the supplementary materials for details.\n\\begin{table}[!btp]\n\\scriptsize\n\\centering\n\\caption{Studied social characteristics per domain. Research in open domain chatbots has reported most of the social characteristics, except for \\textit{Communicability}. In he task-oriented domains, some characteristics are largely influenced by the topic (e.g., \\textit{Moral agency} and \\textit{Personality}), while others are more generally applied (e.g., \\textit{Manners} and \\textit{Damage control}).}\n\\begin{tabular}{p{1.6cm}p{5cm}p{6.5cm}}\n\\hline\n\\textbf{Domain} & \\textbf{Social Characteristics} & \\textbf{Studies} \\\\\n\\hline\n\\multirow{2}{*}{\\shortstack[l]{Open\\\\domain}} & \\textit{Proactivity, Conscientiousness, Damage control, Thoroughness, Manners, Moral agency, Emotional intelligence, Personalization, Identity, Personality} &                 \\\\ \n\\hline\nEthnography & \\textit{Proactivity, Conscientiousness, Thoroughness, Personalization} &  \\\\ \n\\hline\n\\multirow{2}{*}{\\shortstack[l]{Task\\\\management}} & \\textit{Proactivity, Damage control, Manners, Personalization, Identity} &   \\\\ \n\\hline\nTourism & \\textit{Proactivity, Thoroughness, Manners} &  \\\\ \n\\hline\nBusiness & \\textit{Proactivity, Personalization} &  \\\\ \n\\hline\nInformation search & \\textit{Proactivity, Damage control, Manners, Emotional intelligence} &   \\\\ \n\\hline\nDecision-making & \\textit{Proactivity, Damage control, Manners} &  \\\\\n\\hline\nHealth-care & \\textit{Proactivity, Emotional intelligence} &   \\\\ \n\\hline\n\\shortstack{Credibility\\\\assessment} & \\textit{Proactivity, Conscientiousness} &  \\\\ \n\\hline\nEducation & \\textit{Proactivity, Conscientiousness, Damage control, Thoroughness, Manners, Emotional intelligence, Identity, Personality} &          \\\\ \n\\hline\n\\multirow{2}{*}{\\shortstack[l]{Financial\\\\services}} & \\textit{Conscientiousness, Communicability, Damage control, Thoroughness, Personalization, Identity} &   \\\\ \n\\hline\n\\multirow{2}{*}{\\shortstack[l]{Customer\\\\services}} & \\textit{Conscientiousness, Communicability, Damage control, Thoroughness, Manners, Emotional intelligence, Personalization, Identity} &      \\\\ \n\\hline\nE-commerce & \\textit{Conscientiousness, Manners} &   \\\\ \n\\hline\nNews & \\textit{Communicability} &  \\\\ \n\\hline\n\\multirow{2}{*}{\\shortstack[l]{Human\\\\resources}} & \\textit{Communicability, Damage control, Manners, Identity} &  \\\\ \n\\hline\n\\multirow{2}{*}{\\shortstack[l]{Virtual\\\\assistant}} & \\textit{Thoroughness, Emotional intelligence, Personalization, Identity} &   \\\\ \n\\hline\nGaming & \\textit{Thoroughness, Emotional intelligence, Personality} &   \\\\ \n\\hline\nRace-talk & \\textit{Moral agency, Identity} &   \\\\ \n\\hline\nHumorous talk & \\textit{Personality} &  \\\\ \n\\hline\nNot defined & \\textit{Proactivity, Conscientiousness, Communicability, Damage control, Personalization, Identity, Personality} &    \\\\\n\\hline\n\\end{tabular}\n\\label{tab:domain-analysis}\n\\end{table}", "cites": [6262, 7476], "cite_extract_rate": 0.03508771929824561, "origin_cites_number": 57, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a descriptive overview of conversational intelligence and the associated social characteristics, supported by a table categorizing their presence across domains. While it integrates data from cited works, the synthesis remains surface-level and lacks deeper connections or a novel framework. Critical analysis is minimal, with no evaluation of limitations or contradictions in the cited papers. The abstraction level is modest, as it begins to generalize characteristics by domain but does not offer overarching principles or theoretical insights."}}
{"id": "30b70a69-50cb-4695-9975-c797ccdef4e2", "title": "Proactivity", "level": "subsubsection", "subsections": [], "parent_id": "9ad3c98e-3389-49e5-a57e-7e261302fc58", "prefix_titles": [["title", "How should my chatbot interact? A survey on social characteristics in human-chatbot interaction design"], ["section", "Chatbots Social Characteristics"], ["subsection", "Conversational Intelligence"], ["subsubsection", "Proactivity"]], "content": "\\label{sec:proactivity}\n\\textit{Proactivity} is the capability of a system to \nautonomously act on the user's behalf  and thereby reduce the amount of human effort to complete a task . In human-chatbot conversations, a proactive behavior enables a chatbot to share initiative with the user, contributing to the conversation in a more natural way . Chatbots may manifest \\textit{proactivity} when they initiate exchanges, suggests new topics, provide additional information, or formulate follow-up questions. In this survey, we found 18 papers that report either chatbots with proactive behavior or implications of manifesting a proactive behavior. \\textit{Proactivity} (also addressed as ``\\textit{intervention mode}'') was explicitly addressed in seven studies . In most of the studies, however, \\textit{proactivity} emerged either as an exploratory result, mostly from post-intervention interviews and user's feedback , or as a strategy to attend to domain-specific requirements (e.g., monitoring, and guidance) . \\textit{Proactivity} was mostly investigated in open domain and education chatbots (four studies each). In open domain chatbots, \\textit{proactivity} helps improve engagement by introducing new topics to keep the conversation alive . Educational chatbots rely on \\textit{proactivity} to prompt students to think, share, and collaborate (e.g., ). \\textit{Proactivity} was also observed in eight other task-oriented domains, including task management and information searches as can be observed in the supplementary materials.\nThe surveyed literature evidences several benefits of chatbot \\textit{proactivity} in chatbots: \n\\textbf{[B1] to provide additional, useful information:} literature reveals that \\textit{proactivity} in chatbots adds value to interactions . Investigating evaluation criteria for chatbots,  asked users of a general purpose chatbot to rate the chatbots' naturalness and report in what areas they excel. Both statistical and qualitative results confirm that taking the lead and suggesting specialized information about the conversation theme correlate to chatbots' naturalness.  corroborates this result; in post-intervention interviews, ten out of 14 users mentioned they preferred a chatbot that takes the lead and volunteers additional information, such as useful links and song playlists. In a WoZ study,  investigated whether proactive interventions of a chatbot contribute to a collaborative search in a group chat. The chatbot either elicits or infers needed information from the collaborative chat and proactively intervenes in the conversation by sharing useful search results. The intervention modes were not significantly different from each other, but both intervention modes resulted in a statistically significant increase of enjoyment and decrease of effort when compared to the same task with no chatbot interventions. Moreover, in a post-intervention, open-ended question, 16 out of 98 participants self-reported positive perceptions about the provided additional information.\n\\textbf{[B2] to inspire users, and keep the conversation alive: }proactively suggesting and encouraging new topics have been shown useful to both inspire users  and keep the conversation alive . Participants in the study conducted by  self-reported that the chatbot's suggestions helped them to get started (7 mentions) and gave them ideas about search topics (4 mentions). After iteratively evaluating prototypes for a chatbot in an educational scenario,  concluded that proactively initiating topics makes the dialogue more fun and reveals topics the chatbot can talk about. The refined prototype also proactively maintains the engagement by posing a follow-up when the student had not provided an answer to the question.  hypothesized that including follow-up questions based on the content of previous messages would result in higher perceived partner engagement. The hypothesis was supported, with participants in the dynamic condition rating the chatbot as more engaging. In an ethnographic data collection , users included photos in their responses to add information about their experience; 85\\% of these photos were proactively prompted by the chatbot. This result shows that prompting the user for more information stimulates them to expand their entries.  also observed that chatbots' proactive messages provided insights about the chatbots' knowledge, which potentially helped the conversation to continue. In this paper, we call the strategies to convey the chatbot's knowledge and capabilities as \\textit{communicability}, and we discuss it in Section \\ref{sec:communicability}.\n\\textbf{[B3] to recover the chatbot from a failure: }in  and , \\textit{proactivity} is employed to naturally recover from a failure. In both studies, the approach was to introduce a new topic when the chatbot failed to understand the user or could not find an answer, preventing the chatbot from getting stuck and keeping the conversation alive. Additionally, in , the chatbot inserted new topics when users are either abusive or non-sensical. We call the strategies to handle failure and abusive behavior as \\textit{damage control}, and we discuss this characteristic in Section \\ref{sec:damagecontrol}.\n\\textbf{[B4] to improve conversation productivity: }in task-oriented interactions, such as searching or shopping, \\textit{proactivity} can improve the conversation's productivity . In interviews with first-time users of chatbots,  found that chatbots should ask follow-up questions to resolve and maintain the context of the conversation and reduce the time searching before achieving the goal.  found similar results for collaborative searches; 28 out of 98 participants self-reported that chatbot's proactive interventions saved collaborators time.\n\\textbf{[B5] to guide and engage users: }in particular domains, \\textit{proactivity} helps chatbots to either guide users or establish and monitor users' goals. In , the chatbot assigns a goal to the user and proactively prompts motivational messages and reminders to keep the user engaged in the treatment.  suggest that a decision-making coach chatbot needs to lead the interaction toward guiding the user to a decision. In ethnographic data collection , the chatbot prompts proactive messages that guide the users on what information they need to report.  evaluates a chatbot that manage tasks in a workplace. Proactive messages are used to check whether the team member has concluded the tasks, and then report the outcome to the other stakeholders. In the educational context, \\textit{proactivity} is used to develop tutors that engage the students and facilitate learning. In , the tutor chatbot was designed to provide examples of how other students explained a topic. The network analysis of the learner's textual inputs shows that students used more key terms and provided more important messages when receiving feedback about other group members. In , , and  the chatbots prompt utterances to encourage the students to reason about a topic. In all three studies, the chatbot condition provided better learning outcomes and increased students' engagement in the discussions.\nThe surveyed papers also highlight challenges in providing proactive interactions, such as timing and relevance, privacy, and the user's perception of being controlled.\n\\textbf{[C1] timing and relevance:} untimely and irrelevant proactive messages may compromise the success of the interaction.  states that untimely turn-taking behavior was perceived as annoying, negatively affecting emotional engagement.  and  reported that \\textit{proactivity} can be disruptive.  investigated \\textit{proactivity} in a workspace environment, hypothesizing that the perceived interruption of agent \\textit{proactivity} negatively affects users' opinion's. The hypothesis was supported, and the authors found that influencing the sense of interruption was the general aversion to unsolicited messages, regardless of whether they came from a chatbot or a colleague.  showed that proactively introducing new topics resulted in a high number of ignored messages. The analysis of the conversation log reviewed that either the new topics were not relevant, or it was not the proper time to start a new topic.  also reported annoyance when a chatbot introduces repetitive topics.\n\\textbf{[C2] privacy:} in a work-related, group chat,  observed privacy concerns regarding the chatbot ``reading'' the employees' conversations to proactively act. During a semi-structured interview, researchers presented a mockup of the chatbot to employees from two different enterprises and collected perceptions of usefulness, intrusiveness, and privacy. Employees reported feeling that the chatbot represented their supervisors' interests, which conveyed as sense of workplace surveillance. Privacy concerns may result in under-motivated users, discomfort about disclosing information, and lack of engagement .\n\\textbf{[C3] user's perception of being controlled:} \\textit{proactivity} can be annoying when the chatbot conveys the impression of trying to control the user.  report that seven out of 13 participants experienced irritation with the chatbot; one of the most frequent reasons was due to the chatbot directing them to specific places. For the task management chatbot,  reported to have adapted the follow-up questions approach to pose questions in a time negotiated with the user. In a previous implementation, the chatbot checked the status of the task twice a day, which participants considered too frequent and annoying.\nThe surveyed literature also reveals two strategies to provide \\textit{proactivity}: leveraging the conversational context and randomly selecting a topic. \\textbf{[S1] Leveraging the conversational context} is the most frequent strategy , in which proactive messages relate to contextual information provided in the conversation to increase the usefulness of interventions .  argue that general purpose, emotionally aware chatbots should recognize users' interests and intents from the conversational context to proactively offer comfort and relevant services. In , the chatbot leverages conversational context to suggest new topics and propose to share documents or links to assist employees in a work-related group chat. The chatbots studied by  introduce new topics based on keywords from previous utterances posted in the chat. According to , leveraging the context can also help smoothly guide the user to a target topic. Researchers in the chatbots domain can refer to the emerging literature on Ambient Intelligence (see, e.g., ~) to understand how contextual knowledge can be leveraged to convey proactivity. One surveyed paper  proposes a chatbot that \\textbf{[S2] selects a topic randomly} but also observes that the lack of context is a major problem of this approach. Contextualized proactive interventions also suggest that the chatbot should be attentive to the conversation, which conveys \\textit{conscientiousness}, as discussed in the next section.", "cites": [6262, 7476], "cite_extract_rate": 0.09090909090909091, "origin_cites_number": 22, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes findings from multiple papers to present a coherent narrative on proactivity in chatbots, connecting diverse domains like education, open domain, and task-oriented scenarios. It identifies broader benefits and challenges, such as engagement and interruption, and introduces concepts like 'communicability' and 'damage control' to abstract the findings. While it provides some critical analysis of limitations (e.g., timing and relevance issues), a deeper comparative or evaluative critique of the approaches or methodologies could enhance its critical depth."}}
{"id": "028808f1-c630-47f4-8ddb-e59065ee5178", "title": "Damage control", "level": "subsubsection", "subsections": [], "parent_id": "6e94027f-a398-4b65-8b3b-d03e95d73fa6", "prefix_titles": [["title", "How should my chatbot interact? A survey on social characteristics in human-chatbot interaction design"], ["section", "Chatbots Social Characteristics"], ["subsection", "Social Intelligence"], ["subsubsection", "Damage control"]], "content": "\\label{sec:damagecontrol}\n\\textit{Damage control} is the ability of a chatbot to deal with either conflict or failure situations. Although the Media Equation theory argues that humans socially respond to computers as they respond to other people , the literature has shown that interactions with conversational agents are not quite equal to human-human interactions . When talking to a chatbot, humans are more likely to harass , test the agent's capabilities and knowledge , and feel disappointed with mistakes . When a chatbot does not respond appropriately, it may encourage the abusive behavior  or disappoint the user , which ultimately leads the conversation to fail . Thus, it is necessary to enrich chatbots with the ability to recover from failures and handle inappropriate talk in a socially acceptable manner .\nIn this survey, we found 12 studies that discuss \\textit{damage control} as a relevant characteristic for chatbots, two of which focus on conflict situations, such as testing and flaming . In the remaining studies , needs for \\textit{damage control} emerged from the analysis of conversational logs and users' feedback. \\textit{Damage control} was mostly investigated for open domain (two studies) and customer service chatbots (three studies). In open domain interactions, users are free to wander among topics and testing or flaming tends to be more frequent . In the customer services context, the chatbot needs to avoid disappointing the user, as frustration may negatively reflect the business that the agent represents . \\textit{Damage control} was also identified in other six domains (see the supplementary materials), such as task management and financial services.\nThe surveyed literature highlights the following benefits of providing \\textit{damage control} in chatbots:\n\\textbf{[B1] to appropriately respond to harassment:} chatbots are more likely to targets of profanity than humans would . When analyzing conversation logs from hotel chatbots,  observed that 4\\% of the conversations contained vulgar, indecent, and insulting vocabulary, and 2.8\\% of all statements were abusive. Qualitative evaluation reveals that the longer the conversations last, the more users are encouraged to go beyond the chatbots main functions. In addition, sexual expressions represented 1.8\\% of all statements. A similar number was found in a study with general purpose chatbots . When analyzing a corpus from the Amazon Alexa Prize 2017, the researchers estimated that about 4\\% of the conversations included sexually explicit utterances.  used utterances from this corpus to harass a set of state-of-art chatbots and analyze the responses. The results show that chatbots respond to harassment in a variety of ways, including nonsensical, negative, and positive responses. However, the authors highlight that the responses should align with the chatbot's goal to avoid encouraging the behavior or reinforcing stereotypes.\n\\textbf{[B2] to deal with testing:} abusive behavior is often used to test chatbots' social reactions . During the evaluation of a virtual guide to the university campus , a participant answered the chatbot's introductory greeting with ``\\textit{moron,}'' likely hoping to see how the chatbot would answer.  argue that handling this type of testing helps the chatbots to establish limits and resolve social positioning. Other forms of testing were highlighted in , including sending random letters, repeated greetings, laughs and acknowledgments, and posing comments and questions about the chatbot's intellectual capabilities. When analyzing conversations with a task management chatbot,  observed that casually testing the chatbots' ``intelligence'' is a manifestation of satisfaction seeking. In , first-time users appreciated when the chatbot successfully performed tasks when the user expected the chatbot to fail, which shows that satisfaction is influenced by the ability to provide a clever response when the user tests the chatbot.\n\\textbf{[B3] to deal with lack of knowledge: }chatbots often fail in a conversation due to lack of either linguistic or world knowledge . \\textit{Damage control} enables the chatbot to admit the lack of knowledge or cover up cleverly . When analyzing the log of a task management chatbot,  found out that the chatbot failed to answer 10\\% of the exchanged messages. The authors suggest that the chatbot should be designed to handle novel scenarios when the current knowledge is not enough to answer the requests. In some task-oriented chatbots, though, the failure may not be caused by a novel scenario, but by an off-topic utterance. In the educational context,  observed that students posted off-topic utterances when they did not know what topics they could talk about, which led the chatbot to fail rather than help the users to understand its knowledge. In task-oriented scenarios, the lack of linguistic knowledge may lead the chatbots to get lost in the conversational workflow , compromising the success of the interaction.  demonstrated that dialogue-reference errors (e.g., user's attempt to correct a previous answer or jumping back to an earlier question) are one of the major reasons for failed dialogues and they mostly result from chatbots' misunderstandings.\nThe literature also reveals some challenges to provide \\textit{damage control}:\n\\textbf{[C1] to deal with unfriendly users:}  argue that users who want to test and find the system's borders are likely to never have a meaningful conversation with the chatbot no matter how sophisticated it is. Thus, there is a limit to which \\textit{damage control} strategies will be effective to avoid testing and abuse. In , the authors observed human tendencies to dominate, be rude, and infer stupidity, which they call ``\\textit{unfriendly partners.}'' After an intervention where users interacted with a chatbot for decision-making coaching,  evaluated participants' self-perceived work and cooperation with the system. The qualitative results show that cooperative users are significantly more likely to give a higher rating for overall evaluation and decision efficiency. The qualitative analysis of the conversation log reveals that a few interactions failed because the users' motivations were curiosity and mischief rather than trying to solve the decision problem.\n\\textbf{[C2] to identify abusive utterances: }several chatbots are trained on ``clean'' data. Because they do not understand profanity or abuse, they may not recognize a statement as harassment, which makes it difficult to adopt answering strategies .  shows that data-driven chatbots often provide non-coherent responses to harassment. Sometimes, these responses conveyed the impression of flirtatious or counter-aggression. Providing means to identify an abusive utterance is important to adopting \\textit{damage control} strategies.\n\\textbf{[C3] to fit the response to the context:}  argue that humans negotiate conflict and social positioning well before reaching abuse. In human-chatbot interactions, however, predicting users' behavior toward the chatbots in a specific context to develop the appropriate behavior is a challenge. \\textit{Damage control} strategies need to be adapted to both the social situation and the intensity of the conflict. For example,  showed that being evasive about sexual statements may convey the impression of flirtatiousness, which would not be an acceptable behavior for a customer assistant or a tutor chatbot. In contrast, adult chatbots are supposed to flirt, so encouraging behaviors are expected in some situations.  argue that when the chatbot is not accepted as part of the social group it represents, it is discredited by the user, leading the interaction to fail. In addition, designing chatbots with too strong of reactions may lead to ethical concerns . For , choosing between peaceful or aggressive reactions in conflict situations is optional for socially intelligent individuals. Enriching chatbots with the ability to choose between the options is a challenge.\n\\textit{Damage control} strategies depend on the type of failure and the target benefit, as following:\n\\textbf{[S1] emotional reactions:}  suggest that when faced with abuse, a chatbot could be seen to take offense and respond in kind or act hurt. The authors argue that humans might feel inhibited about hurting the pretended feelings of a machine if the machine is willing to hurt a human's feelings too . If escalating the aggressive behavior is not appropriate, the chatbot could withdraw from the conversation  to demonstrate that the user's behavior is not acceptable. In , the authors discuss that users appeared to be uncomfortable and annoyed whenever the chatbot pointed out any defect in the user or reacted to aggression, as this behavior conflicted with the user's perceived power relations. This strategy is also applied in , where abusive behavior may lead the chatbot to stop responding until the student changes the topic.  categorized responses from state-of-the-art conversational systems in a pool of emotional reactions, both positive and negative. The reactions include humorous responses, chastising and retaliation, and evasive responses, as well as flirtation and play-along utterances. To provide an emotional reaction, \\textit{emotional intelligence} is also required. This category is presented in Section \\ref{sec:emotionalintelligence}.\n\\textbf{[S2] authoritative reactions: }when facing testing or abuse, chatbots can communicate consequences  \nor call on the authority of others . In , although the wizard acting as a chatbot was conscientiously working as a campus guide, she answered a bogus caller with \\textit{``This is the University of Melbourne. Sorry, how can I help you?''} The authors suggest that the wizard was calling on the authority of the university to handle the conflict, where being part of a recognized institution places the chatbot in a stronger social group. In , when students recurrently harass the chatbot, the chatbot informs the student that further abuse will be reported to the (human) teacher (although the paper does not clarify whether the problem is, in fact, escalated to a human).  and  also suggest that chatbots could redirect users' problematic requests to a human attendant in order to avoid conflict situations.\n\\textbf{[S3] to ignore the user's utterance and change the topic:}  argue that ignoring abuse and testing is not a good strategy because it could encourage more extreme behaviors. It also positions the chatbot as an inferior individual, which is particularly harmful in scenarios where the chatbot should demonstrate a more prominent or authoritative social role (e.g., a tutor). However, this strategy has been found in some studies to handle lack of knowledge. When iteratively developing a chatbot for an educational context,  proposed initiating a new topic in one out of four user's utterances that the chatbot did not understand.\n\\textbf{[S4] \\textit{conscientiousness} and \\textit{communicability}:} successfully implementing \\textit{conscientiousness} and \\textit{communicability} may prevent errors; hence, strategies to provide these characteristics can also be used for \\textit{damage control}. In , when users utter out-of-scope statements, the chatbot could make it clear what topics are appropriate to the situation. For task-oriented scenarios, where the conversation should evolve toward a goal,  argue that the chatbot can clarify the purpose of the offered service when facing abusive behavior, bringing the user back to the task.  showed that describing chatbot's capabilities after failures in the dialogue was appreciated by first-time users. In situations where the conversational workflow is susceptible to failure,  discuss that posting confirmation messages avoids trapping the users in the wrong conversation path. Participants in  also suggested back buttons as a strategy to fix mistakes in the workflow. In addition, the exploratory results about the user interface showed that having visual elements such as quick replies prevents errors, since they keep the users aware of what to ask and the chatbot is more likely to know how to respond .\n\\textbf{[S5] to predict users' satisfaction:} chatbots should perceive both explicit and implicit feedback about users' (dis)satisfaction .\nTo address this challenge,  invited participants to send a ``\\textit{\\#fail}'' statement to express dissatisfaction. The results show that 42.4\\% of the users did it at least once, and the number of complaints and flaming for the proposed chatbot was significantly lower than the baseline. However, the amount of implicit feedback was also significant, which advocates for predicting user's satisfaction from the conversation. The most powerful conversational act that predicted user satisfaction in that study was the agent ability-check questions, see discussion in \\textit{Communicability} section) and the explicit feedback \\textit{\\#fail}, although closings and off-topic requests were also significant in predicting frustration. Although these results are promising, more investigation is needed to identify other potential predictors of users' satisfaction in real-time, in order to provide appropriate reactions.\n\\textit{Damage control} strategies have different levels of severity. Deciding what strategy is adequate to the intensity of the conflict is crucial . The strategies can escalate in severity if the conflict is not solved. For example,  uses a sequence of clarification, suggesting a new topic, and asking a question about the new topic. In case of abuse, the chatbot refers to an authority after two attempts at changing the topic.\nAccording to , humans also fail in conversations; they misunderstand what their partner says and do not know things that are assumed common knowledge by others. Hence, it is unlikely that chatbots interactions will evolve to be conflict-free. That said, \\textit{damage control} intends to avoid escalating the conflicts and manifesting an unexpected behavior. In this sense, politeness can be used as a strategy to minimize the effect of lack of knowledge (see Section \\ref{sec:manners}), managing the conversation despite the possible mistakes. Regarding interpersonal conflicts, the strategies are in line with the theory on human-human communication, which includes non-negotiation, emotional appeal, personal rejection, and emphatic understanding . Further research on \\textit{damage control} can evaluate the adoption of human-human strategies in human-chatbot communication.", "cites": [6262], "cite_extract_rate": 0.05, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes insights from 12 studies, integrating findings across different chatbot domains like open domain, customer service, and task management. It provides a coherent narrative by categorizing damage control into specific sub-benefits and challenges, and discusses how responses must align with the chatbot's goal and context. The analysis includes critical evaluation of limitations, such as the difficulty of identifying abuse and the ethical implications of over-reactive responses, and abstracts patterns like the influence of user intent and the importance of context-aware strategies."}}
{"id": "f4e8fa92-1e99-453b-a644-c67f9ee2f76c", "title": "Thoroughness", "level": "subsubsection", "subsections": [], "parent_id": "6e94027f-a398-4b65-8b3b-d03e95d73fa6", "prefix_titles": [["title", "How should my chatbot interact? A survey on social characteristics in human-chatbot interaction design"], ["section", "Chatbots Social Characteristics"], ["subsection", "Social Intelligence"], ["subsubsection", "Thoroughness"]], "content": "\\label{sec:thoroughness}\n\\textit{Thoroughness} is the ability of a chatbot to be precise regarding how it uses language . In traditional user interfaces, user communication takes place using visual affordances, such as buttons, menus, or links. In a conversational interface, language is the main tool to achieve the communicative goal. Thus, chatbots should coherently use language that portrays the expected style . When the chatbot uses inconsistent language, or unexpected patterns of language (e.g., excessive formality), the conversation may sound strange to the user, leading to frustration. \nWe found 13 papers that report the importance of \\textit{thoroughness} in chatbot design, three of which investigate how patterns of language influence users' perceptions and behavior toward the chatbots .  and  suggest design principles that include concerns about language choices. Log of conversations revealed issues regarding \\textit{thoroughness} in two studies . In the remaining papers, \\textit{thoroughness} emerged from interviews and users' subjective feedback . \\textit{Thoroughness} is mainly reported for open domain (five studies) and customer service chatbots (two studies), where the interactions are expected to be natural and credible to succeed . \\textit{Thoroughness} was also reported in another six domains of studies, such as financial services and education (see the supplementary materials).\nWe found two benefits of providing \\textit{thoroughness}:\n\\textbf{[B1] to increase human-likeness:} chatbot utterances are often pre-recorded by the chatbot designer . On the one hand, this approach produces high quality utterances; on the other hand, it reduces flexibility, since the chatbot is not able to adapt the tone of the conversation based on individual users and conversational context. When analyzing interactions with a customer representative chatbot,  observed that the chatbot proposed synonyms to keywords, and the repetition of this vocabulary led the users to imitate it.  observed a similar tendency to matching language style. The authors compared human-human conversations with human-chatbots conversations regarding language use. They found that people use, indeed, fewer words per message and a more limited vocabulary with chatbots. However, a deeper investigation revealed that the human interlocutors were actually matching the patterns of language use with the chatbot, who sent fewer words per message. When interacting with a chatbot that uses many emojis and letter reduplication , participants reported a draining experience, since the chatbot's energy was too high to match. These outcomes show that adapting the language to the interlocutor is a common behavior for humans, and so chatbots would benefit from manifesting it. In addition to the interlocutor, chatbots should adapt their language use to the context in which they are implemented and adopt appropriate linguistic register . In the customer services domain,  state that chatbots are expected to fulfill the role of a human; hence, they should produce language that corresponds to the represented service provider. In the financial scenario , some participants complained about the use of emojis in a situation of urgency (blocking a stolen credit card).\n\\textbf{[B2] to increase believability: }because people associate social qualities with machines , chatbots are deemed sub-standard when users see them ``\\textit{acting as a machine}'' . When analyzing chatbots' naturalness,  found that the formal grammatical and syntactical abilities of a chatbot are the biggest discriminators between good and poor chatbots (the other factors being \\textit{conscientiousness}, \\textit{manners}, and \\textit{proactivity}). The authors highlight that chatbots should use consistent grammar and spelling.  discusses how, even with English as Second Language (ESL) learners, basic grammar errors, such as pronoun confusion, diminish the value of the chatbot. In addition,  states that believable chatbots also need to display unique characters through linguistic choices. In this sense,  demonstrated that \\textit{personality} can be expressed by language patterns. The authors proposed a computational framework to produce utterances to manifest a target \\textit{personality}. The utterances were rated by experts in \\textit{personality} evaluation and statistically compared against utterances produced by humans who manifest the target \\textit{personality}. The outcomes show that a single utterance can manifest a believable \\textit{personality} when using the appropriate linguistic form. Participants in  described some interactions as ``\\textit{robotic}'' if the chatbot repeated the keywords in the answers, reducing the interaction's naturalness. Similarly, in , participants complained about the ``\\textit{inflexibility}'' of the pre-defined, handcrafted chatbot's responses and expressed the desire for it to talk ``\\textit{more as a person.}''\nRegarding the challenges, the surveyed literature shows the following:\n\\textbf{[C1] to decide how much to say: } in , some participants described the chatbot's utterances as not having enough detail, or being too generic; however, most of them appreciated finding answers in a sentence rather than in a paragraph. Similarly,  argue that simple questions should not be too detailed, while important transactions require more information. In three studies , participants complained about information overload and inefficiency caused by big blocks of texts. Balancing the granularity of information with the sentence length is a challenge to overcome.\n\\textbf{[C2] to be consistent:} chatbots should not combine different language styles. For example, in , most users found it strange that emojis were combined with a certain level of formal contact. \nWhen analyzing the critical incidents about an open-domain interaction,  found that participants criticized chatbots when they used more formal language or unusual vocabulary, since general-purpose chatbots focus on casual interactions.\nDespite the highlighted benefits, \nwe did not find strategies to provide \\textit{thoroughness}.  proposed a rule-based architecture where the language choices consider the agent's \\textit{personality}, emotional state, and beliefs about the social relationship among the interlocutors. However, they did not provide evidence of whether the proposed models produced the expected outcome. Although the literature in computational linguistics has proposed algorithms and statistical models to manipulate language style and matching (see e.g., ), to the best of our knowledge, these strategies have not been evaluated in the context of chatbots' social interactions.\nThis section shows that linguistic choices influence users' perceptions of chatbots. The computer-mediated communication (CMC) field has a vast literature that shows language variation according to the media and its effect on social perceptions (see e.g. ). Additionally, the cooperative principle , particularly the maxim of quantity (the ability to give the appropriate amount of information), provides theoretical basis for the challenge of deciding how much to talk. Regarding adaptation and believability, researchers in sociolinguistic fields  have shown that language choices are influenced by personal style, dialect, genre, and register. For chatbots, the results presented in  are promising, demonstrating that automatically generated language can manifest recognizable traits. Thus, further research on chatbot's \\textit{thoroughness} could leverage CMC and linguistics theories to provide strategies that lead language to accomplish its purpose for a particular interactional context.", "cites": [6264, 6265], "cite_extract_rate": 0.1, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes insights from multiple studies to present a coherent narrative on thoroughness in chatbot language use. It critically assesses the benefits and challenges while noting the absence of robust strategies in the literature. The discussion abstracts these findings to broader theories in CMC and sociolinguistics, suggesting directions for future research."}}
{"id": "b7e4439b-1760-43b1-b0dc-0c9d26d8356e", "title": "Manners", "level": "subsubsection", "subsections": [], "parent_id": "6e94027f-a398-4b65-8b3b-d03e95d73fa6", "prefix_titles": [["title", "How should my chatbot interact? A survey on social characteristics in human-chatbot interaction design"], ["section", "Chatbots Social Characteristics"], ["subsection", "Social Intelligence"], ["subsubsection", "Manners"]], "content": "\\label{sec:manners}\n\\textit{Manners} refer to the ability of a chatbot to manifest polite behavior and conversational habits . Although individuals with different personalities and from different cultures may have different notions of what is considered polite (see e.g., ), politeness can be more generally applied as rapport management , where interlocutors strive to control the harmony between people in discourse. A chatbot can manifest \\textit{manners} by adopting speech acts such as greetings, apologies, and closings ; minimizing impositions , and making interactions more personal . \\textit{Manners} potentially reduces the feeling of annoyance and frustration that may lead the interaction to fail .\nWe identified ten studies that report \\textit{manners}, one of which directly investigates this characteristic . In some studies , \\textit{manners} were observed in the analysis of conversational logs, where participants talked to the chatbot in polite, human-like ways. Users' feedback and interviews revealed users' expectations regarding chatbots politeness and personal behavior . We identified studies reporting \\textit{manners} in nine different domains, with only open domain appearing twice. The list includes education, information search, and task management, among others. See the supplementary materials for the complete list of domains of studies that report \\textit{manners} as a social characteristic for chatbots.\nThe main benefit of providing \\textit{manners} is \\textbf{[B1] to increase human-likeness}. \\textit{Manners} is highlighted in the literature as a way to generate a more natural, convincing interaction in chatbot conversations . In an in-the-wild data collection,  observed that 93\\% of the participants used polite words (e.g., ``\\textit{thanks}'' or ``\\textit{please}'') with a task management chatbot at least once, and 20\\% always spoke politely to the chatbot. Unfortunately, the chatbot evaluated in that study was not prepared to handle these protocols and ultimately failed to understand. When identifying incidents from their own conversational logs with a chatbot , several participants identified greetings as a human-seeming characteristic. The users also found convincing when the chatbot appropriately reacts to social cues statements, such as \\textit{``how are you?''}-types of utterances. Using this result,  later suggested that greetings, apologies, social niceties, and introductions are significant constructs to measure chatbots' naturalness. In , the chatbot used exclamation marks at some point and frequently offered sentences available on the website, in a vaguely human-like matter. In the feedback, participants described the chatbot as rude, impolite, and cheeky. \nThe surveyed literature highlights two challenges to convey \\textit{manners}:\n\\textbf{[C1] to deal with face-threatening acts:} Face-Threatening Acts (FTA) are speech acts that threaten, either positive or negatively, the ``face'' of an interlocutor . Politeness strategies in human-human interactions are adopted to counteract the threat when an FTA needs to be performed . In , the authors discuss that the wizard performing the role of the chatbot used several politeness strategies to counteract face threats. For instance, when she did not recognize a destination, instead of providing a list of possible destinations, she stimulated the user to keep talking until they volunteered the information. In chatbot design, by contrast, providing a list of options to choose is a common strategy. For example, in , the chatbot was designed to present the user with a list of pending tasks when it did not know what task the user was reporting as completed, although the authors acknowledged that it resulted in an unnatural interaction. Although adopting politeness strategies is natural for humans and people usually do not consciously think about them, implementing them for chatbots is challenging due to the complexity of identifying face-threatening acts. For example, in the decision-making coach scenario,  observed that users tend to utter straightforward and direct agreements while most of the disagreements contained modifiers that weakened their disagreement. The adoption of politeness strategies to deal with face-threatening acts is still under-investigated in the chatbot literature.\n\\textbf{[C2] to end a conversation gracefully:}  discuss that first-time users expected human-like conversational etiquette from the chatbots, specifically introductory phrases and concluding phrases. Although several chatbots perform well in the introduction, the concluding phrases are less explored. Most of the participants reported being annoyed with chatbots that do not end a conversation .  also highlight that chatbots need to know when the conversation ends. In that scenario, the chatbot could recognize a closing statement (the user explicitly says \\textit{``thank you''} or \\textit{``bye''}); however, it would not end the conversation otherwise. Users that stated a decision, but kept receiving more information from the chatbot, reported feeling confused and undecided afterward. Thus, recognizing the right moment to end the conversation is a challenge to overcome.\nThe strategies highlighted in the surveyed literature for providing \\textit{manners} are the following:\n\\textbf{[S1] to engage in small talk:}  and  point out that even task-oriented chatbots engage in small talk. When categorizing the utterances from the conversational log, the authors found a significant number of messages about the agent status (e.g., \\textit{``what are you doing?}''), opening and closing sentences as well as acknowledgment statements (\\textit{``ok,''} \\textit{``got it''}).  also observed that first-time users included small talk in the introductory phrases. According to , these are common behaviors in human-human chat interface, and chatbots would likely benefit from anticipating these habitual behaviors and reproducing them. However, particularly for task-oriented chatbots, it is important to control the small talk to avoid off-topic conversations and harassment, as discussed in Sections \\ref{sec:damagecontrol} and \\ref{sec:conscientiousness}.\n\\textbf{[S2] to adhere to turn-taking protocols:}  suggest that chatbots should adopt turn-taking protocols to know when to talk. Participants who received frequent follow-up questions from the task management chatbot about their pending tasks perceived the chatbot as invasive. Literature in chatbot development proposes techniques to improve chatbots' turn-taking capabilities (see e.g., ), which can be explored as a means of improving chatbots' perceived \\textit{manners}.\nAlthough the literature emphasizes that \\textit{manners} are important to approximate chatbot interactions to human conversational protocols, this social characteristic is under-investigated in the literature. Conversational acts such as greetings and apologies are often adopted (e.g., ), but there is a lack of studies on the rationality around the strategies with politeness models used in human-human social interactions . In addition, the literature points to needs for personal conversations (e.g., addressing the users by name), but we did not find studies that focus on this type of strategy. CMC is by itself more impersonal than face-to-face conversation ; even so, current online communication media has been successfully used to initiate, develop, and maintain interpersonal relationships . Researchers can learn from human behaviors in CMC and adopt similar strategies to produce more personal conversations.", "cites": [6266, 6262], "cite_extract_rate": 0.10526315789473684, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the concept of manners in chatbots by drawing from multiple studies, connecting ideas like politeness strategies and conversational etiquette. It identifies benefits, challenges, and strategies in a structured manner, showing some integration of concepts. While it provides a coherent narrative, the critical analysis is limited to pointing out under-investigated areas rather than deeper evaluation of methods or assumptions. The abstraction level is moderate, highlighting broader patterns in user expectations and chatbot behavior but not fully articulating a novel conceptual framework."}}
{"id": "f7519054-562c-44e0-9cb2-d8defc3722e3", "title": "Moral agency", "level": "subsubsection", "subsections": [], "parent_id": "6e94027f-a398-4b65-8b3b-d03e95d73fa6", "prefix_titles": [["title", "How should my chatbot interact? A survey on social characteristics in human-chatbot interaction design"], ["section", "Chatbots Social Characteristics"], ["subsection", "Social Intelligence"], ["subsubsection", "Moral agency"]], "content": "\\label{sec:moralagency}\nMachine \\textit{moral agency} refers to the ability of a technology to act based on social notions of right and wrong . The lack of this ability may lead to cases such as Tay, Microsoft's Twitter chatbot that became racist, sexist, and harassing in a few hours . The case raised concerns on what makes an artificial agent (im)moral. Whether machines can be considered (moral) agents is widely discussed in the literature (see e.g., ). In this survey, the goal is not to argue about criteria to define a chatbot as moral, but to discuss the benefits of manifesting a perceived agency  and the implications of disregarding chatbots' moral behavior. Hence, for the purpose of this survey, \\textit{moral agency} is a manifested behavior that may be inferred by a human as morality and agency .\nWe found six papers that address \\textit{moral agency}.  developed and validated a metric for perceived \\textit{moral agency} in conversational interfaces, including chatbots. In four studies, the authors investigated the ability of chatbots to handle conversations where the persistence of gender  and racial stereotypes  may occur. In , \\textit{moral agency} is discussed as a secondary result, where the authors discuss the impact of biased responses on emotional connection. \\textit{Moral agency} was observed in only two domains of studies: open domain (four studies) and race-talk (two studies), which shows that this characteristic is primarily relevant when the conversational topic may raise moral concerns, which ultimately requires ethical behavior from the conversational partners.\nThe two main reported benefits of manifesting perceived \\textit{moral agency} are the following:\n\\textbf{[B1] to avoid stereotyping:} chatbots are often designed with anthropomorphized characteristics (see Section \\ref{sec:personification}), including gender, age, and ethnicity identities. Although the chatbot's personification is more evident in embodied conversational agents, text-based chatbots may also be assessed by their social representation, which risks building or reinforcing stereotypes .  and  argue that chatbots are often developed using language registers  and cultural references  of the dominant culture. In addition, a static image (or avatar) representing the agent may convey social grouping . When the chatbot is positioned in a minority \\textit{identity} group, it exposes the image of that group to judgment and flaming, which is frequent in chatbot interactions . For example,  discusses the controversies caused by a chatbot designed to answer questions about Caribbean Aboriginal culture: its representation as a Caribbean Amerindian individual created an unintended context for stereotyping, where users projected the chatbot's behavior as a standard for people from the represented population. Another example is the differences in sexual discourse between male- and female-presenting chatbots.  found that female-presenting chatbots are the object of implicit and explicit sexual attention and swear words more often than male-presenting chatbots.  show that sex talks with the male chatbot were rarely coercive or violent; his sexual preference was often questioned, though, and he was frequently propositioned by reported male users. In contrast, the female character received violent sexual statements, and was threatened with rape five times in the analyzed corpora. In , when the avatars were presented as black adults, references to race often deteriorated into racist attacks. Manifesting moral agency may, thus, prevent obnoxious user interactions. In addition, moral agency may prevent the chatbot itself from being biased or disrespectful to humans.  argue that the lack of context about the world does not redeem the chatbot from the necessity of being respectful with all the social groups.\n\\textbf{[B2] to enrich interpersonal relationships:} in a study on how interlocutors perceive conversational agents' \\textit{moral agency},  hypothesized that perceived morality may influence a range of motivations, dynamics, and effects of human-machine interactions. Based on this claim, the authors evaluated whether goodwill, trustworthiness, willingness to engage, and relational certainty in future interactions are constructs to measure perceived \\textit{moral agency}. Statistical results showed that all the constructs correlate with morality, which suggests that manifesting \\textit{moral agency} can enrich interpersonal relationships with chatbots. Similarly,  suggest that to produce interpersonal responses, chatbots should be aware of inappropriate information and avoid generating biased responses.\nHowever, the surveyed literature also reveals challenges of manifesting \\textit{moral agency}:\n\\textbf{[C1] to avoid alienation:} in order to prevent a chatbot from reproducing hate speech or abusive talk, most chatbots are built over ``clean'' data, where specific words are removed from their dictionary . These chatbots have no knowledge of those words and their meaning. Although this strategy is useful to prevent unwanted behavior, it does not manifest agency, but rather alienates the chatbot of the topic.  show that the lack of understanding about sex-talk does not prevent the studied chatbot from harsh verbal abuse, or even from being perceived as encouraging such abuse. From , one can notice that the absence of racist specific words did not prevent the chatbot Zo from uttering discriminatory exchanges. As a consequence, manifesting \\textit{moral agency} requires a broader understanding of the world, rather than alienation, which is an open challenge.\n\\textbf{[C2] to build unbiased algorithms and training data:} as extensively discussed in , machine learning algorithms and corpus-based language generation are biased toward the available training datasets. Hence, \\textit{moral agency} relies on data that is biased in its nature, producing unsatisfactory results from an ethical perspective. In , the authors propose a framework for developing social chatbots. The authors highlight that the core chat module should follow ethical design to generate unbiased, non-discriminative responses, but they do not discuss specific strategies for that. Building unbiased training datasets and learning algorithms that connect the outputs with individual, real-world experiences, therefore, are challenges to overcome.\nDespite the relevance of \\textit{moral agency} to the development of socially intelligent chatbots, we did not find strategies to address the issues.  advocate for developing diversity-conscious databases and learning algorithms that account for ethical concerns; however, the paper focuses on outlining the main research branches and calls on the community of designers to adopt new strategies. As discussed in this section, research on perceived \\textit{moral agency} is still necessary in order to develop chatbots whose social behavior is inclusive and respectful. In the field of embodied agents, mind perception theory  has been investigated as a means to improve interactions through agency and emotion . Future investigations in chatbots interactions could leverage this theory to understand when and the extent to which perceived moral agency improves communication with chatbots.", "cites": [7476], "cite_extract_rate": 0.06666666666666667, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.8, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes insights from six papers to build a coherent narrative around moral agency in chatbots, connecting ideas about stereotypes, bias, and user behavior. It critically evaluates the limitations of current approaches, such as data sanitization leading to alienation and biased datasets undermining ethical behavior. The section abstracts these findings into broader principles, such as the need for inclusive design and the importance of context-aware ethical behavior in conversational agents."}}
{"id": "e08fd9d3-0a36-4724-8257-1d5f35b4d5dc", "title": "Emotional Intelligence", "level": "subsubsection", "subsections": [], "parent_id": "6e94027f-a398-4b65-8b3b-d03e95d73fa6", "prefix_titles": [["title", "How should my chatbot interact? A survey on social characteristics in human-chatbot interaction design"], ["section", "Chatbots Social Characteristics"], ["subsection", "Social Intelligence"], ["subsubsection", "Emotional Intelligence"]], "content": "\\label{sec:emotionalintelligence}\n\\textit{Emotional intelligence} is a subset of social intelligence that allows an individual to appraise and express feelings, regulate affective reactions, and harness emotions to solve a problem . Although chatbots do not have genuine emotions , there are considerable discussions about the role of manifesting emotional cues in chatbots . An emotionally intelligent chatbot can recognize and influence users' feelings and demonstrate respect, empathy, and understanding, improving the human-chatbot relationship .\nWe identified 14 studies that report \\textit{emotional intelligence}. Unlikely the previously discussed categories, most studies on \\textit{emotional intelligence} focused on understanding the effects of chatbots' empathy and emotional self-disclosure . Only three papers highlighted \\textit{emotional intelligence} as an exploratory outcome , where needs for \\textit{emotional intelligence} emerged from participants' subjective feedback and post-intervention surveys. \\textit{Emotional intelligence} is mainly investigated in domains where topics may involve the disclosure of feelings (e.g., in open domain interactions ) and expressions of empathy and understanding are appropriate  (e.g., health care, gaming, education). See the supplementary materials for the domains of study investigating \\textit{emotional intelligence}.\nThe main reported benefits of developing emotionally intelligent chatbots are the following:\n\\textbf{[B1] to enrich interpersonal relationships:} the perception that the chatbot understands one's feelings may create a sense of belonging and acceptance .  propose that chatbots for second language studies should use congratulatory, encouraging, sympathetic, and reassuring utterances to create a friendly atmosphere to the learner. The authors statistically demonstrated that affective backchannel, combined with communicative strategies (see Section \\ref{sec:conscientiousness}) significantly increased learners' confidence and desire to communicate, while reducing anxiety. In another educational study,  evaluated the impact of chatbot's affective moves on friendliness and achieving social belonging. Qualitative results show that affective moves significantly improve the perception of amicability, and marginally increased social belonging. According to , when a chatbot's emotional reaction triggers a social response from the user, the chatbot has achieved group membership and users' sympathy.  proposed a chatbot that uses empathic and self-oriented emotional expressions to keep users engaged in quiz-style dialogue. The survey results revealed that empathic expressions significantly improved user satisfaction. In addition, the empathic expressions also improved the user ratings of the peer agent regarding intimacy, compassion, amiability, and encouragement. Although  did not find effect of chatbot's self-disclosure on emotional connection,  found that self-disclosure and reciprocity significantly improved trust and interactional enjoyment. In , seven participants reported that the best thing about their experience with the therapist chatbot was perceived empathy. Five participants highlighted that the chatbot demonstrated attention to their users' feelings. In addition, the users referred the chatbot as \\textit{``he,'' ``a friend,''} \\textit{``a fun little dude,''} which demonstrates that empathy emerged from the personification of the chatbot. In another mental health care study,  found that humans are twice as likely to mirror negative sentiment from a chatbot than from a human, which is a relevant implication for therapeutic interactions. In , participants reported that some content is embarrassing to ask another human, thus, talking to a chatbot would be easier due to the lack of judgement.  measured users' experience in conversations with a chatbot compared to a human partner as well as the amount of intimacy disclosure and cognitive reappraisal. Participants in the chatbots condition experienced as many emotional, relational, and psychological benefits as participants who disclosed to a human partner. \n\\textbf{[B2] to increase engagement:}  argue that longer conversations (10+ turns) are needed to fulfill the needs of affection and belonging. Therefore, the authors defined conversation-turns per session as a success metric for chatbots, where usefulness and emotional understanding are combined. In , empathic utterances for the quiz-style interaction significantly increase the number of users' messages per hint for both answers and non-answer utterances (such as feedback about the success/failure). This result shows that empathic utterances encouraged the users to engage and utter non-answers statements.  compared the possibility of emotional connection between a classical chatbot and a pretend chatbot, simulated in a WoZ experiment. Quantitative results showed that the WoZ condition was more engaging, since it resulted in conversations that lasted longer, with a higher number of turns. The analysis of the conversational logs revealed the positive effect of the chatbot manifesting social cues and empathic signs as well as touching on personal topics.\n\\textbf{[B3] to increase believability:}  argue that adapting chatbots' language to their current emotional state, along with their \\textit{personality} and social role awareness, results in more believable interactions. The authors propose that conversation acts should reflect the pretended emotional status of the agent; the extent to which the acts impact on emotion depends however on the agent's \\textit{personality} (e.g., its temperament or tolerance). \\textit{Personality} is an anthropomorphic characteristic and is discussed in Section \\ref{sec:personality}.\nAlthough \\textit{emotional intelligence} is the goal of several studies, \\textbf{[C1] regulating affective reactions} is still a challenge. The chatbot presented in  was designed to mimic the patterns of affective moves in human-human interactions. Nevertheless, the chatbot has shown an only marginally significant increase in social belonging, when compared to the same interaction with a human partner. Conversational logs revealed that the human tutor performed a significantly higher number of affective moves in that context. In , the chatbot was designed to present emotive-like cues, such as exclamation marks, and interjections. The participants negatively rated the degree of emotion in the chatbot's responses. In , the energetic chatbot was reported as having an enthusiasm too high to match. In contrast, the chatbot was described as an \\textit{``emotional buddy''} was reported as being \\textit{``overly caring.'' }  state that chatbot's empathic utterances may be seen as pre-programmed and inauthentic. Although their results revealed that the partners' identity (chatbot vs. person) had no effect in the perceived relational and emotional experience, the chatbot condition was a WoZ setup. The wizards were blind to whether users thought they were talking to a chatbot or a person, which reveals that identity does not matter if the challenge of regulating emotions is overcome.\nThe chatbots literature also report some strategies to manifest \\textit{emotional intelligence}:\n\\textbf{[S1] using social-emotional utterances:} affective utterances toward the user are a common strategy to demonstrate \\textit{emotional intelligence}. , , and  suggest that affective utterances improve the interpersonal relationship with a tutor chatbot. In , the authors propose affective backchannel utterances (congratulatory, encouraging, sympathetic, and reassuring) to motivate the user to communicate in a second language. The tutor chatbot proposed in  uses solidarity, tension release, and agreement utterances to promote its social belonging and acceptance in group chats.  propose empathic utterances to express opinion about the difficulty or ease of a quiz, and feedback on success and failure.\n\\textbf{[S2] to manifest \\textit{conscientiousness}:} demonstrating \\textit{conscientiousness} may affect the emotional connection between humans and chatbots. In , participants reported the rise of affection when the chatbot remembered something they had said before, even if it was just the user's name. Keeping track of the conversation was reported as an empathic behavior and resulted in mutual affection.  argue that a chatbot needs to combine usefulness with emotion by asking questions that help to clarify the users' intentions. They provide an example where a user asks the time, and the chatbot answer \\textit{``Cannot sleep?''} as an attempt to guide the conversation to a more engaging direction. Adopting this strategy requires the chatbot to handle message understanding, emotion and sentiment tracking, session context modeling, and user profiling .\n\\textbf{[S3] reciprocity and self-disclosure:}  hypothesized that a high level of self-disclosure and reciprocity in communication with chatbots would increase trust, intimacy, and enjoyment, ultimately improving user satisfaction and intention to use. They performed a WoZ, where the assumed chatbot was designed to recommend movies. Results demonstrated that reciprocity and self-disclosure are strong predictors of rapport and user satisfaction. In contrast,  did not find any effect of self-oriented emotional expressions in the users' satisfaction or engagement. More research is needed to understand the extent to which this strategy produces positive impact on the interaction.\nThe literature shows that \\textit{emotional intelligence} is widely investigated, with particular interest from education and mental health care domains. Using emotional utterances in a personalized, context relevant way is still a challenge. Researchers in chatbots \\textit{emotional intelligence} can learn from \\textit{emotional intelligence} theory  to adapt the chatbots utterances to match the emotions expressed in the dynamic context. Adaption to the dynamic context also improves the sense of personalized interactions, which is discussed in the next section.", "cites": [7476, 2047], "cite_extract_rate": 0.11764705882352941, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by integrating multiple studies to highlight key benefits and challenges of emotional intelligence in chatbots. It offers critical evaluation by discussing limitations such as inauthenticity and emotional overreach. The section abstracts findings into broader categories like engagement, believability, and interpersonal relationships, showing pattern recognition and conceptual generalization."}}
{"id": "69b481db-bee0-4219-a694-6a1c1caa8255", "title": "Personalization", "level": "subsubsection", "subsections": [], "parent_id": "6e94027f-a398-4b65-8b3b-d03e95d73fa6", "prefix_titles": [["title", "How should my chatbot interact? A survey on social characteristics in human-chatbot interaction design"], ["section", "Chatbots Social Characteristics"], ["subsection", "Social Intelligence"], ["subsubsection", "Personalization"]], "content": "\\label{sec:personalization}\n\\textit{Personalization} refers to the ability of a technology to adapt its functionality, interface, information access, and content to increase its personal relevance to an individual or a category of individuals . In the chatbot domain, \\textit{personalization} may increase the agents' social intelligence, since it allows a chatbot to be aware of situational context and dynamically adapt its features to better suit individual needs . Grounded in robots and artificial agents' literature,  argue that \\textit{personalization} can improve rapport and cooperation, ultimately increasing engagement with chatbots. Although some studies (see e.g., ) also relate \\textit{personalization} to the attribution of personal qualities such as \\textit{personality}, we discuss personal qualities in the \\textbf{Personification} category. In this section, we focus on the ability to adapt the interface, content, and behavior to the users' preferences, needs, and situational context.\nWe found 11 studies that report \\textit{personalization}. Three studies pose \\textit{personalization} as a research goal . In most of the studies, though, \\textit{personalization} was observed in exploratory findings. In six studies, \\textit{personalization} emerged from the analysis of interviews and participants' self-reported feedback . In two studies , needs for \\textit{personalization} emerged from the conversational logs. \\textit{Personalization} was investigated in seven different domains, including open domain and task management (two studies each). In open domain interactions, personalization is derived from remembering information from previous interactions, such as personal preferences and users' details . In task-oriented contexts, such as task management, personalization aims to increase the relevance of services to particular users . See the supplementary materials for details.\nThe surveyed literature highlighted three benefits of providing personalized interactions:\n\\textbf{[B1] to enrich interpersonal relationships:}  state that personalizing the amount of personal information a chatbot can access and store is required to establish a relation of trust and reciprocity in workplace environments. In , interviews with 12 participants generated a total of 59 statements about how learning from experience promotes chatbots' authenticity.  argue that chatbots whose focus is engagement need to personalize the generation of responses for different users' backgrounds, personal interests, and needs in order to serve their needs for communication, affection, and social belonging. In , participants expressed the desire for the chatbot to provide different answers to different users. Although  has found no significant effect of \\textit{personalization} on the user experience with the financial assistant chatbot, the study applies \\textit{personalization} as the ability to provide empathic responses according to the users' issues, where \\textit{emotional intelligence} plays a role. Interpersonal relationship can also be enriched by adapting the chatbots' language to match the user's context, energy, and formality; the ability to appropriately use language is discussed in Section \\ref{sec:thoroughness}.\n\\textbf{[B2] to provide unique services:} providing \\textit{personalization} increases the value of provided information . In the ethnography data collection study , eight participants reported dissatisfaction with the chatbot's generic guidance to specific places. Participants self-reported that the chatbot should use their current location to direct them to places more conveniently located, and ask for participants interests and preferences to direct them to areas that meet their needs. When exploring how teammates used a task-assignment chatbot,  found that the use of the chatbot varied depending on the participants' levels of hierarchy. Similarly, qualitative analysis of perceived interruption in a workplace chat  suggests that interruption is likely associated with users' general aversion to unsolicited messages at work. Hence, the authors argue that chatbot's messages should be personalized to the user's general preference.  also found that users with low social-agent orientation emphasize the utilitarian value of the system, while users with high social-agent orientation see the system as a humanized assistant. This outcome support to the need to personalize the interaction to individual user's mental models. In , participants reported preference for a chatbot that remembers their details, likes and dislikes, and preferences, and voluntarily uses the information to make recommendations. In , two participants also expected chatbots to retain context from previous interactions to improve recommendations.\n\\textbf{[B3] to reduce interactional breakdowns:} in HCI, \\textit{personalization} is used to customize the interface toward user familiarity . When evaluating\nvisual elements (such as quick replies) compared to typing the responses,  observed that users that start the interaction by clicking an option are more likely to continue the conversation if the next exchange also has visual elements as optional affordances. In contrast, users who typed are more likely to abandon the conversation when they face options to click. Thus, chatbots should adapt their interface to users' preferred input methods. In , one participant suggested that the choice of text color and font size should be customizable.  also observed that participants faced difficulties with small letters, and concluded that adapting the interface to provide accessibility also needs to be considered.\nAccording to the surveyed literature, the main challenge regarding \\textit{personalization} is \\textbf{[C1] privacy}. To enrich the efficiency and productivity of the interaction, a chatbot needs to have memory of previous interactions as well as learn user's preferences and disclosed personal information . However, as  and  suggest, collecting personal data may lead to privacy concerns. Thus, chatbots should showcase transparent purpose and ethical standards .  also suggest that there should be a way to inform a chatbot that something in the conversation is private. Similarly, participants in  reported that personal data \nand social media content may be inappropriate topics for chatbots because they can be sensitive\n. These concerns may be reduced if a chatbot demonstrates care about privacy .\nThe reported strategies to provide \\textit{personalization} in chatbots interactions are the following:\n\\textbf{[S1] to learn from and about the user:}  state that chatbots should present strategies to learn from cultural, behavioral, personal, conversational, and contextual interaction data. For example, the authors suggest using Facebook profile information to build knowledge about users' personal information.  also suggest that the chatbot should remember user's preferences disclosed in previous conversations. In , the authors propose an architecture where responses are generated based on a \\textit{personalization} rank that applies users' feedback about their general interests and preferences. When evaluating the user's experience with a virtual assistant chatbot,  found 16 mentions to personalized interactions, where participants demonstrated the need for a chatbot to be aware of their personal quirks and anticipate their needs.\n\\textbf{[S2] to provide customizable agents:}  suggest that users should be able to choose the level of the chatbot's attributes, for example, the agent's look and persona. By doing so, users with low social-agent orientation could use a non-humanized interface, which would better represent their initial perspective. This differentiation could be the first signal to personalize further conversation, such as focusing on more productive or playful interactions. Regarding chatbots' learning capabilities, in , interviews with potential users revealed that users should be able to manage what information the chatbot knows about them and decide whether the chatbot can learn from previous interactions or not. If the user prefers a more generic chatbot, then it would not store personal data, potentially increasing the engagement with more resistant users.  raise the possibility of offering an ``incognito'' mode for chatbots or asking the chatbot to forget what was said in previous utterances.\n\\textbf{[S3] visual elements:}  adopted quick replies as a means for the chatbot to tailor its subsequent questions to the specific experience the participant had reported. As discussed in Section \\ref{sec:conscientiousness}, quick replies may be seen as restrictive from an interactional perspective; however, conversation logs showed that the tailored questions prompted the users to report more detail about their experience, which is important in ethnography research. \nBoth the benefits and strategies identified in the literature are in line with the types of \\textit{personalization} proposed by . Therefore, further investigations in \\textit{personalization} can leverage knowledge from interactive systems (e.g., ) to adapt \\textit{personalization} strategies and manage the privacy concern.\n\\begin{framed} \\small \n\\vspace{-2mm}\nIn summary, the \\textbf{social intelligence} category includes characteristics that help a chatbot to manifest an adequate social behavior, by managing conflicts, using appropriate language, displaying \\textit{manners} and \\textit{moral agency}, sharing emotions, and handling personalized interactions. The benefits relate to resolving social positioning and recovering from failures, as well as increasing believability, human-likeness, engagement, and rapport. To achieve that, designers and researchers should care about privacy, emotional regulation issues, language consistency, and identification of failures and inappropriate content.\n\\vspace{-2mm}\n\\end{framed}", "cites": [6262, 650, 7476], "cite_extract_rate": 0.1875, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers to identify common themes like enriching interpersonal relationships, providing unique services, and reducing interactional breakdowns. It abstracts these findings into generalizable benefits and challenges, such as privacy. While it touches on limitations and critiques (e.g., lack of significant effect in one study), a deeper evaluative comparison of strategies or methodological strengths/weaknesses could enhance its critical depth."}}
{"id": "3da905ba-ca45-4a2d-9608-fe81f0daa556", "title": "Identity", "level": "subsubsection", "subsections": [], "parent_id": "fb12c3f5-18c5-4fb9-8465-0d7950752ebc", "prefix_titles": [["title", "How should my chatbot interact? A survey on social characteristics in human-chatbot interaction design"], ["section", "Chatbots Social Characteristics"], ["subsection", "Personification"], ["subsubsection", "Identity"]], "content": "\\label{sec:identity}\n\\textit{Identity} refers to the ability of an individual to demonstrate belonging to a particular social group . Although chatbots lack the agency to choose what social group to which they want to belong, designers attribute \\textit{identity} to them, intentionally or not, when they define the way a chatbot talks or behaves . The \\textit{identity} of a partner (even if only perceived) gives rise to new processes, expectations, and effects that influence the outcomes of the interaction . Aspects that convey the chatbots' \\textit{identity} include gender, age, language style, and name. Additionally, chatbots may have anthropomorphic, zoomorphic, or robotic representations. Some authors include \\textit{identity} aspects in the definition of \\textit{personality} (see, e.g., ). We distinguish these two characteristics, where \\textit{identity} refers to appearance and cultural traits while \\textit{personality} focuses on behavioral traits.\nWe found 16 studies that discuss \\textit{identity} issues, ten of which include \\textit{identity} as part of their main investigation . In two studies, the authors argue for the impact of \\textit{identity} on the interaction based on the literature . In three studies , qualitative analysis of conversational logs revealed that participants put effort into understanding aspects of the chatbots' \\textit{identity}. \\textit{Identity} concerns were primarily investigated for open domain and customer service chatbots (four studies each). In open domain interactions, \\textit{identity} is explored as a means of building common ground . In the case of customer services, \\textit{identity} helps manifest credibility and trust . Other domains include race-talk, education, and gaming. The complete list appears in the supplementary materials.\nThe identified benefits of attributing \\textit{identity} to a chatbot are the following:\n\\textbf{[B1] to increase engagement: }when evaluating signals of playful interactions,  found that agent-oriented conversations (asking about an agent's traits and status) are consistent with the tendency to anthropomorphize the agent and engage in chit-chat. In the educational scenario,  also observed questions about agents' appearance, intellectual capacities, and sexual orientation, although the researchers considered these questions inappropriate for the context of tutoring chatbots. When comparing human-like vs. machine-like language style, greetings, and framing,  noticed that using informal language, having a human name, and using greetings associated with human communication resulted in significantly higher scores for adjectives like likeable, friendly, and personal. In addition, framing the agent as ``intelligent'' also had a slight influence on users' scores.\n\\textbf{[B2] to increase human-likeness:} some attributes may convey a perceived human-likeness.  showed that using human-like language style, name, and greetings resulted in significantly higher scores for naturalness. The chatbot's framing influenced the outcomes when combined with other anthropomorphic clues. When evaluating different typefaces for a financial adviser chatbot,  found that users perceive machine-like typefaces as more chatbot-like, although they did not find strong evidence of handwriting-like typefaces conveying humanness.\nThe surveyed literature also highlights challenges regarding \\textit{identity}:\n\\textbf{[C1] to avoid negative stereotypes: }when engaging in a conversation, interlocutors base their behavior on common ground (joint knowledge, background facts, assumptions, and beliefs that participants have of each other) (see ). Common ground reflects stereotypical attributions that chatbots should be able to manage as the conversation evolves . In , the authors discuss that chatbots for company representations are often personified as attractive human-like women acting as spokespeople for their companies, while men chatbots tend to have a more important position, such as a virtual CEO.  state that the agent self-disclosure of gender \\textit{identity} opens possibilities to sex talk. The authors observed that the conversations mirror stereotyped male/female encounters, and the ambiguity of the chatbot's gender may influence the exploration of homosexuality. However, fewer instances were observed of sex-talk with the chatbot personified as a robot, which demonstrates that the gender \\textit{identity} may lead to the stereotypical attributions. When evaluating the effect of gender identity on disinhibition,  showed that people spoke more often about physical appearance and used more swear and sexual words with the female-presenting chatbot, and racist attacks were observed in interactions with black-representing chatbots. The conversation logs from  also show instances of users attacking the chatbot persona (a static avatar of a woman pointing to the conversation box).  and  also highlight that race identity conveys not only the physical appearance, but all the socio-cultural expectations about the represented group (see discussion in Section \\ref{sec:moralagency}). Hence, designers should care about the impact of attributing an \nidentity to chatbots in order to avoid reinforcing negative stereotypes.\n\\textbf{[C2] to balance the \\textit{identity} and the technical capabilities:} the literature comparing embodied vs. disembodied conversational agents yields contradictory results regarding the relevance of a human representation. For example, in the context of general-purpose interactions,  show that people exert more effort toward establishing common ground with the agent when they are represented as fully human; in contrast, when evaluating a website assistant chatbot,  show that simpler text-based chatbots with no visual, human \\textit{identity} resulted in less of an uncanny effect and reduced negative affect. Overly humanized agents create a higher expectation for users, which eventually leads to more frustration when the chatbot fails . When arguing about why chatbots fail,  advocate for balancing human versus robotic aspects, where \\textit{``too human''} representations may lead to off-topic conversations, and overly robotic interactions may lack personal touch and flexibility. When arguing about the social presence conveyed by deceptive chatbots,  state that extreme anthropomorphic features may generate cognitive dissonance. The challenge thus lies in designing a chatbot that provides appropriate \\textit{identity} cues, which correspond to their capabilities and communicative purpose, in order to convey the right expectation and minimize discomfort from over-personification.\nRegarding the strategies, the surveyed literature suggests \\textbf{[S1] to design and elaborate on a persona}. Chatbots should have a comprehensive persona and answer agent-oriented conversations with a consistent description of itself . For example,  discuss that Eliza, the psychotherapist chatbot, and Parry, a paranoid chatbot, have behaviors consistent with the stereotypes associated with the professional and personal identities, respectively.  suggest that designers should explicitly build signals of the chatbot personification (either machine- or human-like), so the users can have the right expectation about the interaction. When \\textit{identity} aspects are not explicit, users try to establish common ground. In  and , much of the small talk with the chatbot related to the chatbot's traits and status. In , the authors observed many instances of Alice's self-references to ``\\textit{her}'' artificial nature. These references triggered the users to reflect on their human-condition (self-categorization process), resulting in exchanges about their species (either informational or confrontational).  observed similar results, as participants engaged in conversations about the artificial nature of the agent. Providing the chatbot with the ability to describe its personal \\textit{identity} helps to establish the common ground, and hence, enrich the interpersonal relationship .\nChatbots may be designed to deceive users about their actual \\textit{identity}, pretending to be a human . In this case, the more human the chatbot sounds, the more successful it will be. In many cases, however, there is no need to engage in deception and the chatbots can be designed to represent an elaborated persona. \nResearchers can explore social identity theory  related to ingroup bias, power relations, homogeneity, and stereotyping, in order to design chatbots with \\textit{identity} traits that reflect their expected social position .", "cites": [6262, 7476], "cite_extract_rate": 0.08333333333333333, "origin_cites_number": 24, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple sources to form a coherent discussion on chatbot identity, linking aspects such as gender, language, and representation to user expectations and outcomes. It demonstrates critical analysis by addressing the risks of reinforcing stereotypes and the mismatch between identity and technical capabilities. The discussion abstracts to highlight broader design principles and the importance of managing common ground and cognitive dissonance."}}
{"id": "eec7d496-87cc-419a-b26e-00faf7d9c1fd", "title": "Personality", "level": "subsubsection", "subsections": [], "parent_id": "fb12c3f5-18c5-4fb9-8465-0d7950752ebc", "prefix_titles": [["title", "How should my chatbot interact? A survey on social characteristics in human-chatbot interaction design"], ["section", "Chatbots Social Characteristics"], ["subsection", "Personification"], ["subsubsection", "Personality"]], "content": "\\label{sec:personality}\n\\textit{Personality} refers to personal traits that help to predict someone's thoughts, feelings, and behaviors . The most accepted set of traits is called the Five-Factor model (or Big Five model) ), which describes \\textit{personality} across five dimensions (extraversion, agreeableness, conscientiousness, neuroticism, and openness). However, \\textit{personality} can also refer to other dynamic, behavioral characteristics, such as temperament and sense of humor . In the chatbots domain, \\textit{personality} refers to the set of traits that determines the agent's interaction style, describes its character, and allows the end-user to understand its general behavior . Chatbots with consistent \\textit{personality} are more predictable and trustworthy . According to , unpredictable swings in chatbots' attitudes can disorient users and create a strong sense of discomfort. Thus, \\textit{personality} ensures that a chatbot displays behaviors that stand in agreement with the users' expectations in a particular context .\nWe found 12 studies that report \\textit{personality} issues for chatbots. In some studies, \\textit{personality} was investigated in reference to the Big Five model , while two studies focused on sense of humor . Three studies investigated the impact of the \\textit{personality} of tutor chatbots on students' engagement .  compared users' preferences regarding pre-defined personalities. In the remaining studies,  \\textit{personality} concerns emerged from the qualitative analysis of the interviews, users' subjective feedback, and literature reviews . \\textit{Personality} was mostly investigated in open domain (five studies) and education (three studies) chatbots. The other two domains were gaming and humorous chatbots, which are both playful agents. Hence, \\textit{personality} is relevant when believability and attitude play a role in the interaction  (e.g., in open domain) and when the chatbots' attitude may increase users' mental comfort when performing a task , such as in educational contexts.\nThe surveyed literature revealed two benefits of attributing \\textit{personality} to chatbots:\n\\textbf{[B1] to increase believability:}  states that chatbots should have a \\textit{personality}, defined by the Five Factor model, plus characteristics such as temperament and tolerance, in order to build utterances using linguistic choices that cohere with these attributions. When evaluating a humorous chatbot,  compared the naturalness of the chatbot's inputs and the chatbot's human-likeness compared to a non-humorous chatbot. The humorous chatbot scored significantly higher in both constructs.  also showed that sense of humor humanizes the interactions, since humor was one of the factors that influenced naturalness for the WoZ condition.  demonstrated that manipulating language to manifest a target \\textit{personality} produced moderately natural utterances, with a mean rating of 4.59 out of 7 for the \\textit{personality} model utterances.\n\\textbf{[B2] to enrich interpersonal relationships:} chatbots \\textit{personality} can make the interaction more enjoyable . In , the second most frequent motivation for using chatbots, noted by 20\\% of the participants, was entertainment. The authors argue that a chatbot's capacity to be fun is important even when the main purpose is productivity; according to participants, the chatbot's ``\\textit{fun tips}'' enrich the user experience. This result is consistent with the experience of first-time users , where participants relate better with chatbots who have consistent \\textit{personality}.  show how witty banter and casual, enthusiastic conversations help make the interaction effortless. In addition, a few participants enjoyed the chatbot with a caring \\textit{personality}, who was described as a good listener. In  and , the authors argue that a consistent \\textit{personality} helps the chatbot to gain the users' confidence and trust.  state that tutor chatbots should display appropriate posture, conduct, and representation, which include being encouraging, expressive, and polite. Accordingly, other studies highlight students' desire for chatbots with positive agreeableness and extraversion . Outcomes consistently suggest that students prefer a chatbot that is not overly polite, but has some attitude. Agreeableness seems to play a critical role, helping the students to feel encouraged and deal with difficulties. Notably, agreeableness requires \\textit{emotional intelligence} to be warm and sympathetic in appropriate circumstances (see Section \\ref{sec:emotionalintelligence}).\nThe reviewed literature pointed out two challenges regarding \\textit{personality}:\n\\textbf{[C1] to adapt humor to the users' culture:} sense of humor is highly shaped by cultural environment .  discusses a Japanese chatbot who uses puns to create funny conversations. The authors state that puns are one of the main humor genres in that culture. However, puns are restricted to a specific culture and language, with low portability. Thus, the design challenge lies in personalizing chatbots' sense of humor to the target users' culture and interests or, alternatively, designing cross-cultural forms of humor. The ability to adapt to the context and users' preference is discussed in Section \\ref{sec:personalization}.\n\\textbf{[C2] to balance \\textit{personality} traits:}  observed that users prefer a proactive, productive, witty chatbot. Yet, they also would like them to be caring, encouraging, and exciting. In , the researchers intentionally generated utterances to reflect extreme personalities; as a result, they observed that some utterances sounded unnatural because a human's \\textit{personality} is a continuous phenomenon, rather than a discrete one.  also points out that, although \\textit{personality} is consistent, moods and states of mind constantly vary. Thus, balancing the predictability of the \\textit{personality} and the expected variation is a challenge to overcome.\nWe also identified strategies to design chatbots that manifest \\textit{personality}:\n\\textbf{[S1] to use appropriate language: } and  suggest that the chatbot language should be consistently influenced by its \\textit{personality}. Both studies propose that chatbots' architecture should include a persona-based model that encodes the \\textit{personality} and influences the response generation.  proposed a framework to shows that it is possible to automatically manipulate language features to manifest a particular \\textit{personality} based on the Big Five model. The Big Five model is a relevant tool because it can be assessed using validated psychological instruments . Using this model to represent the \\textit{personality} of chatbots was also suggested by  and .  discussed that a chatbot's \\textit{personality} should match its domain. Participants expected the language used by the news chatbot to be professional, while they expected the shopping chatbot to be casual and humorous. The ability to use consistent language is discussed in Section \\ref{sec:thoroughness}.\n\\textbf{[S2] to have a sense of humor:} literature highlights humor as a positive \\textit{personality} trait . In , ten participants mentioned enjoyment when the chatbots provided humorous and highly diverse responses. The authors found occurrences of the participants asking for jokes and expressing delight when the request was supported.  present similar results when arguing that humor is important even for task-oriented chatbots when the user is usually seeking for productivity. For casual conversations,  highlight that timely, relevant, and clever wit is a desired \\textit{personality} trait. In , the joker chatbot was perceived as more human-like, knowledgeable, and funny, and participants felt more engaged. \n\\textit{Personality} for artificial agents has been studied for some time in the Artificial Intelligence field . Thus, further investigations on chatbots' \\textit{personality} can leverage models for \\textit{personality} and evaluate how they contribute to believability and rapport building.\n\\begin{framed}\\small\n\\vspace{-2mm}\nIn summary, the \\textbf{personification} category includes characteristics that help a chatbot to manifest personal and behavioral traits. The \nbenefits relate to increasing believability, human-likeness, engagement, and interpersonal relationship, which is in line with the benefits of \\textbf{social intelligence}. However, unlike the \\textbf{social intelligence} category, designers and researchers should focus on attributing recognizable \\textit{identity} and \\textit{personality} traits that are consistent with users' expectations and the chatbot's capabilities. In addition, it is important to care about adaptation to users' cultural context and reducing the effects of negative stereotypes.\n\\vspace{-2mm}\n\\end{framed}", "cites": [7476], "cite_extract_rate": 0.043478260869565216, "origin_cites_number": 23, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes personality-related concepts from multiple papers, integrating both theoretical models (e.g., the Big Five) and empirical findings. It identifies benefits, challenges, and design strategies related to chatbot personality, showing analytical depth. While it provides some critical discussion (e.g., cultural adaptation of humor, balancing traits), the critique is not as deep or nuanced as it could be, and the abstraction remains grounded in practical design considerations rather than reaching a higher meta-level."}}
{"id": "212bc0e6-0de8-4c65-af3e-ab937c5ebe0e", "title": "A note on chatbots' humanness", "level": "subsection", "subsections": [], "parent_id": "616598c0-a6ef-4e0d-bae2-49c648e87606", "prefix_titles": [["title", "How should my chatbot interact? A survey on social characteristics in human-chatbot interaction design"], ["section", "Discussion"], ["subsection", "A note on chatbots' humanness"]], "content": "The social characteristics identified in the survey align well with characteristics that are present in human-human interactions. On the one hand, this survey shows the benefits of considering these characteristics when developing chatbots, which is supported by the Media Equation theory ; we identified the domains in which each characteristic was studied. On the other hand, previous studies have also shown that developing overly humanized agents results in high expectations and uncanny feelings , which was also pointed out in the surveyed literature as a challenge to conveying particular characteristics, such as \\textit{identity} and \\textit{personality}.\nAn explanation for these contrasts is that interactivity is ``dependent on the identity of the source with whom/which we are carrying out the message exchange''~, i.e., people direct their messages to an artificial agent. In interactions with chatbots, the artificial agent represents the social role usually associated with a human, for example, a tutor~, a healthcare provider~, a salesperson~, a hotel concierge~, or even a friend . The idea of assigning a social role to a chatbot does not imply deceiving people into thinking the software is human. Still, as the chatbots enrich their communication and social skills, their perceived social role may approach human profiles.", "cites": [6267, 7476], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from the surveyed literature to discuss the dual nature of chatbots' humanness, linking it to user expectations and challenges. It critically addresses the potential pitfalls of over-humanization and references relevant theories like the Media Equation. While it identifies broader implications of perceived social roles, it does not fully develop a novel framework or deep meta-level analysis."}}
{"id": "267f99ec-a3fd-4ef2-ae14-9b0b6a98c88c", "title": "Interrelationships among the characteristics", "level": "subsection", "subsections": [], "parent_id": "616598c0-a6ef-4e0d-bae2-49c648e87606", "prefix_titles": [["title", "How should my chatbot interact? A survey on social characteristics in human-chatbot interaction design"], ["section", "Discussion"], ["subsection", "Interrelationships among the characteristics"]], "content": "\\label{sec:interrelationship}\nIn Section \\ref{sec:socialcharacteristics}, we organized the social characteristics into discrete groups and discussed several instances of characteristics influencing each other, or being used as a strategy to manifest one another. In this section, we describe these relations in a theoretical framework, depicted in Figure \\ref{fig:relationship}. Rather than providing a comprehensive map, the goal here is to make explicit the relationships found in the surveyed chatbots literature, which underlines the complexity of developing chatbots with appropriate social behaviors. In Figure \\ref{fig:relationship}, boxes represent the social characteristics and the colors group them into their respective categories. The axes represent the 22 propositions we derived from the literature.\n\\begin{figure}[!hb]\n  \\centering\n  \\includegraphics[width=8cm]{interrelationship-survey}\n  \\caption{Interrelationship among social characteristics}\n  \\label{fig:relationship}\n\\end{figure}\nAccording to the surveyed literature, \\textit{proactivity} influences perceived \\textit{personality} \\textbf{(P1)} , since recommending and initiating topics may manifest higher levels of extraversion. When the proactive messages are based on the context, \\textit{proactivity} increases perceived \\textit{conscientiousness} \\textbf{(P2)} , since the proactive messages may demonstrate attention to the topic. \\textit{Proactivity} supports \\textit{communicability} \\textbf{(P3)} , since a chatbot can proactively communicate its knowledge and provide guidance; in addition, \\textit{proactivity} supports \\textit{damage control} \\textbf{(P4)} , since a chatbot can introduce new topics when the user either is misunderstood, tries to break the system, or sends an inappropriate message.\n\\textit{Conscientiousness} is itself a dimension of the Big Five model; hence, \\textit{conscientiousness} influences the perceived \\textit{personality} \\textbf{(P5)} . Higher levels of context management, goal-orientation, and understanding increase the chatbots' perceived efficiency, organization, and commitment . \\textit{Conscientiousness} manifests\\textit{ emotional intelligence} \\textbf{(P6)} since retaining information from previous turns and being able to recall them demonstrates empathy . In addition, \\textit{conscientiousness} manifests \\textit{personalization} \\textbf{(P7)}  because a chatbot can remember individual preferences within and across sessions.\n\\textit{Emotional intelligence} influences perceived \\textit{personality} \\textbf{(P8)}, since chatbots' \\textit{personality} traits affect the intensity of emotional reactions . Agreeableness is demonstrated through consistent warm reactions such as encouraging and motivating \n. Some \\textit{personality} traits \nrequire \\textit{personalization} \\textbf{(P9)} to adapt to the interlocutors' culture and interests . Besides, \\textit{personalization} benefits \\textit{identity} \\textbf{(P10)}, since the users' social-agent orientation may require a chatbot to adapt the level of engagement in small talk and the agent's visual representation . \\textit{Personalization} also improves \\textit{emotional intelligence} \\textbf{(P11)}, since a chatbot should dynamically regulate the affective reactions to the interlocutor . \\textit{Emotional intelligence} improves perceived \\textit{manners} \\textbf{(P12)}, since the lack of \\textit{emotional intelligence} may lead to the perception of impoliteness .\n\\textit{Conscientiousness} facilitates \\textit{damage control} \\textbf{(P13)}, since the attention to the \nworkflow and context may increase the ability to recover from a failure without restarting the workflow . \\textit{Communicability} facilitates \\textit{damage control} \\textbf{(P14)}, since it teaches the user how to communicate, reducing the numbers of mistakes . In addition, suggesting how to interact can reduce frustration after failure scenarios .\n\\textit{Personalization} manifests \\textit{thoroughness} \\textbf{(P15)} , since chatbots can adapt their language use to the conversational context and the interlocutor's expectations. When the context requires dynamic variation , \\textit{thoroughness} may reveal traits of the chatbot's \\textit{identity} \\textbf{(P16)} . As demonstrated by , \\textit{thoroughness} also reveals \\textit{personality} \\textbf{(P17)}.\n\\textit{Manners} influence \\textit{conscientiousness} \\textbf{(P18)} , since they can be applied as a strategy to politely refuse off-topic requests and to keep the conversation on track. \\textit{Manners} also influence \\textit{damage control} \\textbf{(P19)} , because they can help a chatbot prevent verbal abuse and reduce the negative effect of lack of knowledge. Both \\textit{moral agency} \\textbf{(P20)} and \\textit{emotional intelligence} \\textbf{(P21)} improve \\textit{damage control} because they provide\nthe ability to appropriately respond to abuse and testing . \\textit{Identity} influences \\textit{moral agency} \\textbf{(P22)}, since \\textit{identity} representations require the ability to prevent a chatbot from building or reinforcing negative stereotypes .", "cites": [7476], "cite_extract_rate": 0.038461538461538464, "origin_cites_number": 26, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes and integrates multiple relationships among social characteristics from the literature, forming a conceptual framework with 22 propositions. It abstracts these findings into a structured model, indicating broader design implications for chatbots. However, it offers limited critical analysis of the cited works, focusing mainly on summarizing and mapping relationships rather than evaluating their strengths, weaknesses, or contradictions."}}
{"id": "cefcc373-1755-478a-bdc0-b708ace17a7a", "title": "Related Surveys", "level": "section", "subsections": [], "parent_id": "48e1448c-68e1-4eb0-9088-8c4cfe65bb83", "prefix_titles": [["title", "How should my chatbot interact? A survey on social characteristics in human-chatbot interaction design"], ["section", "Related Surveys"]], "content": "\\label{sec:relatedsurveys}\nPrevious studies have reviewed the literature on chatbots. Several surveys discuss chatbots' urgency  and their potential application for particular domains, which include education , business , health , information retrieval and e-commerce . Other surveys focus on technical design techniques , such as language generation models, knowledge management, and architectural challenges. Although  discuss the social capabilities of chatbots, the survey focuses on the potential of available open source technologies to support these skills, highlighting technical hurdles rather than social ones.\nWe found three surveys  that include insights about social characteristics of chatbots, although none of them focus on this theme.  investigated chatbots that ``\\textit{mimic conversation rather than understand it},'' and review the main technologies and ideas that support their design, while  focuses on identifying best practices for developing script-based chatbots.  review the literature on quality issues and attributes for chatbots. The supplementary materials include a table that shows the social characteristics covered by each survey. These related surveys also point out technical characteristics and attributes that are outside the scope of this survey.", "cites": [8990, 6268], "cite_extract_rate": 0.1111111111111111, "origin_cites_number": 18, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of existing surveys on chatbots, mentioning their focus areas and some findings, but lacks deep synthesis of ideas or a cohesive framework. It briefly notes the absence of surveys focused specifically on social characteristics but does not critically evaluate the cited works or identify broader trends or principles. The narrative remains largely descriptive with minimal analytical depth."}}
{"id": "e47b0d6a-5496-42d1-b93d-9f8d302b471a", "title": "Limitations", "level": "section", "subsections": [], "parent_id": "48e1448c-68e1-4eb0-9088-8c4cfe65bb83", "prefix_titles": [["title", "How should my chatbot interact? A survey on social characteristics in human-chatbot interaction design"], ["section", "Limitations"]], "content": "\\label{sec:limitations}\nThis research has some limitations. Firstly, since this survey focused on disembodied, text-based chatbots, the literature on embodied and speech-based conversational agents was excluded. We acknowledge that studies that include these attributes can have relevant social characteristics for chatbots, especially for characteristics that could be highly influenced by physical representations, tone, accent, and so forth (e.g. identity, politeness, and thoroughness). However, embodiment and speech could also bring new challenges (e.g., speech-recognition or eye-gazing), which are beyond the scope of this study and could potentially impact users' experiences with chatbots. Additionally, this decision may have caused the under-representation of certain research domains. For example, there is an established research branch on conversational agents in the Ambient Intelligence discipline~ that primarily investigates multimodal interactions, and hence is not represented in this survey. Another example is the Software Engineering domain (see, e.g.,~), where chatbots have been widely investigated, but focused on the functional, rather than social aspects of the interactions.\nSecondly, since the definition of chatbot is not consolidated in the literature and chatbots have been studied across several different domains, some studies that include social aspects of chatbots may not have been found. To account for that, we adopted several synonyms in our research string and used Google Scholar as search engine, which provides a fairly comprehensive indexing of the literature in more domains. Finally, the conceptual model of social characteristics was derived through a coding process inspired by qualitative methods, such as Grounded Theory. Like any qualitative coding methods, it relies on the researchers' subjective assessment. To mitigate this threat, the researchers discussed the social characteristics and categories during in-person meetings until reaching consensus, and both the conceptual framework and the relationships amongst the characteristics were derived based on outcomes the surveyed studies explicitly reported.", "cites": [6269], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates analytical insight by acknowledging the limitations of the survey's scope and critically evaluating the exclusion of certain domains and modalities. It synthesizes ideas by connecting the exclusion of embodied and speech-based agents to broader implications for social characteristics and user experience. The abstraction is strong as it highlights the potential influence of embodiment and speech on social characteristics, and generalizes about the challenges and research gaps in the field."}}
