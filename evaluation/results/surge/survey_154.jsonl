{"id": "bdfa36b2-2c7c-4b90-9f88-6db90bfa11ed", "title": "Overview", "level": "subsection", "subsections": [], "parent_id": "7b6cd37d-11ff-499c-a935-4de93776c043", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Introduction"], ["subsection", "Overview"]], "content": "In the long history of Natural Language Processing (NLP), teaching computers to read the text and understand the meaning of the text is a major research goal that has not been fully realized. In order to accomplish this task, researchers have conducted machine reading comprehension (MRC) research in many aspects recently with the emergence of the large-scale datasets, higher computing power, and the deep learning techniques, which have boosted the whole NLP research . The concept of MRC comes from the human understanding of text. The most common way to test whether a person can fully understand a piece of text is to require she/he answer questions about the text. Just like the human language test, reading comprehension is a natural way to evaluate a computer's language understanding ability.\nIn the NLP community, machine reading comprehension has received extensive attention in recent years . The goal of a typical MRC task is to require a machine to read a (set of) text passage(s) and then answers questions about the passage(s), which is very challenging .\nMachine reading comprehension could be widely applied in many NLP systems such as search engines and dialogue systems. For example, as shown in Figure \\ref{figure:application}, nowadays, when we enter a question into the search engine Bing, sometimes the Bing can directly return the correct answer by highlight it in the context (if the question is simple enough). Moreover, if we open the \"Chat with Bing\" in the website of Bing, as shown in the right part of the browser in Figure \\ref{figure:application}, we can also ask it questions such as \"How large is the pacific?\", the Bing chatbot will directly give the answer \"63.78 million square miles\". And on Bing's App, we can also open this \"Chat with Bing\", as shown in the right part of Figure \\ref{figure:application}. It is clear that MRC can help improve the performances of search engines and dialogue systems, which can allow users to quickly get the right answer to their questions, or to reduce the workload of customer service staff. \n\\begin{figure}[H]\n\t\\centering\n\t\\includegraphics[width=15cm]{Graphs/Figure_Bing}\n\t\\caption{Examples of machine reading comprehension applied to search engine and dialogue system.}\n\t\\label{figure:application}\n\\end{figure}", "cites": [6919, 549, 6918, 4280, 8763], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.8, "critical": 1.6, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic introduction to MRC, mentioning its goals and applications, and references several survey papers. However, it does not synthesize these sources into a cohesive narrative or highlight connections between them. There is minimal critical analysis or identification of broader trends or principles in the field."}}
{"id": "5bb4bc8e-848f-4475-9c3c-12b0c5f4f671", "title": "History", "level": "subsection", "subsections": [], "parent_id": "7b6cd37d-11ff-499c-a935-4de93776c043", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Introduction"], ["subsection", "History"]], "content": "Machine reading comprehension is not newly proposed. As early as 1977, Lehnert et al.  had already built a question answering program called the QUALM which was used by two story understanding systems. In 1999, Hirschman et al.  constructed a reading comprehension system with a corpus of 60 development and 60 test stories of 3rd to 6th-grade material. The accuracy of the baseline system is between 30\\% and 40\\% on 11 sub-tasks. Most of MRC systems in the same period were rule-based or statistical models . However, due to the lack of high quality MRC datasets, this research field has been neglected for a long time . In 2013, Richardson et al.  created the MCTest  dataset which contained 500 stories and 2000 questions. Later, many researchers began to apply machine learning models on MCTest  despite that the original baseline of MCTest  is a rule-based model and the number of training samples in the MCTest  dataset is not large. A turning point for this field came in 2015 . In order to resolve these bottlenecks, Hermann et al.  defined a new dataset generation method that provides large-scale supervised reading comprehension datasets in 2015. They also developed a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure. Since 2015, with the emergence of various large-scale supervised datasets and neural network models, the field of machine reading comprehension has entered a period of rapid development. Figure \\ref{figure: paper } shows the numbers of research papers on MRC since 2013. As is seen, the number of papers on MRC has been growing at an impressive rate.\n\\begin{figure}[H]\n\t\\centering\n\t\\includegraphics[width=15cm]{Graphs/Figure_Papers}\n\t\\caption{The number of research papers for machine reading comprehension each year: (\\textbf{a}) The number of research papers on MRC in ACL from 2013 to 2019. (\\textbf{b}) The number of research papers on MRC in ENMLP from 2013 to 2019. (\\textbf{c}) The number of research papers on MRC in Web of Science from 2013 to 2019.(\\textbf{d}) The number of research papers on MRC in Google scholar from 2013 to 2019.}\n\t\\label{figure: paper }\n\\end{figure}", "cites": [1140], "cite_extract_rate": 0.1, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a chronological overview of MRC development, citing key papers and datasets. While it connects some ideas (e.g., the shift from rule-based to neural models), it largely describes events and systems without deep analysis or comparison. The abstraction is limited to surface-level observations such as the growth in research papers."}}
{"id": "2d2cc132-bccf-4c34-be32-885d9cfa5be8", "title": "Motivation", "level": "subsection", "subsections": [], "parent_id": "7b6cd37d-11ff-499c-a935-4de93776c043", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Introduction"], ["subsection", "Motivation"]], "content": "The benchmark datasets play a crucial role in speeding up the development of better neural models. In the past few years, we have witnessed an explosion of work that brings various MRC benchmark datasets . Figure \\ref{figure: model} (a) shows the cumulative number of MRC datasets from the beginning of 2014 to the beginning of 2020. It shows that the number of MRC datasets has increased exponentially in recent years. And these novel datasets inspired a large number of new neural MRC models, such as those shown in Figure \\ref{figure: model} (b), just take SQuAD 1.1  for example, we can see that many neural network models were created in recent years, such as BiDAF , ELMo , BERT , RoBERTa  and XLNet . The performance of the state-of-the-art neural network models have already exceeded human performance over the related MRC benchmark datasets.\n\\begin{figure}[H]\n\t\\centering\n\t\\includegraphics[width=15cm]{Graphs/Figure_DatasetsNumber}\n\t\\caption{The number of MRC datasets created in recent years and the F1 scores of state-of-the-art models on SQuAD 1.1 : (\\textbf{a}) The cumulative number of MRC datasets from the beginning of 2014 to the end of 2019. (\\textbf{b}) The progress of state-of-the-art models on SQuAD 1.1 since this dataset was released. The data points are taken from the leaderboard at \\href{https://rajpurkar.github.io/SQuAD-explorer/}{https://rajpurkar.github.io/SQuAD-explorer/}.}\n\t\\label{figure: model}\n\\end{figure}  \nDespite the critical importance of MRC datasets, most of the existing MRC reviews have focused on MRC algorithms for improving system performance , performance comparisons , or general review that has limited coverage of datasets .  In addition, there is also a need for systematic categorization/classification of task types. For example, MRC tasks are usually divided into four categories: cloze style, multiple-choice, span prediction and free form . But this classification method is not precise because the same MRC task could belong to both cloze style and multiple-choice style at the same time, such as the CBT  task in the Facebook bAbi project . Moreover, most researchers focus on few popular MRC datasets while most other MRC datasets are not widely known and studied by the community. To address these gaps, a comprehensive survey of existing MRC benchmark datasets, evaluation metrics and tasks is strongly needed.\nAt present, a lot of neural MRC models have already surpassed human performance on many MRC datasets, but there is still a giant gap between existing MRC and real human comprehension . This shows the need of improving existing MRC datasets in terms of both question and answer challenges and related evaluation criteria. In order to build more challenging MRC datasets, we need to understand existing MRC tasks, evaluation metrics and datasets better.", "cites": [4284, 6919, 6918, 4280, 6920, 439, 1139, 826, 6921, 8385, 8763, 7, 11], "cite_extract_rate": 0.7222222222222222, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple cited papers to argue for a more comprehensive survey of MRC datasets and tasks, highlighting a lack of systematic classification and underutilization of certain datasets. It provides some critical observations, such as the ambiguity in task categorization and the gap between model performance and real human comprehension, but these critiques are not deeply elaborated. The section identifies broader patterns in MRC research and motivates the need for improvement, though it stops short of presenting a meta-level framework or deep abstraction."}}
{"id": "76829700-3362-479a-a416-50707f1d6d13", "title": "Definition of Typical MRC Tasks", "level": "subsection", "subsections": [], "parent_id": "10776ca3-8a89-4127-82ed-8bd8dbb2b4e0", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Tasks"], ["subsection", "Definition of Typical MRC Tasks"]], "content": "In our survey, machine reading comprehension is considered as a special research field, which includes some specific tasks, such as multi-modal machine reading comprehension, textual machine reading comprehension, etc. Since most of the existing machine reading comprehension tasks are in the form of question answering, the textual QA-based machine reading comprehension task is considered to be the typical machine reading comprehension task.\nAccording to previous review papers on MRC , the definition of a typical MRC task is:\n\\begin{table}[H]\n\t\\centering\n\t\\begin{tabular}{p{12cm}c}\n\t\t\\textbf{Definition 1.} Typical machine reading comprehension task could be formulated as a supervised learning problem. Given a collection of textual training examples $\\big\\{\\big(p_{i}, q_{i}, a_{i}\\big)\\big\\}_{i=1}^{n}$, where $p$ is a passage of text, and $q$ is a question regarding the text $p$. The goal of typical machine reading comprehension task is to learn a predictor $f$ which takes a passage of text $p$ and a corresponding question $q$ as inputs and gives the answer $a$ as output, which could be formulated as the following formula :\t\t\n\t\t\\begin{equation}a=f(p, q)\\end{equation}\t\t\n\t\tand it is necessary that a majority of native speakers would agree that the question $q$ does regarding that text $p$, and the answer $a$ is a correct one which does not contain information irrelevant to that question.\\\\\n\t\\end{tabular}\n\\end{table}", "cites": [6921], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic definition of typical MRC tasks, primarily relying on a single cited paper and restating its formulation in a tabular format. It lacks meaningful synthesis across multiple sources, critical evaluation of the cited work, and higher-level abstraction to broader principles or frameworks."}}
{"id": "8f42d6cb-b069-4a71-8196-d40295a70d95", "title": "Multi-modal MRC vs. Textual MRC", "level": "subsubsection", "subsections": [], "parent_id": "0d71ba93-6a85-4ee5-abb4-666a0b3576c8", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Tasks"], ["subsection", "Discussion on MRC Tasks"], ["subsubsection", "Multi-modal MRC vs. Textual MRC"]], "content": "Multi-modal MRC is a new challenging task that has received increasing attention from both the NLP and the CV communities. Compared with existing MRC tasks which are mostly textual, multi-modal MRC requires a deeper understanding of the text and visual information such as images and videos. When human reads, illustrations can help to understand the text. Experiments showed that children with higher mental imagery skills outperformed children with lower mental imagery skills on story comprehension after reading the experimental narrative . These results emphasize the importance of mental imagery skills for explaining individual variability in reading development .\nTherefore, if we want the machine to acquire human-level reading comprehension ability, multi-modal machine reading comprehension is a promising research direction.\nIn fact, there are already many tasks and datasets in this field, such as the TQA , MovieQA , COMICS  and RecipeQA . As seen in Figure \\ref{figure: mmtasks }, TQA is a multi-modal MRC dataset that aims at answering multi-modal questions given a context of text, diagrams and images. \n\\begin{figure}[H]\n    \\centering\n\t\\begin{tabular} {m{6cm} m{6cm}}\n\t\t\\toprule\n\t\t\\textbf{Passage with illustration:}                & \\multirow{2}{*}{\\includegraphics[width=6cm]{Graphs/Figure_TQA1}} \\\\\n\t\tThis diagram shows the anatomy of an Animal cell. Animal Cells have an outer boundary known as the plasma membrane. The nucleus and the organelles of the cell are bound by this membrane. The cell organelles have a vast range of functions to perform like hormone and enzyme production to providing energy for the cells. They are of various sizes and have irregular shapes. Most of the cells size range between 1 and 100 micrometers and are visible only with help of microscope. &                      \\\\\n        \\midrule\t\n\t\t\t\\textbf{Question with illustration:}               & \\multirow{8}{*}{\\includegraphics[width=6cm]{Graphs/Figure_TQA2}} \\\\\n\t\tWhat is the outer surrounding part of the Nucleus? &                      \\\\\n\t\t                                                   &                     \\\\\n\t\t\\textbf{Choices:}                                  &                      \\\\\n\t\t(1) Nuclear Membrane  $\\surd$                      &                      \\\\\n\t\t(2) Golgi Body                                     &                      \\\\\n\t\t(3) Cell Membrane                                  &                      \\\\\n\t\t(4) Nucleolus                                      &                     \\\\\n\t\t                                                   &                 \t \\\\\t\t\n        \\bottomrule\n\t\t\\end{tabular}\n\t\t\\caption{An example of multi-modal MRC task. The illustrations and questions are taken from the TQA  dataset.}\n\t\t\\label{figure: mmtasks }\n\\end{figure}", "cites": [6921, 6923, 6922], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic analytical perspective by distinguishing multi-modal MRC from traditional textual MRC and citing relevant datasets like TQA and MovieQA. However, synthesis is limited to a surface-level integration of these works without a deeper framework or comparison. Critical analysis is minimal, as the section does not evaluate limitations or compare approaches in detail. Some abstraction is attempted by emphasizing the role of mental imagery in comprehension, but broader principles or trends are not clearly generalized."}}
{"id": "58fc7169-0d98-460b-ae86-4f458d1807c9", "title": "Machine Reading Comprehension vs. Question Answering", "level": "subsubsection", "subsections": [], "parent_id": "0d71ba93-6a85-4ee5-abb4-666a0b3576c8", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Tasks"], ["subsection", "Discussion on MRC Tasks"], ["subsubsection", "Machine Reading Comprehension vs. Question Answering"]], "content": "The relationship between question answering and machine reading comprehension is very close. Some researchers consider MRC as a kind of specific QA task . Compared with other QA tasks such as open-domain QA, MRC is characterized by that the computer is required to answer questions according to the specified text. However, other researchers regard the machine reading comprehension as a kind of method to solve QA tasks. For example, in order to answer open-domain questions, Chen et al.  first adopted document retrieval to find the relevant articles from Wikipedia, then used MRC to identify the answer spans from those articles. Similarly, Hu  regarded machine reading as one of the four methods to solve QA tasks. The other three methods are rule-based method, information retrieval method and knowledge-based method.\nHowever, although the typical machine reading comprehension task is usually in the form of textual question answering, the forms of MRC tasks are usually diverse. Lucy Vanderwende  argued that machine reading could be defined as an automatic understanding of text. \"One way in which human understanding of text has been gauged is to measure the ability to answer questions pertaining to the text. An alternative way of testing human understanding is to assess one's ability to ask sensible questions for a given text\". \nIn fact, there are many such benchmark datasets for evaluating such techniques. For example, ShARC  is a conversational MRC dataset. Unlike other conversational MRC datasets, when answering questions in the ShARC, the machine needs to use background knowledge that is not in the context to get the correct answer. The first question in a ShARC conversation is usually not fully explained and does not provide enough information to answer directly. Therefore, the machine needs to take the initiative to ask the second question, and after the machine has obtained enough information, it then answers the first question. \nAnother example is RecipeQA  which is a dataset for multi-modal comprehension of illustrated recipes. There are four sub-tasks in RecipeQA, one of which is ordering task. Ordering task tests the ability of a model in finding a correctly ordered sequence given a jumbled set of representative images of a recipe . As in previous visual tasks, the context of this task consists of the titles and descriptions of a recipe. To successfully complete this task, the model needs to understand the temporal occurrence of a sequence of recipe steps and infer temporal relations between candidates, i.e. boiling the water first, putting the spaghetti next, so that the ordered sequence of images aligns with the given recipe. In addition, in the MS MARCO , ordering tasks are also included.\nIn summary, although most machine reading comprehension tasks are in the form of question answering, it does not mean that machine reading comprehension tasks belong to the question answering. In fact, as mentioned above, the forms of MRC tasks are diverse. Question answering also includes a lot of tasks that do not emphasize that the system must read a specific context to get an answer, such as rule-based question answering systems and knowledge-based question answering systems (KBQA). Figure \\ref{figure: relation } illustrates the relation between machine reading comprehension (MRC) tasks and question answering (QA) tasks. As shown in Figure \\ref{figure: relation }, we regard the general machine reading comprehension and the question answering as two subfields in the research field of natural language processing, both of which contain various specific tasks, such as Visual Question Answering (VQA) tasks, multi-modal machine reading comprehension tasks, etc.\nAmong them, some of these tasks belong to both natural language processing and computer vision research fields, such as the VQA task and the multi-mode reading comprehension task.\nLastly, most of the existing MRC tasks are textual question answering tasks, so we regard this kind of machine reading comprehension task as a typical machine reading comprehension task, and its definition is shown in Definition 1 above.\n\\begin{figure}[H]\n\t\\centering\n\t\\includegraphics[width=10 cm]{Graphs/Figure_Relations}\n\t\\caption{The relations between machine reading comprehension (MRC), question answering (QA), natural language processing (NLP) and computer version (CV). }\n\t\\label{figure: relation }\n\\end{figure}", "cites": [446, 6921], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple perspectives on the relationship between MRC and QA, drawing on different definitions and methodological approaches from the literature. It provides a critical comparison by highlighting the distinctions in task formulations and the roles MRC can play (as a method or as a task), while also pointing out the diversity of MRC forms beyond QA. The abstraction is strong, as it generalizes these ideas into a broader understanding of NLP subfields and their intersections."}}
{"id": "04cb4b17-6832-44dc-8fa5-2a8f890e6f9e", "title": "Machine Reading Comprehension vs. Other NLP tasks", "level": "subsubsection", "subsections": [], "parent_id": "0d71ba93-6a85-4ee5-abb4-666a0b3576c8", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Tasks"], ["subsection", "Discussion on MRC Tasks"], ["subsubsection", "Machine Reading Comprehension vs. Other NLP tasks"]], "content": "There is a close and extensive relationship between machine reading comprehension and other NLP tasks. First of all, many useful methods in the field of machine reading comprehension can be introduced into other NLP tasks. For example, the stochastic answer network (SAN)  is first applied to MRC tasks and achieved results competitive to the state of the art on many MRC tasks such as the SQuAD and the MS MARCO. At the same time, the SAN can also be used in natural language processing (NLP) benchmarks , such as Stanford Natural Language Inference (SNLI), MultiGenre Natural Language Inference (MultiNLI), SciTail, and Quora Question Pairs datasets. For another example, Yin et al.(2017)  regards the document-level multi-aspect sentiment classification task as a machine understanding task, and proposed a hierarchical iterative attention model. The experimental result of this model outperforms the classical baseline in TripAdvisor and BeerAdvocate datasets.\nSecondly, some other NLP research results can also be introduced into the MRC area. Asai et al. (2018)  solved the task of non-English reading comprehension through a neural network translation (NMT) model based on attention mechanism. In detail, the paragraph question pair of non-English language is translated into English using the neural machine translation model, so that the English extraction reading comprehension model can output its answer, and then use the attention weights of the neural machine translation model to align the answers in the target text. Extra knowledge can also be introduced into MRC tasks. The authors of SG-Net  used syntax information to constrain attention in the MRC task. They used the syntactic dependency of interest (SDOI) to form an SDOI-SAN and have achieved state-of-the-art results on SQuAD 2.0 challenge. Minaee et al. (2020)  summarized more than 150 deep learning text classification methods and their performance on more than 40 popular datasets. Many of the methods mentioned in this article have been applied to MRC tasks. \nThirdly, MRC can be used as a step or component in the pipeline of some complex NLP tasks. For example, machine reading comprehension can be used as a step in open domain QA . And in many dialogue tasks, machine reading comprehension can also be regarded as a part of pipeline .", "cites": [6927, 6928, 6926, 446, 6918, 6924, 1098, 6186, 6925], "cite_extract_rate": 0.8181818181818182, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several papers to illustrate how MRC methods can cross-pollinate with other NLP tasks and vice versa, creating a coherent narrative around the interrelations between these areas. It introduces broader patterns, such as the use of MRC as a component in complex pipelines. However, the critical analysis is limited, as it primarily highlights successes without addressing limitations or trade-offs. The abstraction is moderate, as it begins to frame MRC within a larger NLP context but does not reach a meta-level theoretical insight."}}
{"id": "ca799238-097e-4dc8-93f3-2988f9e1da43", "title": "Existing Classification Methods of MRC tasks", "level": "subsubsection", "subsections": [], "parent_id": "ef163978-2945-44ab-9479-9584020b71d5", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Tasks"], ["subsection", "Classification of MRC Tasks"], ["subsubsection", "Existing Classification Methods of MRC tasks"]], "content": "In many research papers , MRC tasks are divided into four categories: cloze style, multiple-choice, span prediction, and free-form answer. Their relationship is shown in Figure \\ref{figure:existingclass}:\n\\begin{figure}[H]\n\t\\centering\n\t\\includegraphics[width=11 cm]{Graphs/Figure_OldTypes}\n\t\\caption{Existing classification method of machine reading comprehension tasks.}\n\t\\label{figure:existingclass}\n\\end{figure} \n\\begin{itemize}[leftmargin=*,labelsep=4.9mm]\n\t\\item\tCloze style\n\\end{itemize}\nIn a cloze style task, there are some placeholders in the question. The MRC system needs to find the most suitable words or phrases which can be filled in these placeholders according to the context content.\n\\begin{itemize}[leftmargin=*,labelsep=4.9mm]\t\n\t\\item\tMultiple-choice\n\\end{itemize}\nIn a multiple-choice task, the MRC system needs to select a correct answer from a set of candidate answers according to the provided context.\n\\begin{itemize}\t[leftmargin=*,labelsep=4.9mm]\t\n\t\\item\tSpan prediction\n\\end{itemize}\nIn a span prediction task, the answer is a span of text in the context. That is, the MRC system needs to select the correct beginning and end of the answer text from the context.\n\\begin{itemize}[leftmargin=*,labelsep=4.9mm]\t\n\t\\item\tFree-form answer\n\\end{itemize}\nThis kind of tasks allows the answer to be any free-text forms, that is, the answer is not restricted to a single word or a span in the passage .\\\\", "cites": [4284, 6921], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of four MRC task types but does not effectively synthesize or integrate insights from the cited papers. There is no critical evaluation or comparison of these classification methods, nor are broader patterns or principles discussed. The content remains at a surface level with minimal analytical depth."}}
{"id": "f245e3b4-9f10-4095-a41e-0c0f58358d2a", "title": "Limitations of Existing Classification Method", "level": "subsubsection", "subsections": [], "parent_id": "ef163978-2945-44ab-9479-9584020b71d5", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Tasks"], ["subsection", "Classification of MRC Tasks"], ["subsubsection", "Limitations of Existing Classification Method"]], "content": "However, the above task classification method does have certain limitations. Here are the reasons:\nFirst, an adequate classification method should be precise at least or can classify each MRC task distinctly. But the existing classification method is a bit ambiguous or indistinct, that is, according to this classification method, a MRC task may belong to multiple task types. \nFor instance, as seen in Figure \\ref{figure: textual_cloze }, a sample in the \"Who did What\" task  are both in the form of \"Cloze style\" and \"Multiple-choice\", and we can see that the answer is a span of a text in the context so that it can also be classified to \"Span prediction\". \n\\begin{figure}[H]\n\t\\centering\n\t\\begin{tabular}{p{13cm}l}\n\t\t\\toprule\n\t\t\\textbf{Passage:}\\quad Tottenham won 2-0 at Hapoel Tel Aviv in UEFA Cup action on Thursday night in a defensive display which impressed Spurs skipper Robbie Keane. ... Keane scored the first goal at the Bloomfield Stadium with \\textbf{\\textit{\\color{red}Dimitar Berbatov}}, who insisted earlier on Thursday he was happy at the London club, heading a second. The 26-year-old Berbatov admitted the reports linking him with a move had affected his performances ... Spurs manager Juande Ramos has won the UEFA Cup in the last two seasons ... \\\\\n\t\t\\midrule\n\t\t\\textbf{Question:}\\quad Tottenham manager Juande Ramos has hinted he will allow \\underline{\\hbox to 18mm{}} to leave if the Bulgaria striker makes it clear he is unhappy. \\\\\n\t\t\\midrule\n\t\t\\textbf{Choices: } \\quad (A) Robbie Keane \\quad(B)\t\\textbf{\\color{red}Dimitar Berbatov }$\\surd$\\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n \t\\caption{An example of MRC task. The question-answer pair and passage are taken from the \"Who did What\" . } \t\n \t\\label{figure: textual_cloze }\t\t\n\\end{figure}\t\nSecondly, with the rapid development of MRC, a large number of novel MRC tasks have emerged in recent years. One example is multi-modal MRC, such as MovieQA , COMICS , TQA  and RecipeQA . Compared with the traditional MRC task which only requires understanding a text, the multi-modal MRC task requires the model to understand the semantics behind the text and visual images at the same time. A fundamental characteristic of human language understanding is multimodality. Our observation and experience of the world bring us a lot of common sense and world knowledge, and the multi-modal information is extremely important for us. In essence, real world information is multi-modal and widely exists in texts, voices, and images. But these multi-modal tasks are ignored by the existing classification method. \nIn addition, as seen in Figure \\ref{figure:fuzzy}, we list several tasks that belong to the fuzzy classification mentioned above, such as ReviewQA, Qangaroo, Who-did-What, MultiRC, LAMBADA, ReCoRD. Due to the limited space, we only list a few of them in the figure. According to our statistics, among the 57 MRC tasks we collected, 29 tasks fall into this situation. \n\\begin{figure}[H]\n\t\\centering\n\t\\includegraphics[width=12 cm]{Graphs/Figure_fuzzy}\n\t\\caption{The indistinct classification caused by existing classification method.}\n\t\\label{figure:fuzzy}\n\\end{figure}", "cites": [1143, 6921, 6923, 6922], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a critical analysis of the limitations in MRC task classification, using examples and statistics to highlight ambiguity and the exclusion of emerging multi-modal tasks. It synthesizes insights from multiple papers to illustrate these limitations and generalizes the issue to broader trends in MRC task evolution, though it does not fully develop a new conceptual framework."}}
{"id": "4bc07e7f-be63-4869-898a-7f50223c0ec0", "title": "Type of Questions", "level": "subsubsection", "subsections": [], "parent_id": "f91b4d08-6d49-4fd2-b246-9f9620414b17", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Tasks"], ["subsection", "Definition of each category in the new classification method"], ["subsubsection", "Type of Questions"]], "content": "According to the type of question, a MRC task can be classified into three categories: cloze style, natural form, and synthetic form:\n\\begin{itemize}[leftmargin=*,labelsep=4.9mm]\t\n\\item\tCloze style\n\\end{itemize}\nThe cloze question is usually a sentence with a placeholder. Its sentence pattern may be a declarative sentence, an imperative sentence, etc., and is not necessarily an interrogative sentence. In addition, the sentence may also contain image information. The system is required to find a correct word, phrase or image that is suitable to be filled in the placeholder so that the sentence is complete. The cloze question can be defined as:\n\\begin{table}[H]\n\t\\centering\n\t\\begin{tabular}{p{13cm}l}\n\t\t\\textbf{Definition 4.} Given the context $C=\\big\\{c_{0}, c_{1}, \\ldots c_{j}, \\ldots c_{j+n} \\ldots,c_{l c}\\big\\}$ $\\big(0 \\leq j \\leq l c, 0 \\leq n \\leq l c-1, c_{j} \\in V \\cup M \\big)$, where $l_{c}$ denotes the length of this context $C$. $A=\\big\\{c_{j}, \\ldots c_{j+n}\\big\\}$ is a short span in context $C$. After replaced $A$ with a placeholder $X$, a cloze style question $Q$ for context $C$ is formed, it can be formulated as $Q=\\big\\{c_{0}, c_{1}, \\ldots  X \\ldots, c_{l c}\\big\\}$, in which the $X$ is a placeholder. The answer to question $Q$ is the $A=\\big\\{c_{j}, \\ldots c_{j+n}\\big\\}$. \n\t\\end{tabular}\n\\end{table}\nAccording to the type of corpus, cloze questions also can be divided into textual and multi-modal. \\\\\nA textual cloze question is usually a sentence with a placeholder. The MRC system is required to find a correct word or phrase that is suitable to be filled in the placeholder so that the sentence is complete. An example of textual cloze question has been shown in Figure \\ref{figure: textual_cloze }.\nA multi-modal cloze question is a natural sentence with visual information such as images, but some parts of these images are missing, and the MRC system is required to fill in the missing images. For example, a sample of visual cloze question in the RecipeQA  dataset is shown in Figure \\ref{figure: mm_cloze }:\n\\begin{figure}[H]\n\t\\centering\n\t\\begin{tabular}{p{13cm}l}\n\t\t\\toprule\n\t\t\\textbf{Passage}\\\\\n\t   Last-Minute Lasagna:\\\\\n\t   1. Heat oven to 375 degrees F. Spoon a thin layer of sauce over the bottom of a 9-by-13-inch baking dish.\\\\\n\t   2. Cover with a single layer of ravioli.\\\\\n\t   3. Top with half the spinach half the mozzarella and a third of the remaining sauce.\\\\\n\t   4. Repeat with another layer of ravioli and the remaining spinach mozzarella and half the remaining sauce.\\\\\n\t   5. Top with another layer of ravioli and the remaining sauce not all the ravioli may be needed. Sprinkle with the Parmesan.\\\\\n\t   6. Cover with foil and bake for 30 minutes. Uncover and bake until bubbly, 5 to 10 minutes.\\\\\n\t   7. Let cool 5 minutes before spooning onto individual plates.\\\\\n\t\t\\midrule\n\t\t\\textbf{Question:}\\quad Choose the best image for the missing blank to correctly complete the recipe.\\\\\n\t\t\\includegraphics[width=13 cm]{Graphs/Figure_RecipeQA2}\\\\\n\t\t\\midrule\n\t\t\\textbf{Choices:}\\\\\n\t\t\\includegraphics[width=13 cm]{Graphs/Figure_RecipeQA1}\\\\\n\t\t\\quad\\quad\\  \\textbf{(A)} \n\t\t$\\surd$\\quad\\quad\\ \\quad\\qquad\\qquad\\qquad (B)  \\quad\\quad\\ \\qquad\\qquad\\qquad\\qquad (C) \\quad\\quad\\ \\qquad\\qquad\\qquad\\qquad (D)\\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t\\caption{An example of multi-modal cloze style question. The images and questions are taken from the RecipeQA  dataset.}\n    \\label{figure: mm_cloze }\n\\end{figure}\n\\begin{itemize}[leftmargin=*,labelsep=4.9mm]\n\t\\item\tNatural form\n\\end{itemize}\nA question in natural form is a natural question that conforms to the grammar of natural language. Different from the cloze question, which contains placeholder, a natural form question is a complete sentence and a question that conforms to the grammatical rules. It could be defined as:\n\\begin{table}[H]\n\t\\centering\n\t\\begin{tabular}{p{13cm}l}\n\t\t\\textbf{Definition 5.} In a MRC task, given a 'Natural' question $Q$, it could be formulated as  $Q_{i}=\\big\\{q_{0}, q_{1}, \\ldots q_{i} \\ldots,  q_{l q}\\big\\}$, where $q_{i} \\in V \\cup M(0 \\leq i \\leq l q)$. $Q$ denotes a complete sentence (may also contain images) that conforms to the natural language grammar and $l_{q}$ denotes the length of the question $Q$.\n\t\t\\\\\n\t\\end{tabular}\n\\end{table}\nIn most cases, a 'Natural' question $Q$ is an interrogative sentence that asks a direct question and is punctuated at the end with a question mark. However, in some cases, $Q$ may not be an interrogative sentence but an imperative sentence, for example, \"please find the correct statement from the following options.\" \nIn addition, according to the type of corpus, natural form questions can be divided into textual and multi-modal. Textual natural question is usually a natural question or imperative sentence. With some graphics or video, the multi-modal natural question is also a natural question or imperative. Example of textual natural question is shown in Figure \\ref{figure: textual_natural } below, and example of multi-modal natural question has been shown in Figure \\ref{figure: mmtasks }.\n\\begin{figure}[H]\n\t\\centering\n\t\\begin{tabular}{p{13cm}l}\n\t\t\\toprule\n\t\t\\textbf{Passage:}\\quad In meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls under \\textit{\\textbf{gravity}}. The main forms of precipitation include drizzle, rain, sleet, snow, graupel and hail... Precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud. Short, in- tense periods of rain in scattered locations are called \"showers\".\\\\\n\t\t\\midrule\n\t\t\\textbf{Question:}\\quad What causes precipitation to fall?\\\\\n\t\t\\midrule\n\t\t\\textbf{Answer:} \\quad \\textbf{gravity}\\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t\\caption{An example of textual natural question.}\n\t\\label{figure: textual_natural }\t\n\\end{figure}\n\\begin{itemize}[leftmargin=*,labelsep=4.9mm]\n\t\\item\tSynthetic style \n\\end{itemize}\nThe synthetic form of the question is just a list of words and do not necessarily conform to normal grammatical rules. Common datasets with synthetic form questions are Qangaroo, WikiReading, and so on. Take Qangaroo as an example, in the Qangaroo dataset, the question is replaced by a collection of attribute words. The 'question' here is not a complete sentence that fully conforms to the natural language grammar, but a combination of words. The synthetic form of the question can be defined as: \n\\begin{table}[H]\n\t\\centering\n\t\\begin{tabular}{p{13cm}l}\n\t\t\\textbf{Definition 6.} In a MRC task, given a 'Synthetic style' question $Q$, it could be formulated as  $Q_{i}=\\big\\{q_{0}, q_{1}, \\ldots q_{j} \\ldots, q_{l q}\\big\\}$ , where $q_{i} \\in V \\cup M(0 \\leq i \\leq l q)$. $Q$ denotes a series of words (may also contain images) that do not conforms to the natural language grammar and $l_{q}$ denotes the length of the $Q$.\n\t\t\\\\\n\t\\end{tabular}\n\\end{table}\nThe example of synthetic style question is in shown in the following:\n\\begin{figure}[H]\n\t\\centering\n\t\\begin{tabular}{p{13cm}l}\n\t\t\\toprule\n\t\t\\textbf{Passage:}\n\t\tThe hanging Gardens, in [Mumbai], also known as Pherozeshah Mehta Gardens, are terraced gardens … They provide sunset views over the [Arabian Sea] …\\\\\n\t\tMumbai (also known as Bombay, the ofﬁcial name until 1995) is the capital city of the Indian state of Maharashtra. It is the most populous city in \\textbf{\\textit{India}} …\\\\\n\t\tThe Arabian Sea is a region of the northern Indian Ocean bounded on the north by Pakistan and Iran, on the west by northeastern Somalia and the Arabian Peninsula, and on the east by India …\\\\\n\t\t\\midrule\n\t\t\\textbf{Synthetic Question:}\\quad (Hanging gardens of Mumbai, country, ?) \\\\\n\t\t\\midrule\n\t\t\\textbf{Choices: } \\quad (A)Iran, \\quad (B)\\textbf{India$\\surd$}, \\quad (C)Pakistan, \\quad (D) Somalia\\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t\\caption{An example of synthetic style question. The passage and question are taken from the Qangaroo  dataset.} \n    \\label{figure: synthetic }\n\\end{figure}", "cites": [6921, 1143], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic classification of MRC tasks based on the type of questions and includes definitions and examples, primarily from datasets. However, it synthesizes little beyond what is explicitly presented in the cited papers, lacks critical evaluation of their approaches, and offers minimal abstraction or meta-level insights into broader trends or principles in MRC question design."}}
{"id": "d18da9b5-f28d-4c4e-a9df-0feaeedfd91b", "title": "Source of Answers", "level": "subsubsection", "subsections": [], "parent_id": "f91b4d08-6d49-4fd2-b246-9f9620414b17", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Tasks"], ["subsection", "Definition of each category in the new classification method"], ["subsubsection", "Source of Answers"]], "content": "According to different sources of answers, we divide the MRC tasks into two categories: span and free-form.\n\\begin{itemize}[leftmargin=*,labelsep=4.9mm]\n\t\\item\tSpan answer\n\\end{itemize}\nIn a MRC task, when the source of answer is 'Spans', it means that the answers come from context and are spans of context, and it can be defined as:\n\\begin{table}[H]\n\t\\centering\n\t\\begin{tabular}{p{13cm}l}\n\t\t\\textbf{Definition 9.} Given the context $C=\\big\\{c_{0}, \\ldots c_{k} \\ldots, c_{l}\\big\\}$, where $l$ denotes the length of the context. $c_{k} \\in V \\cup M(0 \\leq k \\leq l)$. The 'Span' answer $A$ could be formulated as $A=\\big\\{c_{m}, \\ldots, c_{n}\\big\\} (0\\leq m \\leq n \\leq l)$.  \n\t\\end{tabular}\n\\end{table}\nThe example of textual span answer is shown in Figure \\ref{figure: textual_cloze } above. It should be noted that, in this paper, we do not provide example for multi-modal span answers, because such tasks already exist in the field of computer vision, such as semantic segmentation, object detection, or instance segmentation.\n\\begin{itemize}[leftmargin=*,labelsep=4.9mm]\t\n\t\\item\tFree-form answer\n\\end{itemize}\nA free-form answer may be any phrase, word, or even image (not necessarily from the context). In a MRC task, when the source of answer is 'Free-form', it means that the answers can be any free-text or images, and there is no limit to where the answer comes from. It could be defined as follows:\n\\begin{table}[H]\n\t\\centering\n\t\\begin{tabular}{p{13cm}l}\n\t\t\\textbf{Definition 10.} Given the context $C$, the 'Free-form' answer $A$ may or may not come from context $C$, that is, either $A \\subseteq C$ or not. The 'Free-form' answer $A$ could be formulated as $A=\\big\\{w_{0}, w_{1}, \\ldots, w_{l-1}, w_{l}\\big\\}$ where $l$ denotes the length of the context. $w_{k} \\in V \\cup M(0 \\leq k \\leq l)$.\n\t\t\\\\\n\t\\end{tabular}\n\\end{table}\nExample of multi-modal free-form answer are shown in Figure \\ref{figure: mm_cloze } and example of textual free-form answer are shown in Figure \\ref{figure:free_form_answer_question} below:\n\\begin{figure}[H]\n\t\\centering\n\t\\begin{tabular}{p{13cm}l}\n\t\t\\toprule\n\t\t\\textbf{Passage:}\n\t\t That year, his Untitled (1981), a painting of a haloed, black-headed man with a bright red skeletal body, depicted amid the artist’s signature scrawls, was sold by Robert Lehrman for \\$16.3 million, well above its \\$12 million high estimate.\\\\\n\t\t\\midrule\n\t\t\\textbf{Question:}\\quad How many more dollars was the Untitled (1981) painting sold for than the 12 million dollar estimation? \\\\\n\t\t\\midrule\n\t\t\\textbf{Answer: } \\quad 4300000\\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t\\caption{An example of textual free-form answer. The question-answer pair and passage are taken from the DROP  dataset.}\n\t\\label{figure:free_form_answer_question}\n\\end{figure}", "cites": [1142], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section describes two categories of MRC tasks based on the source of answers but offers minimal synthesis beyond a basic categorization. It cites the DROP dataset only to provide an example of a textual free-form answer, without connecting this to broader trends or evaluating different approaches. There is no significant critical analysis or abstraction into overarching principles, making the insight quality low."}}
{"id": "d9e39d8c-5d1f-4301-8841-f35bae76d5b0", "title": "Statistics of MRC Tasks", "level": "subsection", "subsections": ["7cc358b4-2c57-423e-b1eb-b10190a1db0c"], "parent_id": "10776ca3-8a89-4127-82ed-8bd8dbb2b4e0", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Tasks"], ["subsection", "Statistics of MRC Tasks"]], "content": "In this section, we collected 57 different MRC tasks and made a statistical chart of MRC task classification according to four attributes, as shown in Figure \\ref{figure: piechart_tasks }. We can see that for the type of corpus, the textual task still accounts for a large proportion which is 89.47\\%. At present, the proportion of multi-modal reading comprehension tasks is still small, about 10.53\\%, which shows that the field of multi-modal reading comprehension still has many challenge problems for future research. In terms of question types, the most common type is the natural form of questions, followed by cloze type and synthetic type. In terms of answer types, the proportion of natural type and multiple-choice type are 52.63\\% and 47.37\\% respectively. In terms of answer source, 29.82\\% of the answers are of spans type, and 70.18\\% of the answers are of free-form.\n\\begin{figure}[H]\n\t\\centering\n\t\\includegraphics[width=15 cm]{Graphs/Figure_TypesofTask}\n\t\\caption{A pie chart on the proportion of different types of machine reading comprehension tasks: (a) Type of corpus. (b) Type of questions. (c) Type of answers. (d) Source of answers.\n\t} \n\t\\label{figure: piechart_tasks }   \n\\end{figure}  \nAs shown in Table \\ref{table: Statistics_of_Tasks }. The tasks in the table are ordered by the year the dataset was published. It should be noted that note that the names of many specific MRC tasks are often the same as the names of the datasets they may utilize. And the name of a certain category of MRC task and the name of a specific MRC task are two different concepts. For example, the RecipeQA  dataset contains two different tasks which are RecipeQA-Coherence and RecipeQA-Cloze.\n\\begin{center}\n    \\renewcommand\\arraystretch{1.2} \n\t\\topcaption{ Different machine reading comprehension tasks.}\n\t\\label{table: Statistics_of_Tasks }\n\t\\tablehead{\n\t\t      \\toprule{\\textbf{Year}} & {\\textbf{MRC Tasks}}& {\\textbf{Corpus Type}} & {\\textbf{Question Type}} & {\\textbf{Answer Source}} & {\\textbf{Answer Type}}\\\\\n\t         }\n\t\\tabletail{\t\n\t\t\\hline\n\t\t\\multicolumn{6}{r}{\\small\\sl continued on next page}\\\\\n\t\t\\hline \n\t\t}\n\t\\tablefirsthead{\\toprule{\\textbf{Year}} & {\\textbf{MRC Tasks}}& {\\textbf{Corpus Type}} & {\\textbf{Question Type}} & {\\textbf{Answer Source}} & {\\textbf{Answer Type}}\\\\ \\midrule}\n\t\\tablelasttail{\\bottomrule}\n\t\\begin{supertabular}{p{1cm}l p{2cm}l p{1.5cm}l p{1.5cm}l p{1.5cm}l p{1.5cm}l}\t\t\n{2013}          & {MCTest }                               & {Textual}              & {Natural}                & {Free-Form}              & {Multi-choice}    \\\\\n\\hline  {2015}          & {CNN/Daily Mail }                       & {Textual}              & {Cloze}                  & {Spans}                  & {Natural}              \\\\\n\\hline  {2015}          & {CuratedTREC }                          & {Textual}              & {Natural}                & {Free-Form}              & {Natural}              \\\\\n\\hline  {2015}          & {WikiQA }                               & {Textual}              & {Natural}                & {Free-Form}              & {Natural}              \\\\\n\\hline  {2016}          & {WikiMovies }                           & {Textual}              & {Natural}                & {Free-Form}              & {Natural}              \\\\\n\\hline  {2016}          & {SQuAD 1.1 }                             & {Textual}              & {Natural}                & {Spans}                  & {Natural}              \\\\\n\\hline  {2016}          & {Who-did-What }                         & {Textual}              & {Cloze}                  & {Spans}                  & {Natural}              \\\\\n\\hline  {2016}          & {MS MARCO }                             & {Textual}              & {Natural}                & {Free-Form}              & {Natural}              \\\\\n\\hline  {2016}          & {NewsQA }                               & {Textual}              & {Natural}                & {Spans}                  & {Natural}              \\\\\n\\hline  {2016}          & {LAMBADA }                              & {Textual}              & {Cloze}                  & {Free-Form}              & {Natural}              \\\\\n\\hline {2016}          & {WikiReading }                          & {Textual}              & {Synthetic}              & {Free-Form}              & {Natural}              \\\\\n\\hline {2016}          & {Facebook CBT }                         & {Textual}              & {Cloze}                  & {Free-Form}              & {Multi-choice}         \\\\\n\\hline {2016}          & {BookTest }                             & {Textual}              & {Cloze}                  & {Free-Form}              & {Multi-choice}         \\\\\n\\hline {2016}          & {Google MC-AFP }                        & {Textual}              & {Synthetic}              & {Free-Form}              & {Multi-choice}         \\\\\n\\hline {2016}          & {MovieQA }                              & {Multi-modal}          & {Natural}                & {Free-Form}              & {Multi-choice}         \\\\\n\\hline {2017}          & {TriviaQA-Web }                        & {Textual}              & {Natural}                & {Free-Form}              & {Natural}              \\\\\n\\hline {2017}          & {TriviaQA-Wiki }                      & {Textual}              & {Natural}                & {Free-Form}              & {Natural}              \\\\\n\\hline {2017}          & {RACE }                                 & {Textual}              & {Natural}                & {Free-Form}              & {Multi-choice}         \\\\\n\\hline {2017}          & {Quasar-S }                             & {Textual}              & {Cloze}                  & {Spans}                   & {Multi-choice}         \\\\\n\\hline {2017}          & {Quasar-T }                             & {Textual}              & {Natural}                & {Spans}                  & {Natural}              \\\\\n\\hline {2017}          & {SearchQA }                             & {Textual}              & {Natural}                & {Free-Form}              & {Natural}              \\\\\n\\hline {2017}          & {NarrativeQA }                          & {Textual}              & {Natural}                & {Free-Form}              & {Natural}              \\\\\n\\hline {2017}          & {SciQ }                                 & {Textual}              & {Natural}                & {Spans}                  & {Multi-choice}         \\\\\n\\hline {2017}          & {Qangaroo-MedHop }                      & {Textual}              & {Synthetic}              & {Spans}                   & {Multi-choice}         \\\\\n\\hline {2017}          & {Qangaroo-WikiHop }                     & {Textual}              & {Synthetic}              & {Spans}                   & {Multi-choice}         \\\\\n\\hline {2017}          & {TQA }                                  & {Multi-modal}          & {Natural}                & {Free-Form}              & {Multi-choice}         \\\\\n\\hline {2017}          & {COMICS-Coherence }                    & {Multi-modal}          & {Natural}                & {Free-Form}              & {Multi-choice}         \\\\\n\\hline {2017}          & {COMICS-Cloze }                        & {Multi-modal}          & {Cloze}                  & {Free-Form}              & {Multi-choice}         \\\\\n\\hline {2018}          & {QuAC }                                 & {Textual}              & {Natural}                & {Free-Form}              & {Natural}              \\\\\n\\hline {2018}          & {CoQA }                                 & {Textual}              & {Natural}                & {Free-Form}              & {Natural}              \\\\\n\\hline {2018}          & {SQuAD 2.0 }                             & {Textual}              & {Natural}                & {Spans}                  & {Natural}              \\\\\n\\hline {2018}          & {HotpotQA-Distractor }         & {Textual}              & {Natural}                & {Spans}                  & {Natural}              \\\\\n\\hline {2018}          & {HotpotQA-Fullwiki }           & {Textual}              & {Natural}                & {Spans}                  & {Natural}              \\\\\n\\hline {2018}          & {DuoRC-Self }                           & {Textual}              & {Natural}                & {Free-Form}              & {Natural}              \\\\\n\\hline {2018}          & {DuoRC-Paraphrase }                     & {Textual}              & {Natural}                & {Free-Form}              & {Natural}              \\\\\n\\hline {2018}          & {CLOTH }                                & {Textual}              & {Cloze}                  & {Free-Form}              & {Natural}              \\\\\n\\hline {2018}          & {ReCoRD }                               & {Textual}              & {Cloze}                  & {Free-Form}              & {Natural}              \\\\\n\\hline {2018}          & {CliCR }                                & {Textual}              & {Cloze}                  & {Free-Form}              & {Natural}              \\\\\n\\hline {2018}          & {ReviewQA }                             & {Textual}              & {Natural}                & {Spans}                   & {Multi-choice}         \\\\\n\\hline {2018}          & {ARC-Challenge Set }                    & {Textual}              & {Natural}                & {Free-Form}              & {Multi-choice}         \\\\\n\\hline {2018}          & {ARC-Easy Set }                         & {Textual}              & {Natural}                & {Free-Form}              & {Multi-choice}         \\\\\n\\hline {2018}          & {OpenBookQA }                           & {Textual}              & {Natural}                & {Free-Form}              & {Multi-choice}         \\\\\n\\hline {2018}          & {SciTail }                              & {Textual}              & {Natural}                & {Free-Form}              & {Multi-choice}         \\\\\n\\hline {2018}          & {MultiRC }                              & {Textual}              & {Natural}                & {Free-Form}              & {Multi-choice}         \\\\\n\\hline {2018}          & {RecipeQA-Cloze }                      & {Multi-modal}          & {Cloze}                  & {Free-Form}              & {Multi-choice}         \\\\\n\\hline {2018}          & {RecipeQA-Coherence }                  & {Multi-modal}          & {Natural}                & {Free-Form}              & {Multi-choice}         \\\\\n\\hline {2018}          & {PaperQA-Title }         & {Textual}              & {Cloze}                  & {Free-Form}              & {Multi-choice}         \\\\\n\\hline {2018}          & {PaperQA-Last } & {Textual}              & {Cloze}                  & {Free-Form}              & {Multi-choice}         \\\\\n\\hline {2018}          & {PaperQA(Hong et al.) }        & {Textual}              & {Natural}                & {Spans}                  & {Natural}              \\\\\n\\hline {2018}          & {MCScript }                             & {Textual}              & {Natural}                & {Free-Form}              & {Multi-choice}         \\\\\n\\hline {2018}          & {ProPara }                              & {Textual}              & {Natural}                & {Spans}                  & {Natural}              \\\\\n\\hline {2019}          & {Natural Questions-Short }       & {Textual}              & {Natural}                & {Spans}                  & {Natural}              \\\\\n\\hline {2019}          & {Natural Questions-Long }        & {Textual}              & {Natural}                & {Spans}                  & {Natural}              \\\\\n\\hline {2019}          & {DREAM }                                & {Textual}              & {Natural}                & {Free-Form}              & {Multi-choice}         \\\\\n\\hline {2019}          & {ShARC }                                & {Textual}              & {Natural}                & {Free-Form}              & {Multi-choice}         \\\\\n\\hline {2019}          & {CommonSenseQA }                        & {Textual}              & {Natural}                & {Free-Form}              & {Multi-choice}         \\\\\n\\hline {2019}          & {DROP }                                 & {Textual}              & {Natural}                & {Free-Form}              & {Natural}              \\\\\n\t\\end{supertabular}\n\\end{center}", "cites": [1143, 6921, 446, 1142, 6931, 439, 6929, 6930, 6923, 1176, 444, 1147, 1140, 1098, 8304, 6932, 6933, 6920, 441, 6922], "cite_extract_rate": 0.6842105263157895, "origin_cites_number": 38, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.3}, "insight_level": "low", "analysis": "The section provides a descriptive overview of MRC tasks and their attributes, citing relevant papers to support the classification. However, it lacks synthesis of ideas across the cited works and offers limited critical evaluation or abstraction. While it notes the prevalence of textual tasks and the novelty of multi-modal ones, it does not explore implications, limitations, or connections between different task types in depth."}}
{"id": "5cf1d123-484b-4cfe-9714-00ef1f1cd3ab", "title": "Token-level Precision", "level": "subsubsection", "subsections": [], "parent_id": "b954734c-fbc0-4657-a734-8eda1b51c16c", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Evaluation Metrics"], ["subsection", "Precision"], ["subsubsection", "Token-level Precision"]], "content": "The token-level precision represents the percentage of token overlap between the tokens in the correct answer and the tokens in the predicted answer. Following the evaluation method in SQuAD , we treat the predicted answer and correct answer as bags of tokens, while ignoring all punctuation marks and the article words such as \"a\" and \"an\" or \"the\". In order to get the token-level Precision, we first need to understand the token-level true positive (TP), false positive (FP), true negative (TN), and false negative (FN), as shown in Figure \\ref{figure: token_precison }:\n \\begin{figure}[H]\n \t\\centering\n \t\\includegraphics[width=10cm]{Graphs/Figure_Token-level}\n \t\\caption{The token-level true positive (TP), false positive (FP), true negative (TN), and false negative (FN).}\n \t\\label{figure: token_precison }\n \\end{figure}   \n As seen in Figure \\ref{figure: token_precison }, for a single question, the token-level true positive (TP) denotes the same tokens between the predicted answer and the correct answer. The token-level false positive (FP) denotes the tokens which are not in the correct answer but the predicted answer, while the false negative (FN) denotes the tokens which are not in the predicted answer but the correct answer. A token-level Precision for a single question is computed as follows:\n\\begin{equation} {Precision}_{T S}=\\frac{N u m\\big(T P_{T}\\big)}{{Num}\\big(T P_{T}\\big)+{Num}\\big(F P_{T}\\big)}\\end{equation}\nWhere ${Precision}_{T S}$ denotes the token-level Precision for a single question, and $Num\\big(TP_{T}\\big)$ denotes the number of token-level true positive (TP) tokens and  $Num\\big(FP_{T}\\big)$ denotes the number of token-level false positive (FP) tokens. \\\\\nFor example, if a correct answer is \"a cat in the garden\" and the predicted answer is \"a dog in the garden\". We can see, after ignoring the article word \"a\" and \"the\", the number of the shared tokens between the predicted answer and the correct answer is 2, which is also the $Num\\big(TP_{T}\\big)$ , and $Num\\big(FP_{T}\\big)$ is 1, so the token-level Precision for this answer is 2/3.", "cites": [439, 6931], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section describes token-level precision primarily using the methodology from the SQuAD paper and provides a formula and example for its computation. It lacks critical analysis of the method's strengths or limitations and does not connect or compare it with other evaluation approaches from the DREAM paper. The content is factual and concrete without generalizing to broader patterns or principles."}}
{"id": "bfca2576-cc78-4232-b0dd-3b79a21d897a", "title": "Meteor", "level": "subsection", "subsections": [], "parent_id": "aaa41c2c-7e4c-47de-b829-bf1cfc596803", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Evaluation Metrics"], ["subsection", "Meteor"]], "content": "Meteor was first proposed by Banerjee and Lavie  in order to evaluate the machine translation system. Unlike the BLEU using only Precision, the Meteor indicator uses a combination of Recall and Accuracy to evaluate the system. In addition, Meteors also include features such as synonym matching. \\\\\nBesides Meteor, Denkowski and Lavie also proposed Meteor-next  and Meteor 1.3 , the new metric features include improved text normalization, higher-precision paraphrase matching, and discrimination between content and function words. Currently, some MRC datasets use Meteor as one of their evaluation metrics, such as the NarrativeQA  dataset. The Meteor score for the given alignment is computed as follows:\n\\begin{equation}{Meteor}=F_{ {mean}} \\times\\big(1- {Penalty}\\big)\\end{equation}\nWhere $F_{mean}$ is combined by the $Precision$ and $Recall$ via a harmonic-mean  that places most of the weight on $Recall$, and the formula of $F_{mean}$ is:\n\\begin{equation}F_{mean}=\\frac{{Precision} \\times {Recall}}{\\alpha \\times {Precision}+(1-\\alpha) \\times {Recall}}\\end{equation}\nAnd $Penalty$ is a fragmentation penalty to account for differences and gaps in word order, which is calculated using the total number of matched words ($m$, average over hypothesis and reference) and number of chunks ($ch$): \n\\begin{equation}{ Penalty }=\\gamma \\times\\left(\\frac{c h}{m}\\right)^{\\beta}\\end{equation}\nWhere the parameters $\\alpha$, $\\beta$, and $\\gamma$ are tuned to maximize correlation with human judgments .\nIt should be noted that the $Precision$ and $Recall$ in Meteor 1.3 is improved by text normalization, we can see the original paper of Denkowski and Lavie for the detailed calculation method of $Precision$ and $Recall$ in Meteor 1.3 .", "cites": [441], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the Meteor metric, its components, and some of its variants, but does not deeply synthesize or integrate information from the cited papers. It lacks critical evaluation of the strengths and weaknesses of Meteor in the context of MRC, and does not abstract broader principles or patterns in evaluation metric design."}}
{"id": "c91e5568-912a-40c4-ad82-b9654f1fad03", "title": "Statistics of Evaluation Metrics", "level": "subsection", "subsections": [], "parent_id": "aaa41c2c-7e4c-47de-b829-bf1cfc596803", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Evaluation Metrics"], ["subsection", "Statistics of Evaluation Metrics"]], "content": "In this section, we collated the evaluation metrics of 57 MRC tasks. As seen in Table \\ref{table: metrics }, the typical MRC dataset evaluation metrics are Accuracy, Exact Match, F1 score, ROUGE, BLEU, HEQ, and Meteor. Many datasets use more than one evaluation metric. Moreover, some datasets adopt detailed evaluation metrics according to their own characteristics. For example, the HotpotQA  dataset adopts evaluation metrics such as Exact Match of Supportings, F1 of Supportings, Exact Match of Answer, F1 of Answer, etc. And the Facebook CBT  dataset adopts Accuracy on Named Entities, Accuracy on Common Nouns, Accuracy on Verbs, Accuracy on Prepositions.\n\\begin{center}\n\t\\renewcommand\\arraystretch{1.1} \n\t\\topcaption{Evaluation metrics of different machine reading comprehension tasks.}\n\t\\label{table: metrics }\n\t\\tablehead{\n\t\t\\toprule{\\textbf{Year}} & {\\textbf{MRC Tasks}}& {\\textbf{Metric 1}} & {\\textbf{Metric 2}} & {\\textbf{Metric 3}} & {\\textbf{Metric 4}}\\\\\n\t}\n\t\\tabletail{\t\n\t\t\\hline\n\t\t\\multicolumn{6}{r}{\\small\\sl continued on next page}\\\\\n\t\t\\hline \n\t}\n\t\\tablefirsthead{\n\t\t\\toprule{\\textbf{Year}} & {\\textbf{MRC Tasks}}& {\\textbf{Metric 1}} & {\\textbf{Metric 2}} & {\\textbf{Metric 3}} & {\\textbf{Metric 4}}\\\\ \\midrule}\n\t\\tablelasttail{\\bottomrule}\n\t\\begin{supertabular}{p{0.8cm} p{3cm} p{2cm} p{2cm} p{2cm}  p{2cm}}\t\n     {2013} & {MCTest } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2015} & {CNN/Daily Mail } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2015} & {CuratedTREC } & {Exact Match} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2015} & {WikiQA } & {Question-level Precision} & {Question-level Recall} & {Question-level F1} & {N/A} \\\\\n\t\\hline {2016} & {BookTest } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2016} & {Facebook CBT } & {Accuracy on Named Entities} & {Accuracy on Common Nouns} & {Accuracy on Verbs} & {Accuracy on Prepositions} \\\\\n\t\\hline {2016} & {Google MC-AFP } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2016} & {LAMBADA } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2016} & {MovieQA } & {Accuracy of Video Clips} & {Accuracy of Plots and Subtitles} & {N/A} & {N/A} \\\\\n\t\\hline {2016} & {MS MARCO } & {Rouge-L} & {BLEU-1} & {N/A} & {N/A} \\\\\n\t\\hline {2016} & {NewsQA } & {Exact Match} & {Token-level F1} & {N/A} & {N/A} \\\\\n\t\\hline {2016} & {SQuAD 1.1 } & {Token-level F1} & {Exact Match} & {N/A} & {N/A} \\\\\n\t\\hline {2016} & {Who-did-What } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2016} & {WikiMovies } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2016} & {WikiReading } & {Question level F1} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2017} & {COMICS-Cl } & {Accuracy of Text Cloze} & {Accuracy of Visual Cloze} & {N/A} & {N/A} \\\\\n\t\\hline {2017} & {COMICS-Co } & {Accuracy of Coherence} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2017} & {NarrativeQA } & {ROUGE-L} & {BLEU-1} & {BLEU-4} & {Meteor} \\\\\n\t\\hline {2017} & {Qangaroo-M } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2017} & {Qangaroo-W } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2017} & {Quasar-S } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2017} & {Quasar-T } & {Exact Match} & {Token-level F1} & {N/A} & {N/A} \\\\\n\t\\hline {2017} & {RACE } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2017} & {SciQ } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2017} & {SearchQA } & {F1 score (for n-gram)} & {Accuracy} & {N/A} & {N/A} \\\\\n\t\\hline {2017} & {TQA } & {Accuracy of All} & {Accuracy of Diagram} & {N/A} & {N/A} \\\\\n\t\\hline {2017} & {TriviaQA-Wiki } & {Exact Match} & {Question-level F1} & {Verified-EM} & {Verified-F1} \\\\\n\t\\hline {2017} & {TriviaQA-Web } & {Exact Match} & {Document-level F1} & {Verified-EM} & {Verified-F1} \\\\\n\t\\hline {2018} & {ARC-C } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2018} & {ARC-E } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2018} & {CliCR } & {Exact Match} & {Token-level F1} & {BLEU-2} & {BLEU-4} \\\\\n\t\\hline {2018} & {CLOTH } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2018} & {CoQA } & {Token-level F1} & {F1 out of domain} & {F1 in domain} & {N/A} \\\\\n\t\\hline {2018} & {DuoRC-P } & {Accuracy} & {Token-level F1} & {N/A} & {N/A} \\\\\n\t\\hline {2018} & {DuoRC-S } & {Accuracy} & {Token-level F1} & {N/A} & {N/A} \\\\\n\t\\hline {2018} & {HotpotQA-D } & {EM of Answer} & {F1 of Answer (Token-level)} & {EM of Supportings} & {F1 of Supportings} \\\\\n\t\\hline {2018} & {HotpotQA-F } & {EM of Answer} & {F1 of Answer (Token-level)} & {EM of Supportings} & {F1 of Supportings} \\\\\n\t\\hline {2018} & {MCScript } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2018} & {MultiRC } & {F1m} & {Exact Match} & {N/A} & {N/A} \\\\\n\t\\hline {2018} & {OpenBookQA } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2018} & {PaperQA(Hong et al.) } & {F1} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2018} & {PaperQA-LS } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2018} & {PaperQA-T } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2018} & {ProPara } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2018} & {QuAC } & {Token-level F1} & {HEQ-Q} & {HEQ-D} & {N/A} \\\\\n\t\\hline {2018} & {RecipeQA-Cl } & {Accuracy of Textual Cloze} & {Accuracy of Visual Cloze} & {N/A} & {N/A} \\\\\n\t\\hline {2018} & {RecipeQA-Co } & {Accuracy-VO} & {Accuracy-VC} & {N/A} & {N/A} \\\\\n\t\\hline {2018} & {ReCoRD } & {Exact Match} & {Token-level F1} & {N/A} & {N/A} \\\\\n\t\\hline {2018} & {ReviewQA } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2018} & {SciTail } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2018} & {SQuAD 2.0 } & {Token-level F1} & {EM} & {N/A} & {N/A} \\\\\n\t\\hline {2019} & {CommonSenseQA } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2019} & {DREAM } & {Accuracy} & {N/A} & {N/A} & {N/A} \\\\\n\t\\hline {2019} & {DROP } & {EM} & {Token-level F1} & {N/A} & {N/A} \\\\\n\t\\hline {2019} & {Natural Questions-Long } & {Precision} & {Recall} & {N/A} & {N/A} \\\\\n\t\\hline {2019} & {Natural Questions-Short } & {Precision} & {Recall} & {F1} & {N/A} \\\\\n\t\\hline {2019} & {ShARC } & {Micro Accuracy} & {Macro Accuracy} & {BLEU-1} & {BLEU-4} \\\\ \n\t\\end{supertabular}\n\\end{center}\nTable \\ref{table: metrics_usage } shows the statistics on the usage of different evaluation metrics in the 57 MRC tasks collected in this paper. Among them, Accuracy is the most widely used evaluation metric, and 61.40\\% of MRC tasks collected in this paper used it. It is followed by F1 (36.84\\%) and Exact Match (22.81\\%). The rest of these evaluation metrics are less used, as shown in Table \\ref{table: metrics_usage }:\n\\begin{table}[H]\n\t\\caption{Statistics on the usage of different evaluation metrics in 57 machine reading comprehension tasks.}\n\t\\centering\n\t\\label{table: metrics_usage }\n\t\\begin{tabular}{p{1.1cm}| p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm}  p{1.1cm}  p{1.1cm} p{1.1cm} p{1.1cm}}\n\t\t\\toprule\n\t    Metrics & Accuracy & F1 & EM & BLEU & Recall & Precision & ROUGE-L & HEQ-D & Meteor \\\\ \n\t\t\\midrule\n\t\tUsage & 61.40\\% & 36.84\\% & 22.81\\% & 7.02\\% & 5.26\\% & 5.26\\% & 3.51\\% & 1.75\\% & 1.75\\% \\\\ \n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table}\nWe also analyzed the relationship between the evaluation metrics and task types. Figure \\ref{figure: metrics_task_usage } shows the usage of evaluation metrics with different types of tasks. Taking the \"Accuracy\" in Figure \\ref{figure: metrics_task_usage } (b) as an example, a total of 35 MRC tasks use the \"Accuracy\" as the evaluation metric. Among them, 25 tasks have the \"Multi-choice\" type of answers, and the remaining 10 tasks have the \"Natural\" type of answers. It can be seen from Figure \\ref{figure: metrics_task_usage } (b) that tasks with the \"Multi-choice\" type of answers prefer to use the \"Accuracy\" evaluation metric rather than other evaluation metrics. This is because it is impossible to calculate the EM, Precision, BLEU or F1 score of a typical \"Multi-choice\" question which has only one correct answer in the candidates. Among the \"Multi-choice\" tasks we collected, only the MultiRC  task does not use Accuracy, but F1 and Exact Match as the evaluation metric. That is because there are multiple correct answers in the candidates of the MultiRC task. As can be seen from Figure \\ref{figure: metrics_task_usage } (a), tasks with \"Cloze\" questions prefer to use the \"Accuracy\" as evaluation metrics rather than other evaluation metrics, which is because \"Cloze\" tasks tend to have \"Multi-choice\" answers. From Figure \\ref{figure: metrics_task_usage } (c), we can see that tasks with \"Spans\" answers and tasks with \"Free-form\" answers have no special preference in selecting evaluation metrics.\n\\begin{figure}[H]\n\t\\centering\n\t\\includegraphics[width=15cm]{Graphs/Figure12}\n\t\\caption{The usage of evaluation metrics with different types of tasks. Different colors represent different types of tasks. }\n\t\\label{figure: metrics_task_usage }\n\\end{figure}", "cites": [1143, 6921, 446, 1142, 6931, 439, 6929, 6923, 6930, 444, 1176, 1147, 1140, 1098, 8304, 6932, 6933, 6920, 441, 6922], "cite_extract_rate": 0.6842105263157895, "origin_cites_number": 38, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes evaluation metrics from multiple MRC datasets and connects them to task types, such as the preference for Accuracy in multi-choice tasks. It also identifies broader patterns, like the widespread use of Accuracy and F1. However, it lacks deeper critical evaluation of the metrics' strengths and weaknesses, and the analysis stops short of proposing a novel framework or meta-level insights."}}
{"id": "8dcaa470-e1fd-4020-99f1-deaa44b6bc62", "title": "The Size of Datasets", "level": "subsection", "subsections": [], "parent_id": "2aa98efb-e563-422a-a721-633a77bc6fa9", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "The Size of Datasets"]], "content": "The recent success of machine reading comprehension is driven largely by both large-scale datasets and neural models . The size of a dataset affects the generalization ability of the MRC model and determines whether the model is useful in the real world. Early MRC datasets tend to of small sizes. With the continuous development of MRC datasets in recent years, the question set sizes of newly created MRC datasets are generally more than 10K. Here, we have counted the total number of questions in each MRC dataset along with the sizes of its training set, development set, and testing set, as well as the proportion of training set to the total number of questions. The data is shown in Table \\ref{table: question_size } which is sorted by the question set size of the datasets. \n\\begin{center}\n\t\\renewcommand\\arraystretch{1.2} \n\t\\topcaption{The question set size of machine reading comprehension datasets.}\n\t\\label{table: question_size }\n\t\\tablehead{\n\t\t\\toprule{\\textbf{Year}} & {\\textbf{Datasets}}& {\\textbf{Question size}} & {\\textbf{\\#Training questions}} & {\\textbf{\\#Dev questions}} & {\\textbf{\\#Test  questions}}     & {\\textbf{Percentage  of Training set}}  \\\\\n\t}\n\t\\tabletail{\t\n\t\t\\hline\n\t\t\\multicolumn{7}{r}{\\small\\sl continued on next page}\\\\\n\t\t\\hline \n\t}\n\t\\tablefirsthead{\n\t\t\\toprule{\\textbf{Year}} & {\\textbf{Datasets}}& {\\textbf{Question size}} & {\\textbf{\\#Training questions}} & {\\textbf{\\#Dev questions}} & {\\textbf{\\#Test  questions}}     & {\\textbf{Percentage  of Training set}}  \\\\\n\t\t\\midrule}\n\t\\tablelasttail{\\bottomrule}\n\t\\begin{supertabular}{p{0.8cm} p{2.4cm} p{1.5cm} p{1.5cm} p{1.5cm}  p{1.5cm} p{2cm}}\t\n\t\t{2016}          & {WikiReading }                          & {18.87M}                   & {16.03M}                          & {1.89M}                      & {0.95M}                       & {84.95\\%}                      \\\\\n\t\t\\hline {2016}          & {BookTest }                             & {14,160,825}               & {14,140,825}                      & {10,000}                     & {10,000}                      & {99.86\\%}                      \\\\\n\t\t\\hline {2016}          & {Google MC-AFP }                        & {1,742,618}                & {1,727,423}                       & {7,602}                      & {7,593}                       & {99.13\\%}                      \\\\\n\t\t\\hline {2015}          & {Daily Mail}                           & {997,467}                  & {879,450}                         & {64,835}                     & {53,182}                      & {88.17\\%}                      \\\\\n\t\t\\hline {2016}          & {Facebook CBT }                         & {687K}                     & {669,343}                         & {8,000}                      & {10,000}                      & {97.38\\%}                      \\\\\n\t\t\\hline {2018}          & {ReviewQA }                             & {587,492}                  & {528,665}                         & {N/A}                        & {58,827}                      & {89.99\\%}                      \\\\\n\t\t\\hline {2015}          & {CNN }                                  & {387,420}                  & {380,298}                         & {3,924}                      & {3,198}                       & {98.16\\%}                      \\\\\n\t\t\\hline {2019}          & {Natural Questions }                  & {323,045}                  & {307,373}                         & {7,830}                      & {7,842}                       & {95.15\\%}                      \\\\\n\t\t\\hline {2016}          & {Who-did-What }                         & {147,786}                  & {127,786}                         & {10,000}                     & {10,000}                      & {86.47\\%}                      \\\\\n\t\t\\hline {2018}          & {SQuAD 2.0 }                             & {151,054}                  & {130,319}                         & {11,873}                     & {8,862}                       & {86.27\\%}                      \\\\\n\t\t\\hline {2017}          & {SearchQA }                             & {140,461}                  & {99,820}                          & {13,393}                     & {27,248}                      & {71.07\\%}                      \\\\\n\t\t\\hline {2018}          & {CoQA }                                 & {127K}                     & {110K}                            & {7K}                         & {10K}                         & {86.61\\%}                      \\\\\n\t\t\\hline {2018}          & {ReCoRD }                               & {120,730}                  & {100,730}                         & {10,000}                     & {10,000}                      & {83.43\\%}                      \\\\\n\t\t\\hline {2016}          & {NewsQA }                               & {119K}                     & {107K}                            & {6K}                         & {6K}                          & {89.92\\%}                      \\\\\n\t\t\\hline {2018}          & {HotpotQA }                          & {105,374}                  & {90,564}                          & {7,405}                      & {7,405}                       & {85.95\\%}                      \\\\\n\t\t\\hline {2018}          & {CliCR }                                & {104,919}                  & {91,344}                          & {6,391}                      & {7,184}                       & {87.06\\%}                      \\\\\n\t\t\\hline {2018}          & {DuoRC-P }                     & {100,316}                  & {70K}                             & {15K}                        & {15K}                         & {70.00\\%}                      \\\\\n\t\t\\hline {2016}          & {SQuAD 1.1 }                             & {107,702}                  & {87,599}                          & {10,570}                     & {9,533}                       & {81.33\\%}                      \\\\\n\t\t\\hline {2016}          & {WikiMovies }                           & {116K}                     & {96K}                             & {10K}                        & {10K}                         & {82.76\\%}                      \\\\\n\t\t\\hline {2018}          & {CLOTH }                                & {99,433}                   & {76,850}                          & {11,067}                     & {11,516}                      & {77.29\\%}                      \\\\\n\t\t\\hline {2018}          & {QuAC }                                 & {98,275}                   & {83,568}                          & {7,354}                      & {7,353}                       & {85.03\\%}                      \\\\\n\t\t\\hline {2017}          & {RACE }                                 & {97,687}                   & {87,866}                          & {4,887}                      & {4,934}                       & {89.95\\%}                      \\\\\n\t\t\\hline {2019}          & {DROP }                                 & {96,567}                   & {77,409}                          & {9,536}                      & {9,622}                       & {80.16\\%}                      \\\\\n\t\t\\hline {2017}          & {TriviaQA-Web }                        & {95,956}                   & {76,496}                          & {9,951}                      & {9,509}                       & {79.72\\%}                      \\\\\n\t\t\\hline {2018}          & {PaperQA-T }         & {84,803}                   & {77,298}                          & {3,752}                      & {3,753}                       & {91.15\\%}                      \\\\\n\t\t\\hline {2018}          & {DuoRC-S }                           & {84K}                   & {60K}                             & {12K}                        & {12K}                         & {70.00\\%}                      \\\\\n\t\t\\hline {2018}          & {PaperQA-L } & {80,118}                   & {71,804}                          & {4,179}                      & {4,135}                       & {89.62\\%}                      \\\\\n\t\t\\hline {2017}          & {TriviaQA-Wiki }                      & {77,582}                   & {61,888}                          & {7,993}                      & {7,701}                       & {79.77\\%}                      \\\\\n\t\t\\hline {2017}          & {Qangaroo-W }                     & {51,318}                   & {43,738}                          & {5,129}                      & {2,451}                       & {85.23\\%}                      \\\\\n\t\t\\hline {2017}          & {NarrativeQA }                          & {46,765}                   & {32,747}                          & {3,461}                      & {10,557}                      & {70.02\\%}                      \\\\\n\t\t\\hline {2017}          & {Quasar-T }                             & {43,013}                   & {37,012}                          & {3,000}                      & {3,000}                       & {86.05\\%}                      \\\\\n\t\t\\hline {2017}          & {Quasar-S }                             & {37,362}                   & {31,049}                          & {3,174}                      & {3,139}                       & {83.10\\%}                      \\\\\n\t\t\\hline {2018}          & {RecipeQA }                             & {36K}                      & {29,657}                          & {3,562}                      & {3,567}                       & {80.62\\%}                      \\\\\n\t\t\\hline {2017}          & {TQA }                                  & {26,260}                   & {15,154}                          & {5,309}                      & {5,797}                       & {57.71\\%}                      \\\\\n\t\t\\hline {2016}          & {MovieQA }                              & {21,406}                   & {14,166}                          & {2,844}                      & {4,396}                       & {66.18\\%}                      \\\\\n\t\t\\hline {2018}          & {MCScript }                             & {13,939}                   & {9,731}                           & {1,411}                      & {2,797}                       & {69.81\\%}                      \\\\\n\t\t\\hline {2017}          & {SciQ }                         & {13,679}                   & {11,679}                          & {1,000}                      & {1,000}                       & {85.38\\%}                      \\\\\n\t\t\\hline {2019}          & {CommonSenseQA }                        & {12,102}                   & {9741}                            & {1221}                       & {1140}                        & {80.49\\%}                      \\\\\n\t\t\\hline {2019}          & {DREAM }                                & {10,197}                   & {6,116}                           & {2,040}                      & {2,041}                       & {59.98\\%}                      \\\\\n\t\t\\hline {2018}          & {OpenBookQA }                           & {5,957}                    & {4,957}                           & {500}                        & {500}                         & {83.21\\%}                      \\\\\n\t\t\\hline {2018}          & {ARC-Easy Set }                         & {5,197}                    & {2,251}                           & {570}                        & {2,376}                       & {43.31\\%}                      \\\\\n\t\t\\hline {2015}          & {WikiQA }                               & {3,047}                    & {2,118}                           & {296}                        & {633}                         & {69.51\\%}                      \\\\\n\t\t\\hline {2018}          & {ARC-Challenge Set }                    & {2,590}                    & {1,119}                           & {299}                        & {1,172}                       & {43.20\\%}                      \\\\\n\t\t\\hline {2017}          & {Qangaroo-M }                      & {2,508}                    & {1,620}                           & {342}                        & {546}                         & {64.59\\%}                      \\\\\n\t\t\\hline {2013}          & {MCTest-mc500 }                         & {2,000}                    & {1,200}                           & {200}                        & {600}                         & {60.00\\%}                      \\\\\n\t\t\\hline {2018}          & {SciTail }                              & {1,834}                    & {1,542}                           & {121}                        & {171}                         & {84.08\\%}                      \\\\\n\t\t\\hline {2019}          & {ShARC }                                & {948}                      & {628}                             & {69}                         & {251}                         & {66.24\\%}                      \\\\\n\t\t\\hline {2013}          & {MCTest-mc160 }                         & {640}                      & {280}                             & {120}                        & {240}                         & {43.75\\%}                      \\\\\n\t\t\\hline {2018}          & {ProPara }                              & {488}                      & {391}                             & {54}                         & {43}                          & {80.12\\%}                      \\\\\n\t\\end{supertabular}\n\\end{center}\nWe also use the data in Table \\ref{table: question_size } to make a statistical chart where the Y coordinate is logarithmic, as shown in Figure \\ref{figure: question_size }, we can see that the WikiReading is the dataset with the largest question size  of a total of 18.87M questions; BookTest  is ranked second, and ProPara  is the smallest which has only 488 questions. When it comes to the proportion of training sets, BookTest has the highest proportion, 99.86\\%, while the ARC (challenge set) has the lowest proportion which is 43.20\\%. The development set is generally slightly smaller than the testing set.\n\\begin{figure}[H]\n\t\\centering\n\t\\includegraphics[width=15cm]{Graphs/Figure_DatasetSize}\n\t\\caption{The size of machine Reading Comprehension datasets: (\\textbf{a}) Total question size of each dataset. (\\textbf{b}) Percentages of training sets, development sets and test sets.}\n\t\\label{figure: question_size }\n\\end{figure}   \nBecause different MRC datasets contain different corpora, we also give details of the corpus used in each MRC dataset, including the size of corpus and the unit of corpus, as well as the size of training set, development set, and testing set. As seen in Table \\ref{table: corpus_size }, The units of corpus in MRC datasets are various, such as paragraphs, documents, etc.\n\\begin{center}\n\t\\renewcommand\\arraystretch{1.2} \n\t\\topcaption{The corpus size of machine reading comprehension datasets.}\n\t\\label{table: corpus_size }\n\t\\tablehead{\n\t\t\\toprule{\\textbf{Year}} &{\\textbf{Datasets}}   &{\\textbf{Corpus size}} &{\\textbf{\\#Train Corpus}} &{\\textbf{\\#Dev Corpus}} &{\\textbf{\\#Test Corpus}} &{\\textbf{Unit of Corpus}}\\\\\n\t}\n\t\\tabletail{\t\n\t\t\\hline\n\t\t\\multicolumn{7}{r}{\\small\\sl continued on next page}\\\\\n\t\t\\hline \n\t}\n\t\\tablefirsthead{\n\t\t\\toprule{\\textbf{Year}} &{\\textbf{Datasets}}   &{\\textbf{Corpus size}} &{\\textbf{\\#Train Corpus}} &{\\textbf{\\#Dev Corpus}} &{\\textbf{\\#Test Corpus}} &{\\textbf{Unit of Corpus}}\\\\\n\t\t\\midrule}\n\t\\tablelasttail{\\bottomrule}\n\t\\begin{supertabular}{p{0.8cm} p{2.4cm} p{1.5cm} p{1.5cm} p{1.5cm}  p{1.5cm} p{1.5cm}l}\t\t\t\n\t\t{2016}  &{WikiReading }    &{4.7M}   &{N/A}   &{N/A}   &{N/A}   &{Article}  \\\\\n\t\t\\hline{2016}  &{SQuAD 1.1 }    &{536}   &{442}   &{48}   &{46}   &{Article}  \\\\\n\t\t\\hline{2018}  &{SQuAD 2.0 }    &{505}   &{442}   &{35}   &{28}   &{Article}  \\\\\n\t\t\\hline{2016}  &{BookTest }    &{14062}  &{N/A}   &{N/A}   &{N/A}   &{Book}  \\\\\n\t\t\\hline{2017}  &{COMICS }    &{3948}   &{N/A}   &{N/A}   &{N/A}   &{Book}  \\\\\n\t\t\\hline{2016}  &{Facebook CBT }   &{108}   &{98}   &{5}   &{5}   &{Book}  \\\\\n\t\t\\hline{2019}  &{DREAM }    &{6444}   &{3869}   &{1288}   &{1287}   &{Dialogue}  \\\\\n\t\t\\hline{2016}  &{NewsQA }    &{1010916}  &{909824}   &{50546}   &{50546}   &{Document}  \\\\\n\t\t\\hline{2017}  &{TriviaQA-Web }   &{662659}  &{528979}   &{68621}   &{65059}   &{Document}  \\\\\n\t\t\\hline{2015}  &{Daily Mail }    &{219506}  &{196961}   &{12148}   &{10397}   &{Document}  \\\\\n\t\t\\hline{2017}  &{TriviaQA-Wiki }   &{138538}  &{110648}   &{14229}   &{13661}   &{Document}  \\\\\n\t\t\\hline{2018}  &{ReviewQA }    &{100000}  &{90000}   &{N/A}   &{10000}   &{Document}  \\\\\n\t\t\\hline{2015}  &{CNN }     &{92579}  &{90266}   &{1220}   &{1093}   &{Document}  \\\\\n\t\t\\hline{2017}  &{NarrativeQA }    &{1572}   &{1102}   &{115}   &{355}   &{Document}  \\\\\n\t\t\\hline{2017}  &{TQA } &{1076}   &{666}   & {200}   &{210}   &{Lesson}  \\\\\n\t\t\\hline{2016}  &{MovieQA }    &{548}   &{362}   &{77}   &{109}   &{Movie}  \\\\\n\t\t\\hline{2016}  &{Google MC-AFP }   &{1742618}  &{1727423}   &{7602}   &{7593}   &{Passage}  \\\\\n\t\t\\hline{2016}  &{Who-did-What }    &{147786}  &{127786}   &{10000}   &{10000}   &{Passage}  \\\\\n\t\t\\hline{2017}  &{SearchQA }    &{140461}  &{99820}   &{13393}   &{27248}   &{Passage}  \\\\\n\t\t\\hline{2018}  &{ReCoRD }    &{80121}  &{65709}   &{7133}   &{7279}   &{Passage}  \\\\\n\t\t\\hline{2017}  &{Quasar-T }    &{43012}  &{37012}   &{3000}   &{3000}   &{Passage}  \\\\\n\t\t\\hline{2017}  &{Quasar-S }    &{37362}  &{31049}   &{3174}   &{3139}   &{Passage}  \\\\\n\t\t\\hline{2017}  &{RACE }     &{27933}  &{25137}   &{1389}   &{1407}   &{Passage}  \\\\\n\t\t\\hline{2018}  &{SciTail }   &{27026}  &{23596}   &{1304}   &{2126}   &{Passage}  \\\\\n\t\t\\hline{2016}  &{LAMBADA }    &{12684}  &{2662}   &{4869}   &{5153}   &{Passage}  \\\\\n\t\t\\hline{2018}  &{CoQA }     &{8399}   &{7199}   &{500}   &{700}   &{Passage}  \\\\\n\t\t\\hline{2018}  &{CLOTH }    &{7131}   &{5513}   &{805}   &{813}   &{Passage}  \\\\\n\t\t\\hline{2017}  &{Qangaroo-W }   &{51318}  &{43738}   &{5129}   &{2451}   &{Passage}  \\\\\n\t\t\\hline{2019}  &{DROP }     &{6735}   &{5565}   &{582}   &{588}   &{Passage}  \\\\\n\t\t\\hline{2017}  &{Qangaroo-M }   &{2508}   &{1620}   &{342}   &{546}   &{Passage}  \\\\\n\t\t\\hline{2018}  &{RecipeQA }    &{19779}  &{15847}   &{1963}   &{1969}   &{Recipe}  \\\\\n\t\t\\hline{2015}  &{WikiQA }    &{29258}  & {20360}   &{2733}   &{6165}   &{Sentence}  \\\\\n\t\t\\hline{2013}  &{MCTest-mc500 }   &{500}   &{300}   &{50}   &{150}   &{Story}  \\\\\n\t\t\\hline{2013}  &{MCTest-mc160 }   &{160}   &{70}   &{30}   &{60}   &{Story}  \\\\\n\t\t\\hline{2018}  &{QuAC }     &{8845}   &{6843}   &{1000}   &{1002}   &{Unique section} \\\\\n\t\t\\hline{2019}  &{ShARC }    &{32436}  &{21890}   &{2270}   &{8276}   &{Utterance} \\\\\n\t\t\\hline{2019}  &{Natural Questions }   &{323045}  &{307373}   &{7830}   &{7842}   &{Wikipedia Page} \\\\     \n\t\\end{supertabular}\n\\end{center}", "cites": [1143, 6921, 446, 1142, 6931, 439, 6929, 6930, 6923, 1176, 444, 1147, 1140, 1098, 8304, 6932, 6933, 6920, 441, 6922], "cite_extract_rate": 0.6944444444444444, "origin_cites_number": 36, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of MRC datasets and their sizes, listing quantitative details such as the number of training, development, and test questions. While it references several papers, it primarily uses them to populate a table rather than synthesizing or connecting their contributions. There is minimal critical analysis or abstraction, as the focus remains on factual presentation without deeper evaluation of trends, limitations, or broader implications."}}
{"id": "948adce2-4e02-4166-89d4-119e1de5e891", "title": "The Generation Method of Datasets", "level": "subsection", "subsections": [], "parent_id": "2aa98efb-e563-422a-a721-633a77bc6fa9", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "The Generation Method of Datasets"]], "content": "The generation method of datasets can be roughly described into several categories: Crowdsourcing, Expert, and Automated. \"Crowdsourcing\" is evolving as a distributed problem-solving and business production model in recent years . An example of crowdsourcing website is Amazon Mechanical Turk. Today, many MRC datasets are posed by the distributed workforce on such crowdsourcing websites. The \"Expert\" generation method means that question and answer pairs in the dataset are generated by people with professional knowledge in some fields. For example, in the ARC dataset , there are 7,787 science questions covered by US elementary and middle schools. The \"Automated\" generation method means that question and answer pairs are automatically generated based on corpus, such as many cloze datasets.", "cites": [444], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic classification of dataset generation methods but merely describes them without synthesizing deeper connections or insights. It cites one paper (ARC) to illustrate the 'Expert' method, but does not integrate it into a broader discussion or contrast it meaningfully with other methods. There is little critical analysis or abstraction beyond the specific examples."}}
{"id": "a92eb0fd-dd40-427e-ba23-c1e04bb0347d", "title": "The Type of Context", "level": "subsection", "subsections": [], "parent_id": "2aa98efb-e563-422a-a721-633a77bc6fa9", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "The Type of Context"]], "content": "The type of context can affect the training method of machine reading comprehension model, which produces many special models, such as the multi-hop reading comprehension, and multi-document reading comprehension. There are many types of context in MRC datasets, including Paragraph, Multi-paragraph, Document, Multi-document, URL, Paragraphs with diagrams or images. As shown in Table \\ref{table: generation_method }, we give details of the generation method, corpus source, and context type of each machine's reading comprehension dataset.\n\\begin{center}\n\t\\renewcommand\\arraystretch{1.2} \n\t\\topcaption{The generation method of datasets, source of corpus and type of context. }\n\t\\label{table: generation_method }\n\t\\tablehead{\n\t\t\\toprule { \\textbf{Year}} & { \\textbf{Datasets}}           & { \\textbf{Generation Method}} & { \\textbf{Source of Corpus}}  & { \\textbf{Type of Context}}       \\\\\n\t}\n\t\\tabletail{\t\n\t\t\\hline\n\t\t\\multicolumn{5}{r}{\\small\\sl continued on next page}\\\\\n\t\t\\hline \n\t}\n\t\\tablefirsthead{\n\t\t\\toprule { \\textbf{Year}} & { \\textbf{Datasets}}           & { \\textbf{Generation Method}} & { \\textbf{Source of Corpus}}  & { \\textbf{Type of Context}}       \\\\\n\t\t\\midrule}\n\t\\tablelasttail{\\bottomrule}\n\t\\begin{supertabular}{p{0.8cm} p{3cm} p{3cm} p{3cm}  p{3cm} }\t\n\t\t{2013}     &{MCTest-mc160  }             &{Crowd-sourcing}       &{Factoid  stories}     &{Paragraph}        \\\\\n\t\t\\hline{2013}     &{MCTest-mc500 }             &{Crowd-sourcing}       &{Factoid  stories}     &{Paragraph}        \\\\\n\t\t\\hline{2015}     &{CNN }                &{Automated}         &{News}            &{Document}         \\\\\n\t\t\\hline{2015}     &{CuratedTREC }              &{Crowd-sourcing}       &{Factoid  stories}     &{Paragraph}        \\\\\n\t\t\\hline{2015}     &{Daily  Mail}              &{Automated}         &{News}            &{Document}         \\\\\n\t\t\\hline{2015}     &{WikiQA }                 &{Crowd-sourcing}       &{Wikipedia}         &{Paragraph}        \\\\\n\t\t\\hline{2016}     &{BookTest }                &{Automated}         &{Factoid  stories}     &{Paragraph}        \\\\\n\t\t\\hline{2016}     &{Facebook  CBT }             &{Automated}         &{Factoid  stories}     &{Paragraph}        \\\\\n\t\t\\hline{2016}     &{Google  MC-AFP}            &{Automated}         &{The  Gigaword corpus}   &{Paragraph}        \\\\\n\t\t\\hline{2016}     &{LAMBADA }                &{Crowd-sourcing}       &{Book  Corpus}       &{Paragraph}        \\\\\n\t\t\\hline{2016}     &{MovieQA }                &{Crowd-sourcing}       &{Movie}           &{Paragraph  with Images and Videos}  \\\\\n\t\t\\hline{2016}     &{MS  MARCO}               &{Automated}         &{The  Bing}         &{Paragraph}        \\\\\n\t\t\\hline{2016}     &{NewsQA }                 &{Crowd-sourcing}       &{News}            &{Document}         \\\\\n\t\t\\hline{2016}     &{SQuAD 1.1 }                &{Crowd-sourcing}       &{Wikipedia}         &{Paragraph}        \\\\\n\t\t\\hline{2016}     &{Who-did-What }              &{Automated}         &{News}            &{Document}         \\\\\n\t\t\\hline{2016}     &{WikiMovies }               &{Automated}         &{Movie}           &{Document}         \\\\\n\t\t\\hline{2016}     &{WikiReading }              &{Automated}         &{Wikipedia}         &{Document}         \\\\\n\t\t\\hline{2017}     &{COMICS }                 &{Automated}         &{Comics}           &{Paragraph  with Images}       \\\\\n\t\t\\hline{2017}     &{NarrativeQA }              &{Crowd-sourcing}       &{Movie}           &{Document}         \\\\\n\t\t\\hline{2017}     &{Qangaroo-M }            &{Crowd-sourcing}       &{Wikipedia}         &{Paragraph}        \\\\\n\t\t\\hline{2017}     &{Qangaroo-W }            &{Crowd-sourcing}       &{Scientic  paper}      &{Paragraph}        \\\\\n\t\t\\hline{2017}     &{Quasar-S }                &{Crowd-sourcing}       &{Stack  Overflow}      &{Paragraph}        \\\\\n\t\t\\hline{2017}     &{Quasar-T }                &{Crowd-sourcing}       &{Stack  Overflow}      &{Paragraph}        \\\\\n\t\t\\hline{2017}     &{RACE }                  &{Expert}           &{English  Exam}       &{Document}         \\\\\n\t\t\\hline{2017}     &{SciQ }                  &{Crowd-sourcing}       &{School  science curricula} &{Paragraph}        \\\\\n\t\t\\hline{2017}     &{SearchQA }                &{Crowd-sourcing}       &{J!  Archive and Google}  &{Paragraph  \\& URL}          \\\\\n\t\t\\hline{2017}     &{TQA }                  &{Expert}           &{School  science curricula} &{Paragraph  with Images}       \\\\\n\t\t\\hline{2017}     &{TriviaQA-Wiki }           &{Automated}         &{The  Bing}         &{Paragraph}        \\\\\n\t\t\\hline{2017}     &{TriviaQA-Web }             &{Automated}         &{The  Bing}         &{Paragraph}        \\\\\n\t\t\\hline{2018}     &{ARC-Challenge Set }          &{Expert}           &{School  science curricula} &{Paragraph}        \\\\\n\t\t\\hline{2018}     &{ARC-Easy Set }             &{Expert}           &{School  science curricula} &{Paragraph}        \\\\\n\t\t\\hline{2018}     &{CliCR }                 &{Automated}         &{BMJ  Case Reports}     &{Paragraph}        \\\\\n\t\t\\hline{2018}     &{CLOTH }                 &{Expert}           &{English  Exam}       &{Document}         \\\\\n\t\t\\hline{2018}     &{CoQA }                  &{Crowd-sourcing}       &{Jeopardy}          &{Paragraph}        \\\\\n\t\t\\hline{2018}     &{DuoRC-Paraphrase }            &{Crowd-sourcing}       &{Movie}           &{Paragraph}        \\\\\n\t\t\\hline{2018}     &{DuoRC-Self }               &{Crowd-sourcing}       &{Movie}           &{Paragraph}        \\\\\n\t\t\\hline{2018}     &{HotpotQA-D }     &{Crowd-sourcing}       &{Wikipedia}         &{Multi-paragraph}           \\\\\n\t\t\\hline{2018}     &{HotpotQA-F }      &{Crowd-sourcing}       &{Wikipedia}         &{Multi-paragraph}           \\\\\n\t\t\\hline{2018}     &{MCScript }                &{Crowd-sourcing}       &{Narrative  texts}     &{Paragraph}        \\\\\n\t\t\\hline{2018}     &{MultiRC }                &{Crowd-sourcing}       &{News  and other web pages} &{Multi-sentence}      \\\\\n\t\t\\hline{2018}     &{OpenBookQA }               &{Crowd-sourcing}       &{School  science curricula} &{Paragraph}        \\\\\n\t\t\\hline{2018}     &{PaperQA(Hong et al.) }    &{Crowd-sourcing}       &{Scientic  paper}      &{Paragraph}        \\\\\n\t\t\\hline{2018}     &{PaperQA-L } &{Automated}         &{Scientic  paper}      &{Paragraph}        \\\\\n\t\t\\hline{2018}     &{PaperQA-T }     &{Automated}         &{Scientic  paper}      &{Paragraph}        \\\\\n\t\t\\hline{2018}     &{ProPara }                &{Crowd-sourcing}       &{Process  Paragraph}    &{Paragraph}        \\\\\n\t\t\\hline{2018}     &{QuAC }                  &{Crowd-sourcing}       &{Wikipedia}         &{Document}         \\\\\n\t\t\\hline{2018}     &{RecipeQA }                &{Automated}         &{Recipes}          &{Paragraph  with Images}       \\\\\n\t\t\\hline{2018}     &{ReCoRD }                 &{Crowd-sourcing}       &{News}            &{Paragraph}        \\\\\n\t\t\\hline{2018}     &{ReviewQA }                &{Crowd-sourcing}       &{Hotel  Comments}      &{Paragraph}        \\\\\n\t\t\\hline{2018}     &{SciTail }                &{Crowd-sourcing}       &{School  science curricula} &{Paragraph}        \\\\\n\t\t\\hline{2018}     &{SQuAD 2.0 }                &{Crowd-sourcing}       &{Wikipedia}         &{Paragraph}        \\\\\n\t\t\\hline{2019}     &{CommonSenseQA }             &{Crowd-sourcing}       &{Narrative  texts}     &{Paragraph}        \\\\\n\t\t\\hline{2019}     &{DREAM }                 &{Crowd-sourcing}       &{English  Exam}       &{Dialogues}        \\\\\n\t\t\\hline{2019}     &{DROP }                  &{Crowd-sourcing}       &{Wikipedia}         &{Paragraph}                \\\\\n\t\t\\hline{2019}     &{Natural Questions-L }    &{Crowd-sourcing}       &{Wikipedia}         &{Paragraph}                \\\\\n\t\t\\hline{2019}     &{Natural Questions-S }    &{Crowd-sourcing}       &{Wikipedia}         &{Paragraph}                \\\\\n\t\t\\hline{2019}     &{ShARC }                 &{Crowd-sourcing}       &{Government  Websites}   &{Paragraph}            \\\\   \n\t\\end{supertabular}\n\\end{center}", "cites": [1143, 6921, 446, 1142, 6931, 439, 6930, 6923, 444, 1176, 1147, 1140, 1098, 8304, 6932, 6933, 6920, 441, 6922], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 36, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of MRC datasets and their context types, primarily listing them in a table with metadata such as year, generation method, and corpus source. There is minimal synthesis of ideas across papers, and no critical evaluation or abstraction into broader trends or principles. The content lacks analysis of how context types influence model performance or design."}}
{"id": "48cc5619-0b87-4dc8-bec4-3e1f3821b884", "title": "The Availability of Datasets, Leaderboards and Baselines", "level": "subsection", "subsections": [], "parent_id": "2aa98efb-e563-422a-a721-633a77bc6fa9", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "The Availability of Datasets, Leaderboards and Baselines"]], "content": "The release of MRC baseline projects and leaderboards can help the researchers evaluate the performance of their models. In this section, we try to find all the MRC dataset download links, leaderboards, and baseline projects. As shown in Table \\ref{table: availability }, all the download links of MRC datasets are available except PaperQA . Most of the datasets provide leaderboards and baseline projects except only 19.3\\% of the datasets. We have published all the download links, leaderboards, and the baseline projects on our \\href{https://mrc-datasets.github.io/}{website}.\n\\begin{center}\n\t\\topcaption{The availability of datasets, leaderboards and baselines. }\n\t\\label{table: availability }\n\t\\tablehead{\n\t\t\\toprule {\\textbf{Year}} & {\\textbf{Datasets}}                    & {\\textbf{Dataset Availability}} & {\\textbf{Leaderboard Availability}} & {\\textbf{Baseline Availability}}   \\\\\n\t}\n\t\\tabletail{\t\n\t\t\\hline\n\t\t\\multicolumn{5}{r}{\\small\\sl continued on next page}\\\\\n\t\t\\hline \n\t}\n\t\\tablefirsthead{\n\t\t\\toprule {\\textbf{Year}} & {\\textbf{Datasets}}                    & {\\textbf{Dataset Availability}} & {\\textbf{Leaderboard Availability}} & {\\textbf{Baseline Availability}}   \\\\\n\t\t\\midrule}\n\t\\tablelasttail{\\bottomrule}\n\t\\begin{supertabular}{p{1cm} p{3.5cm} p{2.5cm} p{2.5cm}  p{2.5cm} }\t\n\t\t{2019}          & {CommonSenseQA }                        & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                 \\\\\n\t\t\\hline {2018}          & {MCScript }                             & {$\\surd$}                          & {$\\surd$}                              & {$\\times$}                         \\\\\n\t\t\\hline {2018}          & {OpenBookQA }                           & {$\\surd$}                          & {$\\surd$}                              & {$\\times$}                         \\\\\n\t\t\\hline {2018}          & {ReCoRD }                               & {$\\surd$}                          & {$\\surd$}                              & {$\\times$}                         \\\\\n\t\t\\hline {2018}          & {ARC-Challenge Set }                    & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2018}          & {ARC-Easy Set }                         & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2018}          & {CLOTH }                                & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2016}          & {Facebook CBT }                         & {$\\surd$}                          & {$\\times$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2016}          & {NewsQA }                               & {$\\surd$}                          & {$\\times$}                              & {$\\times$}                         \\\\\n\t\t\\hline {2018}          & {ProPara }                              & {$\\surd$}                          & {$\\surd$}                              & {$\\times$}                         \\\\\n\t\t\\hline {2017}          & {RACE }                                 & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2016}          & {SQuAD 1.1 }                             & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2017}          & {TriviaQA-Wiki }                       & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2017}          & {TriviaQA-Web }                        & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2019}          & {DROP }                                 & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2017}          & {NarrativeQA }                          & {$\\surd$}                          & {$\\times$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2019}          & {ShARC }                                & {$\\surd$}                          & {$\\surd$}                              & {$\\times$}                         \\\\\n\t\t\\hline {2018}          & {CoQA }                                 & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2019}          & {DREAM }                                & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2018}          & {QuAC }                                 & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2013}          & {MCTest-mc160 }                         & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2013}          & {MCTest-mc500 }                         & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2015}          & {WikiQA }                               & {$\\surd$}                          & {$\\times$}                              & {$\\times$}                         \\\\\n\t\t\\hline {2018}          & {CliCR }                                & {$\\surd$}                          & {$\\times$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2018}          & {PaperQA(Hong et al.)}             & {$\\surd$}                          & {$\\times$}                              & {$\\times$}                         \\\\\n\t\t\\hline {2018}          & {PaperQA-L }    & {$\\times$}                          & {$\\times$}                              & {$\\times$}                         \\\\\n\t\t\\hline {2018}          & {PaperQA-T }              & {$\\times$}                          & {$\\times$}                              & {$\\times$}                         \\\\\n\t\t\\hline {2018}          & {ReviewQA }                             & {$\\surd$}                          & {$\\times$}                              & {$\\times$}                         \\\\\n\t\t\\hline {2017}          & {SciQ }                                 & {$\\surd$}                          & {$\\times$}                              & {$\\times$}                         \\\\\n\t\t\\hline {2016}          & {WikiMovies }                           & {$\\surd$}                          & {$\\times$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2016}          & {BookTest }                             & {$\\surd$}                          & {$\\times$}                              & {$\\times$}                         \\\\\n\t\t\\hline {2015}          & {CNN }                                  & {$\\surd$}                          & {$\\times$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2015}          & {Daily Mail}                           & {$\\surd$}                          & {$\\times$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2016}          & {Who-did-What }                         & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2016}          & {WikiReading }                          & {$\\surd$}                          & {$\\times$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2016}          & {Google MC-AFP }                        & {$\\surd$}                          & {$\\times$}                              & {$\\times$}                         \\\\\n\t\t\\hline {2016}          & {LAMBADA }                              & {$\\surd$}                          & {$\\times$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2018}          & {SciTail }                              & {$\\surd$}                          & {$\\surd$}                              & {$\\times$}                         \\\\\n\t\t\\hline {2018}          & {DuoRC-Paraphrase }                     & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2018}          & {DuoRC-Self }                           & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2015}          & {CuratedTREC }                          & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2017}          & {Quasar-S }                             & {$\\surd$}                          & {$\\times$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2017}          & {Quasar-T }                             & {$\\surd$}                          & {$\\times$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2017}          & {SearchQA }                             & {$\\surd$}                          & {$\\times$}                              & {$\\times$}                         \\\\\n\t\t\\hline {2019}          & {Natural Questions-L }        & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2019}          & {Natural Questions-S }       & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2018}          & {SQuAD 2.0 }                             & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2016}          & {MS MARCO }                             & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2017}          & {Qangaroo-MEDHOP }                      & {$\\surd$}                          & {$\\surd$}                              & {$\\times$}                         \\\\\n\t\t\\hline {2017}          & {Qangaroo-WIKIHOP }                     & {$\\surd$}                          & {$\\surd$}                              & {$\\times$}                         \\\\\n\t\t\\hline {2018}          & {MultiRC }                              & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2018}          & {HotpotQA-Distractor }         & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2018}          & {HotpotQA-Fullwiki }           & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                         \\\\\n\t\t\\hline {2017}          & {COMICS }                               & {$\\surd$}                          & {$\\times$}                              & {$\\surd$}                                    \\\\\n\t\t\\hline {2016}          & {MovieQA }                              & {$\\surd$}                          & {$\\surd$}                              & {$\\surd$}                                   \\\\\n\t\t\\hline {2018}          & {RecipeQA }                             & {$\\surd$}                          & {$\\surd$}                              & {$\\times$}                                      \\\\\n\t\t\\hline {2017}          & {TQA } & {$\\surd$}                          & {$\\surd$}                              & {$\\times$}                       \\\\      \n\t\\end{supertabular}\n\\end{center}", "cites": [1143, 446, 6921, 1142, 6931, 439, 6929, 6930, 6923, 444, 1176, 1147, 1140, 1098, 8304, 6922, 6932, 6933, 6920, 441], "cite_extract_rate": 0.6842105263157895, "origin_cites_number": 38, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily provides a descriptive overview of dataset availability, listing which datasets have downloadable links, leaderboards, and baselines. It lacks synthesis by not connecting these data points to broader trends or themes in MRC research. There is minimal critical analysis or abstraction beyond the specific datasets."}}
{"id": "d33046a4-c4a4-48f8-ac91-d5902afe6393", "title": "Prerequisite Skills", "level": "subsection", "subsections": [], "parent_id": "2aa98efb-e563-422a-a721-633a77bc6fa9", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Prerequisite Skills"]], "content": "When humans read passages and answer questions, we need to master various prerequisite skills to answer them correctly. The analysis of these prerequisite skills may help us understand the intrinsic properties of the MRC datasets. In Table \\ref{table: prerequisite_skills }, we quote the descriptions and examples of prerequisite skills proposed by Sugawara et al.. They defined 10 kinds of prerequisite skills, including List/Enumeration, Mathematical operations, Coreference resolution, Logical reasoning, etc. By manually annotate questions in the MCTest  and SQuAD 1.1 , they got the frequencies of each prerequisite skill in the two MRC datasets. As seen in Table \\ref{table: prerequisite_skills }.\nHowever, the definition and classification of these prerequisite skills are often subjective and changeable. Many definitions have been drawn  , but they are still hard to give a standard mathematical definition of them, which is the same as natural language understanding.\n\\begin{center}\n\t\\renewcommand\\arraystretch{1.2} \n\t\\topcaption{Prerequisite skills with descriptions or examples , and their frequencies (in percentage) in SQuAD 1.1  and MCTest  (MC160 development set). }\n\t\\label{table: prerequisite_skills }\n\t\\tablehead{\n\t\t\\toprule { \\textbf{Prerequisite skills}} & { \\textbf{Descriptions or examples}}         & { \\textbf{Frequency SQuAD}} & { \\textbf{Frequency MCTest}}   \\\\\n\t}\n\t\\tabletail{\t\n\t\t\\hline\n\t\t\\multicolumn{4}{r}{\\small\\sl continued on next page}\\\\\n\t\t\\hline \n\t}\n\t\\tablefirsthead{\n\t\t\\toprule { \\textbf{Prerequisite skills}} & { \\textbf{Descriptions or examples}}         & { \\textbf{Frequency SQuAD}} & { \\textbf{Frequency MCTest}}   \\\\\n\t\t\\hline}\n\t\\tablelasttail{\\bottomrule}\n\t\\begin{supertabular}{p{4cm} p{6cm}p{1.5cm}  p{1.5cm} }\t\n\t\t{ List/Enumeration}      & { Tracking, retaining, and list/enumeration of entities or states}   & { 5.00\\%}       & { 11.70\\%}       \\\\\n\t\t\\hline { Mathematical operations}     & { Four basic operations and geometric comprehension}     & { 0.00\\%}       & { 4.20\\%}      \\\\\n\t\t\\hline { Coreference resolution}      & { Detection and resolution of coreferences}        & { 6.20\\%}       & { 57.50\\%}       \\\\\n\t\t\\hline { Logical reasoning}       & { Induction, deduction, conditional statement, and quantifier}   & { 1.20\\%}       & { 0.00\\%}      \\\\\n\t\t\\hline { Analogy}         & { Trope in figures of speech, e.g.,metaphor}       & { 0.00\\%}       & { 0.00\\%}      \\\\\n\t\t\\hline { Spatiotemporal relations}    & { Spatial and/or temporal relations of events}       & { 2.50\\%}       & { 28.30\\%}       \\\\\n\t\t\\hline { Causal relations}      & { Why, because, the reason, etc.}          & { 6.20\\%}       & { 18.30\\%}       \\\\\n\t\t\\hline { Commonsense reasoning}       & { Taxonomic/qualitative knowledge, action and event change}    & { 86.20\\%}      & { 49.20\\%}       \\\\\n\t\t\\hline { Complex sentences}       & { Coordination or subordination of clauses}        & { 20.00\\%}      & { 15.80\\%}       \\\\\n\t\t\\hline { Special sentence structure}    & { Scheme in figures of speech, constructions, and punctuation marks} & { 25.00\\%}      & { 10.00\\%} \\\\\n\t\\end{supertabular}\n\\end{center}", "cites": [6919, 439], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section presents a list of prerequisite skills and their frequencies in SQuAD 1.1 and MCTest, largely drawing from Sugawara et al. It includes a table summarizing the findings, but offers minimal synthesis of ideas from the cited papers. There is little critical evaluation or identification of broader patterns or principles in the analysis of these skills."}}
{"id": "ba1693b6-f701-4885-a0f5-374ec4dc9380", "title": "Citation Analysis", "level": "subsection", "subsections": [], "parent_id": "2aa98efb-e563-422a-a721-633a77bc6fa9", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Citation Analysis"]], "content": "The number of citations of the paper in which a dataset was proposed reveals the dataset’s impact to some extent. As shown in Table \\ref{table: Citation }, we analyze how many times each paper was cited and make a statistical table. We count both the total number of citations and the monthly average citations since they were published. Except for the two PaperQA datasets , the number of citations of all other papers have been found in Google Scholar. Besides, we make a Table \\ref{table: Citation } in which the datasets are sorted by the monthly average citations. As expected, the dataset with the highest monthly average citations is SQuAD 1.1 , followed by CNN/Daily Mail  and SQuAD 2.0 . It shows that these datasets are widely used as a benchmark.\n\\begin{center}\n\t\\renewcommand\\arraystretch{1.2} \n\t\\topcaption{Citation analysis of the paper in which each dataset was proposed.}\n\t\\label{table: Citation }\n\t\\tablehead{\n\t\t\\toprule {\\textbf{Year}} & {\\textbf{Datasets}} & {\\textbf{Average Monthly Citations}} & {\\textbf{Total Citations}} & {\\textbf{Months after Publication}} & {\\textbf{Date of Publication}} & {\\textbf{Date of Statistics}} \\\\\n\t}\n\t\\tabletail{\t\n\t\t\\hline\n\t\t\\multicolumn{7}{r}{\\small\\sl continued on next page}\\\\\n\t\t\\hline \n\t}\n\t\\tablefirsthead{\n\t\t\\toprule {\\textbf{Year}} & {\\textbf{Datasets}} & {\\textbf{Average Monthly Citations}} & {\\textbf{Total Citations}} & {\\textbf{Months after Publication}} & {\\textbf{Date of Publication}} & {\\textbf{Date of Statistics}} \\\\\n\t\t\\midrule}\n\t\\tablelasttail{\\bottomrule}\n\t\\begin{supertabular}{p{1cm} p{2.5cm} p{1.5cm} p{1.5cm} p{1.5cm} p{2cm} p{2cm}}\t\n             {2016}     & {SQuAD 1.1 }     & {33.35}                & {1234}           & {37}                & {2016-10-10}           & {2019-12-01}  \\\\\n  \\hline {2015}     & {CNN/Daily Mail }  & {25.21}                & {1210}           & {48}                & {2015-11-19}           & {2019-12-01}  \\\\\n  \\hline {2018}     & {SQuAD 2.0 }     & {14.65}                & {249}            & {17}                & {2018-06-11}           & {2019-12-01}  \\\\\n  \\hline {2019}     & {Natural Questions } & {9.00}                 & {45}            & {5}                 & {2019-07-01}           & {2019-12-01}  \\\\\n  \\hline {2017}     & {TriviaQA }     & {7.97}                 & {239}            & {30}                & {2017-05-13}           & {2019-12-01}  \\\\\n  \\hline {2018}     & {CoQA }       & {7.93}                 & {119}            & {15}                & {2018-08-21}           & {2019-12-01}  \\\\\n  \\hline {2016}     & {WikiMovies }    & {7.73}                 & {286}            & {37}                & {2016-10-10}           & {2019-12-01}  \\\\\n  \\hline {2016}     & {CBT }   & {6.92}                 & {332}            & {48}                & {2015-11-07}           & {2019-12-01}  \\\\\n  \\hline {2016}     & {MS MARCO }     & {6.65}                 & {246}            & {37}                & {2016-10-31}           & {2019-12-01}  \\\\\n  \\hline {2015}     & {WikiQA }      & {6.43}                 & {328}            & {51}                & {2015-09-01}           & {2019-12-01}  \\\\\n  \\hline {2018}     & {HotpotQA }     & {5.71}                 & {80}            & {14}                & {2018-09-25}           & {2019-12-01}  \\\\\n  \\hline {2016}     & {NewsQA }      & {5.21}                 & {172}            & {33}                & {2017-02-07}           & {2019-12-01}  \\\\\n  \\hline {2016}     & {MovieQA }      & {5.00}                 & {235}            & {47}                & {2015-12-09}           & {2019-12-01}  \\\\\n  \\hline {2017}     & {RACE }       & {4.87}                 & {151}            & {31}                & {2017-04-15}           & {2019-12-01}  \\\\\n  \\hline {2018}     & {QuAC }       & {4.73}                 & {71}            & {15}                & {2018-08-27}           & {2019-12-01}  \\\\\n  \\hline {2013}     & {MCTest }      & {4.69}                 & {347}            & {74}                & {2013-10-01}           & {2019-12-01}  \\\\\n  \\hline {2017}     & {Qangaroo }     & {4.59}                 & {78}            & {17}                & {2018-06-11}           & {2019-12-01}  \\\\\n  \\hline {2018}     & {SciTail }  & {4.16}                 & {79}            & {19}                & {2018-04-27}           & {2019-12-01}  \\\\\n  \\hline {2017}     & {NarrativeQA }    & {3.74}                 & {86}            & {23}                & {2017-12-19}           & {2019-12-01}  \\\\\n  \\hline {2019}     & {DROP }       & {3.00}                 & {27}            & {9}                 & {2019-03-01}           & {2019-12-01}  \\\\\n  \\hline {2018}     & {ARC}        & {2.90}                 & {58}            & {20}                & {2018-03-14}           & {2019-12-01}  \\\\\n  \\hline {2017}     & {SearchQA }     & {2.81}                 & {87}            & {31}                & {2017-04-18}           & {2019-12-01}  \\\\\n  \\hline {2018}     & {OpenBookQA }    & {2.64}                 & {37}            & {14}                & {2018-09-08}           & {2019-12-01}  \\\\\n  \\hline {2016}     & {WikiReading }    & {2.41}                 & {77}            & {32}                & {2017-03-15}           & {2019-12-01}  \\\\\n  \\hline {2019}     & {CommonSenseQA }   & {2.33}                 & {28}            & {12}                & {2018-11-02}           & {2019-12-01}  \\\\\n  \\hline {2017}     & {Quasar }      & {1.82}                 & {51}            & {28}                & {2017-07-12}           & {2019-12-01}  \\\\\n  \\hline {2016}     & {Who-did-What }   & {1.69}                 & {66}            & {39}                & {2016-08-18}           & {2019-12-01}  \\\\\n  \\hline {2018}     & {MultiRC }      & {1.67}                 & {30}            & {18}                & {2018-06-01}           & {2019-12-01}  \\\\\n  \\hline {2017}     & {TQA }        & {1.55}                 & {45}            & {29}                & {2017-07-01}           & {2019-12-01}  \\\\\n  \\hline {2019}     & {DREAM }       & {1.50}                 & {15}            & {10}                & {2019-01-31}           & {2019-12-01}  \\\\\n  \\hline {2018}     & {ReCoRD }      & {1.39}                 & {18}            & {13}                & {2018-10-30}           & {2019-12-01}  \\\\\n  \\hline {2016}     & {LAMBADA }      & {1.29}                 & {53}            & {41}                & {2016-6-20}           & {2019-12-01}  \\\\\n  \\hline {2019}     & {ShARC }       & {1.27}                 & {19}            & {15}                & {2018-08-28}           & {2019-12-01}  \\\\\n  \\hline {2018}     & {MCScript }     & {1.10}                 & {22}            & {20}                & {2018-03-14}           & {2019-12-01}  \\\\\n  \\hline {2015}     & {CuratedTREC }    & {0.98}                 & {47}            & {48}                & {2015-11-20}           & {2019-12-01}  \\\\\n  \\hline {2018}     & {RecipeQA }     & {0.93}                 & {13}            & {14}                & {2018-09-04}           & {2019-12-01}  \\\\\n  \\hline {2017}     & {COMICS }      & {0.86}                 & {31}            & {36}                & {2016-11-16}           & {2019-12-01}  \\\\\n  \\hline {2018}     & {ProPara }      & {0.83}                 & {15}            & {18}                & {2018-05-17}           & {2019-12-01}  \\\\\n  \\hline {2017}     & {SciQ }   & {0.79}                 & {22}            & {28}                & {2017-07-19}           & {2019-12-01}  \\\\\n  \\hline {2016}     & {BookTest }     & {0.73}                 & {27}            & {37}                & {2016-10-04}           & {2019-12-01}  \\\\\n  \\hline {2018}     & {DuoRC }       & {0.63}                 & {12}            & {19}                & {2018-04-21}           & {2019-12-01}  \\\\\n  \\hline {2018}     & {CliCR }       & {0.55}                 & {11}            & {20}                & {2018-03-26}           & {2019-12-01}  \\\\\n  \\hline {2018}     & {CLOTH }       & {0.42}                 & {10}            & {24}                & {2017-11-09}           & {2019-12-01}  \\\\\n  \\hline {2018}     & {ReviewQA }     & {0.08}                 & {1}             & {13}                & {2018-10-29}           & {2019-12-01}    \\\\   \n\t\\end{supertabular}\n\\end{center}\nWe also analyze the monthly average citations. As seen in Figure \\ref{figure: Citation }, on the whole, there is a correlation between the monthly average citations and the total citations of the MRC dataset. For example, the top two citations of the total citations and the monthly average citations are the same which are SQuAD 1.1  and CNN/Daily Mail . However, some papers with lower total citations have higher monthly citations. This shows that these papers have been published for a short time, but they have received a lot of attention from the community, such as SQuAD 2.0 . In addition, some papers with higher total citations have relatively low monthly average citations. Because these datasets have been published for a long time, but are rarely used in recent years. \n\\begin{figure}[H]\n\t\\centering\n\t\\includegraphics[width=12cm]{Graphs/Figure_Citaions}\n\t\\caption{The average number of citations per month of the papers presenting the MRC datasets.}\n\t\\label{figure: Citation }\n\\end{figure}", "cites": [1143, 446, 6921, 1142, 6931, 439, 6930, 6923, 1176, 1147, 1140, 444, 1098, 8304, 6932, 6933, 6920, 441, 6922], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 36, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily presents citation counts and average monthly citations for various MRC datasets without synthesizing or connecting insights from the cited papers. It lacks critical evaluation or comparison of the datasets’ strengths and weaknesses. While it notes that highly cited datasets are widely used, it offers minimal abstraction or deeper analysis of broader trends or principles in dataset development or usage."}}
{"id": "ddd001b6-69c4-410f-8a97-0dfbd2f5912b", "title": "Overview", "level": "subsubsection", "subsections": [], "parent_id": "86f80a79-c635-4232-bd9f-06be55f6dc49", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Characteristics of Datasets"], ["subsubsection", "Overview"]], "content": "In recent years, various large-scale MRC datasets have been created. The growth of large-scale datasets greatly promoted the research process of the machine reading comprehension.\\\\\nIn this section, we analyze the characteristics of existing MRC datasets, including MRC with unanswerable questions, multi-hop MRC, MRC with paraphrased paragraph, MRC which require commonsense (world knowledge), complex reasoning MRC, large-scale dataset, domain-specific dataset, multi-modal MRC, MRC dataset for open-domain QA, and conversational MRC.\\\\\nIt should be noted that many MRC datasets have multiple characteristics. A typical example is the DuoRC  dataset, which has the following four characteristics: 1. DuoRC contains two versions of context, and the meanings of different versions of context are the same while the authors try to avoid words overlap between the two versions, so the DuoRC is a MRC dataset with paraphrased paragraphs. 2. DuoRC requires the use of commonsense and world knowledge. 3. It requires complex reasoning across multiple sentences to infer the answer. 4.There are unanswerable questions in DuoRC .\\\\\nFinally, we summarize the characteristics of each dataset in Table \\ref{table: characteristics}. In the following sections, we will describe each of them separately.\n\\begin{center}\n\t\\renewcommand\\arraystretch{1.2} \n\t\\topcaption{The characteristics of each MRC dataset.}\n\t\\label{table: characteristics}\n\t\\tablehead{\n\t\t\\toprule { \\textbf{Year}} & {\\textbf{Datasets}}        & {\\textbf{Characteristics}} \\\\\n\t}\n\t\\tabletail{\t\n\t\t\\hline\n\t\t\\multicolumn{3}{r}{\\small\\sl continued on next page}\\\\\n\t\t\\hline \n\t}\n\t\\tablefirsthead{\n\t\t\\toprule { \\textbf{Year}} & {\\textbf{Datasets}}        & {\\textbf{Characteristics}} \\\\\n\t\t\\midrule}\n\t\\tablelasttail{\\bottomrule}\n\t\\begin{supertabular}{p{1cm} p{3cm} p{8cm}}\t\n\t\t{2015}     & {WikiQA }             & {With  Unanswerable Questions}      \\\\\n\t\t\\hline {2018}     & {SQuAD 2.0 }            & {With  Unanswerable Questions}      \\\\\n\t\t\\hline {2019}     & {Natural Question }       & {With  Unanswerable Questions}      \\\\\n\t\t\\hline {2016}     & {MS  MARCO }           & {With  Unanswerable Questions ; Multi-hop MRC}       \\\\\n\t\t\\hline {2018}     & {DuoRC }              & {With  Paraphrased Paragraph ; Require Commonsense (World knowledge) ; Complex  Reasoning ; With Unanswerable Questions} \\\\\n\t\t\\hline {2016}     & {Who-did-What }          & {With  Paraphrased Paragraph ; Complex Reasoning}     \\\\\n\t\t\\hline {2018}     & {ARC }               & {Require  Commonsense (World knowledge) ; Complex Reasoning}       \\\\\n\t\t\\hline {2018}     & {MCScript }            & {Require  Commonsense (World knowledge)}          \\\\\n\t\t\\hline {2018}     & {OpenBookQA }           & {Require  Commonsense (World knowledge)}          \\\\\n\t\t\\hline {2018}     & {ReCoRD }             & {Require  Commonsense (World knowledge)}          \\\\\n\t\t\\hline {2019}     & {CommonSenseQA }          & {Require  Commonsense (World knowledge)}          \\\\\n\t\t\\hline {2016}     & {WikiReading }           & {Require  Commonsense (External knowledge) ; Large Scale Dataset}     \\\\\n\t\t\\hline {2016}     & {WikiMovies }           & {Require  Commonsense (External knowledge) ; Domain-specific}       \\\\\n\t\t\\hline {2016}     & {MovieQA }             & {Multi-Modal  MRC}            \\\\\n\t\t\\hline {2017}     & {COMICS }             & {Multi-Modal  MRC}            \\\\\n\t\t\\hline {2017}     & {TQA }               & {Multi-Modal  MRC}            \\\\\n\t\t\\hline {2018}     & {RecipeQA }            & {Multi-Modal  MRC}            \\\\\n\t\t\\hline {2018}     & {HotpotQA }            & {Multi-hop  MRC ; Complex Reasoning}            \\\\\n\t\t\\hline {2017}     & {NarrativeQA }           & {Multi-hop  MRC ; Complex Reasoning}            \\\\\n\t\t\\hline {2017}     & {Qangaroo }            & {Multi-hop  MRC}             \\\\\n\t\t\\hline {2018}     & {MultiRC }             & {Multi-hop  MRC}             \\\\\n\t\t\\hline {2015}     & {CNN/Daily Mail }        & {Large-scale Dataset}          \\\\\n\t\t\\hline {2016}     & {BookTest }            & {Large-scale Dataset}          \\\\\n\t\t\\hline {2013}     & {MCTest }             & {For  Open-domain QA}          \\\\\n\t\t\\hline {2015}     & {CuratedTREC }           & {For  Open-domain QA}          \\\\\n\t\t\\hline {2017}     & {Quasar }             & {For Open-domain  QA}          \\\\\n\t\t\\hline {2017}     & {SearchQA }            & {For  Open-domain QA}          \\\\\n\t\t\\hline {2017}     & {SciQ }              & {Domain-specific}             \\\\\n\t\t\\hline {2018}     & {CliCR }              & {Domain-specific}             \\\\\n\t\t\\hline {2018}     & {PaperQA(Hong et al.) } & {Domain-specific}             \\\\\n\t\t\\hline {2018}     & {PaperQA(Park et al.) }    & {Domain-specific}             \\\\\n\t\t\\hline {2018}     & {ReviewQA }            & {Domain-specific}             \\\\\n\t\t\\hline {2018}     & {SciTail }             & {Domain-specific}             \\\\\n\t\t\\hline {2019}     & {DROP }              & {Complex  Reasoning}           \\\\\n\t\t\\hline {2016}     & {Facebook CBT }         & {Complex  Reasoning}           \\\\\n\t\t\\hline {2016}     & {Google  MC-AFP}         & {Complex  Reasoning}           \\\\\n\t\t\\hline {2016}     & {LAMBADA }             & {Complex  Reasoning}           \\\\\n\t\t\\hline {2016}     & {NewsQA }             & {Complex  Reasoning}           \\\\\n\t\t\\hline {2016}     & {SQuAD 1.1 }            & {Complex  Reasoning}           \\\\\n\t\t\\hline {2017}     & {RACE }              & {Complex  Reasoning}           \\\\\n\t\t\\hline {2017}     & {TriviaQA }            & {Complex  Reasoning}           \\\\\n\t\t\\hline {2018}     & {CLOTH }              & {Complex  Reasoning}           \\\\\n\t\t\\hline {2018}     & {ProPara }             & {Complex  Reasoning}     \\\\\n\t\t\\hline {2019}     & {DREAM }              & {Conversational  MRC ; Require Commonsense (World knowledge)}       \\\\\n\t\\hline {2018}     & {CoQA }              & {Conversational  MRC ; With Unanswerable Questions}          \\\\\n\t\\hline {2018}     & {QuAC }              & {Conversational  MRC ; With Unanswerable Questions}          \\\\\n\t\\hline {2019}     & {ShARC }              & {Conversational  MRC}          \\\\\t\t\n\t\\end{supertabular}\n\\end{center}", "cites": [1143, 6921, 446, 1142, 6931, 439, 6930, 6923, 444, 1176, 1147, 1140, 1098, 8304, 6932, 6933, 6920, 441, 6922], "cite_extract_rate": 0.6756756756756757, "origin_cites_number": 37, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of MRC datasets and their characteristics, listing each dataset with associated traits. It includes a detailed table but does not meaningfully synthesize or compare the cited works to create a deeper understanding. There is minimal critical analysis or abstraction to broader patterns or principles in the field."}}
{"id": "f9297f67-5d2e-4320-a4c8-9c53e4a20025", "title": "MRC with Unanswerable Questions", "level": "subsubsection", "subsections": [], "parent_id": "86f80a79-c635-4232-bd9f-06be55f6dc49", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Characteristics of Datasets"], ["subsubsection", "MRC with Unanswerable Questions"]], "content": "The existing MRC datasets often lack training sets for unanswerable questions, which weaken the robustness of the MRC systems. As a result, when the MRC models answer unanswerable questions, the models always try to give a most likely answer, rather than refuse to answer these unanswered questions. In this way, no matter how the model answers, the answers must be wrong.\\\\\nTo solve this problem, the researchers proposed many MRC datasets with unanswerable questions which were more challenging. Among the datasets collected by us, the datasets that contain unanswerable questions include: SQuAD 2.0, MS MARCO , Natural Questions  and NewsQA . We will give a detailed description of these datasets in section in section~\\ref{sec: Descriptions}.\\\\", "cites": [446], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic analytical perspective by identifying a limitation in existing MRC datasets: the lack of unanswerable questions. It mentions the MS MARCO dataset as part of the response to this issue but does not deeply synthesize or integrate insights across multiple papers. The critique is clear but limited in scope and lacks a broader conceptual framework."}}
{"id": "5c239038-9feb-4a4a-a611-2a0283c34018", "title": "Multi-hop Reading Comprehension", "level": "subsubsection", "subsections": [], "parent_id": "86f80a79-c635-4232-bd9f-06be55f6dc49", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Characteristics of Datasets"], ["subsubsection", "Multi-hop Reading Comprehension"]], "content": "In most MRC dataset, the answer to a question usually can be found in a single paragraph or a document. However, in real human reading comprehension, when reading a novel, we are very likely to extract answers from multiple paragraphs. Compared with single passage MRC, the multi-hop machine reading comprehension is more challenging and requires multi-hop searching and reasoning over confusing passages or documents. \\\\\nIn different papers, multi-hop MRC is named in different ways such as multi-document machine reading comprehension , multi-paragraph machine reading comprehension , multi-sentence machine reading comprehension . Compared with single paragraph MRC, multi-hop MRC is more challenging and is naturally suitable for unstructured information processing.\nAmong the datasets collected by us, the datasets that contain unanswerable questions including SQuAD 2.0 , MS MARCO , Natural Questions , and NewsQA .", "cites": [446, 1147, 7046, 6931], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of multi-hop reading comprehension and lists some datasets that include unanswerable questions. It makes minimal synthesis by noting different terminology used across papers, but does not integrate the cited works into a cohesive discussion. There is no critical evaluation of the papers' strengths or weaknesses, and abstraction is limited to a general observation about the nature of multi-hop MRC."}}
{"id": "bb15fc5e-85a7-4ea3-83de-f088bdff3202", "title": "Multi-modal Reading Comprehension", "level": "subsubsection", "subsections": [], "parent_id": "86f80a79-c635-4232-bd9f-06be55f6dc49", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Characteristics of Datasets"], ["subsubsection", "Multi-modal Reading Comprehension"]], "content": "When humans read, they often do it in a multi-modal way. For example, in order to understand the information and answer the questions, sometimes, we need to read both the texts and illustrations, and we also need to use our brains to imagine, reconstruct, reason, calculate, analyze or compare. Currently, most of the existing machine reading comprehension datasets belong to plain textual machine reading comprehension, which has some limitations. some complex or precise concepts can not be described or communicated only via text. For example, if we need the computer to answer some precise questions related to aircraft engine maintenance, we may have to input the image of the aircraft engine. \\\\\nMulti-modal machine reading comprehension is a dynamic interdisciplinary field that has great application potential. Considering the heterogeneity of data, multi-modal machine reading comprehension brings unique challenges to NLP researchers, because the model has to understand both texts and images. In recent years, due to the availability of large-scale internet data, many multi-modal MRC datasets have been created, such as TQA , RecipeQA , COMICS , and MovieQA .", "cites": [6922, 6921, 6923], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces the concept of multi-modal reading comprehension and cites three relevant papers, but it does not effectively synthesize or connect their contributions into a broader narrative. It lacks critical evaluation of the cited works and does not generalize beyond specific examples to highlight overarching trends or principles in the field."}}
{"id": "b65d67d8-c296-4272-994d-2bb76bb3dec0", "title": "Reading Comprehension Require Commonsense or World knowledge", "level": "subsubsection", "subsections": [], "parent_id": "86f80a79-c635-4232-bd9f-06be55f6dc49", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Characteristics of Datasets"], ["subsubsection", "Reading Comprehension Require Commonsense or World knowledge"]], "content": "Human language is complex. When answering questions, we often need to draw upon our commonsense or world knowledge. Moreover, in the process of human language, many conventional puns and polysemous words have been formed. The use of the same words in different scenes also requires the computer to have a good command of the relevant commonsense or world knowledge.\\\\\nConventional MRC tasks usually focus on answering questions about given passages. In the existing machine reading comprehension datasets, only a small proportion of questions need to be answered with commonsense knowledge. In order to build MRC models with commonsense or world knowledge, many Commonsense Reading Comprehension (CRC) datasets have been created, such as CommonSenseQA , ReCoRD  and OpenBookQA .", "cites": [1176, 1147, 6931], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly describes the importance of commonsense knowledge in MRC and lists a few CRC datasets, including ReCoRD, ReCoRD, and OpenBookQA, but does not integrate their core contributions or connect them in a coherent narrative. It lacks critical evaluation of the papers' strengths or limitations and provides only minimal generalization about the broader role of commonsense in MRC datasets. The overall content remains descriptive rather than insightful."}}
{"id": "a6429205-26b6-4b51-aaa0-3cb20315b391", "title": "Complex Reasoning MRC", "level": "subsubsection", "subsections": [], "parent_id": "86f80a79-c635-4232-bd9f-06be55f6dc49", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Characteristics of Datasets"], ["subsubsection", "Complex Reasoning MRC"]], "content": "The reasoning is an innate ability of human beings, which can be embodied in logical thinking, reading comprehension, and other activities. The reasoning is also a key component in artificial intelligence and a fundamental goal of MRC. In recent years, reasoning has been an essential topic among the MRC community. We hope that the MRC system can not only read and learn the representation of the language but also can really understand the context and answer complex questions. In order to push towards complex reasoning MRC system, many datasets have been generated, such as Facebook bAbI , DROP ,RACE , and CLOTH .", "cites": [8304, 1142, 1147], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions the importance of reasoning in MRC and lists some relevant datasets, including DROP and OpenBookQA. However, it provides minimal synthesis of the cited works, lacks critical evaluation or comparison, and does not abstract broader patterns or principles. The content remains largely descriptive with little analytical depth."}}
{"id": "cc4e500a-9c3e-4d63-8056-d5966240ba3b", "title": "Conversational Reading Comprehension", "level": "subsubsection", "subsections": [], "parent_id": "86f80a79-c635-4232-bd9f-06be55f6dc49", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Characteristics of Datasets"], ["subsubsection", "Conversational Reading Comprehension"]], "content": "It is a natural way for human beings to exchange information through a series of conversations. In the typical MRC tasks, different question and answer pairs are usually independent of each other. However, in real human language communication, we often achieve an efficient understanding of complex information through a series of interrelated conversations. Similarly, in human communication scenarios, we often ask questions on our own initiative, to obtain key information that helps us understand the situation. In the process of conversation, we need to have a deep understanding of the previous conversations in order to answer each other's questions correctly or ask meaningful new questions. Therefore, in this process, historical conversation information also becomes a part of the context.\\\\\nIn recent years, conversational machine reading comprehension (CMRC) has become a new research hotspot in the NLP community, and there emerged many related datasets, such as CoQA , QuAC , DREAM  and ShARC .", "cites": [1098, 446, 6931], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section introduces the concept of conversational reading comprehension and mentions several datasets (CoQA, QuAC, DREAM, ShARC) but does not provide a synthesis of their features, nor does it compare or analyze their strengths and weaknesses. The cited papers are only referenced to support the general idea without deeper critical or abstract engagement."}}
{"id": "65653874-ba42-41f2-9ea8-ce9300c52bfe", "title": "Domain-specific Datasets", "level": "subsubsection", "subsections": [], "parent_id": "86f80a79-c635-4232-bd9f-06be55f6dc49", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Characteristics of Datasets"], ["subsubsection", "Domain-specific Datasets"]], "content": "In this paper, a domain-specific dataset refers to the MRC dataset whose context comes from a particular domain, such as science examinations, movies, clinical reports. Therefore, the neural network models trained by those datasets usually can be directly applied to a certain field. For example, CliCR  is a cloze MRC dataset in the medical domain. There are approximately 100,000 cloze questions about the clinical case reports. SciQ  is a multiple-choice MRC dataset containing 13.7K crowdsourced science exam questions about physics, chemistry and biology, and others. The context and questions of SciQ are derived from scientific exam questions. In addition, domain-specific datasets also include ReviewQA , SciTail , WikiMovies , PaperQA .", "cites": [444, 6920, 1147], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section lists several domain-specific MRC datasets and briefly describes their contexts and purposes. It includes some cited papers but does not synthesize or integrate their contributions meaningfully. There is no critical evaluation or comparison of the datasets, nor is there abstraction to broader trends or principles in domain-specific MRC."}}
{"id": "af678794-d850-4dcc-b718-e49e56bb0837", "title": "MRC with Paraphrased Paragraph", "level": "subsubsection", "subsections": [], "parent_id": "86f80a79-c635-4232-bd9f-06be55f6dc49", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Characteristics of Datasets"], ["subsubsection", "MRC with Paraphrased Paragraph"]], "content": "Paragraph paraphrasing refers to rewriting or rephrasing a paragraph using different words, while still conveying the same messages as before. The MRC dataset with paraphrased paragraph has at least two versions of context which expresses the same meanings while there is little word overlap between the different versions of context. The task of paraphrased MRC requires the computer to answer questions about contexts. To answer these questions correctly, the computer needs to understand the true meaning of different versions of context. So far, we only find that the DuoRC  and Who-did-What  are datasets of this type.", "cites": [1143, 1147], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly introduces the concept of paraphrased MRC datasets but does not effectively synthesize the two cited papers to highlight their relevance to the topic. It lacks critical evaluation of the papers and offers no abstraction or generalization beyond a basic definition and listing of datasets."}}
{"id": "13dd1a36-3b57-471b-b065-3737e4dc5c8a", "title": "MRC dataset for Open-Domain QA", "level": "subsubsection", "subsections": [], "parent_id": "86f80a79-c635-4232-bd9f-06be55f6dc49", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Characteristics of Datasets"], ["subsubsection", "MRC dataset for Open-Domain QA"]], "content": "The open-domain question answering was originally defined as finding answers in collections of unstructured documents . With the development of MRC research, many MRC datasets tend to be used to solve open-domain QA. The release of new MRC datasets such as MCTest , CuratedTREC , Quasar , SearchQA  greatly promotes open-domain QA recently.", "cites": [8304, 6930], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of open-domain QA and mentions the contribution of specific MRC datasets like SearchQA and Quasar. However, it lacks synthesis by not connecting the unique features or methodologies of these datasets in a broader context. There is no critical evaluation or comparison, and no abstraction to identify overarching patterns or principles in the field."}}
{"id": "50d87fea-289b-40f0-8589-c625303898d8", "title": "SQuAD 2.0", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "SQuAD 2.0"]], "content": "SQuAD 2.0  is the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines the data from the existing version of SQuAD 1.1  with more than 50,000 unanswerable questions written by crowd workers. To acquire a good performance on SQuAD 2.0, the MRC model not only needs to answer questions when possible, but also needs to identify issues without correct answers in the context and not to answer them . For existing models, SQuAD 2.0 is a challenging natural language understanding task. The author also compares the test data of similar model architecture in SQuAD 1.1. Compared with SQuAD 1.1, the gap between human accuracy and machine accuracy in SQuAD 2.0 is much larger, which confirms that square2.0 is a more difficult data set for existing models. As mentioned in the authors' paper, the powerful nervous model that achieved 86\\% F1 on SQuAD 1.1 received only 66\\% of F1 on SQuAD 2.0. Data for both SQuAD 1.1 and SQuAD 2.0 are available on \\href{https://rajpurkar.github.io/SQuAD-explorer/}{https://rajpurkar.github.io/SQuAD-explorer/}.", "cites": [439, 6931], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes the SQuAD 2.0 dataset and its differences from SQuAD 1.1, including the addition of unanswerable questions. While it mentions performance comparisons and some model behavior, it does not deeply synthesize the cited papers or provide broader analytical insights. The critical evaluation is minimal, and abstraction is largely absent."}}
{"id": "845b0dcb-8923-4f6e-a1e2-d0a62e90850b", "title": "MS MARCO", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "MS MARCO"]], "content": "MS MARCO  is a large-scale machine reading comprehension dataset containing unanswerable questions. The dataset consists of 1,010,916 questions and answers collected from Bing's search query logs. Besides, the dataset contains 8,841,823 paragraphs extracted from 3,563,535 Web documents retrieved by Bing, which provide the information for answering questions. MS MARCO contains three different tasks: (1) Identify unanswerable questions; (2) Answer the question if it is answerable; (3) Rank a set of retrieved passages given a question . The MRC model needs to estimate whether these paragraphs contain correct answers, and then sort them depending on how close they are to the answers. The dataset and leaderboard of MS MARCO are available on \\href{http://www.msmarco.org/}{http://www.msmarco.org/}.", "cites": [446], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a factual description of the MS MARCO dataset based primarily on a single cited paper. It lists the dataset's size, content, tasks, and availability without synthesizing insights from multiple sources or offering critical evaluation or broader abstraction. The narrative remains surface-level and lacks analytical depth or comparison with other datasets."}}
{"id": "b6b86438-b6f3-4fa0-857c-68e89c69cce5", "title": "DuoRC", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "DuoRC"]], "content": "DuoRC  is a MRC dataset which contains 186,089  question-answer pairs generated from 7,680 pairs of movie plots. Each pair of movie plots reflects two versions of the same movie: one from Wikipedia and the other from IMDb. The texts of these two versions are written by two different authors. In the process of building question-answer pairs, the authors require crowd workers to create questions from one version of the story and a different set of crowd workers to extract or synthesize answers from another version. This is the unique feature of DuoRC in which there is almost no vocabulary overlap between the two versions. Additionally, the narrative style of the paragraphs generated from the movie plots (compare to the typical descriptive paragraphs in the existing dataset) indicates the need for complex reasoning of events in multiple sentences . DuoRC is a challenging dataset, and the authors observed that the state-of-the-art model on the SQuAD 1.1  also performed poorly on DuoRC, with F1 score of 37.42\\% while 86\\% on SQuAD 1.1. The dataset, paper and, leaderboard of DuoRC can be obtained at \\href{https://duorc.github.io/}{https://duorc.github.io/}.", "cites": [439, 446, 1147], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a factual description of the DuoRC dataset, highlighting its structure, generation process, and performance metrics. It mentions the SQuAD 1.1 paper but only in the context of benchmark comparison without deeper synthesis or analysis. There is little abstraction or critical evaluation of the dataset's design or implications for MRC research."}}
{"id": "c714aa3b-6a76-42af-94c2-d142cfdef004", "title": "Who-did-What", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "Who-did-What"]], "content": "The Who-did-What  dataset contains more than 200,000 fill-in-the-gap (cloze) multiple-choice reading comprehension questions constructed from the LDC English Gigaword newswire corpus. Compared to other existing machine reading comprehension datasets, such as CNN/Daily Mail , the Who-did-What dataset avoided using the same article summaries to create a sample in the dataset. Instead, each sample is formed by two separate articles. One article is given as the passage to be read and the other article on the same events is used to form the question. Second, the authors avoided anonymization — each choice is a person named entity. Third, the questions that can be easily solved by simple baselines have been removed, while humans can still solve 84\\% of the questions . The dataset and leaderboard of Who-did-What are available on \\href{https://tticnlp.github.io/who_did_what/index.html}{https://tticnlp.github.io/who\\_did\\_what/index.html}.", "cites": [1143, 1140], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes the Who-did-What dataset, its construction, and key features, but does so in a largely factual and summary-driven manner. It integrates minimal information from the cited papers beyond stating their titles and abstracts. There is little critical evaluation or abstraction to broader patterns or principles in MRC dataset design."}}
{"id": "7eaeb509-b35c-4781-a61b-9e36cb9a6d17", "title": "ARC", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "ARC"]], "content": "AI2 Reasoning Challenge (ARC)  is a MRC dataset and task to encourage AI research in question answering that requires deep reasoning. To finish the ARC task, the MRC model requires far more powerful knowledge and reasoning than previous challenges such as SQuAD  or SNLI . The ARC dataset contains 7,787 elementary-level scientific questions that are in the form of multiple-choices. The dataset is divided into a Challenge Set and an Easy Set, where the Challenge Set only contains questions that are not correctly answered by both a retrieval-based algorithm and a word co-occurrence algorithm. The ARC dataset contains only natural, primary-level science questions (written for the human exam) and is the largest collection of such datasets. The authors tested several baselines on the Challenge Set, including state-of-the-art models from the SQuAD and SNLI, and found that none of them were significantly better than the random baseline, reflecting the difficulty of the task. The author also publishes the ARC corpus, which is a corpus of 14M scientific sentences related to this task, and the implementation of three neural baseline models tested . Information about the ARC dataset and leaderboards is available on \\href{http://data.allenai.org/arc/}{http://data.allenai.org/arc/}.", "cites": [444, 1174, 439, 6931], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual overview of the ARC dataset, integrating key details from the primary paper and briefly referencing related datasets like SQuAD and SNLI. While it makes basic comparisons (e.g., difficulty of ARC vs. others), it lacks deeper analysis of the methods or limitations. The discussion remains concrete and descriptive without offering broader conceptual insights or a nuanced critical evaluation."}}
{"id": "0315c53d-14c0-432d-8c5e-08c84bd41706", "title": "MCScript", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "MCScript"]], "content": "MCScript  is a large-scale MRC dataset with narrative texts and questions that require reasoning using commonsense knowledge. The dataset focuses on narrative texts about everyday activities, and the commonsense knowledge are required to answer multiple-choice questions based on these texts. The feature of the MCScript dataset is to evaluate the contribution of script knowledge to machine understanding. A script is a series of events (also called scenarios) that describe human behavior. The MCScript dataset also forms the basis of a shared task on commonsense and script knowledge organized at SemEval 2018 . The official web page and CodaLab competition page of the SemEval 2018 Shared Task 11 are available on \\href{https://competitions.codalab.org/competitions/17184}{https://competitions.codalab.org/competitions/17184}.", "cites": [1147], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.0, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a basic description of the MCScript dataset and its focus on commonsense reasoning in narrative texts but does not effectively synthesize or integrate information from the cited paper. It lacks critical analysis of the dataset's strengths or weaknesses and offers no abstraction or generalization to broader trends or principles in MRC."}}
{"id": "dea4069b-01ff-4249-a32e-f6e6a353a5be", "title": "OpenBookQA", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "OpenBookQA"]], "content": "OpenBookQA  consists of about 6,000 elementary level science questions in the form of multi-choice (4,957 training sets, 500 validation sets, and 500 test sets). Answering the questions in OpenBookQA requires broad common knowledge. OpenBookQA also requires a deeper understanding of both the topic (in the context of common knowledge) and the language it is expressed in . The baseline model provided by the authors has reached about 50\\% in this dataset, but many state-of-the-art pre-trained QA methods perform surprisingly even worse . Dataset and leaderboard of OpenBookQA are available on \\href{https://leaderboard.allenai.org/open_book_qa/}{https://leaderboard.allenai.org/open\\_book\\_qa/}.", "cites": [1147], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the OpenBookQA dataset and its characteristics, such as the number of questions and the need for common knowledge. However, it largely paraphrases information from the cited paper without deeper synthesis or critical evaluation. There is no abstraction to broader trends or principles in MRC datasets."}}
{"id": "d0cedd5c-4c32-4036-9788-41cd0f6837c0", "title": "ReCoRD", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "ReCoRD"]], "content": "ReCoRD  is a large-scale MRC dataset that requires deep commonsense reasoning. Experiments on the ReCoRD show that the performance of the state-of-the-art MRC model lags far behind human performance. The ReCoRD represents the challenge of future research to bridge the gap between human and machine commonsense reading comprehension. The ReCoRD dataset contains more than 120,000 queries from over 70,000 news articles. Each query has been verified by crowd workers . The feature of the data set is that all queries and paragraphs in the records are automatically mined from news articles, which minimizes the artificially induced bias. So most records need deep commonsense reasoning. Since July 2019, the ReCoRD has been added to SuperGLUE as an evaluation suite. The ReCoRD dataset and leaderboard are available on \\href{https://sheng-z.github.io/ReCoRD-explorer/}{https://sheng-z.github.io/ReCoRD-explorer/}.", "cites": [1176], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of the ReCoRD dataset and its characteristics, largely paraphrasing the abstract of the cited paper. It mentions the dataset's size, task requirements, and inclusion in SuperGLUE but lacks deeper synthesis with other works or critical evaluation of its strengths and limitations. There is minimal abstraction or generalization beyond the specific dataset description."}}
{"id": "f6f2d3f5-b329-4750-b5a8-2088b26e84a8", "title": "CommonSenseQA", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "CommonSenseQA"]], "content": "CommonSenseQA  is a MRC dataset that requires different types of commonsense knowledge to predict the correct answer. It contains 12,247 questions. The CommonSenseQA dataset is split into a training set, validation set and, test set. The authors performed two types of splits: \"Random split\" which is the main evaluation split, and \"Question token split\" where each of the three sets has disjoint question concepts . To capture common sense beyond association, the authors of CommonSenseQA extracted multiple target concepts from Conceptnet 5.5  that have the same semantic relationship to a single source concept. Crowd workers were asked to propose multiple-choice questions, mention source concepts, and then distinguished each goal concept. This encouraged crowd workers to ask questions with complex semantics that often require prior knowledge . The dataset and leaderboard of CommonSenseQA are available on \\href{https://www.tau-nlp.org/commonsenseqa}{https://www.tau-nlp.org/commonsenseqa}.", "cites": [6931, 7046], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes the CommonSenseQA dataset, its structure, question generation process, and availability. It integrates minimal information from the cited papers, only mentioning ConceptNet as a source of concepts, without linking it to broader themes in MRC. There is no critical evaluation or comparison of the dataset to others, nor any abstraction to general principles or trends in commonsense reading comprehension."}}
{"id": "29471073-b04a-439f-ae91-20c838ee7896", "title": "WikiReading", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "WikiReading"]], "content": "WikiReading  is a large-scale machine reading comprehension dataset that contains 18 million instances. The dataset consists of 4.7 million unique Wikipedia articles, which means that about 80\\% of the English language Wikipedia is represented. The WikiReading dataset is composed of a variety of challenging classification and extraction subtasks, which makes it very suitable for neural network models. In the WikiReading dataset, multiple instances can share the same document, with an average of 5.31 instances per article (median: 4, maximum: 879). The most common document categories are humans, categories, movies, albums, and human settlements, accounting for 48.8\\% of documents and 9.1\\% of instances respectively. The average and median document lengths are 489.2 and 203 words . The WikiReading dataset is available on \\href{https://github.com/google-research-datasets/wiki-reading}{https://github.com/google-research-datasets/wiki-reading}.", "cites": [1142], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.0, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a factual description of the WikiReading dataset, including its size, content sources, and subtask types. However, it does not synthesize or integrate information from the cited paper (DROP), nor does it offer any critical analysis or abstraction beyond the specific dataset's statistics. The section reads like a dataset summary rather than an insightful academic survey analysis."}}
{"id": "f709eddb-90b3-4b5f-8891-c7496fcc13eb", "title": "MovieQA", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "MovieQA"]], "content": "The MovieQA  dataset is a multi-modal machine reading comprehension dataset designed to evaluate the automatic understanding of both pictures and texts. The dataset contains 14,944 questions from 408 movies. The types of questions in the MovieQA dataset are multiple-choice, and the questions range from simpler \"Who\" did \"What\" to \"Whom\", to \"Why\" and \"How\" certain events occurred. The MovieQA dataset is unique because it contains multiple sources of information-video clips, episodes, scripts, subtitles, and DVS . Download links and evaluation benchmarks of the MovieQA dataset can be obtained for free from \\href{http://movieqa.cs.toronto.edu/home/}{http://movieqa.cs.toronto.edu/home/}.", "cites": [6922], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of the MovieQA dataset and its characteristics, largely paraphrasing the cited paper without significant synthesis or comparison to other datasets. It lacks critical evaluation or broader abstraction, focusing only on basic attributes and no deeper analysis of the dataset's strengths, weaknesses, or implications for the field."}}
{"id": "f6a96887-7bd1-4749-9f2f-1dcd48c91623", "title": "COMICS", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "COMICS"]], "content": "COMICS  is a multi-modal machine reading comprehension dataset, which is composed of more than 1.2 million comic panels (120 GB) and automatic text box transcriptions. In the COMICS task, the machine is required to read and understand the text and images in the comic panels at the same time. Besides the traditional textual cloze tasks, the authors also designed two novel MRC tasks (visual cloze, and character coherence) to test the model's ability to understand narratives and characters in a given context . The dataset and baseline of COMICS are available on \\href{https://obj.umiacs.umd.edu/comics/index.html}{https://obj.umiacs.umd.edu/comics/index.html}.", "cites": [6923], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section on COMICS primarily describes the dataset and its novel MRC tasks without synthesizing ideas from the cited paper or placing them in a broader context. It lacks critical evaluation of the work and does not abstract to general principles or trends in MRC research."}}
{"id": "4f6aad91-2e01-4823-8f6f-7d4c15f4e549", "title": "TQA", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "TQA"]], "content": "The TQA  (Textbook Question Answering) challenge encourages multi-modal machine reading (M3C) tasks. Compared with Visual Question Answering (VQA) , the TQA task provides the multi-modal context and question-answer pair which consists of text and images. TQA dataset is constructed from the science curricula of middle school. The textual and diagrammatic content in middle school science reference fairly complex phenomena that occur in the world. Many questions need not only simple search, but also complex analysis and reasoning of multi-mode context.\\\\\nThe TQA dataset consists of 1,076 courses and 26,260 multi-modal questions . The analysis shows that a high proportion of questions in the TQA dataset require complex text analysis, graphing, and reasoning, which indicates that the TQA dataset is related to previous machine understanding and VQA dataset  The TQA dataset and leaderboards are available on \\href{http://vuchallenge.org/tqa.html}{http://vuchallenge.org/tqa.html}.", "cites": [7796], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the TQA dataset, its purpose, and its characteristics. It makes a minimal connection to the cited VQA paper by contrasting the two tasks, but this synthesis is superficial. There is no critical evaluation or deeper abstraction to broader trends or principles in multi-modal MRC."}}
{"id": "bbd2023f-3333-4033-b624-1421e90e5982", "title": "RecipeQA", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "RecipeQA"]], "content": "RecipeQA  is a MRC dataset for multi-modal comprehension of recipes. It consists of about 20K instructional recipes with both texts and images and more than 36K automatically generated question-answer pairs. RecipeQA is a challenging multi-modal dataset for evaluating reasoning on real-life cooking recipes. The RecipeQAtask consists of many specific tasks. A sample in RecipeQA contains a multi-modal context, such as headings, descriptions, or images. To find an answer, the model needs (i) a joint understanding of the pictures and texts; (ii) capturing the temporal flow of events; and (iii) understanding procedural knowledge . The dataset and leaderboard of RecipeQA are available on \\href{http://hucvl.github.io/recipeqa}{http://hucvl.github.io/recipeqa}.", "cites": [6921], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic factual description of the RecipeQA dataset, outlining its components, tasks, and availability. It does not synthesize information from multiple sources, nor does it engage in critical analysis or comparison with other datasets. There is no abstraction or identification of broader trends or principles in MRC datasets."}}
{"id": "5c881195-6c31-4585-a109-6cc530263150", "title": "HotpotQA", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "HotpotQA"]], "content": "HotpotQA  is a multi-hop MRC dataset with multi-paragraphs. There are 113k Wikipedia-based QA pairs in HotpotQA. Different from other MRC datasets, In the HotpotQA, the model is required to perform complex reasoning and provide explanations for answers from multi-paragraphs. HotpotQA has four key features: (1) the questions require the machine to read and reason over multiple supporting documents to find the answer; (2) The questions are diverse and not subject to any pre-existing knowledge base; (3) The authors provided sentence-level supporting facts required for reasoning; (4) The authors offered a new type of factoid comparison questions to test QA systems’ ability to extract relevant facts and perform necessary comparison . Dataset and leaderboard of HotpotQA are publicly available on \\href{https://hotpotqa.github.io/}{https://hotpotqa.github.io/}.", "cites": [1147], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the HotpotQA dataset, highlighting its four key features. However, it does not effectively synthesize or integrate information from the cited paper, which is unrelated to HotpotQA. There is no critical analysis or comparison of the dataset with others, and no broader patterns or principles are identified. The content remains largely factual and lacks deeper insight."}}
{"id": "52740acf-d679-4ba5-8c22-a4b6ef07b80c", "title": "NarrativeQA", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "NarrativeQA"]], "content": "NarrativeQA  is a multi-paragraph machine reading comprehension dataset and a set of tasks. To encourage progress on deeper comprehension of language, the authors designed the NarrativeQA dataset. Unlike other datasets in which the questions can be solved by selecting answers using superficial information, in the NarrativeQA, the machine is required to answer questions about the story by reading the entire book or movie script. In order to successfully answer questions, the model needs to understand the underlying narrative rather than relying on shallow pattern matching or salience . NarrativeQA is available on \\href{https://github.com/deepmind/narrativeqa}{https://github.com/deepmind/narrativeqa}.", "cites": [441], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the NarrativeQA dataset and its purpose, citing the relevant paper to highlight its focus on deeper comprehension. However, it lacks synthesis of broader themes or connections to other datasets, offers minimal critical evaluation beyond stating the paper’s motivation, and does not abstract the information into general principles or patterns within MRC datasets."}}
{"id": "b449c6d3-f5b8-4e97-96d0-083cbecc2374", "title": "Qangaroo", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "Qangaroo"]], "content": "Qangaroo  is a multi-hop machine reading comprehension dataset. Most reading comprehension methods limit themself to questions that can be answered using a single sentence, paragraph, or document . Therefore, the authors of Qangaro proposed a new task and dataset to encourage the development of text understanding models across multiple documents and to study the limitations of existing methods. In the Qangaroo task, the model is required to seek and combine evidence – effectively performing multihop,\nalias multi-step, inference . The dataset, papers, and leaderboard of Qangaroo are publicly available on \\href{http://qangaroo.cs.ucl.ac.uk/index.html}{http://qangaroo.cs.ucl.ac.uk/index.html}.", "cites": [1143], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the Qangaroo dataset and its purpose, but it lacks deeper synthesis with other works, critical evaluation of its strengths or weaknesses, and abstraction to broader trends or principles in MRC datasets. It primarily summarizes the dataset and task proposed in the cited paper without integrating it into a larger context or offering analysis."}}
{"id": "c973b7e6-a198-4fdb-887a-731b0988e650", "title": "MultiRC", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "MultiRC"]], "content": "MultiRC (Multi-Sentence Reading Comprehension)  is a MRC dataset in which questions can only be answered by considering information from multiple sentences. The purpose of creating this dataset is to encourage the research community to explore more useful methods than complex lexical matching. MultiRC consists of about 6,000 questions from more than 800 paragraphs across 7 different areas (primary science, news, travel guides, event stories, etc.) . MultiRC is available on \\href{http://cogcomp.org/multirc/}{http://cogcomp.org/multirc/}. Since May 2019, MultiRC is part of SuperGLUE, so the authors will no longer provide the leaderboard on the above website.", "cites": [1147], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the MultiRC dataset without effectively synthesizing or connecting it to the cited paper, which is unrelated to MultiRC. It lacks critical evaluation or comparative analysis and does not abstract broader principles or trends in MRC datasets. The insight quality is minimal, focusing only on factual information."}}
{"id": "daf8fbb6-d754-45fc-ba6d-e9fc7e11a696", "title": "CNN/Daily Mail", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "CNN/Daily Mail"]], "content": "In order to solve the problem of lack of large-scale datasets, Hermann et al.  created a new dataset generation method that provided a large-scale supervised reading comprehension dataset in 2015. They also extracted text from the websites of CNN and Daily Mail and created two MRC datasets, which is the CNN/Daily Mail  dataset. In the CNN dataset, there are 90,266 documents and 380,298 questions. The Daily Mail dataset consist of 196,961 documents and 879,450 questions. The creation of the CNN/Daily Mail dataset allows the community to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure . The CNN/Daily Mail dataset and related materials are available on \\href{https://github.com/deepmind/rc-data}{https://github.com/deepmind/rc-data}.", "cites": [1140], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of the CNN/Daily Mail dataset and its creation by Hermann et al., but it lacks deeper synthesis with other works or broader context. There is no critical evaluation of the dataset's limitations or comparisons with other MRC datasets. The abstraction level is minimal, focusing only on the specific details of this dataset."}}
{"id": "9ab297b5-f0b7-465f-9675-b6b4694e18b7", "title": "BookTest", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "BookTest"]], "content": "The BookTest  is a large-scale MRC dataset with 14,140,825 training examples and 7,917,523,807 tokens. The BookTest dataset is derived from books available through the project Gutenberg . The training dataset contains the original CBT NE and CN data  and extends the new NE and CN examples. The authors of BookTest extracted 10,507 books for NE instances from the project Gutenberg and also used 3,555 copyright-free books to extract CN instances . The BookTest dataset can be downloaded from \\href{https://ibm.biz/booktest-v1}{https://ibm.biz/booktest-v1}.", "cites": [6932], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a factual description of the BookTest dataset, including its size, source (Project Gutenberg), and how it extends the CBT data. However, it lacks synthesis with other datasets or deeper connections to the broader MRC literature. There is no critical evaluation of the dataset’s strengths or weaknesses, and no abstraction or generalization to broader principles or trends in MRC dataset development."}}
{"id": "bd5cae26-6818-4e37-8706-44ac877abf37", "title": "CuratedTREC", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "CuratedTREC"]], "content": "The CuratedTREC  dataset is a curated version of the TREC corpus . The Text REtrieval Conference (TREC)  was started in 1992 by the U.S. Department of Defense and the National Institute of Standards and Technology (NIST). Its purpose was to support the research of the information retrieval system. The large version of CuratedTREC is based on the QA tasks of TREC 1999, 2000, 2001 and 2002 which have been curated by Baudiš and JŠedivý  and contains a total of 2,180 questions. CuratedTREC is also used to evaluate the ability of the machine reading comprehension model to answer open-domain questions . The TREC corpus is available in \\href{https://github.com/brmson/dataset-factoid-curated}{https://github.com/brmson/dataset-factoid-curated}.", "cites": [6934], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a basic factual description of the CuratedTREC dataset, its origin, and purpose without synthesizing or connecting it to the cited paper (Weaver). There is no critical evaluation or comparison of the dataset with others, nor any abstraction to broader trends or principles in MRC datasets."}}
{"id": "72ced4e3-feba-4b42-839c-75859f49d422", "title": "Quasar", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "Quasar"]], "content": "Quasar  is a MRC dataset for open-domain questions, it contains two sub-datasets: Quasar-T and Quasar-S. Quasar is designed to evaluate the model's ability of understanding natural language queries and extract answers from large amounts of texts. The Quasar-S dataset consists of 37,000 cloze-style questions, and the Quasar-T dataset contains 43,000 open-domain trivia issues questions. ClueWeb09  serves as a background corpus for extracting these answers. \nThe Quasar dataset is a challenge to two related sub-tasks of the factoid questions: (1) searching for relevant text segments containing the correct answers to the query, and (2) reading the retrieved passages to answer the questions . The dataset and paper of Quasar are available on \\href{https://github.com/bdhingra/quasar}{https://github.com/bdhingra/quasar}.", "cites": [6930], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic factual description of the Quasar dataset and its components but lacks synthesis with other datasets or deeper analysis. There is no critical evaluation of the dataset's strengths or weaknesses, nor any abstraction to broader trends in MRC benchmarking."}}
{"id": "2561eeeb-0c6b-41c3-95dc-662440b24371", "title": "SearchQA", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "SearchQA"]], "content": "SearchQA  is a MRC dataset with retrieval systems. To answer open-domain questions in SearchQA, the model needs to read the text retrieved by the search engine, so it can also be regarded as a machine reading comprehension dataset. The question-answer pairs in the SearchQA dataset are all collected from the J!Archive, and the context is retrieved from Google. SearchQA consists of more than 140k QA pairs, with an average of 49.6 clips per pair. Each QA environment tuple in SearchQA comes with additional metadata, such as the URL of the fragment, which the authors believe will be a valuable resource for future research. The authors perform a manual evaluation on SearchQA and test two baseline methods, one simple word selection, and another deep learning . The paper suggests that the SearchQA can be obtained at \\href{https://github.com/nyu-dl/SearchQA}{https://github.com/nyu-dl/SearchQA}.", "cites": [8304], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides basic factual information about the SearchQA dataset, including its origin, structure, and baseline evaluations, but lacks synthesis with other datasets or deeper analysis. It does not critically evaluate the dataset's design or limitations, nor does it generalize to broader trends in MRC dataset development."}}
{"id": "ec8d063a-b6d8-42ba-931e-17064c3a9a77", "title": "SciQ", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "SciQ"]], "content": "SciQ  is a domain-specific multiple-choice MRC dataset containing 13.7K crowdsourced science questions about Physics, Chemistry, and Biology, etc. The context and questions are derived from real 4th and 8th-grade exam questions. The questions are in the form of multiple-choices, with an average of four choices for each question. For the majority of the questions, an additional paragraph with supporting evidence for the correct answer is provided. In addition, the authors proposed a new method for generating domain-specific multiple-choice MRC datasets from crowd workers . The SciQ dataset can be downloaded at \\href{http://data.allenai.org/sciq/}{http://data.allenai.org/sciq/}.", "cites": [444], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the SciQ dataset but lacks synthesis with other datasets or methods discussed in the survey. It does not critically analyze the dataset or its generation method, nor does it abstract broader trends or principles in domain-specific MRC datasets. The mention of a related paper (ARC) is superficial and not integrated into a deeper discussion."}}
{"id": "9b7ffc9d-cc14-4c7f-a493-1153d8daea52", "title": "CliCR", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "CliCR"]], "content": "CliCR  is a cloze MRC dataset in the medical domain. There are approximately 100,000 cloze questions about the clinical case reports. The authors applied several baselines and a state-of-the-art neural model and observed the performance gap (20\\% F1) between the human and the best neural models . They also analyzed the skills required to correctly answer the question and explained how the model's performance changes based on the applicable skills, and they found that reasoning using domain knowledge and object tracking is the most frequently needed skill, and identifying missing information and spatiotemporal reasoning is the most difficult for machines . The code of the baseline project can be publicly available on \\href{https://github.com/clips/clicr}{https://github.com/clips/clicr}, where the author claims that the CliCR dataset can be obtained by contacting the author via email.", "cites": [1147], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 2.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a factual description of the CliCR dataset, including its purpose, size, and the findings of the authors. However, it does not synthesize information from multiple sources, lacks critical evaluation of the dataset or the cited work, and offers no broader abstraction or meta-level insight into trends in MRC datasets."}}
{"id": "697d5db3-803d-4ab4-a3b8-ebe6d43aeb36", "title": "ReviewQA", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "ReviewQA"]], "content": "ReviewQA  is a domain-specific MRC dataset about hotel reviews. ReviewQA contains over 500,000 natural questions and 100,000 hotel reviews. The authors hope to improve the relationship understanding ability of the machine reading comprehension model by constructing the ReviewQA dataset. Each question in ReviewQA is related to a set of relationship understanding capabilities that the model is expected to master . The ReviewQA dataset, summary of the tasks, and results of models are available on \\href{https://github.com/qgrail/ReviewQA/}{https://github.com/qgrail/ReviewQA/}.", "cites": [6920], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal description of the ReviewQA dataset, stating its size, purpose, and availability. It cites the source paper but does not synthesize or connect ideas beyond a superficial level. There is no critical evaluation or comparison of the dataset to others, nor any abstraction to broader trends in MRC datasets."}}
{"id": "2900a77c-2973-4c41-a5b3-832be28df26b", "title": "DROP", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "DROP"]], "content": "DROP  is an English MRC dataset that requires the Discrete Reasoning Over the content of Paragraphs. The DROP dataset contains 96k questions created by crowd workers. Unlike the existing MRC task, in the DROP, the MRC model is required to resolve references in a question, and perform discrete operations on them (such as adding, counting, or sorting) . These operations require a deeper understanding of the content of paragraphs than what was necessary for prior datasets . The dataset of DROP can be downloaded at \\href{https://s3-us-west-2.amazonaws.com/allennlp/datasets/drop/drop_dataset.zip}{https://s3-us-west-2.amazonaws.com/allennlp/datasets/drop/drop\\_dataset.zip}. The Leaderboard is available on \\href{https://leaderboard.allenai.org/drop}{https://leaderboard.allenai.org/drop}.", "cites": [1142], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the DROP dataset, including its purpose, size, and requirements for discrete reasoning. It cites one paper and paraphrases its key features but does not integrate ideas from other sources, offer critical analysis of the dataset, or generalize to broader trends in MRC research. As such, it is primarily descriptive with minimal insight."}}
{"id": "d64bb789-1507-4f17-9eaf-57be3c2530b5", "title": "Google MC-AFP", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "Google MC-AFP"]], "content": "Google MC-AFP  is a MRC dataset which has about 2 million examples. It is generated from the AFP portion of LDC’s English Gigaword corpus . The authors of MC-AFP also provided a new method for creating large-scale MRC datasets using paragraph vector models. In the MC-AFP, the upper limit of accuracy achieved by human testers is approximately 91\\%. Among all models tested by the authors, the authors' hybrid neural network architecture achieves the highest accuracy of 83.2\\%. The remaining gap to the human-performance ceiling provides enough room for future model improvements . Google MC-AFP is available on \\href{https://github.com/google/mcafp}{ https://github.com/google/mcafp}.", "cites": [6929], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the Google MC-AFP dataset and its key characteristics, such as size, generation method, and performance benchmarks. However, it does not synthesize ideas from other papers, lacks critical evaluation of the method or results, and offers no abstraction or broader insights into trends in MRC dataset creation or evaluation."}}
{"id": "56ec89f2-6c3c-43c2-8a20-52d7ae8acce5", "title": "LAMBADA", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "LAMBADA"]], "content": "The main task of the LAMBADA  is to read the text and predict the missing last word. The authors of LAMBADA hoped to encourage the development of new models capable of genuine understanding of broad context in natural language text , therefore, it can also be understood as a MRC task. The LAMBADA dataset consists of narrative passages in which human subjects could guess the last word if they read the whole paragraph, but not if they only read the last sentence preceding the answer word. In order to get high scores in LAMBADA, the models have to track information in a wider discourse. For the above reasons, LAMBADA as a challenging dataset and exempliﬁes a wide range of linguistic phenomena . LAMBADA can be obtained at \\href{https://zenodo.org/record/2630551}{ https://zenodo.org/record/2630551}.", "cites": [6933], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the LAMBADA dataset and its purpose, largely paraphrasing the abstract of the cited paper. It mentions the task and the challenge of requiring broader discourse understanding but does not synthesize this information with other datasets or tasks in MRC. There is minimal critical analysis or abstraction to broader principles in MRC research."}}
{"id": "19a81e74-408b-4a49-a554-88a9b0838b2c", "title": "SQuAD 1.1", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "SQuAD 1.1"]], "content": "The Stanford Question Answering Dataset (SQuAD)  is a well-known machine reading comprehension dataset that contains more than 100,000 questions generated by crowd-workers, in which the answer of each question is a segment of text from the related paragraph . Since it was released in 2016, SQuAD 1.1 quickly became the most widely used MRC dataset. Now it has been updated to SQuAD 2.0 . In the leaderboards of SQuAD 1.1 and SQuAD 2.0, we have witnessed the birth of a series of state-of-the-art neural models, such as BiDAF , BERT , RoBERTa  and XLNet , etc.  The data and leaderboard of SQuAD 1.1 and SQuAD 2.0 are available on \\href{https://rajpurkar. github.io/SQuAD-explorer/}{https://rajpurkar. github.io/SQuAD-explorer/}.", "cites": [439, 826, 1139, 7, 11, 6931], "cite_extract_rate": 1.0, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic factual description of SQuAD 1.1 but lacks synthesis of ideas from the cited papers. It mentions models like BERT and BiDAF without explaining their relevance to the dataset or comparing their contributions. There is no critical analysis or abstraction to broader trends in MRC research."}}
{"id": "56b80179-e1fe-4ae2-b195-6a42b1e1e78f", "title": "RACE", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "RACE"]], "content": "RACE  is a MRC dataset collected from the English exams for Chinese students. There are approximately 28,000 articles and 100,000 questions provided by humans (English teachers), covering a variety of carefully designed topics to test students' understanding and reasoning ability. Different from the existing MRC dataset, the proportion of questions that need reasoning ability in RACE is much large than other MRC datasets, and there is a great gap between the performance of the state-of-the-art models (43\\%) and the best human performance (95\\%) . The authors hope that this new dataset can be used as a valuable resource for machine understanding research and evaluation . The dataset of RACE is available on \\href{http://www.cs.cmu.edu/~glai1/data/race/}{http://www.cs.cmu.edu/~glai1/data/race/}.The baseline project is available on \\href{https://github.com/qizhex/RACE_AR_baselines}{https://github.com/qizhex/RACE\\_AR\\_baselines}.", "cites": [8304], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a basic description of the RACE dataset, mentioning its origin, content, and performance gaps, but fails to synthesize or integrate information meaningfully from the cited paper. There is no critical evaluation or comparison of RACE with other datasets like SearchQA. The content remains at a concrete level without abstracting broader patterns or insights."}}
{"id": "0447d8c3-a6fb-45a9-a443-ac3c6dc76753", "title": "TriviaQA", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "TriviaQA"]], "content": "TriviaQA  is a challenging MRC dataset, which contains more than 650k question-answer pairs and their evidence. TriviaQA has many advantages over other existing MRC datasets: (1) relatively complex combinatorial questions; (2) considerable syntactic and lexical variability between the questions and the related passages; (3) more cross sentence reasoning is required to answer the question . The TriviaQA dataset and baseline project are available on \\href{http://nlp.cs.washington.edu/triviaqa/}{http://nlp.cs.washington.edu/triviaqa/} and information about the Codalab competition of TriviaQA is available on \\href{https://competitions.codalab.org/competitions/17208}{https://competitions.codalab.org/competitions/17208}.", "cites": [1098], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a basic description of the TriviaQA dataset, listing its characteristics and linking to external resources, but does not integrate or synthesize insights from the cited paper (CoQA). There is no critical evaluation or comparison with other datasets, nor any abstraction to broader trends or principles in MRC datasets."}}
{"id": "84916411-bdc7-4c9b-a6fa-5ad256c5bc96", "title": "CLOTH", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "CLOTH"]], "content": "CLOTH  is a large-scale cloze MRC dataset with 7,131 passages and 99,433 questions collected from English examinations. CLOTH requires a deeper language understanding of multiple aspects of natural language including reasoning, vocabulary and grammar. In addition, CLOTH can be used to evaluate language models' abilities in modeling long text . CLOTH's leaderboard is available on \\href{http://www.qizhexie.com/data/CLOTH_leaderboard}{ http://www.qizhexie.com/data/CLOTH\\_leaderboard} and dataset can be downloaded from \\href{http://www.cs.cmu.edu/~glai1/data/cloth/}{http://www.cs.cmu.edu/~glai1/data/cloth/}. The code of baseline project can be downloaded at \\href{https://github.com/qizhex/Large-scale-Cloze-Test-Dataset-Created-by-Teachers}{https://github.com/qizhex/Large-scale-Cloze-Test-Dataset-Created-by-Teachers}.", "cites": [1147], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.0, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides only factual descriptions of the CLOTH dataset, listing its size, purpose, and availability without synthesizing it with the cited paper or other datasets. There is no critical analysis or abstraction beyond the specific details of CLOTH, making it purely descriptive in nature."}}
{"id": "ad45bf93-fac1-42ec-ac6e-65a391fc4ead", "title": "ProPara", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "ProPara"]], "content": "ProPara  is a MRC dataset for understanding contexts about processes (such as photosynthesis). In the ProPara task, the model is required to identify the actions described in the procedural text and tracking the state changes that have occurred to the entities involved. The ProPara dataset contains 488 paragraphs and 3,300 sentences (about 81,000 notes) generated by crowd workers. The purpose of creating ProPara is to predict the presence and location of each participant based on the sentences in the context . \nThe dataset of Propara can be downloaded from \\href{http://data.allenai.org/propara}{http://data.allenai.org/propara}, and the leaderboard of Propara is available on \\href{https://leaderboard.allenai.org/propara/submissions/public}{https://leaderboard.allenai.org/propara/submissions/public}.", "cites": [1176], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a basic factual description of the ProPara dataset, focusing on its purpose, content, and availability. It cites one paper (ReCoRD), but there is no meaningful synthesis or connection between ProPara and the cited work. There is also no critical analysis or abstraction beyond the specific dataset described."}}
{"id": "01d577fe-1a87-4317-8c6e-14e728f69805", "title": "DREAM", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "DREAM"]], "content": "DREAM  is a conversational, multiple-choice MRC dataset. The dataset was collected from English exam questions designed by human experts to evaluate the reading comprehension level of English learners. The DREAM dataset consists of 10,197 questions in the form of multiple-choice with a total of 6,444 dialogues. Compared to the existing conversational reading comprehension (CRC) dataset, DREAM is the first to focus on in-depth multi-turn multi-party dialogue understanding . In the DREAM dataset, 84\\% of answers are non-extractive, 85\\% require more than one sentence of reasoning, and 34\\% of questions involve common sense knowledge.\nDREAM's authors applied several neural models on DREAM that used surface information in the text and found that they could barely surpass rule-based methods. In addition, the authors also studied the effects of incorporating dialogue structures and different types of general world knowledge into several models on the DREAM dataset. The experimental results demonstrated the effectiveness of the dialogue structure and general world knowledge . DREAM is available on: \\href{https://dataset.org/dream/}{https://dataset.org/dream/}.", "cites": [6931], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of the DREAM dataset, including its structure, characteristics, and initial model experiments. While it integrates some details from the cited paper, it lacks deeper synthesis with other datasets or methods, offers minimal critical evaluation of the dataset's strengths or limitations, and does not abstract broader patterns or principles in conversational MRC."}}
{"id": "6b48f208-412e-4014-aadb-864226fe58e8", "title": "CoQA", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "CoQA"]], "content": "CoQA  is a conversational MRC dataset that contains 127K questions and answers from 8k dialogues in 7 different fields. Through an in-depth analysis of CoQA, the authors showed that conversational questions in CoQA have challenging phenomena that are not presented in existing MRC datasets, such as coreference and pragmatic reasoning. The authors also evaluated a set of state-of-the-art conversational MRC models on CoQA. The best F1 score achieved by those models is 65.1\\%, and human performance is 88.8\\%, indicating that there was plenty of room for future advance . Dataset and leaderboard of CoQA can be found at \\href{https://stanfordnlp.github.io/coqa/}{https://stanfordnlp.github.io/coqa/}.", "cites": [1098], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic description of the CoQA dataset, integrating some key information from the cited paper. It mentions the dataset's unique features, such as conversational questions and the need for coreference and pragmatic reasoning. However, it lacks deeper synthesis with other datasets or models, critical evaluation of the dataset's strengths and weaknesses, and abstraction to broader trends or principles in conversational MRC."}}
{"id": "47f14d1a-49e2-4886-8442-17dcd0b28a8d", "title": "ShARC", "level": "subsubsection", "subsections": [], "parent_id": "7e472e19-5b53-4159-a7cb-906b75118ace", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Benchmark Dataset"], ["subsection", "Descriptions of each MRC dataset"], ["subsubsection", "ShARC"]], "content": "ShARC  is a conversational MRC dataset. Unlike existing conversational MRC datasets, when answering questions in the ShARC, the model needs to use background knowledge that is not in the context to get the correct answer. The first question in a ShARC conversation is usually not fully explained and does not provide enough information to answer directly. Therefore, the model needs to take the initiative to ask the second question, and after the model has got enough information, it then answers the first question . The dataset, paper, and leaderboard of ShARC are available on \\href{https://sharc-data.github.io}{https://sharc-data.github.io}.", "cites": [446], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.0, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a brief factual description of the ShARC dataset, highlighting its conversational nature and the need for background knowledge. It does not synthesize or connect this information with the cited paper (MS MARCO), nor does it offer any critical analysis or abstract insights. The content remains descriptive without deeper integration or evaluation."}}
{"id": "e8367d3f-46a6-460c-8feb-ec7beaaff385", "title": "Commonsense and World Knowledge", "level": "subsubsection", "subsections": [], "parent_id": "54de8e46-434e-4a95-b3a4-64a4b26723c6", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Open Issues"], ["subsection", "What needs to be improved?"], ["subsubsection", "Commonsense and World Knowledge"]], "content": "Commonsense and world knowledge are the main bottlenecks in machine reading comprehension. Among different kinds of commonsense and world knowledge, two types of commonsense knowledge are considered fundamental for human reasoning and decision making: intuitive psychology and intuitive physics .\nAlthough there are some MRC datasets about commonsense, such as CommonSenseQA , ReCoRD , DREAM , OpenBookQA , this field is still in a very early stage. In these datasets, there is no strict division of commonsense types, nor research on commonsense acquisition methods combined with psychology.\nUnderstanding how the commonsense knowledge is acquired in the process of human growth may help to reveal the computing model of commonsense.\nObserving the world is the first step for us to acquire commonsense and world knowledge. For example, \"this book can't be put into a school bag, it's too small\" and \"this book can't be put into a schoolbag, it's too big\". In these two sentences, human beings can know from commonsense that the former \"it\" refers to a school bag, and the latter \"it\" refers to a book. But this is not intuitive for computers.\nHuman beings receive a great deal of multi-modal information in our daily life, which forms commonsense. When the given information is insufficient, we can make up the gap by predicting. Correct prediction is the core function of our commonsense.\nIn order to gain real understanding ability comparable to human beings, machine reading comprehension models must need massive data to provide commonsense and world knowledge. Algorithms are needed to get a better commonsense corpus and we need to create multi-modal MRC datasets to help machines acquire commonsense and world knowledge.", "cites": [6931, 1176, 1147], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.3, "critical": 3.0, "abstraction": 3.3}, "insight_level": "medium", "analysis": "The section integrates three cited datasets (DREAM, ReCoRD, OpenBookQA) to highlight the early stage and limited scope of commonsense knowledge in MRC. It identifies a lack of typology and psychological grounding in current datasets. While it provides some general insights into the importance of multi-modal and predictive learning for commonsense, it does not deeply critique the methodologies or limitations of the cited works, nor does it propose a novel framework."}}
{"id": "1d942829-17de-467a-a78d-20e1324ec63d", "title": "Complex Reasoning", "level": "subsubsection", "subsections": [], "parent_id": "54de8e46-434e-4a95-b3a4-64a4b26723c6", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Open Issues"], ["subsection", "What needs to be improved?"], ["subsubsection", "Complex Reasoning"]], "content": "Many of the existing MRC datasets are relatively simple. In these datasets, the answers are short, usually a word or a phrase. Many of the questions can be answered by understanding a single sentence in the context, and there are very few datasets that need multi-sentences reasoning . This shows that most of the samples in existing MRC datasets are lack of complex reasoning.\nIn addition, researchers found that after input-ablation, many of the answers in existing MRC datasets are still correct . This shows that many existing benchmark datasets do not really require the machine reading comprehension model to have reasoning skills. From this perspective, high-quality MRC datasets that need complex reasoning is needed to test the reasoning skill of MRC modals.", "cites": [6919], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview by highlighting limitations in current MRC datasets, particularly their lack of complex reasoning requirements. It synthesizes a single cited paper to support the claim about input-ablation and reasoning, but integration across multiple sources is limited. The critique is valid but not deeply nuanced, and the abstraction remains at the level of identifying a general trend rather than formulating overarching principles."}}
{"id": "851afb4c-c2d3-423d-9e0a-86225bc6b331", "title": "Robustness", "level": "subsubsection", "subsections": [], "parent_id": "54de8e46-434e-4a95-b3a4-64a4b26723c6", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Open Issues"], ["subsection", "What needs to be improved?"], ["subsubsection", "Robustness"]], "content": "Robustness is one of the key desired properties of a MRC model. Jia and Liang  found that existing benchmark datasets are overly lenient on models that rely on superficial cues . They tested whether MRC systems can answer questions that contain distracting sentences. In their experiment, a distracting sentence that contains words that overlap with the question was added at the end of the context. These distracting sentences will not mislead human understanding, but the average scores of the sixteen models on SQuAD will be significantly reduced. This shows that these state-of-the-art MRC models still rely too much on superficial cues, and there is still a huge gap between MRC and human-level reading comprehension . How to avoid the above situation and improve the robustness of MRC model is an important challenge.", "cites": [6921], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides an analytical perspective by discussing the robustness of MRC models and referencing Jia and Liang's findings. It synthesizes the idea that models rely on superficial cues and highlights the implications for model performance versus human understanding. While it identifies a problem, it lacks deeper abstraction or comparison with multiple sources to elevate the insight."}}
{"id": "cb0645c8-9f0f-4fe4-a94b-30861fac92fb", "title": "Interpretability", "level": "subsubsection", "subsections": [], "parent_id": "54de8e46-434e-4a95-b3a4-64a4b26723c6", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Open Issues"], ["subsection", "What needs to be improved?"], ["subsubsection", "Interpretability"]], "content": "In the existing MRC tasks, the model is only required to give the answer to the question directly, without explaining why it gets the answer. So it is very difficult to really understand how the model makes decisions . Regardless of whether the complete interpretability of these models is absolutely necessary, it is fair to say that a certain degree of understanding of the internal model can greatly guide the design of neural network structure in the future. In future MRC datasets, sub-tasks can be set up to let the model give the reasoning process, or the evidence used in reasoning.", "cites": [6921], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section briefly addresses the issue of interpretability in MRC models, pointing out a key limitation and suggesting potential improvements. While it cites one paper, the synthesis is minimal and not deeply connected to broader ideas in the field. The critical perspective is present in identifying the lack of interpretability as a problem and its impact on model design, but there is limited depth or comparison. The abstraction is moderate, as it identifies a general need for interpretability and proposes a high-level solution (adding reasoning sub-tasks)."}}
{"id": "10ea467c-5e50-4622-8b91-9577ab9f864f", "title": "Evaluation of the quality of MRC datasets?", "level": "subsubsection", "subsections": [], "parent_id": "54de8e46-434e-4a95-b3a4-64a4b26723c6", "prefix_titles": [["title", "A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics and Benchmark Datasets"], ["section", "Open Issues"], ["subsection", "What needs to be improved?"], ["subsubsection", "Evaluation of the quality of MRC datasets?"]], "content": "There are many evaluation metrics for machine reading comprehension models, such as F1, EM, accuracy, etc. However, different MRC datasets also need to be evaluated. How to evaluate the quality of MRC datasets? \nOne metric of MRC dataset is the readability. The classical measures of readability are based on crude approximations of the syntactic complexity (using the average sentence length as a proxy) and lexical complexity (average length in characters or syllables of words in a sentence). One of the most well-known measures along these lines is the Flesch-Kincaid readability index  which combines these two measures into a global score . \nHowever, recent studies have shown that the readability of MRC dataset is not directly related to the question difficulty .\nThe experiment results suggest that while the complexity of datasets is decreasing, the performance of MRC model will not be improved to the same extent and the correlation is quite small . Another possible metric is the frequencies of different prerequisite skills needed in MRC datasets. Sugawara et al. defined 10 prerequisite skills , including Object tracking, Mathematical reasoning, Coreference resolution, Analog, Causal relation, etc. However, the definition of prerequisite skills is often arbitrary and changeable. Different definitions can be drawn from different perspectives .\nMoreover, at present, the frequency of prerequisite skills is still manually counted, and there is no automated statistical method. In summary, how to evaluate the quality of MRC datasets is still an unsolved problem.", "cites": [6919], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of dataset evaluation in MRC, discussing readability metrics and prerequisite skill frequency. It integrates information from recent studies and identifies limitations, such as the weak correlation between readability and question difficulty and the manual nature of skill frequency counting. However, the synthesis is somewhat limited to two primary ideas and lacks a deeper, more nuanced framework or broader abstraction."}}
