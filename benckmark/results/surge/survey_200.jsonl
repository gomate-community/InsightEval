{"id": "5c6b8f54-fe87-4ed8-a412-b1d6f7d82c6a", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "ef335899-9555-4748-997e-5374356c2779", "prefix_titles": [["title", "A Survey on Knowledge Graph Embeddings with Literals:  Which model links better Literal-ly?\n"], ["section", "Introduction"]], "content": "\\label{sec:introduction}\n\\lettrine[findent=2pt]{\\textbf{K}}{ }nowledge Graphs (KGs) have become quite crucial for storing structured information. There has been a sudden attention towards using KGs for various applications mainly in the area of artificial intelligence. For instance, in a more general sense, KGs can be used to support decision making process and to improve different machine learning applications such as question answering , recommender systems , and relation extraction . Some of the most popular publicly available general purpose KGs are DBpedia , Wikidata , and YAGO . These general purpose KGs often consist of huge amount of facts constructed using \nmillions \nof entities (represented as nodes) and relations (as edges connecting these nodes). \nAlthough KGs are effective in representing structured data, there exist some issues which hinder their efficient manipulation such as i) different KGs are usually based on different rigorous symbolic frameworks and this makes it hard to utilize their data in other applications  and ii) the fact that a significant number of important graph algorithms needed for the efficient manipulation and analysis of graphs have proven to be NP-complete . In order to address these issues and use a KG more efficiently, it is beneficial to transform it into a low dimensional vector space while preserving its underlying semantics. To this end, various attempts have been made so far to learn vector representations (embeddings) for KGs. \nAs discussed in {}, a typical KG embedding approach, which uses only structured information from the KG, generally follows three steps: (i) determining the form of entity and relation representations, (ii) defining a scoring function, and (iii) learning entity and relation representations. In the first step, the forms in which entities and relations are represented in the vector space are determined. Entities can be represented as vectors or modeled as multivariate Gaussian distributions whereas relations can be encoded as operations, matrices, tensors, multivariate Gaussian distributions, or mixtures of Gaussians. Once the form of the entities are determined, in the second step, a scoring function which measures the plausibility of a triple is defined. The main goal is to enable the model to assign higher score to true triples and lower scores to false/negative/corrupted triples. Thus, in order to achieve this, the third step solves an optimization problem which maximizes the plausibility of true facts in order to learn the embeddings of entities and relations. Note that the method used to generate false/negative/corrupted triples has an impact on the performance of a model. The various negative triple generation methods and their differences are discussed in detail in {}.\nAmong the different embedding approaches proposed so far, TransE {} is, to the best of our knowledge, the very first attempt to use distance-based scoring function to learn KG embedding. Given a triple $<h,r,t>$ where $h$ and $t$ are head and tail entities respectively and $r$ is a relation, TransE represents $h$, $r$, and $t$ as vectors $\\textbf{h}$, $\\textbf{r}$, and $\\textbf{t}$ respectively by modeling the relation $r$ as a translation vector which connects the vectors $\\textbf{h}$ and $\\textbf{t}$. The problem with TransE is that it fails to model certain type of relations such as one-to-many or many-to-one. In order to address such limitations, different embedding techniques which are extension of TransE or are entirely new have been proposed. However, most of the existing approaches, including the current state-of-the-art models such as ConvE {}, are structure-based embeddings which do not make use of any literal information i.e., only triples consisting of entities connected via properties are usually considered. \nThis is a major disadvantage because information encoded in the literals will be left unused when capturing the semantics of a certain entity. \nLiterals can bring advantages to the process of learning KG embeddings in two major ways:\n\\begin{enumerate}\n    \\item \\textit{Learning embeddings for novel entities:} Novel entities are the entities which are not linked to any other entity in the KG but have literal values associated with them such as \n    their \\textit{textual description, numeric literals, and images}. \n    In most existing structure-based embedding models, it is not possible to learn embeddings for such novel entities. However, this can be addressed by utilizing the information represented in literals to learn embeddings. For example, considering the dataset FB15K-20 , which is a subset of Freebase, the entity \\texttt{'/m/0gjd61t'} is a novel entity which does not occur in any of the training triples, but it has a description given as follows in the form \\textit{<subject, relation, object>}.\n    \\begin{lstlisting}[basicstyle=\\footnotesize\\ttfamily]\n    </m/0gjd61t, http://rdf.freebase.com/ns/common.topic.description, \"Vincent Franklin is an English actor best known for his roles in comedy television programmes...\">\n    \\end{lstlisting}\n    In order to learn the embedding for this particular entity (i.e., \\texttt{/m/0gjd61t}),\n    the model can make use of the entity's textual description. \n    DKRL  is one of those approaches which provide embeddings for novel entities using their descriptions.\n    \\item \\textit{Improving the representation of entities in structure based embedding models:} Literals play a vital role in improving the representation learning where an entity is required to appear in at least a minimum number of relational triples. For example, taking into consideration only the information provided in a sample KG presented in Figure~\\ref{fig:kg}, which is extracted from DBpedia, it is not possible to tell apart the entities \n    \\texttt{dbr:Gina\\_Torres}, \\texttt{dbr:Patrick\\_J.Adams}, and \\texttt{dbr:Sarah\\_Rafferty} \n    from one another. This is the case due to the fact that the only information that is available regarding these entities in this KG is that they all star in the series \\texttt{dbr:Suits\\_(season\\_1)} and this is not enough to know which entities are similar to each other and which are not. Therefore, if some KG embedding model is trained using only this KG, it is not possible to get good representations for the entities \n    \\texttt{dbr:Gina\\_Torres}, \\texttt{dbr:Patrick\\_J.Adams}, and \\texttt{dbr:Sarah\\_Rafferty}. \n \\begin{figure}[h!]\n  \\centering\n  {\\includegraphics[width=0.45\\textwidth]{tex/ex1.jpg}}\n    \\caption{\n    A small fraction of triples taken from the KG DBpedia {}. }\n    \\label{fig:kg}\n\\end{figure}\n    However, having the model trained with more triples containing literal values for these entities, as shown in Figure \\ref{fig:kg-literal}, would improve the embeddings for the entities. \n    For instance, based on the values of the data relation\n    \\texttt{dbr:birthDate}, it is possible to deduce the fact that \\texttt{dbr:Sarah\\_Rafferty} and \\texttt{dbr:Gina\\_Torres} are almost the same age but they are both older than \\texttt{dbr:Patrick\\_J.Adams}. On the other hand, the images of the entities along with the textual descriptions (\\texttt{dbo:abstract}) would allow us to infer the entities' gender, i.e., \\texttt{dbr:Sarah\\_Rafferty} and \\texttt{dbr:Gina\\_Torres} are female and \\texttt{dbr:Patrick\\_J.Adams} is male.\n\\begin{figure*}\n  \\centering\n  {\\includegraphics[width=0.7\\textwidth]{tex/ex2.jpg}}\n  \\caption{\n  A small fraction of triples with literals taken from the KG DBpedia {}.}\n  \\label{fig:kg-literal}\n\\end{figure*}\n     The above example indicates that the use of literals along with their respective entities would add more semantics so that similar entities can be represented close to each other in the vector space while those dissimilar are further apart. \n\\end{enumerate} \nRecently, \nsome approaches have been proposed which leverage the information present in literals to learn KG embeddings. \nThe types of literals considered in these embedding methods are either text, numeric, images, or multi-modal literals, i.e., a combination of more than one medium of information. These methods use different techniques in order to incorporate the literals into the KG embeddings. However, data typed literals are not addressed in these KG embedding models and surveys that are conducted on KG embeddings. The main challenge with data typed literals, such as date and time, is that they require additional semantics to be represented in KG embeddings.\nThis survey analyses different embedding approaches, which make use of literals, and highlights their advantages and drawbacks in handling different challenges \nsuch as multi-valued data properties/relations, data typed literals, and units of literals. \nA review of the different applications used for model evaluation by different KG embedding models is also presented. \nFurthermore, experiments with some of the models have been conducted specifically on the link prediction task. \nThe contribution of this paper is summarized as follows:\n\\begin{enumerate}\n    \\item A detailed analysis of the existing literal enriched KG embedding models and their approaches. In addition, the models are categorized into different classes based on the type of literals used.\n    \\item An evaluation oriented comparison of the existing models on the link prediction task is performed under same experimental settings.\n    \\item The research gaps in the area of KG embeddings in using literals are indicated which can open directions for further research.\n\\end{enumerate}\nThe rest of this paper is organized as follows: Section~\\ref{sec:related_work} presents a brief overview of related work. In Section~\\ref{sec:problem_statement}, the problem formulation including definitions, preliminaries, types of literals and research questions are provided while Section~\\ref{sec:techniques} analyses different KG embedding techniques with literals is discussed. Section~\\ref{sec:application} reviews different tasks used to train or evaluate the embedding models is given. Section~\\ref{sec:experimentation} discusses the experiment conducted with the existing KG embedding models with literals on the link prediction task.  \nFinally, concluding remarks summarize our findings on KGs with literals and are presented along with future directions in Section~\\ref{sec:discussion}.", "cites": [1167, 4953, 4952], "cite_extract_rate": 0.23076923076923078, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates information from the cited papers to build a narrative around the importance of literals in KG embeddings, showing how existing models like TransE and ConvE fall short in utilizing such data. It demonstrates some synthesis by connecting the limitations of structure-based approaches with the potential of literal-based models. While it identifies limitations and benefits of current methods, the critical analysis is moderate and primarily focused on structural issues rather than deeper evaluation of the cited papers' contributions. The section also highlights general patterns in how literals can improve entity representation, but does not fully abstract to a meta-level theoretical framework."}}
{"id": "c831646e-b1c2-41f4-8333-3926f27a25cc", "title": "Related Work", "level": "section", "subsections": [], "parent_id": "ef335899-9555-4748-997e-5374356c2779", "prefix_titles": [["title", "A Survey on Knowledge Graph Embeddings with Literals:  Which model links better Literal-ly?\n"], ["section", "Related Work"]], "content": "\\label{sec:related_work}\nThis section describes the state-of-the-art algorithms proposed for generating KG embeddings. It also gives a brief overview of the surveys already published following these lines and what is lacking in those studies. \nA brief overview of the most popular KG embedding techniques, including the state-of-the-art approaches are short listed in Table~{\\ref{table:embeddings}}. The categories presented in this table are inspired by a previous survey work {} for the models without literals (column 1). The categories are translation based models, semantic matching models, models incorporating entity types, models incorporating relation paths, models using logical rules, models with temporal information, and models using graph structures. We have also categorized the techniques which use literals with respect to the same set of categories.  Since a detailed discussion on these categories on the models without literals has already been presented in {}, in the current study, the main focus lies on analysing the models which make use of literals. The standard KG embedding techniques which are extended by the models with literals are listed in Table~{\\ref{table:extensions}}. \n\\begin{table*}[ht!]\n    \\centering\n    \\caption{KG embedding models and their categories.} \n    \\begin{tabular}{p{4.2cm}|p{6cm}|p{4cm}}\n    \\hline\n    \\textbf{Categories} &  \\textbf{Models without literals} & \\textbf{Models with Literals}\\\\\n    \\hline\n    Translational Distance Models & TransE  and its extensions: TransH  TransR , TransD , TranSparse , TransA  etc. & \n    TransEA {}, DKRL {}, IKRL {}, Jointly(desp) {}, Jointly {}, SSP {}, KDCoE {}, EAKGAE {}\n    \\\\\n    \\hline\n    \\vspace{1mm}\n    Semantic Matching Models & \n    RESCAL  and Its Extensions: DistMult , HolE , ComplEx , and etc.\n    Semantic Matching with Neural Networks: SME , NTN , MLP , and etc. & \n    LiteralE {}, MKBE {}, MTKGNN {}, KGlove with literals {}, Extended RESCAL {},  LiteralE with blocking {} \n    \\\\ \n    \\hline \\vspace{1mm}\n    Models using Entity Types & SSE , TKRL , Type constrained representation learning , Rules incorporated KG completion models ,  TRESCAL , Entity Hierarchy Embedding  & \n    Extended RESCAL {}\n    \\\\\n    \\hline \\vspace{1mm}\n    Models using Relation Paths & PTransE , Traversing KGs in Vector Space , RTRANSE , Compositional vector space , Reasoning using RNN , Context-dependent KG embedding  & \n    KBLRN {}\n    \\\\\n    \\hline \\vspace{1mm}\n    Models using Logical Rules & Rules incorporated KG completion models , Large-scale Knowledge Base Completion , KALE , Logical Background Knowledge for Relation Extraction , and etc. & \\\\\n    \\hline \n    Models using Temporal Information & Time-Aware Link Prediction , co-evolution of event and KGs , Know-evolve  & \\\\\n    \\hline\n    Models using Graph Structures & GAKE , Link Prediction in Multi-relational Graphs  & \n    KBLRN {}\n    \\\\\n    \\hline\n    \\end{tabular}\n    \\label{table:embeddings}\n\\end{table*}\n\\begin{table*}[ht!]\n    \\centering\n    \\caption{\n    KG embedding models with literals and their corresponding base models.}\n    \\begin{tabular}{p{4cm}|p{4cm}}\n    \\hline\n    \\textbf{Models with literals} &  \\textbf{The standard models they extend} \\\\\n    \\hline\n    Extended RESCAL {} & RESCAL  \\\\ \n    Jointly(desp) {} & TransE \\\\ \n    DKRL {} & TransE \\\\ \n    Jointly {} & TransE \\\\\n    SSP {} & TransE \\\\\n    KDCoE {} & TransE \\\\\n    KGlove with literals {} & KGlove\\\\\n    LiteralE {} & DistMult , ComplEx , ConvE \\\\\n    TransEA {} & TransE  \\\\\n    IKRL {} & TransE \\\\\n    MTKGRL {} & TransE  \\\\ \n    EAKGAE {} & TransE \\\\\n    MKBE {} & DistMult , ConvE \\\\\n    \\hline\n    \\end{tabular}\n    \\label{table:extensions}\n\\end{table*}\nFew attempts have been made to conduct surveys on the techniques and applications of KG embeddings~. The survey~ is conducted on factorization based, random walk based, and deep learning based network embedding approaches such as DeepWalk, Node2vec, and etc.  discuss only RESCAL~ and KREAR~ as methods which use attributes of entities for KG embeddings, and focus mostly on the structure-based embedding methods, i.e., methods using non-attributive triples, for example, translation based embedding models listed in Table~\\ref{table:embeddings}. However, RESCAL is a matrix-factorization method for relational learning which encodes each object/data property as a slice of the tensor leading to an increase in the dimensionality of the tensor automatically. This method suffers  from efficiency issues if literals are utilized while generating KG embeddings. Similarly, KREAR only considers those data properties which have categorical values, i.e., fixed number of values and ignores those which take any random literals  as values. One of the recent surveys~ summarizes the methods proposed so far on refining KGs. However, this survey does not confine itself to embedding techniques and also does not consider most of the approaches which are making use of literals. \nAnother very recent related study {}, discusses different aspects of KG embedding models such as model architectures, training strategies, and hyperparameter optimization but it takes into consideration only those models without literals. \nNone of the surveys mentioned above include all the existing KG embedding models which make use of literals, such as the ones categorized as models incorporating information represented in literals in Table~\\ref{table:embeddings}. To the best of our knowledge, this is the first attempt to analyse the algorithms proposed so far for generating KG embeddings using literals. In this paper, discussions on the type of literals, the embedding approaches, and the applications/tasks on which the embedding models are evaluated are given. A categorization of the models based on the type of literals they use is also provided. \nThis survey is an extension of an already published short survey~. The major difference between the two versions is that (i) this survey contains a much more detailed theoretical analysis of the KG embedding models with literals proposed so far, and (ii) it performs empirical evaluation of the discussed models under the same experimental settings under the example of link prediction.", "cites": [4956, 4962, 1168, 1166, 4954, 4957, 4955, 1929, 4959, 4960, 7236, 1167, 1937, 4958, 215, 219, 4961], "cite_extract_rate": 0.35294117647058826, "origin_cites_number": 51, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a structured overview of existing KG embedding models with and without literals, using tables to categorize them. While it makes some effort to connect the models to broader categories, the synthesis remains largely at the level of classification. The critical analysis is limited, focusing on what previous surveys lack rather than deeply evaluating the cited papers. There is minimal abstraction beyond the immediate categorization of models."}}
{"id": "3a02b8ca-abfc-4d20-92fa-783e1f9990ac", "title": "Models with Text Literals", "level": "subsection", "subsections": ["f10b2fa8-d58a-431b-a6ac-de7c52d81cf0"], "parent_id": "b17a9ad2-7c37-4320-837d-0c98c36965df", "prefix_titles": [["title", "A Survey on Knowledge Graph Embeddings with Literals:  Which model links better Literal-ly?\n"], ["section", "Knowledge Graph Embeddings with Literals"], ["subsection", "Models with Text Literals"]], "content": "\\label{text}\nIn this section, seven KG embedding models utilizing text literals are discussed, namely, Extended RESCAL , \nJointly(desp) {} \n, DKRL , \nJointly {}, SSP {} \n, KDCoE , and KGloVe with literals . A detailed description followed by a summary presenting the comparison of these models is given along with their drawback. \nMoreover, in order to show the differences between the models based on complexity, the number of parameters of each model is presented in Table {\\ref{tab:complexity_text}}.", "cites": [1937, 4961], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section lists seven models that use text literals but provides minimal synthesis of their ideas. It mentions a comparison based on complexity and parameters, yet lacks a detailed discussion of connections or contrasts between the works. There is little critical evaluation or abstraction, as the narrative remains largely descriptive and does not offer deeper insights into broader patterns or principles."}}
{"id": "f10b2fa8-d58a-431b-a6ac-de7c52d81cf0", "title": "{\\bf Extended RESCAL", "level": "paragraph", "subsections": ["7af20b31-242d-4754-a333-cfa65d467966"], "parent_id": "3a02b8ca-abfc-4d20-92fa-783e1f9990ac", "prefix_titles": [["title", "A Survey on Knowledge Graph Embeddings with Literals:  Which model links better Literal-ly?\n"], ["section", "Knowledge Graph Embeddings with Literals"], ["subsection", "Models with Text Literals"], ["paragraph", "{\\bf Extended RESCAL"]], "content": "} aims to improve the original RESCAL approach by extending its algorithm to process literal values more efficiently and to deal with the drawback of sparsity that accompanies tensors. In the original RESCAL approach, relational data is modeled as a three-way tensor X of size $n \\times n \\times m$, where $n$ is the number of entities and $m$ is the number of relations.  An entry $X_{ijk} = 1$ denotes the existence of the triple with i-th entity as a subject, k-th relation as a predicate, and j-th entity as an object. If $X_{ijk}$ is set to 0, it indicates that the triple doesn't exist. A new approach for tensor factorization is proposed which is performed on X. For further details refer to . If attributive triples have to be modeled in such a way, then the literals will be taken as entities even if they cannot occur as subject in the triples.  Including literals may lead to an increment in the runtime since a larger tensor has to be factorized. \nIn contrast to the original algorithm, the extended RESCAL algorithm handles the attributive triples in a separate matrix. The matrix factorization is performed jointly with the tensor factorization of the non-attributive triples. The attributive triples containing only text literals are encoded in an entity-attribute matrix $D$ in such a way that the rows are entities and the columns are $<data\\ type\\ relation, value>$ pairs. Given a triple with a textual data type such as \\texttt{rdfs:label} or \\texttt{yago:hasPreferredMeaning}, one or more such pairs are created by tokenizing and stemming the text in the object literal. The matrix $D$ is then factorized into $D \\approx AV$ with A and V being the latent-component representations of entities and attributes respectively. Despite the advantage that this approach has for handling multi-valued literals, it does not consider the sequence of words of the literal values. Note that Extended RESCAL represents RDF(S) data in such a way that there is no distinction drawn among A-Box and T-Box, i.e., both classes and instances are modeled equally as entities in a tensor. The T-Box is rather taken as soft constraints instead of letting them impose hard constraints on the model.\n\\noindent \n{\\bf Jointly(Disp)}\nis an approach which jointly learns embeddings of KGs and a text corpus of entity descriptions, i.e, it uses an alignment model to make sure the entities, relations, and words are represented in the same vector space. This approach consists of three components, namely, knowledge model, text model, and alignment model. The knowledge model is used to capture the semantics of the structured information from the KG. Given a triple $<h,r,t>$, the model defines the plausibility of the triple, same as in {}:\n\\begin{equation} \\label{kmodel}\nPr(h|r,t) = \\frac{exp\\{z(h,r,t)\\}}{\\sum_{\\tilde{h}\\in I} exp\\{z(\\tilde{h},r,t)\\}},\n\\end{equation}\n\\noindent \nwhere $z(h,r,t) = b-0.5 \\cdot \\lVert h +r - t \\rVert_2^2$,  $b=7$.  Analogously, $Pr(r|h,t)$ and $Pr(t|h,r)$ are defined. \n\\\\\nThen, the loss function of the knowledge model is defined as follows: \n\\begin{equation}\n{\\begin{split}\nL_{K} = \\sum_{(h,r,t)}[ \\log Pr(h|r,t) + \\log Pr(t|h,r)  \\\\\n+ \\log Pr(r|h,t)].\n\\end{split}}\n\\end{equation}\nThe text model adopts the same assumption made in {} that is if two words occur in the same context then there is a relation between them. Based on this assumption, the text model defines the probability of a pair of words $w$ and $v$ co-occurring in a text window as follows:\n\\begin{equation}\nPr(w|v) = \\frac{exp\\{z(w,v)\\}}{\\sum_{\\tilde{w}\\in V} exp\\{z(\\tilde{w},v)\\}},\n\\end{equation}\n\\noindent \nwhere $z(w,v) = b-0.5 \\cdot \\lVert \\textbf{w} - \\textbf{v} \\rVert_2^2$. \nThen, the loss function of the text model is given as:\n\\begin{equation}\nL_{T} = \\sum_{(w,v)}\\log Pr(w|v).\n\\end{equation}\nThe role of the third component, the alignment model, is to put the embeddings of the entities, relations, and words into the same vector space. This submodel works by utilizing entity descriptions to align these embeddings. For every word $w$ in the description of entity $e$,  the conditional probability of predicting $w$ given $e$ is defined as :\n\\begin{equation} \\label{amodel}\nPr(w|e) = \\frac{exp\\{z(e,w)\\}}{\\sum_{\\tilde{w}\\in V} exp\\{z(e,\\tilde{w})\\}},\n\\end{equation}\n\\noindent \nwhere $z(e,w) = b-0.5 \\cdot \\lVert \\textbf{e} - \\textbf{w} \\rVert_2^2$. The entity vector $\\textbf{e}$ in Eq~{\\ref{amodel}} is the same as the entity vector appearing in Eq~{\\ref{kmodel}}, i.e., an entity has a single unified representation which captures the semantics from both the structured KG and the entity descriptions. $Pr(e|w)$ is defined analogously.\nBased on the definition given in Eq~{\\ref{amodel}}, the loss function of the alignment model is defined as:\n\\begin{equation}\nL_{A} = \\sum_{e\\in \\mathcal{E}} \\sum_{w\\in D_{e}}[\\log Pr(w|e) + \\log Pr(e|w)],\n\\end{equation}\nwhere $\\mathcal{E}$ and $D_{e}$ denote the set of entities and the description of the entity $e$ respectively.\nBy adopting the joint embedding framework in {}, the main loss of Jointly(desp) is defined as follows:\n\\begin{equation}\nL(\\{e_{i}\\}, \\{r_{j}\\}, \\{w_{l}\\}) = L_K + L_T + L_A.\n\\end{equation}\n\\noindent{\\bf DKRL} extends TransE~ by utilizing the descriptions of entities. For each entity $e$, two kinds of vector representations are learned, i.e., structure-based $e_s$ and description-based $e_d$. These two kinds of entity representations are learned simultaneously into the same vector space but not forced to be unified so that novel entities with only descriptions can be represented. In order to achieve this, given a certain triple $<h,r,t>$ the energy function of the DKRL model is defined as: \n\\begin{equation}\n\\begin{split}\n    E = ||h_s + r - t_s|| + ||h_d + r - t_d|| \\\\\n    + ||h_s + r - t_d|| + ||h_d + r - t_s||,\n\\end{split}\n\\end{equation}\nwhere $h_s$ and $t_s$ are the structure-based representations, and $h_d$ and $t_d$ are the description-based representations of their corresponding entities.\nIn order to learn structure-based representations, the TransE approach is directly applied which considers the relation in a triple as the translation from the head entity to the tail entity. On the other hand, Continuous Bag of Words (CBOW) and a deep Convolutional Neural Network (CNN) model have been used to generate the description-based representations of the head and tail entities. In case of CBOW, short text is generated from the description based on keywords and their corresponding word embeddings are summed up to generate the entity embedding. In the CNN model, after preprocessing the description, pretrained word vectors from Wikipedia are provided as input. This CNN model has five layers and after every convolutional layer pooling is applied to decrease the parameter space of CNN and filter noises. Max-pooling is applied for the first pooling and mean pooling for the last one. The activation function used is either tanh or ReLU. The CNN model works better than CBOW because it preserves the sequence of words. \nIn order to train DKRL, the following margin-based score function is considered as an objective function and minimized using a standard back propagation using stochastic gradient descent (SGD)\n\\begin{equation}\\label{eq:dkrl}\n\\begin{split}\n  L = \\sum_{(h,r,t) \\in T}\\sum_{(h',r',t') \\in T'} max(\\gamma + d(h+r, t)\\\\\n  - d(h'+ r', t'),0),  \n\\end{split}\n\\end{equation}\nwhere $\\gamma > 0$ is a margin hyperparameter, $d$ is a dissimilarity function and $T'$ is the set of corrupted triples. The representation of the entities can be either structure-based or description-based. \n\\noindent \n{\\bf Jointly} {} learns KG embeddings by leveraging entity descriptions. Specifically, it learns a joint embedding of an entity by combining its structure-based and description-based representations with a gating mechanism. The gate is used to find balance between the structure-based and the description-based representations. For a certain entity a representation can be encoded from its descriptions by converting the description into fixed length vector. In Jointly, different text encoders have been used such as bag-of-words, LSTM, and Attentive LSTM.\nFor an entity $e$, its joint representation \\textbf{e} is a linear interpolation between its structure-based representation $(\\textbf{e}_{\\textbf{s}})$ and description-based representation $(\\textbf{e}_{\\textbf{d}})$, which is defined as:\n\\begin{equation}\n   \\textbf{ e }= g_{e} \\odot \\textbf{e}_{\\textbf{s}} + (1 - g_{e}) \\odot \\textbf{e}_{\\textbf{d}},\n\\end{equation}\nwhere $\\odot$ is an element-wise multiplication and $g_{e}$ is a gate to balance the two information sources (structure and text) which is computed as $g_{e}= \\alpha(\\tilde{g_{e}})$ with $g_{e}= \\tilde{g_{e}} \\in \\mathcal{R}^d$ being real-value vector stored in a lookup table.\nThe entity descriptions are encoded using either bag-of-words, LSTM, or Attentive LSTM (ALSTM) encoders in order to generate text-based representation for the corresponding entities. On the other hand, to better model the structure-based embedidngs, entities and relations can be pre-trained with any existing KG embedding models, such as TransE.\nJointly's score function is inspired by TransE and defined as follows:\n\\begin{equation}\n   {\\begin{split}\n   f(h,r,t;d_{h}, d_{t})= \\lVert ( \\textbf{g}_{h}  \\odot \\textbf{h}_{s} + (1 - \\textbf{g}_{h}) \\\\ \n   \\odot  \\textbf{h}_{d}) + r - ( g_{t} \\odot \\textbf{h}_{t} + (1 - \\textbf{g}_{t}) \\odot \\textbf{t}_{d}) \\rVert _{2}^{2}.\n   \\end{split}}\n\\end{equation}\nwhere $\\textbf{h}_{s}$, $\\textbf{h}_{d}$, and $\\textbf{g}_h$ are the head entity's structure-based embedding, description-based embedding, and gate respectively whereas $\\textbf{t}_{s}$, $\\textbf{t}_{d}$, and $\\textbf{g}_{t}$ are the tail entity's structure-based embedding, description-based embedding, and gate respectively.\n\\noindent \n{\\bf SSP} (Semantic Space Projection) {} is a joint embedding model which learns from both structured/symbolic triples and textual descriptions. Differently from DKRL and Jointly(Desp), where first-order constraints which are weak in capturing the correlation of textual descriptions and symbolic triples are applied, SSP follows the principle that triple embedding is considered always as the main procedure and textual descriptions must interact with triples in order to learn better representation. Therefore, triple embedding is projected onto a semantic subspace such as a hyperplane to allow strong correlation by adopting quadratic constraint.\nSSP applies the following scoring function: \n\\begin{equation}\n   f_{r} (h,t)= -\\lambda \\lVert \\textbf{e} -\\textbf{s}^{T}\\textbf{es} \\rVert _{2}^{2} +  \\lVert \\textbf{e} \\rVert  _{2}^{2},\n\\end{equation}\nwhere\n\\begin{equation}\n   \\textbf{e} \\doteq \\textbf{h} + \\textbf{r} - \\textbf{t},\n\\end{equation}\nand\n\\begin{equation}\n   \\textbf{s} \\doteq \\frac{\\textbf{s}_{\\textbf{h}} + \\textbf{s}_{\\textbf{t}}}{\\lVert \\textbf{s}_{\\textbf{h}} + \\textbf{s}_{\\textbf{t}} \\rVert }. \n\\end{equation}\nNote that $\\lambda$ is a suitable hyper-parameter, \\textbf{h} and \\textbf{t} are the structure (symbolic triples) based embedding of the head and tail entities respectively, $\\textbf{s}_{\\textbf{h}}$ and $\\textbf{s}_{\\textbf{t}}$ are the semantic vectors generated from the textual descriptions of the head and tail entities respectively. SSE adopts the Non-negative Matrix Factorization (NMF) topic model to generate description-based semantic vectors for entities ($\\textbf{s}_{\\textbf{h}}$ and $\\textbf{s}_{\\textbf{t}}$), i.e., by treating each entity description as a document and taking the topic distribution of the document as the representation of the corresponding entity.\nSSP provides two different settings for training which are  referred to as \\textbf{Std} and \\textbf{Joint}. In Std, a pre-trained topic model with NMF is used to obtain description-based semantic vectors. These description-based vectors are fixed during training but the other parameters are optimized.  On the other hand, in the Joint setting the topic model is also learnt simultaneously with the KG embeddings instead of using a fixed pre-trained vectors. \n\\noindent{\\bf KDCoE} focuses on the creation of an alignment between entities of multilingual KGs by creating new Inter-Lingual Links (ILLs) based on an embedding approach which exploits entity descriptions. The model uses a weakly aligned multilingual KG for semi-supervised cross-lingual learning. It performs co-training of a multilingual KG embedding Model (KGEM) and a multilingual entity Description Embedding Model (DEM) iteratively in order for each model to propose a new ILL alternately. KGEM is composed of two components, i.e., a knowledge model and an alignment model, to learn embeddings based on structured information from the KGs (the non attributive triples). Given a set of languages $\\mathcal{L}$, a separate k$_1$-dimensional embedding space $\\mathbb{R}_L^{k_1}$ is used for each language $L \\in \\mathcal{L}$ to represent the corresponding relations $R_L$ and entities $E_L$.  In order to learn the embeddings for $R_L$ and $E_L$, the knowledge model adopts TransE and thus uses hinge loss as its objective function. On the other hand, a linear-transformation-based technique which has the best performance in case of cross-lingual inferences is adopted for the alignment model. This technique employs the following objective function:\n\\begin{equation}\\label{eq:kdcoe}\nS_A = \\sum_{(e,e') \\in I(L_i, L_j)} \\lVert M_{ij}\\textbf{e} -\\textbf{e}'  \\rVert_2,\n\\end{equation}\nwhere $I(L_i, L_j)$ is ILLs between the languages $L_i$ and $L_j$, and $M_{ij}$ is a $k_1 \\times k_1$ matrix used as a linear transformation on entity vectors from $L_i$ to $L_j$.\nLet $S_K$ be the hinge loss function used by the knowledge model, the KGEM model then minimizes $S_{KG} = S_K + \\alpha S_A$, where $\\alpha$ is a positive hyperparameter. In case of DEM model, an attentive gated recurrent unit encoder (AGRU) is used to encode the multilingual entity descriptions. DEM applies multilingual word embeddings in order to capture the semantic information of multilingual entity descriptions from the word level. The two models, i.e.,  KGEM and DEM, are iteratively co-trained in order for each model to propose a new ILL alternately. \n\\noindent{\\bf KGloVe with literals} is an experimental attempt to incorporate entity descriptions in KGloVe KG embedding approach. The experiment is conducted on DBpedia considering the abstracts and comments of entities as their descriptions. The main goal is to extract named entities from the textual description and for every entity in the text, to replace those words representing it with the entity itself and then take its neighbouring words and entities as its context. The approach works by creating two co-occurrence matrices independently and then by merging them at the end so that a joint embedding can be performed. The first matrix is generated using the same technique as in KGloVe , i.e.,  by performing Personalized PageRank (PPR) on the (weighted) graph followed by the same optimisation used in the GloVe  approach.\nIn order to create the second matrix, the Named Entity Recognition (NER) task is performed on the entity description text using the list of entities and predicates of the KG as an input. The NER step employs a simple exact string matching technique which leads to numerous drawbacks such as missing entities due to having different keywords with the same semantics. All the English words that do not match any entity labels are added to the entity-predicate list. Then GloVe co-occurrence for text is applied to the modified text (i.e., DBpedia abstract and comments) using the entity-predicate and word list as input. Finally, the two co-occurrence matrices are summed up together to create a single unified matrix. The proposed approach has been evaluated on classification and regression tasks and the result indicates that for most of the classifiers used, except SVM, the approach does not bring significant improvement to KGloVe. However, the approach can be improved using parameter tuning with extensive experiments.", "cites": [1937, 4961], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of Extended RESCAL, Jointly(Disp), and DKRL, integrating their approaches to handle text literals in KG embeddings. It connects ideas across methods by highlighting differences in how they model and align structured and textual information, and it critiques certain limitations, such as the lack of sequence modeling in Extended RESCAL. While the section introduces a coherent narrative, the abstraction remains somewhat limited to specific techniques rather than broader conceptual trends."}}
{"id": "4b667491-f94e-4667-8ad4-392fcf2985c9", "title": "Models with Image Literals", "level": "subsection", "subsections": ["c0144c3f-6b6b-44ac-95a6-266e93a1181f"], "parent_id": "b17a9ad2-7c37-4320-837d-0c98c36965df", "prefix_titles": [["title", "A Survey on Knowledge Graph Embeddings with Literals:  Which model links better Literal-ly?\n"], ["section", "Knowledge Graph Embeddings with Literals"], ["subsection", "Models with Image Literals"]], "content": "\\label{image}\n\\vspace{0.25cm}\nIn this section, KG embedding models utilizing images of entities, namely, IKRL {} and MTKGRL {} are discussed. First, a detailed analysis of the models is presented followed by a summary. Moreover, in order to show the differences between the models based on complexity, the number of parameters of each model is presented in Table {\\ref{tab:complexity_image}}.", "cites": [4957], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces two models (IKRL and MTKGRL) and mentions a table comparing their complexity, but it lacks deeper synthesis of ideas from the cited papers. It provides a minimal overview without critical evaluation or broader abstraction, focusing mainly on description rather than analysis or integration."}}
{"id": "c0144c3f-6b6b-44ac-95a6-266e93a1181f", "title": "{\\bf IKRL", "level": "paragraph", "subsections": ["acfe2795-fa72-4fa9-8ab7-c89a0fc1020b"], "parent_id": "4b667491-f94e-4667-8ad4-392fcf2985c9", "prefix_titles": [["title", "A Survey on Knowledge Graph Embeddings with Literals:  Which model links better Literal-ly?\n"], ["section", "Knowledge Graph Embeddings with Literals"], ["subsection", "Models with Image Literals"], ["paragraph", "{\\bf IKRL"]], "content": "} learns embeddings for KGs by jointly training a structure-based representation with an image-based representation. The structure-based representation of an entity is learned by adapting a conventional embedding model like TransE. For the image-based representation, given the fact that an entity may have multiple image instances, an image encoder is applied to generate an embedding for each instance of a multi-valued image relation. The image encoder consists of a neural representation module and a projection module to extract discriminative features from images and to project these representations from image space to entity space respectively. \nFor the i-th image, its image-based representation $p_i$ in entity space is computed as:\n\\begin{equation}\n\\textbf{p}_i = \\textbf{M} \\cdot f(img_i),\n\\end{equation}\nwhere $M \\in \\mathbb{R}^{d_i \\times d_s}$ is the projection matrix with $d_i$ and $d_s$ representing the dimension of image features and the dimension of entities respectively. $f(img_i)$ is the i-th image feature representation in image space.\nAttention-based multi-instance learning is used to integrate the representations learned for each image instance by automatically calculating the attention that should be given to each instance. The attention for the i-th image representation $p_i^{(k)}$ of the k-th entity is given as:\n\\begin{equation}\natt(\\textbf{p}_i^{(k)}, \\textbf{e}_S^{(k)}) = \\frac{exp(\\textbf{p}_i^{(k)} \\cdot \\textbf{e}_S^{(k)})}{\\sum_{j=1}^n exp(\\textbf{p}_j^{(k)} \\cdot \\textbf{e}_S^{(k)})},\n\\end{equation}\nwhere $\\textbf{e}_S^{(k)}$ denotes the structure-based representation of the k-th entity. The higher the attention the more similar the image-based representation is to its corresponding structure-based representation which indicates that it should be given more importance when aggregating the image-based representations.  The aggregated image-based representation for the k-th entity is defined as follows:\n\\begin{equation}\n\\textbf{e}_I^{(k)} = \\sum_{i=1}^n\\frac{att(\\textbf{p}_i^{(k)}, \\textbf{e}_S^{(k)}) \\cdot \\textbf{p}_i^{(k)}}{\\sum_{j=1}^n att(\\textbf{p}_j^{(k)}, \\textbf{e}_S^{(k)})}.\n\\end{equation}\nGiven a triple, the overall energy function is defined by combining four energy functions (i.e., $E(h,r,t) = E_{SS} + E_{II} + E_{SI} + E_{IS}$. These energy functions are based on two kinds of entity representations (i.e, structure-based and image-based representations). The first energy function (i.e., $E_{SS} = \\lVert h_S + r - t_S\\rVert$) is same as TransE and the second function (i.e., $E_{II} = \\lVert h_I + r - t_I \\rVert$) uses their corresponding image-based representations for both head and tail entities. The third function (i.e., $E_{SI} = \\lVert h_S + r - t_I \\rVert$) is based on the structure-based representation of the head entity and the image-based representation of the tail entity whereas the fourth function (i.e., $E_{IS} = \\lVert h_I + r - t_S \\rVert$) is the exact opposite. These third and forth functions ensure that both structure-based representation and image-based representations are learned into the same vector space.\nGiven the energy function $E(h,r,t)$, a margin-based scoring function is defined as follows: \n\\begin{equation}\n\\begin{split}\n   L = \\sum_{(h,r,t)\\in T} \\sum_{(h',r',t') \\in T'} max(\\gamma + E(h,r,t)\\\\\n-  E(h',r',t'), 0),\n\\end{split}\n\\end{equation}\nwhere $\\gamma$ is a margin hyperparameter and $T'$  is the negative sample set of T generated by replacing the head entity, tail entity or the relation for each triple in T. Note that triples which are already in T are removed from $T'$. \n\\noindent \n\\textbf{MTKGRL} {} is a KG embedding approach which combines structural (symbolic), visual, and linguistic KG representations. The structural representations are created by adopting TransE embedding technique whereas visual embeddings are obtained from the feature layers of deep networks for image classification on the images that are associated with entities. For linguistic representations, pre-trained word embedding technique, specifically the skipgram model, is used. However, the information source for the linguistic representation are not literals from the KG but an external source, i.e., the word embedding model, trained on Google 100B token news dataset. Due to this fact, the model MTKGRL is not considered as multi-modal KG embedding model in the context of this survey and thus, it is not categorized under `Models with Multi-modal Literals' (Sec~{\\ref{multi-modal}}).\nMTKGRL defines an energy function for each kind of representation and also their combinations, i.e., structural energy, multimodal energies, and structural-multimodal energies. Structural energy is adopted from TransE, which is defined as \\textit{$ E_{S} = \\lVert \\textbf{h}_{\\textbf{s}} + \\textbf{r}_{\\textbf{s}} - \\textbf{t}_{\\textbf{s}}  \\rVert$}. The multimodal representations for the head and tail entities are computed as \\textit{${\\textbf{h}_{\\textbf{m}} = \\textbf{h}_{\\textbf{w}} \\oplus \\textbf{h}_{\\textbf{i}}}$} and \\textit{${\\textbf{t}_{\\textbf{m}} =  \\textbf{t}_{\\textbf{w}} \\oplus \\textbf{t}_{\\textbf{i}} }$} respectively, where the operator $\\oplus$ can be a concatenation operator or a mapping function.\nThe multimodal energy function under the translational assumption is given as:\n\\begin{equation}\n{E_{M1} = \\lVert \\textbf{h}_{\\textbf{m}} + \\textbf{r}_{\\textbf{s}} - \\textbf{t}_{\\textbf{m}}  \\rVert }. \n\\end{equation}\n$E_{M1}$ can be extended by considering the structural embeddings in addition to the multimodal embeddings as follows:\n\\begin{equation}\n{E_{M2} = \\lVert (\\textbf{h}_{\\textbf{m}} + \\textbf{h}_{\\textbf{s}}) \\textbf{r}_{\\textbf{s}} - (\\textbf{t}_{\\textbf{m}} + \\textbf{t}_{\\textbf{s}})  \\rVert }. \n\\end{equation}\nOn the other hand, in order to allow the structural and multimodal embeddings to be learned in the same vector space, the following structural-multimodal energies are defined as shown below:\n\\begin{subequations}\n\\begin{align}\n{E_{SM} = \\lVert \\textbf{h}_{\\textbf{s}} + \\textbf{r}_{\\textbf{s}} - \\textbf{t}_{\\textbf{m}}  \\rVert } \n\\\\\n{E_{MS} = \\lVert \\textbf{h}_{\\textbf{m}} + \\textbf{r}_{\\textbf{s}} - \\textbf{t}_{\\textbf{s}}  \\rVert } \n\\end{align}\n\\end{subequations}\nThe overall energy function, shown in Equation~{\\ref{energy}}, is defined by combining the aforementioned energy functions, i.e., $E_{S}$, $E_{M1}$, $E_{M2}$, $E_{SM}$, $E_{MS}$.\n\\begin{equation} \\label{energy}\n\\begin{split}\n    E(h,r,t) = E_{S} + E_{M1} + E_{M2} +  E_{SM}\\\\\n    + E_{MS}\n\\end{split}\n\\end{equation}\nFinally, a margin-based ranking loss function is minimized in order to train the model.", "cites": [4957], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear explanation of the IKRL model and its components, synthesizing the modelâ€™s approach to integrating image literals and structure-based representations. It also introduces the MTKGRL model and its various energy functions, offering a structured comparison of how different representations are combined. While it includes some analysis (e.g., the rationale behind attention-based learning), it lacks deeper critical evaluation or limitations of the approaches. The discussion identifies some general strategies for incorporating visual data but does not abstract to broader principles or frameworks."}}
{"id": "e552a27d-82e0-482b-b004-5980a7acc0a7", "title": "{\\bf MKBE", "level": "paragraph", "subsections": [], "parent_id": "4d4ce1d2-7013-4969-8504-ddac63801aa5", "prefix_titles": [["title", "A Survey on Knowledge Graph Embeddings with Literals:  Which model links better Literal-ly?\n"], ["section", "Knowledge Graph Embeddings with Literals"], ["subsection", "Models with Multi-modal Literals"], ["subsubsection", "Models with Numeric, Text, and Image Literals"], ["paragraph", "{\\bf MKBE"]], "content": "}   is a multi-modal KG embedding, in which the text, numeric and image literals are modelled together.  The main objective of this approach is to utilize all the observed subjects, objects, and relations (object properties and data properties) in order to predict whether any fact holds. It extends DistMult, which creates embedding for entities and relations, by adding neural encoders for different data types.\nGiven a triple $<s, r, o>$, the head entity $s$ and the relation $r$ are encoded as independent embedding vectors using one-hot encoding through a dense layer. Similarly, if the object $o$ is a categorical value, then it will be represented through a dense layer with a relu activation which has the same number of nodes as the embedding space dimension. On the other hand, if the object $o$ is rather a numerical value, then a feed forward layer, after standardizing the input, is used in order to learn embeddings for $o$ by projecting it to a higher-dimensional space. If $o$ is a short text (such as names and titles), it is encoded using character-based stacked, bidirectional GRUs and the final output of the top layer will be taken as the representation of $o$.  On the contrary, if $o$ is a long text such as entity descriptions, CNN over word embeddings will be used to get the embeddings for $o$. The object $o$ can also be an image, and in such a case, the last hidden layer of VGG pretrained network on ImageNet , followed by compact bilinear pooling, is used to obtain the embedding of $o$. Given the vector representations of the entities, relations and attributes, the same scoring function from DistMult is used to determine the correctness probability of triples. \nThe binary cross-entropy loss, as defined below, is used to train the model:\n\\begin{equation}\n\\sum_{(s,r)}\\sum_{o}t_o^{s,r}\\log(p_o^{s,r}) + (1-t_o^{s,r})\\log(1-p_o^{s,r}),\n\\end{equation}\nwhere for a given subject relation pair $(s, r)$, binary label vector $t^{s,r}$  over all entities is used to indicate whether $<s, r, o>$ is observed during training. $p_o^{s,r}$  denotes the model's probability  of truth for any triple $<s, r, o>$ computed  using a sigmoid function. \nMoreover, using these learned embeddings and different neural decoders, a novel multimodal imputation model is introduced to generate missing multimodal values, such as numerical data, categorical data, text, and images, from information in the knowledge base. In order to predict the missing numerical and categorical data such as dates, gender, and occupation, a simple feed-forward network on the entity embedding is used. For text, the adversarially regularized autoencoder (ARAE) has been used to train generators that decodes text from continuous codes, having the generator conditioned on the entity embeddings instead of random noise vector. Similarly, the combination of BE-GAN structure with pix2pix-GAN model is used to generate images, conditioning the generator on the entity embeddings.", "cites": [514, 4960], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the MKBE model in detail, outlining its architecture and training methodology. It integrates information from the cited paper (4960) but does not synthesize it with broader research trends or other works beyond the model itself. There is minimal critical evaluation or abstraction, as the focus is on explaining the model's components rather than analyzing its strengths, limitations, or placing it within a larger framework."}}
{"id": "cd3459b9-70b3-45e6-8435-8109c3994bfd", "title": "Summary", "level": "paragraph", "subsections": [], "parent_id": "4d4ce1d2-7013-4969-8504-ddac63801aa5", "prefix_titles": [["title", "A Survey on Knowledge Graph Embeddings with Literals:  Which model links better Literal-ly?\n"], ["section", "Knowledge Graph Embeddings with Literals"], ["subsection", "Models with Multi-modal Literals"], ["subsubsection", "Models with Numeric, Text, and Image Literals"], ["paragraph", "Summary"]], "content": "} Despite the attempt made in incorporating text literals, numeric literals, and images into the KG embedding, the model (MKBE) fails to capture the semantics of the data types/units of (numeric) literal values. Besides, similar to IKRL, it takes an image $I$ as an instance of a certain entity $e$ only if, $I$ is initially associated with $e$ in the  dataset considered (refer to Section~\\ref{para:IKRL-Summary} for more details). \n\\begin{table}[]\n    \\centering\n    \\caption{\n    Complexity of the models with multimodal literals in terms of the number of parameters. $\\Theta$ is the number of parameters in the base model, $H$ is the entity embedding size, $N_d$ is the number of data relations, $N_{char}$ is the number of characters, and $N_i$ is the number of images, $\\Theta_{CNN}$ is the number of parameters in the CNN model used in {}, $\\Theta_{ARAE}$ is the number of parameters in ARAE {} where instead of using the random noise vector $z$, the generator is conditioned on the entity embeddings, $\\Theta_{GAN}$ denotes the sum of the number of parameters in BE-GAN {} and in pix2pix-GAN {}. }\n    \\begin{tabular}{c|c}\n    \\hline\n       Model  &  \\#Parameter\\\\\n       \\hline\n       LiteralE with blocking &  $\\Theta + (N_dH + H)H$\\\\\n       EAKGAE &  $\\Theta + (N_{d} + N_{char})H$\\\\\n       MKBE & $\\Theta + (2(N_d + 3(N_{char} + H)) +  N_i)H$ \\\\\n       & $+ \\Theta_{CNN} + \\Theta_{ARAE} + \\Theta_{GAN}  $\\\\\n    \\hline\n    \\end{tabular}\n    \\label{tab:complexity_multimodal}\n\\end{table}", "cites": [896, 4964, 4963], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates information from multiple cited papers to analyze the MKBE model's limitations, particularly in handling numeric literals and images. It provides a critical perspective by highlighting specific shortcomings and connects ideas across sources, such as linking MKBE to IKRL. However, the analysis remains focused on specific models and does not generalize to broader principles or synthesize a novel framework."}}
{"id": "4010cb1e-1f9e-492b-9314-21f17df9577a", "title": "Link prediction.", "level": "paragraph", "subsections": ["1091a109-57b7-40ae-acab-36a126f6adb2"], "parent_id": "6c2ae1d2-8d12-4447-aa9f-d0bfcd2570dc", "prefix_titles": [["title", "A Survey on Knowledge Graph Embeddings with Literals:  Which model links better Literal-ly?\n"], ["section", "Applications"], ["paragraph", "Link prediction."]], "content": "} In general terms, link prediction can be defined as a task of identifying missing information in complex networks . Specifically in the case of KGs,  link prediction models aim at predicting new relations between entities leveraging the existing links for training. Along with predicting relations between the entities link prediction also focuses on the task of predicting either the head or the tail entity with respect to a relation. Then it decides if a new triple, which is not observed in the KG, is valid or not.  Formally, let $G$ be a KG with a set of entities $E = \\{e_{1}, \\dots, e_{n}\\}$ and a set of object relations $R = \\{r_{1}, \\dots , r_{m}\\}$, then link prediction can be defined by a mapping function $\\psi : E \\times E \\times R \\rightarrow R$ which assigns a score to every possible triple $(e_{i}, e{_j}, r_{k}) \\in E \\times E \\times R$. A high score indicates that the triple is most likely to be true.\nLink prediction is one of the most common tasks used for evaluating the performance of KG embeddings. Head prediction, tail prediction, and relation prediction are different kinds of sub-tasks related to link prediction. Head prediction aims at identifying a missing head entity where the relation and tail entity are given, and analogously for tail prediction and relation prediction. Most of the models  discussed in Section~\\ref{sec:techniques} have been evaluated on some or all of these prediction tasks. Head and tail prediction are used to evaluate the models LiteralE~, TransEA~, KBLRN~, KDCoE~, EAKGAE~,  IKRL~, MKBE~, \nMTKGRL {}, Jointly(Desp) {}, Jointly {}, and SSP {}. \nOn the other hand, DKRL~ has been evaluated on all kinds of link prediction tasks: head, tail, and relation predictions. \nIn Extended RESCAL~, two kinds of link prediction experiments have been conducted on the Yago 2~, i.e., i) tail prediction by fixing the relation type to {\\tt rdf:type}, and ii) general link prediction experiments for all relation types. Unfortunately, it is not possible to compare the obtained evaluation results of all these models because the experiments have been carried out on different datasets and also different link prediction procedures have been followed. Taking this into consideration, in this survey, experiments have been conducted on head and tail prediction tasks for these models (see Section~\\ref{sec:experimentation}).", "cites": [4957, 1937, 4960, 4961, 8863], "cite_extract_rate": 0.3125, "origin_cites_number": 16, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic definition and description of link prediction in the context of KGs and lists several models evaluated on these tasks. However, it lacks meaningful synthesis of the cited papers, does not critically evaluate their approaches or results, and does not abstract broader principles or trends from the models. The content remains largely descriptive and surface-level."}}
{"id": "1091a109-57b7-40ae-acab-36a126f6adb2", "title": "Triple Classification.", "level": "paragraph", "subsections": ["7196adf6-af3b-4cfa-b835-a15838a6adcc", "fd0b7680-3b36-4abe-8514-4c652b5201d7", "ad49702c-869b-4db8-8d41-f87972bc4325"], "parent_id": "4010cb1e-1f9e-492b-9314-21f17df9577a", "prefix_titles": [["title", "A Survey on Knowledge Graph Embeddings with Literals:  Which model links better Literal-ly?\n"], ["section", "Applications"], ["paragraph", "Link prediction."], ["paragraph", "Triple Classification."]], "content": "}\nThe goal of the triple classification task is the same as that of link prediction. A potential triple $<h, r, t>$ is classified as 0 (false) or 1 (true), i.e., a binary classification task. The embedding models MTKGNN , IKRL , \nMTKGRL {}, Jointly(Desp) {}, and Jointly {} \nhave been evaluated on this task. However, since they do not use a common evaluation dataset, it is not possible to compare the reported results directly.", "cites": [4957, 4961], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal description of the triple classification task and lists a few models that have been evaluated, but it lacks synthesis of their approaches or meaningful connections between the cited works. There is no critical evaluation of the models or their limitations, and the analysis does not abstract beyond the individual papers to highlight broader trends or principles."}}
{"id": "7196adf6-af3b-4cfa-b835-a15838a6adcc", "title": "Entity Classification.", "level": "paragraph", "subsections": [], "parent_id": "1091a109-57b7-40ae-acab-36a126f6adb2", "prefix_titles": [["title", "A Survey on Knowledge Graph Embeddings with Literals:  Which model links better Literal-ly?\n"], ["section", "Applications"], ["paragraph", "Link prediction."], ["paragraph", "Triple Classification."], ["paragraph", "Entity Classification."]], "content": "}\nGiven a KG $G$, with a set of entities $E$ and types $T$ and with an entity $e \\in E$ and type $t \\in T$, the task of entity classification is to determine if a potential entity type pair $(e,t)$ which is not observed in G ($(e,t) \\notin G$) is a missing fact or not. This task is an entity type  prediction using a multi-label classification algorithm considering the entity types in G as given classes. In DKRL , \nExtended RESCAL~{}, and SSP {}, Entity classification has been used for model evaluation.", "cites": [1937], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a minimal description of the entity classification task in knowledge graphs and mentions three models (DKRL, Extended RESCAL, and SSP) that have used this task for evaluation. However, it lacks synthesis by not connecting or contrasting the approaches in these models. There is no critical analysis of their methodologies or limitations, and no abstraction to broader principles or trends in how literals influence entity classification."}}
{"id": "ad49702c-869b-4db8-8d41-f87972bc4325", "title": "Other Applications.", "level": "paragraph", "subsections": [], "parent_id": "1091a109-57b7-40ae-acab-36a126f6adb2", "prefix_titles": [["title", "A Survey on Knowledge Graph Embeddings with Literals:  Which model links better Literal-ly?\n"], ["section", "Applications"], ["paragraph", "Link prediction."], ["paragraph", "Triple Classification."], ["paragraph", "Other Applications."]], "content": "} Attribute-value prediction, near\\-est-neighbor analysis, data linking, document classification, and \nrelational fact extraction \nare other application scenarios used for the evaluation of the models under discussion. \nAttribute-value prediction is the process of predicting the values of (discrete or non-discrete) attributes in a KG. For example, a missing value of a person's weight can be identified using the attribute value prediction task which is commonly seen as a KG completion task. \nIn MTKGNN , attribute-value prediction is applied using an attribute-specific Linear Regression classifier for evaluation. The same task has been employed in MKBE  for model evaluation by imputing different multi-modal attribute values.\nNearest Neighbor Analysis is a task of detecting the nearest neighbors of some given entities in the latent space learned by an embedding model. This task has been performed in LiteralE  to compare DistMult+LiteralE with the base model DistMult. On the other hand, data linking and document classification tasks have been used in LiteralE with blocking  and KGlove with literals  respectively (refer to  and  for more details). \nRelational fact extraction is a task of extracting facts/triples from plain text and has been used as a model evaluation task in Jointly(Desp) {}. \nTable~\\ref{table:application-summary} summarizes all the applications on which the KG embedding models with literals have been evaluated.", "cites": [4960], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual summary of different application scenarios for knowledge graph embeddings with literals, citing specific models and their evaluation methods. However, it lacks deeper synthesis of ideas across the cited works, critical evaluation of their strengths or weaknesses, and abstraction to broader principles or trends."}}
{"id": "8cec08d2-f7bc-4df5-9dbe-37559a8f99f9", "title": "Experiment with Images", "level": "subsection", "subsections": [], "parent_id": "e757ff13-3dc7-406c-ad8a-36bbbc5b98a6", "prefix_titles": [["title", "A Survey on Knowledge Graph Embeddings with Literals:  Which model links better Literal-ly?\n"], ["section", "Experiments on Link Prediction"], ["subsection", "Experiment with Images"]], "content": "\\label{sub:exp-images} \nNote that it is not possible to compare the whole of MKBE  with any other model as it is the only embedding model which utilizes the three types of literals together: text, numeric, and images. Therefore, its sub model S+I which uses only images has been compared with the embedding model IKRL . Since this comparison has already been done by the authors of MKBE , the result shown in Table \\ref{table:results-image} is directly taken from their paper. They have compared  the models DistMult+S+I, ConvE+S+I, and IKRL where S stands for structure and I for Image. Both DistMult+S+I and ConvE+S+I are sub models of MKBE which use only relational triples and Images. The result indicates that ConvE+S+I outperforms the other two models in all metrics on the YAGO-10 dataset (refer to MKBE  for more details).\n\\begin{table*}[ht]\n\\centering \n\\caption{MRR results on link prediction task on YAGO-10 taken from MKBE\n.}\n\\label{table:results-image}\n\\begin{tabular}{ccccc}\n\\hline\n\\multicolumn{5}{c}{\\textbf{YAGO-10}}\\\\\nModels & MRR & Hits@1 & Hits@3 & Hits@10 \\\\\n\\hline\nDistMult+S+I & 0.342 & 0.235 & 0.352 & 0.618 \\\\\nConvE+S+I &   \\textbf{0.566} & \\textbf{0.471} & \\textbf{0.597} & \\textbf{0.72} \\\\\nIKRL &   0.509 & 0.423 & 0.556 & 0.663 \\\\\n\\hline\n\\end{tabular}\n\\label{fig:figures}\n\\end{table*}", "cites": [4960, 4957], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual comparison of models using image literals but lacks deeper synthesis of the broader implications or connections between the approaches. It does not critically analyze the strengths and weaknesses of the models or generalize findings into broader patterns or principles."}}
