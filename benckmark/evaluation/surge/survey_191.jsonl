{"id": "55b61d62-0081-44ae-99bb-55df5876dd57", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "e3f6de83-1921-4425-92d8-01891c4414bf", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Introduction"]], "content": "The connection between creativity and machines is as old as computer science itself. In 1842, Lady Lovelace, an English mathematician and writer, who is recognized by many as the first computer programmer, issued what is now known as ``Lovelaceâ€™s Objection'' . She stated that the Analytical Engine (the digital programmable machine proposed by Charles Babbage ) \\textit{``has no pretensions to originate anything. It can do whatever we know how to order it to perform''} . Indeed, in the centuries that followed, numerous projects and studies have been undertaken with the aim of designing machines capable of ``originating something'' . We have witnessed the emergence of a specialized field in computer science, namely Computational Creativity , which concerns the study of the relationship between creativity and artificial systems .\nIn this context, the adoption of deep learning (DL) techniques has led to substantial breakthroughs in recent years. Vast computational power and very large amounts of available data are at the basis of the increasing success of deep generative models (i.e., generative models based on DL ). Indeed, generative deep learning technologies have been used to write newspaper articles\\footnote{\\url{www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3}}, generate human faces  and voices , design drugs and proteins , and even create artworks sold for hundred thousand dollars\\footnote{\\url{www.christies.com/features/a-collaboration-between-two-artists-one-human-one-a-machine-9332-1.aspx}}.\nWhile it is apparent that current technologies are able to generate impressive outputs, at the same it is also possible to argue that they cannot be considered \\textit{creative} in general . In fact, the goal of generative deep learning is to produce synthetic data that closely resemble real ones fed in input . On the other hand, creativity involves novelty and diversity . While for some problems mere content generation  might be sufficient, for other tasks, e.g., in the Arts, the ability to create different (but still valuable) outputs is essential: a creative model can find practical applications in the arts and industrial design as support for artists, content creators, designers and researchers, just to name a few. Moreover, generating more diverse data might mitigate legal and ethical issues related to content reproduction .\n\\noindent \\textbf{Goal and contributions of the survey.} The goal of this survey is to present and critically discuss the state of the art in generative deep learning from the point of view of machine creativity. \nMoreover, to the best of our knowledge, this is the first survey that explores how current DL models have been or can be used as a basis for both generation (i.e., producing creative artifacts) and evaluation (i.e., recognizing creativity in artifacts). \nThe contribution of this survey can be summarized as follows. After a brief overview of the meaning and definitions of creativity in Section \\ref{whatcreativityis},\nSection \\ref{generativemodels} presents an in-depth analysis of generative deep learning through the introduction of a new taxonomy and a critical analysis of machine creativity. Then, several machine learning (ML)-based methodologies for evaluating creativity are presented and discussed in Section \\ref{creativitymeasures}. Finally, Section \\ref{conclusion} concludes the paper, outlining open questions and research directions for the field.\n\\noindent \\textbf{Related surveys.} We now provide an overview of other surveys in areas related to the present work. For readers interested in a survey on deep generative models, we recommend ; for an analysis of the state of the art in evaluation in computational creativity,  is an essential reading; for a review on AI and creativity in general, we recommend~; for a practical view of generative deep learning, we suggest ; finally, for an in-depth examination of artistic AI works (also in human-computer co-creations ),  is a very comprehensive source of information.", "cites": [5892, 1515, 5891, 8020], "cite_extract_rate": 0.14814814814814814, "origin_cites_number": 27, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a general analytical overview of creativity and machine learning, referencing the broader historical and technical context. While it mentions the role of deep generative models and raises questions about their creative capabilities, it does not deeply synthesize or connect the cited papers to form a cohesive argument. The critical perspective is present in questioning whether deep learning can be considered truly creative, but it lacks detailed comparison or evaluation of the cited works. It abstracts to some extent by identifying the importance of novelty and diversity in creativity, but not at a meta-level that reveals broader principles."}}
{"id": "c94d844b-5541-4cd4-b103-5bcd3c417722", "title": "Generative Models", "level": "section", "subsections": ["d3b2407f-fa15-4904-8079-6b8f165603fc", "80cd08bd-b7d0-405e-bebf-5786c2d242a6", "a65d83e3-2cb6-4dd1-9fb1-1361cbc3a9d7", "fcc9765a-abe4-4945-86b3-2e2b1d1b5a29", "78d3688b-85c7-4c04-80ed-092f8e0a336c", "1fd7a3bc-b11f-41d1-ab39-6560d8d2fd88", "0499071e-70b3-4a75-8c97-489bf487dec2", "593cd1e1-91fe-46f4-a3af-16a976eaa38f"], "parent_id": "e3f6de83-1921-4425-92d8-01891c4414bf", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Generative Models"]], "content": "\\label{generativemodels}\nA generative model can be defined as follows: given a dataset of observations $X$, and assuming that $X$ has been generated according to an unknown distribution $p_{data}$, a generative model $p_{model}$ is a model able to mimic $p_{data}$. By sampling from $p_{model}$, observations that appear to have been drawn from $p_{data}$ can be generated . Generative deep learning is just the application of deep learning techniques to form $p_{model}$.\nAt first glance, this definition appears to be incompatible with those presented in Section \\ref{whatcreativityis}. Indeed, mimicry is the opposite of novelty. However, what a generative model should aim at mimicking is the underlying distribution representative of the artifacts, and not the specific artifacts themselves; in other words, it should aim at learning the conceptual space defined by the cultural context considered. A generative model can be said to exhibit combinatorial creativity if it can sample new and valuable works that are combinations of real data and exploratory creativity if the works actually differ from real ones. Vice versa, transformational creativity emerges if and only if the distribution for sampling diverges in some way from the underlying one (e.g., due to a different training process, by altering the distribution after learning, or by changing the sampling technique). In summary, what matters is how the space of solutions is learned and how artifacts are sampled from it.\nIn this section, we aim at studying the level of creativity of existing generative deep learning models. Following the discussion above, we analyze how the models learn their spaces of solutions and how the observations are generated from them. A new generative deep learning taxonomy is then introduced based on the different training and sampling techniques at the basis of each method. Figure \\ref{fig:models} provides a summary of the seven generative classes considered in this survey. Since our focus is on machine creativity, we do not discuss the implementation details of each class of methods. We instead present the \\textit{core concepts} at the basis of each class; some relevant \\textit{examples of models}; potential \\textit{applications}; and a \\textit{critical discussion} evaluating the level of machine creativity considering the definitions above.\n\\begin{figure}[ht]\n  \\centering\n  \\includegraphics[width=1\\linewidth]{images/generative_models.pdf}\n  \\caption{A schematic view of the seven classes of generative learning methods presented in this survey. Top, left to right: Variational Autoencoder (\\ref{vae}), with a decoder generating $\\mathbf{x'}$ given a latent vector $\\mathbf{z}$, and an encoder representing $\\mathbf{x}$ into a latent distribution; Generative Adversarial Network (\\ref{gan}), with a generator to produce $\\mathbf{x'}$, and a discriminator to distinguish between real $\\mathbf{x}$ and synthetic $\\mathbf{x'}$; Sequence prediction model (\\ref{sequenceprediction}), with a generator to output $\\mathbf{x}$ one token after the other given in input previous tokens; Transformer-based model (\\ref{transformers}), with a Transformer outputting $\\mathbf{x}$ one token after the other given in input previous tokens, or a masked version of $\\mathbf{x}$. Bottom, left to right: Diffusion model (\\ref{diffusion}), with a model to learn an error $\\mathbf{\\epsilon}$, which is used to incrementally reconstruct $\\mathbf{x_0}$; Reinforcement Learning (RL)-based method (\\ref{rlbased}), with a generative model acting (i.e., progressively generating $\\mathbf{x}$) to maximize a given reward function; Input-based methods (\\ref{inputbased}), with an input optimized by a given loss. The input can be a vector $\\mathbf{z}$ given to a generative model to obtain the desired output, or directly a product $\\mathbf{x}$ becoming the desired output.}\n  \\Description{Summary of Classes of Generative Learning Methods.}\n  \\label{fig:models}\n\\end{figure}\nAs a final remark, it is worth noting that we limit our examples to the Arts (e.g., poems, music, or paintings). Indeed, generative learning can be applied to design ; game content generation (see  for a comprehensive survey); recipes ; scientific discovery ; and in general to any activity, which has a non-trivial solution .", "cites": [5893, 679], "cite_extract_rate": 0.2222222222222222, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section provides a strong synthesis of generative deep learning by connecting the concept of mimicry to the broader definitions of creativity, showing how different models can lead to various types of creative outputs. It includes critical evaluation of the models' creative capabilities through the lens of transformational, combinatorial, and exploratory creativity. The abstraction level is high, as the section introduces a new taxonomy based on training and sampling techniques, moving beyond specific papers to a generalized framework."}}
{"id": "2185135c-5858-4b2c-a22a-cc750d47c129", "title": "Core Concepts", "level": "subsubsection", "subsections": [], "parent_id": "d3b2407f-fa15-4904-8079-6b8f165603fc", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Generative Models"], ["subsection", "Variational Auto-Encoders"], ["subsubsection", "Core Concepts"]], "content": "A Variational Auto-Encoder (VAE)  is a learning architecture composed of two models: an encoder (or recognition model) and a decoder (or generative model). The former compresses high-dimensional input data into a latent space, i.e., a lower-dimensional space whose features are not directly observable, yet provide a meaningful representation. The latter decompresses the representation vector back to the original domain . Classic autoencoders directly learn to represent each input in a latent representation vector. Conversely, VAEs learn a (Gaussian) distribution over the possible values of the latent representation, i.e., the encoder learns the mean and the (log of the) variance of the distribution. \nVAEs are trained by optimizing two losses: the reconstruction loss and the regularization loss. The former is the log-likelihood of the real data $\\mathbf{x}$ from the decoder given their latent vectors $\\mathbf{z}$, i.e., it is the error of the decoder in reconstructing $\\mathbf{x}$. The latter is the Kullback-Leibler (KL) divergence between the distribution learned by the encoder and a prior distribution, e.g., a Gaussian. Notably, the latent vector $\\mathbf{z}$ in input to the decoder is obtained by means of the so-called \\textit{reparameterization trick}, i.e., by sampling from the distribution defined by the mean and the variance. Without it, sampling would induce noise in the gradients required for learning .\nThe mathematical derivation of the whole loss has its roots in variational inference . Indeed, VAEs can be seen as an efficient and stochastic variational inference method, in which neural networks (NNs) and stochastic gradient descent are used to learn an approximation (i.e., the encoder) of the true posterior .\nIn VAEs, similar high-dimensional data are mapped to close distributions. This makes it possible to sample a random point $\\mathbf{z}$ from the latent space, and still obtain a comprehensible reconstruction . On the other hand, VAE tends to produce blurred images . It may also happen that high-density regions under the prior have a low density under the approximate posterior, i.e., these regions are not decoded to data-like samples . Finally, the objective can lead to overly simplified representations without using the entire capacity, obtaining only a sub-optimal generative model .", "cites": [5897, 5894, 2073, 5680, 5896, 5895, 8961], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers to explain the core concepts of VAEs, including their structure, training objectives, and limitations. It offers a critical perspective by discussing issues such as blurry outputs and the prior-posterior mismatch. The content also abstracts these ideas to highlight broader principles like latent space organization and the impact of the VAE objective on generative quality."}}
{"id": "69ded76f-4e8e-47a4-b5cf-a93e828188ac", "title": "Examples of Models", "level": "subsubsection", "subsections": [], "parent_id": "d3b2407f-fa15-4904-8079-6b8f165603fc", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Generative Models"], ["subsection", "Variational Auto-Encoders"], ["subsubsection", "Examples of Models"]], "content": "Several models based on VAEs have been proposed  in recent years. In the following, we focus on those relevant to our discussion on machine creativity.\nIn $\\beta$-VAE , a parameter $\\beta$ is used to scale the magnitude of the regularization loss, which allows a better disentanglement of the latent space . Another example is VAE-GAN , which merges VAE and Generative Adversarial Networks (GAN; see Section \\ref{gan}) . This is done by treating the decoder as the generator of the GAN, thus training it by means of the GAN loss function. This leads to the generation of substantially less blurred images. Similarly, Adversarially Learned Inference (ALI)  merges VAE and GAN by asking the discriminator to distinguish between pairs of real data (and their latent representations) and pairs of sampled representations and synthetic data. Instead, Adversarial Autoencoders (AAE)  substitute the regularization loss with a discriminative signal, where the discriminator has to distinguish between random latent samples and encoded latent vectors. Another way to address the problem of ``sample blurriness'' is with PixelVAE , where the autoregressive PixelCNN  is used as the decoder. In , to deal with sequential data such as texts, where generation requires more steps, the encoder learns to produce a latent representation of a sentence, while the recurrent neural network RNN-based decoder learns to reproduce it word after word. However, VAE can also generate text by means of convolution and deconvolution .\nTo solve the problem of low-density regions, the authors of  propose an energy-based model called noise contrastive prior (NCP), trained by contrasting samples from the aggregate posterior to samples from a base prior. Finally, another interesting model is Vector Quantised-VAE (VQ-VAE) ; in this case, the encoder outputs discrete, rather than continuous, codes, and the prior is learned rather than static.", "cites": [2509, 5897, 5896, 5898, 507, 3018, 8962, 1252, 1256, 5899], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 14, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of various VAE-based models, mentioning key modifications and hybrid approaches. However, it lacks deeper synthesis of ideas across the cited papers and offers minimal critical evaluation or comparison of their strengths and weaknesses. The abstraction is limited to pointing out common problems (e.g., blur, prior mismatch) without articulating broader theoretical or practical principles."}}
{"id": "75174421-ab64-486f-a6e0-25ebedfd79a2", "title": "Applications", "level": "subsubsection", "subsections": [], "parent_id": "d3b2407f-fa15-4904-8079-6b8f165603fc", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Generative Models"], ["subsection", "Variational Auto-Encoders"], ["subsubsection", "Applications"]], "content": "VAEs can be used for semi-supervised classification to provide an auxiliary objective, improving the data efficiency ; to perform iterative reasoning about objects in a scene ; to model the latent dynamics of an environment .\nOf course, VAEs have also been used to generate synthetic data, including for conditional generation. For example, a layered foreground-background generative model can be used to generate images based on both the latent representation and a representation of the attributes . \nIn  the latent space of a VAE is trained on chemical structures by means of gradient-based optimization toward certain properties (see Section \\ref{inputbased}). AAEs have also been applied to the same problem . Finally, another interesting application of VAE is Deep Recurrent Attentive Writer (DRAW) . DRAW constructs scenes in an iterative way, by accumulating changes emitted by the decoder (then given to the encoder in input). This allows for iterative self-corrections and a more natural form of image construction. RNNs and attention mechanism are used to consider previous generations and to decide at each time step where to focus attention, respectively.", "cites": [8022, 5900, 5901, 258, 7289, 7159, 8021], "cite_extract_rate": 0.875, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of VAE applications by listing various domains and models, such as semi-supervised learning, scene reasoning, and chemical structure modeling. However, it lacks deeper synthesis by not connecting the applications to broader themes in computational creativity. Critical analysis and abstraction are minimal, with no evaluation of trade-offs or overarching principles among the cited works."}}
{"id": "7b3d5774-2f11-4f27-8237-12e3db437fd8", "title": "Critical Discussion", "level": "subsubsection", "subsections": [], "parent_id": "d3b2407f-fa15-4904-8079-6b8f165603fc", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Generative Models"], ["subsection", "Variational Auto-Encoders"], ["subsubsection", "Critical Discussion"]], "content": "Models based on VAEs can be considered as an example of exploratory creativity. The latent space is learned with the goal of representing data in the most accurate way. The random sampling performed during generation is therefore an exploration of that space: regions not seen during training can be reached as well, even though they can lead to poor generation~ and some more complex variants may be needed, as discussed. On the other hand, there is no guarantee that the results will be valuable, novel, or surprising.\nThere is no guarantee that the generation from random sampling is of good quality or diverse from training data. Indeed, given their characteristics, VAEs discourage novelty in a sense. In particular, diversity could be achieved in theory using VAEs and gradient-based optimization techniques, such as those presented in~, with novelty and surprise as target properties. We will discuss these aspects in Section \\ref{creativityoriented}.", "cites": [258, 5896], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 4.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides critical analysis by discussing the limitations of VAEs in terms of generating novel and high-quality outputs, and it references relevant papers to support these points. While it synthesizes some ideas (e.g., relating VAEs to exploratory creativity), the integration is not fully novel or deep. The abstraction is moderate, as it generalizes about the role of VAEs in creativity but stops short of offering a comprehensive meta-framework."}}
{"id": "c55014c9-c3f8-4c15-8e80-1ac6e674b094", "title": "Core Concepts", "level": "subsubsection", "subsections": [], "parent_id": "80cd08bd-b7d0-405e-bebf-5786c2d242a6", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Generative Models"], ["subsection", "Generative Adversarial Networks"], ["subsubsection", "Core Concepts"]], "content": "A Generative Adversarial Network  is an architecture composed by two networks: a generative model and a discriminative model. The latter learns to distinguish between real samples and samples generated by the former. In parallel, the former learns to produce samples from random noise vectors such that they are recognized as real by the latter. This competition drives both models to improve their methods until the generated samples are indistinguishable from the original ones. \nThe adversarial training allows the generator to learn to produce seemingly real samples from random noise without being exposed to data. The simplicity of the idea and the quality of results are at the basis of the success of GANs. However, few limitations exist. For instance, GAN can suffer from mode collapse, where the generator only learns to produce a small subset of the real samples . In addition, the latent space of random inputs is typically not disentangled and it is necessary to introduce constraints in order to learn an interpretable representation~.", "cites": [5902, 5903], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear explanation of GANs and references two papers, one addressing style and content disentanglement and the other proposing a method to stabilize GAN training and address mode collapse. It integrates these concepts by linking them to general GAN limitations, such as mode collapse and latent space disentanglement. While it does include some critical discussion of GAN shortcomings, it does not deeply compare or evaluate the cited works in detail. The abstraction is limited to general patterns in GAN behavior rather than deeper meta-level insights."}}
{"id": "9bd52217-5cca-4878-be77-0a1de88aba4c", "title": "Examples of Models", "level": "subsubsection", "subsections": [], "parent_id": "80cd08bd-b7d0-405e-bebf-5786c2d242a6", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Generative Models"], ["subsection", "Generative Adversarial Networks"], ["subsubsection", "Examples of Models"]], "content": "The number of proposed variants is still growing\n. An in-depth survey on GANs is~. \nIndeed, several refinements have been proposed in the past years, such as using deep convolutional networks~ or self-attention~, incrementally growing the networks , or scaling the model parameters . In the following, we present examples that are relevant to the issue of machine creativity.\nThe problem of non-meaningful representation has been addressed in different ways. For instance, InfoGAN  adds a latent code $\\mathbf{c}$ to $\\mathbf{z}$. An auxiliary model learns to predict $\\mathbf{c}$ given the sample generated by means of it. In this way, it can learn disentangled representations in a completely unsupervised manner. Another possibility is Bidirectional GAN (BiGAN) . In order to include an inverse mapping from data to latent representation, an encoder is added to the architecture. The discriminator is then trained to distinguish between pairs of random noise and synthetic data and pairs of real data and latent encoding.\nIt is possible to condition the generation by means of a target content , a text , or even an image . In order to do so, it is sufficient to use the conditional information as input for both generator and discriminator~. Similarly, image-to-image translation is possible also without paired datasets. CycleGAN  trains two generators (from one domain to another, and vice versa) so that each of them produces images both from the target domain and correctly reconstructed by the counterpart.\nIn StyleGAN~, the generator architecture is re-designed in order to control the image synthesis process. The style of the image is adjusted at each layer based on the latent code (the specific intermediate code to control each layer is provided by a non-linear mapping network). This allows for the automatic separation of high-level attributes from stochastic variations in the generated images. It also allows for mixing regularization, where two latent codes are used alternatively to guide the generator. StyleGAN-V  builds on top of it to learn to produce videos by only using a few frames of it. To generate longer and more realistic motions, a two-stage approach can be used as well: first, a low-resolution generator is adversarially trained on long sequences; then, a high-resolution generator transforms a portion of the produced, low-resolution video in a high-resolution one .\nFinally, it is also worth mentioning variants that adapt GANs to sequential tasks (e.g., text generation). Since GANs require the generator to be differentiable, they cannot generate discrete data . However, several techniques have been proposed to avoid this problem. One possibility is to transform the discrete generation into a continuous one. Music can be processed like an image by considering its waveform (as in WaveGAN  and GANSynth ) or its musical score composed of tracks and bars (as in MuseGAN ). Music in a desired style can be obtained through conditional inputs.\nAnother possibility is to consider a soft-argmax function as an approximation of the inference for each step . TextGAN  uses it together with feature matching to learn the production of sentences. In place of the discriminative signal, it uses the difference between the latent feature distributions of real and synthetic sentences learned by the discriminator.\nAnother solution is to transform the GAN into a Reinforcement Learning (RL) framework, as in Sequential GAN (SeqGAN)~. The generative model is the agent; the tokens generated so far form the state; the selection of the next token to be generated is the action to be performed; and the discriminative signal is the reward. The REINFORCE algorithm  can then be used to adversarially train the generative model. Other policy gradient methods can be used as well . \nOn the other hand, the learning signal (i.e., the reward) might be very sparse. A way to solve this issue is to use inverse RL~. For example, the authors of~ use inverse RL to learn a reward function able to associate positive rewards to real state-action pairs, and non-positive rewards to synthetic state-action pairs. Notably, this can help solve mode collapse too. Another variant is LeakGAN . Here, a hierarchical generator composed of a Manager and a Worker is used. The Worker produces a sentence conditioned by a goal vector provided by the Manager. The Worker and the discriminative model are trained following SeqGAN; the Manager is trained to predict goal vectors that lead to the identification of advantageous directions in the discriminative feature space. More specifically, the Manager receives a feature vector from the discriminator, i.e., its last convolutional layer, at each generated token. By means of this \\textit{leaked} information and the hierarchical architecture, LeakGAN produces longer and higher-quality texts. Finally, another possibility is to use Gumbel-softmax relaxation , as in Relational GAN (RelGAN) . Controlled TExt generation Relational Memory GAN (CTERM-GAN)  builds on the latter by also conditioning the generator on an external embedding input. In addition, it uses both a syntactic discriminator to predict whether a sentence is correct and a semantic discriminator to infer if a sentence is coherent with the external input.", "cites": [529, 8430, 8023, 1002, 5781, 896, 7194, 978, 1003, 3986, 5904, 1001, 62, 782, 5905, 9095, 56, 157, 4422, 8024, 7022], "cite_extract_rate": 0.65625, "origin_cites_number": 32, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.3, "abstraction": 3.8}, "insight_level": "high", "analysis": "The section effectively synthesizes key GAN variants relevant to machine creativity, integrating concepts from multiple papers into a coherent overview. It provides analytical insights into design choices (e.g., disentangled representations, conditional inputs, hierarchical structures) and discusses their implications for creativity. While it includes some critical discussion (e.g., challenges with discrete data generation), it could offer deeper evaluation of trade-offs between different approaches."}}
{"id": "8005502e-2354-49ec-ae71-d44339ec858e", "title": "Applications", "level": "subsubsection", "subsections": [], "parent_id": "80cd08bd-b7d0-405e-bebf-5786c2d242a6", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Generative Models"], ["subsection", "Generative Adversarial Networks"], ["subsubsection", "Applications"]], "content": "GANs have been applied to a variety of practical problems in several application scenarios. They have been widely used for semi-supervised learning ; for generating adversarial examples  to better train image classifiers ; and, in general, in computer vision (see  for a detailed discussion). The generative power of GANs has also found its place in recommender systems (see ) to generate fashion items; in science and chemistry . Of course, its ability to generate high-quality samples has been exploited in many other areas, from anime design  and 3D object modeling  to photo-realistic consequences of climate change . Conditional inputs also allow the production of artistic works by controlling stylistic properties such as genre  or influencer .\nFinally, the most famous example of the artistic power of GAN is the collection of paintings by Obvious, a French art collective ; one of their works has been sold to more than 400,000 dollars\\footnote{Fun fact: the sold painting is called \\textit{Portrait of Edmond De Belamy} because Belamy sounds like \\textit{bel ami}, a sort of French translation of... \\textit{Goodfellow}.}.", "cites": [8026, 918, 5906, 8025, 5907, 917, 5908], "cite_extract_rate": 0.5384615384615384, "origin_cites_number": 13, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of GAN applications across multiple domains, citing relevant papers, but lacks deep synthesis or a unifying narrative. It does not critically evaluate the cited works or identify broader trends or limitations in the use of GANs for creativity. The abstraction level remains low, as it primarily lists use cases without elevating to general principles."}}
{"id": "7bf849b0-d6c4-4328-8061-5cdb36320d12", "title": "Examples of Models", "level": "subsubsection", "subsections": [], "parent_id": "a65d83e3-2cb6-4dd1-9fb1-1361cbc3a9d7", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Generative Models"], ["subsection", "Sequence Prediction Models"], ["subsubsection", "Examples of Models"]], "content": "Several models have been proposed, most of them based on RNN, and especially on long short-term memory (LSTM) . The reason is that RNNs use internal states based on previous computation: inputs received at earlier time steps can affect the response to the current input. However, RNNs tend to perform worse with longer sequences~. LSTM is a specific RNN architecture that addresses the problem of long-term dependencies through the use of additional gates determining what to remember and what to forget at each step.\nRNNs can be used to model joint probabilities of characters (Char-RNN) ; words ; phonemes ; syllables ; and even tokens from transcriptions of folk music (Folk-RNN) . They can also receive conditional inputs like the encoding of the previous lines . Richer architectures that combine models focusing on different properties can be used to generate more complex text, e.g., poetry based on pentameter and rhymes . Finally, sequence modeling can also be combined with reinforcement learning. For example, the authors of  use a Note-RNN model (based on single notes) trained using Deep Q-Network ; as rewards, they consider both the classic loss of sequence prediction models and a reward based on rules of music theory. In this way, the model learns a set of composition rules, while still maintaining information about the transition probabilities of the training data. The advantages of adopting an RL-based approach are described in Section \\ref{rlbased}.\nDue to the difficulties in working with long sequences, results in tasks like narrative generation are affected by a lack of coherence . Many approaches have been proposed to address this problem. For instance, stories can be generated in terms of events  (i.e., tuples with subject, verb, object, and an additional \\textit{wildcard}) by an encoder-decoder RNN (also known as Sequence-to-Sequence, see ); events are modeled by another encoder-decoder RNN. Instead of events, it is also possible to focus on entities (i.e., vectors representing characters)~.\nSequence prediction models are also used for domains not commonly modeled as sequences, like images. Image modeling can be defined in a discrete way by means of a joint distribution of pixels: the model learns to predict the next pixel given all the previously generated ones. It starts at the top left pixel and then proceeds towards the bottom right. The two seminal architectures for sequence prediction of images are PixelRNN and PixelCNN . The former is a two-dimensional RNN (based on rows or diagonals). The latter is a convolutional neural network (CNN) with an additional fixed dependency range (i.e., the convolution filters are masked in order to only use information about pixels above and to the left of the current one). To obtain better results, gated activation units can be used in place of rectified linear units between the masked convolutions; conditional inputs encoding high-level image descriptions can be used as well . Notably, the Gated PixelCNN architecture can also be used for other types of data: WaveNet  implements it to generate audio based on the waveform, possibly guiding the generation with conditional inputs.\nWhile intuitive in terms of architecture, RNNs are limited by the vanishing gradient problem and non-parallelizability in the time dimension . Very recent works explore solutions to tackle these issues by means of structured state spaces  and a combination of RNNs and Transformers  (see Section \\ref{transformers}).", "cites": [5910, 8835, 8027, 5909, 5911, 5912, 2401, 1252, 507, 4722], "cite_extract_rate": 0.47619047619047616, "origin_cites_number": 21, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.5, "abstraction": 3.7}, "insight_level": "high", "analysis": "The section provides a well-structured overview of sequence prediction models in creativity, integrating multiple papers to discuss applications in poetry, music, and image generation. It connects different models (e.g., RNN, LSTM, PixelCNN, WaveNet) and their adaptations, showing a coherent narrative. While primarily descriptive, it includes some critical evaluation (e.g., coherence issues in narrative generation, limitations of RNNs), and abstracts patterns such as the use of conditional inputs and hybrid approaches."}}
{"id": "22eec04a-1027-4c90-bad8-21a2b1189502", "title": "Applications", "level": "subsubsection", "subsections": [], "parent_id": "a65d83e3-2cb6-4dd1-9fb1-1361cbc3a9d7", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Generative Models"], ["subsection", "Sequence Prediction Models"], ["subsubsection", "Applications"]], "content": "As discussed, sequence prediction models have been used to learn to write poems or stories (by predicting a character, syllable, or word after the other); to compose music (by predicting a note or a waveform after the other); to draw images (by predicting a pixel after the other). In general, they can be used for any kind of time series forecasting . They can also be used for co-creativity, as in Creative Help .\nDespite their simplicity, sequence prediction models are one of the most successful generative techniques. An interesting example is \\textit{Sunspring}. It might be considered as the first AI-scripted movie: it was generated by a Char-RNN trained on thousands of sci-fi scripts . The quality of the result is demonstrated by the fact that it was able to reach the top ten at the annual Sci-Fi London Film Festival in its 48-Hour Film Challenge\\footnote{Quite interestingly, the AI that wrote \\textit{Sunspring} declared that its name was Benjamin, probably in honor of Walter Benjamin, the German philosopher who, already in 1935 , understood that new mechanical techniques related to art can radically change the public attitude to art and artists.}.", "cites": [5913], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of sequence prediction model applications in creative domains, referencing a single survey paper for time series forecasting. It integrates a general idea from the cited work but does not deeply synthesize multiple sources or critically evaluate the approaches. The mention of 'Sunspring' adds a concrete example, and the historical reference to Walter Benjamin offers a slight abstraction, but overall the section lacks in-depth analysis or a broader conceptual framework."}}
{"id": "33bfe66a-f3fb-4f0a-ab21-90457c6358e4", "title": "Core Concepts", "level": "subsubsection", "subsections": [], "parent_id": "fcc9765a-abe4-4945-86b3-2e2b1d1b5a29", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Generative Models"], ["subsection", "Transformer-Based Models"], ["subsubsection", "Core Concepts"]], "content": "Transformer-based models are neural networks based on the Transformer architecture . They represent the main example of foundation models , because of the leading role they have been assuming in language, vision, and robotics. A Transformer is an architecture for sequential modeling that does not require recurrent or convolutional layers. Instead, it only relies on a self-attention mechanism  that models long-distance context without a sequential dependency. Each layer consists of multi-head attention (i.e., several self-attention mechanisms running in parallel), a feed-forward network, and residual connections. Since self-attention is agnostic to token order, a technique called positional embedding is used to capture the ordering .\nIn principle, a Transformer is nothing more than an autoregressive model: it works by predicting the current token given the previous ones (see Section \\ref{sequenceprediction}). However, few fundamental differences exist. A Transformer can also be trained by means of masked modeling: some of the input tokens are randomly masked, and the model has to learn how to reconstruct them from the entire context, and not only from the previous portions . The possibility of dealing with very long sequences allows for prompting. By providing a natural language prompt in input, the model can generate the desired output, e.g., the answer to a question, a class between a set of classes for a given text, or a poem in a particular style . This is done by simply passing the prompt in input as a text, and then leveraging the model to predict what comes next (e.g., the answer to a question).\nThese advantages, together with the very large amount of data available, the increasing computational power, and the parallelism induced by their architecture, have led Transformer-based models to become the state of the art for several tasks. Nevertheless, the computational costs of the architecture from  grow quadratically with the input size.", "cites": [168, 679, 38, 7, 1550], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key concepts from multiple foundational Transformer-related papers (e.g., BERT, Attention Is All You Need) to explain the architecture and functionality of Transformer-based models. It abstracts these concepts into a broader understanding of their role as foundation models and their implications for creativity. While it does offer some critical points, such as the quadratic computational cost, it could delve deeper into the trade-offs and limitations of specific models."}}
{"id": "efb7c569-18fa-4d6d-b057-f5d7401dcae1", "title": "Examples of Models", "level": "subsubsection", "subsections": [], "parent_id": "fcc9765a-abe4-4945-86b3-2e2b1d1b5a29", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Generative Models"], ["subsection", "Transformer-Based Models"], ["subsubsection", "Examples of Models"]], "content": "Several Transformer-based approaches have been proposed in recent years. The design of specific Transformers for a variety of applications is presented in several surveys (e.g., ) and books (e.g., ).\nThe domain mostly influenced by Transformers is natural language processing (NLP). Bidirectional Encoder Representations from Transformers (BERT)  is a Transformer-based encoder trained for both predicting the next sentence (in an autoregressive fashion) and reconstructing masked tokens from the context. Several enhanced variations of the original model have been proposed, such as, for instance, solutions that remove the next-sentence pre-training objective , use inter-sentence coherence as an additional loss , or employ distillation  to train a smaller model .\nThe other main approach is that used by the Generative Pre-trained Transformer (GPT) family . Here, a Transformer-based decoder is trained in an autoregressive way by additionally conditioning on the task of interest. After training, it can be used to perform a wide range of tasks by providing a description or a few demonstrations of the task. The effectiveness of this text-to-text generative approach has then been explored by T5 . Many other large language models  have been proposed to achieve better results by means of more parameters and computation , or more qualitative data . \nMixture of Experts  can be used as well in place of the feed-forward network to train a larger but lighter model (since only portions of it are used per task), as done by Generalist Language Model (GLaM) . Finally, Bidirectional and Auto-Regressive Transformer (BART)  ideally merges together a BERT-encoder (trained by corrupting text with an arbitrary noising function) and a GPT-decoder (trained to reconstruct the original text autoregressively). Such an encoder-decoder architecture is able to achieve state-of-the-art results in machine translations, as well as other text-to-text tasks.\nTransformer-based models have been used in domains different from language modeling (LM). Few have been proposed for music generation. One of the first examples was Music Transformer , which can generate one-minute music in Bach's style with internal consistency; another remarkable one is Musenet , which is able to produce 4-minute musical composition with a GPT-2 architecture; and, finally, it is worth mentioning Jukebox , which can generate multiple minutes of music from raw audio by training a Sparse Transformer  (i.e., a Transformer with sparse factorization of the attention matrix in order to reduce from quadratic to linear scaling) over the low-dimensional discrete space induced by a VQ-VAE. Conditioning is always considered by means of genre, author, or instruments. MusicLM  additionally allows to generate music from text descriptions by aligning text and audio representation from different state-of-the-art models .\nAnother important application domain is video-making. Video Vision Transformer (ViViT)  generates videos using classic Transformer architectures; Video Transformer (VidTr)  achieves state-of-the-art performance thanks to the standard deviation-based pooling method; and VideoGPT  does so by learning discrete latent representations of raw video with VQ-VAE, and then training a GPT autoregressively. \nTransformers have been highly influential in computer vision too. The first model was Image Transformer . It restricts the self-attention mechanism to attend to local neighborhoods, so larger images can be processed. Class-conditioned generation is also supported, by passing the embedding of the relative class in input. To avoid restricting self-attention to local neighborhoods, Vision Transformer~ divides an image into fixed-size patches, linearly embeds each of them, adds position embeddings, and then feeds the resulting sequence of vectors to a standard Transformer encoder. Masked Autoencoders (MAE)  instead uses an encoder-decoder architecture based on Transformers trained with masked image modeling (i.e., to reconstruct randomly masked pixels). A BERT adaptation to images called Bidirectional Encoder representation from Image Transformers (BEiT)  has also been proposed. Masked image modeling has also been used together with classic autoregressive loss . Conversely, Vector Quantised-GAN (VQ-GAN)  allows a Transformer to be based on vector quantization. A GAN learns an effective codebook of image constituents. To do so, the generator is implemented as an auto-encoder; vector quantization is applied over the latent representation returned by the encoder. It is then possible to efficiently encode an image in a sequence corresponding to the codebook indices of their embeddings. The Transformer is finally trained on that sequence to learn long-range interactions. These changes also allow us to avoid quadratic scaling, which is intractable for high-resolution images. Finally, DALL-E  takes advantage of a discrete VAE. To generate images based on an input text, it learns a discrete image encoding; it concatenates the input text embedding with the image encoding; it learns autoregressively on them. CogView implements a similar architecture~.\nFinally, Transformer-based models have also been used in multimodal settings, in which data sources are of different types. A survey can be found in . The first examples of these systems consider text and images together as the output of the Transformer architecture. By aligning their latent representations, images and texts can be generated by Transformer-based decoders given a multimodal representation. For instance,  Contrastive Language-Image Pre-training (CLIP)  has an image encoder pre-trained together with a text encoder to generate a caption for an image. A Large-scale ImaGe and Noisy-text embedding (ALIGN) , based on similar mechanisms, can achieve remarkable performance through training based on a noisier dataset. In~ the authors propose a frozen language model for multimodal few-shot learning: a vision encoder is trained to represent each image as a sequence of continuous embeddings, so that the frozen language model prompted with this embedding can generate the appropriate caption. In~ the authors present Bridging-Vision-and-Language (BriVL), which performs multimodal tasks by learning from weak semantic correlation data. Finally, there is a trend toward even more complex multimodal models. For example, Video-Audio-Text Transformer (VATT)  learns to extract multimodal representations from video, audio, and text; instead, Gato  serializes all data (e.g., text, images, games, other RL-related tasks) into a flat sequence of tokens that is then embedded and passed to a standard large-scale language model. Similarly, Gemini  achieves state-of-the-art performance in multimodal tasks by working on interleaved sequences of text, image, audio, and video as inputs;  extends it to Mixture-of-Experts setting. Finally, NExT-GPT  handles any combination of four modalities (text, audio, image, and video) by connecting a language model with multimodal adaptors and diffusion decoders (see Section \\ref{diffusion}).", "cites": [5915, 1469, 8030, 5247, 793, 2564, 856, 1639, 681, 826, 1550, 2513, 8029, 8028, 1470, 4765, 3003, 2204, 1456, 680, 7040, 679, 8727, 7, 8033, 9149, 7361, 8031, 1150, 5914, 5916, 8032, 7463, 5342, 9, 732, 1552], "cite_extract_rate": 0.7872340425531915, "origin_cites_number": 47, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of various Transformer-based models across different domains, showing some synthesis by grouping them into categories (e.g., NLP, music, vision). However, it lacks in-depth critical evaluation of the models or their limitations. Some abstraction is evident in the discussion of multimodal and vision applications, but the overall analysis remains surface-level without a strong framework or nuanced critique."}}
{"id": "bce65676-65b3-441c-86a4-e679bede2e9d", "title": "Applications", "level": "subsubsection", "subsections": [], "parent_id": "fcc9765a-abe4-4945-86b3-2e2b1d1b5a29", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Generative Models"], ["subsection", "Transformer-Based Models"], ["subsubsection", "Applications"]], "content": "Transformer-based large language models can be used for almost any NLP task, including text summarization, generation, and interaction. In order to do so, the model can be used as frozen (i.e., to provide latent representations in input to other models); can be fine-tuned for the specific objective; can be exploited with zero-shot, one-shot or few-shot setting by prompting the task or few demonstrations in input. Transfer learning can instead be used to perform image classification by means of Transformer-based models trained on images. Other domain-specific techniques can be used as well: for instance, PlotMachines  learns to write narrative paragraphs not by receiving prompts, but by receiving plot outlines and representations of previous paragraphs.\nFrom a generative learning perspective, Transformers have shown impressive performance in producing long sequences of texts and music or speech , as well as in generating images based on input text. \nTheir application has not been limited to these data sources. For instance, AlphaFold uses a Transformer architecture to predict protein structure ; RecipeGPT employs it to generate recipes ; and GitHub Copilot relies on it to support code development .", "cites": [7465, 8034, 3004], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of Transformer-based models and their applications in various creative domains. While it integrates a few examples from cited papers (RecipeGPT, GitHub Copilot, and text-to-speech), it does so in a superficial manner without deeper synthesis or comparison. There is minimal critical evaluation or abstraction beyond the individual systems described."}}
{"id": "3dfaee1d-e121-49cf-9f9e-c2e4bf26d9a0", "title": "Critical Discussion", "level": "subsubsection", "subsections": [], "parent_id": "fcc9765a-abe4-4945-86b3-2e2b1d1b5a29", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Generative Models"], ["subsection", "Transformer-Based Models"], ["subsubsection", "Critical Discussion"]], "content": "Given the fact that the Transformers can be considered as an evolution of sequence prediction models, the observations made for that class of models (see Section \\ref{sequenceprediction}) apply also to them. However, the inherent characteristics of their architecture allow for larger models and higher-quality outputs, which are also able to capture a variety of dependencies of text across data sources. More in general, a broader conceptual space is induced. This means that domain-specific tasks might be addressed by means of solutions outside or at the boundary of the sub-space linked with that domain.\nMoreover, possibly also through careful use of inputs (see Section \\ref{inputbased}), their adoption might lead to transformational creativity. As far as Boden's criteria are concerned, there is no guarantee that the output of the Transformer architecture would be valuable, novel, or surprising, even though current state-of-the-art large language models (LLMs) achieve almost human-like performance in creative tests .", "cites": [5917, 8035], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a basic analytical perspective by linking the characteristics of Transformer-based models to broader concepts in creativity, referencing prior sections and citing two relevant papers. It connects ideas about model capabilities and input-based influence on creativity, but lacks deeper synthesis or novel frameworks. The critique is limited to noting the lack of guarantees regarding novelty, value, and surprise, without extensive evaluation of the cited works."}}
{"id": "ca9c4ede-d0f0-46b0-a336-85f216b5b95e", "title": "Core Concepts", "level": "subsubsection", "subsections": [], "parent_id": "78d3688b-85c7-4c04-80ed-092f8e0a336c", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Generative Models"], ["subsection", "Diffusion Models"], ["subsubsection", "Core Concepts"]], "content": "Diffusion models are a family of methods able to generate samples by gradually removing noise from a signal . The most representative approach is the Denoising Diffusion Probabilistic Model (DDPM) . An input $\\mathbf{x_0}$ is corrupted by gradually adding noise until obtaining an $\\mathbf{x_T}$ from a pre-defined distribution; the model then has to reverse the process. Each timestep $t$ corresponds to a certain noise level; $\\mathbf{x_t}$ can be seen as a mixture of $\\mathbf{x_0}$ with some noise $\\mathbf{\\epsilon}$ whose ratio is determined by $t$. The model learns a function $\\epsilon_{\\theta}$ to predict the noise component of $\\mathbf{x_t}$ by minimizing the mean-squared error. $\\mathbf{x_{t-1}}$ is then obtained from a diagonal Gaussian with mean as a function of $\\epsilon_{\\theta}\\!\\left(\\mathbf{x_t},t\\right)$, and with a fixed  or learned  variance. In other words, it learns to associate points from a predefined random distribution with real data through iterative denoising. Because of this, at inference time, a diffusion model can generate a new sample by starting from pure random noise. The generation can also be conditioned by simply modifying the noise perturbation so that it depends on the conditional information. However, this iterative sampling process might potentially lead to slow generation; a proposed solution is to induce self-consistency, i.e., ensuring that points on the same trajectory map to the same initial ones . In this way, the output can be obtained in a single step.\nThe aforementioned diffusion process is similar to the one followed by score-based generative models . Instead of noise, here a model is trained to learn the score, i.e., the gradient of the log probability density with respect to real data. The samples are then obtained using Langevin dynamics . Despite the differences, both of them can be seen as specific, discrete cases of Stochastic Differential Equations .", "cites": [5921, 5920, 4827, 5919, 8036, 5918], "cite_extract_rate": 0.75, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes core concepts of diffusion models from multiple foundational papers, linking DDPMs with score-based models and consistency models. It provides a critical perspective by highlighting the trade-off between iterative sampling and generation speed, and mentions proposed solutions like self-consistency. The abstraction is strong, as it frames diffusion models within the broader context of stochastic differential equations and generative modeling paradigms."}}
{"id": "f150754b-b400-4c85-a309-7f7e3784b216", "title": "Examples of Models", "level": "subsubsection", "subsections": [], "parent_id": "78d3688b-85c7-4c04-80ed-092f8e0a336c", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Generative Models"], ["subsection", "Diffusion Models"], ["subsubsection", "Examples of Models"]], "content": "Diffusion models have been primarily used for image generation. In order to generate higher-quality images and to allow text-to-image generation, a variety of effective methods for conditioning have been proposed. \nA possibility is to use classifier guidance : the diffusion score (i.e., the added noise) includes the gradient of the log-likelihood of an auxiliary classifier model. An alternative is classifier-free guidance : to avoid learning an additional model, a single neural network is used to parameterize two diffusion models, one conditional and one unconditional; the two models are then jointly trained by randomly setting the class for the unconditional model. Finally, the sampling is performed using a linear combination of conditional and unconditional score estimates. Guided Language to Image Diffusion for Generation and Editing (GLIDE)  demonstrates how classifier-free guidance can be effectively used to generate text-conditional images. In addition, it shows how diffusion models can be used for image editing by fine-tuning in order to reconstruct masked regions. Performance improvement can be obtained by means of a cascade of multiple diffusion models performing conditioning augmentation . Notably, the diffusion model can operate on latent vectors instead of real images. Stable Diffusion  employs a diffusion model in the latent space of a pre-trained autoencoder. Similarly, DALL-E 2  generates images by conditioning with image representations. At first, it learns a prior diffusion model to generate possible CLIP image embeddings from a given text caption, i.e., conditioned by its CLIP text embedding. Then, a diffusion decoder produces images conditioned by the image embedding. The generation quality can be further improved by means of generated captions for the images in the training set . Imagen  uses instead a cascaded diffusion decoder, together with a frozen language model as a text encoder to increase the quality of output.\nAlthough the approach is particularly suitable for images, applications to other data sources have been developed as well. DiffWave  and WaveGrad  use diffusion models to generate audio. They overcome the continuous-discrete dichotomy by working on waveform. Another possibility is to use an auto-encoder like MusicVAE  to transform the sequence into a set of continuous latent vectors, on which training a diffusion model . Resembling image generators, Contrastive Language-Audio Pretraining (CLAP) embeddings  can be used to generate audio by conditioning on text descriptions . Diffusion-LM  employs diffusion models to write text by denoising a sequence of Gaussian vectors into continuous word vectors (then converted into discrete words by a rounding step); DiffuSeq  performs sequence-to-sequence generation tasks by embedding source and target sequences in the same embedding space through a Transformer architecture. Diffusion models have been used for 3D generation as well . Finally, diffusion models for video have also been proposed, based on gradient-based conditioning , and on processing latent spacetime patches. In particular, with respect to the latter, Sora  first turns videos into sequences of patches and then uses a diffusion Transformer to predict the original patches from random noise (and conditioning inputs like text prompts), improving sample quality and flexibility.", "cites": [8038, 5927, 5922, 2559, 8037, 5925, 165, 4090, 5924, 7959, 5923, 2558, 5926, 2240, 7465], "cite_extract_rate": 0.7894736842105263, "origin_cites_number": 19, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 2.8}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of diffusion models, listing various implementations and their applications. It integrates multiple cited works into a coherent narrative about conditioning strategies and data domains, but lacks deeper critical evaluation of their strengths and limitations. Some generalization is present, such as the use of latent spaces across modalities, but the analysis remains at a surface level without identifying overarching principles or trends."}}
{"id": "776063a6-07a9-4dff-8f0f-fa06590a9f72", "title": "Applications", "level": "subsubsection", "subsections": [], "parent_id": "78d3688b-85c7-4c04-80ed-092f8e0a336c", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Generative Models"], ["subsection", "Diffusion Models"], ["subsubsection", "Applications"]], "content": "Despite their recent introduction, diffusion models have been used to generate audio, music, and video, as well as to generate and edit images conditioned on input text, e.g., with in-painting  or subject-driven generation ; we refer to  for a comprehensive survey of this area.\nIndeed, they lead to higher-quality outputs than the previous state-of-the-art models. In particular, DALL-E 2 and Stable Diffusion have been able to produce images from textual instructions with superior fidelity and variety.", "cites": [8039, 5928], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a brief, factual overview of diffusion model applications, citing two relevant papers but without integrating their contributions into a broader narrative. There is minimal critical evaluation or comparison of the cited works, and no abstraction to identify overarching principles or trends in the use of diffusion models for creative tasks."}}
{"id": "b20cbe4f-3485-4496-89cb-b52c86f21817", "title": "Examples of Models", "level": "subsubsection", "subsections": [], "parent_id": "1fd7a3bc-b11f-41d1-ab39-6560d8d2fd88", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Generative Models"], ["subsection", "Reinforcement Learning-Based Methods"], ["subsubsection", "Examples of Models"]], "content": "A first example is Objective-Reinforced GAN (ORGAN) .\nHere, RL is used not only to adapt GANs to sequential tasks but also to provide additional learning signals, such as rewards from specific-domain objectives (e.g., tonality and ratio of steps for music generation). Also  follows this path by using rewards like fluency, coherence, meaningfulness, and overall quality to generate poems. Another possibility is to use the metrics used at test time (e.g., BLEU or ROUGE) . Instead, RL-DUET  casts online music accompaniment generation as an RL problem with an ensemble of reward models, i.e., autoregressive models trained with or without the whole context, and with or without the human-produced counterpart. In this way, inter-coherence (between humans and machines) and intra-coherence can be obtained. Finally, Intelli-Paint  can paint in human style by using a sequential planner that learns a painting policy to predict vectorized brushstroke parameters from the current state.\nRL can also be used to fine-tune a pre-trained generative model. Doodle-SDQ  first learns to draw simple strokes using supervised learning; then, it improves its generation by means of rewards about similarity, color, and line movement. Conversely, the authors of  suggest to consider a pre-trained LSTM language model as a policy model. Fine-tuning then aims at maximizing the probability that a given event occurs at the end of the narrative. RL Tuner  uses RL to fine-tune a Note-RNN  to produce music that follows a set of music theory rules. To avoid forgetting note probabilities learned from the data, the probability value returned by a copy of the pre-trained Note-RNN can be used as an additional reward. Sequence Tutor  generalizes this idea of learning a policy that trades off staying close to the data distribution while improving performance on specific metrics. A comprehensive critical discussion of the rewards for RL-based generative models can be found in .\nFinally, RL can be used to help models follow human preferences  or feedback. The latter technique is referred to as Reinforcement Learning from Human Feedback (RLHF) . For example, ChatGPT  is an interactive version of GPT-3  (initially) and GPT-4  (at the time of writing), fine-tuned to maximize a learned reward of human values. RLHF improves its conversational skills while mitigating mistakes and biases; because of this, it has become a standard de facto for fine-tuning large language models, e.g., in . It is also possible to use AI and not human feedback . However, RLHF has some limitations , and alternative RL-free strategies are increasingly popular (e.g., ).", "cites": [9115, 8042, 1354, 8040, 5929, 679, 2445, 8041, 5930, 2389, 5931, 8472, 8043], "cite_extract_rate": 0.5909090909090909, "origin_cites_number": 22, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by connecting various RL-based generative models and explaining how they use rewards for different creative tasks (e.g., music, painting, language). It includes critical analysis, especially regarding the limitations of RLHF and the rise of alternative strategies. The abstraction is evident in the identification of broader patterns, such as the use of RL for fine-tuning pre-trained models and aligning with human preferences."}}
{"id": "59159a6d-cc29-49b9-adb7-6c94a3bcd934", "title": "Applications", "level": "subsubsection", "subsections": [], "parent_id": "1fd7a3bc-b11f-41d1-ab39-6560d8d2fd88", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Generative Models"], ["subsection", "Reinforcement Learning-Based Methods"], ["subsubsection", "Applications"]], "content": "As seen, RL-based models can be used to fully train or fine-tune generative models for different tasks; ideally, for any task that could benefit from domain-specific objectives. This is the case of music and molecule generation , but also of dialogue generation , image generation  and painting . In addition, the sequential nature of RL can help as well in all the tasks requiring to deal with new directives during generation (e.g., music interaction). Finally, RLHF can be used to directly optimize models for creative tasks, e.g., poetry .", "cites": [3834, 8040, 5932, 1348], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of the applications of reinforcement learning in creative tasks, referencing specific domains like painting, music, dialogue, and poetry. It connects the cited papers by highlighting the common theme of using RL for domain-specific optimization, but lacks in-depth comparison, critique, or abstraction to broader principles. While it begins to show some integration, it remains primarily a surface-level summary of applications."}}
{"id": "01c2be26-38e3-43d8-86f2-645cb18f2afc", "title": "Examples of Models", "level": "subsubsection", "subsections": [], "parent_id": "0499071e-70b3-4a75-8c97-489bf487dec2", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Generative Models"], ["subsection", "Input-Based Methods"], ["subsubsection", "Examples of Models"]], "content": "The first approach consists of carefully modifying the input of a generative model until the output matches the desired properties. The main example is VQGAN-CLIP . Given a text description, VQGAN produces a candidate image from a random latent vector; the vector is then optimized by minimizing the distance between the embeddings of the description and the candidate image. Both embeddings are computed using CLIP . Variants can be implemented as in Wav2CLIP , where an audio encoder is learned to match the CLIP encoders so that VQGAN-CLIP can be used from raw audio; or as in music2video , where videos are generated from audio a frame after the other by both minimizing the distance between subsequent frames, and the distance between image and music segment embedded by Wav2CLIP. In addition to the random latent vector, the text or audio description can be optimized as well. This can be performed by the users through many iterations of careful adjustments, or by means of an automated procedure. The latter is commonly known as prompt tuning. Prompt tuning is about producing prompts via backpropagation; the optimized prompts can then condition frozen language models in order to perform specific tasks without having to fine-tune them . An additional model can also be trained to output the desired prompt . Finally, image generators such as VQGAN can also be exploited in other ways, i.e., with binary-tournament genetic algorithm  or more complex evolution strategies . Another possibility is to optimize the input so that the generated output maximizes a target neuron of an image classifier . This helps generate what that neuron has learned. The desired latent vector can also be produced by an additional model .\nThe second approach consists of optimizing the inputs to transform them into the desired outputs. DeepDream  generates ``hallucinated'' images by modifying the input to maximize the activation of a certain layer from a pre-trained classifier. Artistic style transfer is based on the same idea. Given an input image and a target image, the former is modified by means of both style and content losses thanks to a pre-trained classifier. The content loss is minimized if the current and the original input images generate the same outputs from the hidden layers. The style loss is minimized if the current and target images have the same pattern of correlation between feature maps in the hidden layers . Control over results can be improved by considering additional losses about color, space, and scale .", "cites": [5937, 5934, 8045, 1639, 5935, 8044, 2083, 5933, 5936, 8536], "cite_extract_rate": 0.7692307692307693, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a coherent synthesis of input-based generative models by grouping related methods and explaining their shared principles, such as optimizing inputs to achieve desired outputs. It includes some critical analysis, such as the distinction between manual and automated prompt tuning, and the potential of evolution strategies over traditional gradient-based approaches. While it identifies broader patterns in input optimization, it does not fully abstract to a meta-level framework or provide deep critiques of the limitations of these methods."}}
{"id": "c0052677-10ac-4457-b702-0bc8c96449f7", "title": "Applications", "level": "subsubsection", "subsections": [], "parent_id": "0499071e-70b3-4a75-8c97-489bf487dec2", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Generative Models"], ["subsection", "Input-Based Methods"], ["subsubsection", "Applications"]], "content": "Input-based methods can be used with any generative model to produce the desired output. With language models, they can exploit their generality in several specific tasks without fine-tuning them. For instance, prompt tuning can be used by writers for co-creation  or to force LLMs to \\textit{brainstorm} . With image generators, they can obtain drawings adherent to given descriptions, or high-quality but yet peculiar paintings like colorist , abstract  or alien  artworks. We believe applications to other domains are yet to come. Both types of input-based methods can be used not only to produce desired outputs or to transfer styles; they can also be used to better analyze what is inside the network .", "cites": [5934, 7566, 5935, 5936], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates the cited papers by linking input-based methods across different domains like language and image generation, and it touches on both creative output and internal analysis. However, the synthesis is somewhat limited and does not present a novel framework. The critical analysis is minimal, as the section does not deeply evaluate or contrast the strengths and weaknesses of the cited works. It does offer some abstraction by suggesting broader potential for input-based methods beyond current applications."}}
{"id": "593cd1e1-91fe-46f4-a3af-16a976eaa38f", "title": "Practical Assessment of Creativity-Oriented Methods", "level": "subsection", "subsections": [], "parent_id": "c94d844b-5541-4cd4-b103-5bcd3c417722", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Generative Models"], ["subsection", "Practical Assessment of Creativity-Oriented Methods"]], "content": "\\label{creativityoriented}\nWe conclude this analysis of generative models with a discussion of how they might increase their \\textit{creativity} according to Boden's definition. We have discussed how the presence of a recognition model (e.g., a discriminative model or a reward model) helps ensure the value of the products. In the same way, novelty and surprise can be fostered by the integration of other components.\nA straightforward way to obtain novel and surprising outputs is to train a generative model by means of novelty and surprise objectives. This is the core idea behind Creative Adversarial Network (CAN) . In addition to the classic discriminative signal, i.e., a value loss, the generator is also trained to optimize a novelty loss. This is defined as the deviation from style norms, i.e., the error related to the prediction of the style of the generated image. The sum of the two training signals helps the model learn to produce artworks that are different (in style) from the training data. The same approach has been used to develop a creative StyleGAN, i.e., StyleCAN . \nAnother, very simple way to augment the training signal of a generative model with \\textit{creativity-oriented} objectives is by means of RL-based methods.\nThe choice of the reward structure is the fundamental element in the design of effective generative reinforcement learning systems. Rewards should teach the model to generate an output with a high level of novelty and surprise. An example is ORGAN , where appropriate reward functions can be used. For instance, statistical measures (e.g., Chi-squared) or metrics of distance between distributions (e.g., KL divergence) might be used to ground ideas of novelty and surprise.\nAnother possibility is the development of an input-based method where the input is optimized to obtain a product that is valuable, novel, and surprising. This may be achieved either by forcing a further exploration of the latent space (e.g., by means of evolutionary search ), or by defining appropriate loss functions to perform gradient descent over the input. All these methodologies are also called \\textit{active divergence}  since they aim to generate in ways that do not simply reproduce training data. A survey on active divergence can be found in . A different output can also be obtained by carefully altering the probability distribution of the model, e.g., by scaling its probabilities with learned functions to maximize target properties .\nA different approach is followed by the Composer-Audience architecture . Two models are considered: the Audience, a simple sequence prediction model trained on a given dataset; the Composer, another sequence prediction model trained on a different dataset. In addition, the Composer also receives the next-token expectations from the Audience, and it learns when to follow its guidance and when to diverge from expectations, i.e., when to be surprising. For instance, it can learn to produce jokes by considering non-humorous texts to train the Audience, and humorous texts to train the Composer. Even though this approach is useful for learning how to generate valuable and surprising output, it is only applicable when paired datasets are available.\n\\begin{table}[t]\n    \\centering\n    \\begin{tabular}{cccc}\n        \\toprule\n        Generative family & Type of creativity & Boden's criteria & Creative suggestions \\\\\n        \\midrule\n        VAE & Exploratory & \\specialcell[l]{$\\pmb{\\sim}$ Value \\\\ $\\pmb{\\sim}$ Novelty \\\\ $\\pmb{\\sim}$ Surprise} & \\specialcell{Creativity-oriented \\\\ input-based methods} \\\\ \n        \\cmidrule(r){1-4}\n        GAN & Exploratory & \\specialcell[l]{$\\pmb{\\checkmark}$ Value \\\\ $\\pmb{\\sim}$ Novelty \\\\ $\\pmb{\\sim}$ Surprise} & \\specialcell{CAN; \\\\ Creativity-oriented \\\\ input-based methods} \\\\\n        \\cmidrule(r){1-4}\n        \\specialcell{Sequence \\\\ prediction \\\\ model} & \\specialcell{Combinatorial, \\\\ Exploratory} & \\specialcell[l]{$\\pmb{\\sim}$ Value \\\\ $\\pmb{\\sim}$ Novelty \\\\ $\\pmb{\\times}$ Surprise} & \\specialcell{Composer-Audience; \\\\ Creativity-oriented \\\\ RL-based methods} \\\\ \n        \\cmidrule(r){1-4}\n        \\specialcell{Transformer-\\\\ based \\\\ models} & \\specialcell{Combinatorial, \\\\ Exploratory, \\\\ Transformational} & \\specialcell[l]{$\\pmb{\\sim}$ Value \\\\ $\\pmb{\\sim}$ Novelty \\\\ $\\pmb{\\times}$ Surprise} & \\specialcell{Creativity-oriented \\\\ prompt tuning or \\\\ RL-based methods} \\\\\n        \\cmidrule(r){1-4}\n        \\specialcell{Diffusion \\\\ models} & Exploratory & \\specialcell[l]{$\\pmb{\\sim}$ Value \\\\ $\\pmb{\\sim}$ Novelty \\\\ $\\pmb{\\sim}$ Surprise} & \\specialcell{Creativity-oriented \\\\ input-based methods} \\\\\n        \\cmidrule(r){1-4}\n        \\specialcell{RL-based \\\\ methods} & \\specialcell{Combinatorial, \\\\ Exploratory, \\\\ Transformational} & \\specialcell[l]{$\\pmb{\\checkmark}$ Value \\\\ $\\pmb{\\sim}$ Novelty \\\\ $\\pmb{\\sim}$ Surprise} & \\specialcell{Intrinsic rewards; \\\\ Novelty-based rewards} \\\\\n        \\cmidrule(r){1-4}\n        \\specialcell{Input-based \\\\ methods} & \\specialcell{Exploratory, \\\\ Transformational} & \\specialcell[l]{$\\pmb{\\checkmark}$ Value \\\\ $\\pmb{\\sim}$ Novelty \\\\ $\\pmb{\\sim}$ Surprise} & \\specialcell{Evolutionary search; \\\\ Novelty-based optimization} \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{Summary of all the methods explained so far, considering their type of creativity as discussed in the corresponding subsections; the possible presence of Boden's criteria ($\\pmb{\\checkmark}$ if induced by the training process; $\\pmb{\\sim}$ if not considered; $\\pmb{\\times}$ if excluded); and some practical suggestions to achieve a higher degree of creativity.}\n    \\label{tab:table_generative_models}\n\\end{table}\nAs far as the type of creativity is concerned, there can be ways to achieve a better exploration or even transformation of the space of solutions. For example, since CAN novelty loss is used during training, it learns to diverge from the distribution of real data. The same is true for RL-based methods with novelty and surprise rewards (especially if the training happens from scratch). Finally, a more explored or transformed space may be reached using RL-based methods driven by curiosity : an agent can learn to be creative and discover new patterns thanks to intrinsic rewards to measure novelty, interestingness, and surprise. This can be done by training a predictive model of the growing data history and by using its learning progress as the reward. In this way, the agent is motivated to make things the predictor does not yet know. If an external qualitative reward is considered as well, the agent should in theory learn to do things that are new, but still valuable . The same idea can also be applied to different techniques like evolutionary strategies . Deep Learning Novelty Explorer (DeLeNoX)  uses a denoising autoencoder to learn low-dimensional representations of the last generated artifacts. Then, a population of candidate artifacts (in terms of their representation) is evolved through a feasible-infeasible novelty search  in order to maximize the distances between them, i.e., to increase their novelty, while still considering qualitative constraints. Other evolutionary strategies might be considered as well to search the space of artifacts for novel  and surprising  results. Instead of relying on manually crafted metrics, Quality Diversity through Human Feedback (QDHF)  uses human feedback for computing quality and distance in learned latent projection for computing diversity. Quality-Diversity through AI Feedback (QDAIF)  makes the model more independent in searching and innovating by totally relying on its own feedback for both quality and diversity.\nTable \\ref{tab:table_generative_models} summarizes all the generative approaches discussed in this section, highlighting their characteristics from a machine creativity perspective.", "cites": [8048, 8047, 8040, 5938, 2499, 8046, 7353, 3135], "cite_extract_rate": 0.4, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.5, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers by connecting creativity-oriented methods like CAN, RL-based approaches, and evolutionary strategies under a unified framework. It abstracts these methods to broader concepts such as 'active divergence' and Boden's criteria for creativity, and provides some critical evaluation of their limitations (e.g., requirement for paired datasets in Composer-Audience). However, it could offer more nuanced critique of the assumptions and trade-offs in these methods."}}
{"id": "38ef4e7f-f690-439f-a981-933eb614489a", "title": "Creativity Measures", "level": "section", "subsections": ["05e3509c-e2b4-46d7-9e50-0b096ab2d030", "98757c24-cb13-4236-997c-fa9af4883ff6", "3ce40056-7d43-4bd6-a2db-30a27f4ad3ec", "6b9b0040-1f35-45e5-8af6-b6aec2b88986", "70b2798d-fe4a-458d-a629-53f2c815e071", "406841f9-af4a-4263-9d4f-62bc48af7dbc", "c071b1fc-99a5-49d7-9f73-048c814f57a3", "c7a78b6b-2587-453c-92b4-3ee6375b698e", "a878e429-cf78-404e-b515-05350e141397", "6a057b7e-56a5-4d1c-8610-d3983b142f3e", "cca81013-0cbd-47d5-9e5f-50b5407e6287"], "parent_id": "e3f6de83-1921-4425-92d8-01891c4414bf", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Creativity Measures"]], "content": "\\label{creativitymeasures}\nIn this section, we present different methodologies to evaluate the creativity of artifacts generated by artificial agents. These can typically be extended to human-generated artifacts. For each of them, we explore the core concepts, the dimensions of creativity that are considered, the evaluation protocol, and, finally, we critically assess them.\nThe presence of several different proposals can be associated with the fact that it is not always straightforward to determine the â€˜â€˜rightâ€™â€™ question to ask in an evaluation of a creative artifact . For instance, the Generation, Evaluation, and Metrics (GEM) Benchmark for natural language generation  does not contain any creativity evaluation measure. This is due to the inadequacy of the creative metrics proposed to date to correctly capture and measure all the necessary dimensions of creativity. The focus of our overview is on measures that are based on (or associated with) machine learning techniques. It is worth noting that some of them might be calculated without using machine learning, but we will refer to an implementation based on the latter. For an in-depth overview of creativity measures not strictly related to machines, we refer to .\nTable \\ref{tab:table_creativity_measures} reports all the evaluation methods considered in this section, highlighting the dimensions they try to capture, their applicability, and their limitations. We will discuss these aspects in the remainder of this section.\n\\begin{table}[htp]\n    \\centering\n    \\begin{tabular}{ccccl}\n        \\toprule\n        Name & What evaluates & How evaluates & Applicability & \\centercell{Limits}\n        \\\\\n        \\midrule\n        \\specialcell{Lovelace 2.0 \\\\ Test} & \\specialcell{Evaluators' \\\\creativity definition} & \\specialcell{Mean number of \\\\challenges per evaluator} & General & \\specialcell[l]{\n            \\tabitem Requires a substantial\\\\ human intervention} \\\\ \\cmidrule(r){1-5}\n        \\specialcell{Ritchie's \\\\ criteria} & \\specialcell{Quality, \\\\ novelty, \\\\typicality} & \\specialcell{Human opinions \\\\ (then elaborated \\\\ through 18 criteria)} & General & \\specialcell[l]{\n            \\tabitem Requires human evaluation \\\\\n            \\tabitem Requires to state thresholds \\\\\n            \\tabitem No innovation definition} \\\\ \\cmidrule(r){1-5}\n        FACE & \\specialcell{Tuples of \\\\ generative \\\\ acts} & \\specialcell{Volume of acts, number \\\\ of acts, quality (through\\\\ aesthetic measure)} & General & \\specialcell[l]{\n            \\tabitem Abstract method \\\\ \n            \\tabitem Definition of aesthetic \\\\ measure left to the user} \\\\ \\cmidrule(r){1-5}\n        SPECS & \\specialcell{What we state \\\\ creativity is} & \\specialcell{Identification and test \\\\ of standards for the \\\\ creativity components} & General & \\specialcell[l]{\n            \\tabitem More a framework for eva-\\\\luation method definition \\\\ than a real method} \\\\ \\cmidrule(r){1-5}\n        \\specialcell{Creativity \\\\ implication \\\\ network} & \\specialcell{Value,\\\\ novelty} & \\specialcell{Similarity between works \\\\ (considering subsequent \\\\ works for value and pre-\\\\vious works for novelty)} & General & \\specialcell[l]{\n            \\tabitem Not possible to accurately\\\\ measure the creativity of\\\\\n            the most recent works \\\\ \n            \\tabitem Wrong creativity and \\\\ time-positioning correlation} \\\\ \\cmidrule(r){1-5}\n        \\specialcell{Chef Watson \\\\ (assessment part)} & \\specialcell{Novelty,\\\\ quality} & \\specialcell{Bayesian surprise, smell \\\\ pleasantness regression} & \\specialcell{Specific \\\\ (recipes)} & \\specialcell[l]{\n            \\tabitem Requires human ratings \\\\ of pleasantness} \\\\ \\cmidrule(r){1-5}\n        DARCI & \\specialcell{Art \\\\ appreciation} & \\specialcell{Neural network to \\\\ associate image featu-\\\\ res and description} & \\specialcell{Specific \\\\ (visual art)} & \\specialcell[l]{\n            \\tabitem Not based on product \\\\\n            \\tabitem Considers just one of \\\\ the creative tripod} \\\\ \\cmidrule(r){1-5}\n        \\specialcell{PIERRE - \\\\ Evaluation part} & \\specialcell{Novelty,\\\\ quality} & \\specialcell{Count of new combi-\\\\nations; user ratings} & \\specialcell{Specific \\\\ (recipes)} & \\specialcell[l]{\n            \\tabitem Requires user ratings \\\\ over ingredients} \\\\ \\cmidrule(r){1-5}\n        \\specialcell{EVE'} & \\specialcell{Feelings,\\\\ meanings} & \\specialcell{Negative log of prediction \\\\ and posterior probability} & \\specialcell{General} & \\specialcell[l]{\n            \\tabitem Requires a way to explain \\\\\n            \\tabitem Value only through meaning} \\\\ \\cmidrule(r){1-5}\n        \\specialcell{Common model \\\\ of creativity \\\\ for design} & \\specialcell{Novelty, \\\\ value, \\\\ surprise} & \\specialcell{K-Means on a description \\\\ space and a performance \\\\ space; degree of violation \\\\ of anticipated patterns} & \\specialcell{Specific \\\\(design)} & \\specialcell[l]{\n            \\tabitem Requires to define \\\\ attribute-value pairs \\\\\n            \\tabitem Requires to define \\\\ clustering parameters} \\\\ \\cmidrule(r){1-5}\n        Unexpectedness & \\specialcell{Novelty, sur-\\\\prise, trans-\\\\formativity} & \\specialcell{Possibility to update,\\\\ and degree of violation \\\\ of expectations} & General & \\specialcell[l]{\n            \\tabitem Does not take care of value} \\\\ \\cmidrule(r){1-5}\n        \\specialcell{Essential criteria \\\\ of creativity} & \\specialcell{Value, \\\\ novelty, \\\\ surprise} & \\specialcell{Sum of performance va-\\\\ riables, distance between\\\\ artifacts and between re-\\\\ al and expected artifact} & General & \\specialcell[l]{\n            \\tabitem Requires to define \\\\ performance variables \\\\\n            \\tabitem Requires to define \\\\ clustering parameters} \\\\ \\cmidrule(r){1-5}\n        \\specialcell{Computational \\\\ metrics for \\\\ storytelling} & \\specialcell{Novelty, rarity, \\\\ recreational \\\\ effort, surprise} & \\specialcell{Distance between do-\\\\minant terms, consecutive \\\\ fragments/clusters of terms} & \\specialcell{Specific \\\\ (storytelling)} & \\specialcell[l]{\n            \\tabitem Requires to define domination \\\\\n            \\tabitem Requires to define \\\\ clustering parameters} \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{Summary of creativity evaluation methods and their characteristics.}\n    \\label{tab:table_creativity_measures}\n\\end{table}", "cites": [7127], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.0, "critical": 4.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a structured overview of various creativity measures and compares their evaluation dimensions, protocols, applicability, and limitations. It critically addresses the inadequacy of current metrics, especially in the context of machine learning, but lacks deeper synthesis across sources and does not fully abstract overarching principles. The inclusion of a table enhances comparative clarity."}}
{"id": "458ad3c7-f1fa-45a8-9609-226a447d4b00", "title": "Overview", "level": "subsubsection", "subsections": [], "parent_id": "05e3509c-e2b4-46d7-9e50-0b096ab2d030", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Creativity Measures"], ["subsection", "Lovelace 2.0 Test"], ["subsubsection", "Overview"]], "content": "The Lovelace Test (LT)  was proposed in 2001 as a \\textit{creativity-oriented} alternative to the world-famous Turing test . More formally, LT is defined as follows:\n\\begin{definition}\nAn artificial agent $A$, designed by $H$, passes LT if and only if:\n1) $A$ outputs $o$;\n2) $A$'s outputting $o$ is not the result of a fluke hardware error but of processes $A$ can repeat;\n3) $H$ (or someone who knows what $H$ knows, and has $H$'s resources) cannot explain how $A$ produced $o$ by appealing to $A$'s architecture, knowledge-base, and core functions.\n\\end{definition}\nLT provides several insights for understanding and quantifying machine creativity, but it is rather abstract. For these reasons, a 2.0 version has been proposed . The so-called Lovelace 2.0 Test is defined as:\n\\begin{definition}\nArtificial agent $A$ is challenged as follows:\n1) $A$ must create an artifact $o$ of type $t$;\n2) $o$ must conform to a set of constraints $C$ where $c_i \\in C$ is any criterion expressible in natural language;\n3) a human evaluator $h$, having chosen $t$ and $C$, is satisfied that $o$ is a valid instance of $t$ and meets $C$;\n4) a human referee $r$ determines the combination of $t$ and $C$ to not be unrealistic for an average human.\n\\end{definition}", "cites": [8049], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear definition of the Lovelace Test and its 2.0 version, synthesizing the core elements from the cited paper. It includes some critical commentary by noting the abstract nature of the original test and the improvements in the 2.0 version. However, the analysis remains limited to the definitions and does not deeply compare or evaluate the implications of the test across different frameworks or broader trends in machine creativity."}}
{"id": "c59e5bcb-983b-404f-9600-9e19d37fbbaa", "title": "Overview", "level": "subsubsection", "subsections": [], "parent_id": "70b2798d-fe4a-458d-a629-53f2c815e071", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Creativity Measures"], ["subsection", "Creativity Implication Network"], ["subsubsection", "Overview"]], "content": "A different method of quantifying creativity is based on building an art graph called Creativity Implication Network . Given a set of artworks, a directed graph can be defined by considering a vertex for each artwork. More specifically, an arc connecting artwork $p_i$ to $p_j$ is inserted if $p_i$ has been created before $p_j$. A positive weight $w_{ij}$ quantifying the similarity score between the two artworks under consideration is associated with each arc. The creativity of an artwork is then derived by means of computations on the resulting graph.", "cites": [5939], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the Creativity Implication Network without integrating it with other creativity measures or critically analyzing its strengths and limitations. It presents the method in isolation, following the structure of the cited paper closely without deeper synthesis or abstraction."}}
{"id": "125f1062-541b-4e6d-8b21-c7b113236730", "title": "Critical Examination", "level": "subsubsection", "subsections": [], "parent_id": "70b2798d-fe4a-458d-a629-53f2c815e071", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Creativity Measures"], ["subsection", "Creativity Implication Network"], ["subsubsection", "Critical Examination"]], "content": "The Creativity Implication Network represents an effective way to deal with the creativity of sets of artworks. It considers both value and novelty, and it allows for using automated techniques in the computation of similarity. On the other hand, two potential drawbacks should be highlighted. The first one is related to artworks that occupy the position of ``leaves'' in the graph: if there are no subsequent works in the graph, their creativity would only be based on novelty, and not on value. \nThe second one is more subtle, and it is about time-positioning. As demonstrated by , moving back an artwork has the effect of increasing its creativity; however, this appears conceptually wrong. As discussed in , the time location of an artwork is fundamental in the quantification of its creativity. It may happen that, due to the surprise component of creativity, an artwork that appears too \\textit{early} might not be considered as surprising because observers are not able to truly understand it; on the contrary, if it appears too \\textit{late}, it might be considered as obvious and not surprising at all. In conclusion, even if this approach is able to correctly capture value and novelty, it cannot capture the concept of surprise.", "cites": [5939], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 4.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a focused analytical critique of the Creativity Implication Network, highlighting its strengths in capturing value and novelty but pointing out conceptual issues with time-positioning and the omission of surprise for leaf artworks. While it draws on a single cited paper for its argument, it identifies meaningful limitations and evaluates the framework beyond mere description. The analysis is conceptually grounded but lacks synthesis with broader literature or abstraction to higher-level principles."}}
{"id": "05dce329-a72d-484b-bb4f-2d87481deae0", "title": "Outlook and Conclusion", "level": "section", "subsections": [], "parent_id": "e3f6de83-1921-4425-92d8-01891c4414bf", "prefix_titles": [["title", "Creativity and Machine Learning: A Survey"], ["section", "Outlook and Conclusion"]], "content": "\\label{conclusion}\nIn this survey, we have provided the reader with an overview of the state of the art at the intersection between creativity and machine learning. Firstly, we have introduced the concept of machine creativity, including key concepts and definitions. Secondly, we have described a variety of generative learning techniques, considering their potential applications and limitations. Finally, we have discussed several evaluation frameworks for quantifying machine creativity, highlighting their characteristics and the dimensions they are able to capture.\nEven if the field of machine creativity has witnessed increasing interest and popularity in recent years, there are still several open challenges. First of all, an interesting direction is the exploration of  \\textit{creativity-oriented} objective functions, to directly train models to be creative or to navigate the induced latent space to find creative solutions. Another open problem is the definition of techniques to explore or transform the space of solutions. \nA fundamental area is the definition of novel and robust evaluation techniques for both generated and real artifacts. As discussed in Section \\ref{creativitymeasures}, deep learning might be used as a basis for the definition of metrics for machine creativity.\nIt should be noted that there is also an ongoing debate about the role of human versus machine evaluation .\nAnother promising research direction concerns the machine interpretation of art . Moreover, machine learning techniques might also be used to investigate psychological dimensions of creativity~.\nThere are also foundational questions related to generative deep learning and copyright . For example, it is not clear if machine-generated works could be protected by Intellectual Property, and, if they are, who should be the owner of the related rights . In addition, other problems concerning copyright should be considered, such as if and when training over protected work is permitted . \nAnother important ongoing debate is about authorship and the human role in creative fields in the era of AI\\footnote{This is one of the ethical dilemmas highlighted by UNESCO in its \\href{https://unesdoc.unesco.org/ark:/48223/pf0000367823}{Preliminary study on the Ethics of Artificial Intelligence}.}.\nThe models and framework discussed in this work show the remarkable potential of generative learning for machine creativity. We hope that this survey will represent a valuable guide for researchers and practitioners working in this fascinating area.\n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{main}\n\\end{document}\n\\endinput", "cites": [5940], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a coherent analytical overview of current challenges and future directions in machine creativity, referencing broader themes like evaluation frameworks and copyright. It synthesizes some ideas across the survey and the cited paper on copyright, but does not deeply compare or critique multiple sources. The abstraction is moderate, as it identifies general research directions and ethical issues rather than offering high-level conceptual unification."}}
