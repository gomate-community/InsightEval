{"id": "87ccd777-d058-4a76-9575-0c136cb10697", "title": "Introduction", "level": "section", "subsections": ["d77f2bd6-4077-4eb5-ba4e-7c6288149714", "846c213c-7851-4f7e-b5f1-17d26ed0cdb1"], "parent_id": "515509a6-319a-4385-b1b5-4418a83d2bc2", "prefix_titles": [["title", "Deep Learning for Instance Retrieval: A Survey"], ["section", "Introduction"]], "content": "\\label{sec:intro}}\n\\IEEEPARstart{C}{ontent} Based Image Retrieval (CBIR) is the problem of searching for relevant images in an image gallery by analyzing the visual content (colors, textures, shapes, objects \\emph{etc.}), given a query image ,. CBIR has been a longstanding research topic in the fields of computer vision and multimedia ,. With the exponential growth of image data, the development of appropriate information systems that efficiently manage such large image collections is of utmost importance, with image searching being one of the most indispensable techniques. Thus there is a nearly endless potential for applications of CBIR, such as person/vehicle reidentification ,, landmark retrieval , \nremote sensing , online product searching .\nGenerally, CBIR methods can be grouped into two different tasks ,: Category level Image Retrieval (CIR) and Instance level Image Retrieval (IIR). The goal of CIR is to find an arbitrary image representative of the same category as the query (\\eg dogs, cars) ,.  By contrast, in the IIR task, a query image of a particular instance (\\eg the Eiffel Tower, my neighbor's dog) is given and the goal is to find images containing the same instance that may be captured under different conditions like different imaging distances, viewing angles, backgrounds,  illuminations, and weather conditions (reidentifying exemplars of the same instance) ,. The focus of this survey is the IIR task\\footnote{If not further specified, ``image retrieval'', ''IIR'', and ``instance retrieval'' are considered equivalent and will be used interchangeably.}.\nIn many real world applications, IIR is usually to find a desired image requiring a search among thousands, millions, or even billions of images. Hence, searching efficiently is as critical as searching accurately, to which continued efforts have been devoted  ,,. To enable accurate and efficient retrieval over a large-scale image collection, \\emph{developing compact yet discriminative feature representations} is at the core of IIR.\n\\iffalse\n\\begin {figure}[!t]\n\\centering\n\\begin{tabular}{c}\n\\includegraphics[width=0.9\\columnwidth]{./Figures/TheProblem.pdf} \\\\\n(\\textit{a}) The CBIR problem \\\\\n\\includegraphics[width=0.9\\columnwidth]{./Figures/InstancevsCategory.pdf} \\\\\n(\\textit{b}) instance-level retrieval versus category-level retrieval\n \\end{tabular}\n\\caption{ Illustration of (a) the CBIR problem, and (b)\nCBIR categorization. The images in green frame are retrieved correctly, while the ones in red frame are matched incorrectly.\n}\n\\label{category_vs_instance}\n\\vspace{-1em}\n\\end {figure}\n\\fi\n\\begin{table*}[!ht]\n\\caption{A summary and comparison of the primary surveys in the field of image retrieval.}\\label{SurveyCompare}\n\\vspace{-1em}\n\\centering\n\\setlength{\\tabcolsep}{3.0mm}\\footnotesize\n\\begin{tabular}{!{\\vrule width0.3bp}p{5cm}|p{0.5cm}|p{1.6cm}|p{9.2cm}!{\\vrule width0.3bp}}\n\\Xhline{1.0pt}\n\\multicolumn{1}{|c|}{ \\multirow{2}*{ Title} } &  \\multirow{2}*{ $\\!\\!\\!\\!$ Year}  & \\multirow{2}*{ Published in}  &  \\multicolumn{1}{c|}{ \\multirow{2}*{ Main Content } }  \\\\ \n\\hline\n\\multirowcell{2}{Content-Based Image Retrieval at the \\\\ End of the Early Years  } & \\multirow{2}*{2000}   &  \\multirowcell{2}{ TPAMI } & \\multirowcell{2}{This paper discusses the steps for image retrieval systems, including image \\\\ processing, feature extraction, user interaction, and similarity evaluation. }\\\\\n\\hline\n\\multirowcell{2}{Image Search from Thousands to Billions \\\\in 20 Years }  & \\multirow{2}*{2013}   &  \\multirowcell{2}{ TOMM}  & \\multirowcell{2}{This paper gives a good presentation of image search achievements from \\\\ 1970 to 2013, but the methods are not deep learning-based.} \\\\\n\\hline\n\\multirowcell{2}{Deep Learning for Content-Based Image \\\\ Retrieval: A Comprehensive Study }  & \\multirow{2}*{2014}   &  \\multirowcell{2}{ ACM MM}  & \\multirowcell{2}{This paper introduces supervised metric learning methods for fine-tuning \\\\ AlexNet. Details of instance-based image retrieval are limited.}\\\\\n\\hline\n\\multirowcell{2}{Semantic Content-based Image Retrieval:\\\\ A Comprehensive Study }  & \\multirow{2}*{2015}   &  \\multirowcell{2}{ JVCI }  & \\multirowcell{2}{This paper presents a comprehensive study about CBIR using traditional \\\\ methods; deep learning is introduced as a section with limited details.}\\\\\n\\hline\n\\multirowcell{3}{Socializing the Semantic Gap: A Compa-\\\\rative Survey on Image Tag Assignment, \\\\Refinement, and Retrieval } & \\multirowcell{1}{2016}  &  \\multirowcell{3}{CSUR} &  \\multirowcell{3}{A taxonomy is introduced to structure the growing literature of image \\\\ retrieval.  Deep learning methods for feature learning is introduced as \\\\future work.} \\\\\n\\hline\n\\multirowcell{2}{Recent Advance in Content based Image \\\\ Retrieval: A  Literature Survey }  & \\multirow{2}*{2017}   &  \\multirowcell{2}{ arXiv }  & \\multirowcell{2}{This survey presents image retrieval from 2003 to 2016.  Neural networks \\\\ are introduced in a section and  mainly discussed as a future direction.}\\\\\n\\hline\n\\multirowcell{3}{Information Fusion in Content-based \\\\Image  Retrieval: A Comprehensive \\\\ Overview } & \\multirowcell{1}{2017}  & \\multirowcell{3}{Information\\\\Fusion} &  \\multirowcell{3}{This paper presents information fusion strategies in CBIR. \\\\ Deep convolutional networks for feature learning are introduced \\\\ briefly but not covered thoroughly.} \\\\\n\\hline\n\\multirowcell{2} { $\\!\\!\\!$ A Survey on Learning to Hash  }   & \\multirow{2}*{2018}  & \\multirowcell{2}{ TPAMI} &  \\multirowcell{2}{This paper focuses on hash learning algorithms and introduces the \\\\ similarity-preserving methods and discusses their relationships. } \\\\\n\\hline\n\\multirowcell{2}{$\\!\\!\\!$ SIFT Meets CNN: A Decade Survey of \\\\  Instance Retrieval }  & \\multirow{2}*{2018}   &  \\multirowcell{2}{ TPAMI}  & \\multirowcell{2}{This paper presents a comprehensive review of instance retrieval based on \\\\SIFT and CNN methods.}\\\\\n\\hline\n\\multirowcell{3}{ \\bf Deep Learning for Instance \\\\ \\bf Retrieval: A Survey }  & \\multirowcell{1}{\\bf 2021}  & \\multirowcell{3}{ \\bf Ours} & \\multirowcell{3}{\\bf Our survey focuses on deep learning methods. We expand the review \\\\ \\bf  with indepth details on IIR, including methods of feature extraction, \\\\ \\bf feature embedding and aggregation, and network fine-tuning. }\n \\\\\n\\Xhline{1.0pt}\n\\end{tabular}\n\\end{table*}\nDuring the past two decades, startling progress has been witnessed in image  representation which mainly consists of two important periods, \\ie feature engineering and deep learning. In the feature engineering era, the field was dominated by various milestone handcrafted image representations like SIFT  and Bag of visual Words (BoW) . The deep learning era was reignited in 2012 when a Deep Convolutional Neural Network (DCNN) referred as ``AlexNet''  won the first place in the ImageNet classification contest  with a breakthrough reduction in classification error rate. Since then, the dominating role of SIFT like local descriptors has been replaced by data driven Deep Neural Networks (DNNs) which can learn powerful feature representations with multiple levels of abstraction directly from data. During the past decade, DNNs have set the state of the art in various classical computer vision tasks, including image classification~,, object detection~, semantic segmentation , \nand image retrieval~.\nGiven this period of rapid evolution, the goal of this paper is to provide a comprehensive survey of the recent achievements in IIR. In comparison with existing excellent surveys on traditional image retrieval ,,,, as contrasted in Table~\\ref{SurveyCompare}, our focus in this paper is reviewing deep learning based methods for IIR, particularly on issues of retrieval accuracy and efficiency.", "cites": [2093, 2092, 2091, 2094, 2090, 97, 8490, 2089, 2095, 209], "cite_extract_rate": 0.35714285714285715, "origin_cites_number": 28, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key developments in CBIR, particularly the shift from feature engineering to deep learning, and integrates insights from multiple cited surveys and methods to highlight the evolution of the field. It provides critical comparisons between traditional and deep learning approaches and identifies the limitations of prior works in covering deep learning-based IIR. The section abstracts broader trends such as the importance of feature representation, metric learning, and scalability, setting the stage for a more in-depth survey."}}
{"id": "d77f2bd6-4077-4eb5-ba4e-7c6288149714", "title": "Summary of Progress since 2012", "level": "subsection", "subsections": [], "parent_id": "87ccd777-d058-4a76-9575-0c136cb10697", "prefix_titles": [["title", "Deep Learning for Instance Retrieval: A Survey"], ["section", "Introduction"], ["subsection", "Summary of Progress since 2012"]], "content": "\\label{Summary_of_Progress}\nAfter the highly successful image classification implementation of AlexNet~, significant exploration of DCNNs for instance retrieval tasks was undertaken and representative methods are shown in Figure \\ref{fig1}. Building on these methods, more recent progress for IIR can be achieved from the perspectives of off-the-shelf models and fine-tuned models, which form the basis for this survey.\nOff-the-shelf models, based on DCNNs with fixed parameters ,,, extract features at image scales or patch scales, which correspond to single-pass and multiple-pass schemes, respectively. These methods focus on effective feature use, for which researchers have proposed embedding and aggregation methods, such as R-MAC , CroW , and SPoC  to promote the discriminativity of the extracted features. Fine-tuned models, based on DCNNs in which parameters are updated , are more popular since deep networks themselves have been investigated extensively. To learn better retrieval features, researchers have proposed to improve the network architectures and/or update the pre-stored parameters . \nThis survey will explore recent progress in detail in the context of the three following themes:\n\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=\\textwidth]{./Figures/RepresentativeMethods.pdf} \n\\vspace{-1.5em}\n\\caption{Representative methods in IIR. Off-the-shelf models have model parameters which are not further updated or tuned when extracting retrieval features. For single-pass schemes, the key step is the feature embedding and aggregation to promote the discriminativity of the extracted image-level activations ,,,, whereas for multiple-pass schemes the goal is to extract instance features at region scales and eliminate image clutter as much as possible ,,,. In contrast, for fine-tuned models, the model parameters are tuned towards the retrieval task and address the issue of domain shifts. For supervised fine-tuning, the key step lies in the design of objective functions and sample sampling strategies ,,,,, while the success of unsupervised fine-tuning is to mine the relevance among training samples ,,,,. See Sections \\ref{Retrieval_with_Off_the_Shelf_DCNN_Models} and \\ref{Retrieval_via_Learning_DCNN_Representations} for details.}\n\\label{fig1}\n\\end{figure*}\n\\medskip\n\\noindent (1) \\emph{Deep Feature Extraction} \\hfill (Section \\ref{Deep_Feature_Extraction})\n\\smallskip\n\\noindent \nOne key step in IIR is to make the descriptors as semantic-aware , as possible. For this, some recent work focus on the input data of DCNNs, thereby instance features can be extracted from the whole image, \\eg CroW , VLAD-CNN  or from image patches, \\eg MOP-CNN , FAemb . For instance, evaluated on the Holidays dataset , patch-level input scheme can improve mAP by 8.29\\% compared to the results (70.53\\%) achieved using image-level input . Others focus on exploring different feature extractors, \\eg one layer of a given DCNN, to get the output activations. Initially, fully-connected layers are usually chosen to extract features ,, and then convolutional layers are popularly used ,. Afterwards, some work leverage the complementarity of different extractors to explore layer-level fusion, such as  MoF , and model-level fusion, such as DeepIndex   to promote the retrieval performance.\n\\medskip\n\\noindent \n(2) \\emph{Feature Embedding and Aggregation} \\hfill (Section \\ref{Deep_Feature_Enhancement})\n\\smallskip\n\\noindent \nRecent work revisit the classical embedding and aggregation methods and apply on deep features. Most work have a preference towards mapping individual vector from convolutional layer ,, and then aggregating into a global feature. The mapping process can be realized by using a pre-trained codebook (\\ie built separately), such as VLAD-CNN , DeepIndex  or learned as parameters during training (built simultaneously), such as NetVLAD , GeM-DSM . Some work aggregate local features into a global one by direct pooling  or sophisticated pooling-based methods  without the aggregation operation, such as R-MAC .\n\\medskip\n\\noindent \n(3) \\emph{Network Fine-tuning for Learning Representations} \\hfill (Section \\ref{Retrieval_via_Learning_DCNN_Representations})\n\\smallskip\n\\noindent \nDCNNs pretrained on source datasets for image classification are influenced by domain shifts when performing retrieval tasks on new datasets. Thus, it is necessary to fine-tune deep networks to the specific domain , by using supervised or unsupervised fine-tuning methods. As depicted in Figure \\ref{fig1}, recent supervised fine-tuning methods focus on designing objective functions (\\eg Listwise loss ) and sample sampling strategies, such as NetVLAD , Triplet Network . Unsupervised methods focus on mining the relevance among training samples by using clustering, such as SfM-GeM  or manifold learning, such as AILIR .  Recently,  convolution-free models that only rely on transformer layers have shown competitive performance and been used as a powerful alternative to DCNNs, such as IRT , reranking Transformers .", "cites": [7543, 2101, 514, 2096, 2098, 2102, 2105, 2108, 2094, 2104, 2103, 8524, 7544, 2109, 2099, 2097, 2106, 97, 2107, 629, 2100, 7545], "cite_extract_rate": 0.5789473684210527, "origin_cites_number": 38, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers by organizing them into three themes (deep feature extraction, feature embedding and aggregation, and network fine-tuning) and connecting their contributions to the broader context of instance retrieval. It provides some abstraction by identifying trends in feature extraction strategies and model training approaches. However, the critical analysis is limited, as it mainly describes methods without evaluating their strengths, weaknesses, or limitations in a nuanced way."}}
{"id": "846c213c-7851-4f7e-b5f1-17d26ed0cdb1", "title": "Key Challenges", "level": "subsection", "subsections": [], "parent_id": "87ccd777-d058-4a76-9575-0c136cb10697", "prefix_titles": [["title", "Deep Learning for Instance Retrieval: A Survey"], ["section", "Introduction"], ["subsection", "Key Challenges"]], "content": "\\label{Keychallenges}\nThe goal of each of the preceding three themes is to address the competing objectives of \\emph{accuracy} and \\emph{efficiency}, with both objectives continuing to present challenges:\n\\textbf{A) Accuracy related challenges} depend on the input data, the feature extractor, and the way in which the extracted features are processed:\n\\begin{itemize}\n\\item Invariance challenge: The instance in an image may be rotated, translated, or scaled differently, so the final features are affected by these transformations and retrieval accuracy may be degraded . It is necessary to incorporate invariance into the IIR pipeline ,.\n\\item Distraction challenge: IIR systems may need to focus on only a certain object, or even only a small portion. DCNNs may be affected by image clutter or background, so multiple-pass schemes have been examined where region proposals are studied before feature extraction.\n\\item Discriminativity challenge: Deep features for IIR are needed to be as discriminative as possible to distinguish instances with small differences, leading to many explorations in feature processing.  These include feature embedding and aggregation methods, to promote feature discriminativity; and attention mechanisms, to highlight the most relevant regions within the extracted features or to enable deep networks to focus on regions of interest. \n\\item Fine-tune challenge: DCNNs can be fine-tuned as powerful extractors to capture fine semantic distinctions among instances. These explorations have offered improved accuracy, however the area remains a major challenge. \n\\end{itemize}\n\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=0.95\\textwidth]{./Figures/PipelineofImageRetrieval.pdf}\n \\vspace{-0.5em}\n\\caption{General framework of IIR, which includes feature extraction on image or image patches, followed by feature embedding and aggregation methods to improve feature discriminativity. Feature matching can be performed by using global features (initial filtering) or use local features to rerank the top-ranked images matched by global features. } \n\\label{PipelineofImageRetrieval}\n\\end{figure*}\n\\textbf{B) Efficiency related challenges} are important, especially for large-scale datasets . Retrieval systems should respond quickly when given a query image. Deep features are high-dimensional and contain semantic-aware information to support higher accuracy, yet is often at the expense of efficiency. \nOn the one hand, the efficiency is related to the \nformat of features, \\ie real valued or binary. Hash codes have advantages in storage and searching  ,, however for hashing methods one needs to carefully consider the loss function design ,, to obtain optimal codes for high retrieval accuracy. \nOn the other hand, efficiency is also related to the mechanism of feature matching. For example, instead of time-consuming cross-matching between local features, one can choose to use global features to perform an initial ranking and then a post-step re-ranking via the features of top-ranked images. \n\\vspace{-1em}", "cites": [2095, 2110, 2112, 2111, 2109, 2105], "cite_extract_rate": 0.75, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the challenges in instance retrieval by categorizing them into accuracy and efficiency, and links these to relevant techniques and cited works. It abstracts some broader concepts like invariance, discriminativity, and feature matching strategies. However, the critical analysis is limited to general observations and does not deeply evaluate or contrast the strengths and weaknesses of the cited papers."}}
{"id": "8a512096-a741-4540-8715-b12b364b3d9e", "title": "General Framework of IIR", "level": "section", "subsections": [], "parent_id": "515509a6-319a-4385-b1b5-4418a83d2bc2", "prefix_titles": [["title", "Deep Learning for Instance Retrieval: A Survey"], ["section", "General Framework of IIR"]], "content": "\\label{General_Framework_of_IIR}\nFigure \\ref{PipelineofImageRetrieval} offers an overview of the general framework for deep-learning-based IIR, involving three main stages. \n\\medskip\n\\noindent \n\\textbf{1) Deep feature extraction}: \\hfill (Section \\ref{Deep_Feature_Extraction})\n\\smallskip\n\\noindent\nFeature extraction is the first step of IIR and can be realized in a single-pass or multiple-pass way. Single-pass methods take as input the whole image, whereas multiple-pass methods depend on region extraction, as depicted in Figure \\ref{MultiplePass}.\nThe activations from fully-connected layers of a given DCNN can be used as retrieval features whether based on a whole image or on patches. The tensors from convolutional layers can be used when further processed by sophisticated pooling, as shown in Figure \\ref{PipelineofImageRetrieval}. Different layers of the same deep network can be combined as a more powerful extractor \n,. Furthermore, it is possible to fuse the activations from layers of different models \n,. Feature extraction is the step to produce vanilla network activations (\\ie 3D tensors or a single vector), these activations, in most cases, are needed to be further processed. \n\\medskip\n\\noindent \n\\textbf{2) Embedding and aggregation}: \\hfill (Section \\ref{Deep_Feature_Enhancement})\n\\smallskip\n\\noindent\nFeature embedding and aggregation are two essential steps to produce global or local features. Feature embedding maps individual local features into higher-dimensional space whereas feature aggregation summarizes the multiple mapped vectors or all individual features into a global vector. Global features may come from pooling convolutional feature maps directly , or using some sophisticated weighting methods , (\\ie both without feature embedding). Feature embedding method using a pre-generated codebook can be performed to encode individual convolutional vectors and then aggregated ,,. For local features, the well-embedded representations for all regions of interest are stored individually and used for cross-matching in the reranking stage without aggregation.\n\\medskip\n\\noindent \n\\textbf{3) Feature matching}:\n\\smallskip\n\\noindent\nFeature matching is a process to measure the feature similarity between images and then return a ranked list.  Global matches can be computed efficiently via such as Euclidean distance. For local features ,, the image similarity is usually evaluated by summarizing the similarities across local features, using classical RANSAC  or more recent variations ,. Storing local features separately and then estimating their similarity individually lead to additional memory and search costs ,, therefore in most cases local features are used to re-rank the initial ranking image matched by global features ,,,.\nThe three preceding stages for IIR rely on DCNNs as backbone architectures. In almost all cases, pre-stored parameters in these backbones can be fine-tuned (Section \\ref{Retrieval_via_Learning_DCNN_Representations}) to be better suited for instance retrieval and to contribute to better performance.  \nThe detailed categorization of the material of the following sections is shown in Figure~\\ref{FourAspectsofprogress}.", "cites": [2093, 2113, 2094, 2115, 2114, 2110, 7545], "cite_extract_rate": 0.3684210526315789, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple cited papers into a coherent framework for deep learning-based IIR, organizing the process into three stages with clear references. It shows a critical perspective by noting limitations, such as the memory and search costs of local features. Additionally, the section abstracts beyond individual papers to outline broader methodologies and principles like feature extraction, embedding, and matching."}}
{"id": "1b3ca748-df26-4b7f-85f3-3eb438a055f5", "title": "Retrieval with Off-the-Shelf DCNN Models", "level": "section", "subsections": ["f1ce1975-10d7-4d60-8e5d-320750dbf247", "f6eab6de-c628-4a89-bfb0-43b4f87b1151"], "parent_id": "515509a6-319a-4385-b1b5-4418a83d2bc2", "prefix_titles": [["title", "Deep Learning for Instance Retrieval: A Survey"], ["section", "Retrieval with Off-the-Shelf DCNN Models"]], "content": "\\label{Retrieval_with_Off_the_Shelf_DCNN_Models}\n\\textcolor{black}{Because of their size, DCNNs need to be trained, initially for classification tasks, on exceptionally large-scale datasets and be able to recognize images from different classes.} One possible scheme, then, is that DCNNs effectively trained for classification directly serve as off-the-shelf feature detectors for the image retrieval task, the topic of this survey.  That is, one can propose to undertake image retrieval on the basis of DCNNs, trained for classification and with their pre-trained parameters frozen. \n\\begin{figure}[!htbp]\n\\centering\n\\footnotesize\n\\begin{tikzpicture}[xscale=0.8, yscale=0.4]\n\\draw [thick, -] (0, 16.5) -- (0, -4); \\node [right] at (-0.5, 17) {\\bf \\em Deep Learning for Instance-level Image Retrieval (overall survey)};\n\\draw [thick, -] (0, 16) -- (0.5, 16);\\node [right] at (0.5, 16) {Retrieval with Off-the-Shelf DCNN Models (Section \\ref{Retrieval_with_Off_the_Shelf_DCNN_Models})};\n\\draw [thick, -] (1, 15.5) -- (1, 5);\n\\draw [thick, -] (1, 15) -- (1.5, 15);\\node [right] at (1.5, 15) {\\bf Deep Feature Extraction (Section \\ref{Deep_Feature_Extraction})};\n\\draw [thick, -] (2, 14.5) -- (2, 8); \n\\draw [thick, -] (2, 14) -- (2.5, 14);\\node [right] at (2.5, 14) {Network Feedforward Scheme (Section \\ref{Network_Feedforward_Scheme})};\n\\draw [thick, -] (3, 13.5) -- (3, 12); \n\\draw [thick, -] (3, 13) -- (3.5, 13);\\node [right] at (3.5, 13) {Single Feedforward Pass: MAC , R-MAC  };\n\\draw [thick, -] (3, 12) -- (3.5, 12);\\node [right] at (3.5, 12) {Multiple Feedforward Pass: SPM , RPNs  };\n\\draw [thick, -] (2, 11) -- (2.5, 11);\\node [right] at (2.5, 11) {Deep Feature Selection (Section \\ref{Deep_Feature_Selection}) };\n\\draw [thick, -] (3, 10.5) -- (3, 9); \n\\draw [thick, -] (3.0, 10) -- (3.5, 10);\\node [right] at (3.5, 10) { Fully-connected Layer: Neural codes  };\n\\draw [thick, -] (3.0, 9) -- (3.5, 9);\\node [right] at (3.5, 9) { Convolutional Layer: SPoC , CroW  };\n\\draw [thick, -] (2.0, 8) -- (2.5, 8);\\node [right] at (2.5, 8) {Feature Fusion Strategy (Section \\ref{Feature_Fusion_Strategy}) };\n\\draw [thick, -] (3.0, 7.5) -- (3.0, 6);\n\\draw [thick, -] (3.0, 7) -- (3.5, 7);\\node [right] at (3.5, 7) { Layer-level Fusion: MoF , MOP  };\n\\draw [thick, -] (3.0, 6) -- (3.5, 6);\\node [right] at (3.5, 6) { Model-level Fusion: ConvNet fusion  };\n\\draw [thick, -] (1, 5) -- (1.5, 5);\\node [right] at (1.5, 5) {\\bf Feature Embedding and Aggregation (Section \\ref{Deep_Feature_Enhancement}) };\n\\draw [thick, -] (2, 4.5) -- (2, -1); \n\\draw [thick, -] (2.0, 4) -- (2.5, 4);\\node [right] at (2.5, 4) { Matching with Global Features (Section \\ref{Matching_with_Global_Feature})};\n\\draw [thick, -] (2.0, 3) -- (2.5, 3);\\node [right] at (2.5, 3) {Matching with Local Features (Section \\ref{Matching_with_Local_Feature})};\n\\draw [thick, -] (2.0, 2) -- (2.5, 2);\\node [right] at (2.5, 2) { Attention Mechanism (Section \\ref{Attention_Mechanism}) };\n\\draw [thick, -] (3.0, 1.5) -- (3.0, 0);\n\\draw [thick, -] (3.0, 1) -- (3.5, 1);\\node [right] at (3.5, 1) { Non-parameteric: CroW , SPoC , SWVF  };\n\\draw [thick, -] (3.0, 0) -- (3.5, 0);\\node [right] at (3.5, 0) { Parameteric: CRN , DeepFixNet+SAM , };\n\\draw [thick, -] (2.0, -1) -- (2.5, -1);\\node [right] at (2.5, -1) {Deep Hash Embedding (Section \\ref{Deep_Hash_Embedding}) };\n\\draw [thick, -] (3.0, -1.5) -- (3.0, -3);\n\\draw [thick, -] (3.0, -2) -- (3.5, -2);\\node [right] at (3.5, -2) { Supervised Hashing: SSDH   };\n\\draw [thick, -] (3.0, -3) -- (3.5, -3);\\node [right] at (3.5, -3) { Unsupervised Hashing: DeepBit , DSTH  };\n\\draw [thick, -] (0, -4) -- (0.5, -4);\\node [right] at (0.5, -4) {\\bf Fine-Tuning for Learning DCNN Representations (Section \\ref{Retrieval_via_Learning_DCNN_Representations})};\n\\draw [thick, -] (1, -4.5) -- (1, -11);\n\\draw [thick, -] (1, -5) -- (1.5, -5);\\node [right] at (1.5, -5) {Supervised Fine-tuning (Section \\ref{Supervised_Fine-tuning})  };\n\\draw [thick, -] (2, -5.5) -- (2, -7); \n\\draw [thick, -] (2.0, -6) -- (2.5, -6);\\node [right] at (2.5, -6) {Fine-tuning via classification loss (Section \\ref{Classification-based_Fine-tuning}) }; \n\\draw [thick, -] (2.0, -7) -- (2.5, -7);\\node [right] at (2.5, -7) {Fine-tuning via pairwise ranking loss (Section \\ref{Verification_based_Learning}) };\n\\draw [thick, -] (3.0, -7.5) -- (3.0, -10);\n\\draw [thick, -] (3.0, -8) -- (3.5, -8);\\node [right] at (3.5, -8) { Transformation Matrix: Non-metric  };\n\\draw [thick, -] (3.0, -9) -- (3.5, -9);\\node [right] at (3.5, -9) { Siamese Networks };\n\\draw [thick, -] (3.0, -10) -- (3.5, -10);\\node [right] at (3.5, -10) { Triplet Networks  };\n\\draw [thick, -] (1.0, -11) -- (1.5, -11);\\node [right] at (1.5, -11) { Unsupervised Fine-tuning (Section \\ref{Unsupervised_Fine-tuning})};\n\\draw [thick, -] (2, -11.5) -- (2, -13); \n\\draw [thick, -] (2.0, -12) -- (2.5, -12);\\node [right] at (2.5, -12) { Manifold Learning Samples Mining: Diffusion Net  };\n\\draw [thick, -] (2.0, -13) -- (2.5, -13);\\node [right] at (2.5, -13) {Mining Samples by Clustering: SfM-GeM , };\n\\end{tikzpicture}\n\\vspace{-2em}\n\\caption{This survey is organized around three key themes in instance-level image retrieval, shown in bold.}\n\\label{FourAspectsofprogress}\n\\end{figure}\nThere are limitations to this approach, most fundamentally that there is a model-transfer or domain-shift challenge between tasks~,,, meaning that models trained for classification do not necessarily extract features well suited to image retrieval. In particular, a classification decision can be successful as long as features remain within classification boundaries, however features from such models may show insufficient capacity for retrieval where feature matching itself is more important than classification. This section will survey the strategies which have been developed to improve the quality of feature representations, particularly based on feature extraction / fusion (Section \\ref{Deep_Feature_Extraction}) and feature embedding / aggregation (Section \\ref{Deep_Feature_Enhancement}).", "cites": [2118, 514, 2096, 2113, 2105, 2094, 2103, 7544, 2119, 2097, 2109, 2116, 629, 7545, 2117], "cite_extract_rate": 0.5769230769230769, "origin_cites_number": 26, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes a range of approaches related to using off-the-shelf DCNN models for retrieval, organizing them into sub-themes such as feature extraction, fusion, and embedding. It provides some critical analysis by highlighting the domain-shift challenge and limitations of classification-trained models for retrieval. While it identifies broader strategies (e.g., global vs. local features), the abstraction remains moderate without reaching a highly generalized or meta-level insight."}}
{"id": "d9000dca-9ea2-4e50-b3cd-ac34b0c46c16", "title": " Network Feedforward Scheme", "level": "subsubsection", "subsections": [], "parent_id": "f1ce1975-10d7-4d60-8e5d-320750dbf247", "prefix_titles": [["title", "Deep Learning for Instance Retrieval: A Survey"], ["section", "Retrieval with Off-the-Shelf DCNN Models"], ["subsection", " Deep Feature Extraction"], ["subsubsection", " Network Feedforward Scheme"]], "content": "\\label{Network_Feedforward_Scheme}\nNetwork feedforward schemes focus on how images are fed into a DCNN, which includes single-pass and multiple-pass.\n\\begin{flushleft}\n\\emph{a. Single Feedforward Pass Methods}.   \n\\end{flushleft}\nSingle feedforward pass methods take the whole image and feed it into an off-the-shelf model to extract features. The approach is relatively efficient since the input image is fed only once. For these methods, both the fully-connected layer and last convolutional layer can be used as feature extractors .\nEarly network-based IIR work focused on leveraging DCNNs as a fixed extractor to obtain global features, especially based on the fully-connected layers ,, requiring close to zero engineering effort. However, extracting features in this way affects retrieval accuracy since the extracted features may include background information or activations for irrelevant objects.\nThe key to single-pass schemes is to embed and aggregate features to improve their discriminativity, such that features of two related images (\\ie including the same object) are more similar than these of two unrelated images . For this purpose, it is possible to first map the features $ \\boldsymbol{B} $  into a high-dimensional space and then to aggregate them into a final global feature . Another direction is to treat regions in convolutional features $ \\boldsymbol{A} $ as different sub-vectors, such that a combination of sub-vectors of all feature maps are used to represent the input image ,.\n\\begin{flushleft}\n\\emph{b. Multiple Feedforward Pass Methods}.   \n\\end{flushleft}\nCompared to single-pass schemes, multiple-pass methods are more time-consuming  because several patches are generated and then fed into the network, \n\\textcolor{black}{\nmultiple-pass schemes are more helpful for addressing the ``\\textit{invariance challenges} and ``\\textit{distraction challenges}'' in Section \\ref{Keychallenges}. Local patches at multiple scales become more robust for image translation, scaling and rotation ,. Also, these patches are helpful to filter several irrelevant background information. \n}\nThe representations are usually produced from two stages: patch detection and patch description. Multi-scale image patches are obtained using sliding windows , or spatial pyramid model (SPM) ,,, as shown in Figure \\ref{MultiplePass}. For example, Zheng \\etal  partition an image by using SPM and extract features at increasing scales, thus enabling the integration of global, regional, local contextual information.\nPatch detection methods lack retrieval efficiency since irrelevant patches are also detected . For example, Cao \\etal  propose to merge image patches into larger regions with different hyper-parameters, where the hyper-parameter selection is viewed as an optimization problem to maximize the similarity between query and candidate features.\n\\begin {figure}[!t]\n\\centering\n {    \n   \\includegraphics[width=\\columnwidth]{./Figures/MultiplePass.pdf} \n }\n \\vspace{-2em}\n\\caption{Image patch generation schemes: (a) Sliding windows ,; (b) Spatial pyramid modeling ; (c) Dense sampling ,; (d) Region proposals from region proposal networks ,.}\n\\vspace{-1em}\n\\label{MultiplePass}\n\\end {figure}\nInstead of generating multi-scale image patches randomly or densely, region proposal methods introduce a degree of purpose. Region proposals can be generated using object detectors, such as selective search , edge boxes ,, and BING . For example, Yu \\etal  propose fuzzy object matching (FOM) for instance search in which the fuzzy objects are generated from 300 object proposals and then clustered to filter out overlapping proposals. Region proposals can also be learned using such as region proposal networks (RPNs) , and convolutional kernel networks (CKNs) , and then to apply these networks into end-to-end fine-tuning for learning similarity . \\textcolor{black}{This usually requires the datasets provide well-localized bounding boxes as supervision, \\eg the datasets INSTRE , Oxford-5k , Paris-6k , GLD-v2 variant . Also, in the off-the-shelf scenarios, the way that using the bounding boxes to crop the query images and use as input the DCNNs has been shown to provide better retrieval performance since only the information relevant to the instance is extracted ,.\n}", "cites": [7543, 2105, 8525, 7546, 2094, 2103, 2109, 2120, 2121, 2114, 2107, 2112, 209, 7545], "cite_extract_rate": 0.5185185185185185, "origin_cites_number": 27, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.8}, "insight_level": "high", "analysis": "The section effectively synthesizes information from multiple papers, organizing them into single-pass and multiple-pass feedforward schemes while highlighting key design choices and trade-offs. It critically addresses limitations, such as inefficiency in patch detection and contamination from irrelevant patterns, and evaluates the strengths of alternative strategies like region proposals. The abstraction is strong in identifying general challenges (invariance, distraction) and categorizing methods based on their goals and mechanisms, though it stops short of proposing a unified theoretical framework."}}
{"id": "4be11049-ece2-473a-babd-9ac8ce82a05f", "title": "Deep Feature Selection", "level": "subsubsection", "subsections": [], "parent_id": "f1ce1975-10d7-4d60-8e5d-320750dbf247", "prefix_titles": [["title", "Deep Learning for Instance Retrieval: A Survey"], ["section", "Retrieval with Off-the-Shelf DCNN Models"], ["subsection", " Deep Feature Extraction"], ["subsubsection", "Deep Feature Selection"]], "content": "\\label{Deep_Feature_Selection}\nFeature selection decides the receptive field of the extracted features, \\ie global-level from fully-connected layers and regional-level from convolutional layers.\n\\begin{flushleft}\n\\emph{a. Extracted from Fully-connected Layers}\n\\end{flushleft}\nIt is straightforward to select a fully-connected layer as a global feature extractor ,,. With PCA dimensionality reduction and normalization~ image similarity can be measured. Extracting features $ \\boldsymbol{B} $ from fully-connected layer leads to two obvious limitations for IIR: including irrelevant information, and a lack of local geometric invariance .\nWith regards to the first limitation, image-level global descriptors may include irrelevant patterns or background clutter, especially when a target instance is only a small portion of an image. It may then be more reasonable to extract region-level features at finer scales, \\ie using multiple passes~,,.\nFor the second limitation, \nan alternative is to extract multi-scale features on a convolutional layer ,. Further, it makes the global features incompatible with techniques such as spatial verification and re-ranking. Several methods then choose to leverage intermediate convolutional layers ,,,.\n\\begin{flushleft}\n\\emph{b. Extracted from Convolutional Layers}\n\\end{flushleft}\nThe neurons in a convolutional layer are connected only to a local region of the input image, and this smaller receptive field ensures that the produced features $ \\boldsymbol{A} $, usually from the last layer, preserve more local structural information , and are more robust to image transformations  \\textcolor{black}{ thereby address the ``\\textit{invariance challenge}''. For instance, Razavian \\etal  extract multi-scale features on the last convolutional layer and Mor{\\`e}re \\etal  incorporate a series of nested pooling layers into CNN. Both of them provide higher feature invariance. Thus, many image retrieval methods use convolutional layers as feature extractors~ ,,,.}\nSum/average and max pooling are two simple aggregation methods to produce global features . For a pooled layer, the last convolutional layer usually yields superior accuracy over other shallower or later fully-connected layers . There is no other operation on the feature maps before pooling, so we illustrate these methods as ``direct pooling'' in Figure~\\ref{PipelineofImageRetrieval}.\n\\begin {figure}[!t]\n\\centering\n {  \n   \\includegraphics[width=\\columnwidth]{./Figures/SinglePass.pdf} \n }\n \\vspace{-1.5em}\n\\caption{Representative methods in single-pass methods, focusing on convolutional feature tensor $ \\boldsymbol{A} $. We denote the entry in $ \\boldsymbol{A} $ corresponding to channel $ c $, at spatial location ($i, j$) as $ A_{ijc} $:\nMAC , R-MAC , SPoC with the per-channel Gaussian weighting $\\alpha^{\\prime}_{ij} A_{ij}$ where $\\alpha^{\\prime}_{ij} = \\exp\\left\\{-{\\textstyle\\frac{\\left(i-\\frac H2\\right)^2+\\left(j-\\frac W2\\right)^2}{2\\sigma^2}}\\right\\} $ , CroW with $ \\alpha^{\\prime\\prime} $ computed by summing all $C$ feature maps at location ($i, j$)  and $ \\beta $ computed by summing the $H\\times W$-array at each feature map $c$ , GeM with channel-wise powers operation , and CAM+CroW by performing $ M_{ij}^{(l)}=\\sum_{c=1}^C\\omega_{lc}A_{ijc} $ where $\\omega_{lc}$ are weights activated by $l$-th class . \n}\n\\vspace{-1em}\n\\label{SinglePass}\n\\end {figure}\nInstead of direct pooling, many sophisticated aggregation methods have been explored, such as channel-wise or spatial-wise feature weighting on the convolutional feature maps ,,. These aggregation methods aim to highlight feature importance  or reduce the undesirable influence of bursty descriptors of some regions ,. For clarity, we illustrate the representative strategies in Figure \\ref{SinglePass}. \nNote that these feature aggregation methods are usually performed before channel-wise sum/max pooling and does not embed features into a higher dimensional space.\nOne rationale behind using convolutional features is that each such vector can act as a ``dense SIFT'' feature  since each vector corresponds to a region in the input image. Inspired by this perception, many works leverage embedding methods (\\eg BoW) used for SIFT features  on the regional feature vectors and then aggregate them (\\eg by sum pooling) into a global descriptor. \n\\textcolor{black}{Feature embedding methods address the discriminativity challenge via mapping individual features into a high-dimensional space and make them distinguishable .} Feature embedding is followed by PCA to reduce feature dimensionality and whitening to down-weight co-occurrence between features.", "cites": [7547, 7543, 2122, 2102, 2113, 2105, 2108, 2094, 2104, 2109, 2110, 2112, 7548, 7545], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 21, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers to discuss feature selection in deep instance retrieval, highlighting differences between using fully-connected and convolutional layers. It identifies limitations of global features and connects these to broader challenges like invariance and discriminativity. While it offers some analytical perspective, it is more focused on summarizing methods than providing a novel framework or deep critique."}}
{"id": "f5333f99-42b6-4668-a652-9777bc115d47", "title": "Feature Fusion Strategies", "level": "subsubsection", "subsections": [], "parent_id": "f1ce1975-10d7-4d60-8e5d-320750dbf247", "prefix_titles": [["title", "Deep Learning for Instance Retrieval: A Survey"], ["section", "Retrieval with Off-the-Shelf DCNN Models"], ["subsection", " Deep Feature Extraction"], ["subsubsection", "Feature Fusion Strategies"]], "content": "\\label{Feature_Fusion_Strategy}\nFusion studies the complementarity of different features which includes layer-level and model-level fusion explorations.\n\\begin{flushleft}\n\\emph{a. Layer-level Fusion}\n\\end{flushleft}\nWith layer-level fusion it is possible to fuse multiple fully-connected layers in a deep network ,. For instance, Liu \\etal  introduce DeepIndex to incorporate multiple global features from different fully connected layers. The activation from the first fully-connected layer is taken as column indexing, and that from the second layer serves as row indexing.  Similarly, it is also possible to fuse the activations from multiple convolutional layers. For instance, Li \\etal  apply the R-MAC encoding scheme on five convolutional layers of VGG-16 and then concatenate them into a multi-scale feature vector.\nFeatures from fully-connected layers retain global high-level semantics, whereas features from convolutional layers can present local low- and intermediate-level cues. Global and local features therefore complement each other when measuring semantic similarity and can, to some extent, guarantee retrieval performance ,. Such features can be concatenated directly ,, with  convolutional features normally filtered by sliding windows or region proposal nets. Direct concatenation can also be replaced by other advanced methods, such as orthogonal operations  or pooling-based methods, such as Multi-layer Orderless Fusion (MOF) of Li \\etal , which is inspired by Multi-layer Orderless Pooling (MOP) . However local features cannot play a decisive role in distinguishing subtle feature differences if global and local features are treated identically. Yu \\etal  use a mapping function to assert local features in refining the return ranking lists, via an exponential mapping function for tapping the complementary strengths of convolutional and fully-connected layers. Similarly, \nLiu \\etal  design two sub-networks on top of convolutional layers to obtain global and local features and then learn to fuse these features, thereby adaptively adjusting the fusion weights. Instead of directly fusing the layer activations, Zhang \\etal  fuse the index matrices which are generated based on the two feature types extracted from the same CNN, a feature fusion which has low computational complexity.\nIt is worth considering which layer combinations are better for fusion given their differences and complementarity. Yu \\etal  compare the performance of different combinations between fully-connected and convolutional layers on the Oxford 5k, Holiday, and UKBench datasets. The results show that the combinations including the first fully-connected layer always perform better. Li \\etal  demonstrate that fusing convolutional and fully-connected layers outperforms the fusion of only convolutional layers. Fusing two convolutional layers with one fully-connected layer achieves the best performance on the Holiday and UKBench datasets.\n\\begin{flushleft}\n\\emph{b. Model-level Fusion}\n\\end{flushleft}\nIt is possible to combine features from different models; such a fusion focuses more on model complementarity, with methods categorized into \\emph{intra-model} and \\emph{inter-model}.\nIntra-model fusion suggests multiple deep models having similar or highly compatible structures, while inter-model fusion involves models with differing structures. For example, \nSimonyan \\etal  introduce a ConvNet intra-model fusion strategy to improve the feature learning capacity of VGG where VGG-16 and VGG-19 are fused.  \nTo attend to different parts of an image object, Wang \\etal  realize the multi-feature fusion by selecting all convolutional layers of VGG-16 to extract image representations, which is demonstrated to be more robust than using only single-layer features.\nInter-model fusion is a way to bridge different features given the fact that different deep networks have different receptive fields ,,,. For instance, a two-stream attention network  is introduced to implement image retrieval where the main network for semantic prediction is VGG-16 while an auxiliary network is used for predicting attention maps. Similarly, considering the importance and necessity of inter-model fusion to bridge the gap between mid-level and high-level features, Liu \\etal  and Zheng \\etal  combine VGG-19 and AlexNet to learn combined features, while Ozaki \\etal  concatenate descriptors from six different models. \nInter-model and intra-model fusion are relevant to model selection. There are some strategies to determine how to combine the features from two models. It is straightforward to fuse all features from the candidate models and then learn a metric based on the concatenated features ,, which is a kind of ``\\emph{early fusion}'' strategy. Alternatively, it is also possible to learn optimal metrics separately for the features from each model, and then to combine these metrics for final retrieval ranking ,, which is a kind of ``\\emph{late fusion}'' strategy.\n\\textbf{Discussion.} Layer-level fusion and model-level fusion are conditioned on the fact that the associated layers or networks have different feature description capacities. For these fusion strategies, the key question is \\textit{what features are the best to be combined?} Some explorations have been made on the basis of off-the-shelf models, such as Xuan \\etal , who illustrates the effect of combining different numbers of features and different sizes within the ensemble. Chen \\etal  analyze the performance of embedded features from off-the-shelf image classification and object detection models with respect to image retrieval.", "cites": [2124, 2115, 2123, 7548, 2125, 514, 2105], "cite_extract_rate": 0.3888888888888889, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple cited papers to present a coherent narrative on feature fusion strategies, distinguishing between layer-level and model-level approaches and explaining their complementarity. It offers some critical evaluation by highlighting limitations such as the inability of local features to dominate if treated equally with global ones. The discussion generalizes to broader principles like the importance of feature diversity and fusion timing (early vs. late), though it does not reach the level of proposing a novel meta-framework."}}
{"id": "96cef0e9-d2bb-419e-8aa3-3757a002290b", "title": "Matching with Global Features", "level": "subsubsection", "subsections": [], "parent_id": "f6eab6de-c628-4a89-bfb0-43b4f87b1151", "prefix_titles": [["title", "Deep Learning for Instance Retrieval: A Survey"], ["section", "Retrieval with Off-the-Shelf DCNN Models"], ["subsection", "Feature Embedding and Aggregation"], ["subsubsection", "Matching with Global Features"]], "content": "\\label{Matching_with_Global_Feature}\nGlobal features can be extracted from fully-connected layers, followed by dimensionality reduction and normalization ,. They are easy to implement and there is no further aggregation process. Gong \\etal  extract fully-connected activations for local image patches at three scale levels and embed patch-level activations individually using VLAD. Thus, the final concatenated features significantly tackle the invariance challenge caused by image rotations.\nConvolutional features can also be aggregated into compact a global feature. Simple aggregation methods are sum/average or max pooling ,. \nSum/average pooling is less discriminative, because it takes into account all activated convolutional outputs, thereby weakening the effect of highly activated features . As a result, max pooling is particularly well suited for sparse features having a low probability of being active, however max pooling may be inferior to sum/average pooling when image features are whitened .\nFigure \\ref{SinglePass} illustrates sophisticated feature aggregation methods using channel-wise or spatial-wise weighting ,. For example, Babenko \\etal  propose sum-pooling convolutional features (SPoC) to obtain compact descriptors weighted by $ \\alpha^{\\prime} $ with a Gaussian center prior. Similarly, \nit is possible to treat regions in feature maps as different sub-vectors ,,, thus combinations of $R$ sub-vectors are used to represent the input image, such as R-MAC . Since convolutional features may include repetitive patterns and each vector may correspond to identical regions, the resulting descriptors may be bursty, which makes the final aggregated global feature less distinguishable. As a solution, Pang \\etal  leverage heat diffusion to weigh convolutional features at the aggregation stage, and reduce the undesirable influence of burstiness. \nConvolutional features have an interpretation as descriptors of local regions, thus many works leverage embedding methods, including BoW, VLAD, and FV, to encode regional feature vectors and then aggregate them into a global descriptor.  Note that BoW and VLAD can be extended by using other metrics, such as a Hamming distance . Here we briefly describe the principle of Euclidean embeddings.\nBoW  is a widely used feature embedding which leads to a sparse vector of occurrence. Let $ \\boldsymbol{a} = \\left\\{a_{1}, a_{2},...,a_{R} \\right\\} $ be a set of $R$ local features, each of dimensionality $d$. BoW requires a pre-defined codebook $ \\boldsymbol{c} = \\left\\{c_{1}, c_{2},...,c_{K} \\right\\} $ with $ K $ centroids, usually learned offline, to cluster these local descriptors, and maps each descriptor $a_{t}$ to the nearest centroid $c_{k}$. For each centroid, one can count and normalize the number of occurrences as\n\\begin{equation}\ng(c_{k}) = \\frac{1}{R} \\sum_{r=1}^{R}\\phi(a_{r}, c_{k})\n\\end{equation}\n\\begin{equation}\n\\phi(a_{r}, c_{k}) = \\left\\{ \\begin{array}{ll}\n1 & \\textrm{if $ c_{k} $ is the closest codeword for $a_{r}$ }\\\\\n0 & \\textrm{otherwise}\\\\\n\\end{array} \\right.\n\\label{eqn-phi}\n\\end{equation}\nThus BoW considers the number of descriptors belonging to each $c_{k}$ (\\emph{i.e.} 0-order feature statistics), so the BoW representation is the concatenation of all mapped vectors:\n\\begin{equation}\n\\label{BoW}\n G_{_{BoW}}(\\boldsymbol{a}) =\n   \\left[\n   \\begin{array}{ccc}\n   g(c_{1}), \\cdots ,g(c_{k}) ,\\cdots, g(c_{K})\n   \\end{array}\n   \\right] \\rm ^{\\top}\n\\end{equation}\nBoW is simple to implement the encoding of local descriptors, such as convolutional feature maps , or fully-connected activations ,, or to encode regional descriptors ,. Mukherjee \\etal  extract image patches based on information entropy and feed into a pre-trained VGG-16, then use BoW to embed and aggregate the patch-level descriptors from a fully-connected layer.  Embedded BoW vectors are typically high-dimensional and sparse, so not well suited to large-scale datasets in terms of the mentioned efficiency challenge.\nVLAD  stores the sum of residuals for each visual word.  Similar to BoW, it generates $K$ visual word centroids, then each feature $a_{r}$ is assigned to its nearest visual centroid $ c_{k} $:\n\\begin{equation}\ng(c_{k}) = \\frac{1}{R} \\sum_{r=1}^{R}\\phi(a_{r} , c_{k})(a_{r} - c_{k})\n\\end{equation}\nThe VLAD representation is stacked by the residuals for all centroids, with dimension ($ d \\times K $), \\ie \n\\begin{equation}\nG_{_{VLAD}}(\\boldsymbol{a}) \\! = \\! \\left[\n   \\begin{array}{ccc}\n   \\cdots, g(c_{k}) \\rm ^{\\top},\\cdots \n   \\end{array}\n   \\right] \\rm ^{\\top} .\n\\end{equation}\nVLAD captures first-order feature statistics, \\ie ($a_{r} - c_{k}$). Similar to BoW, the performance of VLAD is affected by the number of clusters: more centroids produce larger vectors that are harder to index. For instance-level image retrieval, \nGong \\etal  concatenate the activations of a fully-connected layer with VLAD applied to image-level and patch-level inputs . Ng \\etal  replace BoW~ with VLAD~, and are the first to encode local features into VLAD representations. This idea inspired another milestone work  where, for the first time, VLAD is plugged into the last convolutional layer, which allows  end-to-end training via back-propagation.\nFV  extends BoW by encoding the first and second order statistics. FV clusters the set of local descriptors by a Gaussian Mixture Model (GMM) with $K$ components to generate a dictionary $ \\boldsymbol{c} = \\left\\{ \\mu_{k}; \\Sigma_{k}; w_{k}\\right\\}_{k=1}^{K} $ made up of mean / covariance / weight triples , where the covariance may be simplified by keeping only its diagonal elements. For each local feature  $ a_{r} $, a GMM is given by \n\\begin{equation}\n\\begin{aligned}\n\\gamma_{k}(a_{r})  =  w_{k}\\times p_{k}(a_{r})/\\Big(\\sum_{k=1}^{K}w_{k}p_{k}(a_{r})\\Big) \\quad s.t. \\sum_{k=1}^K w_{k} = 1\n\\end{aligned}\n\\end{equation}\nwhere $ p_{k}(a_{r})=\\mathcal{N}(a_{r}, \\mu_{k},  \\sigma_{k}^2 )$. All local features are assigned into each component $k$ in the dictionary, which is computed as\n\\begin{equation}\n\\begin{aligned}\n &g_{w_{k}} =  \\frac{1}{R \\sqrt{w_{k}}} \\sum_{r=1}^{R} \\Big(\\gamma_{k}(a_{r}) - w_{k}\\Big) \\\\\n& g_{u_{k}}  = \\frac{\\gamma_{k}(a_{r})}{R \\sqrt{w_{k}}} \\sum_{r=1}^{R} \\left( \\frac{a_{r} - \\mu_{k}}{\\sigma_{k}} \\right),\\\\ \n& g_{\\sigma_{k}^2} = \\frac{\\gamma_{k}(a_{r})}{R \\sqrt{2w_{k}}} \\sum_{r=1}^{R}\\left[ {\\left( \\frac{a_{r} - \\mu_{k}}{\\sigma_{k}} \\right)}^{2} - 1 \\right]\n\\end{aligned}\n\\end{equation}\nThe FV representation is produced by concatenating vectors from the $K$ components: \n\\begin{equation}\n\\begin{aligned}\n\\!\\!\\! G_{_{FV}}(\\boldsymbol{a}) \\! = \\! \\left[\n   \\begin{array}{ccc}\n   g_{w_{1}}, \\cdots , g_{w_{K}}, g_{u_{1}}, \\cdots, g_{u_{K}}, g_{\\sigma_{1}^2}, \\cdots, g_{\\sigma_{K}^2} \n   \\end{array}\n   \\right] \\rm ^{\\top}\n\\end{aligned}\n\\end{equation}\nThe FV representation defines a kernel from a generative process and captures more statistics than BoW and VLAD. FV vectors do not increase the computational cost significantly but require more memory.  Applying FV without memory controls may lead to suboptimal performance . \n\\textbf{Discussion.} Traditionally, pooling-based aggregation methods (\\eg in Figure \\ref{SinglePass}) are directly plugged into deep networks and then the whole model is used  end-to-end. The three embedding methods (BoW, VLAD, FV) are initially trained with large pre-defined vocabularies ,. One needs to pay attention on their properties before choosing an embedding: BoW and VLAD are computed in the rigid Euclidean space where performance is closely related to the number of centroids, whereas FV can capture higher-order statistics and improves the effectiveness of feature embedding at the expense of a higher memory cost. Further, although vocabularies are usually built separately and pre-trained before encoding deep features, it is necessary to integrate the training of networks and the learning of vocabulary parameters into a unified framework so as to guarantee training and testing efficiency. For example, VLAD is integrated into deep networks where each spatial column feature is used to construct clusters via k-means . This idea led to NetVLAD , where deep networks are fine-tuned with the VLAD vectors. The FV method is also combined with deep networks for retrieval tasks ,.", "cites": [7543, 2122, 2102, 2113, 2105, 8525, 2108, 2109, 2097, 2123, 2126, 7548, 7545], "cite_extract_rate": 0.4482758620689655, "origin_cites_number": 29, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.8, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers to present a structured overview of global feature matching methods, including BoW, VLAD, and FV, and highlights their statistical properties and trade-offs. It critically evaluates the strengths and limitations of each method (e.g., memory vs. efficiency) and situates them within broader trends in deep feature aggregation. The discussion abstracts key principles such as the importance of capturing higher-order statistics and the need for end-to-end training."}}
{"id": "22557b43-bfa7-48d0-9dad-fad3f1f25551", "title": "Matching with Local Features", "level": "subsubsection", "subsections": [], "parent_id": "f6eab6de-c628-4a89-bfb0-43b4f87b1151", "prefix_titles": [["title", "Deep Learning for Instance Retrieval: A Survey"], ["section", "Retrieval with Off-the-Shelf DCNN Models"], ["subsection", "Feature Embedding and Aggregation"], ["subsubsection", "Matching with Local Features"]], "content": "\\label{Matching_with_Local_Feature}\nAlthough matching with global features has high efficiency for both feature extraction and similarity computation, global features are not compatible with spatial verification and correspondence estimation, which are important procedures for instance-level retrieval tasks, motivating work on matching with local features. In terms of the matching process, global features are matched only once while local feature matching is evaluated by summarizing the similarity across all individual local features (\\ie many-to-many matching).\nOne important aspect of local features is to detect the keypoints for an instance within an image, and then to describe the detected keypoints as a set of local descriptors. Inspired by , the common strategies of this whole procedure for IIR can be categorized as \\textit{detect-then-describe} and \\textit{describe-then-detect}.\nIn terms of \\textit{detect-then-describe}, we regard the descriptors around keypoints as local features, similar to ,. Coarse regions can be detected, for example, by using the methods depicted in Figure~\\ref{MultiplePass}, and regions of interest in an image can be detected by using region proposal networks (RPNs) ,. The extracted coarse regions around the keypoints are fed into a DCNN, followed by feature description.  Traditional detectors can also be used to detect fine regions around a keypoint. For instance, Zheng \\etal  employ the popular Hessian-Affine detector  to get an affine-invariant local region. Paulin \\etal  and Mishchuk \\etal  detect regions using the Hessian-Affine detector and feed into patch-convolutional kernel networks (Patch-CKNs) . \\textcolor{black}{Note that it becomes more convenient for the case where bounding boxes annotations have been provided by datasets (see Section \\ref{Datasets_and_Evaluation_Criteria}), and then the image regions can be cropped directly for further reranking . }\nRather than performing keypoint detection early on, it is possible to postpone the detection stage on the convolutional feature maps, \\ie \\textit{describe-then-detect}. One can select regions on the convolutional feature maps to obtain a set of local features ,,; the local maxima of the feature maps are then detected as keypoints . A similar strategy is also used in network fine-tuning ,,,,, where the keypoints on the convolutional feature maps can be selected based on attention scores predicted by an attention network ,, or based on single-head and multi-head attention modules in transformers ,. This approach to keypoint selection is better for achieving computational efficiency. \nAfter keypoint detection and description, a large number of local features are used in the matching stage to perform instance-level retrieval, and the image similarity is evaluated by matching across all local features. Local matching techniques include spatial verification and selective match kernels (SMK) . Spatial verification assumes object instances are rigid so that local matches between images can be estimated as an affine transformation using RANdom SAmple Consensus (RANSAC) . One limitation of RANSAC is its high computational complexity of estimating the transformation model when all local descriptors are considered; instead, it is possible to apply RANSAC to a small number of top-ranked local descriptors, such as those selected by approximate nearest neighbor . \nSMK weighs the contributions of individual matches with a non-linear selective function, but is still memory intensive. Its extension, the Aggregated Selective Match Kernel (ASMK), focuses more on aggregating similarities between local features without explicitly modeling the geometric alignment, which can produce a more compact representation ,. Recently, Teichmann \\etal  introduced Regional Aggregated Selective Match Kernel (R-ASMK) to combine information from detected regions, boosting image retrieval accuracy compared to the ASMK.\n\\textbf{Discussion.} Using local descriptors to perform instance retrieval tasks has two limitations. First, the local descriptors for an image are stored individually and independently, which is memory-intensive, and not well-suited for large-scale scenarios.\nSecond, estimating the similarity between the query and database images depends on cross-matching all local descriptor pairs, which incurs additional searching cost and then a low retrieval efficiency. Therefore, most instance retrieval systems using local features follow a two-stage paradigm: initial filtering and re-ranking ,,,,, as in Figure \\ref{PipelineofImageRetrieval}. The initial filtering stage is to employ a global descriptor to select a set of candidate matching images, thereby reducing the solution space; the re-ranking stage is to use local descriptors to re-rank the top-ranked images from the global descriptor.", "cites": [7543, 2093, 2101, 7546, 8525, 2127, 2103, 8524, 2121, 2106, 2114, 2126, 2110, 209, 7545], "cite_extract_rate": 0.6, "origin_cites_number": 25, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes a wide range of methods for local feature-based instance retrieval, connecting different strategies (e.g., detect-then-describe vs. describe-then-detect) and their implementations. It also includes critical evaluation of the limitations of local descriptors and discusses the two-stage retrieval paradigm as a solution. The content abstracts key concepts and trends (like attention mechanisms and computational efficiency), though some deeper comparative analysis is missing."}}
{"id": "3362dbb1-5a27-4b56-85a4-413bc5c384f6", "title": "Attention Mechanism", "level": "subsubsection", "subsections": [], "parent_id": "f6eab6de-c628-4a89-bfb0-43b4f87b1151", "prefix_titles": [["title", "Deep Learning for Instance Retrieval: A Survey"], ["section", "Retrieval with Off-the-Shelf DCNN Models"], ["subsection", "Feature Embedding and Aggregation"], ["subsubsection", "Attention Mechanism"]], "content": "\\label{Attention_Mechanism}\nAttention mechanism can be regarded as a kind of feature aggregation, whose aim is to \\textcolor{black}{highlight the most relevant feature parts. It can effectively address the ``\\textit{distraction challenge}'' and also promote feature discriminativity , realized by computing an attention map.} Approaches to obtaining attention maps can be categorized into non-parametric and parametric groups, as shown in Figure~\\ref{fig3}, where the main difference is whether the importance weights in the attention map are learnable.\nNon-parametric weighting is a straightforward method to highlight feature importance, and the corresponding attention maps can be obtained by channel-wise or spatial-wise pooling, as in Figure \\ref{fig3} (a,b). For spatial-wise pooling, \nKalantidis \\etal  propose an effective CroW method to weight and pool feature maps, which concentrate on weighting activations at different spatial locations, without considering the relations between these activations. In contrast, Ng \\etal  explore the correlations among activations at different spatial locations on the convolutional feature maps. \nChannel-wise weighting methods are also popular non-parametric attention mechanisms ,. Xu \\etal  rank the weighted feature maps to build ``probabilistic proposals'' to select regional features. Jimenez \\etal  combine CroW and R-MAC to propose Classes Activation Maps (CAM) to weigh the feature map per class. Xiang \\etal  employ a Gram matrix to analyze the correlations between different channels and then obtain channel sensitivity information to tune the importance of each feature map. Channel-wise and spatial-wise weighting methods are usually integrated into a deep model to highlight feature importance ,.\nParametric attention maps, shown in Figure \\ref{fig3} (c,d), can be learned via deep networks, where the input can be either image patches or feature maps ,,, approaches which are commonly used in supervised metric learning .  Kim \\etal  make the first attempt to propose a shallow network (CRN) to take as input the feature maps of convolutional layers and outputs a weighted mask indicating the importance of spatial regions in the feature maps. The resulting mask modulates feature aggregation to create a global representation of the input image. \nNoh \\etal  design a 2-layer CNN with a softplus output layer to compute scores which indicate the importance of different image regions. Inspired by R-MAC, Kim \\etal  employ a pre-trained ResNet101 to train a context-aware attention network using multi-scale feature maps.\nApart from using feature maps as inputs, a whole image can be used to learn feature importance, for which specific networks are needed ,,. Mohedano  explores different saliency models, including DeepFixNet and Saliency Attentive Model. Yang \\etal  and Wei \\etal  introduce a two-stream network for image retrieval in which the auxiliary stream, DeepFixNet, is used specifically for predicting attention maps, which are then fused with the feature maps produced by the main network.\nFor image retrieval, attention mechanisms can be combined with supervised metric learning .", "cites": [2124, 7547, 2094, 2104, 2093, 2118, 2099], "cite_extract_rate": 0.5, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers on attention mechanisms for instance retrieval, organizing them into non-parametric and parametric categories and highlighting their goals and methods. It connects different approaches (e.g., spatial vs. channel-wise weighting) and discusses how attention improves feature discriminativity and addresses the distraction challenge. While it provides some comparative structure (e.g., contrasting CroW and Nohs methods), it lacks deeper critical evaluation of limitations or trade-offs. The abstraction is moderate, as it identifies general categories of attention mechanisms but does not rise to a meta-level theoretical or conceptual synthesis."}}
{"id": "b5eed66c-a402-4570-be6b-8b9987293fc6", "title": " Hashing Embedding ", "level": "subsubsection", "subsections": [], "parent_id": "f6eab6de-c628-4a89-bfb0-43b4f87b1151", "prefix_titles": [["title", "Deep Learning for Instance Retrieval: A Survey"], ["section", "Retrieval with Off-the-Shelf DCNN Models"], ["subsection", "Feature Embedding and Aggregation"], ["subsubsection", " Hashing Embedding "]], "content": "\\label{Deep_Hash_Embedding}\nReal-valued features extracted by deep networks are typically high-dimensional, and therefore are not well-suited to retrieval efficiency. As a result, there is significant motivation to transform deep features into more compact codes. \\textcolor{black}{Since their computational and storage efficiency are beneficial for the ``\\textit{efficiency challenge}'', hashing algorithms have been widely used for global , and local descriptors ,,.}\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=0.5\\textwidth]{./Figures/AttentionMachanism.pdf}\n\\vspace{-1.5em}\n\\caption{Illustration of attention mechanisms. (a)-(b) Non-parametric schemes:  The attention is based on convolutional feature $ \\boldsymbol{A} $. Channel-wise attention in (a) produces a $C$-dimensional importance vector \\textbf{$\\beta_{1}$} ,; Spatial-wise attention in (b) computes a 2-dimensional attention map \\textbf{$\\alpha$} ,,. (c)-(d) Parametric schemes: The attention weights are learned by a trainable network. In (c), \\textbf{$\\beta_2$} are provided by a sub-network with parameters $\\theta_{\\gamma}$ ,,,. In (d), the attention maps, as a tensor, are predicted by some auxiliary saliency extraction models from the input image directly ,,.} \\label{fig3}\n\\end{figure}\nHash functions can be plugged as a layer into deep networks, so that hash codes and deep networks can be  simultaneously trained and optimized, either supervised  or unsupervised . During hash function training, the hash codes of originally similar images are embedded as closely as possible, and the hash codes of dissimilar images are as separated as possible. $d$-dim hash codes from a hash function $h(\\cdot)$ for an image $x$ can be formulated as $ b_{x} = h(x) = h\\big(f(x;\\bm{\\theta})\\big) \\in  \\{+1, -1\\}^d $. Because hash codes are non-differentiable their optimization is difficult, so $h(\\cdot)$ can be relaxed to be differentiable by using \\textit{tanh} or \\textit{sigmoid} functions .\nWhen binarizing real-valued features, it is crucial to preserve image similarity and to improve hash code quality . These two aspects are at the heart of hashing algorithms to maximize retrieval accuracy. \n\\begin{flushleft}\n\\emph{a. Hash Functions to Preserve Image Similarity}\n\\end{flushleft}\nPreserving similarity seeks to minimize the inconsistencies between real-valued features and corresponding hash codes, for which a variety of strategies have been adopted. \nLoss functions can significantly influence similarity preservation, which includes both supervised and unsupervised methods. With class labels available, many loss functions are designed to learn hash codes in a Hamming space. As a straightforward method, one can optimize either the difference between matrices computed from the binary codes and their supervision labels , or the difference between the hash codes and real-valued deep features ,.  Song \\etal  propose to learn hash codes for regional features in which each local feature is converted to a set of binary codes by multiplying a hash function and the raw RoI features, then the differences between RoI features and hash codes are characterized by an L$_2$ loss. Do \\etal  regularize hash codes with a reconstruction loss, which ensure that codes can be reconstructed to their inputs so that similar/dissimilar inputs are mapped to similar/dissimilar hash codes. Lin \\etal  learn hash codes and address the ``\\textit{invariance challenge}'' by introducing an objective function which characterize the difference between the binary codes which are computed from the original image and the geometric transformed one.\n\\begin{flushleft}\n\\emph{b. Improving Hash Function Quality}\n\\end{flushleft}\nA good hash function seeks to have binary codes uniformly distributed; that is, maximally filling and using the hash code space, normally on the basis of bit uncorrelation and bit balance ,. Bit uncorrelation implies that different bits are as independent as possible, so that a given set of bits can aggregate more information within a given code length .  Bit balance means that each bit should have a 50\\% chance of being +1 or -1, thereby maximizing code variance and information .  Mor{\\`e}re \\etal  use the uniform distribution $U$(0,1) to build a regularization term to make hash codes distribute evenly where the codes are learned by a Restricted Boltzmann Machine layer. Likewise, Lin \\etal  optimize the mean of learned hash codes to be close to 0.5 to prevent any bit bias towards zero or one.", "cites": [7547, 2118, 2094, 2104, 2099, 2116, 2128, 2095, 2110], "cite_extract_rate": 0.45, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of hashing embedding in the context of deep learning for instance retrieval, integrating key concepts from multiple cited works. It synthesizes ideas about hash function design and training, particularly around similarity preservation and hash code quality. However, the critical evaluation is limited, with few explicit comparisons or limitations discussed among the methods. The section abstracts general principles of hashing, such as bit balance and uncorrelation, but stops short of offering a novel or meta-level framework."}}
{"id": "acffc1fe-bf03-450b-857b-db143905236e", "title": "$\\!\\!\\!\\!\\!$Retrieval via Learning DCNN Representations ", "level": "section", "subsections": ["45dba903-566e-4f9f-a48c-8f8adf0593ca", "871a2087-f4a4-4968-91d1-0752906a7612"], "parent_id": "515509a6-319a-4385-b1b5-4418a83d2bc2", "prefix_titles": [["title", "Deep Learning for Instance Retrieval: A Survey"], ["section", "$\\!\\!\\!\\!\\!$Retrieval via Learning DCNN Representations "]], "content": "\\label{Retrieval_via_Learning_DCNN_Representations}\nThe off-the-shelf DCNNs pre-trained on source datasets for classification are quite robust to inter-class variability. However, in most cases, deep features extracted based on off-the-shelf models may not be sufficient for accurate retrieval, even with the strategies discussed in Section \\ref{Retrieval_with_Off_the_Shelf_DCNN_Models}. In order for models  to be more effective for retrieval, a common practice is network fine-tuning, \\ie updating the pre-stored parameters . Fine-tuning methods have been studied extensively to learn better features, \\textcolor{black}{whose primary aim is to address the ``\\textit{fine-tune challenge}''.} A standard dataset with clear and well-defined ground-truth labels is indispensable for the supervised fine-tuning and subsequently pair-wise supervisory information is incorporated into ranking loss to update networks by regularizing on retrieval representations, otherwise it is necessary to develop unsupervised fine-tuned methods. After network fine-tuning, features can be organized as global or local to perform retrieval. \n\\textcolor{black}{For the most feature strategies we presented in Section \\ref{Retrieval_with_Off_the_Shelf_DCNN_Models}, including feature extraction, feature embedding and feature aggregation. Note that fine-tuning does not contradict or render irrelevant these feature processing methods; indeed, these strategies are complementary and can be equivalently incorporated as part of network fine-tuning. To this end, this section will survey the strategies which have been developed, based on the patch-level, image-level, or class-level supervision, to fine-tune deep networks for better instance retrieval.}\n\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=0.9\\textwidth]{./Figures/All_Metric_Learning.pdf}\n\\vspace{-0.5em}\n\\caption{Schemes of supervised fine-tuning. Anchor, positive, and negative images are indicated by $x_{a}$, $x_{p}$, $x_{n}$, respectively. (a) classification loss ;  (b) similarity learning by using a transformation matrix ; (c) Siamese loss ,,,; (d) triplet loss ; (e) an attention block into DCNNs to highlight regions ; (f) combining classification loss and pairwise ranking loss ,; (g) region proposal networks (RPNs) to locate the RoI and highlight specific regions or instances ; (h) inserting the RPNs of (g) into DCNNs, such that the RPNs extract regions or instances at the convolutional layer ,.  }\n\\vspace{-1em}\n\\label{All_Metric_Learning}\n\\end{figure*}", "cites": [2117, 7546, 2110, 2103, 2129, 2096, 629, 2109, 2097], "cite_extract_rate": 0.6923076923076923, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a coherent synthesis of cited works by integrating the idea of fine-tuning deep CNNs with different levels of supervision for instance retrieval. It connects feature extraction, embedding, and aggregation strategies to fine-tuning, offering some abstraction by positioning them within a broader framework. However, the critical analysis is limited to a general mention of challenges (e.g., noisy data) and lacks in-depth evaluation or nuanced critique of the specific methods cited."}}
{"id": "c05b41ca-005d-493d-96fa-98315460aeed", "title": "Fine-tuning via Classification Loss", "level": "subsubsection", "subsections": [], "parent_id": "45dba903-566e-4f9f-a48c-8f8adf0593ca", "prefix_titles": [["title", "Deep Learning for Instance Retrieval: A Survey"], ["section", "$\\!\\!\\!\\!\\!$Retrieval via Learning DCNN Representations "], ["subsection", "Supervised Fine-tuning "], ["subsubsection", "Fine-tuning via Classification Loss"]], "content": "\\label{Classification-based_Fine-tuning}\nWhen class labels of a new dataset are available (\\eg INSTRE , GLDv2 ,), it is preferable to begin with a previously-trained DCNN, trained on a separate dataset, with the backbone DCNN typically chosen from one of AlexNet, VGG, GoogLeNet, or ResNet.\nThe DCNN can then be fine-tuned, as shown in Figure \\ref{All_Metric_Learning} (a), by optimizing its parameters on the basis of a cross entropy loss \n\\begin{equation}\n\\mathcal{L}_{CE}(\\hat{p_{i}},y_{i})= - \\! \\sum^{c}_{i} \\! \\big(y_{i} \\! \\times \\!log(\\hat{p}_{i}) \\big)\n\\label{cross_entropy_loss}\n\\end{equation}\nHere $y_{i}$ and $\\hat{p}_{i}$ are the ground-truth labels and the predicted logits, respectively, and $c$ is the total number of categories. The milestone work in such fine-tuning is , in which AlexNet is re-trained on the Landmarks dataset. \\textcolor{black}{\nAccording to the class labels, the image-level features are required to compute the logits. Thus, the descriptors extracted from local regions on convolutional feature maps , or image patch inputs  are further needed to be aggregated. }\n\\textcolor{black}{A classification-based fine-tuning method enables to enforce higher similarity for intra-class samples and diversity for inter-class samples. Cao \\etal  employ the ArcFace loss , which uses the margin-adjusted cosine similarity in the form of softmax loss, to induce smaller intra-class variance and show excellent results for instance retrieval.} Recently, Boudiaf \\etal  claim that cross entropy loss can minimize intra-class distances while maximizing inter-class distances. Cross entropy loss is, in essence, maximizing a common mutual information between the retrieval features and the ground-truth labels. Therefore, it can be regarded as an upper bound on a new pairwise loss, which has a structure similar to various pairwise ranking losses, of which representatives are introduced below.", "cites": [2114, 309, 2130, 2093, 2111, 2109], "cite_extract_rate": 0.75, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of classification-based fine-tuning in instance retrieval, connecting several papers to explain how cross-entropy loss can be used to implicitly perform metric learning. It offers some abstraction by framing cross-entropy as a mutual information maximization problem and notes its similarity to pairwise ranking losses. However, the critical analysis is limitedwhile it mentions the effectiveness of methods like ArcFace, it does not deeply evaluate their limitations or compare them to alternatives in a systematic way."}}
{"id": "db1c70e4-39b7-4baf-8fa7-4ccb702563d0", "title": "Fine-tuning via Pairwise Ranking Loss", "level": "subsubsection", "subsections": [], "parent_id": "45dba903-566e-4f9f-a48c-8f8adf0593ca", "prefix_titles": [["title", "Deep Learning for Instance Retrieval: A Survey"], ["section", "$\\!\\!\\!\\!\\!$Retrieval via Learning DCNN Representations "], ["subsection", "Supervised Fine-tuning "], ["subsubsection", "Fine-tuning via Pairwise Ranking Loss"]], "content": "\\label{Verification_based_Learning}\nWith affinity information (\\eg samples from the same group) indicating similar and dissimilar pairs, fine-tuning methods based on pairwise ranking loss learn an optimal metric which minimizes or maximizes the distance of pairs to maintain their similarity. Network fine-tuning via ranking loss involves two types of information :\n\\begin{enumerate}\n\\item A pair-wise constraint, corresponding to a Siamese network as in Figure \\ref{All_Metric_Learning} (c), in which input images are paired with either a positive or negative sample;\n\\item A triplet constraint, associated with triplet networks as in Figure \\ref{All_Metric_Learning} (e), in which anchor images are paired with both similar and dissimilar samples .\n\\end{enumerate}\nThese pairwise ranking loss based methods are categorized into globally supervised approaches (Figure \\ref{All_Metric_Learning} (c,d)) and locally supervised approaches (Figure \\ref{All_Metric_Learning} (g,h)), where the former ones learn a metric on global features by satisfying all constraints, whereas the latter ones focus on local areas by only satisfying the given local constraints (\\emph{e.g.} region proposals).\nTo be specific, consider a triplet set $ X \\!\\! = \\!\\! \\lbrace(x_{a}, x_{p}, x_{n})\\} $ in a mini-batch, where $ (x_{a}, x_{p} )$ indicates a similar pair and $ (x_{a}, x_{n} )$ a dissimilar pair. Features $ f(x;\\bm{\\theta}) $ of one image are extracted by a network $f(\\cdot)$ with parameters $\\bm{\\theta}$, for which we can represent the affinity information for each similar or dissimilar pair as\n\\begin{equation}\n\\begin{aligned}\n\\mathcal{D}_{ij} = \\mathcal{D}(x_{i}, x_{j}) = || f(x_{i}; \\bm{\\theta}) - f(x_{j}; \\bm{\\theta})||_{2}^{2}\n\\end{aligned}\\label{Distance_pre_defined}\n\\end{equation}\n\\begin{flushleft}\n\\emph{a. Refining with Transformation Matrix}.   \n\\end{flushleft}\nLearning the similarity among input samples can be implemented by optimizing the weights of a linear transformation matrix . It  transforms the concatenated feature pairs into a common latent space using a transformation matrix $ \\bm{W} \\!\\! \\in \\!\\! \\mathbb{R}^{2d \\times 1} $, where $d$ is the final feature dimension. The similarity score of these pairs are predicted via a sub-network $ \\mathcal{S}_{W}(x_{i}, x_{j}) =  f_{W}(f(x_{i}; \\bm{\\theta})\\cup f(x_{j};\\bm{\\theta}); \\bm{W}) $ . In other words, the sub-network $f_{W}$ predicts how similar the feature pairs are. Given the affinity information of feature pairs $ \\mathcal{S}_{ij}=\\mathcal{S}(x_{i}, x_{j}) \\! \\in \\! \\{0,1\\} $,  the binary labels 0 and 1 indicate the similar (positive) or dissimilar (negative) pairs, respectively. The training of function $f_{W}$ can be achieved by using a regression loss: \n\\begin{equation}\n\\begin{aligned}\n\\!\\!\\!\\mathcal{L}_{W}(x_{i}, x_{j}) = & | \\mathcal{S}_{W}(x_{i}, x_{j}) - \\mathcal{S}_{ij}\\big(\\text{sim}(x_{i}, x_{j}) + m\\big) - \\\\\n&(1-\\mathcal{S}_{ij})\\big(\\text{sim}(x_{i}, x_{j})-m\\big) | \\label{regressionLoss} \n\\end{aligned}\n\\end{equation}\nwhere $\\text{sim}(x_{i}, x_{j}) $ can be the cosine function for guiding the training of $\\bm{W}$ and $m$ is a margin. By optimizing the regression loss and updating $ \\bm{W} $, deep networks maximize the similarity of similar pairs and minimize that of dissimilar pairs. It is worth noting that the pre-stored parameters in the deep models are frozen when optimizing $ \\bm{W} $. The pipeline of this approach is depicted in Figure \\ref{All_Metric_Learning} (b). \n\\begin{flushleft}\n\\emph{b. Fine-tuning with Siamese Networks}.   \n\\end{flushleft}\nSiamese networks represent important options in implementing metric learning for fine-tuning, as in Figure \\ref{All_Metric_Learning} (c) and Figure \\ref{mAP_loss_illustration} (a). It is a structure composed of two branches that share the same weights across layers. Siamese networks are trained on paired data, consisting of an image pair $ (x_{i}, x_{j}) $ such that $ \\mathcal{S}(x_{i}, x_{j}) \\! \\in \\! \\{0,1\\} $. A Siamese loss \nis formulated as\n\\begin{equation}\n\\begin{aligned}\n\\mathcal{L}_{Siam}(x_{i}, x_{j})& =  \\frac{1}{2}\\mathcal{S}(x_{i}, x_{j})\\mathcal{D}(x_{i}, x_{j}) \\; + \\\\\n&\\frac{1}{2}\\big(1 - \\mathcal{S}(x_{i}, x_{j})\\big)\\max\\big(0,\\; m - \\mathcal{D}(x_{i}, x_{j})\\big)\n\\end{aligned}\\label{contrastive}\n\\end{equation}\nSiamese loss has recently been reaffirmed as a very effective metric in category-level image retrieval, outperforming many more sophisticated losses if implemented carefully . Enabled by the standard Siamese network, this objective function is used to learn the similarity between semantically relevant samples under different scenarios ,. For example, Radenovi{\\'c} \\etal  employ a Siamese network on matching and non-matching global feature pairs which are aggregated by GeM-based pooling. The deep network fine-tuned by the Siamese loss generalizes better and converges at higher retrieval performance. Ong \\etal  leverage the Siamese network to learn image features which are then fed into the Fisher Vector model for further encoding.  Siamese networks can also be applied to hashing learning in which the Euclidean distance $\\mathcal{D}(\\cdot)$ in Eq. \\ref{contrastive} is computed for binary codes .\nAn implicit drawback of the Siamese loss is that it may penalize similar image pairs even if the margin between these pairs is small or zero , if the constraint is too strong and unbalanced.  At the same time, it is hard to map the features of similar pairs to the same point when images contain complex contents or scenes. To tackle this limitation, Cao \\etal  adopt a double-margin Siamese loss  to relax the penalty for similar pairs by setting a margin $ m_1 $ instead of zero, in which case the original single-margin Siamese loss is re-formulated as \n\\begin{equation}\n\\begin{aligned}\n\\!\\!\\!\\! \\mathcal{L}_{\\mathcal{D}\\_Siam}(x_{i}, x_{j}) & =  \\frac{1}{2}\\mathcal{S}(x_{i}, x_{j})\\max \\big(0, \\mathcal{D}(x_{i}, x_{j}) - m_{1} \\big) + \\\\\n& \\!\\! \\frac{1}{2}\\big(1 - \\mathcal{S}(x_{i}, x_{j})\\big)\\max\\big(0, m_{2} - \\mathcal{D}(x_{i}, x_{j})\\big)\n\\end{aligned}\\label{doublemargincontrastive}\n\\end{equation}\nwhere $ m_{1} \\!\\! > \\!\\! 0 $ and $ m_{2} \\!\\! > \\!\\! 0  $ are the margins affecting the similar and dissimilar pairs, respectively, as in Figure \\ref{mAP_loss_illustration} (b), meaning that the double margin Siamese loss only applies a contrastive force when the distance of a similar pair is larger than $ m_{1} $. The mAP metric of retrieval is improved when using the double margin Siamese loss . \nMore recently, transformers have been trained under the regularization of cross entropy  and Siamese loss  for instance-level retrieval and achieved competitive performance, positioning it as an alternative to convolutional architectures. As observed by , the transformer-based architecture is less impacted than convolutional networks by feature collapse since each input feature is projected to different sub-spaces before the multi-headed attention. Moreover, the transformer backbone operates as a learned aggregation operator, thereby avoiding the design of sophisticated feature aggregation methods.\n\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=\\linewidth]{./Figures/mAP_loss_illustration.pdf}\n\\vspace{-2em}\n\\caption{Illustrations of different losses for network fine-tuning. The same shape with different colors denotes images that include the same instance. (a)-(c) have been introduced in the text ,,. (d) Listwise AP loss considers a mini-batch of $N$ features simultaneously and directly optimizes the Average-Precision computed from these features ,.\n}  \\label{mAP_loss_illustration}\n\\end{figure*}\n\\begin{flushleft}\n\\emph{c. Fine-tuning with Triplet Networks}.   \n\\end{flushleft}\nTriplet networks optimize similar and dissimilar pairs simultaneously. As shown in Figure \\ref{All_Metric_Learning} (d) and Figure \\ref{mAP_loss_illustration} (c), the plain triplet networks adopt a ranking loss for training:\n\\begin{equation}\n\\begin{aligned}\n\\mathcal{L}_{Triplet}(x_{a}, x_{p}, x_{n}) =  \\max \\big( 0, m + \\mathcal{D}(x_{a}, x_{p}) - \n \\mathcal{D}(x_{a}, x_{n})\\big)\\label{triplet}\n\\end{aligned}\n\\end{equation}\nwhich indicates that the distance of an anchor-negative pair  $ \\mathcal{D}(x_{a}, x_{n}) $ should be larger than that of an anchor-positive pair $ \\mathcal{D}(x_{a}, x_{p}) $ by a certain margin $ m $. \n\\textcolor{black}{Given the datasets that provide bounding box annotations, such as INSTRE, Oxford-5k, Paris-6k, and their variants, the bounding box annotations are used as patch-level supervision to train a region detector which enables the final DCNNs to locate specific regions or objects. As an example, region proposal networks (RPNs)  is fine-tuned and subsequently plugged into DCNNs and trained end-to-end , as shown in Figure \\ref{All_Metric_Learning} (g). RPNs yield the regressed bounding box coordinates of objects and are trained by the multi-class classification loss. Once fine-tuned, RPNs can produce regional features for each detected region by RoI pooling and perform better instance search. }\n\\textcolor{black}{Further, local supervised metric learning has been explored based on the fact that RPNs  enable deep models to learn regional features for particular instance objects ,,,. RPNs used in the triplet formulation are shown in Figure \\ref{All_Metric_Learning} (h). Firstly, regression loss (RPNs loss) is used to minimize the regressed bounding box relative to ground-truth. Then, the regional features for all detected RoIs are aggregated into a global one and L2-normalized for the triplet loss.} Note that, in some cases, jointly training an RPN loss and triplet loss leads to unstable results, a problem addressed in  by first training a CNN to produce R-MAC using a rigid grid, after which the parameters in convolutional layers are fixed and RPNs are trained to replace the rigid grid.\nAttention mechanisms can also be combined with metric learning for fine-tuning , as in Figure \\ref{All_Metric_Learning} (e), where the attention module is typically end-to-end trainable and takes as input the convolutional feature maps. Song \\etal  introduce a convolutional attention layer to explore spatial-semantic information, highlighting regions in images to significantly improve the discrimination for image retrieval.\nRecent studies , have jointly optimized the triplet loss and classification loss to further improve network capacity, as shown in Figure \\ref{All_Metric_Learning} (f). The overall joint function is \n\\begin{equation}\n\\begin{aligned}\n\\!\\!\\!\\mathcal{L}_{Joint} = &\\lambda_{1} \\! \\cdot \\! \\mathcal{L}_{Triplet}(x_{i,a}, x_{i,p}, x_{i,n}) \\! + \\! \\lambda_{2} \\!\\cdot \\! \\mathcal{L}_{CE}(\\hat{p_{i}},y_{i})\n\\end{aligned}\n\\end{equation}\nwhere the cross entropy loss (CE loss) $ \\mathcal{L}_{CE} $ is defined in Eq. (\\ref{cross_entropy_loss}) and the triplet loss $ \\mathcal{L}_{Triplet} $ in Eq. (\\ref{triplet}). $ \\lambda_{1} $ and $ \\lambda_{2} $ are hyper-parameters tuning the tradeoff between the two loss functions.", "cites": [2101, 2096, 2098, 7546, 7549, 2103, 2097, 2106, 2129, 2110, 209, 2117], "cite_extract_rate": 0.631578947368421, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple works on pairwise ranking loss-based fine-tuning methods, including Siamese and triplet networks, while providing a structured explanation of their principles and applications. It includes critical points such as the limitations of Siamese loss and how modifications like double-margin loss address them. The section abstracts over specific methods to identify broader patterns in metric learning for instance retrieval."}}
{"id": "14afec8b-a454-4ba0-a24a-2c2e37482b34", "title": "Discussion", "level": "subsubsection", "subsections": [], "parent_id": "45dba903-566e-4f9f-a48c-8f8adf0593ca", "prefix_titles": [["title", "Deep Learning for Instance Retrieval: A Survey"], ["section", "$\\!\\!\\!\\!\\!$Retrieval via Learning DCNN Representations "], ["subsection", "Supervised Fine-tuning "], ["subsubsection", "Discussion"]], "content": "\\label{Discussion_ranking_loss}\nIn some cases, pairwise ranking loss cannot effectively learn the variations between samples and still suffers from a weaker generalization capability if the training set is not ordered correctly. Therefore, pairwise ranking loss requires careful sample mining and weighting strategies to obtain the most informative pairs, especially when considering mini-batches.  The hard negative mining strategy is commonly used ,,, however further  sophisticated mining strategies have recently been developed.  Mishchuk \\etal  calculate a pair-wise distance matrix on all mini-batch samples to select two closest negative and one anchor-positive pair to form a triplet. \nInstead of traversing all possible two-tuple or three-tuple combinations, it is possible to consider all positive samples in one cluster and negative samples together. Liu \\etal  introduce a group-group loss to decrease the intra-group distance and increase the inter-group distance.  Considering all samples may be beneficial for stabilizing optimization and promoting generalization due to a larger data diversity, however the extra computational cost remains an issue to be addressed.\nSubstantial research has been devoted to pair-wise ranking loss, while cross entropy loss, mainly used for classification, has been largely overlooked. Recently, Boudiaf \\etal  claim that cross entropy loss can match and even surpass the pair-wise ranking loss when carefully tuned on fine-grained category-level retrieval tasks. In fact, the greatest improvements have come from enhanced training schemes (\\eg data augmentation, learning rate polices, batch normalization freeze) rather than intrinsic properties of pairwise ranking loss. Further, although several sophisticated ranking losses have been explored and validated for category-level retrieval, Musgrave \\etal  revisited these losses and found that most of them perform on par to vanilla Siamese loss and triplet loss, so there is merit to consider these losses also for instance-level image retrieval tasks.\nBoth cross entropy loss and pair-wise ranking loss regularize on the embedded features and the corresponding labels so as to maximize their mutual information . Their effectiveness is not guaranteed to give retrieval results that also optimize mAP .  To tackle this limitation one can directly optimize the average precision (AP) metric using the listwise AP loss,\n\\begin{equation}\n\\begin{aligned}\n\\!\\!\\!\\mathcal{L}_{mAP} = 1 - \\frac{1}{N} \\sum^{N}_{i=1} {\\rm AP}(x_{i}^{\\top}X_{N}, Y_{i})\n\\label{AP_loss}\n\\end{aligned}\n\\end{equation}\nwhich optimizes the global ranking of thousands of images simultaneously, instead of only a few images at a time. Here $Y_{i}$ is the binary label to evaluate the relevance between batch images. $X_{N} = \\{x_{1}, x_{2},...x_{j},...,x_{N}\\}$ denotes the features of all images, where each $x_{i}$ is used as a potential query to rank the remaining batch images. Each similarity score $x_{i}^{\\top}x_{j}$ can be measured by a cosine function. \nIt is demonstrated that training with AP-based loss improves retrieval performance ,. However average precision, as a metric, is normally non-differentiable. To directly optimize the AP loss during back-propagation, the key is that the indicator function for AP computing  needs to be relaxed using methods such as triangular kernel-based soft assignment  or sigmoid function , as shown in Figure \\ref{mAP_loss_illustration} (d).", "cites": [2126, 2127, 2130, 2129, 7549, 2098], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 4.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers by connecting different loss functions (pairwise ranking, cross-entropy, and AP-based) and their implications for instance retrieval. It offers critical analysis by questioning the effectiveness of pairwise ranking loss and highlighting how recent gains may stem from training schemes rather than loss design. The discussion abstracts beyond individual works to identify broader principles in loss optimization and retrieval performance."}}
{"id": "871a2087-f4a4-4968-91d1-0752906a7612", "title": "Unsupervised Fine-tuning ", "level": "subsection", "subsections": ["f6c0b11c-f5b0-47c6-9e48-358712ee7aee", "7ab93d94-79dd-47b1-b0d5-6e73b13d8f40"], "parent_id": "acffc1fe-bf03-450b-857b-db143905236e", "prefix_titles": [["title", "Deep Learning for Instance Retrieval: A Survey"], ["section", "$\\!\\!\\!\\!\\!$Retrieval via Learning DCNN Representations "], ["subsection", "Unsupervised Fine-tuning "]], "content": "\\label{Unsupervised_Fine-tuning}\nSupervised network fine-tuning becomes infeasible when there is insufficient supervisory information, normally because of cost or unavailability. Therefore unsupervised fine-tuning methods for image retrieval are quite necessary, but less studied . \nFor unsupervised fine-tuning, two directions are to mine relevance among features via manifold learning, and via clustering techniques, each discussed below.", "cites": [2131], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces the need for unsupervised fine-tuning in instance retrieval and mentions two directions (manifold learning and clustering) without elaboration or integration. It cites one paper and discusses its general idea, but lacks synthesis of multiple sources, deeper critical evaluation, or abstraction to broader principles. The narrative remains at a descriptive level."}}
{"id": "f6c0b11c-f5b0-47c6-9e48-358712ee7aee", "title": "Mining Samples with Manifold Learning", "level": "subsubsection", "subsections": [], "parent_id": "871a2087-f4a4-4968-91d1-0752906a7612", "prefix_titles": [["title", "Deep Learning for Instance Retrieval: A Survey"], ["section", "$\\!\\!\\!\\!\\!$Retrieval via Learning DCNN Representations "], ["subsection", "Unsupervised Fine-tuning "], ["subsubsection", "Mining Samples with Manifold Learning"]], "content": "\\label{Mining_Samples_with_Manifold_Learning}\nManifold learning focuses on capturing intrinsic correlations on a manifold structure to mine or deduce relevance, as illustrated in Figure \\ref{manifold_learning_pipeline}. Initial similarities between the extracted global features  or local features , are used to construct an affinity matrix, which is then re-evaluated and updated using manifold learning . According to the manifold similarity in the updated affinity matrix, positive and hard negative samples are selected for metric learning using pairwise ranking loss based functions such as pair loss , or triplet loss ,.  Note that this is different from the aforementioned methods for pairwise ranking loss based fine-tuning methods, where the hard positive and negative samples are explicitly selected from an ordered dataset according to the given affinity information.\nIt is important to capture the geometry of the manifold of deep features, generally involving two steps , known as diffusion. First, the affinity matrix (Figure \\ref{manifold_learning_pipeline}) is interpreted as a weighted kNN graph, where each vector is represented by a node, and edges are defined by the pairwise affinities of two connected nodes. Then, the pairwise affinities are re-evaluated in the context of all other elements by diffusing the similarity values through the graph ,,,, with recent strategies proposed such as regularized diffusion (RDP)  and  regional diffusion . For more details on diffusion methods refer to survey .\nMost algorithms follow the two steps of ; the differences among methods lie primarily in three aspects: \n\\begin{enumerate}\n\\item {\\bf Similarity initialization},\nwhich affects the subsequent kNN graph construction in an affinity matrix. Usually, an inner product  or Euclidean distance  is directly computed for the affinities.  A Gaussian kernel function can be used ,, or consider regional similarity from image patches .\n\\item {\\bf Transition matrix definition}, a row-stochastic matrix , determines the probabilities of transiting from one node to another in the graph. These probabilities are proportional to the affinities between nodes, which can be measured by Geodesic distance (\\emph{e.g.} the summation of weights of relevant edges).\n\\item {\\bf Iteration scheme},\nto re-valuate and update the values in the affinity matrix by the manifold similarity until some convergence is achieved. Most algorithms are iteration-based ,, as illustrated in Figure \\ref{manifold_learning_pipeline}. \n\\end{enumerate}\nDiffusion process algorithms are indispensable for unsupervised fine-tuning. Better image similarity is guaranteed when it is improved based on initialization (\\emph{e.g.} regional similarity  or higher order information ). Diffusion is normally iterative and is computationally demanding , a limitation which cannot meet the efficiency requirements of image retrieval. To reduce the  computational complexity, Bai \\etal  propose a regularized diffusion process, facilitated by an efficient iteration-based solver.  Zhao \\etal  regard the diffusion process as a non-linear kernel mapping function, which is then modelled by a deep neural network.  Other studies replace the diffusion process on a kNN graph with a diffusion network , which is derived from graph convolution networks , an end-to-end trainable framework which allows efficient computation during training and testing.\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=0.5\\textwidth]{./Figures/manifoldlearningpipeline.pdf}\n\\caption{Paradigm of manifold learning for unsupervised metric learning, based on triplet loss ,.} \n\\label{manifold_learning_pipeline}\n\\end{figure}\nOnce the manifold space is learned, samples are mined by computing geodesic distances based on the Floyd-Warshall algorithm or by comparing the set difference . The selected samples are fed into deep networks to perform fine-tuning.", "cites": [2131, 2103, 2132, 2100], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the concept of manifold learning across multiple papers, integrating them into a coherent framework involving similarity initialization, transition matrix definition, and iteration schemes. It provides a critical evaluation of the computational limitations of diffusion methods and highlights how recent approaches aim to address these issues. The abstraction level is strong, as it generalizes the process of manifold learning and identifies common patterns and strategies in the field."}}
{"id": "7ab93d94-79dd-47b1-b0d5-6e73b13d8f40", "title": "Mining Samples by Clustering", "level": "subsubsection", "subsections": [], "parent_id": "871a2087-f4a4-4968-91d1-0752906a7612", "prefix_titles": [["title", "Deep Learning for Instance Retrieval: A Survey"], ["section", "$\\!\\!\\!\\!\\!$Retrieval via Learning DCNN Representations "], ["subsection", "Unsupervised Fine-tuning "], ["subsubsection", "Mining Samples by Clustering"]], "content": "\\label{Mining_Samples_by_Clustering}\nClustering is used to explore proximity information that has been studied in instance-level retrieval ,,,,. The rationale behind these methods is that samples in a cluster are likely to satisfy a degree of similarity. \nOne class of methods for clustering deep features is via k-means. Given $k$ cluster centroids, during each training epoch a deep network alternates between two steps: first, a soft assignment between the feature representations and the cluster centroids; second, the cluster centroids are refined and, at the same time, the deep network is updated by learning from current high confidence assignments using a certain regularization. These two steps are repeated until a convergence criterion is met, at which point the cluster assignments are used as pseudo-labels ,. Alternatively, the pseudo-labels can be calculated from the samples in a cluster, \\eg the mean values. For example, Tzelepi \\etal  compute $ k $ nearest feature representations with respect to a query feature and then compute their mean vectors, which is used as a target for the query feature. In this case, fine-tuning is performed by minimizing the squared distance between each query feature and the mean of its $ k $ nearest features. Liu \\etal  propose a self-taught hashing algorithm using a kNN graph construction to generate pseudo labels that are used to analyze and guide network training. Shen \\etal  and Radenovi{\\'c} \\etal , use Structure-from-Motion (SfM) for each image cluster to explore sample reconstructions to select images for triplet loss. Clustering methods depend on the Euclidean distance, making it difficult to reveal the intrinsic relationship between objects.\nThere are further techniques for instance retrieval, such as by using AutoEncoder ,, generative adversarial networks (GANs)  , convolutional kernel networks ,, and graph convolutional networks . For these methods, they focus on devising novel unsupervised frameworks to realize unsupervised learning, instead of iterative similarity diffusion or cluster refinement on feature space. For example, instead of performing iterative traversal on a set of nearest neighbors defined by kNN graph, Liu \\etal  employ graph convolutional networks  to directly encode the neighbor information into image descriptors and then train the deep models to learn a new feature space. This method is demonstrated to significantly improve retrieval accuracy while maintaining efficiency.  GANs are also explored, for the first time, for instance-level retrieval in an unsupervised fashion . The generator retrieves images that contain similar instances as a given image, while the discriminator judges whether the retrieved images have the specified instance which appeared in the query image. During training, the discriminator and the generator play a min-max game via an adversarial reward which is computed based on the cosine distance between the query image and the images retrieved by the generator.", "cites": [9094, 2135, 2133, 2128, 7544, 2134], "cite_extract_rate": 0.4, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes key clustering-based fine-tuning methods and connects them to broader themes in unsupervised learning for retrieval. It identifies common patterns, such as the use of pseudo-labels and regularization, and introduces adversarial and graph-based approaches as alternatives. While it offers some critique, such as the limitations of Euclidean distance in clustering, it could provide deeper analysis of trade-offs or methodological shortcomings. The abstraction is moderate, as it generalizes to a few overarching ideas but does not fully articulate a novel conceptual framework."}}
{"id": "347ae713-adb4-418f-9e62-4191f141d96c", "title": "Datasets", "level": "subsection", "subsections": [], "parent_id": "a6c69e00-d8a1-492b-8514-fab570f109d2", "prefix_titles": [["title", "Deep Learning for Instance Retrieval: A Survey"], ["section", "State of the Art Performance"], ["subsection", "Datasets"]], "content": "\\label{Datasets_and_Evaluation_Criteria}\nTo demonstrate the effectiveness of methods, we choose the following commonly-used datasets for performance comparison:\n\\textbf{UKBench (UKB)}  consists of 10,200 images of objects. This dataset has 2,550 groups of images, each group having four images of the same object from different viewpoints or illumination conditions, which can be regarded as a kind of class-level supervision information. All images can be used as a query.\n\\textbf{Holidays}  consists of 1,491 images collected from personal holiday albums. Most images are scene-related. The dataset comprises 500 groups of similar images with a single query image for each group. The dataset also provides position information of the interest regions for each image.\n\\textbf{Oxford-5k}  consists of 5,062 images for 11 Oxford buildings. Each building is associated with five hand-drawn bounding box queries. \\textcolor{black}{According to the relevance level, each image of the same building is assigned a label \\textit{Good} (\\ie \\textit{positive}), \\textit{OK} (\\ie \\textit{positive}), \\textit{Junk}, or \\textit{Bad} (\\ie \\textit{negative}). \\textit{Junk} images can be \ndiscarded or regarded as \\textit{negative} examples ,.\nTo build a tuple for each given query, one can select a positive example whose label corresponds to \\textit{Good} or \\textit{OK} in the same category, and select one negative example from each of the remaining building categories. Furthermore, an additional disjoint set of 100,000 distractor images is added to obtain Oxford-105k.}\n\\textbf{Paris-6k}  includes 6,412 images and is categorized into 12 groups by architecture. \nThe supervision information can be used like that of Oxford-5k. Likewise, an additional disjoint set of 100,000 distractor images is added to obtain Paris-106k.\n\\textbf{INSTRE}  consists of 28,543 images from 250 different object classes, including three disjoint subsets\\footnote{https://github.com/imatge-upc/salbow}: INSTRE-S1, INSTRE-S2, INSTRE-M. \n\\textcolor{black}{INSTRE dataset has bounding box annotations, providing single-labelled and double-labelled class information for single- and multiple-object retrieval, respectively. One can use the class information to build a tuple, with two positive examples from the same class and one negative from one of the remaining classes. The performance evaluation on INSTRE in our experiments follows the protocol in . }\n\\textbf{Google Landmarks Dataset (GLD)} , consists of GLD-v1 and GLD-v2. \\textcolor{black}{GLD-v2 is mainly recommended to use and it has the advantage of stability where all images have permissive licenses . GLD-v2 is divided into three subsets: (i) 118k query images with ground-truth annotations, (ii) 4.1M training images of 203k landmarks with labels, and (iii) 762k index images of 101k landmarks. Due to its large scale, GLD-v2 provides class-level ground-truth which can be used to build training tuples. Due to its image diversity, it may produce clutter images for each landmark so it is necessary to introduce pre-processing methods to select the more relevant images . Finally, the training set is cleaned by removing these clutters, consisting of a subset ``GLD-v2-clean'' containing 1.6M images of 81k landmarks. Since Google landmarks dataset stills lack bounding box for objects of interest, Teichmann \\etal  provide a new dataset of landmark bounding boxes, based on GLD. This patch-level supervision information can help locate the most relevant regions.  }\n\\textcolor{black}{Note that, additional queries and distractor images have been added into Oxford-5k and Paris-6k, producing the Revisited Oxford ($\\mathcal{R}$Oxford) and Revisited Paris ($\\mathcal{R}$Paris) datasets where each image of the same building is assigned a label \\textit{Easy}, \\textit{Hard}, \\textit{Unclear}, or \\textit{Negative} . Different label combinations are used as \\textit{positive} according to the difficulty level of different setups. During testing, if there are no positive images for a query, then that query is excluded from the evaluation. For details, we refer the reader to . We undertake partial comparisons under the \\textit{hard} evaluation protocol on these revisited datasets. }", "cites": [2114, 2136, 2137, 2093, 2111], "cite_extract_rate": 0.38461538461538464, "origin_cites_number": 13, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear and factual overview of widely used datasets in instance retrieval, integrating basic descriptions from cited papers. It synthesizes the structure and evaluation protocols of these datasets but does so in a largely descriptive manner. Critical analysis and abstraction are limited, as the section does not deeply evaluate the strengths/weaknesses of the datasets or generalize their roles in the broader context of deep learning for retrieval."}}
{"id": "4289168f-a812-4899-b8a0-52669cd0ba36", "title": "Performance Comparison and Analysis", "level": "subsection", "subsections": [], "parent_id": "a6c69e00-d8a1-492b-8514-fab570f109d2", "prefix_titles": [["title", "Deep Learning for Instance Retrieval: A Survey"], ["section", "State of the Art Performance"], ["subsection", "Performance Comparison and Analysis"]], "content": "\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.248\\textwidth}\n\\resizebox{\\columnwidth}{!}{\n\\begin{tikzpicture} \n\\scriptsize\n\\begin{axis}[legend style={at={(0.95,0.3)}, scale=0.5, anchor=north east, font= \\scriptsize}, symbolic x coords={2014, 2015, 2016, 2017, 2018, 2019, 2020 }, grid=both, ylabel={mAP(\\%)}, ymin=77.5, ymax=99, xmin=2014, xmax=2020, xtick=data, x tick label style={rotate=45,anchor=east} ];\n\\addlegendentry{Holidays}\n\\addplot[mark=square*,thick, line width=0.45mm, color={rgb, 255:red,152;green,179;blue,134}, on layer=main] coordinates { (2014, 80.18) (2015,89.7) (2016,94.2) (2017,95.13) (2018,95.67) (2019, 95.5) (2020, 94.0) };\n\\node [above right=0cm, font=\\tiny] at (axis cs:2014, 80.5) {80.18 }; \\node [ font=\\tiny]  at (axis cs:2015, 90.4) {89.7 }; \\node [above right=-0.3cm, font=\\tiny]  at (axis cs:2016, 93.4) {94.2 }; \\node [above right=-0.3cm, font=\\tiny]  at (axis cs:2017, 94.8) {95.13 }; \\node [above right=-0.4cm, font=\\tiny]  at (axis cs:2018, 95.6) {95.7 };  \\node [above right=-0.5cm, font=\\tiny]  at (axis cs:2019, 95.9) {95.5 }; \\node [above right=-0.8cm, font=\\tiny]  at (axis cs:2020, 96) {94.0 };\n\\addlegendentry{UKBench}\n\\addplot[mark=diamond*,thick,line width=0.45mm, color={rgb,255:red,141;green,170;blue,196}, on layer=main,] coordinates {(2014,91.1) (2015,91.3) (2016,96.3) (2018,98.08) (2020,98.84) }; \\node [above right=0cm, font=\\tiny] at (axis cs:2014, 91) {91.1 }; \\node [above right=-0.4cm, font=\\tiny]  at (axis cs:2015, 92.8) {91.3 }; \\node [above right=-0.3cm, font=\\tiny]  at (axis cs:2016, 97.4) {96.3 }; \\node [above right=-0.4cm, font=\\tiny]  at (axis cs:2018, 99.5) {98.1 }; \\node [above right=-0.8cm, font=\\tiny]  at (axis cs:2020, 100.8) {98.8 };\n\\addlegendentry{Oxford-5k}\n\\addplot[mark=*,thick, on layer=background, line width=0.45mm, color={rgb, 255:red,237;green,160;blue,155}] coordinates { (2014,78.34) (2015,84.4) (2016,88.95) \n(2018,95.8) (2019,96.2)  (2020,96.2) }; \\node [above right=0cm, font=\\tiny] at (axis cs:2014, 77.5) {78.34 };  \\node [ font=\\tiny]  at (axis cs:2015, 85.15) {84.4 }; \\node [above right=-0.3cm, font=\\tiny]  at (axis cs:2016, 88.6) {88.95  }; \n\\node [above right=-0.4cm, font=\\tiny]  at (axis cs:2018, 97.2) {95.8 }; \\node [above right=-0.5cm, font=\\tiny]  at (axis cs:2019, 98) {96.2 }; \\node [above right=-0.8cm, font=\\tiny]  at (axis cs:2020, 99.1) {96.2 };\n\\addlegendentry{Paris-6k}\n\\addplot[mark=triangle*,thick,line width=0.45mm, color={rgb, 255:red,217;green,188;blue,102}, on layer=main,] coordinates { (2014,86.83) (2015,86.5) (2016,95.88) (2017,96.0) (2018, 97.0) (2019, 97.8) (2020, 97.4) };\n\\node [above right=0cm, font=\\tiny] at (axis cs:2014, 86.8) {86.83 }; \\node [above right=-0.3cm, font=\\tiny]  at (axis cs:2015, 87.8) {86.5 }; \\node [above right=-0.3cm, font=\\tiny]  at (axis cs:2016, 95.6) {95.8 };\\node [above right=-0.3cm, font=\\tiny]  at (axis cs:2017, 97.0) {96.0 }; \\node [above right=-0.4cm, font=\\tiny]  at (axis cs:2018, 98.4) {97.0 }; \\node [above right=-0.5cm, font=\\tiny]  at (axis cs:2019, 99.3) {97.8 }; \\node [above right=-0.8cm, font=\\tiny]  at (axis cs:2020, 100) {97.4 };\n\\end{axis}\n\\end{tikzpicture}\n} \n\\vspace{-2.1em}\n\\end{subfigure}\n\\begin{subfigure}{0.235\\textwidth}\n\\resizebox{\\columnwidth}{!}{\n\\begin{tikzpicture} \n\\scriptsize\n\\begin{axis}[legend style={at={(0.425,0.3)}, scale=0.5, anchor=north east, font=\\scriptsize}, symbolic x coords={2017, 2018, 2019, 2020}, grid=both, ylabel={}, ymin=15, ymax=95, xmin=2017, xmax=2020, xtick=data, x tick label style={rotate=45, anchor=east} ];\n\\addlegendentry{$\\mathcal{R}$Oxford-5k}\n\\addplot[mark=*,thick,line width=0.45mm, color={rgb, 255:red,181;green,144;blue,202}, on layer=main,] coordinates { (2018, 54.8) (2019, 65.8) (2020, 64.0) };\n\\node [above right=-0.3cm, font=\\tiny]  at (axis cs:2018, 53.8) {54.8 }; \\node [above right=-0.3cm, font=\\tiny]  at (axis cs:2019, 72.5) {65.8 }; \\node [above right=-0.8cm, font=\\tiny]  at (axis cs:2020, 74.0) {64.0 };\n\\addlegendentry{$\\mathcal{R}$Paris-6k}\n\\addplot[mark=triangle*,thick,line width=0.45mm, color={rgb, 255:red,117;green,207;blue,184}, on layer=main,] coordinates { (2018, 84.0) (2019, 85.3) (2020, 80.4) };\n\\node [above right=-0.3cm, font=\\tiny]  at (axis cs:2018, 92.5) {84.0 }; \\node [above right=-0.3cm, font=\\tiny]  at (axis cs:2019, 84.0) {85.3 }; \\node [above right=-0.9cm, font=\\tiny]  at (axis cs:2020, 93.5) {80.4 };\n\\addlegendentry{INSTRE}\n\\addplot[mark=diamond*,thick,line width=0.45mm, color={rgb, 255:red,83;green,82;blue,4}, on layer=main,] coordinates { (2017, 80) (2018, 80.5) (2019, 92.4)};\n\\node [above right=-0.05cm, font=\\tiny]  at (axis cs:2017, 71.5) {80.0 }; \\node [above right=-0.3cm, font=\\tiny]  at (axis cs:2018, 79.5) {80.5 }; \\node [above right=0cm, font=\\tiny]  at (axis cs:2019, 86) {92.4 } ;\n\\addlegendentry{GLD-v2}\n\\addplot[mark=square*,thick,line width=0.45mm, color={rgb, 255:red,130;green,54;blue,203}, on layer=main,] coordinates {(2020, 26.8) };\n\\node [above right=-0.8cm, font=\\tiny] at (axis cs:2020, 38) {26.8 };\n\\end{axis};\n\\end{tikzpicture}\n}\n\\vspace{-2.1em}\n\\end{subfigure}\n\\caption{Performance improved from 2014 to 2020.} \n\\label{datasets_progress_2014_2_2020}\n\\end{figure}\n\\textbf{Overview.} Figure \\ref{datasets_progress_2014_2_2020} summarizes the performance over 6 datasets from 2014 to 2020. Early on, the powerful feature extraction of DCNNs led to rapid improvements. Subsequent key ideas have been to extract instance features at the region level to reduce image clutter , and to improve feature discriminativity by using methods including feature fusion ,,, feature aggregation ,, and feature embedding . Fine-tuning is an important strategy to improve performance by tuning deep networks specific for learning instance features ,. For instance, the accuracy increases steadily from 78.34\\%  to 96.2\\%  on the Oxford-5k dataset when manifold learning is used to fine-tune deep networks. The mAP on $\\mathcal{R}$Paris-6k and $\\mathcal{R}$Oxford-5k is smaller than Paris-6k and Oxford-5k, leaving room for improvement.\nWe report results using off-the-shelf models (Table \\ref{Table_retrieval_off_the_shelf}) and fine-tuning networks (Table \\ref{Metric_learning_table_B}). In Table \\ref{Table_retrieval_off_the_shelf}, single-pass and multiple-pass are analyzed, while supervised and unsupervised fine-tuning are compared in Table \\ref{Metric_learning_table_B}. Since there are many aspects that vary across the different methods, making them not directly comparable, we mainly draw some general claims or trends based on the collected results.\n\\begin{table*}[t]\n\\centering \n\\caption{\\footnotesize Performance evaluation of off-the-shelf DCNN models. ``$ \\bullet $'' indicates that  the models or layers are combined to learn features; ``PCA$_{w}$ indicates PCA with whitening on the extracted features to improve robustness; ``MP'' means Max Pooling; ``SP'' means Sum Pooling. The CNN-M network with ``$\\ast$'' has an architecture similar to that of AlexNet. ``-'' means that the results were not reported. }\n\\vspace{-1em}\n\\label{Table_retrieval_off_the_shelf}\n\\renewcommand{\\arraystretch}{1.0}\n\\resizebox{19.5cm}{!}{\n\\begin{tabular}{!{\\vrule width1.2bp}c|c|c|c|c|c|c|c|c|c|p{9cm}!{\\vrule width1.2bp}}\n\\Xhline{1pt}\n\\footnotesize Type  & \\footnotesize \\shortstack [c]  { Method }  & \\footnotesize \\shortstack [c] {Backbone \\\\ DCNN}\t& \\footnotesize \\shortstack [c] {Output \\\\ Layer} & \\footnotesize \\shortstack [c] {Embed. \\\\ Aggre.}\t& \\footnotesize \\shortstack [c] { Feat. \\\\ Dim } & \\footnotesize \\shortstack [c] {Holidays} & \\footnotesize \\shortstack [c] {UKB} & \\footnotesize \\shortstack [c] {Oxford5k \\\\ (+100k)} &  \\footnotesize \\shortstack [c] {Paris6k \\\\ (+100k)} &  \\footnotesize Brief Conclusions and Highlights \\\\\n\\Xhline{1.0pt}\n\\multirow{5}{*}{  \\raisebox{-15ex}[0pt]{\\begin{tabular}[c]{@{}c@{}}  \\rotatebox{90}{Single-pass} \\end{tabular}}}  & \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] {  Neural \\\\  codes } } & \\footnotesize\t\\raisebox{-2ex}[0pt]{ AlexNet} & \\footnotesize\t\\raisebox{-2ex}[0pt]{ \\shortstack [c] { FC6}}\t& \\footnotesize \\raisebox{-2ex}[0pt]{\\shortstack [c] { PCA}}   & \\footnotesize \\raisebox{-2ex}[0pt]{ $128$} & \\footnotesize \\raisebox{-2ex}[0pt]{ $74.7$} & \\footnotesize \\raisebox{-3ex}[0pt]{\\shortstack [c] { $ 3.42 $ \\\\ \\textcolor[rgb]{0,0,0}{(N-S)}}}& \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] {  $43.3$\\\\ (38.6)}} & \\footnotesize\t\\raisebox{-2.0ex}[0pt]{\\shortstack [c] {$-$}} & \\footnotesize  Compressed neural codes of different layers are explored. AlexNet is also fine-tuned for retrieval.\\\\   \\cline{2-11} \n& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] { SPoC } } & \\footnotesize\t\\raisebox{-2ex}[0pt]{ VGG16} & \\footnotesize\t\\raisebox{-2ex}[0pt]{ \\shortstack [c] { Conv5}}\t& \\footnotesize \\raisebox{-2.5ex}[0pt]{\\shortstack [c] { SPoC + \\\\ PCA$_{w} $}}   & \\footnotesize \\raisebox{-2ex}[0pt]{ 256} & \\footnotesize \\raisebox{-2ex}[0pt]{ 80.2} & \\footnotesize \\raisebox{-3ex}[0pt]{\\shortstack [c] { $ 3.65 $ \\\\ \\textcolor[rgb]{0,0,0}{(N-S)}}} & \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] { $58.9$\\\\ (57.8)}} & \\footnotesize\t\\raisebox{-2.0ex}[0pt]{\\shortstack [c] {$-$}} & \\footnotesize Exploring Gassian weighting scheme \\ie the centering prior, to improve the discrimination of\nfeatures. \\\\   \\cline{2-11} \n& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {  CroW } } & \\footnotesize\t\\raisebox{-2ex}[0pt]{VGG16} & \\footnotesize\t\\raisebox{-2ex}[0pt]{ \\shortstack [c] { Conv5}}\t& \\footnotesize \\raisebox{-2.5ex}[0pt]{\\shortstack [c] { CroW + \\\\ PCA$_{w} $}}   & \\footnotesize \\raisebox{-2ex}[0pt]{256} & \\footnotesize \\raisebox{-2ex}[0pt]{ $85.1$} & \\footnotesize \\raisebox{-2ex}[0pt]{\\shortstack [c] { $-$}} & \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] {$68.4$\\\\ (63.7)}} & \\footnotesize\t\\raisebox{-3.0ex}[0pt]{\\shortstack [c] { $ 76.5 $\\\\ (69.1) }} & \\footnotesize  The spatialwise and channelwise weighting mechanisms are utilized to highlight crucial convolutional features. \\\\   \\cline{2-11} \n& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {  R-MAC } } & \\footnotesize\t\\raisebox{-2ex}[0pt]{ VGG16} & \\footnotesize\t\\raisebox{-2ex}[0pt]{ \\shortstack [c] {  Conv5}}\t& \\footnotesize \\raisebox{-2.5ex}[0pt]{\\shortstack [c] { R-MAC + \\\\  PCA$_{w} $}}   & \\footnotesize \\raisebox{-2ex}[0pt]{512} & \\footnotesize\t\\raisebox{-2.0ex}[0pt]{\\shortstack [c] {$-$}} & \\footnotesize \\raisebox{-2ex}[0pt]{\\shortstack [c] {$-$ }} & \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] {  $66.9$\\\\ (61.6)}} & \\footnotesize\t\\raisebox{-3.0ex}[0pt]{\\shortstack [c] { $-$ \\\\ (75.7) }} & \\footnotesize  Sliding windows with different scales on convolutional feature maps to encode multiple image regions.\\\\   \\cline{2-11} \n& \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] { Multi-layer \\\\  CNN } } & \\footnotesize\t\\raisebox{-2ex}[0pt]{VGG16} & \\footnotesize\t\\raisebox{-3ex}[0pt]{ \\shortstack [c] { FC6 $\\bullet$ \\\\ Conv4$\\sim$5 }}\t& \\footnotesize \\raisebox{-2ex}[0pt]{\\shortstack [c] { SP}}   & \\footnotesize \\raisebox{-2ex}[0pt]{4096} & \\footnotesize \\raisebox{-2ex}[0pt]{  91.4} & \\footnotesize \\raisebox{-3ex}[0pt]{\\shortstack [c] { $ 3.68 $ \\\\ \\textcolor[rgb]{0,0,0}{(N-S)}}} & \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] { $61.5$\\\\($-$)}} & \\footnotesize\t\\raisebox{-2.0ex}[0pt]{\\shortstack [c] {$-$}} & \\footnotesize Layer-level feature fusion and the complementary properties of different layers are explored.\\\\   \\cline{2-11} \n& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] { BLCF } } & \\footnotesize\t\\raisebox{-2ex}[0pt]{ VGG16} & \\footnotesize\t\\raisebox{-2ex}[0pt]{ \\shortstack [c] {  Conv5}}\t& \\footnotesize \\raisebox{-2.5ex}[0pt]{\\shortstack [c] { BoW + \\\\ PCA$_{w} $}}   & \\footnotesize \\raisebox{-2ex}[0pt]{ 25k} & \\footnotesize \\raisebox{-2ex}[0pt]{$-$} & \\footnotesize \\raisebox{-2ex}[0pt]{\\shortstack [c] { $-$}}& \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] {  $73.9$\\\\ (59.3)}} & \\footnotesize\t\\raisebox{-3.0ex}[0pt]{\\shortstack [c] { $ 82.0 $\\\\  (64.8) }} & \\footnotesize  Both global features and local features are explored, demonstrating that local features have higher accuracy. \\\\   \\cline{2-11} \n\\Xhline{1.0pt}\n\\multirow{7}{*}{  \\raisebox{-20ex}[0pt]{\\begin{tabular}[c]{@{}c@{}}  \\rotatebox{90}{Multiple-pass} \\end{tabular}}}\n& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {  OLDFP  } } & \\footnotesize \\raisebox{-2ex}[0pt]{\\shortstack [c] {AlexNet }} & \\footnotesize\t\\raisebox{-2ex}[0pt]{ \\shortstack [c] {  FC6}}\t& \\footnotesize \\raisebox{-2.6ex}[0pt]{\\shortstack [c] { MP \\\\  +  PCA$_{w} $ }}   & \\footnotesize \\raisebox{-2ex}[0pt]{ 512} & \\footnotesize \\raisebox{-2ex}[0pt]{ 88.5} & \\footnotesize \\raisebox{-3ex}[0pt]{\\shortstack [c] {$ 3.81 $ \\\\ \\textcolor[rgb]{0,0,0}{(N-S)}}}  & \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] {  $60.7$\\\\ ($-$)}} & \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] { $66.2$\\\\ ($-$)}} & \\footnotesize  Exploring the impact of proposal number. Patches are extracted by RPNs (see Figure \\ref{MultiplePass} (d)) and the features are encoded in an orderless way.\\\\   \\cline{2-11} \n& \\footnotesize \\raisebox{-1.6ex}[0pt]{ \\shortstack [c] {  MOP-CNN   } } & \\footnotesize \\raisebox{-2ex}[0pt]{\\shortstack [c] { AlexNet }} & \\footnotesize\t\\raisebox{-2ex}[0pt]{ \\shortstack [c] { FC7}}  &  \\footnotesize \\raisebox{-2.5ex}[0pt]{\\shortstack [c] { VLAD \\\\  +  PCA$_{w} $ }}  & \\footnotesize \\raisebox{-2ex}[0pt]{ 2048} & \\footnotesize \\raisebox{-2ex}[0pt]{ 80.2} & \\footnotesize \\raisebox{-2ex}[0pt]{\\shortstack [c] { $-$}}   & \\footnotesize\t\\raisebox{-2.0ex}[0pt]{\\shortstack [c] {$-$}} &  \\footnotesize\t\\raisebox{-2.0ex}[0pt]{\\shortstack [c] {$-$}} & \\footnotesize Image patches are extracted densely, as shown in Figure \\ref{MultiplePass} (c). Multi-scale patch features are further embedded into VLAD descriptors. \\\\   \\cline{2-11} \n& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {  CNNaug-ss } } & \\footnotesize \\raisebox{-3ex}[0pt]{\\shortstack [c] { Overfeat\\\\  }} & \\footnotesize\t\\raisebox{-2ex}[0pt]{ \\shortstack [c] { FC}}\t\t& \\footnotesize \\raisebox{-2ex}[0pt]{\\shortstack [c] {  PCA$_{w} $ }}   & \\footnotesize \\raisebox{-2ex}[0pt]{ 15k} & \\footnotesize \\raisebox{-2ex}[0pt]{ 84.3} & \\footnotesize \\raisebox{-3ex}[0pt]{\\shortstack [c] { $ 91.1 $ \\\\ \\textcolor[rgb]{0,0,0}{(mAP)}}}  & \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] { $68.0$\\\\($-$)}} & \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] { $79.5$\\\\ ($-$)}} & \\footnotesize Image patches are extracted densely, as shown in Figure \\ref{MultiplePass} (c). Image regions at different locations with different sizes are included.\\\\   \\cline{2-11} \n& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {  MOF } } &  \\footnotesize\t\\raisebox{-3ex}[0pt]{ \\shortstack [c] { CNN-M$^{\\ast}$ \\\\  }} & \\footnotesize\t\\raisebox{-3ex}[0pt]{ \\shortstack [c] { FC7 $\\bullet$ \\\\  Conv }}\t& \\footnotesize \\raisebox{-3ex}[0pt]{\\shortstack [c] { SP or MP\\\\ +  BoW }}   & \\footnotesize \\raisebox{-2ex}[0pt]{ 20k} & \\footnotesize \\raisebox{-2ex}[0pt]{ 76.8} & \\footnotesize \\raisebox{-3ex}[0pt]{\\shortstack [c] { $ 3.00 $ \\\\ \\textcolor[rgb]{0,0,0}{(N-S)}}}  & \\footnotesize\t\\raisebox{-2.0ex}[0pt]{\\shortstack [c] {$-$}} &  \\footnotesize\t\\raisebox{-2.0ex}[0pt]{\\shortstack [c] {$-$}} & \\footnotesize Exploring layer-level fusion scheme. Image patches are extracted using spatial pyramid modeling, as shown in Figure \\ref{MultiplePass} (b).\\\\   \\cline{2-11} \n& \\footnotesize \\raisebox{-2.7ex}[0pt]{ \\shortstack [c] { Multi-scale \\\\  CNN } } & \\footnotesize\t\\raisebox{-2ex}[0pt]{ VGG16} & \\footnotesize\t\\raisebox{-2ex}[0pt]{ \\shortstack [c] {  Conv5}} & \\footnotesize \\raisebox{-2.7ex}[0pt]{\\shortstack [c] { SP or MP\\\\  +  PCA$_{w} $ }}   & \\footnotesize \\raisebox{-2ex}[0pt]{ 32k} & \\footnotesize \\raisebox{-2ex}[0pt]{  89.6} & \\footnotesize \\raisebox{-3ex}[0pt]{\\shortstack [c] { $ 95.1 $ \\\\ \\textcolor[rgb]{0,0,0}{(mAP)}}}  & \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] {  $84.3$\\\\ ($-$)}} & \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] {  $87.9$\\\\ ($-$)}} & \\footnotesize Image patches are extracted in a dense manner, as shown in Figure \\ref{MultiplePass} (c). Geometric invariance is considered when aggregating patch features.\\\\   \\cline{2-11} \n& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {  LDD } } & \\footnotesize\t\\raisebox{-2ex}[0pt]{ VGG19} & \\footnotesize\t\\raisebox{-2ex}[0pt]{ \\shortstack [c] {  Conv5}}\t& \\footnotesize \\raisebox{-2.6ex}[0pt]{\\shortstack [c] {BoW \\\\ +  PCA$_{w} $ }}   & \\footnotesize \\raisebox{-2ex}[0pt]{500k} & \\footnotesize \\raisebox{-2ex}[0pt]{ 84.6 } & \\footnotesize \\raisebox{-2ex}[0pt]{\\shortstack [c] {$-$  }}  & \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] {  $83.3$ \\\\ ($-$)}} & \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] {  $87.2$ \\\\ ($-$)}} & \\footnotesize Image patches are obtained using a uniform square mesh, as shown in Figure \\ref{MultiplePass} (a). Patch features are encoded into BoW descriptors.\\\\   \\cline{2-11} \n& \\footnotesize \\raisebox{-1.7ex}[0pt]{ \\shortstack [c] {  DeepIndex } } &  \\footnotesize\t\\raisebox{-2.7ex}[0pt]{ \\shortstack [c] { AlexNet \\\\  $\\bullet$  VGG19 }} & \\footnotesize\t\\raisebox{-2.7ex}[0pt]{ \\shortstack [c] { FC6-7 $\\bullet$\\\\ FC17-18 }}\t& \\footnotesize \\raisebox{-2.7ex}[0pt]{\\shortstack [c] {BoW + \\\\  PCA }}   & \\footnotesize \\raisebox{-2ex}[0pt]{ 512} & \\footnotesize \\raisebox{-2ex}[0pt]{ 81.7} & \\footnotesize \\raisebox{-2.7ex}[0pt]{\\shortstack [c] { $ 3.32 $ \\\\ \\textcolor[rgb]{0,0,0}{(N-S)}}}  & \\footnotesize\t\\raisebox{-2.0ex}[0pt]{\\shortstack [c] {$-$}} & \\footnotesize \\raisebox{-2.7ex}[0pt]{ \\shortstack [c] { $75.4$\\\\ ($-$)}} & \\footnotesize Exploring layer-level and model-level fusion methods. Image patches are extracted using spatial pyramid modeling, as shown in Figure \\ref{MultiplePass} (b).\\\\   \\cline{2-11} \n& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {  CCS } } & \\footnotesize \\raisebox{-2ex}[0pt]{\\shortstack [c] { GoogLeNet }} & \\footnotesize\t\\raisebox{-2ex}[0pt]{ \\shortstack [c] { Conv}}\t& \\footnotesize \\raisebox{-2.7ex}[0pt]{\\shortstack [c] { VLAD \\\\   +  PCA$_{w} $ }}   & \\footnotesize \\raisebox{-2ex}[0pt]{ 128} & \\footnotesize \\raisebox{-2ex}[0pt]{ 84.1} & \\footnotesize \\raisebox{-3ex}[0pt]{\\shortstack [c] { $ 3.81 $ \\\\ \\textcolor[rgb]{0,0,0}{(N-S)}}}  & \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] { $64.8$\\\\ ($-$)}} & \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] { $76.8$\\\\ ($-$)}} & \\footnotesize Object proposals are extracted by RPNs, as shown in Figure \\ref{MultiplePass} (d). Object level and point level feature concatenation schemes are explored.\\\\   \\cline{2-11} \n\\Xhline{1.0pt}\n\\end{tabular}\n}\n\\vspace{-0.5em}\n\\end{table*}\n\\begin{table*}[!t]\n\\centering \n \\caption{ \\footnotesize Performance evaluation of methods in which DCNN models are fine-tuned, in a supervised or an unsupervised manner. ``CE Loss'' means the models are fine-tuned using the classification-based loss function in the form of Eq. \\ref{cross_entropy_loss}. ``Siamese Loss'' is in the form of Eq. \\ref{contrastive}. ``Regression Loss'' is in the form of Eq. \\ref{regressionLoss}. ``Triplet Loss'' is in the form of Eq. \\ref{triplet}. }\n \\vspace{-1em}\n \\label{Metric_learning_table_B} \n \\renewcommand{\\arraystretch}{1.0}\n\\resizebox{19.5cm}{!}{\n\\begin{tabular}{!{\\vrule width1.2bp}c|p{2.0cm}|c|p{1.35cm}|c|p{1.1cm}|p{1.25cm}|c|c|p{0.7cm}|c|c|p{7.8cm}!{\\vrule width1.2bp}}\n\\Xhline{1pt}\n\\footnotesize Type  & \\footnotesize \\shortstack [c]  { Method }  & \\footnotesize \\shortstack [c] {Backbone \\\\ DCNN} &   \\footnotesize \\shortstack [c] {Training \\\\ set}\t& \\footnotesize \\shortstack [c] {Output \\\\ Layer} & \\footnotesize \\shortstack [c] {Embed. \\\\ Aggre.}\t& \\footnotesize \\shortstack [c] { Loss \\\\ Function } & \\footnotesize \\shortstack [c] { Feat. \\\\ Dim } & \\footnotesize \\shortstack [c] {Holidays} & \\footnotesize \\shortstack [c] {UKB} & \\footnotesize \\shortstack [c] {Oxford5k \\\\ (+100k)} &  \\footnotesize \\shortstack [c] {Paris6k \\\\ (+100k)} &  \\footnotesize Brief Conclusions and Highlights \\\\ \\Xhline{1.0pt}\n\\multirow{5}{*}{  \\raisebox{-24ex}[0pt]{\\begin{tabular}[c]{@{}c@{}}  \\rotatebox{90}{Supervised Fine-tuning}\\end{tabular}}}  \n& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] { $\\!\\!\\!\\!\\!\\! $ Neural codes } } & \\footnotesize\t\\raisebox{-2ex}[0pt]{ AlexNet} & \\footnotesize \\raisebox{-3.5ex}[0pt]{\\shortstack [c] { Landmarks \\\\  }}  & \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [l] {  FC6}}\t&  \\footnotesize \\raisebox{-2ex}[0pt]{\\shortstack [c] {  PCA }} & \\footnotesize\t\\raisebox{-2ex}[0pt]{ \\shortstack [c] { CE Loss}}  & \\footnotesize \\raisebox{-2ex}[0pt]{ 128} & \\footnotesize \\raisebox{-2ex}[0pt]{ 78.9 } &  \\footnotesize \\raisebox{-3.5ex}[0pt]{\\shortstack [c] { $ 3.29 $ \\\\ \\textcolor[rgb]{0,0,0}{(N-S)}}} & \\footnotesize \\raisebox{-3.5ex}[0pt]{ \\shortstack [c] { $55.7$\\\\ (52.3)}} & \\footnotesize \\raisebox{-2.0ex}[0pt]{\\shortstack [c] {$-$}} & \\footnotesize The first work which fine-tunes deep networks for IIR. Compressed neural codes and different layers are explored. \\\\  \\cline{2-13} \n& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {  Nonmetric } } & \\footnotesize\t\\raisebox{-2ex}[0pt]{VGG16} & \\footnotesize\t\\raisebox{-2ex}[0pt]{ $\\!\\!\\! $ Landmarks }  & \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [l] { Conv5}}\t&  \\footnotesize \\raisebox{-2ex}[0pt]{\\shortstack [c] { PCA$_w$ }} & \\footnotesize\t\\raisebox{-3.2ex}[0pt]{ \\shortstack [c] { $\\!\\!\\!\\! $ Regression \\\\ $\\!\\!\\!\\! $ Loss}}  & \\footnotesize \\raisebox{-2ex}[0pt]{512} &  \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {$-$} } & \\footnotesize\t\\raisebox{-2ex}[0pt]{\\shortstack [c] {$-$}} & \\footnotesize \\raisebox{-3.5ex}[0pt]{ \\shortstack [c] { $88.2$\\\\ (82.1)}} &  \\footnotesize \\raisebox{-3.5ex}[0pt]{ \\shortstack [c] {  $88.2$\\\\ (82.9)}} & \\footnotesize Similarity learning of similar and dissimilar pairs is performed by a neural network, optimized using regression loss. \\\\   \\cline{2-13} \n& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] { SIAM-FV  } } & \\footnotesize\t\\raisebox{-2ex}[0pt]{ VGG16}  & \\footnotesize\t\\raisebox{-2ex}[0pt]{ $\\!\\!\\!\\! $ Landmarks } & \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [l] {  Conv5}}\t&  \\footnotesize\t\\raisebox{-2.7ex}[0pt]{ \\shortstack [c] {  FV + \\\\ PCA$_w$ }} & \\footnotesize\t\\raisebox{-3ex}[0pt]{ \\shortstack [c] { Siamese \\\\  Loss}} & \\footnotesize \\raisebox{-2ex}[0pt]{ 512} &  \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {$-$} } & \\footnotesize\t\\raisebox{-2ex}[0pt]{\\shortstack [c] {$-$}} & \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] {  $81.5$\\\\ (76.6)}} &  \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] {  $82.4$\\\\ ($-$)}} & \\footnotesize Fisher Vector is integrated on top of VGG and is trained with VGG simultaneously. \\\\   \\cline{2-13} \n& \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] {  Faster \\\\  R-CNN  } } & \\footnotesize\t\\raisebox{-2ex}[0pt]{ VGG16} & \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] {  Oxford5k \\\\  Paris6k } } & \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [l] { Conv5}}\t&  \\footnotesize \\raisebox{-2ex}[0pt]{\\shortstack [c] {  MP / SP }} & \\footnotesize\t\\raisebox{-3.2ex}[0pt]{ \\shortstack [c] { $\\!\\!\\!\\! $ Regression \\\\ $\\!\\!\\!\\! $ Loss}}  & \\footnotesize \\raisebox{-2ex}[0pt]{ 512} &  \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {$-$} } & \\footnotesize\t\\raisebox{-2ex}[0pt]{\\shortstack [c] {$-$}} & \\footnotesize \\raisebox{-3.5ex}[0pt]{ \\shortstack [c] {  $75.1$\\\\ $(-)$}} &  \\footnotesize \\raisebox{-3.5ex}[0pt]{ \\shortstack [c] { $80.7$\\\\ ($-$)}} & \\footnotesize RPN is fine-tuned, based on bounding box coordinates and class scores for specific region query which is region-targeted. \\\\  \\cline{2-13} \n& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {  SIFT-CNN } } & \\footnotesize\t\\raisebox{-2ex}[0pt]{VGG16} &  \\footnotesize\t\\raisebox{-3ex}[0pt]{ \\shortstack [c] { Holidays \\\\  UKB }} & \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [l] {  Conv5}}\t&  \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {  SP }} & \\footnotesize\t\\raisebox{-3ex}[0pt]{ \\shortstack [c] { Siamese \\\\  Loss}}  & \\footnotesize \\raisebox{-2ex}[0pt]{ 512} &  \\footnotesize \\raisebox{-2ex}[0pt]{ 88.4 } &  \\footnotesize \\raisebox{-3.5ex}[0pt]{\\shortstack [c] { $ 3.91 $ \\\\ \\textcolor[rgb]{0,0,0}{(N-S)}}} &  \\footnotesize\t \\raisebox{-2ex}[0pt]{ \\shortstack [c] {$-$}} & \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {$-$}} & \\footnotesize SIFT features are used as supervisory information for mining positive and negative samples. \\\\   \\cline{2-13} \n& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] { $\\!\\!\\!\\!\\! $  Quartet-Net } } & \\footnotesize\t\\raisebox{-2ex}[0pt]{ VGG16} &  \\footnotesize\t\\raisebox{-3.5ex}[0pt]{ \\shortstack [c] { GeoPair \\\\   }} & \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [l] {  FC6}}\t&  \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {  PCA }} & \\footnotesize\t\\raisebox{-3ex}[0pt]{ \\shortstack [c] { Siamese \\\\  Loss}}  & \\footnotesize \\raisebox{-2ex}[0pt]{ 128} &  \\footnotesize \\raisebox{-2ex}[0pt]{71.2 } &  \\footnotesize \\raisebox{-3.5ex}[0pt]{\\shortstack [c] { $ 87.5 $ \\\\ \\textcolor[rgb]{0,0,0}{(mAP)}}} & \\footnotesize \\raisebox{-3.5ex}[0pt]{ \\shortstack [c] {  $48.5$\\\\ ($-$)}} &  \\footnotesize \\raisebox{-3.5ex}[0pt]{ \\shortstack [c] {$48.8$\\\\($-$)}} & \\footnotesize Quartet-net learning is explored to improve feature discrimination where double-margin contrastive loss is used. \\\\   \\cline{2-13} \n& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {  NetVLAD } } & \\footnotesize\t\\raisebox{-2ex}[0pt]{ VGG16} & \\footnotesize\t\\raisebox{-3ex}[0pt]{ \\shortstack [c] { $\\!\\!\\!\\!\\! $ Tokyo Time \\\\  Machine }}  & \\footnotesize\t\\raisebox{-3ex}[0pt]{ \\shortstack [c] {  VLAD \\\\  Layer }}\t&  \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] { PCA$_{w}$ }} & \\footnotesize\t\\raisebox{-3ex}[0pt]{ \\shortstack [c] { Triplet \\\\  Loss}}  & \\footnotesize \\raisebox{-2ex}[0pt]{ 256} &  \\footnotesize \\raisebox{-2ex}[0pt]{  79.9 } &  \\footnotesize\t\\raisebox{-2ex}[0pt]{\\shortstack [c] {$-$}} & \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] {  $62.5$\\\\ ($-$)}} &  \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] {  $72.0$\\\\ ($-$)}} & \\footnotesize VLAD is integrated at the last convolutional layer of VGG16 network as a plugged layer. \\\\   \\cline{2-13} \\textbf{}\n& \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] {  Deep \\\\ Retrieval } } & \\footnotesize\t\\raisebox{-2ex}[0pt]{ ResNet-101} &  \\footnotesize\t\\raisebox{-2ex}[0pt]{ $\\!\\!\\!\\!\\! $ Landmarks } & \\footnotesize\t\\raisebox{-3ex}[0pt]{ \\shortstack [c] {  Conv5 \\\\  Block }}\t&  \\footnotesize\t\\raisebox{-3ex}[0pt]{ \\shortstack [c] {  MP + \\\\  PCA$_w$ }} & \\footnotesize\t\\raisebox{-3ex}[0pt]{ \\shortstack [c] { Triplet \\\\Loss}}  & \\footnotesize \\raisebox{-2ex}[0pt]{ 2048} &  \\footnotesize \\raisebox{-2ex}[0pt]{  90.3 } &  \\footnotesize\t\\raisebox{-2ex}[0pt]{\\shortstack [c] {$-$}} & \\footnotesize \\raisebox{-3.5ex}[0pt]{ \\shortstack [c] {  $86.1$\\\\ (82.8)}} &  \\footnotesize \\raisebox{-3.5ex}[0pt]{ \\shortstack [c] {  $94.5$\\\\ (90.6)}} & \\footnotesize Dataset is cleaned automatically. Features are encoded by R-MAC. RPN is used to extract the most relevant regions. \\\\  \\cline{2-13} \n& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {  DELF } } & \\footnotesize\t\\raisebox{-2ex}[0pt]{ ResNet-101}  & \\footnotesize\t\\raisebox{-2ex}[0pt]{ GLDv1 } & \\footnotesize\t\\raisebox{-3ex}[0pt]{ \\shortstack [c] {  Conv4 \\\\ Block }}\t& \\footnotesize\t\\raisebox{-2.6ex}[0pt]{ \\shortstack [c] { $\\!\\!\\!\\! $ Attention \\\\ + PCA$_w$ }} & \\footnotesize\t\\raisebox{-2ex}[0pt]{ \\shortstack [c] { CE Loss}}  & \\footnotesize \\raisebox{-2ex}[0pt]{ 2048} & \\footnotesize \\raisebox{-2ex}[0pt]{$-$} &  \\footnotesize \\raisebox{-2ex}[0pt]{$-$} & \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] { $83.8$\\\\ (82.6)}} & \\footnotesize \\raisebox{-3ex}[0pt]{ \\shortstack [c] { $85.0$\\\\ (81.7)}} & \\footnotesize Exploring the FCN to extract region-level features and construct feature pyramids of different sizes. \\\\   \\cline{2-13}  \n \\Xhline{1.0pt}\n\\multirow{7}{*}{ \\raisebox{-13ex}[0pt]{\\begin{tabular}[c]{@{}c@{}}  \\rotatebox{90}{Unsupervised Fine-tuning} \\end{tabular}}}\n& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {  MoM } } &  \\footnotesize\t\\raisebox{-2ex}[0pt]{ \\shortstack [c] {  VGG16  }} & \\footnotesize\t\\raisebox{-2ex}[0pt]{ Flickr 7M } & \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [l] {  Conv5}}\t& \\footnotesize \\raisebox{-3ex}[0pt]{\\shortstack [c] { MP + \\\\  PCA$_{w} $ }} & \\footnotesize\t\\raisebox{-3ex}[0pt]{ \\shortstack [c] { Siamese \\\\  Loss}} & \\footnotesize \\raisebox{-2ex}[0pt]{ 64} & \\footnotesize \\raisebox{-2ex}[0pt]{ 87.5} & \\footnotesize\t\\raisebox{-2ex}[0pt]{\\shortstack [c] {$-$}}  & \\footnotesize \\raisebox{-3.5ex}[0pt]{ \\shortstack [c] {  $78.2$\\\\ (72.6)}} & \\footnotesize \\raisebox{-3.5ex}[0pt]{ \\shortstack [c] {  $85.1$\\\\ (78.0)}} & \\footnotesize Exploring manifold learning for mining dis/similar samples. Features are tested globally and regionally.\\\\  \\cline{2-13} \n& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] { GeM } } &  \\footnotesize\t\\raisebox{-2ex}[0pt]{ \\shortstack [c] {  VGG16  }}  & \\footnotesize\t\\raisebox{-2ex}[0pt]{ Flickr 7M } & \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [l] {  Conv5}}\t& \\footnotesize \\raisebox{-3ex}[0pt]{\\shortstack [c] { GeM  \\\\  Pooling }} & \\footnotesize\t\\raisebox{-3ex}[0pt]{ \\shortstack [c] { Siamese \\\\  Loss}} & \\footnotesize \\raisebox{-2ex}[0pt]{ 512} & \\footnotesize \\raisebox{-2ex}[0pt]{ 83.1} & \\footnotesize\t\\raisebox{-2ex}[0pt]{\\shortstack [c] {$-$}}  & \\footnotesize \\raisebox{-3.5ex}[0pt]{ \\shortstack [c] { $82.0$\\\\ (76.9)}} & \\footnotesize \\raisebox{-3.5ex}[0pt]{ \\shortstack [c] { $79.7$\\\\ (72.6)}} & \\footnotesize Fine-tuning CNNs on an unordered dataset. Samples are selected from an automated 3D reconstruction system.  \\\\ \\cline{2-13} \n& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {  SfM-CNN  } } &  \\footnotesize\t\\raisebox{-2ex}[0pt]{ \\shortstack [c] {  VGG16  }} &  \\footnotesize\t\\raisebox{-2ex}[0pt]{ Flickr 7M } & \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [l] {Conv5}}\t& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] { PCA$_{w}$ }}  & \\footnotesize\t\\raisebox{-3ex}[0pt]{ \\shortstack [c] { Siamese \\\\  Loss}} & \\footnotesize \\raisebox{-2ex}[0pt]{512} & \\footnotesize \\raisebox{-2ex}[0pt]{  82.5} & \\footnotesize\t\\raisebox{-2ex}[0pt]{\\shortstack [c] {$-$}}  & \\footnotesize \\raisebox{-3.5ex}[0pt]{ \\shortstack [c] {  $77.0$\\\\ (69.2)}} & \\footnotesize \\raisebox{-3.5ex}[0pt]{ \\shortstack [c] {  $83.8$\\\\ (76.4)}} & \\footnotesize  Employing Structure-from-Motion to select positive and negative samples from unordered images.  \\\\   \\cline{2-13} \n& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {  MDP-CNN  } } &  \\footnotesize\t\\raisebox{-2ex}[0pt]{ ResNet-101} &  \\footnotesize\t\\raisebox{-2ex}[0pt]{ $\\!\\!\\!\\!\\! $ Landmarks } & \\footnotesize\t\\raisebox{-3ex}[0pt]{ \\shortstack [c] {  Conv5 \\\\ Block }}\t& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {  SP }}  & \\footnotesize\t\\raisebox{-3ex}[0pt]{ \\shortstack [c] { Triplet \\\\  Loss}}  & \\footnotesize \\raisebox{-2ex}[0pt]{ 2048} &  \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {$-$} }   & \\footnotesize\t\\raisebox{-2ex}[0pt]{\\shortstack [c] {$-$}}  & \\footnotesize \\raisebox{-3.5ex}[0pt]{ \\shortstack [c] {  $85.4$\\\\ (85.1)}} & \\footnotesize \\raisebox{-3.5ex}[0pt]{ \\shortstack [c] {  $96.3$\\\\ (94.7)}} & \\footnotesize Exploring global feature structure by modeling the manifold learning to select positive and negative pairs.   \\\\   \n\\cline{2-13}  \n& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {  IME-CNN } } &  \\footnotesize\t\\raisebox{-2ex}[0pt]{ ResNet-101} &  \\footnotesize\t\\raisebox{-3ex}[0pt]{ \\shortstack [c] { $\\!\\!\\!\\! $ Oxford105k \\\\  Paris106k }}  & \\footnotesize\t\\raisebox{-3ex}[0pt]{ \\shortstack [c] {  IME \\\\  Layer }}\t& \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {  MP }}  & \\footnotesize\t\\raisebox{-3ex}[0pt]{ \\shortstack [c] { $\\!\\!\\!\\! $ Regression \\\\ $\\!\\!\\!\\!\\! $ Loss}} & \\footnotesize \\raisebox{-2ex}[0pt]{ 2048} &  \\footnotesize \\raisebox{-2ex}[0pt]{ \\shortstack [c] {$-$} }   & \\footnotesize\t\\raisebox{-2ex}[0pt]{\\shortstack [c] {$-$}}  & \\footnotesize \\raisebox{-3.5ex}[0pt]{ \\shortstack [c] {  $92.0$\\\\ (87.2)}} & \\footnotesize \\raisebox{-3.5ex}[0pt]{ \\shortstack [c] {  $96.6$\\\\ (93.3)}} & \\footnotesize Graph-based manifold learning is explored to mine the matching and non-matching pairs in unordered datasets.  \\\\   \\cline{2-13} \n\\Xhline{1.0pt}\n\\end{tabular}\n}\n\\end{table*}\n\\textbf{Evaluation for single feedforward pass.} \\textcolor{black}{In general, we observe that fully-connected layers used as feature extractors may give a lower accuracy (\\eg 74.7\\% on Holidays in ), compared to using convolutional layers in Table \\ref{Table_retrieval_off_the_shelf}. For the case where the same VGG net is used, the way to embed or aggregate features is critical. The methods shown in Figure \\ref{SinglePass} improve the discrimination of convolutional feature maps and perform differently in Table \\ref{Table_retrieval_off_the_shelf}, 66.9\\% of R-MAC  and 58.9\\% of SPoC  on Oxford-5k, differences which we see as critical factors for further analysis. If embedded by a BoW model, the results are competitive on Oxford-5k and Paris-6k (73.9\\% and 82.0\\%, respectively), while its codebook size is 25k, which may affect retrieval efficiency. Moreover, layer-level feature fusion improves retrieval accuracy. Yu \\etal  combine three layers (mAP of 91.4\\% on Holidays), outperforming the performance of non-fusion method  (mAP of 80.2\\%).}\n\\textbf{Evaluation for multiple feedforward pass.}\nResults for the methods of Figure \\ref{MultiplePass} are reported in Table \\ref{Table_retrieval_off_the_shelf}. Among them, extracting image patches densely using VGG  has the highest performance on the 4 datasets , and rigid grid with BoW encoding  is competitive (mAP of 87.2\\% on Paris-6k). These two methods consider more patches, even background information, when used for feature extraction. Instead of generating patches densely, region proposals and spatial pyramid modeling introduce a degree of purpose and efficiency in processing image objects. Spatial information is better maintained using multiple-pass schemes than with single-pass. For example, a shallower network (AlexNet) and region proposal networks in  have a UKBench N-Score of 3.81, higher than using deeper networks ,,. Besides feeding image patches into the same network, model-level fusion also exploits complementary spatial information to improve accuracy. For instance, as reported in , which combines AlexNet and VGG, the results on Holidays (81.7\\% of mAP) and UKBench (3.32 of N-Score) are better than these in  (76.75\\% and 3.00, respectively). \n\\textbf{Evaluation for supervised fine-tuning.} Compared to off-the-shelf models, fine-tuning deep networks usually improves accuracy, see Table \\ref{Metric_learning_table_B}. For instance, the result on Oxford-5k  by using a pre-trained VGG is improved from 66.9\\% to 81.5\\% in  when a single-margin Siamese loss is used. Similar trends can be also observed on the Paris-6k dataset. For classification-based fine-tuning, its performance may be improved by using powerful DCNNs and feature enhancement methods such as the attention mechanism in , with an mAP increased from 55.7\\% in  to 83.8\\% in  on Oxford-5k. As for pairwise ranking loss fine-tuning, in some cases the loss used for fine-tuning is essential for performance improvement. For example, RPN is re-trained using regression loss on Oxford-5k and Paris-6k (75.1\\% and 80.7\\%, respectively) . Its results are lower than the results from  (88.2\\% and 88.2\\%, respectively) where a transformation matrix is used to learn visual similarity. However, when RPN is trained by using triplet loss such as , the effectiveness of retrieval is improved significantly where the results are 86.1\\% (on Oxford-5k) and 94.5\\% (on Paris-6k). Feature embedding methods are important for retrieval accuracy; Ong \\etal  embedded \\textit{Conv5} feature maps by Fisher Vector and achieved an mAP of 81.5\\% on Oxford-5k, while embedding feature maps by using VLAD achieves an mAP of 62.5\\% on this dataset ,.\n\\textbf{Evaluation for unsupervised fine-tuning.} \nCompared to supervised fine-tuning, unsupervised fine-tuning methods are relatively less explored. The difficulty for unsupervised fine-tuning is to mine sample relevance without ground-truth labels. In general, unsupervised fine-tuning methods should be expected to have lower performance than supervised. For instance, supervised fine-tuning using Siamese loss  achieves an mAP 88.4\\% on Holidays, while unsupervised fine-tuning using the same loss function in ,, achieves 82.5\\%, 83.1\\%, and 87.5\\%, respectively. However, unsupervised fine-tuning methods can achieve a similar accuracy, even outperform the supervised fine-tuning, if a suitable feature embedding method is used. For instance, Zhao \\etal  explore global feature structure modeling the manifold learning, producing an mAP of 85.4\\% (on Oxford-5k) and 96.3\\% (on Paris-6k), which is similar to supervised results  of 86.1\\% (on Oxford-5k) and 94.5\\% (on Paris-6k). As another example, the precision of ResNet-101 fine-tuned by cross entropy loss achieves 83.8\\% on Oxford-5k , while the precision is further improved to 92.0\\% when an IME layer is used to embed features and fine-tuned in an unsupervised way . Note that fine-tuning strategies are related to the type of the target retrieval datasets. As demonstrated in Table \\ref{GLD_evaluation_on_training_dataset} and , fine-tuning on different datasets may produce a different final retrieval performance. \n\\textbf{Network depth.} We compare the efficacy of DCNNs by depth, following the fine-tuning protocols\\footnote{https://github.com/filipradenovic/cnnimageretrieval-pytorch} in . For fair comparisons, all convolutional features from these backbone DCNNs are aggregated by MAC , and fine-tuned by using the same loss function with the same learning rate, thus the adopted methods are the same except for the DCNN depth. We use the default feature dimension (\\emph{i.e.} AlexNet (256), VGG (512), GoogLeNet (1024), ResNet-50/101 (2048)). The results are reported in Figure \\ref{CNNandaggregation_depth_dim_dataset} (a). We observe that the deeper networks consistently lead to better accuracy due to extracting more discriminative features.\n\\textbf{Feature aggregation methods.} The methods of embedding convolutional feature maps were illustrated in Figure \\ref{SinglePass}. We use the off-the-shelf VGG (without updating parameters) on the Oxford and Paris datasets. The results are reported in Figure \\ref{CNNandaggregation_depth_dim_dataset} (b). We observe that the different ways to aggregate the same off-the-shelf DCNN leads to differences in retrieval performance. These reported results provide a reference for feature aggregation when one uses convolutional layers for performing retrieval tasks.\n\\textbf{Global feature dimension.} We add fully-connected layers on the top of pooled convolutional features of ResNet-50 to obtain global descriptors with their dimensions varying from 32 to 8192. The results of 5 datasets are shown in Figure \\ref{CNNandaggregation_depth_dim_dataset} (c). It is expected that higher-dimension features usually capture more semantics and are helpful for retrieval. The performance\ntends to be stable when the dimension is very large.\n\\textcolor{black}{\\textbf{Number of image regions.} We compare the retrieval performance when different number of regions are fed and other components are kept the same, as depicted in Figure \\ref{CNNandaggregation_depth_dim_dataset} (d). Convolutional features of each region are pooled as 2048-dim regional features by MAC and then aggregated into a global one. Note that the final memory requirement is identical for the case that a holistic image is used as input (\\ie regarded as the case where only one region is used). Regional inputs on an image are extracted with a 40\\% overlap of neighboring regions and the number varying from 1 to 41. For Oxford-5k, the best result is given by the case where 9 image regions are used. For the rest datasets, 3 image regions give the best results. Finally, more regions extracted from one image decline the retrieval mAP. A reason is that features of background or irrelevant regions have also been aggregated, and negatively affect the performance.}\n\\begin{table}[t]\n\\caption{Evaluations of training sets and retrieval reranking. Numerical results are cited from .\n}\n\\vspace{-1em}\n\\label{GLD_evaluation_on_training_dataset}\n\\footnotesize\n\\setlength{\\tabcolsep}{0.6mm}\n\\begin{tabular}{|cc|c|c|cc|c|c|c|}\n\\hline\n\\multicolumn{2}{|c|}{\\multirow{3}{*}{Conditions}} \n & \\multirow{3}{*}{Global} & \\multirow{3}{*}{\\begin{tabular}[c]{@{}c@{}}Local\\\\ reranking\\end{tabular}} & \\multicolumn{2}{c|}{Training set}   & \\multirow{3}{*}{$\\mathcal{R}$Oxf} & \\multirow{3}{*}{$\\mathcal{R}$Par} & \\multirow{3}{*}{\\begin{tabular}[c]{@{}c@{}}GLD-v2\\\\ testing\\end{tabular}} \\\\ \\cline{5-6}\n &  &  &   & \\multicolumn{1}{c|}{GLD-v1} & \\begin{tabular}[c]{@{}c@{}}GLD-v2\\\\ -clean\\end{tabular} &  &  &    \\\\ \\hline\n\\multicolumn{1}{|c|}{\\multirow{4}{*}{\\begin{tabular}[c]{@{}c@{}}ResNet\\\\ -50\\end{tabular}}} &  Case 1 &  \\tickYes  &  \\tickNo  & \\multicolumn{1}{c|}{\\tickYes}   & \\tickNo  &  45.1   & 63.4 &  20.4 \\\\ \\cline{2-9} \n\\multicolumn{1}{|c|}{} & Case 2  & \\tickYes  & \\tickNo  & \\multicolumn{1}{c|}{\\tickNo} & \\tickYes  &  51.0  & 71.5 &  24.1  \\\\ \\cline{2-9} \n\\multicolumn{1}{|c|}{} & Case 3 &  \\tickYes   & \\tickYes  & \\multicolumn{1}{c|}{\\tickYes}   & \\tickNo  &  54.2  &  64.9  &  22.3 \\\\ \\cline{2-9} \n\\multicolumn{1}{|c|}{}& Case 4 &  \\tickYes  &  \\tickYes & \\multicolumn{1}{c|}{\\tickNo}    &   \\tickYes  &  57.9   &  71.0  & 24.3 \\\\ \\hline\n\\multicolumn{1}{|c|}{\\multirow{4}{*}{\\begin{tabular}[c]{@{}c@{}}ResNet\\\\ -101 \\end{tabular}}} & Case 5 &  \\tickYes  &  \\tickNo  & \\multicolumn{1}{c|}{\\tickYes}   & \\tickNo  &  51.2  &  64.7 &  21.7  \\\\ \\cline{2-9} \n\\multicolumn{1}{|c|}{} & Case 6 & \\tickYes  & \\tickNo  & \\multicolumn{1}{c|}{\\tickNo} & \\tickYes  &  55.6  & 72.4 &  26.0  \\\\ \\cline{2-9} \n\\multicolumn{1}{|c|}{} & Case 7  &  \\tickYes   & \\tickYes  & \\multicolumn{1}{c|}{\\tickYes}   & \\tickNo  &  59.3  &  65.5 &  24.3 \\\\ \\cline{2-9} \n\\multicolumn{1}{|c|}{} & Case 8 &  \\tickYes  &  \\tickYes & \\multicolumn{1}{c|}{\\tickNo}    &   \\tickYes  &   64.0  &   72.8 &  26.8 \\\\ \\hline\n\\end{tabular}\n\\end{table}\n\\textbf{Fine-tuning datasets and retrieval reranking.} We compare performance on $\\mathcal{R}$Oxford-5k, $\\mathcal{R}$Paris-6k, and GLD-v2, aiming at comparing the role of different fine-tuning training sets and the effectiveness of retrieval reranking. Table \\ref{GLD_evaluation_on_training_dataset} lists 8 experimental scenarios using two network backbones, as in . \n\\textcolor{black}{Since GLD-v2 provides class-level ground-truth, its including images show large context diversity and may pose challenges to the network fine-tuning. Thus, the pre-processing steps, as proposed in ,, are necessary to select the more coherent images, referring to the GLD-v2-clean subset. As a result, when using the global features only, this cleaned version of the training set improves the performance, as observed in Cases 1/5 and Cases 2/6 for ResNet-50/ResNet-101, respectively.} As an important postprocessing strategy, reranking further boosts the accuracy after the initial filtering step by using global features.\n \\begin{figure*}[!t]\n\\centering  \n {\n  \\includegraphics[width=\\textwidth]{./Figures/CNNandaggregation_depth_dim_dataset.pdf}  \n }     \n  \\vspace{-2em}\n  \\caption{(a) The effectiveness of different DCNNs; (b) Comparison of the feature aggregation methods in Figure \\ref{SinglePass}; (c) The impact of global feature dimension by using ResNet-50; (d) Performance comparison when aggregating different numbers of image regions.} \n  \\label{CNNandaggregation_depth_dim_dataset}    \n \\end{figure*}", "cites": [2136, 2140, 7543, 2093, 514, 2096, 2105, 2113, 8525, 8526, 7546, 2094, 2137, 2131, 2103, 2139, 2138, 7544, 2119, 2109, 2097, 2132, 2100, 2117, 2112, 7545, 2111], "cite_extract_rate": 0.5094339622641509, "origin_cites_number": 53, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes a wide range of papers by organizing performance data across multiple datasets and years, and connects key ideas like region-level features, feature aggregation, and fine-tuning strategies. It offers some critical analysis by pointing out performance drops or plateaus and limitations, such as the mAP being lower in certain variants. The section also identifies broader trends in the evolution of deep learning for retrieval, moving from single-pass models to more refined pipelines, though deeper abstraction to meta-level principles could be enhanced."}}
{"id": "6d16c13c-e552-486e-8c1d-7e8590b69688", "title": "Conclusions and Outlooks", "level": "section", "subsections": [], "parent_id": "515509a6-319a-4385-b1b5-4418a83d2bc2", "prefix_titles": [["title", "Deep Learning for Instance Retrieval: A Survey"], ["section", "Conclusions and Outlooks"]], "content": "\\label{Conclusion_and_Future_Directions}\n\\textcolor{black}{As a comprehensive yet timely survey on instance retrieval using deep learning, this paper has discussed the main challenges, presented a  taxonomy of recent developments according to their roles in implementing instance retrieval,  highlighted the recent representative methods and analyzed their merits and demerits,  discussed the datasets, evaluation protocols, and SOTA performance. Nowadays the exponentially increasing amount of image and video data due to surveillance, e-commerce, medical images, handheld devices, robotics, \\etc, offers an endless potential for applications of instance retrieval. \nAlthough significant progress has been made, as discussed in Section \\ref{Keychallenges}, the main challenges in  instance retrieval have not been fully addressed. Below we identify a number of promising directions for\nfuture research. }\n\\textcolor{black}{\\textbf{(1) Accurate and robust feature representations.} \nOne of the main challenges in instance retrieval is the large intra-class variations due to changes in viewpoint, scale,  illumination, weather condition and background clutter \\emph{etc.}, as we discussed in Section~\\ref{Keychallenges}. However, DCNN representations have very little invariance, even though trained with lots of augmented data . Fortunately, before deep learning, with instance retrieval there are lots of important ideas in handling such intra-class variations like local interest point detectors and local invariant descriptors. Therefore, it is worth enabling DCNN to learn more accurate and robust representations via leveraging such traditional wisdom to design better DCNNs. In addition, unlike most objects in existing benchmarks which are rigid, planar and textured, textureless objects, 3D objects, transparent objects, reflective surfaces, \\etc  are still very hard to find.}\n\\textcolor{black}{In addition, pursuing accuracy alone is not sufficient, as instance retrieval systems should be able to resist potential adversarial attacks. \nRecently, deep networks have been proven to be fooled rather easily by adversarial examples , \n\\ie images added with intentionally designed yet nearly imperceptible perturbations, which raises serious safety and robustness concerns. However, adversarial robustness in instance retrieval , has received very little attention, and should merit further effort.}\n\\textcolor{black}{\\textbf{(2) Compact and efficient deep representations.} In instance retrieval, \nsearching efficiently is as critical as searching accurately, especially for the pervasive \nmobile or wearable devices with very limited computing resources. However, existing methods adopt large scale, energy hungry DCNNs that are very difficult to be deployed in mobile devices. Hence, there has been pressing needs to develop compact, efficient, yet reusable deep representations tailored to the resource limited devices, like using binary neural networks ,,.}\n\\textcolor{black}{\\textbf{(3) Learning with fewer labels.} Deep learning require a large amount of high-quality labeled dataset to achieve high accuracy. The presence of labels errors or the limited amount of labeled data can greatly degrade DCNN's accuracy. However, collecting massive amounts of accurately labeled data is costly. In practical scenarios, datasets like GLDv2 , have long-tailed distributions, and noisy labels. Thus, to address such limitations, few shot learning , self-supervised learning , \nimbalanced learning , \n noisy label aware learning  \\etc should be paid more attention in instance retrieval in the future. }\n\\textcolor{black}{\\textbf{(4) Continual learning for instance retrieval.}\nIn specific, the current IIR methods make restrictive assumptions, such as the training data being enough and stationary, retraining from scratch being possible when new data becomes available, which is problematic in realistic conditions. Our living world is continuously varying, and in\ngeneral data distributions are often non-stationary, new data may be added, and\npreviously unseen classes may be encountered. Thus, continual learning plays a vital role in continuously updating the IIR systems. The key issues are how to retain and utilize the previously learned knowledge,  how to update the retrieval system as new images becomes available, and how to learn and improve over time.\n}\n\\textbf{(5) Privacy-aware instance retrieval.} Most IIR systems concentrate on improving the accuracy or efficiency performance, and the higher performance might come at the cost of users' privacy. Therefore, in some cases, such as personalized search systems, the privacy protection problem is also an important issue to be considered. Deep models should be privacy-aware and protect users' personalized searching experience to avoid their worries about using such IIR systems.\n\\textbf{(6) Video instance retrieval.} Searching a specific instance in an image cannot always meet the requirements in some scenarios such as the video surveillance system in the field of searching criminals. Currently, with the rapid growth of video data, retrieving a certain object, place, or action in videos has become more and more important and highly necessary in the future. For video instance retrieval, 3D-CNNs models need to be built to learn videos spatio-temporal representations to compute the semantic similarity of instances.\n\\footnotesize\n\\bibliographystyle{IEEEtran}\n\\bibliography{IEEEabrv,myreference}\n\\vspace{-2.0 cm}\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./Figures/WeiChen.pdf}}]{Wei Chen} received the Ph.D. degree at Leiden University, in 2021. Before starting with PhD study in Leiden University, he received his Master degree from the National University of Defense Technology, China, in 2016. His research interest focuses on cross-modal retrieval with deep learning methods, and also in the context of incremental learning. He has published papers in international conferences and journal including CVPR, ACM MM, PR, Neurocomputing, and IEEE TMM \\etc\n\\end{IEEEbiography}\n\\vspace{-1.7 cm} \n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./Figures/Yu_Liu.pdf}}]{Yu Liu} is currently an Associate Professor in International School\nof Information Science and Engineering at Dalian University of Technology, China.\nHe was a post-doctoral researcher in the PSI group of KU Leuven, Belgium. \nIn 2018, he received the PhD degree from Leiden University, Netherlands.\nHis research interests include object recognition and retrieval in the context of continual learning and zero-shot learning. \nHe has co-organized several workshops at ICCV, CVPR and ECCV, respectively. \nHe has published papers in CVPR, ICCV, ECCV, ACM MM, IEEE TIP, etc, and \nreceived the best paper award at MMM2017.\n\\end{IEEEbiography}\n\\vspace{-1.7 cm} \n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./Figures/Weiping_Wang.pdf}}]{Weiping Wang} received the Ph.D. degree in systems engineering from the National University of Defense Technology, Changsha, China. He is a Professor at Academy of Advanced Technology Research of Hunan. He has more than 200 papers published on journals and conferences including IEEE Transactions multimedia, Transactions on Vehicular Technology, \\etc His current research interests focus on intelligent decision experimentation, multi-modal knowledge extraction and knowledge integrated computation.\n\\end{IEEEbiography}\n\\vspace{-2.0 cm} \n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./Figures/Erwin.pdf}}]{Erwin M. Bakker} is co-director of the LIACS Media Lab at Leiden University. He has published widely in the fields of image retrieval, audio analysis and retrieval and bioinformatics. He was closely involved with the start of the International Conference on Image and Video Retrieval (CIVR). Moreover, he regularly serves as a program committee member or organizing committee member for scientific multimedia and human-computer interaction conferences and workshops.\n\\end{IEEEbiography}\n\\vspace{-2.0 cm} \n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./Figures/Theodoros_Georgiou.pdf}}]{Theodoros Georgiou} received the Ph.D. degree at Leiden University, in 2021. His research interest focuses on deep learning methods applied on higher than two dimensional data. Before starting with PhD study in Leiden University, he received his Master degree from the Leiden University in 2016. He has published  papers in international conferences  and journal including ICPR, CBMI, WCCI and IJMIR.\n\\end{IEEEbiography}\n\\vspace{-2.0 cm} \n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./Figures/Paul_Fieguth.pdf}}]{Paul Fieguth} is co-director of the Vision \\& Image Processing Group in Systems Design Engineering at the University of Waterloo, where he is Professor and Associate Dean.  He received the Ph.D.\\ degree from the Massachusetts Institute of Technology, Cambridge, in 1995, and has held visiting appointments at the University of Heidelberg in Germany, at INRIA/Sophia in France, at the Cambridge Research Laboratory in Boston, at Oxford University and the Rutherford Appleton Laboratory in England. His research interests include statistical signal and image processing, hierarchical algorithms, data fusion, machine learning, and the interdisciplinary applications of such methods.  He has published textbooks on Statistical Image Processing and on Complex Systems theory.\n\\end{IEEEbiography}\n\\vspace{-1.7 cm} \n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./Figures/LiSmileGoodFace.pdf}}]{Li Liu} received the Ph.D. degree in information and communication engineering from the National University of Defense Technology, China, in 2012. She is currently a Professor with the College of System Engineering. She has held visiting appointments at the University of Waterloo, Canada, at the Chinese University of Hong Kong, and at the University of Oulu, Finland. Her current research interests include computer vision, pattern recognition and machine learning. Her papers have currently over 7800 citations in Google Scholar.\n\\end{IEEEbiography}\n\\vspace{-1.7 cm}\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{./Figures/Michael_Lew.pdf}}]{Michael S. Lew} is the head of the Deep Learning and Computer Vision Research Group and a full Professor at LIACS, Leiden University. He has published over a dozen books and 190 peer-reviewed scientific articles in the areas of image retrieval, computer vision, and deep learning. Notably, he had the most cited paper in the ACM Transactions on Multimedia and one of the top 10 most cited articles in the history (out of more than 16,000 articles) of the ACM SIGMM.  He was also a founding member of the advisory committee for the TRECVID video retrieval evaluation project, chair of the steering committee for the ACM International Conference on Multimedia Retrieval and a member of the ACM SIGMM Executive Committee. \n\\end{IEEEbiography}\n\\vspace{-1.7 cm} \n \\newpage\n\\end{document}", "cites": [2141, 2136, 2144, 2142, 2110, 2093, 2111, 2143], "cite_extract_rate": 0.7272727272727273, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong analytical insight by synthesizing key challenges and future directions in instance retrieval, drawing from cited papers on adversarial robustness, few-shot learning, and noisy label handling. It abstracts these issues into broader themes such as feature representation, efficiency, and privacy. While it identifies several gaps and limitations, it could provide deeper comparative analysis of the cited works."}}
