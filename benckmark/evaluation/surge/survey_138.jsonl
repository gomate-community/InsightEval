{"id": "a19807e3-7e75-4cea-b6e6-217a7ec484bf", "title": "Introduction", "level": "section", "subsections": ["204686e0-bd1b-4ffb-adf4-412542bb4fcf"], "parent_id": "9ed5454f-c634-4de3-9fa8-bd1914145f63", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Introduction"]], "content": "Machine translation (MT) automatically translates from one language to another without human labor, which brings convenience and significantly reduces the labor cost in international exchange and cooperation. Powered by deep learning, neural machine translation (NMT)~ has become the dominant approach for machine translation. Compared to conventional rule-based approaches and statistical machine translation (SMT), NMT enjoys two main advantages. First, it does not require professional human knowledge and design on translation perspective (e.g., grammatical rules). Second, neural network can better capture the contextual information in the entire sentence, and thus conduct high quality and fluent translations.\nOne limitation of NMT is that it needs large scale of parallel data for model training. While there are thousands of languages in the world\\footnote{https://en.wikipedia.org/wiki/Language}, major popular commercial translators (e.g., Google translator, Microsoft translator, Amazon translator) only support tens or a hundred languages because of the lack of large-scale parallel training data for most languages. To handle those languages with limited parallel data, many algorithms have been designed for low-resource NMT in recent years. Therefore, a review on low-resource NMT is very helpful for fresh researchers entering this area and industry practitioners. Although there already exists surveys on many aspects of NMT (e.g., domain adaptation~, multilingual translation~), a comprehensive survey for low-resource NMT is still missing. Therefore, in this paper, we conduct a comprehensive and well-structured survey on low-resource NMT to fill in this blank. \n\\begin{figure}[t] \n\\centering\n\\includegraphics[width=2.5in]{NMTStructure} \n\\caption{NMT model}\n\\label{fig:NMT} \n\\end{figure}", "cites": [168, 7200, 38], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic overview of NMT and its limitations in low-resource settings, mentioning the need for a survey in this area. While it cites relevant papers, it does so without significant synthesis or integration of their ideas into a cohesive narrative. There is minimal critical analysis or abstraction, focusing instead on general descriptions of NMT and its challenges."}}
{"id": "204686e0-bd1b-4ffb-adf4-412542bb4fcf", "title": "NMT basics.", "level": "paragraph", "subsections": ["cc332a4d-52ca-4ae6-b944-fe699523ce0f"], "parent_id": "a19807e3-7e75-4cea-b6e6-217a7ec484bf", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Introduction"], ["paragraph", "NMT basics."]], "content": "An NMT model $\\theta$ translates a sentence $\\mathbf{x}$ in the source language to a sentence $\\mathbf{y}$ in the target language. With a parallel training corpus $\\mathbf{C}$, the model $\\theta$ is trained by minimizing the negative log-likelihood loss:\n\\begin{equation}\n    L_{\\theta} = \\sum\\nolimits_{(\\mathbf{x},\\mathbf{y}) \\in \\mathbf{C}} -\\mbox{log} p \\left( \\mathbf{y} | \\mathbf{x};\\theta\\right).\n\\end{equation}\nAs shown in Fig. \\ref{fig:NMT}, NMT models are commonly auto-regressive and generate the target sentence from left to right. Considering that $\\mathbf{y}$ contains $m$ words, the conditional probability $p \\left( \\mathbf{y} | \\mathbf{x};\\theta\\right)$ can be written as:\n\\begin{equation}\n    p \\left( \\mathbf{y} | \\mathbf{x};\\theta\\right) = \\Pi_{t=1}^{m} p\\left(y_t|\\mathbf{y}_{<t},\\mathbf{x};\\theta\\right).\n\\end{equation}\nThe encoder-decoder framework is widely used in NMT, where the encoder converts the source sentence into a sequence of hidden representations and the decoder generates target words conditioned on the source hidden representations and previously generated target words. The encoder and decoder can be recurrent neural networks ~, convolutional neural networks~, and Transformer~. In the inference stage, beam search is usually used to generate the target sentence based on the decoder output.\n\\begin{figure*}[t] \n\\centering\n\\includegraphics[width=6in]{survey_fig_overview} \n\\caption{Overview}\n\\label{fig:Overview} \n\\end{figure*}", "cites": [6550, 38], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a basic overview of NMT fundamentals and mentions different model architectures (RNNs, CNNs, Transformers) by citing relevant papers. However, it does so in a primarily descriptive manner without connecting the cited works to broader themes or trends in low-resource NMT. There is minimal critical analysis or abstraction of underlying principles."}}
{"id": "b2275196-b97a-4c62-afc2-b3354db20039", "title": "Exploiting Monolingual Data", "level": "section", "subsections": ["fb465252-834e-4e71-8fe8-15547f20ee88", "44c322e4-3d6d-43cc-a2de-f36ffa2ae24c", "44c2c62c-d37e-42ba-8aa1-ef3e806c9149", "874201f6-9545-4289-8011-43c9c9efb934", "18c3e13a-83fe-41a9-be4e-2284a249bbd7", "771e93f6-3b6a-4b0b-b076-0f6e86bfc29b", "8203c093-250f-4142-a8fa-7c333e4daf28"], "parent_id": "9ed5454f-c634-4de3-9fa8-bd1914145f63", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Exploiting Monolingual Data"]], "content": "\\label{sec:mono}\n\\begin{figure*}[t] \n\\centering\n\\includegraphics[width=7in]{monolingual_overview} \n\\caption{Overview of works exploiting monolingual data}\n\\label{fig:Monolingual} \n\\end{figure*} \nMonolingual data contains a wealth of linguistic information (e.g., grammar and contextual information) and is more abundant and easier to obtain than bilingual parallel data, which is useful to improve the translation quality especially in low-resource scenario. Plenty of works have exploited monolingual data in NMT systems, which we categorize into several aspects: (1) back translation, which is a simple and promising approach to take advantage of the target-side monolingual data~, (2) forward translation also called knowledge distillation, which utilizes the source-side monolingual data~, (3) joint training on both translation directions, which can take advantage of the monolingual data on both the source and target sides~, (4) unsupervised NMT, which builds NMT models with only monolingual data, and can be applied to the language pairs without any parallel data~, (5) pre-training, which leverages monolingual data with self-supervised training for language understanding and generation, and thus improves the quality of NMT models~, (6) comparable monolingual corpus, which contains implicit parallel information and can improve the translation quality~, and (7) enhancing with bilingual dictionary, where the bilingual dictionary is used together with monolingual data to enhance the translation on low-resource languages. In this section, we provide an overview of these methods on exploiting monolingual data in NMT.", "cites": [5005, 5000, 8868, 4988, 7096, 6551, 4983, 1551], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear categorization of methods that use monolingual data but mainly describes each approach in a general way without deeply synthesizing or contrasting the cited papers. There is minimal critical evaluation of the methods or their limitations, and while some broader patterns (like the use of monolingual data for pre-training) are noted, the section remains largely at the level of individual techniques without meta-level insights."}}
{"id": "fb465252-834e-4e71-8fe8-15547f20ee88", "title": "Back \\& Forward Translation", "level": "subsection", "subsections": [], "parent_id": "b2275196-b97a-4c62-afc2-b3354db20039", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Exploiting Monolingual Data"], ["subsection", "Back \\& Forward Translation"]], "content": "In back translation, pseudo parallel sentence pairs are generated by translating the target-side monolingual sentences to the source language via a translation system in the reverse direction~, while in forward translation, pseudo parallel sentence pairs are generated by translating the source-side monolingual sentences to the target language via a translation system in the same direction~. Then, the pseudo parallel data is mixed with the original parallel data to train an NMT model. It has been shown that back and forward translation provides promising performance gain on NMT systems~.\nBesides the typically used beam search~, there are also some other methods to generate the pseudo parallel data: (1) random sampling according to the output probability distribution~, (2) adding noise to source sentences generated by beam search~, and (3) prepending a tag to the source sentences generated by beam search~. It is observed that random sampling and adding noise only works well on high resource setting compared to standard beam search~, while prepending a tag performs the best on both high and low resource settings~. In addition, a mixed  pseudo parallel data generated by BT and copying the target monolingual sentences as the source sentences can further improve the translation quality on low-resource languages~.", "cites": [6552, 7164, 4988, 8866], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes information from multiple cited papers, comparing and integrating different approaches for generating pseudo parallel data. It provides a critical analysis by evaluating the effectiveness of various methods (e.g., random sampling, adding noise, tagging) and noting their performance in different resource settings. The abstraction is strong as it generalizes the role of synthetic data and identifies broader patterns in the utility of tagging versus noise-based approaches."}}
{"id": "44c322e4-3d6d-43cc-a2de-f36ffa2ae24c", "title": "Joint Training on Both Translation Directions", "level": "subsection", "subsections": [], "parent_id": "b2275196-b97a-4c62-afc2-b3354db20039", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Exploiting Monolingual Data"], ["subsection", "Joint Training on Both Translation Directions"]], "content": "\\label{sec:both-side}\nConsidering that both the source and target sides monolingual data has valuable information, some works leverage both of them via joint training on the two translation directions. Dual learning~ simultaneously improves the two models on both translation directions by aligning the original monolingual sentences $x$ and the sentences $x’$ translated forward and then backward ($x\\to y’\\to x’$) by the two models. \\citeay{wang2019multi} further improve dual learning by introducing multi-agent for both translation directions. Intuitively, a better reverse translation model leads to better back-translation sentences, and thus leading to a better NMT system. Iterative back translation~ simultaneously trains the NMT models on both translation directions and iteratively updates the back-translated corpus via the updated better NMT models. Bi-directional NMT~ trains both the translation directions in the same model with a tag indicating the direction at the beginning of source sentences, and then leverages both source-side and target-side monolingual data by back and forward translation. Mirror-generative NMT~ jointly trains the translation models on both directions and the language models for both source and target languages with a shared latent variable.", "cites": [6551, 4983], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of methods that use monolingual data from both source and target languages, mentioning several approaches like dual learning, multi-agent dual learning, and bi-directional NMT. However, it lacks synthesis by not deeply connecting the ideas across the cited works, and it does not offer critical evaluation or abstraction into broader principles, remaining mostly at the level of individual method descriptions."}}
{"id": "44c2c62c-d37e-42ba-8aa1-ef3e806c9149", "title": "Unsupervised NMT", "level": "subsection", "subsections": ["bd336433-eccf-4eab-916d-8b0039795692"], "parent_id": "b2275196-b97a-4c62-afc2-b3354db20039", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Exploiting Monolingual Data"], ["subsection", "Unsupervised NMT"]], "content": "To deal with the zero-resource translation scenario without any parallel sentences, a common approach is unsupervised learning for NMT~, which typically relies on two components to ensure the learning efficiency and quality: (1) bilingual alignment, which enables the model with good alignments between the two languages, and (2) translation improvement, which gradually improves the translation quality by iterative learning, typically through back translation~.", "cites": [5005, 5009, 8868, 4988], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of unsupervised NMT and mentions two general components—bilingual alignment and translation improvement—but does not synthesize or connect ideas from the cited papers in a meaningful way. It lacks critical analysis of the approaches or limitations and does not abstract beyond the specific papers to present overarching patterns or principles."}}
{"id": "bd336433-eccf-4eab-916d-8b0039795692", "title": "Bilingual alignment.", "level": "paragraph", "subsections": ["9af5a552-f593-458d-85ef-5987408154f8"], "parent_id": "44c2c62c-d37e-42ba-8aa1-ef3e806c9149", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Exploiting Monolingual Data"], ["subsection", "Unsupervised NMT"], ["paragraph", "Bilingual alignment."]], "content": "How to initially align between the two languages is an open problem. There are mainly four kinds of approaches: (1) bilingual word embedding~, where the NMT system can either start from a word-by-word translation derived from the bilingual word embedding~ or initialize the embedding parameters according to bilingual word embedding~, (2) denoising auto-encoder (DAE)~, which can build a shared latent space of two languages by learning to reconstruct sentences in the two languages from a noised version~, (3) unsupervised statistical machine translation (SMT), where an initial alignment can be obtained through the back-translated corpora generated by an unsupervised SMT system~, and (4) language model pre-training~, which is discussed in detail in Section \\ref{sec:Pre-training}.", "cites": [4996, 5005, 8369, 6553, 1684, 8868, 5004, 7877, 8869, 4997, 7096, 1551], "cite_extract_rate": 0.8, "origin_cites_number": 15, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of four main approaches for bilingual alignment in unsupervised NMT. While it lists the methods and briefly mentions how they work, it does not deeply integrate or connect the cited papers into a cohesive framework. There is minimal critical analysis or evaluation of the strengths and limitations of the approaches, and no broader patterns or principles are abstracted from the works."}}
{"id": "9af5a552-f593-458d-85ef-5987408154f8", "title": "Translation improvement.", "level": "paragraph", "subsections": [], "parent_id": "bd336433-eccf-4eab-916d-8b0039795692", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Exploiting Monolingual Data"], ["subsection", "Unsupervised NMT"], ["paragraph", "Bilingual alignment."], ["paragraph", "Translation improvement."]], "content": "The translation quality need to be further improved based on the initial alignment, where iterative back translation is commonly used~. Some works study on improving the iterative back translation process in unsupervised NMT. Filtering out low-quality pseudo parallel sentence pairs is one strait-forward and effective method . \\citeay{sun2019unsupervised,sun2020unsupervised} propose to add a term in the training objective to avoid forgetting the alignment from bilingual word embedding during the interactively training. Moreover, unsupervised SMT can also be utilized to boost the iterative back translation. One approach is to first construct pseudo parallel data by leveraging both the unsupervised SMT and NMT systems for back translation and then train the NMT models with the pseudo parallel data~. In addition, SMT can also act as a posterior regularization to denoise the pseudo parallel data generated by NMT systems~.", "cites": [8868, 7877, 6554, 7096], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple cited papers to explain how translation quality in unsupervised NMT can be improved, particularly through iterative back translation and the use of SMT. It provides some analytical depth by connecting methods across works, such as using SMT both for pseudo-data construction and as a regularization tool. However, it lacks deeper critical evaluation of limitations or comparative strengths of the approaches and does not offer a high-level abstraction of broader principles in low-resource NMT."}}
{"id": "874201f6-9545-4289-8011-43c9c9efb934", "title": "Language model pre-training", "level": "subsection", "subsections": ["29c7cfbd-b6f2-4437-8d4f-b87aeb594f14"], "parent_id": "b2275196-b97a-4c62-afc2-b3354db20039", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Exploiting Monolingual Data"], ["subsection", "Language model pre-training"]], "content": "\\label{sec:Pre-training}\nLeveraging monolingual data to pre-train language models is effective for many language understanding and generation tasks~. Since NMT requires the capability of both language understanding (e.g., NMT encoder) and generation (e.g., NMT decoder), pre-training language model can be very helpful for NMT, especially low-resource NMT. Previous works on language model pre-training for NMT can be divided into two categories depending on the encoder and decoder in NMT are pre-trained separately or jointly. We then review the works according to the two categories.", "cites": [7], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of language model pre-training for NMT and mentions a categorization of works, but it only cites one paper (BERT) and does not integrate or synthesize insights from multiple sources. It lacks critical evaluation of the cited work and does not abstract or generalize to broader principles or trends in the field."}}
{"id": "29c7cfbd-b6f2-4437-8d4f-b87aeb594f14", "title": "Separate pre-training.", "level": "paragraph", "subsections": ["e0428e16-5072-4fc3-8d2e-06c8883f416b"], "parent_id": "874201f6-9545-4289-8011-43c9c9efb934", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Exploiting Monolingual Data"], ["subsection", "Language model pre-training"], ["paragraph", "Separate pre-training."]], "content": "Some works pre-train the encoder or/and the decoder separately. \\citeay{ramachandran2016unsupervised} first separately initialize the encoder and decoder with language models proposed by \\citeay{jozefowicz2016exploring}, and then fine-tune with supervised parallel data. XLM~ initialize the encoder and decoder with separate language models training by a combination of masked language modeling (MLM)~, where some tokens in the text are masked and the model learns to predict the masked tokens, and translation language modeling (TLM), which extends MLM by concatenating parallel sentence pairs as the input sentences. \\citeay{rothe2020leveraging} investigate to initialize the encoder and decoder with variant models, including BERT~, GPT-2~, RoBERTa~ and random initialization. It is observed the best performance on English-Germany by a model with BERT-initialized encoder and randomly initialized decoder, or a model with shared encoder and decoder initialized with BERT. After initialing with separately pre-trained encoder and decoder, \\citeay{varis2019unsupervised} introduce Elastic Weight Consolidation~ into fine-tuning in order to avoid forgetting the language models. \\citeay{zhu2019incorporating} fuse the representations extracted by BERT to the encoder and decoder via attention mechanisms. A drawback of separately pre-training encoder and decoder is that it cannot well train the encoder-decoder-attention, which is very important in NMT to connect the source and target representations for translation. Therefore, some works propose to jointly pre-train the encoder, decoder and attention for better translation accuracy.", "cites": [826, 1551, 1562, 7], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key works on separate pre-training of encoder and decoder in low-resource NMT, connecting them through the common theme of using pre-trained language models. It includes some critical evaluation by highlighting a drawback—namely, the inability to well train encoder-decoder attention. While it identifies a broader trend (separate vs. joint pre-training), the abstraction remains focused on the specific context of low-resource translation."}}
{"id": "e0428e16-5072-4fc3-8d2e-06c8883f416b", "title": "Joint pre-training.", "level": "paragraph", "subsections": [], "parent_id": "29c7cfbd-b6f2-4437-8d4f-b87aeb594f14", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Exploiting Monolingual Data"], ["subsection", "Language model pre-training"], ["paragraph", "Separate pre-training."], ["paragraph", "Joint pre-training."]], "content": "In order to simultaneously learn to understand the input sentences and improve the language generation capability, as well as jointly pre-train each component in NMT models (encoder, decoder and encoder-decoder-attention), MASS~ proposes masked sequence to sequence learning that randomly masks a fragment (several consecutive tokens) in the input sentence of the encoder, and predicts the masked fragment in the decoder. Later, BART~ proposes to add noises and randomly mask some tokens in the input sentences in the encoder, and learn to reconstruct the original text in the decoder. T5~ randomly masks some tokens and replace the consecutive tokens with a single sentinel token.", "cites": [7096, 9], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section describes the methods of MASS, BART, and T5 in a straightforward manner without connecting their underlying principles or contrasting their approaches. It lacks critical evaluation of their strengths or weaknesses and does not abstract these techniques into a broader conceptual framework. The content is factual but offers minimal synthesis or analytical depth."}}
{"id": "18c3e13a-83fe-41a9-be4e-2284a249bbd7", "title": "Exploiting Comparable Corpus", "level": "subsection", "subsections": [], "parent_id": "b2275196-b97a-4c62-afc2-b3354db20039", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Exploiting Monolingual Data"], ["subsection", "Exploiting Comparable Corpus"]], "content": "Monolingual data of different languages that refer to the same entity (e.g., English and Chinese Wikipedia pages that describe the same object) can be regarded as comparable corpus, which is easier to be obtained compared to parallel data and contains implicit parallel information for NMT systems. The challenge is how to mine the parallel sentences from the comparable corpus and some approaches are proposed to solve this problem. LASER~ is a toolkit based on cross-lingual sentence embeddings, which is a good choice to mine parallel data~. \\citeay{wu2019extract} propose to first extract potential aligned target sentences given a source sentence, and then make the target sentences better aligned with the source sentence by revising them via an editing mechanism. A self-supervised learning method is proposed in~, where finding semantically aligned sentences is considered as an auxiliary task for translation. Besides mining parallel sentence pairs, \\citeay{wu2019machine} take advantage of the aligned topic distribution for weakly paired documents, which is suitable for documents related to the same event or entity but not implicitly aligned in sentences.", "cites": [4982, 6555], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly describes methods for exploiting comparable corpora in low-resource NMT, referencing specific papers but not synthesizing their approaches into a broader framework. There is minimal critical analysis or comparison of the methods, and no clear abstraction of underlying principles or trends. The content remains largely descriptive, summarizing techniques without deeper insight."}}
{"id": "771e93f6-3b6a-4b0b-b076-0f6e86bfc29b", "title": "Enhancing With Bilingual Dictionary", "level": "subsection", "subsections": [], "parent_id": "b2275196-b97a-4c62-afc2-b3354db20039", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Exploiting Monolingual Data"], ["subsection", "Enhancing With Bilingual Dictionary"]], "content": "The bilingual dictionary of a low-resource language pair can be collected either by human annotation or word embedding based alignment~, which is much easier to obtain than the bilingual parallel sentences. Since the bilingual dictionary contains only word-level information, it is usually used with monolingual data to improve the translation. Existing works utilizing the bilingual dictionary can be categorized into three ways. First, bilingual dictionary is used to improve the rare words translation. \\citeay{zhang2016bridging} build pseudo parallel sentences by translating source-side monolingual sentences (that contain rare words) to target language via SMT (that is built based on the bilingual dictionary). \\citeay{fadaee2017data} augment the parallel data by replacing some words in parallel sentences with rare words. Second, bilingual dictionary can also be used to perform word-by-word translation on monolingual data, and accordingly help to improve the low-resource NMT. \\citeay{pourdamghani2019translating} propose a two-step approach, which first translates the source monolingual sentence to translationese sentence word-by-word using bilingual dicrionary, and then trains a translation model on translationese-to-target. \\citeay{zhou2019handling} augment the parallel training data by first re-ordering monolingual sentences in the target language to match the source language and then obtaining pseudo source sentences via word-by-word translation. Third, a recent study~ propose to close the gap of the embedding spaces between the source and target languages by establishing anchoring points based on dictionary, which can help to build NMT models based on only dictionary and monolingual data.", "cites": [6556, 4995, 8559, 5004], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of how bilingual dictionaries are used in low-resource NMT, grouping the methods into three categories. It synthesizes the cited papers to a reasonable extent by organizing them under these categories, but does not offer deep connections or a novel framework. There is minimal critical analysis or identification of broader principles, focusing mainly on summarizing approaches rather than evaluating their strengths and weaknesses."}}
{"id": "11f12d8b-d7f9-4367-9b1c-98d177b64bf1", "title": "Exploiting Data From Auxiliary Languages", "level": "section", "subsections": ["0f121242-0bee-40f0-939a-10c0d3ba5bef", "642e28af-d47e-4763-adaf-0e134f5d14a4", "89cfc63c-4871-481f-9ed1-e482ba53fe96", "8124b28b-adaf-4e13-93e3-f385bcfe3254"], "parent_id": "9ed5454f-c634-4de3-9fa8-bd1914145f63", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Exploiting Data From Auxiliary Languages"]], "content": "\\label{sec:multi}\n\\begin{figure*}[t] \n\\centering\n\\includegraphics[width=7in]{multilingual_overview} \n\\caption{Overview of works exploiting data from auxiliary languages}\n\\label{fig:Multilingual} \n\\end{figure*} \nHuman languages share similarities with each other in several aspects: (1) languages in the same/similar language family or typology may share similar writing script, word vocabulary, word order and grammar, (2) languages can influence each other, and a foreign word from another language can be incorporated into a language as it is (referred as loanword). Accordingly, corpora of related languages can be exploited to assist the translation between a low-resource language pair~. The methods to leverage multilingual data into low-resource NMT can be categorized into several types: (1) multilingual training, where the low-resource language pair is jointly trained with other language pairs in one model~, (2) transfer learning~, where a parent NMT model usually containing rich-resource language pairs is first trained and then fine-tuned on low-resource language pair, and (3) pivot translation, where one or more pivot languages are selected as a bridge between the source and target languages and in this way the source-pivot and pivot-target data can be exploited to help the source-target translation. In the following subsections, we introduce the works in each category, respectively.", "cites": [4967], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of methods for exploiting auxiliary languages in low-resource NMT, including multilingual training, transfer learning, and pivot translation. It integrates basic ideas from cited papers but lacks deeper synthesis, critical evaluation of their effectiveness or limitations, and broader abstraction of underlying principles."}}
{"id": "4f82b268-7604-4391-b1a6-07a0f0b05a25", "title": "Parameter sharing.", "level": "paragraph", "subsections": ["aedb133e-ba8a-4c13-9d78-9978b3a83af0", "ccd682b1-230f-4a25-a692-3dd262352de3"], "parent_id": "0f121242-0bee-40f0-939a-10c0d3ba5bef", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Exploiting Data From Auxiliary Languages"], ["subsection", "Multilingual training"], ["paragraph", "Parameter sharing."]], "content": "There are different ways to share model parameters in multilingual training. First, all the encoder, decoder and attention components are independent among different languages~. Second, fully shared encoder, decoder and attention components are considered across languages, where a language-specific token is added in the source sentence to specify the target language~. Third, in order to simultaneously exploit the characteristic and commonality of different languages, as well as keeping the model compact, some works consider to partially share the model parameters. \\citeay{firat2016multi} adopt a shared attention mechanism and language-specific encoders and decoders. \\citeay{blackwood2018multilingual} propose to use a specific attention mechanism in the decoder for each target language and share all the remaining model parameters, which is shown to improve the word alignments. \\citeay{sachan2018parameter} propose to partially share the attention mechanism. \\citeay{wang2018three} focus on one-to-many scenario and consider to use language-dependent positional embeddings and partially share the hidden layers in the decoder. \\citeay{platanios2018contextual} introduces a contextual parameter generator for each language pair, which generates the parameters of the encoder and decoder based on the source and target language embeddings. \\citeay{wang2019compact} improves the translation quality by using language-sensitive embeddings and attentions, as well as incorporating language-sensitive discriminators in the decoder. \\citeay{zhang2020improving} introduce a linear transformation between the shared encoder and decoder for each target language, which requires only one more weight matrix for an additional target language.", "cites": [8872, 4507, 6555, 8254, 5023], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section describes different approaches to parameter sharing in multilingual NMT but primarily presents them in a list-like manner without connecting the underlying ideas or evaluating their effectiveness. There is minimal critical analysis or abstraction to broader principles, focusing mostly on methodological descriptions from the cited works."}}
{"id": "aedb133e-ba8a-4c13-9d78-9978b3a83af0", "title": "Designs for low-resource languages.", "level": "paragraph", "subsections": [], "parent_id": "4f82b268-7604-4391-b1a6-07a0f0b05a25", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Exploiting Data From Auxiliary Languages"], ["subsection", "Multilingual training"], ["paragraph", "Parameter sharing."], ["paragraph", "Designs for low-resource languages."]], "content": "To better exploit the knowledge from multiple languages to help low-resource languages, a lot of works design to improve the multilingual training from different aspects:   \n\\begin{itemize}\n    \\item \\emph{Auxiliary language selection.} How to effectively select and utilize auxiliary languages is critical to improve the performance of low-resource language pairs in multilingual NMT. Most works consider to select rich-resource languages in the same language family as auxiliary languages, and achieve significant improvement~. \\citeay{tan2019multilingual}  propose to cluster the languages based on language embedding, which shows better performance than clustering by language family. \\citeay{wang2019target} focus on translating low-resource languages to English with the help of a target conditioned sampling algorithm, where a target sentence is sampled and the source sentences from all the corresponding parallel sentences in multiple languages are chosen based on language-level and sentence-level similarity. \n    \\item \\emph{Training sample balance.} Considering the limited model capacity and the various training data sizes among different languages, the model may have a bias to rich-resource languages. Accordingly, balancing the data sizes is important for low-resource languages in multilingual NMT. Temperature based sampling is one promising approach, where the temperature term needs to be manually chosen~. \\citeay{wang2020balancing} propose a method to automatically weight the training data sizes.\n    \\item \\emph{Word reordering in auxiliary language.} Pre-ordering the words in auxiliary language sentences to align with the desired low-resource language also brings benefits to low-resource NMT~.\n    \\item \\emph{Monolingual data from auxiliary languages} can also be utilized to improve the low-resource languages by introducing back translation~, cross-lingual pre-training~ and meta-learning~ in multilingual model. Furthermore, multilingual NMT can also be trained with monolingual data only by extending the unsupervised NMT to multilingual model~, or aligning the translations to the same language via different paths in a multilingual model~.\n\\end{itemize}", "cites": [5034, 5031, 2489, 5032, 4988, 5003, 8874, 5020], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of different research directions but remains largely descriptive, listing approaches without deeper comparative or critical analysis. It identifies some recurring themes like language selection and training balance, yet lacks a novel framework or meta-level abstraction. Limited evaluation of methods or identification of limitations reduces the depth of insight."}}
{"id": "ccd682b1-230f-4a25-a692-3dd262352de3", "title": "Zero-shot translation.", "level": "paragraph", "subsections": [], "parent_id": "4f82b268-7604-4391-b1a6-07a0f0b05a25", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Exploiting Data From Auxiliary Languages"], ["subsection", "Multilingual training"], ["paragraph", "Parameter sharing."], ["paragraph", "Zero-shot translation."]], "content": "Multilingual training brings the possibility of zero-shot translation~. For example, a multilingual NMT model trained on $X \\leftrightarrow$ English and $Y \\leftrightarrow$ English parallel data is possible to translate between $X$ and $Y$ even if it has never seen the parallel data between $X$ and $Y$. \\citeay{firat2016zero} achieve zero-resource translation by first training a multilingual model and generating a pseudo-parallel corpus for the target language pair via back translation, then fine-tuning on the pseudo-parallel corpus. Based on a fully shared multilingual NMT model, zero-shot translation shows reasonable quality without any additional steps . Some designs can further improve the zero-shot translation in a fully shared multilingual NMT model: (1) introducing an attentional neural interlingua component between the encoder and decoder~, (2) introducing additional terms to the training objective~, and (3) correcting the off-target zero-shot translation issue via a random online back translation algorithm~. There are two important observations: (1) incorporating more languages may provide benefits on zero-shot translation~, and (2) the quality of zero-shot between similar languages is quite good~.", "cites": [6557, 5025, 5038, 5040, 5018, 5029, 5039, 8871, 5020], "cite_extract_rate": 0.9, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple papers by connecting the concept of multilingual training with zero-shot translation, highlighting methods like pseudo-corpus generation, neural interlingua, and training objective modifications. It also identifies key observations such as the benefit of incorporating more languages and the performance of similar languages in zero-shot settings. While it provides some critical analysis by pointing out issues like off-target translations and limitations in model generalization, the critique is not deeply nuanced and could offer more comparative depth."}}
{"id": "642e28af-d47e-4763-adaf-0e134f5d14a4", "title": "Transfer learning", "level": "subsection", "subsections": ["a747d0d3-f592-4438-b330-45bb7063942d"], "parent_id": "11f12d8b-d7f9-4367-9b1c-98d177b64bf1", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Exploiting Data From Auxiliary Languages"], ["subsection", "Transfer learning"]], "content": "A typical method of transfer learning for low-resource NMT is to first train an NMT model on some auxiliary (usually rich-resource) language pairs, which is called parent model, and then fine-tune all or some of the model parameters on a low-resource language pair, which is called child model~. There are three main design aspects in transfer learning: (1) how to select the auxiliary language, (2) how to design the joint vocabulary between the auxiliary and low-resource languages, and (3) how to fine-tune the model for low-resource languages.", "cites": [4967], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of transfer learning in low-resource NMT, mentioning a common method and three design aspects. It references one paper and paraphrases its approach without connecting it to broader trends or contrasting it with other methods. There is minimal synthesis, critical evaluation, or abstraction beyond the cited work."}}
{"id": "a747d0d3-f592-4438-b330-45bb7063942d", "title": "Auxiliary language selection.", "level": "paragraph", "subsections": ["b93c69ef-b647-4b82-ba5b-ece41578e44c", "f37a8dc8-7c40-4463-ac80-f061904b7e70"], "parent_id": "642e28af-d47e-4763-adaf-0e134f5d14a4", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Exploiting Data From Auxiliary Languages"], ["subsection", "Transfer learning"], ["paragraph", "Auxiliary language selection."]], "content": "A common approach is to select rich-resource languages as auxiliary languages~, where the languages share the same/similar language family or typology with the given low-resource language performs better~. LANGRANK is a framework to automatically detect the optimal auxiliary language based on typological and corpus statistical information~.", "cites": [5041, 4967, 8873], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a brief description of auxiliary language selection in transfer learning for low-resource NMT, mentioning the general strategy and the LANGRANK framework. However, it lacks synthesis of ideas across the cited papers and offers little critical evaluation or abstraction. The content remains largely descriptive and does not connect the works to form a deeper understanding of the topic."}}
{"id": "b93c69ef-b647-4b82-ba5b-ece41578e44c", "title": "Joint vocabulary design.", "level": "paragraph", "subsections": [], "parent_id": "a747d0d3-f592-4438-b330-45bb7063942d", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Exploiting Data From Auxiliary Languages"], ["subsection", "Transfer learning"], ["paragraph", "Auxiliary language selection."], ["paragraph", "Joint vocabulary design."]], "content": "A shared vocabulary including learned sub-words of the auxiliary and the desired low-resource language pairs is commonly used~. The shared vocabulary is often built by Byte Pair Encoding (BPE)~ and Sentencepiece~. However, the shared vocabulary is not suitable for transferring a pre-trained parent model to languages with unseen scripts in the vocabulary. To address this problem, \\citeay{kim2019effective} propose to learn a cross-lingual linear mapping between the embeddings of the unseen language and the bilingual parent model.", "cites": [5045, 5046, 8559, 8873], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from multiple papers, such as the use of shared vocabularies via BPE and Sentencepiece, and the issue of script mismatch. It identifies a limitation (unseen scripts) and presents a solution proposed in the literature. While it offers some analytical perspective, it lacks deeper evaluation or comparison of methods and does not abstract to broader principles beyond the immediate topic of vocabulary design."}}
{"id": "f37a8dc8-7c40-4463-ac80-f061904b7e70", "title": "Fine-tuning.", "level": "paragraph", "subsections": [], "parent_id": "a747d0d3-f592-4438-b330-45bb7063942d", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Exploiting Data From Auxiliary Languages"], ["subsection", "Transfer learning"], ["paragraph", "Auxiliary language selection."], ["paragraph", "Fine-tuning."]], "content": "One simple method of fine-tuning is to use a parent model on one rich-resource language to initialize the child model and then fine-tune all the parameters on the low-resource language pair~. Compared to fine-tune only on the desired low-resource language pair, a multistage fine-tuning procedure performs better, where the pre-trained parent model is first fine-tuned on a mixed corpora of the rich-resource and low-resource languages, and then fine-tuned on the desired low-resource language pair~. Some parameters can be fixed while fine-tuning, where \\citeay{bapna2019simple} fix the parameters of the parent model and add light-weight residual adapters when fine-tuning. Moreover, besides using a bilingual parent model, a multilingual parent model can also be used, which enjoys two main advantages. First, a low-resource language can benefit from multiple auxiliary languages. Second, considering the limited model capacity of a multilingual model, fine-tuning may force the model to focus on the desired low-resource language, and thus improve the performance. \\citeay{neubig2018rapid} compare different settings when fine-tuning a low-resource NMT model from a multilingual model on many-to-English direction, and come up with the conclusions: (1) Warm start, where the parent model is trained with both auxiliary languages and low-resource language, is better than cold start, where the parent model is trained only on auxiliary languages; (2) Fine-tuning from a universal model containing dozens of languages outperforms fine-tuning from a model with one similar auxiliary language; (3) Fine-tuning with the data of the low-resource language and one similar rich-resource language outperforms fine-tuning with only low-resource language data. In addition, \\citeay{tan2019study} suggest warm start for many-to-one setting and cold start for one-to-many setting.", "cites": [4967], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from multiple papers, including Bapna & Johnson (2019) and Neubig et al. (2018), to explain fine-tuning strategies in low-resource NMT. It provides a coherent narrative by contrasting warm and cold start approaches and identifying the benefits of multilingual models. While it offers some critical insights by evaluating different settings and their outcomes, it does not deeply critique the methodologies or limitations of the cited works. The abstraction is moderate, as it generalizes to broader patterns like the effectiveness of multilingual models over bilingual ones."}}
{"id": "89cfc63c-4871-481f-9ed1-e482ba53fe96", "title": "Pivot translation", "level": "subsection", "subsections": [], "parent_id": "11f12d8b-d7f9-4367-9b1c-98d177b64bf1", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Exploiting Data From Auxiliary Languages"], ["subsection", "Pivot translation"]], "content": "In pivot-based approaches, a pivot language, which is usually a rich-resource language, is selected as a bridge. Then, the source-pivot and pivot-target corpora and model can be exploited to build the source-target translation.\nThere are mainly three ways to take advantage of pivot language. The first approach is to train the source-pivot and pivot-target models and directly combine them as a source-pivot-target model~. Second, another widely used method is to train the source-target model by pseudo-parallel data, which is generated with the help of the pivot language. \\citeay{zheng2017maximum} translate the pivot language in a pivot-target parallel corpus to source language by a pivot-source NMT model, while \\citeay{chen2017teacher} build the pseudo-parallel corpora by the source-pivot corpus and pivot-target model. Besides the parallel corpus, the monolingual data of source and target languages can also be used to generate pseudo-parallel corpora~. Moreover, the abundant monolingual data on pivot language can also be utilized to get the source-target pseudo-parallel corpora~. Third, leveraging the parameters of source-pivot and pivot-target models is also one way to utilize the pivot language. \\citeay{kim2019pivot} transfer the encoder of source-pivot model and the decoder of the pivot-target model to the source-target model. \\citeay{ji2020cross} pre-train a universal encoder for source and pivot languages based on cross-lingual pre-training~, and then train on pivot-target parallel data with part of the encoder frozen. Pivot languages selection is critical in pivot-translation, which greatly influences the translation quality. In most cases, one pivot language is selected based on prior knowledge. There also exists a learning to route (LTR) method to automatically select one or several pivot languages to translate via multiple hops~.", "cites": [5055, 6558, 5047, 1551], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear synthesis of pivot-based translation methods by categorizing them into three main approaches, and links these with relevant cited works. It abstracts to some extent by identifying the general role of pivot languages and how they are used in different ways (e.g., via pseudo-parallel data or parameter sharing). However, it lacks deeper critical analysis of the methods' strengths, weaknesses, or trade-offs, and the narrative is primarily focused on summarizing approaches rather than evaluating them."}}
{"id": "665fcd88-81a9-442e-8711-ab9808c5dfb9", "title": "Exploiting Multi-Modal Data", "level": "section", "subsections": [], "parent_id": "9ed5454f-c634-4de3-9fa8-bd1914145f63", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Exploiting Multi-Modal Data"]], "content": "\\label{sec:multimodal}\nThe parallel data in other modality is also useful for NMT, such as image, video, speech, etc. \\citeay{chen2019words} and \\citeay{huang2020unsupervised} built a pseudo parallel corpus by generating captions of the same image in both source and target languages via pre-trained image captioning models. In addition, the image caption/description and translation tasks can be jointly learned to incorporate image information~. Moreover, the image data can be utilized by introducing an additional image component (e.g., encoder, decoder or attention) into the NMT model and aligning the two languages with the corresponding image in the latent space~. Currently, the application of using image-text parallel data on NMT is limited, since such image-text data is always hard to collect for low-resource languages. One potential data source to build new image-text dataset is the images and corresponding captions on websites (e.g., Wikipedia and news pages). For the languages with only speech but no text scripts, speech data can be leveraged to develop the translation capability~.", "cites": [6560, 6567, 6559, 6565, 8254, 6564, 6566, 6563, 6561, 6562], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers by grouping them under the theme of incorporating multi-modal data into NMT, and it outlines common approaches such as building pseudo corpora and integrating visual features. It shows some abstraction by identifying limitations (e.g., scarcity of image-text data for low-resource languages) and potential solutions (e.g., leveraging web-based image-text resources). However, it lacks deeper critical analysis of the methods or evaluation of their effectiveness."}}
{"id": "40f15a7d-d77f-4feb-a88b-059580134f6a", "title": "Datasets", "level": "section", "subsections": [], "parent_id": "9ed5454f-c634-4de3-9fa8-bd1914145f63", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Datasets"]], "content": "\\label{sec:data}\n\\begin{table}[h]\n\\begin{tabular}{llll}\n\\toprule\nDateset & Type & \\#Language & Size \\\\ \\midrule \nWikipedia & mo & 300+ & $\\sim55$M documents \\\\\nCommonCrawl & mo & 150+ & Billions of URLs \\\\\nCC-100 & mo & 100+ & $\\sim0.5$B sents/lang \\\\\nJW300 & bi & 300+ & $\\sim0.1$M sents/pair \\\\\nCCAligned & bi & 137 & $\\sim0.3$M sents/pair \\\\\nCCMatrix & bi & 80 & $\\sim1+$M sents/pair \\\\\nWikiMatrix & bi & 85 & $\\sim0.1$M sents/pair \\\\\\bottomrule\n\\end{tabular}\n\\caption{List of datasets, where mo and bi stand for monolingual and bilingual data, sents/lang and sents/pair stand for the number of sentences in a language and language pair, respectively.}\n\\label{tab:dataset}\n\\end{table}\nData is critical for low-resource NMT. In this section, we introduce some corpora that are widely used in low-resource NMT, as shwon in Tab. \\ref{tab:dataset}. Wikipedia\\footnote{https://www.wikipedia.org/} and Common Crawl\\footnote{http://commoncrawl.org/} contain abundant monolingual data, where Wikipedia covers more than 300 languages and Common Crawl contains billions of web pages crawled from the Internet. CC-100~ is a monolingual corpus covering 100+ languages processed from Common Crawl. JW300~, CCAligned~, CCMatrix~ and WikiMatrix~ extract parallel sentences from monolingual data, where JW300 is from the website jw.org, CCAligned and CCMatrix are aligned from Common Crawl, and WikiMatrix is based on Wikipedia. Moreover, OPUS~ and HuggingFace\\footnote{https://huggingface.co/} provide a collection of open source corpora, which makes it much convenient to collect data from multiple data sources.", "cites": [4982, 6568, 7097], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a descriptive overview of datasets used in low-resource NMT, listing their types, language coverage, and sizes. It integrates basic information from the cited papers but lacks deeper synthesis or analysis of how these datasets relate to or influence NMT methodologies. There is little abstraction or critical evaluation of the papers' approaches or limitations."}}
{"id": "2855fe5f-f0b9-4fab-871a-52c4a5b3a4d4", "title": "Conclusion and Future Directions", "level": "section", "subsections": [], "parent_id": "9ed5454f-c634-4de3-9fa8-bd1914145f63", "prefix_titles": [["title", "A Survey on Low-Resource Neural Machine Translation"], ["section", "Conclusion and Future Directions"]], "content": "\\label{sec:con}\nIn this paper, we provided a literature review for low-resource NMT. Different techniques are classified based on the type of auxiliary data: monolingual data from the source/target languages, data from other languages, and multi-modal data. We hope this survey can help readers to better understand the field and choose appropriate techniques for their applications. \nThough lots of efforts have been made on low-resource NMT as surveyed, there still remain some open problems: \n\\begin{itemize}\n    \\item In multilingual and transfer learning, how many and which auxiliary languages to use is unclear. LANGRANK~ trains a model to select one auxiliary language. Intuitively, using multiple auxiliary languages may outperform only one, which is worth exploration.  \n    \\item Training a multilingual model including multiple rich-resource languages is costly. Transferring a multilingual model to an unseen low-resource language is an efficient approach, where the challenge is how to handle the new vocabulary of the unseen language.\n    \\item How to efficiently select pivot language(s) is important but has not been well investigated.\n    \\item Bilingual dictionary is useful and easy-to-get. Current works focus on taking advantage of bilingual dictionary on the source and target language. It is also possible to use bilingual dictionary between a low-resource language and auxiliary languages in multilingual and transfer training.\n    \\item In terms of multi-modality, speech data has potential to boost NMT, but such studies are limited. For example, some languages are close in speech but different in script (e.g., Tajik and Persian).\n    \\item Current approaches have made significant improvements for low-resource languages that either have sufficient monolingual data or are related to some rich-resource languages. Unfortunately, some low-resource languages (e.g., Adyghe and Xibe) have very limited monolingual data and are distant from rich-resource languages. How to handle such languages is challenging and worth further studies.\n\\end{itemize}\n\\bibliographystyle{named}\n\\bibliography{ref}\n\\end{document}", "cites": [5041], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from the cited paper (e.g., LANGRANK) and connects it to broader themes in low-resource NMT, showing a basic level of integration. It identifies gaps and limitations in current methods, particularly in the selection of auxiliary languages and handling of unseen languages, but does not deeply critique or compare multiple approaches. The section abstracts some general challenges, such as the need for better pivot language selection and the potential of speech data, but stops short of developing a comprehensive framework or meta-level insights."}}
