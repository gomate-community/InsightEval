{"id": "bb8d881f-5a9a-4d40-9b4d-6a1e4f8408ac", "title": "Introduction", "level": "section", "subsections": ["209ae615-a488-4929-bb23-a528a6a66b13"], "parent_id": "26e7485a-9b67-4600-a0c0-c92ba6777268", "prefix_titles": [["title", "Explanation-Based Human Debugging of NLP Models: A Survey"], ["section", "Introduction"]], "content": "Explainable AI focuses on generating explanations for AI models as well as for their predictions. \nIt is gaining more and more attention these days since explanations are necessary in many \napplications, especially in high-stake domains such as healthcare, law, transportation, and finance . \nSome researchers have explored various merits of explanations to humans, such as \nsupporting human decision making ,\nincreasing human trust in AI \nand even teaching humans to perform challenging tasks .\nOn the other hand, explanations can benefit the AI systems as well, e.g.,\nwhen explanations are used\nto promote system acceptance ,\nto verify the model reasoning ,\nand to find potential causes of errors . \nIn this paper, we review progress to date  \nspecifically on how explanations have been used in the literature to enable humans to fix bugs in NLP models. We refer to this research area as \\emph{\\ebhd (\\EBHD)}, as a general umbrella term encompassing  \\emph{explanatory debugging}  and \\emph{human-in-the-loop debugging} .\nWe define \\EBHD as the process of fixing or mitigating bugs in a trained model using human feedback given in response to explanations for the model.\n\\EBHD is helpful when the training data at hand leads to suboptimal models (due, for instance, to biases \nor artifacts in the data), and hence human knowledge is needed to verify and improve the trained models.  \nIn fact, \\EBHD is related to three challenging and intertwined issues in NLP: explainability , interactive and human-in-the-loop learning , and knowledge integration . \nAlthough there are overviews for each of these topics\n(as cited above), our paper is the first to draw connections among the three towards the final application of model debugging in NLP.\n\\begin{figure*}[t] \n    \\centering\n    \\includegraphics[width=0.90\\linewidth]{figs/workflow-options-7.pdf}\n    \\caption{A general framework for \\ebhd (\\EBHD) of NLP models, consisting of the inspected (potentially buggy) model, the humans providing feedback, and a three-step workflow. Boxes list examples of the options (considered in the selected studies) for the components or steps in the general framework.} \\label{fig:overview}\n\\end{figure*}\n\\begin{figure*}[t] \n    \\centering\n    \\includegraphics[width=0.90\\linewidth]{figs/LIME2.pdf}\n    \\caption{The proposal by  as an instance of the general \\EBHD framework.} \\label{fig:lime}\n\\end{figure*}\nWhereas most people agree on the meaning of the term \\emph{bug} in software engineering, various meanings have been ascribed to this term in machine learning (ML) research.\nFor example,  considered bugs as implementation errors, similar to software bugs, while\n defined a bug as a particularly damaging or inexplicable test error.\nIn this paper, we follow the definition of (model) bugs from  as contamination in the learning and/or prediction pipeline that makes the model produce incorrect predictions or learn error-causing associations. Examples of bugs include spurious correlation, labelling errors, and undesirable behavior in out-of-distribution (OOD) testing.\nThe term \\emph{debugging} is also interpreted differently by different researchers. \nSome consider debugging as a process of identifying or uncovering causes of model errors , while others stress that debugging must not only reveal the causes of problems but also fix or mitigate them . In this paper, we adopt the latter interpretation.", "cites": [4865, 7507, 4864, 7696, 8847, 4863], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 18, "insight_result": {"error": "Failed to parse LLM response", "raw_response": "{\n    \"type\": \"analytical\",\n    \"scores\": {\"synthesis\": 4.2, \"critical\": 3.3, \"abstraction\": 4.5},\n    \"insight_level\": \"high\",\n    \"analysis\": \"The section provides a strong synthesis by connecting ideas from multiple cited papers into a unified framework for \\EBHD, including the integration of explainability, human-in-the-loop learning, and knowledge integration. It includes a critical discussion of differing definitions of 'bug' and 'debugging' in ML, referencing specific works to contrast in"}}
{"id": "209ae615-a488-4929-bb23-a528a6a66b13", "title": "Scope of the survey.", "level": "paragraph", "subsections": ["8daaf7fc-c2db-4919-9807-e70bee7ffa77"], "parent_id": "bb8d881f-5a9a-4d40-9b4d-6a1e4f8408ac", "prefix_titles": [["title", "Explanation-Based Human Debugging of NLP Models: A Survey"], ["section", "Introduction"], ["paragraph", "Scope of the survey."]], "content": "We focus on work \nusing\nexplanations of NLP models to expose whether there are bugs\nand exploit human feedback to fix the bugs (if any). \nTo collect relevant papers, we started from some pivotal \\EBHD work, e.g., , and added \\EBHD papers citing or being cited by the pivotal work, e.g., .\nNext, to ensure that we did not miss any important work, we searched for papers on Semantic Scholar\\footnote{\\url{https://www.semanticscholar.org/}} using the Cartesian product of five keyword sets: \\{debugging\\}, \\{text, NLP\\}, \\{human, user, interactive, feedback\\}, \\{explanation, explanatory\\}, and \\{learning\\}.\nWith 16 queries in total, we collected the top 100 papers (ranked by relevancy) for each query and kept only the ones appearing in at least 2 out of the 16 query results.\nThis resulted in 234 papers which we then manually checked, leading to selecting a few additional papers, including . \nThe overall process resulted in \\numpapers papers in Table~\\ref{tab:papers} as the \\textit{selected studies} primarily discussed in this survey. \nIn contrast, \nsome papers from the following categories appeared in the search results, but were not selected\nsince, strictly speaking, they are not in the main scope of this survey:\ndebugging without explanations ,\ndebugging outside the NLP domain , \nrefining the ML pipeline instead of the model , \nimproving the explanations instead of the model , \nand\nwork centered on revealing but not fixing bugs .", "cites": [4867, 4866, 4868, 7507, 4869], "cite_extract_rate": 0.3125, "origin_cites_number": 16, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the scope of the survey and the methodology used to select papers. While it mentions several key papers and their general contributions, it lacks deeper synthesis of their ideas, critical evaluation of their strengths or limitations, and abstraction to broader trends or principles in the field. The section serves more as a procedural overview than an insightful analysis."}}
{"id": "8daaf7fc-c2db-4919-9807-e70bee7ffa77", "title": "General Framework.", "level": "paragraph", "subsections": ["1e9de09b-b4d3-4874-9241-07ac21aac0cb", "73714224-8905-4062-8e33-4905c175af89"], "parent_id": "209ae615-a488-4929-bb23-a528a6a66b13", "prefix_titles": [["title", "Explanation-Based Human Debugging of NLP Models: A Survey"], ["section", "Introduction"], ["paragraph", "Scope of the survey."], ["paragraph", "General Framework."]], "content": "\\EBHD \nconsists of\nthree main \nsteps as shown in Figure~\\ref{fig:overview}.\nFirst, the explanations, which provide interpretable insights into the inspected model and possibly reveal bugs, are \ngiven to humans.\nThen, the humans inspect the explanations and give feedback in response.\nFinally, the feedback is used to update and improve the model.\nThese steps can be carried out once, as a one-off improvement, or iteratively, depending on how the debugging framework is designed. \nAs a concrete example, Figure~\\ref{fig:lime} illustrates how  improved an SVM text classifier trained on the 20Newsgroups dataset .\nThis dataset has many\nartifacts which could make the model rely on wrong words or tokens when making predictions, reducing its generalizability\\footnote{For more details, please see section~\\ref{subsec:bugsources}.}. \nTo perform \\EBHD,  recruited humans \nfrom a crowdsourcing platform (i.e., Amazon Mechanical Turk) and asked them to inspect LIME explanations\\footnote{LIME stands for Local Interpretable Model agnostic Explanations . For each model prediction, it returns relevance scores for words in the input text to show how important each word is for the prediction.} (i.e., word relevance scores) for model predictions of ten examples.\nThen, the humans gave feedback by identifying words in the explanations that should not have got high relevance scores (i.e., supposed to be the artifacts). These words were then removed from the training data, and the model was retrained. The process was repeated for three rounds, and the results show that the model generalized better after every round. \nUsing the general framework in Figure~\\ref{fig:overview}, we can break the framework of  into components \nas depicted in Figure~\\ref{fig:lime}.  \nThroughout the paper, when reviewing \nthe selected studies,\nwe will use the general framework in Figure~\\ref{fig:overview} for analysis, comparison, and discussion.", "cites": [7507], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"error": "Failed to parse LLM response", "raw_response": "{\n    \"type\": \"analytical\",\n    \"scores\": {\"synthesis\": 3.5, \"critical\": 2.0, \"abstraction\": 3.0},\n    \"insight_level\": \"medium\",\n    \"analysis\": \"The section introduces a general framework for \\EBHD and uses a specific example from the LIME-based debugging process to illustrate its application. It synthesizes the core idea of \\EBHD by connecting the concept of explanations (via LIME) to human feedback and model improvement, which suggests some integration of the cited paper. However, the analys"}}
{"id": "c98144b9-7fac-4a44-8376-9fd8e9053e10", "title": "Categorization of Existing Work", "level": "section", "subsections": ["529a4abe-46e1-4bad-8356-542ec52c9967", "046f9b18-3513-4d56-82c3-c60190ba0aa9", "5b122951-af49-414e-bead-48a02462c5e7"], "parent_id": "26e7485a-9b67-4600-a0c0-c92ba6777268", "prefix_titles": [["title", "Explanation-Based Human Debugging of NLP Models: A Survey"], ["section", "Categorization of Existing Work"]], "content": "\\label{sec:categorization}\n\\begin{table*}[t]\n\\setlength{\\tabcolsep}{4.5pt}\n\\centering\n\\small\n\\begin{tabular}{|L{0.28\\textwidth}| C{0.05\\textwidth} C{0.07\\textwidth} C{0.10\\textwidth}| C{0.05\\textwidth} C{0.06\\textwidth} C{0.08\\textwidth} C{0.05\\textwidth} | C{0.07\\textwidth}|} \n \\hline\n \\multirow{2}{*}{Paper} & \\multicolumn{3}{c|}{Context} & \\multicolumn{4}{c|}{Workflow} & \\multirow{2}{*}{Setting}\\\\ \\cline{2-8}\n  & Task & Model & Bug sources & Exp. scope & Exp. method & Feedback & Update & \\\\\n \\hline\n &TC&NB&AR&G,L&SE&LB,WS&M,D&SP\\\\\n &TC&NB&SS&L&SE&WO&T&SP\\\\\n &TC&NB&SS&G,L&SE&WO,LB&M,D&SP\\\\\n &TC&NB&AR&G,L&SE&WO,WS&M&SP\\\\\n &TC&SVM&AR&L&PH&WO&D&CS\\\\\n &TC&LR&WL&L&PH&LB&D&SM\\\\\n \\multirow{2}{*}{}&VQA&TellQA&AR&\\multirow{2}{*}{G}&\\multirow{2}{*}{PH}&\\multirow{2}{*}{RU}&\\multirow{2}{*}{D}&\\multirow{2}{*}{SP}\\\\\n &TC&fastText&AR,OD&&&&&\\\\\n &TC&LR&AR&L&PH&WO&D&SM\\\\\n &TQA&NeOp&AR&L&SE&AT&T&NR\\\\\n &TC&LR&WL&L&PH&LB&D&SM\\\\\n &TC&CNN&AR,SS,OD&G&PH&FE&T&CS\\\\\n &TC&NB&AR,SS&L&SE&LB,WO&M,D&CS\\\\\n &TC&LR&WL&L&PH&LB&D&SM\\\\\n &TC&BERT*&AR,OD&L&PH&RE&D,T&SP\\\\\n &NLI&BERT&AR&L&PH&ES&D&SP\\\\\n \\hline\n \\end{tabular}\n\\caption{Overview of existing work on \\EBHD of NLP models. We use abbreviations as follows: \\textbf{Task}: TC = Text Classification (single input), VQA = Visual Question Answering, TQA = Table Question Answering, NLI = Natural Language Inference / \n\\textbf{Model}: NB = Naive Bayes, SVM = Support Vector Machines, LR = Logistic Regression, TellQA = Telling QA, NeOp = Neural Operator, CNN = Convolutional Neural Networks, BERT* = BERT and RoBERTa /\n\\textbf{Bug sources}: AR = Natural artifacts, SS = Small training subset, WL = Wrong label injection, OD = Out-of-distribution tests /\n\\textbf{Exp. scope}: G = Global explanations, L = Local explanations /\n\\textbf{Exp. method}: SE = Self-explaining, PH = Post-hoc method /\n\\textbf{Feedback} (form): LB = Label, WO = Word(s), WS = Word(s) Score, ES = Example Score, FE = Feature, RU = Rule, AT = Attention, RE = Reasoning /\n\\textbf{Update}: M = Adjust the model parameters, D = Improve the training data, T = Influence the training process /\n\\textbf{Setting}: SP = Selected participants, CS = Crowdsourced participants, SM = Simulation, NR = Not reported.\n} \\label{tab:papers}\n\\end{table*} \nTable~\\ref{tab:papers} summarizes the selected studies\nalong three dimensions, amounting to \nthe debugging context (i.e., tasks, models, and bug sources), the workflow (i.e., the three steps in our general framework), and the experimental setting (i.e., the mode of human engagement).\nWe will discuss these dimensions with respect to the broader knowledge of explainable NLP and human-in-the-loop learning, to shed light on the current state of \\EBHD of NLP models.", "cites": [7516, 4868, 7507, 4870, 4869], "cite_extract_rate": 0.4166666666666667, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section primarily presents a descriptive overview of selected studies using a table with various abbreviations, but it lacks a coherent narrative or synthesis of the cited works. There is minimal critical analysis or evaluation of the approaches, and abstraction is limited to identifying broad categories rather than deeper principles or trends."}}
{"id": "19ba55c1-e53d-44f6-b54b-236a658bbf6e", "title": "Tasks", "level": "subsubsection", "subsections": [], "parent_id": "529a4abe-46e1-4bad-8356-542ec52c9967", "prefix_titles": [["title", "Explanation-Based Human Debugging of NLP Models: A Survey"], ["section", "Categorization of Existing Work"], ["subsection", "Context"], ["subsubsection", "Tasks"]], "content": "Most papers in Table~\\ref{tab:papers} focus on text classification with single input (TC) for \na variety of specific problems such as e-mail categorization , topic classification , spam classification , sentiment analysis \nand auto-coding of transcripts .\nBy contrast,  targeted natural language inference (NLI) which is a type of text-pair classification, predicting whether a given premise entails a given hypothesis.\nFinally, two papers involve question answering (QA), i.e.,  (focusing on visual question answering (VQA)) and \n(focusing on table question answering (TQA)).\n suggested that most researchers work on TC because, for this task, it is much easier for lay \nparticipants to understand explanations and give feedback (e.g., which keywords should be added or removed from the list of top features)\\footnote{Nevertheless, some specific TC tasks, such as authorship attribution  and deceptive review detection , are exceptions because lay people are generally not good at these tasks. Thus, they are not suitable for \\EBHD.}.\nMeanwhile, some other NLP tasks require the feedback providers\nto have linguistic knowledge such as part-of-speech tagging, parsing, and machine translation. \nThe need for linguists or experts renders experiments for these tasks more difficult and costly.\nHowever, we suggest that there are several tasks \nwhere the trained models are prone to be buggy but the tasks are underexplored in the \\EBHD setting, though they are not too difficult to experiment on with lay people.\n\\textit{NLI}, the focus of , is one of them. Indeed,\n and  showed that NLI models can exploit annotation artifacts and fallible syntactic heuristics to make predictions rather than learning the logic of the actual task.\nOther tasks and their bugs \ninclude:\n\\textit{QA}, where  found that the answers from models \nare sometimes inconsistent (i.e., contradicting previous answers); and \\textit{reading comprehension}, where \n showed that \nmodels, which answer a question by reading a given paragraph, can be fooled by an irrelevant sentence \nbeing appended to the paragraph. \nThese non-TC NLP tasks would be worth exploring further in the \\EBHD setting.", "cites": [7516, 4865], "cite_extract_rate": 0.2222222222222222, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the NLP tasks used in EBHD research, synthesizing information to highlight why certain tasks are more suitable for lay participants. While it makes connections between the tasks and the feasibility of human feedback, the critical evaluation and abstraction remain moderate, with some identification of gaps but not a comprehensive, novel framework or deep critique of the cited works."}}
{"id": "f77bbe8b-0a16-4291-a160-8fcbb8182d98", "title": "Models", "level": "subsubsection", "subsections": [], "parent_id": "529a4abe-46e1-4bad-8356-542ec52c9967", "prefix_titles": [["title", "Explanation-Based Human Debugging of NLP Models: A Survey"], ["section", "Categorization of Existing Work"], ["subsection", "Context"], ["subsubsection", "Models"]], "content": "Early work used Naive Bayes models with bag-of-words (NB) as text classifiers , which are relatively easy to generate explanations for and to incorporate human feedback into (discussed in section~\\ref{subsec:workflow}).\nOther traditional models used include logistic regression (LR)  \nand support vector machines (SVM) , both with bag-of-words features.\nThe next generation of tested models involves word embeddings.\nFor text classification,  \nfocused on convolutional neural networks (CNN)  and touched upon bidirectional LSTM networks , while  used fastText, relying also on n-gram features .\nFor VQA and TQA, the inspected models used attention mechanisms for attending to relevant parts of the input image or table. These models are Telling QA  and Neural Operator (NeOp) , used by  and , respectively.\nWhile the NLP community nowadays is mainly driven by pre-trained language models \nwith many papers studying their behaviors , \nonly  and  have used pre-trained language models, including BERT  and RoBERTa , as test beds for \\EBHD.", "cites": [4871, 4868, 826, 7507, 1445, 4869], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 14, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic overview of the models used in EBHD research, listing different model types and their features. However, it lacks meaningful synthesis between the cited works and offers little in the way of critical evaluation or abstraction to broader themes. It primarily serves as a factual summary of model usage in the field."}}
{"id": "fdd7cacd-e596-496f-a18e-ea86a4361eb2", "title": "Bug Sources", "level": "subsubsection", "subsections": [], "parent_id": "529a4abe-46e1-4bad-8356-542ec52c9967", "prefix_titles": [["title", "Explanation-Based Human Debugging of NLP Models: A Survey"], ["section", "Categorization of Existing Work"], ["subsection", "Context"], ["subsubsection", "Bug Sources"]], "content": "\\label{subsec:bugsources}\nMost of the papers in Table~\\ref{tab:papers} experimented on training datasets with natural artifacts (AR), which cause spurious correlation bugs (i.e., the input texts having signals which are correlated to but not the reasons for specific outputs) and undermine models' generalizability.\nOut of the \\numpapers papers we surveyed, 5 used the 20Newsgroups dataset  as a case study, since it has lots of natural artifacts. \nFor example, some punctuation marks appear more often in one class due to the writing styles of the authors contributing to the class, so the model uses these punctuation marks as clues to make predictions. However, because 20Newsgroups is a topic classification dataset, a better model should focus more on the topic of the content since the punctuation marks can also appear in other classes, especially when we apply the model to texts in the wild.\nApart from classification performance drops, natural artifacts can also cause model biases, as shown in  and debugged in .\nIn the absence of strong natural artifacts, bugs can still be simulated using several techniques. \nFirst, using only a small subset of labeled data (SS) for training could cause the model to exploit spurious correlation leading to poor performance . \nSecond, injecting wrong labels (WL) into the training data can obviously blunt the model quality . \nThird, using out-of-distribution tests (OD) can reveal that the model does not work effectively in the domains that it has not been trained on .\nAll of these techniques give rise to undesirable model behaviors, requiring debugging.\nAnother technique, not found in Table~\\ref{tab:papers} but suggested in related work , is contaminating input texts in the training data with decoys (i.e., injected artifacts) which could deceive the model into predicting for the wrong reasons. This has been experimented with in the computer vision domain , and its use in the \\EBHD setting in NLP could be an interesting direction to explore.", "cites": [7516, 4872, 8848, 4869, 4873], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of different bug sources in NLP models and connects them to the concept of spurious correlations, referencing multiple papers. It integrates ideas about data artifacts, label injection, and out-of-distribution testing. However, it lacks deeper critical analysis of the cited works and offers limited abstract generalization, though it does suggest broader implications for model debugging and bias."}}
{"id": "4ba6754b-5693-434b-a98e-4936c742ad3a", "title": "Explanation scopes.", "level": "paragraph", "subsections": [], "parent_id": "65c2c958-84ce-431f-a866-b1d203b8794a", "prefix_titles": [["title", "Explanation-Based Human Debugging of NLP Models: A Survey"], ["section", "Categorization of Existing Work"], ["subsection", "Workflow"], ["subsubsection", "Providing Explanations"], ["paragraph", "Explanation scopes."]], "content": "Basically, there are two main types of explanations that could be provided to feedback providers. Local explanations (L) explain the predictions by the model for individual inputs. \nIn contrast, global explanations (G) explain the model overall, independently of any specific inputs.\nIt can be seen from Table~\\ref{tab:papers} that most existing work use local explanations.\nOne reason for this may be that,\nfor complex models, \nglobal explanations can hardly reveal \ndetails of the models' inner workings \nin a comprehensible way to users. \nSo, some bugs are imperceptible in such high-level global explanations and then not corrected by the users.\nFor example, the debugging framework \\textit{FIND}, proposed by , uses only global explanations, and it was shown to work more effectively on significant bugs (such as gender bias in abusive language detection) than on less-obvious bugs (such as dataset shift between product types of sentiment analysis on product reviews). \nOtherwise,  presented adversarial replacement rules as global explanations to reveal the model weaknesses only, without explaining how the whole model worked. \nOn the other hand, using local explanations has limitations in that it demands a large amount of effort from feedback providers to inspect the explanation of every single example in the training/validation set.\nWith limited human resources, efficient ways to rank or select examples to explain would be required .\nFor instance, \n and  targeted explanations of incorrect predictions in the validation set.\n picked sets of non-redundant local explanations to illustrate the global picture of the model. \nInstead,  leveraged heuristics from active learning to choose unlabeled examples that maximize some informativeness criteria.\nRecently, some work in explainable AI considers generating explanations for a group of predictions  (e.g., for all the false positives of a certain class), thus staying in the middle of the two extreme explanation types (i.e., local and global). \nThis kind of explanation is not too fine-grained, yet it can capture some suspicious model behaviors if we target the right group of examples.\nSo, it would be worth studying in the context of \\EBHD \n(to the best of our knowledge, no existing study experiments with it).", "cites": [4868, 4872, 4874, 4870, 7507], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.8, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers to build a nuanced understanding of explanation scopes in EBHD, highlighting local, global, and intermediate approaches. It critically evaluates the limitations of each type, such as the inefficiency of local explanations and the insufficient detail in global ones, and identifies open research directions. The analysis abstracts beyond individual studies to propose a broader framework for understanding explanation utility in debugging."}}
{"id": "9160f106-1e22-4924-8c1f-f9ab659496ec", "title": "Generating explanations.", "level": "paragraph", "subsections": [], "parent_id": "65c2c958-84ce-431f-a866-b1d203b8794a", "prefix_titles": [["title", "Explanation-Based Human Debugging of NLP Models: A Survey"], ["section", "Categorization of Existing Work"], ["subsection", "Workflow"], ["subsubsection", "Providing Explanations"], ["paragraph", "Generating explanations."]], "content": "To generate explanations in general, there are two important questions we need to answer. \nFirst, \nwhich format should the explanations have?\nSecond, how do we generate the explanations?\nFor the first question, we see many possible answers in the literature of explainable NLP (e.g., see the survey by ). For instance, \\textit{input-based explanations} (so called feature importance explanations) identify parts of the input that are important for the prediction. \nThe explanation could be \na list of importance scores of words in the input, so called \\textit{attribution scores} or \\textit{relevance scores} .\n\\textit{Example-based explanations} select influential, important, or similar examples from the training set to explain why the model makes a specific prediction .\n\\textit{Rule-based explanations} provide interpretable decision rules that approximate the prediction process\n.\n\\textit{Adversarial-based explanations} return the smallest changes in the inputs that could change the predictions, revealing the model misbehavior .\nIn most NLP tasks, \ninput-based explanations are the most popular approach for explaining predictions . This is also the case for \\EBHD as most selected studies use input-based explanations \nfollowed by example-based explanations\n. \nMeanwhile, only  use adversarial-based explanations, whereas  experiment with input-based, rule-based, and example-based explanations. \nFor the second question, there are two ways to generate the explanations: self-explaining methods and post-hoc explanation methods. Some models, e.g., Naive Bayes, logistic regression, and decision trees, are \\textit{self-explaining} (SE) , \nalso referred to as transparent  or inherently interpretable .\nLocal explanations of self-explaining models can be obtained at the same time as predictions, usually from the process of making those predictions, while \nthe models themselves can often serve directly as global explanations.\nFor example, feature importance explanations for a Naive Bayes model can be directly derived from the likelihood terms in the Naive Bayes equation, as done by several papers in Table~\\ref{tab:papers} .\nAlso, using attention scores on input as explanations, as done in , is a self-explaining method because the scores were obtained during the prediction process. \nIn contrast, \\textit{post-hoc explanation methods} (PH) perform additional steps to extract explanations after the model is trained (for a global explanation) or after the prediction is made (for a local explanation).\nIf the method is allowed to access model parameters, it may calculate word relevance scores by propagating the output scores back to the input words  or analyzing the derivative of the output with respect to the input words . \nIf the method cannot access the model parameters, it may perturb the input and see how the output changes to estimate the importance of the altered parts of the input .\nThe important words and/or the relevance scores can be presented to the feedback providers in the \\EBHD workflow in many forms such as a list of words and their scores , word clouds , and a parse tree . \nMeanwhile, the influence functions method, used in , identifies training examples which influence the prediction by analyzing how the prediction would change if we did not have each training point. This is another post-hoc explanation method as it takes place after prediction. \nIt is similar to the other two example-based explanation methods used in .", "cites": [8849, 4870, 4868, 1824, 7507, 3699, 1813, 7517, 7516, 4875, 4869], "cite_extract_rate": 0.5238095238095238, "origin_cites_number": 21, "insight_result": {"error": "Failed to parse LLM response", "raw_response": "{\n    \"type\": \"analytical\",\n    \"scores\": {\"synthesis\": 4.2, \"critical\": 3.0, \"abstraction\": 4.0},\n    \"insight_level\": \"high\",\n    \"analysis\": \"The section effectively synthesizes various explanation formats (input-based, example-based, rule-based, adversarial-based) and links them to \\EBHD, drawing from multiple papers. It abstracts these methods into a general framework of self-explaining vs. post-hoc explanation methods, offering broader insights. While it provides some analysis of methodolo"}}
{"id": "63a75679-8702-4ba0-91cb-d01a889e8005", "title": "Collecting Feedback", "level": "subsubsection", "subsections": [], "parent_id": "046f9b18-3513-4d56-82c3-c60190ba0aa9", "prefix_titles": [["title", "Explanation-Based Human Debugging of NLP Models: A Survey"], ["section", "Categorization of Existing Work"], ["subsection", "Workflow"], ["subsubsection", "Collecting Feedback"]], "content": "After seeing explanations, \nhumans generally desire to improve the model by giving feedback .\nSome existing work asked humans to confirm or correct machine-computed explanations.\nHence, the form of feedback fairly depends on the form of the explanations,\nand in turn this shapes how to update the model too (discussed in section \\ref{subsec:update}).\nFor text classification, \nmost \\EBHD papers asked humans to decide which words (WO) in the explanation (considered important by the model) are in fact relevant or irrelevant .\nSome papers even allowed humans to adjust the word importance scores (WS) .\nThis is analogous to specifying relevancy scores for example-based explanations (ES) in .\nMeanwhile, feedback at the level of learned features (FE) (i.e., the internal neurons in the model) and learned rules (RU) rather than individual words, was asked in   and , respectively.\nAdditionally, humans may be asked to check the predicted labels  or even the ground truth labels (collectively noted as LB in Table~\\ref{tab:papers}) . \nTargeting the table question answering,  asked humans to identify where in the table and the question the model should focus (AT). This is analogous to identifying relevant words to attend for text classification.\nIt is likely that identifying important parts in the input is sufficient to make the model accomplish simple text classification tasks.\nHowever, this might not be enough for complex tasks which require reasoning. \nRecently,  asked humans to provide, as feedback, compositional explanations \nto show how the humans would reason (RE) about the modelsâ€™ failure cases.\nAn example of the feedback for a hate speech detection is ``Because $X$ is the word dumb, $Y$ is a hateful word, and $X$ is directly before $Y$, the attribution scores of both $X$ and $Y$ as well as the interaction score between $X$ and $Y$ should be increased''.\nTo acquire richer information like this as\nfeedback, their framework requires more expertise from the feedback providers.\nIn the future, it would be interesting to explore how we can collect and utilize other forms of feedback, e.g., natural language feedback , \nnew training examples , and other forms of decision rules used by humans .", "cites": [7516, 7802, 4868, 7507, 4870, 4869], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple feedback collection strategies from cited papers, connecting them through the lens of explanation types and task complexity. It also abstracts these methods into general categories (e.g., WO, WS, ES, FE, RU, LB, AT, RE), suggesting a structured understanding. While it briefly touches on limitations (e.g., requiring expertise for compositional feedback), it could offer deeper critical evaluation of trade-offs between different approaches."}}
{"id": "8fdfada4-8e63-4bc6-808a-fa238dbce062", "title": "(2) Improve the training data (D).", "level": "paragraph", "subsections": [], "parent_id": "6c064860-8785-4a00-b33f-ee42ff0539c8", "prefix_titles": [["title", "Explanation-Based Human Debugging of NLP Models: A Survey"], ["section", "Categorization of Existing Work"], ["subsection", "Workflow"], ["subsubsection", "Updating the Model"], ["paragraph", "(2) Improve the training data (D)."]], "content": "We can use human feedback to improve the training data and retrain the model to fix bugs.\nThis approach includes \ncorrecting mislabeled training examples ,\nassigning noisy labels to unlabeled examples , \nremoving irrelevant words from input texts ,\nand creating augmented training examples to reduce the effects of the artifacts .\nAs this approach modifies the training data only, it is applicable to any model regardless of the model complexity.", "cites": [7516, 4868, 7507, 4869], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of approaches that use human feedback to improve training data, listing specific methods without deeply connecting or contrasting the cited papers. It mentions the applicability of these approaches to any model, but does not elaborate on how the cited works define or operationalize this. There is limited critical analysis or abstraction to broader principles or frameworks."}}
{"id": "e0fdcd8d-308a-404d-bc74-65efeda71978", "title": "(3) Influence the training process (T).", "level": "paragraph", "subsections": [], "parent_id": "6c064860-8785-4a00-b33f-ee42ff0539c8", "prefix_titles": [["title", "Explanation-Based Human Debugging of NLP Models: A Survey"], ["section", "Categorization of Existing Work"], ["subsection", "Workflow"], ["subsubsection", "Updating the Model"], ["paragraph", "(3) Influence the training process (T)."]], "content": "Another approach is to influence the (re-)training process in a way that the resulting model will behave as the feedback suggests.\nThis approach could be either model-specific (such as attention supervision) or model-agnostic (such as user co-training).\n used human feedback to supervise attention weights of the model.\nSimilarly,  added a loss term to regularize explanations guided by human feedback.\n proposed \n\\textit{(i)} constraint optimization, translating human feedback into constraints governing the training process\nand \\textit{(ii)} \nuser co-training, using feedback as another classifier working together with the main ML model in a semi-supervised learning setting.\n disabled some learned features deemed irrelevant, \nbased on the feedback, and re-trained the model, forcing it to use only the remaining features.\nWith many techniques available, however, there has not been a study testing which technique is more appropriate for which task, domain, or model architecture. The comparison issue is one of the open problems for \\EBHD research (to be discussed in section~\\ref{sec:open_problems}).", "cites": [4869], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes a few distinct approaches (attention supervision, constraint optimization, user co-training) and connects them under the theme of influencing the training process. It provides a basic critical perspective by pointing out the lack of comparative studies across techniques. It also identifies a general abstraction by framing these methods in the broader workflow of model updating. However, the synthesis and abstraction remain somewhat limited, and deeper evaluation or contrast between methods is absent."}}
{"id": "ee7f185f-0b80-4374-931e-1d4296023b81", "title": "Iteration", "level": "subsubsection", "subsections": [], "parent_id": "046f9b18-3513-4d56-82c3-c60190ba0aa9", "prefix_titles": [["title", "Explanation-Based Human Debugging of NLP Models: A Survey"], ["section", "Categorization of Existing Work"], ["subsection", "Workflow"], ["subsubsection", "Iteration"]], "content": "The debugging workflow (explain, feedback, and update) can be done iteratively to gradually improve the model \nwhere the presented explanation changes after the model update. \nThis allows humans to fix vital bugs first and finer bugs in later iterations, as reflected in  via the performance plots.\nHowever, the interactive process could be susceptible to \n\\textit{local decision pitfalls} where local improvements for individual predictions could add up to inferior overall performance .\nSo, we need to ensure that the update in the current iteration is generally favorable and does not overwrite the good effects of previous updates.", "cites": [7516, 7507], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a general discussion of iterative debugging workflows in NLP, mentioning the potential for humans to prioritize bug fixes and the risk of local decision pitfalls. While it introduces a concept relevant to the cited works, it does not deeply synthesize or connect their ideas. It does, however, briefly evaluate a potential limitation of iterative approaches, showing some critical thought and abstraction beyond individual papers."}}
{"id": "5b122951-af49-414e-bead-48a02462c5e7", "title": "Experimental Setting", "level": "subsection", "subsections": [], "parent_id": "c98144b9-7fac-4a44-8376-9fd8e9053e10", "prefix_titles": [["title", "Explanation-Based Human Debugging of NLP Models: A Survey"], ["section", "Categorization of Existing Work"], ["subsection", "Experimental Setting"]], "content": "\\label{subsec:setting}\nTo conduct experiments, some studies in Table~\\ref{tab:papers} selected human participants (SP) to be their feedback providers.\nThe selected participants could be people without ML/NLP knowledge  or with ML/NLP knowledge  depending on the study objectives and the complexity of the feedback process.\nEarly work even conducted experiments with the participants in-person .\nAlthough this limited the number of participants (to less than 100), the researchers could closely observe their behaviors and gain some insights concerning human-computer interaction.\nBy contrast, some used a crowdsourcing platform, Amazon Mechanical Turk\\footnote{\\url{https://www.mturk.com/}} in particular, to collect human feedback for debugging the models.\nCrowdsourcing (CS) enables researchers to conduct experiments at a large scale; however, the quality of human responses could be varying. So, it is important to ensure some quality control such as specifying required qualifications , using multiple annotations per question , having a training phase for participants, and setting up some obvious questions to check if the participants are paying attention to the tasks\n.\nFinally,  simulation (SM), without real humans involved but using oracles as human feedback instead, has also been considered (for the purpose of testing the \\EBHD framework only). \nFor example,  set 20\\% of input words as relevant using feature selection.\nThese were used to respond to \npost-hoc explanations, i.e., top $k$ words selected by LIME.\n simulated mislabeled examples by flipping the labels of a random 10\\% of the training data.\nSo, when the explanation showed suspicious training examples, the true labels could be used to provide feedback.\nCompared to the other settings,\nsimulation is faster and cheaper, yet its results may not reflect the effectiveness of the framework when deployed with real humans. Naturally, human feedback is sometimes inaccurate and noisy, and humans could also be interrupted or frustrated while providing feedback . These factors, discussed in detail in the next section, cannot be thoroughly studied in only simulated experiments.", "cites": [7516], "cite_extract_rate": 0.1111111111111111, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes experimental approaches from multiple studies by grouping them into categories (SP, CS, SM) and connecting them to research objectives and feedback complexity. It critically discusses trade-offs (e.g., scalability vs. quality in crowdsourcing) and limitations of each approach, especially simulation. However, the analysis is somewhat constrained by the lack of deeper meta-level generalization or a novel framework that pulls together broader insights from the cited works."}}
{"id": "21371609-87a7-492e-a772-a56d119a418e", "title": "Willingness", "level": "subsection", "subsections": [], "parent_id": "49eb1249-b5bf-4767-a98f-51e47db0ee54", "prefix_titles": [["title", "Explanation-Based Human Debugging of NLP Models: A Survey"], ["section", "Research on Human Factors"], ["subsection", "Willingness"]], "content": "We would like humans to provide feedback for improving models, but do humans naturally want to?\nPrior to the emerging of \\EBHD, studies found that humans are not willing to be constantly asked about labels of examples as if they were just simple oracles .\nRather, they want to provide more than just data labels after being given explanations .\nBy collecting free-form feedback from users,  and \ndiscovered various feedback types. The most prominent ones include removing-adding features (words), tuning weights, and leveraging feature combinations.\n further analyzed categories of background knowledge underlying the feedback and found, in their experiment, that it was mainly based on commonsense knowledge and English language knowledge. \nSuch knowledge may not be efficiently injected into the model if we exploit human feedback which contains only labels.\nThis agrees with some participants, in , who described their feedback as inadequate when they could only confirm or correct predicted labels.\nAlthough human feedback beyond labels contains helpful information, it is naturally neither complete nor precise. \nobserved that human feedback usually focuses on a few features that are most different from human expectation, ignoring the others.\nAlso, they found that humans, especially lay people, are not good at correcting model\nexplanations quantitatively (e.g., adjusting weights). \nThis is consistent with the findings of\n\nthat human explanations are selective (in a biased \nway) and rarely refer to probabilities but express causal relationships instead.", "cites": [1798], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 7, "insight_result": {"error": "Failed to parse LLM response", "raw_response": "{\n    \"type\": \"analytical\",\n    \"scores\": {\"synthesis\": 4.0, \"critical\": 3.5, \"abstraction\": 4.0},\n    \"insight_level\": \"high\",\n    \"analysis\": \"The section synthesizes insights from multiple studies to build a coherent narrative on human willingness to provide feedback beyond labels in \\EBHD. It critically engages with findings, pointing out limitations such as the imprecision and incompleteness of feedback and the difficulty of quantitative corrections. The discussion abstracts these observati"}}
{"id": "b7c1d9c7-6b32-4bba-98f5-b6169797a125", "title": "Trust", "level": "subsection", "subsections": [], "parent_id": "49eb1249-b5bf-4767-a98f-51e47db0ee54", "prefix_titles": [["title", "Explanation-Based Human Debugging of NLP Models: A Survey"], ["section", "Research on Human Factors"], ["subsection", "Trust"]], "content": "Trust (as well as frustration and expectation, discussed next) is an important issue when the system end users are feedback providers in the \\EBHD framework.\nIt has been discussed widely that explanations engender human trust in AI systems .\nThis trust may be misplaced at times. \nShowing more detailed explanations can cause users to over rely on the system, leading to misuse where users agree with incorrect system predictions .\nMoreover, some users may over trust the explanations (without fully understanding them) only because the tools generating them are publicly available, widely used, and showing appealing visualizations \n.\nHowever, recent research reported that explanations do not necessarily increase trust and reliance.\n\nfound that, even though explanations help users comprehend systems, they cannot increase human trust in using the systems in high-stakes applications involving lots of qualitative factors, such as graduate school admissions.    \n reported that explanations of low-quality models decrease trust and system acceptance as they reveal model weaknesses to the users.\nAccording to , despite correct predictions, the trust still drops if the users see from the explanations that the model relies on the wrong reasons. \nThese studies go along with a perspective by  that explanations should help calibrate user perceptions to the model quality, signaling whether the users should trust or distrust the AI.\nAlthough, in some cases, explanations successfully warned users of faulty models , this is not easy when the model flaws are not obvious . \nBesides explanations, the effect of feedback on human trust is quite inconclusive according to some (but fewer) studies.\nOn one hand, \n found that, after lay humans see explanations of low-quality models and lose their trust, the ability to provide feedback makes human trust and acceptance rally, remedying the situation.\nIn contrast,  reported that providing feedback decreases human trust in the system as well as their perception of system accuracy no matter whether the system truly improves after being updated or not.", "cites": [4876, 7507], "cite_extract_rate": 0.18181818181818182, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes insights from multiple studies to form a cohesive narrative about the role of trust in EBHD systems. It critically evaluates how explanations can both increase and decrease trust, referencing conflicting findings from different works. The discussion abstracts from specific papers to present broader implications, such as the importance of calibrating user perception to model quality."}}
{"id": "4f378e49-98a4-45ef-b353-096c9a7956e2", "title": "Frustration", "level": "subsection", "subsections": [], "parent_id": "49eb1249-b5bf-4767-a98f-51e47db0ee54", "prefix_titles": [["title", "Explanation-Based Human Debugging of NLP Models: A Survey"], ["section", "Research on Human Factors"], ["subsection", "Frustration"]], "content": "Working with explanations can cause frustration \nsometimes.\nFollowing the discussion on trust, explanations of poor models increase user frustration (as they reveal model flaws), whereas the ability to provide feedback reduces frustration. Hence, in general situations, the most frustrating condition is showing explanations to the users without allowing them to give feedback . \nAnother cause of frustration is the risk of detailed explanations overloading users .\nThis is especially a crucial issue for inherently interpretable models where all the internal workings can be exposed to the users.\nThough presenting all the details is comprehensive and faithful, it could create\nbarriers for lay users .\nIn fact, even ML experts may feel frustrated if they need to understand a decision tree with a depth of ten or more.\n found that showing all the model internals undermined users' ability to detect flaws in the model, likely due to information overload. So, they suggested that model internals should be revealed only when the users request to see them.", "cites": [4877], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the key idea from the cited paper regarding how detailed model explanations can lead to frustration due to information overload, integrating it into a discussion about human factors in explanation-based debugging. It provides some analysis on how feedback mechanisms can mitigate frustration, but lacks deeper comparative or evaluative perspectives across multiple works. The abstraction level is moderate, as it identifies a general issue (information overload) but does not present overarching principles or a meta-framework."}}
{"id": "e94edf9e-18ef-40cb-b309-9bfb8acc2417", "title": "Tackling More Challenging Bugs", "level": "subsection", "subsections": [], "parent_id": "02a9b3d3-7c0f-4572-b628-bf3bb8efe52a", "prefix_titles": [["title", "Explanation-Based Human Debugging of NLP Models: A Survey"], ["section", "Open Problems"], ["subsection", "Tackling More Challenging Bugs"]], "content": " remarked that the evaluation setup of existing \\EBHD work is often too easy or unrealistic. For example, bugs are obvious artifacts which could be removed using simple text pre-processing (e.g., removing punctuation and redacting named entities). \nHence, it is not clear how powerful such \\EBHD frameworks are when dealing with real-world  bugs. \nIf bugs are not dominant and happen less often, global explanations may be too coarse-grained to capture them while \nmany local explanations may be needed to spot a few appearances of the bugs, leading to inefficiency.\nAs reported by ,\nfeedback results in minor improvements when the model is already reasonably good. \nOther open problems, whose solutions may help deal with  \nchallenging bugs, include the following.\nFirst, different people may give different feedback for the same explanation. As raised by , how can we integrate their feedback to get robust signals for model update? How should we deal with conflicts among feedback and training examples ?\nSecond, confirming or removing what the model has learned is easier than injecting, into the model, new knowledge (which may not even be apparent in the explanations). How can we use human feedback to inject new knowledge, especially when the model is not transparent?\nLastly, \\EBHD techniques have been proposed for tabular data and image data . Can we adapt or transfer them across modalities to deal with NLP tasks?", "cites": [4867], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"error": "Failed to parse LLM response", "raw_response": "{\n    \"type\": \"analytical\",\n    \"scores\": {\"synthesis\": 3.5, \"critical\": 3.5, \"abstraction\": 3.5},\n    \"insight_level\": \"medium\",\n    \"analysis\": \"The section provides an analytical perspective by identifying limitations of current \\EBHD evaluation setups and discussing challenges in debugging less obvious bugs. It synthesizes observations from cited work to highlight issues such as feedback inconsistency and difficulty in injecting new knowledge. The section also abstracts these challenges into"}}
{"id": "7fe04ab0-d178-4bce-a668-44d36b584bb0", "title": "Analyzing and Enhancing Efficiency", "level": "subsection", "subsections": [], "parent_id": "02a9b3d3-7c0f-4572-b628-bf3bb8efe52a", "prefix_titles": [["title", "Explanation-Based Human Debugging of NLP Models: A Survey"], ["section", "Open Problems"], ["subsection", "Analyzing and Enhancing Efficiency"]], "content": "Most selected studies focus on improving correctness of the model (e.g., by expecting a higher F1 or a lower bias after debugging).\nHowever, \nonly some of them discuss efficiency of the proposed frameworks.\nIn general, we can analyze the efficiency of an \\EBHD framework by looking at the efficiency of each main step in Figure~\\ref{fig:overview}.\nStep 1 generates the explanations, so its efficiency depends on the explanation method used and, in the case of local explanation methods, the number of local explanations needed.\nStep 2 lets humans give feedback, so its efficiency concerns the amount of time they spend to understand the explanations and to produce the feedback.\nStep 3 updates the model using the feedback, so its efficiency relates to the time used for processing the feedback and retraining the model (if needed).\nExisting work mainly reported efficiency of step 1 or step 2.\nFor instance, approaches using example-based explanations \nmeasured the improved performance with respect to the number of explanations computed (step 1) . \n compared the improved F1 of \\EBHD with the F1 of instance labelling given the same amount of time for humans to perform the task (step 2).\nConversely,  compared the time humans need to do \\EBHD versus instance labelling in order to achieve the equivalent degree of correctness improvement (step 2).\nNone of the selected studies considered the efficiency of the three steps altogether. In fact, the efficiency of step 1 and 3 is important especially for black box models where the cost of post-hoc explanation generation and model retraining is not negligible. \nIt is even more crucial for iterative or responsive \\EBHD.\nThus, analyzing and enhancing efficiency of \\EBHD frameworks (for both  machine  and  human sides) require further research.", "cites": [7516, 4868, 4870, 4869], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes multiple papers to analyze the efficiency of EBHD frameworks across three key steps. It provides a critical assessment by highlighting that most studies focus on correctness rather than efficiency and identifies a gap in holistic efficiency evaluation. The abstraction level is strong in identifying broader patterns in how efficiency is measured and the implications for black-box and iterative debugging."}}
