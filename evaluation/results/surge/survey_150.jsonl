{"id": "364ddf17-5f3b-4200-8a2c-e83bb79bf2e5", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "b611e4f1-f132-4230-8c44-06bb4dd9aaf3", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Introduction"]], "content": "\\label{sec:introduction}\nArtificial Intelligence (AI) based algorithms, especially using deep neural networks, are transforming the way we approach real-world tasks done by humans. Recent years have seen a surge in the use of Machine Learning (ML) algorithms in automating various facets of science, business, and social workflow. The surge is partly due to the uptick of research in a field of ML, called Deep Learning (DL), where thousands (even billions) of neuronal parameters are trained to generalize on carrying out a particular task. Successful use of DL algorithms in healthcare , ophthalmology , developmental disorders , in autonomous robots and vehicles , in image processing classification and detection , in speech and audio processing , cyber-security , and many more indicate the reach of DL algorithms in our daily lives. \nEasier access to high-performance compute nodes using cloud computing ecosystems, high-throughput AI accelerators to enhance performance, and access to big-data scale datasets and storage enables deep learning providers to research, test, and operate ML algorithms at scale in small edge devices , smartphones , and AI-based web-services using Application Programming Interfaces (APIs) for wider exposure to any applications.\nThe large number of parameters in Deep Neural Networks (DNNs) make them complex to understand and undeniably harder to interpret. Regardless of the cross-validation accuracy or other evaluation parameters which might indicate a good learning performance, deep learning (DL) models could inherently learn or fail to learn representations from the data which a human might consider important. Explaining the decisions made by DNNs require knowledge of the internal operations of DNNs, missing with non-AI-experts and end-users who are more focused on getting accurate solution. Hence, often the ability to interpret AI decisions are deemed secondary in the race to achieve state-of-the-art results or crossing human-level accuracy. \nRecent interest in XAI, even from governments especially with the European General Data Protection Regulation (GDPR)  regulation, shows the important realization of the ethics , trust , bias  of AI, as well as the impact of adversarial examples  in fooling classifier decisions. \nIn , Miller et al. describes that curiosity is one of the primary reason why people ask for explanations to specific decisions. Another reason might be to facilitate better learning - to reiterate model design and generate better results. Each explanation should be consistent across similar data points and generate stable or similar explanation on the same data point over time . Explanations should make the AI algorithm expressive to improve human understanding, confidence in decision making, and promote impartial and just decisions. Thus, in order to maintain transparency, trust, and fairness in the ML decision-making process, an explanation or an interpretable solution is required for ML systems.\n\\begin{figure*}[!t]\n    \\centering\n    \\footnotesize\n    \\begin{tikzpicture}[\n        level 1/.style={sibling distance=18em},\n        edge from parent/.style={->,draw},\n        >=latex]\n        \\node[root] {XAI}\n        child {node[level 2] (c1) {\\textbf{Scope}: Where is the XAI method focusing on? Is it on a local instance or trying to understand the model as a whole?}}\n        child {node[level 2] (c2) {\\textbf{Methodology}: What is the algorithmic approach? Is it focused on the input data instance or the model parameters?}}\n        child {node[level 2] (c3) {\\textbf{Usage}: How is the XAI method developed? Is it integrated to the model or can be applied to any model in general?}};\n        \\begin{scope}[every node/.style={level 3}]\n            \\node [below of = c1, xshift=15pt] (c11) {\\textbf{Local}: Mainly focus on explanation of individual data instances. Generates one explanation map \\bm{$g$} per data \\bm{$x \\in X$}.};\n            \\node [below of = c11] (c12) {\\textbf{Global}: Tries to understand the model as a whole. Generally takes a group of data instances to generate one or more explanation maps.};\n            \\node [below of = c2, xshift=15pt] (c21) {\\textbf{BackProb}: Core algorithmic logic is dependent on gradients that are backpropagated from the output prediction layer back to the input layer.};\n            \\node [below of = c21] (c22) {\\textbf{Perturbation}: Core algorithmic logic is dependent on random or carefully chosen changes to features in the input data instance.};\n            \\node [below of = c3, xshift=15pt] (c31) {\\textbf{Intrinsic}: Explainability is baked into the neural network architecture itself and is generally not transferrable to other architectures.};\n            \\node [below of = c31] (c32) {\\textbf{Post-Hoc}: XAI algorithm is not dependent on the model architecture and can be applied to already trained neural networks.};\n        \\end{scope}\n        \\foreach \\value in {1,2}\n          \\draw[->] (c1.west) |- (c1\\value.west);\n        \\foreach \\value in {1,2}\n          \\draw[->] (c2.west) |- (c2\\value.west);\n        \\foreach \\value in {1,2}\n          \\draw[->] (c3.west) |- (c3\\value.west);\n    \\end{tikzpicture}\n    \\caption{General categorization of the survey in terms of scope, methodology, and usage.}\n    \\label{fig:surveycategory}\n\\end{figure*}\nAn explanation is a way to verify the output decision made by an AI agent or algorithm. For a cancer detection model using microscopic images, an explanation might mean a map of input pixels which contribute to the model output. For a speech recognition model, an explanation might be the power spectrum information during a specific time which contributed more towards the current output decision. Explanations can be also based on the parameters or activations of the trained models explained either by using surrogates such as decision trees or by using gradients or other methods. In the context of reinforcement learning algorithms, an explanation might be given as to why an agent made a certain decision over another. However, the definitions of interpretable and explainable AI are often generic and might be misleading  and should integrate some form of reasoning . \nA collection of AI models, such as decision-trees and rule-based models, is inherently interpretable. However, there are affected by the drawbacks of Interpretability-versus-Accuracy trade-off compared to the Deep Learning models.  This paper discusses the different approaches and perspectives of researchers to address the problem of the explainability of deep learning algorithms. Methods can be used effectively if the model parameters and architecture are already known. However, modern API-based AI services produce more challenges because of the relative `black-box'  nature of the problem where the end-user has information only on the input provided to the deep learning model and not the model itself. \nIn this survey, we present a comprehensive overview of explainable and interpretable algorithms with a timeline of important events and research publications into three well-defined taxonomies as illustrated in Figure \\ref{fig:surveycategory}. Unlike many other surveys which only categorize and summarize the published research in a high-level, we provide additional mathematical overviews and algorithms of seminal works in the field of XAI. The algorithms presented in the survey are clustered into three well-defined categories which are described in detail in the following sections. Various evaluation techniques for XAI presented in literature are also discussed along with discussion on the limitations and future directions of these methods. \nOur contributions can be summarized as the following:\n\\begin{enumerate}\n    \\item In order to systematically analyze explainable and interpretable algorithms in deep learning, we taxonomize XAI to three well-defined categories to improve clarity and accessibility of the approaches.\n    \\item We examine, summarize and classify the core mathematical model and algorithms of recent XAI research on the proposed taxonomy and discuss the timeline for seminal work.\n    \\item We generate and compare the explanation maps for eight different XAI algorithms, outline the limitations of this approach, and discuss potential future directions to improve trust, transparency, and bias and fairness using deep neural network explanations.\n\\end{enumerate}\nOur survey is based on published research, from the year 2007 to 2020, from various search sources including Google Scholar, ACM Digital Library, IEEEXplore, ScienceDirect, Spinger, and preprints from arXiv. Keywords such as \\textit{explainable artificial intelligence}, \\textit{XAI}, \\textit{explainable machine learning}, \\textit{explainable deep learning}, \\textit{interpretable machine learning} were used as search parameters.", "cites": [6075, 923, 892, 1798, 914, 547, 6076, 8098], "cite_extract_rate": 0.19047619047619047, "origin_cites_number": 42, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates XAI concepts by referencing relevant papers on adversarial examples, human-centered explanations, and deep learning applications, and introduces a three-level taxonomy. It provides a moderate level of synthesis and abstraction, but the critical analysis is somewhat limited to general observations rather than in-depth evaluation of specific cited works."}}
{"id": "949fe65f-49a1-4098-a1da-6e8ec6ed5d52", "title": "Why Is Research on XAI Important?", "level": "subsection", "subsections": [], "parent_id": "ab87aa1b-f3a8-4995-9b22-3c8309380046", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Definitions and Preliminaries"], ["subsection", "Why Is Research on XAI Important?"]], "content": "With the use of AI algorithms in healthcare , credit scoring , loan acceptance , and more, the need to explain an ML model result is important for ethical, judicial, as well as safety reasons. Even though there are different facets to why XAI is important, our study suggests that the most important concerns are three-fold: 1) trustability, 2) transparency, and 3) bias and fairness of AI algorithms. Current business models include interpretation as a step before serving the ML models on production systems, however are often limited to small tree-based models. With the use of highly non-linear deep learning algorithms with millions of parameters in ML pipelines, XAI techniques must improve all three concerns mentioned above.\n\\begin{figure}[!t]\n\\begin{center}\n\\includegraphics[width=0.95\\linewidth]{images/adversarial_example.png}\n\\end{center}\n\\caption{Illustration from  showing an adversarial attack where an image class Panda is deliberately attacked to predict as a Gibbon with high confidence. Note that the attacked image is visually similar to the original image and humans are unable to understand any changes.}\n\\label{fig:adversarial}\n\\end{figure}\n\\begin{figure}[!t]\n\\begin{center}\n\\includegraphics[width=0.95\\linewidth]{images/misclassification_sourcetag.png}\n\\end{center}\n\\caption{Illustration from  showing how text in images can fool classifiers into believing that the text is a feature for a particular task.}\n\\label{fig:sourcetagmiss}\n\\end{figure}\n\\begin{enumerate}\n    \\item \\textbf{Improves Transparency}: XAI improves transparency and fairness by creating a human-understandable justification to the decisions and could find and deter adversarial examples  if used properly. \n    \\textbf{Definition 6: } \\textit{A deep learning model is considered \\textbf{transparent} if it is expressive enough to be human-understandable. Here, transparency can be a part of the algorithm itself or using external means such as model decomposition or simulations.}\n    Transparency is important to assess the quality of output predictions and to ward off adversaries. An adversarial example could hinder accurate decision making capabilities of a classifier by fooling the classifier into believing that a fake image is infact real. Figure \\ref{fig:adversarial} illustrates such an example where an image of a Panda is predicted as a Gibbon with high confidence after the original Panda image was tampered by adding some adversarial noise. Figure \\ref{fig:sourcetagmiss} illustrates a classifier learning to classify based on text data such as source tags or watermarks in advertisements in images. As we rely more on autonomous algorithms to aid our daily lives, quality of AI algorithms to mitigate attacks  and provide transparency in terms of model understanding, textual, or visual reports should be of prime importance. \n    \\item \\textbf{Improves Trust}: As a social animal, our social lives, decisions, and judgements are primarily based on the knowledge and available explanations to situations and the trust we generate. A scientific explanation or logical reasoning for a sub-optimal decision is better than a highly confident decision without any explanations.\n\\begin{figure}[!t]\n\\begin{center}\n\\includegraphics[width=0.99\\linewidth]{images/whyxai.png}\n\\end{center}\n\\caption{Significant expected improvements when using XAI techniques to support decision making of end-users. We believe XAI is important due to improvements in trust, transparency, and in understanding bias and fairness.}\n\\label{fig:whyxai}\n\\end{figure}\n\\begin{figure}[!t]\n\\scriptsize{\n\\begin{timeline}{2020}{2007}{0.5cm}{1cm}{6.5cm}{20.5cm}\n\\entry{2020}{Neural Additive Model, 2020,                       \\textit{}       \\scope{GL}   \\method{Other}  \\usage{IN}}\n\\entry{2019}{Causal Concept Effect (CaCE), 2019,                \\textit{}     \\scope{GL}   \\method{Other}  \\usage{PH}}\n\\entry{2019}{Automatic Concept-based Explanations, 2019,        \\textit{}     \\scope{GL}   \\method{Other}  \\usage{PH}}\n\\entry{2019}{Global Attribution Mapping, 2019,                  \\textit{}       \\scope{GL}   \\method{PER}    \\usage{PH}}\n\\entry{2019}{Spectral Relevance Analysis, 2019,         \\textit{}          \\scope{GL}   \\method{BP}     \\usage{PH}}\n\\entry{2019}{Salient Relevance (SR) Map, 2019,                  \\textit{}                  \\scope{LO}   \\method{BP}     \\usage{PH}}\n\\entry{2019}{Randomization and Feature Testing, 2019,           \\textit{}               \\scope{LO}   \\method{PER}    \\usage{PH}}\n\\entry{2018}{Grad-CAM++, 2018,                                  \\textit{}        \\scope{LO}   \\method{BP}     \\usage{PH}}\n\\entry{2018}{Randomized Input Sampling for Explanation, 2018,   \\textit{}             \\scope{LO}   \\method{PER}    \\usage{PH}}\n\\entry{2018}{Concept Activation Vectors, 2018,                  \\textit{} \\scope{GL}   \\method{Other}  \\usage{PH}}\n\\entry{2017}{Axiomatic Attributions, 2017,                      \\textit{}        \\scope{LO}   \\method{BP}     \\usage{PH}}\n\\entry{2017}{Deep Attribution Maps, 2017,                       \\textit{}              \\scope{LO}   \\method{BP}     \\usage{PH}}\n\\entry{2017}{Deep Taylor Expansion, 2017,                       \\textit{}            \\scope{LO}   \\method{Other}  \\usage{PH}}\n\\entry{2017}{Prediction Difference Analysis, 2017,              \\textit{}            \\scope{LO}   \\method{PER}    \\usage{PH}}\n\\entry{2017}{Grad-CAM, 2017,                                    \\textit{}           \\scope{LO}   \\method{BP}     \\usage{PH}}\n\\entry{2017}{SHapley Additive exPlanations (SHAP), 2017,        \\textit{}            \\scope{Both} \\method{PER}    \\usage{PH}}\n\\entry{2016}{Local Interpretable Model-Agnostic Explanations (LIME), 2016,    \\textit{}             \\scope{Both} \\method{PER}    \\usage{PH}}\n\\entry{2016}{Class Activation Mapping (CAM), 2016,              \\textit{}        \\scope{LO}   \\method{BP}     \\usage{PH}}\n\\entry{2015}{Guided Backprop, 2015,                             \\textit{}        \\scope{LO}   \\method{BP}     \\usage{PH}}\n\\entry{2015}{Bayes Rule List, 2015,                             \\textit{}              \\scope{GL}   \\method{Other}  \\usage{IN}}\n\\entry{2015}{Layer-wise Relevance BackPropagation (LRP), 2015,  \\textit{}                \\scope{Both} \\method{BP}     \\usage{PH}}\n\\entry{2015}{Generalized Additive Models, 2015,           \\textit{}             \\scope{GL}   \\method{Other}  \\usage{IN}}\n\\entry{2014}{DeConvolution Nets, 2014,                          \\textit{}              \\scope{LO}   \\method{BP}     \\usage{PH}}\n\\entry{2014}{Bayesian Case Model, 2014,                         \\textit{}                 \\scope{GL}   \\method{Other}  \\usage{IN}}\n\\entry{2013}{Gradient-based Saliency Maps, 2013,                \\textit{}            \\scope{LO}   \\method{BP}     \\usage{PH}}\n\\entry{2010}{Activation Maximization, 2010,                     \\textit{}               \\scope{LO}   \\method{BP}     \\usage{PH}}\n\\entry{2008}{Sparse Penalized Discriminant Analysis, 2008,      \\textit{}           \\scope{GL}   \\method{Other}  \\usage{IN}}\n\\entry{2007}{Bayesian Averaging, 2007,                          \\textit{}           \\scope{GL}   \\method{Other}  \\usage{IN}}\n\\end{timeline}\n}\n    \\caption{A timeline of seminal works towards explainable AI algorithms is illustrated. The grey highlights indicate \\textbf{scope} (GL: global, LO: local, Both: GL and LO), \\textbf{methodology} (BP: backprop, PER: perturbation, Other: neither BP or PER, and \\textbf{usage} level (IN: intrinsic or PH: post-hoc) of the algorithms. }\n    \\label{fig:timeline}\n\\end{figure}\n\\begin{table}[!t]\n\\caption{Table of Abbreviations}\n\\label{tab:abbrv}\n\\renewcommand{\\arraystretch}{1.15}\n\\centering\n\\begin{tabular}{||l|l||}\n\\hline\nAbbreviation & Definition                                      \\\\ \\hline \\hline\nACE          & Automatic Concept-based Explanations            \\\\ \\hline\nAI           & Artificial Intelligence                         \\\\ \\hline\nAPI          & Application Programming Interface               \\\\ \\hline\nBAM          & Benchmarking Attribution Methods                \\\\ \\hline\nBRL          & Bayesian Rule List                              \\\\ \\hline\nCaCE         & Causal Concept Effect                           \\\\ \\hline\nCAM          & Class Activation Mapping                        \\\\ \\hline\nCAV          & Concept Activation Vectors                      \\\\ \\hline\nCNN          & Convolutional Neural Network                    \\\\ \\hline\nDeConvNet    & Deconvolution Neural Network                    \\\\ \\hline\nDL           & Deep Learning                                   \\\\ \\hline\nDNN          & Deep Neural Network                             \\\\ \\hline\nEG           & Expected Gradients                              \\\\ \\hline\nFMRI         & Functional Magnetic Resonance Imaging           \\\\ \\hline\nGAM          & Generalized Additive Models                     \\\\ \\hline\nIG           & Integrated Gradients                            \\\\ \\hline\nIRT          & Interpretability Randomization Test             \\\\ \\hline\nLIME         & Local Interpretable Model-Agnostic Explanations \\\\ \\hline\nLRP          & Layer-wise Relevance BackPropagation            \\\\ \\hline\nML           & Machine Learning                                \\\\ \\hline\nNAM          & Neural Additive Models                          \\\\ \\hline\nOSFT         & One-Shot Feature Test                           \\\\ \\hline\nReLU         & Rectified Linear Unit                           \\\\ \\hline\nRISE         & Randomized Input Sampling for Explanation       \\\\ \\hline\nRNN          & Recurrent Neural Network                        \\\\ \\hline\nSCS          & System Causability Scale                        \\\\ \\hline\nSHAP         & SHapley Additive exPlanations                   \\\\ \\hline\nSPDA         & Sparse Penalized Discriminant Analysis          \\\\ \\hline\nSpRAy        & Spectral Relevance Analysis                     \\\\ \\hline\nSR           & Salient Relevance                               \\\\ \\hline\nTCAV         & Testing with Concept Activation Vectors         \\\\ \\hline\nt-SNE        & t-Stochastic Neighbor Embedding                 \\\\ \\hline\nVAE          & Variational Auto Encoders                       \\\\ \\hline\nXAI          & Explainable Artificial Intelligence             \\\\ \\hline\n\\end{tabular}\n\\end{table}\n    \\textbf{Definition 7: } \\textit{\\textbf{Trustability} of deep learning models is a measure of confidence, as humans, as end-users, in the intended working of a given model in dynamic real-world environments.}\n     Thus, `Why a particular decision was made' is of prime importance to improve the trust  of end-users including subject matter experts, developers, law-makers, and laypersons alike . Fundamental explanations to classifier prediction is ever so important to stake-holders and government agencies to build trustability as we transition to a connected AI-driven socio-economic environment.\n    \\item \\textbf{Improves Model Bias Understanding and Fairness}: XAI promotes fairness and helps mitigate biases introduced to the AI decision either from input datasets or poor neural network architecture. \n    \\textbf{Definition 8: } \\textit{\\textbf{Bias} in deep learning algorithms indicate the disproportionate weight, prejudice, favor, or inclination of the learnt model towards subsets of data due to both inherent biases in human data collection and deficiencies in the learning algorithm.}\n    Learning the model behavior using XAI techniques for different input data distributions could improve our understanding of the skewness and biases in the input data. This could generate a robust AI model . Understanding the input space could help us invest in bias mitigation methods and promote fairer models. \n    \\textbf{Definition 9: } \\textit{\\textbf{Fairness} in deep learning is the quality of a learnt model in providing impartial and just decisions without favoring any populations in the input data distribution.}\n    XAI techniques could be used as a way to improve the expressiveness and generate meaningful explanations to feature correlations for many subspaces in the data distribution to understand fairness in AI. By tracing back the output prediction discriminations back to the input using XAI techniques, we can understand the subset of features correlated to particular class-wise decisions .\n\\end{enumerate}\nAs we discussed previously, the use of XAI could provide a software-engineering design on AI with a continuously evolving model based on prior parameters, explanations, issues, and improvements to overall design thereby reducing human bias. However, choosing the right methods for explanation should be done with care, while considering to bake-in interpretability to machine learning models . We now proceed with detailed discussions as per the taxonomies.", "cites": [8600, 8099, 6078, 923, 7511, 1816, 737, 892, 7514, 6079, 1824, 8080, 1813, 1812, 6080, 1814, 6081, 7358, 6077], "cite_extract_rate": 0.5, "origin_cites_number": 40, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates several XAI techniques across cited papers and links them to broader themes like transparency, trust, and fairness, showing moderate synthesis and abstraction. While it includes relevant examples and a timeline to contextualize the importance of XAI, the critical analysis is limited, with few evaluations or comparisons of the cited methods' strengths and weaknesses."}}
{"id": "1eedd4a7-8ce1-46c6-86a5-544ea9a9a251", "title": "Saliency Map Visualization", "level": "subsubsection", "subsections": [], "parent_id": "aa4c5382-86e9-484d-bbf6-945abdc14cfb", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Scope of Explanation"], ["subsection", "Local Explanations"], ["subsubsection", "Saliency Map Visualization"]], "content": "\\label{subsec:saliencymap}\nSaliency map generation in deep neural networks were first introduced by Simonyan et al.  as a way of computing the gradient of the output class category with respect to an input image. By visualizing the gradients, a fair summary of pixel importance can be achieved by studying the positive gradients which had more influence to the output. Authors introduced two techniques of visualization: 1) class model visualizations and 2) image-specific class visualizations as illustrated in Figure \\ref{fig:simnonyan_grad}. \nWe discuss class model visualization under the global explainable methods in Section \\ref{subsec:global}. Image-specific class saliency visualization technique tries to find an approximate class score function \\bm{$S_c(I)$}, where \\bm{$x$} is the input image with a label class \\bm{$c$} using first-order Taylor expansion: \n\\begin{equation}\nS_{c}(I) \\approx w^{T} x+b\n\\end{equation}\nwhere $w$ is the derivative of the class score function \\bm{$S_c$} with respect to the input image \\bm{$x$} at a specific point in the image $x_{0}$ such that:\n\\begin{equation}\nw=\\left.\\frac{\\partial S_{c}}{\\partial x}\\right|_{x_{0}}\n\\end{equation}\nHere, with light image processing, we can visualize the saliency map with respect to the location of input pixels with positive gradients.\n\\begin{figure}[!t]\n\\begin{center}\n\\includegraphics[width=0.8\\linewidth]{images/simonyan_gradient_figure.jpg}\n\\end{center}\n  \\caption{Image-specific class saliency maps using gradient based attribution method is shown. Image courtesy .}\n\\label{fig:simnonyan_grad}\n\\end{figure}", "cites": [1814], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic description of saliency map visualization, integrating one cited paper to explain the method. It outlines the mathematical formulation and connects the concept to the broader scope of XAI by referencing related sections. However, there is limited synthesis across multiple sources, no critical evaluation of the approach, and minimal abstraction beyond the specific technique described."}}
{"id": "9b2f3438-c03a-4680-a4ac-f84c6fd1ea31", "title": "Layer-wise Relevance BackPropagation (LRP)", "level": "subsubsection", "subsections": [], "parent_id": "aa4c5382-86e9-484d-bbf6-945abdc14cfb", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Scope of Explanation"], ["subsection", "Local Explanations"], ["subsubsection", "Layer-wise Relevance BackPropagation (LRP)"]], "content": "\\label{subsec:LRP}\nLRP technique introduced in 2015 by Bach et al.  is used to find relevance scores for individual features in the input data by decomposing the output predictions of the DNN.\nThe relevance score for each atomic input is calculated by backpropagating the class scores of an output class node towards the input layer. The propagation follows a strict conservation property whereby a equal redistribution of relevance received by a neuron must be enforced. In CNNs, LRP backpropagates information regarding relevance of output class back to input layer, layer-by-layer. In Recurrent Neural Networks (RNNs), relevance is propagated to hidden states and memory cell. Zero relevance is assigned to gates of the RNN. If we consider a simple neural network with input instance \\bm{$x$}, a linear output \\bm{$y$}, and activation output \\bm{$z$}, the system can be described as:\n\\begin{equation}\n\\begin{array}{l}\ny_{j}=\\sum_{i} w_{i j} x_i +b_{j} \\\\\nz_{j}=f\\left(y_{j}\\right)\n\\end{array}\n\\end{equation}\nIf we consider \\bm{$R(z_j)$} as the relevance of activation output, the goal is to get \\bm{$R_{i\\leftarrow j}$}, that is to distribute \\bm{$R(z_j)$} to the corresponding input \\bm{$x$}:\n\\begin{equation}\nR_{i \\leftarrow j}=R\\left(z_{j}\\right) \\frac{x_i w_{i j}}{y_{j}+\\epsilon \\operatorname{sign}\\left(y_{j}\\right)}\n\\end{equation}\nFinal relevance score of individual input \\bm{$x$} is the summation of all relevance from \\bm{$z_j$} for input \\bm{$x_i$}:\n\\begin{equation}\nR\\left(x\\right)=\\sum_{j} R_{i \\leftarrow j}\n\\end{equation}\nThe LRP method have been recently extended to learn the global explainability by using LRP explanation maps as an input to global attribution algorithms. We discuss some such models in section \\ref{subsec:global}. Newer research has also shown the importance of using methods such as LRP for model specific operations such as network pruning . Here, authors prune the least important weights or filters of a model by understanding the feature attributions of individual layer. This reduces the computation and storage cost of the AI models without significant drop in the model accuracy. This shows another aspect of using AI in understanding the model behavior and utilizing the new knowledge to improve model performance.", "cites": [6082], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from the cited paper to explain how LRP is used in model-specific tasks like pruning, integrating this with the general concept of LRP. It provides some abstraction by highlighting the broader utility of LRP in improving model efficiency and understanding. However, it lacks deeper critical analysis of LRP's limitations or comparisons with alternative methods for local explanations."}}
{"id": "916fac85-4547-41d0-8b25-e4521fdbec50", "title": "Local Interpretable Model-Agnostic Explanations (LIME)", "level": "subsubsection", "subsections": [], "parent_id": "aa4c5382-86e9-484d-bbf6-945abdc14cfb", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Scope of Explanation"], ["subsection", "Local Explanations"], ["subsubsection", "Local Interpretable Model-Agnostic Explanations (LIME)"]], "content": "\\label{subsec:limelocal}\nIn 2016, Ribeiro et al. introduced Local Interpretable Model-Agnostic Explanations (LIME) . To derive a representation that is understandable by humans, LIME tries to find importance of contiguous superpixels (a patch of pixels) in a source image towards the output class. Hence, LIME finds a binary vector \\bm{$x^{'} \\in \\{0,1\\}$} to represent the presence or absence of a continuous path or 'superpixel' that provides the highest representation towards class output. This works on a patch-level on a single data input. Hence, the method falls under local explanations. There is also a global explanation model based on LIME called SP-LIME described in the global explainable model sub section. Here, we focus on local explanations.\n\\begin{algorithm}[!b]\n    \\small\n    \\caption{LIME algorithm for local explanations}\n    \\label{alg:limelocal}\n\\begin{algorithmic}[1]\n    \\Statex \\textbf{Input:} classifier \\bm{$f$}, input sample \\bm{$x$}, number of superpixels \\bm{$n$}, number of features to pick \\bm{$m$}\n    \\Statex \\textbf{Output:} explainable coefficients from the linear model\n    \\State \\bm{$\\Bar{y} \\leftarrow f.\\texttt{predict(}x\\texttt{)}$}\n    \\For {i in \\bm{$n$}}\n        \\State \\bm{$p_i \\leftarrow \\texttt{Permute(}x\\texttt{)}$} \\Comment{Randomly pick superpixels}\n        \\State \\bm{$obs_i \\leftarrow f.\\texttt{predict(}p\\texttt{)}$}\n        \\State \\bm{$dist_i \\leftarrow |\\Bar{y}-obs_i|$}\n    \\EndFor\n    \\State \\bm{$simscore \\leftarrow  \\texttt{SimilarityScore(}dist\\texttt{)}$}\n    \\State \\bm{$x_{pick} \\leftarrow \\texttt{Pick(}p, simscore, m\\texttt{)}$}\n    \\State \\bm{$L \\leftarrow \\texttt{LinearModel.fit(}p,m,simscore\\texttt{)}$}\n    \\State return \\bm{$L$}.weights\n\\end{algorithmic}\n\\end{algorithm}\n\\begin{figure*}[!t]\n\\begin{center}\n\\includegraphics[width=0.8\\linewidth]{images/lime_example.png}\n\\end{center}\n  \\caption{Local explanations of an image classification prediction described using LIME . Here, top three classes are \"electric guitar\" $(p=0.32)$, \"acoustic guitar\" $(p=0.24)$ and \"labrador\" $(p=0.21)$. By selecting a group of `superpixels' from the input image, the classifier provides visual explanations to the top predicted labels.}\n\\label{fig:lime}\n\\end{figure*}\nConsider \\bm{$g \\in G$}, the explanation as a model from a class of potentially interpretable models \\bm{$G$}. Here, \\bm{$g$} can be decision trees, linear models, or other models of varying interpretability. \nLet explanation complexity be measured by \\bm{$\\Omega(g)$}. \nIf \\bm{$\\pi_x(z)$} is a proximity measure between two instances \\bm{$x$} and $z$ around \\bm{$x$}, and \\bm{$\\mathcal{L}(f, g, \\pi_x)$} represents faithfulness of \\bm{$g$} in approximating \\bm{$f$} in locality defined by \\bm{$\\pi_x$}, then, explanation \\bm{$\\EModel$} for the input data sample \\bm{$x$} is given by the LIME equation:\n\\begin{equation}\\label{eq:limeeq}\n\\EModel(x) = \\argmin_{g \\in G}\\;\\;\\mathcal{L}(f, g, \\pi_x) + \\Omega(g)\n\\end{equation}\nNow, in Equation \\ref{eq:limeeq}, the goal of LIME optimization is to minimize the locality-aware loss \\bm{$\\mathcal{L}(f, g, \\pi_x)$} in a model agnostic way. Example visualization of LIME algorithm on a single instance is illustrated in Figure \\ref{fig:lime}. Algorithm \\ref{alg:limelocal} shows the steps to explain the model for a single input sample and the overall procedure of LIME. Here, for the input instance we permute data by finding a superpixel of information (`fake' data). Then, we calculate distance (similarity score) between permutations and original observations. Now, we know how different the class scores are for the original input and the new `fake' data.\nWe can then make predictions on new `fake' data using the complex model $f$. This depends on the amount of superpixels you choose from the original data. The most descriptive feature can be picked which improved prediction on the permuted data. If we fit a simple model, often times a locally weighted regression model, to the permuted data with \\bm{$m$} features and similarity scores as weights, we can use the feature weights, or coefficients, from the simple model to make explanations for the local behavior of the complex model. Recent years have seen many research improving and extending the LIME algorithm to a variety of new tasks. We summarize a few of them below:\n\\begin{itemize}\n    \\item In , Mishra et al. extended LIME algorithm to music content analysis by temporal segmentation, and frequency and time-frequency segmentation of input mel-spectogram. Their approach was called Sound-LIME (SLIME) and was applied to explain the predictions of a deep vocal detector. \n    \\item In , Tomi Peltola described a Kullbackâ€“Leibler divergence based LIME called KL-LIME to explain Bayesian predictive models. Similar to LIME, the explanations are generated using an interpretable model, whose parameters are found by minimizing the KL-divergence from the predictive model. Thus, local interpretable explanations are generated by projecting information from the predictive distribution to a simpler interpretable probabilistic explanation model.\n    \\item In , Rehman et al. used agglomerative Hierarchical Clustering (HC) and K-Nearest Neighbor (KNN) algorithms to replace the random perturbation of the LIME algorithm. Here, authors use the HC method to group training data together as clusters and the KNN is used to find closest neighbors to a test instance. Once the KNN picks a cluster, that cluster is passed as the input data perturbation instead of a random perturbation as in LIME algorithm. Authors report that their approach generates model explanations which are more stable than traditional LIME algorithm.\n    \\item In , Bramhall et al. adjusted the linear relations of LIME to consider non-linear relationships using a quadratic approximation framework called Quadratic-LIME (QLIME). They achieve this by considering the linear approximations as tangentials steps within a complex function. Results on a global staffing company dataset suggests that the mean square loss (MSE) of LIME's linear relationship at local level improves while using QLIME. \n    \\item In , Shi et al. introduced a replacement method to pick superpixels of information for image data using Modified Perturbed Sampling operation for LIME (MPS-LIME). Authors converted the traditional superpixel picking operation into a clique set construction problem by converting the superpixels to an undirected graph. The clique operation improves the runtime due to a considerable reduction in the number of perturbed samples in the MPS-LIME method. Authors compared their method with LIME using Mean Absolute Error (MAE) and Coefficient of determination $R^2$ and reported better results in terms of understandability, fidelity, and efficiency.\n\\end{itemize}", "cites": [6083], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a clear synthesis of the LIME method and its extensions, connecting multiple cited papers to illustrate variations and improvements in the algorithm. It includes critical points, such as the 'defective' sampling in standard LIME, and discusses how different approaches address specific limitations. However, while it identifies patterns (e.g., perturbation-based improvements), it stops short of offering deeper abstraction or overarching theoretical insights about the implications of these variations for the XAI field as a whole."}}
{"id": "9efa9ced-1d9c-4007-94db-e8193a67a540", "title": "SHapley Additive exPlanations (SHAP)", "level": "subsubsection", "subsections": [], "parent_id": "aa4c5382-86e9-484d-bbf6-945abdc14cfb", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Scope of Explanation"], ["subsection", "Local Explanations"], ["subsubsection", "SHapley Additive exPlanations (SHAP)"]], "content": "A game theoretically optimal solution using Shapley values for model explainability was proposed by Lundberg et al. . SHAP explains predictions of an input \\bm{$x$} by computing individual feature contributions towards that output prediction. By formulating the data features as players in a coalition game, Shapley values can be computed to learn to distribute the payout fairly. \nIn SHAP method, a data feature can be individual categories in tabular data or superpixel groups in images similar to LIME. SHAP then deduce the problem as a set of linear function of functions where the explanation is a linear function of features . If we consider \\bm{$g$} as the explanation model of an ML model \\bm{$f$}, \\bm{$z'\\in\\{0,1\\}^M$} as the coalition vector, \\bm{$M$} the maximum coalition size, and $\\boldmath{\\phi_j\\in\\mathbb{R}}$ the feature attribution for feature \\bm{$j$}, \\bm{$g(z')$} is the sum of bias and individual feature contributions such that:\n\\begin{equation} \\label{eq:shap}\n    g(z')=\\phi_0+\\sum_{j=1}^M\\phi_jz_j'\n\\end{equation}\nLundberg et al.  further describes several variations to the baseline SHAP method such as KernelSHAP which reduces evaluations required for large inputs on any ML model, LinearSHAP which estimates SHAP values from a linear model's weight coefficients given independent input features, Low-Order SHAP which is efficient for small maximum coalition size \\bm{$M$}, and DeepSHAP which adapts DeepLIFT method  to leverage the compositional nature of deep neural networks to improve attributions. Since KernelSHAP is applicable to all machine learning algorithms, we describe it in Algorithm \\ref{alg:kernalshap}. The general idea of KernelSHAP is to carry out an additive feature attribution method by randomly sampling coalitions by removing features from the input data and linearizing the model influence using SHAP kernels.\n\\begin{algorithm}[!b]\n    \\small\n    \\caption{KernelSHAP Algorithm for Local Explanations}\n    \\label{alg:kernalshap}\n\\begin{algorithmic}[1]\n    \\Statex \\textbf{Input:} classifier \\bm{$f$}, input sample \\bm{$x$}\n    \\Statex \\textbf{Output:} explainable coefficients from the linear model\n    \\State \\bm{$z_k \\leftarrow \\text{SampleByRemovingFeature}(x)$}\n    \\State \\bm{$z_k \\leftarrow h_x(z_k) $} \\Comment{$h_x$ is a feature transformation to reshape to $x$}\n    \\State \\bm{$y_k \\leftarrow f(z_k) $}\n    \\State \\bm{$W_x \\leftarrow SHAP(f,z_k, y_k)$}\n    \\State \\bm{$\\texttt{LinearModel(}W_x\\texttt{).fit()}$}\n    \\State Return \\texttt{LinearModel.coefficients()}\n\\end{algorithmic}\n\\end{algorithm}\nSHAP was also explored widely by the research community, was applied directly, and improved in many aspects. Use of SHAP in the medical domain to explain clinical decision-making and some of the recent works which have significant merits are summarized here:\n\\begin{itemize}\n    \\item In , Antwarg et al. extended SHAP method to explain autoencoders used to detect anomalies. Authors classify anomalies using the autoencoder by comparing the actual data instance with the reconstructed output. Since the final output is a reconstruction, authors suggests that the explanations should be based on the reconstruction error. SHAP values are found for top performing features and were divided into those contributing to and offsetting anomalies.\n    \\item In , Sundararajan et al. express various disadvantages of SHAP method such as generating counterintuitive explanations for cases where certain features are not important. This `uniqueness' property of attribution method is improved using Baseline Shapley (BShap) method. Authors further extend the method using Integrated Gradients to the continuous domain.\n    \\item In , Aas et al. explored the dependence between SHAP values by extending KernelSHAP method to handle dependent features. Authors also presented a method to cluster Shapley values corresponding to dependent features. A thorough comparison of the KernelSHAP method was carried out with four proposed methods to replace the conditional distributions of KernelSHAP method using empirical approach and either the Gaussian or the Gaussian copula approaches.\n    \\item In , Lundberg et al. described an extension of SHAP method for trees under a framework called TreeExplainer to understand the global model structure using local explanations. Authors described an algorithm to compute local explanation for trees in polynomial time based on exact Shapley values. \n    \\item In , VegaGarcia et al. describe a SHAP-based method to explain the predictions of time-series signals involving Long Short-Term Memory (LSTM) networks. Authors used DeepSHAP algorithm to explain individual instances in a test set based on the most important features from the training set. However, no changes in the SHAP method was done, and explanations were generated for each time step of each input instances.\n\\end{itemize}", "cites": [1824, 6085, 1813, 6084], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple papers to provide a coherent overview of SHAP and its variations, such as KernelSHAP, TreeExplainer, and DeepSHAP. It critically discusses limitations and improvements, particularly referencing Sundararajan et al. and Aas et al. on issues like counterintuitive explanations and dependent features. While it offers some abstraction by framing SHAP as a general attribution method, it primarily focuses on concrete applications and methodological extensions without reaching deeper meta-level insights."}}
{"id": "17ffd7db-b58e-49fc-8278-7ed884efaa18", "title": "Class Model Visualization", "level": "subsubsection", "subsections": [], "parent_id": "775909e0-66a6-4b4d-8ae5-878cad025e03", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Scope of Explanation"], ["subsection", "Global Explanations"], ["subsubsection", "Class Model Visualization"]], "content": "Activation maximization  introduced in Section \\ref{subsec:actmax} can be also expanded as a global method using Class Model Visualization as described by Simonyan et al. . Here, a given a trained ConvNet \\bm{$f$} and a class of interest \\bm{$c$}, the goal is to generate image visualizations \\bm{$I'$} which is representative of $c$. This is based on the scoring methods used to train \\bm{$f$} which maximizes the class probability score \\bm{\\bm{$S_c(I)$}} for \\bm{$c$}, such that:\n\\begin{equation}\nI^{'} = \\arg \\max _{I} S_{c}(I)-\\lambda\\|I\\|_{2}^{2}\n\\end{equation}\nThus, the generated images provides insight to what the blackbox model had learnt for a particular class in the dataset. Images generated using this technique is often called `deep dream' due to the colorful artefacts generated in the visualizations corresponding to the output class under consideration. Figure \\ref{fig:simonyan-class-model} illustrates three numerically computed class appearance models learnt by a CNN model for \\textit{goose}, \\textit{ostrich}, and \\textit{limousine} classes respectively.\n\\begin{figure}[!t]\n\\begin{center}\n\\includegraphics[width=\\linewidth]{images/simonyan_class_model_viz.jpg}\n\\end{center}\n  \\caption{Numerically computed images from  which uses the class-model visualization method to generate images representing the target class mentioned in the illustration.}\n\\label{fig:simonyan-class-model}\n\\end{figure}", "cites": [1814], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the class model visualization technique introduced by Simonyan et al., including a mathematical formulation and a reference to the term 'deep dream.' However, it lacks deeper synthesis of ideas from multiple sources, critical evaluation of the method's strengths or weaknesses, and broader abstraction to connect this technique with other global explanation methods or overarching XAI principles."}}
{"id": "053f0e23-3633-4725-a89c-1f27bb0a483a", "title": "Concept Activation Vectors (CAVs)", "level": "subsubsection", "subsections": [], "parent_id": "775909e0-66a6-4b4d-8ae5-878cad025e03", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Scope of Explanation"], ["subsection", "Global Explanations"], ["subsubsection", "Concept Activation Vectors (CAVs)"]], "content": "\\label{subsec:tcav}\nIn , Kim et al. introduced Concept Activation Vectors (CAVs), a global explainability method to interpret the internal states of a neural network in human-friendly concept domain. Here, if we consider the machine learning model \\bm{$f(.)$} as a vector space \\bm{$E_m$} spanned by basis vector \\bm{$e_m$}, we see that human understanding can be modelled as vector space \\bm{$E_h$} and implicit vectors \\bm{$e_h$} which correspond to human-understandable concepts \\bm{$C$}. Hence, the explanation function of the model in a global sense, \\bm{$g$}, becomes \\bm{$g: E_m \\rightarrow E_h$}.\nNow, human understandable concepts are generated from either input features of training data or user-provided data to simplify the lower-level features of the input domain. For example, a zebra can be deduced to positive concepts \\bm{$P_C$} such as stripes as illustrated in Figure \\ref{fig:tcav}. A negative set of concepts, $N$, can be gathered, for example a set of random photos, to contrast the concepts for zebra. Layer activations for layer \\bm{$j$} of \\bm{$f$}, \\bm{$z_j$} is calculated for both positive and negative concepts. The set of activations are trained using a binary classifier to distinguish between: \n\\bm{$\\{f_j(\\bm{x}) : \\bm{x} \\in P_C\\}$} and\n\\bm{$\\{f_j(\\bm{x}) : \\bm{x} \\in N\\}$}.\n\\begin{figure*}[ht]\n\\begin{center}\n\\includegraphics[width=0.9\\linewidth]{images/tcav.jpg}\n\\end{center}\n  \\caption{Figure illustrates the TCAV process  where (a) describe random concepts and examples, (b) labelled examples from training data, (c) trained neural network, (d) linear model segregating the activations extracted from specific layers in the neural network for the concepts and random examples, and (e) finding conceptual sensitivity using directional derivatives.}\n\\label{fig:tcav}\n\\end{figure*}\nAuthors proposed a new method, Testing with CAVs (TCAV), which uses directional derivatives similar to gradient based methods to evaluate the sensitivity of class predictions of \\bm{$f$} to the changes in given inputs towards the direction of the concept \\bm{$C$} for a specific layer \\bm{$j$}. If \\bm{$h(j,k)$} is the logit of layer \\bm{$j$} for class \\bm{$k$} for a particular input, conceptual sensitivity of a class \\bm{$k$} to \\bm{$C$} can be computed as directional derivative \\bm{$S_{C,k,j}(\\bm{x})$} for a concept vector \\bm{$\\bm{v}_C^j \\in \\mathbb{R}^m$}:\n\\begin{eqnarray}\nS_{C,k,j}(\\bm{x}) & = &\n\\lim\\limits_{\\epsilon \\rightarrow 0} \\frac{h_{j, k}(z_j(\\bm{x}) + \\epsilon \\bm{v}_C^j) - h_{j,k}(z_j(\\bm{x}))}{\\epsilon} \n\\nonumber \\\\\n& = & \\nabla h_{j,k}(z_j(\\bm{x})) \\cdot \\bm{v}_C^j ,\n\\vspace*{-2ex}\n\\end{eqnarray}\nA TCAV score can be calculated to find the influence of inputs towards \\bm{$C$}. If \\bm{$X_k$} denotes all inputs with label \\bm{$k$}, TCAV score is given by:\n\\begin{equation}\n TCAV_{C, k, j} = \\frac{\\left\\lvert \\left\\{ \n                    \\bm{x}\\in X_k : \n                    S_{C,k,j}(\\bm{x}) > 0 \n                    \\right\\} \\right\\rvert}\n                   {\\left\\lvert X_k \\right\\rvert}\n\\end{equation}\nTCAV unfortunately could generate meaningless CAVs if the input concepts are not picked properly. For example, input concepts generated randomly would inherently generate bad linear models for binary classification and thus TCAV score wouldn't be a good identifier for global explainability. Also, concepts with high correlations or shared objects in the data, such as cars and roads, could decrease the efficiency of TCAV method. Human bias in picking the concepts also is a considerable disadvantage of using concepts for explainability. The CAV method was further improved in numerous research papers which involved the primary author of CAV :\n\\begin{itemize}\n    \\item In , Ghorbani et al. described a method called  Automatic Concept-based Explanations (ACE) to globally explain a trained classifier without human supervision unlike TCAV method. Here, authors carry out a multi-resolution segmentation of instance to be explained. This generates multiple resolution segments from the same class. All segments are reshaped to similar input sizes and activations of each segment is found with respect to a specific chosen bottleneck layer. Clustering the activations and removing outliers reveals similarities within activations. \n    TCAV scores of individual concepts provide an importance score of the same for particular classification. Authors carried out human subject experiments to evaluate their method and found inspiring results. One research question that arise is the importance of clusters in decision-making. Authors showed that, by stitching the clustered concepts together as an image, a trained InceptionV3 deep neural network was capable of classifying the stitched image as the correct class category. This tends to show that the extracted concepts are suitable for decision-making within the deep learning model.\n    \\item Work done by Goyal et al.  improved TCAV method by proposing a Causal Concept Effect (CaCE) model which looks at the causal effect of presence or absence of high-level concepts towards deep learning model's prediction. Methods such as TCAVs can suffer from confounding of concepts which could happen if the training data instances have multiple classes in them, even with low correlation between the classes. Also, biases in dataset could influence concepts, as well as colors in the input data.\n    CaCE can be computed exactly if we can change concepts of interest by intervening the counterfactual data generation. Authors call this Ground truth CaCE (GT-CaCE) and also elaborate a way to estimate CaCE using Variational Auto Encoders (VAEs) called VAE-CaCE. Experimental results on four datasets suggest improved clustering and performance of the CaCE method even when there are biases or correlations in the dataset.\n    \\item In , Yeh et al. introduced ConceptSHAP to define an importance or ``completeness\" score for each discovered concept. Similar to ACE method mentioned earlier, one of the aims of ConceptSHAP is to have concepts consistently clustered to certain coherent spatial regions. However, ConceptSHAP finds the importance of each individual concepts with high completeness score from a set of $m$ concept vectors \\bm{$C_s = \\{c_1, c_2, \\ldots, c_m\\}$} by utilizing Shapley values for importance attribution.\n\\end{itemize}", "cites": [1816, 8080], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section integrates and explains the concept of CAVs and TCAV, while also connecting it to subsequent improvements such as ACE and CaCE. It critically evaluates limitations of TCAV, such as the impact of random or correlated concepts and human bias. The content moves beyond a mere description by highlighting broader implications and methodological shortcomings, but it does not fully abstract to overarching principles of XAI."}}
{"id": "c563c15e-5796-47fc-84d1-d2ed2ddd8b81", "title": "Spectral Relevance Analysis (SpRAy)", "level": "subsubsection", "subsections": [], "parent_id": "775909e0-66a6-4b4d-8ae5-878cad025e03", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Scope of Explanation"], ["subsection", "Global Explanations"], ["subsubsection", "Spectral Relevance Analysis (SpRAy)"]], "content": "\\label{subsec:spray}\nSpRAy technique by Lapuschkin et al.  builds on top of the local instance based LRP explanations. In specific, authors described a spectral clustering algorithm on local explanations provided by LRP to understand the decision-making process of the model globally. By analyzing the spatial structure of frequently occurring attributions in LRP instances, SpRAy identifies normal and abnormal behavior of machine learning models.\nAlgorithm \\ref{alg:spary} explains the SpRAy technique in detail. We start by finding local relevance map explanations to every individual data instances $x \\in X$ using LRP method. The relevance maps are downsized to uniform shape and size to improve computation overhead and generate tractable solutions. Spectral cluster analysis (SC) is carried out on the LRP attribution relevance maps to cluster the local explanations in a high-dimensional space. An eigenmap analysis is carried out to find relevant clusters by finding the eigengap (difference in two eigenvalues) of successive clusters. After completion, important clusters are returned to users. The clusters can be optionally visualized using t-Stochastic Neighbor Embedding (t-SNE) visualizations.\n\\begin{algorithm}[ht]\n    \\small\n    \\caption{SpRAy Analysis Algorithm on LRP Attributions}\n    \\label{alg:spary}\n\\begin{algorithmic}[1]\n    \\Statex \\textbf{Input:} classifier \\bm{$f$}, input samples \\bm{$x^{(1)}, \\ldots, x^{(n)}$}\n    \\Statex \\textbf{Output:} clustered input samples\n    \\For {\\bm{$x^{(i)} \\in X$}}\n        \\State \\bm{$f_{SpRAy} \\leftarrow LRP(f, x^{(i)})$}\n    \\EndFor\n    \\State \\texttt{Reshape} \\bm{$f_{SpRAy}$}\n    \\State \\bm{$clusters \\leftarrow SC(f_{SpRAy})$}\n    \\State \\bm{$clusters^* \\leftarrow \\texttt{EigenMapAnalysis(} clusters \\texttt{)}$ }\n    \\State \\texttt{Return} \\bm{$clusters^*$}\n    \\State \\texttt{Optional: Visualize t-SNE(}\\bm{$clusters^*$}\\texttt{)}\n\\end{algorithmic}\n\\end{algorithm}", "cites": [7514], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a clear, step-by-step description of the SpRAy technique and its algorithm, but it largely summarizes the method without connecting it to broader themes or contrasting it with other XAI approaches. There is minimal critical evaluation of the method or the cited work, and while some generalization is attempted (e.g., 'global explanations'), the discussion remains focused on the specifics of SpRAy rather than offering deeper abstraction or synthesis."}}
{"id": "ba3f1174-d379-4a59-b391-fdd65884bb9e", "title": "Global Attribution Mapping", "level": "subsubsection", "subsections": [], "parent_id": "775909e0-66a6-4b4d-8ae5-878cad025e03", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Scope of Explanation"], ["subsection", "Global Explanations"], ["subsubsection", "Global Attribution Mapping"]], "content": "When features have well defined semantics, we can treat attributions as weighted conjoined rankings  with each feature as a rank vector \\bm{$\\sigma$}. After finding local attributions, global attribution mapping finds a pair-wise rank distance matrix and cluster the attribution by minimizing cost function of cluster distances. This way, global attribution mapping can identify differences in explanations among subpopulations within the clusters which can trace the explanations to individual samples with tunable subpopulation granularity.", "cites": [6081], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the concept of global attribution mapping from the cited paper, explaining its methodology and purpose in an integrated manner. It abstracts the idea to describe how it can reveal subpopulation differences in explanations, suggesting a broader understanding of XAI applicability. However, it lacks deeper critical analysis or limitations of the approach, and while it connects to the paper's contribution, it does not offer a novel framework or meta-level insights."}}
{"id": "1cd29707-b96e-439f-98f6-d32b66e2f282", "title": "Neural Additive Models (NAMs)", "level": "subsubsection", "subsections": [], "parent_id": "775909e0-66a6-4b4d-8ae5-878cad025e03", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Scope of Explanation"], ["subsection", "Global Explanations"], ["subsubsection", "Neural Additive Models (NAMs)"]], "content": "\\label{subsec:nam}\nIn , Agarwal et al. introduced a novel method to train multiple deep neural networks in an additive fashion such that each neural network attend to a single input feature. Built as an extension to generalized additive models (GAM), NAM instead use deep learning based neural networks to learn non-linear patterns and feature jumping which traditional tree-based GAMs cannot learn. NAMs improved accurate GAMs introduced in  and are scalable during training to several GPUs. \nConsider a general GAM of the form:\n\\begin{equation}\\label{eq:GAM}\ng(\\mathbb{E}[y])=\\beta+f_{1}\\left(x_{1}\\right)+f_{2}\\left(x_{2}\\right)+\\cdots+f_{K}\\left(x_{K}\\right)\n\\end{equation}\nwhere \\bm{$f_i$} is a univariate shape function with \\bm{$\\mathbb{E}[f_i] = 0$}, \\bm{$x \\in x_1, x_2, \\ldots, x_K$} is the input with K features, \\bm{$y$} is the target variable, and \\bm{$g(.)$} is a link function. NAMs can be generalized by parameterizing the functions \\bm{$f_i$} with neural networks with several hidden layers and neurons in each layer. We can see individual neural networks applied to each features $x_i$. The outputs of each $f_i$ is combined together using a summing operation before applying an activation. A high-level diagram of NAM is provided in \\ref{fig:nam} taken from the source paper. \n\\begin{figure}[!ht]\n\\begin{center}\n\\includegraphics[width=0.9\\linewidth]{images/nam.png}\n\\end{center}\n  \\caption{A high-level diagram of the interpretable NAM architecture for binary classification is illustrated . Functions $f_i$ is used to learn from corresponding. individual features in $x_i$.}\n\\label{fig:nam}\n\\end{figure}\nAuthors proposed exp-centered (ExU) hidden units to overcome the failure of ReLU activated neural networks with standard initializations to fit jagged functions. NAMs should be able to learn jagged functions due to sharp changes in features in real-world datasets often encountered in GAMs. For ExU hidden units, the unit function can be calculated as \\bm{$h(x) = f(e^w*(x-b))$}, where \\bm{$x$}, \\bm{$w$}, and \\bm{$b$} are the inputs, weights, and biases parameters. Authors used a weight initialization of training from a normal distribution \\bm{$\\mathcal{N}(x, 0.5)$} with \\bm{$x \\in[3,4]$}. This globally explainable model provides average score of shape functions of individual neural networks to provide interpretable contributions of each features as positive and negative values. Negative values reduce the class probability while positive values improve the same.\nNAM is an interesting architecture because we can generate exact explanations of each feature space with respect of an output prediction. Newer research could open up venues to expand the ideas to CNNs and for other domains such as text.", "cites": [6079], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a basic description of Neural Additive Models (NAMs) and their relation to generalized additive models (GAMs), but it lacks deeper synthesis of ideas from multiple sources. It includes minimal critical analysis, merely describing the method without evaluating its strengths, weaknesses, or comparing it with alternatives. There is a small abstraction by noting the potential of NAMs for interpretability, but no broader theoretical or conceptual insights are drawn."}}
{"id": "9c01b37d-ff71-4d51-8619-13635152d7ed", "title": "Perturbation-Based", "level": "subsection", "subsections": ["fef0668e-c289-4df6-8cc9-cdf054889559", "45e15029-7ec9-4be0-b1ed-c49b515761c8", "ba44e3a6-d676-4480-85cf-116b36895954", "bc548633-c8e4-4fac-8716-505bd9d6060b"], "parent_id": "4633735f-c90c-4dc1-ba1e-fb1bd29a6f96", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Differences in the Methodology"], ["subsection", "Perturbation-Based"]], "content": "Explanations generated by iteratively probing a trained machine learning model with different variations of the inputs generally fall under perturbation based XAI techniques. These perturbations can be on a feature level by replacing certain features by zero or random counterfactual instances, picking one or group of pixels (superpixels) for explanation, blurring, shifting, or masking operations, etc. As we discussed in the prior sections, LIME algorithm works on superpixels of information or features as illustrated in Figure \\ref{fig:lime}. By iteratively providing input patches, visual explanations of individual superpixels are generated. SHAP has a similar method of probing feature correlations by removing features in a game theoretic framework. Intuitively, we see that methods trying to understand neuronal activities and the impact of individual features to a corresponding class output by any input perturbations mentioned above can be categorized as a group of method, which we here call \\textbf{perturbation-based} XAI method. The methods described in this section are further summarized in Table \\ref{tab:perturbationmethods}.\n\\begin{table*}[ht]\n\\caption{Summary of published research in perturbation-based methods}\n\\label{tab:perturbationmethods}\n\\centering\n\\begin{tabular}{||p{3cm}|p{5.5cm}|p{2cm}|p{5cm}||}\n\\hline\nMethod Name \n    & Interpretation Perspective \n        & Applied Network \n            & Comments and Discussions \\\\ \n\\hline\\hline\nDeConv Nets by Zeiler et al.   \n    & Neural activation of individual layers by occluding input instance and visualizing using DeConv Nets \n        & AlexNet \n            & Authors trained an AlexNet model on ImageNet dataset and layer-wise filter visualizations were carried out, studied feature generalization, and brought  important insights in dataset bias and issues with small training samples. \\\\\n\\hline\nLIME by Ribeiro et al.        \n    & Iterative perturbation to input data instance by finding superpixels \n        & - \n            & Authors generated locally faithful explanations using input perturbations around a point of interest. A human/user study was carried out to assess the impact of using LIME as an explanation and found that explanations can improve a untrustworthy classifier. \\\\\n\\hline\nSHAP by Lundberg et al.        \n    & Probing feature correlations by removing features in a game theoretic framework \n        & - \n            & SHAP produced consistently better results than LIME. A user study indicated that SHAP explanations are consistent with human explanations. However, as we will see in the evaluation section, some recent studies argue that SHAP values, albeit good in generating explanations, does not improve final decision making.\\\\\n\\hline\nPrediction Difference Analysis by Zintgraf et al. \n    & By studying $f$ removing individual features from $x$, find the positive and negative correlation of individual features towards the output. \n        & AlexNet, GoogLeNet, VGG \n            & One of the first works to look at positive and negative correlation of individual features towards the output by finding a relevance value to each input feature. Trained various models on ImageNet dataset to understand the support for the output classes from various layers of deep nets.\\\\\n\\hline\nRandomized Input Sampling for Explanation by Petsiuk et al. \n    & Study saliency maps by randomized masking of inputs \n        & ResNet50, VGG16 \n            &  - \\\\\n\\hline\nRandomization and Feature Testing by Burns et al. \n    & Counterfactual replacements of features to study feature importance \n        & Inception V3, BERT \n            &  - \\\\\n\\hline\n\\end{tabular}\n\\end{table*}", "cites": [8600, 8099, 7511, 1813], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of perturbation-based XAI methods by grouping them under a common theme and referencing specific examples. However, the integration is limited and primarily descriptive. Some critical analysis is attempted, such as the mention of SHAP's performance and limitations in decision-making, but it is not deeply developed. The section lacks abstraction or a broader conceptual framework that could elevate the discussion to a more general level."}}
{"id": "fef0668e-c289-4df6-8cc9-cdf054889559", "title": "DeConvolution nets for Convolution Visualizations", "level": "subsubsection", "subsections": [], "parent_id": "9c01b37d-ff71-4d51-8619-13635152d7ed", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Differences in the Methodology"], ["subsection", "Perturbation-Based"], ["subsubsection", "DeConvolution nets for Convolution Visualizations"]], "content": "Zeiler et al.  visualized the neural activations of individual layers of a deep convolutional network by occluding different segments of the input image and generating visualizations using a deconvolution network (DeConvNet). DeConvNets are CNNs designed with filters and unpooling operations to render opposite results than a traditional CNN. Hence, instead of reducing the feature dimensions, a DeConvNet, as illustrated in Figure \\ref{fig:deconvnet}, is used to create an activation map which maps back to the input pixel space thereby creating a visualization of the neural (feature) activity. The individual activation maps could help understand what and how the internal layers of the deep model of interest is learning - allowing for a granular study of DNNs.\n\\begin{figure}[!b]\n\\begin{center}\n\\includegraphics[width=\\linewidth]{images/deconvnet.png}\n\\end{center}\n  \\caption{Deconvolution operation is applied using a DeConv layer attached to the end of a ConvNet. Here the DeConvNet generates an approximate version of the convolution features thereby providing visual explanations. Figure from .}\n\\label{fig:deconvnet}\n\\end{figure}", "cites": [8600], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of DeConvNet and its function in visualizing neural activations, citing one paper. It lacks synthesis with other methodologies, critical evaluation of the approach or its limitations, and abstraction to broader trends or principles in XAI. The narrative is primarily factual and does not contribute deeper insights."}}
{"id": "45e15029-7ec9-4be0-b1ed-c49b515761c8", "title": "Prediction Difference Analysis", "level": "subsubsection", "subsections": [], "parent_id": "9c01b37d-ff71-4d51-8619-13635152d7ed", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Differences in the Methodology"], ["subsection", "Perturbation-Based"], ["subsubsection", "Prediction Difference Analysis"]], "content": "A conditional sampling based multi-variate approached was used by Zintgraf et al.  to generate more targeted explanations on image classification CNNs. By assigning a relevance value to each input features with respect to the predicted class $c$, the authors summarize the positive and negative correlation of individual data features to a particular model decision. Given an input feature $x$, its feature relevance can be estimated by studying the changes in model output prediction for the inputs with different hidden features. Hence, if $\\xbacki$ denotes the set of all input features except $x$, the task is to find the difference between $p(c|\\mathbf{x})$ and $p(c|\\xbacki)$.", "cites": [8600], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the prediction difference analysis method proposed by Zintgraf et al., but it does not integrate this work with other methodologies in the field. There is minimal critical evaluation or comparison with alternative approaches, and the abstraction is limited to reiterating the method's purpose without identifying broader patterns or principles."}}
{"id": "ba44e3a6-d676-4480-85cf-116b36895954", "title": "Randomized Input Sampling for Explanation (RISE)", "level": "subsubsection", "subsections": [], "parent_id": "9c01b37d-ff71-4d51-8619-13635152d7ed", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Differences in the Methodology"], ["subsection", "Perturbation-Based"], ["subsubsection", "Randomized Input Sampling for Explanation (RISE)"]], "content": "The RISE method introduced by Petsiuk et al.  perturb an input image by multiplying it with randomized masks. The masked images are given as inputs and the saliency maps corresponding to individual images are captured. Weighted average of the masks according to the confident scores is used to find the final saliency map with a positive valued heatmap for individual predictions. Importance maps of the blackbox prediction is estimated using Monte Carlo sampling.  \nA high-level architecture is illustrated in Figure \\ref{fig:RISE}.", "cites": [7511], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the RISE method, focusing on its procedural aspects such as perturbation and Monte Carlo sampling. It does not synthesize or integrate information from other papers, lacks critical evaluation or comparison with alternative methods, and offers no abstraction or meta-level insights into broader XAI patterns or principles."}}
{"id": "bc548633-c8e4-4fac-8716-505bd9d6060b", "title": "Randomization and Feature Testing", "level": "subsubsection", "subsections": [], "parent_id": "9c01b37d-ff71-4d51-8619-13635152d7ed", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Differences in the Methodology"], ["subsection", "Perturbation-Based"], ["subsubsection", "Randomization and Feature Testing"]], "content": "The Interpretability Randomization Test (IRT) and the One-Shot Feature Test (OSFT) introduced by Burns et al.  focuses on discovering important features by replacing the features with uninformative counterfactuals. Modeling the feature replacement with a hypothesis testing framework, the authors illustrate an interesting way to examine contextual importance. Unfortunately, for deep learning algorithms, removing one or more features from the input isn't possible due to strict input dimensions for a pre-trained deep model. Zero-ing out values or filling in counterfactual values might lead to unsatisfactory performance due to correlation between features.\n\\begin{figure}[!ht]\n\\begin{center}\n\\includegraphics[width=\\linewidth]{images/rice_arch.jpg}\n\\end{center}\n  \\caption{The input image given to a deep learning model is perturbed using various randomized masks. A confidence score is found out for individual masked inputs. A final saliency map is generated using a weighting function .}\n\\label{fig:RISE}\n\\end{figure}", "cites": [7511, 8099], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview by introducing the IRT and OSFT methods within a hypothesis testing framework and linking them to the broader challenge of perturbation in deep learning. It critiques these approaches by pointing out practical limitations for deep models, such as fixed input dimensions and feature correlation, but does not extensively synthesize or compare them with other methods. The abstraction is limited to identifying general issues rather than forming a higher-level conceptual understanding."}}
{"id": "a32ac8bc-23ff-4c0e-9ea1-d3952ae40170", "title": "Saliency Maps", "level": "subsubsection", "subsections": [], "parent_id": "aecfb515-59e9-4e38-9aa5-4e920c126b9d", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Differences in the Methodology"], ["subsection", "BackPropagation- or Gradient-Based"], ["subsubsection", "Saliency Maps"]], "content": "As mentioned in sub-section \\ref{subsec:saliencymap}, Simonyan et al.  introduced a gradient based method to generate saliency maps for convolutional nets. DeConvNet work by Zeiler et al.  mentioned previously as a perturbation method uses backpropagation for activation visualizations. DeConvNet work was impressive due to relative importance given to gradient value during backprop. With Rectified Linear Unit (ReLU) activation, a backprop on traditional CNNs would result in zero values for negative gradients. However, in DeConvNets, the gradient value is not clipped at zero. This allowed for accurate visualizations. Guided backpropagation methods  are also another class of gradient based explanation which improved upon .", "cites": [1812, 8600, 1814], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of saliency maps and mentions specific papers, but lacks deeper synthesis, critical evaluation, or abstraction. It does not clearly connect the cited works into a broader narrative or evaluate their strengths and limitations in a meaningful way."}}
{"id": "664f598a-4e82-46db-a27e-4e13a6822852", "title": "Gradient class activation mapping (CAM)", "level": "subsubsection", "subsections": [], "parent_id": "aecfb515-59e9-4e38-9aa5-4e920c126b9d", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Differences in the Methodology"], ["subsection", "BackPropagation- or Gradient-Based"], ["subsubsection", "Gradient class activation mapping (CAM)"]], "content": "Most saliency methods use global average pooling layer for all pooling operations instead of maxpooling. Zhou et al.  modified global average pooling function with class activation mapping (CAM) to localize class-specific image regions on an input image with a single forward-pass. Grad-CAM  and Grad-CAM++  improved the CAM operation for deeper CNNs and better visualizations.\nGradCAM is a class-discriminative attribution technique for localizing the neuronal activity of a CNN network. It allows class-specific query of an input image and also counterfactual explanations which highlights regions in the image which negatively contribute to a particular model output. GradCAM is successfully applied to explain classifiers in image classification, image segmentation, visual question answering (VQA), etc. Figure \\ref{fig:gradcam_segmentation} illustrates a segmentation method utilizing GradCAM to improve the segmentation algorithm. Here, we see another example of using XAI explanations to improve performance of deep neural networks.", "cites": [737], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of Grad-CAM and its variants, mentioning their purpose and applications, but lacks in-depth synthesis of the cited paper's contributions. It does not critically evaluate the methods or their limitations, nor does it abstract to broader principles or frameworks within XAI. The narrative is primarily descriptive with minimal analytical depth."}}
{"id": "9fc379dd-a019-46a9-9a9c-a1c6f62448d8", "title": "Salient Relevance (SR) Maps", "level": "subsubsection", "subsections": [], "parent_id": "aecfb515-59e9-4e38-9aa5-4e920c126b9d", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Differences in the Methodology"], ["subsection", "BackPropagation- or Gradient-Based"], ["subsubsection", "Salient Relevance (SR) Maps"]], "content": "Li et al.  proposed Salient Relevance (SR) map which is a context aware salience map based on the LRP of input image. Hence, the first step is to find LRP relevance map for input image of interest with the same input dimensions. A context aware salience relevance map algorithm takes the LRP relevance maps and finds a saliency value for individual pixels. Here, a pixel is salient if a group of neighboring pixels are distinct and different from other pixel patches in the same and multiple scales. This is done to differentiate between background and foreground layers of the image. \nTo aid visualization, a canny-edge based detector is superimposed with the SR map to provide context to the explanation. We place SR in gradient based methods due to the use of LRP. Other relevance propagation methods based on Taylor decomposition  are also explored in literature, which are slightly different in the methodology but have the same global idea.\nAlgorithm \\ref{alg:sr} describes the SR map generation in detail. Similar to SpRAy technique, we start with the LRP of the input instance. In contrast, we only find LRP attribution relevance score for a single input of interest $x$. Then a context aware saliency relevance (SR) map is generated by finding a dissimilarity measure based on the euclidean distance in color space and position. Multi-scale saliency at scales $r, \\frac{r}{2}, \\frac{r}{4}$ are found out and the immediate context of image $x$ based on an attention function is added to generate the SR map.\n\\begin{figure}[!t]\n\\begin{center}\n\\includegraphics[width=\\linewidth]{images/gradcam_segmentation.jpg}\n\\end{center}\n  \\caption{Illustration from  showing segmentation results by using Grad-CAM output as a seed.}\n\\label{fig:gradcam_segmentation}\n\\end{figure}\n\\begin{algorithm}[ht]\n  \\caption{Salient Relevance (SR) Algorithm}\n  \\label{alg:sr}\n\\begin{algorithmic}[1]\n    \\Statex \\textbf{Input:} classifier $f$, input sample $x$, scale factor $r$\n    \\Statex \\textbf{Output:} relevance map\n    \\State \\bm{$f_{LRP} \\leftarrow LRP(x)$}\n    \\State \\bm{$\\texttt{GenerateSRMap(}x, f_{LRP}\\texttt{)}$}\n    \\State \\bm{$S \\leftarrow \\texttt{MultiScaleSaliency(} r, \\frac{r}{2}, \\frac{r}{4} \\texttt{)}$}\n    \\State \\bm{$SRMap \\leftarrow \\texttt{AttentionFunction(} x, S\\texttt{)}$}\n    \\State \\texttt{Return} \\bm{$SRMap$}\n\\end{algorithmic}\n\\end{algorithm}\n\\begin{table*}[!t]\n\\caption{Summary of published research in gradient-based methods}\n\\label{tab:gradientmethods}\n\\centering\n\\begin{tabular}{||p{3cm}|p{5.5cm}|p{2cm}|p{5cm}||}\n\\hline\nMethod Name \n    & Interpretation Perspective \n        & Applied Network \n            & Comments and Discussions \\\\ \n\\hline\\hline\nSaliency Maps  \n    & Visualizing gradients, neural activation of individual layers using DeConv nets, guided backpropagation, etc. as images. \n        & AlexNet, GoogLeNet, ResNet18, VGG16 \n            &  A group of techniques which kicked-off gradient-based XAI research. As we will see in the evaluation section, these methods have serious disadvantages which needs to be improved.\\\\\n\\hline\nGrad-CAM by Selvaraju et al. \n    & Localize neuronal activity flowing to last convolutional layer of a CNN to allow class-specific query with counterfactual explanations describing negative influence of input features as well. \n        & AlexNet, VGG16, ResNet, and more.  \n            &  -\\\\\n\\hline\nSalient Relevance by Li et al. \n    & Takes the LRP relevance maps and finds a saliency value for individual pixels. \n        & - \n            &  -\\\\\n\\hline\nAxiomatic Attribution Maps by Sundararajan et al. \n    & Feature importance based on distance from a baseline instance \n        & GoogLeNet, LSTM based NMT, and more. \n            &  Introduced axioms or desirable qualities for gradient-based methods. Improved the saliency maps and gradient times input maps.\\\\\n\\hline\nPatternNet and PatternAttribution by Kindermans et al. \n    & LRP-based method with back-projection of estimated signals to input space. Cleaner attributions based using root point selection algorithm. \n        & VGG16 \n            &  - \\\\\n\\hline\n\\end{tabular}\n\\end{table*}", "cites": [1812, 8600, 1824, 6080, 1814], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "low", "analysis": "The section provides a basic description of the Salient Relevance (SR) map method and its relation to LRP and other gradient-based techniques. While it references multiple papers, it largely summarizes their contributions without connecting them into a broader narrative or evaluating their strengths and weaknesses. There is minimal critical analysis or abstraction to identify overarching principles in XAI methodologies."}}
{"id": "30c05e91-a523-4d4e-85e8-f3b45bb68c02", "title": "Attribution Maps", "level": "subsubsection", "subsections": [], "parent_id": "aecfb515-59e9-4e38-9aa5-4e920c126b9d", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Differences in the Methodology"], ["subsection", "BackPropagation- or Gradient-Based"], ["subsubsection", "Attribution Maps"]], "content": "In , Ancona et al. shows that the gradient method, where the gradient of output corresponding to input is multiplied by the input, is useful in generating an interpretable explanation to model outcomes. However, in , authors proposed Integrated Gradients (IG) and argue that most gradient based lack in certain `axioms' which are desirable characteristics of any gradient based technique. Authors argue that methods such as DeepLift , Layer-wise relevance propagation (LRP) , Deconvolutional networks (DeConvNets) , and Guided back-propagation  have specific back-propagation logic that violates some axioms.\nFor each input data instance $x$, if we consider a baseline instance $x^{'} \\in \\mathbb{R}^{n}$, the attributions of $x$ on model $f$ can be summarized by computing the integral of gradients at all points of a straight-line path from baseline $x^{'}$ to $x$. This method is called the Integrated Gradients such that:\n\\begin{equation} \\label{eq:intgrad}\n\\small\n\\text {IG}_j(x, x^{\\prime}) := (x_{j}-x^{\\prime}_{j})\\times\\int_{\\alpha=0}^{1} \\tfrac{\\partial F(x^{\\prime} + \\alpha\\times(x-x^{\\prime}))}{\\partial x_{j}  }~d\\alpha\n\\end{equation}\nwhere $j$ describes the dimension along which the gradient is calculated. \nDuring calculation in computers, the integral in equation \\ref{eq:intgrad} is efficiently approximated using summation instead.\nIn many cases, baseline instance $x^{'}_i$ is chosen as a zero matrix or vector. For example, for image domain, the baseline image is chosen as a black image by default. For text classification, the baseline is a zero valued vector. However, choosing baselines arbitrarily could cause issues downstream. For example, a black baseline image could cause the attribution method to diminish the importance of black pixels in the source image.\nAttribution prior  concept tries to regularize the feature attributions during model training to encode domain knowledge. A new method, Expected Gradients (EG) was also introduced in the paper as a substitute feature attribution method instead of Integrated Gradients. Together, the attribution prior and EG methods encodes prior knowledge from the domain to aid training process leading to better model interpretability. Equation \\ref{eq:expgrad} shows how authors remove the influence of baseline images from integrated gradients by still following all the axioms of Integrated Gradient method. Here, $D$ is the distribution of underlying data domain.\n\\begin{equation} \\label{eq:expgrad}\n\\small\n\\begin{split}\n\\text {EG}(x):=\\int_{x^{\\prime}}\\left(\\left(x_{j}-x_{j}^{\\prime}\\right) \\int_{\\alpha=0}^{1} \\tfrac{\\delta f\\left(x^{\\prime}+\\alpha \\times\\left(x-x^{\\prime}\\right)\\right)}{\\delta x_j} \\delta \\alpha\\right)\\\\ . p_{D}\\left(x^{\\prime}\\right) \\delta x^{\\prime}\n\\end{split}\n\\end{equation}\nSince an integration over the whole training distribution is intractable, authors proposed to reformulate the integral as expectations such that:\n\\begin{equation} \\label{eq:expgrad2}\n\\small\n\\begin{split}\n\\text { EG}(x):=\\underset{x^{\\prime} \\sim D, \\alpha \\sim U(0,1)}{\\mathbb{E}}\\left[\\left(x_{j}-x_{j}^{\\prime}\\right) \\tfrac{\\delta f\\left(x^{\\prime}+\\alpha \\times\\left(x-x^{\\prime}\\right)\\right)}{\\delta x_{j}}\\right]\n\\end{split}\n\\end{equation}", "cites": [6086, 6078, 1824, 8600, 1812], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple gradient-based attribution methods and links them through the concept of axioms, as introduced in the seminal work on Integrated Gradients. It critically evaluates the limitations of existing techniques and highlights the significance of baseline choice and domain knowledge through attribution prior and Expected Gradients. The section abstracts beyond individual papers to present a theoretical and practical framework for evaluating and improving attribution methods in XAI."}}
{"id": "9f74cff5-cf48-4d9f-9794-8829fe37c8ac", "title": "Desiderata of Gradient-based Methods", "level": "subsubsection", "subsections": [], "parent_id": "aecfb515-59e9-4e38-9aa5-4e920c126b9d", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Differences in the Methodology"], ["subsection", "BackPropagation- or Gradient-Based"], ["subsubsection", "Desiderata of Gradient-based Methods"]], "content": "Gradient-based methods, as we saw, mainly use saliency maps, class activation maps, or other gradient maps for visualization of important features. Recent research have found numerous limitations in gradient-based methods. To improve gradient-based XAI techniques, Sundararajan et al.  describes four desirable qualities (axioms) that a gradient based method needs to follow:\n\\begin{enumerate}\n    \\item \\textbf{Sensitivity: } \\textit{If for every input and baseline that differ in one feature but have different predictions then the differing feature should be given a non-zero attribution }. For simple functions such as $f(x) = 1 - ReLU(1-x)$, the function value saturates for $x$ values greater than or equal to one. Hence, if we take simple gradients as an attribution method, sensitivity won't hold.\n    \\item \\textbf{Implementation invariance: } \\textit{Two networks are \\emph{functionally equivalent} if their outputs are equal for all inputs, despite having very different implementations. Attribution methods should satisfy \\emph{Implementation Invariance}, i.e., the attributions are always identical for two functionally equivalent networks .} Methods such as DeepLift and LRP break implementation invariance because they use discrete gradients, and chain rule doesn't old for discrete gradients in general. Generally, if the model fails to provide implementation invariance, the attributions are potentially sensitive to unimportant features and aspects of the model definition.\n    \\item \\textbf{Completeness: } Attributions should add up to the difference between output of model function $f$ for the input image $x$ and another baseline image $x^{'}$. $\\Sigma_{i=1}^n Gradients_i(x) = f(x) - f(x^{'})$.\n    \\item \\textbf{Linearity: } For a linearly composed neural network model $f_3$ which is a linear combination of two neural network models $f_1$ and $f_2$ such that $f_3 = a \\times f_1 + b \\times f_2$, then the attributions of the $f_3$ is expected to be a weighted sum of attributions for $f_1$ and $f_2$ with weights $a$ and $b$ respectively.\n\\end{enumerate}\nDespite human understandable explanations, gradient-based explanation maps have practical disadvantages and raises various concerns in mission-critical applications. We explain some of these concerns in later sections.", "cites": [1824], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.5, "critical": 4.0, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section effectively synthesizes the core contributions of Sundararajan et al. by framing their axioms as key desiderata for gradient-based XAI methods. It critically evaluates the shortcomings of existing techniques (e.g., DeepLift and LRP) in meeting these axioms and explains the implications. The abstraction is strong, as it generalizes these axioms to highlight principles that guide the development and evaluation of attribution methods."}}
{"id": "2f403c46-5245-48f9-9d37-19e65c664a25", "title": "Sparse LDA and Discriminant Analysis", "level": "subsubsection", "subsections": [], "parent_id": "cebdf9f5-5776-4ddf-8ef0-29ab5cfeb304", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Model Usage or Implementation Level"], ["subsection", "Model Intrinsic"], ["subsubsection", "Sparse LDA and Discriminant Analysis"]], "content": "A Bayesian non-parametric model, Graph-Sparse LDA, was introduced in  to find interpretable, predictive topic summaries to textual categories on datasets with hierarchical labeling. Grosenick et al.  introduced a method called Sparse Penalized Discriminant Analysis (SPDA) to improve the spatio-temporal interpretability and classification accuracy of learning algorithms on Functional Magnetic Resonance Imaging (FMRI) data. \nAs we see in published research, there are several restrictions to use model intrinsic architectures as it requires careful algorithm development and fine-tuning to the problem setting. The difficulty in using concepts from model intrinsic architectures and apply them in existing high-accuracy models to improve interpretability is a disadvantage of model-intrinsic methods. However, as long as a reasonable performance limit is set, model intrinsic architectures for XAI could help accelerate inherently interpretable models for future AI research.", "cites": [6087], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section briefly synthesizes two cited papers but lacks a deeper integration of their ideas into a broader XAI context. It offers some critical analysis by pointing out the limitations of model-intrinsic methods, such as the need for careful algorithm development and difficulty in applying them to high-accuracy models. While it touches on general challenges in model intrinsic architectures, it does not abstract beyond specific methods to reveal more meta-level insights or overarching principles in XAI."}}
{"id": "fe17e4c6-a2b0-4695-b4a2-e62ac0ee9dfb", "title": "Post-Hoc", "level": "subsection", "subsections": [], "parent_id": "bcc4810c-b938-4899-9f33-7f1bf2148d0d", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Model Usage or Implementation Level"], ["subsection", "Post-Hoc"]], "content": "Explaining pre-trained classifier decisions require algorithms to look at AI models as black or white boxes. A black box means the XAI algorithm doesn't know the internal operations and model architectures. In white box XAI, algorithms have access to the model architecture and layer structures. Post-hoc explanation methodology is extremely useful as existing accurate models can benefit from added interpretability. Most post-hoc XAI algorithms are hence model-agnostic such that the XAI algorithm will work on any network architectures as illustrated in Figure \\ref{fig:posthocmodel}. This is one of the main advantages of post-hoc explainable algorithms. For example, an already trained well established neural network decision can be explained without sacrificing the accuracy of the trained model.\n\\begin{figure}[!b]\n\\begin{center}\n\\includegraphics[width=0.95\\linewidth]{images/posthoc_model.png}\n\\end{center}\n  \\caption{High-level illustration of post-hoc model explainability algorithms. Here, the explainability algorithm is applied on \\bm{$f$} such that \\bm{$f$} is made explainable externally.}\n\\label{fig:posthocmodel}\n\\end{figure}\nDeconvolution network  could be used to generate post-hoc explanations of layer-wise activations. Saliency maps  and most attribution based methods  are applied considering the network as a white or black box. LRP technique  discussed above is done after training the model completely. Shapley sampling methods  are also post-hoc and model agnostic. Activation maximization technique  is applicable to any network in which we can find gradients values to optimize activations. \n\\begin{table*}[!t]\n\\caption{Summary of published research in explainability and interpretability of deep learning algorithms. \\textbf{*} indicates that a preprint version was published an year prior to the conference or journal version.}\n\\label{tab:papers}\n\\centering\n\\begin{tabular}{||l|l|l|l|l|l|p{1.4cm}|l||}\n\\hline\nMethod Name & Publication & Year & Scope  & Methodology  & Usage &  Agnostic or Specific & Domain \\\\ \n\\hline\\hline\nBayesian averaging over decision trees  & Schetinin et al.               & 2007 & GL     & OT    & IN    & MS    & TAB \\\\ \n\\hline      \nSPDA                                    & Grosenick et al.               & 2008 & GL     & OT    & IN    & MS    & TXT \\\\ \n\\hline      \nActivation Maximization                 & Erhan et al.                   & 2010 & LO     & BP    & PH    & MA    & IMG \\\\ \n\\hline          \nGradient-based Saliency Maps            & Simonyan et al.                & 2013 & LO     & BP    & PH    & MA    & IMG \\\\ \n\\hline          \nBayesian Case Model (BCM)               & Kim et al.                     & 2014 & GL     & OT    & IN    & MS    & Any \\\\ \n\\hline          \nDeConvolutional Nets                    & Zeiler et al.                  & 2013 & LO     & BP    & PH    & MA    & IMG \\\\ \n\\hline          \nGAM                                     & Caruana et al.                 & 2015 & GL     & OT    & IN    & MS    & TAB \\\\ \n\\hline          \nLRP                                     & Back et al.                    & 2015 & Both   & BP    & PH    & MA    & IMG \\\\ \n\\hline\nGuided Backprop                         & Springenberg et al.            & 2015 & LO     & BP    & PH    & MA    & IMG \\\\ \n\\hline\nBayes Rule Lists                        & Letham et al.                  & 2015 & GL     & OT    & IN    & MS    & TAB \\\\ \n\\hline\nCAM                                     & Zhou et al.            & 2016 & LO     & BP    & PH    & MA    & IMG \\\\ \n\\hline\nLIME                                    & Ribeiro et al.                 & 2016 & Both   & PER   & PH    & MA    & Any \\\\ \n\\hline      \nShapley Sampling                        & Lundberg et al.                & 2017 & Both   & PER   & PH    & MA    & Any \\\\ \n\\hline      \nGrad-CAM                                & Selvaraju et al.               & 2017* & LO    & BP    & PH    & MA    & IMG \\\\ \n\\hline      \nPrediction Difference Analysis (PDA)    & Zintgraf et al.                & 2017 & LO     & PER   & PH    & MA    & IMG \\\\ \n\\hline\nDeep Taylor Expansion                   & Montavon et al.                & 2017 & LO     & OT    & PH    & MA    & IMG \\\\ \n\\hline\nDeep Attribution Maps                   & Ancona et al.                  & 2017 & LO     & BP    & PH    & MA    & IMG \\\\ \n\\hline\nAxiomatic Attributions                  & Sundararajan et al.            & 2017 & LO     & BP    & PH    & MA    & IMG \\\\ \n\\hline\nPatternNet and PatternAttribution       & Kindermans et al.      & 2017 & LO     & BP    & PH    & MA    & IMG \\\\ \n\\hline\nConcept Activation Vectors              & Kim et al.     & 2018 & GL     & OT    & PH    & MA    & IMG \\\\ \n\\hline\nRISE                                    & Petsiuk et al.                 & 2018 & LO     & PER   & PH    & MA    & IMG \\\\ \n\\hline\nGrad-CAM++                              & Chattopadhay et al.            & 2018 & LO     & BP    & PH    & MA    & IMG \\\\ \n\\hline\nRandomization and Feature Testing       & Burns et al.                   & 2019 & LO     & PER   & PH    & MA    & IMG \\\\ \n\\hline\nSalient Relevance (SR) map              & Li et al.                      & 2019 & LO     & BP    & PH    & MA    & IMG \\\\ \n\\hline\nSpectral Relevance Analysis             & Lapuschkin et al.              & 2019 & GL     & BP    & PH    & MA    & IMG \\\\ \n\\hline\nGlobal Attribution Mapping              & Ibrahim et al.           & 2019 & GL     & PER   & PH    & MA    & IMG \\\\ \n\\hline\nAutomatic Concept-based Explanations    & Ghorbani et al.         & 2019 & GL     & OT    & PH    & MA    & IMG \\\\ \n\\hline\nCaCE                                    & Goyal et al.         & 2019 & GL     & OT    & PH    & MA    & IMG \\\\ \n\\hline\nNeural Additive Models                  & Agarwal et al.           & 2020 & GL     & OT    & IN    & MS    & IMG \\\\\n\\hline\n\\multicolumn{8}{p{.8\\linewidth}}{Global: GL, Local: LO, Others: OT, BackProp: BP, Perturbation: PER, Model-specific: MS, Model-agnostic: MA, Tabular: TAB, Image: IMG, Test: TXT, Any: Image, Text, or Tabular.}\n\\end{tabular}\n\\end{table*}", "cites": [8600, 6078, 8099, 7511, 1816, 737, 7514, 1824, 1812, 8080, 1813, 6079, 6080, 1814, 6081], "cite_extract_rate": 0.5517241379310345, "origin_cites_number": 29, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of post-hoc XAI techniques, listing several methods and their properties in a table. While it briefly mentions the model-agnostic nature of post-hoc methods and their utility in preserving model accuracy, it lacks deeper synthesis of ideas, critical evaluation of the approaches, or abstraction to broader principles. The narrative is primarily a factual summary of methods with minimal cross-referencing or insight."}}
{"id": "8fb012ae-8135-4518-83ae-ecfbb938462c", "title": "Evaluation Methodologies, Issues, and Future Directions", "level": "section", "subsections": ["765cd114-cfa8-4e7f-b16d-aefaf4c48f87", "598c7455-aaa7-4d56-8577-dc1df683089c", "7682f46e-dc75-4498-9e5c-0fcbe9ee726b", "3ef9f8e7-3d23-44c3-a434-9e8ed4ce81ff"], "parent_id": "b611e4f1-f132-4230-8c44-06bb4dd9aaf3", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Evaluation Methodologies, Issues, and Future Directions"]], "content": "\\label{sec:evaluation}\nSo far, we focused on XAI algorithms and methods categorized under scope, methodology, and usage. The seminar works discussed in the survey is tabulated in Table \\ref{tab:papers}. A fundamental challenge in XAI research is to evaluate the several proposed algorithms on real-world settings. \nOur survey on evaluation techniques suggested that the field is still immature with primary focus on a human-in-the-loop evaluations. Quantitative general evaluation schemes are yet to be explored. However, we summarize here some of the methods which improve human understandability of explainability method results based on . In general, each explanation should follow the below constraints to be usable by humans in a real-world setting:\n\\begin{enumerate}\n    \\item Identity or Invariance: Identical data instances must produce identical attributions or explanations.\n    \\item Stability: Data instances belonging to the same class $c$ must generate comparable explanations $g$.\n    \\item Consistency: Data instances with change in all but one feature must generate explanations which magnifies the change.\n    \\item Separability: Data instances from different populations must have dissimilar explanations.\n    \\item Similarity: Data instances, regardless of class differences, closer to each other, should generate similar explanations.\n    \\item Implementation Constraints: Time and compute requirement of the explainable algorithm should be minimal.\n    \\item Bias Detection: Inherent bias in data instances should be detectable from the testing set. Similarity and separability measures help achieve this.\n\\end{enumerate}", "cites": [1798, 8500], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key ideas from the cited papers to propose a coherent framework of constraints for usable XAI explanations. It abstracts from specific methods to highlight general principles such as identity, stability, and bias detection. While it identifies the immaturity of the field and a lack of quantitative evaluation schemes, the critical analysis is somewhat limited in depth, focusing more on outlining issues than evaluating specific limitations of the cited works."}}
{"id": "765cd114-cfa8-4e7f-b16d-aefaf4c48f87", "title": "Evaluation Schemes", "level": "subsection", "subsections": [], "parent_id": "8fb012ae-8135-4518-83ae-ecfbb938462c", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Evaluation Methodologies, Issues, and Future Directions"], ["subsection", "Evaluation Schemes"]], "content": "Several evaluation schemes have been suggested by the research community in the recent years. We present here some of the evaluation techniques that are actively gaining traction from the research community:\n\\begin{itemize}\n\\item{System Causability Scale (SCS):}\nAs the explainability methods are applied to human-facing AI systems which does automated analysis of data, evaluation of human-AI interfaces as a whole is also important. A System Causability Scale (SCS) was introduced in  to understand the requirements for explanations of a user-facing human-AI machine-interface, which are often domain specific. Authors described a medical scenario where the SCS tool was applied to Framingham Risk Tool (FRT) to understand the influence and importance of specific characteristics of the human-AI interface.\n\\item{Benchmarking Attribution Methods (BAM):}\nIn a preprint publication,  introduced a framework called Benchmarking Attribution Methods (BAM) to evaluate the correctness of feature attributions and their relative importance. A BAM dataset and several models were introduced. Here, the BAM dataset is generated by copying pixel groups, called Common Features (CF), representing object categories from MSCOCO dataset  and pasting them to MiniPlaces dataset. The hypothesis is that, if we have the same pixel group of information in the same spatial location of all of $X$, then the model should ignore it as a feature of relative importance. Hence, attribution methods focusing on pasted objects are simply not doing a good job at enhancing feature attributions of important features. Authors provided model contrast score (MCS) to compare relative feature importance between difference models, input dependence rate (IDR) to learn the dependence of CF on a single instance, and input independence rate (IIR) as a percentage score of images whose average feature attributions $g_r \\in \\mathbb{R}$ for region $r$ with and without CF is less than a set threshold.\n\\item{Faithfulness and Monotonicity:}\nIn , authors described a metric, named Faithfullness, to evaluate the correlation between importance scores of features to the performance effect of each feature towards a correct prediction. By incrementally removing important features and predicting on the edited data instance, we measures the effect of feature importance and later compare it against the interpreter's own prediction of relevance. \nIn , authors introduce monotonic attribute functions and thus the Monotonicity metric which measures the importance or effect of individual data features on the performance of the model by incrementally adding each feature in the increasing order of importance to find model performance. The model performance is expected to increase as more important features are added.\n\\item{Human-grounded Evaluation Benchmark:}\nIn , Moshseni et al. introduced a human-grounded evaluation benchmark to evaluate local explanations generated by an XAI algorithm. Authors created a subset of ImageNet dataset  and asked human annotators to manually annotate the images for the particular classes. A weighted explanation map was generated which summarized an average human representation of explanations. By comparing the explanations generated by locally explainable algorithms, authors presented a method to understand the precision of XAI explanations compared to human generated explanations. One fundamental flaw of this method could be added human bias in the explanations. However, human labels of individual data points from a large population could nullify the effect of inherent bias.\n\\end{itemize}\n\\begin{figure*}[!t]\n\\begin{center}\n\\includegraphics[width=\\linewidth]{images/evaluationxai.jpg}\n\\end{center}\n    \\caption{We evaluate different gradient-based and perturbation-based techniques in this figure. LIME and SHAP uses segmented superpixels to understand feature importance, while gradient based saliency maps, Integrated Gradients, LRP, DeepLIFT, and Grad-CAM use backpropagation based feature importance in a pixel level. Original prediction accuracies of a pre-trained InceptionV3 model on the images in each rows provided are as follows: (a) `koala', $94.5\\%$, (b) `sandbar', $38.0\\%$, (c) `arabian camel', $17.4\\%$, and (d) `leaf beetle', $95.5\\%$. Each column represents the attribution map generated by individual XAI methods. Scales to assess Grad-CAM and SHAP values are provided in the lower right section of the image. Gradient visualizations of this figure are created using DeepExplain package while visualizations for Grad-CAM, LIME, and SHAP are created by their own individual implementations. The experiments were carried out in Jetstream cloud . This image is better viewed in color.}\n\\label{fig:evaluatioxai}\n\\end{figure*}", "cites": [8100, 486, 6089, 6088], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple evaluation schemes from different papers, providing a coherent overview of how XAI methods are being evaluated. It includes some critical remarks, such as the potential for human bias in the human-grounded evaluation benchmark. While it identifies key metrics and methodologies, the analysis remains focused on specific approaches without offering broader, meta-level insights into evaluation trends in XAI."}}
{"id": "598c7455-aaa7-4d56-8577-dc1df683089c", "title": "Software Packages", "level": "subsection", "subsections": [], "parent_id": "8fb012ae-8135-4518-83ae-ecfbb938462c", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Evaluation Methodologies, Issues, and Future Directions"], ["subsection", "Software Packages"]], "content": "OpenSource packages have greatly improved reproducible research and has been a real boon to recent research in deep learning and XAI alike. We mention here some XAI software packages available in GitHub.\n\\begin{itemize}\n    \\item \\textbf{Interpret} by InterpretML can be used to explain blackbox models and currently supports explainable boosting, decision trees, decision rule list, linear\\/logistic regression, SHAP kernel explainer, SHAP tree explainer, LIME, morris sensitivity analysis, and partial dependence. Available at \\url{https://github.com/interpretml/interpret}.\n    \\item \\textbf{IML} package  is maintained by Christoph Molnar, author of . The package covers feature importance, partial dependence plots, individual conditional expectation plots, accumulated local effects, tree surrogates, LIME, and SHAP. Available at \\url{https://github.com/christophM/iml}.\n    \\item \\textbf{DeepExplain} package is maintained by Marco Ancona, author of . The package supports various gradient-based techniques such as saliency maps, gradient\\*input, integrated gradients, DeepLIFT, LRP, etc. and perturbation-based methods such as occlusion, SHAP, etc. Available at \\url{https://github.com/marcoancona/DeepExplain}.\n    \\item \\textbf{DrWhy} by ModelOriented is a package with several model agnostic and model specific XAI techniques including feature importance, ceteris paribus, partial dependency plots, conditional dependency, etc. Available at \\url{https://github.com/ModelOriented/DrWhy}\n\\end{itemize}", "cites": [6078], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual listing of XAI software packages without significant synthesis or analysis. It integrates minimal content from the cited paper, merely mentioning the availability of gradient-based methods without elaborating on theoretical or empirical insights. There is no critical evaluation of the packages or broader abstraction of trends in XAI software development."}}
{"id": "7682f46e-dc75-4498-9e5c-0fcbe9ee726b", "title": "A Case-study on Understanding Explanation Maps", "level": "subsection", "subsections": [], "parent_id": "8fb012ae-8135-4518-83ae-ecfbb938462c", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Evaluation Methodologies, Issues, and Future Directions"], ["subsection", "A Case-study on Understanding Explanation Maps"]], "content": "In Figure \\ref{fig:evaluatioxai}, we illustrate the explanation maps generated using various gradient- and perturbation-based XAI techniques for four images from ImageNet  dataset to explain the decisions an InceptionV3 model pre-trained on ImageNet. Here, each row starts with an original image from ImageNet followed by explanation map generated by gradient algorithms such as 1) saliency maps, 2) gradient times input, 3) integrated gradients, 4) LRP, 5) DeepLIFT, and 6) GradCAM, and perturbation-based techniques such as 1) LIME and 2) SHAP. \nGradCAM generates a heatmap of values ranging from 0 to 1, where 0 means no influence and 1 means highest influence of individual pixels towards the model output decision. Similarly, SHAP method follows a scale for SHAP values. However, SHAP scale ranges from -0.3 to +0.3 indicating that negative values decrease output class probability and positive values increase the output class probability for the corresponding input. Here 0.3 is the largest SHAP value generated for the set of four images considered. Gradient visualizations of this figure are created using DeepExplain package while visualizations for Grad-CAM, LIME, and SHAP are created by their own individual implementations.\nOriginal image column of row (a) in Figure \\ref{fig:evaluatioxai} indicates a correct prediction of an image of a Koala with $94.5\\%$ prediction accuracy, row (b) indicates a correct prediction of a sandbar image with $38.0\\%$ accuracy, row (c) indicates an incorrect prediction of a horse as an arabian camel with $17.4\\%$ accuracy, and row (d) indicates correct prediction of a leaf beetle with $95.5\\%$ percentage accuracy. We then compare the explanation maps, in different columns, generated by various XAI techniques as discussed above.\n\\begin{figure*}[!t]\n\\begin{center}\n\\includegraphics[width=0.9\\linewidth]{images/adv_feat_imp_attack.jpg}\n\\end{center}\n  \\caption{Illustration from  showing adversarial attacks involving small perturbations to input layer of neural network. We see that small perturbations doesn't affect the accuracy of predictions. However, feature importance maps are highly affected by the small changes. This illustrates the flaws in current gradient-based techniques.}\n\\label{fig:advfeatimp}\n\\end{figure*}\nFocusing on saliency maps, gradient times input, and integrated gradients in Figure \\ref{fig:evaluatioxai}, we can visually verify the improvements achieved by integrated gradients over the prior gradient-based methods. This is apparent in the images with lower class probabilities. For example, in row (b), we can verify that the integrated gradients generated high attributions around the sandy beach, plastic chairs, and a little bit of the blue sky. As human evaluators, we can make sense of this output because human experience suggests that a sandbar involve a beach, hopefully on a sunny day with bright blue clouds. A stark difference is apparent in Grad-CAM visualizations where the class output generated a heatmap which is focused primarily on the plastic chair and sandy beach, without much emphasis on the clouds. Perturbation-based methods such as LIME and SHAP generated superpixels which maximized the class probability. Here, we see that LIME is focusing on primarily the chairs and the sky, whereas SHAP is focusing on the beach and the sky. We also note that SHAP values generated are very low, indicating lesser influence to the confidence score.", "cites": [6090], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information by integrating explanation maps from multiple XAI techniques into a visual comparison, providing a coherent narrative on their relative strengths and weaknesses. It includes critical analysis by highlighting how different methods emphasize varying features and by noting the low SHAP values as indicative of lesser influence. While it identifies some patterns in feature attribution behavior, the abstraction remains limited to the specific examples in the case study without forming a broader meta-level framework."}}
{"id": "3ef9f8e7-3d23-44c3-a434-9e8ed4ce81ff", "title": "Limitations of XAI Visualizations and Future Directions", "level": "subsection", "subsections": [], "parent_id": "8fb012ae-8135-4518-83ae-ecfbb938462c", "prefix_titles": [["title", "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey"], ["section", "Evaluation Methodologies, Issues, and Future Directions"], ["subsection", "Limitations of XAI Visualizations and Future Directions"]], "content": "The discussion above brings some important flaws of XAI visualizations and interpretability techniques - 1) the inability of human-attention to deduce XAI explanation maps for decision-making, and 2) unavailability of a quantitative measure of completeness and correctness of the explanation map. This suggests that the further use of visualization techniques for mission-critical applications must be reconsidered moving forward. Also, better ways of representing and presenting explanations should be considered. For example, in , Weerts et al. studied the impact of SHAP explanations in improving human performance for alert processing tasks. The authors presented a human-grounded study to evaluate whether certain decision-making scenarios can be improved by providing explanations to decisions. Results showed that additional SHAP explanations to class output probability did not improve the decision-making of individuals. Authors saw more interest in final class score for making decisions which could be catastrophic in mission-critical scenarios.\nSimilarly, in , Mohseni et al. presented a human-grounded evaluation benchmark and evaluated performance of LIME algorithm by comparing the explanation map generated by LIME to that of weighted explanation map of 10 human annotations. Results suggested that LIME creates some attributions irrelevant to human explanations which causes low explanation precision compared to weighted explanation map generated by human annotators. This sheds light to the importance of understanding the mode of explanations as application-grounded, human-grounded, and functionally-grounded explanations  to improve explanation maps by meta information generated by humans, adding more constraints to explanations, or introducing formal definitions of explanations to the optimization problem.\nSeveral other flaws of explanation map visualization are explained by researchers in recent publications. In , Ghorbani et al. showed that small perturbations on the input instance generate large changes in the output interpretations that popular XAI methods generate. These \\textit{adversarial} examples, thus threw off the interpretable saliency maps generated by popular methods such as DeepLIFT and Integrated Gradients. This is illustrated in Figure \\ref{fig:advfeatimp}. Additionally, in , Wang et al. showed that bias term which is often ignored could have high correlations towards attributions.\nIn , Kindermans et al. explained that explanations of networks are easily manipulable by simple transformations. Authors note that expressiveness of Integrated Gradients  and Deep Taylor Decomposition  highly depend on the reference point, for example a baseline image \\bm{$x^{'}$}, and suggest that the reference point should be a hyperparameter instead of being determined \\textit{a priori}. Authors mentioned that most gradient-based methods attribute incorrectly to constant vector transformations and that input invariances should be a prerequisite for reliable attributions.\nIn , Adebayo et al. suggested that gradient-based methods are inherently dependent on the model and data generating process. Authors proposed two randomization tests for gradient methods namely model parameter randomization test and data randomization test. Model parameter randomization test compared the output of saliency method for a trained model versus the same model with random weights. Data randomization test applied the same saliency method for an input instance and the same instance with a set of invariances. Authors found that Gradients and GradCAM passed the sanity checks while Guided Backprop and Guided GradCAM methods failed the tests suggesting that these methods will generate some explanations even without proper training.\nNewer methods proposed in literature such as explaining with Concepts  which we discussed in subsection \\ref{subsec:tcav} could be viewed as a new class of meta-explanations which improve both perturbation- and gradient-based XAI methods. By exploring explanations as concepts, one could have additional meta information on the factors which contributed to individual class predictions along with traditional explanation by locally explainable algorithms. \nIn , Zhou et al. introduced Interpretable Basis Decomposition as a way of decomposing individual explanation based on different objects or scenes in the input instance. By decomposing the decision to several individual concept explanations, IBD could help evaluate importance of each concepts towards a particular decision.\nIn , Kindermans et al. suggested improvements to gradient-based methods and proposed PatternNet and PatternAttribution which can estimate the component of the data that caused network activations. Here, PatternNet is similar to finding gradients but is instead done using a layer-wise backprojection of the estimated signal (data feature) to the input space. PatternAttribution improves upon LRP to provide a neuron-wise attribution of input signal to the corresponding output class.", "cites": [8500, 6091, 1824, 8080, 1617, 1816, 6090, 6080, 6088], "cite_extract_rate": 0.6428571428571429, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong analytical depth by synthesizing multiple studies on XAI visualization limitations, drawing connections between human perception and algorithmic output, and identifying recurring issues such as fragility and irrelevance of explanations. It also critically evaluates the shortcomings of specific XAI methods and proposes abstraction through the discussion of broader categories like human-grounded and functionally-grounded explanations."}}
