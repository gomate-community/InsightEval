{"id": "504b42a2-053a-4d14-a0e9-a9517b24369f", "title": "Introduction", "level": "section", "subsections": ["baca8b6b-5536-41e9-a0f9-bc4e2294ec62"], "parent_id": "31670094-45b4-4854-914d-94aaf932ce65", "prefix_titles": [["title", "From Statistical Relational to Neurosymbolic \\\\ Artificial Intelligence: a Survey."], ["section", "Introduction"]], "content": "The integration of learning and reasoning is a key challenge in artificial intelligence and machine learning today. Various communities are addressing it, especially\n\t the field of neurosymbolic artificial intelligence (NeSy) .\n\tNeSy's goal is to integrate symbolic reasoning with neural networks.\n\t\\nesy{} already has a long tradition, and it has recently attracted a lot of attention.\nIndeed, the topic has been addressed by prominent researchers such as Y. Bengio and H. Kautz  in their keynotes at AAAI 2020, by Y. Bengia and G. Marcus in the AI Debate  and Hochreiter has recently stated  that \\nesy{} is ``the most promising approach to a broad AI''.\n\tAnother domain with a rich tradition in integrating learning and reasoning is that of statistical relational learning and artificial intelligence (StarAI) . StarAI focuses on integrating logical and probabilistic reasoning.\nHistorically, these two endeavours have adopted  different learning paradigms, probabilistic versus neural, for integrating logic into machine learning. This in turn has resulted in two different subcommunities.  StarAI has focused on probabilistic logics,  their semantics and making inference more tractable, while learning is usually based on parameter learning techniques from probabilistic graphical models.\nOn the other hand,  \\nesy{} extends neural networks with symbolic knowledge, focusing on scalable approximate models, paying less attention to semantical issues. In particular, \\nesy{} techniques can often be characterized by a clear parameterization in terms of neural networks, i.e. layered structures of latent representations, and by resorting to the gradient-based backpropagation paradigm for learning.\n\\rev{Despite a different focus and approach, the  two domains want to achieve the same goal, that is, to integrate learning and reasoning. \nIt is therefore surprising that there has been relatively little interaction between the two domains, but see }.\n\tThis discrepancy is the key motivation behind this survey: it aims at pointing out the similarities between these two endeavours and, in this way, it wants to stimulate \n\tcross-fertilization. \n\tWe start from the literature on StarAI, following the key concepts, \\rev{definitions} and techniques outlined in several textbooks and tutorials such as , because it turns out that  the same issues and techniques that arise in StarAI apply to \\nesy{}  as well. \n\t\\rev{The key contributions of this paper are:}\n\t\\begin{enumerate}\n\t\t\\item \n\t\t{\\em We identify seven dimensions that these fields have in common and that can be used to categorize both StarAI and \\nesy{} approaches}.  \n\t\tThese seven dimensions are concerned with (1) model vs proof-based inference,  \\rev{(2) logic syntax,} (3) semantics, (4) learning parameters or structure, (5) representing entities as symbols or subsymbols, (6)  integrating logic with probabilistic and/or neural concepts, and \\rev{(7) learning tasks}.\n\t\t\\item We provide evidence for our claim by positioning a wide range of StarAI and \\nesy{} systems along these dimensions and pointing out analogies between them. \n\t\tThis provides not only new insights into the relationships between StarAI and NeSy, but it also allows one to carry over and adapt techniques from one field to another. These  insights provide  opportunities for cross-fertilization between StarAI and NeSy, by focusing on those dimensions that have not  been fully exploited yet.\n\t\\item  \\rev{We gently introduce key logical concepts and techniques inherited from StarAI. In this way, the paper also provides a gentle introduction to symbolic AI and  StarAI techniques for the interested ``connectionist'' practitioner.} \n\t\t\\item We illustrate each dimension using  existing methods, \n  and in this  way, also present an intuitive and concrete overview of the research field. \n\t\\end{enumerate}\n\tUnlike some other perspectives on neurosymbolic computation , the present survey limits itself to a logical perspective and to developments in neurosymbolic computation that are consistent with this perspective. \\rev{Therefore, we usually refer to symbols and symbolic algorithms as synonyms for logical representations and logical reasoning.}\n\tFurthermore, the survey focuses on representative and prototypical systems rather than aiming at completeness (which would not be possible given the fast developments in the field). \n\tSeveral other surveys about neurosymbolic AI have been proposed. An early overview of neurosymbolic computation is that of . Unlike the present survey it focuses very much on a logical and a reasoning perspective. Today, the focus has shifted very much to learning. More recently,  analysed the intersection between NeSy and graph neural networks (GNN).  described neurosymbolic systems in terms of the composition of blocks described by few patterns, concerning processes and exchanged data. In contrast, this survey is more focused on the underlying principles that govern such a composition. Finally,  exploits  a neural network viewpoint by investigating in which components (i.e. input, loss or structure) symbolic knowledge is injected.", "cites": [7601, 8475], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes ideas from multiple cited papers to establish a logical framework for understanding both NeSy and StarAI, highlighting shared dimensions. It abstracts these systems into a set of principles and positions them along a conceptual spectrum. While it provides some critical perspective by contrasting the fields' focus areas, a deeper evaluation of cited works' limitations would enhance its critical depth."}}
{"id": "de2541d6-848e-4cbb-aea2-6b2aa605f9f0", "title": "Implications for StarAI", "level": "subsection", "subsections": [], "parent_id": "71d65d84-75cd-4c92-b14d-bddcaa016dd0", "prefix_titles": [["title", "From Statistical Relational to Neurosymbolic \\\\ Artificial Intelligence: a Survey."], ["section", "Proof- vs Model-theoretic View of Logic"], ["subsection", "Implications for StarAI"]], "content": "Statistical Relational AI's  focus is on unifying logical and probabilistic graphical models (PGMs). A PGM  is a graphical model that compactly\n\trepresents a (joint) probability distribution $P(X_1, ... , X_n)$ over $n$ discrete or continuous random variables $X_1, ... ,X_n$.\n\tThe key idea is that the joint factorizes over some factors $f_i$ specified over subsets $X^i$ of the variables $\\{X_1, ... ,X_n\\}$.\n\t$$P(X_1, ... , X_n) = \\frac{1}{Z} f_1(X^1) \\times ... \\times f_k(X^k) $$\n\tThe random variables correspond to the nodes in the graphical structure, and the factorization is determined\n\tby the edges in the graph. \n\tThere are two classes of graphical models: \\textit{directed} ones, or Bayesian networks, and \\textit{undirected} ones, or Markov Networks.\n\tIn Bayesian networks, the underlying graph structure is a directed acyclic graph,\n\tand the factors $f^i(X_i | parents(X_i))$ correspond to the conditional probabilities $P(X_i | parents(X_i))$, where $parents(X_i)$\n\tdenotes the set of random variables that are a parent of $X_i$ in the graph.\n\tIn Markov networks, the graph is undirected and the factors  $f^i(X^i)$  correspond to the set of nodes $X^i$ that form (maximal) cliques in the graph. Furthermore, the factors are non-negative and $Z$ is a normalisation constant.\n\t\\begin{evidencebox}\n\t\t\\rev{The distinction between directed and undirected graphical models is parallel to the proof- vs model-theoretic view of logic. This parallel is at the very core of  StarAI. In fact, by viewing each variable $X_i$  (or proposition) \\textit{at the same time} as a random  and as a logical variable , clausal theories can be extended to define probabilistic models. Clauses can then be translated into binary valued factors by labeling them with  weights (or probabilities), thus parameterizing the corresponding factors.}\n\t\\end{evidencebox}\n\tIn the remainder of this section, we will show how  StarAI  has used  this parallel to define two  types of systems .\n\tThe first type of StarAI system generalizes directed models and resembles Bayesian networks. It includes well-known representations such as plate notation , probabilistic relational models (PRMs) , probabilistic logic programs (PLPs) ,  and Bayesian logic programs (BLPs) . \n\tToday the most typical and popular representatives of this category are the probabilistic (logic) programs. \n\tProbabilistic logic programs were introduced by \n\tPoole~ and the first learning algorithm is due to Sato~. \n\tProbabilistic logic programs are essentially definite clause programs where every fact is annotated with the probability that it is \\textit{True}. \n\tThis then results in a possible world semantics.\n\tThe reason why probabilistic logic programs are viewed as directed models is clear when looking at the derivations\n\tfor a query, cf. Example \\ref{ex:logic_program}. At the top of the AND-OR tree, there is the query that one wants to prove and the structure of the tree\n\tis that of a directed graph (even though it need not be acyclic).  One can straightforwardly map\n\tdirected graphical models, that is, Bayesian networks,\n\tonto such probabilistic logic programs by associating one definite clause to every entry in the conditional probability tables,\n\tyielding factors of the form $P(X | Y_1, ... , Y_n)$. Assuming boolean random variables, each entry ${x,y_1, ...,y_n}$ with parameter value $v$ can be represented using the definite clause $X(x) \\leftarrow Y_1 (y_1) \\wedge ... \\wedge Y_n(y_n) \\wedge p_{x,y_1, ...,y_n}$ and \n\tprobabilistic fact $v::p_{x,y_1, ...,y_n}$. \n\tA probabilistic version of Example~\\ref{ex:logic_program} is shown in Example~\\ref{ex:problog} using the syntax of ProbLog .\n\t\\begin{figure}[t]\n\t\t\\centering\n\t\t\\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]\n\\draw   (160,105) .. controls (160,96.72) and (177.91,90) .. (200,90) .. controls (222.09,90) and (240,96.72) .. (240,105) .. controls (240,113.28) and (222.09,120) .. (200,120) .. controls (177.91,120) and (160,113.28) .. (160,105) -- cycle ;\n\\draw   (110,40) .. controls (110,28.95) and (125.67,20) .. (145,20) .. controls (164.33,20) and (180,28.95) .. (180,40) .. controls (180,51.05) and (164.33,60) .. (145,60) .. controls (125.67,60) and (110,51.05) .. (110,40) -- cycle ;\n\\draw   (220,40) .. controls (220,28.95) and (239.7,20) .. (264,20) .. controls (288.3,20) and (308,28.95) .. (308,40) .. controls (308,51.05) and (288.3,60) .. (264,60) .. controls (239.7,60) and (220,51.05) .. (220,40) -- cycle ;\n\\draw   (271.04,105) .. controls (271.04,96.72) and (299.91,90) .. (335.52,90) .. controls (371.13,90) and (400,96.72) .. (400,105) .. controls (400,113.28) and (371.13,120) .. (335.52,120) .. controls (299.91,120) and (271.04,113.28) .. (271.04,105) -- cycle ;\n\\draw   (100,175) .. controls (100,166.72) and (120.15,160) .. (145,160) .. controls (169.85,160) and (190,166.72) .. (190,175) .. controls (190,183.28) and (169.85,190) .. (145,190) .. controls (120.15,190) and (100,183.28) .. (100,175) -- cycle ;\n\\draw    (200,120) -- (252.57,158.24) ;\n\\draw [shift={(255,160)}, rotate = 216.03] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;\n\\draw    (200,120) -- (147.43,158.24) ;\n\\draw [shift={(145,160)}, rotate = 323.97] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;\n\\draw    (335.52,120) -- (257.69,158.67) ;\n\\draw [shift={(255,160)}, rotate = 333.58000000000004] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;\n\\draw    (60,120) -- (142.29,158.72) ;\n\\draw [shift={(145,160)}, rotate = 205.2] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;\n\\draw    (260,60) -- (202.74,85.77) ;\n\\draw [shift={(200,87)}, rotate = 335.77] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;\n\\draw    (150,60) -- (197.36,85.57) ;\n\\draw [shift={(200,87)}, rotate = 208.37] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;\n\\draw   (1.04,105) .. controls (1.04,96.72) and (29.91,90) .. (65.52,90) .. controls (101.13,90) and (130,96.72) .. (130,105) .. controls (130,113.28) and (101.13,120) .. (65.52,120) .. controls (29.91,120) and (1.04,113.28) .. (1.04,105) -- cycle ;\n\\draw   (210,175) .. controls (210,166.72) and (230.15,160) .. (255,160) .. controls (279.85,160) and (300,166.72) .. (300,175) .. controls (300,183.28) and (279.85,190) .. (255,190) .. controls (230.15,190) and (210,183.28) .. (210,175) -- cycle ;\n\\draw (119,31) node [anchor=north west][inner sep=0.75pt]   [align=left] {\\texttt{burglary}};\n\\draw (228,31) node [anchor=north west][inner sep=0.75pt]   [align=left] {\\texttt{earthquake}};\n\\draw (181.14,95.38) node [anchor=north west][inner sep=0.75pt]   [align=left] {\\texttt{alarm}};\n\\draw (276,96.13) node [anchor=north west][inner sep=0.75pt]   [align=left] {\\texttt{ hears\\_alarm\\_mary}};\n\\draw (106.92,166.13) node [anchor=north west][inner sep=0.75pt]   [align=left] {\\texttt{calls\\_john}};\n\\draw (13,96.13) node [anchor=north west][inner sep=0.75pt]   [align=left] {\\texttt{hears\\_alarm\\_john}};\n\\draw (216.92,166.13) node [anchor=north west][inner sep=0.75pt]   [align=left] {\\texttt{calls\\_mary}};\n\\end{tikzpicture}\n\t\t\\caption{The Bayesian network corresponding to the ProbLog program in Example \\ref{ex:problog}}\n\t\t\\label{fig:bayes_net_problog}\n\t\\end{figure}\n\t\\begin{boxedexample}\n [label = ex:problog]{ProbLog}\n\t\tWe show a probabilistic extension of the alarm program using ProbLog.\n\t\t\\begin{lstlisting}\n\t\t0.1::burglary.\n\t\t0.3::hears_alarm_mary. \n\t\t0.05::earthquake.\n\t\t0.6::hears_alarm_john.\n\t\talarm <- earthquake. \n\t\talarm <- burglary.\n\t\tcalls_mary <- alarm,hears_alarm_mary.\n\t\tcalls_john <- alarm,hears_alarm_john.  \n\t\t\\end{lstlisting}\n\t\tThis program can be mapped to the Bayesian network in Figure~\\ref{fig:bayes_net_problog}\n\t\tThis probabilistic logic program defines a distribution $p$ over possible worlds $\\omega$. Let $P$ be a Problog program and $F = \\{p_1::c_1, \\cdots, p_n::c_n\\}$ be the set of ground probabilistic facts $c_i$ of the program and $p_i$ their corresponding probabilities.\n\t\tProbLog defines a probability distribution over $\\omega$ in the following way:\n\t\t\\begin{equation*}\n\t\tp(\\omega) = \\begin{cases}  0, &\\mbox{if } \\omega \\not\\models P \\\\\n\t\t\\displaystyle \\prod_{c_i \\in \\omega: c_i = T} p_i  \\cdot \\prod_{c_j \\in \\omega: c_j = F} (1 - p_j), & \\mbox{if } \\omega \\models P  \\end{cases}\n\t\t\\end{equation*}\n\t\\end{boxedexample}\n\tThe second type of StarAI system generalizes undirected graphical models such as Markov networks or random fields.\n\tThe prototypical example is  Markov Logic Networks (MLNs) , and also Probabilistic Soft Logic (PSL)  follows this idea.\n\tUndirected StarAI models consist of  a set of weighted clauses $w:h_1 \\vee ... \\vee h_k \\leftarrow  b_1 \\wedge  ... \\wedge b_m$\n  that become soft constraints. The higher the weight of a ground clause, the less likely  possible worlds that violate these constraints are. In the limit, when the weight is $+\\infty$ the constraint must be satisfied and becomes a purely logical constraint, a hard constraint.\n\tThe weighted clauses specify a more general relationship between the conclusion and the condition than the definite clauses of directed models. \n\tWhile clauses of undirected models can still be used in (resolution) theorem provers, they are commonly viewed as constraints that relate these two sets of atoms.\n\tSuch undirected  StarAI models can be mapped to an undirected probabilistic graphical model in which there is a one-to-one correspondence between grounded weighted clauses and factors, as we show in Example \\ref{ex:mln}.\n\t\\begin{figure}[t]\n\t\t\\centering\n\t\t\\include{to_include/markov_field}\n\t\t\\caption{The Markov Field corresponding to the Markov logic network in Example \\ref{ex:mln}}\n\t\t\\label{fig:mln}\n\t\\end{figure}\n\t\\begin{boxedexample}[label = ex:mln]{Markov Logic Networks}\n\t\tWe show a probabilistic extension (adapted from ) of the theory in Example~\\ref{ex:model_based} using the formalism of Markov Logic Networks. We use a First Order language with domain $D = \\{\\textit{john},\\textit{mary}\\}$ and weighted clauses $\\alpha_1$ and $\\alpha_2$, i.e.:\n\t\t\\begin{align*}\n\t\t\\alpha_1: \\quad &\\mathtt{\n\t\t\t2.0::smokes(Y) \\leftarrow smokes(X), influences(X,Y)} \\\\\n\t\t\\alpha_2: \\quad &\\mathtt{0.5::smokes(X) \\leftarrow  stress(X)}\n\t\t\\end{align*}\n\t\tIn Figure~\\ref{fig:mln}, we show the corresponding Markov field.\n\t\tA Markov Logic Network defines a probability distribution over possible worlds as follows.\n\t\tLet $ A = [\\alpha_1, \\cdots, \\alpha_n]$ be a set of logical clauses and let $B = [\\beta_1, \\cdots, \\beta_n]$ the corresponding positive weights. Let $\\theta_j$ be a possible assignment of constants (from the domain $D$) to the variables (e.g. $\\mathtt{X,Y}$) of the clause $\\alpha_i$, that is, a substitution. Let  $\\alpha_i\\theta_j$ the grounded clause where all variables in $\\theta_j$ have been replaced by their corresponding constants. Finally, let $\\mathbbm{1}(\\omega,\\alpha_i\\theta_i)$ be an indicator function, evaluating to 1 if the ground clause is \\textit{True} in $\\omega$, 0 otherwise. \n\t\tThe probabilistic semantics of Markov Logic is the distribution (with $Z$ the normalization constant):\n\t\t\\begin{equation*}\n\t\tp(\\omega) = \\frac{1}{Z} \\exp \\big(\\sum_i \\beta_i \\sum_{j} \\mathbbm{1}(\\omega, \\alpha_i\\theta_j) \\big)\n\t\t\\end{equation*}\n\t\tIntuitively, in MLNs, a world is more probable if it makes many of its ground instances \\textit{True}. Notice that MLNs are usually defined on first-order clause theories, with variables and domains. We will further investigate this issue in Section \\ref{sec:syntax}.\n\t\\end{boxedexample}", "cites": [5706], "cite_extract_rate": 0.1, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section draws a conceptual parallel between directed/undirected graphical models and proof-theoretic/model-theoretic logic, synthesizing foundational ideas from StarAI. It abstracts these ideas to present a unifying perspective, particularly through the example of ProbLog. While it provides some critical context in mapping logic to probabilistic models, it focuses more on explanation than deep evaluation or comparison of limitations."}}
{"id": "dfea86d8-fb1c-484d-a9d2-d17c03c4d644", "title": "Implications for NeSy", "level": "subsection", "subsections": [], "parent_id": "71d65d84-75cd-4c92-b14d-bddcaa016dd0", "prefix_titles": [["title", "From Statistical Relational to Neurosymbolic \\\\ Artificial Intelligence: a Survey."], ["section", "Proof- vs Model-theoretic View of Logic"], ["subsection", "Implications for NeSy"]], "content": "\\rev{The distinction between proof vs models and inference rules vs constraints, turns out to be fundamental for neurosymbolic systems as well.}\n\t\\begin{evidencebox}\n\t\t\\rev{In neurosymbolic AI, weighted clauses are not used to construct a probabilistic graphical model, but they are likewise used to construct neural models. More specifically, NeSy systems that exploit a proof-theoretic approach use the proofs to build the \\textit{architecture} of the neural net. On the other side of the spectrum, NeSy systems that exploit a model-theoretic approach use the constraints to build a \\textit{loss function} for the neural net. }\n\t\\end{evidencebox}\n\t\\rev{Both choices are extremely natural.\n     Proof trees capture the structure of the inference process in a graphical representation. Therefore, they can be used as the structure of the neural network computation, which corresponds to their architecture.  On the other hand, the desired behaviour of the variables is expressed in terms of constraints. \n     Loss functions are the de facto standard to enforce desired behaviours on the output variables of a neural network.}\n\\rev{First, we survey proof-based NeSy models, which use  theorem proving for  logical inference and  proofs to template the neural architecture.  In particular, when proving a specific query atom, they keep track of all the used rules in a proof tree, such as the one  shown in Example \\ref{ex:logic_program}.\tWeights on facts and rules are then used to label leaves or edges of the tree, respectively, while real valued activation functions are used to label the AND and OR nodes. The result is a computational graph that can be executed (or evaluated) bottom-up, starting from the leaves up to the root. Generally speaking, the output of the computational graph is a \\textit{score} for the query atom. Different semantics  can be exploited in building the computational graph, ranging from relaxations of truth values (such as in fuzzy logic) to probabilities (see Section \\ref{sec:semantics}). The connection between the proof tree and the neural network suggests schemes for learning the parameters of these models. Indeed, the obtained computational graph is always differentiable. Thus, given a set of atoms that are known to be \\textit{True} (resp. \\textit{False}), one can maximize (resp. minimize) their score using the corresponding computational graphs.\n\tInference in these models is turned into \\textit{evaluation} of the computational graph. The direction of the rules indicates the direction of the evaluation, in the same way as it indicates the direction of inference in logic programming. \n\tAmong this category are} systems based on Prolog or Datalog, such as  TensorLog , Neural Theorem Provers (NTPs) , NLProlog , DeepProbLog , NLog  and DiffLog .\n\tLifted Relational Neural Networks (LRNNs)  and $\\partial$ILP  are other examples of non-probabilistic directed models, where weighted definite clauses are compiled into a neural network architecture in a forward chaining fashion.\n\tThe systems that imitate logical reasoning with tensor calculus, Neural Logic Programming (NeuralLP)  and Neural Logic Machines (NLM) ,  are likewise instances of directed logic. An example of a proof-based NeSy model is given in Example \\ref{ex:kbann}. \n\t\\begin{figure}[t]\n\t\t\\centering\n\t\t\\begin{subfigure}[b]{0.55\\textwidth}\n\t\t\t\\centering\n\t\t\t\\begin{prolog}\nalarm <- earthquake. \nalarm <- burglary.\ncalls_mary <- alarm, \n               hears_alarm_mary.\n\t\t\t\\end{prolog}\n\t\t\t\\caption{}\n\t\t\\end{subfigure}\n\t\t\\begin{subfigure}[b]{0.43\\textwidth}\n\t\t\t\\centering\n\t\t\t\\include{to_include/and_or_simple}\n\t\t\t\\caption{}\n\t\t\\end{subfigure}\n\t\t\\begin{subfigure}[b]{0.48\\textwidth}\n\t\t\t\\centering\n\t\t\t\\include{to_include/and_or_nn}\n\t\t\t\\caption{}\n\t\t\\end{subfigure}\n\t\t\\begin{subfigure}[b]{0.48\\textwidth}\n\t\t\t\\centering\n\t\t\t\\include{to_include/and_or_hidden}\n\t\t\t\\caption{}\n\t\t\\end{subfigure}\n\t\t\\begin{subfigure}[b]{0.48\\textwidth}\n\t\t\t\\centering\n\t\t\t\\include{to_include/and_or_fully}\n\t\t\t\\caption{}\n\t\t\\end{subfigure}\n\t\t\\caption{Knowledge-Based Artificial Neural Network. Network creation process. (1) the initial logic program; (2) the AND-OR tree for the query \\textit{calls\\_mary}; (3) mapping the tree into a neural network; (4) adding hidden neurons, (5) adding interlayer connections.}\n\t\t\\label{fig:kbann}\n\t\\end{figure}\n\\begin{boxedexample}[label = ex:kbann]{Knowledge-Based Artificial Neural Networks}\n\t\\noindent Knowledge-Based Artificial Neural Networks (KBANN) is the first method to use definite clausal logic and theorem proving to template the architecture of a neural network. \n\t\\noindent KBANN turns a program into a neural network in several steps:\n\t\\begin{enumerate}\n\t\t\\item KBANN starts from a definite clause program and a set of queries.\n\t\t\\item The program is turned into an AND-OR tree using the proofs for the queries.\n\t\t\\item The AND-OR tree is turned into a neural network with a similar structure. Nodes are divided into layers. The weights and the biases are set such that evaluating the network returns the same outcome of querying the program.\n\t\t\\item New hidden units are added. Hidden units play the role of unknown rules that need to be learned. They are initialized with zero weights; i.e. they are inactive.\n\t\t\\item New links are added from each layer to the next one, obtaining the final neural network. \n\t\\end{enumerate}\n\t\\noindent An example of this process is shown in Figure~\\ref{fig:kbann}. KBANN needs some restrictions over the kind of rules. In particular, the rules are assumed to be conjunctive, non-recursive, and variable-free (or propositional). Many of these restrictions are removed by more recent systems.\n\\end{boxedexample}\n\\rev{We now survey the second class of NeSy systems, the  model-based ones. These systems use logic to define a loss function (usually a regularization term) for  neural networks. The networks compute scores for the set of atoms that  correspond to the output neurons. At each training step, the logic-based loss function determines the degree to which the assigned scores violate the logical theory and uses this to determine the penalty. \nLogical inference is turned into a learning problem (i.e. ``learning to satisfy'') and it is usually cast in a variational optimization scheme.\\footnote{This is reminiscent of the variational approach to probabilistic inference in probabilistic graphical models and constitutes a further parallel between the fields.}\nAs a consequence, in constraint-based models, the neural network has to solve two tasks at the same time: solving a subsymbolic learning problem (e.g. perception) as well as approximating the logical inference process  .}\nA large group of NeSy approaches, including Semantic Based Regularization (SBR) , Logic Tensor Networks (LTN) ,  Semantic Loss (SL)  and DL2 ,  exploits logical knowledge as a soft regularization constraint that favours solutions that satisfy the logical constraints. SBR and LTN compute atom (fuzzy) truth assignments as the output of the neural network and translate the provided logical formulas into a real valued regularization loss term using fuzzy logic.  SL uses marginal probabilities of the target atoms to define the regularization term and relies on  arithmetic circuits  to evaluate it efficiently, as detailed in Example \\ref{ex:sl}.  DL2 defines a numerical loss providing no specific fuzzy or probabilistic semantics, which allows for including numerical variables in the formulas (e.g. by using a logical term $x > 1.5$).  Another group of approaches, including Neural Markov Logic Networks (NMLN)  and Relational Neural Machines (RNM)  extend MLNs,  allowing factors of exponential distributions to be implemented as neural architectures.  Finally,  compute ground atoms scores as dot products between relation and entity embeddings; implication rules are then translated into a logical loss through a continuous relaxation of the implication operator.\n\t\\begin{boxedexample}[label = ex:sl]{Semantic Loss}\n\t\t\\noindent The Semantic Loss  is an example of an undirected model where (probabilistic) logic is exploited as a \\textit{regularization} term in training a neural model.\n\t\t\\noindent Let $p = [p_1,\\dots,p_n]$ be a vector of probabilities for a list of propositional variables $X = [X_1,\\dots,X_n]$. In particular, $p_i$ denotes the probability of variable $X_i$ being \\textit{True} and corresponds to a single output of a neural net having $n$ outputs.\n\t\tLet  $\\alpha$ be a logic sentence defined over $X$.\n\t\t\\noindent Then, the \\textit{semantic loss} between $\\alpha$ and $p$ is:\n\t\t\\begin{equation*}\n\t\tLoss(\\alpha,p) \\propto - \\log \\,\\, \\sum_{x \\models \\alpha} \\,\\,\\, \\prod_{i: x \\models X_i} p_i \\,\\,\\, \\prod_{i: x \\models \\neg X_i}  (1-p_i).\n\t\t\\end{equation*}\n\t\t\\noindent The authors provide the intuition behind this loss: \n\t\t\\begin{quote}\n\t\t\t\\textit{The semantic loss is proportional to the negative logarithm of the probability of generating a state that satisfies the constraint  when sampling values according to $p$.}\n\t\t\\end{quote} \n\t\t\\noindent Suppose you want to solve a multi-class classification task (example adapted from ), where each input example must be assigned to a single class. Then, one would like to enforce \\textit{mutual exclusivity} among the classes. This can be easily done on supervised examples, by coupling a softmax activation layer with a cross entropy loss. However, there is no standard way to impose this constraint for unlabeled data, which can be useful in a semi-supervised setting.\n\t\t\\noindent The solution provided by the Semantic Loss framework is to encode mutual exclusivity into the propositional constraint $\\beta$:\n\t\t\\begin{equation*}\n\t\t\\beta = (X_1 \\land \\neg X_2 \\land \\neg X_3) \\lor\n\t\t(\\neg X_1 \\land  X_2 \\land \\neg X_3) \\lor\n\t\t(\\neg X_1 \\land  \\neg X_2 \\land  X_3)\n\t\t\\end{equation*}\n\t\tConsider a neural network classifier with three outputs $ p =[ p_1, p_2, p_3]$.  Then, for each input example (whether labeled or unlabeled), we can build the semantic loss term:\n\t\t\\begin{equation*}\n\t\tL(\\beta,p) = p_1(1 - p_2)(1 - p_3) +\n\t\t(1 - p_1)p_2(1 - p_3) +\n\t\t(1 - p_1)(1 - p_2)p_3\n\t\t\\end{equation*}\n\t\t\\noindent It can be summed up to the standard cross-entropy term for the labeled examples.\n\t\tUnlike for directed methods such as KBANN (Example \\ref{ex:kbann}) and TensorLog, \n     the logic is turned into a loss-function that is used during training. The function constrains the underlying probabilities, but there are no directed or causal relationships among them.\n\tMoreover, during inference only the probabilities $p$ are used while the logic formula $\\beta$ is not used anymore. On the contrary, in KBANN, the logic is compiled into the architecture of the network and, therefore, it it is also exploited at evaluation time. \n\t\\end{boxedexample}\n\t\\rev{To conclude, let us stress a key difference between \n the two classes of NeSy systems w.r.t. \\textit{the way they incorporate the knowledge expressed in the logical clauses}. Proof-based, directed models use logic to define the architecture of a neural symbolic network. \n Thus, logic is part of the inference of the model and acts as a structural constraint. The designer has full control of where and how the logic is used inside the network. Thus, logical knowledge can easily be extended or modified at test-time, without the need to retrain, leading to a high degree of modularity and out-of-distribution generalization . On the other hand, when logic is only encoded in an objective function, the neural net learns to (approximately) satisfy it. Therefore, the  knowledge is only latently encoded in the weights of the network, which leads to a loss of control and interpretability. However, the latter techniques are often much more scalable, especially at inference time. The balance between control and interpretability, on the one hand, and scalability, on the other hand, is an open and important research question in the \\nesy{} community. }", "cites": [2665, 5707, 3726, 5709, 3727, 3741, 8942, 7988, 5710, 5711, 4323, 7304, 3672, 5708], "cite_extract_rate": 0.6363636363636364, "origin_cites_number": 22, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the core distinction between proof- and model-theoretic approaches in NeSy, connecting multiple cited papers to build a coherent narrative about how logical reasoning informs neural architecture and loss functions. It offers critical analysis by highlighting limitations (e.g., KBANNâ€™s propositional restrictions) and by explaining the trade-offs between reasoning and learning. The abstraction is strong, as it frames these systems within a unified conceptual model involving computational graphs, regularization, and end-to-end learning."}}
{"id": "7f592cdb-5aaa-4d78-a9de-739ab8dc18a6", "title": "Implications for  \\nesy{", "level": "subsection", "subsections": [], "parent_id": "394ef4e2-3d6b-46dc-9354-ff1b43665302", "prefix_titles": [["title", "From Statistical Relational to Neurosymbolic \\\\ Artificial Intelligence: a Survey."], ["section", "Logic - Syntax"], ["subsection", "Implications for  \\nesy{"]], "content": "}\n\t\\rev{NeSy  exploits the internal structure of literals, resulting in many relational and first-order systems. System exploiting propositional logic are  Semantic Loss (SL) ~ and DL2~.  Relational logic-based systems are  DiffLog~, $\\theta$ILP , Lifted Relational Neural Networks (LRNN) ,  Neural Theorem Provers (NTP)  \n\tand NeurASP~.\n\t\tFinally, many systems are based on  first-order logic or first-order logic programs, such as \n\t\tDeepProbLog , NLog , NLProlog , \n\t\tDeepStochLog ,\n\t\tLogic Tensor Networks , Semantic Based Regularization , Relational Neural Machines  and Logical Neural Networks .}\n\t\\rev{The focus in \\nesy{} on structured terms is strongly related to that in StarAI and plays a fundamental role in \\nesy{}. \n\t In fact, grounding a relational or first-order theory can often be seen as unrolling either the architecture (e.g., DeepStochLog, LRNN ) or the loss function (e.g., SBR , LTN ) of the corresponding neural model. Unrolling fixed modules over multiple elements of a complex data structure is fundamental to neural networks on sequences (recurrent nets, RNN), trees (recursive nets,  RvNN) and graphs (graph nets, GNN).  \\nesy{} can be regarded as unrolling more complex logical structures, with similar benefits in terms of model capacity, modularization and generalization, and strong control due to the formal semantics.}\n\t\\rev{Moreover, first-order  \\nesy{} models can explicitly deal with how subsymbolic data (e.g. images or audio) are fed to the neural components of the system.\n In fact, \\nesy{} systems often use subsymbolic data samples as elements of the domain of discourse. For example, the element \\textit{mary} can be used to refer to an image, e.g. $mary = \\smallimg{mary.jpeg}$. Feeding such samples as input to a neural network can then be naturally encoded as  grounding  a  predicate over the domain of interest. When the internal structure of the literals is absent, as in SL, this mapping must be handled outside the logical framework.}\nWhile both relational logic and first-order logic have their advantages, there is a noteworthy distinction in the latter. \nFirst-order logic allows representing real valued functions through the use of functors. For example, segmentation can be modeled as a functor returning the bounding box of an object inside an image, e.g. \\textit{location(mary,image)} . Therefore, FOL-based systems can address regression tasks, diverging from the conventional classification tasks associated with relational logic systems.", "cites": [5714, 3741, 5712, 5710, 5707, 7304, 4323, 5709, 5713, 8943, 3726, 3727], "cite_extract_rate": 0.75, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the cited papers by grouping them based on the logical syntax they use (propositional, relational, first-order), highlighting how these choices affect their functionality and integration with neural components. It provides a critical evaluation by pointing out limitations, such as the need to handle mappings outside the logical framework for propositional systems. Furthermore, it abstracts these findings by connecting them to broader AI concepts like unrolling architectures and modularization, offering meta-level insights."}}
{"id": "09725807-0a82-437e-980b-ae2733823d35", "title": "Implications for StarAI", "level": "subsection", "subsections": [], "parent_id": "3f30c5c1-1c7c-4ae0-b5f0-88220243574e", "prefix_titles": [["title", "From Statistical Relational to Neurosymbolic \\\\ Artificial Intelligence: a Survey."], ["section", "Logic - Semantics"], ["subsection", "Implications for StarAI"]], "content": "Statistical Relational AI has extended the previous semantics by defining probability distributions $p(\\omega)$ over models, or \\textit{possible worlds}\\footnote{In this paper, we use the distribution semantics as representative of the probabilistic approach to logic. While this is the most common solution in StarAI, many other solutions exist , whose description is out of the scope of the current survey. A detailed overview of the different flavours of formal reasoning about uncertainty can be found in .}.\nThe goal is to reason about the uncertainty of logical statements. In particular, the probability that a certain formula $\\alpha$ holds is computed as the sum of the probabilities of the possible worlds that are models of $\\alpha$ (i.e. where $\\alpha$ is True):\n\t\\begin{equation}\n\t\\label{eq:wmc}\n\tp(\\alpha) = \\sum_{\\omega \\models \\alpha} p(\\omega) \n\t\\end{equation}\n\tThis is an instance of the Weighted Model Counting (WMC) problem. In fact, we are counting how many worlds are models of $\\alpha$ and we are weighting each of them by its  probability according to the distribution $p(\\omega)$.\n\t\\begin{table}\n\t\t\\centering\n\t\t\\begin{tabular}{cccc|l}\n\t\t\tB & E & J & M & $p(\\omega)$ \\\\\n\t\t\t\\hline\n\t\t\tF&F&F&F& 0.2394 \\\\\n\t\t\tF&F&F&T& 0.1026 \\\\\n\t\t\tF&F&T&F& 0.3591 \\\\\n\t\t\tF&F&T&T& 0.1539 \\\\\n\t\t\tF&T&F&F& 0.0126 \\\\\n\t\t\tF&T&F&T& 0.0054 \\\\\n\t\t\tF&T&T&F& 0.0189 \\\\\n\t\t\tF&T&T&T& 0.0081 \\\\\n\t\t\tT&F&F&F& 0.0266 \\\\\n\t\t\tT&F&F&T& 0.0114 \\\\\n\t\t\tT&F&T&F& 0.0399 \\\\\n\t\t\tT&F&T&T& 0.0171  \\\\\n\t\t\tT&T&F&F& 0.0014 * \\\\\n\t\t\tT&T&F&T& 0.0006 * \\\\\n\t\t\tT&T&T&F& 0.0021 * \\\\\n\t\t\tT&T&T&T& 0.0009 * \n\t\t\\end{tabular}\n\t\t\\caption{A distribution over possible worlds for the four propositional variables $burglary$ (B), $earthquake$ (E), $hears\\_alarm\\_john$ (J) and  $hears\\_alarm\\_mary$ (M). The $*$ indicates those worlds where $burglary \\wedge earthquake$ is \\textit{True}.}\n\t\t\\label{tab:distribution_semantics}\n\t\\end{table}\n\t\\begin{boxedexample}[label = ex:distr_semantics]{Probabilistic Logic}\n\t\tLet us consider the following set of propositions $B = burglary$, $E = earthquake$, $J = hears\\_alarm\\_john$ and $M = hears\\_alarm\\_mary$. In probabilistic logic, a probability distribution over all the possible worlds is defined. For example, Table \\ref{tab:distribution_semantics} represents a valid distribution.\n\t\tSuppose we want to compute the probability of the formula $burglary \\wedge earthquake$. This is done by summing up the probabilities of all the worlds where both $burglary$ and $earthquake$ are \\textit{True} (indicated by a $*$ in Table \\ref{tab:distribution_semantics}). \n\t\\end{boxedexample}\n    The StarAI community has provided several formalisms to define such  probability distributions over possible worlds using labeled logic theories. Probabilistic Logic Programs (cf. Example \\ref{ex:problog}) and Markov logic networks (cf. Example \\ref{ex:mln}) are two prototypical frameworks. For example, the distribution in Table~\\ref{tab:distribution_semantics} is the one modeled by the ProbLog program in Example \\ref{ex:problog}.\n\\rev{It is interesting to compare Markov Logic (Example \\ref{ex:mln}) to ProbLog (Example \\ref{ex:problog}) in terms of their model-theoretic semantics. Markov Logic is defined as a set of weighted full clauses, i.e. as an unnormalized  probability distribution over full clausal theories.\nThis means that, given any subset of the theory, there can be many possible models. For instance, the theory $a \\vee b$, has three possible models. To obtain a probability distribution over models, Markov Logic needs to distribute the probability mass over its models. \nTo do this, the maximum entropy principle is used, which results in equal distributions of the probability mass.} \n\\rev{Conversely, ProbLog defines a probability distribution over definite clause theories, each obtained as subsets of the provided probabilistic facts. However, since each of these theories has a unique least Herbrand model, the probability mass corresponding to the selected facts is assigned to the corresponding unique Herbrand model. This means that when working with definite clauses only, there is no need to distribute the probability mass to multiple models and, therefore, no extra assumptions such as maximum entropy are necessary.} \nProbabilistic inference (i.e. weighted model counting) is generally intractable. That is why, in StarAI, techniques such as \\textit{knowledge compilation} (KC)~ are used.\n\tKnowledge compilation transforms a logical formula $\\alpha$ into a new representation in an  offline step, which can be computationally expensive. Using this new representation a particular set of queries can be answered efficiently (i.e. in poly-time in the size of the new representation). \n\tFrom a probabilistic point of view, this translation solves the disjoint-sum problem, which states that one cannot simply sum up the probability \n of two disjuncts but also has to subtract the probability of the intersection.\n After the translation, the probabilities of any conjunction and of any disjunction can be simply computed by multiplying, resp. summing up, the probabilities of their operands. Thus a  logical formula $\\alpha$ can be compiled into an arithmetic circuit $ac(\\alpha)$. The weighted model count of the query formula can then simply be computed by  evaluating  the corresponding arithmetic circuit bottom up; i.e.  $p(\\alpha) = ac(\\alpha)$. \n\t\\begin{figure}[t]\n\t\t\\centering\n\t\t\\noindent\n\t\t\\begin{subfigure}{0.4\\linewidth}\n\t\t\t\\include{to_include/dDNNF}\n\t\t\\end{subfigure}\n\t\t\\hfill\n\t\t\\begin{subfigure}{0.4\\linewidth}\n\t\t\t\\include{to_include/arithmetic_circuit}\n\t\t\\end{subfigure}\n\t\t\\caption{dDNNF (left) and arithmetic circuit (right) corresponding to the ProbLog program in Example \\ref{ex:problog}}\n\t\t\\label{fig:kc}\n\t\\end{figure}\n\t\\begin{boxedexample}[label = ex:kc]{Knowledge Compilation}\n\t\tLet us consider the ProbLog program in Example \\ref{ex:problog} and the corresponding tabular representation in Table \\ref{tab:distribution_semantics}. Let us consider the query $q = calls(mary)$. Now we can use Equation~\\ref{eq:wmc} to compute the probability $p(q)$. To do this, we iterate over the table and we sum all the probabilities of the worlds where $calls(mary)$ is True, which we know from Example \\ref{ex:logic_program} are those where either $burglary=T$ or $earthquake=T$ and where $hears\\_alarm(mary)=T$. This yields $p(q) = 0.0435$. This method would require us to iterate over $2^N$ terms (where $N$ is the number of probabilistic facts).\n\t\tKnowledge compilation compiles $\\alpha$ into some normal form that is logically equivalent. In Figure \\ref{fig:kc}, the target representation is a decomposable, deterministic negative normal form (d-DNNF)~, for which weighted model counting is poly-time in the size of the formula. Decomposability means that, for every conjunction, the two conjuncts do not share any variables. Deterministic means that, for every disjunction, the two disjuncts are \\rev{mutually exclusive, i.e., only one of the disjuncts can be true at the same time}. \n\t\tThe formula in d-DNNF can then be straightforwardly turned into an arithmetic circuit by substituting AND nodes with multiplication and OR nodes by summation. \n  In Figure \\ref{fig:kc}, we show the d-DNNF and the arithmetic circuit of the distribution defined by the ProbLog program in Example \\ref{ex:problog}. The bottom-up evaluation of this arithmetic circuit computes the correct marginal probability $p(\\alpha)$ much more efficiently than the naive iterative sum that we have computed before. \n\t\\end{boxedexample}\n\tEven though probabilistic Boolean logic is the most common choice in StarAI, some approaches use probabilistic fuzzy logic. The most prominent approach is Probabilistic Soft Logic (PSL) , illustrated in Example~\\ref{ex:psl}. Similarly to Markov logic networks, Probabilistic Soft Logic (PSL) defines log linear models where features are represented by ground clauses. However, PSL uses a fuzzy semantics of the logical theory. Therefore, atoms are mapped to real valued  variables and ground clauses to real valued factors.\n\t\\begin{boxedexample}[label = ex:psl]{Probabilistic Soft Logic}\n\t\tLet us consider the logical rule $\\alpha =  smokes(X) \\leftarrow stress(X)$ with weight $\\beta$.\n\t\tAs we have seen in Example~\\ref{ex:mln}, Markov Logic translates the formula into a discrete factor by using the indicator functions $\\mathbbm{1}(\\omega, \\alpha\\theta)$:\n\t\t\\begin{equation*}\n\t\t\\phi^{MLN}(\\omega, \\alpha) = \\beta \\mathbbm{1}(\\omega, \\alpha\\{X/mary\\})  + \\beta \\mathbbm{1}(\\omega, \\alpha\\{X/john\\})\n\t\t\\end{equation*}\n\t\tInstead of discrete indicator functions, PSL  translates the formula into a continuous t-norm based function:\n\t\t\\begin{equation*}\n\t\tt(\\omega, \\alpha) = \\min(1, 1 - stress(X) + smokes(X))\n\t\t\\end{equation*}\n\t\tand the corresponding potential is then translated into the continuous and differentiable function:\n\t\t\\begin{equation*}\n\t\t\\phi^{PSL}(\\omega, \\alpha) = \\beta t(\\omega, \\alpha\\{X/mary\\})  + \\beta t(\\omega, \\alpha\\{X/john\\})\n\t\t\\end{equation*}\n\t\tAnother important task in StarAI is MAP inference. In MAP inference, given the distribution $p(\\omega)$, one is interested in finding the interpretation $\\omega^\\star$ where $p$ is maximal, i.e.\n\t\t\\begin{equation}\n\t\t\\label{eq:map}\n\t\t\\omega^\\star = \\text{arg}\\max_\\omega p(\\omega)\n\t\t\\end{equation}\n\t\tWhen the $\\omega$ is a boolean interpretation, i.e. $\\omega \\in \\{0,1\\}^n$, like in ProbLog or MLNs, this problem is \\rev{related to maxSAT, which is NP-hard}.  However, in PSL, $\\omega$ is a fuzzy interpretation, i.e. $\\omega \\in [0,1]^n$ and $p(\\omega) \\propto \\exp\\big(\\sum_i \\beta_i \\phi(\\omega, \\alpha_i)\\big)$ is a continuous and differentiable function. The MAP inference problem can thus be \\textit{approximated} more efficiently than its boolean counterpart using gradient-based techniques.\n\t\\end{boxedexample}", "cites": [8685, 5706], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section insightfully integrates concepts from cited papers to explain the semantics of StarAI and knowledge compilation. It synthesizes the use of probabilistic logic programs and Markov logic networks, and abstracts these into general principles like decomposability and determinism in d-DNNF. It also provides critical comparisons between formalisms (e.g., Markov Logic vs. ProbLog) and highlights the computational benefits of compilation techniques."}}
{"id": "0c28445b-24b7-4df7-a61f-1e0537492ed8", "title": "Implications for NeSy", "level": "subsection", "subsections": [], "parent_id": "3f30c5c1-1c7c-4ae0-b5f0-88220243574e", "prefix_titles": [["title", "From Statistical Relational to Neurosymbolic \\\\ Artificial Intelligence: a Survey."], ["section", "Logic - Semantics"], ["subsection", "Implications for NeSy"]], "content": "We have seen that in StarAI, one can turn inference tasks into the evaluation (as in KC) or gradient-based optimization (as in PSL) of a differentiable parametric circuit. The parameters are scalar values (e.g. probabilities or truth degrees) that are attached to basic elements of a logical theory (facts or clauses).\n\tA natural way of carrying over the StarAI approach to NeSy is the reparameterization method. Reparameterization substitutes the scalar values assigned to facts or formulas with the output of a neural network. One can interpret this substitution in terms of a different parameterization of the original model. Many probabilistic methods parameterize the underlying distribution in terms of neural components. In particular, as we show in Example \\ref{ex:deepproblog}, DeepProbLog exploits neural predicates to compute the probabilities of probabilistic facts as the output of neural computations over vectorial representations of the constants, which is similar to SL in the propositional counterpart (see Example \\ref{ex:sl}).   NeurASP also inherits the concept of a neural predicate from DeepProbLog.\n\t\\begin{boxedexample}[label = ex:deepproblog]{Probabilistic semantics reparameterization in  DeepProbLog}\n\t\tDeepProbLog  is a neural extension of the probabilistic logic programming language ProbLog. DeepProbLog allows images or other subsymbolic representations as terms of the program.\n\t\tLet us consider a possible neural extension of the program in Example~\\ref{ex:problog}. We could extend the predicate $calls(X)$ with two extra inputs, i.e. $calls(B,E,X)$. $B$ is supposed to contain an image of a security camera, while $E$ is supposed to contain the time-series of a seismic sensor. We would like to answer queries like $calls(\\smallimg{burglary1.png},\\smallimg{earthquake1.png},mary)$, i.e. what is the probability that $mary$ calls, given that the security camera has captured the image $\\smallimg{burglary1.png}$ and the sensor the signal $\\smallimg{earthquake1.png}$ .\n\t\tDeepProbLog can answer this query using  the following program:\n\t\t\\begin{lstlisting}\n\t\tnn(nn_burglary, [B]) :: burglary(B).\n\t\tnn(nn_earthquake, [E]) :: earthquake(E).\n\t\t0.3::hears_alarm(mary). \n\t\t0.6::hears_alarm(john). \n\t\talarm(B,_) <- burglary(B).\n\t\talarm(_,E) <- earthquake(E).\n\t\tcalls(B,E, X) <- alarm(B,E), hears_alarm(X).\n\t\t\\end{lstlisting}\n\t\tHere, the program has been extended in two ways. First, new arguments (i.e. $B$ and $E$) have been introduced in order to deal with the subsymbolic inputs. Second, the probabilistic facts $burglary$ and $earthquake$ have been turned into \\textit{neural predicates}. Neural predicates are special probabilistic facts that are annotated by neural networks instead of by scalar probabilities.\n\t\tInference in DeepProbLog mimics that of ProbLog. Given the query and the program, knowledge compilation is used to build the arithmetic circuit in Figure \\ref{fig:deepproblog}.\n\t\tSince the program is structurally identical to the purely symbolic one in Example \\ref{ex:kc}, the arithmetic circuit is exactly the same. The only  only difference is that some leaves of the tree (i.e. capturing probabilities of  facts) can now also be neural networks.\n\t\tGiven a set of queries that are \\textit{True}, i.e.:\n\t\t\\begin{align*}\n\t\t\\mathcal{D} = \\{&calls(\\smallimg{burglary1.png},\\smallimg{earthquake1.png},mary),\\\\  &calls(\\smallimg{burglary2.png},\\smallimg{earthquake2.png},john), \\\\ &calls(\\smallimg{burglary3.png},\\smallimg{earthquake3.png},mary), ...\\},\n\t\t\\end{align*} we can train the parameters $\\theta$ of the DeepProbLog program (both neural networks and scalar probabilities) by maximizing the log-likelihood of the training queries using gradient descent:\n\t\t\\begin{equation*}\n\t\t\\max_{\\theta} \\sum_{q \\in \\mathcal{D}} \\log p(q)\n\t\t\\end{equation*}\n\t\\end{boxedexample}\n\t\\begin{figure}[t]\n\t\t\\centering\n\t\t\\include{to_include/arithmetic_circuit_dpl}\n\t\t\\caption{A neural reparametrization of the arithmetic circuit in Example \\ref{ex:kc} as done by DeepProbLog (cf. Example \\ref{ex:deepproblog}). Dashed lines indicate a negative output, i.e 1 - x. \\rev{We use a different notation for negation than in Figure \\ref{fig:kc} to stress that both  leaves are parameterized by the same neural network}.}\n\t\t\\label{fig:deepproblog}\n\t\\end{figure}\n\tSimilarly to DeepProbLog, NMLNs and RNMs use neural networks to parameterize the factors (or the weights) of a Markov Logic Network.\n\t computes marginal probabilities as logistic functions over similarity measures between embeddings of entities and relations. An alternative solution to exploit a probabilistic semantics is to use knowledge graphs (see also  \\ref{sec:kge_gnn}) to define probabilistic priors to neural network predictions, as done in .\n\tSBR and LTN reparametrize fuzzy atoms using neural networks that take as inputs the feature representation of the constants and return the corresponding truth value, as shown in Example~\\ref{ex:sbr}. Logical rules are then relaxed into soft constraints using fuzzy logic. Many other systems exploit fuzzy logic to inject knowledge into neural models . These methods  can be regarded as variants of a unique conceptual framework as the differences are often minor and in the implementation details. \n\t\\begin{boxedexample}[label = ex:sbr]{Semantic-Based Regularization}\n\t\t\\noindent Semantic-Based Regularization (SBR)  is an example of an undirected model where fuzzy logic is exploited as a \\textit{regularization} term when training a neural model. \n\t\tLet us consider a possible grounding for the rule in Example \\ref{ex:psl}:\n\t\t\\begin{lstlisting}\n\t\tsmokes(mary) $\\leftarrow$ stress(mary)\n\t\t\\end{lstlisting}\n\t\tFor each grounded rule $r$, SBR builds a regularization loss term  $L(r)$ in the following way. First, it maps each constant $c$ (e.g. \\textit{mary}) to a set of (perceptual) features $x_c$ (e.g. a tensor of pixel intensities $x_\\texttt{mary}$). Each relation $r$ (e.g. \\textit{smokes, stress}) is then mapped to a neural network $f_r(x)$, where $x$ is the tensor of features of the input constants and the output is a truth degree in $[0,1]$. For example, the atom \\textit{smokes(mary)} is mapped to the function call $f_\\texttt{smokes}(x_\\texttt{mary})$.  \n\t\tThen, a fuzzy logic t-norm is selected and  logic connectives are mapped to the corresponding real valued functions. For example, when the \\L ukasiewicz t-norm is selected, the implication is mapped to the binary real function $f(x,y) = \\min(1, 1 - x + y)$.\n\t\tFor the rule above, the Semantic-Based Regularization loss term is (for the \\L ukasiewicz t-norm):\n\t\t\\begin{equation*}\n\t\tL^{\\text{\\L}}(r) = \\min \\Big(1, 1 - f_\\texttt{stress}(x_\\texttt{mary}) + f_\\texttt{smokes}(x_\\texttt{mary}) \\Big)\n\t\t\\end{equation*}\n\t\tThe aim of Semantic-Based Regularization is to use the regularization term together a with classical loss function for supervised learning to learn the functions associated to the relations (here $f_\\texttt{stress}$ and $f_\\texttt{smokes}$).\n\t\tIt is worth comparing this method with the Semantic Loss  (Example \\ref{ex:sl}). Both methods turn a logic formula (either propositional or first-order) to a real valued function that is used as a regularization term. However, because of the different semantics, these two methods have different properties. On the one hand, SL preserves the original logical semantics, by using probabilistic logic. However, due to the probabilistic assumption, the input formula cannot be compiled directly into a differentiable loss but needs to be first translated, i.e. compiled, into an equivalent deterministic and decomposable formula. While this step is necessary for the probabilistic model to be sound, the size of the resulting formula can be exponential in the size of the grounded theory. On the other hand, in SBR, the formula can be compiled directly into a differentiable loss, whose size is linear in the size of the grounded theory. However, in order to do so, the semantics of logic is altered, by turning it into fuzzy logic.\n\t\\end{boxedexample}\n\tFuzzy logic can also be used to relax rules. For example, in LRNN, $\\partial$ILP, DiffLog and the approach of , the scores of the proofs are computed  using fuzzy logic connectives. \n\tThe theory  t-norms has  identifying parameterized (i.e. weighted) classes of t-norms  that are very close to standard neural computation patterns (e.g. ReLU or sigmoidal layers). This creates an interesting, still not fully understood, connection between soft logical inference and inference in neural networks. A large class of methods  relaxes logical statements numerically, without explicitly defining a specific semantics.\n\tUsually, the atoms are assigned scores in $\\mathbb{R}$ computed by a neural scoring function over embeddings.\n\tNumerical approximations are then applied  either to combine these scores according to logical formulas or to aggregate proofs scores.\n\tThe resulting neural architecture is usually differentiable and, thus, trained end-to-end.\n\tSome NeSy methods, such as PSL, have used mixed probabilistic and fuzzy semantics. In particular, Deep Logic Models (DLM) extend PSL by adding neurally parameterized factors to the Markov field, while  uses fuzzy logic to train posterior regularizers for standard deep networks using knowledge distillation  . \n \\rev{The semantics of computational logic has also been explored and extended along other directions that have also been used within AI, for example, \\textit{modal} and \\textit{temporal} logics . While their analysis is out of the scope of the paper, it is worth mentioning that also such formalisms have been  investigation from a neurosymbolic perspective .}", "cites": [5712, 5707, 3726, 5718, 5717, 5715, 681, 5716, 3727, 5719, 5721, 5710, 5711, 5720, 4323, 3672], "cite_extract_rate": 0.6956521739130435, "origin_cites_number": 23, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.8, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes ideas from multiple cited papers (e.g., DeepProbLog, SBR, LTN) by presenting a unified perspective on how logical semantics can be reparameterized using neural networks in the NeSy context. It provides critical comparisons between probabilistic and fuzzy logic approaches, highlighting trade-offs in compilation complexity and semantic fidelity. The section abstracts these methods into a broader conceptual framework of neural reparameterization of logic, showing how different systems align with this principle despite their variations."}}
{"id": "9af5d6bb-eaff-4088-a25c-009703783367", "title": "Implications for  NeSy", "level": "subsection", "subsections": [], "parent_id": "3cf78a3e-98ed-45de-848f-3a61063e51b3", "prefix_titles": [["title", "From Statistical Relational to Neurosymbolic \\\\ Artificial Intelligence: a Survey."], ["section", "Structure versus Parameter Learning"], ["subsection", "Implications for  NeSy"]], "content": "While StarAI learning techniques are categorised exclusively as either structure or parameter learning, \\nesy{} learning techniques combine both.\n\tWe will now discuss four groups of NeSy learning approaches: neurally-guided search, structure learning via parameter learning, program sketching, and implicitly structure learning.\n\t\\textit{Neurally guided structure search}  is the \\nesy{} paradigm most similar to structure learning in StarAI.\n\tIt addresses one of the major weaknesses of StarAI structure learning methods - uninformed search over valid theories.\n\tInstead, neurally guided search relies on a \\textit{recognition model}, typically a neural network, to prioritise parts of the symbolic search space so that the target model can be found faster.\n\tGenerally speaking, the recognition model predicts the probability of a certain structure, e.g. a predicate or an entire clause, to be a part of the target model.\n\tFor instance, Deepcoder~ uses input-output examples to predict the probability of each predicate appearing in the target model.\n\tTherefore, Deepcoder turns a systematic search into an informed one by introducing a ranking over predicates in the search space.\n\tLikewise, EC$^{2}$~  derives the probability of a program solving the task at hand.\n\tSeveral approaches push this direction  further and explore the idea of replacing an explicit symbolic model space with an implicit generative model over symbolic models ~. \n\tFor instance, in , the authors learn a generative model over grammar rules, conditioned on the examples.\n\tStructure learning is then performed by sampling grammar rules from the generative model, according to their probability, and evaluating them symbolically on the provided examples. \n\tThese approaches clearly show how symbolic search can be made tractable by introducing various forms of guidance via neural models.\n\tThese guidance-based approaches reduce, to a large extent, the most important weakness of symbolic structure learning approaches - the generation of many useless clauses or models.\n\tOn the other hand, these approaches often need large amounts of data for training, sometimes millions of examples  even though creating data is relatively easy by enumerating random model structures and sampling examples from them~.\n\t\\begin{boxedexample}[label = ex:ngps]{Neurally-guided structure learning}\n\t\t\\rev{To illustrate neurally-guided search, we use the approach of  Zhang et al. .\n        StarAI techniques for structure learning typically perform a systematic search, which results in many useless models being tested.\n        Given $N$ atoms, we can construct $N^l$ clauses of length $l$; this is an enormous space that is difficult to search efficiently.} \\\\\n        \\rev{Zhang et al. sidestep the systematic search by introducing a neural network that chooses which programs to explore next.\n        This search space is made of clauses such that an empty clause is at the top and children are extensions of the empty clause with all possible predicates; their children are further extensions with all individual atoms.} \\\\\n        \\rev{The approach follows a top-down search strategy, exploring shorter clauses before longer ones, with a twist: instead of following a predefined order, the approach uses a neural network to decide which child to expand next. \n        The approach can be viewed as a best-first search with a heuristic function implemented by a neural model. To this end, the network  (1) encodes all literals in each clause separately, (2) scores all literals, (3) pools the scores of each literal per candidate, and (4) chooses the best candidate based on their scores.\n        Ordering the search space in this way leads to substantial improvements in computation time, typically several orders of magnitude.}\n\t\\end{boxedexample}\n\tAn alternative way to reduce the combinatorial complexity of learning is to  learn only a part of the program.\n\tThis is known as \\textit{program sketching:} a user provides an almost complete target model with certain parts being unspecified (known as \\textit{holes}). \n\tFor instance, when learning a model in the form of a (logic) program for sorting numbers or strings, the user might leave the comparison operator unspecified and provide the rest of the program.\n\tThe learning task is then  to fill  in the holes.\n\tExamples of \\nesy{} systems based on sketching are DeepProbLog and $\\partial$4, which fill in the holes in a (symbolic) program via neural networks.\n\tThe advantage of sketching is that it provides a nice interface for \\nesy{} systems, as the holes can be filled either symbolically or neurally.\n\tHoles provide a clear interface in terms of inputs and outputs and are agnostic to the specific implementation.\n\tThe disadvantage of sketching is that the user still needs to know, at least approximatively, the structure of the program.\n\tThe provided structure, the sketch,  acts as a strong  bias.\n\tDeciding which functionality is left as a hole is a non-trivial issue: as the sketch becomes less strict,  the search space becomes larger. \n\t\\textit{Structure learning via parameter learning} (Example \\ref{ex:difflog}) is arguably the most prominent learning paradigm in \\nesy{}, positioned in between the two StarAI learning paradigms.\n\tStructure learning via parameter learning is technically equivalent to parameter learning in that the learning tasks consists of learning the probabilities of a fixed set of clauses.\n\tHowever, in contrast to StarAI in which the user carefully selects the informative clauses, the clauses are typically enumerated from  user-provided templates of predefined complexity.\n\tConstructed in this way, the majority of clauses are noisy and erroneous  and are  of little use.\n\tThey would receive very low, but non-zero, probabilities.\n\t Approaches that follow this learning principle include NTPs , $\\partial$ILP , DeepProbLog, NeuralLP  and DiffLog .\n\tThe advantage of structure learning via parameter learning is that it removes the combinatorial search from the learning.\n\tHowever, the number of clauses that needs to be considered is still extremely large, which  leads to difficult optimisation problems (cf. ). Furthermore, irrelevant clauses are never removed from the model and are thus always considered during inference.\n\tThis can lead to spurious interactions even when low probabilities are associated to irrelevant clauses: as the number of irrelevant clauses is extremely large, their cumulative effect can be substantial.\n\t\\begin{boxedexample}[label = ex:difflog]{Structure learning via parameter learning}\n\t\tAs an illustration of structure learning via parameter learning, we focus on DiffLog .\n\t\tDiffLog expects the candidate clauses to be provided by the user.\n\t\tThe user can either provide the rules she knows are useful or construct them by using a clause template and instantiating it .. \n\t\tGiven a set of positive examples, DiffLog proceeds by constructing \\textit{derivation trees} for each example.\n\t\tConsider the problem of learning the connectivity relation over a graph.\n\t\tThe input tuples (background knowledge in StarAI terminology) specify edges in a graph\n\t\t\\begin{lstlisting}\n\tedge(a,b).  edge(b,c).  edge(b,d).  edge(d,e). edge(c,f).\n\t\t\\end{lstlisting}\n\t\tThe examples indicate the connectivity relations among the nodes in the graph (for simplicity, consider only the following two examples)\n\t\t\\begin{lstlisting}\n\t\tconnected(a,b). connected(a,c). \n\t\t\\end{lstlisting}\n\t\tAlso assume that the candidate clause set contains the following clauses (with $p_1$ and $p_2$ their weights):\n\t\t\\begin{lstlisting}\n\t\t$p_1$::connected(X,Y) <- edge(X,Y).      \n\t\t$p_2$::connected(X,Y) <- edge(X,Z), connected(Z,Y).\n\t\t\\end{lstlisting}\n\t\tDerivation trees are essentially proofs of individual examples that correspond to branches in the SLD-tree .\n\t\tFor instance, the example \\textit{connected(a,b)} can be proven using the first clause, whereas the example \\textit{connected(a,c)} can be proven by chaining the two clauses ($connected(a,c) \\leftarrow edge(a,b), connected(b,c)$ and $connected(b,c) \\leftarrow edge(b,c)$).\n\t\tDiffLog uses derivation trees to formulate the learning problem as numerical optimisation over the weights associated with the rules.\n\t\tMore precisely, DiffLog defines the probability of deriving an example as the product of the weights associated to the clauses used in the derivation tree of the corresponding example.\n\t\tFor instance, DiffLog would formulate the learning problem for the two examples as follows\n\t\t$$\\min_{p_1, p_2} \\  \\underbrace{(1 - p_1)}_{\\small \\tt connected(a,b)} + \\underbrace{(1 - p_1\\times p_2)}_{\\small \\tt connected(a,c)}.$$\n\t\\end{boxedexample}\n\tThe last group of approaches learns the structure of a program only \\textit{implicitly}.\n\tFor instance, Neural Markov Logic Networks (NMLN) , a generalisation of MLNs, extract structural features from relational data.\n\tWhereas MLNs define potentials only over cliques defined by the structure (logical formulas) of a model, NMLNs add potentials over \\textit{fragments} of data (projected over a subset of constants).\n\tNMLNs thus do not necessarily depend on the symbolic structure of the model, be it learned or provided by a user, but can still learn to exploit relational patterns present in data.\n\tMoreover, NMLNs can incorporate embeddings of constants.\n\tThe benefit of this approach is that it removes combinatorial search from learning and performs learning via more scalable gradient-based methods.\n\tHowever, one loses the ability to inspect and interpret the discovered structure.\n\tAdditionally, to retain tractability, NMLNs limit the size of fragments which  imposes limits on the complexity of the discovered relational structure.", "cites": [5724, 8942, 2665, 5710, 5722, 7989, 3726, 5723, 3727], "cite_extract_rate": 0.6, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.8, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section synthesizes multiple cited papers to create a coherent narrative about NeSy learning approaches, particularly neurally-guided search and structure learning via parameter learning. It critically analyzes the advantages and disadvantages of these methods, identifying limitations such as the need for large data and the issue of spurious interactions. The discussion abstracts beyond individual papers, identifying overarching patterns and principles in the NeSy paradigm."}}
{"id": "a5362428-8a2e-441c-a432-e33fed8ed15b", "title": "Translating between representations", "level": "paragraph", "subsections": [], "parent_id": "9758e29f-afe8-4348-b5ab-e226dfae12d2", "prefix_titles": [["title", "From Statistical Relational to Neurosymbolic \\\\ Artificial Intelligence: a Survey."], ["section", "Symbolic vs subsymbolic representations"], ["paragraph", "Comparing representations"], ["paragraph", "Translating between representations"]], "content": "Many systems need to translate back and forth between symbolic and subsymbolic representations. In fact, a lot of research on deep learning is devoted to efficiently representing symbols so that neural networks can properly leverage them.\n\tA straightforward example is to translate symbols to a subsymbolic representation that can serve as  input for a neural network. Generally, these symbols are replaced by a one-hot encoding or by learned embeddings. \n    \\rev{Note, however, that this does not imply that the system can perform symbolic manipulation on this input. Rather, it serves as an index to a set of learned, latent embeddings.}\n\tA more interesting example is encoding relations in subsymbolic space. The wide variety of methods~ developed for this purpose indicates that this is far from a solved problem. \n\tDifferent encodings have different benefits. For example, TransE~ encodes relations as vector translations from subject to object embeddings. A disadvantage is that symmetric relations are represented by the null vector, and entities in symmetric relations are pushed towards each other.\n\tMore complex structures are even harder to represent. For example, there is currently a lot of research in how to utilize graph-structured data in neural networks (cf. \\ref{sec:kge_gnn}).\n\tTranslating from a subsymbolic representation back to a symbolic one happens, for  example, at the end of a neural network classifier. Here, a subsymbolic vector needs to be translated to discrete classes. Generally, this happens through the use of a final layer with a soft-max activation function which then models the confidence scores of these classes as a categorical distribution. However, other options are possible. For example, some methods are only interested in the most likely class, and will use an arg-max instead. Alternatively, a Gumbel-softmax activation can be used as a differentiable approximation of sampling from the categorical distribution.", "cites": [1166, 1168], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates information from the cited papers by discussing how symbolic relations are encoded into subsymbolic representations, referencing TransE and complex embeddings. It provides a basic comparison of different methods and their limitations, such as TransE's issue with symmetric relations. The discussion generalizes to broader challenges in encoding symbolic structures, suggesting that the problem is still unsolved, which reflects some abstraction, though not at a highly meta-level."}}
{"id": "a1474eee-df30-4fdc-964c-c5b1bb43d21d", "title": "Implications for  StarAI and NeSy", "level": "subsection", "subsections": [], "parent_id": "bd238fce-3df5-4531-ba35-f4316c49d81b", "prefix_titles": [["title", "From Statistical Relational to Neurosymbolic \\\\ Artificial Intelligence: a Survey."], ["section", "Symbolic vs subsymbolic representations"], ["subsection", "Implications for  StarAI and NeSy"]], "content": "In StarAI systems, the input, intermediate and output representations are all using the same symbolic representations. \\rev{Although there are StarAI systems that can support numerical values, these are still treated as symbols, which is  different than a latent, subsymbolic representation.}\n\tIn neural systems, the input and intermediate representations are subsymbolic. The output representation can be either symbolic (e.g. classifiers) or subsymbolic (e.g. auto-encoders, GANs).\n\tThe most important aspect of neurosymbolic systems is that they combine symbolic and subsymbolic representations.\n\tNeSy systems can be categorized by how they do this. \n We distinguish several approaches.\n\tIn the first approach, the inputs are symbolic, but they are translated to subsymbols in a single translation step, after which  the intermediate representations used during reasoning are purely subsymbolic. This approach is followed by the majority of NeSy methods. Some examples include Logic Tensor Networks~, Semantic-based Regularization~, Neural Logic Machines~ and TensorLog~.\n\t\\begin{boxedexample}[label = ex:ltn]{Logic Tensor Networks}\n\t\tLogic tensor networks~ make this translation step explicit. The authors introduce the concept of a \\textit{grounding} (not to be confused with the term grounding used in logic). Here, a grounding is a mapping of all symbolic entities onto their subsymbolic counterpart. More formally, the authors define a grounding as a mapping $\\mathcal{G}$ where:\n\t\t\\begin{itemize}\n\t\t\t\\item $\\mathcal{G}(c) \\in \\mathbb{R}^n$ for every constant symbol $c$\n\t\t\t\\item $\\mathcal{G}(f) \\in \\mathbb{R}^{n.m} \\rightarrow \\mathbb{R}^n$ for every function $f$ of arity $n$\n\t\t\t\\item $\\mathcal{G}(p) \\in \\mathbb{R}^{n.m} \\rightarrow [0,1]$ for every predicate $p$ of arity $n$\n\t\t\\end{itemize}\n\t\tThe grounding of a clause is then performed by combining the aforementioned groundings using a t-norm.\n\t\\end{boxedexample}\n\tIn the second approach, intermediate representations are both symbolic and subsymbolic, but not simultaneously. This means that some parts of the reasoning work on the subsymbolic representation, and other parts deal with the symbolic representation, but not at the same time.\n\tThis is indicative of NeSy methods that implement an interface between the logic and neural aspect.\n\tThis approach is more natural for systems that originate from a logical framework  such as DeepProbLog~, NeurASP~), ABL~ and NLog~.\n\t\\begin{boxedexample}[label = ex:abl]\n{ABL}\n\t\tIn ABL~ there are three components that function in an alternating fashion. There is a perception model, a consistency checking component and an abductive reasoning component.\n\t\tTake for example the task where there are 3 MNIST images that need to be recognized such that the last is the result of applying an operation on  the first two (e.g. $\\digit{3}+\\digit{5}=\\digit{8}$). The structure of the expression is given as background knowledge, but the exact operation (addition) needs to be abduced.\n\t\tFirst, the perception model classifies the images into pseudo-labels, using the most likely prediction (i.e. arg-max). \n\t\tThe abductive reasoning component then tries to abduce a logically consistent hypothesis. \n\t\tFor example, if the digits are correctly classified as $3$, $5$ and $8$, the only logically consistent hypothesis is that the operation is an addition.\n\t\tIf this is not possible, there is an error in the pseudo-labels. \n\t\tA heuristic function is then used to determine which pseudo-labels are wrong.\n\t\tThe reasoning module then searches for logically consistent pseudo-labels. These revised pseudo-labels are then used to retrain the perception model.\n\t\\end{boxedexample}\n\tIn the final approach, intermediate representations are considered simultaneously as symbolic and subsymbolic by the  reasoning mechanism.\n This is implemented in only a few methods, such as the NTP and the CTP. \n\t\\begin{boxedexample}[label = ex:ntp]{Neural Theorem Prover}\n\t\tIn the Neural Theorem Prover, two entities can be unified if they are similar, and not just if they are identical. As such, the NTP   interweaves both symbols and subsymbols during inference. For each symbol $S$, there is a learnable subsymbol $T_S$.\n\t\tSoft-unification happens by applying the normal unification procedure where possible. However, if two symbols $S_1$ and $S_2$ can not be unified, the comparison is assigned a score based on the similarity between $T_{S_1}$ and $T_{S_2}$. The similarity is calculated using a radial basis function $\\varphi(||x-y||_2)$.\n\t\tFor example, to unify   \\texttt{mother(an,bob)} and \\texttt{parent(X,bob)}, soft-unification proceeds as follows:\n\t\t\\begin{align*}\n\t\t\\{\\mathtt{mother(an,bob)} &= \\mathtt{parent(X,bob)}\\} \\\\\n\t\t&\\Downarrow \\quad \\varphi(\\mathtt{mother}, \\mathtt{parent}) \\\\\n\t\t\\{\\mathtt{an} = \\mathtt{X}&, \\mathtt{bob} = \\mathtt{bob}\\} \\\\\n\t\t&\\Downarrow \\quad X = an \\\\\n\t\t\\{\\mathtt{bob} &= \\mathtt{bob}\\} \\\\\n\t\t&\\Downarrow\\\\\n\t\t&\\{~\\}\n\t\t\\end{align*}\n\t\tSoft-unification is not only used to learn  which constants and predicates are similar, but can also be used to perform rule learning. By adding new, parameterized rules with unique predicates, soft-unification allows these new predicates to become very similar to other predicates and as such behave as newly introduced rules.\n\t\tFor example, consider the  program consisting of the fact \\textit{mother(an,bob)} and a single parameterized rule $r1(X,Y) \\leftarrow r2(Y,X)$. The Neural Theorem Prover can answer the query \\textit{child(bob,an)} as follows: \n\t\t\\begin{center}\n\\tikzset{every picture/.style={line width=0.75pt}} \n\\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]\n\\draw    (240,60) -- (140,100) ;\n\\draw    (240,60) -- (340,100) ;\n\\draw    (340,140) -- (340,180) ;\n\\draw (201,32) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\\mathtt{child(bob,an)}$};\n\\draw (299,111) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\\mathtt{r2(ann,bob)}$};\n\\draw (335,191) node [anchor=north west][inner sep=0.75pt]   [align=left] { $\\mathtt{T}$ };\n\\draw (131,112) node [anchor=north west][inner sep=0.75pt]   [align=left] { $\\mathtt{T}$ };\n\\draw (151,62) node [anchor=north west][inner sep=0.75pt]   [align=left] {(1)};\n\\draw (299,62) node [anchor=north west][inner sep=0.75pt]   [align=left] {(2)};\n\\draw (349,151) node [anchor=north west][inner sep=0.75pt]   [align=left] {(3)};\n\\end{tikzpicture}\n\t\t\\end{center}\n\t\t\\begin{align*}\n\t\t(1) ~& \\mathtt{child} = \\mathtt{mother} &\\varphi(||T_{child}-T_{mother}||_2)\\\\\n\t\t& \\mathtt{bob} = \\mathtt{an}  &\\varphi(||T_{an}-T_{bob}||_2)\\\\\n\t\t(2)~ & \\mathtt{child} = \\mathtt{r1} &\\varphi(||T_{child}-T_{r1}||_2)\\\\\n\t\t(3) ~& \\mathtt{child} = \\mathtt{r2}  &\\varphi(||T_{r2}-T_{mother}||_2)\n\t\t\\end{align*}\n\t\tThe figure above shows the two possible derivations the neural theorem prover can make to infer \\textit{child(bob, an)}. One the one hand, it can soft-unify with the fact \\textit{mother(an, bob)}, where \\textit{mother} unifies with \\textit{child} and \\textit{an} with \\textit{bob}. On the other hand, it can use the parameterized rule which encodes an inverse relation. In that case, \\textit{mother} unifies with \\textit{r1} and \\textit{r2} with \\textit{child}. If we optimize the subsymbolic embeddings for the latter, this will be equivalent to learning the rule $mother(X,Y) \\leftarrow child(Y,X)$. This example  also shows that soft-unification potentially adds a lot of different proofs, which can result in computational problems. This problem was solved in later iterations of the system .\n\t\\end{boxedexample}\n\\tikzset{every picture/.style={line width=0.75pt}} \n\\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]\n\\draw    (240,60) -- (140,100) ;\n\\draw    (240,60) -- (340,100) ;\n\\draw    (340,140) -- (340,180) ;\n\\draw (201,32) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\\mathtt{child(bob,an)}$};\n\\draw (299,111) node [anchor=north west][inner sep=0.75pt]   [align=left] {$\\mathtt{r2(ann,bob)}$};\n\\draw (335,191) node [anchor=north west][inner sep=0.75pt]   [align=left] { $\\mathtt{T}$ };\n\\draw (131,112) node [anchor=north west][inner sep=0.75pt]   [align=left] { $\\mathtt{T}$ };\n\\draw (151,62) node [anchor=north west][inner sep=0.75pt]   [align=left] {(1)};\n\\draw (299,62) node [anchor=north west][inner sep=0.75pt]   [align=left] {(2)};\n\\draw (349,151) node [anchor=north west][inner sep=0.75pt]   [align=left] {(3)};\n\\end{tikzpicture}", "cites": [5708, 5707, 5711, 5726, 7304, 8943, 5725, 3727], "cite_extract_rate": 0.7272727272727273, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section synthesizes several key ideas from multiple cited papers by categorizing NeSy and StarAI systems according to how they handle symbolic and subsymbolic representations. It abstracts these methods into three distinct approaches and provides illustrative examples that highlight the implications of each. While it offers some critical insight into computational challenges (e.g., with NTP), it could more explicitly compare limitations across frameworks to strengthen its analysis."}}
{"id": "b873315e-f300-4fef-8037-818fbe2be872", "title": "NeSy: Logic + Probability + Neural", "level": "subsection", "subsections": [], "parent_id": "322e754b-6517-46c9-9006-90e863943e5b", "prefix_titles": [["title", "From Statistical Relational to Neurosymbolic \\\\ Artificial Intelligence: a Survey."], ["section", "Logic vs Probability vs Neural"], ["subsection", "NeSy: Logic + Probability + Neural"]], "content": "In NeSy, we consider a third paradigm: neural computation.\n\tWith neural computation, we refer mainly to the set of models and techniques that allows for exploiting (deep) latent spaces to learn intermediate representations. This includes dealing with perceptual inputs and also dealing directly with embeddings of symbols. \n\t\\textbf{\\textit{lN}}: Many \\nesy{} approaches  focus on the neural aspect (i.e., they originated as a neural method to which logical components have been added). For example, LTNs and SBRs turn the logic into a regularization function to provide a penalty whenever the  logical constraints are violated. At test time the logical loss component is dropped and only the network is used to make predictions. Moreover, by using fuzzy logic, these methods do not integrate the probabilistic paradigm. \n\t\\textbf{\\textit{Ln}}: Another class of \\nesy{} methods does retain the focus on logic. \n\tThese methods usually expand an existing logical framework into a differentiable version. Examples include LRNNs , TensorLog , DiffLog , $\\partial$ILP , $\\partial$4  and NTPs .\n\tThe key inference concepts are mapped onto an analogous concept that behaves identically for the edge cases but is continuous and differentiable in non-deterministic cases. \n\tAs  described in the previous sections,  many such systems cast logical inference  as forward or backward chaining. The focus on logic is clear if one considers that logical inference is performed symbolically  to build the network and the semantics is relaxed only in a subsequent stage to learn the parameters. \\rev{While the architecture mimics the logical reasoning, it is often far from the deep-stacked architecture of neural networks.}\n    \\textbf{\\textit{LN}}: \\rev{It is worth mentioning a later iteration of LRNN, where the framework has been extended to allow for tensorial weights on  atoms and custom aggregation functions . In that framework, it is shown how specifying logic rules can be regarded as specifying the layers of a deep architecture. This provides a nice  and complete integration between forward-chaining logical reasoning and neural networks\n    that is able to implement any existing neural architecture.\n    }\n\t\\textbf{\\textit{lPN} and \\textit{LpN}} There are two final classes of methods that start from existing StarAI methods, \\textit{lP} and \\textit{Lp} respectively, and extend them with primitives that can be interfaced with neural networks and allow for differentiable operations.\n\tIn the \\textit{lPN} class,  NeSy methods such as SL, RNMs and NMLNs follow the knowledge-based model construction paradigm.  In the \\textit{LpN} class, methods such as DeepProbLog and NeurASP  extend PLP. \nThere is usually a trade-off that one must make: \t\nsystems in the \\textit{lN} or \\textit{Ln} classes are usually more scalable but  \\textit{(i)} do not model a probability distribution and \\textit{(ii)} often relax the logic. On the contrary, \\textit{LpN} or \\textit{lPN} systems preserve the original paradigms but at the cost of more complex inference (e.g. they usually resort to exact probabilistic inference).\n\tAn aspect that significantly aids in developing a common framework, and analysing its properties, is the development of an intermediate representation language that can serve as a kind of \\emph{assembly language} . \n\tOne such idea concerns performing probabilistic inference by mapping it onto a  weighted model counting (WMC) problem. \n\tThis can then in turn be solved by compiling it into a structure (e.g. an arithmetic circuit) that allows for efficient inference.\n\tThis has the added benefit that this structure is differentiable, which  facilitates the integration between logic based systems and neural networks. \n\t StarAI based systems often use this approach.", "cites": [5727, 5710, 5711, 5719, 3726], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section synthesizes several NeSy and StarAI methods by categorizing them into distinct classes (lN, Ln, lPN, LpN) and showing how they relate to each other in terms of logic, probability, and neural computation. It offers critical insight by highlighting trade-offs between scalability and probabilistic modeling. The abstraction level is high, as it introduces a unifying concept (an intermediate representation language) and connects different systems through the idea of differentiable inference."}}
{"id": "d6105bd2-5f38-4f97-a7bd-ae4de168c174", "title": "Distant Supervision", "level": "paragraph", "subsections": ["d9eb4e04-ce78-4d16-9f72-94154473deea"], "parent_id": "528b4897-8064-4ac1-a40f-dc1e7d30614f", "prefix_titles": [["title", "From Statistical Relational to Neurosymbolic \\\\ Artificial Intelligence: a Survey."], ["section", "Tasks"], ["paragraph", "Distant Supervision"]], "content": "\\rev{A classical task in NeSy is to use logic as distant supervision for a learning model.  Here, input $X$ is paired with label $y$. However, instead of using a single model to map $X$ to $y$, the input $X$ is firstly mapped to a set of intermediate concepts $C$ by a (set of) neural networks. Then, these concepts are used to compute $y$ in a symbolic way. Logic programs are usually exploited to map the concepts $C$, represented as logical atoms, to the label $y$, which represents the logical query.  Therefore,  the neural networks are not directly supervised (on $C$) but they are only distantly supervised through the label $y$ and the knowledge contained in the logic program. The intuition  is that when the label $y$ is only weakly linked to the input, it is  more convenient to break the task in several easier subtasks and then compose them using background knowledge in the form of a logic program. Notice that the logic program is fundamental for the inference. Without the program, the networks will not be able to solve their subtasks, as there is no direct supervision. Moreover, by splitting the task into subtasks, the inference done by the composite system (neural + logic) is far more explainable than a corresponding end-to-end neural network. A classical example is the MNIST addition , shown in Example \\ref{ex:mnist_addition}. Distant supervision tasks are very common in prototypical systems such as DeepProbLog, DeepStochLog, NLog, NeurASP, SATNet . A downside of such tasks is that, to enable learning of untrained neural subtasks, the logic has to consider all possible combinations of concepts that are compatible with the label $y$, even though only few (or one) are correct. The challenge is to balance the exploration of multiple combinations with  a greedy strategy for scaling to larger problems . Other problems falling in this category are scene parsing, image segmentation and semantic image interpretation }\n        \\begin{boxedexample}[label = ex:mnist_addition]{MNIST Addition} \n        Given the classical MNIST dataset, $\\mathcal{D} = \\{(x_i,y_i)\\}$, with $x_i$ an MNIST image, and $y_i$ its numeric label, the MNIST addition dataset is built by mapping pairs of images to the label representing their sum. In particular, $\\mathcal{D}_{\\text{add}} = \\{(x_i,x_j,z_{ij}) : z_{ij} = y_i + y_j \\land (x_i,y_i),(x_j,y_j) \\in \\mathcal{D}\\}$. The idea is to learn to classify the digits without direct supervision on their labels, but only using distant supervision about sums of such images. \n     The task is often also coupled to  background knowledge of what  addition is, e.g. in Prolog syntax:\n        \\begin{lstlisting}\naddition(X1, X2, Z) <- digit(X1,Y1), digit(X2,Y2), Z is Y1 + Y2. \n        \\end{lstlisting}\n        Such knowledge is used to reason about the (most-likely) pairs \\texttt{Y1,Y2} that sum to the provided label \\texttt{Z}. Logic is then used to link the actual outputs of the learning model \\texttt{Y1,Y2} to the distant supervision \\texttt{Z}.\n        \\end{boxedexample}", "cites": [181, 7304, 1806, 3727], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section effectively synthesizes the concept of distant supervision across multiple cited works, presenting a unified view of how logic and neural components interact. It provides a critical analysis by highlighting a key downside (the need to consider all possible combinations of concepts for the label) and the challenge of balancing exploration with scalability. The abstraction is strong, as it generalizes the task into a broader framework involving symbolic and subsymbolic composition, with the MNIST addition example serving as a clear illustration."}}
{"id": "d9eb4e04-ce78-4d16-9f72-94154473deea", "title": "Semi-supervised Classification", "level": "paragraph", "subsections": ["c47d6a00-5a1b-40c6-9278-09edc1c79d9b", "d7c376c1-f6e7-41a6-9b6e-28a3f939810e", "f4d6de92-316d-4daa-a0ba-2c746822d5de"], "parent_id": "d6105bd2-5f38-4f97-a7bd-ae4de168c174", "prefix_titles": [["title", "From Statistical Relational to Neurosymbolic \\\\ Artificial Intelligence: a Survey."], ["section", "Tasks"], ["paragraph", "Distant Supervision"], ["paragraph", "Semi-supervised Classification"]], "content": "\\rev{A related class of tasks is semi-supervised classification  } with knowledge. Here, the starting point is a standard classification task, where a set of inputs $X$ is mapped by a neural model to a set of labels $C$. However, we are also provided with some additional knowledge $y$ related to the labels $C$ of the inputs. This knowledge is often expressed in terms of logical rules and programs. The setting is very similar to  distant supervision, where we have three levels: inputs $X$, concepts $C$ and additional labels $y$. However, in this case, we have also access to supervision for some (usually few) concepts $C$. Although this task could be tackled in a purely supervised way by discarding the information contained in $y$, \\nesy{} approaches can improve the predictions of several input patterns using the external knowledge. When the external knowledge is  relating  concepts $C$ of multiple patterns, the task is called collective classification , as one can improve the accuracy on multiple patterns by collectively predicting their classes. A classical example in this setting is document classification in citation networks, cf. Example \\ref{ex:citation}. By treating the information contained in $y$ as extra knowledge, these tasks are often tackled using regularization based systems, like SBR, DLM, RNM or Semantic Loss. However, logic programs can also be  used to simulate a label-passing scheme along the citation network, as done in DeepStochLog . A characteristic of this class of tasks is that the additional information $y$ is often very noisy (e.g. the manifold rule in the citation network is not always valid). While this task is closely related to  distant supervision, there is an important difference: in semi-supervised classification, the additional knowledge $y$ is meant to provide an additional signal, which, however, would not suffice in the absence of direct supervision on the concepts $C$. \n    \\begin{boxedexample}[label = ex:citation]{Document classification in citation networks}\n    In document classification in citation networks, we are provided with both labelled and unlabelled scientific papers. A label is often the domain area of the paper (e.g. Machine Learning, Artificial Intelligence, Databases, etc.). \n    However, a network of citations between papers is also provided, linking papers between domains.\n    The idea of the document classification task is that in many domains, a paper cited by other papers with a certain label is likely to belong to the same domain. When classifying a document,  one has to balance  the signal coming from the features of the document (i.e. words) and that coming from neighbors in the citation network to  provide a \\textit{collective} prediction.\n    In \\nesy{} systems, this is usually done by coupling the subsymbolic model with a rule of the following type:\n    \\begin{lstlisting}\n w:: domain(X,Y) <- cite(X,X1), domain(X1,Y). \n    \\end{lstlisting}\n    The rules get a different weight according to the  domain to account for the differences between them. \n    \\end{boxedexample}", "cites": [5714, 3741, 5720, 5709], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple cited works to present a coherent framework for semi-supervised classification with external knowledge, particularly in the context of citation networks. It abstracts key characteristics of the task, such as the noisy nature of knowledge and the role of collective prediction, and provides a critical distinction between semi-supervised classification and distant supervision. While it does not deeply critique the methodologies, it effectively integrates and contextualizes different NeSy approaches."}}
{"id": "c47d6a00-5a1b-40c6-9278-09edc1c79d9b", "title": "Knowledge Graph Completion", "level": "paragraph", "subsections": [], "parent_id": "d9eb4e04-ce78-4d16-9f72-94154473deea", "prefix_titles": [["title", "From Statistical Relational to Neurosymbolic \\\\ Artificial Intelligence: a Survey."], ["section", "Tasks"], ["paragraph", "Distant Supervision"], ["paragraph", "Semi-supervised Classification"], ["paragraph", "Knowledge Graph Completion"]], "content": "\\rev{Another  common task in \\nesy{} is knowledge graph completion (KGC) or link prediction. A knowledge graph (KG) is a pair of $(E,R)$, where $N$ is the set of entities and $R$ the set of edges. In a KG, an edge is a triple $(e_1, r, e_2)$, where $e_1$ and $e_2$ are the head and tail of the edge and $r$ is the  relation between them. In a KGC task, the goal is to predict missing edges in the input graph. Link prediction has been one of the key tasks in StarAI , and more recently also in NeSy as NeSy allows to merge symbolic reasoning (from StarAI) with the recent geometric deep learning approaches based on Knowledge Graph Embeddings (KGE)  and Graph Neural Networks . \\nesy{} systems focusing on this task include NTPs , NMLN , DLM , DiffLog , TensorLog .}", "cites": [8942, 5710, 5711, 5720], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a brief description of knowledge graph completion as a task in NeSy and mentions several relevant papers, but it lacks deeper synthesis of their contributions. It does not compare or contrast the systems or methods, nor does it critically evaluate their strengths or limitations. The abstraction is minimal, focusing largely on naming the approaches without identifying broader trends or principles."}}
{"id": "d7c376c1-f6e7-41a6-9b6e-28a3f939810e", "title": "Generative Tasks", "level": "paragraph", "subsections": [], "parent_id": "d9eb4e04-ce78-4d16-9f72-94154473deea", "prefix_titles": [["title", "From Statistical Relational to Neurosymbolic \\\\ Artificial Intelligence: a Survey."], ["section", "Tasks"], ["paragraph", "Distant Supervision"], ["paragraph", "Semi-supervised Classification"], ["paragraph", "Generative Tasks"]], "content": "Most previously mentioned tasks can be described as classification\\footnote{Even though, many of them use a generative model to tackle the classification task, instead of a conditional one.}.  \\nesy{} has recently also focused on tasks concerned with modeling the input data distribution as accurately as possible. The goal is then to sample new patterns from the learned distribution. The idea behind \\nesy{} generative approaches is that one can learn important features  from data  using deep generative models (e.g. variational auto-encoders or Markov Chain Monte Carlo methods). Combining symbolic features with logic reasoning can be used to control,  stratify and simplify the inference. The generative modeling can either refer to the relational structure, e.g. molecule generation in NMLNs , or to the subsymbolic space, e.g. image generation in VAEL  or .", "cites": [8942, 7988], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the general concept of generative tasks in NeSy by connecting the ideas from both NMLNs and VAEL, highlighting how symbolic and subsymbolic components are used to enhance generative modeling. It abstracts by distinguishing between relational and subsymbolic generative modeling, but critical evaluation of the limitations or trade-offs of these approaches is limited. The section offers some analytical value but could provide deeper comparative or evaluative insights."}}
{"id": "f4d6de92-316d-4daa-a0ba-2c746822d5de", "title": "Knowledge Induction", "level": "paragraph", "subsections": [], "parent_id": "d9eb4e04-ce78-4d16-9f72-94154473deea", "prefix_titles": [["title", "From Statistical Relational to Neurosymbolic \\\\ Artificial Intelligence: a Survey."], ["section", "Tasks"], ["paragraph", "Distant Supervision"], ["paragraph", "Semi-supervised Classification"], ["paragraph", "Knowledge Induction"]], "content": "\\rev{Rather than exploiting symbolic knowledge  predictive tasks, one can also induce  \n     \\textit{symbolic knowledge}. In all  previous tasks,  symbolic knowledge is provided by the user as part of the input. However, as explored in Section \\ref{sec:struct}, we can still apply several neurosymbolic techniques by learning the symbolic knowledge when this is not the case. The unknown symbolic knowledge is then the actual target to be learned. A classical example is \\textit{program synthesis}, where the goal is to learn the program from positive and negative examples of the desired input-output behaviour. Ideally, all positive pairs and none of the negatives should be covered. Many systems learn logic programs, i.e. NTPs , $\\partial$ILP , DeepProbLog, NeuralLP, DiffLog, DeepCoder.\n     Sometimes, the input-output pairs are not part of the training dataset, but are actually generated by a black-box neural model. The induced programs  then explain the behaviour of the model, which relates NeSy to the domain of \\textit{explainability} .}", "cites": [2665, 5710, 5728, 3726, 3727], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates cited works by linking the theme of knowledge induction to neurosymbolic techniques and explainability, showing some synthesis of ideas. It identifies program synthesis as a key example and notes connections to explainability, but does not deeply compare or critique the cited systems. Some abstraction is present through the framing of symbolic knowledge as a learnable target, but broader patterns or a novel framework are not fully articulated."}}
{"id": "986b4d82-d655-4355-b6bd-d14bee40b72f", "title": "Symbolic representation learning", "level": "paragraph", "subsections": [], "parent_id": "6f37ac7d-158b-4d60-89b1-c3be4d76632d", "prefix_titles": [["title", "From Statistical Relational to Neurosymbolic \\\\ Artificial Intelligence: a Survey."], ["section", "Open Challenges"], ["paragraph", "Semantics"], ["paragraph", "Probabilistic reasoning"], ["paragraph", "Symbolic representation learning"]], "content": "The effectiveness of deep learning  comes from the ability to change the representation of the data so that the target task becomes easier to solve.\n\tThe ability to change the representation also at the symbolic level would significantly increase the capabilities of \\nesy{} systems.\n\tThis is a major open challenge for which neurally inspired methods could help achieve progress .\n\t\\section*{Acknowledgements}\n\tThis work has received funding from the Research Foundation-Flanders (FWO)\n\t(G. Marra: 1239422N, S. DumanÄiÄ‡: 12ZE520N, R. Manhaeve: 1S61718N). \n\tLuc De Raedt has received funding from the Flemish Government (AI Research Program), from the FWO, from the European Research Council (ERC) under the European Unionâ€™s Horizon 2020 research and innovation programme (grant agreement No 694980 SYNTH: Synthesising Inductive Data Models) and the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. This work was also supported by TAILOR, a project funded by EU Horizon 2020 research and innovation programme under GA No 952215.\n\t\\bibliographystyle{plain}\n\t\\bibliography{main}\n\t\\appendix", "cites": [5729, 5730], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly mentions symbolic representation learning and suggests that neurally inspired methods could help in this area, but it does not meaningfully synthesize or integrate the cited papers. There is no critical evaluation of the approaches in the cited works, nor is there abstraction to broader patterns or principles. The content remains superficial and lacks analytical depth."}}
{"id": "0c29e7f0-ea36-44aa-a1f0-0b0c69b42bfb", "title": "Knowledge graphs embeddings and Graph Neural Networks for neurosymbolic AI", "level": "section", "subsections": ["525771a4-9400-42b9-9e72-981457921a8b", "128f81ba-b844-472a-86b3-8ca1662bba8a"], "parent_id": "31670094-45b4-4854-914d-94aaf932ce65", "prefix_titles": [["title", "From Statistical Relational to Neurosymbolic \\\\ Artificial Intelligence: a Survey."], ["section", "Knowledge graphs embeddings and Graph Neural Networks for neurosymbolic AI"]], "content": "\\label{sec:kge_gnn}\n\tIn this appendix, we introduce two approaches, namely Knowledge Graph Embeddings and Graph Neural Networks,  which are commonly used in relational tasks in the deep learning community. We analyze them in the spirit of the seven dimensions introduced in the paper. In fact, it turns out that they share many features with neurosymbolic systems. In this way, we would like to suggest that NeSy can also be found at the intersection of statistical relational and geometric deep learning approaches.\n\tOne of the most popular relational representations is that of a knowledge graph (KG). A KG is\n\ta multi-relational graph composed of entities (i.e. nodes) and relations (i.e. edges). It is common to represent an edge as a triple of the form: \\textit{(head entity, relation, tail entity)}, e.g. ($homer$, $fatherOf$, $bart$).\n\tStarAI has been extensively used  to solve many tasks on KGs: prediction of missing relationships (i.e. knowledge graph completion), prediction of properties of entities, or clustering entities based on their connectivity patterns. StarAI is particularly well suited to reasoning with knowledge graphs since it models explicitly the probabilistic dependencies among different relationships. \n\tHowever, in order to scale to larger knowledge graphs, the probabilistic dependencies of StarAI models have been relaxed to give rise to a new class of scalable models based on latent features, which are particularly interesting from a neurosymbolic viewpoint.  The key intuition behind relational latent feature\n\tmodels is that the relationships between entities can be more efficiently predicted by modeling simpler interactions in a latent feature space. Knowledge Graph Embeddings and Graph Neural Networks represent two of the ways to encode such latent representations.", "cites": [7233], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes concepts from the cited paper on relational machine learning with the broader context of neurosymbolic AI, showing how knowledge graph embeddings and GNNs relate to both StarAI and NeSy. It abstracts well by identifying the role of latent feature spaces as a bridge between symbolic and subsymbolic methods. However, it lacks deeper critical analysis of the cited works' limitations or trade-offs, relying more on descriptive and integrative insights."}}
{"id": "525771a4-9400-42b9-9e72-981457921a8b", "title": "Knowledge Graph Embeddings", "level": "subsection", "subsections": [], "parent_id": "0c29e7f0-ea36-44aa-a1f0-0b0c69b42bfb", "prefix_titles": [["title", "From Statistical Relational to Neurosymbolic \\\\ Artificial Intelligence: a Survey."], ["section", "Knowledge graphs embeddings and Graph Neural Networks for neurosymbolic AI"], ["subsection", "Knowledge Graph Embeddings"]], "content": "Knowledge Graph Embedding (KGE) models  assume that triples (i.e. relations) are conditionally independent  given a set of global latent variables, called embeddings, for entities and relations. Therefore, the existence of a triple can be predicted by a scoring function $f(e_h,e_r,e_t)$, where $e_i$ is the embedding of the corresponding object.\n\tKGE models mainly differ in terms of the scoring function  and the embedding space   they use. Translation or distance-based models  use a scoring function measuring to what extent the tail of a triple can be obtained by a relation-specific translation of the head, i.e. $f(e_h,e_r,e_t) = || e_h + e_r - e_t||$. Semantic matching methods  instead exploit similarity-based scoring\n\tfunctions, such as $f(e_h,W_r,e_t) = || e_hW_re_t^\\top||$. It is interesting to note that standard multi-layer perceptrons are often used to learn this similarity measure, i.e. $f(e_h,W_r,e_t) = nn([e_h, e_t]; W_r)$. This is similar to neural interfaces typical of $lPN$ and $LpN$ models of Section \\ref{sec:paradigms}, cf. the neural predicates of DeepProbLog in Example~\\ref{ex:deepproblog}.\n\tKGE learn from ground relations only. They learn the embeddings of entities and relations as to maximize the score for a set of known \\textit{True} triples.  However, some methods   incorporate higher level information, like first-order logical clauses  or logical queries , bridging KGE and multiple NeSy systems (like SL or SBR). \n\tIt is interesting to analyze KGE methods in terms of the dimensions we described in our paper. KGE methods mostly work as model-based, undirected methods. They constrain the embeddings to be coherent with the logical facts and rules, which are no longer used after learning. They are heavily based on subsymbolic representations. These models learn the correct parameters (i.e. embeddings) for the task at hand. Even if no explicit semantics needs to be given to the scoring function $f(e_h,W_r,e_t)$,  a fuzzy logic interpretation is often used when injecting logical rules ", "cites": [5732, 3741, 3672, 5731], "cite_extract_rate": 0.4, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a coherent synthesis of KGE models by categorizing them based on scoring functions and embedding spaces, and connects them to broader neurosymbolic AI paradigms. It critically evaluates their limitations, such as reliance on ground relations and the use of fuzzy logic for rule injection. The abstraction level is strong, as it maps KGE methods to the survey's framework of dimensions for AI systems, offering a meta-level view of their role in learning and reasoning."}}
{"id": "128f81ba-b844-472a-86b3-8ca1662bba8a", "title": "Graph Neural Networks", "level": "subsection", "subsections": [], "parent_id": "0c29e7f0-ea36-44aa-a1f0-0b0c69b42bfb", "prefix_titles": [["title", "From Statistical Relational to Neurosymbolic \\\\ Artificial Intelligence: a Survey."], ["section", "Knowledge graphs embeddings and Graph Neural Networks for neurosymbolic AI"], ["subsection", "Graph Neural Networks"]], "content": "Graph Neural Networks (GNN) () are   deep learning models for dealing with graphs as input. Inference in these models can be cast as  \\textit{message passing}  : at each inference step (i.e. GNN layer), a node sends a message to all its outgoing neighbors and  updates its state by aggregating all the messages coming from its ingoing neighbors. Messages are computed by standard neural networks. Each inference step computes a transformation of the representations of the nodes. In the last layer, these representations are used to classify the nodes or are aggregated to classify the entire graph. \n\tThere are many connections between GNNs and neurosymbolic computation  since both of them    apply neural methods to relational data: graphs for GNNs, and logic representations for NeSy. While   many works try to close the gap between these two representations , the application of GNN techniques to logic-based graph structures is still very limited. The most related line of work is about using GNNs on knowledge graphs . The underlying idea is to differentiate the messages exchanged by two nodes if they are related by different kinds of edges. For example, given two nodes \\texttt{homer} and \\texttt{bart},  the message corresponding to the edge \\texttt{fatherOf(homer,bart)} will be different from the one corresponding to \\texttt{sameFamily(homer,bart)}. However, these models were intended as classifiers of known relational structures (nodes and edges) and not to reason about the knowledge graph itself (e.g. determining whether an edge exists between two nodes). To perform relational reasoning, GNN-based models rely on techniques from the KGE community on top of the representations extracted by the GNN. This often takes the shape of an auto-encoding scheme: a GNN encodes an input graph in a latent representation and a KGE-based factorization technique is used to reconstruct the whole graph . \n\tAn important characteristic of GNNs  is that they rely exclusively on neural computation to perform inference (i.e. to compute messages) and there is no clear direction on how to inject external knowledge about inference, e.g. as logical rules. This contrasts with NeSy, where this is one of the main goals. \n\tThere are also some interesting connections between GNNs and StarAI models. In , GNNs based on knowledge-graphs are  not used as a modeling choice but rather to approximate inference in Markov Logic Networks, which is somewhat similar to regularization based methods (see Section \\ref{sec:proof_vs_model}). Similarly, in  GNNs are used to encode logical formulae expressed as graphs  to approximate a weighted model counting problem.\n\tFinally, it is interesting to analyse GNNs in the spirit of some of the dimensions of  NeSy. GNNs act as directed models with a proof-based inference scheme: they perform a series of inference steps to compute the final answer. In the original version of GNNs , the node states are updated until a fixed point is reached, which resembles forward-chaining in logic programming. The representation of nodes belongs to a  subsymbolic numerical space. Finally, GNNs can be considered as implicit structure learners: inference rules are learned through the learning of the neural message passing functions. \n\tGraph Neural Networks have recently received a lot of attention from many different communities, thanks to the representation power of neural networks and the capability of learning in complex relational settings. It is no surprise that people have started to study the expressivity of this class of models. One of the most interesting analyses from a neurosymbolic viewpoint is measuring the expressivity of GNNs in terms of variable counting logics. Recently,  showed that GNNs are as expressive as 2-variable counting logic\n\t$C_2$. This fragment of first order logic admits formulas with at most two variables extended with counting quantifiers. The expressivity of this fragment is limited  compared to many neurosymbolic models, especially those based on logic. However, GNNs learn the logical structure of the problem implicitly as part of the message passing learning scheme and they rely neither on expert-provided knowledge nor on heavy combinatorial search strategies to structure learning (see Section \\ref{sec:struct}).  An open and challenging question that unites the GNN and NeSy communities is how to bring the expressivity to higher-order fragments , like in NeSy and StarAI, while keeping both the learning and the inference tractable, like in GNNs.", "cites": [259, 5733, 281, 208, 7332, 8475, 5734, 5735, 7045, 216, 1059], "cite_extract_rate": 0.7333333333333333, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers to draw connections between GNNs and neurosymbolic/StarAI models, especially in terms of inference schemes, expressivity, and knowledge integration. It offers a critical perspective by highlighting limitations, such as the lack of explicit logical rule injection in GNNs and their limited expressivity compared to logic-based systems. The section also abstracts key concepts like expressivity in terms of variable counting logics and presents a broader analytical framework for evaluating GNNs in a neurosymbolic context."}}
{"id": "df605b7c-7a16-4a8a-9a34-34eff8f50d0f", "title": "Fuzzy logic, fuzzyfication and soft-Satisfability", "level": "section", "subsections": ["c6063ac6-99de-4318-925a-eab4859602f1"], "parent_id": "31670094-45b4-4854-914d-94aaf932ce65", "prefix_titles": [["title", "From Statistical Relational to Neurosymbolic \\\\ Artificial Intelligence: a Survey."], ["section", "Fuzzy logic, fuzzyfication and soft-Satisfability"]], "content": "Fuzzy logic, as many-valued extension over Boolean logic, has a very long tradition . However, the use of fuzzy logic in StarAI and NeSy is not dictated by the need of dealing with vagueness, but by the advantageous computational properties of t-norms. Indeed, a common use case is to have an initial theory defined in Boolean logic which is \\textit{fuzzyfied}. Inference is then carried out with the fuzzyfied theory and the answers are eventually discretized back to Boolean values (usually using a threshold at 0.5). \n\tThe reason for this approach is that one would like to exploit the differentiability of t-norms to address logical inference of FOL theories in a more scalable way than standard combinatorial optimization algorithms (e.g. SAT solvers). This is particularly important in undirected and regularization-based methods (such as PSL  and  LTN ). In fact, it has been shown  that there are fragments of fuzzy logic that can even provide convex inference problems. Example \\ref{ex:bool_vs_fuzzy}, however, shows that naively approaching logical inference through a fuzzy relaxation and gradient-based optimization can introduce unexpected behaviours.\n\t\\begin{boxedexample}[label = ex:bool_vs_fuzzy]{Fuzzyfication and soft-Satisfability}\n\t\tLet us consider a disjunction, like $A \\vee B \\vee C$. In Boolean logic, if we state that the disjunction is \\textit{satisfied} (i.e. \\textit{True}), then we expect at least one among the three variables to be \\textit{True}. \n\t\tSuppose we want to find a truth assignment for all the variables that satisfies the disjunction above. The approach of the majority of NeSy fuzzy approaches is the following. First, the rule is relaxed into a fuzzy real function. For example, using the \\L ukasiewicz t-norm, $F_\\oplus(A,B,C)= min(1, A+B+C)$. Secondly, a gradient-based algorithm (e.g. backpropagation with Adam ) is used to maximize the value of the formula with respect to the \\textit{fuzzy} truth degree of the three variables. Finally, the obtained \\textit{fuzzy} solution $A^\\star, B^\\star, C^\\star$ is translated back into a Boolean assignment using a $0.5$ threshold.\n\t\tLet us consider a possible optimal fuzzy solution, like $(A^\\star, B^\\star, C^\\star) = (0.34, 0.34, 0.34)$ and its discretized version $(A^\\star, B^\\star, C^\\star) = (False, False,$ $False)$, using a threshold at $0.5$. The discretized solution does not satisfy the initial Boolean formula, even though it is a global optimum in the fuzzyfied problem. \n\t\\end{boxedexample}\n\tSimilarly,  shows that, while it is very common to reason about universally quantified formulae in the form of $\\forall x: A(x) \\to B(x)$, like `all humans are mortal', using gradients and fuzzy logic to make inference can be extremely counterintuitive, especially with specific t-norms such as the product t-norm. It is unclear whether there exists a generally accepted subset of properties of Boolean logic that one wants to preserve and whether one can define a t-norm that guarantees such properties.", "cites": [5736, 5737, 5707, 5706], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key concepts from multiple papers to explain how fuzzy logic is used in NeSy and StarAI, particularly for enabling differentiable inference. It provides a critical example highlighting potential pitfalls of fuzzyfication and gradient-based optimization. The abstraction level is strong, as it identifies broader implications and patterns, such as the trade-off between computational efficiency and logical fidelity."}}
