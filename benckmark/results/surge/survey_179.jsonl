{"id": "aa7c7932-8732-4d41-bdd3-717b669db4a5", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "d18a1e2f-b791-4691-8197-267bc8e649b4", "prefix_titles": [["title", "A Short Survey of Pre-trained Language Models for Conversational AI-A New Age in NLP"], ["section", "Introduction"]], "content": "Despite much progress in the field of natural language processing and intelligent agents, the conversational part for communication between a human and a machine is still in its inception phase. It is only recently that neural generative models had gotten the attention of researchers and improved the field of conversational AI drastically. With a large amount of available `big data' and advanced deep learning methods, the objective of designing digital conversation systems as our virtual assistant is no longer a dream.\nBased on functionality, conversational AI can be categorized into three categories: 1) task-oriented systems, 2) chat-oriented systems, and 3) question answering systems . Task-oriented dialogue systems are designed to complete a specific task on the user's behalf such as booking hotels, making a restaurant reservation or finding products. The second category mainly focuses on carrying out a conversation with the user on open-domain topics, and question answering bots are designed to find an appropriate answer to user's query using all its available knowledge and resources. Though, these systems have come a long way in terms of progress but conversing with such models for even a short amount of time quickly unveils the inconsistency in generated responses.\nA different number of strategies have been introduced over a period to address this issue. One of the standard methods of designing an NLP based project is to utilize word embeddings, pre-trained on a huge amount of unlabelled data using distributed word representations such as GloVe and Word2Vec, to initialize the first layer of the neural network. The rest of the layers are then trained on a task-specific dataset. However, these techniques failed to capture the correct context of the word used in a sentence. For example ``an apple a day, keeps the doctor away\" and ``I own an Apple Macbook, two ``apple\" words refer to very different things but they would still share the same word embedding vector. Recently, the concept of pre-trained language modeling is introduced. The word embeddings generated by these language models are pre-trained on large corpora and are then utilized as either distributed word embeddings or fine-tuned according to the specific task needs. This comes under the category of transfer learning and has achieved state-of-the-art results in various NLP tasks. The introduction of pre-trained language models has given the field of conversational AI a new direction especially to question answering. Thus, we focus more on the question answering systems than the other two dialogue systems as this field has been totally transformed by these pre-trained language models.\nIn this short survey paper, we first discuss the different pre-trained language modeling techniques introduced till now. Then, we extend our discussion to the implementation of these models in dialogue systems with a special emphasis on question answering systems. In the end, we present the open challenges pertaining to these language models that need to be addressed.", "cites": [7478], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The introduction synthesizes the key ideas from the cited paper by grouping conversational AI systems into categories and discussing neural approaches. It provides a basic comparison between traditional word embeddings and pre-trained language models, highlighting limitations of the former. However, it lacks deeper critical analysis of the cited works and offers limited meta-level abstraction beyond the specific systems and methods described."}}
{"id": "22ce99d4-c976-4c19-b969-629f09e566ed", "title": "Pre-trained Language Modeling", "level": "section", "subsections": [], "parent_id": "d18a1e2f-b791-4691-8197-267bc8e649b4", "prefix_titles": [["title", "A Short Survey of Pre-trained Language Models for Conversational AI-A New Age in NLP"], ["section", "Pre-trained Language Modeling"]], "content": "Most datasets available for NLP tasks are rather small. This data scarcity problem makes it difficult to train the deep neural networks, as they would result in an over-fitted model and not generalize well on these small datasets. The concept of pre-training a model on ImageNet corpus has been employed for quite a few years in the field of computer vision . The idea is to make the model learn the general features of an image and this learning can then be utilized in any vision task such as image captioning etc. to achieve the state-of-the-art results.  \nPre-trained language modeling can be considered an equivalent of ImageNet in NLP and has achieved state-of-the-art results on various downstream NLP tasks such as sentiment analysis, data classification, and question answering, etc. The timeline of pre-trained language models can be divided into two categories:\n\\begin{figure*}[htb]\n    \\centering\n    \\includegraphics[width=\\textwidth]{diff.pdf}\n     \\Description[comparison of pre-trained language models]{Architectural comparison of BERT, OpenAI GPT, and ELMo explaining the attention mechanism in them.}\n    \\caption{Comparison of BERT, OpenAI GPT and ELMo model architectures .}\n    \\label{fig:diff}\n\\end{figure*}\n\\textbf{Feature based Approaches:} Learning the appropriate representation of the words has been an active research area for a long time. The pre-trained word embeddings provide an edge to modern NLP systems over the embeddings learned from scratch and can be word level, sentence level, or paragraph level based on their granularity. These learned representations are also utilized as features in NLP downstream tasks. \nEmbeddings from Language Models (ELMo)  revolutionize the concept of general word embeddings by proposing the idea of extracting the context-sensitive features from the language model. Incorporating these context-sensitive embeddings into task-specific model results in leading-edge results on several NLP tasks such as named entity recognition, sentiments analysis, and question answering on SQuAD  dataset.\n\\textbf{Fine tuning based Approaches:} There has been a trend of transfer learning from language models recently. The main idea is to pre-train a model on unsupervised corpora and then fine-tune the same model for the supervised downstream task. The advantage of implementing these approaches is that only a few parameters need to be learned from scratch. These models are the adaptation of Google's Transformer model. First in the series is OpenAI's Generative pre-training Transformer (GPT) . The model is trained to predict the words only from left-to-right hence, capture the context unidirectionally. The work was further improved by Devlin who introduced a bi-directional pre-trained model called Bi-directional Encoder Representations from Transformers (BERT) . They addressed the unidirectional constraint by integrating the concept of `Masked Language Model' in their model that randomly masks some of the input tokens and the goal is to predict the masked token based on the captured context from both directions. They also trained their model on `Next Sentence Prediction' task that further improved the model's performance by a good margin. Unlike GPT, BERT is based on Transformer's encoder block which is designed for language understanding. The model achieved state-of-the-art results on 11 NLP downstream tasks including question natural language inference, Quora question pairs, and question answering on SQuAD. Later, Radford et al.  improved their predecessor, GPT, by introducing GPT2. The model is bidirectionally trained on 8M web pages and has 1.5B parameters, 10 times greater than the original model. The model is based on the Transformer's decoder and is designed to generate language naturally.\nAlthough out of all the models BERT seems to perform very well, it still has some loopholes in its implementation. First of all, it uses [MASK] token during the pre-training process, but these token are missing from real data during the fine tuning phase, which results in a pre-train-finetune discrepancy. Another weakness of BERT is that it assumes that masked tokens are independent of each other and are only predicted using unmasked tokens. Recently, another language model, XLNet , improved the shortcomings of BERT by using Transformer-XL as base architecture and introducing the dependency between the masked positions. It is an autoregressive language model that utilizes the context to predict the next word. The context word here is constrained to two directions, either backward or forward. It uses permutation language modeling to overcome the drawbacks of BERT. \nThe visual comparison between different language models is shown in Figure~\\ref{fig:diff}. It can be seen that BERT is deeply bidirectional, Open AI GPT is unidirectional, and ELMo is shallowly bidirectional.", "cites": [11, 8385, 439, 7], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively compares feature-based and fine-tuning-based pre-trained language models, integrating key concepts from ELMo, BERT, and GPT. It identifies strengths and some limitations, such as the pre-train-fine-tune discrepancy in BERT, but the critique is relatively surface-level. The discussion generalizes to some extent by categorizing models into broader approaches, but deeper abstraction or synthesis of novel insights is limited."}}
{"id": "5b65020e-fe65-4041-8553-ef5e25929249", "title": "Single-turn MC", "level": "subsubsection", "subsections": [], "parent_id": "a126a6ee-5cf6-47bb-88c7-f59b6cfcdc23", "prefix_titles": [["title", "A Short Survey of Pre-trained Language Models for Conversational AI-A New Age in NLP"], ["section", "Pre-trained Language Modeling Approaches for Dialogue Systems"], ["subsection", "Question Answering System"], ["subsubsection", "Single-turn MC"]], "content": "} There has been rapid a progress after the introduction of pre-trained language models and most of the question answering systems have achieved human-level accuracy on Stanford Question Answering Dataset (SQuAD) .\nThe SQuAD dataset is a standard benchmark for the machine comprehension problem consisting of Wikipedia articles and questions posed on those articles by a group of co-workers. The system must select the right answer span for the question from all the possible answers in the given passage.\nWord embeddings generated using ELMo are used with BiLM and BiDAF  set up to achieve higher accuracy scores on SQuAD leaderboard \\footnote{https://rajpurkar.github.io/SQuAD-explorer/}. The results of ELMo are improved by BERT model when used in different architecture settings such as  introduced semantic information into the BERT model. Another model introduced by Facebook called RoBERTa  improved BERT's performance significantly by training it again on huge corpus and modifying its hyper-parameters more carefully. Recently, the top accuracy score on SQuAD's leader board is held by a model based on XLNet which has managed to improve the accuracy more than the actual human's performance.", "cites": [8245, 439, 1139, 826], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of the application of pre-trained language models to single-turn machine comprehension in dialogue systems, mentioning ELMo, BERT, RoBERTa, and XLNet in sequence. It lacks critical evaluation of these models, deeper synthesis of the cited papers, and fails to generalize or abstract broader trends or principles in the use of pre-trained models for question answering."}}
{"id": "e346431a-8581-45bc-a670-544b9789b70b", "title": "Multi-turn MC", "level": "subsubsection", "subsections": [], "parent_id": "a126a6ee-5cf6-47bb-88c7-f59b6cfcdc23", "prefix_titles": [["title", "A Short Survey of Pre-trained Language Models for Conversational AI-A New Age in NLP"], ["section", "Pre-trained Language Modeling Approaches for Dialogue Systems"], ["subsection", "Question Answering System"], ["subsubsection", "Multi-turn MC"]], "content": "}\nMulti-turn machine comprehension, also known as conversational machine comprehension (CMC), combines the elements of chit-chat and question answering. The difference between MC and CMC is that questions in CMC form a series of conversations and require the proper modeling of history to comprehend the context of the current question correctly. High-quality conversational datasets such as QuAC  and CoQA  have provided the researchers a great source to work deeply in the field of CMC. \nThe first BERT based model for QuAC was based on history answer embeddings to provide extra information to input tokens . Later,  improved accuracy by introducing the last two contexts when answering the current question.  introduced the reasoning process in BERT-based architecture that improved the accuracy on the leader board drastically as compared to the previous models. Currently, the top scores QuAC leaderboard \\footnote{http://quac.ai/} are of BERT-based question answering models.  \nThe BERT and XLNet based models that tested the accuracy on SQuAD dataset also evaluated their models on CoQA. The top positions on CoQA leaderboard \\footnote{https://stanfordnlp.github.io/coqa/} are occupied by pre-trained language models. \nFigure~\\ref{fig:arch} shows how to adapt a BERT-based model for MC or CMC tasks. The input to the model is a question and a paragraph, and the output is the answer span in the given paragraph. A special classification token [CLS] is added before the given question. Then, the question is concatenated with the paragraph into one sequence using [SEP] token. The sequence is provided as an input to the BERT-based model with segment and positional embeddings. Finally, the hidden state of BERT is then converted into the probabilities of start and end answer span by a linear layer and softmax function. \n\\begin{figure}[!h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{architecture2.pdf}\n    \\Description[BERT for QA task.]{The adaptation of BERT to suit the task of machine comprehension.}\n    \\caption{Adaptation of BERT for question answering task}\n    \\label{fig:arch}\n\\end{figure}", "cites": [1098, 8246, 1145], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of the cited papers by outlining the evolution of BERT-based models for multi-turn machine comprehension. However, it largely describes the methods and results without critical evaluation or comparison of their strengths and weaknesses. The abstraction is limited, focusing on specific adaptations of BERT rather than broader principles in conversational QA."}}
{"id": "029ac9c4-7c4d-47a8-8b87-5249fe5ec70d", "title": "Task-Oriented Dialogue System", "level": "subsubsection", "subsections": [], "parent_id": "7c542cf7-a164-43d5-ac94-49d6286e60b4", "prefix_titles": [["title", "A Short Survey of Pre-trained Language Models for Conversational AI-A New Age in NLP"], ["section", "Pre-trained Language Modeling Approaches for Dialogue Systems"], ["subsection", "Other Dialogue Systems"], ["subsubsection", "Task-Oriented Dialogue System"]], "content": "}\nA traditional task-oriented model consists of four modules namely: i) natural language understanding, ii) dialogue state tracking (DST), iii) policy learning, and iv) natural language generation. The goal of such systems is to assist the user by generating a valuable response. This response generation requires a considerable amount of labeled data fro the training purpose. A question that comes naturally to mind is: Can we take advantage of transfer learning through pre-trained language models to enable the modeling of task-oriented systems. The question has been addressed by  which introduces a GPT based framework to evaluate the ability to transfer the generation capability of GPT to task-specific multi-domains. They used multi-domain dataset MultiWoz  to learn and understand the domain-specific tokens which make it easier to adapt to unseen domains. \nChao \\& Lane  recently utilize the strengths of BERT in improving the scalability of DST module. The DST module is use to maintain the state of user's intentions through out the dialogue. The key component of the model is BERT dialogue context encoding module which generates contextualized representations of the words which are very effective for mining slot values from the contextual patterns.", "cites": [7331], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of how pre-trained language models like GPT and BERT are applied to task-oriented dialogue systems, but it lacks deeper synthesis of ideas across papers. There is minimal critical evaluation of the approaches or limitations, and the content remains focused on specific components rather than offering broader conceptual insights."}}
