{"id": "6739ea70-953a-4fff-856c-b68ead691bec", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "11aa860a-a47f-4799-8633-7727b5a3edae", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Introduction"]], "content": "Modeling complex physical dynamics over a wide range of spatial and temporal scales is a fundamental task in a wide range of fields including, for example, fluid dynamics , cosmology , economics, and neuroscience .\nDynamical systems are mathematical objects that are used to describe the evolution of phenomena over time and space occurring in nature. Dynamical systems are commonly described with differential equations which are equations related to one or more unknown functions and their derivatives. \n\\begin{dfn}\\label{dfn:dynamics}\nFix an integer $k \\geq 1$ and let $U$ denote an open subset of $\\mathbb{R}^n$. Let $u: U \\mapsto \\mathbb{R}^m$ and we write $\\bm{u} = (u^1, ..., u^m)$, where $x \\in U$. Then an expression of the form \n\\begin{equation}\n    \\mathcal{F}(D^k \\bm{u}(x), D^{k-1} \\bm{u}(x), ..., D\\bm{u}(x), \\bm{u}(x), x) = 0 \\label{eqn:dynamics}\n\\end{equation}\nis called a $k^{\\text{-th}}$-order system of partial differential equation (or ordinary differential equation when $n=1$), where $\\mathcal{F}: \\mathbb{R}^{mn^k}\\times \\mathbb{R}^{mn^{k-1}}\\times...\\times\\mathbb{R}^{mn}\\times \\mathbb{R}^{m}\\times U \\mapsto \\mathbb{R}^m $.\n\\end{dfn}\n$\\mathcal{F}$ models the dynamics of a $n$-dimensional state $x \\in \\mathbb{R}^n$ and it can be either a linear or non-linear operator. Since most dynamics evolve over time, one of the variables of $u$ is usually the time dimension. In general, one must specify appropriate boundary and initial conditions of Equ.\\ref{eqn:dynamics} to ensure the existence of a solution. Learning dynamical systems is to search for a model $\\mathcal{F}$ that can accurately describe the behavior of the physical process insofar as we are interested. \nPhysics as a discipline has a long tradition of using first principles to describe spatiotemporal dynamics. The laws of physics have greatly improved our understanding of the physical world. Many physics laws are described by systems of highly nonlinear differential equations that have direct implications for understanding and predicting physical dynamics. However, these equations are usually too complicated to be solvable. The current paradigm of numerical methods for solution approximation is purely physics-based: known physical laws encoded in systems of coupled differential equations are solved over space and time via numerical differentiation and integration schemes . However, these methods are tremendously computationally intensive, requiring significant computational resources and expertise. An alternative is seeking simplified models that are based on certain  assumptions and roughly can describe the dynamics, such as Reynolds-averaged Navier-stokes equations for turbulent flows and Euler equations for gas dynamics . But it is highly nontrivial to obtain a simplified model that can describe a phenomenon with satisfactory accuracy. More importantly, for many complex real-world  phenomena, only partial knowledge of their dynamics is known. The equations may not fully represent the true system states.\nDeep Learning (DL) provides efficient alternatives to learn high-dimensional spatiotemporal dynamics from massive datasets. It achieves so by directly predicting the input-output mapping and bypassing numerical integration. Recent works have shown that DL can generate realistic predictions and significantly accelerate the simulation of physical dynamics relative to numerical solvers, from turbulence modeling to weather prediction . This opens up new opportunities at the intersection of DL and physical sciences, such as molecular dynamics, epidemiology, cardiology and material science . \nDespite the tremendous progress, DL is purely data-driven by nature, which has many limitations. DL models still adhere to the fundamental rules of statistical inference. The nonlinear and chaotic nature of real-world dynamics poses significant challenges to existing DL frameworks. Without explicit constraints, DL models are prone to make physically implausible forecasts, violating the governing laws of physical systems. Additionally, DL models often struggle with generalization: models trained on one dataset cannot adapt properly to unseen scenarios with different distributions,  known as distribution shift. For dynamics learning, the distribution shift occurs not only because the dynamics are non-stationary and nonlinear, but also due to the changes in system parameters, such as external forces and boundary conditions. In a word, the current limitation of DL models for learning complex dynamics is their lack of ability to understand the system solely from data and cope with the distributional shifts that naturally occur.\nNeither DL alone nor purely physics-based approaches can be considered sufficient for learning complex dynamical systems in scientific domains. Therefore, there is a growing need for integrating traditional physics-based approaches with DL models so that we can make the best of both types of approaches. There is already a vast amount of work about physics-guided DL , but the focus on deep learning for dynamical systems is still nascent. Physics-guided DL offers a set of tools to blend these physical concepts such as differential equations and symmetry with deep neural networks. On one hand, these DL models offer great computational benefits over traditional numerical solvers. On the other hand, the physical constraints impose appropriate inductive biases on the DL models, leading to accurate simulation, scientifically valid predictions, reduced sample complexity, and guaranteed improvement in generalization to unknown environments.  \nThis survey paper aims to provide a structured overview of existing methodologies of incorporating prior physical knowledge into DL models for learning dynamical systems. The paper is organized as below. \n\\begin{itemize}[leftmargin=*,itemsep=1pt]\n    \\item Section \\ref{obj} describes the significance of physics-guided DL.\n    \\item Section \\ref{prob} formulates the four main learning problems of physics-guided DL, including solving differential equations, dynamics forecasting, learning dynamics residuals, and equation discovery.\n    \\item Section \\ref{sec:loss}$\\sim$\\ref{sec:symmetry} categorizes existing physics-guided DL approaches into four groups based on the way how physics and DL are combined. Each lead with a detailed review of recent work as a case study and further categorized based on objectives or model architecture. \n    \\begin{itemize}[leftmargin=10pt,itemsep=1pt]\n    \\item Section \\ref{sec:loss}: \\texttt{Physics-guided loss function}: prior physics knowledge is imposed as additional soft constraints in the loss function.\n    \\item Section \\ref{sec:architecture}: \\texttt{Physics-guided architecture design}: prior physics knowledge is strictly incorporated into the design of neural network modules. \n    \\item Section \\ref{sec:hybrid}: \\texttt{Hybrid physics-DL models}: complete physics-based approaches are directly combined with DL models. \n    \\item Section \\ref{sec:symmetry}: \\texttt{Invariant and equivariant DL models}: DL models are designed to respect the symmetries of a given physical system.  \n    \\end{itemize}\n    \\item Section \\ref{dis} summarizes the challenges in this field and discusses the emerging opportunities for future research. \n\\end{itemize}", "cites": [6360, 6356, 6364, 6355, 6359, 6363, 6358, 6362, 6361, 9004, 6357, 9003], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 28, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The introduction synthesizes multiple cited papers to establish a clear narrative on the integration of physics and deep learning for dynamical systems. It critically addresses the limitations of both traditional physics-based and purely data-driven DL approaches, while abstracting the discussion to a broader framework of combining physical constraints with deep learning for improved modeling, simulation, and generalization."}}
{"id": "02bc0f7d-fafd-49e6-81e5-3604506d3183", "title": "Accelerate Data Simulation.", "level": "subsection", "subsections": [], "parent_id": "2c786f4c-ff88-40c4-b2c9-bbe2cc54b91e", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Significance of Physics-Guided Deep Learning"], ["subsection", "Accelerate Data Simulation."]], "content": "\\label{data_simulation}\nSimulation is an important method of analyzing, optimizing, and designing real-world processes, which are easily verified, communicated, and understood. It serves as the surrogate modeling and digital twin and  provides valuable  insights into complex physical systems. Traditional physics-based simulations often rely on running numerical methods: known physical laws encoded in systems of coupled differential equations are solved over space and time via numerical differentiation and integration schemes . Although the governing equations of many physical systems are known, finding approximate solutions using numerical algorithms and computers is still prohibitively expensive. Because the discretization step size is usually confined to be very small due to stability constraints when the dynamics are complex. Moreover, the performance of numerical methods can highly depend on the initial guesses of unknown parameters .  Recently, DL has demonstrated great success in the automation, acceleration, and streamlining of highly compute-intensive workflows for science . \nDeep dynamics models can directly approximate high-dimensional spatiotemporal dynamics by directly forecasting the future states and bypassing numerical integration . These models are trained to make forward predictions given the historic frames as input with one or more steps of supervision and can roll out up to hundreds of steps during inference. DL models are usually faster than classic numerical solvers by orders of magnitude since DL is able to take much larger space or time steps than classical solvers .\nAnother common approach is that deep neural networks can directly approximate the solution of complex coupled differential equations via gradient-based optimization, which is the so-called physics-informed neural networks (PINNs). This approach has shown success in approximating a variety of PDEs . Additionally, deep generative models, such as diffusion models and score-based generative models, have been shown effective in accurate molecule graph generation . The computer graphics community has also investigated using DL to speed up numerical simulations for generating realistic animations of fluids such as water and smoke . However, the community focuses more on the visual realism of the simulation rather than the physical characteristics.", "cites": [6368, 6370, 9006, 6358, 6362, 6369, 6357, 6367, 9005, 6365, 4714, 6366], "cite_extract_rate": 0.5454545454545454, "origin_cites_number": 22, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers to highlight how deep learning accelerates physical data simulation, particularly in fluid dynamics and quantum systems. It abstracts these methods into broader categories such as deep dynamics models and physics-informed neural networks. While it includes some critical remarks (e.g., noting the focus of computer graphics work on visual realism over physical accuracy), it primarily offers a structured analytical view rather than deep comparative critique."}}
{"id": "e3e6062a-f714-4c5e-87d8-d0cae69fef76", "title": "Build Scientifically Valid Models.", "level": "subsection", "subsections": [], "parent_id": "2c786f4c-ff88-40c4-b2c9-bbe2cc54b91e", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Significance of Physics-Guided Deep Learning"], ["subsection", "Build Scientifically Valid Models."]], "content": "Despite the tremendous progress of DL for science, e.g., atmospheric science , computational biology , material science , quantum chemistry , it remains a grand challenge to incorporate physical principles in a systematic manner to the design, training, and inference of such models.  DL models are essentially statistical models that learn patterns from the data they are trained on. Without explicit constraints, DL models, when trained solely on data, are prone to make scientifically implausible predictions, violating the governing laws of physical systems. In many scientific applications, it is important that the predictions made by DL models are consistent with the known physical laws and constraints. For example, in fluid dynamics, a model that predicts the velocity field of a fluid must satisfy the conservation of mass and momentum. In materials science, a model that predicts the properties of a material must obey the laws of thermodynamics and the principles of quantum mechanics.\nThus, to build trustworthy predictive models for science and engineering, we need to leverage  known physical principles to guide DL models to learn the correct underlying dynamics instead of simply fitting the observed data. For instance,  improve the physical and statistical consistency of DL models by explicitly regularising the loss function with physical constraints. Hybrid DL models, e.g.,  integrate differential equations in DL for temporal dynamics forecasting and achieve promising performance.   and  studied tensor invariant neural networks that can learn the Reynolds stress tensor while preserving Galilean invariance.  presented a hybrid model that combines the numerical RANS-LES coupling method with a custom-designed U-net. The model uses the temporal and spatial filters in the RANS-LES coupling method to guide the U-net in learning both large and small eddies. This approach improves the both accuracy and physical consistency of the model, making it more effective at representing the complex flow phenomena observed in many fluid dynamics applications.", "cites": [6356, 6372, 6371, 9009, 9007, 7970, 9008], "cite_extract_rate": 0.4666666666666667, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers by connecting the theme of incorporating physical principles into deep learning models, with specific examples in fluid dynamics and materials science. It provides a coherent narrative on the importance of scientific validity. However, it lacks in-depth critical analysis of the methods or limitations of the cited works. There is some abstraction by highlighting the broader need for physical consistency in DL, but not at a meta-level that unifies all approaches into a comprehensive framework."}}
{"id": "74649ae6-c6fe-4c42-9f69-63f48cb5aed2", "title": "Improve the generalizability of DL models", "level": "subsection", "subsections": [], "parent_id": "2c786f4c-ff88-40c4-b2c9-bbe2cc54b91e", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Significance of Physics-Guided Deep Learning"], ["subsection", "Improve the generalizability of DL models"]], "content": "DL models often struggle with generalization: models trained on one dataset cannot adapt properly to unseen scenarios with distributional shifts that may naturally occur in dynamical systems . Because they learn to represent the statistical patterns in the training data, rather than the underlying causal relationships. In addition, most current approaches are still trained to model a specific system and multiple systems with close distributions, making it challenging to meet the needs of the scientific domain with heterogeneous environments. Thus, it is imperative to develop generalizable DL models that can learn and generalize well across systems with various parameter domains.\nPrior physical knowledge can be considered as an inductive bias that can place a prior distribution on the model class and shrink the model parameter search space. With the guide of inductive bias, DL models can better capture the underlying dynamics from the data that are consistent with physical laws. Across different data domains and systems, the laws of physics stay constant. Hence, integrating physical laws in DL enables the models to generalize  outside of the training domain and even to different systems. \nEmbedding symmetries into DL models is one way to improve the generalization, which we will discuss in detail in subsection \\ref{sec:symmetry}. For example,  designed deep equivariant dynamics models that respect the rotation, scaling, and uniform motion symmetries in fluid dynamics. The models are both theoretically and experimentally robust to distributional shifts by symmetry group transformations and enjoy favorable sample complexity compared with data augmentation.\nThere are many other ways to improve the generalization of DL models by incorporating other physical knowledge. \n proposed a meta-learning framework to forecast systems with different parameters. It leverages prior\nphysics knowledge to distinguish different systems. Specifically, it uses an encoder to infer the physical parameters of\ndifferent systems and a prediction network to adapt and forecast giving the inferred system. Moreover,  encodes Lyapunov stability into an autoencoder model for predicting fluid flow and sea surface temperature. They show improved generalizability and reduced prediction uncertainty for neural nets that preserve Lyapunov stability.  shows adding spectral normalization to DNN to regularize its Lipschitz continuity can greatly improve the generalization to new input domains on the task of drone landing control.", "cites": [9010, 6374, 6368, 1356, 6365, 5066, 6373], "cite_extract_rate": 0.8888888888888888, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers to present a coherent narrative on how physics-guided deep learning can improve generalizability. It abstracts key concepts such as inductive bias, symmetry, and stability to frame broader implications. While it provides some critical analysis by highlighting limitations of standard DL approaches, it does not extensively critique or compare the cited works in depth."}}
{"id": "93160546-0995-4122-ae5f-b1a49b951ff2", "title": "Discover Governing Equations", "level": "subsection", "subsections": [], "parent_id": "2c786f4c-ff88-40c4-b2c9-bbe2cc54b91e", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Significance of Physics-Guided Deep Learning"], ["subsection", "Discover Governing Equations"]], "content": "One of the main objectives of science is to discover fundamental laws that can solve practical problems . The discovery of governing equations is crucial as it enables us to comprehend the underlying physical laws that regulate complex systems. By identifying the mathematical models that describe the behavior of a system, we can make accurate predictions and gain insights into how the system will behave under different conditions. This knowledge can be applied to optimize the performance of engineering systems, improve the precision of weather forecasts, and understand the mechanisms behind biological processes, among other applications . However, discovering governing equations is a challenging task for various reasons. Firstly, real-world systems are frequently complex and involve many interdependent variables, making it difficult to identify the relevant variables and their relationships. Secondly, many systems are nonlinear and involve interactions between variables that are hard to model using linear equations. Thirdly, the available data may be noisy or incomplete, making it challenging to extract meaningful patterns and relationships. Despite these challenges, recent advances in machine learning have made it possible to automate the process of governing equations discovery and identify complex, nonlinear models from data. These approaches may lead to new discoveries and insights into the behavior of complex systems for a wide range of applications.\nDiscovering governing equations from data is often accomplished by defining a large set of possible mathematical basis functions and learning the coefficients.  proposed to find ordinary differential equations by creating a dictionary of possible basis functions and discovering sparse, low-dimensional, and nonlinear models from data using the sparse identification. More recent work, such as , incorporated neural networks to further augment the dictionary to model more complex dynamics.   contributed to this trend by introducing an efficient first-order conditional gradient algorithm for solving the optimization problem of finding the best sparse fit to observational data in a large library of potential nonlinear models. Alternatively,  presented a shallow neural network approach, \\textit{EQL} to identify concise equations from data. They replaced the activation functions with predefined basis functions, including identity and trigonometry functions, and used specially designed division units to model division relationships in the potential governing equations. Similarly,  designed \\textit{PDE-Nets} that use convolution to approximate differential operators and symbolic neural networks to approximate and recover multivariate functions. These models could learn various functional relations, with and without divisions, from noisy data in a confined domain. However, scalability, overfitting, and over-reliance on high-quality measurement data remain critical concerns in this research area .", "cites": [9013, 9012, 6375, 6377, 6376, 9011, 9014, 6363], "cite_extract_rate": 0.5333333333333333, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a coherent synthesis of multiple papers by connecting their shared goal of discovering governing equations from data and highlighting common techniques like sparsity and neural network-based methods. It offers some critical discussion, such as scalability and overfitting issues, but does not deeply evaluate or contrast the strengths and weaknesses of each approach. The section identifies general patterns like the use of basis functions and hybrid numeric-symbolic methods, but stops short of deriving meta-level principles or a novel theoretical framework."}}
{"id": "2df7047e-84c8-46a6-a89a-df3ad349ad29", "title": "Solving Differential Equations", "level": "subsection", "subsections": [], "parent_id": "a9633632-c2f0-4164-8ce4-578c209e43ee", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Problem Formulation"], ["subsection", "Solving Differential Equations"]], "content": "\\label{prob:solving}\nWhen $\\mathcal{F}$ in Eq. \\ref{eqn:dynamics} is \\textit{known} but Eq. \\ref{eqn:dynamics} is too complicated to be solvable, researchers tend to directly solve the differential Eq.ations by approximating solution of $\\bm{u}(x)$ with a deep neural network, and enforcing the governing equations as a soft constraint on the output of the neural nets during training at the same time. This approach can be formulated as the following optimization problem, \n\\begin{equation}\\label{equ:pinn_loss}\n    \\text{min}_{\\theta} \\; \\mathcal{L}(\\bm{u}) + \\lambda_\\mathcal{F} \\mathcal{L}_\\mathcal{F}(\\bm{u})\n\\end{equation}\n$\\mathcal{L}(\\bm{u})$ denotes the misfit of neural net predictions and the training data points. $\\theta$ denotes the neural net parameters. $\\mathcal{L}_\\mathcal{F}(\\bm{u})$ is a constraint on the residual of the differential equation system under consideration and $\\lambda_\\mathcal{F}$ is a regularization parameter that controls the emphasis on this residual. The goal is then to train the neural nets to minimize the loss function in Eq. \\ref{equ:pinn_loss}.", "cites": [6378], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic formulation of how physics-guided deep learning can be applied to solve differential equations, but it does so primarily through a descriptive account without substantial synthesis of multiple ideas or in-depth critical evaluation. It mentions one cited paper briefly and does not connect it meaningfully to the formulation. The abstraction is limited, as it does not elevate the discussion to broader principles or trends in the field."}}
{"id": "24e9dc4d-abdb-482d-a5a0-3015f920ffe1", "title": "Learning Dynamics Residuals", "level": "subsection", "subsections": [], "parent_id": "a9633632-c2f0-4164-8ce4-578c209e43ee", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Problem Formulation"], ["subsection", "Learning Dynamics Residuals"]], "content": "When $\\mathcal{F}$ in Eq. \\ref{eqn:dynamics} is \\textit{partially know}, we can use neural nets to learn the errors or residuals made by physics-based models . The key is to learn the bias of physics-based models and correct it with the help of deep learning. The final prediction of the state is composed of the simulation from the physics-based models and the residual prediction from neural nets as below, \n\\begin{equation}\n\\hat{\\bm{u}} = \\hat{\\bm{u}}_\\mathcal{F} + \\hat{\\bm{u}}_{\\text{NN}}.\n\\end{equation}\nwhere $\\hat{u}_\\mathcal{F}$ is the prediction obtained by numerically solving $\\mathcal{F}$, $\\hat{u}_{\\text{NN}}$is the prediction from neural networks and $\\hat{u}$ is the final prediction made by hybrid physics-DL models. \nThis learning problem generally involves two training strategies: 1) joint training: optimizing the parameters in the differential equations and the neural networks at the same time by minimizing the prediction errors of the system states. 2) two-stage training: we first fit differential equations on the training data and obtain the residuals, then directly optimize the neural nets on predicting the residuals.", "cites": [6379, 6380], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the two cited papers by framing the concept of learning dynamics residuals as a strategy for integrating physics-based models with deep learning, showing a basic level of integration. While it does not deeply analyze or critique the approaches, it does provide a general framework for the method. The abstraction is moderate, identifying a broader modeling strategy rather than just specific implementations."}}
{"id": "25fb28d5-de22-49b6-9dd1-7aad94581991", "title": "Dynamics Forecasting", "level": "subsection", "subsections": [], "parent_id": "a9633632-c2f0-4164-8ce4-578c209e43ee", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Problem Formulation"], ["subsection", "Dynamics Forecasting"]], "content": "When $\\mathcal{F}$ in Eq. \\ref{eqn:dynamics} is \\textit{unknown} or numerically solving Eq. \\ref{eqn:dynamics} requires too much computation, many works studied learning high-dimensional spatiotemporal dynamics by directly predicting the input-output system state mapping and bypassing numerical discretization and integration . If we assume the first dimension $x_1$ of $\\bm{u}$ in Eq. \\ref{eqn:dynamics} is the time dimension $t$, then the problem of dynamics forecasting can be defined as learning a map $f: \\mathbb{R}^{n \\times k} \\mapsto \\mathbb{R}^{n \\times q} $ that maps a sequence of historic states to future states of the dynamical system,\n\\begin{equation}\\label{equ_task}\nf(\\bm{u} \\text{\\footnotesize $(t-k+1, \\cdot)$}, ..., \\bm{u}\\text{\\footnotesize $(t, \\cdot)$}) = \\bm{u}\\text{\\footnotesize $(t+1, \\cdot)$}, ..., \\bm{u}\\text{\\footnotesize $(t+q, \\cdot)$}\n\\end{equation}\nwhere $k$ is the input length and $q$ is the output length. $f$ is commonly approximated with purely data-driven or physics-guided neural nets and the neural nets are optimized by minimizing the prediction errors of the state $\\mathcal{L}(\\bm{u})$.", "cites": [4714, 6364, 6368], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 1.7, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic formulation of the dynamics forecasting problem in the context of physics-guided deep learning, referencing cited papers as general motivations. However, it does not synthesize their contributions or connect their ideas in a meaningful way, nor does it offer critical evaluation or identify broader patterns or principles. The content remains largely descriptive and lacks analytical depth."}}
{"id": "4c0f9bdf-90b8-4dbf-bf96-f4340f873224", "title": "Case Study: Physics-informed Neural Networks", "level": "subsection", "subsections": [], "parent_id": "431d9769-2309-472c-bd5c-88ad48003975", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Physics-Guided Loss Functions and Regularization"], ["subsection", "Case Study: Physics-informed Neural Networks"]], "content": "The Physics-informed Neural Networks (PINNs) approach  is a prime example of incorporating physics knowledge into the design of loss functions. PINNs have shown efficiency and accuracy in learning simple differential equations. Using fully connected neural networks, PINNs directly approximate the solution of differential equations with space coordinates and time stamps as inputs. These networks are trained by minimizing both the loss on measurements and the residual function error through the partial differential equation. More specifically, based on the Def. \\ref{dfn:dynamics}, a fully connected neural network is employed to model solution $\\bm{\\hat{u}}(x, t | \\bm{\\theta}_{\\text{PINN}})$, where $\\bm{\\theta}_{\\text{PINN}}$ denotes the weights of the PINN  and be optimized by minimizing the following loss function. \n\\begin{equation}\n    \\mathcal{L}_{\\text{PINN}}=\\mathcal{L}(\\bm{u}) + \\lambda_\\mathcal{F} \\mathcal{L}_\\mathcal{F}(\\bm{u})\n\\end{equation}\n$\\mathcal{L}(\\bm{u})=\\Vert \\bm{\\hat{u}}-\\bm{y}\\Vert _{\\Gamma }$ is the error between the $\\bm{\\hat{u}}(x, t | \\bm{\\theta}_{\\text{PINN}})$ and the set of boundary conditions and measured data on the set of points $\\Gamma$ where the boundary conditions and data are defined. $\\mathcal{L}_{\\mathcal{F}}=\\Vert \\mathcal{F}(\\bm{\\hat{u}}(x, t|\\bm{\\theta}_{\\text{PINN}}), x, t)\\Vert _{\\Gamma}$ is the mean-squared error of the residual function to enforce the predictions generated by PINNs satisfy the desired differential equations. \nHowever, while PINNs have shown some success in capturing the underlying physical phenomena,  has pointed out that they often struggle to learn complex physical systems due to the difficulties posed by PDE regularizations in the optimization problem. Furthermore, the effectiveness of PINNs is highly dependent on the quality of the input data, and performance may suffer in the presence of noise or limited data .  Moreover, limited by poor generalizability of neural networks, PINNs have trouble generalizing to the space and time domain that is not covered in the training set . These limitations present significant challenges for the development and application of PINNs in real-world applications. Nonetheless, continued research into PINNs  may help to overcome these challenges and improve their ability to capture and predict complex physical phenomena.", "cites": [9010, 1356, 6381, 6378, 6995, 6382], "cite_extract_rate": 0.5, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple cited papers to provide a coherent explanation of PINNs and their loss function formulation. It critically analyzes limitations, such as struggles with complex systems, data dependency, and generalizability, as highlighted by relevant studies. While it offers some abstraction by identifying overarching challenges like PDE regularization and domain generalization, it does not fully develop a meta-level conceptual framework."}}
{"id": "2d94600a-9305-415e-97ce-9f02b018ebd3", "title": "Solving Differential Equations", "level": "subsection", "subsections": [], "parent_id": "431d9769-2309-472c-bd5c-88ad48003975", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Physics-Guided Loss Functions and Regularization"], ["subsection", "Solving Differential Equations"]], "content": "Continuing from the previous discussion of PINNs in the previous section, to overcome the optimization difficulties of PINNs,  proposed two ways to alleviate this optimization problem. One is to start by training the PINN on a small constraint coefficient and then gradually increase the coefficient instead of using a big coefficient right away. The other one is training the PINN to predict the solution one time step at a time instead of the entire space-time at once. Furthermore,  found that PINNs can overfit and propagate errors on domain boundaries, even when using physics-inspired regularizers. To address this, they introduced Gaussian Process-based smoothing on boundary conditions to recover PINNs' performance against noise and errors in measurements. Moreover,  proposed a Bayesian framework that combines PINNs with a Bayesian network. Compared to PINNs, the hybrid model can provide uncertainty quantification and more accurate predictions in scenarios with large noise because it can avoid overfitting. Apart from PINNs,  proposed to use flow-based generative models to learn the solutions of probabilistic PDEs while the PDE constraints are enforced in the loss function.  investigated using neural nets to learn the evolution of the velocity fields of incompressible turbulent flow, the divergence of which is always zero. It found that constraining the model with a divergence-free regularizer can reduce the divergence of prediction and improve prediction accuracy.", "cites": [6381, 6356, 6378], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from the cited papers by connecting different approaches to address PINN optimization and overfitting issues, such as gradual coefficient scaling, time-step-wise training, and hybrid Bayesian frameworks. It provides some critical analysis by highlighting problems like overfitting and error propagation in PINNs. However, the abstraction is limited to identifying recurring issues (e.g., noise handling, regularization) rather than offering a deeper, meta-level understanding of the broader field."}}
{"id": "5d1b6a94-b47d-4cf8-8bb8-d59899b49a77", "title": "Improving Prediction Performance", "level": "subsection", "subsections": [], "parent_id": "431d9769-2309-472c-bd5c-88ad48003975", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Physics-Guided Loss Functions and Regularization"], ["subsection", "Improving Prediction Performance"]], "content": "Physics-guided loss functions or regularization have shown great success in improving prediction performance, especially the physical consistency of DL models.  \n used neural nets to model lake temperature at different times and different depths. They ensure that the predictions are physically meaningful by regularizing that the denser water predictions are at lower depths than predictions of less dense water. \n further introduced a loss term that ensures thermal energy conservation between incoming and outgoing heat fluxes for modeling lake temperature.  \n designed conservation layers to strictly enforce conservation laws in their NN emulator of atmospheric convection.\n introduced a more systematic way of enforcing nonlinear analytic constraints in neural networks via constraints in the loss function. \n incorporated the loss of atomic force and atomic energy into neural nets for improved accuracy of simulating molecular dynamics.\n proposed a novel multi-fidelity physics-constrained neural network for material modeling, in which the neural net was constrained by the losses caused by the violations of the model, initial conditions, and boundary conditions.   proposed a novel paradigm for spatiotemporal dynamics forecasting that performs spatiotemporal disentanglement using the functional variable separation. The specific-designed time invariance and regression loss functions ensure the separation of spatial and temporal information. \nHamiltonian mechanics is a mathematical framework that describes the dynamics of a system in terms of the total energy of the system, which is the sum of the kinetic and potential energy.  proposed \\textit{Hamiltonian Neural Nets (HNN)} that parameterizes a Hamiltonian with a neural network and then learn it directly from data. The conservation of desired quantities is constrained in the loss function during training. The proposed \\textit{HNN} has shown success in predicting mass-spring and pendulum systems. \nLagrangian mechanics describes the dynamics of a system in terms of the difference between the kinetic energy and the potential energy of the system.  proposed \\textit{Lagrangian Neural Nets (LNN)} used a neural network to parameterize the Lagrangian function that is the kinetic energy minus the potential energy. They trained the neural network with the Euler-Lagrange constraint loss functions such that it can learn to approximately conserve the total energy of the system.  further simplify the \\textit{HNN} and \\textit{LNN} via explicit constraints.\n further introduced a meta-learning approach in \\textit{HNN} to find the structure of the Hamiltonian that can be adapted quickly to a new instance of a physical system.\n benchmark recent energy-conserving neural network models based on Lagrangian/Hamiltonian dynamics on four different physical systems.", "cites": [6385, 6383, 9009, 6372, 6371, 6384, 9015], "cite_extract_rate": 0.7, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers by connecting the theme of using physics-guided loss functions and architectures to enforce conservation laws in deep learning for dynamical systems. It provides an analytical overview by discussing how different approaches (e.g., HNN, LNN, constraint-based regularization) address physical consistency. However, it lacks deeper critical evaluation of limitations or trade-offs between methods and only briefly abstracts to broader principles like energy conservation, rather than offering a meta-level theoretical framework."}}
{"id": "83f7e0c8-e55d-4858-bf74-074e9b8222ca", "title": "Data Generation", "level": "subsection", "subsections": [], "parent_id": "431d9769-2309-472c-bd5c-88ad48003975", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Physics-Guided Loss Functions and Regularization"], ["subsection", "Data Generation"]], "content": "Simulation is an important method of analyzing, optimizing and designing real-world processes. Current numerical methods require significant computational resources when solving chaotic and complex differential equations. Because numerical discretization step size is confined to be very small due to stability constraints . Also, the estimation of unknown parameters by fitting equations to the observed data requires much manual engineering in each application since the optimization of the unknown parameters in the system highly depends on the initial guesses. Thus, there is an increasing interest in utilizing deep generative models for simulating complex physical dynamics. Many works also imposed physical constraints in the loss function for better physical consistency. \nFor instance,  enforced the constraints of covariance into standard Generative Adversarial Networks (GAN) via statistical regularization, which leads to faster training and better physical consistency compared with standard GAN. \n proposed \\textit{tempoGAN} for super-resolution fluid flow, in which an advection difference loss is used to enforce the temporal coherence of fluid simulation. \n modified \\textit{ESRGAN}, which is a conditional GAN designed for super-resolution, by replacing the adversarial loss with a loss that penalizes errors in the energy spectrum between the generated images and the ground truth data. \nConditional GAN is also applied to emulating numeric hydroclimate models in . The simulation performance is further improved by penalizing the snow water equivalent via the loss function.\n proposed a generative model to simulate fluid flows, in which a novel stream function-based loss function is designed to ensure divergence-free motion for incompressible flows.  proposed a physics-informed convolutional model for flow super-resolution, in which the physical consistency of the generated high-resolution flow fields is improved by minimizing the residuals of Navier-Stokes equations.", "cites": [6370, 6357], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key methods from multiple papers, focusing on how physical constraints are embedded into generative models through loss functions. It connects these works by highlighting a common theme: improving physical consistency in simulations. However, the analysis is limited, as it primarily summarizes the approaches without deep evaluation or contrast. There is some abstraction in identifying the use of physics-based losses, but the insights remain grounded in specific techniques rather than forming a broader conceptual framework."}}
{"id": "5edd73fd-5c5f-4d91-ad68-1ccd440d4ba5", "title": "Pros and Cons", "level": "subsection", "subsections": [], "parent_id": "431d9769-2309-472c-bd5c-88ad48003975", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Physics-Guided Loss Functions and Regularization"], ["subsection", "Pros and Cons"]], "content": "While physics-guided loss functions are easy to design and use, and can improve prediction accuracy and physical consistency, they do have several limitations. Firstly, the physical constraints incorporated in the loss functions are usually considered soft constraints, and may not be strictly enforced. This means that the desired physical properties may not be guaranteed when the models are applied to new datasets. Secondly, PDE regularization can make loss landscapes more complex and cause optimization issues that are difficult to address, as noted in . Finally, there may be a trade-off between prediction errors and physics-guided regularizers. For example,  investigated incompressible turbulent flow prediction using neural nets and found that while constraining the model with a divergence-free regularizer can reduce the divergence of predictions, too much regularization may smooth out small eddies in the turbulence, resulting in a larger prediction error.", "cites": [6356, 6378], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.0}, "insight_level": "high", "analysis": "The section provides a balanced and critical overview of physics-guided loss functions, integrating key limitations identified in the cited works. It connects the general issue of soft constraints and optimization complexity (Paper 2) with a specific example from turbulent flow prediction (Paper 1), demonstrating synthesis. The critical evaluation of trade-offs between accuracy and physical consistency shows depth. While it abstracts to a degree, it could offer more meta-level insights on the broader implications of these trade-offs."}}
{"id": "3141ab9a-f80d-492f-aefc-bc83d6707b59", "title": "Case Study: Turbulent-Flow Net", "level": "subsection", "subsections": [], "parent_id": "86c528e0-a15b-4253-9f86-cd1bed4952cf", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Physics-Guided Design of Architecture"], ["subsection", "Case Study: Turbulent-Flow Net"]], "content": "\\textit{TF-Net}  is a physics-guided DL model for turbulent flow prediction. As shown in Figure \\ref{fig:tfnet}, it applies scale separation to model different ranges of scales of the turbulent flow individually. Computational fluid dynamics (CFD) techniques are at the core of present-day turbulence simulation. Direct Numerical simulations (DNS) are accurate\nbut not computationally feasible for practical applications. Great emphasis was placed on the alternative approaches including Large Eddy Simulation (LES) and Reynolds-averaged Navier-Stokes\n(RANS). Both resort to resolving large scales while modeling small scales, using various averaging techniques and/or low-pass filtering of the governing equations .\nOne of the widely used CFD techniques, the RANS-LES coupling approach ,  combines both Reynolds-averaged Equations (RANs) and Large Eddy Simulation (LES) approaches in order to take advantage of both methods. Inspired by RANS-LES coupling, \\textit{TF-Net} replaces a priori spectral filters with trainable convolutional layers. The turbulent flow is decomposed into three components, each of which is approximated by a specialized U-net to preserve the multiscale properties of the flow. A shared decoder learns the interactions among these three components and generates the final prediction. The motivation for this design is to explicitly guide the ML model to learn the nonlinear dynamics of large-scale and Subgrid-Scale Modeling motions as relevant to the task of spatiotemporal prediction. In other words, we need to force the model to learn not only the large eddies but also the small ones. When we train a predictive model directly on the data with MSE loss, the model may overlook the small eddies and only focus on large eddies to achieve reasonably good accuracy.\nBesides RMSE, physically relevant metrics including divergence and energy spectrum are used to evaluate the performance of the model's prediction. Figure \\ref{results} shows \\textit{TF-Net} consistently outperforms all baselines on physically relevant metrics (Divergence and Energy Spectrum). Constraining it with the divergence-free regularizer that we described in the previous section can further reduce the Divergence. Figure \\ref{rbc_pred} shows the ground truth and predicted velocity along $x$ direction by \\textit{TF-Net} and three best baselines. We see that the predictions by our TF-Net model are the closest to the target based on the shape and frequency of the motions. \nWe also perform an ablation study to understand each component of TF-Net and investigate whether the model has actually learned the flow with different scales. Figure \\ref{results} right includes  predictions and the outputs of each small U-net while the other two encoders are zeroed out. We observe that the outputs of each small U-net are the flow with different scales, which demonstrates that can learn multi-scale behaviors. In summary, \\textit{TF-Net} is able to generate both accurate and physically meaningful predictions of the velocity fields that preserve critical quantities of relevance. \n\\begin{figure*}[hbt!]\n  \\begin{minipage}[b]{0.3\\textwidth}\n     \\includegraphics[width=\\textwidth]{Figures/divergence.png}\n  \\end{minipage} \n  \\begin{minipage}[b]{0.3\\textwidth}\n\\includegraphics[width=\\linewidth]{Figures/spec_ci_square.png}\n  \\end{minipage} \n    \\begin{minipage}[b]{0.37\\textwidth}\n  \\includegraphics[width= \\linewidth]{Figures/ablation.png}\n \\end{minipage} \n \\caption{From left to right: Mean absolute divergence of different models' predictions at varying forecasting horizon; The Energy Spectrum of the target, \\textit{TF-Net}, U-net and ResNet on the leftmost square sub-region; Ablation Study: Ground truth, prediction from \\textit{TF-Net} and the outputs of each small U-net while the other two encoders are zeroed out.}\n \\label{results}\n\\end{figure*}\n\\begin{figure*}[htb!]\n\\centering\n  \\begin{minipage}[b]{0.326\\textwidth}\n\\includegraphics[width= \\linewidth]{Figures/im_09.png}\n  \\end{minipage} \n\\begin{minipage}[b]{0.326\\textwidth}\n\\includegraphics[width= \\linewidth]{Figures/im_19.png}\n  \\end{minipage} \n\\begin{minipage}[b]{0.326\\textwidth}\n\\includegraphics[width= \\linewidth]{Figures/im_59.png}\n  \\end{minipage} \n\\caption{Ground truth and predicted velocity $u$ by \\textit{TF-Net} and three best baselines (U-Net, ResNet, and GAN) at time $T+10$, $T+30$ to $T+60$ (suppose $T$ is the time step of the last input frame).}\n\\label{rbc_pred}\n\\end{figure*}", "cites": [6356], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the TF-Net model, drawing on the cited paper to explain its design and performance. It integrates the model's physics-guided approach with traditional CFD techniques, such as RANS-LES coupling, and discusses the motivation and outcomes of using physically relevant metrics. While it offers some critical evaluation (e.g., the tendency of models to overlook small eddies), the analysis remains grounded in the specific example and does not generalize widely or connect to other works in depth."}}
{"id": "7cf73a89-86f8-4418-9a60-3ecb23bb75f3", "title": "Convolutional architecture", "level": "subsection", "subsections": [], "parent_id": "86c528e0-a15b-4253-9f86-cd1bed4952cf", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Physics-Guided Design of Architecture"], ["subsection", "Convolutional architecture"]], "content": "Convolutional architecture remains dominant in most tasks of  computer vision, such as objection, image classification, and video prediction. Thanks to their efficiency and desired inductive biases, such as locality and translation equivariance, convolution neural nets have  been widely applied to emulate and predict complex spatiotemporal physical dynamics. Researchers have proposed various ways to bake desired physical properties into the design of convolutional models. \nFor example,  proposed to enforce hard linear spatial PDE constraints within CNNs using the Fast Fourier Transform algorithm.\n modified the LSTM units to introduce an intermediate variable to  preserve monotonicity in a convolutional auto-encoder model for lake temperature.  \n proposed a physics-guided convolutional model, \\textit{PhyDNN}, which uses physics-guided structural priors and physics-guided aggregate supervision for modeling the drag forces acting on each particle in a computational fluid dynamics-discrete element Method.  designed \\textit{HybridNet} for dynamics predictions that combine ConvLSTM for predicting external forces with model-driven computation with CeNN for system dynamics. \\textit{HybridNet} achieves higher accuracy on the tasks of forecasting heat convection-diffusion and fluid dynamics. \n proposed to combine deep learning and a differentiable PDE solver for understanding and controlling complex nonlinear physical systems over a long time horizon. \n proposed continuous-filter convolutional layers for modeling quantum interactions. The convolutional kernel is parametrized by neural nets that take relative positions between any two points as input. They obtained a joint model for the total energy and interatomic forces that follow fundamental quantum-chemical principles.\nIn addition, convolution layers have the potential to uncover governing equations. For instance,   developed \\textit{PDE-Net}, which utilizes convolution to approximate differential operators over spatial domains of different orders. It also includes a symbolic neural network based on \\textit{EQL}  to approximate and recover multivariate functions. The authors demonstrated that \\textit{PDE-Net} is more compact than \\textit{SINDy} dictionaries  and numerical experiments suggest that it can uncover the hidden PDE of various observed dynamics.", "cites": [6375, 9007, 6376, 9014], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of various physics-guided convolutional architectures but lacks a deeper synthesis of ideas across the cited works. It mentions different methods and their applications without comparing or contrasting them in a meaningful way. There is minimal critical analysis or abstraction to broader principles."}}
{"id": "83152565-3856-4693-8d4f-42b8ba8a6e4a", "title": "Graph Neural Networks", "level": "subsection", "subsections": [], "parent_id": "86c528e0-a15b-4253-9f86-cd1bed4952cf", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Physics-Guided Design of Architecture"], ["subsection", "Graph Neural Networks"]], "content": "Standard convolutional neural nets only operate on regular or uniform mesh such as images. Graph neural networks move beyond data on the regular grid towards modeling objects with arbitrary positions. For instance, graph neural networks can model the fluid dynamics on irregular meshes that CNNs cannot.  designed a deep encoder-processor-decoder graphic architecture for simulating fluid dynamics under Lagrangian description. The rich physical states are represented by graphs of interacting particles, and complex interactions are approximated by learned message-passing among nodes. \n utilized the same architecture to learn mesh-based simulation. The authors directly construct graphs on the irregular meshes constructed in the numerical simulation methods. In addition, they proposed an adaptive re-meshing algorithm that allows the model to accurately predict dynamics at both large and small scales. \n further proposed two tricks to address the instability and error accumulation issues of training graph neural nets for solving PDEs. One is perturbing the input by a certain noise and only backpropagating errors on the last unroll step, and the other one is predicting multiple steps simultaneously in time. Both tricks make the model faster and more stable. \n proposed a \\textit{Neural Operator} approach that learns the mapping between function spaces, and is invariant to different approximations and grids. More specifically, it used the message-passing graph network to learn Green's function from the data, and then the learned Green's function can be used to compute the final solution of PDEs. \n further extended it to \\textit{Fourier Neural Operator} by replacing the kernel integral operator with a convolution operator defined in Fourier space, which is much more efficient than \\textit{Neural Operator}. In , graph networks were also used to represent, learn, and infer robotics systems, bodies, and joints.  proposed to learn compositional Koopman operators, using graph neural networks to encode the state into object-centric embeddings and using a block-wise linear transition matrix to regularize the shared structure across objects. Another important line of work is incorporating symmetries to design equivariant graph neural nets for modeling molecular dynamics, which will discuss in detail in Section \\ref{sec:symmetry}.", "cites": [6386, 6388, 6387, 6369], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several works on graph neural networks (GNNs) for physics-guided deep learning, effectively connecting their use in modeling complex physical systems such as fluid dynamics and PDEs. While it provides a coherent narrative of how GNNs extend beyond regular grids and introduce various techniques (e.g., re-meshing, noise perturbation), it lacks deeper critical analysis of the limitations or trade-offs of these approaches. It offers some abstraction by highlighting the shift toward object-centric and relation-based modeling, but broader principles or unifying frameworks are not explicitly developed."}}
{"id": "c4430677-3a7e-4d4b-851b-a075c3e0a591", "title": "Multilayer Perceptron", "level": "subsection", "subsections": [], "parent_id": "86c528e0-a15b-4253-9f86-cd1bed4952cf", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Physics-Guided Design of Architecture"], ["subsection", "Multilayer Perceptron"]], "content": "One of the main applications of Multilayer perceptron(MLP) in physics-guided architecture design is finding the linear Koopman operator. Koopman theory  provides a way to represent a nonlinear dynamical system using an infinite-dimensional linear Koopman operator that acts on a Hilbert space of measurement functions of the system state. However, finding the appropriate measurement functions that map the dynamics to the function space, as well as an approximate and finite-dimensional Koopman operator, is highly nontrivial. One way to obtain an approximation of the Koopman operator is through the Dynamic Mode Decomposition algorithm , but this requires manually preparing nonlinear observables, which is not always feasible as prior knowledge about them may be lacking.\nTo address this challenge, recent research has explored using neural networks to learn the Koopman operator. One popular approach hypothesizes that there exists a data transformation that can be learned by neural networks, which yields an approximate finite-dimensional Koopman operator. For example,  and  have proposed using fully connected neural networks to directly map the observed dynamics to a dictionary of nonlinear observables that span a Koopman invariant subspace. This mapping is represented through an autoencoder network, which embeds the observed dynamics onto a low-dimensional latent space where the Koopman operator is approximated by a linear layer.  have further generalized this approach to enable learning the Koopman operator for systems with continuous spectra.  have also designed a similar autoencoder architecture for forecasting physical processes. But in the latent space, the consistency of both the forward and backward systems is ensured, while other models only consider the forward system. This approach performs well on systems that have both forward and backward dynamics, enabling long time prediction.\nKoopman theory can also be used to model real-world dynamics without known governing laws.  have developed a novel approach, \\textit{Koopman Neural Forecaster (KNF)}, to forecast highly non-stationary time series in an interpretable and robust manner. This approach uses Koopman theory to simplify non-linear real-world dynamics into linear systems, which then can be easily manipulated by modifying the Koopman matrix. It employs predefined measurement functions to impose appropriate inductive biases and uses a Koopman operator that varies over time to capture the underlying changing distribution. The model outperforms the state-of-the-art on highly non-stationary time series datasets, including M4, cryptocurrency return forecasting, and sports player trajectory prediction.", "cites": [6390, 6391, 2924, 6389], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple papers on using MLPs and autoencoders for learning Koopman operators, connecting their shared goal of linearizing nonlinear dynamics. It offers a critical perspective by highlighting limitations, such as the need for manual observables in DMD and the novelty of data-driven approaches. The abstraction is moderate, as it identifies the broader theme of using neural networks to approximate Koopman operators but does not fully elevate this to a meta-level framework."}}
{"id": "3df92053-44fc-41a6-9d76-77723b8aeabc", "title": "Case Study: Neural Differential Equations", "level": "subsection", "subsections": [], "parent_id": "02e44777-812c-4110-aad6-ca1628c72747", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Hybrid Physics-DL Model"], ["subsection", "Case Study: Neural Differential Equations"]], "content": "Neural Ordinary Differential Equations (\\textit{Neural ODEs})  generalize traditional RNNs that process data sequentially in discrete time steps by modeling data as continuous functions that change over time, allowing for a more flexible way to capture complex dynamics. They changed the traditionally discretized neuron layer depths into continuous equivalents such that the derivative of the hidden state can be parameterized using a neural network. The output of the network is then computed using a black box differential equation solver, making \\textit{Neural ODEs} an efficient combination of neural nets and numerical solvers.\nMore specifically, they parametrize the velocity $\\bm{\\hat{z}}$ of a hidden state $\\bm{z}$ with the\nhelp of a neural network  $\\bm{\\hat{z}} = f_{\\theta}(\\bm{z}, t)$. Given the initial time $t_0$ and target time $t_T$, \\textit{Neural ODEs} predict the target state $\\bm{\\hat{y}}_T$ by performing the following encoding, integration, and decoding operations:\n\\begin{equation}\n    \\bm{z}(t_0) = \\phi_{\\text{enc}}(\\bm{y}_0), \\;\\;\\;\\;\\;\\; \\bm{z}(t_T) = \\bm{z}(t_0) + \\int_{t_0}^{t_T} f_{\\bm{\\theta}}(\\bm{z}, t) dt, \\;\\;\\;\\;\\;\\; \\bm{\\hat{y}}_T= \\psi_{\\text{dec}}(\\bm{z}(t_T))\n\\end{equation}\nwhere the encoder $\\phi_{\\text{enc}}$ and the decoder $\\psi_{\\text{dec}}$ can be neural networks. Solving an ODE numerically is commonly done by discretization and integration, such as the simple Euler method and higher-order variants of the Runge-Kutta method. However, all of these are computationally intensive since they require backpropagating through the operations of the solvers and store any intermediate quantities of the forward pass, incurring a high memory cost. Thus, the adjoint method  is used to efficiently compute gradients during backpropagation. To compute the gradients of a loss function $L$ with respect to the initial state $\\bm{z}(t_0)$ and the parameters $\\bm{\\theta}$, the key idea of the adjoint method is to introduce an adjoint state $\\bm{p}(t)$, $\\bm{p}(t) = \\frac{\\partial L}{\\partial \\bm{z}(t)}$, which satisfies the following differential equation:\n\\begin{equation}\n\\frac{d\\bm{p}(t)}{dt} = -\\bm{p}(t)^T \\frac{\\partial f_{\\bm{\\theta}}(\\bm{z}(t), t)}{\\partial \\bm{z}} \\label{adjoint_1}\n\\end{equation}\nThe adjoint state is used to compute the gradients of the loss function with respect to the initial state and the parameters using the following formulas:\n\\begin{equation}\n \\frac{\\partial L}{\\partial \\bm{\\theta}} =  - \\int_{t_T}^{t_0}\\bm{p}(t)^T  \\frac{\\partial f_{\\bm{\\theta}}(\\bm{z}(t), t)}{\\partial \\bm{\\theta}} dt; \\label{adjoint_2}\n\\end{equation}\nIn a word, these formulas can be computed efficiently by solving the ODE for $\\bm{p}(t)$ using the same numerical method used to solve the forward ODE. During the forward pass, the ODE solver computes the solution of the differential equation $\\bm{z}(t)$ using the initial state $\\bm{z}(t_0)$ and the function $f_{\\bm{\\theta}}(\\bm{z}(t), t)$. During the backward pass, the adjoint state $\\bm{p}(t)$ is computed by solving Eqn. \\ref{adjoint_1} that starts from the final time $t_T$ and backpropagates through time. This adjoint state is then used to compute the gradients of the loss function with respect to the initial state and the parameters of the ODE function in Eqn. \\ref{adjoint_2}, which can then be used to update the model parameters through gradient descent. \n\\textit{Neural ODEs} have broad potential applications, particularly in domains that require continuous and dynamic models. They offer a useful tool for building continuous-time time series models, which can easily handle data coming at irregular intervals. They also allow for building normalizing flow, which makes it easy to track the change in density, even for unrestricted neural architecture. There have been several follow-up works that further extended the idea of continuous neural nets. For instance,   introduced \\textit{Augmented Neural ODE} that is more expressive, empirically more stable, and lower computationally efficient than Neural ODEs. More importantly, it can learn the functions that have continuous trajectories mappings intersecting each other, which \\textit{Neural ODEs} cannot represent.\n further extended this idea of continuous neural nets to graph convolutions, and proposed \\textit{Graph Neural ODE}.  proposed Neural Stochastic Differential Equation (\\textit{Neural SDE}), which models stochastic noise injection by stochastic differential equations. They demonstrated that incorporating the noise injection regularization mechanism into the continuous neural network can reduce overfitting and achieve lower generalization errors.  proposed a Neural ODE-based generative time-series model that uses the known differential equation instead of treating it as hidden unit dynamics so that they can integrate mechanistic knowledge into the Neural ODE.  utilized neural networks to directly approximate the unknown terms in the differential equations. By using the adjoint method, the proposed model can efficiently compute gradients with respect to all parameters in the model, including the initial conditions, the parameters of the ODE, and the boundary conditions.", "cites": [6394, 7970, 6392, 6395, 6393, 6359], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple works on Neural ODEs and their extensions into a coherent narrative, highlighting key technical aspects like the adjoint method and model expressiveness. It critically discusses the limitations of Neural ODEs, such as their inability to represent intersecting trajectories, and contrasts them with more expressive variants like Augmented Neural ODEs and Neural SDEs. The abstraction level is strong, as it generalizes these models under the broader umbrella of physics-guided deep learning for dynamical systems."}}
{"id": "837bf1bd-7805-4197-9d9f-babce7404a8f", "title": "Residual Modeling", "level": "subsection", "subsections": [], "parent_id": "02e44777-812c-4110-aad6-ca1628c72747", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Hybrid Physics-DL Model"], ["subsection", "Residual Modeling"]], "content": "Perhaps the simplest form of hybrid modeling  is residual learning, where DL learns to predict the errors or residuals made by physics-based models. The key is to learn the bias of physics-based models and correct it with the help of DL models . A representative example is \\textit{DeepGLEAM}  for forecasting COVID-19 mortality that combines a mechanistic epidemic simulation model GLEAM with DL. It uses a Diffusion Convolutional RNN  (DCRNN) to learn the correction terms from GLEAM, which leads to improved performance over either purely mechanistic models or purely DL models on the task of one-week ahead COVID-19 death count predictions\nSimilarly,  combines graph neural nets with a CFD simulator run on a coarse mesh to generate high-resolution fluid flow prediction. CNNs are used to correct the velocity field from the numerical solver on a coarse grid in .  utilized neural networks for subgrid modeling of the LES simulation of two-dimensional turbulence. In , a neural network model is implemented in the reduced order modeling framework to compensate for the errors from the model reduction.  proposed \\textit{DR-RNN} that is trained to find the residual minimizer of numerically discretized ODEs or PDEs. They showed that \\textit{DR-RNN} can greatly reduce both computational cost and time discretization error of the reduced order modeling framework.   introduced the \\textit{APHYNITY} framework that can efficiently augment approximate physical models with deep data-driven networks. A key feature of their method is being able to decompose the problem in such a way that the data-driven model only models what cannot be captured by the physical model.", "cites": [6361, 6358, 6380, 9016, 23, 6379], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of residual modeling approaches by listing several applications where deep learning is used to correct physics-based models. While it references different papers (e.g., DeepGLEAM, DR-RNN, APHYNITY), it does so in a largely additive way without drawing explicit connections or synthesizing a broader narrative. There is minimal critical evaluation or abstraction to overarching principles, and no comparison of the strengths or weaknesses of the methods."}}
{"id": "2a882f41-fe40-483b-b066-37d3f4b426ef", "title": "Intermediate Variable Modeling", "level": "subsection", "subsections": [], "parent_id": "02e44777-812c-4110-aad6-ca1628c72747", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Hybrid Physics-DL Model"], ["subsection", "Intermediate Variable Modeling"]], "content": "DL models can be used to replace one or more components of physics-based models that are difficult to compute or unknown. For example,   replaced the numerical solver for solving Poisson's equations with convolution networks in the procedure of Eulerian fluid simulation, and the obtained results are realistic and showed good generalization properties.  proposed to use neural nets to reconstruct the model corrections in terms of variables that appear in the closure model.  applied a U-net to estimate the velocity field given the historical temperature frames, then used the estimated velocity to forecast the sea surface temperature based on the closed-form solution of the advection-diffusion equation.  combined the high-dimensional model representation that is represented as a sum of mode terms each of which is a sum of component functions with NNs to build multidimensional potential, in which NNs are used to represent the component functions that minimize the error mode term by mode term.", "cites": [6362], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of intermediate variable modeling approaches, citing multiple papers to illustrate how neural networks are used to estimate or replace components within physics-based models. While it connects these examples under the umbrella of 'intermediate variable modeling,' it does so in a relatively superficial manner without offering deeper analysis or evaluation of their relative strengths or limitations. The generalization is limited, focusing mainly on specific applications rather than abstracting overarching principles or trends."}}
{"id": "419bd794-c010-4085-a609-4fa92d46e63a", "title": "Invariant and Equivariant DL Models", "level": "section", "subsections": ["6c5ebb8b-9b87-4db3-8b27-c13e7edee45b", "8a812312-e6e1-48da-a720-2251713df6bf", "663d967b-66a2-4a43-ac10-b22f1a779eac", "e7918c1d-d2ad-48cb-b008-0dda91dcf3ca", "f8b8d401-1a90-42ac-8c1a-1179e1c75423"], "parent_id": "11aa860a-a47f-4799-8633-7727b5a3edae", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Invariant and Equivariant DL Models"]], "content": "\\label{sec:symmetry}\nSymmetry has long been implicitly used in DL to design networks with known invariances and equivariances.  Convolutional neural networks enabled breakthroughs in computer vision by leveraging translation equivariance . Similarly, recurrent neural networks , graph neural networks , and capsule networks  all impose symmetries.\nWhile the equivariant DL models have achieved remarkable success in image and text data , the study of equivariant nets in learning dynamical systems has become increasingly popular recently . Since the symmetries can be integrated into neural nets through not only loss functions but also the design of neural net layers and there has been a large volume of works about equivariant and invariant DL models for physical dynamics, we discuss this topic separately in this section. \n\\begin{wrapfigure}{r}{0.4\\textwidth}\n\\centering\n\\includegraphics[width= 0.4\\textwidth,trim={55 130 55 0}]{Figures/equiv.png} \n\\caption{Illustration of equivariance: $f(x)=2x$ w.r.t $T = \\mathrm{rot}(\\pi/4)$}\n\\label{fig:equi}\n\\end{wrapfigure}\nIn physics, there is a deep connection between symmetries and physics. Noethers law gives a correspondence between conserved quantities and groups of symmetries. For instance, translation symmetry corresponds to the conservation of energy and rotation symmetry corresponds to the conservation of angular momentum. By building a neural network that inherently respects a given symmetry, we thus make conservation of the associated quantity more likely and consequently the models prediction more physically accurate. Furthermore, by designing a model that is inherently equivariant to transformations of its inputs, we can guarantee that our model generalizes automatically across these transformations, making it robust to distributional shifts. \nA \\textbf{group of symmetries} or simply \\textbf{group} consists of a set $G$ together with an associative composition map $\\circ \\colon G \\times G \\to G$.  The composition map has an identity $1 \\in G$ and composition with any element of $G$ is required to be invertible. A group $G$ has an \\textbf{action} on a set $S$ if there is an action map $\\cdot \\colon G \\times S \\to S$ which is compatible with the composition law.  We say further that $\\rho : G \\mapsto GL(V)$ is a \\textbf{$G$-representation} if the set $V$ is a vector space and each group element $g \\in G$ is represented by a linear map (matrix) $\\rho(g)$ that acts on $V$. Formally, a function $f \\colon X \\to Y$ may be described as respecting the symmetry coming from a group $G$ using the notion of equivariance. \n\\begin{dfn}\\label{dfn:equivariance}\nAssume a group representation $\\rho_{\\text{in}}$ of $G$ acts on $X$ and $\\rho_{\\text{out}}$ acts on $Y$. We say a function $f$ is \\textbf{$G$-equivariant} if \n\\begin{equation}\n    f( \\rho_{\\text{in}}(g)(x)) = \\rho_{\\text{out}}(g) f(x) \\label{eq:strictequivariance}\n\\end{equation}\nfor all $x \\in X$ and $g \\in G$. The function $f$ is \\textbf{$G$-invariant} if $f( \\rho_{\\text{in}}(g)(x)) = f(x)$ for all $x \\in X$ and $g \\in G$.  This is a special case of equivariance for the case $\\rho_{\\mathrm{out}}(g) = 1$. See Figure \\ref{fig:equi} for an illustration of a rotation equivariant function. \n\\end{dfn}", "cites": [6368, 8154, 8153, 6397, 6399, 9017, 9005, 6401, 6398, 8152, 6400, 6396, 306], "cite_extract_rate": 0.5, "origin_cites_number": 26, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes concepts from multiple papers on symmetry in deep learning, connecting them to the physical principles of conservation laws via Noethers theorem. It abstracts these methods into a general framework of group representations and equivariance, providing theoretical context. While it offers some analysis of the benefits of symmetry incorporation, it lacks deeper comparative or critical evaluation of the cited works limitations or trade-offs."}}
{"id": "6c5ebb8b-9b87-4db3-8b27-c13e7edee45b", "title": "Case Study: Equivariant Deep Dynamics Models", "level": "subsection", "subsections": [], "parent_id": "419bd794-c010-4085-a609-4fa92d46e63a", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Invariant and Equivariant DL Models"], ["subsection", "Case Study: Equivariant Deep Dynamics Models"]], "content": " exploited the symmetries of fluid dynamics to design equivariant networks. The Navier-Stokes equations are invariant under the following five different transformations. Individually, each of these types of transformations generates a group of symmetries in the system. \n\\begin{itemize*}\n    \\item Space translation: \\; $T_{\\bm{c}}^{\\mathrm{sp}}\\bm{w}(\\bm{x}, t) = \\bm{w}(\\bm{x-c}, t)$, \\; $\\bm{c} \\in \\mathbb{R}^2$,\n    \\item Time translation: \\;$T_{\\tau}^{\\mathrm{time}}\\bm{w}(\\bm{x}, t) = \\bm{w}(\\bm{x}, t-\\tau)$, \\; $\\tau \\in \\mathbb{R}$,\n    \\item Galilean transformation: \\;$T_{\\bm{c}}^{\\mathrm{gal}}\\bm{w}(\\bm{x}, t) = \\bm{w}(\\bm{x}-\\bm{c}t, t) + \\bm{c}$, \\; $\\bm{c} \\in \\mathbb{R}^2$,\n    \\item Rotation/Reflection: \\; $T_R^{\\mathrm{rot}}\\bm{w}(\\bm{x}, t) = R\\bm{w}(R^{-1}\\bm{x}, t), \\; R \\in O(2)$,\n    \\item Scaling: \\; $T_{\\lambda}^{sc}\\bm{w}(\\bm{x},t) = \\lambda\\bm{w}(\\lambda\\bm{x}, \\lambda^2t)$, \\; $\\lambda \\in \\mathbb{R}_{>0}$.\n\\end{itemize*}\nConsider a system of \ndifferential operators $\\mathcal{D}$ acting on $\\hat{\\mathcal{F}}_V$.  Denote the set of solutions $\\mathrm{Sol}(\\mathcal{D}) \\subseteq \\hat{\\mathcal{F}}_V.$ We say $G$ is \\textbf{a symmetry group of $\\mathcal{D}$} if $G$ preserves $\\mathrm{Sol}(\\mathcal{D})$. That is, if $\\varphi$ is a solution of $\\mathcal{D}$, then for all $g \\in G$, $g(\\varphi)$ is also. \nIn order to forecast the evolution of a system $\\mathcal{D}$, we model the forward prediction function $f$. Let $\\bm{w} \\in \\mathrm{Sol}(\\mathcal{D})$.  The input to $f$ is a collection of $k$ snapshots at times $t - k,\\ldots,t-1$ denoted $\\bm{w}_{t-i} \\in  \\mathcal{F}_d$.  The prediction function $f\\colon \\mathcal{F}_d^k \\to \\mathcal{F}_d$ is defined $f(\\bm{w}_{t-k},\\ldots,\\bm{w}_{t-1}) = \\bm{w}_{t}$.  It predicts the solution at a time $t$ based on the solution in the past. \nLet $G$ be a symmetry group of $\\mathcal{D}$.  Then for $g \\in G$, $g(\\bm{w})$ is also a solution of $\\mathcal{D}$.  Thus  $f(g\\bm{w}_{t-k},\\ldots,g\\bm{w}_{t-1}) = g\\bm{w}_{t}$.  Consequently, $f$ is $G$-equivariant.\nThey tailored different methods for incorporating each symmetry into CNNs for spatiotemporal dynamics forecasting. CNNs are time translation-equivariant when used in an autoregressive manner. Convolutions are also naturally space translation equivariant. Scale equivariance in dynamics is unique as the physical law dictates the scaling of magnitude, space and time simultaneously. To achieve this, they replaced the standard convolution layers with group correlation layers over the group $G=(\\mathbb{R}_{>0},\\cdot)\\ltimes(\\mathbb{R}^2,+)$ of both scaling and translations. The $G$-correlation upgrades this operation by both translating \\textit{and} scaling the kernel relative to the input, \n\\begin{equation}\\label{eqn:groupcorr}\n \\bm{v}(\\bm{p}, s, \\mu) = \\sum_{\\lambda \\in \\mathbb{R}_{>0}, t \\in \\mathbb{R}, \\bm{q} \\in \\mathbb{Z}^2 } \\mu   \\bm{w}(\\bm{p} + \\mu \\bm{q}, \\mu^2 t,    \\lambda ) K(\\bm{q},s,t,\\lambda),\n\\end{equation}\nwhere $s$ and $t$ denote the indices of output and input channels. They add an axis to the tensors corresponding to the scale factor $\\mu$. In addition, the rotational symmetry was modeled using $\\mathrm{SO}(2)$-equivariant convolutions and activations within the \\texttt{E(2)-CNN} framework .\nTo make CNNs equivariant to Galilean transformation, since they are already translation-equivariant, it is only necessary to make them equivariant to uniform motion transformation, which is adding a constant vector field to the vector field. This is part of Galilean invariance and relevant to all non-relativistic physics modeling. And the uniform motion equivariance is enforced by conjugating the model with shifted input distribution. Basically, for each sliding local block in each convolutional layer, they shift the mean of the input tensor to zero and shift the output back after convolution and activation function per sample. In other words,  if the input is $\\bm{\\mathcal{P}}_{b \\times d_{in} \\times s\\times s} $ and the output is $\\bm{\\mathcal{Q}}_{b \\times d_{out}} = \\sigma(\\bm{\\mathcal{P}} \\cdot K)$ for one sliding local block, where $b$ is batch size, $d$ is number of channels, $s$ is the kernel size, and $K$ is the kernel, then\n\\begin{align}\n\\bm{\\mu}_i = \\mathrm{Mean}_{jkl} \\left(\\bm{\\mathcal{P}}_{ijkl}\\right); \\quad\n\\bm{\\mathcal{P}}_{ijkl}\\mapsto \\bm{\\mathcal{P}}_{ijkl} - \\bm{\\mu}_i; \\quad\n\\bm{\\mathcal{Q}}_{ij} \\mapsto \\bm{\\mathcal{Q}}_{ij} + \\bm{\\mu}_i.\n\\end{align}\nThis will allow the convolution layer to be equivariant with respect to uniform motion. If the input is a vector field, this operation is applied to each element.\nThe DL models used are \\textit{ResNet} and \\textit{U-Net}, and their equivariant counterparts. Spatiotemporal prediction is done autoregressively. Standard RMSE and an RMSE computed on the energy spectra are used to measure performance. The models are tested on Rayleigh-B\\'enard convection (RBC) and reanalysis ocean current velocity data. For RBC, the test sets have random transformations from the relevant symmetry groups applied to each sample. This mimics real-world data in which each sample has an unknown reference frame. For ocean data, tests are also performed on different time ranges and different domains from the training set, representing distributional shifts. Figure \\ref{vel_u} shows the equivariant models perform significantly better than their non-equivariant counterparts on both simulated RBC data and real-world reanalysis ocean currents. They also show equivariant models also achieve much lower energy spectrum errors and enjoy favorable sample complexity compared with data augmentation. \n\\begin{figure*}[htb!]\n  \\begin{minipage}[b]{0.241\\textwidth}\n   \\includegraphics[width=\\textwidth]{Figures/um-resnet.png}\n  \\end{minipage} \\hfill\n  \\begin{minipage}[b]{0.241\\textwidth}\n     \\includegraphics[width=\\textwidth]{Figures/mag-resnet.png}\n  \\end{minipage} \\hfill\n  \\begin{minipage}[b]{0.241\\textwidth}\n\\includegraphics[width=\\textwidth]{Figures/rot-resnet.png}\n  \\end{minipage} \\hfill\n  \\begin{minipage}[b]{0.241\\textwidth}\n\\includegraphics[width=\\textwidth]{Figures/scale-resnet.png}\n  \\end{minipage} \n    \\begin{minipage}[b]{\\textwidth}\n\\includegraphics[width=\\textwidth]{Figures/pred_resnet2.png}\n  \\end{minipage} \n\\caption{Top: The ground truth and the predicted velocity norm fields $\\|\\bm{w}\\|_2$ of RBC at time step $1$, $5$ and $10$ by the \\texttt{ResNet} and four \\texttt{Equ-ResNets} on four test samples applied with random uniform motion, magnitude, rotation, and scaling transformations respectively. The first column is the target, the second is \\texttt{ResNet} predictions, and the third is predictions by \\texttt{Equ-ResNets}. Bottom: The ground truth and predicted velocity norm fields of ocean currents by \\texttt{ResNet} (\\texttt{Unet}) and four \\texttt{Equ-ResNets} (\\texttt{Equ-Unets}) on the test set.}\n\\label{vel_u}\n\\end{figure*}", "cites": [6368, 8154], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes concepts from the cited papers to explain how symmetry can be integrated into deep dynamics models. It connects different types of symmetries (e.g., translation, scaling, rotation) with specific architectural modifications, such as group correlation layers and SO(2)-equivariant convolutions. It provides a critical perspective by discussing how equivariance improves generalization and performance under distributional shifts, and abstracts the discussion to broader principles of symmetry in modeling physical systems."}}
{"id": "8a812312-e6e1-48da-a720-2251713df6bf", "title": "Equivariant Convolutional Neural Networks", "level": "subsection", "subsections": [], "parent_id": "419bd794-c010-4085-a609-4fa92d46e63a", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Invariant and Equivariant DL Models"], ["subsection", "Equivariant Convolutional Neural Networks"]], "content": "\\label{fluid_sym}\nHowever, real-world dynamical data rarely conform to strict mathematical symmetries, due to noise and missing values or symmetry-breaking features in the underlying dynamical system.  further explored \\textit{approximately} equivariant convolutional networks that are biased towards preserving symmetry but are not strictly constrained to do so. The key idea is relaxing the weight-sharing schemes by introducing additional trainable weights that can vary across group elements to break the strict equivariance constraints. The proposed approximate equivariant networks can always learn the correct amount of symmetry from the data, and thus consistently perform well on real-world turbulence data with no symmetry, approximate symmetry, and perfect symmetry.  When we incorporate prior knowledge into neural nets, we usually need to choose between strictly enforcing it in the design of the model or softly constraining it via regularizers. But this approach allows the model to decide whether and how to use prior knowledge (symmetry) based on the specific task. Moreover,  built a meta-learning framework, DyAd, to forecast systems with different parameters. Specifically, it utilized an encoder capable of extracting the time-invariant and translation-invariant parts of a dynamical system and a prediction network to adapt and forecast giving the inferred system. Time invariance is achieved by using 3D convolution and time-shift invariant loss. On challenging turbulent flow prediction and real-world ocean temperature and currents forecasting tasks, this is the \\textit{first} framework that can generalize and predict dynamics across a wide range of heterogeneous domains. \nApart from incorporating symmetries into regular convolution, there has been a surge of interest in designing equivariant continuous convolution models. This is due to the fact that continuous convolution allows for convolutional operations to be performed on a continuous input domain. For instance,  proposed \\textit{SchNet}, which is a continuous convolution framework that generalizes the CNN approach to continuous convolutions to model particles at arbitrary positions. Continuous convolution kernels are generated by dense neural networks that operate on the interatomic distances, which ensures rotational and translation invariance of the energy.  In a traffic forecasting application,  proposed a novel model, \\textit{Equivariant Continuous COnvolution (ECCO)} that uses rotationally equivariant continuous convolutions to embed the symmetries of the system for improved trajectory prediction. The rotational equivariance is achieved by a weight-sharing scheme within kernels in polar coordinates. \\textit{ECCO} achieves superior performance to baselines on two real-world trajectory prediction datasets, Argoverse and TrajNet++.", "cites": [6402, 9007, 6365], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes concepts from multiple papers to form a coherent narrative around equivariant and approximately equivariant CNNs, highlighting both their design and real-world applicability. It offers some critical insights by contrasting strict symmetry enforcement with flexible, data-driven approaches and emphasizes the novel contributions of the cited frameworks. The abstraction is strong, as it generalizes the discussion to broader principles of symmetry in modeling dynamical systems and the integration of physical knowledge."}}
{"id": "663d967b-66a2-4a43-ac10-b22f1a779eac", "title": "Equivariant Graph Neural Networks", "level": "subsection", "subsections": [], "parent_id": "419bd794-c010-4085-a609-4fa92d46e63a", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Invariant and Equivariant DL Models"], ["subsection", "Equivariant Graph Neural Networks"]], "content": "In addition to the equivariant convolution, numerous equivariant graph neural nets have also been developed, particularly for modeling atomic systems and molecular dynamics. This is due to the pervasive presence of symmetry in molecular physics, as evidenced by roto-translation equivariance in molecular conformations and coordinates. \n designed E(n)-equivariant graph neural network for predicting molecular properties. It updates edge features with the Euclidean distance between nodes and updates the coordinates of particles with the weighted sum of relative differences of all neighbors. \n proposed to use a score-based generative model for generating molecular conformation. The authors used equivariant graph neural networks to estimate the score function, which is the gradient fields of the log density of atomic coordinates because it is roto-translation equivariant. \n designed \\textit{Cormorant}, a rotationally covariant neural network architecture for learning the behavior and properties of complex many-body physical systems. \\textit{Cormorant} achieves promising results in learning molecular potential energy surfaces on the MD-17 dataset and learning the geometric, energetic, electronic, and thermodynamic properties of molecules on the GDB-9 dataset. \n proposed a model for autoregressive generation of 3D molecular structures with reinforcement learning (RL). The method uses equivariant state representations for autoregressive generation, built largely from \\textit{Cormorant}, and integrates such representations within an existing actor-critic RL generation framework.  \n further designed a series SE(3)-equivariant operations and building blocks for DL architectures operating on geometric point cloud data, which was used to construct \\textit{PhiSNet}, a novel architecture capable of accurately predicting wavefunctions and electronic densities.\nAdditionally, permutation invariance also exists in molecular dynamics. For instance, quantum mechanical energies are invariant if we exchange the labels of identical atoms. However,  stated that enforcing equivariance to all permutations in graph neural nets can be very restrictive when modeling molecules. Thus, they proposed to decompose a graph into a collection of local graphs that are isomorphic to a pre-selected template graph so that the sub-graphs can always be canonicalized to template graphs before convolution is applied. By doing this, the graph neural nets can not only be much more expressive but also locally equivariant.  proposed to build equivariant neural networks based on the idea that nonlinear $O(d)$-equivariant functions can be universally expressed in terms of a lightweight collection of scalars, which are simpler to build. They demonstrated the efficiency and scalability of their proposed approach to two classical physics problems, calculating the total mechanical energy of particles and the total electromagnetic force, that obeys all translation, rotation, reflection, and permutation symmetries. Moreover, since the design of equivariant layers is a difficult task,  proposed a lie point symmetry data augmentation method for training graph neural PDE solvers and this method enables these neural solvers to preserve multiple symmetries.", "cites": [9019, 6404, 6405, 6364, 9018, 6403], "cite_extract_rate": 1.0, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers on equivariant graph neural networks by highlighting their shared goal of preserving physical symmetries, particularly in molecular systems. It also abstracts key principles such as roto-translation equivariance and permutation invariance, showing how these ideas inform different architectures and applications. While it does offer some critical insights, such as noting the restrictiveness of permutation equivariance, it could further deepen its analysis by more explicitly comparing the strengths and weaknesses of each method."}}
{"id": "e7918c1d-d2ad-48cb-b008-0dda91dcf3ca", "title": "Symmetry Discovery", "level": "subsection", "subsections": [], "parent_id": "419bd794-c010-4085-a609-4fa92d46e63a", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Invariant and Equivariant DL Models"], ["subsection", "Symmetry Discovery"]], "content": "There has been also an emerging area that is symmetry discovery, the key idea of which is to find the weight-sharing patterns in neural networks that have been trained on data with symmetries. For instance,  factorized the weight matrix in a fully connected layer into a symmetry (i.e. weight-sharing) matrix and a vector of filter parameters. The two parts are learned separately in the inner and outer loop training with the Model-Agnostic Meta-Learning algorithm (MAML) , which is an optimization-based meta-learning method so that the symmetry matrix can learn the weight-sharing pattern from the data. Furthermore,  proposed \\textit{Lie Algebra Convolutional Network (L-conv)}, a novel architecture that can learn the Lie algebra basis and automatically discover symmetries from data. It can be considered as an infinitesimal version of group convolution.  further leveraged \\textit{L-conv} to construct the \\textit{LieGAN}, to automatically discover equivariances from a dataset using a paradigm akin to generative adversarial training. It represents symmetry as an interpretable Lie algebra basis and can discover various symmetries. Specifically, a generator learns a group of transformations applied to the data, which preserves the original distribution and fools the discriminator.", "cites": [6406, 1695, 9020, 6407], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information across multiple papers by connecting different symmetry-discovery approaches (reparameterization with MAML, L-conv, and LieGAN), and integrates them into a broader theme of learning symmetries from data. While it provides a clear analytical framing of the methods, it lacks deeper critical evaluation of their limitations or trade-offs. The abstraction is moderate, highlighting that symmetry can be learned and represented (e.g., via Lie algebras), but it does not fully generalize to meta-level principles or broader implications for the field."}}
{"id": "f8b8d401-1a90-42ac-8c1a-1179e1c75423", "title": "Pros and Cons", "level": "subsection", "subsections": [], "parent_id": "419bd794-c010-4085-a609-4fa92d46e63a", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Invariant and Equivariant DL Models"], ["subsection", "Pros and Cons"]], "content": "By designing a model that is intrinsically equivariant or invariant to input transformations, we can ensure that our model generalizes automatically across these transformations, making it resilient to distributional shifts. In contrast, data augmentation techniques cannot provide equivariance guarantees when the models are applied to new datasets. Empirically and theoretically, it has been shown that equivariant and invariant neural nets offer superior data and parameter efficiency compared to data augmentation techniques. Furthermore, incorporating symmetries enhances the physical consistency of neural nets because of Noether's Law.\nHowever, incorporating too many symmetries may overly constrain the representation power of neural nets and slow down both training and inference. In addition, many real-world dynamics do not have perfect symmetries. A perfectly equivariant model that respects a given symmetry may have trouble learning partial or approximated symmetries in real-world data. Thus, an ideal model for real-world dynamics should be approximately equivariant and automatically learn the correct amount of symmetry in the data, such as the paper we discussed in Section \\ref{fluid_sym}. There are a few other works that explore the same idea. For instance,  proposed the soft equivariant layer by directly summing up a flexible layer with one that has strong equivariance inductive biases to model the soft equivariance.", "cites": [6408], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of invariant and equivariant DL models, synthesizing the concept with a cited paper on soft equivariance. It connects ideas about symmetry incorporation and model efficiency, but the critical evaluation is limited to general limitations without deeper assessment of the cited work. The abstraction is reasonable, identifying broader principles around symmetry and model design, but stops short of a meta-level synthesis."}}
{"id": "eccbcee2-77e5-4ee0-8e97-633aae82422e", "title": "Improving Generalization", "level": "subsection", "subsections": [], "parent_id": "73734564-db92-46a8-a87d-86945d31f17a", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Discussion"], ["subsection", "Improving Generalization"]], "content": "Generalization is a central problem in machine learning. One current limitation of deep learning models for learning complex dynamics is their inability to understand the system solely from data and handle distributional shifts that naturally occur. Most deep learning models for dynamics modeling are trained to model a specific system and still struggle with generalization. For example, in turbulence modeling, deep learning models trained with fixed boundaries and initial conditions often fail to generalize to fluid flows with different characteristics. To overcome this limitation, one approach is to build physics-guided deep learning models, where the physics part plays a dominant role while the neural networks focus on learning the unknown process . Another promising direction is meta-learning. For instance,  proposed a model-based meta-learning method called \\textit{DyAd} that can generalize across heterogeneous domains of fluid dynamics. However, this model can only generalize well on the dynamics with interpolated physical parameters and cannot extrapolate beyond the range of the physical parameters in the training set. Another idea is to transform data into a canonical distribution that neural networks can learn from and then restore the original data after predictions are made . Since neural networks struggle with multiple distributions, this approach aims to find a single distribution that can represent the dynamics effectively. A trustworthy and reliable model for learning physical dynamics should be able to extrapolate to systems with various parameters, external forces, or boundary conditions while maintaining high accuracy. Therefore, further research into generalizable physics-guided deep learning is crucial.", "cites": [6365, 6394], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section discusses generalization challenges in deep learning for dynamical systems and introduces physics-guided and meta-learning approaches to address them, drawing from the cited works. While it provides some critical evaluation (e.g., DyAd's inability to extrapolate), the synthesis is limited to a few ideas without a deeper integration or novel framework. The abstraction is modest, identifying the need for generalization but not offering overarching principles or a meta-level analysis."}}
{"id": "d70875a1-2a25-4ccf-bfc5-a684deea09d8", "title": "Learning Dynamics in Non-Euclidean Spaces", "level": "subsection", "subsections": [], "parent_id": "73734564-db92-46a8-a87d-86945d31f17a", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Discussion"], ["subsection", "Learning Dynamics in Non-Euclidean Spaces"]], "content": "Spatiotemporal phenomena, from global ocean currents to the spread of infectious diseases, are examples of dynamics in non-Euclidean spaces, which means they cannot be easily represented using traditional Euclidean geometry. To address this issue, the field of geometric deep learning   has emerged. Geometric deep learning aims to generalize neural network models to non-Euclidean domains such as graphs and manifolds. However, most of the existing work in this field has been limited to static graph data. Thus, learning dynamics in non-Euclidean Ssaces is a promising direction, and geometric concepts, such as rent notions of distance, curvature, and parallel transport, must be taken into account when designing models. For example, when modeling the ocean dynamics on the earth, which is a sphere, we need to encode the gauge equivariance  in the design of neural nets since there is no canonical coordinate system on a sphere.", "cites": [8153, 5272], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from the two cited papers on geometric deep learning and gauge equivariance, connecting them to the broader challenge of modeling dynamics in non-Euclidean spaces. It abstracts these ideas to highlight the importance of geometric principles in model design, particularly for spherical domains. However, the critical analysis is limited, as it does not deeply evaluate the limitations or trade-offs of the approaches, nor does it provide a novel integrative framework that would elevate the synthesis and abstraction to a higher level."}}
{"id": "64225fa4-1f2e-4df2-88a3-15964074f7ce", "title": "Theoretical Analysis", "level": "subsection", "subsections": [], "parent_id": "73734564-db92-46a8-a87d-86945d31f17a", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Discussion"], ["subsection", "Theoretical Analysis"]], "content": "The majority of literature on learning dynamics with DL focuses on the methodological and practical aspects. Research into the theoretical analysis of generalization\nis lacking. Current statistical learning theory is based on the typical assumption that training\nand test data are identically and independently distributed (i.i.d.) samples from some unknown\ndistribution . However, this assumption does not hold for most dynamical systems, where observations at different times and locations may be highly correlated.  provided the discrepancy-based generalization guarantees\nfor time series forecasting. On the basis of this,   took the first step to derive generalization bounds for equivariant models and data augmentation in the dynamics forecasting setting. The derived upper bounds are expressed in terms of measures of distributional shifts and group transformations, as well as the Rademacher complexity. But these bounds are sometimes not very informative since many of the inequalities used can be loose. However, to better understand the performance of DL on learning dynamics, we need to derive\ngeneralization bounds expressed in terms of the characteristics of the dynamics, such as the order and Lyapunov exponents. Deriving lower generalization bounds are also necessary since they reveal the best performance scenarios. Theoretical studies can also inspire research into model\ndesign and algorithm development for learning dynamics.", "cites": [6409], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section briefly references cited papers to position theoretical analysis in the context of deep learning for dynamical systems. It provides a general overview of the limitations of current statistical learning theory and mentions some recent work, but lacks deep synthesis of multiple sources. It identifies a gap in the literature and points to potential directions for future research, showing some level of critical and abstract thinking but not at a very high or novel level."}}
{"id": "c4466201-6b1b-45e0-bdc1-834c097f0655", "title": "Causal Inference in Dynamical Systems", "level": "subsection", "subsections": [], "parent_id": "73734564-db92-46a8-a87d-86945d31f17a", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Discussion"], ["subsection", "Causal Inference in Dynamical Systems"]], "content": "A fundamental pursuit in science is to identify causal relationships. In the context of dynamical systems, we may ask which variables directly or indirectly influence other variables through intermediates. While traditional approaches to the discovery of causation involve conducting controlled real experiments , data-driven approaches have been proposed to identify causal relations from observational data in the past few decades . However, most data-driven approaches do not directly address the challenge of learning causality with big data. Many questions remain open, such as using causality to improve deep learning models, disentangling complex and multiple treatments, and designing the environment to control the given dynamics. Additionally, we are also interested in understanding the systems response under interventions. For example, when using deep learning to model climate dynamics, we need to make accurate predictions under different climate policies, such as carbon pricing policies and the development of clean energy, to enable better decisions by governments for controlling climate change.", "cites": [6410], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section introduces the concept of causal inference in dynamical systems and briefly mentions its relevance in deep learning. It cites one paper but does not integrate or synthesize ideas from multiple sources effectively. There is minimal critical analysis of the cited work, and while it does generalize to the broader context of deep learning in climate modeling, it lacks a deeper abstraction or framework that unifies the topic comprehensively."}}
{"id": "9aac7376-bdb8-4341-aa04-e950a4b124d3", "title": "Search for Physical Laws", "level": "subsection", "subsections": [], "parent_id": "73734564-db92-46a8-a87d-86945d31f17a", "prefix_titles": [["title", "Physics-Guided Deep Learning for Dynamical Systems: A Survey"], ["section", "Discussion"], ["subsection", "Search for Physical Laws"]], "content": "Another promising direction is to seek physics laws with the help of DL. The search for fundamental laws of practical problems is the main theme of science. Once the governing equations of dynamical systems are found, they allow for accurate mathematical modeling, increased interpretability, and robust forecasting. However, current methods are limited to selecting from a large dictionary of possible mathematical terms . The extremely large search space, limited high-quality experimental data, and overfitting issues have been critical concerns. Another line of work is to discover symmetry from the observed data instead of the entire dynamics with the help of DL . But these works can only work well on synthetic data and discover known symmetries. Still, research on data-driven methods based on DL for discovering physics laws is quite preliminary.", "cites": [9012, 9020, 1695, 9011], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a critical overview of deep learning approaches for discovering physical laws, referencing four relevant papers. It synthesizes the main ideas from the cited works, such as the use of sparsity-promoting methods and Lie algebra-based symmetry discovery. While it identifies some limitations (e.g., reliance on synthetic data, preliminary state of research), the analysis remains at a moderate level without proposing a novel framework or deeper comparative insight."}}
