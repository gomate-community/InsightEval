{"id": "f16a1a1e-25a7-4e14-aaa7-69bb7d4fc8b0", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "4bb82714-5571-4179-9dac-693fb550f058", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Introduction"]], "content": "Recent years have seen significant progress in recommendation techniques, from traditional recommendation techniques, e.g., collaborative filtering, content-based recommendation and matrix factorization~, to deep learning based techniques.\nIn particular, deep learning show strong advantages in solving complex tasks and dealing with complex data, due to its capability to capture non-linear user-item relationships and deal with various types of data sources such as images and text.\nIt has thus been increasingly used in recommender systems.\nDeep learning-based recommender systems have limitations in capturing interest dynamics~ due to distribution shift, i.e., the training phase is based on an existing dataset which may not reflect real user preferences that undergo rapid change.\nIn contrast, deep reinforcement learning (DRL) aims to train an agent that can learn from interaction trajectories provided by the environment by combining the power of deep learning and reinforcement learning. Since an agent in DRL can actively learn from users' real-time feedback to infer dynamic user preferences,\nDRL is especially suitable for learning from interactions, such as human-robot collaboration; it has also driven significant advances in a range of interactive applications ranging from video games, Alpha Go to autonomous driving~.\nIn light of the significance and recent progresses in DRL for recommender sytsems, we aim to timely summaize and comment on DRL-based recommendation systems in this survey.\nA\nrecent survey on reinforcement learning based recommender systems  provides a general review about reinforcement learning in recommender systems without a sophsiticated investigation of the growing area of deep reinforcement learning. Our survey distinguishes itself in providing a systematic and comprehensive overview of existing methods in DRL-based recommender systems, along with a discussion of emerging topics, open issues, and future directions. This survey introduces researchers, practitioners and educators into this topic\nand fostering an understanding of the key techniques in the area.\nThe main contributions of this survey include the following:\n\\begin{itemize}\n    \\item We provide an up-to-date  comprehensive review of deep reinforcement learning in recommender systems, with state of the art techniques and pointers to core references. To the best of our knowledge, this the first comprehensive survey in deep reinforcement learning based recommender systems.\n    \\item We present a taxonomy of the literature of deep reinforcement learning in recommender systems. Along with the outlined taxonomy and literature overview, we discuss the benefits, drawbacks and give suggestions for future research directions. \n    \\item We shed light on emerging topics and open issues for DRL-based recommender systems. We also point out future directions that could be crucial for advancing DRL-based recommender systems.\n\\end{itemize}\nThe remainder of this survey is organized as follows: Section 2 provides an overview of recommender systems, DRL and their integration. Section 3 provides a literature review with a taxonomy and classification mechanism. Section 4 reviews emerging topics, and Section 5 points out open questions. Finally, Section 6 provides a few promising future directions for further advances in this domain.\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{route.pdf}\n    \\caption{Taxonomy of Deep Reinforcement Learning based Recommender Systems in this survey}\n    \\label{fig:overview}\n\\end{figure}", "cites": [8983, 5194], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The introduction synthesizes the evolution of recommendation techniques and highlights the transition to DRL by connecting it with the limitations of deep learning methods. It integrates two cited papers to position itself as a more focused and comprehensive survey compared to prior work. However, it lacks deeper critical evaluation of these papers and offers only basic abstraction by emphasizing the suitability of DRL for dynamic user preferences without broader conceptual generalization."}}
{"id": "e05ae3e8-b24d-431b-b181-250a9bf1fb50", "title": "Preliminaries of Deep Reinforcement Learning", "level": "subsection", "subsections": [], "parent_id": "a6f31a50-0b05-46c3-ac49-a2a06aea3f6f", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Background"], ["subsection", "Preliminaries of Deep Reinforcement Learning"]], "content": "The typical defining feature of DRL is to use the deep learning to approximate reinforcement learning's value function and solve high-dimensional Markov Decision Processes (MDPs)~. Formally, a MDP can be represented as a tuple ($\\mathcal{S},\\mathcal{A},\\mathcal{P},\\mathcal{R},\\gamma$). The agent chooses an action $a_t\\in\\mathcal{A}$ according to the policy $\\pi_t(s_t)$ at state $s_t\\in\\mathcal{S}$. The environment receives the action and produces a reward $r_{t+1}\\in\\mathcal{R}$ and transfers the reward into the next state $s_{t+1}$ according to the transition probability $P(s_{t+1}|s_t,a_t)\\in\\mathcal{P}$. The transition probability $\\mathcal{P}$ is unknown beforehand in DRL.\nSuch a process continues until the agent reaches the terminal state or exceeds a pre-defined maximum time step. The overall objective is to maximize the expected discounted cumulative reward,\n\\begin{align}\n    \\mathbb{E}_\\pi [r_t] = \\mathbb{E}_\\pi \\big[\\sum_{0}^\\infty \\gamma^k r_{t+k}\\big] \n\\end{align}\nwhere $\\gamma \\in [0,1]$ is the discount factor that balances the future reward and the immediate reward.\n\\begin{wrapfigure}{r}{0.45\\textwidth}\n      \\begin{center}\n        \\includegraphics[width=0.45\\textwidth]{tax.pdf}\n      \\end{center}\n    \\caption{Taxonomy of Deep Reinforcement Learning in Recommender Systems}\n    \\label{fig:tax}\n\\end{wrapfigure}\nDeep reinforcement learning can be divided into two categories: \\emph{model-based} and \\emph{model-free} methods (a detailed taxonomy can be found in \\Cref{fig:tax}). The major {\\emph difference} between the two is whether the agent can learn a model of the environment. Model-based methods aim to estimate the transition function and reward function, while model-free methods aim to estimate the value function or policy from experience. In model-based methods, the agent accesses the environment and plans ahead while model-free methods gain sample efficiency from using models which are more extensively developed and tested than model-based methods in recent literature~. \nDeep reinforcement learning approaches are divided into three streams: \\emph{value-based}, \\emph{policy-based} and \\emph{hybrid} methods. In value-based methods, the agent updates the value function to learn a policy; policy-based methods learn the policy directly; and hybrid methods combine value-based and policy-based methods called \\emph{actor-critic} methods. Actor-critic contains two different networks where an actor network uses a policy-based method and the critic uses a value-based method to evaluate the policy learned by the agent.\n\\begin{table}[ht]\n    \\centering\n    \\caption{Notations}\n    \\begin{tabular}{cc|ccc}\n        \\hline\n        Notations & Name & Notations & Name & Notes \\\\\\hline\n         $Q(\\cdot)$  &  Q-Value Function   &           $s$ &   State     &  users' preference     \\\\\n         $V(\\cdot)$  &  Value Function    &           $a$ &   Action      &  Recommended item(s)    \\\\\n         $\\gamma$ & Discount Factor      &           $\\pi$, $\\mu(\\cdot)$  & Policy  & Recommendation policy       \\\\\n          $\\mathbb{E}$ & Expected Value      &          $r(\\cdot,\\cdot)$ & Reward   &  users' click behavior    \\\\\n          $\\theta$ & Model Parameter      &           $\\alpha$ & constant $\\in [0,1]$      &   -    \\\\\n          $p(\\cdot)$ & Transition Probability    &  $\\tau$ & Sampled Trajectory      &  A tuple $(s_t,a_t,s'_t,r_t)$    \\\\\\hline\n    \\end{tabular}\n    \\label{tab:my_label}\n\\end{table}\nDeep reinforcement learning can be divided into \\emph{on-policy} and \\emph{off-policy} methods.\nIn off-policy methods,\nthe behavior policy $\\pi_b$ is used for exploration while the target policy $\\pi$ is used for decision-making.\nFor on-policy methods, the behavior policy is the same as the target policy. \n\\textbf{Q-learning}~ is an off-policy value-based learning scheme for finding a greedy target policy:\n\\begin{align}\n    \\pi(s) = \\argmax_a Q_\\pi (s,a)\n\\end{align}\nwhere $Q_u (s,a)$ denotes the $Q$-value and is used in a small discrete action space. For a deterministic policy, the $Q$ value can be calculated as follows\n\\begin{align}\n    Q (s_t,a_t) = \\mathbb{E}_{\\tau \\sim \\pi}[r(s_t,a_t) + \\gamma Q(s'_t, a'_t)].\n\\end{align}\nDeep Q learning (DQN)~ uses deep learning to approximate a non-liner Q function parameterized by $\\theta_q$: $Q_{\\theta_q} (s,a)$. DQN designs a network $Q_{\\theta_q}$ that is asynchronously updated by minimizing the MSE:\n\\begin{align}\n    \\mathcal{L}(\\theta_q) = \\mathbb{E}_{\\tau \\sim \\pi}\\Big[Q_{\\theta_q}(s_t,a_t)-(r(s_t,a_t) + \\gamma Q_{\\theta_q}(s'_{t},a'_{t}))\\Big]^2 \\label{dqnloss}\n\\end{align}\nwhere $\\tau$ is the sampled trajectory containing $(s,a,s',r(s,a))$. In particular, $s'_t$ and $a'_t$ come from the behavior policy $\\pi_b$ while $s,a$ comes from the target policy $\\pi$.\nIt is worth mentioning that the value function $V_\\pi(s)$ represents the expected return. $V_\\pi(s)$ is used to evaluate the goodness of the state while $Q_\\pi(s_t,a_t)$ is used to evaluate the action. $V_\\pi(s)$ can be defined as\n\\begin{align}\n    V_\\pi(s) = \\mathbb{E}_{\\tau\\sim \\pi}\\bigg[\\sum_{t=0}^T\\gamma^t r(s,a)|s_0=s\\bigg].\n\\end{align}\n$V_\\pi(\\cdot)$ and $Q_\\pi(\\cdot)$ have the following relationship:\n\\begin{align}\n    V_\\pi(s) = \\mathbb{E}_{a\\sim\\pi}[Q_\\pi(s,a)].\n\\end{align}\nThe value function is updated using the following rule with the Temporal Difference (TD) method,\n\\begin{align}\n    V_\\pi(s_t) \\leftarrow V_\\pi(s_t) + \\alpha[\\underbrace{r(s'_t,a'_t) + \\gamma V_\\pi(s'_t) - V_\\pi(s_t)}_{\\text{TD-error}}]\n\\end{align}\nwhere $\\alpha$ is a constant.\n\\textbf{Policy gradient}~ is an on-policy policy-based method which can handle high-dimensional or continuous actions which cannot be easily handled by Q-learning. Policy gradient\naims to find the parameter $\\theta$ of $\\pi_{\\theta}$ to maximize the accumulated reward. To this end, it maximizes the expected return from the start state:\n\\begin{align}\n    J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[r(\\tau)] = \\int\\pi_{\\theta}(\\tau) r(\\tau)d\\tau\n\\end{align}\nwhere $\\pi_{\\theta}(\\tau)$ is the probability of the occurrence of $\\tau$. Policy gradient learns the parameter $\\theta$ by the gradient $\\nabla_\\theta J(\\pi_\\theta)$ as defined below:\n\\begin{align}\n    \\nabla_\\theta J(\\pi_\\theta) = \\int\\pi_{\\theta}(\\tau) r(\\tau)d\\tau &=  \\int\\pi_{\\theta}(\\tau) \\nabla_{\\theta}\\log \\pi_{\\theta}(\\tau)r(\\tau)d\\tau \\notag \\\\\n    & =\\mathbb{E}_{\\tau \\sim d_{\\pi_\\theta}}[\\sum_{t=1}^Tr(s_t,a_t)\\sum_{t=1}^T\\nabla_{\\theta} \\log \\pi_{\\theta}(s_t,a_t)].\n\\end{align}\nThe above derivations contain the following substitution,\n\\begin{align}\n    \\pi_{\\theta}(\\tau) = p(s_1)\\prod_{t=1}^T \\pi_{\\theta}(s_t,a_t)p(s_{t+1}|s_t,a_t)\n\\end{align}\nwhere $p(\\cdot)$ are independent from the policy parameter $\\theta$, which is omitted during the derivation.\nMonte-Carlo sampling has been used by previous policy gradient algorithm (e.g,. REINFORCE) for $\\tau\\sim d_{\\pi_{\\theta}}$.\n\\textbf{Actor-critic networks} combine the advantages from Q-learning and policy gradient. They can be either on-policy~ or off-policy~. An actor-critic network consists of two components: i) \\textit{an actor}, which optimizes the policy $\\pi_\\theta$ under the guidance of $\\nabla_\\theta J(\\pi_{\\theta})$; and ii) \\textit{a critic}, which evaluates the learned policy $\\pi_\\theta$ by using $Q_{\\theta_q} (s,a)$. The overall gradient is represented as follows:\n\\begin{align}\n    \\mathbb{E}_{s \\sim d_{\\pi_\\theta}}[Q_{\\theta_q}(s,a)\\nabla_{\\theta} \\log \\pi_{\\theta}(s,a)].\n\\end{align}\nWhen dealing with off-policy learning, the value function for $\\pi_\\theta (a|s)$ can be further determined by deterministic policy gradient (DPG) as shown below:\n\\begin{align}\n    \\mathbb{E}_{s \\sim d_{\\pi_\\theta}}[\\nabla_a Q_{\\theta_q}(s,a)|_{a=\\pi_\\theta(s)}\\nabla_{\\theta} \\pi_{\\theta}(s,a)].\n\\end{align}\nWhile traditional policy gradient calculates the integral for both the state space $\\mathcal{S}$ and the action space $\\mathcal{A}$, DPG only requires computing the integral to the state space $\\mathcal{S}$.\nGiven a state $s\\in\\mathcal{S}$, there will be only one corresponding action $a\\in\\mathcal{A}:\\mu_\\theta(s) = a$ using DPG. Specifically, deep Deterministic Policy Gradients (DDPG) is an algorithm that combines techniques from DQN and DPG. DDPG contains four different neural networks: Q Network $Q$, policy network, target Q network $Q^{\\mathit{tar}}$, and target policy network. It uses the target network for both the Q Network $Q$ and policy network $\\mu$ to ensure stability during training. Assume $\\theta_q, \\theta_{\\pi}, \\theta_{q'}$ and $\\theta_{\\pi'}$ are parameters of the above networks; then DDPG soft-updates the parameters for the target network~:\n\\begin{align}\n    \\text{Actor: }\\theta_{\\pi'} \\leftarrow \\alpha \\theta_\\pi + (1-\\alpha)\\theta_{\\pi'}  \\text{ Critic: }\\theta_{q'} \\leftarrow \\alpha \\theta_q + (1-\\alpha)\\theta_{q'} \n\\end{align}", "cites": [8105], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear and factual overview of DRL concepts, including definitions of MDPs, value and policy functions, and distinctions between model-based and model-free, on-policy and off-policy approaches. While it references one cited paper (Off-Policy Actor-Critic), it does so only briefly and without substantial synthesis or critical evaluation of the broader literature. The inclusion of equations and a taxonomy figure enhances clarity, but the section lacks deeper comparative or evaluative analysis to highlight trends or limitations in the field."}}
{"id": "4ebfecfc-b15f-4ce1-abac-7b121665be4a", "title": "Model-based Deep Reinforcement Learning based Methods", "level": "subsection", "subsections": ["8ceb9826-a858-407a-9b78-7a83d0ea8871"], "parent_id": "696bfdf7-6191-445e-82a8-ae4df2378853", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Deep Reinforcement Learning in Recommender Systems"], ["subsection", "Model-based Deep Reinforcement Learning based Methods"]], "content": "Model-based methods assume an expected reward or action available for the next step to help the agent update the policy.\n\\begin{table}[ht]\n\\caption{List of publications in model-based DRL-based RS}\n\\begin{tabular}{ccc}\n\\hline\nMethod  & Work \\\\\\hline\nValue-based &   \\\\\nPolicy-based &  \\\\\nHybrid &  \n\\\\\\hline\n\\end{tabular}\n\\end{table}", "cites": [5189, 5180, 5195], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.3, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a minimal synthesis by merely listing a category of methods (model-based DRL) and referencing three papers without connecting their contributions. There is no critical analysis or abstraction; the content does not evaluate the approaches, identify limitations, or generalize to broader trends or principles."}}
{"id": "8ceb9826-a858-407a-9b78-7a83d0ea8871", "title": "Policy-based methods", "level": "paragraph", "subsections": ["bb6b42ee-6de2-432f-ab2a-fc4b096f3199", "68abbde2-1fcc-48e8-a5f8-eccafeb14c6f", "326848cf-b30c-48e3-8c0b-494e313de16b"], "parent_id": "4ebfecfc-b15f-4ce1-abac-7b121665be4a", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Deep Reinforcement Learning in Recommender Systems"], ["subsection", "Model-based Deep Reinforcement Learning based Methods"], ["paragraph", "Policy-based methods"]], "content": "IRecGAN~ is a model-based method that adopts generative adversarial training to improve the robustness of policy learning. It can reduce the cost of interaction for RS by using offline data instead of the simulated environment.\nIRecGAN employs a generative adversarial network~ to generate user data based on the offline dataset.\nIt trains a recommendation agent using a policy gradient-based DRL method called REINFORCE. The agent aims to learn a policy based on the following gradient,\n\\begin{align}\n    \\mathbb{E}_{\\tau\\sim\\{g,data\\}}\\big[\\sum_{t=0}^T \\sum_{t'=t}^T\\gamma^{t'-t} q_D(\\tau_{0:t}^n)r_t  \\nabla_{\\theta_a}(c_t \\in \\pi_{\\theta_a}(s_t)\\big],q_D(\\tau_{0:t}^n)= \\frac{1}{N}\\sum_{n=1}^N D(\\tau_{0:T}^n),\\tau_{0:T}^n\\in MC^{\\mathcal{U}}(N)\n\\end{align}\nwhere the $\\mathit{MC}^{\\mathcal{U}}(N)$ represents the sampled $N$ sequences from the interaction between $\\mathcal{U}$ and the agent using the Monte-Carlo tree search algorithm, $D$ is the discriminator, $T$ is the length of $\\tau$, $g$ represents the offline data, and \\textit{data} represents the generated data. \n propose NRSS for personalized music recommendation. NRSS uses wireless sensing data to learn users' current preferences. NRSS considers three different types of feedback: score, option, and wireless sensing data. Because multiple factors are considered as the reward, NRSS designs a reward model which consists of users' preference reward $r_p$ and a novel transition reward $r_{\\textit{trans}}$ which are parameterized by $\\theta_{r_p}$ and $\\theta_{r_{\\textit{trans}}}$. The goal for NRSS is to find the optimal parameters $\\theta_{r_p}$ and $\\theta_{r_{\\textit{trans}}}$ by using the Monte-Carlo tree search thus improving recommendation performance.\nHowever, wireless sensing feedback lacks generalization ability as it is only available for certain tasks or scenarios, making it hard to determine dynamic user interest.", "cites": [7217], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section introduces and explains two methods, IRecGAN and NRSS, and ties their use of Monte-Carlo tree search and reward modeling to the broader context of policy-based DRL in recommenders. It includes a mild critique of wireless sensing feedback's generalization limitations. However, synthesis is limited to a few specific systems, and there is minimal abstraction or comparison of broader trends or principles."}}
{"id": "bb6b42ee-6de2-432f-ab2a-fc4b096f3199", "title": "Value-based methods", "level": "paragraph", "subsections": [], "parent_id": "8ceb9826-a858-407a-9b78-7a83d0ea8871", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Deep Reinforcement Learning in Recommender Systems"], ["subsection", "Model-based Deep Reinforcement Learning based Methods"], ["paragraph", "Policy-based methods"], ["paragraph", "Value-based methods"]], "content": "Prior to Q-learning, value iteration is a more traditional value-based reinforcement learning algorithm that focuses on the iteration of the value function. Gradient Value Iteration (GVI)~ is proposed to improve the traditional value iteration algorithm by utilizing the transition probability and a multi-agent setting to predict chronological author collaborations. It introduces a new parameter named `status' to reflect the amount of knowledge that the agent needs to learn from this state. The policy is updated only when the distance between the new status and the old status is lower than a pre-defined threshold. However, value iteration requires the transition probability, which is hard to obtain in most cases. Hence, Q-learning and its variants are widely used in DRL-based RS. Cascading DQN (CDQN) with a generative user model~ is proposed to deal with the environment with unknown reward and environment dynamics. The generative user model adopts GANs to generate a user model based on an offline dataset. Different from previous work, it will generate the reward function for each user to explain the users' behavior. The user model can be written as,\n\\begin{align}\n    \\argmax_{\\phi\\in\\triangle^{k-1}} \\mathbb{E}_{\\phi}[r(s_t,a_t)]-R(\\phi)/\\eta\n\\end{align}\nwhere $\\triangle^{k-1}$ is the probability simplex, $R(\\phi)$ is the regularization term for exploration and $\\eta$ is a constant.\nPseudo Dyna-Q (PDQ)~ points out that Monte-Carlo tree search may lead to an extremely large action space and an unbounded importance weight of training samples. Hence, a world model is proposed to reduce the instability of convergence and high computation cost for interacting with users by imitating the offline dataset. With the world model, the agent will interact with the learned world model instead of the environment to improve the sample efficiency and convergence stability. The world model learning process introduced in PDQ can be described as finding the parameter $\\theta_M$,\n\\begin{align}\n    \\argmin_{\\theta_M}\\mathbb{E}_{\\xi\\in P_{\\xi}^\\pi}[\\sum_{t}^{T-1}\\gamma^t\\prod_{j=0}^t\\frac{\\pi(s_j,a_j)}{\\pi_b(s_j,a_j)}\\Delta_t(\\theta_M)]\n\\end{align}\nwhere $\\xi$ is generated by the logged policy $\\pi_b$, $\\prod_{j=0}^t\\frac{\\pi(s_j,a_j)}{\\pi_b(s_j,a_j)}$ is the ratio used for importance sampling and $\\Delta$ is the difference between the reward in the world model and real reward. \nFurthermore, GoalRec~ designs a disentangled universal value function to be integrated with the world model to help the agent deal with different recommendation tasks. The universal value function is defined as\n\\begin{align}\n    V_{\\pi} (s)= \\mathbb{E}[\\sum_{t=0}^\\infty r(s_t,a_t)\\prod_{k=0}^t\\gamma s_k|s_0=s].\n\\end{align}\nMoreover, GoalRec introduces a new variable goal $g\\in G$ used to represent users' future trajectory and measurement $m\\in M$. $m$ is an indicator that reflects users' response to the given future trajectory based on historical behaviors. Based on that, the optimal action will be selected based on\n\\begin{align}\n    a^* = \\max_a U(M(s,a),g)\n\\end{align}\nwith a customized liner function $U(\\cdot)$.\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{GoalRec_wb.pdf}\n    \\caption{Left is the general structure of model-free methods. Right is the structure for GoalRec which is a model-based method. A sample trajectory is used to demonstrate the difference between them~. }\n    \\label{fig:worldmodel}\n\\end{figure}", "cites": [5195, 5180], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the key ideas from the cited papers, connecting value-based methods like Gradient Value Iteration, Cascading DQN, and GoalRec through their shared focus on model-based reinforcement learning and user behavior modeling. It provides a critical perspective by noting limitations such as the need for transition probabilities and high computational costs. The abstraction is strong as it generalizes the concept of world models and universal value functions in the context of recommendation tasks."}}
{"id": "68abbde2-1fcc-48e8-a5f8-eccafeb14c6f", "title": "Hybrid methods", "level": "paragraph", "subsections": [], "parent_id": "8ceb9826-a858-407a-9b78-7a83d0ea8871", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Deep Reinforcement Learning in Recommender Systems"], ["subsection", "Model-based Deep Reinforcement Learning based Methods"], ["paragraph", "Policy-based methods"], ["paragraph", "Hybrid methods"]], "content": "Hybrid method can be recognized as a midpoint between value-based and policy gradient-based methods. DeepChain~ uses the multi-agent setting to relieve the sub-optimality problem. The sub-optimality problem is caused by the \\textit{one for all} setting that optimizes one policy for all users. Hence, DeepChain designs a multi-agent setting that adopts several agents to learn consecutive scenarios and jointly optimizes multiple recommendation policies. The main training algorithm used is DDPG. To this end, users' actions can be formulated in a model-based form as follows:\n\\begin{align}\n    \\sum_{m,d}[p_m^s (s_t,a_t)\\gamma Q_{\\theta}(s'_{t},\\pi_m(s'_{t})) + p_m^c(s_t,a_t)(r_t+\\gamma Q_{\\theta}(s'_{t},\\pi_d(s'_{t}))) + p_m^l (s_t,a_t)r_t]1_m\n\\end{align}\nwhere $m$ represents the number of actor networks, $c,l,s$ represent the three different scenarios, $1_m$ is used to control the activation of two actors and $(m,d)\\in\\{(1,2),(2,1)\\}$.", "cites": [5189], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section describes a specific hybrid method (DeepChain) and its approach to addressing the sub-optimality problem using a multi-agent setting. However, it primarily presents the method in a descriptive manner without substantial synthesis of ideas from multiple sources or broader abstraction. There is minimal critical evaluation of the workâ€™s limitations or implications."}}
{"id": "326848cf-b30c-48e3-8c0b-494e313de16b", "title": "Discussion", "level": "paragraph", "subsections": [], "parent_id": "8ceb9826-a858-407a-9b78-7a83d0ea8871", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Deep Reinforcement Learning in Recommender Systems"], ["subsection", "Model-based Deep Reinforcement Learning based Methods"], ["paragraph", "Policy-based methods"], ["paragraph", "Discussion"]], "content": "Model-based methods aim to learn a model or representation to represent the whole environment so that the agent can plan ahead and receive better sample efficiency.\nThe drawback of such a method is that the ground-truth representation of the environment is unavailable in recommendation scenarios as it dynamically changes, leading to a biased representation. Moreover, model-based methods use the transition probability function $\\mathcal{P}$ to estimate the optimal policy. As mentioned, the transition probability function is normally equivalent to users' behavior probability which is hard to determine in a recommender system. Hence, existing works~ approximate $\\mathcal{P}$ using a neural network or embedding it into the world model.  design a probability network to estimate $\\mathcal{P}$ while  uses a GAN to generate user behavior where $\\mathcal{P}$ is embedded in the latent space. Different from them,~ relies on the world model to predict users' next behavior and feed it into the policy learning process.\nThe challenges of model-based DRL are not widely used in RS and can be summarized into the following facets:\n\\begin{itemize}\n\\item $\\mathcal{P}$ is hard to determine in real-world recommender systems. \n\\item If approximation is used to estimate $\\mathcal{P}$, the overall model complexity will substantially increase as it requires approximating two different functions $\\mathcal{P}$ and the recommendation policy $\\pi$ by using a large amount of user behavior data. \n\\item World model-based methods require periodic re-training to ensure the model can reflect user interests in time which increases the computation cost. \n\\end{itemize}\n\\begin{figure}\n    \\centering\n    \\begin{subfigure}{0.48\\linewidth}\n        \\includegraphics[width=\\linewidth]{DQN_wb.pdf}\n        \\caption{DQN}\n    \\end{subfigure}\n    \\begin{subfigure}{0.48\\linewidth}\n        \\includegraphics[width=\\linewidth]{DDPG_wb.pdf}\n        \\caption{DDPG}\n    \\end{subfigure}\n    \\caption{The typical structure of DQN and DDPG}\n    \\label{fig:ddpg_fig}\n\\end{figure}", "cites": [5189, 5180, 5195], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the three cited papers by connecting their approaches to modeling the environment and estimating the transition probability function. It offers a critical perspective by highlighting the limitations of model-based DRL in recommendation systems, such as the difficulty in determining the transition function and increased model complexity. While it identifies broader challenges, it does not abstract these into a high-level conceptual or theoretical framework, limiting the abstraction score."}}
{"id": "74b47d11-d3b5-407c-b802-84dc050162e4", "title": "Model-free deep reinforcement learning based methods", "level": "subsection", "subsections": ["6c84bc7f-b9cc-44a6-b8d6-5c59f102ecfe"], "parent_id": "696bfdf7-6191-445e-82a8-ae4df2378853", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Deep Reinforcement Learning in Recommender Systems"], ["subsection", "Model-free deep reinforcement learning based methods"]], "content": "Compared with model-based methods, model-free methods are relatively well-studied. Different from model-based methods, $\\mathcal{P}$ is unknown and not required in model-free methods. Model-based methods enable the agent to learn from previous experiences. In this subsection, we categorize model-free based DRL in RS into three parts: value-based, policy-based and hybrid methods. \n\\begin{table}[ht]\n\\caption{List of reviewed publications in Model-free DRL-based RS}\n\\begin{tabular}{ccc}\n\\hline\nTasks & Note & Work \\\\\\hline\n\\multirow{4}{*}{Value-based} & Vanilla DQN and its extensions & \\\\ \\\n& DQN with state/action space optimization &  \\\\\n& DQN with graph/image input &  \\\\\n& DQN for joint learning & \\\\\\hline\n\\multirow{3}{*}{Policy-based} & Vanilla REINFORCE &  \\\\\n& REINFORCE uses graph structure/input & \\\\\n& Non-REINFORCE based &  \\\\ \\hline\n\\multirow{2}{*}{Hybrid} & Vanilla DDPG &  \\\\\n& with Knowledge Graph & \\\\\n\\hline\n\\end{tabular}\n\\end{table}", "cites": [5190, 3604, 8884, 5165, 6153, 3616, 1389, 5184, 5191, 5177, 5169, 5199, 6154, 5181, 6152, 5172, 5194, 5170, 5185], "cite_extract_rate": 0.4318181818181818, "origin_cites_number": 44, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic categorization of model-free DRL methods in recommender systems into value-based, policy-based, and hybrid approaches, and lists associated papers under each category. However, it lacks meaningful synthesis of the cited works, offering only a surface-level grouping without connecting ideas or themes across papers. There is minimal critical analysis or abstraction beyond the specific systems described."}}
{"id": "6c84bc7f-b9cc-44a6-b8d6-5c59f102ecfe", "title": "Value based methods", "level": "paragraph", "subsections": ["01225554-85b2-42e4-a661-722c3b5972d7", "cb915b6e-26a1-44b9-be2e-47ffa839e199", "47d0fb4b-4dca-4006-ba90-90530959ef38"], "parent_id": "74b47d11-d3b5-407c-b802-84dc050162e4", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Deep Reinforcement Learning in Recommender Systems"], ["subsection", "Model-free deep reinforcement learning based methods"], ["paragraph", "Value based methods"]], "content": "As mentioned, Deep Q-learning and its variants are typical value-based DRL methods widely used in DRL-based RS. DRN~ is the first work utilizing Deep Q-Networks (DQN) in RS. It adopts Double DQN (DDQN)~ to build a user profile and designs an activeness score to reflect how frequently a user returns after one recommendation plus users' action (click or not) as the reward. DRN provides a new approach to integrating DRL into RS when dealing with a dynamic environment. The key objective function can be found as follows,\n\\begin{align}\n    \\mathbb{E}[r_{t+1} + \\gamma Q_{\\theta_t'}(s_{t+1}, \\argmax_{a'} Q_{\\theta_t}(s_t,a'))]\n\\end{align}\nwhere $a'$ is the action that gives the maximum future reward according to $\\theta_t$, $\\theta_t$ and $\\theta_t'$ are different parameters for two different DQNs. \n points out that negative feedback will also affect recommendation performance which DRN does not consider. Moreover, positive feedback is sparse due to the large number of candidate items in RS. Only using positive feedback would lead to convergence problems. Hence, DEERS is proposed to consider both positive and negative feedback simultaneously by using DQN. Gated Recurrent Units (GRU) are employed to capture users' preferences for both a positive state $s^+$ and negative state $s^-$ and the final objective function can be computed as:\n\\begin{align}\n    \\mathbb{E}[r_{t+1} + \\gamma \\max_{a_{t+1}}Q_{\\theta_q}(s^+_{t+1},s^-_{t+1},a_{t+1})|s^+_t,s^-_t,a_t].\n\\end{align}\n introduces attention mechanisms into the DQN to leverage social influence among users. To be specific, a social impact representation $U_v$ is introduced into the state representation. Matrix factorization is adopted to determine \nsimilarity among\nusers and hence present the social influence. Social attention is introduced to distill the final state representation.\nIn addition, a few studies focus on user profiling to improve recommendation performance~.  claims that user feedback contains useful information in the previous feedback even when the user does not like the recommended items. Some existing studies focus on final feedback which ignore the importance from earlier steps to later ones. Hence, user-specific DQN (UQDN) is proposed to consider multi-step feedback from users. It employs Matrix Factorization to generate user-specific latent state spaces. The newly defined objective function with the user-specific latent state space can be represented as\n\\begin{align}\n    \\mathbb{E}[r_{t+1} + \\gamma \\max_{a_{t+1}}\\overline{Q}_{\\theta_q}(s_{t+1},a_{t+1}) + \\overline{\\textbf{b}}_u - Q_{\\theta_q} (s_{t+1},a_{t+1})]\n\\end{align}\nwhere $\\overline{\\textbf{b}}_u$ is the learned user latent representation.\n also points out that most studies do not consider users' long-term engagement in the state representation as they focus on the immediate reward. FeedRec is proposed that combines both instant feedback and delayed feedback into the model to represent the long-term reward and optimize the long-term engagement by using DQN. To be specific, time-LSTM is employed to track users' hierarchical behavior over time to represent the delayed feedback which contains three different operations: $h_{\\mathit{skip}},h_{\\mathit{choose}},h_{\\mathit{order}}$. The state space is the concatenation of those operations and users' latent representation.  Differently,  focuses on the user privacy issue in recommender systems. Deep user profile perturbation (DUPP) is proposed to add perturbation into the user profile by using DQN during the recommendation process. Specifically, DUPP adds a perturbation vector into users' clicked items as well as the state space, which contains users' previous behavior.\nDistinct from previous studies which focus on optimizing user profiles or state spaces, some studies aim to optimize the action space formed by interactions with items. In the situation of basket recommendation, the user is suggested multiple items as a bundle, which is called a recommendation slate. It leads to combinatorially large action spaces making it intractable for DQN based recommendation models.\nSlateQ~ is proposed to decompose slate Q-value to estimate a long-term value for individual items, and it is represented as,\n\\begin{align}\n    Q_{\\theta_q}(s_t,a_t)  = \\sum_{i \\in a_t}p(i|s_t,a_t)\\overline{Q}_{\\theta_q}(s_t,i)\n\\end{align}\nwhere $\\overline{Q}_{\\theta}(s,i)$ is the decomposed Q-value for item $i$. The decomposed Q-value will be updated by the following rule which is similar to traditional DQN,\n\\begin{align}\n    \\overline{Q}_{\\theta_q}(s_t,i) \\leftarrow \\alpha \\bigg(r_t +\\gamma \\sum_{j\\in a_{t+1}}p(j|s_{t+1},a_{t+1}) \\overline{Q}_{\\theta_q}(s_{t+1},j)\\bigg) + (1-\\alpha)\\overline{Q}_{\\theta_q}(s_t,i).\n\\end{align}\nDifferent from other mode-free methods, Slate-Q assumes that the transition probability $p(i|s_t,a_t)$ is known.\nVanilla DQN methods may not have sufficient knowledge to handle complex data such as images and graphs.  firstly models users' click behavior as an embedding matrix in the latent space to include the skip behaviors of sequence patterns for sequential recommendation. Based on that,  propose DRCGR, which adopts CNN and GAN into DQN to help the agent to better understand high-dimensional data, e.g., a matrix. Two different convolution kernels are used to capture users' positive feedback. In the meantime, DRCGR uses GANs to learn a negative feedback representation to improve robustness. \nAnother typical data format is the graph, which is widely used in RS, including knowledge graphs.\n propose GCQN which adopts Graph Convolutional Networks (GCN)~ into the DQN which constructs the state and action space as a graph-aware representation. Differently, GCQN introduces the attention aggregator: $\\sum_{w\\in \\mathcal{N}(i)}\\alpha_{iu}e_u$ which demonstrates better performance than the mean-aggregator and pooling-aggregator. For item $i$, the graph-aware representation can be represented as,\n\\begin{align}\n    \\sigma\\bigg(W_{fc}[e_i\\oplus\\sum_{w\\in \\mathcal{N}(i)}\\alpha_{iu}e_u + b_{fc}]\\bigg)\n\\end{align}\nwhere $W_{fc},b_{fc}$ are the parameters for the fully-connected layer, $e_u$ is the embedding for user $u$ and $\\mathcal{N}(i)$ is the set of one-hot neighbours of item $i$ in graph $G(i)$.  propose KGQR uses a similar strategy to transform the information into a knowledge graph which is fed into the GCN to generate the state representation. Notably, KGQR presents a different state representation generation method. For given node $i$, the neighbourhood representation with a $k$-hop neighborhood aggregator can be represented as,\n\\begin{align}\n    e_i^k = \\sigma\\bigg(W_{k}\\frac{1}{|\\mathcal{N}(i)|}\\sum_{t\\in\\mathcal{N}(i)} e_{t}^{k-1} + B_k e_i^{k-1}\\bigg)\n\\end{align}\nwhere $\\mathcal{N}(i)$ is the set of neighboring nodes, $W_k, B_k$ are the parameter of the aggregator. Those neighbourhood representations will be fed into a GRU and the state representation will be generated. Another application domain for using graph data is job recommendation which requires considering multiple factors jointly such as salary, job description, job location etc. SRDQN~ constructs a probability graph to represent a candidate's skill set and  employs a multiple-task DQN structure to process these different factors concurrently. \nThere are some studies targeting recommendation and advertising simultaneously in e-commerce environments~.  mentions when deploying RS into real-world platforms such as e-commerce scenarios, the expectation is to improve the profit of the system.\nA new metric, Gross Merchandise Volume (GMV), is proposed to measure the profitability of the RS to provide a new view about evaluating RS in advertising. Different from GMV,  separates recommendation and advertising as two different tasks and proposes the Rec/Ads\nMixed display (RAM) framework. RAM designs two agents: a recommendation agent and an advertising agent, where each agent employs a CDQN to conduct the corresponding task.  find that advertising and recommendation may harm each other and formulate a rec/ads trade-off. Their proposed solution, DEARS, contains two RNNs. Two RNNs are employed to capture user preferences toward recommendations and ads separately. Based on that, DQN is employed to take those two outputs as the input to construct the state and output the advertising.", "cites": [5181, 5191, 5170, 6155, 5185, 5199, 1408], "cite_extract_rate": 0.4117647058823529, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "high", "analysis": "The section synthesizes multiple value-based DRL methods in recommender systems and integrates them into a coherent narrative by highlighting their design choices, such as handling positive/negative feedback, user profiling, and action space optimization. It shows critical analysis by pointing out limitations (e.g., ignoring negative feedback, sparse positive feedback, privacy issues) and explaining how later works address these problems. While it generalizes some trends (e.g., the need to model long-term engagement), it remains focused on specific methodological adaptations rather than broader theoretical or conceptual abstractions."}}
{"id": "01225554-85b2-42e4-a661-722c3b5972d7", "title": "Policy-based methods", "level": "paragraph", "subsections": [], "parent_id": "6c84bc7f-b9cc-44a6-b8d6-5c59f102ecfe", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Deep Reinforcement Learning in Recommender Systems"], ["subsection", "Model-free deep reinforcement learning based methods"], ["paragraph", "Value based methods"], ["paragraph", "Policy-based methods"]], "content": "Policy-based DRL can be divided into two parts which are Constrained Policy Optimization (CPO) ~ and policy gradient.\n uses CPO to identify the contradiction between text feedback and historical preferences. It provides a solution for using DRL in the situation where users' feedback is entirely different from previous feedback in RS. \nPolicy gradient-based methods are the other stream in policy-based DRL methods for RS. These methods aims to optimize the policy $\\pi$ directly instead of estimating the Q-value like DQN. A well-known and widely used policy gradient method in RS is REINFORCE which uses the following rule for policy $\\pi_{\\theta_\\pi}$,\n\\begin{align}\n    \\theta \\leftarrow \\theta + \\alpha \\mathbb{E}_{\\tau\\sim d_{\\pi_{\\theta_\\pi}}}\\bigg[\\sum_{t=1}^Tr(s_t^i,a_t^i)\\sum_{t=1}^T\\nabla_{\\theta_\\pi} \\log \\pi_{\\theta_\\pi}(s_t^i,a_t^i)\\bigg]\n\\end{align}\nwhere $i$ is sampled trajectories from $\\pi_{\\theta}(a_t|s_t)$.  propose Policy Gradient for Contextual Recommendation (PGCR), which adopts REINFORCE and considers contextual information. PGCR assumes that the policy follows the multinoulli distribution, in which case the transition probability can be estimated easily through sampling from previously seen context.\n incorporate CNNs and attention mechanisms in REINFORCE for explainable recommendation. Specifically, this work designs a coupled agent structure where one agent generates the explanation and the other makes recommendations based on the generated explanation.\n increases the scalability of REINFORCE to ensure it can deal with the extremely large action space under recommendation scenarios. To be specific, it introduces a policy correction gradient estimator into REINFORCE to reduce the variance of each gradient by doing importance sampling. The new update rule\nbecomes\n\\begin{align}\n    \\theta_\\pi \\leftarrow \\theta_\\pi + \\alpha \\sum_{\\tau\\sim\\beta}\\bigg[\\sum_{t=1}^T\\frac{\\pi_{\\theta_\\pi}(s_t,a_t)}{\\pi_\\beta(s_t,a_t)}r(s_t^i,a_t^i)\\sum_{t=1}^T\\nabla_{\\theta_\\pi} \\log \\pi_{\\theta_\\pi}(s_t^i,a_t^i)\\bigg]\n\\end{align}\nwhere $\\pi_\\beta$ is the behavior policy trained by state-action pairs without the long-term reward and $\\pi_{\\theta}$ is trained based on the long-term reward only. It is worth mentioning that the vanilla REINFORCE algorithm is on-policy, and importance sampling will make REINFORCE behave like an off-policy method with the following gradient format,\n\\begin{align}\n    \\mathbb{E}_{\\tau \\sim d_{\\pi_\\theta}}\\bigg[\\prod_{t'=1}^t\\frac{\\pi_{\\theta}(s_t,a_t)}{\\pi_{\\theta_s}(s_{t'},a_{t'})}\\sum_{t'=t}^Tr(s_t,a_t)\\sum_{t=1}^T\\nabla_{\\theta} \\log \\pi_{\\theta}(s_t,a_t)\\bigg]\n\\end{align}\nwhere $\\pi_{\\theta_s}$ is the sample policy parameter.\n also finds that the REINFORCE method suffers from a high variance gradient problem and Pairwise Policy Gradient (PPG) is proposed. Different from policy correction, PPG uses Monte Carlo sampling to sample two different actions $a,b$ and compare the gradient to update $\\theta$,\n\\begin{align}\n    \\mathbb{E}_{\\tau\\sim d_{\\pi_{\\theta_\\pi}}}\\bigg(\\sum_a\\sum_b(r(s,a)-r(s,b))\\sum_{t=1}^T(\\nabla_{\\theta_\\pi}\\log_{\\pi_{\\theta_\\pi}}(s_t,a_t)-\\nabla_{\\theta_\\pi}\\log_{\\pi_{\\theta_\\pi}}(s_t,b_t))\\bigg).\n\\end{align}\n extends the policy correction gradient estimator into a two-stage setting which are $p(s_t,a^p)$ and $q(s_t,a|a^p)$ and the policy can be written as\n\\begin{align}\n    \\sum_{a^p}p(s_t,a^p)q(s_t,a|a^p).\n\\end{align}\nIn addition, weight capping and self-normalized importance sampling are used to further reduce the variance.\nMoreover, a large state space and action space will cause  sample inefficiency problems as REINFORCE relies on the current sampled trajectories $\\tau$.  finds that the auxiliary loss can help improve the sample efficiency\n~. Specifically, a linear projection is applied to the state $s_t$, the output is combined with action $a_t$ to calculate the auxiliary loss and appended into the final overall objective function for optimization.\nAnother prototype of vanilla policy gradient in DRL-based RS is the policy network.  designs a policy network to extract features and represent the relevant feedback that can help the agent make a decision. Similar to DQN, this work uses a neural network to approximate the Q-value and the policy directly without theoretical analysis.\n extend the policy network by introducing spatio-temporal feature fusion to help the agent understand complex features. Specifically, it considers both the current number and the future number of vacant taxis on the route to recommend routes for taxis.  introduces multi-modal data as new features to conduct vision-language recommendation by using historical data to train REINFORCE. ResNet and attention are used to encode vision and text information, respectively. Moreover, two rewards are introduced with a customized ratio $\\lambda$ to balance vision and text information. \nKnowledge Graphs (KG) are widely used in RS to enrich side information, provide explainability and improve recommendation performance. Similar to DQN, vanilla REINFORCE cannot properly handle graph-like data.  propose a method named Knowledge-guided Reinforcement Learning (KERL), which integrates  knowledge graphs into the REINFORCE algorithm. To be specific, KERL adopts TransE~ to transfer the knowledge graph into a graph embedding and utilizes a multilayer perceptron (MLP) to predict future knowledge of user preferences. The state representation can be written as\n\\begin{align}\n    h_t \\oplus \\mathit{TransE}(\\mathcal{G}) \\oplus \\mathit{MLP}(\\mathit{TransE}(\\mathcal{G}))\n\\end{align}\nwhere $h_t$ is the hidden representation from the GRU for sequential behavior and $\\mathcal{G}$ is the knowledge graph.\nDifferent from KERL,  propose Policy-Guided Path Reasoning (PGPR), which formulates the whole environment as a knowledge graph. The agent is trained to find the policy to find good items conditioned on the starting user in the KG by using REINFORCE. PGPR uses the tuple $(u,e_t,h_t)$ to represent the state instead of the graph embedding where $e_t$ is the entity the agent has reached at $t$ for user $u$ and $h_t$ is the previous action before $t$.\nThe action in PGPR is defined as the prediction of all outgoing edges for $e_t$ based on $h_t$. \n propose a knowledge graph policy network (KGPolicy) which puts the KG into the policy network and adopts REINFORCE to optimize it. In addition, KGPolicy uses negative sampling instead of stochastic sampling to overcome the false negative issue---sampled items behave differently during training and inference. Similar to GCQN, attention is also employed to establish the representation for its neighbors. \nDue to the on-policy nature of REINFORCE, it is difficult to apply it to large-scale RS as the convergence speed will be a key issue. To relieve this,  propose TPGR, which designs a tree-structured policy gradient method to handle the large discrete action space hierarchically. TPGR uses balanced hierarchical clustering to construct a clustering tree. Specifically, it splits a large-scale data into several levels and maintains multiple policy networks for each level to conduct the recommendation. The results are integrated at the final stage.\nAs mentioned, policy gradient can be further extended to deterministic policy gradient (DPG) .  propose Deterministic Policy Gradient with Full Backup\nEstimation (DPG-FBE) to complete a sub-task of recommendation. DPG-FBE considers a search session MDP (SSMDP) that contains a limited number of samples, where the stochastic policy gradient method like REINFORCE cannot work well.", "cites": [5190, 5165, 6153, 5184, 5177, 1911, 6152, 6156], "cite_extract_rate": 0.4, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 3.8}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by integrating and connecting multiple works on policy-based DRL methods in RS, highlighting variations in REINFORCE (e.g., policy correction, PPG, auxiliary loss). It provides some critical analysis by pointing out limitations such as high variance gradients and scalability challenges. The abstraction is moderate, identifying broader issues like the on-policy nature of REINFORCE and methods to address them, but the focus remains on technical details rather than meta-level insights."}}
{"id": "cb915b6e-26a1-44b9-be2e-47ffa839e199", "title": "Hybrid methods", "level": "paragraph", "subsections": [], "parent_id": "6c84bc7f-b9cc-44a6-b8d6-5c59f102ecfe", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Deep Reinforcement Learning in Recommender Systems"], ["subsection", "Model-free deep reinforcement learning based methods"], ["paragraph", "Value based methods"], ["paragraph", "Hybrid methods"]], "content": "The most common model-free hybrid method used would be the actor-critic algorithm where the critic network uses the DQN and the actor uses the policy gradient. The common algorithm used to train actor-critic is DDPG with the following objective function,\n\\begin{align}\n    \\mathbb{E}[r_t+\\gamma Q_{\\theta_q'}(s_{t+1},\\mu_{\\theta_\\pi'}(s_{t+1})) - Q_{\\theta_q}(s_t,a_t)]\n\\end{align}\nwhere $\\theta_q,\\theta_q'$ is the parameter for Q-learning at time $t,t+1$ while $\\theta_\\pi'$ is the parameter for deterministic policy gradient at time $t+1$.\n propose LIRD, which uses the vanilla actor-critic framework to conduct list-wise recommendations. In order to demonstrate the effectiveness of LIRD, a pre-trained user simulator is used to evaluate the effectiveness of LIRD where the transition probability is approximated using the cosine similarity for a given state-action pair $s_t,a_t$.\n further extend LIRD into  page-wise recommendation and proposed DeepPage. Similar to other previous work, GRU is employed to process the sequential pattern. Moreover, similar to DRCGR, DeepPage formulates the state as a page, then CNNs are employed to capture features and fed to the critic network. The final state representation is the concatenation of the sequential pattern and the page features. Additionally, there are a few studies focusing on different scenarios such as top-aware recommendation~, treatment recommendation~, allocating impressions~ etc.  introduces a supervised learning module (SLC) as the indicator to identify the difference between the current policy and historical preferences. SLC will conduct the ranking process to ensure the recommendation policy will not be affected by the positional bias -- the item appearing on top receives more clicks. Similarly,  also integrates the supervised learning paradigm into DRL but in a different way. An expert action $\\hat{a}_t$ is provided when the critic evaluates the policy and the update rule is slightly different than normal DQN,\n\\begin{align}\n    \\theta_q \\leftarrow \\theta_q + \\alpha \\sum_{t}[ Q_{\\theta_q}(s_t,\\hat{a}_t)-r_t-\\gamma Q_{\\theta_{q'}}(s_t,\\mu_{\\theta_{\\pi'}}(s_t))]\\nabla_{\\theta_q}Q_{\\theta_q}(s_t,a_t).\n\\end{align}\nHowever, such a method is not universal as the acquisition of expert action is difficult and depends on the application domain.\nSimilar to policy gradient and DQN, Knowledge Graphs (KG) are also used in actor-critic-based methods.  propose KGRL to incorporate the substantial information of knowledge graphs to help the critic to better evaluate the generated policy. A knowledge graph is embedded into the critic network. Different from previous studies which use the KG as the environment or state representation, KGRL uses KG as a component in the critic, which can guide the actor to find a better recommendation policy by measuring the proximity from the optimal path. Specifically, a graph convolutional network is used to weight the graph and Dijkstra's algorithm is employed to find the optimal path for finally identifying the corresponding Q-value.  claim that human's demonstration could improve path searching and propose ADAC. ADAC also searches for the optimal path in the KG but further adopts adversarial imitation learning and uses expert paths to facilitate the search process.  propose MA-RDPG, which extends the standard actor-critic algorithm to deal with multiple scenarios by utilizing a multi-actor reinforcement learning setting. Specifically, two different actor-networks are initialized while only one critic network will make the final decision. Those two actor networks can communicate with each other to share information and approximate the global state.  find that there are multiple factors can affect the selection of electric charging station. Hence, it uses a similar idea to recommend the electric vehicle charging station by considering current supply, future supply, and future demand.  figure out that the communication mechanism in MA-RDPG will harm actors as they are dealing with independent modules, and there is no intersection. Hence,  extend MA-RDPG into multi-agent settings which contain multiple pairs of actors and critics and remove the communication mechanism to ensure independence. \nDifferent from ,  use `soft' actor-critic (SAC)~, which introduces a maximum entropy term $\\mathcal{H}(\\pi(s_t,\\phi_t))$ to actor-critic to improve exploration and stability with the stochastic policy $\\pi(s_t,\\phi_t)$. \nSimilar to the multi-agent idea,  use a hierarchical setting to help the agent learn multiple goals by setting multiple actors and critics. In comparison, hierarchical RL uses multiple actor-critic networks for the same task. It splits a recommendation task into two sub-tasks: discovering long-term behavior and capturing short-term behavior. The final recommendation policy is the combination of the optimal policies for the two sub-tasks. Similarly,  use the hierarchical setting for integrated recommendation by using different sourced data. The objective is to work out the sub-polices for each source hierarchically and form the final recommendation policy afterward.", "cites": [5172, 6154, 3604, 8884, 5194, 5170, 3616, 1389], "cite_extract_rate": 0.6153846153846154, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively synthesizes key hybrid methods in model-free DRL for recommenders, connecting actor-critic variants across multiple papers and highlighting their unique applications (e.g., list-wise, page-wise, and knowledge graph-based methods). It includes some critical evaluation, such as noting limitations in expert action dependency and communication mechanisms in multi-agent settings. While it identifies patterns in the use of hierarchical and multi-agent structures, the abstraction remains moderate and focused on specific methodological adaptations."}}
{"id": "47d0fb4b-4dca-4006-ba90-90530959ef38", "title": "Discussion", "level": "paragraph", "subsections": [], "parent_id": "6c84bc7f-b9cc-44a6-b8d6-5c59f102ecfe", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Deep Reinforcement Learning in Recommender Systems"], ["subsection", "Model-free deep reinforcement learning based methods"], ["paragraph", "Value based methods"], ["paragraph", "Discussion"]], "content": "In RS, model-free methods are generally more flexible than model-based methods as they do not require knowing the transition probability. We summarize the advantages and disadvantages of the three kinds of methods described under the model-free category. DQN is the first DRL method used in RS, which is suitable for small discrete action spaces. The problems with DQN in RS are: \n\\begin{itemize}\n    \\item RS normally contains large and high-dimensional action spaces.\n    \\item The reward function is hard to determine which will lead to inaccurate value function approximation.\n\\end{itemize} \nSpecifically, the high dimensional action space in context of recommender systems is recognized as a major drawback of DQN~. The reason lies in the large number of the candidate items. Hence, DQN, as one of the most popular schemes, is not the best choice for RS in many situations. Moreover, some unique factors need to be considered when designing the reward function for RS such as social inference. It introduces extra parameters to the Q-network and hinders the convergence.\nPolicy gradient does not require the reward function to estimate the value function. Instead, it estimates the policy directly. However, policy gradient is designed for continuous action spaces. More importantly, it will introduce high variance in the gradient. Actor-critic algorithms combine the advantages of DQN and policy gradient. Nonetheless, actor-critic will map the large discrete action space into a small continuous action space to ensure it is differentiable, which may cause potential information loss. Actor-critic uses DDPG and thus inherits disadvantages from DQN and DPG, including difficulty in determining the reward function and poor exploration ability.", "cites": [6157], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear comparison of model-free DRL methods in recommender systems, identifying their strengths and limitations. It critically evaluates DQN, policy gradient, and actor-critic methods, particularly in the context of RS challenges like high-dimensional action spaces and reward function design. While it references the cited paper to contextualize action space issues, it does not deeply synthesize insights across multiple sources, and the abstraction remains at a moderate level, focusing on specific technical properties rather than broader conceptual trends."}}
{"id": "c30b91c8-48bc-4d92-ba66-c418e39b9327", "title": "Component Optimization in Deep Reinforcement Learning based RS", "level": "subsection", "subsections": ["3de2362c-6535-43b9-860b-ebc2cbce5821", "45229725-ce80-46d4-8644-8011bc19bde5", "0c8d5ce0-24e2-4a9b-aa04-14e44c18baf4"], "parent_id": "696bfdf7-6191-445e-82a8-ae4df2378853", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Deep Reinforcement Learning in Recommender Systems"], ["subsection", "Component Optimization in Deep Reinforcement Learning based RS"]], "content": "\\label{sec:component}\nThere are a few studies that use DRL in RS for goals other than improving recommendation performance or proposing new application domains. We split the literature based on the following components: environment, state representation, and reward function. Exisitng studies usually focus on optimizing one single component in the DRL setting (as illustrated in~\\Cref{fig:overview}).\n\\begin{table}[h]\n    \\centering\n    \\caption{List of publications reviewed in this section}\n    \\begin{tabular}{c|c}\n    \\hline\n      Component  &  Work \\\\\\hline\n       Environment  &  \\\\\n       State & \\\\\n       Reward & \\\\\n       \\hline\n    \\end{tabular}\n    \\label{tab:component}\n\\end{table}\n\\\\", "cites": [6158, 5192, 5201, 8984, 5204, 3610], "cite_extract_rate": 0.5, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section begins to introduce the idea of component optimization in DRL-based recommender systems but fails to synthesize or integrate the cited papers meaningfully. There is no discussion connecting the environment, state, and reward function across the works. No critical evaluation or abstraction is provided, and the table is left incomplete, suggesting minimal analytical engagement."}}
{"id": "3de2362c-6535-43b9-860b-ebc2cbce5821", "title": "Environment Simulation and Reconstruction", "level": "subsubsection", "subsections": [], "parent_id": "c30b91c8-48bc-4d92-ba66-c418e39b9327", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Deep Reinforcement Learning in Recommender Systems"], ["subsection", "Component Optimization in Deep Reinforcement Learning based RS"], ["subsubsection", "Environment Simulation and Reconstruction"]], "content": "~\\\\\nMany environments are available for evaluating deep reinforcement learning. Two popular ones are OpenAI gym-based environment  and  MuJoCo\\footnote{http://mujoco.org/}.\nUnfortunately, there is no standardized simulation platform or benchmark specific to reinforcement learning based recommender systems. Existing work on DRL in RS is usually evaluated through offline datasets or via deployment in real applications. The drawback for evaluating offline datasets include:\n\\begin{itemize}\n    \\item Different studies use different environment construction methods which leads to unfair comparison. For instance, some studies use the KG as the environment while some studies assume the environment is gym-like or design a simulator for specific tasks.\n    \\item With offline datasets, users' dynamic interests, and environment dynamics are hard to maintain. Deploying the method into a real application is difficult for academic research as it takes time and costs money. Hence, a standardized simulation environment is a desirable solution.\n\\end{itemize}\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{cofounder.pdf}\n    \\caption{Left is the traditional MDP transition; Right is the POMDP which considers the environmental confounders such as social influence~.}\n    \\label{fig:cofounder}\n\\end{figure}\nThere are several different studies that provide standardized gym\\footnote{https://gym.openai.com/}-based simulation platforms for DRL-based RS research in different scenarios. RecSim~ is a configurable platform that supports sequential interaction between the system and users. RecSim contains three different tasks: interest evolution, interest exploration and long-term satisfaction. RecoGym~ provides an environment for recommendation and advertising. In addition, RecoGym also provides simulation of online experiments such as A/B-tests. However, RecSim and RecoGym are designed for bandit behavior which means users' interests will not change over time. \nVirtualTB~ is proposed to relieve such problems. VirtualTB employs imitation learning to learn a user model to interact with the agent. GANs are employed to generate users' interests. Similar to VirtualTB, Recsimu~ uses a GAN to tackle the complex item distribution.\nIn addition, PyRecGym~ accommodates standard benchmark datasets into a gym-based environment. MARS-Gym~ provides a benchmark framework for marketplace recommendation.~ suggests that  existing simulation environments are biased because of  biased logged data. Two common biases are discussed: popularity bias and positivity bias. To reduce the effect from those biases, SOFA introduces an Intermediate Bias Mitigation Step for debiasing purposes.\nOne work discusses environment reconstruction by considering confounders.~ claims that  users' interests may be affected by social networks which may introduce extra bias to the state and affect the decision-making process. A multi-agent setting is introduced to treat the environment as an agent which can partially relieve the hidden confounder effect. Specifically, a deconfounded environment reconstruction method DEMER is proposed. Different from previously  mentioned methods, DEMER assumes the environment is partially observed and models the whole recommendation task as a Partially Observed MDP (POMDP). Different from an MDP, a POMDP contains one more component observation $o\\in\\mathcal{O}$ and the action $a_t$ is derived based on the observation $o_t$ instead of the state $s_t$ by $a_t = \\pi_a(o_t)$. DEMER assumes there is a confounder policy $\\pi_h$ for observation $o_h$ which is composed by $a_t$ and $o_t$: $a_h = \\pi_h(a_t,o_t)$. Moreover, another observation $o_b$ is introduced to observe the transition as well. $\\pi_b$ is the corresponding policy and $a_b = \\pi_b(o_b) = \\pi_b(o_t,a_t,a_h)$. DEMER uses generative adversarial imitation learning (GAIL) to imitate the policy $A,B$.\nGiven trajectory $\\{o_t,o_h,o_b\\}$ for different policies $A$ and $B$, the objective function is defined as\n\\begin{align}\n    & (\\pi_a,\\pi_b,\\pi_h) = \\argmin_{(\\pi_a,\\pi_b,\\pi_h)}\\mathbb{E}_{s\\sim \\tau }(L(s,a_t,a_b)) \\notag \\\\\n    & \\text{where }L(s,a_t,a_b) = \\mathbb{E}_{(\\pi_a,\\pi_b,\\pi_h)}[\\log D(s,a_t,a_b)]-\\lambda \\sum_{\\pi\\in\\{\\pi_a,\\pi_b,\\pi_h\\}}H(\\pi)\n\\end{align}\nwhere $L(\\cdot)$ is the loss function, $D(\\cdot)$ is a discriminator and $H(\\pi)$ is introduced in GAIL.\n\\\\", "cites": [6158, 5201, 8984, 1346, 5204, 3610], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers by highlighting common themes (e.g., bias, dynamic user behavior) and contrasting approaches (e.g., MDP vs. POMDP). It provides a critical view by pointing out limitations of existing platforms like popularity bias and the static nature of user interests in bandit-based environments. The abstraction level is strong, as it introduces a conceptual discussion of POMDP and confounder modeling, which elevate the analysis beyond individual works."}}
{"id": "45229725-ce80-46d4-8644-8011bc19bde5", "title": "State Representation", "level": "subsubsection", "subsections": [], "parent_id": "c30b91c8-48bc-4d92-ba66-c418e39b9327", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Deep Reinforcement Learning in Recommender Systems"], ["subsection", "Component Optimization in Deep Reinforcement Learning based RS"], ["subsubsection", "State Representation"]], "content": "~\\\\\nState representation is another component in DRL-based RS which exists in both model-based and model-free methods.  find that the state representation in model-free methods would affect  recommendation performance.\nExisting studies usually directly use the embedding as the state representation.  propose a supervised learning method to generate a better state representation by utilizing an attention mechanism and a pooling operation as shown in~\\Cref{fig:staterep}.\nSuch a representation method requires training a representation network when training the main policy network, which increases the model complexity.\n\\begin{figure}[!h]\n    \\centering\n    \\includegraphics[width=0.5\\linewidth]{staterep_wb.pdf}\n    \\caption{State representation used in works~. $h_t$ is the output of an attention layer that takes the representation of users' history at time $t$ as input and $g(\\cdot)$ is the pooling operation. }\n    \\label{fig:staterep}\n\\end{figure}\n\\\\", "cites": [5192], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of state representation in DRL-based recommender systems, referencing a single paper. It lacks synthesis of multiple sources and fails to offer a broader context or comparative analysis. The critique is minimal, and abstraction is limited to repeating paper-specific methods without generalization."}}
{"id": "fd29fd11-361a-4846-b9fe-151e3edf8271", "title": "Multi-Agent and Hierarchical Deep Reinforcement Learning-based RS", "level": "subsection", "subsections": [], "parent_id": "3dd345fb-ce99-4727-b174-a3f36f9f711a", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Emerging Topics"], ["subsection", "Multi-Agent and Hierarchical Deep Reinforcement Learning-based RS"]], "content": "Recommender systems are monolithic systems containing \ntasks such as searching, ranking, recommendation, advertising, personalization, and diverse stakeholders such as users and items. Most existing methods are based on single agent. \nMulti-Agent Reinforcement Learning (MARL) is a subfield of reinforcement learning and it is capable of learning multiple policies and strategies. \nWhile a single-agent reinforcement learning framework can only handle a single task, many studies consider the multi-task situation in RS and employ multi-agent DRL (MADRL) or hierarchical DRL (HDRL).\nHDRL~ is proposed to handle complex tasks by splitting such tasks into several small components and asks the agent to determine sub-policies. HDRL belongs to a single-agent reinforcement learning framework such that the agent contains a meta-controller and several controllers. The meta-controller splits the task, and the controllers learn the value and reward functions for designated tasks to get a series of sub-policies. There are a few studies already utilizing HDRL in RS.\n  target integrated recommendation to capture user preferences on both heterogeneous items and recommendation channels. Specifically, the meta-controller is used for item recommendation, and controllers aim to find the personalized channel according to user channel-level preferences.  uses HDRL for course recommendation in MOOCs, which contains two different tasks: profile reviser and recommendation. The meta-controller aims to make course recommendations by using the revised profile pruned by the controllers.\nDifferent from HDRL, MADRL~ introduces multiple agents to handle the sub-tasks.  uses the MADRL for twitter mention recommendation where three agents are initialized. The three agents need to generate different representations for the following tasks: query text, historical text from authors and historical text from candidate users. Once the representations are finalized, the model will conduct the recommendation based on the concatenation of representations.  and  provide two different views of the communication mechanism in MADRL and demonstrate that agents could work collaboratively or individually.  designs a MADRL framework for two tasks where two agents are designed to conduct advertising and recommendation respectively.  uses MADRL for collaborative recommendation where each agent is responsible for a single user. MADRL is adopted to help the recommender consider both collaboration and potential competition between users.  designs a charging recommender system for intelligent electric vehicles by using decentralized agents to handle sub-tasks and a centralized critic to make the final decision.\nHierarchical multi-agent RL (HMARL)~ proves that MARL and HRL can be combined.\nRecently,  introduces HMADRL into the continuous action space, which provides a direction for RS.  uses HMARL for multi-goal recommendation where the meta-controller considers users' long-term preferences and controllers focus on short-term click behavior. While the meta-controller and controllers in HDRL deal with sub-tasks that belong to a single task, HMARL focuses on multi-task or multi-goal learning where the meta-controller and controllers belong to different agents and deal with different tasks or goals.\nHMADRL would be a suitable solution for future research work in DRL-based RS where HDRL can be used to split a complex task into several sub-tasks such as users' long-term interests and short-term click behavior, and MADRL can jointly consider multiple factors such as advertising .", "cites": [8884, 5185, 3588, 3616], "cite_extract_rate": 0.3076923076923077, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the cited works, distinguishing between HDRL and MADRL and illustrating their application in recommender systems. It abstracts these methods into broader frameworks (e.g., HMARL) and highlights how combining them can address complex, multi-goal recommendation problems. While it provides a coherent narrative and some comparative insights, it could offer more in-depth critique of the limitations or trade-offs of these approaches."}}
{"id": "ca4b40c6-413e-4583-9e93-7c0668b3a545", "title": "Inverse Deep Reinforcement Learning for RS", "level": "subsection", "subsections": [], "parent_id": "3dd345fb-ce99-4727-b174-a3f36f9f711a", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Emerging Topics"], ["subsection", "Inverse Deep Reinforcement Learning for RS"]], "content": "As\nmentioned,\nthe reward function plays a critical role in DRL-based recommender systems. In many existing works, reward functions are manually designed. The common method uses users' click behavior to represent the reward and to reflect users' interests. However, such a setting can not represent users' long-term goals~ as clicking or not only depicts part of the feedback information from users. It requires significant effort to design a reward function, due to the large number of factors that can affect users' decision, such as social engagement or bad product reviews, which may adversely affect recommendation performance. It is difficult to include all potential factors into the reward function because not every factor  can be represented properly. A few works~ show that manually designed reward functions can be omitted by employing inverse reinforcement learning (IRL)~ or generative adversarial imitation learning (GAIL)~. Such inverse DRL-based methods require using expert demonstration as the ground truth. However, expert demonstration is often hard to obtain for recommendation scenarios.\nThose two studies conduct experiments in an offline dataset-based simulation environment that can access expert demonstration. In contrast,  use IRL as the main algorithm to train the agent while  use both demonstration and reward to train the agent.  also employ GAIL to improve recommendation performance. In this work, GAIL is used to learn the reasoning path inside the KG to provide side information to help the agent learn the policy. Although IRL achieves some progress in RS, the lack of demonstration is a key shortcoming that impedes adoption in RS. One possibility is to use the IRL method in casual reasoning to help improve interpretability~ thus boosting recommendation performance. Alternately, IRL may be suitable for learning users' long-term and static behavior to support the reward function.", "cites": [6160, 3592, 6159, 6161, 5199], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to discuss the limitations of manually designed reward functions and the potential of inverse DRL methods in recommender systems. It offers a critical perspective by highlighting the challenge of obtaining expert demonstrations in recommendation scenarios. While it identifies patterns, such as the use of IRL and GAIL, the abstraction remains somewhat limited to the specific context of reward function learning rather than broader theoretical implications."}}
{"id": "04addbb4-b221-4c4b-aaeb-b0d4e6b0d4fb", "title": "Graph Neural Networks for Boosting DRL-based RS", "level": "subsection", "subsections": [], "parent_id": "3dd345fb-ce99-4727-b174-a3f36f9f711a", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Emerging Topics"], ["subsection", "Graph Neural Networks for Boosting DRL-based RS"]], "content": "Graph data and KG are widely used in RS.\nGraph modeling enables an RS to leverage interactions between users and the recommender for reasoning or improving interpretability. According to existing studies about deep learning-based RS~, embedding is a technique used to get the representation for the input data. Graph embedding is a common solution to handle graph-like data. GCN is a type of graph embedding method which\nare broadly used in RS to process  graph data.  propose a variant of GCN to learn the embedding for KG. Specifically, they propose knowledge graph convolutional networks (KGCN) to capture the high-order structural proximity among entities in a knowledge graph. \nIn DRL-based RS, graph data are handled similarly---s underthe transformed into an embedding and fed to the agent.  uses a traditional graph embedding method TransE~ to generate the state representation for DRL-based RS. There are several studies that use GCN in DRL for recommendations under different settings.  propose a graph convolutional RL (DGN) method which integrates the GCN into the Q-learning framework for general RL problems by replacing the state encoding layer with the GCN layer.  extend this method into the deep learning field and apply it to recommender systems. To be specific, multiple GCN layers are employed to process the sub-graphs for a given item $i$.  employs KG inside the actor-critic algorithm to help the agent learn the policy. Specifically, the critic network contains a GCN layer to give weight to the graph and conduct searches in the graph to find an optimal path and hence guide the optimization of policy learning. However, such a method is relatively computationally expensive as it requires jointly training the GCN and the actor-critic network.  adopts a Graph Attention Network (GAT)~ into the actor-critic network to conduct  recommendation. In addition, the GAT is used as an encoder to obtain a state representation.\nA common way of using GCN or its variants in DRL-based RS is the state encoder. \nThe related challenge is the difficulty for the environment to provide a graph-like input to the GCN.", "cites": [1444, 6160, 180, 6162, 5194], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers to present a coherent narrative on the use of graph neural networks in enhancing DRL-based recommender systems. It abstracts the common application (e.g., as state encoders) and identifies a key challenge in graph input provision. While it includes some critical points (e.g., computational expense of joint training), it could offer deeper comparative or evaluative analysis of the methods."}}
{"id": "19a7db65-832f-4d59-9f47-c445aa549fa1", "title": "Self-Supervised DRL-based RS", "level": "subsection", "subsections": [], "parent_id": "3dd345fb-ce99-4727-b174-a3f36f9f711a", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Emerging Topics"], ["subsection", "Self-Supervised DRL-based RS"]], "content": "Self-supervised learning (SSL) is a technique in which the model is trained by itself without external label information. SSL-DRL is receiving growing interest in robotics \n~.  shows that SSL can be used to learn the policy when doing navigation by providing real-world experience.  demonstrates that SSL-DRL can be used to help the agent learn synergies between two similar policies, thus empowering the agent to conduct two different tasks. \nRecent advances in SSL RL show that SSL can also provide interpretability for RL, which is promising for interpretable RS research~.  shows that SSL based RL can highlight the task-relevant information to guide the agent's behavior.\nMoreover,  shows that SSL can be used to provide negative feedback for DRL-based RS to improve recommendation performance. To be specific, a self-supervised loss function is appended into the normal DRL loss function,\n\\begin{align}\n    -\\sum_{i=1}^nY_i\\log\\bigg(\\frac{e^{y_i}}{\\sum_{i'=1}^ne^{y_{i'}}}\\bigg) + L_{\\mathit{DRL}}\n\\end{align}\nwhere $Y_i$ is an indicator function to show users interact with the item $i$ or not. $L_{\\mathit{DRL}}$ could vary, if the DQN is adopted, \\Cref{dqnloss} should be used.\nSSL demonstrates promising performance in visual representation in recent years, which would be a possible solution to generate the state representation as there are a few DRL-based RS studies that adopt CNNs to process image-like data and transform it into a state~. Furthermore, as an unsupervised learning approach, SSL would provide a new direction about defining the reward function by learning common patterns between different states as well as multi-task learning.", "cites": [4711, 6163, 6164, 5188], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple SSL-DRL approaches from robotics and RL, linking them to the potential use in recommender systems. It integrates ideas on task synergies, interpretability, and reward function design, but lacks deeper comparative or critical analysis of the cited works. Some abstraction is present in highlighting broader applications of SSL in DRL for RS, but the insights remain somewhat surface-level."}}
{"id": "e24a9f91-4259-41ce-8cfe-67de33facae4", "title": "Sample Efficiency", "level": "subsection", "subsections": [], "parent_id": "62c3e9c3-0318-4d9a-947c-bf1d76096072", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Open Questions"], ["subsection", "Sample Efficiency"]], "content": "Sample inefficiency is a well-known challenge in model-free DRL methods. Model-free DRL requires a significant number of samples as there is no guarantee that the received state is useful. Normally, after a substantial number of episodes, the agent may start learning as the agent finally receives a useful state and reward signal. A common solution is the experience replay technique, which only works in off-policy methods. \nExperience replay still suffers the sample inefficiency problem~ as not every past experience is worth replaying.  propose selected experience replay (SER) that only stores valuable experience into the replay buffer and thus improves sample efficiency. while traditional DRL environments only contain several\\footnote{For example, the number of actions in MuJoCo is less than one hounded.} candidate items, in DRL-based RS, the agent must deal with a significantly larger action space as RS may contain lots of candidate items. Existing DRL-based RS studies on traditional experience replay methods often demonstrate slow converge speed.  design a user model to improve the sample efficiency through auxiliary learning. Specifically, they apply the auxiliary loss with the state representation, and the model distinguishes low-activity users and asks the agent to update the recommendation policy based on high-activity users more frequently.\nOn the other hand, model-based methods are more sample efficient. However, they introduce extra complexity as the agent is required to learn the environment model as well as the policy. Due to the extremely large action space and possibly large state space (depending on users' contextual information) in RS, approximating the environment model and policy simultaneously\nbecomes challenging.", "cites": [3586, 8985], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes two cited papers by discussing how experience replay can be modified to improve sample efficiency in DRL for recommender systems. It provides a general comparison between model-free and model-based methods, but lacks deeper critical evaluation of the limitations or trade-offs of the proposed solutions. The section identifies a broader issue (sample inefficiency) and contrasts it with the potential of model-based approaches, offering some level of abstraction beyond individual papers."}}
{"id": "f3bc9826-577b-48f2-9fd1-1cbd7502868e", "title": "Exploration and Exploitation", "level": "subsection", "subsections": [], "parent_id": "62c3e9c3-0318-4d9a-947c-bf1d76096072", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Open Questions"], ["subsection", "Exploration and Exploitation"]], "content": "The exploration and exploitation dilemma is a fundamental and challenging problem in reinforcement learning research and receives lots of attention in DRL.\nThis dilemma describes a trade-off between obtaining new knowledge and the need to use that knowledge to improve performance. Many DQN-based methods focus on exploration before the replay buffer is full and exploitation afterward. Consequently, it requires an extremely large replay buffer to allow all possibilities in recommendation can be stored.\nDRN employs Dueling Bandit Gradient Descent (DBGD)~ to encourage exploration while ~ introduces a  regularization or entropy term into the objective function to do so.~ uses the sheer size of the action space to ensure sufficient exploration.~ uses a separate KG or elaborated graph exploration operation to conduct  exploration.~ employs Boltzmann exploration to get the benefit of exploratory data without negatively impacting user experience. \nIn addition, $\\epsilon$-greedy is the most common technique used to encourage  exploration ~. Remaining studies rely on a simulator to conduct exploration. However, it may suffer from noise and over-fitting~ because of the gap between simulation and real online application. For most DRL-based methods such as vanilla DQN, policy gradient, or actor-critic-based methods, $\\epsilon$-greedy would be a good choice for exploration. In addition, injecting noise into the action space would also be helpful for those actor-critic-based methods~. For methods involving KGs, $\\epsilon$-greedy may help, but the elaborated graph exploration methods may receive better performance.", "cites": [5190, 6154, 5180, 5195, 5177, 5165, 5199], "cite_extract_rate": 0.3888888888888889, "origin_cites_number": 18, "insight_result": {"error": "Failed to parse LLM response", "raw_response": "{\n    \"type\": \"analytical\",\n    \"scores\": {\"synthesis\": 3.5, \"critical\": 3.0, \"abstraction\": 3.5},\n    \"insight_level\": \"medium\",\n    \"analysis\": \"The section synthesizes multiple papers to highlight different strategies for addressing exploration and exploitation in DRL-based recommenders, such as $\\epsilon$-greedy, entropy regularization, and graph-based methods. It abstracts these approaches into broader categories and acknowledges the limitations of simulation-based exploration. However, the"}}
{"id": "8fb9ed60-6e10-4fb7-a67b-6f49f9ef3798", "title": "Generalizing from Simulation to Real-World Recommendation", "level": "subsection", "subsections": [], "parent_id": "62c3e9c3-0318-4d9a-947c-bf1d76096072", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Open Questions"], ["subsection", "Generalizing from Simulation to Real-World Recommendation"]], "content": "Existing work generally trains DRL algorithms in simulation environments or offline datasets.\nDeploying DRL algorithms into real applications is challenging due to the gap between simulation and real-world applications.\nSimulation environments do not contain domain knowledge or social impact.\nThey can not cover the domain knowledge and task-specific engineering in the real-world recommendation.\nHow to bridge the gap between simulation and real applications is a challenging topic. Sim2real~ is a transfer learning approach that transfers DRL policies from simulation environments to reality. Sim2real uses domain adaption techniques to help agents transfer learned policy. Specifically, it adopts GANs to conduct adaption by generating different samples.\nRL-CycleGAN~ is a sim2real method for vision-based tasks. It uses CycleGAN~ to conduct pixel-level domain adaption. Specifically, it maintains cycle consistency during GAN training and encourages the adapted image to retain certain attributes of the input image.\nIn DRL-based RS, sim2real would be a possible solution for generalizing the learned policy from simulation environments to reality. However, sim2real is a new technique still under exploration. It shows an adequate capability in simple tasks and requires more effort to handle the complex task such as recommendation. We believe it is a workable solution for generalizing from simulation to reality.", "cites": [7022, 3199, 8106], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the concept of sim2real in the context of DRL-based recommender systems by integrating ideas from three different papers, connecting their approaches to the domain-specific challenges in recommendation. It shows some critical analysis by pointing out the adequacy of sim2real in simple tasks and the need for more effort in complex recommendation tasks. The abstraction is moderate, as it generalizes sim2real as a potential solution but does not fully explore broader implications or propose a new conceptual framework."}}
{"id": "bb9fa85c-840e-4ea9-928c-30b0b38ddbc5", "title": "Bias (Unfairness)", "level": "subsection", "subsections": [], "parent_id": "62c3e9c3-0318-4d9a-947c-bf1d76096072", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Open Questions"], ["subsection", "Bias (Unfairness)"]], "content": " observe that user behavior data are not experimental but observational, which leads to problems of bias and unfairness.\\\\\nThere are two reasons why bias is so common. First, the inherent characteristic of user behavior data is not experimental but observational. In other words, data that are fed into recommender systems are subject to selection bias . For instance, users in a video recommendation system  tend to watch, rate, and comment on those movies that they are interested in. Second, a distribution discrepancy exists, which means the distributions of users and items in the recommender system are not even. Recommender systems may suffer from 'popularity bias', where popular items are recommended far more frequently than the others. However, the ignored products in the â€œlong tailâ€ can be equally critical for businesses as they are the ones less likely to be discovered.\n denote the unfairness as that the system systematically and unfairly discriminates against certain individuals or groups of individuals in favor of others. \nA large number of studies explore dynamic recommendation systems by utilizing the agent mechanism in reinforcement learning (RL), considering the information seeking and decision-making as sequential interactions. How to evaluate a policy efficiently is a big challenge for RL-based recommenders. Online A/B tests are not only expensive and time-consuming but also sometimes hurt the user experience. Off-policy evaluation is an alternative strategy that historical user behavior data are used to evaluate the policy. However, user behavior data are biased, as mentioned before, which causes a gap between the policy of RL-based RS and the optimal policy.\nTo eliminate the effects of bias and unfairness,  use the inverse of the probability of historical policy to weight the policy gradients.  introduce a debiasing step that corrects the biases presented in the logged data before it is used to simulate user behavior.  propose to build a customer simulator that is designed to simulate the environment and handle the selection bias of logged data.", "cites": [6165, 5165, 3891], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the core concepts of bias and unfairness in DRL-based recommender systems by drawing on multiple papers, particularly linking the causes of bias to the nature of observational user data. It abstracts these issues into general challenges like selection bias and popularity bias, and briefly connects them to off-policy evaluation in RL. However, the critical analysis is limited, as it does not deeply evaluate the strengths or weaknesses of the debiasing methods proposed by the cited works."}}
{"id": "317e1a16-83a6-4e39-b6b1-02817b7555bf", "title": "Explainability", "level": "subsection", "subsections": [], "parent_id": "62c3e9c3-0318-4d9a-947c-bf1d76096072", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Open Questions"], ["subsection", "Explainability"]], "content": "Although deep learning-based models can generally improve the performance of recommender systems, they are not easily interpretable. As a result, it becomes an important task to make recommender results explainable, along with providing high-quality recommendations.\nHigh explainability in recommender systems not only helps end-users understand the items recommended but also enables system designers to check the internal mechanisms of recommender systems.  review different information sources and various types of models that can facilitate explainable recommendation. Attention mechanisms and knowledge graph techniques currently play an important role in realizing explainability in RS.\nAttention models have great advantages in both enhancing predictive performance and having greater explainability~.  introduce a reinforcement learning framework incorporated with an attention model for explainable recommendation. Firstly, it achieves model-agnosticism by separating the recommendation model from the explanation generator. Secondly, the agents that are instantiated by attention-based neural networks can generate sentence-level explanations. \nKnowledge graphs contain rich information about users and items, which can help to generate intuitive and more tailored explanations for the recommendation system . Recent work has achieved greater explainability by using reinforcement and knowledge graph reasoning. The algorithm from  learns to find a path that navigates from users to items of interest by interacting with the knowledge graph environment.  extract imperfect path demonstrations with minimum labeling effort and propose an adversarial actor-critic model for  demonstration-guided path-finding. Moreover, it achieves better recommendation accuracy and explainability by reinforcement learning and knowledge graph reasoning.", "cites": [5190, 8881], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from the two cited papers, connecting the use of attention mechanisms and knowledge graphs to enhance explainability in DRL-based recommenders. While it provides some abstraction by highlighting broader trends (e.g., model-agnosticism, causal inference, and path-finding in knowledge graphs), it lacks deeper critical analysis of limitations or trade-offs between the approaches. The narrative is coherent but stops short of offering a novel or meta-level synthesis."}}
{"id": "03cb7efb-1098-4511-ad2a-26dcf319c6b5", "title": "Robustness on Adversarial Samples and Attacks", "level": "subsection", "subsections": [], "parent_id": "62c3e9c3-0318-4d9a-947c-bf1d76096072", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Open Questions"], ["subsection", "Robustness on Adversarial Samples and Attacks"]], "content": "Adversarial samples demonstrate that deep learning-based methods are vulnerable. Hence,\nrobustness becomes an open question for both RS and DRL. Specifically, adversarial attack and defense in RS have received a lot of attention in recent years~ as security is crucial in RS. Moreover, DRL policies are vulnerable to adversarial perturbations to agent's observations~.  provide an adversarial attack method for perturbing the observations, thus affecting the learned policy. Hence, improving the robustness is the common interest for DRL and RS, which would be a critical problem for DRL-based RS.  provide an adversarial attack detection method for DRL-based RS which uses the GRU to encode the action space into a low-dimensional space and design decoders to detect the potential attack. However, it only considers Fast Gradient Sign Method (FGSM)-based attacks and strategically-timed attacks~. Thus, it lacks the capability to detect other types of attack. Moreover, it only provides the detection method while the defence is still an opening question.\nWe believe zero-shot learning techniques would be a good direction for training a universal adversarial attack detector. For defence, it is still an open question for DRL-based RS, though recent advances in adversarial defence in DRL may provide some insights~.", "cites": [6167, 1192, 6166], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from three distinct papers, connecting adversarial attacks in DRL with their implications for recommender systems. It provides a critical evaluation by pointing out the limitations of current detection methods, such as limited attack type coverage and absence of defense mechanisms. While it identifies a broader direction (e.g., zero-shot learning for universal detection), it does not elevate to a higher level of abstraction or propose a unifying framework."}}
{"id": "e714f9af-5ef5-4cc0-b0fc-93a677fe49fd", "title": "Causal and Counterfactual Inference", "level": "subsection", "subsections": [], "parent_id": "db61769c-00fa-49cf-aca1-794fe6209774", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Future Directions"], ["subsection", "Causal and Counterfactual Inference"]], "content": "Causality is a generic relationship between a cause and effect. Moreover, inferring causal effects is a fundamental problem in many applications like computational advertising, search engines, and recommender systems~. \nRecently, some researchers have connected reinforcement learning with learning causality to improve the effects for solving sequential decision-making problems.\nBesides, Learning agents in reinforcement learning frameworks face a more complicated environment where a large number of heterogeneous data are integrated.\nFrom our point of view, causal relationships would be capable of improving the recommendation results by introducing the directionality of cause and the effect. The users' previous choices have impact on the subsequent actions. This can be cast as an interventional data generating the dynamics of recommender systems. By viewing a policy in RL as an intervention, we can detect unobserved confounders in RL and choose a policy on the expected reward to better estimate the causal effect . Some studies improve RL models with causal knowledge as side information. Another line of work uses causal inference methods to achieve unbiased reward prediction~.  \n propose a Causal Inference Q-network which introduces observational inference into DRL by applying extra noise and uncertain inventions to improve resilience. Specifically, in this work, noise and uncertainty are added into the state space during the training state, and the agent is required to learn a causal inference model by considering the perturbation.\n give the first demonstration that model-free reinforcement learning can be used for causal reasoning. They explore meta-reinforcement learning to solve the problem of causal reasoning. The agents trained by a recurrent network able to make causal inferences from observational data and output counterfactual predictions. \n bridge RL and causality by data-fusion for reinforcement learners. Specifically, online agents combine observations, experiments and counterfactual data to learn about the environment, even if unobserved confounders exist. Similarly,  make the model-based RL agents work in a causal way to explore the environment under the Partially-Observable Markov Decision Process (POMDP) setting. They consider interventional data and observational data jointly and interprete model-based reinforcement learning as a causal inference problem. In this way, they bridge the gap between RL and causality by relating common concepts in RL and causality.  \nRegarding explainability in RL,  propose to explain the behavior of agents in reinforcement learning with the help of causal science. The authors encode causal relationships and learn a structural causal model in RL, which is used to generate explanations based on counterfactual analysis. With counterfactual exploration, this work is able to generate two contrastive explanations for `why' and `why not' questions.\nIt is so important to search for a Directed Acyclic Graph (DAG) in causal discovery. Considering traditional methods rely on local heuristics and predefined score functions,  propose to use reinforcement learning to search DAG for causal discovery. They use observational data as an input, RL agents as a search strategy and output the causal graph generated from an encoder-decoder NN model.", "cites": [6168, 4573, 6169, 4428, 3610, 4416], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the cited works effectively by connecting the integration of causality into DRL with various applications such as environment reconstruction, policy explanation, and DAG discovery. It identifies a common theme of using causal relationships to improve decision-making and policy learning in RL-based recommenders. While it provides some critical points, such as the importance of handling unobserved confounders, it lacks deeper evaluation of limitations or trade-offs between approaches. The abstraction is moderate, as it identifies the broader role of causality in DRL but does not propose a unifying framework."}}
{"id": "69b9e3f2-89df-425e-ae0b-359e87390941", "title": "Offline DRL and Meta DRL", "level": "subsection", "subsections": [], "parent_id": "db61769c-00fa-49cf-aca1-794fe6209774", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Future Directions"], ["subsection", "Offline DRL and Meta DRL"]], "content": "Recommender systems often need to deal with multiple scenarios such as joint recommendation and adverting, offline DRL and meta DRL provide a promising direction for achieving multiple scenarios at the same time.\nOffline DRL is a new paradigm of DRL that can be combined with existing methods such as self-supervised learning and transfer learning to move toward real-world settings. \nOffline DRL~ (also known as batch DRL) is designed for tasks which contain huge amounts of data.\nGiven a large dataset that contains past interactions, offline DRL uses the dataset for training across many epochs but does not interact with the environment.\nOffline DRL provides a solution that can be generalized to new scenarios as it was trained by a large sized dataset.\nSuch generalization ability is critical to RSs, which may need to deal with multiple scenarios or multiple customers.\nWhile offline DRL could provide a new direction for DRL-based RS, it still faces a few problems regarding handling the distributional shifts between existing datasets and real-world interactions. \nMeta DRL~ is defined as \nmeta learning in the filed of DRL. Meta DRL is another approach to help agents to generalize to new tasks or environments. Different from offline DRL, meta DRL contains a memory unit which is formed by the recurrent neural network to memorize the common knowledge for different tasks. Different from offline DRL, meta DRL does not require a large amount of data to train.", "cites": [7753, 6170], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates the concepts of offline DRL and meta DRL, drawing from the cited tutorial and meta-learning paper. It begins to establish a connection between these approaches and their relevance to real-world recommender system applications, such as generalization across scenarios. While it provides a basic comparison between offline DRL and meta DRL, it lacks deeper evaluative analysis or a novel framework, limiting its insight quality to medium."}}
{"id": "18638c1b-d943-46f7-8255-6db369c76c4b", "title": "Further Developments in Actor-Critic Methods", "level": "subsection", "subsections": [], "parent_id": "db61769c-00fa-49cf-aca1-794fe6209774", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions"], ["section", "Future Directions"], ["subsection", "Further Developments in Actor-Critic Methods"]], "content": "An actor-critic method uses the traditional policy gradient method, which suffers from the high variance problem due to the gap between behavior policy (i.e., the policy that is being used by an agent for action select) and target policy (i.e., the policy that an agent is trying to learn).\nA method commonly used to \nrelieve\nthe high variance problem is Advantage Actor-critic (A2C). \nDifferent from traditional actor-critic methods, A2C uses an advantage function to replace the Q-function inside the critic network. The advantage function $A(s_t)$ is defined as the expected value of the TD-error. \nThe new objective function for policy gradient can be written as,\n\\begin{align}\n     \\mathbb{E}_{\\tau \\sim d_{\\pi_\\theta}}[\\sum_{t=1}^T\\underbrace{(Q(s_t,a_t) - V(s_t))}_{A(s_t)}\\sum_{t=1}^T\\nabla_{\\theta} \\log \\pi_{\\theta}(s_t,a_t)].\n\\end{align}\nHowever, A2C still uses DDPG as the main training algorithm, which may suffer function approximation errors when estimating the Q value.\nTwin-Delayed DDPG (TD3)~ is designed to improve the function approximation problem in DDPG which uses clipped double Q-learning to update the critic. The gradient update can be expressed as,\n\\begin{align}\n     \\mathbb{E}_{\\tau \\sim d_{\\pi_\\theta}}[\\sum_{t=1}^Tr(s_t,a_t) + \\gamma \\min(Q_1(s_t,a_t+\\epsilon), Q_2(s_t,a_t+\\epsilon))\\sum_{t=1}^T\\nabla_{\\theta} \\log \\pi_{\\theta}(s_t,a_t)].\n\\end{align}\nwhere $\\epsilon\\sim\\textit{clip}(\\mathcal{N}(0,\\sigma,-c,c))$, $\\sigma$ is the standard deviation and $c$ is a constant for clipping.  \nAnother two ways to improve actor-critic methods are Trust Region Policy Optimization (TRPO)~ and Proximal Policy Optimization (PPO)~, which focus on modification of the advantage function. TRPO aims to limit the step size for each gradient to ensure it will not change too much. The core idea is to add a constraint to the advantage function,\n\\begin{align}\n    \\frac{\\pi(a|s)}{\\pi_{old}(a|s)}A(s),\n\\end{align}\nwhere the KL divergence will be used to measure the distance between the current policy and the old policy is small enough. PPO has the same goal as TRPO which is to try to find the biggest possible improvement step on a policy using the current data. PPO is a simplified version of TRPO which introduces the clip operation,\n\\begin{align}\n    \\min\\bigg(\\frac{\\pi(a|s)}{\\pi_{old}(a|s)}A(s),\\text{clip}\n    \\bigg(\\frac{\\pi(a|s)}{\\pi_{old}(a|s)}A(s), 1-\\epsilon, 1+\\epsilon\\bigg)A(s)\\bigg).\n\\end{align}\nSoft Actor-Critic (SAC)~ is another promising variant of the actor-critic algorithm and is widely used in DRL research. SAC uses the entropy term to encourage the agent to explore, which could be a possible direction to solve the exploration and exploitation dilemma. Moreover, SAC assigns an equal probability to actions that are equally attractive to the agent to capture those near-optimal policies.\nAn example of related work  uses SAC to improve the stability of the training process in RS.", "cites": [6171, 2219, 1391, 1389], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key actor-critic methods (A2C, TD3, TRPO, PPO, SAC) and their corresponding innovations from the cited papers, integrating them into a coherent discussion of future developments. It critically evaluates the limitations of earlier methods, such as the function approximation error in DDPG and the need for stable policy updates in TRPO and PPO. The abstraction level is strong, as it identifies broader trends like entropy regularization in SAC and the exploration-exploitation trade-off, offering insights into their potential application in recommender systems."}}
