{"id": "d89feaff-8daf-46c9-b741-b54627b16b16", "title": "Sparse Gaussian Processes", "level": "section", "subsections": [], "parent_id": "6486154a-d131-4b97-8685-8ede70aa3f14", "prefix_titles": [["title", "Deep Gaussian Processes: A Survey"], ["section", "Sparse Gaussian Processes"]], "content": "\\begin{wrapfigure}{r}{0.5\\textwidth}\n  \\begin{center}\n    \\includegraphics[width=0.2\\textwidth]{figs/SGP.pdf}\n  \\end{center}\n    \\caption{Overview of Sparse Gaussian Processes.}\n    \\label{fig:sgp}\n\\end{wrapfigure}\nGiven the computational and storage requirements that hinder the widespread use of GPs, a substantial number of papers have tried to address the problem and are collectively referred to as Sparse Gaussian Processes (SGPs), Fig.~\\ref{fig:sgp} depicts the prominent methods coverd in this section. The terminology stems from the way most of these approaches address the issue. Because the primary problem is the inversion of a covariance matrix, most methods try to introduce sparsity and reduce the matrix size that needs to be inverted while retaining the original matrix's performance. \nThis section focuses on some of the well-known methods crucial for developing some of the Deep Gaussian Process methods that will be detailed in the upcoming section. A complete overview of all SGPs is outside this survey's scope; the readers are referred to  for a thorough summary. \nThe Nyström approximation of  is a well know approach to reducing the inversion cost of the covariance matrix in GPs. The Nyström approximation allows one to generate a low rank approximation of any kernel matrix. The approach is applied to GPs by selecting $m$ data points with $m<<n$ from the training set.  Then a low rank approximation $\\tilde{K}$  to the kernel matrix is computed as shown below\n$$\\tilde{K} = K_{n,m} K_{m,m}^{-1}  K_{m,n}$$\nHere, $K_{n, m}$ represents the kernel matrix computed from the $n$ and $m$ data points in the training dataset and the selected subset, respectively. The same notation is used for the other kernel matrices. The approximation needs the inversion of only an $m \\times m$ matrix, thereby reducing the computation cost from $ O(n^3)$ to  $O(m^3)$.\nHowever, the approximation assumes that the data comes from a low rank manifold, which would be the case if the data dimension $d<n$. In such a case, the low rank approximation would be exact and result in no information loss.  But, the selected $m$ data points also influence the approximation. It is possible to have data points that result in a poor approximation even if the data is from a low dimensional manifold. \nIn practice, most datasets have more data points than the number of features; thus, the method is applicable in most cases. However, selecting the data points is crucial to the approximation's performance. Williams and Seeger used a random subset of $m$ data points in their approach  . Although the method works, the unsophisticated data selection process limited the method's performance. \nSnelson and Ghahramani  addressed the subset selection for Nyström approximation by treating the subset as model parameters and termed them pseudo data. The pseudo data is assumed to be synthetic and does not necessarily correspond to any data point available in the training dataset. Indeed, they could take values that are some combination of the training dataset. \nThe pseudo points were computed using maximum likelihood. However, to use maximum likelihood, one needs to parameterize the GP with pseudo points appropriately. Snelson and Ghahramani  introduced a distribution over the pseudo points and considered a joint distribution over latent representations of data from the training, testing and, pseudo data points given by $\\mathbf{f}, \\mathbf{f_*}$ and $\\mathbf{m}$. The authors then marginalized the pseudo points to obtain the posterior distribution as shown below\n$$\n\\begin{aligned}\np(\\mathbf{y} \\mid \\mathbf{X}, \\mathbf{m}) &= \\mathcal{N}\\left(\\mathbf{y} \\mid K_{nm} K_{mm}^{-1} \\mathbf{m}, \\mathbf{\\Lambda}+\\sigma^{2} \\mathbf{I}\\right) \\\\\n\\mathbf{\\Lambda} &=\\operatorname{diag}(\\boldsymbol{\\lambda}) \\\\\n\\lambda_{n} &= K_{nn}-K_{mn}^{\\top} K_{mm}^{-1} K_{mn} \\\\\np\\left(\\mathbf{f}_{*}, \\mathbf{f}\\right) &= \\int p\\left(\\mathbf{f}_{*}, \\mathbf{f}, \\mathbf{m}\\right) \\mathrm{d} \\mathbf{m} =\\int p\\left(\\mathbf{f}_{*}, \\mathbf{f} \\mid \\mathbf{m}\\right) p(\\mathbf{m}) \\mathrm{d} \\mathbf{m} \\\\\np(\\mathbf{m}) &=\\mathcal{N}\\left(\\mathbf{0}, K_{\\mathbf{m}, \\mathbf{m}}\\right)\n\\end{aligned}\n$$\nAlthough using maximum likelihood to determine the pseudo points' distribution works in practice, using maximum likelihood runs the risk of overfitting. It would be ideal to use a Bayesian approach to compute the pseudo points distribution conditioned with the training set. Unfortunately, such an approach is infeasible as it becomes intractable to analytical solutions of the pseudo points. Moreover, the approach works by assuming that the joint distribution $p\\left(\\mathbf{f}_{*}, \\mathbf{f}\\right)$ can be partitioned as follows\n$$\np\\left(\\mathbf{f}_{*}, \\mathbf{f}\\right) =\\int p\\left(\\mathbf{f}_{*}, \\mid \\mathbf{m}\\right)p\\left(\\mathbf{f} \\mid \\mathbf{m}\\right) p(\\mathbf{m}) \\mathrm{d} \\mathbf{m} \\\\\n$$\nThe assumption limits the information a GP obtains from the training set to be induced only through the pseudo set. Hence, the pseudo points are also referred to as inducing inputs. The factorization assumption limits the capacity of the model and affects the accuracy of the model. Notably, the prior distribution that is assumed for the pseudo set substantially influences the results. \nSnelson and Ghahramani  treated the pseudo points as hyperparameters and introduced a prior over them, resulting in an inaccurate posterior compared to a vanilla GP. The inaccuracy was a consequence of the formulation of the kernel approximation. Titsias  addressed the outfitting and inexact posterior issue by considering a variational approach. The approach introduces a lower bound that can be optimized to determine the inducing inputs and the kernel hyperparameters. The bound shown below, can be used to solve for the inducing points and kernel hyperparameters. We can then use the inducing points for computing the predictive distribution. \n$$\n\\begin{aligned}\n\\mathcal{F} &= \\log \\left[\\mathcal{N}\\left(\\mathbf{y} \\mid \\mathbf{0}, \\sigma^{2} I+Q_{nn}\\right)\\right]-\\frac{1}{2 \\sigma^{2}} \\operatorname{Tr}(\\widetilde{K}) \\\\\nQ_{nn} &= K_{nm} K^{-1}_{mm} K_{mn} \\\\\n\\widetilde{K} &=  K_{nn}-K_{nm} K^{-1}_{mm} K_{mn} \n\\end{aligned}\n$$\nHowever, the marginal in  didn't have the factorization required to apply stochastic gradient descent. Hensman et al.  improved on the work of Titsias  by developing a new bound that could be optimized with stochastic gradient descent. Unlike Titsias's approach, the method in  did not require the entire dataset at once to compute the variational parameters. It used the following bound that could be optimized with stochastic gradient descent. \n$$\n\\begin{aligned}\n\\mathcal{F}=\\sum_{i=1}^{n}\\{& \\log \\mathcal{N}\\left(\\mathbf{y}_{i} \\mid K_{i}^{\\top} K_{mm}^{-1} \\mathbf{v}, \\beta^{-1}\\right) \\\\\n&\\left.-\\frac{1}{2} \\beta \\widetilde{k}_{i, i}-\\frac{1}{2} \\operatorname{tr}\\left(\\mathbf{S} \\mathbf{\\Lambda}_{i}\\right)\\right\\} \\\\\n&-\\mathrm{KL}(q(\\mathbf{u}) \\| p(\\mathbf{u})) \\\\\n\\end{aligned}\n$$\n$$\n\\begin{aligned}\nq(\\mathbf{u}) &= \\mathcal{N} (\\mathbf{u} \\mid \\mathbf{v}, \\mathbf{S}) \\\\\n\\mathbf{\\Lambda} &= \\beta K^{-1}_{mm} k_i k_i^{\\top} K^{-1}_{mm}\n\\end{aligned}\n$$\nHere, $\\mathbf{u}$ is the set of feature space representations of the inducing points, an0d $k_i$ is the $i^{th}$ column of $K_{mn}$.  Hensman et al. showed the approach to scale well to large datasets while retaining the reduced model complexity of  $O(m^3)$.", "cites": [7220], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes key concepts from the cited papers, particularly linking the Nyström method, Snelson and Ghahramani's pseudo points, and Titsias and Hensman et al.'s variational approaches. It provides critical analysis by evaluating limitations such as overfitting risks, intractability, and reduced model capacity due to factorization assumptions. While it abstracts some general principles (e.g., the role of inducing inputs in scalability), it remains somewhat tied to the specific technical formulations of the methods."}}
{"id": "81f57f2c-9b98-47b8-a6ad-8cbc3942b7ca", "title": "Deep Gaussian Processes", "level": "section", "subsections": [], "parent_id": "6486154a-d131-4b97-8685-8ede70aa3f14", "prefix_titles": [["title", "Deep Gaussian Processes: A Survey"], ["section", "Deep Gaussian Processes"]], "content": "Although SGPs addressed the computation cost issue, GPs remain inapplicable to a lot of applications. The reason being the kernel function. The most commonly used kernel functions have relatively simple similarity metrics. However, in specific datasets, one might have to use a different similarity metric in different regions of the input space. A similarity metric that can extract such features would have to utilize a hierarchical structure for feature extraction. \nOne strategy to the problem would be to stack GPs similar to how Perceptrons are stacked in an MLP. But, stacking GPs such that one layer's output becomes the following layer's input makes them highly non-linear and intractable to analytical solutions. Moreover, a stacked GP would not even correspond to a GP anymore as the posterior distribution can take on any arbitrary distribution. However, such methods are usually referred to as Deep Gaussian Processes (DGPs). Several authors have tried to model and fit such models; this section explains such methods' development. \nOne of the earliest DGP approaches is that of Lawrence and Moore . They considered a GPLVM model, but a GP was assumed for the input space prior distribution, making it a two layered DGP. The DGP resulted in the following likelihood function, which cannot be marginalized analytically. \n$$\np\\left(\\mathbf{y}\\mid\\mathbf{t}\\right) =\\int p\\left(\\mathbf{y}\\mid\\mathbf{X}\\right) p\\left(\\mathbf{X}\\mid\\mathbf{t}\\right)\\mathrm{d} \\mathbf{X} \\\\\n$$\nHere, $\\mathbf{t}$ is the input GP at the input layer, and $\\mathbf{X}$ are the intermediate representations passed to the second layer GP. Lawrence and Moore considered a MAP solution to the above problem. This was achieved by maximizing the following. \n$$\np\\left(\\mathbf{X}\\mid\\mathbf{y, t}\\right) =\\log p\\left(\\mathbf{y}\\mid\\mathbf{X}\\right) + \\log p\\left(\\mathbf{X}\\mid\\mathbf{t}\\right) + \\text {const} \\\\\n$$\nThe authors also showed that deeper hierarchies could be modeled with such an approach. However, the model was limited to a MAP solution which is highly susceptible to overfitting.  \nDamianou et al.  proposed a variational approach to the overfitting problem. They also considered a 2-layered stacked GP, but a naive variational bound for such a model introduces intractabilities similar to a GPLVM.  However, the authors showed that the variational approach used for Bayesian GPLMVs in  could also be utilized to formulate a variational bound for a 2-layered GP. The final bound is shown below, with $q(\\mathbf{X})$ as the variational distribution\n$$\n\\mathcal{F}=\\hat{\\mathcal{F}}-\\operatorname{KL}(q(X) \\| p(X \\mid \\mathbf{t})) \\\\\n\\hat{\\mathcal{F}}=\\int q(X) \\log p(y \\mid \\mathbf{f}) p(\\mathbf{f} \\mid X) \\mathrm{d} X \\mathrm{~d} \\mathbf{f}\n$$\nFurthermore, Damianou and Lawrence  improved the bound shown above by generalizing the variational bound to a DGP with an arbitrary number of layers. The bound shown below can be used on a DGP with two or more layers. \n$$\n\\begin{aligned}\n\\mathcal{F}&=\\mathbf{g}_{Y}+\\mathbf{r}_{X}+\\mathcal{H}_{q(\\mathbf{X})}-\\operatorname{KL}(q(\\mathbf{Z}) \\| p(\\mathbf{Z})) \\\\\n\\mathbf{g}_{Y}&=g\\left(\\mathbf{Y}, \\mathbf{F}^{Y}, \\mathbf{U}^{Y}, \\mathbf{X}\\right) \\\\\n\\quad&=\\left\\langle\\log p\\left(\\mathbf{Y} \\mid \\mathbf{F}^{Y}\\right)+\\log \\frac{p\\left(\\mathbf{U}^{Y}\\right)}{q\\left(\\mathbf{U}^{Y}\\right)}\\right\\rangle_{p\\left(\\mathbf{F}^{Y} \\mid \\mathbf{U}^{Y}, \\mathbf{X}\\right) q\\left(\\mathbf{U}^{Y}\\right) q(\\mathbf{X})} \\\\\n\\mathbf{r}_{X}&=r\\left(\\mathbf{X}, \\mathbf{F}^{X}, \\mathbf{U}^{X}, \\mathbf{Z}\\right) \\\\\n&=\\left\\langle\\log p\\left(\\mathbf{X} \\mid \\mathbf{F}^{X}\\right)+\\log \\frac{p\\left(\\mathbf{U}^{X}\\right)}{q\\left(\\mathbf{U}^{X}\\right)}\\right\\rangle_{p\\left(\\mathbf{F}^{X} \\mid \\mathbf{U}^{X}, \\mathbf{Z}\\right) q\\left(\\mathbf{U}^{X}\\right) q(\\mathbf{X}) q(\\mathbf{Z})}\n\\end{aligned}\n$$\nHere, $\\mathbf{Y}$ represents the output multidimensional label space, $\\mathbf{Z}$ represents the latent variables in the input layer, and $\\mathbf{X}$ represents the latent inputs in intermediate layers. $\\mathbf{U}$ and $\\mathbf{F}$ are the values of the latent function corresponding to inducing points and latent inputs, respectively; their superscript denotes the layer to which they belong. Additionally, $\\mathcal{H}$ represents the entropy of the distribution shown in its subscript, and $\\operatorname{KL}$ is the standard KL-divergence. Fig.~\\ref{fig:dgp} shows the DGP model architecture from . \n\\begin{figure}[htp]\n    \\centering\n    \\includegraphics[width=0.45\\textwidth]{figs/DGP.pdf}\n    \\caption{Deep Gaussian Process model overview .}\n    \\label{fig:dgp}\n\\end{figure}\nAgain, the crux of the approach relied on the variational trick of introducing inducing points as presented in . Damianou and Lawrence conducted experiments on the MNIST dataset wherein they showed a 5-layer DGP could be used for the image classification task. \nA limitation of the approach in  is that the number of variational parameters that need to be learned increases linearly with the number of data points in the training set. And it involved inverting matrices which is a computationally expensive operation, thereby limiting its scalability. Dai et al.  addressed this by introducing a back-constraint. The constraint allowed them to define the latent variables' mean terms as deterministic functions of the latent variables themselves via an MLP. The approach reduced the number of variational parameters. Moreover, Dai et al.  also showed that their approach could be trained in a distributed manner, allowing the model to be scaled to large datasets.\nSalimbeni and Deisenroth  recently proposed an approach that addressed the layer independence issue of prior DGP approaches. The DGP in  assumed independence of GPs across layers and only considered the correlations within a layer. However, Salimbeni and Deisenroth argue that such an approach is equivalent to a single GP with each GP's input coming from a GP itself. The authors also stated that they found that some layers get turned off when using the DGP in . \nSalimbeni and Deisenroth  presented a new variational bound that retains the exact model posterior similar to  while maintaining the correlations both within and across adjacent layers. However, Salimbeni and Deisenroth showed such an approach is infeasible to analytic computations, but the bound could still be optimized with MCMC sampling techniques. Such an approach is computationally expensive. But, it can be parallelized by exploiting the factorization of the DGP across output dimensions. Furthermore, the method also required sampling methods during inference, but its performance is substantially better than prior works.  \n$$\n\\mathcal{F}=\\sum_{i=1}^{N} \\mathbb{E}_{q\\left(\\mathbf{f}_{i}^{L}\\right)}\\left[\\log p\\left(\\mathbf{y}_{n} \\mid \\mathbf{f}_{n}^{L}\\right)\\right]-\\sum_{l=1}^{L} \\mathrm{KL}\\left[q\\left(\\mathbf{U}^{l}\\right) \\| p\\left(\\mathbf{U}^{l} ; \\mathbf{Z}^{l-1}\\right)\\right]\n$$\nIn the optimization bound shown above from , the subscripts are used to denote each data sample in the dataset, and superscripts are used to indicate a layer in the DGP. The rest of the terms follow the same convention as the one used by Damianou et al. .\nI briefly mentioned that DGPs do not necessarily correspond to Gaussian processes. Still, the methods discussed so far do model the posterior distribution as a Gaussian, each with its assumptions. Havasi et al.  presented a technique that departs even more from a conventional GP. The authors show that since a Gaussian is uni-modal, using it to model the posterior will result in poor results. Instead, they suggest using a multi-modal distribution that can better capture the true posterior distribution.  \nHowever, it is not possible to formulate an analytical solution to a multi-modal posterior. We can use variational inference to learn a multi-modal posterior. Still, one needs to determine the exact form of the variational distribution, which is difficult as we usually don't know the posterior distribution in advance. Havasi et al.  circumvent this issue by using Stochastic Gradient Hamiltonian Monte Carlo (SGMCMC)  method to estimate the posterior. The approach can determine the inducing points by sampling from the true posterior instead of using a variational distribution.  \nAlthough the approach far exceeds the performance of prior DGPs and is the current state-of-the-art, it still has its limitations. Remarkably, the SGMCMC method is difficult to tune as it introduces its own parameters in addition to the ones that already have to be estimated for DGP. Several MCMC method variants attempt to improve upon SGMCMC, but none of those approaches have been applied to DGPs. \nThe DGPs we discussed so far attempted to develop variants of GPs that can model hierarchical features in data, which was done by assuming a feed forward network with each node of the network being modeled as a GP. It is the most popular method to address the problem and has resulted in approaches that can get reasonably promising results. However, there are other approaches as well which do not consider such an explicit feed forward network.\nWilson et al.  proposed a method that uses deep neural networks as the kernel function, termed deep kernel. Unlike a Gaussian kernel, the deep kernel produced a vector output, and a GP was assigned to each of the vector elements. Wilson et al. further combined the GPs with an additive structure that facilitated its training with an analytical bound. \nWilson et al.  showed that their approach was good at several tasks. However, the deep neural network architecture needs to be task specific, and its parameters are susceptible to overfitting given its large parameter count.  \nAnother interesting perspective on DGPs was presented by Lee et al. . Until now, all the discussed GPs with linear latent functions are combined in different ways to achieve an aggregate non-linear latent function space. Lee et al. developed a method that considers an entire function space that consists of non-linear functions. Unlike prior approaches, the function space was not constrained to a specific subspace as a consequence of a particular kernel function being used. The approach can be regarded as a generalization of Neil  who showed the equivalence of an infinitely wide single layer neural network to a GP. Lee et al. showed the equivalence of a GP to an infinitely wide deep neural. \nLee et al.  showed the approach was on par with some neural networks trained with gradient descent while retaining its uncertainty estimation. Furthermore, the uncertainty estimates were proportional to the model accuracy. But, the approach has a polynomial increasing kernel matrix making it infeasible for some problems. Moreover, the approach only considered deep neural networks with fully connected layers and Relu activation functions.   \nGarnelo et al.  presented an approach with similar ethos and introduced Neural Processes (NPs). However, instead of considering a neural network with asymptotically increasing depth and width, a deep neural network was used in place of a Gaussian distribution parametrized by a kernel function to define $p(\\mathbf{f} \\mid \\mathbf{X})$. \n$$\n\\begin{aligned}\np(\\mathbf{y} \\mid \\mathbf{f}) &= \\mathcal{N}\\left(\\mathbf{f}, \\beta^{-1} \\mathbf{I}\\right) \\\\\np(\\mathbf{y} \\mid \\mathbf{X}) &= \\int p(\\mathbf{y} \\mid \\mathbf{f})\\underbrace{p(\\mathbf{f} \\mid \\mathbf{X})}_\\text{Deep neural network} d \\mathbf{f}\n\\end{aligned}\n$$\nThe deep neural network was trained using amortized variational inference. The consequence of such an approach is that the function space defined by deep neural networks allows us to extract hierarchical features and retain a probabilistic interpretation. The model, however, needs to be trained with Meta-learning, an approach wherein multiple diverse datasets or tasks are used to train the same model. Meta-learning is used because each function in the function space corresponds to a sequence of inputs or a task. Considering multiple tasks allows the DNN to approximate the variability of the function space. While training, a context vector $\\mathbf{r}_c$ is passed to the DNN to indicate the task currently being considered as shown in Fig:~\\ref{fig:NPs}. \n\\begin{figure}[htp]\n    \\centering\n    \\includegraphics[width=0.6\\linewidth]{figs/NPs.pdf}\n    \\caption{Neural Processes model architecture .}\n    \\label{fig:NPs}\n\\end{figure}\nMoreover, to retain the probabilistic interpretation, a latent variable $z$ was introduced, which captured uncertainty in the context data. The implication being that, unlike vanilla GPs whose uncertainty comes from the kernel function and its function space, NPs do this with data. As such, the context being provided could significantly influence the model's performance and can be considered to be similar to inducing points in SGPs. \nAdditionally, the model does not assume a Gaussian prior or posterior, allowing one to fit any data distribution. Garnelo et al. showed that their approach produces good predictive distributions while being parameter efficient and fast compared to a vanilla Gaussian process.\nNonetheless, the method assumed a predefined DNN model architecture which needs to be task specific. Also, the model is only an approximation to some stochastic process using a DNN. But, it isn't possible to guarantee the approximation quality of the DNN. Furthermore, the meta-learning requirement imposes a large training computation requirement, and the datasets considered have to be similar to the primary dataset of interest. \nFinally, Yang et al. recently proposed an Energy-based Process (EBP). EBPs are a generalization of Neural Processes as they utilize Energy-based models  to approximate $p(\\mathbf{f} \\mid \\mathbf{X})$ instead of a MAP trained DNN as shown below, where $f_w$ is the Energy model and $Z$ is the partition function. \n$$\np(\\mathbf{f} \\mid \\mathbf{X}) = \\frac{\\exp (f_w(\\mathbf{X}))}{Z(f_w)}\n$$\nHowever, by utilizing an Energy based model, the authors were able to show that vanilla GPs and NPs can be recovered from EBPs as special cases. The Energy based formulation also allows one to approximate the conditional $p(\\mathbf{f} \\mid \\mathbf{X})$ with arbitrary distributions, unlike GP and NPs, which are limited to Gaussian distributions and the distribution defined by the DNN, respectively. \nUnlike feedforward networks which are trained to predict a label $y$ given an input  $\\mathbf{X}$, Energy based models predict the Energy between a pair of $(\\mathbf{X}, y)$. A well trained Energy-based will output low Energy for well matched $(\\mathbf{X}, y)$ pairs and high Energy for mismatched pairs. As such, the prediction task in these models becomes a minimization task, wherein one needs to find the label $y$, which has low Energy for the given data $\\mathbf{X}$. \nThe consequence of training such a model to approximate our conditional in the stochastic process is that the function space is not constrained to any predefined subspace. However, Energy based models are challenging to train and require several tricks to stabilize the training process. Furthermore, it takes longer to train such models similar to models trained with Meta-learning.", "cites": [7224, 7221, 363, 7222, 7223], "cite_extract_rate": 0.3125, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several key DGP papers effectively, connecting the evolution of methods from MAP-based approaches to variational bounds and multi-modal posteriors. It includes some critical analysis, especially of limitations like scalability and intractability. While it identifies trends in how different works address layer independence and parameter proliferation, it remains somewhat focused on the mathematical formulations of individual approaches rather than offering broader, meta-level insights."}}
{"id": "cbf4d6cf-6b44-4200-9536-7a8dc186c6a4", "title": "Discussion", "level": "section", "subsections": [], "parent_id": "6486154a-d131-4b97-8685-8ede70aa3f14", "prefix_titles": [["title", "Deep Gaussian Processes: A Survey"], ["section", "Discussion"]], "content": "Gaussian processes have come a long way from their origins. Although many of the limitations have been addressed, there remain open problems and research directions that have not been thoroughly explored. \nOne such problem is the assumption of factored output dimensions. The assumption has been made in all the methods mentioned in this paper.  It dictates that each output dimension is independent of the other. The assumption allowed factorizations that simplify some derivations, and in some cases, the assumption is required for the method to be tractable. However, the assumption might not hold in some datasets. Addressing this factorization assumption would be an interesting line of research. \nAnother issue is that most SGP and DGP methods require careful model initializations and hyperparameter tuning without which, the model does not converge. Nevertheless, there aren't any formalized rules for determining model initializations and hyperparameters for most methods that could guarantee good model convergence. The tuning issue is particularly a problem when using MCMC approaches and remains to be solved. \nAdditionally, MCMC methods have been shown to be successful at training DGP. But, the method need not be limited to DGP. Indeed, the technique might result in good results even for SGP. The primary motivation of using MCMC methods was to address the non-Gaussian posterior. Although vanilla GPs might not have a non-Gaussian posterior, the assumptions made in inducing methods often change this. As such, it might be worth exploring the feasibility of MCMC for training SGPs.  \nSimilarly, there are several variants of SG-MCMC methods that have not been benchmarked against DGPs. Havasi et al.  only considered the original SGMCMC   approach. However, numerous variants of the method have been introduced that improve upon SGMCMC, some of which might result in stable training dynamics.\nDeep kernels in themselves were found to be highly susceptible to overfitting. However, Wilson et al.  only considered vanilla DNNs. But, DNNs could be used as Bayesian approximators as shown by Gal and Ghahramani . Such an approach might mitigate some of the overfitting issues. 'Furthermore, one might also consider methods such as Bayes by Backprob  to train deep kernels. It would be interesting to find out the ramifications of such an approach to deep kernels. \nGarnelo et al.  considered a similar approach but, they consider a DNN approximation to a distribution over the function space itself. It required an explicit definition of a DNN, which needs to be task specific. Also, the model performance is dependent on the context vector to estimate prediction uncertainty which isn't consistent with a proper stochastic process. Kim et al.  introduced a variant of Neural Processes which uses attention to improve the context vectors. Perhaps we could modify the attention mechanism to take the test data into account and generate an uncertainty estimate that incorporates test data.\nMoreover, most methods assume relatively constrained function space, either with the formulation of a kernel function or a DNN. However, that need not be the case; perhaps we can consider multiple function spaces by utilizing models such as chunked hypernetworks  to generate both the model parameters and the model architecture. Thus greatly expanding the stochastic process's modeling capacity. \nEnergy-based models appear to be another viable approach to expanding the function space but, the method is challenging to train and incurs substantial computational costs. Moreover, even model inference is an expensive operation requiring Hamiltonian Markov Chain methods for sampling. \nFinally, there is the issue of scalability. Although some DGP methods have been shown to scale well to large datasets, they have not been thoroughly benchmarked on highly structured datasets such as Imagenet . The problem lies in the model depth required to achieve good performance on such a dataset. Unlike MNIST, Imagenet requires DNNs that are substantially deeper. DGPs, however, are usually only tested on models with up to ten layers. It would be indispensable to study and understand how DGPs scale to such a dataset.", "cites": [7224, 7221, 7223], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by integrating ideas from multiple papers (e.g., Neural Processes, Stochastic Gradient Hamiltonian Monte Carlo) and connecting them to broader themes in DGP research. It also shows critical analysis by pointing out limitations such as model initialization, scalability, and assumptions of independence in output dimensions. While it does not reach the highest level of abstraction, it identifies patterns in modeling approaches and suggests ways to generalize them (e.g., using attention mechanisms or hypernetworks)."}}
{"id": "cf9221b2-05ad-498a-bf27-3fcff7c2e0b9", "title": "Conclusion", "level": "section", "subsections": [], "parent_id": "6486154a-d131-4b97-8685-8ede70aa3f14", "prefix_titles": [["title", "Deep Gaussian Processes: A Survey"], ["section", "Conclusion"]], "content": "Gaussian Processes in it themselves are fascinating. Their non-parametric form, analytical properties, and ability to model uncertainty are coveted in machine learning. However, they are plagued with limitations, particularly their significant computation and storage cost. Also, conventional kernel functions restrict the family of functions that a GP could model. \nSparse Gaussian Processes attempt to address the storage and computation cost. One dominant approach to SGPs is to use the Nyström approximation. The approach entails using variational methods to model the distribution of pseudo points for a fully Bayesian treatment. Several methods have been proposed along this line of research, each with its advantages and limitations. \nFurthermore, GPLVM was a step towards DGP. However, hierarchical feature representation was not the intended use case. It was proposed as an approach for probabilistic PCA and unsupervised learning. The Bayesian GPLVM on improved upon the original method by introducing a purely Bayesian training approach. BGPLVMs facilitated the propagation of latent space uncertainty to the posterior, thereby establishing a technique to propagate uncertainties through non-linearities in GPs.\nMost of the DGP approaches considered SGPs and GPLVMs to address the issue of hierarchical feature representation. The primary trend in DGPs is to stack GPs in a feedforward manner and train them with approaches used to train SGPs and GPLVMs. However, such an approach has its limitations. The optimization bounds that were developed were not always tight, and some methods were limited to analytical solutions, which imposed scalability limitations on such techniques. \nMoreover, stacking GPs makes the model parametric as it requires a predefined model depth and layer width. Lee et al.  considered these issues and attempted to solve them by modeling the latent function space as the space of deep neural networks. But, the approach is not feasible for real world applications as of yet and requires more work to be done to get there.\nGarnelo et al.  consider a stochastic process parametrized with a DNN instead of a Gaussian distribution with a kernel function to define the latent function space. Still, the approach requires modeling a task specific neural network and is only an approximation to an unknown stochastic process.  Energy-based Processes address this limitation, but the method isn't mature enough as of yet. \nIn conclusion, GPs are an excellent approach to model datasets. The overall trend of the field seems to be transitioning away from the Gaussian assumption and consider general Stochastic processes. The method has come a long way from its infancy, but there are still open problems that need to be addressed for it to be elevated to the prominence that it deserves. \n\\bibliographystyle{apalike}\n\\bibliography{references}\n\\end{document}", "cites": [7224], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of Deep Gaussian Processes, synthesizing key ideas from related works like SGPs, GPLVMs, and Neural Processes. It critically evaluates the limitations of different approaches and highlights broader trends such as the shift from Gaussian to general stochastic processes. While it identifies patterns and overarching principles, it does not offer a highly novel or meta-level synthesis of the field."}}
