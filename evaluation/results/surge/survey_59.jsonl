{"id": "6ecdbe4c-e0e7-4e29-8a26-046dc4d0c747", "title": "Web Crawlers and Crawling Techniques", "level": "subsection", "subsections": ["67acc0b7-9438-41f9-9c56-86de667bf252", "9a498a3a-0dce-4a45-a81f-908514bf4328", "6d540a9d-eac6-457d-ae28-03ea3e8a1ddc"], "parent_id": "e390f1b3-f7c1-42f4-b37a-c259597b6e50", "prefix_titles": [["title", "Change Detection and Notification of Web Pages: A Survey"], ["section", "Detecting Changes of Webpages"], ["subsection", "Web Crawlers and Crawling Techniques"]], "content": "A web crawler (also known as a spider) is \"a software or a programmed script that browses the WWW in a systematic, automated manner\"~, and systematically downloads numerous webpages starting from a seed URL~. Web crawlers date back to the 1990s, where they were introduced when the WWW was invented. The World Wide Web Worm~ and MOMspider~ were among the early web crawlers. Moreover, commercial search engines developed their own web crawlers as well such as Google crawler~ and  AltaVista~. Later on, web crawlers that could efficiently download millions of webpages were built. \nWe can consider the Internet to be a \"directed graph\" where each node represents a webpage, and the edges represent hyperlinks connecting these webpages~. Web crawlers traverse over this graph-like structure of the Internet, go to webpages, and download their content for indexing purposes. It is crucial to identify which crawling technique should be used according to the purpose of the application.\nWeb crawling can be considered as the main process behind web caching, search engines and web information retrieval. A web crawler begins crawling from a seed URL and visits pages. Then it downloads the page, and retrieves the URLs in it. The discovered URLs will be kept in a queue, and this process repeats as the crawler travels from page to page. Figure~\\ref{fig7:web-crawling} shows an overview of the web crawling process. Many crawling techniques are being used by web crawlers at present~ such as (1) general-purpose crawling, (2) focused crawling and (3) distributed crawling.\n\\begin{figure}[!ht]\n    \\centering\n    \\includegraphics[width=0.9\\textwidth]{images/Web_crawling_process.png}\n    \\caption{Overview of the web crawling process.}\n    \\label{fig7:web-crawling}\n\\end{figure}", "cites": [338], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic overview of web crawlers and their role in CDN systems, mentioning several examples and a general process. However, it lacks synthesis of multiple cited papers, does not compare or critically evaluate approaches, and offers minimal abstraction beyond specific systems. The section reads more as a factual summary rather than a deeper analytical discussion."}}
{"id": "6d540a9d-eac6-457d-ae28-03ea3e8a1ddc", "title": "Distributed Crawling", "level": "subsubsection", "subsections": [], "parent_id": "6ecdbe4c-e0e7-4e29-8a26-046dc4d0c747", "prefix_titles": [["title", "Change Detection and Notification of Web Pages: A Survey"], ["section", "Detecting Changes of Webpages"], ["subsection", "Web Crawlers and Crawling Techniques"], ["subsubsection", "Distributed Crawling"]], "content": "This method uses multiple processes to crawl webpages and download their content. Web crawlers are distributed over different systems, allowing them to operate independently. Distributed crawling techniques were introduced due to the inherent problems faced by centralized crawling solutions, such as reduced throughput of the crawl and link congestion~. \nFigure~\\ref{fig9:distributed-crawl} denotes the general arrangement of components in a distributed crawling system. The crawlers are distributed across different systems, and they crawl webpages in different parts of the Web. The Web can be partitioned using different graph partitioning algorithms, and each crawler is provided with a seed URL to start the crawling process~. All the crawled content is then aggregated and sent to the main server. This content can be processed within the main server or they can be sent to other services for different purposes such as indexing and version comparison within CDN systems. One of the most popular high-performance distributed crawlers found at present is the Googlebot~. Googlebot is the web crawler used by Google to fetch webpages for indexing. It is designed to be distributed on several machines so that the crawler can scale as the Web grows. Table~\\ref{tab3:crawling-techniques} summarizes the three main crawling techniques with their advantages, disadvantages and comparison of features.\n\\input{tables/table3.tex}\nDetection of crashed agents is one of the important concerns of distributed web crawlers. Several distributed crawler solutions have been presented during the past~. To address this concern, crawlers can be designed as reliable failure detectors~, in which timeouts can be used to detect crashed agents. UbiCrawler~ is an example of a web crawler with a reliable failure detector. It is a distributed web crawler, which consists of several agents that are responsible to crawl their own share of the Web. However, it does not guarantee that the same webpage is not visited more than once. Based on the experience with UbiCrawler, the authors have built BUbiNG~, a fully distributed web crawler which can detect (near-)duplicate webpages based on the content. However, it does not support URL prioritization. \nAnother aspect that has drawn the attention of researchers is the efficient partitioning mechanisms of the Web space. Work done by Exposto et al.~ has presented a multi-objective approach for partitioning the Web space by modeling the Web hosts and IP hosts as graphs. These graphs are partitioned, and a new graph is created with the weights calculated using the original weights and the edge-cuts. Results show that efficient partitioning techniques have improved download times, exchange times and relocation times.\nKc et al.~ have introduced LiDi Crawl (which stands for Lightweight Distributed Crawler), which is a centralized crawling application with limited resources. It consists of a central node and several individual crawling components. The distributed crawling nature results in the reduced dependence on expensive resources. Kumar and Neelima~ have proposed a scalable, fully-distributed web crawler, without a central node. It consists of multiple agents, where each agent is coordinated so that they crawl their own region on the Web. An agent runs several threads, where each thread is made responsible to visit a single host. The main objective of having multiple agents is to break down the task of crawling into separate parts, and allow specialized modules to carry them out efficiently. Anjum et al.~ state that web crawlers should be aware of webpage modifications, and have discussed techniques to retrieve information on such modifications. However, the authors have found that the presence of multiple JavaScript and CSS files can reduce the efficiency of certain retrieval techniques. \nEfficient crawling of Rich Internet Applications (RIA) is an open problem as RIAs consist of many characteristics such as, the use of JavaScript and browser plugins, which makes the crawling process complex. Model-Based Crawling~ was introduced to determine an optimal crawling strategy for RIA. An extension of this model is presented by Dincturk et al.~, which uses a statistical approach in determining a crawling strategy. The recent work carried out by Mirtaheri et al.~, intends to lower the time taken to crawl RIAs by introducing Dist-RIA crawler, which is a distributed crawler to crawl RIAs in parallel. However, it assumes that all the nodes have equal processing power, and assigns an equal number of tasks to each node. This can be problematic when there is heterogeneity.\n\\input{tables/table4.tex}\nMulti-Threaded Crawler for Change Detection of Web (MTCCDW) has been introduced by Meegahapola et al.~ to optimize the change detection process of webpages. Many threads were used to carry out the tasks of (1) \"retrieving current versions of webpages\", (2) \"retrieving old versions from a version repository\" and (3) \"comparison of the two versions to detect the changes\"~. Two thread-safe queues were used in between these three tasks to optimize them. The authors have determined the optimum number of threads to be used in each of these tasks, to ensure that the CDN system works optimally without idling. This multi-threading-based algorithm differs from standard process optimization tasks because of the I/O operations which are involved in fetching a webpage from the Web. Hence this algorithm provides a more significant improvement in processing times over doing I/O operations and processing in a central processing unit. Table~\\ref{tab4:crawling-solutions} summarizes the various crawling solutions presented in this subsection.", "cites": [338], "cite_extract_rate": 0.07142857142857142, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.3, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates information from multiple works (e.g., BUbiNG, UbiCrawler, LiDi Crawl, MTCCDW) to describe the concept and evolution of distributed crawling. It provides a coherent narrative on the components and goals of distributed systems and mentions comparative aspects like performance improvements and limitations. However, it lacks deeper synthesis into a novel framework and offers only moderate critique or generalization of broader principles."}}
