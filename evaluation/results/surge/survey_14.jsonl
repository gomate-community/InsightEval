{"id": "e8f02677-6db8-44e4-b098-93f39d760cfb", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "8b43103e-4d72-400c-b8bb-f2836ed57923", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Introduction"]], "content": "\\label{sec:intro}\nIn recent years, deep learning techniques have achieved remarkable successes on various tasks, including game-playing , image recognition , machine translation , and automatic classification in biomedical domains . Despite these advances and recent solutions , ample challenges remain to be solved, such as the large amounts of data and training that are needed to achieve good performance. These requirements severely constrain the ability of deep neural networks to learn new concepts quickly, one of the defining aspects of  human intelligence .\n\\textit{Meta-learning} has been suggested as one strategy to overcome this challenge . The key idea is that meta-learning agents improve their learning ability over time, or equivalently, learn to learn. The learning process is primarily concerned with tasks (set of observations) and takes place at two different levels: an inner- and an outer-level. At the \\textit{inner-level}, a new task is presented, and the agent tries to quickly learn the associated concepts from the training observations. This quick adaptation is facilitated by  knowledge that it has accumulated across earlier tasks at the \\textit{outer-level}. Thus, whereas the inner-level concerns a single task, the outer-level concerns a multitude of tasks. \n\\begin{figure}[tb]\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/newplot.pdf}\n    \\caption{The accuracy scores of the covered techniques on 1-shot miniImageNet classification. The used feature extraction backbone is displayed on the x-axis. As one can see, there is a strong relationship between the network complexity and the classification performance.}\n    \\label{fig:depthandperformance}\n\\end{figure}\nHistorically, the term meta-learning has been used with various scopes. In its broadest sense, it encapsulates all systems that leverage prior learning experience in order to learn new tasks more quickly . This broad notion includes more traditional algorithm selection and hyperparameter optimization techniques for Machine Learning . In this work, however, we focus on a subset of the meta-learning field which develops meta-learning procedures to learn a good \\textit{inductive bias} for (deep) neural networks.\\footnote{Here, inductive bias refers to the assumptions of a model which guide predictions on unseen data .} Henceforth, we use the term \\textit{Deep Meta-Learning} to refer to this subfield of meta-learning. \nThe field of Deep Meta-Learning is advancing at a quick pace, while it lacks a coherent, unifying overview, providing detailed insights into the key techniques.  has surveyed meta-learning techniques, where meta-learning was used in the broad sense, limiting  its account of Deep Meta-Learning techniques. Also, many exciting developments in deep meta-learning have happened after the survey was published. A more recent survey by  adopts the same notion of deep meta-learning as we do, but aims to give a broad overview, omitting technical details of the various techniques.\nWe attempt to fill this gap by providing detailed explications of contemporary Deep Meta-Learning techniques, using a unified notation. \nMore specifically, we cover modern techniques in the field for supervised and reinforcement learning, that have achieved state-of-the-art performance, obtained popularity in the field, and presented novel ideas. \nExtra attention is paid to MAML , and related techniques, because of their impact on the field.\nWe show how the techniques relate to each other, detail their strengths and weaknesses,  identify current challenges, and provide an overview of promising future research directions.\nOne of the observations that we make is that the network complexity is highly related to the few-shot classification performance (see \\autoref{fig:depthandperformance}).\nOne might expect that in a few-shot setting, where only a few examples are available to learn from, the number of network parameters should be kept small to prevent overfitting. \nClearly, the figure shows that this does not hold, as techniques that use larger backbones tend to achieve better performance. \nOne important factor might be that due to the large number of tasks that have been seen by the network, we are in a setting where similarly large amounts of observations have been evaluated.\nThis result suggests that the size of the network should be taken into account when comparing algorithms.\nThis work can serve as an educational introduction to the field of Deep Meta-Learning, and as reference material for  experienced researchers in the field. Throughout, we will adopt the taxonomy used by , which identifies three categories of Deep Meta-Learning approaches: i)~metric-based, ii)~model-based, and iii)~optimization-based meta-learning techniques.   \nThe remainder of this work is structured as follows. Section 2 builds a common foundation on which we will base our overview of Deep Meta-Learning techniques. Sections 3, 4, and 5 cover the main metric-, model-, and optimization-based meta-learning techniques, respectively. Section 6 provides a helicopter view of the field and summarizes the key challenges and open questions. \\autoref{tab:notation} gives an overview of notation that we will use throughout this paper. \n\\begin{table}[tb]\n    \\begin{tabularx}{\\linewidth}{ll}\n    \\toprule\n         Expression & Meaning  \\\\\n         \\midrule\n         Meta-learning & Learning to learn \\\\\n         $\\Tau_{j} = (D^{tr}_{\\Tau_{j}}, D^{test}_{\\Tau_{j}})$& A task consisting of a labeled support and query set \\\\ \n         Support set & The train set $D^{tr}_{\\Tau_{j}}$ associated with a task $\\Tau_{j}$ \\\\\n         Query set & The test set $D^{test}_{\\Tau_{j}}$ associated with a task $\\Tau_{j}$ \\\\\n         $\\boldsymbol{x}_{i}$ & Example input vector $i$ in the support set \\\\\n         $y_{i}$ & (One-hot encoded) label of example input $\\boldsymbol{x}_{i}$ from the support set\\\\\n         $k$ & Number of examples per class in the support set \\\\\n         $N$ & Number of classes in the support and query sets of a task \\\\\n         $\\boldsymbol{x}$ & Input in the query set \\\\\n         $y$ & A (one-hot encoded) label for input $\\boldsymbol{x}$ \\\\\n         $(f/g/h)_{\\circ}$ & Neural network function with parameters $\\circ$ \\\\\n         Inner-level & At the level of a single task \\\\\n         Outer-level & At the meta-level: across tasks \\\\\n         Fast weights & A term used in the literature to denote task-specific parameters \\\\\n         Base-learner & Learner that works at the inner-level \\\\\n         Meta-learner & Learner that operates at the outer-level \\\\\n         $\\boldsymbol{\\theta}$ & The parameters of the base-learner network \\\\\n         $\\mathcal{L}_D$ & Loss function with respect to task/dataset $D$ \\\\\n         Input embedding & Penultimate layer representation of the input \\\\\n         Task embedding & An internal representation of a task in a network/system \\\\\n         SL & Supervised Learning \\\\\n         RL & Reinforcement Learning \\\\\n         \\bottomrule\n    \\end{tabularx}\n    \\caption{Some notation and meaning, which we use throughout this paper.}\n    \\label{tab:notation}\n\\end{table}", "cites": [207, 7109, 3205, 5941, 1695, 620, 7612], "cite_extract_rate": 0.30434782608695654, "origin_cites_number": 23, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The introduction synthesizes key ideas from multiple cited papers to establish a clear and focused definition of Deep Meta-Learning and its importance. It critically evaluates prior surveys, noting their limitations in scope and technical detail. The section abstracts the core concepts of meta-learning, such as inductive bias and inner/outer-level learning, while also identifying broader trends like the relationship between network complexity and performance."}}
{"id": "020a123f-4a2f-4110-a4fe-ccb5fc89a08f", "title": "Regular Supervised Learning", "level": "subsubsection", "subsections": [], "parent_id": "cbaad429-d670-471a-861b-fd7642becbc6", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Foundation"], ["subsection", "The Meta Abstraction"], ["subsubsection", "Regular Supervised Learning"]], "content": "In \\textit{supervised learning}, we wish to learn a function $f_{\\boldsymbol{\\theta}}: X \\rightarrow Y$ that learns to map inputs $\\boldsymbol{x}_{i} \\in X$ to their corresponding outputs $y_{i} \\in Y$. Here, $\\boldsymbol{\\theta}$ are model parameters (e.g.\\ weights in a neural network) that determine the function's behavior. To learn these parameters, we are given a data set of $m$ observations: $D = \\{(\\boldsymbol{x}_{i}, y_{i})\\}_{i=1}^{m}$. Thus, given a data set $\\mathcal{D}$, learning boils down to finding the correct setting for $\\boldsymbol{\\theta}$ that minimizes an empirical loss function $\\mathcal{L}_{D}$, which must capture how the model is performing, such that appropriate adjustments to its parameters can be made. In short, we wish to find \n\\begin{align}\n    \\boldsymbol{\\theta}_{SL} := \\argmin_{\\boldsymbol{\\theta}} \\, \\mathcal{L}_{D}(\\boldsymbol{\\theta}),\n\\end{align}\\label{eq:suplearning}where $SL$ stands for ``supervised learning\". Note that this objective is specific to data set $\\mathcal{D}$, meaning that our model $f_{\\boldsymbol{\\theta}}$ may not \\textit{generalize} to examples outside of $\\mathcal{D}$. To measure generalization, one could evaluate the performance on a separate test data set, which contains unseen examples. A popular way to do this is through \\textit{cross-validation}, where one repeatedly creates train and test splits $D^{tr}, D^{test} \\subset D$ and uses these to train and evaluate a model respectively . \nFinding globally optimal parameters $\\boldsymbol{\\theta}_{SL}$ is often computationally infeasible. We can, however, approximate them,  guided by \\textit{pre-defined} meta-knowledge $\\omega$ , which includes, e.g., the initial model parameters $\\boldsymbol{\\theta}$, choice of optimizer, and learning rate schedule. As such, we approximate \n\\begin{align}\n    \\boldsymbol{\\theta}_{SL} \\approx g_{\\omega}(D, \\mathcal{L}_{D}),\n\\end{align}\nwhere $g_{\\omega}$ is an optimization procedure that uses \\textit{pre-defined} meta-knowledge $\\omega$, data set $\\mathcal{D}$, and loss function $\\mathcal{L}_{D}$, to produce updated weights $g_{\\omega}(D, \\mathcal{L}_{D})$ that (presumably) perform well on $\\mathcal{D}$.", "cites": [7109], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear and factual description of supervised learning and its optimization process, referencing meta-knowledge from a single paper. While it introduces relevant concepts like generalization and cross-validation, it lacks integration across multiple sources and does not critically evaluate or compare approaches. Some abstraction is present in formulating the learning objective as a function of data and meta-knowledge, but the overall insight remains at a descriptive level."}}
{"id": "a874b77b-8e60-4950-a14c-1deb27344df9", "title": "Regular Reinforcement Learning", "level": "subsubsection", "subsections": [], "parent_id": "cbaad429-d670-471a-861b-fd7642becbc6", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Foundation"], ["subsection", "The Meta Abstraction"], ["subsubsection", "Regular Reinforcement Learning"]], "content": "\\label{sec:RL}\nIn \\textit{reinforcement learning}, we have an agent that learns from  experience. That is, it interacts with an environment, modeled by a Markov Decision Process (MDP) $M = (S, A, P, r, p_{0}, \\gamma, T)$. Here, $S$ is the set of states, $A$ the set of actions, $P$ the transition probability distribution defining $P(s_{t+1}| s_{t}, a_{t})$, $r: S \\times A \\rightarrow \\mathbb{R}$ the reward function, $p_{0}$ the probability distribution over initial states, $\\gamma \\in [0,1]$ the discount factor, and $T$ the time horizon (maximum number of time steps) . \nAt every time step $t$, the agent finds itself in state $s_{t}$, in which the agent performs an action $a_{t}$, computed by a policy function $\\pi_{\\boldsymbol{\\theta}}$ (i.e., $a_{t} = \\pi_{\\boldsymbol{\\theta}}(s_{t})$), which is parameterized by weights $\\boldsymbol{\\theta}$. In turn, it receives a reward $r_{t} = r(s_{t}, \\pi_{\\boldsymbol{\\theta}}(s_{t})) \\in \\mathbb{R}$ and a new state $s_{t+1}$. This process of interactions continues until a termination criterion is met (e.g.\\ fixed time horizon $T$ reached). The goal of the agent is to learn how to act in order to maximize its expected reward. The reinforcement learning (RL) goal is to find \n\\begin{align}\n    \\boldsymbol{\\theta}_{RL} := \\argmin_{\\boldsymbol{\\theta}} \\, \\mathbb{E}_{\\mbox{traj}} \\sum_{t=0}^{T} \\gamma^{t}r(s_{t}, \\pi_{\\boldsymbol{\\theta}}(s_{t})),\n\\end{align}\nwhere we take the expectation over the possible \\textit{trajectories} $\\mbox{traj} = (s_{0}, \\pi_{\\boldsymbol{\\theta}}(s_{0}), \\allowbreak \\ldots s_{T}, \\pi_{\\boldsymbol{\\theta}}(s_{T}))$ due to the random nature of MDPs . Note that $\\gamma$ is a hyperparameter that can prioritize short- or long-term rewards by decreasing or increasing it, respectively.\nAlso in the case of reinforcement learning it is often infeasible to find  the global optimum $\\boldsymbol{\\theta}_{RL}$, and thus we  settle for  approximations. In short, given a learning method $\\omega$, we approximate \n\\begin{align}\n    \\boldsymbol{\\theta}_{RL} \\approx g_{\\omega}(\\Tau_{j}, \\mathcal{L}_{\\Tau_{j}}),\n\\end{align}\nwhere again $\\Tau_{j}$ is the given MDP, and $g_{\\omega}$ is the optimization algorithm, guided by pre-defined meta-knowledge $\\omega$.\nNote that in a Markov Decision Process (MDP), the agent knows the state at any given time step $t$. When this is not the case, it becomes a Partially Observable Markov Decision Process (POMDP), where the agent receives only observations $O$, and uses these to update its belief with regard to the state it is in .", "cites": [8050], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear and factual description of reinforcement learning and its mathematical formulation, but it does not go beyond summarizing the standard RL framework. It references one paper (RLÂ²) in the abstract, but does not integrate its ideas or compare it with other works in the context of deep meta-learning. There is minimal critical analysis or abstraction beyond the cited paper, limiting the insight quality."}}
{"id": "10df83ce-b3a3-4eaa-bc3a-f7f04014937b", "title": "Contrast with other Fields", "level": "subsubsection", "subsections": [], "parent_id": "cbaad429-d670-471a-861b-fd7642becbc6", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Foundation"], ["subsection", "The Meta Abstraction"], ["subsubsection", "Contrast with other Fields"]], "content": "Now that we have provided a formal basis for our discussion for both supervised and reinforcement meta-learning, it is time to contrast meta-learning briefly with two related areas of machine learning that also have the goal to improve the speed of learning. We will start with transfer learning.\n\\textbf{Transfer Learning}\nIn Transfer Learning, one tries to \\textit{transfer} knowledge of previous tasks to new, unseen tasks , which can be challenging when the new task comes from a different distribution than the one used for training  . The distinction between Transfer Learning and Meta-Learning has become more opaque over time. \nA key property of meta-learning techniques, however, is their \\textit{meta-objective}, which explicitly aims to optimize performance across a distribution over tasks (as seen in previous sections by taking the expected loss over a distribution of tasks). This objective need not always be present in Transfer Learning techniques, e.g., when one \\textit{pre-trains} a model on a large data set, and \\textit{fine-tunes} the learned weights on a smaller data set.\n\\textbf{Multi-task learning}\nAnother,  closely related field, is that of multi-task learning. In multi-task learning, a model is jointly trained to perform well on multiple fixed tasks . Meta-learning, in contrast, aims to find a model that can learn new (previously unseen) tasks quickly. This difference is illustrated in \\autoref{fig:multitask}. \n\\begin{figure}[tb]\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/MultiTask.png}\n    \\caption[The difference between multi-task learning and meta-learning]{The difference between multi-task learning and meta-learning\\protect\\footnotemark.}\n    \\label{fig:multitask}\n\\end{figure}\n\\FloatBarrier\n\\footnotetext{Adapted from~\\url{https://meta-world.github.io/}}", "cites": [7109], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a basic comparison between meta-learning and related fields like transfer learning and multi-task learning. It synthesizes the concept of the meta-objective from the cited survey and uses it to differentiate meta-learning from transfer learning, but it lacks deeper integration of multiple sources. The critique is minimal, focusing mainly on conceptual distinctions rather than limitations or gaps in the cited works. It offers some abstraction by highlighting the overarching goal of learning to learn, but not at a meta-level depth."}}
{"id": "ab680c56-edfe-46d8-ac56-d58d33718e17", "title": "\\texorpdfstring{$N$", "level": "subsubsection", "subsections": [], "parent_id": "4a02376e-af6a-41ad-9d90-8ff84fa6ccff", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Foundation"], ["subsection", "The Meta-Setup"], ["subsubsection", "\\texorpdfstring{$N$"]], "content": "{N}-way, \\texorpdfstring{$k$}{k}-shot Learning}\nA frequently used instantiation of this general meta-setup is called $N$-way, $k$-shot\\index{$n$-way, $k$-shot learning} classification (see \\autoref{fig:fewshot}). This setup is also divided into the three stages---meta-train, meta-validation, and meta-test---which are used for meta-learning, meta-learner hyperparameter optimization, and evaluation, respectively. Each stage has a corresponding set of disjoint labels, i.e., $L^{tr}, L^{val}, L^{test} \\subset Y$, such that $L^{tr} \\cap L^{val} = \\emptyset, L^{tr} \\cap L^{test} = \\emptyset$, and $L^{val} \\cap L^{test} = \\emptyset$. In a given stage $s$, \\textit{tasks/episodes} $\\Tau_{j} = (D^{tr}_{\\Tau_{j}}, D^{test}_{\\Tau_{j}})$ are obtained by sampling examples $(\\boldsymbol{x}_{i}, y_{i})$ from the full data set $\\mathcal{D}$, such that every $y_{i} \\in L^{s}$. Note that this requires access to a data set $\\mathcal{D}$. The sampling process is guided by the $N$-way, $k$-shot principle, which states that every training data set $D^{tr}_{\\Tau_{j}}$ should contain exactly $N$ classes and $k$ examples per class, implying that $|D^{tr}_{\\Tau_{j}}| = N \\cdot k$. Furthermore, the true labels of examples in the test set $D_{\\Tau_{j}}^{test}$ must be present in the train set $D^{tr}_{\\Tau_{j}}$ of a given task $\\Tau_{j}$. $D^{tr}_{\\Tau{j}}$ acts as a \\textit{support set}\\index{support set}, literally supporting classification decisions on the \\textit{query set}\\index{query set} $D^{test}_{\\Tau_{j}}$.  Importantly, note that with this terminology, the query set (or test set) of a task is actually used during the meta-training phase. Furthermore, the fact that the labels across stages are disjoint ensures that we test the ability of a model to learn \\textit{new} concepts. \nThe meta-learning objective in the training phase is to minimize the loss function of the model predictions on the query sets, conditioned on the support sets. As such, for a given task $\\Tau_j$, the model `sees' the support set, and extracts information from the support set to guide its predictions on the query set. By applying this procedure to different episodes/tasks \n$\\Tau_j$, the model will slowly accumulate meta-knowledge $\\omega$, which can ultimately speed up learning on new tasks. \nThe easiest way to achieve this is by doing this with regular neural networks, but as was pointed out by various authors (see, e.g., ) more sophisticated architectures will vastly outperform such networks. In the remainder of this work, we will review such architectures. \nAt the meta-validation and meta-test stages, or evaluation phases, the learned meta-information in $\\omega$ is fixed. The model is, however, still allowed to make task-specific updates to its parameters $\\boldsymbol{\\theta}$ (which implies that it is learning). After task-specific updates, we can evaluate the performance on the test sets. In this way, we test how well a technique performs at meta-learning.\n$N$-way, $k$-shot classification is often performed for small values of $k$ (since we want our models to learn new concepts quickly, i.e., from few examples). In that case, one can refer to it as \\textit{few-shot learning}\\index{few-shot learning}.", "cites": [1695], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a clear and factual description of the $N$-way, $k$-shot learning setup within the context of deep meta-learning. It integrates basic concepts from the cited paper (e.g., the role of the support and query sets) but does so without substantial synthesis across multiple sources. There is minimal critical evaluation of the cited work or broader trends, and while it introduces some general terms (like few-shot learning), it does not elevate the discussion to a meta-level or present overarching theoretical insights."}}
{"id": "d260bfbd-1228-436a-bdd2-09b001f99f5c", "title": "Common Benchmarks", "level": "subsubsection", "subsections": [], "parent_id": "4a02376e-af6a-41ad-9d90-8ff84fa6ccff", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Foundation"], ["subsection", "The Meta-Setup"], ["subsubsection", "Common Benchmarks"]], "content": "\\label{sec:benchmarks}\nHere, we briefly describe some benchmarks that can be used to evaluate meta-learning algorithms. \n\\begin{itemize}\n    \\item \\textbf{Omniglot :} This data set presents an image recognition task. Each image corresponds to one out of 1\\,623 characters from 50 different alphabets. Every character was drawn by 20 people. Note that in this case, the characters are the classes/labels. \\item \\textbf{ImageNet :} This is the largest image classification data set, containing more than 20K classes and over 14 million colored images. \\textit{miniImageNet} is a mini variant of the large ImageNet data set  for image classification, proposed by  to reduce the engineering efforts to run experiments. The mini data set contains 60\\,000 colored images of size $84 \\times 84$. There are a total of 100 classes present, each accorded by 600 examples. \\textit{tieredImageNet}  is another variation of the large ImageNet data set. It is similar to miniImageNet, but contains a hierarchical structure. That is, there are 34 classes, each with its own sub-classes. \n    \\item \\textbf{CIFAR-10 and CIFAR-100 }: Two other image recognition data sets. Each one contains 60K RGB images of size $32 \\times 32$. CIFAR-10 and CIFAR-100 contain 10 and 100 classes respectively, with a uniform number of examples per class (6\\,000 and 600 respectively). Every class in CIFAR-100 also has a super-class, of which there are 20 in the full data set. Many variants of the CIFAR data sets can be sampled, giving rise to e.g.\\ \\textit{CIFAR-FS}  and \\textit{FC-100} .\n    \\item \\textbf{CUB-200-2011 :} The CUB-200-2011 data set contains roughly 12K RGB images of birds from 200 species. Every image has some labeled attributes (e.g.\\ crown color, tail shape).\n    \\item \\textbf{MNIST :} MNIST presents a hand-written digit recognition task, containing ten classes (for digits 0 through 9). In total, the data set is split into a 60K train and 10K test gray scale images of hand-written digits.    \n    \\item \\textbf{Meta-Dataset :} This data set comprises several other data sets such as Omniglot , CUB-200 , ImageNet , and more . An episode is then constructed by sampling a data set (e.g.\\ Omniglot) and selecting a subset of labels to create train and test splits as before. In this way, broader generalization is enforced since the tasks are more distant from each other.  \n    \\item \\textbf{Meta-world }: A meta reinforcement learning data set, containing 50 robotic manipulation tasks (control a robot arm to achieve some pre-defined goal, e.g.\\ unlocking a door, or playing soccer). It was specifically designed to cover a broad range of tasks, such that meaningful generalization can be measured .\n\\end{itemize}\n\\begin{figure}[tb]\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/RoboticControl.png}\n    \\caption{Learning continuous robotic control tasks is an important application of Deep Meta-Learning techniques. Image taken from .}\n    \\label{fig:robot}\n\\end{figure}", "cites": [5800, 8051, 1690, 3643, 3852], "cite_extract_rate": 0.45454545454545453, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily functions as a descriptive list of benchmarks used in deep meta-learning, with minimal synthesis of ideas across cited papers. While it references relevant datasets and their variations (e.g., miniImageNet, tieredImageNet, Meta-Dataset, and Meta-World), it does so without connecting these to broader theoretical or methodological themes in meta-learning. There is little critical analysis or abstraction beyond the specific datasets and their features."}}
{"id": "d735541a-974c-42af-af16-ce16a6875059", "title": "Some Applications of Meta-Learning", "level": "subsubsection", "subsections": [], "parent_id": "4a02376e-af6a-41ad-9d90-8ff84fa6ccff", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Foundation"], ["subsection", "The Meta-Setup"], ["subsubsection", "Some Applications of Meta-Learning"]], "content": "Deep neural networks have achieved remarkable results on various tasks including image recognition, text processing, game playing, and robotics , but their success depends on the amount of available data  and computing resources. Deep meta-learning reduces this dependency by allowing deep neural networks to learn new concepts quickly. As a result, meta-learning widens the applicability of deep learning techniques to many application domains. Such areas include few-shot image classification , robotic control policy learning  (see \\autoref{fig:robot}), hyperparameter optimization , meta-learning learning rules , abstract reasoning , and many more. For a larger overview of applications, we refer interested readers to .", "cites": [207, 3850, 5943, 7109, 8757, 2952, 1695, 861, 5942, 620, 5944], "cite_extract_rate": 0.6470588235294118, "origin_cites_number": 17, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section lists several application areas of meta-learning but provides minimal synthesis of the cited papers. It mentions tasks like few-shot image classification and robotic control policy learning without connecting the methods or insights from the referenced works. The lack of critical evaluation or abstraction prevents it from offering a deeper understanding of trends or principles in deep meta-learning."}}
{"id": "08faafdb-4ce8-4a19-b2de-c0f7c94b0666", "title": "The Meta-Learning Field", "level": "subsection", "subsections": [], "parent_id": "e0eb6c82-460f-4b8c-aea2-60ac8d268e9c", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Foundation"], ["subsection", "The Meta-Learning Field"]], "content": "As mentioned in the introduction, meta-learning is a broad area of research, as it encapsulates all techniques that leverage prior learning experience to learn new tasks more quickly . We can classify two distinct communities in the field with a different focus: i) algorithm selection and hyperparameter optimization for machine learning techniques, and ii) search for inductive bias in deep neural networks. We will refer to these communities as group i) and group ii) respectively. Now, we will give a brief description of the first field, and a historical overview of the second.\nGroup i) uses a more traditional approach, to select a suitable machine learning algorithm and hyperparameters for a new data set $\\mathcal{D}$ . This selection can for example be made by leveraging prior model evaluations on various data sets $D'$, and by using the model which achieved the best performance on the most similar data set . Such traditional approaches require (large) databases of prior model evaluations, for many different algorithms. This has led to initiatives such as OpenML , where researchers can share such information. \nThe usage of these systems would limit the freedom in picking the neural network architecture as they would be constrained to using architectures that have been evaluated beforehand.  \nIn contrast, group ii) adopts the view of a self-improving (neural) agent, which improves its learning ability over time by finding a good inductive bias (a set of assumptions that guide predictions).\nWe now present a brief historical overview of developments in this field of Deep Meta-Learning, based on . \nPioneering work was done by  and . Schmidhuber developed a theory of \\textit{self-referential} learning, where the weights of a neural network can serve as input to the model itself, which then predicts updates . In that same year,  proposed to use two weights per neural network connection, i.e., \\textit{slow} and \\textit{fast} weights, which serve as long- and short-term memory respectively. Later came the idea of meta-learning learning rules . Meta-learning techniques that use gradient-descent and backpropagation were proposed by  and . These two works have been pivotal to the current field of Deep Meta-Learning, as the majority of techniques rely on backpropagation, as we will see on our journey of contemporary Deep Meta-Learning techniques.", "cites": [7612, 7109, 8052], "cite_extract_rate": 0.2727272727272727, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview by distinguishing two main communities in meta-learning and situating Deep Meta-Learning within the broader context. It synthesizes the cited papers by linking historical developments and highlighting the role of inductive bias and backpropagation. However, the critical evaluation is limited, as it does not deeply assess the strengths or weaknesses of the approaches. It offers some abstraction by framing the evolution of ideas, but the insights remain grounded in existing categorizations without introducing novel frameworks."}}
{"id": "891db0bd-f500-4c96-8acb-65ccc1dd9772", "title": "Matching Networks", "level": "subsection", "subsections": [], "parent_id": "4c7e70a8-200e-4800-aed3-3de6b8438402", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Metric-based Meta-Learning"], ["subsection", "Matching Networks"]], "content": "\\label{sec:matchingNets}\nMatching networks\\index{matching networks}  build upon the idea that underlies Siamese neural networks . That is, they leverage pair-wise comparisons between the given support set $D^{tr}_{\\Tau_{j}} = \\{ (\\boldsymbol{x}_{i}, y_{i}) \\}_{i=1}^{m}$ (for a task $\\Tau_{j}$), and new inputs $\\boldsymbol{x} \\in D^{test}_{\\Tau_{j}}$ from the query set which we want to classify. However, instead of assigning the class $y_{i}$ of the most similar example input $\\boldsymbol{x}_{i}$, matching networks use a weighted combination of \\textit{all} example labels $y_{i}$ in the support set, based on the similarity of inputs $\\boldsymbol{x}_{i}$ to new input $\\boldsymbol{x}$. More specifically, predictions are computed as follows: $\\hat{y} = \\sum_{i=1}^{m} a(\\boldsymbol{x}, \\boldsymbol{x}_{i})y_{i}$, where $a$ is a non-parametric (non-trainable) attention mechanism, or similarity kernel. This classification process is shown in \\autoref{fig:matchingNets}. In this figure, the input to $f_{\\boldsymbol{\\theta}}$ has to be classified, using the support set $D^{tr}_{\\Tau_{j}}$ (input to $g_{\\boldsymbol{\\theta}}$).\n\\begin{figure}[tb]\n  \\centering\n  \\includegraphics[scale=0.55]{images/MatchingNets.png}\n  \\caption{Architecture of matching networks. Source: .}\n  \\label{fig:matchingNets}\n\\end{figure}\nThe  attention that is used consists of a softmax over the cosine similarity $c$ between the input representations, i.e., \n\\begin{align}\n  a(\\boldsymbol{x}, \\boldsymbol{x}_{i}) = \\frac{e^{c( f_{\\boldsymbol{\\phi}}(\\boldsymbol{x}), g_{\\boldsymbol{\\varphi}}(\\boldsymbol{x}_{i}) )}}{\\sum_{j=1}^{m} e^{c( f_{\\boldsymbol{\\phi}}(\\boldsymbol{x}), g_{\\boldsymbol{\\varphi}}(\\boldsymbol{x}_{j}) )}},\n\\end{align}\nwhere $f_{\\boldsymbol{\\phi}}$ and $g_{\\boldsymbol{\\varphi}}$ are neural networks, parameterized by $\\boldsymbol{\\phi}$ and $\\boldsymbol{\\varphi}$, that map raw inputs to a (lower-dimensional) latent vector, which corresponds to the output of the final hidden layer of a neural network. As such, the neural networks act as embedding functions. The larger the cosine similarity between the embeddings of $\\boldsymbol{x}$ and $\\boldsymbol{x}_{i}$, the larger $a(\\boldsymbol{x}, \\boldsymbol{x}_{i})$, and thus the influence of label $y_{i}$ on the predicted label $\\hat{y}$ for input $\\boldsymbol{x}$. \n propose two main choices for the embedding functions. The first is to use a single neural network, granting us $\\boldsymbol{\\theta} = \\boldsymbol{\\phi} = \\boldsymbol{\\varphi}$ and thus $f_{\\boldsymbol{\\phi}} = g_{\\boldsymbol{\\varphi}}$. This setup is the default form of matching networks, as shown in \\autoref{fig:matchingNets}. The second choice is to make $f_{\\boldsymbol{\\phi}}$ and $g_{\\boldsymbol{\\varphi}}$ dependent on the support set $D^{tr}_{\\Tau_{j}}$ using Long Short-Term Memory networks (LSTMs). In that case, $f_{\\boldsymbol{\\phi}}$ is represented by an attention LSTM, and $g_{\\boldsymbol{\\varphi}}$ by a bidirectional one. This choice for embedding functions is called \\textit{Full Context Embeddings} (FCE), and yielded an accuracy improvement of roughly 2\\% on miniImageNet compared to the regular matching networks, indicating that task-specific embeddings can aid the classification of new data points from the same distribution. \nMatching networks learn a good feature space across tasks for making pair-wise comparisons between inputs. In contrast to Siamese neural networks , this feature space (given by weights $\\boldsymbol{\\theta}$) is learned across tasks, instead of on a distinct verification task. \nIn summary, matching networks are an elegant and simple approach to metric-based meta-learning. However, these networks are not readily applicable outside of supervised learning settings and suffer from performance degradation when label distributions are biased .", "cites": [1690], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear description of matching networks and their core mechanism, effectively synthesizing the main concepts from the cited paper. However, it lacks deeper critical evaluation or comparison with other methods, and while it touches on the idea of feature space learning, it does not abstract this into broader meta-learning principles or trends. The narrative remains focused on explaining the method rather than analyzing its implications or limitations in depth."}}
{"id": "95f03710-fb27-4f82-bab6-74ee0b87718a", "title": "Prototypical Networks", "level": "subsection", "subsections": [], "parent_id": "4c7e70a8-200e-4800-aed3-3de6b8438402", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Metric-based Meta-Learning"], ["subsection", "Prototypical Networks"]], "content": "\\label{sec:proto}\nJust like matching networks , prototypical networks  base their class predictions on the entire support set $D^{tr}_{\\Tau_{j}}$. However, instead of computing the similarity between new inputs and examples in the support set, prototypical networks only compare new inputs to \\textit{class prototypes} (centroids), which are single vector representations of classes in some embedding space. Since there are fewer (or equal) class prototypes than the number of examples in the support set, the amount of required pair-wise comparisons decreases, saving computational costs. \n\\begin{figure}[thb]\n    \\centering\n    \\includegraphics[scale=0.8]{images/PrototypicalNetworks.png}\n    \\caption{Prototypical networks for the case of few-shot learning. The $\\boldsymbol{c}_{k}$ are class prototypes for class $k$ which are computed by averaging the representations of inputs (colored circles) in the support set. Note that the representation space is partitioned into three disjoint areas, where each area corresponds to one class. The class with the closest prototype to the new input $\\boldsymbol{x}$ in the query set is then given as prediction.  Source: .}\n    \\label{fig:protoNets}\n\\end{figure}\nThe underlying idea of class prototypes is that for a task $\\Tau_{j}$, there exists an embedding function that maps the support set onto a space where class instances cluster nicely around the corresponding class prototypes . Then, for a new input $\\boldsymbol{x}$, the class of the prototype nearest to that input will be predicted. As such, prototypical networks perform nearest centroid/prototype classification in a meta-learned embedding space. This is visualized in \\autoref{fig:protoNets}.  \nMore formally, given a distance function $d: X \\times X \\rightarrow [0, +\\infty)$ (e.g.\\ Euclidean distance) and embedding function $f_{\\boldsymbol{\\theta}}$, parameterized by $\\boldsymbol{\\theta}$, prototypical networks compute class probabilities $p_{\\boldsymbol{\\theta}}(Y | \\boldsymbol{x}, D^{tr}_{\\Tau_{j}})$ as follows\n\\begin{align}\n    p_{\\boldsymbol{\\theta}}(y = k | \\boldsymbol{x}, D^{tr}_{\\Tau_{j}}) = \\frac{exp[-d(f_{\\theta}(\\boldsymbol{x}), \\boldsymbol{c}_{k})]}{\\sum_{y_{i}} exp[-d(f_{\\theta}(\\boldsymbol{x}), \\boldsymbol{c}_{y_{i}}) ]},\n\\end{align}\nwhere $\\boldsymbol{c}_{k}$ is the prototype/centroid for class $k$ and $y_{i}$ are the classes in the support set $D^{tr}_{\\Tau_{j}}$. Here, a class prototype for class $k$ is defined as the average of all vectors $\\boldsymbol{x}_{i}$ in the support set such that $y_{i} = k$. Thus, classes with prototypes that are nearer to the new input $\\boldsymbol{x}$ obtain larger probability scores.  \n found that the squared Euclidean distance function as $d$ gave rise to the best performance. With that distance function, prototypical networks can be seen as linear models. To see this, note that $-d(f_{\\theta}(\\boldsymbol{x}), \\boldsymbol{c}_{k}) = -|| f_{\\theta}(\\boldsymbol{x}) - \\boldsymbol{c}_{k}||^{2} = - f_{\\theta}(\\boldsymbol{x})^{T}f_{\\theta}(\\boldsymbol{x}) + 2\\boldsymbol{c}_{k}^{T}f_{\\theta}(\\boldsymbol{x}) - \\boldsymbol{c}_{k}^{T}\\boldsymbol{c}_{k}$. The first term does not depend on the class $k$, and does thus not affect the classification decision. The remainder can\nbe written as $\\boldsymbol{w}_{k}^{T}f_{\\theta}(\\boldsymbol{x}) + \\boldsymbol{b}_{k}$, where $\\boldsymbol{w}_{k} = 2\\boldsymbol{c}_{k}$ and $\\boldsymbol{b}_{k} = -\\boldsymbol{c}_{k}^{T}\\boldsymbol{c}_{k}$. \nNote that this is linear in the output of network $f_\\theta$, not linear in the input of the network $\\boldsymbol{x}$. \nAlso,  show that prototypical networks (coupled with Euclidean distance) are equivalent to matching networks in one-shot learning settings, as every example in the support set will be its prototype. \nIn short, prototypical networks save computational costs by reducing the required number of pair-wise comparisons between new inputs and the support set, by adopting the concept of class prototypes. Additionally, prototypical networks were found to outperform matching networks  in 5-way, $k$-shot learning for $k=1,5$ on Omniglot  and miniImageNet , even though they do not use complex task-specific embedding functions. Despite these advantages, prototypical networks are not readily applicable outside of supervised learning settings.", "cites": [8757, 1690], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the core idea of Prototypical Networks from the cited paper, while also drawing connections to Matching Networks and highlighting theoretical equivalences and differences. It provides critical insights by discussing performance advantages and limitations, such as applicability only in supervised settings. The explanation abstracts the method to its mathematical formulation and conceptual implications, such as linear behavior in certain conditions, offering a meta-level understanding."}}
{"id": "6835df74-b034-4911-b69b-fa2f7d97004d", "title": "Relation Networks", "level": "subsection", "subsections": [], "parent_id": "4c7e70a8-200e-4800-aed3-3de6b8438402", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Metric-based Meta-Learning"], ["subsection", "Relation Networks"]], "content": "\\label{sec:relnets}\n\\begin{figure}[tb]\n    \\centering\n    \\includegraphics[scale=0.72]{images/RelationNets.png}\n    \\caption{Relation network architecture. First, the embedding network $f_{\\boldsymbol{\\varphi}}$ embeds all inputs from the support set $D^{tr}_{\\Tau_{j}}$ (the five example inputs on the left), and the query input (below the $f_{\\boldsymbol{\\varphi}}$ block). All support set embeddings $f_{\\boldsymbol{\\varphi}}(\\boldsymbol{x}_{i})$ are then concatenated to the query embedding $f_{\\boldsymbol{\\varphi}}(\\boldsymbol{x})$. These concatenated embeddings are passed into a relation network $g_{\\boldsymbol{\\phi}}$, which computes a relation score for every pair $(\\boldsymbol{x}_{i}, \\boldsymbol{x})$. The class of the input $\\boldsymbol{x}_{i}$ that yields the largest relation score $g_{\\boldsymbol{\\phi}}([f_{\\boldsymbol{\\varphi}}(\\boldsymbol{x}), f_{\\boldsymbol{\\varphi}}(\\boldsymbol{x}_{i})])$ is then predicted. Source: .}\n    \\label{fig:relnet}\n\\end{figure}\nIn contrast to previously discussed metric-based techniques, Relation networks  employ a trainable similarity metric, instead of a pre-defined one (e.g.\\ cosine similarity as used in  matching networks ). More specifically, matching networks consist of two chained, neural network modules: the \\textit{embedding} network/module $f_{\\boldsymbol{\\varphi}}$ which is responsible for embedding inputs, and the \\textit{relation} network $g_{\\boldsymbol{\\phi}}$ which computes similarity scores between new inputs $\\boldsymbol{x}$ and example inputs $\\boldsymbol{x}_{i}$ of which we know the labels. A classification decision is then made by picking the class of the example input which yields the largest \\textit{relation score} (or similarity). Note that Relation networks thus do not use the idea of class prototypes, and simply compare new inputs $\\boldsymbol{x}$ to all example inputs $\\boldsymbol{x}_{i}$ in the support set, as done by, e.g., matching networks .  \nMore formally, we are given a support set $D^{tr}_{\\Tau_{j}}$ with some examples $(\\boldsymbol{x}_{i}, y_{i})$, and a new (previously unseen) input $\\boldsymbol{x}$. Then, for every combination $(\\boldsymbol{x}, \\boldsymbol{x}_{i})$, the Relation network produces a \\textit{concatenated} embedding $[f_{\\boldsymbol{\\varphi}}(\\boldsymbol{x}), f_{\\boldsymbol{\\varphi}}(\\boldsymbol{x}_{i})]$, which is vector obtained by concatenating the respective embeddings of $\\boldsymbol{x}$ and $\\boldsymbol{x}_{i}$.\nThis concatenated embedding is then fed into the \\textit{relation} module $g_{\\boldsymbol{\\phi}}$. Finally, $g_{\\boldsymbol{\\phi}}$ computes the relation score between $\\boldsymbol{x}$ and $\\boldsymbol{x}_{i}$ as\n\\begin{align}\n    r_{i} = g_{\\boldsymbol{\\phi}}([f_{\\boldsymbol{\\varphi}}(\\boldsymbol{x}), f_{\\boldsymbol{\\varphi}}(\\boldsymbol{x}_{i}) ]).\n\\end{align}\nThe predicted class is then $\\hat{y} = y_{\\argmax_{i} r_{i}}$. This entire process is shown in \\autoref{fig:relnet}. Remarkably enough, Relation networks use the Mean-Squared Error (MSE) of the relation scores, rather than the more standard cross-entropy loss. The MSE is then propagated backwards through the entire architecture (\\autoref{fig:relnet}).\nThe key advantage of Relation networks is their expressive power, induced by the usage of a trainable similarity function. This expressivity makes this technique very powerful. As a result, it yields better performance than previously discussed techniques that use a fixed similarity metric.", "cites": [7147, 1690], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear and factual description of Relation Networks and their differences from Matching Networks, but the synthesis is limited to contrasting the two methods rather than connecting broader ideas. It lacks deeper critical analysis of the strengths and weaknesses of the approach or its implications within the field. The abstraction is modest, as it identifies the use of a trainable similarity function but does not generalize this concept into a meta-level discussion of metric-based meta-learning."}}
{"id": "76d0bf96-9727-4141-8f9b-511902af8878", "title": "Graph Neural Networks", "level": "subsection", "subsections": [], "parent_id": "4c7e70a8-200e-4800-aed3-3de6b8438402", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Metric-based Meta-Learning"], ["subsection", "Graph Neural Networks"]], "content": "\\label{sec:graph}\nGraph neural networks\\index{graph neural networks}  use a more general and flexible approach than previously discussed techniques for $N$-way, $k$-shot classification. \nAs such, graph neural networks subsume Siamese  and prototypical networks . The graph neural network approach represents each task $\\Tau_{j}$ as a fully-connected graph $G = (V,E)$, where $V$ is a set of nodes/vertices and $E$ a set of edges connecting nodes. In this graph, nodes $\\boldsymbol{v}_{i}$ correspond to input embeddings $f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i})$, concatenated with their one-hot encoded labels $y_{i}$, i.e., $\\boldsymbol{v}_{i} = [f_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}), y_{i}]$. \nFor inputs $\\boldsymbol{x}$ from the query set (for which we do not have the labels), a uniform prior over all $N$ possible labels is used: $y = [\\frac{1}{N}, \\ldots ,\\frac{1}{N}]$. \nThus, each node contains an input and label section. Edges are weighted links that connect these nodes.\nThe graph neural network then propagates information in the graph using a number of local operators. The underlying idea is that label information can be transmitted from nodes of which we do have the labels, to nodes for which we have to predict labels. Which local operators are used, is out of scope for this paper, and the reader is referred to  for details. \nBy exposing the graph neural network to various tasks $\\Tau_{j}$, the propagation mechanism can be altered to improve the flow of label information in such a way that predictions become more accurate. As such, in addition to learning a good input representation function $f_{\\boldsymbol{\\theta}}$, graph neural networks also learn to propagate label information from labeled examples to unlabeled inputs.\nGraph neural networks achieve good performance in few-shot settings  and are also applicable in semi-supervised and active learning settings.", "cites": [286, 8757], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of how graph neural networks are applied in deep meta-learning, drawing from one key paper (doc_id: 286) and briefly mentioning prototypical networks (doc_id: 8757). While it connects the graph-based approach to prior techniques, the synthesis remains surface-level without deeper conceptual integration. There is limited critical analysis, and while some abstraction is attempted, it is not sufficiently elevated to capture overarching principles."}}
{"id": "552d85be-29ed-44c6-b1ac-c32e70dfc9a4", "title": "Attentive Recurrent Comparators (ARCs)", "level": "subsection", "subsections": [], "parent_id": "4c7e70a8-200e-4800-aed3-3de6b8438402", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Metric-based Meta-Learning"], ["subsection", "Attentive Recurrent Comparators (ARCs)"]], "content": "\\begin{figure}[tb]\n    \\centering\n    \\includegraphics[scale=0.45]{images/arc.png}\n    \\caption{Processing in an attentive recurrent comparator. At every time step, the model takes a glimpse of a part of an image and incorporates this information into the hidden state $h_t$. The final hidden state after taking various glimpses of a pair of images is then used to compute a class similarity score.  Source: .}\n    \\label{fig:arc}\n\\end{figure}\nAttentive recurrent comparators\\index{attentive recurrent comparators} (ARCs)  differ from previously discussed techniques as they do not compare inputs as a whole, but by parts. This approach is inspired by how humans would make a decision concerning the similarity of objects. That is, we  shift our attention from one object to the other, and move back and forth to take glimpses of different parts of both objects. In this way, information of  two objects is fused from the  beginning, whereas other techniques (e.g., matching networks  and graph neural networks ) only combine information at the end (after embedding both images) .  \nGiven two inputs $\\boldsymbol{x}_{i}$ and $\\boldsymbol{x}$, we feed them in interleaved fashion repeatedly into a recurrent neural network (controller): $\\boldsymbol{x}_{i}, \\boldsymbol{x}, \\ldots ,\\boldsymbol{x}_{i},\\boldsymbol{x}$. Thus, the image at time step $t$ is given by $I_{t} = \\boldsymbol{x}_{i}$ if $t$ is even else $\\boldsymbol{x}$. Then, at each time step $t$, the attention mechanism focuses on a square region of the current image: $G_{t} = attend(I_{t}, \\Omega_{t})$, where $\\Omega_{t} = W_{g}h_{t-1}$ are attention parameters, which are computed from the previous hidden state $h_{t-1}$. The next hidden state $h_{t+1} = \\mbox{RNN}(G_{t}, h_{t-1})$ is given by the glimpse at time t, i.e., $G_{t}$, and the previous hidden state $h_{t-1}$. The entire sequence consists of $g$ glimpses per image. After this sequence is fed into the recurrent neural network (indicated by RNN($\\circ$)), the final hidden state $h_{2g}$ is used as combined representation of $\\boldsymbol{x}_{i}$ relative to $\\boldsymbol{x}$. This process is summarized in \\autoref{fig:arc}. Classification decisions can then be made by feeding the combined representations into a classifier. Optionally, the combined representations can be processed by bi-directional LSTMs before passing them to the classifier.\nThe attention approach is biologically inspired, and biologically plausible. A downside  of attentive recurrent comparators is the higher computational cost, while the performance is often not better than less biologically plausible techniques, such as graph neural networks .", "cites": [286, 8053, 1690], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear explanation of ARCs and contrasts them with other metric-based methods like Matching Networks and Graph Neural Networks by highlighting the interleaved attention mechanism. This shows some synthesis and abstraction by identifying a unique design principle. However, the critical analysis is limited to a single sentence about computational cost and performance, without deeper evaluation or comparative insight."}}
{"id": "b716d85b-7334-4c25-b3b0-b0ef576dddd7", "title": "Metric-based Techniques, in conclusion", "level": "subsection", "subsections": [], "parent_id": "4c7e70a8-200e-4800-aed3-3de6b8438402", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Metric-based Meta-Learning"], ["subsection", "Metric-based Techniques, in conclusion"]], "content": "In this section, we have seen various metric-based\\index{meta-learning!metric-based} techniques. The metric-based techniques meta-learn an informative feature space that can be used to compute class predictions based on input similarity scores. \n\\autoref{fig:metricbasedrels} shows the relationships between the various metric-based techniques that we have covered. \nAs we can see, Siamese networks  mark the beginning of metric-based, deep meta-learning techniques in few-shot learning settings.\nThey are the first to use the idea of predicting classes by comparing inputs from the support and query sets. \nThis idea was generalized in graph neural networks (GNNs)  where the information flow between support and query inputs is parametric and thus more flexible.\nMatching networks  are directly inspired by Siamese networks as they use the same core idea (comparing inputs for making predictions), but directly train in the few-shot setting and use cosine similarity as a similarity function. \nThus, the auxiliary, binary classification task used by Siamese networks is left out, and matching networks directly train on tasks. \nPrototypical networks  increase the robustness of input comparisons by comparing every query set input with a class prototype instead of individual support set examples. \nThis reduces the number of required input comparisons for a single query input to $N$ instead of $k \\cdot N$.  \nRelation networks  replace the fixed, pre-defined similarity metrics used in matching and prototypical networks by a neural network, which allows for learning a domain-specific similarity function.\nLastly, attentive recurrent comparators  take a more biologically plausible approach by not comparing entire inputs but by taking multiple interleaved glimpses at various parts of the inputs that are being compared.   \nKey advantages of these metric-based techniques are that i)~the underlying idea of similarity-based predictions is conceptually simple, and ii)~they can be  fast at test-time when tasks are small, as the networks do not need to make task-specific adjustments. However, when tasks at meta-test time become more distant from the tasks that were used at meta-train time, metric-learning techniques are unable to absorb new task information into the network weights. Consequently, performance may degrade.   \nFurthermore, when tasks become larger, pair-wise comparisons may become prohibitively expensive. Lastly, most metric-based techniques rely on the presence of labeled examples, which make them inapplicable outside of supervised learning settings. \n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/Metric-based.pdf}\n    \\caption{The relationships between the covered metric-based meta-learning techniques.}\n    \\label{fig:metricbasedrels}\n\\end{figure}", "cites": [8053, 286, 1690, 7147, 8757, 242], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.3, "critical": 3.8, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the cited papers by tracing the evolution of metric-based techniques and highlighting how each method builds upon or diverges from its predecessors. It offers a critical evaluation by pointing out limitations such as computational cost, lack of adaptability to new tasks, and dependency on labeled data. The section abstracts the core idea of similarity-based prediction, emphasizing its conceptual simplicity and broader applicability while identifying common shortcomings across the approaches."}}
{"id": "1fb1bfc8-56b2-4ddc-86a3-42735d0276aa", "title": "Recurrent Meta-Learners", "level": "subsection", "subsections": [], "parent_id": "8952f75b-bf15-40db-a5e0-2746cfbc8ee4", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Model-based Meta-Learning"], ["subsection", "Recurrent Meta-Learners"]], "content": "Recurrent meta-learners  are, as the name suggests, meta-learners based on recurrent neural networks. \nThe recurrent network serves as dynamic task embedding storage. These recurrent meta-learners were specifically proposed for reinforcement learning problems, hence we will explain them in that setting. \nThe recurrence is implemented by e.g.\\ an LSTM  or a GRU . The internal dynamics of the chosen Recurrent Neural Network (RNN) allows for fast adaptation to new tasks, while the algorithm used to train the recurrent net gradually accumulates knowledge about the task structure, where each task is modelled as an episode (or set of episodes). \nThe idea of recurrent meta-learners is quite simple. That is, given a task $\\Tau_{j}$, we simply feed the (potentially processed) environment variables $[s_{t+1},a_{t},r_{t},d_{t}]$ (see \\autoref{sec:RL}) into an RNN at every time step $t$. Recall that $s,a,r,d$ denote the state, action, reward, and termination flag respectively. At every time step $t$, the RNN outputs an action and a hidden state. Conditioned on its hidden state $h_{t}$, the network outputs an action $a_{t}$. The goal is to maximize the expected reward in each trial. See \\autoref{fig:rlRNN} for a visual depiction. From this figure, it also becomes clear why these techniques are model-based. That is, they embed information from previously seen inputs in the hidden state. \n\\begin{figure}[tb]\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/RLMetaLearners.png}\n    \\caption{Workflow of recurrent meta-learners in reinforcement learning contexts. As mentioned in \\autoref{sec:RL}, $s_{t}, r_{t},$ and $d_{t}$ denote the state, reward, and termination flag at time step $t$. $h_{t}$ refers to the hidden state at time $t$. Source: .}\n    \\label{fig:rlRNN}\n\\end{figure}\nRecurrent meta-learners have shown to perform almost as well as asymptotically optimal algorithms on simple reinforcement learning tasks . However, their performance degrades in more complex settings, where temporal dependencies can span a longer horizon. Making recurrent meta-learners better at such complex tasks is a direction for future research.", "cites": [8050, 7753], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section describes the concept of recurrent meta-learners and their implementation using RNNs in reinforcement learning, referencing two papers. It provides a basic synthesis by connecting their shared idea of using recurrence for task embedding and fast adaptation. However, the analysis remains largely descriptive with limited critical evaluation or abstraction into broader principles. The mention of performance degradation in complex settings hints at a limitation but lacks deeper comparative or trend-based insight."}}
{"id": "66f571e7-e817-466f-b649-cfe9772d8749", "title": "Memory-Augmented Neural Networks (MANNs)", "level": "subsection", "subsections": [], "parent_id": "8952f75b-bf15-40db-a5e0-2746cfbc8ee4", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Model-based Meta-Learning"], ["subsection", "Memory-Augmented Neural Networks (MANNs)"]], "content": "\\label{sec:MANN}\nThe key idea of memory-augmented\\index{memory-augmented neural networks} neural networks (MANNs)  is to enable neural networks to learn quickly with the help of an \\textit{external memory}. The main \\textit{controller} (the recurrent neural network interacting with the memory) then gradually accumulates knowledge across tasks, while the external memory allows for quick task-specific adaptation. For this,  used Neural Turing Machines . Here, the controller is parameterized by $\\boldsymbol{\\theta}$ and acts as the long-term memory of the memory-augmented neural network, while the external memory module is the short-term memory. \nThe workflow of memory-augmented neural networks is displayed in \\autoref{fig:flowMANN}. Note that the data from a task is processed as a sequence, i.e., data are fed into the network one by one. The support set is fed into the memory-augmented neural network first. Afterwards, the query set is processed. \nDuring the meta-train phase, training tasks can be fed into the network in arbitrary order.\nAt time step $t$, the model receives input $\\boldsymbol{x}_{t}$ with the label of the previous input, i.e., $y_{t-1}$. This was done to prevent the network from mapping class labels directly to the output . \n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/FlowMANN.png}-\n    \\caption{Workflow of memory-augmented neural networks. Here, an episode corresponds to  a given task $\\Tau_j$. After every episode, the order of labels, classes, and samples should be shuffled to minimize dependence on arbitrarily assigned orders. Source: .}\n    \\label{fig:flowMANN}\n\\end{figure}\n\\begin{figure}[tb!]\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/MemoryMANN.png}\n    \\caption{Controller-memory interaction in memory-augmented neural networks. Source: .}\n    \\label{fig:memoryMANN}\n\\end{figure}\nThe interaction between the controller and memory is visualized in \\autoref{fig:memoryMANN}. The idea is that the external memory module, containing representations of previously seen inputs, can be used to make predictions for new inputs. In short, previously obtained knowledge is leveraged to aid the classification of new inputs. Note that neural networks also attempt to do this, however, their prior knowledge is slowly accumulated into the network weights, while an external memory module can directly store such information. \nGiven an input $\\boldsymbol{x}_{t}$ at time $t$, the controller generates a key $\\boldsymbol{k}_{t}$, which can be stored in memory matrix $M$ and can be used to retrieve previous representations from memory matrix $M$. \nWhen reading from memory, the aim is to produce a linear combination of stored keys in memory matrix $M$, giving greater weight to those which have a larger cosine similarity with the current key $\\boldsymbol{k}_{t}$. More specifically, a read vector $\\boldsymbol{w}^{r}_{t}$ is created, in which each entry $i$ denotes the cosine similarity between key $\\boldsymbol{k}_{t}$ and the memory (from a previous input) stored in row $i$, i.e., $M_{t}(i)$. Then, the representation $\\boldsymbol{r}_{t} = \\sum_{i}w_{t}^{r}(i)M(i)$ is retrieved, which is simply a linear combination of all keys (i.e., rows) in memory matrix $M$. \nPredictions are made as follows. Given an input $\\boldsymbol{x}_{t}$, memory-augmented neural networks use the external memory to compute the corresponding representation $\\boldsymbol{r}_{t}$, which could be fed into a softmax layer, resulting in class probabilities. \nAcross tasks, memory-augmented neural networks learn a good input embedding function $f_{\\boldsymbol{\\theta}}$ and classifier weights, which can be exploited when presented with new tasks.\nTo write input representations to memory,  propose a new mechanism called Least Recently Used Access (LRUA). LRUA either writes to the least, or most recently used memory location. In the former case, it preserves recent memories, and in the latter it updates recently obtained information. The writing mechanism works by keeping track of how often every memory location is accessed in a usage vector $\\boldsymbol{w}_{t}^{u}$, which is updated at every time step according to the following update rule: $\\boldsymbol{w}_{t}^{u} := \\gamma \\boldsymbol{w}^{u}_{t-1} + \\boldsymbol{w}_{t}^{r} + \\boldsymbol{w}_{t}^{w}$, where superscripts $u,w$ and $r$ refer to usage, write and read vectors, respectively. In words, the previous usage vector is decayed (using parameter $\\gamma$), while current reads ($\\boldsymbol{w}_{t}^{r}$) and writes ($\\boldsymbol{w}_{t}^{w}$) are added to the usage.  Let $n$ be the total number of reads to memory, and $\\ell u(n)$ ($\\ell u$ for `least used') be the $n$-th smallest value in the usage vector $\\boldsymbol{w}^{u}_{t}$. Then, the least-used weights are defined as follows: $$\\boldsymbol{w}^{\\ell u}_{t}(i) = \n\\begin{cases} \n  0 & \\text{if $w^{u}_{t}(i) > \\ell u(n)$} \\\\\n  1 & else\n\\end{cases}.$$ Then, the write vector $\\boldsymbol{w}_{t}^{w}$ is computed as $\\boldsymbol{w}^{w}_{t} = \\sigma(\\alpha) \\boldsymbol{w}^{r}_{t-1} + (1 - \\sigma(\\alpha))\\boldsymbol{w}^{\\ell u}_{t-1}$, where $\\alpha$ is a parameter that interpolates between the two weight vectors. As such, if $\\sigma(\\alpha) = 1$, we write to the most recently used memory, whereas when $\\sigma(\\alpha) = 0$, we write to the least recently used memory locations. Finally, writing is performed as follows: $M_{t}(i) := M_{t-1}(i) + w_{t}^{w}(i)\\boldsymbol{k}_{t}$, for all $i$. \nIn summary, memory-augmented neural networks  combine external memory and a neural network to achieve meta-learning. The interaction between a controller, with long-term memory parameters $\\boldsymbol{\\theta}$, and memory $M$, may also be interesting for studying human meta-learning . In contrast to many metric-based techniques, this model-based technique is applicable to both classification and regression problems. A downside of this approach is the architectural complexity.", "cites": [1905], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a clear and factual description of Memory-Augmented Neural Networks (MANNs), focusing primarily on their structure and workflow. It integrates the concept of Neural Turing Machines from the cited paper, but does not connect it to other relevant works or broader meta-learning themes. The analysis is minimal, with only a brief mention of a potential downside (architectural complexity) and no in-depth evaluation of trade-offs or limitations. There is some abstraction in explaining how external memory differs from internal knowledge in neural networks, but the section remains largely grounded in specific mechanisms without identifying overarching trends or principles."}}
{"id": "bd92b60f-5098-4b3b-80a6-05daefc87b15", "title": "Meta Networks", "level": "subsection", "subsections": [], "parent_id": "8952f75b-bf15-40db-a5e0-2746cfbc8ee4", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Model-based Meta-Learning"], ["subsection", "Meta Networks"]], "content": "\\label{sec:metanets}\n\\begin{figure}[tb]\n    \\centering\n    \\includegraphics[scale=0.6]{images/MetaNetwork.png}\n    \\caption{Architecture of a Meta Network. Source: .}\n    \\label{fig:metaNet}\n\\end{figure}\nMeta networks are divided into two distinct subsystems (consisting of neural networks), i.e., the base- and meta-learner (whereas in memory-augmented neural networks the base- and meta-components are intertwined). The base-learner is responsible for performing tasks, and for providing the meta-learner with meta-information, such as loss gradients. The meta-learner can then compute fast task-specific weights for itself and the base-learner, such that it can perform better on the given task $\\Tau_{j} = (D^{tr}_{\\Tau_{j}}, D^{test}_{\\Tau_{j}})$. This workflow is depicted in \\autoref{fig:metaNet}. \nThe meta-learner consists of neural networks $u_{\\boldsymbol{\\phi}}, m_{\\boldsymbol{\\varphi}}$, and $d_{\\boldsymbol{\\psi}}$. Network $u_{\\boldsymbol{\\phi}}$ is used as input representation function. Networks $d_{\\boldsymbol{\\psi}}$ and $m_{\\boldsymbol{\\varphi}}$ are used to compute task-specific weights $\\boldsymbol{\\phi}^{*}$ and example-level fast weights $\\boldsymbol{\\theta}^{*}$. Lastly, $b_{\\boldsymbol{\\theta}}$ is the base-learner which performs input predictions. Note that we used the term fast-weights throughout, which refers to task- or input-specific versions of slow (initial) weights. \nIn similar fashion to memory-augmented neural networks , meta networks\\index{meta networks}  also leverage the idea of an external memory module. However, meta networks use the memory for a different purpose.\nThe memory stores for each observation $\\boldsymbol{x}_i$ in the support set two components, i.e., its representation $\\boldsymbol{r}_i$ and the fast weights $\\boldsymbol{\\theta}_i^*$. \nThese are then used to compute a attention-based representation and fast weights for new inputs, respectively. \n\\begin{algorithm}\n\\caption{Meta networks, by }\\label{alg:metanets}\n\\begin{algorithmic}[1]\n\\State Sample $S = \\{ (\\boldsymbol{x}_{i},y_{i}) \\backsim D^{tr}_{\\Tau_{j}} \\}_{i=1}^{T}$ from the support set\n\\For{$(\\boldsymbol{x}_{i},y_{i}) \\in S$}\n    \\State $\\mathcal{L}_{i} = \\mbox{error}(u_{\\boldsymbol{\\phi}}(\\boldsymbol{x}_{i}), y_{i})$ \n\\EndFor\n\\State $\\boldsymbol{\\phi}^{*} = d_{\\boldsymbol{\\psi}}(\\{ \\nabla_{\\boldsymbol{\\phi}} \\mathcal{L}_{i} \\}_{i=1}^{T})$\n\\For{$(\\boldsymbol{x}_{i},y_{i}) \\in D^{tr}_{\\Tau_{j}}$}\n    \\State $\\mathcal{L}_{i} = \\mbox{error}(b_{\\boldsymbol{\\theta}}(\\boldsymbol{x}_{i}),y_{i})$\n    \\State $\\boldsymbol{\\theta}_{i}^{*} = m_{\\boldsymbol{\\varphi}}(\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{i})$\n    \\State Store $\\boldsymbol{\\theta}_{i}^{*}$ in $i$-th position of example-level weight memory $M$\n    \\State $\\boldsymbol{r}_{i} = u_{\\boldsymbol{\\phi}, \\boldsymbol{\\phi}^{*}}(\\boldsymbol{x}_{i})$\n    \\State Store $\\boldsymbol{r}_{i}$ in $i$-th position of representation memory $R$\n\\EndFor\n\\State $\\mathcal{L}_{task} = 0$\n\\For{$(\\boldsymbol{x},y) \\in D^{test}_{\\Tau_{j}}$}\n    \\State $\\boldsymbol{r} = u_{\\boldsymbol{\\phi}, \\boldsymbol{\\phi}^{*}}(\\boldsymbol{x})$\n    \\State $\\boldsymbol{a} = \\mbox{attention}(R, \\boldsymbol{r})$ \\Comment{$a_{k}$ is the cosine similarity between $\\boldsymbol{r}$ and $R(k)$}\n    \\State $\\boldsymbol{\\theta}^{*} = \\mbox{softmax}(\\boldsymbol{a})^{T}M$\n    \\State $\\mathcal{L}_{task} = \\mathcal{L}_{task} + \\mbox{error}(b_{\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{*}}(\\boldsymbol{x}), y)$\n\\EndFor\n\\State Update $\\Theta = \\{ \\boldsymbol{\\theta}, \\boldsymbol{\\phi}, \\boldsymbol{\\psi}, \\boldsymbol{\\varphi} \\}$ using $\\nabla_{\\Theta} \\mathcal{L}_{task}$\n\\end{algorithmic}\n\\end{algorithm}\nThe pseudocode for meta networks is displayed in \\autoref{alg:metanets}. First, a sample of the support set is created (line 1), which is used to compute task-specific weights $\\boldsymbol{\\phi}^{*}$ for the representation network $u_{\\boldsymbol{\\phi}}$ (lines 2-5). \nNote that  $u_{\\boldsymbol{\\phi}}$ has two tasks, i) it should compute a representation for inputs $(\\boldsymbol{x}_{i}$ (line 10 and 15), and ii) it needs to make predictions for inputs $(\\boldsymbol{x}_{i}$, in order to compute a loss (line 3). \nTo achieve both goals, a conventional neural network can be used that makes class predictions. The states of the final hidden layer are then used as representations. \nTypically, the cross entropy is calculated over the predictions of representation network $u_{\\boldsymbol{\\phi}}$. When there are multiple examples per class in the support set, an alternative is to use a contrastive loss function . \nThen, meta networks iterate over every example $(\\boldsymbol{x}_{i}, y_{i})$ in the support set $D^{tr}_{\\Tau_{j}}$. The base-learner $b_{\\boldsymbol{\\theta}}$ attempts to make class predictions for these examples, resulting in loss values $\\mathcal{L}_{i}$ (line 7-8). The gradients of these losses are used to compute fast weights $\\boldsymbol{\\theta}^{*}$ for example $i$ (line 8), which are then stored in the $i$-th row of memory matrix $M$ (line 9). Additionally, input representations $\\boldsymbol{r}_{i}$ are computed and stored in memory matrix $R$ (lines 10-11). \nNow, meta networks are ready to address the query set $D^{test}_{\\Tau_{j}}$. They iterate over every example $(\\boldsymbol{x}, y)$, and compute a representation $\\boldsymbol{r}$ of it (line 15). This representation is matched against the representations of the support set, which are stored in memory matrix $R$. This matching gives us a similarity vector $\\boldsymbol{a}$, where every entry $k$ denotes the similarity between input representation $\\boldsymbol{r}$ and the $k$-th row in memory matrix R, i.e., $R(k)$ (line 16). A softmax over this similarity vector is performed to normalize the entries. The resulting vector is used to compute a linear combination of weights that were generated for inputs in the support set (line 17). These weights $\\boldsymbol{\\theta}^{*}$ are specific for input $\\boldsymbol{x}$ in the query set and can be used by the base-learner $b$ to make predictions for that input (line 18). The observed error is added to the task loss. After the entire query set is processed, all involved parameters can be updated using backpropagation (line 20).     \n\\begin{figure}[tb]\n    \\centering\n    \\includegraphics[scale=0.6]{images/MetaNetsAugmentation.png}\n    \\caption{Layer augmentation setup used to combine slow and fast weights. Source: .}\n    \\label{fig:layeraugmentation}\n\\end{figure}\nNote that some neural networks use both slow- and fast-weights at the same time.   use a so-called augmentation setup for this, as depicted in \\autoref{fig:layeraugmentation}.\nIn short, meta networks rely on a reparameterization of the meta- and base-learner for every task. Despite the flexibility and applicability to both supervised and reinforcement learning settings, the approach is quite complex. It consists of many components, each with its own set of parameters, which can be  a burden on memory usage and computation time. Additionally, finding the correct architecture for all the involved components can be  time-consuming.", "cites": [3644], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a clear and structured description of Meta Networks, integrating the key components from the cited paper (e.g., base-learner, meta-learner, memory, and attention mechanism). However, it primarily describes the system in a step-by-step manner without significant synthesis of ideas across multiple sources. There is limited critical analysis or identification of broader patterns, focusing instead on the mechanics of the approach."}}
{"id": "1f6a63b2-a250-418a-ac4e-18cd285496fc", "title": "Simple Neural Attentive Meta-Learner (SNAIL)", "level": "subsection", "subsections": [], "parent_id": "8952f75b-bf15-40db-a5e0-2746cfbc8ee4", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Model-based Meta-Learning"], ["subsection", "Simple Neural Attentive Meta-Learner (SNAIL)"]], "content": "Instead of an external memory matrix, SNAIL\\index{simple neural attentive meta-learner}  relies on a special model architecture to serve as memory.  argue that it is not possible to use Recurrent Neural Networks for this, as they have limited memory capacity, and cannot pinpoint specific prior experiences . Hence, SNAIL uses a different architecture, consisting of 1D \\textit{temporal convolutions}  and a \\textit{soft attention} mechanism . The temporal convolutions allow for `high bandwidth' memory access, and the attention mechanism allows one to pinpoint specific experiences. \\autoref{fig:SNAIL} visualizes the architecture and workflow of SNAIL for supervised learning problems. From this figure, it becomes clear why this technique is model-based. That is, model outputs are based upon the internal state, computed from earlier inputs.\n\\begin{figure}[tb]\n    \\centering\n    \\includegraphics[scale=0.5]{images/SNAIL.png}\n    \\caption{Architecture and workflow of SNAIL for supervised and reinforcement learning settings. The input layer is red. Temporal Convolution blocks are orange; attention blocks are green. Source: .}\n    \\label{fig:SNAIL}\n\\end{figure}\nSNAIL consists of three building blocks. The first is the \\textit{DenseBlock}, which applies a single 1D convolution to the input, and concatenates (in the feature/horizontal direction) the result. The second is a \\textit{TCBlock}, which is simply a series of DenseBlocks with exponentially increasing dilation rate of the temporal convolutions . Note that the dilation is nothing but the temporal distance between two nodes in a network. For example, if we use a dilation of 2, a node at position $p$ in layer $L$ will receive the activation from node $p-2$ from layer $L-1$. The third block is the \\textit{AttentionBlock}, which learns to focus on the important parts of prior experience. \nIn similar fashion to memory-augmented neural networks  (\\autoref{sec:MANN}), SNAIL also processes task data in sequence, as shown in \\autoref{fig:SNAIL}. However, the input at time $t$ is accompanied by the label at time $t$, instead of $t-1$ (as was the case for memory-augmented neural networks).  SNAIL learns internal dynamics from  seeing various tasks so that it can make good predictions on the query set, conditioned upon the support set.\nA key advantage of SNAIL is that it can be applied to both supervised and reinforcement learning tasks. In addition, it achieves  good performance compared to previously discussed techniques. A downside of SNAIL is that finding the correct architecture of TCBlocks and DenseBlocks can be  time-consuming.", "cites": [4722, 38], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a clear description of SNAIL's architecture and its components, referencing the use of temporal convolutions and attention mechanisms from the cited papers. However, it mainly summarizes the method without deeply synthesizing it with the cited works or identifying broader patterns. Some limited critical analysis is present in noting the limitations of RNNs and SNAIL's architecture search challenges, but the evaluation remains shallow."}}
{"id": "2bbb096a-2040-493d-930f-91319f0b517c", "title": "Conditional Neural Processes (CNPs)", "level": "subsection", "subsections": [], "parent_id": "8952f75b-bf15-40db-a5e0-2746cfbc8ee4", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Model-based Meta-Learning"], ["subsection", "Conditional Neural Processes (CNPs)"]], "content": "\\begin{figure}[tb]\n    \\centering\n    \\includegraphics[scale=0.4]{images/cnp.png}\n    \\caption{Schematic view of how conditional neural processes work. Here, $h$ denotes a network outputting a representation for a observation, $a$ denotes an aggregation function for these representations, and $g$ denotes a neural network that makes predictions for unlabelled observations, based on the aggregated representation. Source: .}\n    \\label{fig:cnp}\n\\end{figure}\nIn contrast to previous techniques, a conditional neural process\\index{conditional neural processes} (CNP)  does not rely on an  external memory module. Instead, it aggregates the  support set into a single aggregated latent representation. The general architecture is shown in \\autoref{fig:cnp}. As we can see, the conditional neural process operates in three phases on task $\\Tau_{j}$. First, it observes the support set $D^{tr}_{\\Tau_{j}}$, including the ground-truth outputs $y_{i}$. Examples $(\\boldsymbol{x}_{i},y_{i}) \\in D^{tr}_{\\Tau_{j}}$ are embedded using a neural network $h_{\\boldsymbol{\\theta}}$ into representations $\\boldsymbol{r}_{i}$. Second, these representations are aggregated using operator $a$ to produce a single representation $\\boldsymbol{r}$ of $D^{tr}_{\\Tau_{j}}$ (hence it is model-based). Third, a neural network $g_{\\boldsymbol{\\phi}}$ processes this single representation $\\boldsymbol{r}$, new inputs $\\boldsymbol{x}$, and produces predictions $\\hat{y}$.\nLet the entire conditional neural process model be denoted by $Q_{\\boldsymbol{\\Theta}}$, where $\\Theta$ is a set of all involved parameters $\\{ \\boldsymbol{\\theta}, \\boldsymbol{\\phi} \\}$. The training process is different compared to other techniques. Let $\\boldsymbol{x}_{\\Tau_{j}}$ and $\\boldsymbol{y}_{\\Tau_{j}}$ denote all inputs and corresponding outputs in $D_{\\Tau_{j}}^{tr}$. Then, the first $\\ell \\backsim U(0, \\ldots , k \\cdot N -1)$ examples in $D^{tr}_{\\Tau_{j}}$ are used as a conditioning set $D^{c}_{\\Tau_{j}}$ (effectively splitting the support set in a true training set and a validation set). Given a value of $\\ell$, the goal is to maximize the log likelihood (or minimize the negative log likelihood) of the labels $\\boldsymbol{y}_{\\Tau_{j}}$ in the entire support set $D^{tr}_{\\Tau_{j}}$   \n\\begin{align}\n    \\mathcal{L}(\\boldsymbol{\\Theta}) = -\\mathbb{E}_{\\Tau_{j} \\backsim p(\\Tau)}\\left[ \\mathbb{E}_{\\ell \\backsim U(0, \\ldots ,k \\cdot N-1)} \\left( Q_{\\boldsymbol{\\Theta}} (\\boldsymbol{y}_{\\Tau_{j}} | D^{c}_{\\Tau_{j}}, \\boldsymbol{x}_{\\Tau_{j}})  \\right) \\right].\n\\end{align} Conditional neural processes are trained by repeatedly sampling various tasks and values of $\\ell$, and propagating the observed loss backwards. \nIn summary, conditional neural processes use compact representations of previously seen inputs to aid the classification of new observations. Despite its simplicity and elegance, a disadvantage of this technique is that it is often outperformed in few-shot settings by other techniques such as matching networks  (see \\autoref{sec:matchingNets}).", "cites": [1690, 5945], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a clear description of Conditional Neural Processes (CNPs) and their training procedure, integrating information from the relevant paper. However, it remains largely descriptive without offering deeper synthesis of ideas from multiple sources, critical evaluation of CNPs' strengths and weaknesses, or abstraction to broader principles. A brief comparison is made to matching networks, but this is limited and lacks analytical depth."}}
{"id": "ff5e7aee-e340-4313-b252-e44997cc8d48", "title": "Neural Statistician", "level": "subsection", "subsections": [], "parent_id": "8952f75b-bf15-40db-a5e0-2746cfbc8ee4", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Model-based Meta-Learning"], ["subsection", "Neural Statistician"]], "content": "\\begin{figure}[tb]\n    \\centering\n    \\includegraphics[scale=0.4]{images/NeuralStat.png}\n    \\caption{Neural statistician architecture. Edges are neural networks. All incoming inputs to a node are concatenated.}\n    \\label{fig:neuralStat}\n\\end{figure}\nA neural statistician  differs from earlier approaches as it learns to compute \\textit{summary statistics}, or \\textit{meta-features}, of data sets in an unsupervised manner. These latent embeddings (making the approach model-based) can then later be used for making predictions.  Despite the broad applicability of the model, we discuss it in the context of Deep Meta-Learning. \nA neural statistician performs both \\textit{learning} and \\textit{inference}. In the learning phase, the model attempts to produce generative models $\\hat{P}_{i}$ for every data set $D_{i}$. The key assumption that is made by   is that there exists a generative process $P_{i}$, which conditioned on a latent context vector $\\boldsymbol{c}_{i}$, can produce data set $D_{i}$. At inference time, the goal is to infer a (posterior) probability distribution over the context $q(\\boldsymbol{c}|D)$. \nThe model uses a variational autoencoder, which consists of an encoder and decoder. The encoder is responsible for producing a distribution over latent vectors $\\boldsymbol{z}$: $q(\\boldsymbol{z}|\\boldsymbol{x}; \\boldsymbol{\\phi})$, where $\\boldsymbol{x}$ is an input vector, and $\\boldsymbol{\\phi}$ are the encoder parameters. The encoded input $\\boldsymbol{z}$, which is often of lower dimensionality than the original input $\\boldsymbol{x}$, can then be decoded by the decoder $p(\\boldsymbol{x}|\\boldsymbol{z};\\boldsymbol{\\theta})$. Here, $\\boldsymbol{\\theta}$ are the parameters of the decoder. To capture more complex patterns in data sets, the model uses multiple latent layers $\\boldsymbol{z}_{1}, \\ldots ,\\boldsymbol{z}_{L}$, as shown in \\autoref{fig:neuralStat}. Given this architecture, the posterior over $c$ and $\\boldsymbol{z}_{1},..,\\boldsymbol{z}_{L}$ (shorthand $\\boldsymbol{z}_{1:L}$) is given by \n\\begin{align}\n    q(\\boldsymbol{c}, \\boldsymbol{z}_{1:L}| D; \\boldsymbol{\\phi}) = q(\\boldsymbol{c}|D;\\boldsymbol{\\phi}) \\prod_{\\boldsymbol{x} \\in D} q(z_{L}| \\boldsymbol{x}, \\boldsymbol{c};\\boldsymbol{\\phi})\\prod_{i=1}^{L-1} q(\\boldsymbol{z}_{i} | \\boldsymbol{z}_{i+1},\\boldsymbol{x}, \\boldsymbol{c};\\boldsymbol{\\phi}). \n\\end{align} The neural statistician is trained to minimize a three-component loss function, consisting of the reconstruction loss (how well it models the data), context loss (how well the inferred context $q(\\boldsymbol{c}|D;\\boldsymbol{\\phi})$ corresponds to the prior $P(\\boldsymbol{c})$, and latent loss (how well the inferred latent variables $\\boldsymbol{z}_{i}$ are modelled). \nThis model can be applied to $N$-way, few-shot learning as follows. Construct $N$ data sets for every of the $N$ classes, such that one data set contains only examples of the same class. Then, the neural statistician is provided with a new input $\\boldsymbol{x}$, and has to predict its class. It computes a context posterior $N_{\\boldsymbol{x}} = q(\\boldsymbol{c}|\\boldsymbol{x};\\boldsymbol{\\phi})$ depending on new input $\\boldsymbol{x}$. In similar fashion, context posteriors are computed for all of the data sets $N_{i} = q(\\boldsymbol{c}|D_{i};\\boldsymbol{\\phi})$. Lastly, it assigns the label $i$ such that the difference between $N_{i}$ and $N_{\\boldsymbol{x}}$ is minimal. \nIn summary, the neural statistician  allows for quick learning on new tasks through data set modeling. Additionally, it is applicable to both supervised and unsupervised settings. A downside is that the approach requires many data sets to achieve good performance .", "cites": [3639], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear explanation of the neural statistician model, integrating its key components and objectives from the cited paper. It links the model to the broader context of Deep Meta-Learning and explains its application in few-shot learning, showing some synthesis. However, it lacks deeper comparative analysis or evaluation of alternatives, and while it highlights a limitation (need for many datasets), the critical and abstract insights remain somewhat surface-level without identifying broader trends or principles across meta-learning methods."}}
{"id": "51f9828f-4531-4d4f-b652-a9d8ec271d22", "title": "Model-based Techniques, in conclusion", "level": "subsection", "subsections": [], "parent_id": "8952f75b-bf15-40db-a5e0-2746cfbc8ee4", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Model-based Meta-Learning"], ["subsection", "Model-based Techniques, in conclusion"]], "content": "In this section, we have discussed various model-based\\index{meta-learning!model-based} techniques. Despite apparent differences, they all build on  the notion of task internalization. That is, tasks are processed and represented in the state of the model-based system. This state can then be used to make predictions. \n\\autoref{fig:modelbasedrels} displays the relationships between the covered model-based techniques.\nMemory-augmented neural networks (MANNs)  mark the beginning of the deep model-based meta-learning techniques. \nThey use the idea of feeding the entire support set in sequential fashion into the model and then making predictions for the query set inputs using the internal state of the model. \nSuch a model-based approach, where inputs sequentially enter the model was also taken by recurrent meta-learners  in the reinforcement learning setting. \nMeta networks  also use a large black-box solution but generate task-specific weights for every task that is encountered.\nSNAIL  tries to improve the memory capacity and ability to pinpoint memories, which is limited in recurrent neural networks, by using attention mechanisms coupled with special temporal layers.\nLastly, the neural statistician and conditional neural process (CPN) are two techniques that try to learn the meta-features of data sets in an end-to-end fashion.\nThe neural statistician uses the distance between meta-features to make class predictions, while the conditional neural process conditions classifiers on these features.\nAdvantages of model-based approaches include the flexibility of the internal dynamics of the systems, and their broader applicability compared to most metric-based techniques. However, model-based techniques are often outperformed by metric-based techniques in supervised settings (e.g. graph neural networks ; \\autoref{sec:graph}), may not perform well when presented with larger data sets , and generalize less well to more distant tasks than optimization-based techniques . We discuss this optimization-based approach next.  \n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/Model-based.pdf}\n    \\caption{The relationships between the covered model-based meta-learning techniques. The neural statistician and conditional neural process (CNP) form an island in the model-based approaches.}\n    \\label{fig:modelbasedrels}\n\\end{figure}", "cites": [7753, 8050, 286, 7109, 3644], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several model-based techniques by highlighting their shared concept of task internalization and distinguishing their individual characteristics, such as memory-augmented models, attention mechanisms, and meta-feature learning. It provides some critical evaluation by mentioning limitations such as underperformance in supervised settings and poor generalization to distant tasks. The section abstracts these methods to a broader principle of internal state representation for adaptation, but the analysis remains focused rather than offering deeper, unifying insights across the field."}}
{"id": "b53d127e-9914-4151-9d0b-4fa49a5bf198", "title": "Example", "level": "subsection", "subsections": [], "parent_id": "234a50dd-9068-4069-b8b8-ce93f6d7f7f3", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Optimization-based Meta-Learning"], ["subsection", "Example"]], "content": "Suppose we are faced with a linear regression problem, where every task is associated with a different function $f(x)$. For this example, suppose our model only has two parameters: $a$ and $b$, which together form the function $\\hat{f}(x) = ax + b$. Suppose further that our meta-training set consists of four different tasks, i.e., A, B, C, and D. Then, according to the optimization-based view, we wish to find a single set of parameters $\\{a, b\\}$ from which we can quickly learn the optimal parameters for each of the four tasks, as displayed in \\autoref{fig:optExample}. In fact, this is the intuition behind the popular optimization-based technique MAML . \nBy exposing our model to various meta-training tasks, we can update the parameters $a$ and $b$ to facilitate quick adaptation. \n\\begin{figure}[tb]\n    \\centering\n    \\includegraphics[scale=0.6]{images/OptExample.png}\n    \\caption{Example of an optimization-based technique, inspired by .}\n    \\label{fig:optExample}\n\\end{figure}\nWe will now discuss the core optimization-based techniques in more detail.", "cites": [1695], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive example of optimization-based meta-learning, primarily paraphrasing the concept behind MAML using a simplified linear regression scenario. It integrates minimal information from the cited paper and does not compare or synthesize multiple sources. There is no critical evaluation or identification of broader principles, limiting its insight quality."}}
{"id": "f029c32e-8259-42ba-ad3c-3db389d73223", "title": "LSTM Optimizer", "level": "subsection", "subsections": [], "parent_id": "234a50dd-9068-4069-b8b8-ce93f6d7f7f3", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Optimization-based Meta-Learning"], ["subsection", "LSTM Optimizer"]], "content": "\\label{sec:l2l}\nStandard gradient update rules have the form\n\\begin{align}\n    \\boldsymbol{\\theta}_{t+1} := \\boldsymbol{\\theta}_{t} - \\alpha \\nabla_{\\boldsymbol{\\theta}_{t}} \\mathcal{L}_{\\Tau_{j}}(\\boldsymbol{\\theta}_{t}),\\label{eq:gradientdescent}\n\\end{align} where $\\alpha$ is the learning rate, and $\\mathcal{L}_{\\Tau_{j}}(\\boldsymbol{\\theta}_{t})$ is the loss function with respect to task $\\Tau_{j}$ and network parameters at time $t$, i.e., $\\boldsymbol{\\theta}_{t}$. The key idea underlying LSTM optimizers\\index{LSTM optimizer}  is to replace the update term ($- \\alpha \\nabla \\mathcal{L}_{\\Tau_{j}}(\\boldsymbol{\\theta}_{t})$) by an update proposed by an LSTM $g$ with parameters $\\boldsymbol{\\varphi}$. Then, the new update becomes \n\\begin{align}\n    \\boldsymbol{\\theta}_{t+1} := \\boldsymbol{\\theta}_{t} + g_{\\boldsymbol{\\varphi}}(\\nabla_{\\boldsymbol{\\theta}_{t}} \\mathcal{L}_{\\Tau_{j}}(\\boldsymbol{\\theta}_{t})).\n\\end{align}\n\\begin{figure}[tb]\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/L2LbGbG.png}\n    \\caption{Workflow of the LSTM optimizer. Gradients can only propagate backwards through solid edges. $f_{t}$ denotes the observed loss at time step $t$. Source: .}\n    \\label{fig:l2l}\n\\end{figure}\nThis new update allows the optimization strategy to be tailored to a specific family of tasks. Note that this is meta-learning, i.e., the LSTM learns to learn. As such, this technique basically learns an update policy. \nThe loss function used to train an LSTM optimizer is:\n\\begin{align}\n    \\mathcal{L}(\\boldsymbol{\\varphi}) = \\mathbb{E}_{\\mathcal{L}_{\\Tau_{j}}}\\left[ \\sum_{t=1}^{T}w_{t}\\mathcal{L}_{\\Tau_{j}}(\\boldsymbol{\\theta}_{t}) \\right],\n\\end{align} where $T$ is the number of parameter updates that are made, and $w_{t}$ are weights indicating the importance of performance after $t$ steps. \nNote that generally, we are only interested in the final performance after $T$ steps. However, the authors found that the optimization procedure was better guided by equally weighting the performance after each gradient descent step. \nAs is often done, second-order derivatives (arising from the dependency between the updated weights and the LSTM optimizer) were ignored due to the computational expenses associated with the computation thereof. This loss function is fully differentiable and thus allows for training an LSTM optimizer (see \\autoref{fig:l2l}). To prevent a parameter explosion, the same network is used for every \\textit{coordinate}/weight in the base-learner's network, causing the update rule to be the same for every parameter. Of course, the updates depend on their prior values and gradients.\nThe key advantage of LSTM optimizers is that they can enable faster learning compared to hand-crafted optimizers, also on different data sets than those used to train the optimizer. However,  did not apply this technique to few-shot learning. In fact, they did not apply it across tasks at all. Thus, it is unclear whether this technique can perform well in few-shot settings, where few data per class are available for training. Furthermore, the question remains whether it can scale to larger base-learner architectures.", "cites": [7740], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear explanation of the LSTM optimizer approach, integrating the core idea from the cited paper into the broader context of optimization-based meta-learning. It critically points out limitations, such as the lack of application to few-shot learning and scalability concerns. While it offers some abstraction by framing the LSTM optimizer as a learned update policy, it does not extensively connect this method to other optimization-based techniques or present a novel synthesis of multiple ideas."}}
{"id": "4362fe16-5cb1-445d-bad1-c6fffdf11be8", "title": "LSTM Meta-Learner", "level": "subsection", "subsections": [], "parent_id": "234a50dd-9068-4069-b8b8-ce93f6d7f7f3", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Optimization-based Meta-Learning"], ["subsection", "LSTM Meta-Learner"]], "content": "Instead of having an LSTM predict gradient updates,  embed the weights of the base-learner parameters into the cell state (long-term memory component) of the LSTM, giving rise to LSTM meta-learners. \nAs such, the base-learner parameters $\\boldsymbol{\\theta}$ are literally inside the LSTM memory component (cell state). In this way, cell state updates correspond to base-learner parameter updates. This idea was inspired by the resemblance between the gradient and cell state update rules. Gradient updates often have the form as shown in \\autoref{eq:gradientdescent}. The LSTM cell state update rule, in contrast, looks as follows\n\\begin{align}\n    \\boldsymbol{c}_{t} := f_{t} \\odot \\boldsymbol{c}_{t-1} + \\alpha_{t} \\odot \\bar{\\boldsymbol{c}}_{t},\\label{eq:cellstate}\n\\end{align} where $f_{t}$ is the forget gate (which determines which information should be forgotten) at time $t$, $\\odot$ represents the element-wise product, $\\boldsymbol{c}_{t}$ is the cell state at time $t$, and $\\bar{\\boldsymbol{c}}_{t}$ the candidate cell state for time step $t$, and $\\alpha_t$ the learning rate at time step $t$. Note that if $f_{t} = \\boldsymbol{1}$ (vector of ones), $\\alpha_{t} = \\alpha$, $\\boldsymbol{c}_{t-1} = \\boldsymbol{\\theta}_{t-1}$, and $\\bar{\\boldsymbol{c}}_{t} = - \\nabla_{\\boldsymbol{\\theta}_{t-1}}\\mathcal{L}_{\\Tau_{t}}(\\boldsymbol{\\theta}_{t-1})$, this update is equivalent to the one used by gradient-descent. This similarity inspired   to use an LSTM as meta-learner that learns to make updates for a base-learner, as shown in \\autoref{fig:lstmMetaLearner}. \n\\begin{figure}[tb]\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/lstmMetaLearner.png}\n    \\caption{LSTM meta-learner computation graph. Gradients can only propagate backwards through solid edges. The base-learner is denoted as $M$. $(X_{t}, Y_{t})$ are training sets, whereas $(X, Y)$ is the test set. Source: .}\n    \\label{fig:lstmMetaLearner}\n\\end{figure}\nMore specifically, the cell state of the LSTM is initialized with $c_{0} = \\boldsymbol{\\theta}_{0}$, which will be adjusted by the LSTM to a good common initialization point across different tasks. \nThen, to update the weights of the base-learner for the next time step $t+1$, the LSTM computes $\\boldsymbol{c}_{t+1}$ and sets the weights of the base-learner equal to that. \nThere is thus a one-to-one correspondence between $\\boldsymbol{c}_{t}$ and $\\boldsymbol{\\theta}_{t}$. The meta-learner's learning rate $\\alpha_{t}$ (see \\autoref{eq:cellstate}), is set equal to $\\sigma(\\boldsymbol{w}_{\\alpha} \\cdot [\\nabla_{\\theta_{t-1}} \\mathcal{L}_{\\Tau_{t}}(\\boldsymbol{\\theta}_{t-1}), \\mathcal{L}_{\\Tau_{t}}(\\boldsymbol{\\theta}_{t}), \\theta_{t-1}, \\alpha_{t-1}] + \\boldsymbol{b}_{\\alpha})$, where $\\sigma$ is the sigmoid function. Note that the output is a vector, with values between 0 and 1, which denote the the learning rates for the corresponding parameters. Furthermore, $\\boldsymbol{w}_{\\alpha}$ and $\\boldsymbol{b}_{\\alpha}$ are trainable parameters that part of the LSTM meta-learner. In words, the learning rate at any time depends on the loss gradients, the loss value, the previous parameters, and the previous learning rate. The forget gate, $f_{t}$, determines what part of the cell state should be forgotten, and is computed in a similar fashion, but with different weights.\nTo prevent an explosion of meta-learner parameters, weight-sharing is used, in similar fashion to LSTM optimizers proposed by  (\\autoref{sec:l2l}). This implies that the same update rule is applied to every weight at a given time step. The exact update, however, depends on the history of that specific parameter in terms of the previous learning rate, loss, etc. For simplicity, second-order derivatives were ignored, by assuming the base-learner's loss does not depend on the cell state of the LSTM optimizer. Batch normalization was applied to stabilize and speed up the learning process.  \nIn short, LSTM optimizers can learn to optimize a base-learner by maintaining a one-to-one correspondence over time between the base-learner's weights and the LSTM cell state. This allows the LSTM to exploit commonalities in the tasks, allowing for quicker optimization. However, there are simpler approaches (e.g.\\ MAML ) that outperform this technique.", "cites": [1695, 7740], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes information from the cited papers by explaining how LSTM meta-learners were inspired by the similarity between gradient updates and LSTM cell state updates. It integrates concepts from both papers to present a coherent explanation of the approach. The section also provides some critical analysis by noting that simpler approaches like MAML can outperform LSTM-based methods, indicating a recognition of limitations. It abstracts the idea of using an LSTM as a meta-optimizer, though the abstraction is not extended to a broader meta-level framework."}}
{"id": "74c13a92-23e7-440c-a2c5-3c6224868bc1", "title": "MAML", "level": "subsection", "subsections": [], "parent_id": "234a50dd-9068-4069-b8b8-ce93f6d7f7f3", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Optimization-based Meta-Learning"], ["subsection", "MAML"]], "content": "\\label{sec:maml}\n\\begin{figure}[!htb]\n    \\centering\n    \\includegraphics[scale=0.30]{images/maml.png}\n    \\caption{MAML learns an initialization point from which it can perform well on various tasks. Source: .}\n    \\label{fig:maml}\n\\end{figure}\nModel-agnostic meta-learning\\index{model-agnostic meta-learning} (MAML)  uses a simple gradient-based inner optimization procedure (e.g.\\ stochastic gradient descent), instead of more complex LSTM procedures or procedures based on reinforcement learning. The key idea of MAML is to explicitly optimize for fast adaptation to new tasks by learning a good set of initialization parameters $\\boldsymbol{\\theta}$. This is shown in \\autoref{fig:maml}: from the learned initialization $\\boldsymbol{\\theta}$, we can quickly move to the best set of parameters for task $\\Tau_{j}$, i.e., $\\boldsymbol{\\theta}^{*}_{j}$ for $j=1,2,3$. The learned initialization can be seen as the \\textit{inductive bias} of the model, or simply the set of assumptions (encapsulated in $\\boldsymbol{\\theta}$) that the model makes concerning the overall task structure. \nMore formally, let $\\boldsymbol{\\theta}$ denote the initial model parameters of a model. The goal is to quickly learn new concepts, which is equivalent to achieving a minimal loss in few gradient update steps. The amount of gradient steps $s$ has to be specified upfront, such that MAML can explicitly optimize for achieving good performance within that number of steps. Suppose we pick only one gradient update step, i.e., $s=1$. Then, given a task $\\Tau_{j} = (D^{tr}_{\\Tau_{j}}, D^{test}_{\\Tau_{j}})$, gradient descent would produce updated parameters (fast weights)\n\\begin{align}\n    \\boldsymbol{\\theta}'_{j} = \\boldsymbol{\\theta} - \\alpha \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{D^{tr}_{\\Tau_{j}} }(\\boldsymbol{\\theta}),\n\\end{align} specific to task $j$. The \\textit{meta-loss} of quick adaptation (using $s=1$ gradient steps) across tasks can then be formulated as \n\\begin{align}\n    \\mathit{ML} := \\sum_{\\Tau_{j} \\backsim p(\\Tau)} \\mathcal{L}_{D^{test}_{\\Tau_{j}}}(\\boldsymbol{\\theta}'_{j}) = \\sum_{\\Tau_{j} \\backsim p(\\Tau)} \\mathcal{L}_{D^{test}_{\\Tau_{j}}}(\\boldsymbol{\\theta} - \\alpha \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{D^{tr}_{\\Tau_{j}}}(\\boldsymbol{\\theta})),\n\\end{align} where $p(\\Tau)$ is a probability distribution over tasks. This expression contains an inner gradient ($\\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{\\Tau_{j}}(\\boldsymbol{\\theta}_{j})$). As such, by optimizing this meta-loss using gradient-based techniques, we have to compute second-order gradients. One can easily see this in the computation below\n\\begin{align}\n     \\nabla_{\\boldsymbol{\\theta}} \\mathit{ML} &=\\nabla_{\\boldsymbol{\\theta}} \\sum_{\\Tau_{j} \\backsim p(\\Tau)}  \\mathcal{L}_{D^{test}_{\\Tau_{j}}}(\\boldsymbol{\\theta}'_{j}) \\nonumber\\\\\n    &= \\sum_{\\Tau_{j} \\backsim p(\\Tau)} \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{D^{test}_{\\Tau_{j}}}(\\boldsymbol{\\theta}'_{j}) \\nonumber \\\\\n    &= \\sum_{\\Tau_{j} \\backsim p(\\Tau)}  \\mathcal{L}'_{D^{test}_{\\Tau_{j}}}(\\boldsymbol{\\theta}'_{j}) \\nabla_{\\boldsymbol{\\theta}}(\\boldsymbol{\\theta}'_{j}) \\nonumber \\\\\n    &= \\sum_{\\Tau_{j} \\backsim p(\\Tau)}  \\mathcal{L}'_{D^{test}_{\\Tau_{j}}}(\\boldsymbol{\\theta}_{j}') \\nabla_{\\boldsymbol{\\theta}} ( \\boldsymbol{\\theta} - \\alpha \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{D^{tr}_{\\Tau_{j}}(\\boldsymbol{\\theta})}) \\nonumber \\\\\n     &= \\underbrace{\\sum_{\\Tau_{j} \\backsim p(\\Tau)}  \\mathcal{L}'_{D^{test}_{\\Tau_{j}}}(\\boldsymbol{\\theta}_{j}')}_\\textrm{FOMAML} (\\nabla_{\\boldsymbol{\\theta}} \\boldsymbol{\\theta} - \\alpha \\nabla_{\\boldsymbol{\\theta}}^{2} \\mathcal{L}_{D^{tr}_{\\Tau_{j}}}(\\boldsymbol{\\theta})),\\label{eq:maml}\n\\end{align} where we used $\\mathcal{L}'_{D^{test}_{\\Tau_{j}}}(\\boldsymbol{\\theta}_{j}')$ to denote the derivative of the loss function with respect to the query set, evaluated at the post-update parameters $\\boldsymbol{\\theta}_{j}'$. The term $\\alpha \\nabla_{\\boldsymbol{\\theta}}^{2} \\mathcal{L}_{D^{tr}_{\\Tau_{j}}}(\\boldsymbol{\\theta})$ contains the second-order gradients. The computation thereof is expensive in terms of time and memory costs, especially when the optimization trajectory is large (when using a larger number of gradient updates $s$ per task).  experimented with leaving out second-order gradients, by assuming $\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{\\theta}'_{j} = I$, giving us First Order MAML (FOMAML, see \\autoref{eq:maml}). They found that FOMAML performed reasonably similar to MAML. This means that updating the initialization using only first order gradients $\\sum_{\\Tau_{j} \\backsim p(\\Tau)}  \\mathcal{L}'_{D^{test}_{\\Tau_{j}}}(\\boldsymbol{\\theta}_{j}')$ is roughly equal to using the full gradient expression of the meta-loss in \\autoref{eq:maml}. One can extend the meta-loss to incorporate multiple gradient steps by substituting $\\boldsymbol{\\theta}_{j}'$ by a multi-step variant.  \nMAML is trained as follows. The initialization weights $\\boldsymbol{\\theta}$ are updated by continuously sampling a batch of $m$ tasks $B = \\{\\Tau_{j} \\backsim p(\\Tau)\\}_{i=1}^{m}$. Then, for every task $\\Tau_{j} \\in B$, an \\textit{inner update} is performed to obtain $\\boldsymbol{\\theta}_{j}'$, in turn granting an observed loss $\\mathcal{L}_{D^{test}_{\\Tau_{j}}}(\\boldsymbol{\\theta}_{j}')$. These losses across a batch of tasks are used in the \\textit{outer update}\n\\begin{align}\n    \\boldsymbol{\\theta} := \\boldsymbol{\\theta} - \\beta \\nabla_{\\boldsymbol{\\theta}} \\sum_{\\Tau_{j} \\in B} \\mathcal{L}_{D^{test}_{\\Tau_{j}}}(\\boldsymbol{\\theta}_{j}').\n\\end{align} \nThe complete training procedure of MAML is displayed in \\autoref{alg:maml}. At test-time, when presented with a new task $\\Tau_{j}$, the model is initialized with $\\boldsymbol{\\theta}$, and performs a number of gradient updates on the task data. Note that the algorithm for FOMAML is  equivalent to \\autoref{alg:maml}, except for the fact that the update on line 8 is done differently. That is, FOMAML updates the initialization with the rule $\\boldsymbol{\\theta} = \\boldsymbol{\\theta} - \\beta \\sum_{\\Tau_{j} \\backsim p(\\Tau)}  \\mathcal{L}'_{D^{test}_{\\Tau_{j}}}(\\boldsymbol{\\theta}_{j}')$.\n\\begin{algorithm}\n\\caption{One-step MAML for supervised learning, by }\\label{alg:maml}\n\\begin{algorithmic}[1]\n\\State Randomly initialize $\\boldsymbol{\\theta}$\n\\While{not done}\n\\State Sample batch of $J$ tasks $B = \\Tau_{1},\\ldots ,\\Tau_{J} \\backsim p(\\Tau)$\n\\For{$\\Tau_{j} = (D^{tr}_{\\Tau_{j}}, D^{test}_{\\Tau_{j}}) \\in B$}\n\\State Compute $\\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{D^{tr}_{\\Tau_{j}}}(\\boldsymbol{\\theta})$\n\\State Compute $\\boldsymbol{\\theta}_{j}' = \\boldsymbol{\\theta} - \\alpha \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{D^{tr}_{\\Tau_{j}}}(\\boldsymbol{\\theta})$\n\\EndFor\n\\State Update $\\boldsymbol{\\theta} = \\boldsymbol{\\theta} - \\beta \\nabla_{\\boldsymbol{\\theta}}\\sum_{\\Tau_{j} \\in B} \\mathcal{L}_{D^{test}_{\\Tau_{j}}}(\\boldsymbol{\\theta}_{j}')$\n\\EndWhile\n\\end{algorithmic}\n\\end{algorithm} \n, in response to MAML, proposed many technical improvements that can improve training stability, performance, and generalization ability. Improvements include i) updating the initialization $\\boldsymbol{\\theta}$ after every inner update step (instead of after all steps are done) to increase gradient propagation, ii) using second-order gradients only after 50 epochs to increase the training speed, iii) learning layer-wise learning rates to improve flexibility, iv) annealing the meta-learning rate $\\beta$ over time, and v) some Batch Normalization tweaks (keep running statistics instead of batch-specific ones, and using per-step biases). \nMAML has obtained great attention within the field of Deep Meta-Learning, perhaps due to its i)~simplicity (only requires two hyperparameters), ii)~general applicability, and iii)~strong performance. A downside of MAML, as mentioned above, is that it can be quite expensive in terms of running time and memory to optimize a base-learner for every task and compute higher-order derivatives from the optimization trajectories.", "cites": [1695], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a clear synthesis of the MAML method, integrating both the original formulation and subsequent improvements from the literature. It offers a critical discussion by highlighting the computational challenges and noting that FOMAML performs similarly to MAML while being less resource-intensive. The abstraction level is strong, as it interprets MAML in terms of inductive bias and generalizes the idea of learning initializations for fast adaptation."}}
{"id": "fa9d2f4b-e732-4f93-aaec-38dd32503f3c", "title": "iMAML", "level": "subsection", "subsections": [], "parent_id": "234a50dd-9068-4069-b8b8-ce93f6d7f7f3", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Optimization-based Meta-Learning"], ["subsection", "iMAML"]], "content": "Instead of ignoring higher-order derivatives (as done by FOMAML), which potentially decreases the performance compared to regular MAML, iMAML  approximates these derivatives in a way that is less memory-consuming. \nLet $\\mathcal{A}$ denote an inner optimization algorithm (e.g., stochastic gradient descent), which takes a support set $D^{tr}_{\\Tau_{j}}$ corresponding to task $\\Tau_{j}$ and initial model weights $\\boldsymbol{\\theta}$, and produces new weights $\\boldsymbol{\\theta}'_{j} = \\mathcal{A}(\\boldsymbol{\\theta}, D^{tr}_{\\Tau_{j}})$. MAML has to compute the derivative \n\\begin{align}\n    \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{D^{test}_{\\Tau_{j}}} (\\boldsymbol{\\theta}'_{j}) =  \\mathcal{L}_{D^{test}_{\\Tau_{j}}}'(\\boldsymbol{\\theta}'_{j})\\nabla_{\\boldsymbol{\\theta}}(\\boldsymbol{\\theta}'_{j}) ,\\label{eq:implicit1}\n\\end{align} where $D^{test}_{\\Tau_{j}}$ is the query set corresponding to task $\\Tau_{j}$. This equation is a simple result of applying the chain rule. Importantly, note that $\\nabla_{\\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_{j}')$ differentiates through $\\mathcal{A}(\\boldsymbol{\\theta}, D^{tr}_{\\Tau_{j}})$, while $ \\mathcal{L}_{D^{test}_{\\Tau_{j}}}'(\\boldsymbol{\\theta}'_{j})$ does not, as it represents the gradient of the loss function evaluated at $\\boldsymbol{\\theta}'_{j}$.  make use of the following lemma. \n\\textit{If $(\\boldsymbol{I} + \\frac{1}{\\lambda} \\nabla^{2}_{\\boldsymbol{\\theta}}\\mathcal{L}_{D^{tr}_{\\Tau_{j}}}(\\boldsymbol{\\theta}'_{j}))$ is invertible (i.e., $(\\boldsymbol{I} + \\frac{1}{\\lambda} \\nabla^{2}_{\\boldsymbol{\\theta}}\\mathcal{L}_{D^{tr}_{\\Tau_{j}}}(\\boldsymbol{\\theta}'_{j}))^{-1}$ exists), then}\n\\begin{align}\n    \\nabla_{\\boldsymbol{\\theta}}(\\boldsymbol{\\theta}_{j}') = \\left( \\boldsymbol{I} + \\frac{1}{\\lambda} \\nabla^{2}_{\\boldsymbol{\\theta}}\\mathcal{L}_{D^{tr}_{\\Tau_{j}}}(\\boldsymbol{\\theta}'_{j}) \\right)^{-1}. \\label{eq:implicit2}\n\\end{align} Here, $\\lambda$ is a regularization parameter. The reason for this is discussed below.\nCombining \\autoref{eq:implicit1} and \\autoref{eq:implicit2}, we have that \n\\begin{align}\n    \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{D^{test}_{\\Tau_{j}}} (\\boldsymbol{\\theta}'_{j}) = \\mathcal{L}'_{D^{test}_{\\Tau_{j}}}(\\boldsymbol{\\theta}'_{j}) \\left( \\boldsymbol{I} + \\frac{1}{\\lambda} \\nabla^{2}_{\\boldsymbol{\\theta}}\\mathcal{L}_{D^{tr}_{\\Tau_{j}}}(\\boldsymbol{\\theta}'_{j}) \\right)^{-1}  .\n\\end{align}\nThe idea is to obtain an approximate gradient vector $\\boldsymbol{g}_{j}$ that is close to this expression, i.e., we want the difference to be small\n\\begin{align}\n    \\boldsymbol{g}_{j} -  \\mathcal{L}'_{D^{test}_{\\Tau_{j}}}(\\boldsymbol{\\theta}'_{j}) \\left( \\boldsymbol{I} + \\frac{1}{\\lambda} \\nabla^{2}_{\\boldsymbol{\\theta}}\\mathcal{L}_{D^{tr}_{\\Tau_{j}}}(\\boldsymbol{\\theta}'_{j}) \\right)^{-1}   = \\boldsymbol{\\epsilon},\n\\end{align} for some small tolerance vector $\\boldsymbol{\\epsilon}$. If we multiply both sides by the inverse of the inverse factor, i.e., $\\left( \\boldsymbol{I} + \\frac{1}{\\lambda} \\nabla^{2}_{\\boldsymbol{\\theta}}\\mathcal{L}_{D^{tr}_{\\Tau_{j}}}(\\boldsymbol{\\theta}'_{j}) \\right)$, we get \n\\begin{align}\n    \\boldsymbol{g}_{j}^{T} \\left( \\boldsymbol{I} + \\frac{1}{\\lambda} \\nabla^{2}_{\\boldsymbol{\\theta}}\\mathcal{L}_{D^{tr}_{\\Tau_{j}}}(\\boldsymbol{\\theta}'_{j}) \\right) \\boldsymbol{g}_{j} - \\boldsymbol{g}_{j}^{T} \\mathcal{L}'_{D^{test}_{\\Tau_{j}}}(\\boldsymbol{\\theta}'_{j}) = \\boldsymbol{\\epsilon}',\n\\end{align} where $\\boldsymbol{\\epsilon}'$ absorbed the multiplication factor. We wish to minimize this expression for $\\boldsymbol{g}_{j}$, and that can be performed using optimization techniques such as the conjugate gradient algorithm . This algorithm does not need to store Hessian matrices, which decreases the memory cost significantly. In turn, this allows iMAML to work with more inner gradient update steps. Note, however, that one needs to perform explicit regularization in that case to avoid overfitting. The conventional MAML did not require this, as it uses only a few number of gradient steps (equivalent to an early stopping mechanism). \nAt each inner loop step, iMAML computes the meta-gradient $\\boldsymbol{g}_{j}$. After processing a batch of tasks, these gradients are averaged and used to update the initialization $\\boldsymbol{\\theta}$. Since it does not differentiate through the optimization process, we are free to use any other (non-differentiable) inner-optimizer.\nIn summary, iMAML reduces memory costs significantly as it need not differentiate through the optimization trajectory, also allowing for greater flexibility in the choice of inner optimizer. Additionally, it can account for larger optimization paths. The computational costs stay roughly the same compared to MAML . Future work could investigate more inner optimization procedures .", "cites": [1695, 8054], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key concepts from both MAML and the implicit gradient approach, connecting mathematical formulations across the papers to explain how iMAML approximates gradients. It provides a critical perspective by highlighting trade-offs (e.g., need for explicit regularization versus memory savings) and compares iMAML with MAML and FOMAML. The abstraction level is strong, as it generalizes the idea of using implicit differentiation and flexible inner optimizers, which are broader principles in meta-learning."}}
{"id": "ca14897f-906f-4025-9fb9-4930bdd47bcd", "title": "Meta-SGD", "level": "subsection", "subsections": [], "parent_id": "234a50dd-9068-4069-b8b8-ce93f6d7f7f3", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Optimization-based Meta-Learning"], ["subsection", "Meta-SGD"]], "content": "\\label{sec:metasgd}\n\\begin{figure}[tb]\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/MetaSGD.png}\n    \\caption{Meta-SGD learning process. Source: .}\n    \\label{fig:metaSGD}\n\\end{figure}\nMeta-SGD , or meta-stochastic gradient descent, is similar to MAML  (\\autoref{sec:maml}). However, on top of learning an initialization, Meta-SGD also learns learning rates for every model parameter in $\\boldsymbol{\\theta}$, building on the insight that the optimizer can be seen as a trainable entity. \nThe standard SGD update rule is given in \\autoref{eq:gradientdescent}. The meta-SGD optimizer uses a more general update, namely\n\\begin{align}\n    \\boldsymbol{\\theta}_{j}' \\gets \\boldsymbol{\\theta} - \\boldsymbol{\\alpha} \\odot \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{D^{tr}_{\\Tau_{j}}}(\\boldsymbol{\\theta}),\n\\end{align} where $\\odot$ is the element-wise product. Note that this means that alpha (learning rate) is now a vector---hence the bold font--- instead of scalar, which allows for greater flexibility in the sense that each parameter has its own learning rate. The goal is to learn the initialization $\\boldsymbol{\\theta}$, and learning rate vector $\\boldsymbol{\\alpha}$, such that the generalization ability is as large as possible. More mathematically precise, the learning objective is \n\\begin{align}\n    min_{\\boldsymbol{\\alpha}, \\boldsymbol{\\theta}} \\mathbb{E}_{\\Tau_{j} \\backsim p(\\Tau)} [\\mathcal{L}_{D^{test}_{\\Tau_{j}}}(\\boldsymbol{\\theta}_{j}')] = \\mathbb{E}_{\\Tau_{j} \\backsim p(\\Tau)} [\\mathcal{L}_{D^{test}_{\\Tau_{j}}}( \\boldsymbol{\\theta} - \\boldsymbol{\\alpha} \\odot \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{D_{\\Tau_{j}}^{tr}}(\\boldsymbol{\\theta}) )],\n\\end{align} where we used a simple substitution for $\\boldsymbol{\\theta}_{j}'$. $\\mathcal{L}_{D_{\\Tau_{j}}^{tr}}$ and $\\mathcal{L}_{D_{\\Tau_{j}}^{test}}$ are the losses computed on the support and query set respectively. Note that this formulation stimulates generalization ability (as it includes the query set loss $\\mathcal{L}_{D^{test}_{\\Tau_{j}}}$, which can be observed during the meta-training phase). The learning process is visualized in \\autoref{fig:metaSGD}. Note that the meta-SGD optimizer is trained to maximize generalization ability after only one update step. Since this learning objective has a fully differentiable loss function, the meta-SGD optimizer itself can be trained using standard SGD.\nIn summary, Meta-SGD is more expressive than MAML as it does not only learn an initialization but also learning rates per parameter. This, however, does come at the cost of an increased number of hyperparameters.", "cites": [1695], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear and factual description of Meta-SGD, integrating the core idea from the cited paper (MAML) by highlighting the differences in learning rates. However, it lacks deeper synthesis with other works, critical evaluation of limitations, or broader abstraction to meta-principles. The explanation is accurate but primarily descriptive, with limited analytical depth."}}
{"id": "714c6ab6-cd5a-4b08-b1c7-7c17cbe92a44", "title": "Reptile", "level": "subsection", "subsections": [], "parent_id": "234a50dd-9068-4069-b8b8-ce93f6d7f7f3", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Optimization-based Meta-Learning"], ["subsection", "Reptile"]], "content": "Reptile\\index{reptile}  is another optimization-based technique that, like MAML , solely attempts to find a good set of initialization parameters $\\boldsymbol{\\theta}$. The way in which Reptile attempts to find this initialization is quite different from MAML. It repeatedly samples a task, trains on the task, and moves the model weights towards the trained weights . \\autoref{alg:reptile} displays the pseudocode describing this simple process.\n\\begin{algorithm}\n\\caption{Reptile, by }\\label{alg:reptile}\n\\begin{algorithmic}[1]\n\\State Initialize $\\boldsymbol{\\theta}$\n\\For{$i=1,2,\\ldots$}\n\\State Sample task $\\Tau_{j} = (D^{tr}_{\\Tau_{j}}, D^{test}_{\\Tau_{j}})$ and corresponding loss function $\\mathcal{L}_{\\Tau_{j}}$\n\\State $\\boldsymbol{\\theta}'_{j} = SGD(\\mathcal{L}_{D^{tr}_{\\Tau_{j}}}, \\boldsymbol{\\theta}, k)$  \\Comment{Perform $k$ gradient update steps to get $\\boldsymbol{\\theta}_{j}'$ }\n\\State $\\boldsymbol{\\theta} := \\boldsymbol{\\theta} + \\epsilon(\\boldsymbol{\\theta}'_{j} - \\boldsymbol{\\theta})$ \\Comment{Move initialization point $\\boldsymbol{\\theta}$ towards $\\boldsymbol{\\theta}_{j}'$}\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm} \n note that it is possible to treat $( \\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{j}')/\\alpha$ as gradients, where $\\alpha$ is the learning rate of the inner stochastic gradient descent optimizer (line 4 in the pseudocode), and to feed that into a meta-optimizer (e.g.\\ Adam). Moreover, instead of sampling one task at a time, one could sample a batch of $n$ tasks, and move the initialization $\\boldsymbol{\\theta}$ towards the average update direction $\\bar{\\boldsymbol{\\theta}} = \\frac{1}{n}\\sum_{j=1}^{n}(\\boldsymbol{\\theta}'_{j} - \\boldsymbol{\\theta})$, granting the update rule $\\boldsymbol{\\theta} := \\boldsymbol{\\theta} + \\epsilon \\bar{\\boldsymbol{\\theta}}$. \nThe intuition behind Reptile is that updating the initialization weights towards updated parameters will grant a good inductive bias for tasks from the same family. By performing Taylor expansions of the gradients of Reptile and MAML (both first-order and second-order),  show that the expected gradients differ in their direction. They argue, however, that in practice, the gradients of Reptile will also bring the model towards a point minimizing the expected loss over tasks. \nA mathematical argument as to why Reptile works goes as follows. Let $\\boldsymbol{\\theta}$ denote the initial parameters, and $\\boldsymbol{\\theta}^{*}_{j}$ the optimal set of weights for task $\\Tau_{j}$. Lastly, let $d$ be the Euclidean distance function. Then, the goal is to minimize the distance between the initialization point $\\boldsymbol{\\theta}$ and the optimal point $\\boldsymbol{\\theta}^{*}_{j}$, i.e.,\n\\begin{align}\n    min_{\\boldsymbol{\\theta}} \\, \\mathbb{E}_{\\Tau_{j} \\backsim p(\\Tau)}[ \\frac{1}{2}d(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{*}_{j})^{2}].\\label{eq:distanceObj}\n\\end{align}\nThe gradient of this expected distance with respect to the initialization $\\boldsymbol{\\theta}$ is given by \n\\begin{align}\n    \\nabla_{\\boldsymbol{\\theta}} \\mathbb{E}_{\\Tau_{j} \\backsim p(\\Tau)} [\\frac{1}{2} d(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{*}_{j})^{2}]  \n    &=  \\mathbb{E}_{\\Tau_{j} \\backsim p(\\Tau)} [\\frac{1}{2} \\nabla_{\\boldsymbol{\\theta}} d(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{*}_{j})^{2}]   \\nonumber \\\\\n    &= \\mathbb{E}_{\\Tau_{j} \\backsim p(\\Tau)} [\\boldsymbol{\\theta} - \\boldsymbol{\\theta}^{*}_{j}], \n\\end{align} where we used the fact that the gradient of the squared Euclidean distance between two points $\\boldsymbol{x}_{1}$ and $\\boldsymbol{x}_{2}$ is the vector $2(\\boldsymbol{x}_{1} - \\boldsymbol{x}_{2})$.  go on to argue that performing gradient descent on this objective would result in the following update rule\n\\begin{align}\n    \\boldsymbol{\\theta} &= \\boldsymbol{\\theta} - \\epsilon \\nabla_{\\boldsymbol{\\theta}} \\frac{1}{2}d(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{*}_{j})^{2} \\nonumber \\\\\n    &= \\boldsymbol{\\theta} - \\epsilon(\\boldsymbol{\\theta}^{*}_{j} - \\boldsymbol{\\theta}).\n\\end{align} Since we do not know $\\boldsymbol{\\theta}^{*}_{\\Tau_{j}}$, one can approximate this by term by $k$ steps of gradient descent $SGD(\\mathcal{L}_{\\Tau_{j}}, \\boldsymbol{\\theta}, k)$. In short, Reptile can be seen as gradient descent on the distance minimization objective given in \\autoref{eq:distanceObj}. A visualization is shown in \\autoref{fig:reptile}. The initialization $\\boldsymbol{\\theta}$ is moving towards the optimal weights for tasks 1 and 2 in interleaved fashion (hence the oscillations). \n\\begin{figure}[htb]\n    \\centering\n    \\includegraphics[scale=0.3]{images/Reptile.png}\n    \\caption{Schematic visualization of Reptile's learning trajectory. Here, $\\boldsymbol{\\theta}_{1}^*$ and $\\boldsymbol{\\theta}_{2}^*$ are the optimal weights for tasks $\\Tau_{1}$ and $\\Tau_{2}$ respectively. The initialization parameters $\\boldsymbol{\\theta}$ oscillate between these. Adapted from  .}\n    \\label{fig:reptile}\n\\end{figure}\nIn conclusion, Reptile is an extremely simple meta-learning technique, which does not need to differentiate through the optimization trajectory like, e.g., MAML , saving time and memory costs. However, the theoretical foundation is a bit weaker due to the fact that it does not directly optimize for fast learning as done by MAML, and performance may be a bit worse than that of MAML in some settings.", "cites": [7739, 1695], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the key ideas from the cited papers, particularly Paper 2 (MAML) and Paper 1 (first-order meta-learning), by contrasting Reptile with these approaches. It provides a critical analysis by discussing Reptile's simplicity and computational advantages while acknowledging its weaker theoretical foundation and potential performance trade-offs. The section abstracts the concept of meta-learning as a gradient-based minimization of distance to optimal parameters, offering a mathematical interpretation that generalizes beyond the specific implementation."}}
{"id": "93f3a001-89a7-42ca-ab0a-d0e3caacb40c", "title": "Latent embedding optimization (LEO)", "level": "subsection", "subsections": [], "parent_id": "234a50dd-9068-4069-b8b8-ce93f6d7f7f3", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Optimization-based Meta-Learning"], ["subsection", "Latent embedding optimization (LEO)"]], "content": "Latent Embedding Optimization, or LEO, was proposed by  to combat an issue of gradient-based meta-learners, such as MAML (see \\autoref{sec:maml}), in few-shot settings ($N$-way, $k$-shot). These techniques operate in a high-dimensional parameter space using gradient information from only a few examples, which could lead to poor generalization. \n\\begin{figure}[hb]\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/LEO.png}\n    \\caption{Workflow of LEO. adapted from . }\n    \\label{fig:leo}\n\\end{figure}\nLEO alleviates this issue by learning a lower-dimensional latent embedding space, which indirectly allows us to learn a good set of initial parameters $\\boldsymbol{\\theta}$. Additionally, the embedding space is conditioned upon tasks, allowing for more expressivity.  In theory, LEO could find initial parameters for the entire base-learner network, but the authors only experimented with setting the parameters for the final layers. \nThe complete workflow of LEO is shown in \\autoref{fig:leo}. As we can see, given a task $\\Tau_{j}$, the corresponding support set $D^{tr}_{\\Tau_{j}}$ is fed into an encoder, which produces hidden codes for each example in that set. These hidden codes are paired and concatenated in every possible manner, granting us $(Nk)^{2}$ pairs, where $N$ is the number of classes in the training set, and $k$ the number of examples per class. These paired codes are then fed into a relation net  (see \\autoref{sec:relnets}). The resulting embeddings are grouped by class, and parameterize a probability distribution over latent codes $\\boldsymbol{z}_{n}$ (for class $n$) in a low dimensional space $\\mathcal{Z}$. More formally, let $\\boldsymbol{x}^{\\ell}_{n}$ denote the $\\ell$-th example of class $n$ in $D^{tr}_{\\Tau_{j}}$. Then, the mean $\\boldsymbol{\\mu}^{e}_{n}$ and variance $\\boldsymbol{\\sigma}^{e}_{n}$ of a Gaussian distribution over latent codes for class $n$ are computed as\n\\begin{align}\n    \\boldsymbol{\\mu}_{n}^{e}, \\boldsymbol{\\sigma}^{e}_{n} = \\frac{1}{Nk^{2}}\\sum_{\\ell_{p}=1}^{k}\\sum^{N}_{m=1} \\sum_{\\ell_{q}=1}^{k} g_{\\boldsymbol{\\phi}_{r}}\\left( g_{\\boldsymbol{\\phi}_{e}}(\\boldsymbol{x}^{\\ell_{p}}_{n}), g_{\\boldsymbol{\\phi}_{e}}(\\boldsymbol{x}^{\\ell_{q}}_{m})  \\right),\n\\end{align} where $\\boldsymbol{\\phi}_{r}, \\boldsymbol{\\phi}_{e}$ are parameters for the relation net and encoder respectively. Intuitively, the three summations ensure that every example with class $n$ in $D^{tr}_{\\Tau_{j}}$ is paired with every example from all classes $n$. Given $\\boldsymbol{\\mu}_{n}^{e}$, and $\\boldsymbol{\\sigma}_{n}^{e}$, one can sample a latent code $\\boldsymbol{z}_{n} \\backsim N(\\boldsymbol{\\mu}_{n}^{e}, diag(\\boldsymbol{\\sigma}_{n}^{e2}))$ for class $n$, which serves as latent embedding of the task training data.\nThe decoder can then generate a task-specific initialization $\\boldsymbol{\\theta}_{n}$ for class $n$ as follows. First, one computes a mean and variance for a Gaussian distribution using the latent code\n\\begin{align}\n    \\boldsymbol{\\mu}_{n}^{d}, \\boldsymbol{\\sigma}_{n}^{d} = g_{\\boldsymbol{\\phi}_{d}}(\\boldsymbol{z}_{n}).\n\\end{align} These are then used to sample initialization weights $\\boldsymbol{\\theta}_{n} \\backsim N(\\boldsymbol{\\mu}^{d}_{n}, diag(\\boldsymbol{\\sigma}^{d2}_{n}))$. The loss from the generated weights can then be propagated backwards to adjust the embedding space. In practice, generating such a high-dimensional set of parameters from a low-dimensional embedding can be quite problematic. Therefore, LEO uses pre-trained models, and only generates weights for the final layer, which limits the expressivity of the model.   \nA key advantage of LEO is that it optimizes in a lower-dimensional latent embedding space, which aids generalization performance. However, the approach is more complex than e.g.\\ MAML , and its applicability is limited to few-shot learning settings.", "cites": [1695, 7147, 8055], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the key components of LEO by connecting its motivation with MAML and the use of relation networks, building a coherent narrative. It critically evaluates LEO's trade-offs, such as its complexity and limited applicability, and highlights its practical limitations. While it identifies a broader principle (using lower-dimensional spaces to improve generalization), it remains somewhat grounded in the specific method without reaching a high level of abstraction."}}
{"id": "c1cc01a8-66ea-4b9a-b5ac-063dbb63b88a", "title": "Online MAML (FTML)", "level": "subsection", "subsections": [], "parent_id": "234a50dd-9068-4069-b8b8-ce93f6d7f7f3", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Optimization-based Meta-Learning"], ["subsection", "Online MAML (FTML)"]], "content": "Online MAML  is an extension of MAML  to make it applicable to \\textit{online learning} settings . In the online setting, we are presented with a sequence of tasks $\\Tau_{t}$ with corresponding loss functions $\\{ \\mathcal{L}_{\\Tau_{t}} \\}_{t=1}^{T}$, for some potentially infinite time horizon $T$. The goal is to pick a sequence of parameters $\\{ \\boldsymbol{\\theta}_{t} \\}_{t=1}^{T}$ that performs well on the presented loss functions. This objective is captured by the $Regret_{T}$ over the entire sequence, which is defined by  as follows\n\\begin{align}\n    Regret_{T} = \\sum_{t=1}^{T}\\mathcal{L}_{\\Tau_{t}}(\\boldsymbol{\\theta}_{t}') - min_{\\boldsymbol{\\theta}} \\sum_{t=1}^{T}\\mathcal{L}_{\\Tau_{t}}(\\boldsymbol{\\theta}'_{t}),\n\\end{align} where $\\boldsymbol{\\theta}$ are the initial model parameters (just as MAML), and $\\boldsymbol{\\theta}_{t}'$ are parameters resulting from a one-step gradient update (starting from $\\boldsymbol{\\theta}$) on task $t$. Here, the left term reflects the updated parameters chosen by the agent $(\\boldsymbol{\\theta}_{t})$, whereas the right term presents the minimum obtainable loss (in hindsight) from a single fixed set of parameters $\\boldsymbol{\\theta}$. Note that this setup assumes that the agent can make updates to its chosen parameters (transform its initial choice at time $t$ from $\\boldsymbol{\\theta}_{t}$ to $\\boldsymbol{\\theta}_{t}'$).\n propose FTML (Follow The Meta Leader), inspired by FTL (Follow The Leader) , to minimize the regret. The basic idea is to set the parameters for the next time step ($t+1$) equal to the best parameters in hindsight, i.e.,\n\\begin{align}\n    \\boldsymbol{\\theta}_{t+1} := argmin_{\\boldsymbol{\\theta}} \\sum_{k=1}^{t}\\mathcal{L}_{\\Tau_{k}}(\\boldsymbol{\\theta}_{k}').\\label{eq:ftmlGradient} \n\\end{align} The gradient to perform meta-updates is then given by \n\\begin{align}\n    g_{t}(\\boldsymbol{\\theta}) := \\nabla_{\\boldsymbol{\\theta}} \\mathbb{E}_{\\Tau_{k} \\backsim p_{t}(\\Tau)} \\mathcal{L}_{\\Tau_{k}}(\\boldsymbol{\\theta}_{k}'),\n\\end{align} where $p_{t}(\\Tau)$ is a uniform distribution over tasks $1,\\ldots ,t$ (at time $t$).\n\\autoref{alg:ftml} contains the full pseudocode for FTML. In this algorithm, $\\mathit{MetaUpdate}$ performs a few ($N_{meta}$) meta-steps. In each meta-step, a task is sampled from $B$, together with train and test mini-batches to compute the gradient $g_{t}$ in \\autoref{eq:ftmlGradient}. The initialization $\\boldsymbol{\\theta}$ is then updated ($\\boldsymbol{\\theta} := \\boldsymbol{\\theta} - \\beta g_{t}(\\boldsymbol{\\theta})$), where $\\beta$ is the meta-learning rate. Note that the memory usage keeps increasing over time, as at every time step $t$, we append tasks to the buffer $B$, and keep task data sets in memory.  \n\\begin{algorithm}\n\\caption{FTML by }\\label{alg:ftml}\n\\begin{algorithmic}[1]\n\\Require{Performance threshold $\\gamma$}\n\\State Initialize empty task buffer $B$\n\\For{$t = 1,\\ldots$}\n    \\State Initialize data set $D_{t} = \\emptyset$\n    \\State Append $\\Tau_{t}$ to B\n    \\While{$|D_{t}| < N$}\n        \\State Append batch of data $\\{(\\boldsymbol{x}_{i}, y_{i})\\}_{i=1}^{n}$ to $D_{t}$\n        \\State $\\boldsymbol{\\theta}_{t} = \\mathit{MetaUpdate}(\\boldsymbol{\\theta}_{t}, B, t)$\n        \\State Compute $\\boldsymbol{\\theta}'_{t}$\n        \\If{$\\mathcal{L}_{D^{test}_{\\Tau_{t}}}(\\boldsymbol{\\theta}'_{t}) < \\gamma$}\n            \\State Save $|D_{t}|$ as the efficiency for task $\\Tau_{t}$\n        \\EndIf\n    \\EndWhile\n    \\State Save final performance $\\mathcal{L}_{D^{test}_{\\Tau_{t}}}$($\\boldsymbol{\\theta}'_{t}$)\n    \\State $\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_{t}$\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\nIn summary, Online MAML is a robust technique for online-learning . A downside of this approach is the computational costs that keep growing over time, as all encountered data are stored. Reducing these costs is a direction for future work. Also, one could experiment with how well the approach works when more than one inner gradient update steps per task are used, as mentioned by .", "cites": [8056, 1695], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the core concepts from both the MAML and Online Meta-Learning papers, explaining how FTML extends MAML to an online setting and formalizes the notion of regret. It provides a critical point about the growing computational costs due to memory usage. While it abstracts the idea of online meta-learning, it does not fully generalize to broader principles or offer deep comparative analysis with other techniques."}}
{"id": "494d1482-d2af-4129-9c17-da3d1392bc91", "title": "LLAMA", "level": "subsection", "subsections": [], "parent_id": "234a50dd-9068-4069-b8b8-ce93f6d7f7f3", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Optimization-based Meta-Learning"], ["subsection", "LLAMA"]], "content": " mold MAML into a probabilistic framework, such that a probability distribution over task-specific parameters $\\boldsymbol{\\theta}_{j}'$ is learned, instead of a single one. In this way, multiple potential solutions can be obtained for a task. The resulting technique is called LLAMA (Laplace Approximation for Meta-Adaptation). Importantly, LLAMA is only developed for supervised learning settings.\nA key observation is that a neural network $f_{\\boldsymbol{\\theta}'_{j}}$, parameterized by updated parameters $\\boldsymbol{\\theta}'_{j}$ (obtained from few gradient updates using $D^{tr}_{\\Tau_{j}}$), outputs class probabilities $p(y_{i}| \\boldsymbol{x}_{i}, \\boldsymbol{\\theta}'_{j})$.\nTo minimize the error on the query set $D^{test}_{\\Tau_{j}}$, the model must output large probability scores for the true classes.\nThis objective is captured in the maximum log-likelihood loss function\n\\begin{align}\n    \\mathcal{L}_{D^{test}_{\\Tau_{j}}}(\\boldsymbol{\\theta}'_{j}) = - \\sum_{\\boldsymbol{x}_{i},y_{i} \\in D^{test}_{\\Tau_{j}}} log \\, p(y_{i} | \\boldsymbol{x}_{i}, \\boldsymbol{\\theta}'_{j}).\n\\end{align}\nSimply put, if we see a task $j$ as a probability distribution over examples $p_{\\Tau_{j}}$, we wish to maximize the probability that the model predicts the correct class $y_{i}$, given an input $\\boldsymbol{x}_{i}$. This can be done by plain gradient descent, as shown in \\autoref{alg:llama}, where $\\beta$ is the meta-learning rate. Line 4 refers to ML-LAPLACE, which is a subroutine that computes task-specific updated parameters $\\boldsymbol{\\theta}'_{j}$, and estimates the negative log likelihood (loss function) which is used to update the initialization $\\boldsymbol{\\theta}$, as shown in \\autoref{alg:llamaSUB}.  approximated the quadratic curvature matrix $\\hat{H}$ using K-FAC . \nThe trick is that the initialization $\\boldsymbol{\\theta}$ defines a distribution $p(\\boldsymbol{\\theta}'_{j}|\\boldsymbol{\\theta})$ over task-specific parameters $\\boldsymbol{\\theta}'_{j}$. This distribution was taken to be a diagonal Gaussian . Then, to sample solutions for a new task $\\Tau_{j}$, one can simply generate possible solutions $\\boldsymbol{\\theta}'_{j}$ from the learned Gaussian distribution. \n\\begin{algorithm}\n\\caption{LLAMA by }\\label{alg:llama}\n\\begin{algorithmic}[1]\n\\State Initialize $\\boldsymbol{\\theta}$ randomly\n\\While{not converged}\n    \\State Sample a batch of $J$ tasks: $B = \\Tau_{1},\\ldots ,\\Tau_{J} \\backsim p(\\Tau)$\n    \\State Estimate $\\mathbb{E}_{(\\boldsymbol{x}_{i}, y_{i}) \\backsim p_{\\Tau_{j}}}[-log \\, p(y_{i} | \\boldsymbol{x}_{i}, \\boldsymbol{\\theta})] \\, \\forall \\Tau_{j} \\in B$ using ML-LAPLACE\n    \\State $\\boldsymbol{\\theta} = \\boldsymbol{\\theta} - \\beta \\nabla_{\\boldsymbol{\\theta}}\\sum_{j} \\mathbb{E}_{(\\boldsymbol{x}_{i}, y_{i}) \\backsim p_{\\Tau_{j}}}[-log \\, p(y_{i} | \\boldsymbol{x}_{i}, \\boldsymbol{\\theta})$\n\\EndWhile\n\\end{algorithmic}\n\\end{algorithm}\n\\begin{algorithm}\n\\caption{ML-LAPLACE }\\label{alg:llamaSUB}\n\\begin{algorithmic}[1]\n\\State $\\boldsymbol{\\theta}'_{j} = \\boldsymbol{\\theta}$\n\\For{$k=1,\\ldots ,K$}\n    \\State $\\boldsymbol{\\theta}'_{j} = \\boldsymbol{\\theta}'_{j} + \\alpha \\nabla_{\\boldsymbol{\\theta}'_{j}} log \\, p(y_{i} \\in D^{tr}_{\\Tau_{j}}|\\boldsymbol{\\theta}'_{j}, \\boldsymbol{x}_{i} \\in D^{tr}_{\\Tau_{j}})$\n\\EndFor\n\\State Compute curvature matrix $\\hat{H} = \\nabla_{\\boldsymbol{\\theta}'_{j}}^{2}[-log \\, p(y_{i} \\in D^{test}_{\\Tau_{j}}| \\boldsymbol{\\theta}'_{j}, \\boldsymbol{x}_{i} \\in D^{test}_{\\Tau_{j}})] + \\nabla_{\\boldsymbol{\\theta}'_{j}}^{2}[-log \\, p(\\boldsymbol{\\theta}'_{j}|\\boldsymbol{\\theta})]$\n\\State \\Return $-log \\, p(y_{i} \\in D^{test}_{\\Tau_{j}}| \\boldsymbol{\\theta}'_{j}, \\boldsymbol{x}_{i} \\in D^{test}_{\\Tau_{j}}) + \\eta \\,log[det(\\hat{H})]$\n\\end{algorithmic}\n\\end{algorithm}\nIn short, LLAMA extends MAML in a probabilistic fashion, such that one can obtain multiple solutions for a single task, instead of one. This does, however, increase the computational costs. On top of that, the used Laplace approximation (in ML-LAPLACE) can be quite inaccurate .", "cites": [8057, 4662], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates the concepts from the cited papers by reformulating MAML in a probabilistic Bayesian framework as proposed in Paper 1, and connects it to the K-FAC method for curvature estimation from Paper 2. It provides some analytical depth by explaining the probabilistic interpretation and algorithmic structure. However, while it notes computational costs and potential inaccuracy of the Laplace approximation, it could have more critically evaluated the trade-offs or alternatives. The abstraction level is moderate, as it generalizes the probabilistic approach beyond the specific implementation but does not offer overarching theoretical frameworks."}}
{"id": "f870b99e-0952-4aba-8cd2-6847e5a5a855", "title": "PLATIPUS", "level": "subsection", "subsections": [], "parent_id": "234a50dd-9068-4069-b8b8-ce93f6d7f7f3", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Optimization-based Meta-Learning"], ["subsection", "PLATIPUS"]], "content": "PLATIPUS  builds upon the probabilistic interpretation of LLAMA , but learns a probability distribution over initializations $\\boldsymbol{\\theta}$, instead of task-specific parameters $\\boldsymbol{\\theta}_{j}'$. Thus, PLATIPUS allows one to sample an initialization $\\boldsymbol{\\theta} \\backsim p(\\boldsymbol{\\theta})$, which can be updated with gradient descent to obtain task-specific weights (fast weights) $\\boldsymbol{\\theta}_{j}'$.  \n\\begin{algorithm}\n\\caption{PLATIPUS training algorithm by }\\label{alg:platipus}\n\\begin{algorithmic}[1]\n\\State Initialize $\\boldsymbol{\\Theta} = \\{ \\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}, \\boldsymbol{\\sigma}^{2}_{\\boldsymbol{\\theta}}, \\boldsymbol{v}_{q}, \\boldsymbol{\\gamma}_{p}, \\boldsymbol{\\gamma}_{q} \\}$\n\\While{Not done}\n    \\State Sample batch of tasks $B = \\{\\Tau_{j} \\backsim p(\\Tau)\\}_{i=1}^{m}$\n    \\For{$\\Tau_{j} \\in B$}\n        \\State $D^{tr}_{\\Tau_{j}}, D^{test}_{\\Tau_{j}} = \\Tau_{j}$\n        \\State Compute $\\nabla_{\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}} \\mathcal{L}_{D^{test}_{\\Tau_{j}}}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}})$\n        \\State Sample $\\boldsymbol{\\theta} \\backsim q = N(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}} - \\boldsymbol{\\gamma}_{q} \\nabla_{\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}} \\mathcal{L}_{D^{test}_{\\Tau_{j}}}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}), \\boldsymbol{v}_{q})$\n        \\State Compute $\\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{D^{tr}_{\\Tau_{j}}}(\\boldsymbol{\\theta})$\n        \\State Compute fast weights $\\boldsymbol{\\theta}'_{i} = \\boldsymbol{\\theta} - \\alpha \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{D^{tr}_{\\Tau_{j}}}(\\boldsymbol{\\theta})$\n    \\EndFor\n    \\State $p(\\boldsymbol{\\theta} | D^{tr}_{\\Tau_{j}}) = N(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}} - \\boldsymbol{\\gamma}_{p} \\nabla_{\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}} \\mathcal{L}_{D^{tr}_{\\Tau_{j}}}(\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}), \\boldsymbol{\\sigma}^{2}_{\\boldsymbol{\\theta}}  ) $\n    \\State Compute $\\nabla_{\\boldsymbol{\\Theta}}\\left[ \\sum_{\\Tau_{j}} \\mathcal{L}_{D^{test}_{\\Tau_{j}}}(\\boldsymbol{\\phi}_{i}) + D_{\\mathit{KL}}( q(\\boldsymbol{\\theta} | D^{test}_{\\Tau_{j}}), p(\\boldsymbol{\\theta} | D^{tr}_{\\Tau_{j}}) )   \\right]$\n    \\State Update $\\boldsymbol{\\Theta}$ using the Adam optimizer\n\\EndWhile\n\\end{algorithmic}\n\\end{algorithm}\nThe approach is best explained by its pseudocode, as shown in \\autoref{alg:platipus}. In contrast to the original MAML, PLATIPUS introduces five more parameter vectors (line 1). All of these parameters are used to facilitate the creation of Gaussian distributions over prior initializations (or simply priors) $\\boldsymbol{\\theta}$. That is, $\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}$ represents the vector mean of the distributions. $\\boldsymbol{\\sigma}^{2}_{\\boldsymbol{q}}$, and $\\boldsymbol{v}_{q}$ represent the covariances of train and test distributions respectively. $\\boldsymbol{\\gamma}_{x}$ for $x = q,p$ are learning rate vectors for performing gradient steps on distributions $q$ (line 6 and 7) and $P$ (line 11). \nThe key difference with the regular MAML is that instead of having a single initialization point $\\boldsymbol{\\theta}$, we now learn distributions over priors: $q$ and $P$, which are based on query and support data sets of task $\\Tau_{j}$ respectively. Since these data sets come from the same task, we want the distributions $q(\\boldsymbol{\\theta} | D^{test}_{\\Tau_{j}})$, and $p(\\boldsymbol{\\theta} | D^{tr}_{\\Tau_{j}})$ to be close to each other. This is enforced by the KullbackâLeibler divergence ($D_{\\mathit{KL}}$) loss term on line 12, which measures the distance between the two distributions. Importantly, note that $q$ (line 7) and $P$ (line 11) use vector means which are computed with one gradient update steps using the query and support data sets respectively. The idea is that the mean of the Gaussian distributions should be close to the updated mean $\\boldsymbol{\\mu}_{\\boldsymbol{\\theta}}$ because we want to enable fast learning.  As one can see, the training process is very similar to that of MAML  (\\autoref{sec:maml}), with some small adjustments to allow us to work with the probability distributions over $\\boldsymbol{\\theta}$.\nAt test-time, one can simply sample a new initialization $\\boldsymbol{\\theta}$ from the prior distribution $p(\\boldsymbol{\\theta} | D^{tr}_{\\Tau_{j}})$ (note that $q$ cannot be used at test-time as we do not have access to $D^{test}_{\\Tau_{j}}$), and apply a gradient update on the provided support set $D^{tr}_{\\Tau_{j}}$. Note that this allows us to sample multiple potential initializations $\\boldsymbol{\\theta}$ for the given task. \nThe key advantage of PLATIPUS is that it is aware of its uncertainty, which greatly increases the applicability of Deep Meta-Learning in critical domains such as medical diagnosis . Based on this uncertainty, it can ask for labels of some inputs it is unsure about (active learning). A downside to this approach, however, is the increased computational costs, and the fact that it is not applicable to reinforcement learning.", "cites": [7738, 1695, 8057], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively synthesizes the key contributions of PLATIPUS by integrating ideas from MAML and its probabilistic reinterpretation, showing how it extends these approaches. It provides a critical evaluation by highlighting both the advantages (uncertainty awareness, active learning) and limitations (computational cost, inapplicability to reinforcement learning). While it offers some abstraction by framing PLATIPUS in the context of probabilistic modeling, it stops short of identifying broader theoretical principles or trends in the field."}}
{"id": "3ca39351-5a76-4f61-935b-c015a66cbd0a", "title": "Bayesian MAML (BMAML)", "level": "subsection", "subsections": [], "parent_id": "234a50dd-9068-4069-b8b8-ce93f6d7f7f3", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Optimization-based Meta-Learning"], ["subsection", "Bayesian MAML (BMAML)"]], "content": "Bayesian MAML  is another probabilistic variant of MAML that can generate multiple solutions. However, instead of learning a distribution over potential solutions, BMAML simply keeps $M$ possible solutions, and optimizes them in joint fashion. Recall that probabilistic MAMLs (e.g., PLATIPUS) attempt to maximize the data likelihood of task $\\Tau_{j}$, i.e., $p(\\boldsymbol{y}^{test}_{j}| \\boldsymbol{\\theta}'_{j})$, where $\\boldsymbol{\\theta}'_{j}$ are task-specific fast weights obtained by one or more gradient updates.  model this likelihood using Stein Variational Gradient Descent (SVGD) . \nTo obtain $M$ solutions, or equivalently, parameter settings $\\boldsymbol{\\theta}^{m}$, SVGD keeps a set of $M$ \\textit{particles} $\\boldsymbol{\\Theta} = \\{ \\boldsymbol{\\theta}^{m} \\}_{i=1}^{M}$. At iteration $t$, every $\\boldsymbol{\\theta}_{t} \\in \\boldsymbol{\\Theta}$ is updated as follows\n\\begin{align}\n    \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_{t} + \\epsilon (\\phi (\\boldsymbol{\\theta}_{t})) \\\\ \\text{ where } \\phi(\\boldsymbol{\\theta}_{t}) = \\frac{1}{M}\\sum_{m=1}^{M} \\left[ k(\\boldsymbol{\\theta}^{m}_{t}, \\boldsymbol{\\theta}_{t}) \\nabla_{\\boldsymbol{\\theta}^{m}_{t}} log \\, p(\\boldsymbol{\\theta}_{t}^{m}) + \\nabla_{\\boldsymbol{\\theta}_{t}^{m}}k(\\boldsymbol{\\theta}^{m}_{t}, \\boldsymbol{\\theta}_{t}) \\right]. \n\\end{align} Here, $k(\\boldsymbol{x}, \\boldsymbol{x}')$ is a similarity kernel between $\\boldsymbol{x}$ and $\\boldsymbol{x}'$. The authors used a radial basis function (RBF) kernel, but in theory, any other kernel could be used. Note that the update of one particle depends on the other gradients of particles. The first term in the summation ($k(\\boldsymbol{\\theta}^{m}_{t}, \\boldsymbol{\\theta}_{t}) \\nabla_{\\boldsymbol{\\theta}^{m}_{t}} log \\, p(\\boldsymbol{\\theta}_{t}^{m})$) moves the particle in the direction of the gradients of other particles, based on particle similarity. The second term ($\\nabla_{\\boldsymbol{\\theta}_{t}^{m}}k(\\boldsymbol{\\theta}^{m}_{t}, \\boldsymbol{\\theta}_{t})$) ensures that particles do not collapse (repulsive force) . \nThese particles can then be used to approximate the probability distribution of the test labels\n\\begin{align}\n    p(\\boldsymbol{y}^{test}_{j}| \\boldsymbol{\\theta}'_{j}) \\approx \\frac{1}{M} \\sum_{m=1}^{M} p(\\boldsymbol{y}_{j}^{test} | \\boldsymbol{\\theta}^{m}_{\\Tau_{j}}),\n\\end{align} where $\\boldsymbol{\\theta}_{\\Tau_{j}}^{m}$ is the $m$-th particle obtained by training on the support set $D^{tr}_{\\Tau_j}$ of task $\\Tau_{j}$.\n proposed a new meta-loss to train BMAML, called the \\textit{Chaser Loss}. This loss relies on the insight that we want the approximated parameter distribution (obtained from the support set $p^{n}_{\\Tau_{j}}(\\boldsymbol{\\theta}_{\\Tau_{j}} | D^{tr}, \\boldsymbol{\\Theta}_{0})$) and true distribution $p^{\\infty}_{\\Tau_{j}}(\\boldsymbol{\\theta}_{\\Tau_{j}}|D^{tr} \\cup D^{test})$ to be close to each other (since the task is the same). Here, $n$ denotes the number of SVGD steps, and $\\boldsymbol{\\Theta}_{0}$ is the set of initial particles, in similar fashion to the initial parameters $\\boldsymbol{\\theta}$ seen by MAML. \nSince the true distribution is unknown,  approximate it by running SVGD for $s$ additional steps, granting us the \\textit{leader} $\\boldsymbol{\\Theta}^{n + s}_{\\Tau_{j}}$, where the $s$ additional steps are performed on the combined support and query set. The intuition is that as the number of updates increases, the obtained distributions become more like the true ones. $\\boldsymbol{\\Theta}^{n}_{\\Tau_{j}}$ in this context is called the \\textit{chaser} as it wants to get closer to the leader. The proposed meta-loss is then given by \n\\begin{align}\n    \\mathcal{L}_{BMAML}(\\boldsymbol{\\Theta}_{0}) = \\sum_{\\Tau_{j} \\in B}\\sum_{m=1}^{M} || \\boldsymbol{\\theta}_{\\Tau_{j}}^{n,m} - \\boldsymbol{\\theta}_{\\Tau_{j}}^{n+s,m} ||^{2}_{2}. \n\\end{align} \nThe full pseudocode of BMAML is shown in \\autoref{alg:baymaml}. Here, $\\boldsymbol{\\Theta}^{n}_{\\Tau_{j}}(\\boldsymbol{\\Theta}_{0})$ denotes the set of particles after $n$ updates on task $\\Tau_{j}$, and $SG$ means ``stop gradients\" (we do not want the leader to depend on the initialization, as the leader must lead).  \n\\begin{algorithm}\n\\caption{BMAML by }\\label{alg:baymaml}\n\\begin{algorithmic}[1]\n\\State Initialize $\\boldsymbol{\\Theta}_{0}$\n\\For{$t=1,\\ldots$ until convergence}\n\\State Sample a batch of tasks B from $p(\\Tau)$\n\\For{task $\\Tau_{j} \\in B$}\n\\State Compute chaser $\\boldsymbol{\\Theta}^{n}_{\\Tau_{j}}(\\boldsymbol{\\Theta}_{0}) = SVGD_{n}(\\boldsymbol{\\Theta}_{0}; D^{tr}_{\\Tau_{j}}, \\alpha)$\n\\State Compute leader $\\boldsymbol{\\Theta}^{n+s}_{\\Tau_{j}}(\\boldsymbol{\\Theta}_{0}) = SVGD_{s}(\\boldsymbol{\\Theta}^{n}_{\\Tau_{j}}(\\boldsymbol{\\Theta}_{0}); D^{tr}_{\\Tau_{j}} \\cup D^{test}_{\\Tau_j}, \\alpha)$\n\\EndFor\n\\State $\\boldsymbol{\\Theta}_{0} = \\boldsymbol{\\Theta}_{0} - \\beta \\nabla_{\\boldsymbol{\\Theta}_{0}}\\sum_{\\Tau_{j} \\in B} d(\\boldsymbol{\\Theta}^{n}_{\\Tau_{j}}(\\boldsymbol{\\Theta}_{0}), SG(\\boldsymbol{\\Theta}^{n+s}_{\\Tau_{j}}(\\boldsymbol{\\Theta}_{0})))$\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\nIn summary, BMAML is a robust optimization-based meta-learning technique that can propose $M$ potential solutions to a task. Additionally, it is applicable to reinforcement learning by using Stein Variational Policy Gradient instead of SVGD. A downside of this approach is that one has to keep $M$ parameter sets in memory, which does not scale well. Reducing the memory costs is a direction for future work . Furthermore, SVGD is sensitive to the selected kernel function, which was pre-defined in BMAML. However,  point out that it may be beneficial to learn the kernel function instead. This is another possibility for future research.", "cites": [8058, 8059], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the key concepts from the cited papers, explaining how BMAML integrates SVGD and Bayesian principles. It also offers critical analysis by pointing out limitations such as memory constraints and kernel sensitivity, and suggests potential improvements. The discussion abstracts beyond the specific method to highlight broader issues in meta-learning and Bayesian approaches."}}
{"id": "4262bff4-4e32-48f9-87a2-f11d7cc6f279", "title": "Simple Differentiable Solvers", "level": "subsection", "subsections": [], "parent_id": "234a50dd-9068-4069-b8b8-ce93f6d7f7f3", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Optimization-based Meta-Learning"], ["subsection", "Simple Differentiable Solvers"]], "content": " take a quite different approach. That is, they pick simple base-learners that have an analytical closed-form solution. The intuition is that the existence of a closed-form solution allows for good learning efficiency. They propose two techniques using this principle, namely \\textbf{R2-D2} (Ridge Regression Differentiable Discriminator), and \\textbf{LR-D2} (Logistic Regression Differentiable Discriminator). We cover both in turn. \nLet $g_{\\boldsymbol{\\phi}}: X \\rightarrow \\mathbb{R}^{e}$ be a pre-trained input embedding model (e.g.\\ a CNN), which outputs embeddings with a dimensionality of $e$. Furthermore, assume that we use a linear predictor function $f(g_{\\boldsymbol{\\phi}}(\\boldsymbol{x}_{i})) = g_{\\boldsymbol{\\phi}}(\\boldsymbol{x}_{i})W$, where $W$ is a $e \\times o$ weight matrix, and $o$ is the output dimensionality (of the label). When using (regularized) Ridge Regression (done by R2-D2), one uses the optimal $W$, i.e.,\n\\begin{align}\n    W^{*} &= \\argmin_{W} \\, || XW - Y||^{2}_{2} + \\gamma ||W||^{2} \\nonumber \\\\\n    &= (X^{T}X + \\gamma I)^{-1}X^{T}Y, \\label{eq:ridgeregression}\n\\end{align} where $X \\in \\mathbb{R}^{n \\times e}$ is the input matrix, containing $n$ rows (one for each embedded input $g_{\\boldsymbol{\\phi}}(\\boldsymbol{x}_{i})$), $Y \\in \\mathbb{R}^{n \\times o}$ is the output matrix with correct outputs corresponding to the inputs, and $\\gamma$ is a regularization term to prevent overfitting. Note that the analytical solution contains the term $(X^{T}X) \\in \\mathbb{R}^{e \\times e}$, which is quadratic in the size of the embeddings. Since $e$ can become quite large when using deep neural networks,  use Woodburry's identity\n\\begin{align}\n    W^{*} = X^{T}(XX^{T} + \\gamma I)^{-1} Y,\n\\end{align} where $XX^{T} \\in \\mathbb{R}^{n \\times n}$ is linear in the embedding size, and quadratic in the number of examples, which is more manageable in few-shot settings, where $n$ is very small. To make predictions with this Ridge Regression based model, one can compute \n\\begin{align}\n    \\hat{Y} = \\alpha X_{test}W^{*} + \\beta,\n\\end{align} where $\\alpha$ and $\\beta$ are hyperparameters of the base-learner that can be learned by the meta-learner, and $X_{test} \\in \\mathbb{R}^{m \\times e}$ corresponds to the $m$ test inputs of a given task. Thus, the meta-learner needs to learn $\\alpha, \\beta, \\gamma$, and $\\boldsymbol{\\phi}$ (embedding weights of the CNN).  \nThe technique can also be applied to iterative solvers when the optimization steps are differentiable . LR-D2 uses the Logistic Regression objective and Newton's method as solver. Outputs $\\boldsymbol{y} \\in \\{-1,+1\\}^{n}$ are now binary. Let $\\boldsymbol{w}$ denote a parameter row of our linear model (parameterized by $W$). Then, the $i$-th iteration of Newton's method updates $\\boldsymbol{w}_{i}$ as follows\n\\begin{align}\n    \\boldsymbol{w}_{i} = (X^{T}\\mbox{diag}(\\boldsymbol{s}_{i})X + \\gamma I)^{-1}X^{T}\\mbox{diag}(\\boldsymbol{s}_{i})\\boldsymbol{z}_{i}, \n\\end{align} where $\\boldsymbol{\\mu}_{i} = \\sigma(\\boldsymbol{w}^{T}_{i-1}X)$, $\\boldsymbol{s}_{i} = \\boldsymbol{\\mu}_{i}(1 - \\boldsymbol{\\mu}_{i})$, $\\boldsymbol{z}_{i} = \\boldsymbol{w}^{T}_{i-1}X + (\\boldsymbol{y} - \\boldsymbol{\\mu}_{i})/\\boldsymbol{s}_{i}$, and $\\sigma$ is the sigmoid function. Since the term $X^{T}\\mbox{diag}(\\boldsymbol{s}_{i})X$ is a matrix of size $e \\times e$, and thus again quadratic in the embedding size, Woodburry's identity is also applied here to obtain\n\\begin{align}\n    \\boldsymbol{w}_{i} = X^{T}(XX^{T} + \\lambda \\mbox{diag}(\\boldsymbol{s}_{i})^{-1})^{-1}\\boldsymbol{z}_{i},\n\\end{align} making it quadratic in the input size, which is not a big problem since $n$ is small in the few-shot setting. The main difference compared to R2-D2 is that the base-solver has to be run for multiple iterations to obtain $W$.\nIn the few-shot setting, the base-level optimizers compute the weight matrix $W$ for a given task $\\Tau_{i}$. The obtained loss on the query set of a task $\\mathcal{L}_{D_{test}}$ is then used to update the parameters $\\boldsymbol{\\phi}$ of the input embedding function (e.g.\\ CNN) and the hyperparameters of the base-learner.\n have done similar work to , but with linear Support Vector Machines (SVMs) as base-learner. Their approach is dubbed \\textbf{MetaOptNet} and achieved state-of-the-art performance on few-shot image classification. \nIn short, simple differentiable solvers are simple, reasonably fast in terms of computation time, but limited to few-shot learning settings. Investigating the use of other simple base-learners is a direction for future work.", "cites": [8060], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes two techniques, R2-D2 and LR-D2, and their mathematical formulations. It integrates the core idea from the cited paper about using differentiable convex optimization for meta-learning but does so in a largely descriptive manner. There is minimal critical evaluation or comparison of these methods with others, and the generalization is limited to a brief note on future work without identifying broader principles."}}
{"id": "d2d8669d-05ff-491d-93fd-c8cffc5b1476", "title": "Optimization-based Techniques, in conclusion", "level": "subsection", "subsections": [], "parent_id": "234a50dd-9068-4069-b8b8-ce93f6d7f7f3", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Optimization-based Meta-Learning"], ["subsection", "Optimization-based Techniques, in conclusion"]], "content": "Optimization-based\\index{meta-learning!optimization-based} aim to learn new tasks quickly through (learned) optimization procedures. Note that this closely resembles base-level learning, which also occurs through optimization (e.g., gradient descent). However, in contrast to base-level techniques, optimization-based meta-learners can learn the optimizer and/or are exposed to multiple tasks, which allows them to learn how to learn new tasks quickly.   \n\\autoref{fig:optbasedrels} shows the relationships between the covered optimization-based techniques.\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/Optimization-based.pdf}\n    \\caption{The relationships between the covered optimization-based meta-learning techniques. As one can see, MAML has a central position in this graph of techniques as it has inspired many other works.}\n    \\label{fig:optbasedrels}\n\\end{figure}\nAs we can see, the LSTM optimizer , which replaces hand-crafted optimization procedures such as gradient descent by a trainable LSTM, can be seen as the starting point for these optimization-based meta-learning techniques. \n also aim to learn the optimization procedure with reinforcement learning instead of gradient-based methods. \nThe LSTM meta-learner  extends the LSTM optimizer to the few-shot setting by not only learning the optimization procedure but also a good set of initial weights.\nThis way, it can be used across tasks.\nMAML  is a simplification of the LSTM meta-learner as it replaces the trainable LSTM optimizer by hand-crafted gradient descent.\nMAML has received considerable attention within the field of deep meta-learning, and has, as one can see, inspired many other works. \nMeta-SGD is an enhancement of MAML that not only learns the initial parameters, but also the learning rates . \nLLAMA , PLATIPUS , and online MAML  extend MAML to the active and online learning settings. \nLLAMA and PLATIPUS are probabilistic interpretations of MAML, which allow them to sample multiple solutions for a given task and quantify their uncertainty.\nBMAML  takes a more discrete approach as it jointly optimizes a discrete set of $M$ initializations. \niMAML  aims to overcome the computational expenses associated with the computation of second-order derivatives, which is needed by MAML. \nThrough implicit differentiation, they also allow for the use of non-differentiable inner loop optimization procedures. \nReptile  is an elegant first-order meta-learning algorithm for finding a set of initial parameters and removes the need for computing higher-order derivatives. \nLEO  tries to improve the robustness of MAML by optimizing in lower-dimensional parameter space through the use of an encoder-decoder architecture. \nLastly, R2-D2, LR-D2 , and  use simple classical machine learning methods (ridge regression, logistic regression, SVM, respectively) as a classifier on top of a learned feature extractor.  \nA key advantage of optimization-based approaches is that they can achieve better performance on wider task distributions than, e.g., model-based approaches . However, optimization-based techniques optimize a base-learner for every task that they are presented with and/or learn the optimization procedure, which is computationally expensive . \nOptimization-based meta-learning is a very active area of research. We expect future work to be done in order to reduce the computational demands of these methods and improve the solution quality and level of generalization. We think that benchmarking and reproducibility research will play an important role in these improvements.", "cites": [7738, 8054, 8060, 8056, 8059, 7739, 7740, 8057, 7109, 1695, 8055], "cite_extract_rate": 0.6875, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers by tracing the evolution and relationships between optimization-based meta-learning techniques, particularly around the MAML framework. It provides critical insights by highlighting computational limitations and proposing future research directions. The abstraction is strong as it identifies key advantages and challenges of the entire class of optimization-based methods, not just individual papers."}}
{"id": "f42888d1-564d-4853-92f9-c484bfa0845c", "title": "Overview", "level": "subsection", "subsections": [], "parent_id": "40e0aa82-f23b-412b-9c67-ba9c21cd288b", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Concluding Remarks"], ["subsection", "Overview"]], "content": "In recent years, there has been a shift in focus in the broad meta-learning community. Traditional algorithm selection and hyperparameter optimization for classical machine learning techniques (e.g.\\ Support Vector Machines, Logistic Regression, Random Forests, etc.) have been augmented by Deep Meta-Learning, or equivalently, the pursuit of self-improving neural networks that can leverage prior learning experience to learn new tasks more quickly. Instead of training a new model from scratch for different tasks, we can use the same (meta-learning) model across tasks. As such, meta-learning can widen the applicability of powerful deep learning techniques to domains where fewer data are available and computational resources are limited.\nDeep Meta-Learning techniques are characterized by their meta-objective, which allows them to maximize performance across various tasks, instead of a single one, as is the case in base-level learning objectives. This meta-objective is reflected in the training procedure of meta-learning methods, as they learn on a set of different meta-training tasks. The few-shot setting lends itself nicely towards this end, as tasks consist of few data points. This makes it computationally feasible to train on many different tasks, and it allows us to evaluate whether a neural network can learn new concepts from few examples. \nTask construction for training and evaluation does require some special attention. That is, it has been shown beneficial to match training and test conditions , and perhaps train in a more difficult setting than the one that will be used for evaluation . \nOn a high level, there are three categories of Deep Meta-Learning techniques, namely i)~metric-\\index{meta-learning!metric-based}, ii)~model-\\index{meta-learning!model-based}, and iii)~optimization-based\\index{meta-learning!optimization-based} ones, which rely on i) computing input similarity, ii) task embeddings\\index{task embedding} with states, and iii) task-specific updates, respectively. Each approach has strengths and weaknesses. Metric-learning techniques are simple and effective  but are not readily applicable outside of the supervised learning setting . Model-based techniques, on the other hand, can have very flexible internal dynamics, but lack generalization ability to more distant tasks than the ones used at meta-train time . Optimization-based approaches have shown greater generalizability, but are in general computationally expensive, as they optimize a base-learner for every task .\n\\autoref{tab:categorization} provides a concise, tabular overview of these approaches. Many techniques have been proposed for each one of the categories, and the underlying ideas may vary greatly, even within the same category. \\autoref{tab:overview}, therefore, provides an overview of all methods and key ideas that we have discussed in this work, together with their applicability to supervised learning (SL) and reinforcement learning (RL) settings, key ideas, and benchmarks that were used for testing them. \n\\autoref{tab:performance} displays an overview of the 1- and 5-shot classification performances (reported by the original authors) of the techniques on the frequently used miniImageNet benchmark. \nMoreover, it displays the used backbone (feature extraction module) as well as the final classification mechanism. \nFrom this table, it becomes clear that the 5-shot performance is typically better than the 1-shot performance, indicating that data scarcity is a large bottleneck for achieving good performance. \nMoreover, there is a strong relationship between the expressivity of the backbone and the performance. \nThat is, deeper backbones tend to give rise to better classification performance.\nThe best performance is achieved by MetaOptNet, yielding a 1-shot accuracy of $64.09$\\% and a 5-shot accuracy of $80.00$\\%.\nNote however that MetaOptNet used a deeper backbone than most of the other techniques.\n\\begin{table}[phtb]\n    \\begin{tabularx}{\\linewidth}{lllll}\n        \\toprule \n         Name & Backbone & Classifier & 1-shot & 5-shot \\\\\n         \\midrule \n         \\textbf{Metric-based} \\\\\n         \\, Siamese nets & - & - & -   \\\\\n         \\, Matching nets & 64-64-64-64 & Cosine sim. & $43.56 \\pm 0.84$ & $55.31 \\pm 0.73$     \\\\\n         \\, Prototypical nets &  64-64-64-64 & Euclidean dist. & $49.42 \\pm 0.78$ & $68.20 \\pm 0.66$  \\\\\n         \\, Relation nets &64-96-128-256 &  Sim. network & $50.44 \\pm 0.82$ & $65.32 \\pm 0.70$ \\\\\n         \\, ARC & - & 64-1 dense & $49.14 \\pm -$ & - \\\\\n         \\, GNN  & 64-96-128-256 & Softmax & $50.33 \\pm 0.36$& $66.41 \\pm 0.63$ \\\\\n         \\midrule\n         \\textbf{Model-based}  \\\\\n         \\, RMLs  & - & - & - \\\\\n         \\, MANNs & - & - & - \\\\\n         \\, Meta nets & 64-64-64-64-64 & 64-Softmax & $49.21 \\pm 0.96$ & - \\\\\n         \\, SNAIL  & Adj. ResNet-12 &  Softmax & $55.71 \\pm 0.99$ & $68.88 \\pm 0.92$ \\\\\n         \\, CNP  & - & - & - \\\\\n         \\, Neural stat. & - & - & - \\\\\n         \\midrule\n         \\textbf{Opt.-based} &  & &  \\\\\n         \\, LSTM optimizer & - & - & -  \\\\\n         \\, LSTM ml. & 32-32-32-32 & Softmax & $43.44 \\pm 0.77$ & $60.60 \\pm 0.71$  \\\\\n         \\, RL optimizer  & - & - & -  \\\\\n         \\, MAML  & 32-32-32-32 & Softmax & $48.70 \\pm 1.84$ & $63.11 \\pm 0.92$  \\\\\n         \\, iMAML  & 64-64-64-64 & Softmax & $49.30 \\pm 1.88$ & - \\\\\n         \\, Meta-SGD  & 64-64-64-64 & Softmax & $50.47 \\pm 1.87$ & $64.03 \\pm 0.94$ \\\\\n         \\, Reptile  & 32-32-32-32 & Softmax & $48.21 \\pm 0.69$& $66.00 \\pm 0.62$  \\\\\n         \\, LEO  & WRN-28-10 & Softmax & $61.76 \\pm 0.08$ & $77.59 \\pm 0.12$  \\\\\n         \\, Online MAML  & - & -& - \\\\\n         \\, LLAMA  & 64-64-64-64 & Softmax & $49.40 \\pm 1.83$ & -  \\\\\n         \\, PLATIPUS & - & -& - \\\\\n         \\, BMAML & 64-64-64-64-64 & Softmax & $53.80 \\pm 1.46$ & - \\\\\n         \\, Diff. solvers  & \\\\\n         \\, \\, \\, \\, R2-D2 & 96-192-384-512 & Ridge regr. &$51.8 \\pm 0.2$ & $68.4 \\pm 0.2$ \\\\\n         \\, \\, \\, \\, LR-D2 &  96-192-384-512 & Log. regr. & $51.90 \\pm 0.20$ & $68.70 \\pm 0.20$ \\\\\n         \\, \\, \\, \\, MetaOptNet & ResNet-12 & SVM & {$\\boldsymbol{64.09 \\pm 0.62}$} & {$\\boldsymbol{80.00 \\pm 0.45}$} \\\\\n         \\bottomrule\n    \\end{tabularx}\n    \\caption{Comparison of the accuracy scores of the covered meta-learning techniques on 1- and 5-shot miniImageNet classification. Scores are taken from the original papers. The $\\pm$ indicates the 95\\% confidence interval. The backbone is the used feature extraction module. The classifier column shows the final layer(s) that were used to transform the features into class predictions. Used abbreviations: ``sim.\": similarity, ``Adj.\": adjusted, and ``dist.\": distance, ``log.\": logistic, ``regr.\": regression, ``ml.\": meta-learner, ``opt.\": optimization.}\n    \\label{tab:performance}\n\\end{table}", "cites": [286, 1690, 7109, 8757], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple cited papers by categorizing methods into metric-, model-, and optimization-based approaches and comparing their performance on the miniImageNet benchmark. It offers some abstraction by identifying general trends such as the impact of backbone depth and data scarcity. However, the critical analysis is limited, as it primarily reports results and mentions general limitations without deeply evaluating the strengths, weaknesses, or trade-offs of the individual methods."}}
{"id": "5b25361a-1ca6-4faf-8191-f3d98b07632b", "title": "Open Challenges and Future Work", "level": "subsection", "subsections": [], "parent_id": "40e0aa82-f23b-412b-9c67-ba9c21cd288b", "prefix_titles": [["title", "A Survey of Deep Meta-Learning\n"], ["section", "Concluding Remarks"], ["subsection", "Open Challenges and Future Work"]], "content": "Despite the great potential of Deep Meta-Learning techniques, there are still open challenges, which we discuss here.  \n\\autoref{fig:depthandperformance}  in Section~\\ref{sec:intro} displays the accuracy scores of the covered meta-learning techniques on 1-shot miniImageNet classification. \nTechniques that were not tested in this setting by the original authors are omitted.\nAs we can see, the performance of the techniques is related to the expressivity of the used backbone (ordered in increasing order on the x-axis).\nFor example, the best-performing techniques, LEO and MetaOptNet, use the largest network architectures.\nMoreover, the fact that different techniques use different backbones poses a problem as it is difficult to fairly compare their classification performance.\nAn obvious question arises to which degree the difference in performance is due to methodological improvements, or due to the fact that a better backbone architecture was chosen.\nFor this reason, we think that it would be useful to perform a large-scale benchmark test where techniques are compared when they use the same backbones. \nThis would also allow us to get a more clear idea of how the expressivity of the feature extraction module affects the performance. \nAnother challenge of Deep Meta-Learning techniques is that they can be susceptible to the \\textit{memorization problem} (meta-overfitting), where the neural network has memorized tasks seen at meta-training time and fails to generalize to new tasks. More research is required to better understand this problem.  Clever task design and meta-regularization may prove useful to avoid such problems . \nAnother problem is that most of the meta-learning techniques discussed in this work are evaluated on narrow benchmark sets. This means that the data that the meta-learner used for training are not too distant from the data used for evaluating its performance. As such, one may wonder how well these techniques are able to adapt to  more distant tasks.  showed that the ability to adapt to new tasks decreases as they become more distant from the tasks seen at training time. Moreover, a simple non-meta-learning baseline (based on pre-training and fine-tuning) can outperform state-of-the-art meta-learning techniques when meta-test tasks come from a different data set than the one used for meta-training.\nIn reaction to these findings,  have recently proposed the Meta-Dataset benchmark, which consists of various previously used meta-learning benchmarks such as Omniglot  and ImageNet . This way, meta-learning techniques can be evaluated in more challenging settings where tasks are diverse. Following , we think that this new benchmark can prove to be a good means towards the investigation and development of meta-learning algorithms for such challenging scenarios. \nAs mentioned earlier in this section, Deep Meta-Learning has the appealing prospect of widening the applicability of deep learning techniques to more real-world domains. For this, increasing the generalization ability of these techniques is very important. Additionally, the computational costs associated with the deployment of meta-learning techniques should be small. While these techniques can learn new tasks quickly, meta-training can be quite computationally expensive. Thus, decreasing the required computation time and memory costs of Deep Meta-Learning techniques remains an open challenge. \nSome real-world problems demand systems that can perform well in online, or active learning settings. The investigation of Deep Meta-Learning in these settings  remains an important direction for future work.  \nYet another direction for future research is the creation of \\textit{compositional} Deep Meta-Learning systems, which instead of learning flat and associative functions $\\boldsymbol{x} \\rightarrow y$,  organize knowledge in a \\textit{compositional} manner. This would allow them to decompose an input $\\boldsymbol{x}$ into several (already learned) components $c_{1}(\\boldsymbol{x}),\\ldots ,c_{n}(\\boldsymbol{x})$, which in turn could help the performance in low-data regimes . \nThe question has been raised whether contemporary Deep Meta-Learning techniques actually learn how to perform rapid learning, or simply learn a set of robust high-level features, which can be (re)used for many (new) tasks.  investigated this question for the most popular Deep Meta-Learning technique MAML and found that it largely relies on feature reuse. It would be interesting to see whether we can develop techniques that rely more upon fast learning, and what the effect would be on performance.   \nLastly, it may be useful to add more meta-abstraction levels, giving rise to, e.g., meta-meta-learning, meta-meta-...-learning .  \n\\begin{acknowledgements}\nThanks to Herke van Hoof for an insightful discussion on LLAMA. Thanks to Pavel Brazdil for his encouragement and feedback on a preliminary version of this work.\n\\end{acknowledgements}\n\\bibliographystyle{spbasic}      \n\\bibliography{refs}   \n\\end{document}", "cites": [5800, 8061, 7738, 8062, 8056, 7109, 8059, 3644], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes insights from multiple papers to highlight key challenges in Deep Meta-Learning, such as backbone fairness, generalization, and computational costs. It critically assesses the limitations of existing methods and benchmarks, suggesting directions like compositional learning and higher-order meta-abstraction. While it identifies patterns across works, the abstraction is slightly constrained by its focus on specific issues rather than broader meta-level principles."}}
