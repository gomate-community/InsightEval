{"id": "f4bd41e2-dc95-40dd-a294-168affd295c2", "title": "Introduction", "level": "section", "subsections": ["445091ae-c505-4b58-a2b4-5c467eaea0b0", "051b239c-2af8-4765-b1b2-7e334249fdeb", "c6234a5d-bf13-4122-9a0a-8dec280529e6", "43a7a742-8345-4c8a-a830-8baf2e53d713"], "parent_id": "1ca90fda-21b2-4c6d-bcb1-6e612173f620", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "Introduction"]], "content": "\\label{sec:introduction}}\n\\IEEEPARstart{G}{AIT}, defined as the way people walk, contains relevant cues about human subjects~. As a result, it has been widely used in different application areas such as affect analysis~, sport science~, health~, and user identification~. Gait information can be captured using a number of sensing modalities such as wearable sensors attached to the human body, for instance accelerometers, gyroscopes, and force and pressure sensors~. Non-wearable gait recognition systems predominantly use vision, and are therefore mostly known as vision-based gait recognition. These systems capture gait data using imaging sensors with no cooperation from the subjects and even from far away distances~. The focus of this paper is to survey vision-based gait recognition systems that have mainly relied on deep learning. We focus solely on vision-based gait recognition as a comprehensive review paper has recently been published, surveying wearable-based gait recognition approaches~.\nThe performance of vision-based gait recognition systems, hereafter only referred to only as gait recognition, can be affected by \\textit{i)} variations in the appearance of the individual, such as carrying a handbag/backpack or wearing items of clothing such as a hat or a coat; \\textit{ii)} variations in the camera viewpoint; \\textit{iii)} occlusion factors, for instance where parts of the subject's body are partially covered by an object or by a part of the subject's own body in certain viewpoints (known as self-occlusion)~; and \\textit{iv)} variations in the environment, such as complex backgrounds~ and high or low levels of lighting~, which generally make the segmentation and recognition processes more difficult.\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=0.95\\columnwidth]{Images/fig1_v2.pdf}\n\\caption{The number of gait recognition papers published after 2015 using non-deep ({orange}) and deep ({blue}) gait recognition methods. These papers have been published in top-tier journals and conferences in the field. Journal publications include IEEE Transactions ($19\\%$) including \\textit{T-PAMI}, \\textit{T-IP}, \\textit{T-IFS}, \\textit{T-MM}, \\textit{T-CSVT}, and \\textit{T-Biom}, as well as other top journals ($24\\%$) such as \\textit{Pattern Recognition} and \\textit{Pattern Recognition Letter}. Conference publications include highly ranked computer vision and machine learning conferences ($22\\%$) including \\textit{CVPR}, \\textit{AAAI}, \\textit{ICCV}, \\textit{ECCV}, \\textit{ACCV}, \\textit{BMVC}, as well as other top relevant conferences ($35\\%$) such as \\textit{ICASSP}, \\textit{ICIP}, \\textit{ICPR}, \\textit{ICME}, \\textit{ACM Multimedia}, and \\textit{IJCB}. The figure shows clear opposing trends between the two approaches, indicating that, unsurprisingly, deep learning methods have become the dominant approach is recent years.}\n\\label{fig:Evolution}\n\\end{figure}\n\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=0.9\\textwidth]{Images/Timeline.pdf}\n\\caption{The evolution of deep gait recognition methods.}\n\\label{fig:timeline}\n\\end{figure*}", "cites": [287], "cite_extract_rate": 0.058823529411764705, "origin_cites_number": 17, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic introduction to gait recognition and the challenges it faces, referencing a few application areas and sensor types. It includes a figure that shows a trend in the number of deep learning versus non-deep learning papers but does not synthesize or integrate the cited paper in depth. The section lacks critical analysis of the cited work and only generalizes slightly by highlighting common challenges in vision-based gait recognition."}}
{"id": "445091ae-c505-4b58-a2b4-5c467eaea0b0", "title": "Unique Characteristics of Gait Recognition", "level": "subsection", "subsections": [], "parent_id": "f4bd41e2-dc95-40dd-a294-168affd295c2", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "Introduction"], ["subsection", "Unique Characteristics of Gait Recognition"]], "content": "Gait recognition systems pose challenges that are unique to this field, making it a problem that demands an independent treatment. From a `biometrics' perspective, gait recognition has several unique characteristics that distinguishes it from other biometric modalities. For instance, in contrast to many other biometric systems such as face~, ear~, iris, and fingerprint~ recognition that require subjects to be quite close to acquisition systems, gait data can be captured from far away distances~. As a result, gait recognition videos may often be recorded with low spatial resolution, hence many details regarding the scene become challenging to detect by automated systems. Moreover, while most biometric recognition systems need the subjectsâ€™ active cooperation towards acquisition, gait recognition data can be captured in a discrete manner~. As a result, the likelihood of recording gait patterns in an uncontrolled and non-obscured manner considerably increases. Interestingly, this very property makes gait difficult to forge by imposters, making it reliable for sensitive applications such as crime analysis~.\nWhat makes some of the challenges in gait recognition unique and distinct from general `computer vision' problems is that most gait recognition methods learn representations from analysis of the \\textit{skeletons} or \\textit{silhouettes} of subjects. Meanwhile, other visual classification problems often heavily rely on derived features from \\textit{texture} in addition to shape and structure information. \nFor example, despite the similarities of computer vision problems such as `person re-identification'~ and `human activity recognition'~ to gait recognition, gait data still pose challenges and properties that are unique to this field. \nSpecifically, person re-identification methods identify subjects across multiple non-overlapping surveillance cameras, or possibly from the same camera but at different time instances. To this end, these methods aim to learn representations that capture appearance characteristics of individuals such as clothing and skin color tone, that are shared across multiple cameras~. On the contrary, gait recognition methods aim to learn suitable representations with which \\textit{walking patterns} can be disentangled from the visual appearance of the subjects and subsequently used for classification~.\nWhen comparing gait recognition to human activity recognition methods~, the goal of the latter is to identify specific movements or actions of a subject from video clips, which can be considered as `macro' motion patterns. Meanwhile, gait characteristics can be considered nuanced `micro' patterns that sit on top of a specific activity class, namely \\textit{walking}. As a result, the detection of such subtle discriminative information are often more challenging than those dealt with for activity recognition. Furthermore, given the subtlety of gait patterns that make them unique to different subjects, they can often be highly influenced by the temporary personal state of the subject, for instance, fatigue~, excitement and fear~, and even injuries~.", "cites": [289, 290, 293, 288, 292, 291], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes information from multiple biometric recognition papers to highlight the unique characteristics of gait recognition, such as the ability to work at a distance and the reliance on micro-patterns rather than texture. It also makes critical distinctions between gait recognition and related fields like person re-identification and activity recognition, emphasizing the specific challenges and advantages of gait. The abstraction is strong, as it generalizes these ideas into broader principles of biometric and computer vision systems."}}
{"id": "051b239c-2af8-4765-b1b2-7e334249fdeb", "title": "Motivation", "level": "subsection", "subsections": [], "parent_id": "f4bd41e2-dc95-40dd-a294-168affd295c2", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "Introduction"], ["subsection", "Motivation"]], "content": "In the past two decades, many gait recognition methods have been developed to tackle the above-mentioned problems. In recent years, there has been a clear trend in migrating from non-deep methods to deep learning-based solutions for gait recognition. To visualize this trend, we present Figure \\ref{fig:Evolution}, which illustrates the number of gait recognition papers published after 2015. It is observed that the majority of gait recognition methods in 2019 and 2020 have been designed based on deep neural networks. In Figure \\ref{fig:timeline}, we illustrate the evolution of some of the most important gait recognition methods along with their associated accuracy on the CASIA-B~ (perhaps the most popular dataset for gait recognition) when available. The first gait recognition method was proposed in 1997~, followed by the first shallow neural network for gait recognition in 2008~, consisting of only one input layer, one hidden layer, and one output layer. In 2015, the field witnessed significant breakthroughs, notably due to the popularization of deep neural networks~. The method entitled GaitNet~ was then proposed in 2016 based on a 6-layer convolutional neural network (CNN). In 2017, DBNGait~ was proposed based on a deep belief network (DBN), and in~ three different deep CNN architectures with different depths and architectures were fused for gait recognition. VGR-Net~ was one of the important contributions in 2018, followed by the introduction of several significant methods in 2019, including PoseGait~, DisentangledGait~, and GaitSet~, where the best recognition accuracy of 84.2\\% was achieved by GaitSet~. Remarkable advances have been made in 2020, notably by the appearance of several highly efficient methods, including PartialRNN~, GaitPart~, GLN~, HMRGait~, and 3DCNNGait~. The current state-of-the-art results on CASIA-B dataset~ have been reported by 3DCNNGait~ with a recognition accuracy of 90.4\\%. \nSeveral survey papers~ have so far reviewed recent advances in gait recognition, where some of these papers, for instance~, have focused on \\textit{non-vision-based} gait recognition methods. The most recent survey papers on \\textit{vision-based} gait recognition are~, which only cover the papers published until mid 2018. Nonetheless, many important breakthroughs in gait recognition with deep learning have occurred since 2019, as observed in Figures \\ref{fig:Evolution} and \\ref{fig:timeline}. Additionally, none of the surveys~ have specifically focused on deep learning methods for gait recognition.", "cites": [294, 296, 9138, 295], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 24, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a chronological overview of deep learning-based gait recognition methods and their performance on the CASIA-B dataset, which offers some synthesis of trends over time. However, it largely functions as a descriptive timeline of key papers without deeply analyzing their contributions or limitations. There is minimal abstraction or generalization beyond the specific papers and methods cited."}}
{"id": "f43a8f77-5363-4071-8947-1057e7984e96", "title": "Review Methodology", "level": "section", "subsections": [], "parent_id": "1ca90fda-21b2-4c6d-bcb1-6e612173f620", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "Review Methodology"]], "content": "We employed a search protocol to ensure other scholars can confidently use this survey in their future research. To do so, we first discovered candidate papers through the Google Scholar~ search engines and digital libraries, namely IEEE Xplore~, ACM Digital Library~, ScienceDirect~, and CVF Open Access~. Our search terms included combinations of the following queries: ``gait recognition'', ``gait identification'', ``gait biometric'', ``neural architecture'', ``deep learning'', and ``deep representations''. We then filtered the search results, thus excluding papers that neither use deep learning methods for gait recognition nor demonstrate enough technical clarity/depth. To be more specific about the `clarity/depth' criteria, we excluded the papers that \\textit{i}) use non-vision sensors for gait recognition; \\textit{ii}) do not propose a new solution; \\textit{iii}) use non-standard or private datasets for performance evaluation; \\textit{iv}) do not compare the performance of their solution to the state-of-the-art. In cases where other modalities were combined with vision-based sensors, only the technical solution focusing on the vision-based aspect was studied. \n\\begin{table*}\n  \\centering\n  \\setlength\\tabcolsep{2.3pt}\n    \\caption{Summary of well-known gait datasets used in the literature.}\n    \\begin{tabular}{l|l|l|l|l|l|l}\n    \\hline\n    \\textbf {Dataset}& \\textbf{Year}& \\textbf{Data Type} & \\textbf{\\# of Subjects}& \\textbf{Environment} & \\textbf{\\# of} & \\textbf{Variations} \\\\\n    \\textbf { }& \\textbf{ }& \\textbf{ } & \\textbf{\\& Sequences}& \\textbf{ } & \\textbf{Views} & \\textbf{ } \\\\\n    \\hline\\hline\n     CMU MoBo~& 2001 & RGB; Silhouette & 25 / 600 & Indoor & 6 & 3 Walking Speeds; Carrying a Ball\\\\\n    SOTON~ & 2002 & RGB; Silhouette & 115 / 2,128 & Indoor \\& Outdoor & 2 & Normal Walking on a Treadmill\\\\\n    CASIA-A~& 2003 & RGB & 20 / 240 & Outdoor & 3 & Normal Walking\\\\\n    USF HumanID~ & 2005 & RGB & 122 / 1,870 & Outdoor & 2 & Outdoor Walking; Carrying a Briefcase; Time Interval\\\\\n    CASIA-B~ & 2006 & RGB; Silhouette & 124 / 13,680 & Indoor & 11 & Normal Walking; Carrying a Bag; Wearing a Coat\\\\\n    CASIA-C~ & 2006 & Infrared; Silhouette & 153 / 1,530 & Outdoor & 1 & 3 Walking Speeds; Carrying a Bag \\\\\n    OU-ISIR Speed~  & 2010 & Silhouette & 34 / 306 & Indoor & 4 & Nine walking speeds\\\\\n    OU-ISIR Clothing~  & 2010 & Silhouette & 68 / 2,746 & Indoor & 4 & Up to 32 combinations of clothing\\\\\n    OU-ISIR MV~  & 2010 & Silhouette & 168 / 4,200 & Indoor & 25 & 24 azimuth views and 1 top view\\\\\n    OU-ISIR~  & 2012 & Silhouette & 4,007 / 31,368 & Outdoor & 4 & Normal Walking\\\\\n    TUM GAID~  & 2012 & RGB; Depth; Audio & 305 / 3,737  & Indoor & 1 & Normal Walking; Backpack; Wearing coating shoes\\\\\n    OU-ISIR LP Bag~  & 2017 & Silhouette & 62,528 / 187,584     & Indoor  & 1  & Seven different carried objects \\\\\n    OU-MVLP~  & 2018 & Silhouette & 10,307 / 259,013  & Indoor & 14 & Normal Walking\\\\\n    CASIA-E~  & 2020 &  Silhouette  &  1014 / Undisclosed   &  Indoor \\& Outdoor & 15  & 3 Scenes; Normal Walk; Carrying a Bag; Wearing a Coat  \\\\\n    OU-MVLP Pose~  & 2020 & Skeleton & 10,307 / 259,013  & Indoor & 14 & Normal Walking\\\\\n    \\hline\n    \\end{tabular}\n  \\label{tabDB}\n\\end{table*}\nNaturally, we imposed restrictions on the date of publications to only include search results after 2014, when deep neural networks were first used for biometric recognition~. We then used the returned results in order to perform forward and backward searches, respectively identifying the other resources that have cited the returned articles and the references cited by the returned articles. We repeated this process with the new identified resources until we collected the most relevant papers to the best of our knowledge. We eventually ended up with a final set of publications that have used deep learning for gait recognition.", "cites": [297], "cite_extract_rate": 0.05263157894736842, "origin_cites_number": 19, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a clear methodology for identifying and filtering relevant deep gait recognition papers, including search terms, exclusion criteria, and iterative citation-based expansion. However, it primarily describes the process without synthesizing insights across the cited works or offering critical evaluations of their strengths and weaknesses. The abstraction is limited to basic patterns like dataset characteristics rather than deeper theoretical or conceptual generalizations."}}
{"id": "d929d97d-c95d-4152-9099-5e650bfff992", "title": "Proposed Taxonomy", "level": "section", "subsections": ["2c79114d-2675-440e-b99e-ca8ea8efdf6b", "59ef2d5c-0e3f-4744-b619-fd7fd9f272da", "2747149b-6c79-4256-a26a-fec152f302fe", "a7854008-5c73-44f1-8dd8-92174fd2a6f4"], "parent_id": "1ca90fda-21b2-4c6d-bcb1-6e612173f620", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "Proposed Taxonomy"]], "content": "In order to help illustrate an overall structure for the available gait recognition approaches, a few taxonomies have been proposed in the literature~, which have organized the available solutions from different perspectives. The taxonomy proposed in~ is based on the type of sensors, classifiers, and covariate factors such as occlusion types. The taxonomy in~ categorizes gait recognition methods based on the type of features used. Finally, the one proposed in~ considers user appearance, camera, light source, and environment-related factors. Nevertheless, despite the availability of these taxonomies, none focus on deep gait recognition methods that are most successful nowadays. We thus propose a new taxonomy in this paper to better illustrate the technological landscape of gait recognition methods with a particular focus on deep learning techniques. \nFigure \\ref{fig:taxonomy} presents our proposed taxonomy which considers four dimensions, namely body representation, temporal representation, feature representation, and neural architecture. The details of each of these dimensions are described in the following.", "cites": [287], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes existing taxonomies from the literature, showing how prior works have approached the classification of gait recognition methods. It identifies a gap by pointing out that these taxonomies do not focus on deep learning techniques, motivating the proposed four-dimensional framework. The abstraction is somewhat strong, as it introduces a new classification structure, but the critical analysis is limited, as the section does not deeply evaluate the strengths or weaknesses of the cited works."}}
{"id": "2c79114d-2675-440e-b99e-ca8ea8efdf6b", "title": "Body Representation", "level": "subsection", "subsections": [], "parent_id": "d929d97d-c95d-4152-9099-5e650bfff992", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "Proposed Taxonomy"], ["subsection", "Body Representation"]], "content": "This dimension relates to the way the body is represented for recognition, which can be based on \\textit{silhouettes} or \\textit{skeletons}. Silhouette is the most frequently used body representation in the literature that can be easily computed by subtracting each image containing the subject from its background, followed by binarization. Gait silhouettes are proven to be effective and convenient for describing the body state in a single frame with low computational cost. This body representation forces recognition solutions to focus on `gait' as opposed to clothing and other non-gait factors that could, from the perspective of a classifier, be used for identification.\nA sequence of silhouettes can represent useful gait features such as speed, cadence, leg angles, gait cycle time, step length, stride length, and the the ratio between swing and stance phases~. It can also be processed to extract motion data, for example using optical flow calculation~. Nonetheless, gait silhouettes are more sensitive to changes in the appearance of the individuals, for instance via different clothing and carrying conditions. \nSkeleton body representation can be captured using depth cameras~ or alternatively be estimated using pose-estimation methods~. Static and dynamic features, for instance stride length, speed, distances, and angles between joints, can be obtained from skeleton joints~. Gait recognition methods based on this type of body representation are generally more robust against viewpoint changes due to the consideration of joint positions~, as opposed to silhouette-based methods. Skeleton-based methods are also more robust against appearance changes~ as the pose-estimation step generally learns to detect body joints over different clothing conditions, which is not the case for gait silhouettes. However, since these approaches rely heavily on accurate detection of body joints, they are generally more sensitive to occlusions~. Additionally, the use of pose-estimators imposes a computational overhead to these recognition systems~.", "cites": [294, 298], "cite_extract_rate": 0.2, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a structured analysis of body representation in deep gait recognition, integrating key aspects from silhouette and skeleton-based approaches. It synthesizes the strengths and limitations of each method, referencing relevant papers, and identifies general trends such as robustness to appearance changes and sensitivity to occlusions. While it offers some critical evaluation, deeper comparative or meta-level insights could be enhanced."}}
{"id": "59ef2d5c-0e3f-4744-b619-fd7fd9f272da", "title": "Temporal Representation", "level": "subsection", "subsections": [], "parent_id": "d929d97d-c95d-4152-9099-5e650bfff992", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "Proposed Taxonomy"], ["subsection", "Temporal Representation"]], "content": "This dimension deals with approaches used to represent the temporal information in gait sequences. Two types of representations, \\textit{templates} and \\textit{volumes}, have been commonly used in the literature. Following we describe these representations.\nTemplates aggregate temporal walking information over a sequence of silhouettes in a single map, for example by averaging the silhouettes over at least one gait cycle. This operation enables recognition solutions to be independent of the number of frames once template maps have been created. With respect to deep gait recognition architectures, gait silhouettes can be aggregated in the initial layer of a network (Figure \\ref{fig:template_Layer}-a), also known as \\textit{temporal templates}, where the aggregated map can then be processed by subsequent layers~. Gait silhouettes can alternatively be aggregated in an intermediate layer of the network after several convolution and pooling layers (Figure \\ref{fig:template_Layer}-b), also known as \\textit{convolutional template}~. \nExamples of temporal templates include: (\\textit{i}) gait energy images (GEI)~, which average gait silhouettes over one period/sequence (Figure \\ref{fig:template_Layer}-c); (\\textit{ii}) chrono gait images (CGI)~, which extract the contour in each gait image to be then encoded using a multi-channel mapping function in the form a single map (Figure \\ref{fig:template_Layer}-d); (\\textit{iii}) frame-difference energy images (FDEI)~, which preserve the kinetic information using clustering and denoising algorithms, notably when the silhouettes are incomplete (Figure \\ref{fig:template_Layer}-e); (\\textit{iv}) gait entropy images (GEnI)~, computing entropy for each pixel in the gait frames to be then averaged in a single gait template (Figure \\ref{fig:template_Layer}-f); and (\\textit{v}) period energy images (PEI)~, a generalization of GEI that preserves more spatial and temporal information by exploiting a multi-channel mapping function based on the amplitudes of frames (Figure \\ref{fig:template_Layer}-g). Examples of convolutional templates include set pooling~ and gait convolutional energy maps (GCEM)~, which average convolutional maps obtained by several convolution and pooling layers, over the whole sequence. \nTo preserve and learn from the order and relationship of frames in gait sequences, instead of aggregating them, sequence volume representations have be adopted (see Figure~\\ref{fig:taxonomy}, second box from the left). Then, to learn the temporal information, two different approaches have been adopted. In the first approach, the temporal dynamics over the sequences are learned using recurrent learning strategies, for example recurrent neural networks, where each frame is processed with respect to its relationships with the previous frames~. The second approach first creates 3D tensors from spatio-temporal information available in sequences, where the depth of the tensors represent the temporal information. These tensors are then learned, for example using 3D CNNs~ or graph convolutional networks (GCNs)~. \n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=0.70\\columnwidth]{Images/Template_Merged.pdf}\n\\caption{Overview of temporal representations. Generating templates in: (a) the initial layer of a deep network; (b) an intermediate layer of the network after several convolution and pooling layers. Illustration of (c) GEI~, (d) CGI~, (e) FDEI~, (f) GEnI~, and (g) PEI~ temporal gait templates.}\n\\label{fig:template_Layer}\n\\end{figure}", "cites": [296, 9138], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes the concept of temporal representation by integrating the key ideas from the cited papers into a structured taxonomy of 'templates' and 'volumes.' It abstracts these methods into broader categories and provides a coherent narrative on how temporal information is preserved in deep gait recognition. While it introduces critical perspectives by noting limitations (e.g., templates lose temporal information, sequences impose constraints), it stops short of deeper, nuanced evaluations of trade-offs or contextual weaknesses."}}
{"id": "2747149b-6c79-4256-a26a-fec152f302fe", "title": "Feature Representation", "level": "subsection", "subsections": [], "parent_id": "d929d97d-c95d-4152-9099-5e650bfff992", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "Proposed Taxonomy"], ["subsection", "Feature Representation"]], "content": "This dimension encapsulates the region of support for representation learning, which can be either \\textit{global} or \\textit{partial}. \nThe process of learning silhouettes or skeletons holistically is referred to as global representation learning. \nOn the other hand, when learning partial representations, gait data is split into local regions, e.g., patches, body components, and vertical/horizontal bins (see Figure \\ref{fig:taxonomy}, third box from the left). These local regions are then further processed, for example by recurrent neural networks~, capsule networks~, attention-based networks~, or fully connected layers~. Methods based on global representations tend to be more sensitive to occlusions and appearance changes as well as missing key body parts~. On the other hand, partial regions often maintain different contributions towards the final recognition performance, thus learning their importance can improve the overall performance of gait recognition methods~. Additionally, the relations between these partial features can be learned, to preserve positional attributes such as scale, rotation, and location, which improve the robustness of gait recognition methods against orientation and view changes~.", "cites": [287, 296, 300, 9138, 299], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers to establish a framework for feature representation in deep gait recognition, distinguishing between global and partial approaches. It provides critical insights by highlighting the limitations of global representations (e.g., sensitivity to occlusions) and the benefits of modeling partial features. The abstraction is strong as it generalizes the role of partial features and their relationships, pointing to broader design principles for robust gait recognition systems."}}
{"id": "560291c9-4237-466b-8c6f-3b2f50e6857b", "title": "Convolutional Neural Networks", "level": "subsubsection", "subsections": [], "parent_id": "a7854008-5c73-44f1-8dd8-92174fd2a6f4", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "Proposed Taxonomy"], ["subsection", "Neural Architectures"], ["subsubsection", "Convolutional Neural Networks"]], "content": "Convolutional neural networks (CNNs) have been used the most for gait recognition. {These models are generally used to learn an embedding where the body shape, represented as a silhouette or skeleton, is encoded in the feature space.\n}\nSpecifically, CNNs generally consist of different types of layers including convolutional, pooling, and fully connected layers. Convolutional layers convolve learned filters with the input image to create activation feature maps that capture features with varying levels of detail. The convolutional layers also include activation functions such as a ReLU~ or a tanh~ functions, to increase the non-linearity in the output. \nPooling layers then reduce the spatial size of the feature maps by using nonlinear down-sampling strategies, such as average or maximum pooling, thus decreasing the complexity of the network. Fully connected layers are finally used to learn the resulting 2D feature maps into 1D vectors for further processing.\nTo better analyze CNNs adopted in the state-of-the-art gait recognition methods, we provide an overview of the most successful used architectures in Table \\ref{tab:CNN}. Note that for the methods combining CNNs with other types of networks, e.g., autoencoder, capsule, and Long Short-Term Memory (LSTM), we only present the architectures of the CNN components in the table. {As can be seen, there is no need for state-of-the-art gait recognition models to exploit very deep CNN architectures. This is due to the fact that input gait data, either in the from of silhouettes or skeletons, do not present considerable complexity in terms of texture information. Hence, even fewer than 10 layers are shown to be sufficient for encoding gait frames. This is contrary to many other domains, such as face or activity recognition, where very deep networks such as ResNet~ and inception~ are used to learn highly discriminative features.} In Table \\ref{tab:CNN}, we also present the size of CNN inputs, showing a trend toward a 64$\\times$64 resolution in the recent literature. Additionally, an analysis in~ showed that the resolutions of 64$\\times$64 and 128$\\times$128 lead to the best gait recognition results for several tested CNNs, where the input resolution of 128$\\times$128 works slightly better than 64$\\times$64. However, as a higher input resolution implies more convolutional and pooling layers, the input resolution of 64$\\times$64 has been most widely adopted to limit the computational complexity of the solutions.", "cites": [97, 301], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes CNN concepts and integrates them with trends in gait recognition, making connections between general CNN usage and domain-specific characteristics. It critically observes that gait recognition does not require very deep networks, contrasting with other fields like face recognition, which indicates a thoughtful analysis. The section abstracts from individual papers to highlight overarching patterns in model depth and input resolution, contributing to a broader understanding of design choices in the field."}}
{"id": "aeed6389-48f4-4a15-9c96-2ff298684b77", "title": "Recurrent Neural Networks", "level": "subsubsection", "subsections": [], "parent_id": "a7854008-5c73-44f1-8dd8-92174fd2a6f4", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "Proposed Taxonomy"], ["subsection", "Neural Architectures"], ["subsubsection", "Recurrent Neural Networks"]], "content": "Recurrent Neural Networks (RNNs) have been widely applied to temporal or sequence learning problems, achieving competitive performances for different tasks~, including gait recognition~. A layer of RNN is typically composed of several cells, each corresponding to one input element of the sequence, e.g., one frame of a gait video. RNNs can also stacks several layers to make the model deeper, where the output of the $i^{th}$ cell in $j^{th}$ layer feeds the $i^{th}$ cell in the $(j+1)^{th}$ layer. Each cell is connected to its previous and subsequent cells, thus memorizing information from the previous time steps~. Among different RNN architectures, LSTM~ and Gated Recurrent Units (GRU)~ are the most widely used RNN architectures that have been used to learn the relationships available in a gait sequence using memory states and learnable gating functions. In an LSTM network~, the cells have a common cell state, which keeps long-term dependencies along the entire LSTM cell chain, controlled by two gates, the so-called input and forget gates, thus allowing the network to decide when to forget the previous state or update the current state with new information. The output of each cell, the hidden state, is controlled by an output gate that allows the cell to compute its output given the updated cell state. GRU~ is another form of RNN that does not use output activation functions as opposed to LSTM. This architecture also includes an update gate that allows the network to update the current state with respect to the new information. The output of the gate, also known as the reset gate, only maintains connections with the cell input.\n{There have been three different approaches to use RNNs for gait recognition systems. The first approach~ (Figure \\ref{fig:taxonomy}-a) that has been mostly adopted for skeleton representations, uses RNNs in order to learn from temporal relationships of joint positions. In the second approach~ (Figure \\ref{fig:taxonomy}-b), as will be discussed later in detail in Section \\ref{sec:hib}, RNNs are combined with other types of the neural architectures, notably CNNs, for learning both spatial and temporal information. The last approach that has been recently adopted in~ (Figure \\ref{fig:taxonomy}-c) uses RNNs to recurrently learn the relationships between partial representations from a single gait template, for instance GCEM~}.\n\\begin{table}\n  \\centering\n  \\setlength\\tabcolsep{2.5pt}\n    \\caption{Overview of recent CNN architectures adopted for deep gait recognition.}\n    \\begin{tabular}{l|l|l|l|l|l}\n    \\hline\n    \\textbf {Method}& \\textbf{Input}& \\textbf{Total \\# of}& \\textbf{\\# of Conv.} & \\textbf{\\# of Pool.} & \\textbf{\\# of FC}  \\\\\n     & \\textbf{Size} & \\textbf{Layers}& \\textbf{Layers} & \\textbf{Layers} & \\textbf{Layers}  \\\\\n    \\hline\\hline\n    GEINet~ & 88$\\times$128  & 6 & 2 & 2 & 2  \\\\\n    Ensem. CNNs~ & 128$\\times$128 & 7 &  3 & 2 & 2 \\\\\n    MGANs~ & 64$\\times$64 & 8 &  4 & 1 & 3 \\\\\n    EV-Gait~ & 128$\\times$128 & 9 & 6 & 0& 2   \\\\\n    Gait-joint~ & 64$\\times$64  & 16 & 12 & 2& 2  \\\\\n    Gait-Set~ & 64$\\times$64 & 9 &  6 & 2 & 1 \\\\ \n    Gait-RNNPart~ & 64$\\times$64 & 9 &  6 & 2 & 1 \\\\ \n    Gait-Part~ & 64$\\times$64 & 9 &  6 & 2 & 1 \\\\ \n    SMPL~ & 64$\\times$64 & 5 &  3 & 1 & 1 \\\\ \n    Caps-Gait~ & 64$\\times$64 & 9 &  6 & 2 & 1 \\\\ \n    \\hline\n    \\end{tabular}\n  \\label{tab:CNN}\n\\end{table}\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=1 \\columnwidth]{Images/RNN.pdf}\n\\caption{Three different approaches for using RNNs in the context of deep gait recognition systems: (a) RNNs directly learn from the movement of joint positions; (b) RNNs are combined with CNNs; and (c) RNNs recurrently learn the relationships between partial representations in gait templates.}\n\\label{fig:RNN}\n\\end{figure}", "cites": [304, 295, 296, 300, 302, 303, 9138], "cite_extract_rate": 0.3888888888888889, "origin_cites_number": 18, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section describes RNNs and their role in gait recognition, referencing several papers to outline three approaches. While it connects the general concept of RNNs to specific gait recognition methods, the synthesis is limited to categorization rather than deeper integration. There is minimal critical analysis or identification of broader patterns, making the insight level medium."}}
{"id": "3b70f05a-9bb4-4e51-9b1e-59db041d06ba", "title": "Deep AutoEncoders", "level": "subsubsection", "subsections": [], "parent_id": "a7854008-5c73-44f1-8dd8-92174fd2a6f4", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "Proposed Taxonomy"], ["subsection", "Neural Architectures"], ["subsubsection", "Deep AutoEncoders"]], "content": "Deep auto-encoder (DAE) is a type of network that aims to extract so called bottleneck features or latent space representations, using an encoder-decoder structure. The encoder transforms the input data into a feature representation and the decoder part transforms the representation back to the original input data. The encoder generally includes several fully connected and/or convolutional layers while the decoder consists of layers that perform the inverse operations. DAE networks are generally trained with the aim of minimizing the reconstruction error that measures the difference between the original input and the reconstructed version. Once a DAE is trained, the bottleneck features which are a latent/compressed representation of the knowledge of the original input, are extracted to be used for classification, i.e., gait recognition in our case. The method proposed in~ uses a DAE network, first encoding the input temporal templates using four convolutional layers to extract feature. The decoder then reconstructs the input from the extracted features using four deconvolutional layers. In~, an auto-encoder with 7 fully connected layers along with input and output layers was used to extract robust gait features. In~, a DAE was used to disentangle the input temporal template into identity and covariate features. The backbone of the encoder was based on the Inception module in GoogLeNet~, extracting multi-scale identity and covariate features. The decoder then took those features as input to reconstruct the temporal template using deconvolutional layers.", "cites": [305], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic description of deep auto-encoders and their application in gait recognition, mentioning a few cited works. It synthesizes the role of encoder-decoder structures and connects them to the broader theme of feature extraction, but lacks deeper integration or a novel framework. There is minimal critical analysis or identification of broader patterns or principles."}}
{"id": "94182df1-674c-49eb-88b3-48314f521e64", "title": "Generative Adversarial Networks", "level": "subsubsection", "subsections": [], "parent_id": "a7854008-5c73-44f1-8dd8-92174fd2a6f4", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "Proposed Taxonomy"], ["subsection", "Neural Architectures"], ["subsubsection", "Generative Adversarial Networks"]], "content": "Generative Adversarial Networks (GANs) include a generator and a discriminator~, where the generator aims to deceive the discriminator by synthesizing fake samples that resemble the real ones. In turn, the discriminator aims to distinguish between the fake and real samples. As a result of this minimax game between these two components, GANs can generate realistic synthesized samples. {In the context of gait recognition, GANs can be used to solve the problem of gait variations due to clothing, viewpoints, and carrying conditions. For instance, GANs can transform gait data from one view to another, or change the type of clothing worn by the subject, or even remove a backpack that was originally carried by the subject.\nSuch disentanglement of identity from confounding factors often results in improvements in the performance of gait recognition systems~. Nevertheless, one of the most important challenges toward manipulating gait data is the preservation of human identity features while modifying appearance characteristics in the representation space. To this end, two discriminators are often used~. The first discriminator is used to distinguish real vs. fake samples in order to ensure that the generated images appear realistic. The second discriminator is exploited to ensure that identity information are preserved by taking a pair of source and target images as input and producing a scalar probability of whether the input pair belongs to the same person or not.}\nDifferent types of GANs have recently been adopted for gait recognition. Multi-task GAN (MGANs)~ have been proposed for cross-view gait recognition, where a CNN is used to learn the temporal template as view-specific features in a latent space. Then, the features are transformed from one view to another using a view transform layer. The network is then trained with multi-task adversarial and pixel-wise losses. In another paper, Discriminant Gait GAN (DiGGAN)~ considered the mechanisms of using two independent discriminators in order to transfer GEIs form a certain viewpoint to a different viewing angle while also preserving identity information. In~ a Two-Stream GAN (TS-GAN) was proposed to learn both global and partial feature representations when transforming GEI temporal templates with different viewing angles to a GEI temporal templates with a standard view, i.e., 90$^{\\circ}$.", "cites": [7217, 235], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear analytical overview of how GANs are applied in deep gait recognition, synthesizing the roles of different GAN variants and their components. It connects ideas across the cited papers to explain how identity preservation and appearance modification are addressed in the field. However, it lacks deeper critical analysis of limitations or comparative strengths and weaknesses of the approaches, and while it identifies general uses of GANs, it does not elevate the discussion to a more meta-level or principle-driven abstraction."}}
{"id": "921587ea-28db-423d-af31-7c33141fe12f", "title": "Capsule Networks", "level": "subsubsection", "subsections": [], "parent_id": "a7854008-5c73-44f1-8dd8-92174fd2a6f4", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "Proposed Taxonomy"], ["subsection", "Neural Architectures"], ["subsubsection", "Capsule Networks"]], "content": "Capsule Networks (CapsNet)~ have been proposed to address two important shortcomings in CNNs, namely the limits of scalar activations and poor information routing through pooling operations by respectively exploiting capsule activation values and routing-by-agreement algorithms. CapsNets are composed of capsules which are groups of neurons that explicitly encode the intrinsic viewpoint-invariant relationships available in different parts of the objects. {In the context of gait representation learning, a CapsNet can model and understand the structural relationship between the various parts of the body, such as the relationships between legs and feet, upper body and lower body, and trunk and limbs, using a learnable pose matrix. A CapsNet can also be used to model internal hierarchical representations between multiple gait silhouettes or skeleton joint coordinates of a subject in a video. This is in contrast to the standard pooling layers often used in CNNs, which fail to preserve positional attributes in the human body, such as locations, scales, rotations, and relationships between the body parts.} CapsNets generally include two blocks, primary and high-level group of capsules. The first block encodes spatial information with several layers including convolutional, reshaping, and squashing layers, followed by the second block that learns deeper part-whole relationships between hierarchical sub-parts. The concept of capsule network has been recently adopted for gait recognition~. The method proposed in~ first learns the properties of GEI templates using a CNN. It then uses a CapsNet with dynamic routing to retain the relationship within each template with the aim of finding more robust features. Capsule networks have also been combined with other types of deep networks in~ and ~, which we will review in Section \\ref{sec:hib}.", "cites": [300, 306], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of capsule networks in the context of gait recognition, integrating core concepts from the cited papers to explain their relevance. It synthesizes the idea of capsules as hierarchical and pose-aware structures and links this to gait-specific applications. However, it lacks deeper critical evaluation of the limitations or comparative strengths of capsule networks against other architectures, and the abstraction remains at a moderate level without identifying broader trends or principles."}}
{"id": "c53084bb-267f-4d01-bcd0-637964ddf775", "title": "Graph Convolutional Networks", "level": "subsubsection", "subsections": [], "parent_id": "a7854008-5c73-44f1-8dd8-92174fd2a6f4", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "Proposed Taxonomy"], ["subsection", "Neural Architectures"], ["subsubsection", "Graph Convolutional Networks"]], "content": "Graph convolutional networks (GCNs) have been recently developed to extend CNNs to a higher dimensional domain using arbitrarily structured graphs and graph convolution filters~. {Given the inherent hierarchical and graph-like nature of the human body, GCNs can jointly model both the structural information of the human body and temporal relationships available between gait frames in order to learn discriminative and robust features with respect to camera viewpoints and subject appearances. Gait recognition methods based on GCNs consider gait sequence volumes as the spatio-temporal representations for gait recognition~}. In~, gait features were extracted by forming a spatio-temporal graph from the available video sequences. The final features were then obtained using a joint relationship learning scheme by mapping the features onto a more discriminative subspace with respect to human body structure and walking pattern.", "cites": [235], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces the use of Graph Convolutional Networks (GCNs) in gait recognition and mentions a method involving spatio-temporal graphs and feature mapping, but it lacks detailed integration or synthesis of multiple cited works. There is minimal critical evaluation or comparison of approaches, and while it hints at the relevance of graph structures, it does not generalize or abstract the concept into a broader framework or principle."}}
{"id": "12af581c-affd-4974-9f07-3336875ad3dc", "title": "Hybrid Networks", "level": "subsubsection", "subsections": [], "parent_id": "a7854008-5c73-44f1-8dd8-92174fd2a6f4", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "Proposed Taxonomy"], ["subsection", "Neural Architectures"], ["subsubsection", "Hybrid Networks"]], "content": "\\label{sec:hib}\nA large number of hybrid deep networks that make use of two or more types of networks have been proposed to boost the performance of gait recognition systems. Among these, architectures with CNN+RNN, DAE+GAN, DAE+RNN, and RNN+CapsNet components are the most popular in the deep gait recognition literature (see Figure \\ref{fig:taxonomy}). Following we provide the descriptions and examples of these four hybrid architectures.\n\\textbf{CNN+RNN.} Integration of CNNs with RNNs (notably LSTM and GRU) for learning the temporal relationships following spatial encoding is perhaps the most popular approach for spatio-temporal learning, which has also been used for gait recognition in the literature. \nIn~, a deep gait recognition system was proposed by combining eight different CNN architectures with LSTM to obtain spatio-temporal features from image sequences. The proposed method in~ first divides gait silhouettes into 4 horizontal parts, where each part was fed to an individual CNN with 10 layers. An attention-based LSTM was then used to output frame-level attention scores for each sequence of CNN features. The CNN features were finally multiplied by their corresponding weights to selectively focus on the most important frames for gait recognition. In~, convolutional maps from gait frames were first learned using an 8-layer CNN. The convolutional maps were then aggregated to form GCEM templates which were then split into horizontal bins. These partial features (horizontal bins) were finally learned by an attentive bi-directional GRU to exploit the relations between these parts of the embedding.\n\\textbf{DAE+GAN.} Recently, DAEs have been considered as the backbone of the generator and/or discriminator components in GANs for gait recognition~. GaitGAN~ and GaitGANv2~ used two discriminators with encoder-decoder structures, respectively for fake/real discrimination and identification. These two discriminators ensured that the generated gait images were realistic and that the generated images contained identity information. The Alpha-blending GAN (Ab-GAN) proposed in~ exploits an encoder-decoder network as the generator to generate gait templates without carried objects. Cycle-consistent Attentive GAN (CA-GAN) was proposed in~ and used an encoder-decoder structure for gait view synthesis. The proposed GAN contains two branches to simultaneously exploit both global and partial feature representations. \n\\textbf{DAE+RNNs.} \nThe combination of DAEs and RNNs has recently been proposed for generating sequence-based disentangled features using an LSTM RNN~. In this context, a deep encoder-decoder network with novel loss functions was first used to disentangle gait features, namely identity information from appearance and canonical features that mostly contain spurious information for gait recognition. A multi-layer LSTM was then used to capture temporal dynamics of the gait features to be finally aggregated for the recognition purpose~. \n\\textbf{RNNs+CapsNets.} Recurrently learned features obtained by RNNs can be treated as capsules~, thus learning coupling weights between these capsules through dynamic routing. This encapsulated hierarchical part-whole relationships between the recurrently learned features that can make the hybrid network more robust against appearance and view changes. Additionally, the CapsNet can act as an attention mechanism, thus assigning more importance to the more relevant features. In~, a CapsNet was used to treat the recurrently learned partial representations of a convolution template as capsules, thus learning the coupling weights between the partial features. This led to exploiting the relationships between the partial features while also preserving positional attributes. So, the model could generalize better to unseen gait viewpoints during testing. In ~, a capsule network with dynamic routing was used to exploit the spatial and structural relations between body parts. In this context, the recurrently learned features were first extracted using an LSTM network from a sequence of gait frames to feed the capsule network.\n\\begin{table*}[]\n\\centering\n\\setlength\\tabcolsep{1.80pt}\n\\caption{Classification of deep gait recognition methods based on our proposed taxonomy.}\n\\begin{tabular}{l|l|l|l|l|l|l|l|l}\n\\hline \n\\textbf{Ref.} &\\textbf{Year} & \\textbf{Venue}         & \\textbf{Body Rep.}                 & \\textbf{Temporal Rep.}     & \\textbf{Feat. Rep.}    & \\textbf{Neural Architecture}   & \\textbf{Loss Function}      & \\textbf{Dataset}                         \\\\ \\hline\\hline\n      & 2015 & \\textit{T-MM}           & Silhouettes           & Tmp: GEI          & Global          & CNN                   & Cross-Entropy                   & CASIA-B                         \\\\\n    & 2015 & \\textit{CISP}          & Silhouettes           & Tmp: GEI          & Global          & CNN                   & Undisclosed                     & CASIA-B                         \\\\\n\\hline\n    & 2016 & \\textit{ICPR}          & Skeleton              & Sequence Volume  & Partial         & LSTM                   & Undisclosed                     & CASIA-B                         \\\\\n       & 2016 & \\textit{ICB}           & Silhouettes           & Tmp: GEI          & Global          & CNN                   & Cross-Entropy                   & OU-ISIR                         \\\\\n& 2016 & \\textit{ICIP}          & Silhouettes           & Sequence Volume  & Global          & 3D CNN                & Undisclosed                      & CMU Mobo; USF HumanID           \\\\\n    & 2016 & \\textit{ICASSP}        & Silhouettes           & Tmp: GEI          & Global          & CNN                   & Contrastive                     & OU-ISIR                         \\\\\n    & 2016 & \\textit{BMVC}          & Skeleton              & Sequence Volume  & Global          & CNN + LSTM            & Cross-Entropy                    & CASIA-B; CASIA-A                \\\\\n\\hline\n     & 2017 & \\textit{Int. J. Biom.} & Silhouettes           & Tmp: GEI          & Partial         & DBN                   & Undisclosed                             & CASIA-B                         \\\\\n    & 2017 & \\textit{CVIU}          & Silhouettes           & Tmp: GEI          & Global          & CNN                   & Undisclosed                             & CASIA-B                         \\\\\n       & 2017 & \\textit{IEEE T-PAMI}   & Silhouettes           & Tmp: GEI          & Global          & CNN                   & Cross-Entropy                   & CASIA-B; OU-ISIR                \\\\\n    & 2017 & \\textit{Applied Sci.}  & Silhouettes           & Tmp: Norm. AC     & Global          & CNN                   & Undisclosed                             & OU-ISIR                         \\\\\n    & 2017 & \\textit{IEEE T-CSVT}   & Silhouettes           & Tmp: GEI          & Global          & CNN                   & Triplet Loss                    & OU-ISIR                         \\\\\n    & 2017 & \\textit{BIOSIG}        & Silhouettes           & Tmp: GEI          & Global          & CNN                   & Undisclosed                             & TUM-GAID                        \\\\\n   & 2017 & \\textit{MM}            & Silhouettes           & Tmp: GEI          & Global          & CNN                   & Triplet Loss                    & OU-ISIR                         \\\\\n   & 2017 & \\textit{CCBR}          & Skeleton              & Sequence Volume  & Global          & CNN + LSTM             & Undisclosed                             & CASIA-B                         \\\\\n     & 2017 & \\textit{CVPRW}         & Silhouettes           & Tmp: GEI          & Global          & GAN                   & Cross-Entropy                   & CASIA-B                         \\\\\n     & 2017 & \\textit{Neurocomp.}    & Silhouettes           & Tmp: GEI          & Global          & DAE                   & Euclidean                       & CASIA-B; SZU RGB-D              \\\\\n\\hline\n   & 2018 & \\textit{Elect. Imaging}& Silhouettes           & Sequence Volume  & Global          & 3D CNN                & Undisclosed                             & CASIA-B                         \\\\\n & 2018 & \\textit{IEEE Access}   & Silhouettes           & Sequence Volume  & Global          & CNN + LSTM            & Cross-Entropy                   & CASIA-C                         \\\\\n   & 2018 & \\textit{Neuroinform.}  & Silhouettes           & Seq. Vol. + GEI  & Global          & 3D CNN                & Contrastive                     & OU-ISIR                         \\\\\n      & 2018 & \\textit{DIC}           & Skeleton              & Tmp: GEI          & Global          & CNN                   & Cross-Entropy                  & CASIA-B                         \\\\\n    & 2018 & \\textit{IEEE Access }  & Silhouettes           & Sequence Volume  & Global          & CNN + LSTM            & Cross-Entropy                   & CASIA-B; OU-ISIR                \\\\\n    & 2018 & \\textit{ISBA}          & Silhouettes           & Sequence Volume  & Global          & 3D CNN                & Undisclosed                             & CASIA-B                         \\\\\n  & 2018 & \\textit{ICME}          & Silhouettes           & Tmp: GEI          & Part; Glob.      & DAE + GAN             & Cross-Entropy                   & CASIA-B                         \\\\\n   & 2018 & \\textit{JVCIR}         & Silhouettes           & Tmp: GEI          & Global          & CNN                   & Cross-Entropy                   & CASIA-B; OU-ISIR                \\\\\n   & 2018 & \\textit{CCBR}          & Skeleton              & Sequence Volume  & Global          & CNN + LSTM           & Undisclosed                             & CASIA-B                         \\\\\n\\hline\n      & 2019 & \\textit{PRL}           & Skel.; Silh.          & Sequence Volume  & Global          & LSTM                 & Undisclosed                             & CASIA-B; TUM-GAID               \\\\\n  & 2019 & \\textit{IET Biom.}     & Silhouettes           & Tmp: Weight Avg.  & Partial         & CNN                   & Undisclosed                             & CASIA-B; TUM; OU-ISIR      \\\\\n       & 2019 & \\textit{CVPR}          & Skel.; Silh.          & Sequence Volume  & Global          & DAE + LSTM            & Multiple Loss Functi\nons         & CASIA-B; FVG                    \\\\\n   & 2019 & \\textit{J. Sys. Arch.} & Silhouettes           & Tmp: GEI          & Global          & DAE + GAN             & Multiple Loss Functions         & CASIA-B; OU-ISIR                \\\\\n      & 2019 & \\textit{PR}            & Silhouettes           & Tmp: GEI          & Global          & CNN                   & Siamese                         & CASIA-B; SZU                    \\\\\n       & 2019 & \\textit{IEEE T-IFS }   & Silhouettes           & Tmp: GEI          & Global          & GAN                   & Adversarial\\&Cross-Entropy      & CASIA-B; OU-ISIR                \\\\\n   & 2019 & \\textit{PRL}           & Silhouettes           & Tmp: GEI          & Global          & CNN                   & Restrictive Triplet             & CASIA-B; OU-ISIR                \\\\\n   & 2019 & \\textit{CVPR}          & Silhouettes           & Tmp: GEI          & Global          & CNN                   & Quintuplet                      & CASIA-B; OU-ISIR LP Bag         \\\\\n     & 2019 & \\textit{Neurocomp.}    & Silhouettes           & Tmp: GEI          & Global          & GAN                   & Pixel-wise and Entropy          & CASIA-B; OU-ISIR                \\\\\n   & 2019 & \\textit{IJCNN}         & Silhouettes           & Tmp: GEI          & Global          & GAN                   & Multiple Loss Functions         & CASIA-B                         \\\\\n       & 2019 & \\textit{IEEE T-IFS}    & Skeleton              & Tmp: GEI          & Global          & DAE                   & Contrastive\\&Triplet Loss       & OU-ISIR LP Bag; TUM-GAID        \\\\\n     & 2019 & \\textit{ICVIP}         & Silhouettes           & Tmp: Weight Avg.  & Partial         & CNN                   & View\\&Cross-Entropy             & CASIA-B                         \\\\\n   & 2019 & \\textit{IEEE T-MM}     & Silhouettes           & Sequence Volume  & Global          & CNN + LSTM             & Contrastive                     & CASIA-B; OU-ISIR                \\\\\n   & 2019 & \\textit{IJCNN}         & Silhouettes           & Tmp: GEI          & Global          & GAN                   & Multiple Loss Functions         & CASIA-B                         \\\\\n   & 2019 & \\textit{NCAA}          & Silhouettes           & Tmp: GEI          & Global          & CNN                   & Undisclosed                             & CASIA-B; CASIA-A; OU-ISIR       \\\\\n   & 2019 & \\textit{PR}            & Silhouettes           & Tmp: Set pooling  & Global          & CNN                   & Center\\&Soft-Max                & CASIA-B                         \\\\\n     & 2019 & \\textit{NCAA}          & Silhouettes           & Tmp: GEI          & Global          & CNN                   & Undisclosed                             & CASIA-B; OU-ISIR                \\\\\n & 2019 & \\textit{JVCI}          & Silhouettes           & Tmp: GEI          & Global          & CapsNet               & Standard Capsule Loss           & CASIA-B                         \\\\\n       & 2019 & \\textit{AAAI}          & Silhouettes           & Tmp: Set Pooling  & Partial         & CNN                   & Batch All Triplet loss          & CASIA-B; OU-MVLP                \\\\\n\\hline\n    & 2020 & \\textit{IEEE Access}   & Skeleton              & Sequence Volume  & Global          & DAE + LSTM            & Mean Square Error               & Walking Gait                    \\\\\n      & 2020 & \\textit{PR}            & Skeleton              & Sequence Volume  & Partial         & CNN                   & Center\\&Soft-Max                & CASIA-B; CASIA-E                \\\\\n       & 2020 & \\textit{IEEE T-PAMI}   & Skel.; Silh.          & Sequence Volume  & Global          & DAE + LSTM            & Multiple Loss Functions         & CASIA-B; FVG                    \\\\\n      & 2020 & \\textit{IEEE T-IP }    & Silhouettes           & Sequence Volume  & Partial         & CNN + LSTM            & Angle Center                    & CASIA-B; OU-MVLP; OU-LP         \\\\\n     & 2020 & \\textit{PR}            & Silhouettes           & Tmp: GEI          & Global          & GAN                   & Multiple Loss Functions         & OULP-BAG; OU-ISIR LP Bag        \\\\\n   & 2020 & \\textit{MTAP}          & Silhouettes           & Tmp: MF-GEI       & Global          & CNN                   & Undisclosed                             & CASIA-B                         \\\\\n   & 2020 & \\textit{KBS}           & Silhouettes           & Sequence Volume  & Global          & LSTM + Capsule        & Capsule\\&Memory                 & CASIA-B; OU-MVLP                \\\\\n   & 2020 & \\textit{JINS}          & Silhouettes           & Tmp: GEI          & Global          & CNN + LSTM           & Undisclosed                             & CASIA-B; OU-ISIR                \\\\\n     & 2020 & \\textit{IEEE T-CSVT}   & Silhouettes           & Tmp: GEI          & Global          & CNN                   & Contrastive\\&Triplet Loss       & CASIA-B; OU-MVLP; OU-ISIR       \\\\\n    & 2020 & \\textit{arXiv}         & Silhouettes           & Sequence Volume  & Global          & 3D CNN                & Triplet Loss                    & CASIA-B; OU-MVLP                \\\\\n    & 2020 & \\textit{MTAP}          & Silhouettes           & Tmp: GEI          & Partial         & CNN                   & Undisclosed                             & CASIA-B; OU-ISIR                \\\\\n     & 2020 & \\textit{arXiv}         & Skeleton              & Sequence Volume  & Global          & GCN                   & Triplet Loss\\&ArcFace           & CASIA-B                         \\\\\n    & 2020 & \\textit{MTAP}          & Silhouettes           & Tmp: GEI          & Global          & CNN                   & Undisclosed                             & CASIA-B; OU-ISIR                \\\\\n   & 2020 & \\textit{JIPS}          & Silhouettes           & Tmp: GEI          & Global          & CNN                   & Soft-Max                        & CASIA-B; OU-ISIR                \\\\\n    & 2020 & \\textit{MTAP}          & Silhouettes           & Tmp: GEI          & Global          & CNN                   & Undisclosed                             & CASIA-B                         \\\\\n & 2020 & \\textit{J. SuperComp.} & Silhouettes           & Tmp: GEI          & Global          & CNN                   & Undisclosed                             & CASIA-B; OU-ISIR; OU-MVLP       \\\\\n & 2020 & \\textit{ITNEC}         & Silhouettes           & Tmp: GEI          & Global          & CapsNet               & Standard Capsule Loss           & CASIA-B; OU-ISIR                \\\\\n& 2020 & \\textit{CVPR}          & Silhouettes           & Tmp: Hor. Pooling & Partial         & CNN                   & Batch All Triplet loss          & CASIA-B; OU-MVLP                \\\\\n& 2020 & \\textit{CVPR}          & Silhouettes           & Tmp: GEI          & Global          & DAE                   & Contrastive\\&Triplet Loss       & CASIA-B; OU-ISIR LP Bag         \\\\\n   & 2020 & \\textit{IEEE T-Biom}   & Skeleton              & Sequence Volume  & Global          & CNN + LSTM             & Cross-Entropy\\&Center           & OUMVLP-Pose                     \\\\\n   & 2020 & \\textit{ACCVW}         & Silhouettes           & Tmp: Set Pooling  & Partial         & CNN                   & Batch All Triplet loss          & CASIA-E                         \\\\\n   & 2020 & \\textit{ACCVW}         & Silhouettes           & Tmp: Set Pooling  & Partial         & CNN                   & Batch All Triplet loss          & CASIA-E                         \\\\\n     & 2020 & \\textit{ICPR}          & Silhouettes           & Tmp: Set Pooling  & Partial         & CNN + GRU + Caps.     & Triplet Loss\\&Cosine Prox.    & CASIA-B; OU-MVLP                \\\\\n    & 2020 & \\textit{IEEE T-Biom.}  & Silhouettes           & Tmp: GCEM         & Partial         & CNN + GRU             & Triplet Loss\\&Cross-Entropy     & CASIA-B; OU-MVLP                \\\\\n   & 2020 & \\textit{IEEE Access}   & Silhouettes           & Tmp: Set Pooling  & Global          & CNN                   & Triplet Loss                    & CASIA-B                         \\\\\n  & 2019 & \\textit{ICASSP}        & Silhouettes           & Tmp: Pooling      & Global          & CNN                   & Center-Ranked                   & CASIA-B; OU-MVLP                \\\\\n   & 2020 & \\textit{IJCB}          & Silhouettes           & Tmp: GEI          & Global          & DAE + GAN             & Center\\&Soft-Max                & CASIA-B; OU-ISIR                \\\\\n     & 2020 & \\textit{ACCV}          & Skel.; Silh.          & Sequence Volume  & Global          & CNN + LSTM            & Multiple Loss Functions         & CASIA-B; OU-MVLP                \\\\\n       & 2020 & \\textit{MM}            & Silhouettes           & Sequence Volume  & Global          & 3D CNN                & Multiple Triplet Losses                    & CASIA-B; OU-ISIR                \\\\\n     & 2020 & \\textit{ECCV}          & Silhouettes           & Tmp: Set Pooling  & Global          & CNN                   & Triplet Loss\\&Cross-Entropy     & CASIA-B; OU-MVLP                \\\\\n\\hline\n\\end{tabular}\n\\label{tab:Status}\n\\end{table*}\n\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=1\\textwidth]{Images/fig_7_new.pdf}\n\\caption{(a) Visualization of deep gait recognition methods, according to three levels of our taxonomy and publication date; (b) The frequency of different neural architectures, loss functions, and gait datasets used in the literature.}\n\\label{fig:tax}\n\\end{figure*}", "cites": [294, 296, 307, 235, 304, 9138, 295, 306, 300], "cite_extract_rate": 0.11842105263157894, "origin_cites_number": 76, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to present a coherent framework for hybrid architectures in deep gait recognition, particularly in how CNNs, RNNs, DAEs, and CapsNets are combined. While it provides a descriptive overview of each architecture, it only briefly mentions challenges and benefits without deep critical evaluation. Some abstraction is achieved by highlighting general design patterns and purposes, such as attention mechanisms and disentanglement of features."}}
{"id": "b3854b91-cf86-4aaf-aacc-69b31f0ce176", "title": "Analysis and Trends", "level": "subsection", "subsections": [], "parent_id": "c203cf63-21d3-48d6-90ac-d1b1607a9797", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "State-of-the-art"], ["subsection", "Analysis and Trends"]], "content": "Our analysis based on Table~\\ref{tab:Status} and Figure \\ref{fig:tax}(a) allows us to reach some interesting conclusions about the recent evolution and trends in deep gait recognition technologies with respect to our proposed taxonomy. Following are the key ideas from the analysis.\n\\textbf{Body Representation.} Silhouettes are the most widely adopted body representation for deep gait recognition, corresponding to over 81\\% of the literature. Skeletons have been considered less frequently compared to silhouettes, corresponding to only 13\\% of the available solutions. There have also been a few methods, i.e., approximately 5\\% of the available literature, that exploit both skeleton and silhouette representations, notably using disentangled representation learning or fusion strategies. Based on our analysis,\nhigh performing gait recognition methods such as~ have all adopted silhouette body representation. Nonetheless, due to recent advancements in effective pose-estimation techniques~ capable of extracting accurate and robust skeleton data from videos, we anticipate methods based on hybrid silhouettes-skeleton body representations to gain popularity in the near future.\n\\textbf{Temporal Representation.} Gait templates have been the most considered representation for capturing temporal gait information, corresponding to 70\\% of the proposed deep methods. Among different types of gait templates, GEI and set-pooling have been adopted the most. Around 30\\% of solutions adopt sequence volumes to preserve the order of available gait frames and to learn from their relationships. Given the frequent use of convolutional templates in some of the recent high-performing literature~, we anticipate that these templates gain further popularity and surpass temporal templates in the future.\n\\textbf{Feature Representation.} Our analysis shows that over 87\\% of available methods are based on global feature representations, where deep features are learned by considering gait information as a whole. Recently, many interesting and high performing methods~ have adopted partial representations by splitting the gait data into local regions. The performance of such techniques points to promising potential in partial representation learning for discriminating key gait features. Hence, we anticipate further research with convincing results in this area. \n\\textbf{Neural Architectures.}  As presented in Figure~\\ref{fig:tax}(b), 2D CNNs are the most widely used DNN type for deep gait recognition with 48\\% of the published solutions utilizing only 2D CNN architectures for classification. \n3D CNNs and GANs are the next popular categories, each corresponding to 8\\% of the literature. DAEs, RNNs, CapsNets, DBNs, and GCNs are less considered among DNNs, respectively corresponding to 4\\%, 2\\%, 2\\%, 1\\%, and 1\\% of the methods. Concerning hybrid methods which constitute 26\\% of the published solutions, CNN-RNN combinations are the most widely adapted approach with 16\\% share,\nwhile the combination of DAEs with GANs and RNNs corresponds to 8\\% of the methods, followed by RNN-CapsNet methods that make up 2\\% of the solutions. We expect that hybrid methods that make use of two or more types of DNN attract more attention in the near future and demonstrate robust performance in the field.\n\\textbf{Loss Functions.}\nLoss functions calculate a model's error during training and should ideally be designed to efficiently capture the properties of the problem for facilitating an effective training process~. Figure~\\ref{fig:tax}(b) shows the usage frequency of different well-known loss functions that have been used by deep gait recognition literature. Among the single loss functions, cross-entropy~ has been the most widely adopted with 20\\% of solutions having used it. This loss function takes the output probabilities of the predicated classes\nand makes the model output as close as possible to the ground-truth output. Triplet loss~ is the next popular type with a usage frequency of 17\\%. This loss has been notably used by some of the most recent and state-of-the-art solutions~ and ~. This loss function compares a baseline input, also known as \\textit{anchor}, to a \\textit{positive} sample with the same identity, and a \\textit{negative} sample with a different identity. The loss function then ensures that the dissimilarity between two feature vectors belonging to the same subject is lower than that between feature vectors belonging to two different subjects. Contrastive loss~ corresponds to 7\\% of the recognition methods, and uses pairs of samples including anchor-neighbor or anchor-distant. If the pair of samples is anchor-neighbor, the loss function minimizes their distance; otherwise, it increases their distance. Th next popular loss function, corresponding to 6\\% of the recognition methods, is based on the softmax loss~. There have also been some other loss functions, such as arcface~, center loss~, and Euclidean loss~ with a combined usage frequency of 9\\% that have been less considered for gait recognition. Finally, there have been two classes of deep gait recognition methods that use multiple loss functions (usage frequency of 41\\%), including (\\textit{i}) methods such as~ that add together two or more loss functions to complement each other and compensate their weaknesses; and (\\textit{ii}) methods that have been designed based on networks with multiple components, such GANs with generators and discriminators~ and hybrid networks~, where different loss functions have been used to train different components. We expect that deep gait recognition methods based on multiple losses attract more attention and surpass other approaches in the near future.\n\\textbf{Datasets.}\nWe tally the number of times each dataset has been used by the published literature and present the results in Figure~\\ref{fig:tax}(b). Figure~\\ref{fig:tax}(b) does not include the datasets that have appeared less than 3 times in Table~\\ref{tab:Status}. In addition, a point to consider is that many of the literature use more than one dataset to perform the experiments. We observe that CASIA-B~ is the most widely used dataset, appearing in 80\\% of the published literature, as it provides a large number of samples with variations in carrying and wearing conditions. OU-ISIR~ was the largest gait dataset prior to 2018; we therefore found OU-ISIR to be the second most popular dataset having been used by 40\\% of the solutions. Since the introduction of OU-MVLP~ in 2018, this dataset has been receiving considerable attention from the community and has been used by 18\\% of the methods in a span of only 2 years. The OU-ISIR LP Bag dataset~ only consists of gait data with carried objects, so naturally it was only considered when designing solutions for specific applications such as those intended to be invariant to carrying conditions from a single viewpoint. As a result, this dataset was used for evaluation purposes by only 5\\% of methods. TUM GAID~ has also been less considered by the community, corresponding to 5\\% of published literature. Finally, CASIA-E~ which was developed in 2020 is the sixth most widely used, appearing in 4\\% of the literature. However, we anticipate that this dataset will become the standard benchmark dataset for gait recognition in the near future, due to the fact that it provides hundreds of video sequences per each subject with high variations in appearance and acquisition environments.", "cites": [311, 310, 296, 309, 308, 9138, 295, 300, 298], "cite_extract_rate": 0.2571428571428571, "origin_cites_number": 35, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes information from the cited papers into a structured analysis across five dimensions (body, temporal, feature, neural architecture, and loss functions), connecting trends and methodological choices. It shows critical awareness by pointing out limitations (e.g., underutilization of certain neural architectures) and making predictions based on recent advancements. The section abstracts from individual papers to identify overarching patterns and anticipate future directions, offering meta-level insights."}}
{"id": "29a6cad4-aa4d-450b-9b53-67e616f6bae1", "title": "Performance Comparison", "level": "subsection", "subsections": [], "parent_id": "c203cf63-21d3-48d6-90ac-d1b1607a9797", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "State-of-the-art"], ["subsection", "Performance Comparison"]], "content": "\\label{sec:comp}\nTo shed more light on the performance of deep gait recognition methods, we summarize the performance of the methods tested on the three most popular gait datasets, namely  CASIA-B~, OU-ISIR~, and OU-MVLP~ datasets in Tables~\\ref{tab:CASIA},~\\ref{tab:OU}, and~\\ref{tab:OU2} respectively. To perform a fair comparison, these tables only include methods that followed the standard test protocols designed for these datasets, as discussed in Section 3.2. The results show that the method proposed in~ currently provides the best recognition results on CASIA-B (average performance result of 90.4\\%) and OU-ISIR (performance result of 99.9\\%). Concerning the OU-MVLP dataset, results show the superiority of the method proposed in~ (performance result of 89.18\\%) over other methods. Apart from~ and~, there are several other methods including those proposed in~, whose performances are near the state-of-the-art for these datasets. Our analysis shows that some of these best performing methods, including~, make use of two or more types of neural architectures to boost the performance. Some other methods including ~ use multiple loss functions to complement each other and compensate their weaknesses to boost performance. This analysis reveals the effectiveness of hybrid approaches, in term of either neural architectures as well as loss functions, for achieving strong performance in the area.\n\\begin{table}[!t]\n\\centering\n \\setlength\\tabcolsep{6pt}\n\\caption{State-of-the-art results on CASIA-B dataset ~. NM, BG, and CL are respectively normal walking, walking with a bag, and walking with a coat test protocols.}\n\\begin{tabular}{ l l l | l l l  | l }\n\\hline\n\\multicolumn{3}{ c |}{\\textbf{Method}} & \\multicolumn{4}{ c }{\\textbf{Performance}}  \\\\ \n\\hline\n\t \\textbf{Reference} & \\textbf{Year} & \\textbf{Venue} &  \\textbf{NM} & \\textbf{BG} & \\textbf{CL} &  \\textbf{Average} \\\\ \\hline\\hline\n\t~ & 2015 & \\textit{IEEE T-MM} & 78.9 & --- & --- & --- \\\\\n\t~ & 2017 & \\textit{Int. J. Biom.} & 90.8 & 45.9 & 45.3 & 60.7\\\\\n\t~ & 2017 & \\textit{IEEE T-PAMI} & 94.1 & 72.4 & 54.0 & 73.5 \\\\ \n\t~ & 2018 & \\textit{DIC} & 83.3 & --- & 62.5 & --- \\\\\n\t~   & 2019 & \\textit{PR} & 75.0 & --- & --- & --- \\\\         \n\t~ & 2019 & \\textit{IEEE T-IFS} & 79.8 & --- & --- & ---\\\\ \n\t~ & 2019 & \\textit{PR} & 89.9 & --- & --- & --- \\\\\n\t~ & 2019 & \\textit{CVPR} & 93.9 & 82.6 & 63.2 & 79.9 \\\\\n\t~ & 2019 & \\textit{CVPR} & 89.9 & --- & --- & --- \\\\\n\t~ & 2019 & \\textit{IET Biom.} & 94.5  & 78.6  & 51.6 & 74.9\\\\\n\t~ & 2019 & \\textit{PRL} & 86.1 & --- & --- & --- \\\\\n\t~ & 2019 & \\textit{AAAI} & 95.0 & 87.2 & 70.4 & 84.2 \\\\\n\t~ & 2020 & \\textit{IEEE T-PAMI} & 92.3 & 88.9 & 62.3 & 81.2\\\\\n\t~ & 2020 & \\textit{IEEE T-IP} & 96.0 & --- & --- & --- \\\\\n\t~ & 2020 & \\textit{IEEE T-CSVT} & 92.7 & --- & --- & --- \\\\\n\t~ & 2020 & \\textit{IEEE Access} & 95.1 & 87.9 & 74.0 & 85.7 \\\\\n\t~ & 2020 & \\textit{ICPR} & 95.7 & 90.7 & 72.4 & 86.3\\\\\n\t~ & 2020 & \\textit{IEEE T-Biom.} & 95.2 & 89.7 & 74.7 & 86.5\\\\\n\t~ & 2020 & \\textit{CVPR} & 96.2 & 91.5 & 78.7 & 88.8 \\\\\n    ~ & 2020 & \\textit{CVPR} & 94.5 & --- & --- & --- \\\\\n    ~& 2020 & ECCV & 96.8 & \\textbf{94.0} & 77.5 & 89.4 \\\\\n\t~ & 2020 & \\textit{ACCV} & \\textbf{97.9} & 93.1 & 77.6 & 89.5\\\\\n\t~ & 2020 & \\textit{MM} & 96.7 & 93.0 & \\textbf{81.5} & \\textbf{90.4} \\\\\n\\hline\n\\end{tabular}\n\\label{tab:CASIA}\n\\end{table}\n\\begin{table}[!t]\n\\centering\n \\setlength\\tabcolsep{8pt}\n\\caption{State-of-the-art results on OU-ISIR~ dataset.}\n\\begin{tabular}{ l l l | l  }\n\\hline\n\\multicolumn{3}{ c |}{\\textbf{Method}} & \\multicolumn{1}{ c }{}  \\\\ \n\t \\textbf{Reference} & \\textbf{Year} & \\textbf{Venue} &   \\textbf{Performance}  \\\\ \\hline\\hline\n          & 2016  & \\textit{ICASSP}         &90.71            \\\\  \n             & 2017  & \\textit{IEEE T-PAMI}   &92.77           \\\\  \n          & 2017  & \\textit{Applied Sci.}  &91.25            \\\\\n         & 2018  & \\textit{Neuroinform.}  &88.56            \\\\\n          & 2018  & \\textit{IEEE Access}   &95.67            \\\\\n        & 2019  & \\textit{IET Biom.}     &97.40            \\\\\n             & 2019  & \\textit{IEEE T-IFS }   &93.20            \\\\\n         & 2019  & \\textit{PRL}           &94.62            \\\\\n         & 2019  & \\textit{IEEE T-MM}     &97.26            \\\\\n         & 2020  & \\textit{IJCB}          &94.17            \\\\\n           & 2020  & \\textit{IEEE T-CSVT}   &98.93            \\\\\n            & 2020  & \\textit{IEEE T-IP}     &99.27            \\\\\n             & 2020  & \\textit{MM}            &\\textbf{99.90}   \\\\\n\\hline\n\\end{tabular}\n\\label{tab:OU}\n\\end{table}\n\\begin{table}[!t]\n\\centering\n \\setlength\\tabcolsep{8pt}\n\\caption{State-of-the-art results on OU-MVLP~ dataset.}\n\\begin{tabular}{ l l l | l  }\n\\hline\n\\multicolumn{3}{ c |}{\\textbf{Method}} & \\multicolumn{1}{ c }{}  \\\\ \n\t \\textbf{Reference} & \\textbf{Year} & \\textbf{Venue} & \\textbf{Performance}    \\\\ \\hline\\hline\n             & 2019  & \\textit{AAAI}          &  83.40   \\\\\n        & 2020  & \\textit{ICASSP}        &  57.80   \\\\\n           & 2020  & \\textit{IEEE T-CSVT}   &  63.10   \\\\\n            & 2020  & \\textit{IEEE T-IP}     &  84.60   \\\\\n           & 2020  & \\textit{ICPR}          &  84.50   \\\\\n          & 2020  & \\textit{IEEE T-Biom.}  &  84.30   \\\\\n      & 2020  & \\textit{CVPR}          &  88.70   \\\\\n           & 2020  & \\textit{ECCV}          &  \\textbf{89.18}   \\\\\n\\hline\n\\end{tabular}\n\\label{tab:OU2}\n\\end{table}\n{", "cites": [294, 307, 296, 304, 9138, 295, 300], "cite_extract_rate": 0.20588235294117646, "origin_cites_number": 34, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes results from multiple papers into a structured performance comparison across three popular datasets, indicating some integration. It offers critical observations about the methods achieving top performance, such as the use of hybrid architectures and multiple loss functions. The abstraction level is moderate as it identifies general trends like the effectiveness of hybrid approaches, but does not offer a comprehensive theoretical framework or deeper meta-level insights."}}
{"id": "d74cb17e-ad4d-4784-8c2d-f13df1f49804", "title": "Vulnerability to Adversarial Attacks", "level": "section", "subsections": [], "parent_id": "1ca90fda-21b2-4c6d-bcb1-6e612173f620", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "Vulnerability to Adversarial Attacks"]], "content": "}\n{Traditional spoofing attacks to gait recognition systems were attempted by trained impostors, notably with similar body shapes and clothes, imitating the walking styles of target subjects~. However, these attacks are rather hard and limited as they require trained and qualified imitators. Different from the traditional spoofing attacks, \\textit{adversarial attacks} to gait recognition systems have been designed to imperceptibly fool recognition systems by synthesizing input gait videos with both good quantitative similarity and visual realism.}\n{Despite the strong performance of deep learning solutions in computer vision, such solutions have been surprisingly vulnerable to adversarial attacks~. These attacks introduce perturbations in visual content that can manipulate the predictions of deep models by resulting in embeddings capable of fooling the classifiers~. \nSince the introduction of the first adversarial attacks to deep neural networks in 2014~, these models have attracted significant attention from the research community in computer vision and pattern analysis. Among the adversarial attack approaches, GANs~ have been one of the most powerful methods for image and video manipulation.}\n{The first attempt to investigates the vulnerability of gait recognition systems to adversarial attacks was done in~ using a GAN for synthesizing gait images. In this context, the foreground of each frame of the source video is first segmented to be then fed to the generator along with the target background. The generator is composed of two parallel encoder-decoder networks, respectively dealing with foreground and background information. The corresponding feature representations from these two networks are fused at multiple scales. Static and dynamic silhouette-based losses have been designed in order to force the model to generate more realistic results for gait recognition. Additionally, triplet loss is used to preserve the similarity between the individuals in the source and generated videos. The performance of two state-of-the-art gait recognition systems, including CNNGait~ and GaitSet~, were evaluated using the generated gait samples. The results showed that the generated samples provide sufficient discriminative information to bypass gait recognition systems. To show how the sequence-based gait recognition is vulnerable to adversarial attacks, another study was done in~. In this study, a novel temporal sparse adversarial attack method was proposed based on a GAN to synthesize high-quality sequences of silhouette frames. To ensure imperceptibility of the proposed method, a few adversarial gait silhouettes are substituted or inserted in the sequence. Experimental results show that the state-of-the-art GaitSet~ method has low robustness to this adversarial attack.}\n{The presented results in~ suggest that adversarial attacks can surprisingly degrade the performance of deep gait recognition methods, thus posing a real threat to such recognition systems. This demonstrates the necessity of adopting efficient countermeasure techniques against adversarial attacks aimed towards deep gait recognition systems.}", "cites": [315, 7217, 8338, 7015, 314, 9138, 313, 312], "cite_extract_rate": 0.5333333333333333, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to discuss how adversarial attacks can compromise deep gait recognition systems, particularly through GAN-based methods. It provides a coherent narrative by connecting traditional spoofing techniques with modern adversarial approaches. While it includes some critical analysis by highlighting the vulnerabilities exposed in the studies, it does not deeply critique the methodologies or compare their strengths and weaknesses. It abstracts to some extent by framing adversarial attacks as a broader threat to deep learning in gait recognition."}}
{"id": "ef418d92-183b-4dbf-84cd-f4581eba68d5", "title": "Disentanglement", "level": "subsection", "subsections": [], "parent_id": "73e8a8f0-de22-4ddf-bbaa-fec216d6b3cc", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "Challenges and Future Research Directions"], ["subsection", "Disentanglement"]], "content": "Complex gait data arise from the interaction between many factors such as occlusion, camera view-points, appearance of individuals, sequence order, body part motion, or lighting sources present in the data~. These factors can interact in complex manners that can complicate the recognition task. There have recently been a growing number of methods in other research areas, such as face recognition~, action recognition~, emotion recognition~, and pose estimation~, that focus on learning disentangled features by extracting representations that separate the various explanatory factors in the high-dimensional space of the data~. However, the majority of available deep gait recognition methods have not explored disentanglement approaches, and hence are not explicitly able to separate the underlying structure of gait data in the form of meaningful disjoint variables. Despite the recent progress in using disentanglement approaches in a few gait recognition methods~, there is still room for improvement. To foster further progress in this area, the adaptation of novel generative models~ and loss functions~ can be considered to learn more discriminative gait representations by explicitly disentangling identity and non-identity components.", "cites": [318, 8340, 7218, 304, 319, 7016, 295, 8339, 317, 316], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.3, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple papers to build a coherent narrative on the importance and current state of disentanglement in deep gait recognition. It abstracts the concept of disentanglement beyond the specific domain, linking it to broader recognition tasks such as face and action recognition. While it identifies a gap in the application of disentanglement methods to gait recognition, the critical analysis could be deeper by more explicitly comparing the effectiveness or limitations of the cited disentanglement approaches."}}
{"id": "4a1ba97f-c323-4eb6-9f69-39c63702afa1", "title": "Self-supervised Learning", "level": "subsection", "subsections": [], "parent_id": "73e8a8f0-de22-4ddf-bbaa-fec216d6b3cc", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "Challenges and Future Research Directions"], ["subsection", "Self-supervised Learning"]], "content": "A considerable majority of available deep gait recognition methods follow the supervised learning paradigm, and thus require labeled data during training. Nevertheless, in real-world applications, labeled data may not always be readily available and labels are generally expensive and time-consuming to obtain. In order to utilize unlabeled gait data to learn more efficient and generalizable gait representations, self-supervised learning~ can be exploited. In this context, general and rich high-level semantics can be captured without using any annotated labels. Self-supervised approaches can define various pretext tasks, such as body part motion or sequence order recognition for input sequences~, to be solved by a network. Through learning these pretext tasks, the network can then learn generic features. The network trained with the generated pre-text labels can then be fine-tuned with the actual labels in order to recognize the identity. Among self-supervised approaches, contrastive learning methods~, including SimCLR~, are promising approaches that learn representations by defining an \\textit{anchor} and a \\textit{positive} sample in the feature space, and then aim to make the anchor separable from the \\textit{negative} samples. One important challenge in using self-supervised learning in the context of gait recognition is to design effective pretext tasks to ensure the network can learn meaningful representations.\nAdditionally, \njoint learning of several pretext tasks in a network~, instead of a single pretext task, notably using several loss functions~, can provide the network with more representative features~.\nWe expect these challenges to gain increased popularity in the context of deep gait recognition in the near future.", "cites": [321, 7000, 7020, 322, 7019, 320, 7018, 7017], "cite_extract_rate": 0.8888888888888888, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates concepts from multiple papers to highlight the potential and challenges of self-supervised learning in gait recognition. It connects the use of pretext tasks across domains like skeleton data and ECG, and introduces contrastive learning as a promising direction. However, it lacks deeper critical evaluation or detailed comparison of the approaches, and while it identifies general principles, it does not offer a novel or meta-level synthesis."}}
{"id": "2f8e6d7f-8501-474a-8b6b-a74553ed5cb3", "title": "Multi-task Learning", "level": "subsection", "subsections": [], "parent_id": "73e8a8f0-de22-4ddf-bbaa-fec216d6b3cc", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "Challenges and Future Research Directions"], ["subsection", "Multi-task Learning"]], "content": "Multi-task learning is generally performed to simultaneously learn multiple tasks using a shared model, thus learning more generalized and often reinforced representations~. In many cases, these approaches offer advantages such as increased convergence speed, improved learning by leveraging auxiliary information, and reduced overfitting through shared representations~.\nDespite the effectiveness of multi-task learning in a number of other domains~, most deep gait recognition solutions in the literature focus on the single task of identification. Thus, most existing works learn features that are sensitive to identity without considering interactions with other latent factors, such as affective states, gender, and age~. \nIn this context, simultaneous learning of multiple tasks for gait recognition may present new design paradigms and optimization challenges, notably in terms of task identification and loss functions~. \nWe expect these challenges to attract further attention in the near future and be tackled in the context of gait recognition with multi-task learning.", "cites": [7021, 8341, 324, 326, 325, 323], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a moderate level of synthesis by connecting ideas from multiple papers on multi-task learning, such as attention-based architectures, geometric loss strategies, and curriculum learning. It also identifies a gap in the gait recognition literature by noting the dominance of single-task approaches and the potential for multi-task learning to introduce new design and optimization challenges. The abstraction is reasonable, pointing to broader implications for future research, but the critical analysis remains somewhat surface-level and lacks deeper evaluation of limitations or trade-offs among cited approaches."}}
{"id": "9e57c9fd-2598-401a-98a6-1ffaaf205af2", "title": "Data Synthesis and Domain Adaptation", "level": "subsection", "subsections": [], "parent_id": "73e8a8f0-de22-4ddf-bbaa-fec216d6b3cc", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "Challenges and Future Research Directions"], ["subsection", "Data Synthesis and Domain Adaptation"]], "content": "Deep gait recognition methods require large amounts of data for effective training and reliable evaluation. This issue is evident in Figure~\\ref{fig:tax}(b) where most of the deep gait recognition solutions~ used large-scale gait datasets for instance CASIA-B~, OU-ISIR~, and OU-MVLP~. In the context of deep gait recognition, data synthesis, for instance using GANs~, can be considered for creating large datasets or data augmentation~. Furthermore, developing synthesized datasets can also be advantageous in that subject privacy concerns could be alleviated with fake subject data. Similar approaches have been carried out for the more \\textit{privacy-sensitive} area of facial recognition~, where large datasets comprised only of fake data have been developed to be used in deep learning research~. In addition, such approaches can be used to increase the variance of existing datasets. For instance, large-scale gait datasets such as OU-ISIR~ and OU-MVLP~ only provide normal walking sequences with no variations in occlusion or carrying and clothing conditions. Thus, solutions trained on these datasets usually fail to generalize well when facing variations in appearance and environment during the testing phase. Here, domain adaptation~ is a potential remedy for this problem that can modify existing datasets to include the desired variations, thus eliminating the necessity for collecting new data. \nFurthermore, gait synthesis can be performed for computer animation~ and by game engines~\nto generate large-scale synthetic gait datasets. Hence, we anticipate that with advances in gait data synthesis and domain adaptation techniques, more complementary gait datasets will be constructed to enable the development of more robust solutions.", "cites": [327, 328, 329, 296, 157, 7023, 330, 7022, 300], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 27, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers on GAN-based data synthesis and domain adaptation, connecting these ideas to the problem of limited and constrained gait datasets. It provides a critical perspective by pointing out the limitations of existing datasets and how domain adaptation can help. The abstraction is moderate, as it generalizes the idea of using synthesis and adaptation techniques for improving gait recognition systems but does not reach a higher meta-level of conceptual insight."}}
{"id": "74e9a790-4f79-4c0c-b201-edd4b360bb05", "title": "Cross-Dataset Evaluation", "level": "subsection", "subsections": [], "parent_id": "73e8a8f0-de22-4ddf-bbaa-fec216d6b3cc", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "Challenges and Future Research Directions"], ["subsection", "Cross-Dataset Evaluation"]], "content": "The practical value of gait recognition systems is strongly dependant in its ability to generalize to unseen data. To the best of our knowledge, cross-dataset gait recognition on well-known datasets such as CASIA-B~, OU-ISIR dataset~, and OU-MVLP~, has not been performed in the literature as notable solutions available in the literature all use the same gait dataset for both training and testing. However, in many real applications such as deployed products, test or run-time data are often obtained in a variety of different conditions with respect to the training data. In order to examine the generalizability of gait recognition systems in real-world applications, cross-dataset evaluations should be adopted, for example using transfer learning techniques~. In this context, a solution trained on one dataset can be used to extract features from the test data (gallery and probe sets) of another dataset. The extracted features can then feed a classifier to perform gait recognition. Cross-dataset gait recognition can potentially be formulated as an out-of-distribution (OOD) testing problem, where the generalization ability of a deep model beyond the biases of the training set is evaluated~. We expect that OOD tests~ become increasingly popular for evaluating the generalization ability of gait recognition methods.", "cites": [7219], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the lack of cross-dataset evaluation in gait recognition, linking this gap to the practical need for generalizability. It synthesizes the idea of OOD testing from the cited paper to frame cross-dataset evaluation as a broader problem. While it identifies a critical research gap, it does not deeply critique existing works or compare them in detail, limiting its critical depth."}}
{"id": "a1025c3d-68e9-40c7-a32c-b33f93ac7746", "title": "Multi-View Recognition", "level": "subsection", "subsections": [], "parent_id": "73e8a8f0-de22-4ddf-bbaa-fec216d6b3cc", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "Challenges and Future Research Directions"], ["subsection", "Multi-View Recognition"]], "content": "A large number of gait datasets contain multi-view sequences, providing gait information captured from different view-points. Most of the current methods available in the literature only perform single-view gait recognition. These methods generally learn intra-view relationships and ignore inter-view information between multiple viewpoints.\nBy casting the problem as multi-view, descriptors such as gate-level fusion LSTM~, state-level fusion LSTM~, spatio-temporal LSTM~, multi-perspective LSTM~, and multi-view LSTM~, can be adopted to jointly learn both the intra-view and inter-view relationships. Another challenge in multi-view gait recognition is that most existing multi-view descriptors consider a well defined camera network topology with fixed camera positions. However, data collection in real-world environments is often uncontrollable, i.e. data might be captured from unpredictable viewing angles~ or even from moving cameras~. To this end, existing multi-view methods, which mostly rely on pre-trained descriptors, fail to bridge the domain gap between the training and run-time multi-view data. We expect that future research direction in this area will be shaped by proposing novel approaches, for example using clustering algorithms~, combinatorial optimization~, and self-supervised learning~, for adopting generic gait descriptors for multi-view geometry.", "cites": [334, 332, 331, 333], "cite_extract_rate": 0.4, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple cited papers to frame the issue of multi-view gait recognition, connecting the concept of intra-view vs. inter-view learning with various LSTM-based approaches. It also critically addresses limitations, such as the reliance on fixed camera topologies and the domain gap. The section abstracts the problem to propose broader research directions like self-supervised learning and combinatorial optimization."}}
{"id": "aaa73ee6-e50d-4ab2-ad19-7591a0a71a0b", "title": "Multi-biometric Recognition", "level": "subsection", "subsections": [], "parent_id": "73e8a8f0-de22-4ddf-bbaa-fec216d6b3cc", "prefix_titles": [["title", "Deep Gait Recognition: A Survey"], ["section", "Challenges and Future Research Directions"], ["subsection", "Multi-biometric Recognition"]], "content": "Some literature in the field have fused gait information with other biometric information such as face~ and ear~, which can be obtained from high-quality gait videos. As we discussed earlier, gait recognition systems are generally challenged when facing variations in subject appearance and clothing, camera view-points, and body occlusions. On the other hand, additional sources of biometric information, notably face and ear, are less sensitive to some of these challenging factors. Instead, face and ear recognition systems can be negatively affected by some other factors such as low image quality, for instance blurred or low resolution images, varying lighting, or facial occlusions, which in turn have limited impact on the performance of gait recognition systems. Hence, various biometric modalities and gait can complement one another to compensate each others' weaknesses in the context of a multi-biometric system~. \nApart from the complementary (hard-)biometric traits, soft-biometric traits such as age~, height~, weight~, gender~, and particular body marks including tattoos~ can also be included to boost overall performance. The combination of other soft- and hard- biometric traits with gait has mostly been done in the literature based on non-deep methods~, while multi-modal deep learning methods~, notably based on fusion~, joint learning~, and attention~ networks, can also be adopted. Hence, we anticipate that research on deep multi-biometric recognition systems that include gait, gain popularity in the coming years.", "cites": [337, 335, 290, 332, 288, 336, 297], "cite_extract_rate": 0.2916666666666667, "origin_cites_number": 24, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the role of gait in multi-biometric systems by integrating ideas from papers on face, ear, and tattoo recognition, as well as deep fusion architectures. It provides a nuanced comparison of the strengths and weaknesses of different biometric traits, showing some critical insight into the limitations of traditional methods and the potential of deep learning for fusion. The abstraction is strong as it generalizes the idea of modality complementarity and points to future research directions in multi-biometric deep learning."}}
