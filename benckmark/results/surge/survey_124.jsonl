{"id": "ab1ab2db-6a69-42b1-bffa-aa304b490923", "title": "Definitions of the four phases", "level": "subsection", "subsections": [], "parent_id": "93654628-99a6-4537-9533-0c31e4f97c48", "prefix_titles": [["title", "Using Deep Learning to Solve Computer Security Challenges: A Survey"], ["section", "A four-phase workflow framework can summarize the existing works in a unified manner"], ["subsection", "Definitions of the four phases"]], "content": "The four phases, shown in Figure \\ref{fig:4phases}, are defined as follows. To make the definitions of the four phases more tangible, we use a running example to illustrate each of the four phases. \n\\begin{figure}[!ht]\n\\begin{center}\n\\includegraphics[scale=0.80]{figure/overview.pdf}\n\\caption{Overview of the four-phase workflow} \n\\label{fig:4phases}\n\\end{center}\n\\end{figure}\n\\begin{phase}{Obtaining the Raw Data}\\label{phase1}\nIn this phase, certain raw data are collected.\n\\begin{runexample}\nWhen Deep Learning is used to detect suspicious events in a Hadoop distributed file system (HDFS), the raw data are usually the events (e.g., a block is allocated, read, written, replicated, or deleted) that have happened to each block.  \nSince these events are recorded in Hadoop logs, the log files hold the raw data. \nSince each event is uniquely identified by a particular (block ID, timestamp) tuple, we could simply view\nthe raw data as $n$ event sequences. Here $n$ is the total number of blocks in the HDFS. \nFor example, the raw data collected in \nin total consists of 11,197,954 events. \nSince 575,139 blocks were in the HDFS, there were \n575,139 event sequences in the raw data, and on average\neach event sequence had 19 events. One such event sequence is shown as follows: \n\\footnotesize\n\\begin{verbatim}\n081110 112428 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock:\n  /user/root/rand/_temporary/_task_200811101024_0001_m_001649_0/\n  part-01649.blk_-1033546237298158256\n081110 112428 9602 INFO dfs.DataNode$DataXceiver: \n  Receiving block blk_-1033546237298158256 src: /10.250.13.240:54015\n  dest:/10.250.13.240:50010\n081110 112428 9982 INFO dfs.DataNode$DataXceiver: \n  Receiving block blk_-1033546237298158256 src: /10.250.13.240:52837 \n  dest:/10.250.13.240:50010\n081110 112432 9982 INFO dfs.DataNode$DataXceiver: \n  writeBlock blk_-1033546237298158256 received exception\n  java.io.IOException:Could not read from stream\n\\end{verbatim}\n\\normalsize\n\\end{runexample}\n\\end{phase}\n\\begin{phase}{Data Preprocessing}\\label{phase2}\nBoth Phase~\\ref{phase2} and Phase~\\ref{phase3} aim to properly \nextract and represent the useful   \ninformation held in the raw data collected in Phase I. \nBoth Phase~\\ref{phase2} and Phase~\\ref{phase3} are closely related\nto feature engineering. \nA key difference between Phase~\\ref{phase2} and Phase~\\ref{phase3} \nis that Phase~\\ref{phase3} is completely dedicated to \nrepresentation learning, while Phase~\\ref{phase2} is \nfocused on all the information extraction and data \nprocessing operations that are \\textbf{not} based on \nrepresentation learning. \n\\begin{runexample}\nLet's revisit the \naforementioned HDFS. \nEach recorded event is described by unstructured text. \nIn Phase~\\ref{phase2}, the unstructured text\nis parsed to a data structure that shows the event type and a list of event variables in (name, value) pairs. \nSince there are 29 types of events in the HDFS, \neach event is represented by an integer from 1 to 29 according to its type. \nIn this way, the aforementioned example event sequence \ncan be transformed to:  \n  \\begin{verbatim} \n   22, 5, 5, 7\n  \\end{verbatim}\n\\end{runexample}\n\\end{phase}\n\\begin{phase}{Representation Learning}\\label{phase3}\nAs stated in , ``Learning representations of the data that make it easier to extract useful information when building classifiers or other predictors.\"  \n\\begin{runexample}\nLet's revisit the same HDFS. \nAlthough DeepLog  directly employed one-hot vectors to represent the event types without representation learning,  \nif we view an event type as a word in a structured language, \none may actually use the word embedding technique to represent each event type. It should be noticed that \nthe word embedding technique is a representation \nlearning technique. \n\\end{runexample}\n\\end{phase}\n\\begin{phase}{Classifier Learning}\\label{phase4}\nThis phase aims to build specific classifiers or other predictors through Deep Learning. \n\\begin{runexample}\nLet's revisit the same HDFS.\nDeepLog  used Deep Learning to build a stacked LSTM neural network for anomaly detection. \nFor example, let's consider event sequence  \\{22,5,5,5,11,9,11,9,11,9,26,26,26\\} in which each \ninteger represents the event type of the corresponding event in the event sequence.  \nGiven a window size $h$ = 4, the input sample and the output label pairs to train DeepLog will be: \\{22,5,5,5 $\\rightarrow$ 11 \\}, \\{5,5,5,11 $\\rightarrow$ 9 \\}, \\{5,5,11,9 $\\rightarrow$ 11 \\}, and so forth. \nIn the detection stage, DeepLog examines each individual event.  It determines if an event is treated as normal or abnormal according to whether the event's type is predicted by the LSTM neural network, given the history of event types.\nIf the event's type is among the top $g$ predicted types, \nthe event is treated as normal; otherwise, it is treated as abnormal.\n\\end{runexample}\n\\end{phase}", "cites": [318], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes information from the cited paper on representation learning to define a four-phase workflow. It abstracts broader principles of data processing and representation learning by using a running example to illustrate generalizable steps across different security applications. However, the critical analysis is limited, as it does not extensively evaluate or contrast the strengths/weaknesses of different approaches or highlight unresolved issues in the field."}}
{"id": "cd39d418-ca95-49d8-bc15-bb9f46240292", "title": "Using the four-phase workflow framework to summarize some representative research works", "level": "subsection", "subsections": [], "parent_id": "93654628-99a6-4537-9533-0c31e4f97c48", "prefix_titles": [["title", "Using Deep Learning to Solve Computer Security Challenges: A Survey"], ["section", "A four-phase workflow framework can summarize the existing works in a unified manner"], ["subsection", "Using the four-phase workflow framework to summarize some representative research works"]], "content": "In this subsection, we use the four-phase workflow framework to summarize two representative works for each security problem. \n{\\color{myblack}\nSystem security includes many sub research topics. \nHowever, not every research topics are suitable to adopt deep learning-based methods due to their intrinsic characteristics. \nFor these security research subjects that can combine with deep-learning, \nsome of them has undergone intensive research in recent years, \nothers just emerging. \nWe notice that there are 5 mainstream research directions in system security. \nThis paper mainly focuses on system security, so the other mainstream research directions (e.g., deepfake) are out-of-scope.\nTherefore, we choose these 5 widely noticed research directions,\nand 3 emerging research direction in our survey:\n\\begin{enumerate}\n\\item In security-oriented program analysis, malware classification (MC), system-event-based anomaly detection (SEAD), memory forensics (MF), and defending network attacks, deep learning based methods have already undergone intensive research. \n\\item In defending return-oriented programming (ROP) attacks, Control-flow integrity (CFI), and fuzzing, deep learning based methods are emerging research topics.\n\\end{enumerate}\nWe select two representative works for each research topic in our survey.\nOur criteria to select papers mainly include: 1) Pioneer (one of the first papers in this field); \n2) Top (published on top conference or journal);\n3) Novelty; 4) Citation (The citation of this paper is high); \n5) Effectiveness (the result of this paper is pretty good);\n6) Representative (the paper is a representative work for a branch of the research direction). \nTable~\\ref{tab:reason} lists the reasons why we choose each paper, which is ordered according to their importance.\n}\n\\begin{table*}[ht]\n    \\footnotesize\n    \\caption{List of criteria we used to choose representative work for each research topic.}\n    \\label{tab:reason}\n  \\makeatletter\n  \\newcommand{\\widenhline}{\n      \\noalign {\\ifnum 0=`}\\fi \\hrule height 0.5pt\n      \\futurelet \\reserved@a \\@xhline\n  }\n  \\newcolumntype{I}{!{\\vrule width 1pt}}\n  \\makeatother\n  \\centering\n  \\small\n  \\begin{tabular}{cIc|c|c|c}  \n  \\hline  \n  \\hline  \n  \\diagbox[dir=NW]{Paper}{Order} & 1 & 2 & 3  & 4 \\\\ \\widenhline\n  \\hline  \n  RFBNN~ & Pioneer & Top & Novelty & Citations \\\\ \n  \\hline  \n  EKLAVYA~ & Top & Novelty & Citation & N/A \\\\  \n  \\hline  \n  ROPNN~ & Pioneer & Novelty & Effectiveness & N/A \\\\ \n  \\hline  \n  HeNet~ & Effectiveness & Novelty & Citation & N/A \\\\ \n  \\hline  \n  Barnum~ & Pioneer & Novelty & N/A & N/A \\\\  \n  \\hline  \n  CFG-CNN~ & Representative & N/A & N/A & N/A \\\\ \n  \\hline  \n  50b(yte)-CNN & Novelty & Effectiveness & N/A & N/A \\\\  \n  \\hline  \n  PCNN~  & Novelty & Effectiveness & N/A & N/A \\\\  \n  \\hline  \n  Resenberg~ & Novelty & Effectiveness & Top & Representative \\\\  \n  \\hline  \n  DeLaRosa~ & Novelty & Representative & N/A & N/A  \\\\ \n  \\hline  \n  DeepLog~ & Pioneer & Top & Citations & N/A \\\\  \n  \\hline  \n  DeepMem~ & Pioneer & Top & N/A & N/A \\\\   \n  \\hline  \n  NeuZZ~ & Novelty & Top & Effectiveness & N/A \\\\  \n  \\hline  \n  Learn \\& Fuzz~ & Pioneer & Novelty & Top & N/A \\\\  \n  \\hline \n  \\hline   \n  \\end{tabular}  \n  \\end{table*}\nThe summary for each paper we selected is shown in Table~\\ref{Table:Summary}. \nThere are three columns in the table. \nIn the first column, we listed eight security problems, including security-oriented program analysis, defending return-oriented programming (ROP) attacks, control-flow integrity (CFI), defending network attacks (NA), malware classification (MC), system-event-based anomaly detection (SEAD), memory forensics (MF), and fuzzing for software security. \nIn the second column, we list the very recent two representative works for each security problem. From the $3$-th to $6$-th columns, we sequentially describe how the four phases are deployed at each work. \nIn the  ``Summary\" column, we sequentially describe how the four phases are deployed at each work, then, we list the evaluation results for each work in terms of accuracy (ACC), precision (PRC), recall (REC), F1 score (F1), false-positive rate (FPR), and false-negative rate (FNR), respectively.\n\\begin{center} \n\\scriptsize\n\\begin{ThreePartTable}\n\\begin{TableNotes}\n    \\item [1] Deep Learning metrics are often not available in fuzzing papers. Typical fuzzing metrics used for evaluations are: code coverage, pass rate and bugs.\n\\end{TableNotes}\n\\newcolumntype{C}[1]{>{\\centering\\arraybackslash}p{#1}}\n\\begin{longtable}{p{1.4cm}<{\\centering}p{1.4cm}<{\\centering}cccccc}\n\\captionsetup{width=.95\\textwidth}\n\\caption{\\footnotesize Solutions using Deep Learning for eight security problems. The metrics in the Evaluation column include accuracy (ACC), precision (PRC), recall (REC), $F_{1}$ score ($F_{1}$), false positive rate (FPR), and false negative rate (FNR).}\n\\label{Table:Summary}\\\\\n\\toprule \\toprule\n\\textbf{Security Problem} & \\textbf{Works} & \\multicolumn{6}{c}{\\textbf{Summary}} \\\\\n\\cmidrule[1pt]{1-8}\n\\endfirsthead\n\\multicolumn{7}{c}{{Continued on Next Page\\ldots}} \\\\\n\\endfoot\n\\insertTableNotes \n\\endlastfoot\n    {\\multirow{5}{1.5cm}{Security Oriented Program Analysis~}} \n            & {\\multirow{2}{*}{RFBNN~}} & \\multicolumn{3}{C{5cm}}{Phase I} & \\multicolumn{3}{C{5cm}}{Phase II}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    && \\multicolumn{3}{C{5.7cm}}{Dataset comes from previous paper~, consisting of 2200 separate binaries. \n                        2064 of the binaries were for Linux, obtained from the coreutils, binutils, and findutils packages. \n                        The remaining 136 for Windows consist of binaries from popular open-source projects.\n                        Half of the binaries were for x86, and the other half for x86-64. } \n     & \\multicolumn{3}{C{5.7cm}}{They extract fixed-length subsequences (1000-byte chunks) from code section of binaries,\n                        Then, use “one-hot encoding”, which converts a\n                        byte into a $\\mathbb{Z}^{256}$ vector.} \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{Phase III} & \\multicolumn{2}{C{3.8cm}}{Phase IV} & \\multicolumn{2}{C{2.8cm}}{Evaluation}  \\\\\n     \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{ N/A } \n      & \\multicolumn{2}{C{3.8cm}}{ Bi-directional RNN}\n      &  \n        \\begin{tabular}[t]{p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}p{0cm}p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}}\n        ACC:&98.4\\%&&PRE:&N/A\\\\\n        REC:&0.97&&$F_{1}$:&0.98\\\\\n        FPR:&N/A&&FNR:&N/A\\\\\n        \\end{tabular} \n       \\\\\n    \\cmidrule[1pt]{2-8}\n    & {\\multirow{2}{*}{EKLAVYA}} & \\multicolumn{3}{C{5cm}}{Phase I} & \\multicolumn{3}{C{5cm}}{Phase II}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{3}{C{5.7cm}}{They adopted source code from previous work~ as their rawdata,\n                                            then obtained two datasets by using two commonly used compilers: gcc and clang, with different optimization levels ranging from O0 to O3 for both x86 and x64. \n                                            They obtained the ground truth for the function arguments by parsing the DWARF debug information.\n                                            Next, they extract functions from the binaries and remove functions which are duplicates of other functions in the dataset. \n                                            Finally, they match caller snipper and callee body.  } \n     & \\multicolumn{3}{C{5.7cm}}{Tokenizing the hexadecimal value of each instruction.} \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{Phase III} & \\multicolumn{2}{C{3.8cm}}{Phase IV} & \\multicolumn{2}{C{2.8cm}}{Evaluation}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{ Word2vec technique to compute word embeddings. } \n      & \\multicolumn{2}{C{3.8cm}}{RNN}\n      & \n      \\begin{tabular}[t]{p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}p{0cm}p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}}\n      ACC:&81.0\\%&&PRE:&N/A\\\\\n      REC:&N/A&&$F_{1}$:&N/A\\\\\n      FPR:&N/A&&FNR:&N/A\\\\\n      \\end{tabular} \\\\\n    \\cmidrule[0.8pt]{1-8}\n    {\\multirow{5}{1.5cm}{Defending Return Oriented Programming Attacks\n    }} \n            & {\\multirow{2}{*}{ROPNN   }} & \\multicolumn{3}{C{5cm}}{Phase I} & \\multicolumn{3}{C{5cm}}{Phase II}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    && \\multicolumn{3}{C{5.7cm}}{The data is a set of gadget chains obtained from existing programs. A gadget searching tool, ROPGadget is used to find available gadgets. Gadgets are chained based on whether the produced gadget chain is executable on a CPU emulator. The raw data is represented in hexadecimal form of instruction sequences. } \n        & \\multicolumn{3}{C{5.7cm}}{Form one-hot vector for bytes.} \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{Phase III} & \\multicolumn{2}{C{3.8cm}}{Phase IV} & \\multicolumn{2}{C{2.8cm}}{Evaluation}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{ N/A } \n        & \\multicolumn{2}{C{3.8cm}}{ 1-D CNN}\n        &  \n        \\begin{tabular}[t]{p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}p{0cm}p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}}\n        ACC:&99.9\\%&&PRE:&0.99\\\\\n        REC:&N/A&&$F_{1}$:&0.01\\\\\n        FPR:&N/A&&FNR:&N/A\\\\\n        \\end{tabular} \\\\\n    \\cmidrule[1pt]{2-8}\n    & {\\multirow{2}{*}{HeNet }} & \\multicolumn{3}{C{5cm}}{Phase I} & \\multicolumn{3}{C{5cm}}{Phase II}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{3}{C{5.7cm}}{Data is acquired from Intel PT, which is a processor trace tool that can log control flow data. Taken Not-Taken (TNT) packet and Target IP (TIP) packet are the two packets of interested. Logged as binary numbers, information of executed branches can be obtained from TNT, and binary executed can be obtained from TIP. Then the binary sequences are transferred into sequences of values between 0-255, called pixels, byte by byte.\n    } \n        & \\multicolumn{3}{C{5.7cm}}{Given the pixel sequences, slice the whole sequence and reshape to form sequences of images for neural network training.\n        } \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{Phase III} & \\multicolumn{2}{C{3.8cm}}{Phase IV} & \\multicolumn{2}{C{2.8cm}}{Evaluation}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{ Word2vec technique to compute word embeddings. } \n        & \\multicolumn{2}{C{3.8cm}}{DNN}\n        &  \n        \\begin{tabular}[t]{p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}p{0cm}p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}}\n        ACC:&98.1\\%&&PRE:&0.99\\\\\n        REC:&0.96&&$F_{1}$:&0.97\\\\\n        FPR:&0.01&&FNR:&0.04\\\\\n        \\end{tabular} \\\\                                        \n        \\cmidrule[0.8pt]{1-8}\n        {\\multirow{5}{1.5cm}{Achieving Control Flow Integrity }} \n                & {\\multirow{2}{*}{Barnum}} & \\multicolumn{3}{C{5cm}}{Phase I} & \\multicolumn{3}{C{5cm}}{Phase II}  \\\\\n        \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n        && \\multicolumn{3}{C{5.7cm}}{The raw data, which is the exact sequence of instructions executed, was generated by combining the program binary, get immediately before the program opens a document, and Intel\\textsuperscript{\\textregistered} PT  trace. While Intel\\textsuperscript{\\textregistered} PT  built-in filtering options are set to CR3 and current privilege level (CPL), which only traces the program activity in the user space.  } \n            & \\multicolumn{3}{C{5.7cm}}{The raw instruction sequences are summarized into Basic Blocks with IDs assigned and are then sliced into manageable subsequences with a fix window size 32, founded experimentally. Only sequences ending on indirect calls, jumps and returns are analyzed, since control-flow hijacking attacks always occur there. The label is the next BBID in the sequence.  } \\\\\n        \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n        & & \\multicolumn{2}{C{4.4cm}}{Phase III} & \\multicolumn{2}{C{3.8cm}}{Phase IV} & \\multicolumn{2}{C{2.8cm}}{Evaluation}  \\\\\n        \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n        & & \\multicolumn{2}{C{4.4cm}}{ N/A } \n            & \\multicolumn{2}{C{3.8cm}}{LSTM}\n            &  \n            \\begin{tabular}[t]{p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}p{0cm}p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}}\n            ACC:&N/A\\%&&PRE:&0.98\\\\\n            REC:&1.00&&$F_{1}$:&0.98\\\\\n            FPR:&0.98&&FNR:&0.02\\\\\n            \\end{tabular} \\\\  \n        \\cmidrule[1pt]{2-8}\n        & {\\multirow{2}{*}{CFG-CNN }} & \\multicolumn{3}{C{5cm}}{Phase I} & \\multicolumn{3}{C{5cm}}{Phase II}  \\\\\n        \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n        & & \\multicolumn{3}{C{5.7cm}}{The raw data is instruction level control-flow graph constructed from program assembly code by an algorithm proposed by the authors. While in the CFG, one vertex corresponds to one instruction and one directed edge corresponds to an execution path from one instruction to another. The program sets for experiments are obtained from popular programming contest CodeChief.\n        } \n            & \\multicolumn{3}{C{5.7cm}}{Since each vertex of the CFG represents an instruction with complex information that could be viewed from different aspects, including instruction name, type, operands etc., a vertex is represented as the sum of a set of real valued vectors, corresponding to the number of views (e.g. \\textit{addq 32,\\%rsp} is converted to linear combination of randomly assigned vectors of \\textit{addq value, reg}). The CFG is then sliced by a set of fixed size windows sliding through the entire graph to extract local features on different levels.} \\\\\n        \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n        & & \\multicolumn{2}{C{4.4cm}}{Phase III} & \\multicolumn{2}{C{3.8cm}}{Phase IV} & \\multicolumn{2}{C{2.8cm}}{Evaluation}  \\\\\n        \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n        & & \\multicolumn{2}{C{4.4cm}}{ N/A } \n            & \\multicolumn{2}{C{3.8cm}}{DGCNN with different numbers of views and with or without operands}\n            &  \n            \\begin{tabular}[t]{p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}p{0cm}p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}}\n            ACC:&84.1\\%&&PRE:&N/A\\\\\n            REC:&N/A&&$F_{1}$:&N/A\\\\\n            FPR:&N/A&&FNR:&N/A\\\\\n            \\end{tabular} \\\\                                                                                  \n   \\cmidrule[0.8pt]{1-8}\n    {\\multirow{5}{1.5cm}{Defending Network Attacks }} \n            & {\\multirow{2}{*}{50b(yte)-CNN}} & \\multicolumn{3}{C{5cm}}{Phase I} & \\multicolumn{3}{C{5cm}}{Phase II}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    && \\multicolumn{3}{C{5.7cm}}{Open dataset UNSW-NB15 is used. First, tcpdump tool is utilised to capture 100 GB of the raw traffic (i.e. PCAP files) containing benign activities and 9 types of attacks. The Argus, Bro-IDS (now called Zeek) analysis tools are then used and twelve algorithms are developed to generate totally 49 features with the class label. In the end, the total number of data samples is 2,540,044 which are stored in CSV files.  } \n     & \\multicolumn{3}{C{5.7cm}}{The first 50 bytes of each network traffic flow are picked out and each is directly used as one feature input to the neural network. } \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{Phase III} & \\multicolumn{2}{C{3.8cm}}{Phase IV} & \\multicolumn{2}{C{2.8cm}}{Evaluation}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{ N/A } \n      & \\multicolumn{2}{C{3.8cm}}{ CNN with 2 hidden fully connected layers}\n      &  \n      \\begin{tabular}[t]{p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}p{0cm}p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}}\n      ACC:&N/A\\%&&PRE:&N/A\\\\\n      REC:&N/A&&$F_{1}$:&0.93\\\\\n      FPR:&N/A&&FNR:&N/A\\\\\n      \\end{tabular} \\\\    \n    \\cmidrule[1pt]{2-8}\n    & {\\multirow{2}{*}{PCCN}} & \\multicolumn{3}{C{5cm}}{Phase I} & \\multicolumn{3}{C{5cm}}{Phase II}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{3}{C{5.7cm}}{Open dataset CICIDS2017, which contains benign and 14 types of attacks, is used. Background benign network traffics are generated by profiling the abstract behavior of human interactions. Raw data are provided as PCAP files, and the results of the network traffic analysis using CICFlowMeter are pvodided as CSV files. In the end the dataset contains 3,119,345 data samples and 83 features categorized into 15 classes (1 normal + 14 attacks).   } \n     & \\multicolumn{3}{C{5.7cm}}{Extract a total of 1,168,671 flow data, including 12 types of attack activities, from original dataset. Those flow data are then processed and visualized into grey-scale 2D graphs. The visualization method is not specified.} \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{Phase III} & \\multicolumn{2}{C{3.8cm}}{Phase IV} & \\multicolumn{2}{C{2.8cm}}{Evaluation}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{ N/A } \n      & \\multicolumn{2}{C{3.8cm}}{Parallel cross CNN.}\n      &  \n      \\begin{tabular}[t]{p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}p{0cm}p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}}\n      ACC:&N/A\\%&&PRE:&0.99\\\\\n      REC:&N/A&&$F_{1}$:&0.99\\\\\n      FPR:&N/A&&FNR:&N/A\\\\\n      \\end{tabular} \\\\    \n    \\cmidrule[0.8pt]{1-8}\n    {\\multirow{5}{1.5cm}{Malware Classification }} \n            & {\\multirow{2}{*}{Rosenberg}} & \\multicolumn{3}{C{5cm}}{Phase I} & \\multicolumn{3}{C{5cm}}{Phase II}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    && \\multicolumn{3}{C{5.7cm}}{The android dataset has the latest malware families and their variants, each with the same number of samples. The samples are labeled by VirusTotal. Then Cuckoo Sandbox is used to extract dynamic features (API calls) and static features (string). To avoid some anti-forensic sample, they applied YARA rule and removed sequences with less than 15 API calls. After preprocessing and balance the benign samples number, the dataset has 400,000 valid samples. } \n        & \\multicolumn{3}{C{5.7cm}}{Long sequences cause out of memory during training LSTM model. So they use sliding window with fixed size and pad shorter sequences with zeros. One-hot encoding is applied to API calls. For static features strings, they defined a vector of 20,000 Boolean values indicating the most frequent Strings in the entire dataset. If the sample contain one string, the corresponding value in the vector will be assigned as 1, otherwise, 0.} \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{Phase III} & \\multicolumn{2}{C{3.8cm}}{Phase IV} & \\multicolumn{2}{C{2.8cm}}{Evaluation}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{ N/A } \n        & \\multicolumn{2}{C{3.8cm}}{ They used RNN, BRNN, LSTM, Deep LSTM, BLSTM, Deep BLSTM, GRU, bi-directional GRU, Fully-connected DNN, 1D CNN in their experiments}\n        &  \n        \\begin{tabular}[t]{p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}p{0cm}p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}}\n        ACC:&98.3\\%&&PRE:&N/A\\\\\n        REC:&N/A&&$F_{1}$:&N/A\\\\\n        FPR:&N/A&&FNR:&N/A\\\\\n        \\end{tabular} \\\\    \n    \\cmidrule[1pt]{2-8}\n    & {\\multirow{2}{*}{DeLaRosa}} & \\multicolumn{3}{C{5cm}}{Phase I} & \\multicolumn{3}{C{5cm}}{Phase II}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{3}{C{5.7cm}}{The windows dataset is from Reversing Labs including XP, 7, 8, and 10 for both 32-bit and 64-bit architectures and gathered over a span of twelve years (2006-2018). They selected nine malware families in their dataset and extracted static features in terms of bytes, basic, and assembly features.  } \n        & \\multicolumn{3}{C{5.7cm}}{For bytes-level features, they used a sliding window to get the histogram of the bytes and compute the associated entropy in a window; for basic features, they created a fixed-sized feature vector given either a list of ASCII strings, or extracted import and metadata information from the PE Header(Strings are hashed and calculate a histogram of these hashes by counting the occurrences of each value); for assembly features, the disassembled code generated by Radare2 can be parsed and transformed into graph-like data structures such as call graphs, control flow graph, and instruction flow graph. } \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{Phase III} & \\multicolumn{2}{C{3.8cm}}{Phase IV} & \\multicolumn{2}{C{2.8cm}}{Evaluation} \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{ N/A} \n        & \\multicolumn{2}{C{3.8cm}}{N/A}\n        &  \n        \\begin{tabular}[t]{p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}p{0cm}p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}}\n        ACC:&90.1\\%&&PRE:&N/A\\\\\n        REC:&N/A&&$F_{1}$:&N/A\\\\\n        FPR:&N/A&&FNR:&N/A\\\\\n        \\end{tabular} \\\\    \n    \\cmidrule[0.8pt]{1-8}\n    {\\multirow{5}{1.5cm}{System Event Based Anomaly Detection\\\\}} \n            & {\\multirow{2}{*}{DeepLog }} & \\multicolumn{3}{C{5cm}}{Phase I} & \\multicolumn{3}{C{5cm}}{Phase II}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    && \\multicolumn{3}{C{5.7cm}}{More than 24 million raw log entries with the size of 2412 MB are recorded from the 203-node HDFS. Over 11 million log entries with 29 types are parsed, which are further grouped to 575,061 sessions according to block identifier. These sessions are manually labeled as normal and abnormal by HDFS experts. Finally, the constructed dataset HDFS 575,061 sessions of logs in the dataset, among which 16,838 sessions were labeled as anomalous } \n        & \\multicolumn{3}{C{5.7cm}}{The raw log entries are parsed to different log type using Spell which is based a longest common subsequence. There are total 29 log types in HDFS dataset} \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{Phase III} & \\multicolumn{2}{C{3.8cm}}{Phase IV} & \\multicolumn{2}{C{2.8cm}}{Evaluation}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{DeepLog directly utilized one-hot vector to represent 29 log key without represent learning} \n        & \\multicolumn{2}{C{3.8cm}}{ A stacked LSTM with two hidden LSTM layers.}\n        &  \n        \\begin{tabular}[t]{p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}p{0cm}p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}}\n        ACC:&N/A\\%&&PRE:&0.95\\\\\n        REC:&0.96&&$F_{1}$:&0.96\\\\\n        FPR:&N/A&&FNR:&N/A\\\\\n        \\end{tabular} \\\\    \n    \\cmidrule[1pt]{2-8}\n    & {\\multirow{2}{*}{LogAnom }} & \\multicolumn{3}{C{5cm}}{Phase I} & \\multicolumn{3}{C{5cm}}{Phase II}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{3}{C{5.7cm}}{LogAnom also used HDFS dataset, which is same as DeepLog. } \n        & \\multicolumn{3}{C{5.7cm}}{The raw log entries are parsed to different log templates using FT-Tree  according the frequent combinations of log words. There are total 29 log templates in HDFS dataset} \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{Phase III} & \\multicolumn{2}{C{3.8cm}}{Phase IV} & \\multicolumn{2}{C{2.8cm}}{Evaluation}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{ LogAnom employed Word2Vec to represent the extracted log templates with more semantic information } \n        & \\multicolumn{2}{C{3.8cm}}{Two LSTM  layers with 128 neurons}\n        &  \n        \\begin{tabular}[t]{p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}p{0cm}p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}}\n        ACC:&N/A\\%&&PRE:&0.97\\\\\n        REC:&0.94&&$F_{1}$:&0.96\\\\\n        FPR:&N/A&&FNR:&N/A\\\\\n        \\end{tabular} \\\\       \n    \\cmidrule[0.8pt]{1-8}\n    {\\multirow{5}{1.5cm}{Memory Forensics }} \n            & {\\multirow{2}{*}{DeepMem}} & \\multicolumn{3}{C{5cm}}{Phase I} & \\multicolumn{3}{C{5cm}}{Phase II}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    && \\multicolumn{3}{C{5.7cm}}{400 memory dumps are collected on Windows 7 x86 SP1 virtual machine with simulating various random user actions and forcing the OS to randomly allocate objects. The size of each dump is 1GB. } \n        & \\multicolumn{3}{C{5.7cm}}{Construct memory graph from memory dumps, where each node represents a segment between two pointers and an edge is created if two nodes are neighbor} \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{Phase III} & \\multicolumn{2}{C{3.8cm}}{Phase IV} & \\multicolumn{2}{C{2.8cm}}{Evaluation}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{Each node is represented by a latent numeric vector from the embedding network.} \n        & \\multicolumn{2}{C{3.8cm}}{ Fully Connected Network (FCN) with ReLU layer.}\n        &  \n        \\begin{tabular}[t]{p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}p{0cm}p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}}\n        ACC:&N/A\\%&&PRE:&0.99\\\\\n        REC:&0.99&&$F_{1}$:&0.99\\\\\n        FPR:&0.01&&FNR:&0.01\\\\\n        \\end{tabular} \\\\    \n    \\cmidrule[1pt]{2-8}\n    & {\\multirow{2}{*}{MDMF~}} & \\multicolumn{3}{C{5cm}}{Phase I} & \\multicolumn{3}{C{5cm}}{Phase II}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{3}{C{5.7cm}}{Create a dataset of benign host memory snapshots running normal, non-compromised software, including software that executes in many of the malicious snapshots. The benign snapshot is extracted from memory after ample time has passed for the chosen programs to open. By generating samples in parallel to the separate malicious environment, the benign memory snapshot dataset created. } \n        & \\multicolumn{3}{C{5.7cm}}{Various representation for the memory snapshots including byte sequence and image, without relying on domain-knowledge of the OS.} \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{Phase III} & \\multicolumn{2}{C{3.8cm}}{Phase IV} & \\multicolumn{2}{C{2.8cm}}{Evaluation}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{ N/A} \n        & \\multicolumn{2}{C{3.8cm}}{Recurrent Neural Network with LSTM cells and Convolutional Neural Network composed of multiple layers, including pooling and fully connected layers. for image data}\n        &  \n        \\begin{tabular}[t]{p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}p{0cm}p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}}\n        ACC:&98.0\\%&&PRE:&N/A\\\\\n        REC:&N/A&&$F_{1}$:&N/A\\\\\n        FPR:&N/A&&FNR:&N/A\\\\\n        \\end{tabular} \\\\    \n    \\cmidrule[0.8pt]{1-8}\n    {\\multirow{5}{1.5cm}{Fuzzing }} \n            & {\\multirow{2}{*}{L-Fuzz}} & \\multicolumn{3}{C{5cm}}{Phase I} & \\multicolumn{3}{C{5cm}}{Phase II}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    && \\multicolumn{3}{C{5.7cm}}{The raw data are about 63,000 non-binary PDF objects, sliced in fix size, extracted from 534 PDF files that are provided by Windows fuzzing team and are previously used for prior extended fuzzing of Edge PDF parser.  } \n        & \\multicolumn{3}{C{5.7cm}}{N/A} \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{Phase III} & \\multicolumn{2}{C{3.8cm}}{Phase IV} & \\multicolumn{2}{C{2.8cm}}{Evaluation}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{N/A} \n        & \\multicolumn{2}{C{3.8cm}}{ Char-RNN}\n        &  \n        \\begin{tabular}[t]{p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}p{0cm}p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}}\n        ACC:&N/A\\%&&PRE:&N/A\\\\\n        REC:&N/A&&$F_{1}$:&0.93\\\\\n        FPR:&N/A&&FNR:&N/A\\\\\n        \\end{tabular} \\\\    \n    \\cmidrule[1pt]{2-8}\n    & {\\multirow{2}{*}{NEUZZ }} & \\multicolumn{3}{C{5cm}}{Phase I} & \\multicolumn{3}{C{5cm}}{Phase II}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{3}{C{5.7cm}}{For each program tested, the raw data is collected by running AFL-2.52b on a single core machine for one hour. The training data are byte level input files generated by AFL, and the labels are bitmaps corresponding to input files. For experiments, NEUZZ is implemented on 10 real-world programs, the LAVA-M bug dataset, and the CGC dataset.\n    } \n        & \\multicolumn{3}{C{5.7cm}}{N/A } \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{Phase III} & \\multicolumn{2}{C{3.8cm}}{Phase IV} & \\multicolumn{2}{C{2.8cm}}{Evaluation}  \\\\\n    \\cmidrule[\\lightrulewidth](lr){3-8}\\addlinespace[0ex]\n    & & \\multicolumn{2}{C{4.4cm}}{ N/A} \n        & \\multicolumn{2}{C{3.8cm}}{NN }\n        &  \n        \\begin{tabular}[t]{p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}p{0cm}p{0.2cm}<{\\centering}p{0.1cm}<{\\centering}}\n        ACC:&N/A\\%&&PRE:&N/A\\\\\n        REC:&N/A&&$F_{1}$:&0.93\\\\\n        FPR:&N/A&&FNR:&N/A\\\\\n        \\end{tabular} \\\\                                                                                                                          \n    \\bottomrule\n    \\bottomrule\n    \\end{longtable}\n    \\begin{TableNotes}\n        \\item [1] Deep Learning metrics are often not available in fuzzing papers. Typical fuzzing metrics used for evaluations are: code coverage, pass rate and bugs.\n    \\end{TableNotes}\n\\end{ThreePartTable}\n\\end{center}", "cites": [8016, 8017, 8014, 3859, 8019, 3947, 8015, 8018], "cite_extract_rate": 0.17391304347826086, "origin_cites_number": 46, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section attempts to organize cited works into a four-phase workflow framework, showing some synthesis by mapping methods to specific phases. However, it mainly describes each paper's approach and evaluation without deep integration, comparison, or critique of methods. There is limited abstraction beyond the framework, with the analysis remaining focused on individual papers rather than broader trends or principles."}}
{"id": "b3244be8-d71e-4845-98c5-8fb6977b0e43", "title": "Introduction", "level": "subsection", "subsections": [], "parent_id": "7a06b173-0676-41cd-b4b7-f793958a3ed0", "prefix_titles": [["title", "Using Deep Learning to Solve Computer Security Challenges: A Survey"], ["section", "A closer look at applications of Deep Learning in solving security-oriented program analysis challenges"], ["subsection", "Introduction"]], "content": "Recent years, security-oriented program analysis is widely used in software security. For example, symbolic execution and taint analysis are used to discover, detect and analyze vulnerabilities in programs. Control flow analysis, data flow analysis and pointer/alias analysis are important components \nwhen enforcing many secure strategies, \nsuch as control flow integrity, data flow integrity and doling dangling pointer elimination.\nReverse engineering was used by defenders and attackers to understand the logic of a program without source code.\n{\\color{myblack} In the security-oriented program analysis, there are many open problems, such as precise pointer/alias analysis, \naccurate and complete reversing engineer, complex constraint solving, program de-obfuscation, and so on. \nSome problems have theoretically proven to be NP-hard, and others still need lots of human effort to solve. \nEither of them needs a lot of domain knowledge and experience from expert to develop better solutions. \nEssentially speaking, the main challenges when solving them through traditional approaches \nare due to the sophisticated rules between the features and labels, which may change in different contexts. \nTherefore, on the one hand, it will take a large quantity of human effort to develop rules to solve the problems, \non the other hand, even the most experienced expert cannot guarantee completeness. \nFortunately, the deep learning method is skillful to find relations between features and labels if given a large amount of training data. \nIt can quickly and comprehensively find all the relations if the training samples are representative and effectively encoded.}\nIn this section, we will review the very recent four representative works that use Deep Learning for security-oriented program analysis. We observed that they focused on different goals. Shin, et al. designed a model~ to identify the function boundary. EKLAVYA~ was developed to learn the function type. Gemini~ was proposed to detect similarity among functions.  DEEPVSA~ was designed to learn memory region of an indirect addressing from the code sequence. Among these works, we select two representative works~ and then, \nsummarize the analysis results in Table~\\ref{Table:Summary} in detail.\nOur review will be centered around three questions described in Section~\\ref{threequestions}. In the remaining of this section, we will first provide a set of observations, and then we provide the indications. Finally, we provide some general remarks.", "cites": [3947], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic introduction to security-oriented program analysis and lists four recent works using Deep Learning without substantial synthesis or comparison. It lacks critical evaluation of the cited papers and does not abstract key patterns or principles from the reviewed works. The narrative remains largely descriptive and factual."}}
{"id": "191a0f6f-88fb-4873-81a3-03aec397dafc", "title": "Key findings from a closer look", "level": "subsection", "subsections": [], "parent_id": "7a06b173-0676-41cd-b4b7-f793958a3ed0", "prefix_titles": [["title", "Using Deep Learning to Solve Computer Security Challenges: A Survey"], ["section", "A closer look at applications of Deep Learning in solving security-oriented program analysis challenges"], ["subsection", "Key findings from a closer look"]], "content": "From a close look at the very recent applications using Deep Learning for solving security-oriented program analysis challenges, we observed the followings: \n\\begin{itemize}[label={}]\n\\item  \\observation{} \\textit{All of the works in our survey used binary files as their raw data.}\nPhase~\\ref{phase2} in our survey had one similar and straightforward goal~\\textendash~extracting code sequences from the binary. Difference among them was that the code sequence was extracted directly from the binary file when solving problems in static program analysis, \nwhile it was extracted from the program execution when solving problems in dynamic program analysis.\n\\item  \\observation{*} \\textit{Most data representation methods generally took into account the domain knowledge.}\nMost data representation methods generally took into the domain knowledge, i.e., what kind of information they wanted to reserve when processing their data. Note that the feature selection has a wide influence on Phase~\\ref{phase2} and Phase~\\ref{phase3}, for example, embedding granularities, representation learning methods. Gemini~ selected function level feature and other works in our survey selected instruction level feature. To be specifically, all the works except Gemini~ vectorized code sequence on instruction level.\n\\item  \\observation{} \n\\textit{To better support data representation for high performance, some works adopted representation learning.}\nFor instance, DEEPVSA~ employed a representation learning method, i.e., bi-directional LSTM, to learn data dependency within instructions. EKLAVYA~ adopted representation learning method, i.e., word2vec technique, to extract inter-instruciton information. It is worth noting that Gemini~ adopts the Structure2vec embedding network in its siamese architecture in Phase~\\ref{phase4} (see details in Observation~3.\\ref{obs:bi}). The Structure2vec embedding network learned information from an attributed control flow graph.\n\\item  \\observation{} \\textit{According to our taxonomy, most works in our survey were classified into class 4.}\nTo compare the Phase~\\ref{phase3}, we introduced a \nclassification tree with three layers as shown in Figure~\\ref{fig:dec} to group different works into four categories. The decision tree grouped our surveyed works into four classes according to whether they considered representation learning or not, whether they adopted representation learning or not, and whether they compared their methods with others', respectively, when designing their framework. According to our taxonomy, EKLAVYA~, DEEPVSA~ were grouped into class 4 shown in Figure~\\ref{fig:dec}. Also, Gemini's work~ and Shin, et al.'s work~ belonged to class 1 and class 2 shown in Figure~\\ref{fig:dec}, respectively.\n\\item  \\observation{} \\label{obs:reason} \\textit{All the works in our survey explain why they adopted or did not adopt one of representation learning algorithms.}\nTwo works in our survey adopted representation learning for different reasons: to enhance model's ability of generalization~; and to learn the dependency within instructions~.\nIt is worth noting that Shin, et al. did not adopt representation learning because they wanted to preserve the ``attractive'' features of neural networks over other machine learning methods~\\textendash~simplicity. As they stated, ``first, neural networks can learn directly from the original representation with minimal preprocessing (or ``feature engineering”) needed.\" and ``second, neural networks can learn end-to-end, where each of its constituent stages are trained simultaneously in order to best solve the end goal.\" Although Gemini~ did not adopt representation learning when processing their raw data, the Deep Learning models in siamese structure consisted of two graph embedding networks and one cosine function.\n\\item  \\observation{*} \\textit{The analysis results showed that a suitable representation learning method could improve accuracy of Deep Learning models.}\nDEEPVSA~ designed a series of experiments to evaluate the effectiveness of its representative method. By combining with the domain knowledge, EKLAVYA~ employed t-SNE plots and analogical reasoning to explain the effectiveness of their representation learning method in an intuitive way.\n\\item  \\observation{*} \\label{obs:bi} \\textit{Various Phase IV methods were used.}\nIn Phase~\\ref{phase4}, Gemini~ adopted siamese architecture model which consisted of two Structure2vec embedding networks and one cosine function. \nThe siamese architecture took two functions as its input, and produced the similarity score as the output. The other three works~ adopted bi-directional RNN, RNN, bi-directional LSTM respectively. Shin, et al. adopted bi-directional RNN because they wanted to combine both the past and the future information in making a prediction for the present instruction~. DEEPVSA~ adopted bi-directional RNN to enable their model to infer memory regions in both forward and backward ways. \n\\end{itemize} \nThe above observations seem to indicate the following indications: \n\\begin{itemize}[label={}]\n\\item \\indication{} \\textit{Phase~\\ref{phase3} is not always necessary.}\nNot all authors regard representation learning as a good choice\neven though some case experiments show that representation learning can improve the final results.\nThey value more the simplicity of Deep Learning methods and suppose that the adoption of representation learning weakens the simplicity of Deep Learning methods.\n\\item \\indication{}\n\\textit{Even though the ultimate objective of Phase~\\ref{phase3} in the four surveyed works is to train a model with better accuracy, \nthey have different specific motivations as described in Observation~3.\\ref{obs:reason}.}\nWhen authors choose representation learning, \nthey usually try to convince people the effectiveness of their choice by \nempirical or theoretical analysis. \n\\item \\indication{*} \n\\textit{3.\\ref{obs:bi} indicates that authors usually refer to the domain knowledge when designing the architecture of Deep Learning model.}\nFor instance, the works we reviewed commonly adopt bi-directional RNN when their prediction partly based on future information in data sequence.\n\\end{itemize} \n{\\color{myblack}", "cites": [3947], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple cited works by identifying commonalities and differences in data representation and model architecture. It provides some level of abstraction by grouping works into classes based on their design choices and notes trends like the use of bi-directional RNNs in specific contexts. However, the critical analysis is limited to general observations about motivations and simplicity rather than deeper evaluation of methodological strengths or weaknesses."}}
{"id": "ef251879-f266-4be9-8fa8-01fd19ef0e47", "title": "Introduction", "level": "subsection", "subsections": [], "parent_id": "2b839e56-71e1-4fc1-8d17-ba5b1a399e32", "prefix_titles": [["title", "Using Deep Learning to Solve Computer Security Challenges: A Survey"], ["section", "A closer look at applications of Deep Learning in defending ROP attacks"], ["subsection", "Introduction"]], "content": "Return-oriented programming (ROP) attack is one of the most dangerous code reuse attacks, which allows the attackers to launch control-flow hijacking attack without injecting any malicious code. Rather, It leverages particular instruction sequences (called ``gadgets'') widely existing in the program space to achieve Turing-complete attacks~. Gadgets are instruction sequences that end with a \\textit{RET} instruction. \nTherefore, they can be chained together by specifying the return addresses on program stack.\nMany traditional techniques could be used to detect ROP attacks, such as control-flow integrity (CFI~), \nbut many of them either have low detection rate or have high runtime overhead. \nROP payloads do not contain any codes. \nIn other words, analyzing ROP payload without the context of the program’s memory dump is meaningless. \nThus, the most popular way of detecting and preventing ROP attacks is control-flow integrity. \n{\\color{myblack} The challenge after acquiring the instruction sequences is that it is hard to recognize whether the control flow is normal. \nTraditional methods use the control flow graph (CFG) to identify whether the control flow is normal, \nbut attackers can design the instruction sequences which follow the normal control flow defined by the CFG.\nIn essence, it is very hard to design a CFG to exclude every single possible combination of instructions that can be used to launch ROP attacks. \nTherefore, using data-driven methods could help eliminate such problems.}\nIn this section, we will review the very recent three representative works that use Deep Learning for defending ROP attacks: ROPNN , HeNet  and DeepCheck . \nROPNN  aims to detect ROP attacks, HeNet  aims to detect malware using CFI, and DeepCheck  aims at detecting all kinds of code reuse attacks.\n{\\color{myblack} Specifically, ROPNN is to protect one single program at a time, and its training data are generated from real-world programs along with their execution. \nFirstly, it generates its benign and malicious data by ``chaining-up\" the normally executed instruction sequences and\n``chaining-up'' gadgets with the help of gadgets generation tool, respectively, \nafter the memory dumps of programs are created.\nEach data sample is byte-level instruction sequence labeled as ``benign'' or ``malicious''. \nSecondly, ROPNN will be trained using both malicious and benign data.\nThirdly, the trained model is deployed to a target machine. After the protected program started, \nthe executed instruction sequences will be traced and fed into the trained model, \nthe protected program will be terminated once the model found the instruction sequences are likely to be malicious.\nHeNet is also proposed to protect a single program.\nIts malicious data and benign data are generated by collecting trace data through Intel PT from malware and normal software, respectively.\nBesides, HeNet preprocesses its dataset and shape each data sample in the format of image,\nso that they could implement transfer learning from a model pre-trained on ImageNet. \nThen, HeNet is trained and deployed on machines with features of Intel PT to collect and classify the program's execution trace online. \nThe training data for DeepCheck are acquired from CFGs, \nwhich are constructed by dissembling the programs and using the information from Intel PT. \nAfter the CFG for a protected program is constructed, \nauthors sample benign instruction sequences by chaining up basic blocks that are connected by edges, \nand sample malicious instruction sequences by chaining up those that are not connected by edges.\nAlthough a CFG is needed during training, there is no need to construct CFG after the training phase. \nAfter deployed, instruction sequences will be constructed by leveraging Intel PT on the protected program. \nThen the trained model will classify whether the instruction sequences are malicious or benign.} \nWe observed that none of the works considered Phase~\\ref{phase3}, so all of them belong to class 1 according to our taxonomy as shown in Figure~\\ref{fig:dec}. The analysis results of ROPNN  and HeNet  are shown in Table \\ref{Table:Summary}. Also, we observed that three works had different goals. \nOur review will be centered around three questions described in Section~\\ref{threequestions}. In the remaining of this section, we will first provide a set of observations, and then we provide the indications. Finally, we provide some general remarks.", "cites": [8016, 8018], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of three Deep Learning-based approaches (ROPNN, HeNet, and DeepCheck) for defending against ROP attacks. It integrates some information from the cited papers by explaining their general methodologies but lacks deeper synthesis or a novel framework. There is minimal critical evaluation or abstraction beyond the specific systems described."}}
{"id": "32365617-f623-49b8-900b-d6d6aa7866c0", "title": "Key findings from a closer look", "level": "subsection", "subsections": [], "parent_id": "2b839e56-71e1-4fc1-8d17-ba5b1a399e32", "prefix_titles": [["title", "Using Deep Learning to Solve Computer Security Challenges: A Survey"], ["section", "A closer look at applications of Deep Learning in defending ROP attacks"], ["subsection", "Key findings from a closer look"]], "content": "From a close look at the very recent applications using Deep Learning for defending return-oriented programming attacks, we observed the followings: \n\\begin{itemize}[label={}]\n\\item  \\observation{}\\label{rop:obs1} \\textit{All the works in this survey focused on data generation and acquisition.}\nIn ROPNN , both malicious samples (gadget chains) were generated using an automated gadget generator (i.e. ROPGadget~) and a CPU emulator (i.e. Unicorn~). ROPGadget was used to extract instruction sequences that could be used as gadgets from a program, and Unicorn was used to validate the instruction sequences.\nCorresponding benign sample (gadget-chain-like instruction sequences) were generated by disassembling a set of programs.\nIn DeepCheck~ refers to the key idea of control-flow integrity~. It generates program's run-time control flow through new feature of Intel CPU (Intel Processor Tracing), then compares the run-time control flow with the program's control-flow graph (CFG) that generates through static analysis. Benign instruction sequences are that with in the program's CFG, and vice versa. In HeNet~, program's execution trace was extracted using the similar way as DeepCheck. Then, each byte was transformed into a pixel with an intensity between 0-255. Known malware samples and benign software samples were used to generate malicious data benign data, respectively.\n\\item  \\observation{}\\label{rop:obs2} \\textit{None of the ROP works in this survey deployed Phase~\\ref{phase3}.}\nBoth ROPNN and DeepCheck used binary instruction sequences for training. In ROPNN, one byte was used as the very basic element for data pre-processing. Bytes were formed into one-hot matrices and flattened for 1-dimensional convolutional layer. In DeepCheck , half-byte was used as the basic unit. Each half-byte (4 bits) was transformed to decimal form ranging from 0-15 as the basic element of the input vector, then was fed into a fully-connected input layer. On the other hand, HeNet  used different kinds of data. By the time this survey has been drafted, the source code of HeNet was not available to public and thus, the details of the data pre-processing was not be investigated. However, it is still clear that HeNet used binary branch information collected from Intel PT rather than binary instructions. In HeNet, each byte was converted to one decimal number ranging from 0 to 255. Byte sequences was sliced and formed into image sequences (each pixel represented one byte) for a fully-connected input layer.\n\\item  \\observation{}\\label{rop:obs3} \\textit{Fully-connected neural network was widely used.}\nOnly ROPNN  used 1-dimensional convolutional neural network (CNN) when extracting features. Both HeNet  and DeepCheck used fully-connected neural network (FCN). None of the works used recurrent neural network (RNN) and the variants.\n\\end{itemize}\nThe above observations seem to indicate the following indications:\n\\begin{itemize}\n\\item \\indication{} \\textit{It seems like that one of the most important factors in ROP problem is feature selection and data generation.} \nAll three works use very different methods to collect/generate data, and all the authors provide very strong evidences and/or arguments to justify their approaches. ROPNN~ was trained by the malicious and benign instruction sequences. However, there is no clear boundary between benign instruction sequences and malicious gadget chains. This weakness may impair the performance when applying ROPNN to real world ROP attacks. As oppose to ROPNN, DeepCheck  utilizes CFG to generate training basic-block sequences. However, since the malicious basic-block sequences are generated by randomly connecting nodes without edges, it is not guaranteed that all the malicious basic-blocks are executable. HeNet  generates their training data from malware. Technically, HeNet could be used to detect any binary exploits, but their experiment focuses on ROP attack and achieves 100\\% accuracy. This shows that the source of data in ROP problem does not need to be related to ROP attacks to produce very impressive results.\n\\item \\indication{} \\textit{Representation learning seems not critical when solving ROP problems using Deep Learning.} \nMinimal process on data in binary form seems to be enough to transform the data into a representation that is suitable for neural networks. Certainly, it is also possible to represent the binary instructions at a higher level, such as opcodes, or use embedding learning. However, as stated in , it appears that the performance will not change much by doing so. The only benefit of representing input data to a higher level is to reduce irrelevant information, but it seems like neural network by itself is good enough at extracting features.\n\\item \\indication{} \\textit{Different Neural network architecture does not have much influence on the effectiveness of defending ROP attacks.}\nBoth HeNet  and DeepCheck  utilizes standard DNN and achieved comparable results on ROP problems. One can infer that the input data can be easily processed by neural networks, and the features can be easily detected after proper pre-process. \n\\end{itemize} \nIt is not surprising that researchers are not very interested in representation learning for ROP problems as stated in Observation~4.\\ref{rop:obs1}. \nSince ROP attack is focus on the gadget chains, it is straightforward for the researcher to choose the gadgets as their training data directly.\nIt is easy to map the data into numerical representation with minimal processing. An example is that one can map binary executable to hexadecimal ASCII representation, which could be a good representation for neural network.\nInstead, researchers focus more in data acquisition and generation. In ROP problems, the amount of data is very limited. Unlike malware and logs, ROP payloads normally only contain addresses rather than codes, which do not contain any information without providing the instructions in corresponding addresses. It is thus meaningless to collect all the payloads. At the best of our knowledge, all the previous works use pick instruction sequences rather than payloads as their training data, even though they are hard to collect.\n{\\color{myblack}", "cites": [8016, 8018], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key details from ROPNN, DeepCheck, and HeNet to identify commonalities and differences in data generation and model architecture. It provides critical analysis by pointing out limitations, such as ambiguity in distinguishing benign and malicious sequences in ROPNN and potential infeasibility of malicious sequences in DeepCheck. The abstraction level is strong, as it generalizes these findings into broader insights about the importance of data over representation learning and the effectiveness of simple neural architectures in the ROP defense domain."}}
{"id": "4fef8609-2a73-4dd1-bd78-9ca01992a913", "title": "Introduction", "level": "subsection", "subsections": [], "parent_id": "6ae1eea5-7d10-4735-bfd3-ef2d1286e877", "prefix_titles": [["title", "Using Deep Learning to Solve Computer Security Challenges: A Survey"], ["section", "A closer look at applications of Deep Learning in malware classification"], ["subsection", "Introduction"]], "content": "The goal of malware classification is to identify malicious behaviors in software with static and dynamic features like control-flow graph and system API calls. \nMalware and benign programs can be collected from open datasets and online websites. \n{\\color{myblack}\nBoth the industry and the academic communities have provided approaches to detect malware with static and dynamic analyses. \nTraditional methods such as behavior-based signatures, dynamic taint tracking, and static data flow analysis require experts to manually investigate unknown files. \nHowever, those hand-crafted signatures are not sufficiently effective because attackers can rewrite and reorder the malware. \nFortunately, neural networks can automatically detect large-scale malware variants with superior classification accuracy.\n}\nIn this section, we will review the very recent twelve representative works that use Deep Learning for malware classification .  selects three different kinds of static features to classify malware.  also use static features from the PE files to classify programs.  extracts behavioral feature images using RNN to represent the behaviors of original programs. transforms malicious behaviors using representative learning without neural network.  explores RNN model with the API calls sequences as programs' features.   skip Phase~\\ref{phase2} by directly transforming the binary file to image to classify the file.  applies dynamic features to analyze malicious features.  combines static features and dynamic features to represent programs' features. Among these works, we select two representative works  and identify four phases in their works shown as Table~\\ref{Table:Summary}. \nOur review will be centered around three questions described in Section~\\ref{threequestions}. In the remaining of this section, we will first provide a set of observations, and then we provide the indications. Finally, we provide some general remarks.", "cites": [3859], "cite_extract_rate": 0.08333333333333333, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic overview of malware classification and lists several representative works using Deep Learning, but lacks a coherent synthesis of ideas. It does not critically evaluate the cited papers or identify broader trends or principles, instead focusing on a descriptive listing of methods and features used. The mention of four phases and three questions suggests some structure, but without elaboration or analysis, the insight remains minimal."}}
{"id": "61b44751-fc07-481a-9c8d-113f52966cad", "title": "Key findings from a closer look", "level": "subsection", "subsections": [], "parent_id": "6ae1eea5-7d10-4735-bfd3-ef2d1286e877", "prefix_titles": [["title", "Using Deep Learning to Solve Computer Security Challenges: A Survey"], ["section", "A closer look at applications of Deep Learning in malware classification"], ["subsection", "Key findings from a closer look"]], "content": "From a close look at the very recent applications using Deep Learning for solving malware classification challenges, we observed the followings: \n\\begin{itemize}[label={}]\n\\item \\observation{} \\textit{Features selected in malware classification were grouped into three categories: static features, dynamic features, and hybrid features.}\nTypical static features include metadata, PE import Features, Byte/Entorpy, String, and Assembly Opcode Features derived from the PE files  . De LaRosa, Kilgallon, et al. took three kinds of static features: byte-level, basic-level ( strings in the file, the metadata table, and the import table of the PE header), and assembly features-level. Some works directly considered binary code as static features.\nDifferent from static features, dynamic features were extracted by executing the files to retrieve their behaviors during execution. The behaviors of programs, including the API function calls, their parameters, files created or deleted, websites and ports accessed, etc, were recorded by a sandbox as dynamic features. The process behaviors including operation name and their result codes were extracted . The process memory, tri-grams of system API calls and one corresponding input parameter were chosen as dynamic features. An API calls sequence for an APK file was another representation of dynamic features~. \nStatic features and dynamic features were combined as hybrid features~. For static features, Xu and others in~ used permissions, networks, calls, and providers, etc. For dynamic features, they used system call sequences.\n\\item  \\observation{} \\textit{In most works, Phase~\\ref{phase2} was inevitable because extracted features needed to be vertorized for Deep Learning models.}\nOne-hot encoding approach was frequently used to vectorize features. Bag-of-words (BoW) and \\textit{n}-gram were also considered to represent features . Some works brought the concepts of word frequency in NLP to convert the sandbox file to fixed-size inputs. Hashing features into a fixed vector was used as an effective method to represent features. Bytes histogram using the bytes analysis and bytes-entropy histogram with a sliding window method were considered~. In , De La Rosa and others embeded strings by hashing the ASCII strings to a fixed-size feature vector. For assembly features, they extracted four different levels of granularity: operation level (instruction-flow-graph), block level (control-flow-graph), function level (call-graph), and global level (graphs summarized). bigram, trigram and four-gram vectors and \\textit{n}-gram graph were used for the hybrid features . \n\\item  \\observation{} \\textit{Most Phase~\\ref{phase3} methods were classified into class 1.}\nFollowing the classification tree shown in Figure~\\ref{fig:dec}, most works were classified into class 1 shown in Figure~\\ref{fig:dec} except two works, which belonged to class 3 shown in Figure~\\ref{fig:dec}. To reduce the input dimension, Dahl et al. performed feature selection using mutual information and random projection. Tobiyama et al. generated behavioral feature images using  RNN.\n\\item {\\bf Observation 4:} \\textit{After extracting features, two kinds of neural network architectures, i.e., one single neural network and multiple neural networks with a combined loss function, were used.}\nHierarchical structures, like convolutional layers, fully connected layers and classification layers, were used to classify programs . A deep stack of denoising autoencoders was also introduced to learn programs' behaviors . De La Rosa and others trained three different models with different features to compare which static features are relevant for the classification model. Some works investigated LSTM models for sequential features~. \nTwo networks with different features as inputs were used for malware classification by combining their outputs with a dropout layer and an output layer. In , one network transformed PE Metadata and import features using feedforward neurons, another one leveraged convolutional network layers with opcode sequences. Lifan Xu et al. constructed a few networks and combined them using a two-level multiple kernel learning algorithm.\n\\end{itemize}\nThe above observations seem to indicate the following indications: \n\\begin{itemize}[label={}]\n\\item \\indication{} \\textit{Except two works transform binary into images, most works surveyed need to adapt methods to vectorize extracted features.} \nThe vectorization methods should not only keep syntactic and semantic information in features, but also consider the definition of the Deep Learning model. \n\\item \\indication{} \\textit{Only limited works have shown how to transform features using representation learning.} \nBecause some works assume the dynamic and static sequences, like API calls and instruction, and have similar syntactic and semantic structure as natural language, some representation learning techniques like word2vec may be useful in malware detection. In addition, for the control-flow graph, call graph and other graph representations, graph embedding is a potential method to transform those features.\n\\end{itemize} \n{\\color{myblack}", "cites": [3859], "cite_extract_rate": 0.08333333333333333, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple works on malware classification using Deep Learning, categorizing features and methods into static, dynamic, and hybrid approaches. It provides a basic level of integration but lacks deeper critical evaluation or nuanced comparison of these methods. The abstraction is moderate, as it identifies general patterns like the need for vectorization and the potential of representation learning, but does not offer a novel or overarching framework."}}
{"id": "97e6b700-3b7a-4ab7-98d6-2b09214a3c77", "title": "Introduction", "level": "subsection", "subsections": [], "parent_id": "6ae1eea5-7d10-4735-bfd3-ef2d1286e877", "prefix_titles": [["title", "Using Deep Learning to Solve Computer Security Challenges: A Survey"], ["section", "A closer look at applications of Deep Learning in malware classification"], ["subsection", "Introduction"]], "content": "System logs recorded significant events at various critical points, which can be used to debug the system's performance issues and failures. \nMoreover, log data are available in almost all computer systems and are a valuable resource for understanding system status. \n{\\color{myblack}\nThere are a few challenges in anomaly detection based on system logs. \nFirstly, the raw log data are unstructured, while their formats and semantics can vary significantly. \nSecondly, logs are produced by concurrently running tasks. \nSuch concurrency makes it hard to apply workflow-based anomaly detection methods. \nThirdly, logs contain rich information and complexity types, including text, real value, IP address, timestamp, and so on. \nThe contained information of each log is also varied. \nFinally, there are massive logs in every system. \nMoreover, each anomaly event usually incorporates a large number of logs generated in a long period.  \n}\nRecently, a large number of scholars employed deep learning techniques  \nto detect anomaly events in the system logs and diagnosis system failures. \nThe raw log data are unstructured, while their formats and semantics can vary significantly. \nTo detect the anomaly event, the raw log usually should be parsed to structure data, \nthe parsed data can be transformed into a representation that supports an effective deep learning model. \nFinally, the anomaly event can be detected by deep learning based classifier or predictor.\nIn this section, we will review the very recent six representative papers that use deep learning for system-event-based anomaly detection. DeepLog  utilizes LSTM to model the system log as a natural language sequence, which automatically learns log patterns from the normal event, and detects anomalies when log patterns deviate from the trained model. LogAnom employs Word2vec to extract the semantic and syntax information from log templates. Moreover, it uses sequential and quantitative features simultaneously. Desh  uses LSTM to predict node failures that occur in super computing systems from HPC logs. Andy Brown et al.  presented RNN language models augmented with attention for anomaly detection in system logs. LogRobust  uses FastText to represent semantic information of log events, which can identify and handle unstable log events and sequences. Christophe Bertero et al. map log word to a high dimensional metric space using Google's word2vec algorithm and take it as features to classify. Among these six papers, we select two representative works  and summarize the four phases of their approaches. We direct interested readers to Table~\\ref{Table:Summary} for a concise overview of these two works.\nOur review will be centered around three questions described in Section~\\ref{threequestions}. In the remaining of this section, we will first provide a set of observations, and then we provide the indications. Finally, we provide some general remarks.", "cites": [8019], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic overview of deep learning applications in system-event-based anomaly detection and lists several methods used by different papers. While there is some attempt to connect common themes (e.g., log parsing and representation), it lacks deeper synthesis, comparison, or critique of the cited works. The section is primarily descriptive, offering minimal analytical depth or abstraction beyond specific methods."}}
{"id": "2325c563-de2f-4499-a9b4-54f36b2f6906", "title": "Key findings from a closer look", "level": "subsection", "subsections": [], "parent_id": "6ae1eea5-7d10-4735-bfd3-ef2d1286e877", "prefix_titles": [["title", "Using Deep Learning to Solve Computer Security Challenges: A Survey"], ["section", "A closer look at applications of Deep Learning in malware classification"], ["subsection", "Key findings from a closer look"]], "content": "From a close look at the very recent applications using deep learning for solving security-event-based anomaly detection challenges, we observed the followings: \n\\begin{itemize}[label={}]\n\\item  \\observation{} \\textit{Most works of our surveyed papers evaluated their performance using public datasets.} \nBy the time we surveyed this paper, only two works in  used their private datasets. \n\\item  \\observation{} \\textit{Most works in this survey adopted Phase~\\ref{phase2} when parsing the raw log data.} \nAfter reviewing the six works proposed recently, we found that five works employed parsing technique, while only one work  did not.\n\\par DeepLog parsed the raw log to different log type using Spell which is based a longest common subsequence. Desh  parsed the raw log to constant message and variable component. Loganom parsed the raw log to different log templates using FT-Tree  according to the frequent combinations of log words. Andy Brown et al.  parsed the raw log into word and character tokenization. LogRobust extracted its log event by abstracting away the parameters in the message. Christophe Bertero et al. considered logs as regular text without parsing.\n\\item  \\observation{}\\label{log:obs:3} \\textit{Most works have considered and adopted Phase~\\ref{phase3}.} \nAmong these six works, only DeepLog represented the parsed data using the one-hot vector without learning. Moreover, Loganom compared their results with DeepLog. That is, DeepLog belongs to class 1 and Loganom belongs to class 4 in Figure \\ref{fig:dec}, while the other four works follow in class 3.\n\\par The four works  used word embedding techniques to represent the log data. Andy Brown et al.  employed attention vectors to represent the log messages. \n\\par DeepLog  employed the one-hot vector to represent the log type without learning. We have engaged an experiment replacing the one-hot vector with trained word embeddings. \n\\item  \\observation{} \\textit{Evaluation results were not compared using the same dataset.}\nDeepLog  employed the one-hot vector to represent the log type without learning, which employed Phase~\\ref{phase2} without Phase~\\ref{phase3}. However, Christophe Bertero et al. considered logs as regular text without parsing, and used Phase~\\ref{phase3} without Phase~\\ref{phase2}. The precision of the two methods is very high, which is greater than 95\\%. Unfortunately, the evaluations of the two methods used different datasets.\n\\item  \\observation{}\\label{log:obs:5} \\textit{Most works empolyed LSTM in Phase~\\ref{phase4}.}\nFive works including employed LSTM in the Phase~\\ref{phase4}, while Christophe Bertero et al. tried different classifiers including naive Bayes, neural networks and random forest.\n\\end{itemize}\nThe above observations seem to indicate the following indications: \n\\begin{itemize}[label={}]\n\\item \\indication{} \\textit{Phase~\\ref{phase2} has a positive effect on accuracy if being well-designed.}\nSince Christophe Bertero et al.~ considers logs as regular text without parsing, we can say that Phase~\\ref{phase2} is not required. However, we can find that most of the scholars employed parsing techniques to extract structure information and remove the useless noise.\n\\item \\indication{} \\textit{Most of the recent works use trained representation to represent parsed data.} \nAs shown in Table \\ref{tab:deeplog}, we can find Phase~\\ref{phase3} is very useful, which can improve detection accuracy.\n\\item \\indication{} \\textit{Phase~\\ref{phase2} and Phase~\\ref{phase3} cannot be skipped simultaneously.} \nBoth Phase~\\ref{phase2} and Phase~\\ref{phase3} are not required. However, all methods have employed Phase~\\ref{phase2} or Phase~\\ref{phase3}.\n\\item \\indication{} \\textit{Observation~8.\\ref{log:obs:3} indicates that the trained word embedding format can improve the anomaly detection accuracy as shown in Table~\\ref{tab:deeplog}.}\n\\begin{center}\n\\normalsize\n  \\begin{threeparttable}\n  \\caption{Comparison between word embedding and one-hot representation.}\n  \\label{tab:deeplog}\n\\begin{tabular}{p{3.5cm}||p{1.5cm}p{1.5cm}p{2cm}p{2cm}p{2cm}}  \n\\hline  \n\\hline  \nMethod  &  FP~\\tnote{1}  & FN~\\tnote{2}  & Precision & Recall& F1-measure\\\\ \n\\hline  \nWord Embedding~\\tnote{3} & 680 & 219 &   96.069\\%  & 98.699\\% & 97.366\\% \\\\ \n\\hline  \nOne-hot Vector~\\tnote{4} & 711 & 705 &   95.779\\%  & 95.813\\% & 95.796\\% \\\\ \n\\hline  \nDeepLog~\\tnote{5} & 833 & 619 &   95\\%  & 96\\% & 96\\% \\\\ \n\\hline \n\\hline   \n\\end{tabular} \n    \\begin{tablenotes}\n        \\item \\tnote{1}FP: false positive; \\tnote{2}FN: False negative;\\tnote{3}Word Embedding: Log keys are embedded by Continuous Bag of words;\\tnote{4} One-hot Vector: We reproduced the results according to DeepLog;\\tnote{5} DeepLog: Orignial results presented in the paper .\n    \\end{tablenotes}\n\\end{threeparttable}\n\\end{center}\n\\item \\indication{} \\textit{Observation~8.\\ref{log:obs:5} indicates that most of the works adopt LSTM to detect anomaly events.}\nWe can find that most of the works adopt LSTM to detect anomaly event, since log data can be considered as sequence and there can be lags of unknown duration between important events in a time series. LSTM has feedback connections, which can not only process single data points, but also entire sequences of data.\n\\end{itemize}\nAs our consideration, neither Phase~\\ref{phase2} nor Phase~\\ref{phase3} is required in system event-based anomaly detection. However, Phase~\\ref{phase2} can remove noise in raw data, and Phase~\\ref{phase3} can learn a proper representation of the data. Both Phase~\\ref{phase2} and Phase~\\ref{phase3} have a positive effect on anomaly detection accuracy. Since the event log is text data that we can't feed the raw log data into deep learning model directly, Phase~\\ref{phase2} and Phase~\\ref{phase3} can't be skipped simultaneously.\n{\\color{myblack}", "cites": [8019], "cite_extract_rate": 0.125, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple works on system-event-based anomaly detection, identifying common trends such as the use of Phase~2 parsing and Phase~3 representation learning. It provides some critical insights, such as the lack of evaluation on the same dataset and the reliance on LSTM for sequence modeling. While it offers analytical observations and attempts to generalize the role of parsing and representation, the abstraction is limited to the specific phases and methods discussed rather than broader, meta-level principles."}}
{"id": "896ebfd0-c54b-4625-834c-50838b67a7e3", "title": "Introduction", "level": "subsection", "subsections": [], "parent_id": "eb166b74-7da0-4abe-af7f-7cfe3d3d385a", "prefix_titles": [["title", "Using Deep Learning to Solve Computer Security Challenges: A Survey"], ["section", "A closer look at applications of Deep Learning in security-oriented fuzzing"], ["subsection", "Introduction"]], "content": "Fuzzing of software security is one of the state of art techniques that people use to detect software vulnerabilities. The goal of fuzzing is to find all the vulnerabilities exist in the program by testing as much program code as possible. Due to the nature of fuzzing, this technique works best on finding vulnerabilities in programs that take in input files, like PDF viewers or web browsers. \nA typical workflow of fuzzing can be concluded as: given several seed input files, the fuzzer will mutate or fuzz the seed inputs to get more input files, with the aim of expanding the overall code coverage of the target program as it executes the mutated files. \n{\\color{myblack} Although there have already been various popular fuzzers, fuzzing still cannot bypass its problem of sometimes redundantly testing input files which cannot improve the code coverage rate. \nSome input files mutated by the fuzzer even cannot pass the well-formed file structure test.} Recent research has come up with ideas of applying Deep Learning in the process of fuzzing to solve these problems.\nIn this section, we will review the very recent four representative works that use Deep Learning for fuzzing for software security.  Among the three, two representative works are already summarized phase-by-phase in Table \\ref{Table:Summary}. We direct interested readers to Table \\ref{Table:Summary} for a concise overview of those two works. \nOur review will be centered around three questions described in Section~\\ref{threequestions}. In the remaining of this section, we will first provide a set of observations, and then we provide the indications. Finally, we provide some general remarks.", "cites": [8014, 8017], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a general description of fuzzing and mentions the application of Deep Learning but lacks integration of the two cited papers into a cohesive narrative. It briefly references the papers without comparing or analyzing their approaches or limitations. There is little abstraction or synthesis of broader trends or principles."}}
{"id": "63c19e14-9f44-4182-a0ff-fa0ca6d47abe", "title": "Key findings from a closer look", "level": "subsection", "subsections": [], "parent_id": "eb166b74-7da0-4abe-af7f-7cfe3d3d385a", "prefix_titles": [["title", "Using Deep Learning to Solve Computer Security Challenges: A Survey"], ["section", "A closer look at applications of Deep Learning in security-oriented fuzzing"], ["subsection", "Key findings from a closer look"]], "content": "From a close look at the very recent applications using Deep Learning for solving security-oriented program analysis challenges, we observed the followings: \n\\begin{itemize}[label={}]\n    \\item  \\observation{} \\textit{Deep Learning has only been applied in mutation-based fuzzing.}\n    Even though various of different fuzzing techniques, \n    including symbolic execution based fuzzing~, tainted analysis based fuzzing~ and hybrid fuzzing~ have been proposed so far, we observed that all the works we surveyed employed Deep Learning method to assist the primitive fuzzing~\\textendash~mutation-based fuzzing.\n    Specifically, they adopted Deep Learning to assist fuzzing tool's input mutation. We found that they commonly did it in two ways: 1) training Deep Learning models to tell how to efficiently mutate the input to trigger more execution path~;\n    2) training Deep Learning models to tell how to keep the mutated files compliant with the program's basic semantic requirement~.\n    Besides, all three works trained different Deep Learning models for different programs,\n    which means that knowledge learned from one programs cannot be applied to other programs.\n    \\item  \\observation{} \\textit{Similarity among all the works in our survey existed when choosing the training samples in Phase I.}\n    The works in this survey had a common practice, i.e., using the input files directly as training samples of the Deep Learning model. Learn\\&Fuzz used character-level PDF objects sequence as training samples. Neuzz regarded input files directly as byte sequences and fed them into the neural network model. Mohit Rajpal et al. also used byte level representations of input files as training samples.\n    \\item  \\observation{} \\textit{Difference between all the works in our survey existed when assigning the training labels in Phase I.}\n    Despite the similarity of training samples researchers decide to use, there was a huge difference in the training labels that each work chose to use. \n    Learn\\&Fuzz directly used the character sequences of PDF objects as labels, same as training samples, but shifted by one position, which is a common generative model technique already broadly used in speech and handwriting recognition. \n    Unlike Learn\\&Fuzz, Neuzz and Rajpal’s work used bitmap and heatmap respectively as training labels, \n    with the bitmap demonstrating the code coverage status of a certain input, \n    and the heatmap demonstrating the efficacy of flipping one or more bytes of the input file. \n    Whereas, as a common terminology well-known among fuzzing researchers, bitmap was gathered directly from the results of AFL. Heatmap used by Rajpal et al. was generated by comparing the code coverage supported by the bitmap of one seed file \n    and the code coverage supported by bitmaps of the mutated seed files. It was noted that if there is acceptable level of code coverage expansion when executing the mutated seed files, demonstrated by more ``1''s, instead of ``0''s in the corresponding bitmaps, the byte level differences among the original seed file and the mutated seed files will be highlighted. Since those bytes should be the focus of later on mutation, heatmap was used to denote the location of those bytes.\n    Different labels usage in each work was actually due to the different kinds of knowledge each work wants to learn. For a better understanding, let us note that we can simply regard a Deep Learning model as a simulation of a ``function''. Learn\\&Fuzz~ wanted to learn valid mutation of a PDF file that was compliant with the syntax and semantic requirements of PDF objects. Their model could be seen as a simulation of $f(x, \\theta)=y$, where $x$ denotes sequence of characters in PDF objects and $y$ represents a sequence that are obtained by shifting the input sequences by one position. They generated new PDF object character sequences given a starting prefix once the model was trained. In Neuzz, an NN(Neural Network) model was used to do program smoothing, which simultated a smooth surrogate function that approximated the discrete branching behaviors of the target program. $f(x, \\theta)=y$, where $x$ denoted program's byte level input and $y$ represented the corresponding edge coverage bitmap. In this way, the gradient of the surrogate function was easily computed, due to NN's support of efficient computation of gradients and higher order derivatives. Gradients could then be used to guide the direction of mutation, in order to get greater code coverage. In Rajpal and others' work, they designed a model to predict good (and bad) locations to mutate in input files based on the past mutations and corresponding code coverage information. Here, the $x$ variable also denoted program's byte level input, but the $y$ variable represented the corresponding heatmap. \n    \\item  \\observation{} \\textit{Various lengths of input files were handled in Phase~\\ref{phase2}.}\n    Deep Learning models typically accepted fixed length input, whereas the input files for fuzzers often held different lengths. Two different approaches were used among the three works we surveyed: splitting and padding. Learn\\&Fuzz~ dealt with this mismatch by concatenating all the PDF objects character sequences together, and then splited the large character sequence into multiple training samples with a fixed size. Neuzz~ solved this problem by setting a maximize input file threshold and then, padding the smaller-sized input files with null bytes. From  additional experiments, they also found that a modest threshold gived them the best result, and enlarging the input file size did not grant them additional accuracy. Aside from preprocessing training samples, Neuzz also preprocessed training labels and reduced labels dimension by merging the edges that always appeared together into one edge, in order to prevent the multicollinearity problem, that could prevent the model from converging to a small loss value. Rajpal and others used the similar splitting mechanism as Learn\\&Fuzz to split their input files into either 64-bit or 128-bit chunks. Their chunk size was determined empirically and was considered as a trainable parameter for their Deep Learning model, and their approach did not require sequence concatenating at the beginning. \n    \\item  \\observation{} \\textit{All the works in our survey skipped Phase~\\ref{phase3}.}\n    According to our definition of Phase~\\ref{phase3}, all the works in our survey did not consider representation learning. Therefore, all the three works fell into class 1 shown in Figure~\\ref{fig:dec}.While as in Rajpal and others' work, they considered the numerical representation of byte sequences. They claimed that since one byte binary data did not always represent the magnitude but also state, representing one byte in values ranging from 0 to 255 could be suboptimal. They used lower level 8-bit representation.\n\\end{itemize}\nThe above observations seem to indicate the following indications: \n\\begin{itemize}[label={}]\n\\item \\indication{} \\textit{No alteration to the input files seems to be a correct approach.}\nAs far as we concerned, it is due to the nature of fuzzing. That is, since every bit of the input files matters, any slight alteration to the input files could either lose important information or add redundant information for the neural network model to learn. \n\\item \\indication{} \\textit{Evaluation criteria should be chosen carefully when judging mutation.}\nInput files are always used as training samples regarding using Deep Learning technique in fuzzing problems. Through this similar action, researchers have a common desire to let the neural network mode learn how the mutated input files should look like. But the criterion of judging a input file actually has two levels: on the one hand, a good input file should be correct in syntax and semantics; on the other hand, a good input file should be the product of a useful mutation, which triggers the program to behave differently from previous execution path. This idea of a fuzzer that can generate semantically correct input file could still be a bad fuzzer at triggering new execution path was first brought up in Learn\\&Fuzz. We could see later on works trying to solve this problem by using either different training labels or use neural network to do program smoothing. We encouraged fuzzing researchers, when using Deep Learning techniques, to keep this problem in mind, in order to get better fuzzing results. \n\\item \\indication{} \\textit{Works in our survey only focus on \\textit{local knowledge}.}\nIn brief, some of the existing works~ \nleveraged the Deep Learning model to learn the relation between program's input and its behavior and used the knowledge that learned from history to guide future mutation.\nFor better demonstration, we defined the knowledge that only applied in one program as \\textit{local knowledge}.\nIn other words, this indicates that the \\textit{local knowledge} cannot direct fuzzing on other programs.\n\\end{itemize} \n{\\color{myblack}", "cites": [8017, 8014], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple cited works and integrates them into a coherent analysis of how Deep Learning is used in security-oriented fuzzing. It identifies key similarities and differences in methodologies and motivations, such as the use of different labels and input handling techniques. The section also abstracts these findings into broader patterns and principles, such as the importance of preserving input fidelity and the implications for mutation evaluation criteria."}}
{"id": "18fd9c41-b209-4bf4-b2a4-ffffb8f5fc68", "title": "Discussion", "level": "section", "subsections": ["f52936f0-ab3c-4c04-a13a-cce90812bd88"], "parent_id": "825459b6-d132-4d0c-9bfb-51997111ee0c", "prefix_titles": [["title", "Using Deep Learning to Solve Computer Security Challenges: A Survey"], ["section", "Discussion"]], "content": "\\label{sec:dis}\nUsing high-quality data in Deep Learning is important as much as using well-structured deep neural network architectures. That is, obtaining quality data must be an important step, which should not be skipped, even in resolving security problems using Deep Learning. So far, this study demonstrated how the recent security papers using Deep Learning have adopted data conversion (Phase~\\ref{phase2}) and data representation (Phase~\\ref{phase3}) on different security problems. Our observations and indications showed a clear understanding of how security experts generate quality data when using Deep Learning.\nSince we did not review all the existing security papers using Deep Learning, the generality of observations and indications is somewhat limited. Note that our selected papers for review have been published recently at one of prestigious security and reliability conferences such as USENIX SECURITY, ACM CCS and so on~-,~,~,~-. Thus, our observations and indications help to understand how most security experts have used Deep Learning to solve the well-known eight security problems from program analysis to fuzzing.\nOur observations show that we should transfer raw data to synthetic formats of data ready for resolving security problems using Deep Learning through data cleaning and data augmentation and so on. Specifically, we observe that Phases~\\ref{phase2} and~\\ref{phase3} methods have mainly been used for the following purposes: \n\\begin{itemize}\n\\item To clean the raw data to make the neural network (NN) models easier to interpret\n\\item To reduce the dimensionality of data (e.g., principle component analysis (PCA), t-distributed stochastic neighbor embedding (t-SNE))\n\\item To scale input data (e.g., normalization)\n\\item To make NN models understand more complex relationships depending on security problems (e.g. memory graphs)\n\\item To simply change various raw data formats into a vector format for NN models (e.g. one-hot encoding and word2vec embedding) \n\\end{itemize} \nIn this following, we do further discuss the question, ``What if Phase~\\ref{phase2} is skipped?\", rather than the question, ``Is Phase~\\ref{phase3} always necessary?\". \nThis is because most of the selected papers do not consider Phase~\\ref{phase3} methods (76\\%), or adopt with no concrete reasoning (19\\%). \nSpecifically, we demonstrate how Phase~\\ref{phase2} has been adopted according to eight security problems, different types of data, various models of NN and various outputs of NN models, \nin depth. \nOur key findings are summarized as follows:\n\\begin{itemize}\n    \\item How to fit security domain knowledge into raw data has not been well-studied yet.\n    \\item While raw text data are commonly parsed after embedding, raw binary data are converted using various Phase~\\ref{phase2} methods.\n    \\item Raw data are commonly converted into a vector format to fit well to a specific NN model using various Phase~\\ref{phase2} methods.\n    \\item Various Phase~\\ref{phase2} methods are used according to the relationship between output of security problem and output of NN models.\n\\end{itemize}", "cites": [8017, 8019], "cite_extract_rate": 0.25, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes ideas from multiple papers to discuss common practices in data conversion and representation within deep learning for security. It abstracts these practices into broader patterns and phases, though the critical analysis is somewhat limited to noting the under-study of domain knowledge integration and the lack of reasoning in method adoption. The narrative is coherent but lacks deeper evaluation and a novel, unifying framework."}}
