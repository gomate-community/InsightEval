{"id": "86d6cdb9-2750-4e63-865d-831683dd115b", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "569f1197-c628-4e9d-9db6-9757cb02d82a", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Introduction"]], "content": "\\label{sec:intro} \n\\sectionauthor{Sebastian Houben\\textsuperscript{1}, Michael Mock\\textsuperscript{1}, Timo Sämann\\textsuperscript{4}, Gesina Schwalbe\\textsuperscript{3}, Joachim Sicking\\textsuperscript{1}}\nIn barely a decade, deep neural networks (DNNs) have revolutionized the field of machine learning by reaching unprecedented, sometimes superhuman, performances on a growing variety of tasks.\nMany of these neural models have found their way into consumer applications like smart speakers, machine translation engines or content feeds. However, in safety-critical systems, where human life might be at risk, the use of recent DNNs is challenging as various model-immanent insufficiencies are yet difficult to address. \n\\par\nThis paper summarizes the promising lines of research in how to identify, address, and at least partly mitigate these DNN insufficiencies.\nWhile some of the reviewed works are theoretically grounded and foster the overall understanding of training and predictive power of DNNs, others provide practical tools to adapt their development, training or predictions. \nWe refer to any such method as a \\emph{safety mechanism} if it addresses one or several safety concerns in a feasible manner.\nTheir effectiveness in mitigating safety concerns is assessed by \\emph{safety metrics}  . \nAs most safety mechanisms target only a particular insufficiency, we conclude that a \\emph{holistic safety argumentation} for a complex DNN-based systems will in many cases rely on a variety of safety mechanisms. \n\\par\nWe structure our review of these mechanisms as follows: Chapter \\ref{sec:dataset_optimization} focuses on \\emph{dataset optimization} for network training and evaluation. It is motivated by the well-known fact that, in comparison to humans, DNNs perform poorly on data that is structurally different from training data. Apart from insufficient generalization capabilities of these models, the data acquisition process and distributional data shifts over time play vital roles. We survey potential counter-measures, \\eg augmentation strategies and outlier detection techniques.\n\\par\nMechanisms that improve on \\emph{robustness} are described in Chapters \\ref{sec:robust_training} and \\ref{sec:adversarial_attacks}, respectively. They deserve attention as DNNs are generally not resilient to common perturbations and adversarial attacks.\n\\par\nChapter \\ref{sec:interpretability} addresses incomprehensible network behavior and reviews mechanisms that aim at \\emph{explainability}, \\eg a more transparent functioning of DNNs.\n\\par\nMoreover, DNNs tend to overestimate their prediction confidence, especially on unseen data. \nStraightforward ways to estimate prediction confidence yield mostly unsatisfying results.\nAmong others, this observation fuelled research on more sophisticated \\emph{uncertainty estimations} (see Chapter \\ref{sec:uncertainty}), \\emph{redundancy mechanisms} (see Chapter \\ref{sec:aggregation}) and attempts to reach \\emph{formal verification} as addressed in Chapter \\ref{sec:verification}.\n\\par\nAt last, many safety-critical applications require not only accurate but also near real-time decisions. This is covered by mechanisms on the DNN \\emph{architectural level}  (see Chapter \\ref{sec:architecture}) and furthermore by \\emph{compression} and \\emph{quantization} methods (see Chapter \\ref{sec:compression}).\n\\par\nWe conclude this review of mechanism categories with an outlook on the steps to transfer a carefully arranged combination of safety mechanisms into an actual holistic safety argumentation.", "cites": [8089, 6010, 8157], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from the cited papers by integrating concepts like coverage-guided fuzzing (Paper 1), dependability metrics (Paper 2), and DNN safety frameworks (Paper 3) into a broader discussion of safety mechanisms. It abstracts these ideas into categories such as robustness, interpretability, and uncertainty estimation, suggesting a structured approach to AI safety. However, it provides limited critical analysis of the strengths or weaknesses of the individual methods or papers."}}
{"id": "36c23394-2c6f-46c4-b022-50c53af7e345", "title": "Outlier/Anomaly Detection", "level": "subsection", "subsections": [], "parent_id": "1fe2887d-3162-404c-bb44-c07ac2772367", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Dataset Optimization"], ["subsection", "Outlier/Anomaly Detection"]], "content": "\\label{subsec:dataset_optimization:anomaly_detection}\n\\sectionauthor{Sujan Sai Gannamaneni\\textsuperscript{1}, Matthias Rottmann\\textsuperscript{5}}\nThe terms anomaly, outlier and \\emph{out-of-distribution} (OOD) data detection are often used interchangeably in literature and refer to task of identifying data samples that are not representative of training data distribution. Uncertainty evaluation (\\cf Chapter \\ref{sec:uncertainty}) is closely tied to this field as self-evaluation of models is one of the active areas of research for OOD detection. In particular, for image classification problems it has been reported that neural networks often produce high confidence predictions on OOD data . The detection of such OOD inputs can either be tackled by post-processing techniques that adjust the estimated confidence  or by enforcing low confidence on OOD samples during training . Even guarantees that neural networks produce low confidence predictions for OOD samples can be provided under specific assumptions (\\cf ). More precisely, this work utilizes Gaussian mixture models that, however, may suffer from high-dimensional data and require strong assumptions on the distribution parameters.\nSome approaches use generative models like \\emph{GANs}  and \\emph{autoencoders}  for outlier detection. The models are trained to learn in-distribution data manifolds and will produce higher reconstruction loss for outliers.\n\\par\nFor OOD detection in semantic segmentation, only a few works have been presented so far.\nAngus \\etal  present a comparative study of common OOD detection methods, which mostly deal with image-level classification. In addition, they provide a novel setup of relevant OOD datasets for this task. Another work trains a fully convolutional binary classifier that distinguishes image patches from a known set of classes from image patches stemming from an unknown class . The classifier output applied at every pixel will give the per-pixel confidence value for an OOD object. Both of these works perform at pixel level and without any sophisticated feature generation methods specifically tailored for the detection of entire OOD instances.\nUp to now, outlier detection has not been  studied extensively for object detection tasks based on benchmark object detection datasets. In , two CNNs are used to perform object detection and binary classification (benign or anomaly) in a sequential fashion, where the second CNN takes the localized object within the image as input.\n\\par\nFrom a safety standpoint, detecting outliers or OOD samples is extremely important and beneficial as training data cannot realistically be large enough to capture all situations. Research in this area is heavily entwined with progress in uncertainty estimation (\\cf Chapter \\ref{sec:uncertainty}) and domain adaptation (\\cf \\refsec{subsec:dataset_optimization:domains}). Extending research works to segmentation and object detection tasks would be particularly significant for leveraging autonomous driving research. In addition to safety, OOD detection can be beneficial in other aspects like when using local expert models. For example, when using an expert model for segmentation of urban driving scenes and another expert model for segmentation of highway driving scenes, an OOD detector could act as trigger on which models can be switched. \n\\par\nWith respect to the approaches presented above, uncertainty-based and generative model-based OOD detection methods are currently promising directions of research. However, it remains an open question whether they can unfold their potential well on segmentation and object detection tasks.", "cites": [1624, 8160, 3251, 3237, 3219, 8159, 8158, 3248, 8161], "cite_extract_rate": 0.6923076923076923, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes multiple papers to present a coherent narrative on outlier and OOD detection, particularly in the context of computer vision. It critically evaluates the limitations of methods like Gaussian mixture models and discusses how approaches such as uncertainty estimation and generative models are promising but face challenges. While it identifies some broader patterns (e.g., the role of uncertainty estimation), the abstraction is somewhat limited by its focus on specific tasks like segmentation and object detection."}}
{"id": "f767152d-e919-4dba-8417-ed1faf4da217", "title": "Active Learning", "level": "subsection", "subsections": [], "parent_id": "1fe2887d-3162-404c-bb44-c07ac2772367", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Dataset Optimization"], ["subsection", "Active Learning"]], "content": "\\label{subsec:dataset_optimization:active_learning}\n\\sectionauthor{Matthias Rottmann\\textsuperscript{5}}\nIt is widely known that, as a rule of thumb, for the training of any kind of artificial neural network, an increase of training data leads to increased performance.\nObtaining labeled training data, however, is often very costly and time consuming.\n\\emph{Active learning} provides one possible remedy to this problem: Instead of labeling every data point, active learning utilizes a \\emph{query strategy} to request labels from a teacher/an oracle which leverage the model performance most.\nThe survey paper by Settles  provides a broad overview regarding query strategies for active learning methods. However, except for \\emph{uncertainty sampling} and \\emph{query by committee}, most of them seem to be infeasible in deep learning applications up to now. Hence, most of the research activities in active deep learning focus on these two query strategies, as we outline in the following.\n\\par\nIt has been shown  for image classification that labels corresponding to uncertain samples can leverage the networks' performance significantly and that a combination with semi-supervised learning is promising. In both works, uncertainty of unlabeled samples is estimated via Monte Carlo (MC) dropout inference. MC dropout inference and a chosen number of training epochs are executed alternatingly, after performing MC dropout inference, the unlabeled samples' uncertainties are assessed by means of sample-wise dispersion measures. Samples for which the DNN model is very uncertain about its prediction are presented to an oracle and labeled.\n\\par\nWith respect to object detection, a moderate number of active learning methods has been introduced .\nThese approaches include uncertainty sampling  and query-by-committee methods . In , additional algorithmic features specifically tailored for object detection networks are presented, \\ie separate treatment of the localization and classification loss , as well as weak and strong supervision schemes . For semantic segmentation, an uncertainty-sampling-based approach has been presented , which queries polygone masks for image sections of a fixed size ($128 \\times 128$). Queries are performed by means of accumulated entropy in combination with a cost estimation for each candidate image section.\n\\par\nRecently, new methods for estimating the quality of a prediction  as well as new uncertainty quantification approaches, \\eg gradient-based ones , have been proposed. It remains an open question whether they are suitable for active learning. Since most of the conducted studies are rather of academic nature, also their applicability to real-life data acquisition is not yet demonstrated sufficiently. In particular, it is not clear whether the proposed active learning schemes, including the label acquistion, for instance in semantic segmentation, is suitable to be performed by human labelers. Therefore, labeling acquisition with a common understanding of the labelers' convenience and suitability for active learning are a promising direction for research and development.", "cites": [8162, 1044, 5527, 3237, 4620, 1043, 1049], "cite_extract_rate": 0.6363636363636364, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.5, "abstraction": 3.3}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers to present active learning strategies for DNNs, focusing on uncertainty sampling and query-by-committee methods. It critically points out the infeasibility of many traditional strategies in deep learning and highlights the academic nature of current studies. While it identifies patterns like the use of MC dropout and the potential of weak supervision, it stops short of forming a novel, overarching framework."}}
{"id": "7b5fda9c-0631-4e3b-bf4f-fb2fb6b0d60d", "title": "Domains", "level": "subsection", "subsections": [], "parent_id": "1fe2887d-3162-404c-bb44-c07ac2772367", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Dataset Optimization"], ["subsection", "Domains"]], "content": "\\label{subsec:dataset_optimization:domains}\n\\sectionauthor{Julia Rosenzweig\\textsuperscript{1}}\nThe classical assumption in machine learning is that the training and testing data sets are drawn from the same distribution, implying that the model is deployed under the same conditions as it was trained under.  However, as  mention, in real-world applications this assumption is often violated in the sense that the training and the testing set stem from different domains having different distributions. This poses difficulties for statistical models and the performance will mostly degrade when they are deployed on a domain $D^{\\mathrm{test}}$, having a different distribution than the training dataset (\\ie generalizing from the training to the testing domain is not possible). This makes the study of domains not only relevant from the machine learning perspective, but also from a safety point of view.\n\\par\nMore formally, there are differing notions of a 'domain' in literature. For , a domain $\\mathcal{D} = \\{\\mathcal{X}, P(X) \\}$  consists of a feature space $\\mathcal{X} \\subset \\mathbb{R}^d$ together with a marginal probability distribution $P(X)$ with $X\\in \\mathcal{X}$. In , a domain is a pair consisting of a distribution over the inputs together with a labeling function.\nHowever, instead of a sharp labeling function, it is also widely accepted to define a (training) domain $\\mathcal{D} = \\{(x_i,y_i)\\}_{i=1}^n$ to consist of $n$ (labeled) samples that are sampled from a joint distribution $P(x,y)$ (\\cf ). \n\\par\nThe reasons for \\emph{distributional shift} are diverse---as are the names to indicate a shift. For example, if the rate of (class) images of interest is  different between training and testing set this can lead to a domain gap and, \\eg result in differing overall error rates. Moreover, as  mentions, changing weather conditions and camera setups in cars lead to a domain mismatch in applications of autonomous driving.\nIn biomedical image analysis, different imaging protocols and diverse anatomical structures can hinder generalization of trained models (\\cf ).\nCommon terms to indicate distributional shift are \\emph{domain shift, dataset shift, covariate shift, concept drift, domain divergence, data fracture, changing environments} or \\emph{dataset bias}. References  provide an overview. \n\\par\nMethods and measures to overcome the problem of domain mismatch between one or more (\\cf ) source domains and target domain(s) and the resulting poor model performance are studied in the field of transfer learning and in particular its subtopic domain adaptation (\\cf ).\nFor instance, adapting a model that is trained on synthetically generated data to work on real data is one of the core challenges, as can be seen .\nFurthermore, detecting when samples are out-of-domain or out-of-distribution is an active field of research (\\cf  and the outlier/anomaly detection in \\refsec{subsec:dataset_optimization:anomaly_detection} as well as the topic of observers in the black-box methods in \\refsec{subsec:verification:black_box_methods} for further reference). This is particularly relevant for machine learning models that operate in the real world: If, \\eg an autonomous vehicle encounters some situation that deviates strongly from what was seen during training (\\eg due to some special event like a biking competition, carnival, etc.) this can lead to wrong predictions and thereby potential safety issues if not detected in time.", "cites": [330, 8163, 3267, 5113, 328, 8495], "cite_extract_rate": 0.4, "origin_cites_number": 15, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of domain-related concepts and distributional shift by integrating definitions and examples from multiple cited papers, but it lacks deeper connections or a novel framework. It offers minimal critical analysis, merely referencing that methods exist without evaluating their effectiveness or limitations. The abstraction is limited to general observations about domain shift and adaptation, without identifying overarching principles or meta-level insights."}}
{"id": "de2e598e-f35b-4b65-945b-25aee539ea0d", "title": "Augmentation", "level": "subsection", "subsections": [], "parent_id": "1fe2887d-3162-404c-bb44-c07ac2772367", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Dataset Optimization"], ["subsection", "Augmentation"]], "content": "\\label{subsec:dataset_optimization:augmentation}\n\\sectionauthor{Falk Kappel\\textsuperscript{13}}\nGiven the need for big amounts of data to train neural networks, one often runs into a situation where data is lacking. This can lead to insufficient generalization and an overfitting to the training data. An overview over different techniques to tackle this challenge can be found in . One approach to try and overcome this issue is the augmentation of data. It aims at optimizing available data and increasing its amount, curating a dataset that represents a wide variety of possible inputs during deployment. Augmentation can as well be of help when having to work with a heavily unbalanced dataset by creating more samples of underrepresented classes. A broad survey on data augmentation is provided by . They distinguish between two general approaches to data augmentation with the first one being data warping augmentations that focus on taking existing data and transforming it in a way that does not effect labels. The other option are oversampling augmentations, which create synthetic data that can be used to increase the size of the dataset.\\newline\nExamples of some of the most basic augmentations are flipping, cropping, rotating, translating, shearing and zooming. These are affecting the geometric properties of the image and are easily implemented . The machine learning toolkit \\texttt{Keras}, for example, provides an easy way of applying them to data using their \\texttt{ImageDataGenerator} class . Other simple methods include adaptations in color space that affect properties such as lighting, contrast and tints, which are common variations within image data. Filters can be used to control increased blur or sharpness . In  random erasing is introduced as a method with similar effect as cropping, aiming at gaining robustness against occlusions. An example for mixing images together as an augmentation technique can be found in . \\newline\nThe abovementioned methods have in common that they work on the input data but there are different approaches that make use of deep learning for augmentation. An example for making augmentations in feature space using autoencoders can be found in . They use the representation generated by the encoder and generate new samples by interpolation and extrapolation between existing samples of a class. The lack of interpretability of augmentations in feature space in combination with the tendency to perform worse than augmentations in image space present open challenges for those types of augmentations . Adversarial training is another method that can be used for augmentation. The goal of adversarial training is to discover cases that would lead to wrong predictions. That means the augmented images won't necessarily represent samples that could occur during deployment but that can help in achieving more robust decision boundaries . An example of such an approach can be found in . Generative modelling can be used to generate synthetic samples that enlarge the dataset in a useful way with GANs, variational autoencoders and the combination of both are important tools in this area . Examples for data augmentation in medical context using a CycleGAN  can be found in  and using a progressively growing GAN  in . Next to neural style transfer  that can be used to change the style of an image to a target style, AutoAugment  and population based augmentation  are two more interesting publications. In the latter two, the idea is to search a predefined search space of augmentations to gather the best selection.", "cites": [8164, 858, 8165, 5980, 5552, 90, 2083, 5548, 7022, 5553, 62], "cite_extract_rate": 0.7333333333333333, "origin_cites_number": 15, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of data augmentation techniques, listing examples and categorizing them as data warping or oversampling. It cites multiple papers but does not deeply connect their ideas or evaluate their strengths and weaknesses in a critical manner. Some general patterns (e.g., domain-specific vs. domain-agnostic approaches) are mentioned, but the section remains largely at the level of summarizing individual methods without offering a deeper analytical or synthetic framework."}}
{"id": "e4ebc416-15f6-4517-a761-acd4315ad09b", "title": "Corner Case Detection", "level": "subsection", "subsections": [], "parent_id": "1fe2887d-3162-404c-bb44-c07ac2772367", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Dataset Optimization"], ["subsection", "Corner Case Detection"]], "content": "\\label{subsec:dataset_optimization:corner_case_detection}\n\\sectionauthor{Alexander Pohl\\textsuperscript{16}, Marco Hoffmann\\textsuperscript{16}, Michael Mlynarski\\textsuperscript{16}, Timo Sämann\\textsuperscript{4}}\nEnsuring that AI-based applications behave correctly and predictably even in unexpected or rare situations is a major concern that gains importance especially in safety-critical applications such as autonomous driving. In the pursuit of more robust AI corner cases play an important role. \n\\par\nThe meaning of the term corner case varies in the literature. Some consider mere erroneous or incorrect behavior as corner cases . For example, in  corner cases are referred to as situations in which an object detector fails to detect relevant objects at relevant locations. Others characterize corner cases mainly as rare combinations of input parameter values . This project adopts the first definition: Inputs that result in unexpected or incorrect behaviour of the AI function are defined as corner cases. \n\\par\nContingent on the hardware, the AI architecture and the training data, the search space of corner cases quickly becomes incomprehensibly large. While manual creation of corner cases (\\eg constructing or re-enacting scenarios) might be more controllable, approaches that scale better and allow for a broader and more systematic search for corner cases require extensive automation.\n\\par\nOne approach to automatic corner case detection is based on transforming the input data. The \\emph{DeepTest} framework  uses three types of image transformations: linear, affine and convolutional transformations. In addition to these transformations, metamorphic relations help detect undesirable behaviors of deep learning systems. They allow changing the input while asserting some characteristics of the result . For example, changing the contrast of input frames should not affect the steering angle of a car . Input-output pairs that violate those metamorphic relations can be considered as corner cases.\n\\par\nAmong other things, the white-box testing framework \\emph{DeepXplore}  applies a method called \\emph{gradient ascent} to find corner cases (\\cf \\refsec{subsec:verification:formal_testing}). In the experimental evaluation of the framework, three variants of deep learning architectures were used to classify the same input image. The input image was then changed according to the gradient ascent of an objective function that reflected the difference in the resulting class probabilities of the three model variants. When the changed (now artificial) input resulted in different class label predictions by the model variants, the input was considered as a corner case.  \n\\par\nIn , corner cases are detected on video sequences by comparing predicted with actual frames. The detector has three components: The first component, semantic segmentation, is used to detect and locate objects in the input frame. As the second component, an image predictor trained on frame sequences predicts the actual frame based on the sequence preceding that  frame. An error is determined by comparing the actual with the predicted (\\ie expected) frame, following the idea that only situations that are unexpected for AI-based perception functions may be potentially dangerous and therefore a corner case. Both the segmentation and the prediction error are then fed into the third component of the detector, which determines a corner case score that reflects the extent to which unexpected relevant objects are at relevant locations.\n\\par\nIn , a corner case detector based on simulations in a \\emph{Carla} environment  is presented. In the simulated world, AI agents control the vehicles. During simulations, state information of both the environment and the AI agents are fed into the corner case detector. While the environment provides the real vehicle states, the AI agents provide estimated and perceived state information. Both sources are then compared to detect conflicts (\\eg collisions). These conflicts are recorded for analysis. \n\\par\nSeveral ways of automatically generating and detecting corner cases exist. However, corner case detection is a task with challenges of its own: Depending on the operational domain including its boundaries, the space of possible inputs can be very large. Also, some types of corner cases are specific to the AI architecture, \\eg the network type or the network layout used. Thus, corner case detection has to assume a holistic point of view on both model and input, adding further complexity and reducing transferability of previous insights. \n\\par\nAlthough it can be argued that rarity does not necessarily characterize corner cases, rare input data might have the potential of challenging the AI functionality (\\cf \\refsec{subsec:dataset_optimization:anomaly_detection}).\nAnother research direction could investigate whether structuring the input space in a way suitable for the AI functionality supports the detection of corner cases. Provided that the operational domain is conceptualized as an ontology, ontology-based testing  may support automatic detection. A properly adapted generator may specifically select promising combinations of extreme parameter values and, thus, provide valuable input for synthetic test data generation.", "cites": [8133, 8166, 7698, 1636, 3500], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers on corner case detection, integrating methods such as input transformation (DeepTest), gradient-based approaches (DeepXplore), and simulation-based detection (CARLA). It provides a coherent narrative and connects concepts like metamorphic relations and ontology-based testing. The critical evaluation is moderate, highlighting challenges such as the large input space and architecture-specific corner cases. The abstraction level is reasonable, as it moves beyond individual systems to suggest broader research directions and a holistic view of model and input interactions."}}
{"id": "380e4e9a-2d10-49f6-add4-04990cddb902", "title": "Robust Training", "level": "section", "subsections": ["62315f40-21bf-40c7-afd5-00e8d80aa4e1", "cf7b5f9f-caf9-44e0-b472-2124535b4d7a", "e4ab90c2-e9e5-49ce-a3ae-b1dfdb4472d4"], "parent_id": "569f1197-c628-4e9d-9db6-9757cb02d82a", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Robust Training"]], "content": "\\label{sec:robust_training}\n\\sectionauthor{Nikhil Kapoor\\textsuperscript{7}}\nRecent works~ have shown that state-of-the-art deep neural networks (DNNs) performing a wide variety of computer vision tasks such as image classification~, object detection~ and semantic segmentation~ are not robust to small changes in the input.\n\\par\nRobustness of neural networks is an active and open research field that can be considered highly relevant for achieving safety in autonomous driving. Currently, most of the research is directed towards either improving adversarial robustness~ (robustness against carefully designed perturbations that aim at causing misclassifications with high confidence), or improving corruption robustness~ (robustness against commonly occurring augmentations such as weather changes, addition of Gaussian noise, photometric changes, etc.). While adversarial robustness might be more of a security issue than a safety issue, corruption robustness, on the other hand, can be considered highly safety-relevant.\n\\par\nEquipped with these definitions, we broadly term \\emph{robust training} here as methods or mechanisms that aim at improving either adversarial or corruption robustness of a DNN, by incorporating modifications into the architecture or into the training mechanism itself.", "cites": [314, 509, 1606, 8429, 1621, 1759, 8167, 8168, 8169, 206, 520, 4169], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 18, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section introduces the concept of robust training and references several papers, but it lacks integration of these works into a cohesive framework. The cited papers are either not directly connected to the topic of robust training or are only mentioned in passing without elaboration. The section provides minimal critical evaluation or abstraction, focusing primarily on defining terms and listing related works without deeper analysis or synthesis."}}
{"id": "62315f40-21bf-40c7-afd5-00e8d80aa4e1", "title": "Hyperparameter Optimization", "level": "subsection", "subsections": [], "parent_id": "380e4e9a-2d10-49f6-add4-04990cddb902", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Robust Training"], ["subsection", "Hyperparameter Optimization"]], "content": "\\label{subsec:robust_training:hyperparameter_optimization}\n\\sectionauthor{Seyed Eghbal Ghobadi\\textsuperscript{8}, Patrick Feifel\\textsuperscript{8}}\nThe final performance of a neural network depends highly on the learning process. The process includes the actual optimization and may additionally introduce training methods such as dropout, regularization, or parametrization of a multi-task loss. \n\\par\nThese methods adapt their behavior for predefined parameters. Hence, their optimal configuration is a priori unknown. We refer to them as \\emph{hyperparameters}. Important hyperparameters comprise, for instance, the initial learning rate, steps for learning rate reduction, learning rate decay, momentum, batch size, dropout rate and number of iterations. Their configuration has to be determined according to the architecture and task of the CNN . The search of an optimal hyperparameter configuration is called hyperparameter optimization (HO).\n\\par\nHO is usually described as an optimization problem . Thereby, the combined configuration space is defined as \n$\\boldsymbol{\\Lambda} = \\lambda_1 \\times \\lambda_2 \\times \\cdots \\lambda_N$,\naccording to each domain $\\lambda_n$. Their individual spaces can be continuous, discrete, categorical or binary. \n\\par\nHence, we aim to find an optimal hyperparameter configuration $\\lambda^{\\star}$ by minimizing an objective function $\\mathcal{O}\\left ( \\right )$, which evaluates a trained model $\\mathcal{M}$ on the validation dataset $\\mathcal{D}^{\\textrm{val}}$ with the loss $\\mathcal{L}$:\n\\begin{equation}\n\\lambda^{\\star} = \n\\text{arg  min}_{\\lambda \\in \\boldsymbol{\\Lambda}} \\; \n\\mathcal{O} \\left( \\mathcal{L}, \\mathcal{M}_{\\lambda}, \\mathcal{D}^{\\textrm{train}}, \\mathcal{D}^{\\textrm{val}} \\right)\n\\end{equation}\nThis problem statement is widely regarded in traditional machine learning and primarily based on Bayesian optimization (BO) in combination with Gaussian processes. However, a straightforward application to deep neural networks encounters problems due to a \\emph{lack of scalability, flexibility and robustness} , . \n\\par\nTo exploit the benefits of BO, many authors proposed different combinations with other approaches. \nHyperband  in combination with BO (BOHB)  frames the optimization as ``... a pure exploration non-stochastic infinite-armed bandit problem ...''. \nThe method of BO for iterative learning (BOIL)  internalizes iteratively collected information about the learning curve and the learning algorithm itself. \nThe authors of  introduce the trace-aware knowledge gradient (taKG) as an acquisition function for BO (BO-taKG) which ``leverages both trace information and multiple fidelity controls''.\nThereby BOIL and BO-taKG achieve state-of-research performance regarding CNNs outperforming Hyperband.\n\\par\nOther approaches such as the orthogonal array tuning method (OATM)  or HO by reinforcement learning (Hyp-RL)  turn away from the Bayesian approaches and offer new research directions.\n\\par\nFinally, the insight that many authors include kernel sizes and number of kernels and layers in their hyperparameter configuration should be emphasized. More work should be spent on the distinct integration of HO in the performance estimation strategy of neural architecture search (\\cf \\refsec{subsec:architecture:neural_architecture_search}).", "cites": [868, 8171, 8173, 8172, 8170], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from multiple cited papers to present a coherent narrative on hyperparameter optimization methods for AI safety, particularly in the context of CNNs. It provides a critical evaluation by pointing out the limitations of Bayesian optimization when applied to deep learning and by comparing the performance of different approaches (e.g., BOIL and BO-taKG). The section abstracts beyond individual papers by discussing broader optimization strategies and suggesting future research directions, such as the integration of hyperparameter optimization with neural architecture search."}}
{"id": "cf7b5f9f-caf9-44e0-b472-2124535b4d7a", "title": "Modification of Loss", "level": "subsection", "subsections": [], "parent_id": "380e4e9a-2d10-49f6-add4-04990cddb902", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Robust Training"], ["subsection", "Modification of Loss"]], "content": "\\label{subsec:robust_training:modification_of_loss}\n\\sectionauthor{Nikhil Kapoor\\textsuperscript{7}}\nThere exist many approaches that aim at directly modifying the loss function with an objective of improving either adversarial or corruption robustness~. One of the earliest approaches for improving corruption robustness was introduced by Zheng \\etal  called \\emph{stability training}, where they introduce a regularization term that penalizes the network prediction to a clean and an augmented image. However, their approach does not scale to many augmentations at the same time. Janocha \\etal  then introduced a detailed analysis on the influence of multiple loss functions to model performance as well as robustness and suggested that expectation-based losses tend to work better with noisy data and squared-hinge losses tend to work better for clean data. Other well-known approaches are mainly based on variations of data augmentation~, which can be computationally quite expensive. \n\\par\nIn contrast to corruption robustness, there exist many more approaches based on adversarial examples. We highlight some of the most interesting and relevant ones here. Mustafa \\etal  proposes to add a loss term that maximally separates class-wise feature map representations, hence increasing the distance from data points to the corresponding decision boundaries. Similarly, Pang \\etal  proposed the Max-Mahalanobis center (MMC) loss to learn more structured representations and induce high-density regions in the feature space. Chen \\etal  proposed a variation of the well-known cross entropy (CE) loss that not only maximizes the model probabilities of the correct class, but in addition, also minimizes model probabilities of incorrect classes. Cisse \\etal  constraints the Lipschitz constant of different layers to be less than one which restricts the error propagation introduced by adversarial perturbations to a DNN. Dezfooli \\etal  proposed to minimize the curvature of the loss surface locally around data points. They emphasize that there exists a strong correlation between locally small curvature and correspondingly high adversarial robustness. \n\\par\nAll of these methods highlighted above are evaluated mostly for image classification tasks on smaller datasets, namely CIFAR-10~, CIFAR-100~, SVHN~, and only sometimes on ImageNet~. Very few approaches have been tested rigorously on complex safety-relevant tasks such as \\emph{object detection} and \\emph{semantic segmentation}, etc. Moreover, methods that improve adversarial robustness are only tested on a small subset of attack types under differing attack specifications. This makes comparing multiple methods difficult. \n\\par\nIn addition, methods that improve corruption robustness are evaluated over a standard data set of various corruption types which may or may not be relevant to its application domain. In order to assess multiple methods for their effect on safety-related aspects, a thorough robustness evaluation methodology is needed, which is largely missing in the current literature. This evaluation would need to take into account relevant disturbances/corruption types present in the real world (application domain) and had to assess robustness towards such changes in a rigorous manner. Without such an evaluation, we run the risk of being overconfident in our network, thereby harming safety.", "cites": [8179, 8177, 958, 8176, 8174, 8167, 8175, 8180, 8178], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 21, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple cited papers by grouping them into themes like corruption robustness and adversarial robustness, and explains how different loss modifications aim to address these challenges. It offers some critical analysis by noting the limited evaluation of methods on real-world safety-relevant tasks and the lack of a comprehensive robustness evaluation methodology. While it identifies patterns in the types of losses used, it does not abstract to a higher-level conceptual framework or propose a novel synthesis."}}
{"id": "e4ab90c2-e9e5-49ce-a3ae-b1dfdb4472d4", "title": "Domain Generalization", "level": "subsection", "subsections": [], "parent_id": "380e4e9a-2d10-49f6-add4-04990cddb902", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Robust Training"], ["subsection", "Domain Generalization"]], "content": "\\sectionauthor{Firas Mualla\\textsuperscript{13}}\nDomain generalization (DG) can be seen as an extreme case of \\emph{domain adaptation} (DA). The latter is a type of transfer learning, where the source and target tasks are the same (\\eg shared class labels) but the source and target domains are different (\\eg another image acquisition protocol or a different background) . \nThe DA can be either supervised (SDA), where there is little available labeled data in the target domain, or unsupervised (UDA), where data in the target domain is not labeled. The DG goes one step further by assuming that the target domain is entirely unknown. Thus, it seeks to solve the train-test domain shift in general. While DA is already an established line of research in the machine learning community, DG is relatively new , though with an extensive list of papers in the last few years. \n\\par\nProbably, the first intuitive solution that one may think of to implement DG is neutralizing the domain-specific features. It was shown in  that the gray-level co-occurrence matrices (GLCM) tend to perform poorly in semantic classification (\\eg digit recognition) but yield good accuracy in textural classification compared to other feature sets such as SURF and LBP. DG was thus implemented by decorrelating the model's decision from the GLCM features of the input image even without the need of domain labels.\nBesides the aforementioned intensity-based statistics of an input image, it is known that characterizing image style can be done based on the correlations between the filter responses of a DNN layer  (neural style transfer). In , the training images are enriched with stylized versions, where a style is defined either by an external style (\\eg cartoon or art) or by an image from another domain. Here, DG is addressed as a \\emph{data augmentation} problem.\nSome approaches  try to learn generalizable latent representations by a kind of adversarial training. This is done by a generator or an encoder, which is trained to generate a hidden feature space that maximizes the error of a domain discriminator but at the same time minimizes the classification error of the task of concern. Another flavor of adversarial training can be seen in , where an adversarial autoencoder  is trained to generate features, which a discriminator cannot distinguish from random samples drawn from a prior Laplace distribution. This regularization prevents the hidden space from overfitting to the source domains, in a similar spirit to how variational autoencoders do not leave gaps in the latent space. In , it is argued that the domain labels needed in such approaches are not always well-defined or easily available. Therefore they assume unknown latent domains which are learned by clustering in a space similar to the style-transfer features mentioned above. The pseudo labels resulting from clustering are then used in the adversarial training.\nAutoencoders have been employed for DG not only in an adversarial setup, but also in the sense of \\emph{multi-task learning} nets , where the classification task in such nets is replaced by a reconstruction one. In , an autoencoder is trained to reconstruct not only the input image but also the corresponding images in the other domains.\nIn the core of both DA and DG we are confronted with a distribution matching problem. However, estimating the probability density in high-dimensional spaces is intractable. The density-based metrics such as Kullback-Leibler divergence are thus not directly applicable. In statistics, the so-called \\emph{two-samples tests} are usually employed to measure the distance between two distributions in a point-wise manner, \\ie without density estimation. For deep learning applications, these metrics need not only to be point-wise but also differentiable. The two-samples tests were approached in the machine learning literature using (differentiable) K-NNs , classifier two-samples tests (C2ST) , or based on the theory of kernel methods . More specifically, the \\emph{maximum mean discrepancy} (MMD) , which belongs to the kernel methods, is widely used for DA  but also for DG . Using the MMD, the distance between two samples is estimated based on pairwise kernel evaluations, \\eg the radial basis function (RBF) kernel.\nWhile the DG approaches generalize to domains from which zero shots are available, the so-called \\emph{zero shot learning} (ZSL) approaches generalize to tasks (\\eg new classes in the same source domains) for which zero shots are available. Typically, the input in ZSL is mapped to a semantic vector per class instead of a simple class label. This can be, for instance, a vector of visual attributes  or a word embedding of the class name . A task (with zero shots at training time) can be then described by a vector in this space. In , there is an attempt to combine ZSL and DG in the same framework in order to generalize to new domains as well as new tasks, which is also referred to as\n \\emph{heterogeneous domain generalization}.\nNote that most discussed approaches for DG require non-standard handling, \\ie modifications to models, data, and/or the optimization procedure. This issue poses a serious challenge as it limits the practical applicability of these approaches. There is a line of research which tries to address this point by linking DG to other machine learning paradigms, especially the model-agnostic meta-learning (MAML)  algorithm, in an attempt to apply DG in a model-agnostic way. Loosely speaking, a model can be exposed to simulated train-test domain shift by training on a small \\emph{support set} to minimize the classification error on a small \\emph{validation set}. This can be seen as an instance of a \\emph{few shot learning} (FSL) problem . Moreover, the procedure can be repeated on other (but related) FSL tasks (\\eg different classes) in what is known as \\emph{episodic training}. The model transfers its knowledge from one task to another task and learns how to learn fast for new tasks. This can be thus seen as a \\emph{meta-learning} objective  (in a FSL setup). Since the goal of DG is to adapt to new domains rather than new tasks, several model-agnostic approaches  try to recast this procedure in a DG setup.", "cites": [2800, 5073, 7109, 3624, 2690, 8589, 2761, 2686, 8181, 8182, 3655, 8588, 2705, 1695, 1256, 8183, 2793], "cite_extract_rate": 0.5666666666666667, "origin_cites_number": 30, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.3, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes a variety of DG techniques from multiple papers, including adversarial training, autoencoders, and meta-learning, while connecting them to broader concepts like domain shift and distribution matching. It provides a critical evaluation by pointing out limitations such as the need for non-standard handling and the availability of domain labels. The abstraction is strong, as it identifies general patterns and concepts such as latent representations and episodic training, placing DG within the context of related paradigms like ZSL and FSL."}}
{"id": "4396c1c3-a3ad-4efa-9f10-99f90060c0b4", "title": "Adversarial Attacks", "level": "section", "subsections": ["945be913-0c94-4b59-92f5-5fa8a6f0bf30", "7e154b91-68e0-47d3-8c11-92f8eb25b769"], "parent_id": "569f1197-c628-4e9d-9db6-9757cb02d82a", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Adversarial Attacks"]], "content": "\\label{sec:adversarial_attacks}\n\\sectionauthor{Andreas Bär\\textsuperscript{15}}\nOver the last few years, deep neural networks (DNNs) consistently showed state-of-the-art performance across several vision-related tasks.\nWhile their superior performance on clean data is indisputable, they show a lack of robustness to certain input patterns, denoted as \\emph{adversarial examples} .\nIn general, an algorithm for creating adversarial examples is referred to as an \\emph{adversarial attack} and aims at fooling an underlying DNN, such that the output changes in a desired and malicious way.\nThis can be carried out without any knowledge about the DNN to be attacked (black-box attack) , or with full knowledge about the parameters, architecture, or even training data of the respective DNN (white-box attack) .\nWhile initially being applied on simple classification tasks, some approaches aim at finding more realistic attacks , which particularly pose a threat to safety-critical applications, such as DNN-based environment perception systems in autonomous vehicles.\nAltogether, this motivated the research in finding ways of defending against such adversarial attacks .\nIn this section, we introduce the current state of research regarding adversarial attacks in general, more realistic adversarial attacks closely related to the task of environment perception for autonomous driving, and strategies for detecting or defending adversarial attacks.\nWe conclude each section by clarifying current challenges and research directions.", "cites": [314, 900, 888, 6146, 8396, 892, 917, 906], "cite_extract_rate": 0.7272727272727273, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic overview of adversarial attacks and mentions related research, but it lacks in-depth synthesis of the cited works, with minimal integration of ideas beyond listing them. There is little critical analysis or evaluation of the strengths and weaknesses of the methods. The abstraction is limited, as the section primarily describes specific attack and defense techniques without generalizing to broader principles or frameworks."}}
{"id": "945be913-0c94-4b59-92f5-5fa8a6f0bf30", "title": "Adversarial Attacks and Defenses", "level": "subsection", "subsections": [], "parent_id": "4396c1c3-a3ad-4efa-9f10-99f90060c0b4", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Adversarial Attacks"], ["subsection", "Adversarial Attacks and Defenses"]], "content": "\\label{subsec:adverarial_attacks:adversarial_attacks_defenses}\n\\sectionauthor{Andreas Bär\\textsuperscript{15}, Seyed Eghbal Ghobadi\\textsuperscript{8}, Ahmed Hammam\\textsuperscript{8}}\nThe term \\emph{adversarial example} was first introduced by Szegedy \\etal .\nFrom there on, many researchers tried to find new ways of crafting adversarial examples more effectively.\nHere, the fast gradient sign method (FGSM) , DeepFool , least-likely class method (LLCM) , C\\&W , momentum iterative fast gradient sign method (MI-FGSM) , and projected gradient descent (PGD)  are a few of the most famous attacks so far.\nIn general, these attacks can be executed in an iterative fashion, where the underlying adversarial perturbation is usually bounded by some norm and is following additional optimization criteria, \\eg minimizing the number of changed pixels.\n\\par\nThe mentioned attacks can be further categorized as image-specific attacks, where for each image a new perturbation needs to be computed.\nOn the other hand, image-agnostic attacks aim at finding a perturbation, which is able to fool an underlying DNN on a set of images.\nSuch a perturbation is also referred to as a \\emph{universal adversarial perturbation} (UAP).\nHere, the respective algorithm UAP , fast feature fool (FFF) , and prior driven uncertainty approximation (PD-UA)  are a few honorable mentions.\nAlthough the creation process of a universal adversarial perturbation typically relies on a white-box setting, they show a high \\emph{transferability} across models .\nThis allows black-box attacks, where one model is used to create a universal adversarial perturbation, and another model is being attacked with the beforehand-created perturbation.\nAnother way of designing black-box attacks is to create a surrogate DNN, which mimics the respective DNN to be attacked and thus can be used in the process of adversarial example creation .\nOn the contrary, some research has been done to create completely incoherent images (based on evolutionary algorithms or gradient ascent) to fool an underlying DNN .\nDifferent from that, another line of work has been proposed to alter only some pixels in images to attack a respective model.\nHere  and  have used optimization approaches to perturb some pixels in images to produce targeted attacks, aiming at a specific class output, or non-targeted attacks, aiming at outputting a class different from the network output or the ground truth.\nThis can be extended up to finding one pixel in the image to be exclusively perturbed to generate adversarial images .\nThe authors of  proposed to train generative models to generate adversarial examples.\nGiven an input image and the target label, a generative model is trained to produce adversarial examples for DNNs.\nHowever, while the produced adversarial examples look rather unrealistic to a human, they are able to completely deceive a DNN.\n\\par\nThe existence of adversarial examples not only motivated research in finding new attacks, but also in finding \\emph{defense strategies} to effectively defend these attacks.\nEspecially for safety-critical applications, such as DNN-based environment perception for autonomous driving, the existence of adversarial examples needs to be handled accordingly.\nSimilar to adversarial attacks, one can categorize defense strategies into two types: \\emph{model-specific} defense strategies and \\emph{model-agnostic} defense strategies.\nThe former refers to defense strategies, where the model of interest is modified in certain ways.\nThe modification can be done on the architecture, training procedure, training data, or model weights.\nOn the other hand, model-agnostic defense strategies consider the model to be a black box.\nHere, only the input or the output is accessible.\nSome well-known model-specific defense strategies include adversarial training , the inclusion of robustness-oriented loss functions during training , removing adversarial patterns in features by denoising layers , and redundant teacher-student frameworks .\nThe majority of model-agnostic defense strategies primarily focuses on various kinds of (gradient masking) pre-processing strategies .\nThe idea is to remove the adversary from the respective image, such that the image is transformed from the adversarial space back into the clean space.\n\\par\nNonetheless, Athalye \\etal  showed that gradient masking alone is not a sufficient criterion for a reliable defense strategy.\nIn addition, detection and out-of-distribution techniques have also been proposed as model-agnostic defense strategies against adversarial attacks.\nHere, the Mahalanobis distance  or area under the receiver operating characteristic curve (AUROC) and area under the precision-recall curve (AUPR)  are used to detect adversarial examples.\nThe authors of  on the other hand proposed to train networks to detect, whether the input image is out-of-distribution or not. \n\\par\nMoreover, Feinman \\etal proved that adversarial attacks usually produce high uncertainty on the output of the DNN.\nAs a consequence, they proposed to use the dropout technique to estimate uncertainty on the output to identify a possible adversarial attack.\n\\par\nRegarding adversarial attacks, the majority of the listed attacks are designed for image classification.\nOnly a few adversarial attacks consider tasks that are closely related to autonomous driving, such as bounding box detection, semantic segmentation, instance segmentation, or even panoptic segmentation.\nAlso, the majority of the adversarial attacks rely on a white-box setting, which is usually not present for a potential attacker.\nEspecially universal adversarial perturbations have to be considered as a real threat due to their high model transferability.\nGenerally speaking, the existence of adversarial examples has not been thoroughly studied yet.\nAn analytical interpretation is still missing, but could help in designing more mature defense strategies.\n\\par\nRegarding defense strategies, adversarial training is still considered as one of the most effective ways of increasing the robustness of a DNN.\nNonetheless, while adversarial training is indeed effective, it is rather inefficient in terms of training time.\nIn addition, model-agnostic defenses should be favored as once being designed, they can be easily transferred to different models.\nMoreover, as most model-agnostic defense strategies rely on gradient-masking and it has been shown that gradient-masking is not a sufficient property for a defense strategy, new ways of designing model-agnostic defenses should be taken into account.\nFurthermore, out-of-distribution and adversarial attacks detection or even correction methods have been a new trend for identifying attacks.\nHowever, as the environment perception system of an autonomous driving vehicle could rely on various information sources, including LiDAR, optical flow, or depth from a stereo camera, techniques of information fusion should be further investigated to mitigate or even eliminate the effect of adversarial examples.", "cites": [923, 912, 8184, 892, 917, 908, 8185, 890, 914, 7313, 975, 314, 953, 954, 1624, 920, 8186, 6140, 919, 899, 902, 3267, 8396, 3234, 906, 6148], "cite_extract_rate": 0.6341463414634146, "origin_cites_number": 41, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes a range of adversarial attack and defense techniques, effectively grouping them into coherent categories like image-specific vs. image-agnostic attacks and model-specific vs. model-agnostic defenses. It critically evaluates the limitations of certain approaches, such as the insufficiency of gradient masking and the inefficiency of adversarial training. Additionally, it abstracts key concepts (e.g., transferability, robustness, and black-box challenges) to highlight broader trends and opportunities in AI safety for autonomous systems."}}
{"id": "7e154b91-68e0-47d3-8c11-92f8eb25b769", "title": "More Realistic Attacks", "level": "subsection", "subsections": [], "parent_id": "4396c1c3-a3ad-4efa-9f10-99f90060c0b4", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Adversarial Attacks"], ["subsection", "More Realistic Attacks"]], "content": "\\label{subsec:adverarial_attacks:more_realistic_attacks}\n\\sectionauthor{Svetlana Pavlitskaya\\textsuperscript{14}}\nWe consider the following two categories of realistic adversarial attacks: (1) image-level attacks, which not only fool a neural network but also pose a provable threat to autonomous vehicles, and (2) attacks which have been applied in a real world or in a simulation environment, such as car learning to act (CARLA) .\n\\par\nSome notable examples in the first category of attacks include attacks on semantic segmentation  or person detection .\n\\par\nIn the second group of approaches, the attacks are specifically designed to survive real world distortions, including different distances, weather and lighting conditions, as well as camera angles. For this, adversarial perturbations are usually concentrated in a specific image area, called \\emph{adversarial patch}. Crafting an adversarial patch involves specifying a patch region in each training image, applying transformations to the patch, and iteratively changing the pixel values within this region to maximize the network prediction error. The latter step typically relies on an algorithm, proposed for standard adversarial attacks, which aim at crafting invisible perturbations while misleading neural networks, \\eg C\\&W , Jacobian-based saliency map attack (JSMA) , and PGD .\n\\par\nThe first printable adversarial patch for image classification was described by Brown \\etal . Expectation over transformations (EOT)  is one of the influential updates to the original algorithm---it permits to robustify patch-based attacks to distortions and affine transformations. Localized and visible adversarial noise (LaVAN)  is a further method to generate much smaller patches (up to 2\\% of the pixels in the image). In general, fooling image classification with a patch is a comparatively simple task, because adversarial noise can mimic an instance of another class and thus lower the prediction probability for a true class.\n\\par\nRecently, patch-based attacks for a more tricky task of object detection have been described . Also, Lee and Kolter  generate a patch using PGD , followed by EOT applied to the patch. With this approach, all detections in an image can be successfully suppressed, even without any overlap of a patch with bounding boxes. Furthermore, several approaches for generating an adversarial T-shirt have been proposed, including .\n\\par\nDeepBillboard  is the first attempt to attack end-to-end driving models with adversarial patches. The authors propose to generate a single patch for a sequence of input images to mislead four steering models, including DAVE-2 in a drive-by scenario. \n\\par\nApart from physical feasibility, inconspicuousness is crucial for a realistic attack. Whereas adversarial patches usually look like regions of noise, several works have explored attacks with an inconspicuous patch. In particular, Eykholt \\etal demonstrate the vulnerability of road sign classification to the adversarial perturbations in the form of only black and white stickers. In , an end-to-end driving model is attacked in CARLA by  painting  of  black lines on the road. Also, Kong and Liu  use a generative adversarial network to get a realistic billboard to attack an end-to-end driving model in a drive-by scenario. In , a method to hide visible adversarial perturbations with customized styles is proposed, which leads to adversarial traffic signs that look unsuspicious to a human.\n\\par\nCurrent research mostly focuses on attacking image-based perception of an autonomous vehicle. Adversarial vulnerability of further components of an autonomous vehicle, \\eg LiDAR-based perception, optical flow and depth estimation, has only recently gained attention. Furthermore, most attacks consider only a single component of an autonomous driving pipeline, the question whether the existing attacks are able to propagate to further pipeline stages has not been studied yet. The first work in this direction  describes an attack on object detection and tracking. The evaluation is, however, limited to a few clips, where no experiments in the real world have been performed. Overall, the research on realistic adversarial attacks, especially combined with physical tests, is currently in the starting phase.", "cites": [900, 7698, 8187, 917, 890, 8189, 904, 901, 8188, 8190, 899, 7308, 6014], "cite_extract_rate": 0.6842105263157895, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers on realistic adversarial attacks, grouping them into meaningful categories like image-level attacks and physically inconspicuous patches. It provides some critical analysis by highlighting limitations such as lack of real-world validation and narrow focus on image-based perception. While it identifies patterns (e.g., the shift towards physical feasibility and inconspicuousness), the abstraction remains at a moderate level without proposing a deeper theoretical or meta-level framework."}}
{"id": "e47014ad-1aec-4b32-9241-6aab90a7f0dd", "title": "Interpretability", "level": "section", "subsections": ["cf04c28f-300a-4cf5-bc98-3f4d9734078e", "a3e2cd79-8634-4ce7-9519-acd205ba8d1d", "110c6df1-6598-4052-8ee0-6595325dfebc", "37f2baca-d82b-4285-9422-b80c37554845"], "parent_id": "569f1197-c628-4e9d-9db6-9757cb02d82a", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Interpretability"]], "content": "\\label{sec:interpretability}\n\\sectionauthor{Felix Brockherde\\textsuperscript{10}}\nNeural networks are, by their nature, black boxes and therefore intrinsically hard to interpret . Due to their unrivaled performance, they still remain first choice for advanced systems even in many safety-critical areas, such as level 4 automated driving. This is why the research community has invested considerable effort to unhinge the black-box character and make deep neural networks more transparent.\nWe can observe three strategies that provide different view points towards this goal in the state of the art. First is the most direct approach of opening up the black box and looking at intermediate representations (\\refsec{subsec:interpretability:intermediate_representations}). Being able to interpret individual layers of the system facilitates interpretation of the whole. The second approach tries to provide interpretability by explaining the network's decisions with pixel attributions (\\refsec{subsec:interpretability:pixel_attribution}). Aggregated explanations of decision can then lead to interpretability of the system itself. Third is the idea of approximating the network with interpretable proxies to benefit from the deep neural networks performance while allowing interpretation via surrogate models (\\refsec{subsec:interpretability:interpretable_proxies}). Underlying all aspects here is the area of visual analytics (\\refsec{subsec:interpretability:visual_analytics}).\nThere exists earlier research in the medical domain to help human experts understand and convince them of machine learning decisions . Legal requirements in the finance industry gave rise to interpretable systems that can justify their decisions. An additional driver for interpretability research was the concern for Clever Hans predictors .", "cites": [7514], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic overview of interpretability strategies in AI safety, mentioning three approaches and briefly referencing relevant domains and a specific concern (Clever Hans predictors). However, it lacks synthesis by not connecting the cited paper's contributions to broader themes, offers minimal critical analysis of the methods or limitations, and provides only surface-level abstraction without identifying deeper patterns or principles."}}
{"id": "cf04c28f-300a-4cf5-bc98-3f4d9734078e", "title": "Visual Analytics", "level": "subsection", "subsections": [], "parent_id": "e47014ad-1aec-4b32-9241-6aab90a7f0dd", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Interpretability"], ["subsection", "Visual Analytics"]], "content": "\\label{subsec:interpretability:visual_analytics}\n\\sectionauthor{Elena Schulz\\textsuperscript{1}}\nTraditional data science has developed a huge tool set of automated analysis processes conducted by computers, which are applied to problems that are well-defined in the sense that the dimensionality of input and output as well as the size of the data set they rely on is manageable. \nFor those problems that in comparison are more complex, the automation of the analysis process is limited and/or might not lead to the desired outcome. \nThis is especially the case with unstructured data like image or video data in which the underlying information cannot directly be expressed by numbers. \nRather, it needs to be transformed to some structured form to enable computers to perform some task of analysis. \nAdditionally, with an ever increasing amount of various types of data being collected, this ``information overload'' cannot solely be analyzed by automatic methods . \n\\par\n\\emph{Visual analytics} addresses this challenge as ``the science of analytical reasoning facilitated by interactive visual interfaces'' . \nVisual analytics therefore does not only focus on either computationally processing data or visualizing results but coupling both tightly with interactive techniques.  \nThus, it enables an integration of the human expert into the iterative visual analytics process: Through visual understanding and human reasoning, the knowledge of the human expert can be incorporated to effectively refine the analysis. This is of particular importance, where a stringent safety argumentation for complex models is required. With the help of visual analytics, the line of argumentation can be built upon arguments that are understandable for humans.\nTo include the human analyst efficiently into this process, a possible guideline is the \\emph{visual analytics mantra} by Keim: ``Analyze first, show the important, zoom, filter and analyze further, details on demand'' \\footnote{Extending the original \\emph{visualization mantra} by Shneiderman ``Overview first, filter and zoom, details on demand''.}.\n\\par\nThe core concepts of visual analytics therefore rely on well-designed interactive visualizations, which support the analyst in the tasks of, \\eg reviewing, understanding, comparing and inferring not only the initial phenomenon or data but also the computational model and its results itself with the goal of enhancing the analytical process. \n\\par\nDriven by various fields of application, visual analytics is a multidisciplinary field with a wide variety of task-oriented development and research. \nAs follows, recent work has been done in several areas: \ndepending on the task, there exist different pipeline approaches to create whole \\emph{visual analytics systems} ; \nthe injection of human expert knowledge into the process of determining trends and patterns from data is the focus of \\emph{predictive visual analytics} ; \nenabling the human to explore \\emph{high-dimensional data}  interactively and visually (\\eg via dimensionality reduction ) is a major technique  to enhance the understandability of complex models (\\eg neural networks); \nthe iterative improvement and the understanding of machine learning models is addressed by using interactive visualizations in the field of \\emph{general machine learning}  or the other way round: using machine learning to improve visualizations and guidance based on user interactions .\nEven more focused on the loop of simultaneously developing and refining machine learning models is the area of \\emph{interactive machine learning}, where the topics of \\emph{interface design}  and the \\emph{importance of users}  are discussed. One of the current research directions is using visual analytics in the area of \\emph{deep learning} . However, due to the interdisciplinarity of visual analytics, there are still open directions and ongoing research opportunities. \n\\par\nEspecially in the domain of neural networks and deep learning, visual analytics is a relatively new approach in tackling the challenge of \\emph{explainability} and \\emph{interpretability} of those often called \\emph{black boxes}. \nTo enable the human to better interact with the models, research is done in enhancing the \\emph{understandability} of complex deep learning models and their outputs with the use of proper visualizations. \nOther research directions attempt to achieve improving the \\emph{trustability} of the models, giving the opportunity to inspect, diagnose and refine the model. \nFurther, possible areas for research are \\emph{online training processes} and the development of \\emph{interactive systems} covering the whole process of training, enhancing and monitoring machine learning models. \nHere, the approach of \\emph{mixed guidance}, where system-initiated guidance is combined with user-initiated guidance, is discussed among the visual analytics community as well. \nAnother challenge and open question is creating ways of \\emph{comparing models} to examine which model yields a better performance, given specific situations and selecting or combining the best models with the goal of increasing performance and overall safety.", "cites": [8192, 8191, 8193], "cite_extract_rate": 0.17647058823529413, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the cited papers by weaving their ideas into a broader discussion of visual analytics in AI safety. It connects concepts like model understandability, trustability, and interactive systems to create a coherent narrative. While it does not deeply critique each paper, it identifies general limitations and ongoing research directions, showing some level of critical evaluation. The abstraction is strong, as it highlights overarching principles such as the role of human-in-the-loop systems and mixed guidance in enhancing model transparency and safety."}}
{"id": "a3e2cd79-8634-4ce7-9519-acd205ba8d1d", "title": "Intermediate Representations", "level": "subsection", "subsections": [], "parent_id": "e47014ad-1aec-4b32-9241-6aab90a7f0dd", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Interpretability"], ["subsection", "Intermediate Representations"]], "content": "\\label{subsec:interpretability:intermediate_representations}\n\\sectionauthor{Felix Hauser\\textsuperscript{11}, Jan Kronenberger\\textsuperscript{9}, Seyed Eghbal Ghobadi\\textsuperscript{8}}\nIn general, representation learning  aims to extract lower dimensional features in latent space from higher dimensional inputs.\nThese features are then used as an effective representation for regression, classification, object detection and other machine learning tasks.\nPreferably, latent features should be disentangled, meaning that they represent separate factors found in the data that are statistically independent.\nDue to their importance in machine learning, finding meaningful intermediate representations has long been a primary research goal.\nDisentangled representations can be interpreted more easily by humans and \ncan for example be used to explain the reasoning of neural networks .\nAmong the longer known methods for extracting disentangled representations are principal component analysis (PCA) , independent component analysis , and nonnegative matrix factorization .\nPCA is highly sensitive to outliers and noise in the data.\nTherefore, more robust algorithms were proposed.\nIn  already a small neural network was used as an encoder and the algorithm proposed in  can deal with high-dimensional data.\nSome robust PCA algorithms are provided with analytical performance guarantees .\nA popular method for representation learning with deep networks is the variational autoencoder (VAE) .\nAn important generalization of the method is the $ \\beta $-VAE variant , which improved the disentanglement capability .\nLater analysis added to the theoretical understanding of $ \\beta $-VAE\n.\nCompared to standard autoencoders, VAEs map inputs to a distribution, instead of mapping them to a fixed vector.\nThis allows for additional regularization of the training to avoid overfitting and ensure good representations.\nIn $ \\beta $-VAEs the trade-off between reconstruction quality and disentanglement can be fine-tuned by the hyperparameter $ \\beta $.\nDifferent regularization schemes have been suggested to improve the VAE method.\nAmong them are Wasserstein autoencoders , attribute regularization  and relational regularization .\nRecently, a connection between VAEs and nonlinear independent component analysis was established  and then expanded .\nBesides VAEs, deep generative adversarial networks can be used to construct latent features .\nOther works suggest centroid encoders  or conditional learning of Gaussian distributions  as alternatives to VAEs.\nIn  concept activation vectors are defined as being orthogonal to the decision boundary of a classifier.\nApart from deep learning, entirely new architectures, such as capsule networks , might be used to disassemble inputs. \n\\par\nWhile many different approaches for disentangling exist, the feasibility of the task is not clear yet and a better theoretical understanding is needed.\nThe disentangling performance is hard to quantify, which is only feasible with information about the latent ground truth .\nModels that overly rely on single directions, single neurons in fully connected networks or single feature maps in CNNs, have the tendency to overfit .\nAccording to , unsupervised learning does not produce good disentangling and even small latent spaces do not reduce the sample complexity for simple tasks.\nThis is in direct contrast to newer findings that show a decreased sample complexity for more complex visual downstream tasks .\nSo far, it is unclear if disentangling improves the performance of machine learning tasks.\nIn order to be interpretable, latent disentangled representations need to be aligned with human understandable concepts.\nIn  training with adversarial examples was used and the learned representations were shown to be more aligned with human perception.\nFor explainable AI, disentangling alone might not be enough to generate interpretable output and additional regularization could be needed.", "cites": [1002, 8198, 6410, 8201, 8197, 8200, 8194, 8199, 5680, 3314, 4423, 8202, 2709, 8196, 306, 318, 8195, 1256], "cite_extract_rate": 0.5142857142857142, "origin_cites_number": 35, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes various methods for disentangled representation learning, connecting concepts from PCA, VAEs, GANs, and newer frameworks like Wasserstein autoencoders and capsule networks. It also critically discusses limitations, such as the difficulty of quantifying disentanglement and the challenges of unsupervised learning. While it identifies broader patterns in regularization techniques and interpretability goals, it does not fully abstract to a meta-level framework for understanding representation learning in AI safety."}}
{"id": "110c6df1-6598-4052-8ee0-6595325dfebc", "title": "Pixel Attribution", "level": "subsection", "subsections": [], "parent_id": "e47014ad-1aec-4b32-9241-6aab90a7f0dd", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Interpretability"], ["subsection", "Pixel Attribution"]], "content": "\\label{subsec:interpretability:pixel_attribution}\n\\sectionauthor{Stephanie Abrecht\\textsuperscript{2}, Felix Brockherde\\textsuperscript{10}, Toshika Srivastava\\textsuperscript{12}}\nThe non-linearity and complexity of DNNs allow them to solve perception problems, like detecting a pedestrian, that cannot be specified in detail.  At the same time, the automatic extraction of features given in an input image and the mapping to the respective prediction is counterintuitive and incomprehensible for humans, which makes it hard to argue safety for a neural network-based perception task. Feature importance techniques are currently predominantly used to diagnose the causes of incorrect model behaviors . So-called \\emph{attribution maps} are a visual technique to express the relationship between relevant pixels in the input image and the network's prediction. Regions in an image that contain relevant features are highlighted accordingly. \nAttribution approaches mostly map to one of three categories.\n\\par\nGradient-based and activation-based approaches (such as  amongst others) rely on the gradient of the prediction with respect to the input. Regions that were most relevant for the prediction are highlighted. Activation-based approaches relate the feature maps of the last convolutional layer to output classes. \n\\par\nPerturbation-based approaches  suggest manipulating the input. If the prediction changes significantly, the input may hold a possible explanation at least.\n\\par\nWhile gradient-based approaches are oftentimes faster in computation, perturbation-based approaches are much easier to interpret.\n\\par\nAs many studies have shown , there is still a lot of research to be done before attribution methods are able to robustly provide explanations for model predictions, in particular erroneous behavior. One key difficulty is the lack of an agreed-upon definition of a good attribution map including important properties. Even between humans, it is hard to agree on what a good explanation is due to its subjective nature. This lack of ground truth makes it hard or even impossible to quantitatively evaluate an explanation method. Instead, this evaluation is done only implicitly. \nOne typical way to do this is the axiomatic approach. Here a set of desiderata of an attribution method are defined, on which different attribution methods are then evaluated. Alternatively, different attribution methods may be compared by perturbing the input features starting with the ones deemed most important and measuring the drop in accuracy of the perturbed models. The best method will result into the greatest overall loss in accuracy as the number of inputs are omitted .\nMoreover, for gradient-based methods it is hard to assess if an unexpected attribution is caused by a poorly performing network or a poorly performing attribution method . How to cope with negative evidence, \\ie the object was predicted because a contrary clue in the input image was missing, is an open research question. Additionally, most methods were shown on classification tasks until now. It remains to be seen how they can be transferred to object detection and semantic segmentation tasks. In the case of perturbation-based methods, the high computation time and single-image analysis inhibit wide-spread application.", "cites": [4875, 499, 8600, 1812, 1822, 1824, 1814, 7517, 1617, 6080, 8203], "cite_extract_rate": 0.6875, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes information from multiple papers to categorize and analyze pixel attribution methods, including gradient-based, activation-based, and perturbation-based approaches. It critically discusses the limitations of these methods, such as the lack of a standard definition for good attribution, subjective evaluation challenges, and open questions in handling negative evidence. While it identifies broader trends in the field, such as the need for robust evaluation and method generalization, its abstraction is slightly constrained by a focus on specific types of tasks (e.g., classification, object detection)."}}
{"id": "37f2baca-d82b-4285-9422-b80c37554845", "title": "Interpretable Proxies", "level": "subsection", "subsections": [], "parent_id": "e47014ad-1aec-4b32-9241-6aab90a7f0dd", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Interpretability"], ["subsection", "Interpretable Proxies"]], "content": "\\label{subsec:interpretability:interpretable_proxies}\n\\sectionauthor{Gesina Schwalbe\\textsuperscript{3}}\nNeural networks are capable of capturing complicated logical\n(cor)relations. However, this knowledge is encoded on a\n\\emph{sub-symbolic} level in the form of learned weights and biases,\nmeaning that the reasoning behind the processing chain cannot be\ndirectly read out or interpreted by humans .\nTo explain the sub-symbolic processing, one can either use attribution\nmethods (\\cf \\refsec{subsec:interpretability:pixel_attribution}), or lift\nthis sub-symbolic representation to a \\emph{symbolic} one\n, meaning a more interpretable one.\nInterpretable proxies or surrogate models try to achieve the latter:\nThe DNN behavior is approximated by a model that\nuses symbolic knowledge representations.\nSymbolic representations can be\nlinear models like LIME  (proportionality),\ndecision trees (if-then-chains) ,\nor loose sets of logical rules.\nLogical connectors can simply be AND and OR but also more general\nones like at-least-M-of-N .\nThe \\emph{expressiveness} of an approach refers to the logic that is\nused: Boolean-only versus first-order logic, and binary versus fuzzy\nlogic truth values .\nOther than attribution methods (\\cf \\refsec{subsec:interpretability:pixel_attribution}), these\nrepresentations can capture combinations of features and (spatial)\nrelations of objects and attributes. As an example consider\n\\enquote{eyes are closed} as explanation for \\enquote{person asleep}:\nAttribution methods only could mark the location of the eyes, dismissing\nthe relations of the attributes .\nAll mentioned surrogate model types (linear, set of rules) require\ninterpretable input features in order to be interpretable themselves.\nThese features must either be directly obtained from the DNN input or\n(intermediate) output, or automatically be extracted from the DNN\nrepresentation.\nExamples for extraction are the super-pixeling used in LIME for input\nfeature detection, or concept activation vectors\n for DNN representation decoding.\n\\par\nQuality criteria and goals for interpretable proxies are :\n\\emph{accuracy} of the standalone surrogate model on unseen examples,\n\\emph{fidelity} of the approximation by the proxy,\n\\emph{consistency} with respect to different training sessions, and\n\\emph{comprehensibility} measured by the complexity of the rule set\n(number of rules, number of hierarchical dependencies).\nThe criteria are usually in conflict and need to be balanced:\nBetter accuracy may require a more complex, thus less expressive sets of rules.\n\\par\nApproaches for interpretable proxies differ in the validity range of\nthe representations:\nSome aim for surrogates that are only valid \\emph{locally} around\nspecific samples, like in LIME~ or in\n via inductive logic programming.\nOther approaches try to more \\emph{globally} approximate aspects of\nthe model behavior.\nAnother categorization is defined by whether full access\n(\\emph{white-box}), some access (\\emph{gray-box}), or no access\n(\\emph{black-box}) to the DNN internals is needed.\nOne can further differentiate between \\emph{post-hoc} approaches that\nare applied to a trained model, and approaches that try to integrate\nor \\emph{enforce symbolic representations} during training.\nPost-hoc methods cover the wide field of rule extraction techniques\nfor DNNs. The reader may refer to\n.\nMost white- and gray-box methods try to turn the DNN connections into\nif-then rules that are then simplified, like done in\nDeepRED~.\nA black-box example is validity interval\nanalysis~, which refines or\ngeneralizes rules on input intervals, either starting from one sample\nor a general set of rules.\nEnforcement of symbolic representations can be achieved by enforcing\nan output structure that provides insights to the decision logic, such as\ntextual explanations, or a rich output structure allowing investigation of correlations\n.\nAn older discipline for enforcing symbolic representations\nis the field of neural-symbolic learning\n. The idea is based on a hybrid learning\ncycle \nin which a symbolic learner and a DNN iteratively update each other via\nrule insertion and extraction.\n\\par\nThe comprehensibility of global surrogate models suffers from the\ncomplexity and size of concurrent DNNs. Thus, stronger rule\nsimplification methods are required .\nThe alternative direction of local approximations mostly\nconcentrates on linear models instead of more expressive rules .\nFurthermore, balancing of the quality objectives is hard since\navailable indicators for interpretability may not be ideal.\nAnd lastly, applicability is heavily infringed by the requirement of\ninterpretable input features. These are usually not readily available\nfrom input (often pixel-level) or DNN output. Supervised extraction\napproaches vary in their fidelity, and unsupervised ones do not\nguarantee to yield meaningful or interpretable results, respectively, such\nas the super-pixel clusters of LIME.", "cites": [8502, 7507, 1821, 8204], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by integrating concepts from multiple papers (e.g., LIME, rule extraction algorithms) to present a coherent framework for interpretable proxies. It critically evaluates the trade-offs between accuracy, fidelity, and comprehensibility, and discusses limitations such as the reliance on interpretable features. The section also abstracts beyond specific methods to highlight broader patterns, including distinctions between local and global approximations, access levels (white-box, gray-box, black-box), and the evolution of neural-symbolic learning."}}
{"id": "9f13c349-4fb4-458e-8f34-84cd3a93fca0", "title": "Uncertainty", "level": "section", "subsections": ["e3a9977a-3391-4691-bc3b-9d131807d8dd", "d4df1917-2df5-4888-9dac-86e261e7d3d8", "329b292f-42f5-496b-9775-5b5a3bd0f517", "dcb846f5-368c-45a9-a56f-6ddb586a0a6b", "81f04d24-1455-4244-a481-8a943c0061e4", "09524fc1-6625-4adb-b57e-b6186b5d4ef0"], "parent_id": "569f1197-c628-4e9d-9db6-9757cb02d82a", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Uncertainty"]], "content": "\\label{sec:uncertainty} \n\\sectionauthor{Michael Mock\\textsuperscript{1}}\nUncertainty refers to the view that a neural network is not conceived as a deterministic function but as a probabilistic function or estimator, delivering a random distribution for each input point. Ideally, the mean value of the distribution should  be as close as possible to the ground truth value of the function being approximated by the neural network and the uncertainty of the neural network refers to its variance when being considered as a random variable, thus allowing to derive a confidence with respect to the mean value.  Regarding safety, the variance may lead to estimations about the confidence associated with a specific network output and opens the option for discarding network outputs with insufficient confidence. \n\\par\nThere are roughly two broad approaches for training neural networks as probabilistic functions: Parametric approaches  and Bayesian neural networks on the one hand, such as , where the transitions along the network edges are modeled as probability distributions, and ensemble-based approaches on the other hand , where multiple networks are trained and considered as samples of a common output distribution. Apart from training as probabilistic function, uncertainty measures have been derived from single, standard neural networks by post-processing on the trained network logits, leading for example to calibration measures (\\cf \\eg ).", "cites": [4616, 4025, 3288], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of uncertainty approaches in neural networks by grouping them into parametric/Bayesian and ensemble-based methods. It mentions the cited papers but does so in a descriptive manner without deep analysis or comparison. There is minimal abstraction beyond the specific methods, and no significant critique or evaluation of their limitations or trade-offs."}}
{"id": "e3a9977a-3391-4691-bc3b-9d131807d8dd", "title": "Generative Models", "level": "subsection", "subsections": [], "parent_id": "9f13c349-4fb4-458e-8f34-84cd3a93fca0", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Uncertainty"], ["subsection", "Generative Models"]], "content": "\\label{subsec:uncertainty:generative_models}\n\\sectionauthor{Sebastian Wirkert\\textsuperscript{6}, Tim Wirtz\\textsuperscript{1}}\n\\emph{Generative models} belong to the class of unsupervised machine learning models. From a theoretical perspective, these are particularly interesting, because they offer a way to analyze and model the density of data. Given a finite data set $\\mathcal{D}$ independently distributed according to some distribution $p(x)$, generative models aim to estimate or enable sampling from the underlying density $p(x)$ in a model $q_\\theta(x)$. The resulting model can be used for data indexing~, data retrieval~, for visual recognition~, speech recognition and generation~, language processing~ and robotics~.\nFollowing , we can group generative models into two main classes:\n\\begin{itemize}\n    \\item Cost function-based models such as autoencoder~, deep belief networks~ and generative adversarial networks~.\n    \\item Energy-based models~, where the joint probability density is modeled by an energy function.\n\\end{itemize}\nBeside these \\emph{deep learning} approaches, generative models have been studied in machine learning in general for quite some time (\\cf ). A very prominent example of generative networks are Gaussian processes~ and their deep learning extensions~ as generative models. \n\\par\nAn example of a generative model being employed for image segmentation uncertainty estimation is the probabilistic U-Net . Here a variational autoencoder (VAE) conditioned on the image is trained to model uncertainties. Samples from the VAE are fed into a segmentation U-Net which can thus give different results for the same image. This was tested in context of medical images, where inter-rater disagreements lead to uncertain segmentation results and Cityscapes segmentation. For the Cityscapes segmentation the investigated use case was label ambiguity (\\eg is a BMW X7 a car or a van) using artificially created, controlled ambiguities. Results showed that the probabilistic U-Net could reproduce the segmentation ambiguity modes more reliably than competing methods such as a dropout U-Net which is based on techniques elaborated in the next section.", "cites": [1003, 5680, 8430, 8207, 8206, 8205, 8208], "cite_extract_rate": 0.23333333333333334, "origin_cites_number": 30, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from multiple papers on generative models, connecting them to the broader theme of AI safety through uncertainty estimation in image segmentation. It includes a specific analytical example (probabilistic U-Net) and compares its performance to a competing method (dropout U-Net), indicating a basic level of critical evaluation. However, the synthesis remains somewhat focused on categorization rather than a novel or deeply integrative framework, and the abstraction could benefit from more meta-level insights into generative modeling paradigms."}}
{"id": "d4df1917-2df5-4888-9dac-86e261e7d3d8", "title": "Monte-Carlo Dropout", "level": "subsection", "subsections": [], "parent_id": "9f13c349-4fb4-458e-8f34-84cd3a93fca0", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Uncertainty"], ["subsection", "Monte-Carlo Dropout"]], "content": "\\label{subsec:uncertainty:mc_dropout} \n\\sectionauthor{Joachim Sicking\\textsuperscript{1}}\nA widely used technique to estimate model uncertainty is Monte-Carlo (MC) dropout , that offers a Bayesian motivation, conceptual simplicity and scalability to application-size networks. This combination distinguishes MC dropout from competing Bayesian neural network (BNN) approximations (like ,, see \\refsec{subsec:uncertainty:bayesian_neural_networks}). However, these approaches and MC dropout share the same goal: to equip neural networks with a \\emph{self-assessment} mechanism that detects unknown input concepts and thus potential model insufficiencies.\nOn a technical level, MC dropout assumes prior distributions on network activations, usually independent and identically distributed (i.i.d.) Bernoulli distributions. Model training with iteratively drawn Bernoulli samples, the so-called \\emph{dropout masks}, then yields a data-conditioned posterior distribution within the chosen parametric family. It is interesting to note that this training scheme was used earlier---independent from an uncertainty context---for better model generalization . At inference, sampling provides estimates of the input-dependent output distributions. The spread of these distributions is then interpreted as the prediction uncertainty that originates from limited knowledge of model parameters. Borrowing ‘frequentist’ terms, MC dropout can be considered as an implicit network ensemble, \\ie as a set of networks that share (most of) their parameters.\nIn practice, MC dropout requires only a minor modification of the optimization objective during training and multiple, trivially parallelizable forward passes during inference. The loss modification is largely agnostic to network architecture and does not cause substantial overhead. This is in contrast to the sampling-based inference that increases the computational effort massively---by estimated factors of 20-100 compared to networks without MC dropout. A common practice is therefore the use of \\emph{last-layer dropout}  that reduces computational overhead to estimated factors of 2-10. Alternatively, analytical moment propagation allows sampling-free MC-dropout inference at the price of additional approximations (\\eg). Further extensions of MC dropout target the integration of data-inherent (aleatoric) uncertainty  and tuned performance by learning layer-specific dropout rate using concrete relaxations .\nThe quality of MC-dropout uncertainties is typically evaluated using negative log-likelihood (NLL), expected calibration error (ECE) and its variants (\\cf \\refsec{subsec:uncertainty:confidence_calibration}) and by considering correlations between uncertainty estimates and model errors (\\eg AUSE ). Moreover, it is common to study how useful uncertainty estimates are for solving auxiliary tasks like out-of-distribution classification  or robustness \\wrt adversarial attacks. \nMC dropout is a working horse of safe ML, being used with various networks and for a multitude of applications (\\eg). However, several authors pointed out shortcomings and limitations of the method: MC dropout bears the risk of over-confident false predictions (), offers less diverse uncertainty estimates compared to (the equally simple and scalable) deep ensembles (, see \\refsec{subsec:aggregation:ensemble_methods}) and provides only rudimentary approximations of true posteriors. \nRelaxing these modelling assumptions and strengthening the Bayesian motivation of MC dropout is therefore an important research avenue. Further directions for future work are the development of \\emph{semantic uncertainty mechanisms} (\\eg ), improved local uncertainty calibrations and a better understanding of the outlined sampling-free schemes to uncertainty estimation.", "cites": [4616, 3288, 4640, 8205, 4025, 8209, 8814], "cite_extract_rate": 0.5384615384615384, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple cited papers to present MC dropout as a practical Bayesian-inspired approach for uncertainty estimation, comparing it with alternatives like BNNs and deep ensembles. It critically evaluates its limitations, such as over-confidence and poor posterior approximation, and highlights directions for improvement. The discussion abstracts beyond individual papers to identify broader themes like scalability, calibration, and the trade-offs inherent in uncertainty modeling."}}
{"id": "329b292f-42f5-496b-9775-5b5a3bd0f517", "title": "Bayesian Neural Networks", "level": "subsection", "subsections": [], "parent_id": "9f13c349-4fb4-458e-8f34-84cd3a93fca0", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Uncertainty"], ["subsection", "Bayesian Neural Networks"]], "content": "\\label{subsec:uncertainty:bayesian_neural_networks}\n\\sectionauthor{Maram Akila\\textsuperscript{1}}\nAs the name suggests, Bayesian neural networks (BNNs) are inspired by a Bayesian interpretation of probability (for an introduction \\cf ).\nIn essence, it rests on Bayes' theorem,\n\\begin{equation}\n\tp(x|y) p(y) = p(x,y) = p(y | x) p(x)\n\t\\quad \\Rightarrow \\quad\n\tp(x| y) = \\frac{ p(y | x) p(x)}{ p(y) }\n\t\\,,\n\t\\label{eq:bnn:bayes}\n\\end{equation}\nstating that the conditional probability density function (PDF) $p(x|y)$ for $x$ given $y$ may be expressed in terms of the inverted conditional PDF $p(y | x)$.\nFor machine learning, where one intends to make predictions $y$ for unknown $x$ given some training data $\\mathcal{D}$, this can be reformulated into\n\\begin{equation}\n\ty = \\operatorname{NN}(x|W)\n\t\\quad\\text{with}\\quad\n\t p(W | \\mathcal{D}) = \\frac{p(\\mathcal{D} | W) p(W)}{p(\\mathcal{D})}\n\t\\,.\n\t\\label{eq:bnn:NNpdf}\n\\end{equation}\nTherein NN denotes a conventional (deep) neural network (DNN) with model parameters $W$, \\eg the set of weights and biases.\nIn contrast to a regular DNN, the weights are given in terms of a probability distribution $ p(W | \\mathcal{D}) $ turning also the output $y$ of a BNN into a distribution.\nThis allows to study the mean $\\mu=\\langle y^1 \\rangle$ of the DNN for a given $x$ as well as higher moments of the distribution, typically the resulting variance $\\sigma^2 = \\langle (y-\\mu)^2 \\rangle$ is of interest, where\n\\begin{equation}\n\t\\left\\langle y^k \\right\\rangle = \\int\\!\n\t\\operatorname{NN}(x|W)^k p(W|\\mathcal{D})\\,\\mathrm{d}W\\,.\n\t\\label{eq:bnn:moments}\n\\end{equation}\nWhile $\\mu$ yields the output of the network, the variance $\\sigma^2$ is a measure for the uncertainty of the model for the prediction at the given point.\nCentral to this approach is the probability of the data given the model, here denoted by $p(\\mathcal{D} | W) $, as it is the key component connecting model and training data.\nTypically, the prior distribution $p(\\mathcal{D})$ is ``ignored'' as it only appears as a normalization constant within the averages, see \\eqref{eq:bnn:moments}.\nIn the cases where the data $\\mathcal{D}$ is itself a distribution due to inherent uncertainty, \\ie presence of aleatoric risk , such a concept seems natural.\nHowever, Bayesian approaches are also applicable for all other cases.\nIn those, loosely speaking, the likelihood of $W$ is determined via the chosen loss function (for the connection between the two concepts \\cf ).\n\\par\nOn this general level, Bayesian approaches are broadly accepted and also find use for many other model classes besides neural networks.\nHowever, the loss surfaces of DNNs are known for their high dimensionality and strong non-convexity.\nTypically, there are abundant parameter combinations $W$ that lead to (almost) equally good approximations to the training data $\\mathcal{D}$ with respect to a chosen loss.\nThis makes an evaluation of $p(W|\\mathcal{D})$ for DNNs close to impossible in full generality.\nAt least no (exact) solutions for this case exist at the moment.\n\\par\nFinding suitable approximations to the posterior distribution $p(W|\\mathcal{D})$  is an ongoing challenge for the construction of BNNs.\nAt this point we only summarize two major research directions in the field.\nOne approach is to assume that the distribution factorizes.\nWhile the full solution would be a joined distribution implying correlations between different weights (etc.), possibly even across layers, this approximation takes each element $w_i$ of $W$ to be independent form the others.\nAlthough this is a strong assumption, it is often made, in this case  parameters for the respective distributions of each element can be learned via training (\\cf ).\nThe second class of approaches focuses on the region of the loss surface around the minimum chosen for the DNN.\nAs discussed, the loss relates to the likelihood and quantities such as the curvature at the minimum, therefore directly connected to the distribution of $W$.\nUnfortunately, already using this type of quantities requires further approximations .\nAlternatively, the convergence of the training process may be altered to sample networks close to the minimum .\nWhile this approach contains information about correlations among the $w_i$, it is usually restricted to a specific minimum.\nFor a non-Bayesian ansatz taking into account several minima, see deep ensembles in \\refsec{subsec:aggregation:ensemble_methods}.\nBNNs also touch other concepts such as MC dropout (\\cf \\refsec{subsec:uncertainty:mc_dropout} or ), or prior networks, which are based on a Bayesian interpretation but use conventional DNNs with an additional (learned) $\\sigma$ output .", "cites": [8633, 4025], "cite_extract_rate": 0.25, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of Bayesian Neural Networks (BNNs), integrating foundational concepts with practical challenges. It synthesizes the key idea from both cited papers (Bayes by Backprop and prior networks) by highlighting the Bayesian framework and its application to DNNs. While it connects these concepts and discusses broader issues like loss surface complexity, it lacks deeper critical evaluation or a novel framework that would elevate the synthesis and abstraction to a higher level."}}
{"id": "dcb846f5-368c-45a9-a56f-6ddb586a0a6b", "title": "Uncertainty Metrics for DNNs in Frequentist Inference", "level": "subsection", "subsections": [], "parent_id": "9f13c349-4fb4-458e-8f34-84cd3a93fca0", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Uncertainty"], ["subsection", "Uncertainty Metrics for DNNs in Frequentist Inference"]], "content": "\\label{subsec:uncertainty:gradient_based_uncertainty}\n\\sectionauthor{Matthias Rottmann\\textsuperscript{5}}\nClassical uncertainty quantification methods in frequentist inference are mostly based on the outputs of statistical models. Their uncertainty is quantified and assessed for instance via dispersion measures in classification (such as entropy, probability margin or variation ratio), or confidence intervals in regression. However, the nature of DNN architectures  and the cutting edge applications tackled by those (\\eg semantic segmentation, \\cf ) open the way towards more elaborate uncertainty quantification methods. Besides the mentioned classical approaches, intermediate feature representations within a DNN (\\cf ) or gradients according to self-affirmation that represent re-learning stress (see ) reveal additional information. In addition, in case of semantic segmentation, the geometry of a prediction may give access to further information, cf.\\ . By the computation of statistics of those quantities as well as low-dimensional representations thereof, we obtain more elaborate uncertainty quantification methods specifically designed for DNNs that can help us to detect misclassifications and out-of-distribution objects (\\cf ). \n\\par\nFeatures gripped during a fordward pass of a data point $x$ through a DNN $f$ can be considered layer-wise, \\ie $f^{(\\ell)}(x)$ after the $\\ell$-th layer. These can be translated into a handful of quantities per layer  or further processed by another DNN that aims that detecting errors . While in particular  presents a proof of concept on small scale classification problems, their applicability to large scale datasets and problems such as semantic segmentation and object detection remain open.\n\\par\nThe development for gradient-based uncertainty quantification methods  is guided by one central question:\nIf the present prediction was true, how much re-learning would this require.\nThe corresponding hypothesis is that wrong predictions would be more in conflict with the knowledge encoded in the deep neural network than correct ones, therefore causing increased re-learning stress.\nGiven a predicted class\n\\begin{equation}\n\\hat{y} = \\mathop{\\mathrm{arg\\,max}}_{y} f_y(x) \n\\end{equation}\nwe compute the gradient of layer $\\ell$ corresponding to the predicted label. That is, given a loss function $\\mathcal{L}$, we compute\n\\begin{equation}\n\\nabla_{w_\\ell} \\mathcal{L}( \\hat{y}, x, w ) \n\\end{equation}\nvia backpropagation. The obtained quantities can be treated similarly to the case of forward pass features. While this concept seems to be prohibitively expensive for semantic segmentation (at least when calculating gradients for each pixel of $\\hat{y}$), its applicability to object detection might be feasible, in particular with respect to offline applications. Gradients are also of special interest in active learning with query by \\emph{expected model change} (\\cf ).\n\\par\nIn the context of semantic segmentation, geometrical information on segments shapes as well as neighborhood relations of predicted segments can be taken into account along side with dispersion measures. It has been demonstrated  that the detection of errors in an in-distribution setting strongly benefits from geometrical information. Recently, this has also been considered in scenarios under moderate domain shift . However, its applicability to out-of-distribution examples and to other sensors than the camera remains subject to further research.", "cites": [1624, 4620, 8211, 8210, 1733, 206], "cite_extract_rate": 0.5, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from cited works to present uncertainty quantification methods in DNNs, particularly in the context of frequentist inference and semantic segmentation. It connects ideas from different papers, such as gradient-based uncertainty and geometric features, to form a coherent discussion. However, it lacks deeper comparative or critical analysis of the methods' strengths and weaknesses. The section abstracts some principles, like the relationship between re-learning stress and prediction reliability, but stops short of offering a meta-level framework."}}
{"id": "81f04d24-1455-4244-a481-8a943c0061e4", "title": "Markov Random Fields", "level": "subsection", "subsections": [], "parent_id": "9f13c349-4fb4-458e-8f34-84cd3a93fca0", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Uncertainty"], ["subsection", "Markov Random Fields"]], "content": "\\label{subsec:uncertainty:markov_random_fields}\n\\sectionauthor{Seyed Eghbal Ghobadi\\textsuperscript{8}, Ahmed Hammam\\textsuperscript{8}}\nAlthough deep neural networks are currently the state of the art for almost all computer vision tasks, Markov random fields (MRF) remain one of the fundamental techniques used for many computer vision tasks, specifically image segmentation ,. MRFs hold its power in the essence of being able to model dependencies between pixels in an image. With the use of energy functions, MRFs integrate pixels into models relating between unary and pairwise pixels together . \nGiven the model, MRFs are used to infer the optimal configuration yielding the lowest energy using mainly maximum a posteriori (MAP) techniques. Several MAP inference approaches are used to yield the optimal configuration such as graph cuts  and belief propagation algorithms . However, as with neural networks, MAP inference techniques result in deterministic point estimates of the optimal configuration without any sense of uncertainty in the output. To obtain uncertainties on results from MRFs, most of the work is directed towards modelling MRFs with Gaussian distributions. Getting uncertainties from MRFs with Gaussian distributions is possible by two typical methods: Either approximate models are inferred to the posterior, from which sampling is easy or the variances can be estimated analytically, or approximate sampling from the posterior is used.\nApproximate models include those inferred using variational Bayesian (VB) methods, like mean-field approximations, and using Gaussian process (GP) models enforcing a simplified prior model , . Examples of approximate sampling methods include traditional Markov chain Monte Carlo (MCMC) methods like Gibbs sampling . Some recent theoretical advances propose the perturb-and-MAP framework and a Gumbel perturbation model (GPM) , to exactly sample from MRF distributions. Another line of work has also been proposed, where  MAP inference techniques are used to estimate the probability of the network output. With the use of graph cuts,  try to estimate uncertainty using the min-marginals associated with the label assignments of a random field. Here, the work by Kohli and Torr  was extended to show how this approach can be extended to techniques other than graph cuts  or compute uncertainties on multi-label marginal distributions .\n\\par\nA current research direction is the incorporation of MRFs with deep neural networks, along with providing uncertainties on the output . This can also be extended to other forms of neural networks such as recurrent neural networks to provide uncertainties on segmentation of streams of videos with extending dependencies of pixels to previous frames , .", "cites": [8213, 8212, 1737, 1745], "cite_extract_rate": 0.23529411764705882, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple cited works to present a coherent narrative on how uncertainty is handled in MRFs and their integration with deep learning. It identifies common themes, such as the use of Gaussian distributions and sampling techniques, and connects different approaches like VB and MCMC. While it does provide some critical perspective (e.g., noting that MAP inference yields deterministic outputs), it does not deeply evaluate limitations or compare methods in detail. The abstraction level is moderate, identifying trends like the transition from two-stage models to integrated deep graphical models."}}
{"id": "09524fc1-6625-4adb-b57e-b6186b5d4ef0", "title": "Confidence Calibration", "level": "subsection", "subsections": [], "parent_id": "9f13c349-4fb4-458e-8f34-84cd3a93fca0", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Uncertainty"], ["subsection", "Confidence Calibration"]], "content": "\\label{subsec:uncertainty:confidence_calibration} \n\\sectionauthor{Fabian Küppers\\textsuperscript{9}, Anselm Haselhoff\\textsuperscript{9}}\nNeural network classifiers output a label $\\hat{Y} \\in \\mathcal{Y}$ on a given input $X \\in \\mathcal{X}$ with an associated confidence $\\hat{P}$. This confidence can be interpreted as a probability of correctness that the predicted label matches the ground truth label $Y \\in \\mathcal{Y}$. Therefore, these probabilities should reflect the ”self-confidence” of the system. If the empirical accuracy for any confidence level matches the predicted confidence, a model is called \\emph{well calibrated}.\nTherefore, a \\emph{classification} model is perfectly calibrated if \n\\begin{align}\n\\label{eq:classification_calibration}\n& \\underbrace{\\mathbb{P}(\\hat{Y} = Y | \\hat{P} = p)}_{\\text{accuracy given } p} = \\underbrace{p}_{\\text{confidence}} \\quad \\forall p \\in [0,1]\n\\end{align}\nis fullfilled . For example, assume 100 predictions with confidence values of 0.9. We call the model well calibrated if 90 out of these 100 predictions are actually correct. However, recent work has shown that modern neural networks tend to be too overconfident in their predictions . The deviation of a model to the perfect calibration can be measured by the expected calibration error (ECE) . \nIt is possible to recalibrate models as a post-processing step after classification. One way to get a calibration mapping is to group all predictions into several bins by their confidence. Using such a binning scheme, it is possible to compute the empirical accuracy for certain confidence levels, as it is known for a long time already in reconstructing confidence outputs for Viterbi decoding . Common methods are histogram binning , isotonic regression  or more advanced methods like Bayesian binning into quantiles (BBQ)  and ensembles of near-isotonic regression (ENIR) . Another way to get a calibration mapping is to use scaling methods based on logistic regression like Platt scaling , temperature scaling  and beta calibration .\n\\par\nIn the setting of probabilistic regression, a model is calibrated if, \\eg 95\\% of the true target values are below or equal to a credible level of 95\\% (so called \\emph{quantile-calibrated regression}) . A regression model is usually calibrated by fine-tuning its predicted CDF in a post-processing step to match the empirical frequency. Common approaches utilize isotonic regression , logistic and beta calibration , as well as Gaussian process models  to build a calibration mapping. In contrast to quantile-calibrated regression,  have recently introduced the concept of distribution calibration, where calibration is applied on a distribution level and naturally leads to calibrated quantiles.\n\\par\nRecent work has shown that miscalibration in the scope of \\emph{object detection} also depends on the position and scale of a detected object . The additional box regression output is denoted by $\\hat{R}$ with $J$ as the size of the used box encoding.\nFurthermore, if we have no knowledge about all anchors of a model (which is a common case in many applications), it is not possible to determine the accuracy. Therefore, K\\\"{u}ppers \\etal~ use the precision as a surrogate for accuracy and propose that an \\emph{object detection model} is perfectly calibrated if\n\\begin{align}\n\\label{eq:detection_calibration}\n& \\underbrace{\\mathbb{P}(M=1 | \\hat{P} = p, \\hat{Y} = y, \\hat{R} = r)}_{\\text{precision given } p, y, r} = \\underbrace{p}_{\\text{confidence}} \\quad \\forall p \\in [0,1], y \\in \\mathcal{Y}, r \\in \\mathbb{R}^J \n\\end{align}\nis fulfilled, where $M=1$ denotes a correct prediction that matches a ground-truth object with a chosen IoU threshold and $M=0$ denotes a mismatch, respectively. The authors propose the detection-expected calibration error (D-ECE) as the extension of the ECE to object detection tasks in order to measure miscalibration also by means of the position and scale of detected objects. Other approaches try to fine-tune the regression output in order to obtain more reliable object proposals  or to add a regularization term to the training objective such that training yields models that are both well-performing and well-calibrated .", "cites": [8214, 8216, 759, 8215, 4141, 8217, 4690], "cite_extract_rate": 0.4117647058823529, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes multiple papers to present a structured discussion on confidence calibration across classification, regression, and object detection tasks. It provides a clear conceptual framework and generalizes calibration principles beyond individual methods. While it includes some critical analysis (e.g., overconfidence in neural networks), the critique could be deeper to fully justify a 5.0 score."}}
{"id": "71d4e278-f67c-443c-84a9-ff0254362119", "title": "Aggregation", "level": "section", "subsections": ["d1f9f7cf-fc5d-426f-8fba-a38a5d51acf5", "61379b41-7fce-4042-aff6-0ec085cd8afc"], "parent_id": "569f1197-c628-4e9d-9db6-9757cb02d82a", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Aggregation"]], "content": "\\label{sec:aggregation}\n\\sectionauthor{Maram Akila\\textsuperscript{1}}\nFrom a high-level perspective, a \\emph{neural network} is based on processing inputs and coming to some output conclusion, \\eg mapping incoming image data onto class labels.\nAggregation or collection of non-independent information on either the input or output side of this network function can be used as a tool to leverage its performance and reliability.\nStarting with the input, any additional ``dimension'' to add data can be of use.\nFor example, in the context of autonomous vehicles this might be input from any further sensor measuring the same scene as the original one, \\eg stereo cameras or LiDAR.\nCombining those sensor sets for prediction is commonly referred to as \\emph{sensor fusion} .\nStaying with the example, the scene will be monitored consecutively providing a whole (temporally ordered)  stream of input information.\nThis may be used either by adjusting the network for this kind of input \n \nor in terms of a post-processing step, in which the predictions are aggregated by some measure of temporal consistency. \n\\par\nAnother more implicit form of aggregation is training the neural network on several ``independent'' tasks, \\eg segmentation and depth regression. Although the individual task is executed on the same input, the overall performance can still benefit from the correlation among all given tasks. We refer to the discussion on \\emph{multi-task networks} in \\refsec{subsec:architecture:multi_task_networks}.\nBy extension, solving the same task in multiple different ways, can be beneficial for performance and provide a measure of redundancy.\nIn this survey, we focus on single-task systems and discuss \\emph{ensemble methods} in the next section and the use of \\emph{temporal consistency} in the one thereafter.", "cites": [6435], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a general description of aggregation in AI safety, mentioning sensor fusion and multi-task networks, but integrates only one cited paper superficially. It lacks critical analysis of the cited work or alternative approaches and does not elevate the discussion to a broader conceptual or theoretical level."}}
{"id": "d1f9f7cf-fc5d-426f-8fba-a38a5d51acf5", "title": "Ensemble Methods", "level": "subsection", "subsections": [], "parent_id": "71d4e278-f67c-443c-84a9-ff0254362119", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Aggregation"], ["subsection", "Ensemble Methods"]], "content": "\\label{subsec:aggregation:ensemble_methods}\n\\sectionauthor{Joachim Sicking\\textsuperscript{1}}\nTraining a neural network is optimizing its parameters to fit a given training data set. The commonly used gradient-based optimization schemes cause convergence in a ‘nearby’ local minimum. As the loss landscapes of neural networks are notoriously non-convex , various locally optimal model parameter sets exist. These local optima differ in the degree of optimality (“deepness”), qualitative characteristics (“optimal for different parts of the training data”) and their generalizability to unseen data (commonly referred to by the geometrical terms of “sharpness” and “flatness” of minima ). \nA single trained network corresponds to one local minimum of such a loss landscape and thus captures only a small part of a potentially diverse set of solutions. \\emph{Network ensembles} are collections of models and therefore better suited to reflect this multi-modality. Various modelling choices shape a loss landscape: the selected model class and its meta-parameters (like architecture and layer width), the training data and the optimization objective. Accordingly, approaches to diversify ensemble components range from combinations of different model classes over varying training data (bagging) to methods that train and weight ensemble components to make up for the flaws of other ensemble members (boosting) .\nGiven the millions of parameters of application-size networks, ensembles of NNs are resource-demanding \\wrt computational load, storage and runtime during training and inference. This complexity increases linearly with ensemble size for naïve ensembling. Several approaches were put forward to reduce some dimensions of this complexity: \\emph{snapshot ensembles}  require only one model optimization with a cyclical learning-rate schedule—leading to an optimized training runtime.  The resulting training trajectory passes through several local minima. The corresponding models compose the ensemble. On the contrary, \\emph{model distillation}  tackles runtime at inference. They ‘squeeze’ a NN ensemble into a single model that is optimized to capture the gist of the model set. However, such a compression goes along with reduced performance compared to the original ensemble.\nSeveral hybrids of single model and model ensemble exist: Multi-head networks  share a backbone network that provides inputs to multiple prediction networks. Another variant are mixture-of-expert models that utilize a gating network to assign inputs to specialized expert networks . Multi-task networks (\\cf \\refsec{subsec:architecture:multi_task_networks}) and Bayesian approximations of NNs (\\cf \\refsec{subsec:uncertainty:bayesian_neural_networks} and \\refsec{subsec:uncertainty:mc_dropout}) can be seen as implicit ensembles. \nNN ensembles (or deep ensembles) are not only used to boost model quality. They pose the frequentist's approach to estimating NN uncertainties and are state-of-the-art in this regard . The emergent field of federated learning is concerned with the integration of decentrally trained ensemble components  and safety-relevant applications of ensembling range from autonomous driving  to medical diagnostics . \nTaking this safe-ML perspective, promising research directions comprise a more principled and efficient composition of model ensembles, \\eg by application-driven diversification, as well as improved techniques to miniaturize ensembles, \\eg by gaining a better understanding of methods like distillation. In the long run, better designed, more powerful learning systems might partially reduce the need for combining weaker models in a network ensemble.", "cites": [4616, 4678, 582, 3288, 680, 681, 8966, 8218], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes insights from multiple papers to explain the rationale, benefits, and challenges of ensemble methods in AI safety, particularly in the context of uncertainty estimation and resource efficiency. It critically addresses limitations such as computational cost and performance trade-offs in distillation. The abstraction is strong, as it frames ensembling as a broader strategy for capturing model diversity and improving robustness, connecting it to related concepts like multi-head and mixture-of-experts architectures."}}
{"id": "61379b41-7fce-4042-aff6-0ec085cd8afc", "title": "Temporal Consistency", "level": "subsection", "subsections": [], "parent_id": "71d4e278-f67c-443c-84a9-ff0254362119", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Aggregation"], ["subsection", "Temporal Consistency"]], "content": "\\label{subsec:aggregation:temporal_consistency}\n\\sectionauthor{Timo Sämann\\textsuperscript{4}}\nThe focus of previous DNN development for semantic segmentation has been on single image prediction. This means that the final and intermediate results of the DNN are discarded after each image. However, the application of a computer vision model often involves the processing of images in a sequence, \\ie there is a temporal consistency in the image content between consecutive frames (for a metric, \\cf, \\eg ). This consistency has been exploited in previous work to increase quality and reduce computing effort. Furthermore, this approach offers the potential to improve the robustness of DNN prediction by incorporating this consistency as a-priori knowledge into DNN development. The relevant work in the field of video prediction can be divided in two major approaches: \n\\begin{enumerate}\n\\item DNNs are specially designed for video prediction. This usually requires training from scratch and the availability of training data in a sequence.\n\\item A transformation from single prediction DNNs to video prediction DNNs takes place. Usually no training is required, \\ie the existing weights of the model can be used unaltered.\n\\end{enumerate}\nThe first set of approaches often involves \\emph{conditional random fields} (CRF) and its variants. \nCRFs are known for their use as postprocessing step in the prediction of semantic segmentation, in which their parameters are learned separately or jointly with the DNN~. \nAnother way to use spatiotemporal features is to include 3D convolutions, which add an additional dimension to the conventional 2D convolutional layer. Tran \\etal use 3D convolution layers for video recognition tasks such as action and object recognition.     \nOne further approach to use spatial and temporal characteristics of the input data is to integrate \\emph{long short-term memory} (LSTM)~, a variant of the \\emph{recurrent neural network} (RNN). Fayyaz \\etal  integrate LSTM layers between the encoder and decoder of their convolutional neural network for semantic segmentation. The significantly higher GPU memory requirements and computational effort are a disadvantage of this method. More recently, Nilsson and Sminchisescu  deployed \\emph{gated recurrent units}, which generally requires significantly less memory.\nAn approach to improve temporal consistency of automatic speech recognition outputs is known as a posterior-in-posterior-out (PIPO) LSTM ``sequence enhancer'', a postfilter which could be applicable to video processing as well .\nA disadvantage of the described methods is that sequential data for training must be available, which may be limited or show a lack of diversity. \n\\par\nThe second class of approaches has the advantage that it is model-independent most of the time. Shelhamer \\etal found that the deep feature maps within the network change only slightly with temporal changes in video content.\nAccordingly,  calculate the optical flow of the input images from time steps $t_0$ and $t_{-1}$ and convert it into the so-called \\emph{transform flow} which is used to transform the feature maps of the time step $t_{-1}$ so that an aligned representation to the feature map $t_0$ is achieved. S{\\\"a}mann \\etal use a confidence-based combination of feature maps from previous time steps based on the calculated optical flow.", "cites": [8219, 1745, 8220, 784], "cite_extract_rate": 0.4, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes insights from four cited papers, effectively grouping them into two broader approaches for exploiting temporal consistency in DNNs. It provides some critical analysis by pointing out disadvantages, such as memory requirements and training data limitations, but could delve deeper into trade-offs between the approaches. The abstraction level is moderate, identifying general patterns (e.g., model-independent vs. model-specific methods) but not offering a meta-level framework for AI safety."}}
{"id": "5ad8ed4b-a3fa-43df-8241-7dd338b6350c", "title": "Formal Testing", "level": "subsection", "subsections": [], "parent_id": "94174462-51f2-497a-b88f-1ceaadd52aaf", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Verification"], ["subsection", "Formal Testing"]], "content": "\\label{subsec:verification:formal_testing}\n\\sectionauthor{Christian Heinzemann\\textsuperscript{2}, Gesina Schwalbe\\textsuperscript{3}, Matthias Woehrle\\textsuperscript{2}}\nFormal testing refers to testing methods that include formalized and\nformally verifiable steps, \\eg for test data acquisition, or for\nverification in the local vicinity of test samples.\nFor image data, local testing around valid samples is usually more\npractical than fully formal verification: (Safety) properties are not\nexpected to hold on the complete input space but only on the much\nsmaller unknown lower-dimensional manifold of real images .\nSources of such samples can be real ones or generated ones using an\ninput space formalization or a trained generative model.\n\\par\nCoverage criteria for the data samples are commonly used for two purposes:\n(a) deciding when to stop testing or\n(b) identifying missing tests.\nFor CNNs, there are at least three different approaches towards\ncoverage:\n(1) approaches that establish coverage based on a model with semantic\nfeatures of the input space~,\n(2) approaches trying to semantically cover the latent feature space\nof neural network or a proxy network (\\eg an\nautoencoder)~, and\n(3) approaches trying to cover neurons and their interactions,\ninspired by classical software white-box\nanalysis~.\n\\par\nTypical types of properties to verify are \nsimple test performance,\nlocal stability (robustness),\na specific structure of the latent spaces like\nembedding of semantic concepts ,\nand more complex logical constraints on inputs and outputs,\nwhich can be used for testing when fuzzified .\nMost of these properties require in-depth semantic information about\nthe DNN inner workings, which is often only available via interpreting\nintermediate representations ,\nor interpretable proxies / surrogates (\\cf \\refsec{subsec:interpretability:interpretable_proxies}), which do not guarantee fidelity.\n\\par\nThere exist different testing and formal verification methods from\nclassical software engineering that have already been applied to CNNs.\n\\emph{Differential testing} as used by\nDeepXPlore~ trains $n$ different CNNs for\nthe same task using independent data sets and compares the individual\nprediction results on a test set. This allows to identify\ninconsistencies between the CNNs but no common weak spots.\n\\emph{Data augmentation} techniques start from a given data set and\ngenerate additional transformed data. \\emph{Generic data augmentation}\nfor images like rotations and translation are state-of-the-art for\ntraining but may also be used for testing.\n\\emph{Concolic testing} approaches incrementally grow test suites with\nrespect to a coverage model to finally achieve completeness. Sun \\etal  use an adversarial input model based on\nsome norm (\\cf \\refsec{subsec:adverarial_attacks:adversarial_attacks_defenses}),\n\\eg an $L_p$-norm, for generating additional images around a given image\nusing concolic testing. \\emph{Fuzzing} generates new test data constrained by\nan input model and tries to identify interesting test cases, \\eg by\noptimizing white-box coverage mentioned\nabove~. Fuzzing techniques may also be\ncombined with the differential testing approach discussed\nabove~.\nIn all these cases it needs to be ensured that the image as well as\nits meta-data remain valid for testing after transformation.\nFinally, \\emph{proving methods} surveyed by Liu \\etal  try to formally prove properties on a\ntrained neural network, \\eg based on satisfiablity modulo theories (SMT).  \nThese approaches require a formal characterization of an input space\nand the property to be checked, which is hard for non-trivial\nproperties like contents of an image. \n\\par\nExisting formal testing approaches can be quite costly to integrate\ninto testing workflows (\\cf ):\nDifferential testing and data augmentation require several inferences\nper initial test sample;\nconcolic and fuzzy testing apply an optimization to each given test\nsample, while convergence towards the coverage goals is not guaranteed;\nalso, the iterative approaches need tight integration into the testing\nworkflow;\nand lastly, proving methods usually have to balance computational\nefficiency against the precision or completeness of the result\n.\nAnother challenge of formal testing is that machine learning\napplications usually solve problems for which no (formal) specification\nis possible. This makes it hard to find useful requirements for\ntesting  and properties that can be formally\nverified. \nEven partial requirements such as specification of useful input\nperturbations, specified corner cases, and valuable coverage goals are\ntypically difficult to identify .", "cites": [8089, 8971, 1636, 6024], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key ideas from multiple papers, integrating concepts like differential testing, concolic testing, fuzzing, and proving methods into a coherent discussion of formal testing for AI safety. It also provides critical insights, such as the high integration cost and challenges in defining formal specifications. The abstraction level is strong as it generalizes these methods into broader categories and highlights overarching limitations and considerations in testing deep neural networks."}}
{"id": "5a9669c2-3369-414d-a5ae-bc8b21dd4bcd", "title": "Black Box Methods", "level": "subsection", "subsections": [], "parent_id": "94174462-51f2-497a-b88f-1ceaadd52aaf", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Verification"], ["subsection", "Black Box Methods"]], "content": "\\label{subsec:verification:black_box_methods}\n\\sectionauthor{Jonas Löhdefink\\textsuperscript{15}, Julia Rosenzweig\\textsuperscript{1}}\nIn machine learning literature, neural networks are often referred to as black boxes due to the fact that their internal operations and their decision making are not completely understood , hinting at a lack of interpretability and transparency. However, in this survey we consider a black box to be a machine learning model to which we only have oracle (query) access . That means we can query the model to get input-output pairs, but we do not have access to the specific architecture (or weights, in case of neural networks). As  describes, black boxes are increasingly wide-spread, \\eg healthcare, autonomous driving or ML as a service in general, due to proprietary, privacy or security reasons.  \n\\par\nAs deploying black boxes gains popularity, so do methods that aim to extract internal information such as architecture and parameters or to find out, whether a sample belongs to the training dataset. These include \n\\emph{model extraction attacks} , \\emph{membership inference attacks} , general attempts to reverse-engineer the model  or to attack it adversarially . Protection and counter-measures are also  actively researched:  proposes a warning system that estimates how much information an attacker could have gained from queries. The authors of  use watermarks for models to prevent illegal re-distribution and to identify intellectual property.\n\\par\nMany papers in these fields make use of so-called \\emph{surrogate}, \\emph{avatar} or \\emph{proxy models} that are trained on input-output pairs of the black box.\nIn case the black-box output is available in soft form (\\eg logits), distillation as first proposed by  can be applied to train the surrogate (student) model. Then, any white-box analysis can be performed on the surrogates (\\cf \\eg ) to craft adversarial attacks targeted at the black box. More generally, (local) surrogates as for example in  can be used to (locally) explain its decision-making. \nMoreover, these techniques are also of interest if one wants to compare or test black-box models (\\cf \\refsec{subsec:verification:formal_testing}, formal verification). \nThis is the case, among others, in ML marketplaces, where you wish to buy a pre-trained model , or if you want to verify or audit that a third-party black-box model obeys  regulatory rules (\\cf ). \nAnother topic of active research are so-called \\emph{observers}.\nThe concept of observers is to evaluate the interface of a black-box module to determine if it behaves as expected within a given set of parameters.\nThe approaches can be divided into \\emph{model-explaining} and \\emph{anomaly-detecting} observers.\nFirst, model explanation methods answer the question of which input characteristic is responsible for changes at the output.\nThe observer is able to alter the inputs for this purpose.\nIf the input of the model under test evolves only slightly but the output changes drastically, this can be a signal that the neural network is mislead, which is also strongly related to adversarial examples (\\cf Chapter \\ref{sec:adversarial_attacks}).\nHence, the reason for changes in the classification result via the input can be very important.\nIn order to figure out in which region of an input image the main reason for the classification is located,  ``delete'' information from the image by replacing regions with generated patches until the output changes.\nThis replaced region is likely responsible for the decision of the neural network.\nBuilding upon this,  adapt the approach to medical images and generate ``deleted'' regions by a variational autoencoder (VAE).\nSecond, anomaly-detecting observers register input and output anomalies, either examining input and output independently or as an input-output pair, and predict the black-box performance in the current situation.\nIn contrast to model-explaining approaches, this set of approaches has high potential to be used in an online scenario since it does not need to modify the model input.\nThe maximum mean discrepancy (MMD)~ measures the domain gap between two data distributions independently from the application and can be used to raise a warning if input or output distributions during inference deviate too strongly from their respective training distributions.\nBy use of a GAN-based autoencoder  perform a domain shift estimation using neural networks in conjunction with the Wasserstein distance as domain mismatch metric.\nThis metric can also be evaluated by use of a casual time-variant aggregation of distributions during inference time.", "cites": [7507, 7610, 681, 1818, 8221, 603, 1822, 2676], "cite_extract_rate": 0.5333333333333333, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes several black-box AI safety methods, connecting papers on model extraction, watermarking, surrogate models, and observers. It highlights how techniques such as distillation, LIME, and MMD relate to broader goals like interpretability, auditing, and anomaly detection. While it provides a coherent narrative, the critical analysis is limited to surface-level observations (e.g., noting potential for online use), and abstraction captures some patterns but does not reach a deep meta-level insight."}}
{"id": "f0f7d7d0-2bd8-48ae-bdb1-126b96887d72", "title": "Architecture", "level": "section", "subsections": ["0c7b2cb4-747c-4997-8a70-14bcffc01831", "9fd0032b-a9e0-4b5c-ae60-155c9130c8d4", "19fdc458-c9c5-4f0b-a62d-6dbcbe8fef85"], "parent_id": "569f1197-c628-4e9d-9db6-9757cb02d82a", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Architecture"]], "content": "\\label{sec:architecture}\n\\sectionauthor{Michael Weber\\textsuperscript{14}}\nIn order to solve a specific task, the architecture of a CNN and its building blocks play a significant role.\nSince the early days of using CNNs in image processing, when they were applied to handwriting recognition~ and the later breakthrough in general image classification~, the architecture of the networks has changed radically.\nDid the term of \\emph{deep learning} for these first convolutional neural networks imply a depth of approximately four layers, their depth increased significantly during the last years and new techniques had to be developed to successfully train and utilize these networks~.\nIn this context, new activation functions~ as well as new loss functions~ have been designed and new optimization algorithms~ were investigated.\nWith regard to the layer architecture, the initially alternating repetition of convolution and pooling layers as well as their characteristics have changed significantly.\nThe convolution layers made the transition from a few layers with often large filters to many layers with small filters.\nA further trend was then the definition of entire modules, which were used repeatedly within the overall architecture as so-called \\emph{network in network}~.\nIn areas such as autonomous driving, there is also a strong interest in the simultaneous execution of different tasks within one single convolutional neural network architecture.\nThis kind of architecture is called \\emph{multi-task learning}~(MTL)~ and can be utilized in order to save computational resources and at the same time to increase performance of each task .\nWithin such multi-task networks, usually one shared feature extraction part is followed by one separate so-called head per task~.\nIn each of these architectures, manual design using expert knowledge plays a major role. The role of the expert is the crucial point here.\nIn recent years, however, there have also been great efforts to automate the process of finding architectures for networks or, in the best case, to learn them.\nThis is known under the name \\emph{neural architecture search}~(NAS).", "cites": [97, 7588, 1455, 3769], "cite_extract_rate": 0.4, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic overview of CNN architecture evolution, mentioning trends such as depth, activation functions, and multi-task learning. It integrates a few cited papers to support the historical narrative but lacks deeper synthesis, critical evaluation, or abstraction to broader principles. The content remains largely descriptive without offering comparative or analytical insights."}}
{"id": "0c7b2cb4-747c-4997-8a70-14bcffc01831", "title": "Building Blocks", "level": "subsection", "subsections": [], "parent_id": "f0f7d7d0-2bd8-48ae-bdb1-126b96887d72", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Architecture"], ["subsection", "Building Blocks"]], "content": "\\label{subsec:architecture:building_blocks}\n\\sectionauthor{Michael Weber\\textsuperscript{14}}\nDesigning a convolutional neural network typically includes a number of design choices.\nThe general architecture usually contains a number of convolutional and pooling layers which are arranged in a certain pattern. \nConvolutional layers are commonly followed by a non-linear activation function. \nThe learning process is based on a loss function which determines the current error and an optimization function that propagates the error back to the single convolution layers and its learnable parameters.\nWhen CNNs became state of the art in computer vision~, they were usually built using a few alternating convolutional and pooling layers having a few fully connected layers in the end.\nIt turned out that better results are achieved with deeper networks and so the number of layers increased~ over the years.\nTo deal with these deeper networks, new architectures had to be developed.\nIn a first step, to reduce the number of parameters, the convolutional layers with partly large filter kernels were replaced by several layers with small $3 \\times 3$ kernels.\nToday, most architectures are based on the \\emph{network in network} principle~, where more complex modules are used repeatedly. \nExamples of such modules are the \\emph{inception module} from GoogleNet~ or the \\emph{residual block} from ResNet~. \nWhile the inception module consists of multiple parallel strings of layers, the residual blocks are based on the \\emph{highway network}~, which means that they can bypass the original information and the layers in between are just learning residuals.\nWith ResNeXt~ and Inception-ResNet~ there already exist two networks that combine both approaches.\nFor most tasks, it turned out that replacing the fully connected layers by convolutional layers is much more convenient making the networks fully convolutional~.\nThese so-called \\emph{fully convolutional networks}~(FCN) are no longer bound to fixed input dimensions.\nNote that with the availability of convolutional long short-term memory (ConvLSTM) structures also fully convolutional recurrent neural networks (FCRNs) became available for fully scalable sequence-based tasks .\nInside the CNNs, the \\emph{rectified linear unit}~(ReLU) has been the most frequently used activation function for a long time.\nHowever, since this function suffers from problems related to the mapping of all negative values to zero like the vanishing gradient problem, new functions have been introduced in recent years.\nExamples are the \\emph{exponential linear unit}~(ELU), \\emph{swish}~ and the \\emph{non-parametric linearly scaled hyperbolic tangent}~(LiSHT)~.\nIn order to be able to train a network consisting of these different building blocks, \nthe loss function is the most crucial part. This function is responsible for how and what the network ultimately learns and how exactly the training data is applied during the training process to make the network train faster or perform better. So the different classes can be weighted in a classification network with fixed values or so-called $\\alpha$-balancing according to their probability of occurrence. Another interesting approach is weighting training examples according to their easiness for the current network~,~. For multi-task learning also weighting tasks based on their uncertainty~ or gradients~ can be done as further explained in Sec.~\\ref{subsec:architecture:multi_task_networks}.\nA closer look on how a modification of the loss function might affect safety-related aspects is given in Sec.~\\ref{subsec:robust_training:modification_of_loss}.", "cites": [502, 97, 810, 7588, 8224, 8223, 514, 305, 1210, 1455, 8222, 3769, 40], "cite_extract_rate": 0.7647058823529411, "origin_cites_number": 17, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual overview of CNN building blocks and relevant advancements, citing multiple papers. It connects ideas (e.g., residual blocks and inception modules) and mentions their combinations in hybrid networks like ResNeXt and Inception-ResNet, showing modest synthesis. However, it lacks critical evaluation of these works and offers little abstraction beyond listing components and functions. The narrative remains descriptive rather than analytical or insightful."}}
{"id": "9fd0032b-a9e0-4b5c-ae60-155c9130c8d4", "title": "Multi-Task Networks", "level": "subsection", "subsections": [], "parent_id": "f0f7d7d0-2bd8-48ae-bdb1-126b96887d72", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Architecture"], ["subsection", "Multi-Task Networks"]], "content": "\\label{subsec:architecture:multi_task_networks}\n\\sectionauthor{Marvin Klingner\\textsuperscript{15}, Varun Ravi-Kumar\\textsuperscript{4}, Timo Sämann\\textsuperscript{4}, Gesina Schwalbe\\textsuperscript{3}}\n\\emph{Multi-task learning} (MTL) in the context of neural networks describes the process of optimizing several tasks simultaneously by learning a unified feature representation  and coupling the task-specific loss contributions, thereby enforcing cross-task consistency .\\par\nUnified feature representation is usually implemented by sharing the parameters of the initial layers inside the encoder (also called feature extractor). It not only improves the single tasks by more generalized learned features but also reduces the demand for computational resources at inference. Not an entirely new network has to be added for each task but only a task-specific decoder head. It is essential to consider the growing amount of visual perception tasks in autonomous driving, e.g., depth estimation, semantic segmentation, motion segmentation, and object detection. While the parameter sharing can be soft, as in \\emph{cross stitch}  and \\emph{sluice networks} , or hard , meaning ultimately sharing the parameters, the latter is usually preferred due to its straightforward implementation and lower computational complexity during training and inference.\\par\nCompared to implicitly coupling tasks via a shared feature representation, there are often more direct ways to optimize the tasks inside cross-task losses jointly. It is only made possible as, during MTL, there are network predictions for several tasks, which can be enforced to be consistent. As an example, sharp depth edges should only be at class boundaries of semantic segmentation predictions. Often both approaches to MTL are applied simultaneously~ to improve a neural network's performance as well as to reduce its computational complexity at inference.\\par\nWhile the theoretical expectations for MTL are quite clear, it is often challenging to find a good weighting strategy for all the different loss contributions as there is no theoretical basis on which one could choose such a weighting with early approaches either involving heuristics or extensive hyperparameter tuning. The easiest way to balance the tasks is to use uniform weight across all tasks. However, the losses from different tasks usually have different scales, and uniformly averaging them suppresses the gradient from tasks with smaller losses. Addressing these problems, Kendall et al.  propose to weigh the loss functions by the \\emph{homoscedastic uncertainty} of each task. One does not need to tune the weighting parameters of the loss functions by hand, but they are adapted automatically during the training process. Concurrently Chen et al.~ propose \\emph{GradNorm}, which does not explicitly weigh the loss functions of different tasks but automatically adapts the gradient magnitudes coming from the task-specific network parts on the backward pass. Liu et al.  proposed dynamic weight average (DWA), which uses an average of task losses over time to weigh the task losses.", "cites": [324, 8226, 8225, 8227], "cite_extract_rate": 0.25, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes key concepts from multiple papers on multi-task learning, integrating them into a coherent narrative about shared feature representations and loss coupling strategies. It provides a critical view by highlighting the lack of theoretical guidance for loss weighting and comparing different approaches like homoscedastic uncertainty, GradNorm, and DWA. While it offers some abstraction by framing these techniques in the broader context of MTL challenges, it stops short of presenting a novel conceptual framework."}}
{"id": "19fdc458-c9c5-4f0b-a62d-6dbcbe8fef85", "title": "Neural Architecture Search", "level": "subsection", "subsections": [], "parent_id": "f0f7d7d0-2bd8-48ae-bdb1-126b96887d72", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Architecture"], ["subsection", "Neural Architecture Search"]], "content": "\\label{subsec:architecture:neural_architecture_search}\n\\sectionauthor{Patrick Feifel\\textsuperscript{8}, Seyed Eghbal Ghobadi\\textsuperscript{8}}\nIn the previous sections we saw manually engineered modifications of existing CNN architectures proposed by ResNet  or Inception . They are results of human design and showed their ability to improve performance. ResNet introduces a \\emph{skip connection} in building blocks and Inception makes use of its specific \\emph{inception module}. Hereby, the intervention by an expert is crucial. The approach of \\emph{neural architecture search} (NAS) aims to automate this time-consuming and manual design of neural network architectures. \n\\par\nNAS is closely related to hyperparameter optimization (HO), which is described in \\refsec{subsec:robust_training:hyperparameter_optimization}. Originally, both tasks were solved simultaneously. Consequently, the kernel size or number of filters were seen as additional hyperparamters. Nowadays, the distinction between HO and NAS should be stressed. The concatenation of complex building blocks or modules cannot be accurately described with single parameters. This simplification is no longer suitable. \n\\par\nTo describe the NAS process, the authors of  define three steps: (1) definition of search space, (2) search strategy and (3) performance estimation strategy. \n\\par\nThe majority of search strategies take advantage of the \\emph{NASNet search space}  which arranges various operations, \\eg convolution, pooling within a single cell. However, other spaces based on a chain or multi-branch structure are possible . The search strategy comprises advanced methods from \nsequential model-based optimization (SMBO) ,\nBayesian optimization , \nevolutionary algorithms , \nreinforcement learning  and \ngradient descent . \nFinally, the performance estimation describes approximation techniques due to the impracticability of multiple evaluation runs. For a comprehensive survey regarding the NAS process we refer to . \n\\par\nRecent research has shown that reinforcement learning approaches such as NASNet-A  and ENAS  are partly outperformed by evolutionary algorithms, \\eg AmoebaNet  and gradient-based approaches, \\eg DARTS . \n\\par\nEach of these approaches focuses on different optimization aspects. Gradient-based methods are applied to a continuous search space and offer faster optimization. On the contrary, the evolutionary approach LEMONADE  enables multi-object optimization by considering the conjunction of resource consumption and performance as the two main objectives. Furthermore, single-path NAS  extends the multi-path approach of former gradient-based methods and proposes the integration of 'over-parameterized superkernels', which significantly reduces memory consumption.\n\\par\nThe focus of NAS is on the optimized combination of humanly predefined CNN elements with respect to objectives such as resource consumption and performance. NAS offers automation, however, the realization of the objectives is strongly limited by the potential of the CNN elements.", "cites": [97, 875, 8229, 871, 305, 870, 872, 543, 544, 8228], "cite_extract_rate": 0.9090909090909091, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key concepts from multiple papers on NAS, connecting different methods (e.g., reinforcement learning, evolutionary algorithms, gradient-based approaches) into a structured framework. It provides critical comparisons by highlighting strengths and weaknesses of specific techniques, such as faster optimization in gradient-based methods versus multi-objective capabilities in evolutionary algorithms. The abstraction is strong as it generalizes the optimization objectives and methodologies across the field, identifying broader trends and limitations in the use of predefined CNN elements."}}
{"id": "e23cf554-7193-4f8f-a54e-3da02ce3d7c2", "title": "Model Compression", "level": "section", "subsections": ["5ed86e29-ba4e-4bd5-8fc0-724c41e2999e", "28ea17f4-7eb4-44e0-85c5-640bd64d35c5"], "parent_id": "569f1197-c628-4e9d-9db6-9757cb02d82a", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Model Compression"]], "content": "\\label{sec:compression}\n\\sectionauthor{Serin Varghese\\textsuperscript{7}}\nRecent developments in CNNs have resulted in neural networks being the state-of-the-art in computer vision tasks like image classification~, object detection~ and semantic segmentation~. This is largely due to the increasing availability of hardware computational power and an increasing amount of training data. We also observe a general upwards trend of the complexities of the neural networks along with their improvement in state-of-the-art performance. These CNNs are largely trained on back-end servers with significantly higher computing capabilities. The use of these CNNs in real-time applications are inhibited due to the restrictions on hardware, model size, inference time, and energy consumption. \nThis led to an emergence of a new field in machine learning, commonly termed as model compression. Model compression basically implies reducing the memory requirements, inference times and model size of DNNs to eventually enable the use of neural networks on edge devices. This is tackled by different approaches such as \\emph{network pruning} (identifying weights or filters that are not critical for network performance), \\emph{weight quantizations} (reducing the precision of the weights used in the network), \\emph{knowledge distillation} (a smaller network is trained with the knowledge gained by a bigger network), and \\emph{low-rank factorization} (decomposing a tensor into multiple smaller tensors).\nIn this section, we introduce some of these methods for model compression and discuss in brief the current open challenges and possible research directions with respect to its use in automated driving applications.", "cites": [509, 8429, 1621, 1759, 8167, 206, 520, 4169], "cite_extract_rate": 0.8, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic introduction to model compression techniques without directly engaging with the cited papers to support or contextualize the discussion. It lists common approaches like pruning and quantization but does not synthesize insights from the cited works, nor does it critically evaluate or compare them. The abstraction is minimal, focusing primarily on concrete techniques rather than broader principles or implications for AI safety."}}
{"id": "5ed86e29-ba4e-4bd5-8fc0-724c41e2999e", "title": "Pruning", "level": "subsection", "subsections": [], "parent_id": "e23cf554-7193-4f8f-a54e-3da02ce3d7c2", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Model Compression"], ["subsection", "Pruning"]], "content": "\\label{subsec:compression:pruning}\n\\sectionauthor{Falk Kappel\\textsuperscript{13}, Serin Varghese\\textsuperscript{7}}\nPruning has been used as a systematic tool to reduce the complexity of deep neural networks. The redundancy in DNNs may exist on various levels, such as the individual weights, filters, and even layers. All the different methods for pruning try to take advantage of these available redundancies on various levels. Two of the initial approaches for neural networks proposed weight pruning in the 1990s as a way of systematically damaging neural networks~. As these weight pruning approaches do not aim at changing the structure of the neural network, these approaches are called \\emph{unstructured pruning}. Although there is reduction in the size of the network when it is saved in sparse format, the acceleration depends on the availability of hardware that facilitate sparse multiplications. As pruning filters and complete layers aim at exploiting the available redundancy in the architecture or structure of neural networks, these pruning approaches are called \\emph{structured pruning}. \nPruning approaches can also be broadly classified into: data-dependent and data-independent methods. Data-dependent methods~ make use of the training data to identify filters to prune. Theis \\etal  and Molchanov \\etal  propose a greedy pruning strategy that identifies the importance of feature maps one at a time from the network and measures the effect of removal of the filters on the training loss. This means that filters corresponding to those feature maps that have least effect on training loss are removed from the network. Within data-independent methods~, the selection of CNN filters to be pruned are based on the statistics of the filter values. Li \\etal  proposed a straightforward method to calculate the rank of filters in a CNN. The selection of filters are based on the $\\ell_1$-norm, where the filter with the lowest norm is pruned away. He \\etal  employ a LASSO regression-based selection of filters to minimize the least squares reconstruction.\\par\nAlthough the above-mentioned approaches demonstrated that a neural network can be compressed without affecting the accuracy, the effect on robustness is largely unstudied.  Dhillon \\etal  proposed pruning a subset of activations and scaling up the survivors to show improved adversarial robustness of a network. Lin \\etal  quantize the precision of the weights after controlling the Lipschitz constant of layers. This restricts the error propagation property of adversarial pertubations within the neural network. Ye \\etal  evaluated the relationship between adversarial robustness and model compression in detail and show that naive compression has a negative effect on robustness. Gui \\etal  co-optimize robustness and compression constraints during the training phase and demonstrate improvement in the robustness along with reduction in the model size. However, these approaches have mostly been tested on image classification tasks and on smaller datasets only. Their effectiveness on safety-relevant automated driving tasks such as object detection and semantic segmentation tasks are not studied and remains an open research challenge.", "cites": [8234, 8236, 945, 8232, 688, 8233, 8230, 6318, 8231, 6319, 8235], "cite_extract_rate": 0.7333333333333333, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes multiple pruning techniques and connects them through the lens of structured vs. unstructured pruning and data dependency. It critically evaluates their impact on robustness and highlights limitations in their application to safety-critical tasks. The abstraction is evident in the classification of methods and identification of broader trends, such as the tension between model compression and adversarial robustness."}}
{"id": "28ea17f4-7eb4-44e0-85c5-640bd64d35c5", "title": "Quantization", "level": "subsection", "subsections": [], "parent_id": "e23cf554-7193-4f8f-a54e-3da02ce3d7c2", "prefix_titles": [["title", "Inspect, Understand, Overcome:\\\\A Survey of Practical Methods\\\\for AI Safety"], ["section", "Model Compression"], ["subsection", "Quantization"]], "content": "\\label{subsec:compression:quantization}\n\\sectionauthor{Firas Mualla\\textsuperscript{13}}\nQuantization of a random variable $x$ having a probability density function $f(x)$ is the process of dividing the range of $x$ into intervals, each is represented using a single value (also called reconstruction value), such that the following reconstruction error is minimized:\n\\begin{equation}\n\\label{eq:quantization}\n\\sum_{i=1}^{L} \\int_{b_i}^{b_{i+1}} (q_i - x)^2 f(x) dx,\n\\end{equation}\nwhere $b_i$ is the left-side border of the i-th interval, $q_i$ is its reconstruction value, and $L$ is the number of intervals, \\eg $L = 8$ for a 3-bit quantization. This definition can be extended to multiple dimensions as well. \nQuantization of neural networks has been around since the 1990s , however, with a focus in the early days on improving the hardware implementations of these networks. In the deep learning literature, a remarkable application of quantization combined with unstructured pruning can be found in the approach of \\textit{deep compression} , where 1-dimensional k-means is utilized to cluster the weights per layer and thus finding the $L$ cluster centers ($q_i$ values in (\\ref{eq:quantization})) iteratively. This procedure conforms to an implicit assumption that $f(x)$ has the same spread inside all clusters. Deep compression can reduce the network size needed for image classification by a factor of 35 for AlexNet and a factor of 49 for VGG-16 without any loss in accuracy. However, as pointed out in , these networks from the early deep learning days are over-parameterized and a less impressing compression factor is thus expected when the same technique is applied to lightweight architectures such as MobileNet and SequeezeNet. For instance, considering SqueezeNet (50 times smaller than AlexNet), the compression factor of deep compression without accuracy loss drops to about 10.\nCompared to scalar quantization used in deep compression, there were attempts to exploit the structural information by applying variants of vector quantization of the weights . Remarkably, in the latter (\\ie ), the reconstruction error of the activations (instead of the weights) is minimized in order to find an optimal codebook for the weights, as the ultimate goal of quantization is to approximate the network's output not the network itself. This is performed in a layer-by-layer fashion (as to prevent error accumulation) using activations generated from unlabeled data. \nOther techniques  apply variants of so-called ``linear'' \\textit{quantization}, \\ie the quantization staircase has a fixed interval size. This paradigm conforms to an implicit assumption that $f(x)$ in (\\ref{eq:quantization}) is uniform and is thus also called \\textit{uniform quantization}. The uniform quantization is widely applied both in specialized software packages such as the \\texttt{Texas Instruments Deep Learning Library} (automotive boards)  and in general-purpose libraries such as the \\texttt{Tensorflow Lite}. The linearity assumption enables practical implementations, as the quantization and dequantization can be implemented using a scaling factor and an intercept, whereas no codebook needs to be stored. In many situations, the intercept can be omitted by employing a symmetric quantization mapping. Moreover, for power of 2 ranges, the scaling ends up being a bitwise shift operator, where quantization and dequantization differ only in the shift direction. It is also straightforward to apply this scheme \\textit{dynamically}, \\ie for each tensor separately using a tensor-specific multiplicative factor. This can be easily applied not only to filters (weight tensors) but also to activation tensors (see for instance ).\nUnless the scale factor in the linear quantization is assumed constant by construction, it is computed based on the statistics of the relevant tensor and can be thus sensitive to outliers. This is known to result in a low precision quantization. In order to mitigate this issue, the original range can be \\textit{clipped} and thus reduced to the most relevant part of the signal. Several approaches are proposed in the literature for finding an optimal clipping threshold: simple percentile analysis of the original range (\\eg clipping 2\\% of the largest magnitude values), minimizing the mean square error between the quantized and original range in the spirit of (\\ref{eq:quantization}) , or minimizing the Kullback-Leibler divergence between the original and the quantized distributions . While the clipping methods trade off large quantization errors of outliers against small errors of inliers , other methods tackle the outliers problem using a different trade-off, see for instance the outlier channel splitting approach in .\nAn essential point to consider when deciding for a quantization approach for a given problem is the allowed or intended interaction with the training procedure. The so-called \\textit{post-training quantization}, \\ie quantization of a pre-trained network, seems to be attractive from a practical point of view: No access to training data is required and the quantization and training toolsets can be independent from each other. On the other hand, the training-aware quantization methods often yield higher inference accuracy and shorter training times. The latter is a serious issue for large complicated models which may need weeks to train on modern GPU clusters.\nThe training-aware quantization can be implemented by inserting fake quantization operators in the computational graph of the forward-pass during training (simulated quantization), whereas the backward pass is done as usual in floating-point resolution . Other approaches  go a step further by quantizing the gradients as well. This leads to much lower training time, as the time of the often computationally expensive backward pass is reduced. The gradient's quantization, however, is not directly applicable as it requires the derivative of the quantization function (staircase-like), which is zero almost everywhere. Luckily, this issue can be handled by employing a \\textit{straight-through estimator}  (approximating the quantization function by an identity mapping). There are also other techniques proposed recently to mitigate this problem .", "cites": [8241, 8375, 8239, 8240, 8238, 4496, 8237, 6488, 9139], "cite_extract_rate": 0.5294117647058824, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.3, "critical": 3.8, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple quantization approaches from various papers, connecting them through a conceptual framework that distinguishes between scalar, vector, uniform, and training-aware methods. It offers critical analysis by discussing limitations (e.g., sensitivity to outliers, instability in training) and trade-offs between different strategies. The content also abstracts key principles such as the impact of quantization on model accuracy, the role of training procedures, and the practical implications of different quantization assumptions."}}
