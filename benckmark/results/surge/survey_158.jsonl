{"id": "5f9ef219-fa93-4354-a024-7532ba2ec3d2", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "bdab4dd0-3cbb-4e33-92f9-7e53d3068eee", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Introduction"]], "content": "In the last years, Deep Learning algorithms have made an important and rapid progress in solving numerous tasks involving complex analysis of raw data. Among some relevant cases, it can be mentioned major advances in speech recognition and natural language processing , games , finacial market analysis , fraud and malware detection , prevention of DDoS attacks  and Computer Vision . In the field of Computer Vision, the Convolutional Neural Networks (CNNs) have become the state-of-the-art Deep Learning algorithms since \\citeauthor{Krizhevsky2012}  have presented innovative results in image classification tasks using the AlexNet architecture. Thereafter, motivated by the continuous popularization of GPUs and frameworks, the CNNs have kept growing in performance, being currently even used in security-critical apllications, such as medical sciences and diagnostics , autonomous vehicles , surveillance systems  and biometric and handwritten characterers recognition .\n{Nevertheless, some researchers have begun to argue if {the same deep learning algorithms, which could even surpass the human performance }, were actually robust enough to be used in safety-critical environments. Unfortunately, since the paper of \\citeauthor{Szegedy2013} , various works have highlighted the vulnerability of deep learning models in different tasks such as speech recognition , text classification , malware detection  and specially image classification  before \\textit{adversarial attacks}. Adversarial attacks are usually conducted in the form of subtle perturbations generated by an optimization algorithm and inserted into a legitimate image in order to produce an adversarial example which, in the field of Computer Vision is specifically known as \\textit{adversarial image}. After being sent to be classified, an adversarial image is often able to lead CNNs to produce a prediction different from the expected, usually with a high confidence. Adversarial attacks on image classifiers are the most common in the literature and, for this reason, are the focus of this paper.}\nThe vulnerability of CNNs and other Deep Learning algorithms to adversarial attacks have forced the scientific community to revisit all the processes related to the construction of intelligent models, from the elaboration of architectures to the formulation of the training algorithms used, as a attempt to hypothesize some possible reasons concerning this lack of robustness\\footnote{Robustness can be defined as the capacity of a model or defense to tolerate adversarial disturbances by delivering reliable and stable outputs .} and thus propose countermeasures that may hold future attacks of adversarial nature. This arms race between attacks and defenses against adversarial examples has ended up forming a recent research area called \\textit{Adversarial Machine Learning} that, in a nutshell, struggles to construct more robust Deep Learning models. \n{Adversarial Machine Learning in Image Classification} is currently a very active research path which is responsible for most of the work in the area, with novel papers produced almost daily. However, there is neither a known efficient solution for securing Deep Leanirng models nor any fully accepted explanations for the existence of adversarial images yet. Taking into account the dynamism and relevance of this research area, it is crucial to be available in literature comprehensive and up-to-date review papers in order to position and orientate their readers about the actual scenario. {Although there are already some extensive surveys , they have already become somewhat outdated due to the great activity in the area. Moreover, they bring out a general overview of the Adversarial Machine Learning field, what, in turn, contributes to these papers neither have focused enough in works that have proposed defenses against adversarial attacks nor have provided proper guidance for those who wishes to invest in novel countermeasures.}\nTherefore, keeping in mind the importance of Adversarial Machine Learning in Image Classification to the development of more robust defenses and architectures against adversarial attacks, this self-contained paper aims to provide for all readerships an exhaustive and detailed review of the {literature, however with a defender's perspective. The present survey covers from the background needed to clarify the reader essential concepts in the area to the techinical formalisms related to adversarial examples and attacks. Futhermore, a comprehensive survey of defenses and coutermeasures against adversarial attacks is made and categorized on a novel taxonomy}. Then, based on the works of \\citeauthor{carlini2017bypassing}  and \\citeauthor{carlini2019evaluating} , {the present paper discusses} some principles for designing and evaluating defenses which are intended to guide researchers to introduce stronger security methods. Essentially, the main contributions of this work are the following:\n\\begin{itemize}[leftmargin=*]\n    \\item The update of some existing taxonomies in order to categorize different types of adversarial images and novel attack approaches that have raised in literature;\\vspace{2mm}\n    \\item The discussion and organization of defenses against adversarial attacks based on a novel taxonomy;\\vspace{2mm}\n    \\item The address of relevant explanations for the existence of adversarial examples;\\vspace{2mm}\n    \\item {The provision of some important guidances that should be followed by researchers when devising and evaluating defenses;}\\vspace{2mm}\n    \\item {The discussion of promising research paths for future works in the field.}\\vspace{2mm}\n\\end{itemize}\nThe remaining of this paper is structured as follows: Section \\ref{sec:2} brings an essential background which covers important topics and concepts for the proper understanding of this work. Section \\ref{sec:3} formalizes and also categorizes adversarial examples and attacks. {Section \\ref{sec:4} makes a deep review on defenses existing in literature and proposes a novel taxonomy for organizing them.} Section \\ref{sec:5} addresses and formalizes relevant explanations for the existence of adversarial examples that have supported the development of attacks and defenses. Section \\ref{sec:6} provides close guidance based on relevant work to help defenders and reviewers to respectivelly design and evaluate security methods. Section \\ref{sec:7} lists promising research paths in Adversarial Machine Learning for Image Classification. Finally, Section \\ref{sec:8} brings the final considerations.", "cites": [889, 97, 891, 305, 894, 7305, 314, 885, 892, 893, 887, 888, 890, 313, 886, 301], "cite_extract_rate": 0.5, "origin_cites_number": 32, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The introduction synthesizes key developments in deep learning and adversarial machine learning, connecting multiple domains like speech, text, and image classification. It critically points out the limitations of existing defenses and outdated surveys, while abstracting broader challenges in robustness and evaluation. The section sets up a novel taxonomy and principles for evaluating defenses, showing strong analytical insight."}}
{"id": "2c36acfc-646e-456f-9c35-cd51a48ca374", "title": "Background", "level": "section", "subsections": ["36aed971-d615-46cb-8d38-3b23233d83a5", "769c2599-aa2f-4f89-83b5-d2350f22a212"], "parent_id": "bdab4dd0-3cbb-4e33-92f9-7e53d3068eee", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Background"]], "content": "\\label{sec:2}\nConventional Machine Learning models (also known as \\textit{shallow models} ) have begun to have high dependency of domain experts and present critical limitations when attempting to extract useful patterns from complex data, such as images and audio speech . Therefore, it has been necessary to develop traditional learning algorithms into more elaborated architectures, forming a recent area in Artificial Intelligence called \\textit{Deep Learning} .\nDeep Learning is a subfield of Machine Learning where its algorithms simulate the operation of the human brain in order to extract and learn hidden representations from raw inputs, oftentimes without any human intervention. Mostly, Deep Learning is based on Deep Neural Networks (DNNs), which are formed by many layers containing numerous processing units, which gathers knowledge from a massive amount of data by appling several linear and non-linear transformations on the received inputs, which in turn, allows these models to learn high-level abstractions from simpler concepts .\nThis paper focuses mostly on Convolutional Neural Networks. CNNs are a special type of Deep Neural Network and currently are the state-of-the-art algorithms for Image Classification . However, {Appendix A briefly covers other tasks Adversarial Machine Learning takes part in Computer Vision. The next section explains, in a nutshell, the main components of a CNN, in addition to list the state-of-the-art architectures over the years, according to the ILSVRC challenge .}", "cites": [166, 893, 895], "cite_extract_rate": 0.375, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of Deep Learning and CNNs, with minimal synthesis of ideas from the cited papers. It paraphrases common definitions and applications but does not critically analyze the cited works or abstract broader principles. The narrative is introductory and lacks depth or comparative or evaluative discussion."}}
{"id": "36aed971-d615-46cb-8d38-3b23233d83a5", "title": "Convolutional Neural Networks", "level": "subsection", "subsections": [], "parent_id": "2c36acfc-646e-456f-9c35-cd51a48ca374", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Background"], ["subsection", "Convolutional Neural Networks"]], "content": "CNN architectures usually perform feature learning by making use of \\textit{(i) convolution} and \\textit{(ii) pooling} layers which, respectivelly, extracts useful features from images and reduces their spatial dimensionality. After feature learning, comes the fully connected layer (FC), which works in a way similar to a common neural network. In a classification task, FCs produce a single probability vector as output, which is called the \\textit{probability vector}. The probability vector contains membership probabilities of a given input $x$ corresponding to each class $c_i \\in C_n$, where $C_n$ is the set containing all the $n$ classes belonging to the original problem. Summing up all the probabilities must result in 1, and the chosen class for $x$ is the one which has the highest membership probability. Figure \\ref{fig:cnn_pipeline} depicts an example of a standard architecture of a CNN.\n\\begin{figure}[h!]\n\t\\centering\n\t\\includegraphics[width=4in]{pictures/cnn_pipeline.jpg}\n\t\\caption{The standard architecture of a CNN. Adapted from \\citeauthor{Guo2016} .}\n\t\\label{fig:cnn_pipeline}\n\\end{figure}\nAn important contest in Computer Vision, called ILSVRC (ImageNet Large Scale Visual Recognition Challenge) , has encouraged until 2017 the creation of more accurate CNN architectures. Figure \\ref{fig:imagenet_contest} shows some relevant CNN architectures over the years in ILSVRC challenge, namely AlexNet , ZFNet , VGGNet , GoogLeNet , ResNet , TrimpsNet\\footnote{There has been no novel scientific contribution which justified the production of a paper, and for this reason, the authors of TrimpsNet only shared the results using the ImageNet and COCO joint workshop in ECCV 2016, which are available at: \\url{http://image-net.org/challenges/talks/2016/Trimps-Soushen@ILSVRC2016.pdf}. Accessed in February 12, 2020.} and SENet . Since 2015, CNNs have surpassed the human performance .\n\\begin{figure}[h!]\n    \\centering\n    \\includegraphics[scale=0.325]{pictures/ImageNet_contest.jpg}\n    \\caption[Performance of relevant CNN architectures in the ILSVRC top-5 error rate classification challenge over the years . Since 2015, the CNNs have surpassed the human's performance .]{Top-5 error rate\\footnotemark of winning CNN architectures in ILSVRC classification challenge over the years . Since 2015, the CNNs have surpassed the human's performance .}\n    \\label{fig:imagenet_contest}\n    \\vspace{-5mm}\n\\end{figure}\n\\footnotetext{In contrast to the traditional top-1 classification, a top-5 classification considers a\nmodel had a correct predition when, given a test sample, its true class is among the five highest output probabilites predicted by this model.}", "cites": [97, 499, 305, 514, 895], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of CNNs and references several influential architectures from the ILSVRC challenge, but it lacks integration of deeper insights from the cited papers. It summarizes the role of convolution, pooling, and FC layers without connecting these concepts to broader implications or trends in adversarial robustness. The content is largely factual and chronological, with minimal critical evaluation or abstraction into general principles."}}
{"id": "e137ca24-d9b5-4525-b7b7-48c6a2d7465c", "title": "Autoencoders", "level": "subsubsection", "subsections": [], "parent_id": "769c2599-aa2f-4f89-83b5-d2350f22a212", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Background"], ["subsection", "Other Deep Learning Algorithms"], ["subsubsection", "Autoencoders"]], "content": "An autoencoder is a neural network which aims to approximate its output to an input sample, or in other words, it tries to approximate an input $x$ to its \\textit{identity function} by generating an output $\\hat{x}$ as similiar as possible to $x$ from a compressed representation learnt. {An example of an autoencoder architecture is depicted by Figure \\ref{fig:autoencoder}.} Despite looking a trivial task, the autoencoder is actually trying to learn the inner representations of the input, regarding the structure of data. Autoencoders are useful for two main purposes: (i) dimensionality reduction, retaining only the most important data features  and (ii) data generation process . \n\\begin{figure}[h!]\n\t\\centering\n\t\\includegraphics[width=3.5in]{pictures/autoencoder.jpg}\n\t\\caption{{An example of an autoencoder architecture.}}\n\t\\label{fig:autoencoder}\n\t\\vspace{-3mm}\n\\end{figure}", "cites": [8394], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of autoencoders without effectively integrating or synthesizing insights from the cited paper. It merely defines the concept and lists uses without critical evaluation or broader abstraction. The mention of a figure does not compensate for the lack of analysis or synthesis."}}
{"id": "7d5b7288-aec6-4266-bdc0-2b33510dea8d", "title": "Generative Adversarial Networks", "level": "subsubsection", "subsections": [], "parent_id": "769c2599-aa2f-4f89-83b5-d2350f22a212", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Background"], ["subsection", "Other Deep Learning Algorithms"], ["subsubsection", "Generative Adversarial Networks"]], "content": "Generative Adversarial Networks (GANs) are a framework introduced by Goodfellow et al.  for building \\textit{generative models} $\\mathcal{P}_{G}$ which resembles the data distribution $\\mathcal{P}_{data}$ used in the training set. GANs can be used to improve the representation of data, to conduct \\textit{unsurpevised learning} and to even construct defenses against adversarial images . There are works that have also used GANs for other purposes, such as image-to-image translation and visual style transfer . The GANs are composed by two models (usually deep networks) trained simultaneously: a \\textit{generator} $G$ and a \\textit{discriminator} $D$. The \\textit{generator} receives an input $x$ and tries to generate an output $z$ from a probability distribution $\\mathcal{P}_G$. In contrast, the \\textit{discriminator} classifies $z$, producing a label that determines if $z$ belongs to the distribution $\\mathcal{P}_{data}$ (\\textit{benign} or \\textit{real input}) or $\\mathcal{P}_G$ (\\textit{fake} or \\textit{adversarial input}). In other words, the \\textit{generator} $G$ is actually being trained to fool the classifier $D$. In this competing scenario, GANs are usually capable of generating data samples that looks close to benign examples.", "cites": [896, 893, 7022], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of GANs and mentions their use in adversarial machine learning and image translation, referencing three papers. However, it lacks synthesis by not connecting the different applications of GANs to the defender's perspective or adversarial robustness. There is no critical evaluation of the cited works, and the abstraction remains minimal, focusing only on surface-level generalizations."}}
{"id": "8135abda-07d8-4d3a-8806-7ab663c9f9ba", "title": "Adversarial Images and Attacks", "level": "section", "subsections": ["23aaff3c-bf44-4273-88f0-23693318b3e3", "23103ae7-08fc-40e3-b61c-bc353e183cde", "95827fbf-2419-4189-be26-35cfbceaea2a"], "parent_id": "bdab4dd0-3cbb-4e33-92f9-7e53d3068eee", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Adversarial Images and Attacks"]], "content": "\\label{sec:3}\n{Formally, an adversarial image can be defined as follows:} let $f$ be a classification model trained with legitimate images (\\textit{i.e.} images which do not have any malicious perturbations) and let $x$ be a legitimate image (where $x \\in \\R^{w \\times h \\times c}$, such that $w$ and $h$ are the dimensions of the image and $c$ is its amount of color channels), then it is crafted, from $x$, an image $x'$, such that $x' = x + \\delta x$, where $\\delta x$ is the perturbation needed to make $x$ cross the decision boundary, resulting $f(x) \\neq f(x')$ {(see Figure \\ref{fig:dog_adv}a)}. The perturbation $\\delta x$ can also be interpreted as a vector $\\vec{\\delta x}$, where its magnitude $\\|\\vec{\\delta x}\\|$ represents the amount of perturbation needed to translate the point represented by the image $x$ in the space beyond the decision boundary. {Figure \\ref{fig:dog_adv}b illustrates a didactic example of inserting a perturbation $\\delta x$ into a legitimate image $x$ on a 2D space.} According to \\citeauthor{Cao} , an adversarial image is considered optimal if it satisfies two requirements: (i) if the perturbations inserted on this image are imperceptible to human eyes and (ii) if these perturbations are able to induce the classification model to produce an incorrect output, preferably with a high confidence.", "cites": [897], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of adversarial images and their definition, citing one paper by Cao. While it introduces the concept and some criteria for optimality, it lacks integration of multiple sources, deeper critical evaluation, and broader abstraction. The narrative remains centered on definitions and does not synthesize or analyze different attack types or defense implications."}}
{"id": "23aaff3c-bf44-4273-88f0-23693318b3e3", "title": "Taxonomy of Adversarial Images", "level": "subsection", "subsections": ["7c4f8291-bbd4-41f6-874f-2832421c6b1e", "6fc7b7a1-4f40-4d94-a0e4-3d54ffcba110", "ea977d10-b85a-4b6e-86da-b7eb1a816d4f"], "parent_id": "8135abda-07d8-4d3a-8806-7ab663c9f9ba", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Adversarial Images and Attacks"], ["subsection", "Taxonomy of Adversarial Images"]], "content": "\\label{sec:tax_advimages}\nThis section is based on the works of \\citeauthor{Barreno2010, huang2011adversarial, yuan2019adversarial, Kumar2017, Xiao2017} and \\citeauthor{ brendel2017decision}  to propose a broader\\footnote{{For comparative purposes, the novel topics proposed by this paper are underlined.}} taxomomy to adversarial images formed by three diferent axes: \\textit{(i) perturbation scope}, \\textit{(ii) perturbation visibility} and \\textit{(iii) perturbation measurement}. The next sections explain each axis in details.", "cites": [898, 7306, 893], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes ideas from multiple papers to propose a novel three-axis taxonomy for adversarial images, demonstrating strong integration of sources. It abstracts well by framing these concepts in a structured, conceptual model. However, the critical analysis is limited, as the section primarily introduces the taxonomy without evaluating its strengths, weaknesses, or comparing it to prior frameworks."}}
{"id": "7c4f8291-bbd4-41f6-874f-2832421c6b1e", "title": "Perturbation Scope", "level": "subsubsection", "subsections": [], "parent_id": "23aaff3c-bf44-4273-88f0-23693318b3e3", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Adversarial Images and Attacks"], ["subsection", "Taxonomy of Adversarial Images"], ["subsubsection", "Perturbation Scope"]], "content": "Adversarial images may contain individual-scoped perturbations or universal-scoped perturbations.\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Individual-scoped perturbations:} individual-scoped perturbations are the most common in literature. They are generated individually for each input image;\n    \\item \\textbf{Universal-scoped perturbations:} universal-scoped perturbations are image-agnostic perturbations, \\textit{i.e.} they are perturbations generated independently from any input sample. Nevertheless, when they are applied to an legitimate image, the resulting adversarial example is often able to lead models to misclassification . Universal perturbations permit adversarial attacks being conducted more easily in real-word scenarios, since these perturbations are crafted just once to be inserted into any sample belonging to a certain dataset. \n\\end{itemize}", "cites": [899], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual distinction between individual-scoped and universal-scoped perturbations, but it lacks deeper synthesis of ideas from multiple sources. It references one paper and merely paraphrases its focus without connecting it to broader trends or contrasting it with other works. There is minimal critical analysis or abstraction, as it does not evaluate the effectiveness or limitations of these perturbation types."}}
{"id": "6fc7b7a1-4f40-4d94-a0e4-3d54ffcba110", "title": "Perturbation Visibility", "level": "subsubsection", "subsections": [], "parent_id": "23aaff3c-bf44-4273-88f0-23693318b3e3", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Adversarial Images and Attacks"], ["subsection", "Taxonomy of Adversarial Images"], ["subsubsection", "Perturbation Visibility"]], "content": "The efficiency and visibility of perturbations can be organized as:\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Optimal perturbations:} these perturbations are imperceptible to human eyes, but are useful to lead deep learning models to misclassification, usually with a high confidence on the prediction;\n    \\item \\textbf{\\underline{Indistinguishable perturbations:}} indistinguishable perturbations are also imperceptible to human eyes, however they are insufficient to fool deep learning models; \n    \\item \\textbf{\\underline{Visible perturbations:}} perturbations that, when inserted into a image, are able to fool deep learning models. However they can also be easily spotted by humans ;\n    \\item {\\textbf{\\underline{Physical perturbations:}} are perturbations designed outside the digital scope and physically added to real-world objects themselves . Although some works have adapted physical perturbations to Image Classification , they are usually directed to tasks involving Object Detection  (see Appendix C);}\n    \\item {\\underline{\\textbf{Fooling images:}}} perturbations which corrupt images to the point of making them unrecognizable by humans. Nevertheless, the classification models believe these corrupted images belong to one of the classes of the original classification problem, sometimes assigning to them a high confidence on the prediction . Fooling images are also known as \\textit{rubbish class examples} ;\n    \\item \\underline{\\textbf{Noise}:} in contrast to the malicious nature of perturbations, noises are non-malicious or non-optimal corruptions that may be present or inserted into a input image. An example of noise is the gaussian noise.\n\\end{itemize}", "cites": [900, 902, 7307, 892, 904, 901, 903], "cite_extract_rate": 0.875, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive classification of perturbation types but lacks meaningful synthesis of the cited papers. It briefly mentions physical perturbations and references an appendix for object detection, but does not connect or integrate the key contributions of the cited works into a broader framework. There is minimal critical analysis or abstraction to highlight general trends or principles."}}
{"id": "ea977d10-b85a-4b6e-86da-b7eb1a816d4f", "title": "Perturbation Measurement", "level": "subsubsection", "subsections": [], "parent_id": "23aaff3c-bf44-4273-88f0-23693318b3e3", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Adversarial Images and Attacks"], ["subsection", "Taxonomy of Adversarial Images"], ["subsubsection", "Perturbation Measurement"]], "content": "Given the fact that it is difficult to define a metric that measures the capability of human vision, the \\textit{p-norms} are most used to control the size and the amount of the perturbations that are inserted into a image . The p-norm $L_p$ computes the distance $\\norm{x - x'}_p$ in the input space between a legitimate image $x$ and the resulting adversarial example $x'$, where $p \\in \\{0, 1, 2, \\infty\\}$. In Equation \\ref{eq:2} is defined the p-norm when $p=1$ (Manhattan Distance) and $p=2$ (Euclidean Distance):\n\\vspace{-4mm}\n\\begin{equation}\\label{eq:2}\n    L_p = \\sqrt[p]{\\sum\\abs{x - x'}^{p}}\n\\end{equation}\n\\noindent When $p=0$, it is counted up the number of pixels that have been modified in a legitimate sample in order to generate the adversarial image. On the other hand, the $L_\\infty$ measures the maximum difference among all pixels in the corresponding positions between two images. For $L_\\infty$ norm, each pixel is allowed to be modified within a maximum limit of perturbation, without having any restriction for the number of modified pixels. Formally, $L_\\infty = \\norm{x - x'}_{\\infty} = \\max\\Big({\\abs{x_1 - x'_1}, \\abs{x_2 - x'_2}, \\cdots, \\abs{x_n - x'_n}}\\Big)$. Despite the norms $p \\in \\{0, 1, 2, \\infty\\}$ be the most used when computing perturbations, there are some works that have defined custom metrics, as can be seen in Table \\ref{tbl:attacks}.\n\\begin{figure}[h!]\n    \t\\centering\n    \t\\includegraphics[width=0.8\\textwidth]{pictures/dog_adv.jpg}\n    \t\\caption{(a): Malicious and usually imperceptible perturbations present in a input image can induce trained models to misclassification. Adapted from \\citeauthor{klarreich2016learning} . (b): The objective of an adversarial attack is to generate a perturbation $\\delta x$ and insert it into a legitimate image $x$ in order to make the resulting adversarial image $x' = x + \\delta x$ cross the decision boundary. Adapted from \\citeauthor{bakhti2019ddsa} .}\n    \t\\label{fig:dog_adv}\n    \t\\vspace{-5mm}\n    \\end{figure}", "cites": [905], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of p-norms used to measure perturbations in adversarial images but lacks deeper synthesis of ideas from multiple sources. It does not critically evaluate the effectiveness or limitations of these norms or their relevance to defender strategies. Generalization is minimal, focusing largely on definitions and a single cited paper without exploring broader implications or trends."}}
{"id": "23103ae7-08fc-40e3-b61c-bc353e183cde", "title": "{Taxonomy of Attacks and Attackers", "level": "subsection", "subsections": ["90313b9f-d6e2-45a3-9879-4420c974328a", "bd9d1e13-eb76-4deb-b9d8-3f48cf162688", "e52be30c-b21f-4b49-b00f-e1419b8c7c61", "3853569a-8157-47bc-9ca6-5914da70a14c", "d76c7890-1bb6-4be6-abd3-f424ed29b040", "d8fac071-1db7-485a-88c3-982f5540bff2"], "parent_id": "8135abda-07d8-4d3a-8806-7ab663c9f9ba", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Adversarial Images and Attacks"], ["subsection", "{Taxonomy of Attacks and Attackers"]], "content": "} \\label{sec:tax_threat}\nThis section is also based on the concepts and definitions of the works of \\citeauthor{Barreno2010, Kumar2017, Xiao2017, brendel2017decision, akhtar2018threat} and \\citeauthor{yuan2019adversarial}  {to extend\\footnote{Again here, the novel topics proposed by this paper are highlighted by undelined font.} existing taxonomies which organize attacks and attackers. In the context of security, adversarial attacks and attackers are categorized under \\textit{threat models}. A threat model defines the conditions under which a defense is designed to provide security garantees against certain types of attacks and attackers .} Basically, a threat model delimiters (i) the knowledge an attacker has about the targeted classifier (such as its parameters and architecture), (ii) his goal with the adversarial attack and (iii) how he will perform the adversarial attack. A threat model can be then classified into six different axes: \\textit{(i) attacker's influency}, \\textit{(ii) attacker's knowledge}, \\textit{(iii) security violation}, \\textit{(iv) attack specificity}, \\textit{(v) attack computation} and \\textit{(vi) attack approach}.", "cites": [7305, 898, 7306, 313, 893], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple taxonomies from the cited works and organizes them under the concept of threat models, which provides a structured perspective on attacks and attackers. While it offers a coherent integration of ideas, the critical evaluation is limited—few limitations or comparative weaknesses of the cited frameworks are discussed. The abstraction is moderate, as it introduces a general framework (threat model axes) but does not deeply explore overarching principles or trends in the field."}}
{"id": "bd9d1e13-eb76-4deb-b9d8-3f48cf162688", "title": "Attacker's Knowledge", "level": "subsubsection", "subsections": [], "parent_id": "23103ae7-08fc-40e3-b61c-bc353e183cde", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Adversarial Images and Attacks"], ["subsection", "{Taxonomy of Attacks and Attackers"], ["subsubsection", "Attacker's Knowledge"]], "content": "\\label{sec:black_white_box_attacks}\nTaking into consideration the attacker's knowlegde with respect to the targeted model, three types of attacks can be performed: \\textit{(i) white-box attacks}, \\textit{(ii) black-box attacks} and \\textit{(iii) grey-box attacks}.\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{White-box attacks}: in a white-box attack, the attacker has fully access to the model's and even the defense's parameters and architectures, whenever such defense exists. This attack scenario probably would be the least frequent in real-world applications, due to the adoption of protection measures (such as users control, for example) in order to prevent unauthorized people access to the system components. By contrast, white-box attacks are usually the most powerful type of adversarial attack, and for this reason, are commonly used to evaluate the robustness of defenses and/or classification models when they are undergone to harsh conditions. Unfortunately, elaborating countermeasures resistant to white-box attacks is, so far, an open problem;\n    \\item \\textbf{Black-box attacks}: in this scenario, the attacker neither has access nor knowledge about any information concerning the classification model and the defense method, when present. Black-box attacks impose more restrictions to attackers, nonetheless they are important when reproducing external adversarial attacks aiming deployed models, which in turn, better represent real-world scenarios . Despite the greater difficulty to perform black-box attacks, the attacker still might be able to evade the target model due to the \\textit{transferability of adversarial examples}. Works such as \\citeauthor{Szegedy2013} and \\citeauthor{Papernot2016c}  have shown the malicious effect of an adversarial image, generated using a certain classifier, is able to transfer and fool other classifiers, including the ones created by different learning algorithms (check Section \\ref{subsec:transferability} for more details). With this property in favor of the attacker, he can create an empirical model through a causative attack called \\textit{substitute} or \\textit{surrogate model}, which has similar parameters to the targeted model's. Therefore, the attacker can use this surrogate model to craft adversarial images and, afterwards, deploy them to be, oftentimes, misclassified by the targeted model;\n    \\item \\underline{\\textbf{Grey-box attacks}:} this attack scenario has been firstly proposed by \\citeauthor{Meng2017} . In grey-box attacks, the attacker has access to the classification model, but does not have access to any information concerning the defense method. Grey-box attacks are an intermediate alternative to evaluate defenses and classifiers, since they impose a greater threat level when compared to the black-box attacks, but without giving a wide advantage to the attacker when providing him as well with all the information concerning the defense method, as performed in white-box scenarios.\n\\end{itemize}", "cites": [905, 314], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"error": "Failed to parse LLM response", "raw_response": "{\n    \"type\": \"descriptive\",\n    \"scores\": {\"synthesis\": 2.0, \"critical\": 2.0, \"abstraction\": 2.0},\n    \"insight_level\": \"low\",\n    \"analysis\": \"The section provides a basic classification of attack types based on the attacker's knowledge but primarily describes each type without synthesizing deeper connections between the cited works. It mentions transferability and surrogate models in passing, referencing \\citeauthor{Szegedy2013} and \\citeauthor{Papernot2016c}, but does not engage in critical "}}
{"id": "d8fac071-1db7-485a-88c3-982f5540bff2", "title": "Attack Approach", "level": "subsubsection", "subsections": [], "parent_id": "23103ae7-08fc-40e3-b61c-bc353e183cde", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Adversarial Images and Attacks"], ["subsection", "{Taxonomy of Attacks and Attackers"], ["subsubsection", "Attack Approach"]], "content": "Adversarial attacks can also be organized with respect to the approach used by the attack algorithm to craft the perturbation. According to , the approach of adversarial attacks can be based on \\textit{(i) gradient}, \\textit{(ii) transferibility/score}, \\textit{(iii) decision} and \\textit{(iv) approximation}.\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Gradient-based attacks:} this attack approach is the most used in literature. The gradient-based algorithms make use of detailed information of the target model concerning its gradient with respect to the given input. This attack approach is usually performed in white-box scenarions, when the attacker has full knowledge and access to the targeted model;\n    \\item \\textbf{Transfer/Score-based attacks:}\n    these attack algortihms either depend on getting access to the dataset used by the targeted model or the scores predicted by it in order to approximate a gradient. Usually, the outputs obtained by querying a targeted deep neural network are used as scores. These scores are then used along with the training dataset to fit a surrogate model which will craft the perturbations that will be inserted into the legitimate images. This attack approach is often useful in black-box attacks;\n    \\item \\underline{\\textbf{Decision-based attacks:}} this approach has been firstly introduced by \\citeauthor{brendel2017decision} , and it is considered by the authors as a simpler and more flexible approach, since requires few changes in parameters than gradient-based attacks. A decision-based attack usually queries the softmax layer of the targeted model and, iteratively, computes smaller perturbations by using a process of rejection sampling;\n    \\item \\underline{\\textbf{Approximation-based attacks:}} attacks algorithms based on this approach try to approximate a gradient for some targeted model or defense formed by a non-differentiable technique usually by applying numerical methods. These approximated gradients are then used to compute adversarial perturbations.\n\\end{itemize}", "cites": [898], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of different attack approaches, citing one primary paper for the decision-based category. It integrates this information minimally by categorizing attack methods, but lacks deeper synthesis of multiple sources or critical evaluation of the strengths and weaknesses of each approach. The generalization to broader patterns or principles is limited, indicating a low level of insight."}}
{"id": "95827fbf-2419-4189-be26-35cfbceaea2a", "title": "Algorithms for Generating Adversarial Images", "level": "subsection", "subsections": ["8d86abe8-410b-49ab-adcc-f303fd10c1f4", "dc53391b-c395-4444-9888-240bc32b4587", "f395510e-bbe5-4ce7-b4ae-26e819286c31", "38d660ca-ec12-4ec9-b16e-92114c71c813"], "parent_id": "8135abda-07d8-4d3a-8806-7ab663c9f9ba", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Adversarial Images and Attacks"], ["subsection", "Algorithms for Generating Adversarial Images"]], "content": "In Computer Vision, the algorithms used to generate adversarial perturbations are optimization methods that usually explore generalization flaws in pretrained models in order to craft and insert perturbations into legitimate images. The next sections will describe with more details four attack algorithms frequently used, namely \\textit{(i) FGSM} , \\textit{(ii) BIM} , \\textit{(iii) DeepFool}  and \\textit{(iv) CW Attack} . Afterwards, Table \\ref{tbl:attacks} organizes, according to the taxonomies presented in Sections \\ref{sec:tax_advimages} and \\ref{sec:tax_threat}, other important attack algorithms.", "cites": [892, 902, 906, 890], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal synthesis by listing attack algorithms without connecting their underlying principles or implications. It lacks critical analysis, merely describing the existence of these algorithms without evaluating their effectiveness or limitations. There is little to no abstraction or generalization to broader patterns or principles in adversarial image generation."}}
{"id": "8d86abe8-410b-49ab-adcc-f303fd10c1f4", "title": "Fast Gradient Sign Method (FGSM)", "level": "subsubsection", "subsections": [], "parent_id": "95827fbf-2419-4189-be26-35cfbceaea2a", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Adversarial Images and Attacks"], ["subsection", "Algorithms for Generating Adversarial Images"], ["subsubsection", "Fast Gradient Sign Method (FGSM)"]], "content": "FGSM is a sequential algorithm proposed by \\citeauthor{Goodfellow2014}  to sustain his linear hypothesis for explaining the existence of adversarial examples (see Section \\ref{subsec:linear_hyp}). The main characteristic of FGSM is its low computational cost, resulted from perturbing, in just one step (limited by a given upper bound $\\epsilon$), a legitimate image at the direction of the gradient that maximizes the model error. Despite its efficiency, the perturbations generated by FGSM are usually greater and less effective to fool models than the perturbations generated by iterative algorithms. Given an image $x \\in \\mathbb{R}^{w \\times h \\times c}$, FGSM generates an adversarial image $x'$ according to Equation \\ref{eq:fgsm}:\n    \\begin{equation}\\label{eq:fgsm}\n        x' = x - \\epsilon \\cdot sign(\\vec{\\nabla}_x J(\\Theta, x, y))\n    \\end{equation}\n\\noindent where $\\vec{\\nabla}_x$ represents the gradient vector, $\\Theta$ represents the network parameters, $y$ the class associated to $x$, $\\epsilon$ the maximum amount of perturbation that can be inserted into $x$ and $J(\\Theta, x, y)$ the cost function used to train the neural network.", "cites": [892], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of the FGSM algorithm, its computational efficiency, and its relation to the linear hypothesis. It cites one paper and explains the method's mathematical formulation. However, it lacks synthesis with other works, critical evaluation of the method’s effectiveness or limitations, and abstraction to broader patterns or principles in adversarial image generation."}}
{"id": "dc53391b-c395-4444-9888-240bc32b4587", "title": "Basic Iterative Method (BIM)", "level": "subsubsection", "subsections": [], "parent_id": "95827fbf-2419-4189-be26-35cfbceaea2a", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Adversarial Images and Attacks"], ["subsection", "Algorithms for Generating Adversarial Images"], ["subsubsection", "Basic Iterative Method (BIM)"]], "content": "This attack is a iterative version of FGSM, initially proposed by \\citeauthor{Kurakin2016a} . In constrast to FGSM, BIM executes several minor steps $\\alpha$, where the total size of the perturbation is limited by an upper bound defined by the attacker. Formally, BIM can be defined as a recursive method, which generates $x'$ according to Equation \\ref{eq:bim}:\n    \\begin{equation}\\label{eq:bim}\n        x' = \n        \\begin{cases}\n            x'_0 = 0 \\\\\n            x'_i = x'_{i-1} - \\mathit{clip}(\\alpha \\cdot \\mathit{sign}({\\vec{\\nabla}_x J(\\Theta, x'_{i-1}, y)})\n        \\end{cases}\n    \\end{equation}\nwhere $clip$ limits values to the lower and higher edges  outside the given interval.", "cites": [902], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the Basic Iterative Method (BIM) and its relationship to FGSM by referencing a single paper. It lacks synthesis of ideas from multiple sources and does not offer critical analysis or abstract insights into the broader implications or effectiveness of the method. The explanation is largely factual and does not contribute to a deeper understanding of the landscape of adversarial attacks."}}
{"id": "f395510e-bbe5-4ce7-b4ae-26e819286c31", "title": "DeepFool", "level": "subsubsection", "subsections": [], "parent_id": "95827fbf-2419-4189-be26-35cfbceaea2a", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Adversarial Images and Attacks"], ["subsection", "Algorithms for Generating Adversarial Images"], ["subsubsection", "DeepFool"]], "content": "The main ideia behind DeepFool, proposed by \\citeauthor{Moosavi-Dezfooli2015} , consists of finding the nearest decision boundary of a given legitimate image $x$ and then subtly perturb this image to make it cross the boundary and fool the classifier. Basically, DeepFool approximates, for each iteration, the solution of this problem by linearizing the classifier around an intermediate $x'$. The intermediate $x'$ is then updated towards the direction of an optimal direction by a small step $\\alpha$. This process is repeated until the small perturbation computed by DeepFool makes $x'$ cross the decision boundary. Similarly to FGSM, DeepFool is also based on the linearity hypothesis to craft perturbations.", "cites": [906], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual summary of the DeepFool algorithm, describing its core idea and iterative process based on the cited paper. It lacks critical analysis of the method's strengths, weaknesses, or effectiveness, and does not compare it meaningfully with other approaches like FGSM. There is minimal synthesis or abstraction beyond the specific algorithm."}}
{"id": "38d660ca-ec12-4ec9-b16e-92114c71c813", "title": "{Carlini \\& Wagner Attack", "level": "subsubsection", "subsections": [], "parent_id": "95827fbf-2419-4189-be26-35cfbceaea2a", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Adversarial Images and Attacks"], ["subsection", "Algorithms for Generating Adversarial Images"], ["subsubsection", "{Carlini \\& Wagner Attack"]], "content": "}\nThe CW attack has been proposed by \\citeauthor{carlini2017towards}  and currently represents the state-of-the-art algorithm for generating adversarial images. Formally, given an DNN $f$ having a logits layer $z$ and a input image $x$ belonging to a class $t$, CW uses the gradient descent to solve iteratively Equation \\ref{eq:cw1}:\n\\begin{equation}\\label{eq:cw1}\n    \\text{minimize } ||{x - x'}||\\SPSB{2}{2} + c \\cdot \\ell(x')\n\\end{equation}\n where, for $x$, the attack seeks for a small perturbation $\\delta_x = x - x'$ that is able to fool the classifier. To do so, a hyperparameter $c$ is used as an attempt to compute the minimal amount of perturbation required. Besides $c$, there is the cost function $\\ell(x')$, which is defined according to Equation \\ref{eq:cw2}.\n\\begin{equation}\\label{eq:cw2}\n    \\ell(x') = \\mathit{max}{(\\mathit{max}\\{{z(x')_i : i \\neq t}\\} - z(x')_t, - conf)}\n\\end{equation} \nIn Equation \\ref{eq:cw2}, the hyperparameter $conf$ refers to the attack confidence rate. Higher $conf$ values contribute to generate adversarial images capable of fooling models with a high confidence rate, \\textit{i.e.} with predictions reaching probabilities up to 100\\% in a incorrect class $t' \\neq t$. On the other hand, higher $conf$ values also produce adversarial images usually containing larger perturbations which are easily perceptible by humans. \n\\begin{table}[h!]\n\\centering\n\\begin{adjustbox}{max width=\\textwidth}\n\\begin{threeparttable}\n\\caption{Main adversarial attack algorithms in Computer Vision.\\tnote{*}}\n {\\scriptsize \n  \\setlength\\tabcolsep{4pt} \n\\begin{tabular}{lcccccc}\n\\toprule\nAlgorithm and Reference & Perturbation Scope & Perturbation Visibility & Perturbation Measurement & Attacker's Knowledge & Attack Specificity & Attack Approach \\\\ \\hline\nFGSM  & individual & optimal, visible & $L_\\infty$ & white-box & untargeted & gradient \\\\\nJSMA  & individual & optimal & $L_0$ & white-box & targeted & gradient \\\\\nL-BFGS  & individual & optimal & $L_\\infty$ & white-box & targeted & gradient \\\\ POBA-GA  & individual & optimal & custom & black-box & targeted, untargeted & decision \\\\ AutoZoom  & individual & optimal & $L_2$ & black-box & targeted, untargeted & decision \\\\\nDeepFool  & individual, universal & optimal & $L_1$, $L_2$, $L_\\infty$ & white-box & untargeted & gradient \\\\\nLaVAN  & individual, universal & visible & $L_2$ & white-box & targeted & gradient \\\\ Universal Adversarial Networks (UAN)  & universal & optimal & $L_2$, $L_{\\infty}$ & white-box & targeted & gradient \\\\ Expectation Over Transformation (EOT)  & individual & optimal & $L_2$ & white-box & targeted & gradient \\\\ Local Search Attack (LSA)  & individual & optimal & $L_0$ & black-box & targeted, untargeted & gradient \\\\ Natural Evolutionary Strategies (NES)  & individual & optimal & $L_\\infty$ & black-box & targeted & approximation \\\\\nBoundary Attack (BA)  & individual & optimal & $L_2$ & black-box & targeted, untargeted & decision \\\\\nCW Attack  & individual & optimal & $L_0$, $L_2$, $L_\\infty$ & white-box & targeted, untargeted & gradient \\\\\nGenAttack  & individual & optimal & $L_2$, $L_\\infty$ & black-box & targeted & decision \\\\\nBIM and ILCM  & individual & optimal & $L_\\infty$ & white-box & untargeted & gradient \\\\\nMomentum Iterative Method (M-BIM)  & individual & optimal & $L_\\infty$ & white-box, black-box & untargeted & gradient \\\\\nZeroth-Order Optimization (ZOO)  & individual & optimal & $L_2$ & black-box & targeted, untargeted & transfer, score \\\\ Hot-Cold Attack  & individual & optimal & $L_2$ & white-box & targeted & gradient \\\\\nProjected Gradient Descent (PGD)  & individual & optimal & $L_1$, $L_\\infty$ & white-box & targeted & gradient \\\\\nUPSET  & universal & optimal & $L_2$ & black-box & targeted & gradient \\\\\nANGRI  & individual & optimal & $L_2$ & black-box & targeted & gradient \\\\\nElastic-Net Attack (EAD)  & individual & optimal & $L_1$ & white-box & targeted, untargeted & gradient \\\\\nHop-Skip-Jump Attack (HSJ)  & individual & optimal & $L_2$, $L_\\infty$ & black-box & targeted, untargeted & decision \\\\\nRobust Physical Perturbations (RP2)  & individual & physical & $L_1$, $L_2$ & white-box & targeted & gradient \\\\\nGround-Truth Attack  & individual & optimal & $L_1$, $L_\\infty$ & white-box & targeted & gradient \\\\ OptMargin  & individual & optimal & $L_01$, $L_2$, $L_\\infty$ & white-box & targeted & gradient \\\\\nOne-Pixel Attack  & individual & visible & $L_0$ & black-box & targeted, untargeted & decision \\\\ BPDA  & individual & optimal & $L_2, L_\\infty$ & black-box & untargeted, targeted & approximation \\\\ SPSA  & individual & optimal & $L_\\infty$ & black-box & untargeted & approximation \\\\\nSpatially Transformed Network (stAdv)  & individual & optimal & custom & white-box & targeted & gradient \\\\ AdvGAN  & individual & optimal & $L_2$ & grey-box, black-box & targeted & gradient\\\\\nHoudini  & individual & optimal & $L_2$, $L_\\infty$ & black-box & targeted & gradient \\\\\nAdversarial Transformation Networks (ATNs)  & individual & optimal & $L_\\infty$ & white-box & targeted & gradient \\\\ \\bottomrule\n\\end{tabular}\n\\label{tbl:attacks}\n\\begin{tablenotes}\n  \\normalsize\n  \\item[*]The axis \\textit{Attacker's Influence} is not present in Table \\ref{tbl:attacks} because it does not depend of any aforementioned attack algorithm. Similarly, the axis \\textit{Attack Computation} is not also present because, except from FGSM and L-BFGS, all other attacks mentioned in Table \\ref{tbl:attacks} have their respective perturbations computed iterativelly.\n\\end{tablenotes}}\n\\end{threeparttable}\n\\end{adjustbox}\n\\vspace{-5mm}\n\\end{table}", "cites": [902, 898, 916, 911, 906, 894, 913, 908, 917, 918, 8395, 7307, 314, 892, 901, 910, 912, 920, 909, 7310, 915, 914, 7308, 890, 907, 7309, 919], "cite_extract_rate": 0.875, "origin_cites_number": 32, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a comparative overview of various adversarial attack algorithms, particularly highlighting the Carlini & Wagner (CW) attack. It integrates information from multiple cited works through a well-structured table, showing attack characteristics like perturbation scope and measurement. However, it lacks deeper synthesis of ideas across papers, and the critical evaluation is limited to surface-level observations (e.g., visibility of perturbations and attacker knowledge). There is minimal abstraction or identification of broader principles, focusing instead on factual and methodological comparisons."}}
{"id": "c28062c9-9a7d-41de-8763-8cf9b43bdb63", "title": "Defenses against Adversarial Attacks", "level": "section", "subsections": ["e0109f5b-4e55-4e2a-8cac-3e5cbde98360"], "parent_id": "bdab4dd0-3cbb-4e33-92f9-7e53d3068eee", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Defenses against Adversarial Attacks"]], "content": "\\label{sec:4}\nThe menace of adversarial images has encouraged the scientific community to elaborate several approaches to defend classification models. However, design such countermeasures has shown to be a difficult task once adversarial inputs are solutions to an optimization problem that is non-linear and non-convex. Since good theoretical tools for describing the solutions to these optimization problems do not exist, it is very hard to put forward a theoretical argument ensuring a defense strategy will be efficient against adversarial examples . Therefore, the existing defense mechanisms have some limitations in the sense that they can provide robustness against attacks in specific threat models. The design of a robust machine learning model against all types of adversarial images and other examples is still an open research problem \\cite[p.~27]{chakraborty2018adversarial}.", "cites": [7306], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces the general challenge of defending against adversarial attacks but fails to synthesize or integrate information from the cited paper meaningfully. It lacks critical evaluation of defense mechanisms and does not compare or contrast different approaches. The abstraction is minimal, focusing only on the difficulty of designing defenses without identifying broader patterns or principles."}}
{"id": "ab739ad3-492d-47cf-bd3d-32d57ae8a509", "title": "Gradient Masking:", "level": "paragraph", "subsections": [], "parent_id": "1baf327e-5751-4c45-a952-f6746d4bec49", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Defenses against Adversarial Attacks"], ["subsection", "Taxonomy of Defenses Against Adversarial Attacks"], ["subsubsection", "Defense Aproach"], ["paragraph", "Gradient Masking:"]], "content": "} defenses based on gradient masking (effect also known as \\textit{obfuscated gradient} )  produce, sometimes unintentionally, models containing smoother gradients that hinders optimization-based attack algorithms from finding wrong directions in space, \\textit{i.e.} without useful gradients for generating adversarial examples. According to \\citeauthor{athalye2018obfuscated} , defenses based on gradient masking can be organized in: \\textit{(i) shattered gradients, (ii) stochastic gradients} and \\textit{(iii) exploding/vanishing gradients}.\n\\begin{itemize}[leftmargin=*]\n    \\item \\textit{Shattered gradients:} are caused by non-differentiable defenses, thus introducing nonexistent or incorrect gradients;\n    \\item \\textit{Stochastic gradients:} are caused by randomized proactive/reactive defenses or randomized preprocessing on inputs before being fed to the classifier. This strategy of gradient masking usually leads an adversarial attack to incorrectly estimate the true gradient;\n    \\item \\textit{Exploding/vanishing gradients:} are caused by defenses formed by very deep architectures, usually consisting of multiple iterations of a neural network evaluation, where the output of one layer is fed as input of the next layer.\n\\end{itemize}\n\\begin{figure*}[h!]\n\t\\centering\n\t\\includegraphics[width=0.7\\textwidth]{pictures/adversarial_training.jpg}\n\t\\caption{Adversarial training increases the robustness of classifiers by training them using an augmented training dataset containing adversarial images. Adapted from \\citeauthor{shen2017ape} .}\n\t\\label{fig:adv_train}\n\\end{figure*}\nBasically, there are many countermeasures based on different strategies of gradient masking, as can be seen in Table \\ref{tbl:countermeasures}. However, two distinct strategies are frequently mentioned by related work in literature, which in turn make them relevant to describe in more details: \\textit{(i) Adversarial Training} and \\textit{(ii) Defensive Distillation}. \nDefenses based on adversarial training are usually considered in literature a brute force approach to protect against adversarial examples. Essentially, the main objetive of adversarial training is to make a classification model more robust by training it in a dataset containing legitimate and adversarial images. Formally, given a tuple $X = (x, y)$, where $x$ is a legitimate image, $y$ the class $x$ belongs to and $T$ a training dataset having only the tuple $X$\\footnote{For didactic purposes, consider the training dataset $T$ formed by only one image.}, such as $T = \\{X\\}$, an adversarial image $x'$ is crafted from $x$ by an attack algorithm $A$, thus forming a new tuple $X'$ that will have the same label $y$ of the clean image $x$, such that $X' = \\{x', y\\}, x' = A(x)$. Afterwards, the training dataset $T$ is augmented with $X'$ and now contains two image tuples: $T' = \\{X, X'\\}$. The learning model is then retrained using the training dataset $T'$, resulting in a theoretically stronger model (see Figure \\ref{fig:adv_train}).\nDespite the good results adversarial training has presented in several works , this gradient masking approach has basically two issues. The first issue is related to the strong coupling adversarial training has with the attack algorithm used during the training process. Retraining a model with adversarial training does not produce a generic model which is able to resist against evasions of adversarial images generated by a different attack algorithm not used in the training process. In order to have a more generic model, it would be necessary to elaborate a training dataset $T$ with a massive amount of adversarial images generated using different attack algorithms and amounts of disturbance. Therefore, the second issue concerning adversarial training raises: this is a procedure computationally inefficient, given two facts: (i) the great number of adversarial images that must be crafted from different attacks, which in turn does not guarantee robustness against adversarial images generated from more complex algorithms and (ii) after generating these malicious images, the model must train using a much larger dataset, which exponentially grows the training time. A robust defense method must be decoupled from any attack algorithm to increase its generalization. Notwithstanding the drawbacks, \\citeauthor{madry2017towards}  proposed training on adversarial samples crafted using Projected Gradient Descent (PGD) attack which is, by the time of this writing, the most promising defense present in literature, since it has shown robustness to various types of attacks in both white-box and black-box settings . However, their method is not model-agnostic and,\ndue to computational complexities, it has not been tested\non large-scale datasets such as ImageNet .\nDefensive Distillation, in turn, is a proactive defense initially proposed by \\citeauthor{Papernot2016b} . This countermeasure is inspired by a technique based on transfer of knowledge among learning models known as distillation . In learning distillation, the knowledge acquired by a complex model, after being trained using a determined dataset, is transfered to a simpler model. In a similar way, defensive distillation firstly trains a model $f$ using a dataset containing samples $X$ and labels $Y$ with a temperature $t$, resulting as output a probabilistic vector $f(X)$. The label set $Y$ is then replaced by the probabilistic vector $f(X)$ and a model $f^d$ with the same architecture of $f$ is created and trained with the sample set $X$, but now using as labels the novel label set $f(X)$. By the end of training, the destilled probabilistic output $f^d(X)$ is produced. Figure \\ref{fig:defensive_dist} depicts the schematic model of defensive distillation.\n\\begin{figure*}[h!]\n\t\\centering\n\t\\includegraphics[width=0.75\\textwidth]{pictures/distillation.jpg}\n\t\\caption{Schematic model of defensive distillation .}\n\t\\label{fig:defensive_dist}\n\t\\vspace{-3mm}\n\\end{figure*}\nDefenses based on gradient masking usually produce models containing smoother gradients in certain regions of space, making harder for the attacker to find promising directions to perturb an image. However, the attacker can instead use an non-differentiable attack, such as BPDA  or SPSA  as well as perform a black-box attack by training a surrogate model. This surrogate model reproduces the behaviour of the targeted model, since the attacker queries it using images carefully crafted by him and watches the outputs the targeted model gives. Then, the attacker takes advantage of the transferibility property of adversarial examples by using the gradients of the surrogate model in order to craft the images that will also lead the target model to misclassifications . Section \\ref{subsec:transferability} gives more information regarding the transferability property of adversarial examples.", "cites": [921, 917, 912, 8395, 924, 314, 923, 891, 925, 7311, 681, 892, 922], "cite_extract_rate": 0.8125, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers to present a coherent framework of gradient masking defenses, particularly through the taxonomy introduced by Athalye et al. (2018). It critically discusses limitations of adversarial training, such as its strong coupling to attack algorithms and computational inefficiency, and highlights the trade-offs of defensive distillation. While it generalizes the concept of gradient masking and its implications, the abstraction remains somewhat grounded in specific techniques rather than rising to a higher meta-level."}}
{"id": "2119a69a-fe0d-48c5-b0ed-5a9acac7ba2e", "title": "Auxiliary Detection Models (ADMs)", "level": "paragraph", "subsections": [], "parent_id": "1baf327e-5751-4c45-a952-f6746d4bec49", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Defenses against Adversarial Attacks"], ["subsection", "Taxonomy of Defenses Against Adversarial Attacks"], ["subsubsection", "Defense Aproach"], ["paragraph", "Auxiliary Detection Models (ADMs)"]], "content": "}\nA defense based on ADMs is, usually, a reactive method that, basically, makes use of adversarial training to elaborate an auxiliary binary model that will act as a filter after being trained, checking whether an input image is legitimate or adversarial before sending it to the application classifier $f$. Works such as \\citeauthor{Grosse2017, Gong2017, Metzen2017} and \\citeauthor{chen2017reabsnet}  have proposed defenses based on ADMs.\n\\citeauthor{Grosse2017}  have adapted an application classifier $f$ to also act as a ADM, training it in a dataset containing $n+1$ classes. The procedure followed by the authors consists of generating adversarial images $x'_i$ for each legitimate image $(x_i, y_j)$ that belongs to the training set $\\mathcal{T}$, where $i \\leq |{\\mathcal{T}}| \\times m$ (where $m$ is the number of attack algorithms used) and $j \\leq n$. After the generation of adversarial images, it has been formed a new training set $\\mathcal{T}_1$, where $\\mathcal{T}_1 = \\mathcal{T} \\cup \\{(x'_i, n + 1), i \\leq |{\\mathcal{T}}| \\times m\\}$. $n + 1$ is the label assigned to an adversarial image. Finally, the model $f$ has been trained using the $\\mathcal{T}_1$ set.\n\\citeauthor{Gong2017}  have elaborated a defense similar to \\citeauthor{Grosse2017}, but instead of adapting the application classifier to predict adversarial images in a class $n+1$, the authors have built and trained an ADM to filter out adversarial images $X'$ (crafted by FGSM and JSMA attacks) from the legitimate images $X$, using a training dataset $\\mathcal{T}_1$, formed from $\\mathcal{T}$. Formally, $\\mathcal{T}_1 = \\{(x_i, 1) : i \\in {|\\mathcal{T}|}\\} \\cup \\{(x'_i, 0) : i \\leq {|\\mathcal{T}|} \\times m\\}$.\nIn \\citeauthor{Metzen2017} , the representation outputs of the hidden layers of a DNN have been used for training some ADMs, in a way similar of what has been made in . The authors have named these ADMs \\textit{subnetworks} and fixed them among specific hidden layers of a DNN in order to detect adversarial images. In this work were performed experiments using the attack algorithms FGSM, BIM, and DeepFool.\nFinally, \\citeauthor{chen2017reabsnet}  have elaborated a detection and reforming architecture called \\textit{ReabsNet}. When Reabsnet receives an image $x$, it uses an ADM (represented by a DNN trained by adversarial training) to check whether $x$ is legitimate or adversarial. In case of being classified as legitimate by the ADM, Reabsnet sends $x$ to the application classifier. However, in case of being classified as adversarial, $x$ is sent by ReabsNet to an iterative process, that reforms the image $x$ while it is classified by adversarial by the ADM. In the end of the reform process, the image $x$ is finally sent to the application classifier.", "cites": [928, 927, 926, 7312], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a clear, factual description of the Auxiliary Detection Models (ADMs) approach across several papers, integrating the general idea of using an auxiliary model to detect adversarial examples. However, it lacks deeper comparative or critical analysis of the methods' effectiveness, limitations, or trade-offs. The writing offers some synthesis by outlining how each paper implements ADMs but does not abstract these methods into broader principles or trends."}}
{"id": "6ed37ac6-079d-4c88-b447-14f625d01815", "title": "Statistical Methods:", "level": "paragraph", "subsections": [], "parent_id": "1baf327e-5751-4c45-a952-f6746d4bec49", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Defenses against Adversarial Attacks"], ["subsection", "Taxonomy of Defenses Against Adversarial Attacks"], ["subsubsection", "Defense Aproach"], ["paragraph", "Statistical Methods:"]], "content": "}\nSome works such as \\citeauthor{Grosse2017} and \\citeauthor{Feinman2017}  have performed statistical comparisons among the distributions of legitimate and adversarial images. \\citeauthor{Grosse2017} have elaborated a reactive defense method that has performed an approximation for the hypothesis test MMD (\\textit{Maximum Mean Discrepancy}) with the Fisher's permutation test in order to verify whether a legitimate dataset $\\mathcal{S}_1$ belongs to the same distribution of another dataset $\\mathcal{S}_2$, which may contain adversarial images. Formally, given two datasets  $\\mathcal{S}_1$ and $\\mathcal{S}_2$, it is initially defined $a = \\text{MMD}(\\mathcal{S}_1, \\mathcal{S}_2)$. Later, there is a permutation of elements of $\\mathcal{S}_1$ and $\\mathcal{S}_2$ in two new datasets $\\mathcal{S}'_1$ and $\\mathcal{S}'_2$, and it is defined $b = \\text{MMD}(\\mathcal{S}'_1, \\mathcal{S}'_2)$. If $a < b$, the null hypothesis is rejected and then it is concluded that the two datasets belong to different distributions. This process is repeated several times and the \\textit{p-value} is defined as the fraction of the number of times which the null hypothesis was rejected.\n\\citeauthor{Feinman2017}  have also proposed a reactive defense called \\textit{Kernel Density Estimation} (KDE). KDE makes use of Gaussian Mixture Models\\footnote{Gaussian Mixture Models are unsupervised learning models that clusters data by representing sub-populations, using normal distributions, within a general population.} to analyze the outputs of the logits layer of a DNN and to verify whether the input images belong to the same distribution of legitimate images. Given an image $x$ classified as a label $y$, the KDE estimates the probability of $x$ according to Equation \\ref{eq:kde}:\n\\begin{equation}\\label{eq:kde}\n    \\text{KDE}(x) = \\frac{1}{\\abs{X_y}} \\sum_{s \\in X_y}{\\exp\\left({\\frac{\\abs{F^{n-1}(x) - F^{n-1}(s)}^2}{\\sigma^2}}\\right)}\n\\end{equation}\n\\noindent where $X_y$ is the training dataset containing images pertaining the class $y$ and $F^{n-1}(x)$ is the logits output $Z$ related to input $x$. Therefore, the detector is built by the selection of a threshold $\\tau$ which classifies $x$ as adversarial if $KDE(x) < \\tau$ or legitimate, otherwise.", "cites": [7313, 7312], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section describes two statistical defense methods from the cited papers (Grosse2017 and Feinman2017) in a clear and factual manner, but does not integrate or synthesize their ideas into a broader framework. It lacks critical evaluation of the methods' strengths or weaknesses and does not abstract beyond the specific techniques to highlight general patterns or principles in statistical-based defenses."}}
{"id": "d98e8d58-ecfd-4e90-8cc5-88af3d2a0a40", "title": "Preprocessing Techniques", "level": "paragraph", "subsections": [], "parent_id": "1baf327e-5751-4c45-a952-f6746d4bec49", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Defenses against Adversarial Attacks"], ["subsection", "Taxonomy of Defenses Against Adversarial Attacks"], ["subsubsection", "Defense Aproach"], ["paragraph", "Preprocessing Techniques"]], "content": "}\nOther works have elaborated countermeasures based on preprocessing techniques, such as image transformations , GANs , noise layers , {denoising autoencoders}  and dimensionality reduction . In the following, each work will be explained in more details.\n\\citeauthor{Xie2017}  have elaborated a proactive defense called Random Resizing and Padding (RRP) that inserts a resizing and a padding layer in the beginning of a DNN architecture. The resizing layer alters the dimensions of a input image, and later, the padding layer inserts null values in random positions on the surrondings of the resized image. In the end of the padding procedure, the resized image is classified by the proactive model. \n\\citeauthor{guo2017countering}  have applied various transformations in input images before classification, such as cropping and rescaling, bit-depth reduction, JPEG compression, total variance minimization (TVM) and image quilting. \\citeauthor{guo2017countering} have implemented TVM as a defense by randomly picking pixels from an input and performing iterative optimization to find an image whose colors are consistent with the randomly picked pixels. On the other hand, image quilting has involved reconstructing an image using small patches taken from the training database by using a nearest neighbor procedure (e.g. kNN). The intuition behind image quilting is to construct an image that is free\nfrom adversarial perturbations, since quilting only uses clean patches to reconstruct the image . The authors claimed that TVM and image quilting have presented the best results when protecting the classifier since both (i) introduce randomness, (ii) are non-differentiable operations which hinders the attacker to compute the model gradient and (iii) are model-agnostic which means the model does not need to be retrained or fine-tuned.\n\\citeauthor{shen2017ape}  have proposed a proactive defense method which have adapted a GAN to preprocess input images before they be sent to the application classifier. \\citeauthor{samangouei2018defense}  have also elaborated a defense based on a GAN framework that uses\na generative transformation network $G$ which projects an input image $x$ onto the range of the generator by minimizing the reconstruction error $||G(z)-x||^2$. After the transformation, the classifier is fed with the reconstruction $G(z)$. Since the generator was trained to model the unperturbed training data distribution, the authors claimed this added step results in a substantial reduction of any potential adversarial noise. In turn, \\citeauthor{liu2017towards}  have adopted an approach based on noise layers. {These noise layers have been inserted among the hidden layers of a CNN in order to apply a gaussian noise randomly crafted on each vector of the input image. According to the authors, this procedure avoids gradient-based attacks.} \\citeauthor{Gu2014}  have elaborated the Deep Contractive Networks (DCNs), which are proactive defense methods that make use denoising autoencoders and evolutionary algorithms as alternatives to remove perturbations from adversarial images.\nIn addition, there are countermeasures that preprocesses input images using dimensionality reduction techniques . These works are based on the hypothesis that, by reducing the dimensions of an input, the likelihood of an attacker creating a perturbation that can affect the classifier's performance decreases, given the fact the attack algorithm will have less information concerning the hyperspace of the image . Keeping this hypothesis in mind, \\citeauthor{Hendrycks2017}  have elaborated a reactive defense based on Principal Component Analysis (PCA)\\footnote{PCA is a dimensionality reduction technique that reduces, by appling a linear transformation, a set of points in $n$-dimensional space to a $k$-dimensional space, where $k \\leq n$.}. The authors have inferred that adversarial images assign greater weights on larger principal components and smaller weights on initial principal components. \\citeauthor{Li2016adv}  have applied PCA on values produced by the convolution layers of a DNN and then used a cascate classifier to detect adversarial images. The cascate classifier $C$ classifies an image $x$ as legitimate only if all its subclassifiers $C_i$ classify $x$ as legitimate, but rejects $x$ if some classifier $C_i$ reject $x$. In this work, the L-BFGS attack has been used to perform the experiments. \n\\citeauthor{xu2017feature}  have introduced \\textit{Feature Squeezing}, which is reactive defense that makes use of two techniques to reduce de dimensionality of a input image: (i) color bit reduction and (ii) spatial smoothing. According to the authors, these techniques have been chosen since they complement each other by treating two different types of perturbation. The bit reduction aims to eliminate small perturbations by covering various pixels, while spatial smoothing aims to eliminate big perturbations by covering some pixels. During the detection process, Feature Squeezing generates two reduced versions of an input image $x$: (i) $\\hat{x}_1$, which represents image $x$ with the color bits reduced and (ii) $\\hat{x}_2$, which represents $x$ reduced with spatial smoothing. Later, \\textit{Feature Squeezing} sends the images $x$, $\\hat{x}_1$ e $\\hat{x}_2$ to be classified by a DNN $f$ and compares the softmax outputs $f(x)$, $f(\\hat{x}_1)$ e $f(\\hat{x}_2)$ using the $L_1$ metric. If the $L_1$ metric exceeds a predefined threshold $\\tau$, Feature Squeezing classifies $x$ as an adversarial example and discarts it. Figure \\ref{fig:squeezing} depicts this workflow. \n\\begin{figure}[h!]\n\t\\centering\n\t\\includegraphics[width=0.65\\textwidth]{pictures/squeezing.jpg}\n\t\\caption{The Feature Squeezing workflow .}\n\t\\label{fig:squeezing}\n\t\\vspace{-3mm}\n\\end{figure}", "cites": [929, 8396, 891, 932, 931, 930, 934, 933, 7314], "cite_extract_rate": 0.9, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of various preprocessing-based defense techniques by listing methods and their implementations from multiple papers. While it occasionally draws basic connections (e.g., the common use of randomness and non-differentiability in defenses), it lacks deeper synthesis, critical evaluation of limitations, or broader abstraction to identify overarching principles or trends in the field."}}
{"id": "9bb5d121-e6cc-4716-a766-c2c19db1a806", "title": "Ensemble of Classifiers", "level": "paragraph", "subsections": [], "parent_id": "1baf327e-5751-4c45-a952-f6746d4bec49", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Defenses against Adversarial Attacks"], ["subsection", "Taxonomy of Defenses Against Adversarial Attacks"], ["subsubsection", "Defense Aproach"], ["paragraph", "Ensemble of Classifiers"]], "content": "}\nDefenses based on ensemble of classifiers are countermeasures formed by two or more classification models that can be chosen in runtime. This approach is based on the assumption that each model reciprocally compensates the weaknesses other model eventually might have when classifying a given input image . Works such as \\citeauthor{strauss2017ensemble, Tramer2017a, abbasi2017robustness} and \\citeauthor{mtdeep2017}  have adopted different techniques to elaborate defenses based on ensemble of classifiers. \\citeauthor{mtdeep2017}  have used a bayesian algorithm to chose a optimal model from an ensemble so as to minimize the chances of evasion and, at the same time, maximize the correct predictions on legitimate images. \\citeauthor{abbasi2017robustness}  have formed ensembles of specialist models which detects and classifies an input image by majority vote. \\citeauthor{strauss2017ensemble}  have made empirical evaluations based on four types of different ensembles and trainings. \\citeauthor{Tramer2017a} , in turn, have used a variation of adversarial training to train the main classifier with adversarial images crafted by an ensemble of DNNs.", "cites": [936, 935, 7311], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of ensemble-based defense approaches, mentioning different papers and their methods without substantial synthesis or comparison. It lacks critical analysis of the techniques or their limitations and offers minimal abstraction beyond the specific implementations described."}}
{"id": "bc67052c-4d04-495d-9999-5411bd558a13", "title": "Proximity Measurements", "level": "paragraph", "subsections": [], "parent_id": "1baf327e-5751-4c45-a952-f6746d4bec49", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Defenses against Adversarial Attacks"], ["subsection", "Taxonomy of Defenses Against Adversarial Attacks"], ["subsubsection", "Defense Aproach"], ["paragraph", "Proximity Measurements"]], "content": "}\nThere are other works such as \\citeauthor{papernot2018deepnearest, Cao, carrara2018adversarial, Meng2017, machado_iceis2019} which have proposed defenses based on proximity measurements among legitimate and adversarial images to the decision boundary. \\citeauthor{papernot2018deepnearest}  have elaborated an proactive defense method called \\textit{Deep k-Nearest Neighbors} (DkNN), which makes use of a variation of the kNN algorithm\\footnote{kNN stands for k-Nearest Neighbors. It is an supervised classification algorithm that assigns, to a given input $x$, the most frequent class $c$ among the $k$ nearest training samples from $x$, according to a certain distance metric.} to compute uncertainty and reliability metrics from the proximity among the hidden representations of training and input images, obtained from each layer of a DNN. The labels representing points in space of training images are analyzed after the input image goes through all the layers of the DNN. In case of the prediction related to the input $x$, given by the DNN, be in accordance with the labels representing the nearest training images to $x$, the uncertainty metric tends to be small. In contrast, in case of the labels of the training images be divergent among them, the uncertainty metric tends to be large . Figure \\ref{fig:dknn} depicts the DkNN operation.\n\\begin{figure}[h!]\n\t\\centering\n\t\\includegraphics[width=0.6\\textwidth]{pictures/dknn.jpg}\n\t\\caption{The DkNN computes uncertainty and reliability metrics to support a prediction made by a DNN for a given input image $x$, by performing a search among the training images with internal representations closest to $x$ .}\n\t\\label{fig:dknn}\n\\end{figure}\n\\citeauthor{Cao}  have also adopted an approach based on proximity metrics to elaborate a proactive countermeasure, called \\textit{Region-based Classification} (RC). RC is a variation of the kNN which defines a region $R$ in hyperspace, having as centroid an input image $x$, assigning it a label corresponding to the class which most intersects the area of this region $R$. Formally, for a given input image $x$ and a DNN $f$ that splits the hyperspace in $C$ distinct regions $R = \\{R_1, R_2, \\cdots, R_C\\}$ (being $C$ the number of classes and $R_i$ the predicted class for $f(x)$), it is created a hypercube $B(x,r)$ around $x$ (being $x$ the centroid of $B(x,r)$) with length $r$. $A_i(B(x, r))$ is the area of hypercube $B(x,r)$ that intersects the region $R_i$. The classifier $RC$ formed from $f$ is defined as $RC_{f,r}$ and its prediction for $x$ is based on the region $R_i$ which has the largest intersection with the area of hypercube, namely $RC_{f, r} = \\text{argmax}_i({A_i(B(x, r))})$.\n\\citeauthor{carrara2018adversarial}  have introduced a reactive defense that resembles somehow the DkNN architecture . The reactive method proposed by \\citeauthor{carrara2018adversarial}, at first, make use of a DNN $f$ to classify an input image $x$. {Afterwards}, the inner representations regarding image $x$, obtained from a hidden layer of $f$ (such layer is chosen empirically), are used by an kNN algorithm to perform a search in the training dataset in order to recover the $k$ images containing the most similar representations to the corresponding representations of $x$. Therefore, it is obtained a confidence metric $conf$ related to the prediction $f(x)$, where $conf$ is computed based on the score of the kNN algorithm. If the confidence is below a predefined threshold, the input image is classified as adversarial and discarted afterwards; otherwise, the prediction $f(x)$ is valid with a confidence level of $c$.\nIn turn, \\citeauthor{Meng2017}  have proposed MagNet: a non-deterministic and reactive architecture composed of two defense layers: (i) a detection layer, which rejects adversarial images containing large perturbations and, for this reason, considered further from the decision boundary, and (ii) the reform layer, which reforms the images derived from the detection layer as an attempt to remove any existing perturbations that are still present in them. According to the authors, the reform layer acts as a \"magnet\", drawing the adversarial images that evaded the detection layer to the regions of the decision boundary corresponding to their respective correct classes. For both layers, MagNet randomly chooses from a repository two defense components, implemented as autoencoders (trained beforehand using legitimate images): one autoencoder for the detection layer and the other for the reformer layer. The non-deterministic choice of the components is, according to the authors, inspired on cryptography techniques to reduce the chances of evasions.\nIn \\citeauthor{vorobeychik2018adversarial}  is said that a defense based on randomness may be an inportant strategy to secure machine learning algorithms. Since randomness can significantly increase the size of perturbations and the computational cost needed to craft adversarial images, \\citeauthor{machado_iceis2019}  have extended the non-deterministic effect of MagNet by proposing a defense called MultiMagNet, which randomly chooses multiple defense components at runtime instead of just one, how is originally made by MagNet. In a way similar to MagNet, the MultiMagNet's defense components have also been implemented as autoencoders trained on\nlegitimate images. Later, the authors have split the MultiMagNet's architecture into two stages, namely \\textit{(i) calibration stage} and \\textit{(ii) deployment stage}. In calibration stage, MultiMagNet makes use of a validation dataset to find the best set of hyperparameters. Once calibrated, MultiMagNet goes to the \\textit{(ii) deployment stage}, where it analyzes input images in order to protect the application classifier against adversarial examples. The authors have made a comparative study with MagNet using legitimate and adversarial images crafted by FGSM, BIM DeepFool and CW attacks, and concluded the increasing of the non-deterministic effect by choosing multiple components can lead to better defense architectures.\nTable \\ref{tbl:countermeasures} makes a comprehensive overview of some relevant defenses against adversarial attacks in Computer Vision available in literature, following the taxonomy presented in Section \\ref{subsec:def_tax}. It also shows which of them have already been circumvented by mentioning the corresponding works on adversarial attacks.\n\\begin{table}[h!]\n\\centering\n\\caption{Summary of some relevant defenses against adversarial attacks in Computer Vision.}\n\\begin{adjustbox}{max width=\\textwidth}\n\\begin{threeparttable}\n\\begin{tabular}{@{}lccccc@{}}\n\\toprule\n\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Defense / Work\\\\and Reference\\end{tabular}} & \\multirow{2}{*}{Objective} & \\multirow{2}{*}{Approach} & \\multicolumn{2}{c}{Robustness Claims} & \\multirow{2}{*}{Bypassed by \\tnote{**}} \\\\ \\cmidrule(lr){4-5}\n &  &  & Attack algorithms & Attacker's knowledge\\tnote{*} &  \\\\ \\midrule\nThermometer Encoding  & Proactive & Preprocessing & PGD & WB, BB & \\citeauthor{athalye2018obfuscated}  \\\\\nVectorDefense  & Proactive & Preprocessing & BIM, JSMA, DeepFool, CW, PGD & WB, GB & \\textemdash \\\\\nPixelDefend  & Proactive, Reactive & Preprocessing, Proximity & FGSM, BIM, DeepFool, CW & WB & \\citeauthor{athalye2018obfuscated}  \\\\ \\citeauthor{mustafa2019SuperResolution}  & Proactive & Preprocessing & \\begin{tabular}[c]{@{}c@{}}FGSM, BIM, MI-BIM, DeepFool, CW\\end{tabular} & WB, BB & \\textemdash \\\\\n\\citeauthor{prakash2018deflecting}  & Proactive & Preprocessing & \\begin{tabular}[c]{@{}c@{}}FGSM, BIM, JSMA, DeepFool, L-BFGS, CW\\end{tabular} & WB & \\citeauthor{athalye2018robustness}  \\\\\nSAP  & Proactive & Gradient Masking & FGSM & WB & \\citeauthor{athalye2018obfuscated}  \\\\\n\\citeauthor{Feinman2017}  & Reactive & Statistics & FGSM, BIM, JSMA, CW & WB & \\citeauthor{carlini2017bypassing}  \\\\\n\\citeauthor{carrara2018adversarial}  & Reactive & Proximity & L-BFGS, FGSM & WB & \\textemdash \\\\ D3 algorithm  & Proactive & Preprocessing & FGSM, DeepFool, CW, UAP & \\begin{tabular}[c]{@{}c@{}}WB BB, GB\\\\\\end{tabular} & \\textemdash \\\\\nRRP  & Proactive & Preprocessing & FGSM, DeepFool, CW & WB & \\citeauthor{uesato2018adversarial}  \\\\\nRSE  & Proactive & Preprocessing, Ensemble & CW & WB, BB & \\textemdash \\\\ \\citeauthor{Bhagoji2017}  & Proactive & Preprocessing & FGSM & WB & \\citeauthor{carlini2017bypassing}   \\\\\n\\citeauthor{li2016adversarial}  & Reactive & Preprocessing, Statistics & L-BFGS & WB & \\citeauthor{carlini2017bypassing}  \\\\\nReabsNet  & Reactive & ADM, Preprocessing & FGSM, DeepFool, CW & WB & \\textemdash \\\\ \\citeauthor{zheng2018robust}  & Reactive & Statistics, Proximity & FGSM, BIM, DeepFool & BB, GB & \\textemdash \\\\ DeT  & Proactive & Preprocessing, Ensemble & FGSM, BIM, DeepFool, CW & BB, GB & \\textemdash \\\\ Deep Defense  & Proactive & Gradient Masking & DeepFool & WB & \\textemdash \\\\\n\\citeauthor{Grosse2017}  & Reactive & Statistics & FGSM, JSMA & WB, BB & \\citeauthor{carlini2017bypassing}  \\\\\nRCE  & Reactive & Gradient Masking & FGSM, BIM, ILCM, JSMA, CW & WB, BB & \\textemdash \\\\\nNIC  & Reactive & ADM, Proximity & FGSM, BIM, JSMA,  DeepFool, CW & WB, BB & \\textemdash \\\\\n\\citeauthor{Cao}  & Proactive & Proximity & FGSM, BIM, JSMA, DeepFool, CW & WB & \\citeauthor{he2018decision_opt}  \\\\\n\\citeauthor{Hendrycks2017}  & Reactive & Preprocessing & FGSM & WB & \\citeauthor{carlini2017bypassing}  \\\\ Feature Distillation  & Proactive & Preprocessing & FGSM, BIM, DeepFool, CW, BPDA & WB, BB, GB & \\textemdash \\\\\nLID  & Reactive & Proximity & FGSM, BIM, JSMA, CW & WB & \\citeauthor{athalye2018obfuscated}  \\\\\n\\citeauthor{cohen2019detecting}  & Reactive & Proximity & FGSM, JSMA, DeepFool, CW & WB & \\textemdash \\\\\nBAT  & Proactive & Gradient Masking & FGSM, PGD & WB & \\textemdash \\\\ \\citeauthor{madry2017towards}  & Proactive & Gradient Masking & PGD & WB, BB & \\citeauthor{athalye2018obfuscated} \\tnote{***} \\\\\nMALADE  & Proactive & Preprocessing  & FGSM, PGD, M-BIM, EAD, BPDA, EOT, BA & WB, BB & \\textemdash \\\\\nS2SNet  & Proactive & Gradient Masking & FGSM, BIM, CW & WB, GB & \\textemdash \\\\\nGong et al.  & Reactive & ADM & FGSM, JSMA & WB & \\citeauthor{carlini2017bypassing}  \\\\\nMetzen et al.  & Reactive & ADM & FGSM, BIM, DeepFool & WB & \\citeauthor{carlini2017bypassing}  \\\\\n\\citeauthor{das2017jpegCompression}  & Proactive & Preprocessing, Ensemble & FGSM, DeepFool & WB & \\textemdash \\\\\nCCNs  & Proactive & Preprocessing & FGSM, DeepFool & WB, BB & \\textemdash \\\\\nDCNs  & Proactive & Gradient Masking, Preprocessing & L-BFGS & WB & \\textemdash \\\\ \\citeauthor{na2017cascade}  & Proactive & Gradient Masking & FGSM, BIM, ILCM, CW & WB, BB & \\textemdash \\\\\nMagNet  & Reactive & Proximity, Preprocessing & FGSM, BIM, DeepFool, CW & BB, GB & \\citeauthor{carlini2017magnet}   \\\\\nMultiMagNet  & Reactive  & Proximity, Preprocessing, Ensemble & FGSM, BIM, DeepFool, CW & WB, BB. GB & \\textemdash \\\\\nWSNNS  & Proactive & Proximity & FGSM, CW, PGD & BB, GB & \\textemdash \\\\\nME-Net  & Proactive & Preprocessing & FGSM, PGD, CW, BA, SPSA & WB, BB & \\textemdash \\\\\nSafetyNet  & Reactive & ADM & FGSM, BIM, JSMA, DeepFool & WB, BB & \\textemdash \\\\\nDefensive Distillation  & Proactive & Gradient Masking & JSMA & WB & \\citeauthor{carlini2017towards}  \\\\\n\\citeauthor{Papernot2017ExtDefensive}  & Proactive & Gradient Masking & FGSM, JSMA & WB, BB & \\textemdash \\\\\nFeature Squeezing  & Reactive & Preprocessing & FGSM, BIM, JSMA, CW & WB & \\citeauthor{He2017}   \\\\\nTwinNet  & Reactive & ADM, Ensemble & UAP & WB & \\textemdash \\\\\n\\citeauthor{abbasi2017robustness}  & Reactive & Ensemble & FGSM, DeepFool & WB & \\citeauthor{He2017}   \\\\\n\\citeauthor{strauss2017ensemble}  & Proactive & Ensemble & FGSM, BIM & WB & \\textemdash \\\\\n\\citeauthor{Tramer2017a}  & Proactive & Gradient Masking, Ensemble & FGSM, ILCM, BIM & WB, BB & \\citeauthor{alzantot2018genattack}  \\\\\nMTDeep  & Proactive & Ensemble & FGSM, CW & WB & \\textemdash \\\\\nDefense-GAN  & Proactive & Preprocessing & FGSM, CW & WB, BB & \\citeauthor{athalye2018obfuscated}  \\\\\nAPE-GAN  & Proactive & Preprocessing & L-BFGS, FGSM, DeepFool, JSMA, CW & WB &  \\citeauthor{carlini2017magnet}  \\\\ \\citeauthor{Zantedeschi2017}  & Proactive & Gradient Masking & FGSM, JSMA, VAT  & WB, BB & \\citeauthor{carlini2017magnet}  \\\\\n\\citeauthor{NingHao2018ModelInterpretation}  & Reactive & Gradient Masking & FGSM, GDA , POE  & WB & \\textemdash \\\\\n\\citeauthor{Liang2017}  & Reactive & Preprocessing & FGSM, DeepFool, CW & WB & \\textemdash \\\\\nParseval Networks  & Proactive & Gradient Masking & FGSM, BIM & BB & \\textemdash \\\\ \\citeauthor{guo2017countering}  & Proactive & Preprocessing & FGSM, BIM, DeepFool, CW & BB, GB & \\citeauthor{dong2019evading}  \\\\\nHGD  & Proactive & Preprocessing & FGSM, BIM & WB, BB & \\citeauthor{dong2019evading}  \\\\\nALP  & Proactive & Gradient Masking & PGD & WB & \\citeauthor{engstrom2018evaluating}  \\\\\n\\citeauthor{sinha2017WRM}  & Proactive & Gradient Masking & FGSM, BIM, PGD & WB & \\textemdash \\\\\nFortified Networks  & Proactive & Preprocessing & FGSM, PGD & WB, BB & \\textemdash \\\\ DeepCloak  & Proactive & Preprocessing & L-BFGS, FGSM, JSMA & WB & \\textemdash \\\\\n\\citeauthor{xie2019FeatureDenoisingBlock}  & Proactive & Preprocessing & FGSM, BIM, M-BIM, PGD & WB, BB & \\citeauthor{kurakin2018adversarial}  \\\\ DDSA  & Proactive & Preprocessing & FGSM, M-BIM, CW, PGD & WB, BB, GB & \\textemdash \\\\   ADV-BNN  & Proactive & Gradient Masking & PGD & WB, BB & \\textemdash \\\\ DkNN  & Proactive & Proximity & FGSM, BIM, CW & WB & \\citeauthor{sitawarin2019robustness}  \\\\ \\hline\n\\end{tabular}\n\\begin{tablenotes}[flushleft]\n  \\Large\n  \\item[*]WB: White-box; BB: Black-box; GB: Grey-box.\n  \\item[**]The \"\\textemdash\" symbol means that it has not been found in literature any work on adversarial attacks that has circunvented the respective defense.\n  \\item[***] Despite beign evaded in \\citeauthor{athalye2018obfuscated} , the method proposed by \\citeauthor{madry2017towards} is considered as the state-of-the-art defense in literature .\n\\end{tablenotes}\n\\end{threeparttable}\n\\end{adjustbox}\n\\label{tbl:countermeasures}\n\\vspace{-3mm}\n\\end{table}", "cites": [8397, 957, 936, 952, 928, 932, 7314, 945, 907, 934, 7312, 954, 941, 7315, 905, 924, 935, 962, 959, 929, 912, 888, 939, 7311, 944, 937, 940, 950, 955, 7317, 926, 943, 7313, 890, 942, 951, 927, 961, 8396, 938, 931, 917, 8395, 953, 948, 960, 925, 958, 947, 963, 946, 7316, 949, 933, 956, 921, 897, 930], "cite_extract_rate": 0.7160493827160493, "origin_cites_number": 81, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of proximity-based defense approaches, including DkNN, RC, and MagNet, but lacks deeper synthesis across these methods. While it mentions some works that have bypassed these defenses, the critical evaluation is minimal and mostly confined to stating that some approaches have been circumvented. There is little abstraction or generalization to broader principles or frameworks in adversarial defense."}}
{"id": "b383621a-0e68-4789-9c2e-53a90ca2e3f4", "title": "Explanations for the Existence of Adversarial Examples", "level": "section", "subsections": ["ac72f8ac-9021-464f-b58e-db24514c1f05", "d17cfaef-42fe-49bf-9d0b-c11e866b1fa3", "068cb0ae-5a8b-4afc-842c-85d1bf6cb7af", "fb36c7e3-25ac-48e0-b317-52017fdf0020", "e3ace533-279a-4b0c-a1a3-24ddf5c4f42e", "f06a80d6-2778-4df0-adaf-dc656f02e5ee", "15fa03b8-9de7-4463-b693-0c4bd979ae04"], "parent_id": "bdab4dd0-3cbb-4e33-92f9-7e53d3068eee", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Explanations for the Existence of Adversarial Examples"]], "content": "\\label{sec:5}\nDeveloping an understanding about the existence and the properties of adversarial examples, by reasoning why they affect the prediction of machine learning models, is usually the first step taken into consideration when elaborating attacks and defenses in Adversarial Machine Learning . The vulnerability that CNNs and other machine learning algorithms present before the malicious effects of adversarial attacks is popularly known as \\textit{Clever Hans Effect}, term somewhat popularized by the advent of CleverHans library . This effect has been named after a german horse called Hans. His owner used to claim Hans has owned intellectual abilities by answering arithmetic questions that people made to it by tapping its hoof the number of times corresponding to the correct answer. However, after several experiments conducted on Hans, psychologists have concluded in fact the horse has not been solving arithmetic questions, but somehow it has developed the ability to identify behavioural signals made by the crowd, such as clappings and yielings, that warned it out to stop hitting its hoof on the ground. In other words, Hans has not developed an adaptive intelligence, but actually means of perceiving and interpreting its surroundings in order to correctly answer the questions.\nSimilar to Hans, learning models are usually able to give correct answers to complex problems, such as image recognition and classification, but without really learning from training data, what make them susceptible to adversarial attacks . Despite the absence of an unanimous accepted explanation for the adversarial paradox\\footnote{\\citeauthor{Tanay2016}  have defined this paradox as the disparity between high performance classification of state-of-the-art deep learning models against their susceptibility to small perturbations that differ so close from one class to another.}, this section will describe some common hypotheses present in literature regarding the existence of adversarial images.", "cites": [948, 7306, 964], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview by introducing the 'Clever Hans Effect' as an analogy and referencing multiple papers to explain the adversarial example phenomenon. It integrates concepts from different sources (e.g., linear explanations from Goodfellow et al. and dimensional properties from local intrinsic dimensionality) to build a narrative on model vulnerability. While it offers some critique (e.g., pointing out limitations in the linear explanation), it lacks a more in-depth evaluation or synthesis into a novel framework, thus remaining at a medium insight level."}}
{"id": "ac72f8ac-9021-464f-b58e-db24514c1f05", "title": "High Non-Linearity Hypothesis", "level": "subsection", "subsections": [], "parent_id": "b383621a-0e68-4789-9c2e-53a90ca2e3f4", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Explanations for the Existence of Adversarial Examples"], ["subsection", "High Non-Linearity Hypothesis"]], "content": "\\citeauthor{Szegedy2013}  firstly concerned about the existence of adversarial examples. The authors have argued that adversarial examples exist due to the high non-linearity of deep neural networks, what contributes to the formation of low probability pockets in the data manifold that are hard to reach by sampling an input space around a given example (see Figure \\ref{fig:tilting}a). According to \\citeauthor{Gu2014} and \\citeauthor{song2017pixeldefend} , the emergence of such pockets is given chiefly due to some deficiencies of objective functions, training procedures and datasets limited in size and diversity of training samples, thus leading models to poor generalizations.", "cites": [950, 314, 7314], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates two distinct hypotheses from the cited papers—Szegedy's focus on high non-linearity and the explanations from Gu and Song regarding training limitations—into a coherent discussion on the existence of adversarial examples. While it connects these ideas, the synthesis is moderate and lacks a more comprehensive or novel framework. The analysis briefly touches on limitations (e.g., poor generalization), but does not deeply critique or compare the hypotheses. It abstracts the causes of adversarial examples into broader concepts like data manifold and model robustness, offering some general insight."}}
{"id": "d17cfaef-42fe-49bf-9d0b-c11e866b1fa3", "title": "Linearity Hypothesis", "level": "subsection", "subsections": [], "parent_id": "b383621a-0e68-4789-9c2e-53a90ca2e3f4", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Explanations for the Existence of Adversarial Examples"], ["subsection", "Linearity Hypothesis"]], "content": "\\label{subsec:linear_hyp}\n\\citeauthor{Goodfellow2014}  contradicted the non-linearity hypothesis of \\citeauthor{Szegedy2013} by assuming DNNs have a very linear behaviour caused by several activation functions like ReLU and sigmoid that perpetuates small perturbed inputs in a same wrong direction. As an attempt to underlie their explanation, the authors have elaborated the FGSM attack. \\citeauthor{fawzi2015fundamental}  said that the robustness of a classifier is independent of the training procedure used and the distance between two classes is larger in high-order classifiers than in linear ones, suggesting that it is harder to find adversarial examples in deeper models. This explanation also goes against the non-linearity hypothesis of \\citeauthor{Szegedy2013}. However, in contrast to the linearity hypothesis, \\citeauthor{tabacof2016exploring}  has found evidences the phenomenon of adversarial paradox may be a more complex problem, since results obtained from empirical experiments have suggested that shallow classifiers present a greater susceptibility to adversarial examples than deeper models. Despite some works that criticize the linearity hypothesis, some relevant attacks (such as FGSM  and DeepFool ) and defenses (such as Thermometer Encoding ) have been based on it.", "cites": [965, 906, 892], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.7, "critical": 3.3, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers to contrast the linearity hypothesis with the non-linearity hypothesis, integrating findings from Goodfellow et al., Fawzi et al., and Tabacof et al. It provides a coherent narrative about the hypothesis and its implications. While it includes some critical evaluation (e.g., noting that some works criticize the linearity hypothesis), the critique remains moderate. The section identifies broader patterns, such as the relationship between model depth and adversarial susceptibility, but does not reach a meta-level abstraction."}}
{"id": "068cb0ae-5a8b-4afc-842c-85d1bf6cb7af", "title": "Boundary Tilting Hypothesis", "level": "subsection", "subsections": [], "parent_id": "b383621a-0e68-4789-9c2e-53a90ca2e3f4", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Explanations for the Existence of Adversarial Examples"], ["subsection", "Boundary Tilting Hypothesis"]], "content": "\\citeauthor{Tanay2016} , on the other hand, have rejected the linear hypothesis proposed by \\citeauthor{Goodfellow2014} by assuming that it is \"insufficient\" and \"unconvincing\". They have proposed instead a \\textit{boundary tilting perspective} to explain the adversarial paradox. This assumption, according to the authors, is more related to the explanation given by \\citeauthor{Szegedy2013}, where a learnt class boundary lies close to the training samples manifold, but this learnt boundary is \"tilted\" with respect to this training manifold. Thereby, adversarial images can be generated by perturbing legitimate samples towards the classification boundary until they cross it. The amount of required perturbation is smaller as the tilting degree decreases, producing high-confidence and misleading adversarial examples, containing visually imperceptible perturbations. The authors also believe this effect might be a result of an overfitted model. Figure \\ref{fig:tilting}b shows a simplified illustration of the boundary tilting perspective compared with the \\citeauthor{Szegedy2013} hypothesis.\n\\begin{figure}[h!]\n    \\centering\n    \\includegraphics[scale=0.375]{pictures/tanay_griffin.jpg}\n    \\caption{Comparison between the \\citeauthor{Szegedy2013} and \\citeauthor{Tanay2016}'s hypotheses . a) \\citeauthor{Szegedy2013}'s hypothesis lies on the assumption that an image space is densely filled with low probability adversarial pockets. Similarly, b) \\citeauthor{Tanay2016}'s hypothesis indicates the existence of tilted boundaries what contributes to the emergence of adversarial examples.}\n    \\label{fig:tilting}\n    \\vspace{-5mm}\n\\end{figure}", "cites": [964], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively synthesizes the boundary tilting hypothesis by connecting it to prior work (e.g., Szegedy 2013), creating a coherent narrative around adversarial example explanations. It also provides a critical evaluation by pointing out the limitations of the linear hypothesis. While it generalizes the concept to some extent, the abstraction remains at a moderate level, focusing primarily on the specific hypothesis and its implications rather than broader meta-level insights."}}
{"id": "fb36c7e3-25ac-48e0-b317-52017fdf0020", "title": "High Dimensional Manifold", "level": "subsection", "subsections": [], "parent_id": "b383621a-0e68-4789-9c2e-53a90ca2e3f4", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Explanations for the Existence of Adversarial Examples"], ["subsection", "High Dimensional Manifold"]], "content": "\\citeauthor{gilmer2018adversarial} , in concordance with other works such as \\citeauthor{shafahi2018adversarial, mahloujifar2019curse} and \\citeauthor{fawzi2018adversarial} , said that the phenomenon of adversarial examples is result from the high dimensional nature of the data manifold. In order to show evidences, \\citeauthor{gilmer2018adversarial} have created a synthetic dataset for better controlling their experiments, and used it afterwards to train a model. After training it, the authors observed that inputs correctly classified by the model were close to nearby misclassified adversarial inputs, meaning that learning models are necessarily vulnerable to adversarial examples, independently of the training procedure used. At last, based on empirical results, \\citeauthor{gilmer2018adversarial} have also denied the assumption that states adversarial examples lie on a different distribution when compared to legitimate data .", "cites": [905, 8398, 968, 930, 950, 966, 967], "cite_extract_rate": 0.875, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers (Gilmer, Shafahi, Mahloujifar, Fawzi) to build a narrative around the high-dimensional manifold explanation for adversarial examples. It connects experimental findings to a broader theoretical assumption, showing how these works collectively suggest that adversarial vulnerability is inherent in high-dimensional data. While it includes some critical analysis (e.g., denying the assumption that adversarial examples lie on a different distribution), it does not deeply critique or compare methodologies or limitations of the cited works."}}
{"id": "e3ace533-279a-4b0c-a1a3-24ddf5c4f42e", "title": "Lack of Enough Training Data", "level": "subsection", "subsections": [], "parent_id": "b383621a-0e68-4789-9c2e-53a90ca2e3f4", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Explanations for the Existence of Adversarial Examples"], ["subsection", "Lack of Enough Training Data"]], "content": "\\citeauthor{schmidt2018adversarially}  claim learning models must generalize in a strong sense, \\textit{i.e.} with the help of robust optimization, in order to achieve robustness. Basically, the authors observed the existence of adversarial examples is not necessarily a shortcoming of specific classification models, but an unavoidable consequence of working in a statistical setting. After gathering some empirical results, the authors concluded that, currently, there are no working approaches which attain adversarial robustness mainly because existing datasets are not large enough to train strong classifiers.", "cites": [8399], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes the main argument of Schmidt et al. regarding the relationship between adversarial robustness and data size, but it does not integrate other perspectives or papers for a more comprehensive narrative. It provides some analytical insight by connecting adversarial vulnerability to the statistical learning framework, but the critique is limited and the abstraction remains at a basic level without identifying broader theoretical principles."}}
{"id": "f06a80d6-2778-4df0-adaf-dc656f02e5ee", "title": "Non-Robust Features Hypothesis", "level": "subsection", "subsections": [], "parent_id": "b383621a-0e68-4789-9c2e-53a90ca2e3f4", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Explanations for the Existence of Adversarial Examples"], ["subsection", "Non-Robust Features Hypothesis"]], "content": "\\citeauthor{ilyas2019adversarial}  have provided a different explanation based on the assumption the existence of adversarial perturbations does not necessarily indicate flaws regarding learning models or training procedures, but actually regarding images' features. By taking into account the human's perception, the authors split the features into (i) robust features, that lead models to correctly predict the true class even when they are adversarially perturbed, and (ii) non-robust features, which are features derived from patterns in the data distribution that\nare highly predictive yet brittle, incomprehensible to humans and more susceptible to be perturbed by an adversary. In order to underlie their assumption, the authors proposed constructing a novel dataset formed by images containing solely robust features which have been filtered out from the original input images by using the logits layer of a trained DNN. Then, this dataset has been used to train another DNN that has been used to perform a comparative study. The results has led the authors to find evidences that adversarial examples might really arise as a result of the presence of non-robust features, what goes in a opposite direction of what it is commonly believed: that adversarial examples are not necessarily tied to the standard training framework. Their conclusion is somehow related to the \\citeauthor{schmidt2018adversarially}  work.\n\\vspace{-2mm}", "cites": [969, 8399], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the main idea from Ilyas et al. by explaining the non-robust features hypothesis and its implications. It makes a clear analytical point that adversarial examples may not be bugs in the model or training but instead a reflection of the data's feature structure. There is some critical engagement, such as contrasting this hypothesis with common beliefs, but deeper limitations or alternative viewpoints are not thoroughly examined. The section abstracts the concept by generalizing it to broader understanding of adversarial vulnerability."}}
{"id": "15fa03b8-9de7-4463-b693-0c4bd979ae04", "title": "Explanations for Adversarial Transferability", "level": "subsection", "subsections": [], "parent_id": "b383621a-0e68-4789-9c2e-53a90ca2e3f4", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Explanations for the Existence of Adversarial Examples"], ["subsection", "Explanations for Adversarial Transferability"]], "content": "\\label{subsec:transferability}\nAs briefly mentioned in Section \\ref{subsub:adv_training}, adversarial examples make heavy use of the transferability property to drastically affect the performance of learning models even in more realistic scenarios, where the attacker does not have access to much or any information regarding the target classifier, as is simulated by grey and black-box settings. \nAdversarial transferability can be formally defined as the property that some adversarial samples have to mislead not only a target model $f$, but also other models $f'$ even when their architectures greatly differ . \\citeauthor{Papernot2016d}  have split adversarial transferability into two main categories: (i) \\textit{intra-technique transferability}, which occurs between two models that share a similar learning algorithm (e.g. DNNs) and are trained using the same dataset, however initialized with different parameters (e.g. transferability between two DNN architectures, such as VGG-16 and ResNet-152); (ii) \\textit{cross-technique transferability}, that occurs between two models which respectivelly belong to different learning algorithms (e.g. DNN and SVM), where can even perform different learning tasks, such as image classification and object detection (see Appendix \\ref{subsec:other_tasks} for more details). According to \\citeauthor{wiyatno2019adversarial} , understanding the transferability phenomenon is critical not only to explain the existence of adversarial examples, but to create safer machine learning\nmodels. \nSome assumptions have arisen in literature as an attempt to explain adversarial transferability. The linearity hypothesis assumed by \\citeauthor{Goodfellow2014}  suggests the direction of perturbations may be the crucial factor that allows the adversarial effect transfer among models, since the disturbances end up acquiring similar functions through training. \\citeauthor{Tramer2017} , in turn, have hypothesized if adversarial transferability is actually a consequence of the intersection between the adversarial subspace of two different models. By estimating the number of orthogonal adversarial directions using a techinique called \\textit{Gradient Aligned Adversarial Subspace (GAAS)}, they found that the separating distance between the decision boundaries of two models was, on average, smaller than the distances between any inputs to the decision boundaries, even on\nadversarially trained models. This suggests their adversarial subspaces were overlapped. At last, they also concluded that transferability is inherent to models that preserve non-robust properties when trying to learn feature representations of the input space, what according to the authors is not a consequence of a lack of robustness, but actually a intrinsic property of the learning algorithms themselves. Their findings agreed with the works of \\citeauthor{Liu2016}  and \\citeauthor{ilyas2019adversarial} .\n\\vspace{-2mm}", "cites": [7318, 891, 7319, 969, 892, 970], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.0, "abstraction": 3.8}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers to explain adversarial transferability by integrating concepts like intra- and cross-technique transferability, as well as the linearity hypothesis and adversarial subspace intersection. It abstracts key principles, such as the role of non-robust features and intrinsic learning properties, offering a cohesive understanding. While it presents findings and connects them, it does not deeply critique the limitations or assumptions of the cited works."}}
{"id": "6a4169a6-0321-4802-84fa-9f4c0e27d269", "title": "Principles for Designing and Evaluating Defenses", "level": "section", "subsections": ["02662689-01e5-4f8b-b4d1-320f8244ee14", "df5b836c-1189-420d-b86e-6fc4175928e6", "25203e7b-60bd-495d-90f5-9468eaf4b40c", "514b56d4-05f9-499e-8820-6d9894eb96e7", "8f6c0d66-a931-416a-a947-50a9ebc414e2"], "parent_id": "bdab4dd0-3cbb-4e33-92f9-7e53d3068eee", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Principles for Designing and Evaluating Defenses"]], "content": "\\label{sec:6}\n{Defending robustly against adversarial attacks is still an open question. \\citeauthor{carlini2019evaluating}  assert defenses often claim robustness against adversarial examples without carrying out common security evaluations}, what in fact contributes to the construction of brittle and limited architectures which are rapidly broken by novel and adaptive attacks. For this reason, the authors have defined a basic set of principles and methodologies that should be followed by both defenders and reviewers to check whether a defense evaluation is thorough and follows currently accepted best practices. This is crucial to prevent researchers from taking deceitful statements and conclusions about their works. In the following are listed and briefly explained some basic and relevant principles based on the \\citeauthor{carlini2019evaluating}'s guide for properly evaluating general defenses. For further orientations, it is recommended consult the authors' paper .", "cites": [7305], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates the principles from Carlini et al. to highlight the importance of proper evaluation in adversarial defense research, showing a basic synthesis. It critically points out the tendency for defenses to make unsubstantiated robustness claims, contributing to brittle models, and emphasizes the need for methodological rigor. While it provides a foundational analytical perspective, it does not deeply compare or contrast multiple papers nor generalize to a broader framework."}}
{"id": "df5b836c-1189-420d-b86e-6fc4175928e6", "title": "Simulate Adaptive Adversaries", "level": "subsection", "subsections": [], "parent_id": "6a4169a6-0321-4802-84fa-9f4c0e27d269", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Principles for Designing and Evaluating Defenses"], ["subsection", "Simulate Adaptive Adversaries"]], "content": "A good evaluation must test the limits of a defense by simulating adaptive adversaries which make use of its threat model to elaborate strong attacks. All settings and attacks scenarios that stand a chance to bypass the defense should be taken into consideration without exceptions. An evaluation conducted only based on non-adaptive adversaries is of very limited utility since the results produced by the experiments do not bring reliable conclusions that support the defense's claims and its robustness bounds. A good evaluation will not try to support or assist the defense's claims, but will try to break it under its threat model at all costs. Therefore, weak attack settings and algorithms, such as FGSM attack\\footnote{FGSM originally was implemented to support the linearity hypothesis made in \\citeauthor{Goodfellow2014} . For this and other reasons related to the attack configurations, such as its sequential execution when computing perturbations, this attack is considered weak and untrustworthy to fully test defenses, usually used only to run sanity tests (see Section \\ref{sec:sanity_tests}).} must not be solely used. It is worth mentioning there are some relevant libraries available online for helping researchers to perform evaluations by simulating adaptive adversaries, such as \\textit{CleverHans} , \\textit{Adversarial Robustness Toolbox (ART)} , \\textit{Foolbox} , \\textit{DEEPSEC } and \\textit{AdvBox} .", "cites": [972, 973, 892, 971], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates key concepts from multiple cited papers to emphasize the importance of testing defenses against adaptive adversaries. It connects the utility of adversarial toolboxes (e.g., Foolbox, Advbox) to the broader principle of rigorous evaluation. While it provides a critical stance by dismissing FGSM as insufficient for real testing, the critique is limited and does not deeply compare or evaluate the cited libraries or their methodologies."}}
{"id": "514b56d4-05f9-499e-8820-6d9894eb96e7", "title": "Perform Basic Sanity Tests", "level": "subsection", "subsections": [], "parent_id": "6a4169a6-0321-4802-84fa-9f4c0e27d269", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Principles for Designing and Evaluating Defenses"], ["subsection", "Perform Basic Sanity Tests"]], "content": "\\label{sec:sanity_tests}\nSanity tests are important to identify anomalies and antagonic results that can lead authors to take incorrect conclusions. \\citeauthor{carlini2019evaluating}  has listed some basic sanity tests that should be run to complement the experiments and support the results.\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Report model accuracy on legitimate samples:} {while the protection of learning models against adversarial examples is a relevant security issue}, a significant decrease on legitimate data on behalf of increasing the robustness of the model might be unreasonable for scenarios where the probability of an actual adversarial attack is low and the cost of a misclassification is not high. For reactive defenses, it is important to evaluate how the rejection of perturbed samples can affect the accuracy of the model on legitimate samples. An analysis of a Receiver Operating Characteristic (ROC) curve may be helpful to check how the choice of a threshold for rejecting adversarial inputs can decrease the model's clean accuracy;\n    \\item \\textbf{Iterative \\textit{vs.} sequential attacks:} iterative attacks are more powerful than sequential attacks. If adversarial examples crafted by a sequential algorithm are able to affect classification models more than examples crafted by iterative ones, it can indicate that the iterative attack is not properly calibrated;\n    \\item \\textbf{Increase the perturbation budget:} attacks when allowed to produce larger amounts of distortion in images usually fool classifiers more often than attacks with smaller perturbation budgets. Therefore, if the attack success rate decreases as the perturbation budget increases, this attack algorithm is likely flawed;\n    \\item \\textbf{Try brute force attacks:} it can be an alternative in scenarios where the attacks do not succeed very often. By performing a random search attack within the defense's threat model can help the attacker or reviewer to find adversarial examples which have not been found by standard adversarial attacks, what indicates these algorithms must be somehow improved. \\citeauthor{carlini2019evaluating} recommend starting this sanity test by sampling random points at larger distances from the legitimate input, limiting the search to strictly smaller distortions whenever an adversarial example is found.\n    \\item \\textbf{White-box vs. black-box attacks:} white-box attacks are generally more powerful than black-box attacks, since the attacker has complete access to the model and its parameters. For this reason, gradient-based attacks should, in principle, present better success rates. If gradient-based attacks have worse performance when compared to other attack approaches, it can indicate the defense is somehow performing a kind of gradient masking and the gradient-based attack needs calibration. \n    \\item \\textbf{Attack similar undefended models:} proactive and reactive defenses typically introduce a couple of modifications in the networks in order to increase their robustness. However, it can be worth trying remove these security components out from the model and evaluate it under attacks without any protection. If the undefended model appears robust nevertheless, it can infer that the defense itself is not  actually protecting the model.  \n\\end{itemize}", "cites": [7305], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.8, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes the key sanity tests outlined in Carlini et al. (2019) and integrates them into a structured framework for evaluating defenses. It provides critical insights by explaining the implications of failing these tests and linking them to potential issues such as gradient masking or ineffective robustness. The discussion abstracts these tests into general principles applicable across various defense mechanisms, supporting a high level of analytical depth."}}
{"id": "d313f2ca-2c1b-44d5-a21e-3a251887a5d9", "title": "Development of Theoretical Lower Bounds of Robustness", "level": "subsection", "subsections": [], "parent_id": "235a32dd-3b8b-4956-b685-ff4d047503f7", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Directions of Future Work"], ["subsection", "Development of Theoretical Lower Bounds of Robustness"]], "content": "\\label{subsec:lower_robustness}\nMost defenses are limited to empirical evaluations and do not claim robustness to unknown attacks . Other works, in turn, devise theoretical robustness bounds which do not generalize to different attacks and threat models studied. A promising research path is the investigation of properties that can theoretically guarantee general lower bounds of robustness to adversarial attacks (see Section \\ref{sec:lower_bounds}).", "cites": [891], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section briefly identifies a research gap regarding the lack of general theoretical robustness bounds in adversarial defenses, referencing one paper. While it moves beyond mere description by highlighting a potential future direction, it does not synthesize multiple sources or provide a deeper comparative or critical analysis of the cited work. The abstraction is limited to a high-level observation without forming broader principles or frameworks."}}
{"id": "0df3a378-c14c-4195-8696-af376901c70c", "title": "Final Considerations", "level": "section", "subsections": [], "parent_id": "bdab4dd0-3cbb-4e33-92f9-7e53d3068eee", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Final Considerations"]], "content": "\\label{sec:8}\nDeep Learning models have revealed to be susceptible to attacks of adversarial nature despite shown impressive abilities when solving complex cognitive problems, specially tasks related to Computer Vision, such as image classification and recognition. This vulnerability severely menaces the application of these learning algorithms in safety-critical scenarios, what in turn may jeopardize the development of the field if this security issue persists in the future. The scientific community has been struggling to find alternatives to defend against adversarial attacks practically since this problem was firstly spotted by the work of \\citeauthor{Szegedy2013} . The numerous proposed defenses, albeit promising at first, have shown to be brittle and innefective to stop strong and adaptive attacks though. This arms race between attacks and defenses makes the field of Adversarial Machine Learning fairly dynamic and active, where the emergence of novel defense approaches almost daily plays a role in becoming review papers quickly outdated. \nBefore this chaotic scenario, this work has aimed to attend the interested readerships by elaborating a comprehensive and self-contained survey that gathers the most relevant research on Adversarial Machine Learning. It has covered topics regarding since Machine Learning basics to adversarial examples and attacks, nevertheless with an emphasis on giving the readers a defender's perspective. An extensive review the literature has allowed recent and promising defenses, not yet mentioned by other works, be studied and categorized following a novel taxonomy. Moreover, existing taxonomies to organize adversarial examples and attacks have been updated in order to cover further approaches. Furthermore, it has been gathered existing relevent explanations for the existence and transferability of adversarial examples, listed some common policies that should be considered by both defenders and reviewers when respectivelly designing and evaluating security methods for deep learning models and provided some promising paths for future work. In summary, the main contributions of this work were the following:\n\\begin{itemize}[leftmargin=*]\n    \\item The provision of a background regarding CNNs and some relevant architectures present in literature ranked according to their respective performance in the ILSVRC top-5 classification challenge from 2012 to 2017. It was also highlighted other important Deep Learning algorithms in Adversarial Machine Learning, such as Autoencoders and Generative Adversarial Networks (GANs); \\vspace{3mm}\n    \\item The update of some existing taxonomies to categorize different types of adversarial images and novel attack approaches that have raised in literature;\\vspace{3mm}\n    \\item A exhaustive review and discussion of defenses against adversarial attacks that were categorized using a novel taxonomy;\\vspace{3mm}\n    \\item The address of relevant explanations for the existence and transferability of adversarial examples;\\vspace{3mm}\n    \\item The discussion of promising research paths for future works on Adversarial Machine Learning.\n\\end{itemize}\nSecuring against adversarial attacks is crucial for the future of several applications. Therefore, this paper has been elaborated to provide a detailed overview of the area in order to help researchers to devise better and stronger defenses. For the best of the authors' knowledge, this is the most comprehensive survey focused on adversarial defenses available in literature and it is hoped that this work can help the community to make Deep Learning models reaching their prime-time soon. \n\\bibliographystyle{ACM-Reference-Format}\n\\bibliography{acmart}\n\\appendix\n{", "cites": [314], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"error": "Failed to parse LLM response", "raw_response": "{\n    \"type\": \"analytical\",\n    \"scores\": {\"synthesis\": 3.5, \"critical\": 3.0, \"abstraction\": 3.5},\n    \"insight_level\": \"medium\",\n    \"analysis\": \"The section provides a general analytical overview of the field, emphasizing the ongoing arms race between attacks and defenses. It synthesizes the work of \\citeauthor{Szegedy2013} and integrates broader themes from the survey, such as the need for robust defenses and updated taxonomies. While it identifies the dynamic nature of the field and highligh"}}
{"id": "cd536717-ba7c-4112-b1a2-47dc3894cc04", "title": "Other Tasks in Adversarial Machine Learning for Computer Vision", "level": "section", "subsections": [], "parent_id": "bdab4dd0-3cbb-4e33-92f9-7e53d3068eee", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Other Tasks in Adversarial Machine Learning for Computer Vision"]], "content": "} \\label{subsec:other_tasks}\nBesides Image Classification, Adversarial Machine Learning also takes part in various other tasks of Computer Vision. Among the mainstream options, two tasks in specific are widely approached in papers: \\textit{(i) {Object Detection}} and \\textit{(ii) {Semantic Segmentation}}. Object Detection tasks aim to identify semantic objects in input images by surrounding each of them usually by drawing a rectangle, also known as \\textit{bounding box}, around the detected objects. In turn, Semantic Segmentation aims to represent an image into something more meaningful and easier to analyze by assigning a label for each pixel in the input image which shares similar characteristics . Table \\ref{tbl:appendixTasks} references for interested readers some related works in Adversarial Machine Learning towards Object Detection and Semantic Segmentation.\n\\vspace{3mm}\n\\begin{table}[h!]\n\\caption{Relevant works on Adversarial Machine Learning for Object Detection and Image Segmentation tasks.}\n\\resizebox{5cm}{!}{\n\\begin{threeparttable}\n\\begin{tabular}{@{}lc@{}}\n\\toprule\nWork and Reference & Task\\tnote{*} \\\\ \\midrule\n\\citeauthor{xie2017dag}  & OBJ, SGS \\\\\n\\citeauthor{metzen2017universal}  & SGS \\\\\n\\citeauthor{moosavi2017universal}  & SGS \\\\\n\\citeauthor{fischer2017adversarial}  & SGS \\\\ \\citeauthor{lu2017no}  & OBJ \\\\ \\citeauthor{chen2018shapeshifter}  & OBJ \\\\ \\citeauthor{thys2019fooling}  & OBJ \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tablenotes}[flushleft]\n  \\footnotesize\n  \\item[*]OBJ: \\textit{Object Detection}; SGS: \\textit{Semantic Segmentation}.\n\\end{tablenotes}\n\\end{threeparttable}\n\\label{tbl:appendixTasks}}\n\\end{table}", "cites": [900, 7042, 975, 899, 976, 977, 903, 974], "cite_extract_rate": 1.0, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of adversarial machine learning in the context of Object Detection and Semantic Segmentation, but it lacks synthesis of ideas across the cited papers. It mainly introduces the tasks and includes a table of references without discussing how these works relate or contribute to a broader understanding. There is minimal critical analysis or abstraction beyond individual papers, making it primarily descriptive in nature."}}
{"id": "c8e75800-db4e-4659-867e-7a2518d6967b", "title": "Standard Datasets in Computer Vision", "level": "section", "subsections": [], "parent_id": "bdab4dd0-3cbb-4e33-92f9-7e53d3068eee", "prefix_titles": [["title", "Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective"], ["section", "Standard Datasets in Computer Vision"]], "content": "Datasets are important tools for evaluating Deep Learning algorithms. In the field of Adversarial Machine Learning and Computer Vision, some most used datasets are summarized by Table \\ref{tbl:datasets}.\n\\begin{table}[h!]\n\\caption{Popular Datasets in Adversarial Machine Learning for Computer Vision}\n\\begin{threeparttable}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{@{}lcccccccc@{}}\n\\toprule\n{Name and Reference} & Main Task\\tnote{*} & Year & Classes & Images' Resolution & Training Samples & Validation Samples & Testing Samples & Total of Images \\\\ \\midrule\nMNIST  & ICR & 1998 & 10 & 28x28x1 & 60,000 & N/A & 10,000 & 70,000 \\\\\nCIFAR-10  & ICR & 2009 & 10 & 32x32x3 & 50,000 & N/A & 10,000 & 60,000 \\\\\nCIFAR-100  & ICR & 2009 & 100 & 32x32x3 & 50,000 & N/A & 10,000 & 60,000 \\\\\nSVHN  & ICR / OBJ & 2011 & 10 & 32x32x3 & 73,257 & 531,131 & 26,032 & 630,420 \\\\\nGTSRB  & ICR / OBJ & 2012 & 43 & {[}15x15x3, 250x250x3{]} & 34,799 & 4,410 & 12,630 & 51,839 \\\\\nImageNet  & ICR / OBJ & 2015 & 1,000 & 482x415x3 (average) & N/A & N/A & N/A & 14,197,122 \\\\\nCelebA  & ICR / OBJ & 2015 & 10,177 & 218x178x3 & 162,770 & 19,867 & 19,962 & 202,599 \\\\\nVOC2012  & ICR / OBJ / SGS & 2012 & 20 & 469x387x3 (average) & 5,717 & 5,823 & 10,991 & 22,531 \\\\\nMS-COCO  & OBJ / SGS & 2014 & 171 & 640x480x3 & 165,482 & 81,208 & 81,434 & 328,124 \\\\\nSTL-10  & ICR & 2011 & 10 & 96x96x3 & 5,000 & 100,000 (unlabeled) & 8,000 & 113,000 \\\\\nToronto Faces Dataset  & ICR / OBJ & 2010 & 7 & 32x32x3 & 2,925 & 98,058 (unlabeled) & 418 & 101,401 \\\\ \\bottomrule\n\\end{tabular}}\n\\begin{tablenotes}[flushleft]\n  \\footnotesize\n  \\item[*]ICR: \\textit{Image Classification and Recognition}; OBJ: \\textit{Object Detection}; SGS: \\textit{Semantic Segmentation}; N/A: \\textit{Not Available}.\n\\end{tablenotes}\n\\end{threeparttable}\n\\label{tbl:datasets}\n\\end{table}\n\\end{document}\n\\endinput", "cites": [485, 486, 895], "cite_extract_rate": 0.4, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of standard datasets in Computer Vision but lacks synthesis of the cited papers into a coherent narrative. It does not analyze or compare the datasets in terms of their relevance or limitations to adversarial machine learning. There is minimal abstraction, as it focuses on listing attributes of each dataset without generalizing broader patterns or principles."}}
