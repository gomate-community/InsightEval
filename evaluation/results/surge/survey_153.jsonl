{"id": "f092a33f-29c0-42be-ae34-3a9a2f6fb957", "title": "Introduction", "level": "section", "subsections": ["4d1510ea-326d-457c-bdfc-b50dbfb8ace8", "994c26f4-77c3-4aae-bd6d-f535854a1f4d"], "parent_id": "baead8c2-91a5-41fd-8dd2-241ca88dd0ef", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Introduction"]], "content": "Large neural NLP models, most notably BERT-like models , have become highly widespread, both in research and industry applications .\nThis increase of model complexity is motivated by a general correlation between model size and test performance .\nDue to their immense complexity, these models are generally considered black-box models.\nA growing concern is therefore if it is responsible to deploy these models.\nConcerns such as safety, ethics, and accountability are particularly important when machine learning is used for high-stakes decisions, such as healthcare, criminal justice, finance, etc. , including NLP-focused applications such as translation, dialog systems, resume screening, search, etc. .\nFor many of these applications, neural models have been shown to exhibit unwanted biases and similar ethical issues .\n argue, among others , that these ethical and safety issues stem from an ``incompleteness in the problem formalization''. \nWhile these issues can be partially prevented with robustness and fairness metrics, it is often not possible to consider all failure modes. Therefore, quality assessment should also be done through model explanations.\nFurthermore, when models do fail in critical applications, explanations must be provided to facilitate the accountability process. Providing these explanations is often a core motivation for interpretability. In \\Cref{sec:motivation-for-interpretability} we provide aditional motivating factors.\n define \\emph{interpretability} as the ``ability to explain or to present in understandable terms to a human''. However, what constitutes as an ``understandable'' explanation is an interdisciplinary question.\nAn important work from social science by , argues that \\emph{effective explanations} must be selective in the sense one must select ``one or two causes from a sometimes infinite number of causes''. Such observation necessitates organizing interpretability methods by how and what they selectively communicate.\nThis survey presents such an organization in \\Cref{tab:overview}, where each row represents a communication approach. For example, the first row describes \\emph{input feature} explanations that communicate what tokens are most relevant for a prediction. In general, each row is ordered by how abstract the communication approach is, although this is an approximation. Organizing by the method of communication is discussed further in \\Cref{sec:introduction:abstraction-level}. \n\\begin{table}[H]\n\\centering\n\\resizebox{\\examplefigurewidth}{!}{\\input{figures/overview-table}}\n\\caption[Overview of \\posthoc{post-hoc} interpretability methods]{Overview of \\posthoc{post-hoc} interpretability methods, where § indicates the section the method is discussed. Rows describe how the explanation is communicated, while columns describe what information is used to produce the explanation. The order of both rows and columns indicates level of abstraction and amount of information, respectively. However, this order is only approximate.\nFurthermore, because this survey focuses on \\posthoc{post-hoc} methods, the \\intrinsic{intrinsic} section of this table is incomplete and merely meant to provide a few comparative examples. The specifc \\intrinsic{intrinsic} methods shown are: \\emph{Attention} , \\emph{GEF} , \\emph{NILE} . \\emph{Prototype Networks} and \\emph{Auxiliary Task} refer to types of models.\n\\textsuperscript{$\\mathcal{C}$}: Depends on checkpoints during training. \\textsuperscript{$\\mathcal{D}$}: Depends on supplementary dataset. \\textsuperscript{$\\mathcal{H}$}: Depends on second-order derivative. \\textsuperscript{$\\mathcal{M}$}: Depends on supplementary model. \\textsuperscript{${\\dagger}$}: Depends only on dataset and white-box access.}\n\\label{tab:overview}\n\\end{table}\nEach interpretability method uses different kinds of information to produce its explanation, in \\Cref{tab:overview} this is indicated by the columns\\footnote{\\emph{Black-box}: the method only evaluates the model. \\emph{Dataset}: the method has access to all training and validation observations. \\emph{Gradient}: the gradient of the model is computed. \\emph{Embeddings}: the method uses the word embedding matrix. \\emph{White-box}:  the method knows everything about the model, such as all weights and all operations. However, the method is not specific to a particular architecture. \\emph{Model specific}: the method is specific to the architecture. Note, neural model in NLP are usually differentiability and have an embedding matrix. We therefore do not consider these properties constraints.}. The columns are ordered by an increasing level of information. Again, this is an inexact ranking but serves as a useful tool to contrast the methods.\n\\Cref{tab:overview} frames the overall structure of this survey. Where each method section from \\ref{sec:input-features} to \\ref{sec:rules} covers a row of \\Cref{tab:overview}. However, first we cover motivation (\\cref{sec:motivation-for-interpretability}), how to validate interpretability  (\\cref{sec:measures-of-interpretability}), and a motivating example (\\cref{sec:motivating-example}). The method sections can be read somewhat independently but will refer back to these general topics. \nIn contrast to other surveys and tutorials on interpretability methods  which only discusses the most popular approaches (usually 3 among \\type{sec:input-features}{input features}, \\type{sec:adversarial-examples}{adversarial examples}, \\type{sec:influential-examples}{influential examples}, \\method{sec:vocabulary:projection}{projection}, and \\type{sec:linguistic-information}{linguistic information}), this survey offers a more diverse overview of communication approaches. We hope this leads to more questioning about how we communicate. Additionally, we consistently comment on how each method is validated (\\measure{groundedness}), an important discussion we find is often missing.\nFinally, the survey limits itself to \\posthoc{post-hoc} interpretability methods. These are methods that provide their explanation after a model is trained and are often model-agnostic. This is in contrast to \\intrinsic{intrinsic} methods, where the model architecture itself helps to provide the explanation. These terms  are described further in \\Cref{sec:introduction:intrinsic-v-post-hoc}.", "cites": [8500, 8068, 8070, 168, 205, 1798, 4875, 7621, 8069, 826, 7, 4325, 4079, 679, 472], "cite_extract_rate": 0.5769230769230769, "origin_cites_number": 26, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key ideas from multiple papers to establish a coherent framework for post-hoc interpretability, particularly highlighting the importance of communication approaches and validation. It abstracts from specific methods to propose a novel categorization and emphasizes broader implications for accountability and ethical concerns. While it does identify some gaps in existing surveys, its critical depth could be enhanced with more direct evaluation of cited works."}}
{"id": "4d1510ea-326d-457c-bdfc-b50dbfb8ace8", "title": "Organizing by method of communication", "level": "subsection", "subsections": [], "parent_id": "f092a33f-29c0-42be-ae34-3a9a2f6fb957", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Introduction"], ["subsection", "Organizing by method of communication"]], "content": "\\label{sec:introduction:abstraction-level}\nAs a categorization of communication strategies, it's standard in the interpretability literature to distinguish between methods that explain a single observation, called \\category{local explanations}, and methods that explain the entire model called \\category{global explanations} . In this survey, we also consider an additional category of methods that explains an entire output-class, which we call \\category{class explanations}.\nTo subdivide these categories further, \\Cref{tab:overview} orders each communication strategy by their abstraction level. As an example, see \\Cref{fig:introduction:explanation-examples}, where an \\type{sec:input-features}{input features} explanation highlights the input tokens that are most responsible for a prediction; because this must refer to specific tokens, its ability to provide abstract explanations is limited. For a highly abstract explanation, consider the \\type{sec:natural-language}{natural language} category which explains a prediction using a sentence and can therefore use abstract concepts in its explanation.\n\\begin{figure}[h]\n    \\centering\n    \\examplefigure{introduction}\n    \\caption{Fictive visualization of an \\type{sec:input-features}{input features} explanation which highlights tokens and a \\type{sec:natural-language}{natural language} explanation, applied on a sentiment classification task . $y = \\mathtt{pos}$ means the gold label is \\textit{positive} sentiment.}\n    \\label{fig:introduction:explanation-examples}\n\\end{figure}\nCommunication methods that have a higher abstraction level are typically easier to understand (more \\measure{human-grounded}), but the trade-off is that they may reflect the model's behavior less (less \\measure{functionally-grounded}). Because the purpose of interpretability is to communicate the model to a human, this trade-off is necessary . Which communication strategy should be used must be decided by considering the applications and to whom the explanation is communicated to. In \\Cref{sec:measures-of-interpretability} we discuss \\measure{human-groundedness} and \\measure{functionally-groundedness} in-depth and how to measure them, such that an informed decision can be made.\n\\Cref{tab:overview} does have some limitations. Firstly, ordering explanation methods by their abstraction level is an approximation, and while \\category{global explanations} are generally more abstract than \\category{local explanations} this is not always true. For example, the explanation ``simply print all weights'' (not included in \\Cref{tab:overview}), is arguably the lowest possible abstraction level, however it's also a \\category{global explanation}. Secondly, there are explanation categories that are not included, such as \\emph{intermediate representations}. This category of explanation depends on models that are \\intrinsic{intrinsically} interpretable, which are not the subject of this paper.\nWe elaborate on this in \\Cref{sec:introduction:intrinsic-v-post-hoc}.", "cites": [4875, 8500, 1798, 1568], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a clear analytical framework by introducing and elaborating on categories of explanation (local, global, class) and the trade-off between abstraction and model fidelity. It connects insights from the cited papers on explainability and interpretability to justify the proposed classification. While not fully synthesizing all perspectives into a novel framework, it identifies limitations of current categorizations and suggests the need for deeper discussion of measurement criteria."}}
{"id": "994c26f4-77c3-4aae-bd6d-f535854a1f4d", "title": "Intrinsic versus post-hoc interpretability", "level": "subsection", "subsections": ["cdcb4677-7929-45ce-a9a6-deda48b19a9e"], "parent_id": "f092a33f-29c0-42be-ae34-3a9a2f6fb957", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Introduction"], ["subsection", "Intrinsic versus post-hoc interpretability"]], "content": "\\label{sec:introduction:intrinsic-v-post-hoc}\nA fundamental motivation for interpretability is accountability. For example, if a predictive mistake happens which caused harm, it is important to explain why this mistake happened . Similarly, for high-stakes decisions, it is important to minimize the risk of model failure by explaining the model before deployment . In other words, it is important to distinguish between when interpretability is applied proactively or retroactively to the model's deployment.\nIt is standard in the literature to categorize if an interpretability method can be applied retroactively or proactively. Unfortunately, the terminology for this taxonomy is not standardized . This survey focuses on the methods that can be applied retroactively, for which the term \\posthoc{post-hoc} is used. Similarly, we use the term \\intrinsic{intrinsic} to refer to models that are interpretable by design. These terms were chosen as the best compromise between established terminology  and correctness in terms of their dictionary definition.", "cites": [8071, 8069], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section introduces the distinction between intrinsic and post-hoc interpretability and briefly references two papers, but it does so primarily to contextualize terminology rather than to synthesize or critically engage with their arguments. While it does provide a clear analytical framework for categorizing interpretability methods, the integration of cited works is minimal and lacks deeper comparative or evaluative analysis."}}
{"id": "cdcb4677-7929-45ce-a9a6-deda48b19a9e", "title": "\\intrinsic{Intrinsic", "level": "paragraph", "subsections": ["314424d8-33d0-460d-ac6d-02be14b33907", "37617aef-786d-4754-a355-147f51234d99"], "parent_id": "994c26f4-77c3-4aae-bd6d-f535854a1f4d", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Introduction"], ["subsection", "Intrinsic versus post-hoc interpretability"], ["paragraph", "\\intrinsic{Intrinsic"]], "content": "methods} These inherently depend on models that by design are interpretable. Because of this relation, it is also often referred to as \\emph{white-box models} . However, the term \\emph{white-box} is slightly misleading, as it is often only a part of the transparent model.\nAs an example, consider \\emph{intermediate representation} explanations, this category depends on a model that is constrained to produce a meaningful \\emph{intermediate representation}. In Neural Modular Networks  this could be \\texttt{find-max-num(filter(find()))}, which represents how to extract an answer from a question-paragraph-pair. However, how this representation is produced is not necessarily \\intrinsic{intrinsically} interpretable.\n\\intrinsic{Intrinsic} methods are attractive because they may be more responsible to use in high-stakes decision processes. However, as  argue, ``a method being \\emph{inherently interpretable} is merely a claim that needs to be verified before it can be trusted''. Verifying this is often non-trivial, as has repeatedly been shown with \\emph{Attention} , where multiple papers have found contradicting conclusions regarding interpretability .", "cites": [8071, 4314, 8072, 4850, 168, 4848, 4303, 8073], "cite_extract_rate": 0.7272727272727273, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a nuanced analytical overview of intrinsic interpretability, integrating key concepts from multiple papers to discuss the limitations and challenges of such methods, particularly around attention mechanisms and neural modular networks. It connects these ideas to broader themes of trust and verification in model interpretability. While it does not fully synthesize a novel framework or abstraction, it critically evaluates the claims and contradictions within the literature, especially regarding the faithfulness and interpretability of attention weights."}}
{"id": "314424d8-33d0-460d-ac6d-02be14b33907", "title": "\\posthoc{Post-hoc", "level": "paragraph", "subsections": [], "parent_id": "cdcb4677-7929-45ce-a9a6-deda48b19a9e", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Introduction"], ["subsection", "Intrinsic versus post-hoc interpretability"], ["paragraph", "\\intrinsic{Intrinsic"], ["paragraph", "\\posthoc{Post-hoc"]], "content": "methods} These are the focus of this paper. While many \\posthoc{post-hoc} methods are model-agnostic, this is not a necessary property, and in some cases does only apply to a category of models. Indeed, in this paper, only methods that apply to neural networks are discussed.\nBecause of the inherent ability to explain the model after training, \\posthoc{post-hoc} methods are valuable in legal proceedings, where models may need to be explained retroactively . Additionally, they fit into existing quality assessment structures, such as those used to regulate banking, where quality assessment is also done after a model has been built . Finally, it is guaranteed that they will not affect model performance.\nHowever, \\posthoc{post-hoc} methods are often criticized for providing false explanations, and it has been questioned if it is reasonable to expect models, that were not designed to be explained, to be explained anyway . This is a valid concern, however producing \\intrinsic{intrinsic} methods is often very task dependent and therefore a difficult process which is rarely done in the industry . \\posthoc{Post-hoc} method are often much more adaptable and their impact can therefore be much greater if they can provide accuate explanations. The question of how to validate explanations is therefore very important and is covered in detail in \\Cref{sec:measures-of-interpretability}. Furthermore, we pay special attention to how each method is validated in the literature throughout the survey.", "cites": [4875, 8069], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from the cited papers regarding the practical use and regulatory implications of post-hoc interpretability methods. It connects these ideas to broader themes like legal accountability and industry deployment challenges. While it offers some critical analysis by highlighting limitations and industry realities, the critique remains moderate and the level of abstraction is constrained to reiterating general concerns without deeper theoretical framing."}}
{"id": "37617aef-786d-4754-a355-147f51234d99", "title": "Comparing", "level": "paragraph", "subsections": [], "parent_id": "cdcb4677-7929-45ce-a9a6-deda48b19a9e", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Introduction"], ["subsection", "Intrinsic versus post-hoc interpretability"], ["paragraph", "\\intrinsic{Intrinsic"], ["paragraph", "Comparing"]], "content": "Both \\intrinsic{Intrinsic} and \\posthoc{post-hoc} methods have their merits, but often provide different values in terms of \\motivation{accountability}. Finally, \\posthoc{post-hoc} methods can often be applied also to \\intrinsic{intrinsicly} interpretable models. Observing a correlation between methods from these two categories can therefore provide validation of both methods .", "cites": [8072], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section briefly contrasts intrinsic and post-hoc interpretability methods and notes a potential correlation between them for validation purposes. While it introduces a general idea, it lacks detailed synthesis of multiple cited works and does not critically engage with the limitations or implications of these methods. The abstraction is minimal, focusing on a surface-level observation rather than deeper principles or trends."}}
{"id": "2f95b059-ea6b-487e-a8d4-f4adb22bec59", "title": "Motivations for Interpretability", "level": "section", "subsections": ["794a338d-c24e-4232-aa0b-42b03443de24"], "parent_id": "baead8c2-91a5-41fd-8dd2-241ca88dd0ef", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Motivations for Interpretability"]], "content": "\\label{sec:motivation-for-interpretability}\nThe need for interpretability comes primarily from an ``incompleteness in the problem formalization'' , meaning if the model was constrained and optimized to prevent all possible ethical issues, interpretability would be much less relevant. However, because perfect optimization is unlikely, hence \\motivation{safety} and \\motivation{ethics} are strong motivations for interpretability.\nAdditionally, when models misbehave there is a need for explanations, to hold people or companies accountable, hence \\motivation{acountability} is often a core motivation for interpretability. Finally, explanations are often useful, or sometimes necessary, for gaining \\motivation{scientific understanding} about models. This section aims to elaborate on what exactly is understood by these terms and how interpretability can address them.", "cites": [8500], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of motivations for interpretability, integrating the concept of 'incompleteness in the problem formalization' from the cited paper to frame broader themes like safety, ethics, accountability, and scientific understanding. It connects these ideas to the role of interpretability but does not go beyond the cited paper's framework or offer deeper critique or comparative analysis."}}
{"id": "794a338d-c24e-4232-aa0b-42b03443de24", "title": "Ethics,", "level": "paragraph", "subsections": ["488addd1-4e2c-4d55-95a6-e826cf6111db"], "parent_id": "2f95b059-ea6b-487e-a8d4-f4adb22bec59", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Motivations for Interpretability"], ["paragraph", "Ethics,"]], "content": "in the context of interpretability, is about ensuring that the model's behavior is aligned with common ethical and moral values. Because there does not exist an exact measure for this desideratum, this is ultimately something that should be judged qualitatively by humans, for example by an \\emph{ethics review committee}, who will inspect the model explanations.\nFor some ethical concerns, such as discrimination, it may be possible to measure and satisfy this ethical concern via fairness metrics and debiasing techniques . However, this often requires a finite list of protected attributes , and such a list will likely be incomplete, hence the need for a qualitative assessment .", "cites": [8074, 8500, 7621], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from the cited papers by connecting the challenges of ethical alignment with the limitations of quantitative fairness metrics, suggesting a need for qualitative review. It shows some critical awareness by highlighting the incompleteness of protected attributes and the legal risks of certain fairness interventions. While it abstracts to a degree by discussing the broader implications of model behavior, it does not offer a novel framework or deep meta-level insights."}}
{"id": "488addd1-4e2c-4d55-95a6-e826cf6111db", "title": "Safety,", "level": "paragraph", "subsections": ["476653ae-d6c5-4d04-9568-cd22e9de25d4", "918969b5-87f5-473f-a6fb-e160251b2bc3"], "parent_id": "794a338d-c24e-4232-aa0b-42b03443de24", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Motivations for Interpretability"], ["paragraph", "Ethics,"], ["paragraph", "Safety,"]], "content": "is about ensuring the model performs within expectations in deployment. As it is nearly impossible to truly test the model, in the end-to-end context that it will be deployed, ensuring \\motivation{safety} does to some extent involve qualitative assessment .  frames this as \\motivation{trust}, and suggests one interpretation of this is ``that we feel comfortable relinquishing control to it''.\nWhile all types of interpretability can help with \\motivation{safety}, in particular, \\type{sec:adversarial-examples}{adversarial examples} and \\type{sec:counterfactuals}{counterfactuals} are useful, as they evaluate the model on data outside the test distribution.  frames this in the broader context of \\motivation{transferability}, which is the model's robustness to adversarial attacks and distributional shifts.", "cites": [8500, 7621], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes ideas from two papers, connecting the concept of safety to trust and interpretability, and introduces related techniques like adversarial examples and counterfactuals. It offers a basic level of abstraction by linking safety to broader concerns such as transferability and robustness, but the critical analysis is limited to pointing out general issues in the field rather than deeper evaluation of the cited works."}}
{"id": "476653ae-d6c5-4d04-9568-cd22e9de25d4", "title": "Accountability,", "level": "paragraph", "subsections": [], "parent_id": "488addd1-4e2c-4d55-95a6-e826cf6111db", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Motivations for Interpretability"], ["paragraph", "Ethics,"], ["paragraph", "Safety,"], ["paragraph", "Accountability,"]], "content": "relates to explaining the model when it does fail in production. The ``right to explanation'', regarding the logic involved in the model's prediction, is increasingly being adopted, most notably in the EU via its GDPR legislation. However, also the US and UK have expressed support for such regulation . Additionally, industries such as banking, are already required to audit their models .\n\\motivation{Accountability} is perhaps the core motivation of interpretability, as  writes ``Interpretability is the degree to which a human can understand the cause of a decision'', and it is exactly the causal reasoning that is relevant in \\motivation{accountability} .", "cites": [4875, 1798, 8069], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.3, "critical": 2.3, "abstraction": 3.3}, "insight_level": "medium", "analysis": "The section integrates the concept of accountability from the cited papers, particularly linking it to the 'right to explanation' and legal frameworks like GDPR. It also connects this to the importance of interpretability for causal reasoning. However, it does not deeply critique the approaches or identify significant limitations. The abstraction is moderate as it generalizes the role of explanation in legal and industrial contexts."}}
{"id": "918969b5-87f5-473f-a6fb-e160251b2bc3", "title": "Scientific Understanding,", "level": "paragraph", "subsections": [], "parent_id": "488addd1-4e2c-4d55-95a6-e826cf6111db", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Motivations for Interpretability"], ["paragraph", "Ethics,"], ["paragraph", "Safety,"], ["paragraph", "Scientific Understanding,"]], "content": "addresses a need from researchers and scientists, which is to generate hypotheses and knowledge. As  frames it, sometimes the best way to start such a process is to ask for explanations. In model development, explanations can also be useful for \\motivation{model debugging} , which is often facilitated by the same kinds of explanations.", "cites": [4875, 8500], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section shows basic synthesis by connecting the role of interpretability in scientific understanding and model debugging, referencing both papers to support this. It provides a critical perspective to some extent by highlighting the utility of explanations for research purposes, but does not deeply critique the papers or their methods. The abstraction is moderate, as it generalizes the idea of using explanations for hypothesis generation and debugging, though it stops short of proposing a meta-level framework or principle."}}
{"id": "3b1b3404-29ad-42f9-a69c-21c92f0b9968", "title": "Motivating Example", "level": "section", "subsections": ["66fa2c89-7121-4078-943d-6a4ece86b36b"], "parent_id": "baead8c2-91a5-41fd-8dd2-241ca88dd0ef", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Motivating Example"]], "content": "\\label{sec:motivating-example}\nBecause \\posthoc{post-hoc} methods are often model-agnostic, explaining and discussing them can often become abstract. To make the method sections as concrete and comparable as possible this survey will show fictive examples often based on the ``Stanford Sentiment Treebank'' (SST) dataset .\nThe SST dataset has been modeled using LSTM , Self-Attention-based models , etc., all of which are popular examples of neural networks.\nWe use a sequence-to-class problem because this is what most interpretability methods applies to. Although some are agnostic to the problem type and others are specific to sequence-to-sequence problems. Throughout this survey we attempt to highlight what problems each method applies to. \n\\begin{figure}[th]\n    \\centering\n    \\examplefigure{base}\n    \\caption{Three examples from the SST dataset . $\\mathbf{x}$ is the input, with each token denoted by an \\underline{underline}. $y$ is the gold target label, where \\texttt{pos} is \\emph{positive} and \\texttt{neg} is \\emph{negative} sentiment. Finally, $p(y|\\mathbf{x})$ is the model's estimate of $\\mathbf{x}$ belonging to category $y$. Note that the model predicts the 3rd (last) wrong, indicated with \\textcolor{rgb,255:red,179; green,0; blue,0}{red}.}\n    \\label{fig:motivating_example:examples}\n\\end{figure}\nThe model responsible for the predictions in \\Cref{fig:motivating_example:examples} can be explained by asking different questions, each of which communicates a different aspect of the model that is covered in the sections of this survey. Sometimes these explanation relates to a single observation, other times the explanation relates to the whole model.", "cites": [7, 1568], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of the SST dataset and the types of models used, such as LSTMs and self-attention-based models, referencing the cited papers to set context. However, it does not synthesize or integrate the broader implications of these models or their interpretability needs. There is no critical evaluation or abstraction to higher-level principles, making it primarily descriptive with low insight quality."}}
{"id": "2c575de8-df45-47be-9319-26c4c1f7166b", "title": "Measures of Interpretability", "level": "section", "subsections": ["908fd6bd-4076-42bf-a6a9-9f8f4cee5d63"], "parent_id": "baead8c2-91a5-41fd-8dd2-241ca88dd0ef", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Measures of Interpretability"]], "content": "\\label{sec:measures-of-interpretability}\nBecause interpretability is by definition about explaining the model to humans , and these explanations are often qualitative, it is not clear how to quantitatively evaluate and compare interpretability methods. This ambiguity has lead to much discussion. Most notable is the \\intrinsic{intrinsically} interpretable method \\emph{Attention}, where different measures of interpretability have been published resulting in conflicting findings . \nIn general, there is no consensus on how to measure interpretability. However, validation is still paramount. As such, this section attempts to cover the general categories, themes, and methods that have been proposed. Additionally, each method section, starting from \\type{sec:input-features}{input features}, in \\Cref{sec:input-features}, will briefly cover how the authors choose to evaluate their method.\nTo describe the evaluation strategies, we use the terminology defined by , which separates the evaluation of interpretability into three categories, \\measure{functionally-grounded}, \\measure{human-grounded}, and \\measure{application-grounded}. This categorization reflects the need to have explanations that are useful to humans (\\measure{human-grounded}) and accurately reflect the model (\\measure{functionally-grounded}).", "cites": [8500, 4850, 8072, 4848, 1798], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key concepts from multiple papers to frame a coherent discussion on how interpretability can be measured, particularly in the context of attention mechanisms. It critically evaluates the interpretability of attention through conflicting findings and introduces a robust categorization of evaluation strategies. The abstraction is strong, as it moves beyond specific examples to establish a general framework for assessing interpretability methods."}}
{"id": "908fd6bd-4076-42bf-a6a9-9f8f4cee5d63", "title": "Application-grounded", "level": "paragraph", "subsections": ["a7ed1eaa-ce97-4dc7-9ea3-a82a1e203e5b"], "parent_id": "2c575de8-df45-47be-9319-26c4c1f7166b", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Measures of Interpretability"], ["paragraph", "Application-grounded"]], "content": "evaluation is when the interpretability method is evaluated in the environment it will be deployed. For example, does the explanations result in higher survival-rates in a medical setting, a higher-grades in a homework-hint system, or a better model in a label-correction setting . Importantly, this evaluation should include the baseline where the explanations are provided by humans.\nDue to the application-specific and time-consuming nature of this approach, \\measure{application-grounded} evaluation is rarely done in NLP interpretability research. Instead, more synthetic and general evaluation setups can be used, which is what \\measure{functionally-grounded} and \\measure{human-grounded} evaluation is about. These categories each provide an important but different aspect for validating interpretability and should therefore be used in combination.", "cites": [8500], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a clear explanation of application-grounded evaluation and situates it within broader interpretability validation categories, showing some synthesis with the cited paper. It also critically notes the infrequency of this approach in NLP due to its application-specific and time-consuming nature, while abstracting the key idea that this form of evaluation is context-driven and should be compared to human explanations. However, it could benefit from deeper integration with multiple sources and more nuanced comparison with alternative evaluation methods."}}
{"id": "a7ed1eaa-ce97-4dc7-9ea3-a82a1e203e5b", "title": "Human-grounded", "level": "paragraph", "subsections": ["001b2b46-11d0-40e5-85d1-05517d94005c"], "parent_id": "908fd6bd-4076-42bf-a6a9-9f8f4cee5d63", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Measures of Interpretability"], ["paragraph", "Application-grounded"], ["paragraph", "Human-grounded"]], "content": "evaluation checks if the explanations are useful to humans. Unlike \\measure{application-grounded}, the task is often simpler and the task itself can be evaluated immediately. Additionally, expert humans are often not required . In other literature this is known as \\measure{simulatability}  and \\measure{comprehensibility} .\nAlthough, \\measure{human-grounded} evaluation is much more efficient than \\measure{application-grounded} evaluation, the human aspect still takes time. An unfortunate but common approach is therefore to replace the human with a simulated user. This is unfortunate as providing explanations that are informative to humans is a non-trivial task, and often involves interdisciplinary knowledge from the human-computer interaction (HCI) and social science fields. Replacing a human with a simulated user, therefore leads to over optimistic results.\n provides an excellent overview on what effective explanation is from the social science perspective, and criticizes current works by saying ``most work in explainable artificial intelligence uses only the researchers’ intuition of what constitutes a `good' explanation''.\nIt is therefore critical that interpretability methods are \\measure{human-grounded}. These are common strategies to measure \\measure{human-grounded}, used both in NLP and other fields:\n\\begin{itemize}\n    \\item Humans have to choose the best model based on an explanation .\n    \\item Humans have to predict the model's behavior on new data .\n    \\item Humans have to identify an outlier example called an intruder .  While it can be used on other fields, it is most common in NLP where it used with \\type{sec:vocabulary}{vocabulary} explanations .\n\\end{itemize}", "cites": [7621, 7507, 8500, 7801, 1798], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes multiple papers to frame human-grounded evaluation within a broader discussion of interpretability criteria, showing good integration across sources. It critically evaluates the use of simulated users and highlights the need for interdisciplinary grounding, especially from social sciences. The section abstracts the concept of human-grounded evaluation and identifies general strategies, though the critique and abstraction could be more nuanced and comprehensive."}}
{"id": "001b2b46-11d0-40e5-85d1-05517d94005c", "title": "Functionally-grounded", "level": "paragraph", "subsections": [], "parent_id": "a7ed1eaa-ce97-4dc7-9ea3-a82a1e203e5b", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Measures of Interpretability"], ["paragraph", "Application-grounded"], ["paragraph", "Human-grounded"], ["paragraph", "Functionally-grounded"]], "content": "evaluation checks how well the explanation reflects the model. This is more commonly known as \\measure{faithfulness}  or sometimes \\measure{fidelity} .\nIt might seem surprising that an explanation, which is directly produced from the model, would not reflect the model. However, even intrinsically interpretable methods such as \\emph{Attention} and \\emph{Neural Modular Networks} have been shown to not reflect the model .\nInterestingly, \\measure{human-grounded} interpretability methods can not reflect the model perfectly, because humans require explanations to be selective, meaning the explanation should select ``one or two causes from a sometimes infinite number of causes'' . Regardless, the explanations must still reflect the model to some extent, which surprisingly is not always the case . Additionally, explanations that provide a similar type of explanation, with similar selectiveness, should compete on proving the explanation that best reflects the model.\nFor some tasks, measuring if an interpretability method is \\measure{functionally-grounded} is trivial. In the case of \\type{sec:adversarial-examples}{adversarial examples}, it is enough to show that the prediction changed and the adversarial example is a paraphrase. In other cases, most notably \\type{sec:input-features}{input features}, providing a \\measure{functionally-grounded} metric can be very challenging . \nIn general, common evaluation strategies, both in NLP and other fields, are:\n\\begin{itemize}\n    \\item Comparing with an intrinsically interpretable model, such as logistic regression .\n    \\item Comparing with other post-hoc methods .\n    \\item Proposing axiomatic desirables .\n    \\item Benchmarking against random explanations .\n\\end{itemize}", "cites": [7507, 8071, 1824, 4302, 8072, 4848, 1798, 1617], "cite_extract_rate": 0.5333333333333333, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of functionally-grounded interpretability, integrating key concepts like faithfulness and fidelity from multiple papers. It connects findings across works (e.g., on attention and NMNs) to highlight broader challenges in ensuring explanations reflect model behavior. While it offers some critical insights, particularly about the limitations of human-grounded and attention-based methods, the synthesis is not fully novel, and the abstraction level remains moderate without a comprehensive meta-framework."}}
{"id": "849530e4-cb29-4354-8735-ede15e5bef8a", "title": "Input Features", "level": "section", "subsections": ["1b255110-8c27-4514-9074-17fc0714f3b2"], "parent_id": "baead8c2-91a5-41fd-8dd2-241ca88dd0ef", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Input Features"]], "content": "\\label{sec:input-features}\nAn \\type{sec:input-features}{Input feature} explanation is a \\category{local explanation}, where the goal is to determine how important an \\type{sec:input-features}{input feature}, e.g. a token, is for a given prediction. This approach is highly adaptable to different problems, as the input features are always known and are often meaningful to humans. Especially in NLP, the input features will often represent words, sub-words, or characters. Knowing which words are the most important, can be a powerful explanation method. An \\type{sec:input-features}{input feature} explanation of the input $\\mathbf{x}$, is represented as\n\\begin{equation}\n    \\mathbf{E}(\\mathbf{x}, c): \\mathrm{I}^\\mathbf{d} \\rightarrow \\mathbb{R}^\\mathbf{d} \\text{, where $\\mathrm{I}$ is the input domain and $\\mathbf{d}$ is the input dimensionality.}\n\\end{equation}\nNote that, when the output is a score of importance the explanation is called an \\emph{importance measure}. \nImportantly, \\type{sec:input-features}{input feature} explanations can only explain one scalar, meaning one class at one timestep. In a sequence-to-sequence application, the explanation is therefore repeated for each time step  although this may not respect the combinatorial complexities . Additionally, the selected class is either the most likely class or the true-label class, in this section the explained class is denoted with $c$. For all methods in this section, except \\method{sec:input-features:anchors}{Anchors}, $c$ can be set as desired.\n\\input{chapters/input_features_gradient}\n\\ifarxiv{\\input{chapters/input_features_ig}}\n\\input{chapters/input_features_lime}\n\\ifarxiv{\\input{chapters/input_features_shap}}\n\\ifarxiv{\\input{chapters/input_features_anchors}}", "cites": [8075], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of input feature explanations in the context of post-hoc interpretability for NLP models. It mentions the adaptability and human-understandability of input features but does not effectively synthesize insights across the cited papers. There is minimal critical evaluation or abstraction into broader patterns or principles."}}
{"id": "9c885506-c60d-4568-b5ee-261c1e08254c", "title": "Groundedness", "level": "paragraph", "subsections": ["5ea34c0a-2c3d-4a6b-92b0-91b56902af61"], "parent_id": "1b255110-8c27-4514-9074-17fc0714f3b2", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Input Features"], ["subsection", "Discussion"], ["paragraph", "Groundedness"]], "content": "The \\measure{functionally-groundedness} of \\type{sec:input-features}{input feature} explanations have recived a lot of attention and discussion, however there is still little consensus on what is \\measure{functionally-grounded} or how to even measure it .", "cites": [4850, 8072, 8073, 4848, 1617], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section introduces the concept of groundedness and notes the lack of consensus around it, which is a critical observation. However, it does not synthesize or integrate the cited papers into a cohesive narrative, nor does it abstract broader principles. It sets the stage for deeper analysis but remains relatively high-level and does not fully leverage the cited works to build a comprehensive understanding."}}
{"id": "5ea34c0a-2c3d-4a6b-92b0-91b56902af61", "title": "Future work", "level": "paragraph", "subsections": [], "parent_id": "9c885506-c60d-4568-b5ee-261c1e08254c", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Input Features"], ["subsection", "Discussion"], ["paragraph", "Groundedness"], ["paragraph", "Future work"]], "content": "It has been suggested, that a general \\measure{functionally-grounded} post-hoc \\type{sec:input-features}{input feature} explanation method just doesn't exists , an analogue to the no-free-lunch theorem.\nFor this reason, a new trend in NLP is to develop architecture specific \\type{sec:input-features}{input feature} explanations , for example using attention. Although others are aganist this direction and do not think that attention can provide more \\measure{functionally-grounded} explanations than general alternatives .\nSuch high-level questions are likely difficult to answer without a more fundamental understanding of what the \\measure{functionally-groundedness} desirables are. We therefore advocate for continuing the effort in measuring \\measure{functionally-groundedness} but to focus more on establishing the fundamental desirables.", "cites": [5808], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the concept of functionally-grounded explanations and relates it to architecture-specific methods like attention in NLP, drawing on Paper 1. It shows some critical analysis by acknowledging conflicting viewpoints on whether attention improves groundedness. The abstraction level is moderate as it identifies the broader need for understanding desirables in explanation methods but does not yet develop a comprehensive framework."}}
{"id": "4d62ab2e-afb5-4eb1-bc92-d4a92ff64e00", "title": "Adversarial Examples", "level": "section", "subsections": ["dd80ce8e-bc4a-46c6-ae50-44c562325490"], "parent_id": "baead8c2-91a5-41fd-8dd2-241ca88dd0ef", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Adversarial Examples"]], "content": "\\label{sec:adversarial-examples}\nAn \\type{sec:adversarial-examples}{adversarial example}, is an input that causes a model to produce a wrong prediction, due to limitations of the model. The adversarial example is often produced from an existing example, for which the model produces a correct prediction. Because the \\type{sec:adversarial-examples}{adversarial example} serves as an explanation, in the context of an existing example it is a \\category{local explanation}.\n provide a thorough survey on \\type{sec:adversarial-examples}{adversarial example} explanations, and also goes in depth regarding taxonomy, using \\type{sec:adversarial-examples}{adversarial examples} for robustness, and similarity scores between the existing example and the \\type{sec:adversarial-examples}{adversarial example}. Additonally, the survey by  also have a section on adversarial examples.\nIn this survey we therefore focus on just two explanation methods. These \\type{sec:adversarial-examples}{adversarial example} methods informs us about the support boundaries of a given example, which then informs us about the logic involved and therefore provides interpretability. In fact, this explanation can be similar to the \\type{sec:input-features}{input feature} methods, discussed in \\Cref{sec:input-features}. Many of those methods also indicate what words should be changed to alter the prediction. An important difference is that \\type{sec:adversarial-examples}{adversarial} explanations are contrastive, meaning they explain by comparing with another example, while \\type{sec:input-features}{input features} explain only concerning the original example. Contrastive explanations are, from a social science perspective, generally considered more \\measure{human-grounded} .\nIn the following discussions, we refer the original example as $\\mathbf{x}$ and the adversarial example as $\\tilde{\\mathbf{x}}$. The goal is to develop an adversarial method $A$, that maps from $\\mathbf{x}$ to $\\tilde{\\mathbf{x}}$:\n\\begin{equation}\n    A(\\mathbf{x}) \\rightarrow \\tilde{\\mathbf{x}}\n\\end{equation}\nImportantly, to ensure that an \\type{sec:adversarial-examples}{adverserial example} method is \\measure{functionally-grounded}, one only needs to assert that the predicted label changes while the gold label remains the same. Additionally, it's a desireable to have the original and adverserial example to be similar, in many applications this can be framed as paraphrasing. Compared to other explanation types, these properties are reasonably trivial to measure. See \\Cref{sec:measures-of-interpretability} for a general discussion on measures of interpretability.\nFinally, because \\type{sec:adversarial-examples}{adverserial example} explanations are framed by the output class, these explanations do not generalize easily to sequence-to-sequence problems. Although one could imagine for example an offensive-text classifier, which reduces the sequence-to-sequence model back to a sequence-to-class model.\n\\input{chapters/adversarial_examples_hotflip}\n\\ifarxiv{\\input{chapters/adversarial_examples_sea}}", "cites": [1798, 205], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of adversarial examples as a post-hoc interpretability method, integrating ideas from both cited papers to frame their use as contrastive, human-grounded explanations. It contrasts adversarial examples with input feature methods and discusses their functional grounding and limitations, particularly in sequence-to-sequence settings. However, the synthesis remains limited, as it only draws from two papers and does not fully explore the broader implications or deeper critiques of the methods."}}
{"id": "f32eb2c8-4d3b-4834-bcee-3e109f98e77c", "title": "Groundedness", "level": "paragraph", "subsections": ["98b9406d-d346-473f-ae52-3c6ea0862154"], "parent_id": "dd80ce8e-bc4a-46c6-ae50-44c562325490", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Adversarial Examples"], ["subsection", "Discussion"], ["paragraph", "Groundedness"]], "content": "\\type{sec:adversarial-examples}{Adversarial example} are as mentioned, easy to measure \\measure{functionally-groundedness} on and should be \\measure{human-grounded} due to their contrastive nature . However, we are not aware of any work which explicitly tests for \\measure{human-groundedness}. This is likely because it is considered to be a given, but we advocate for testing such a hypothesis anyway.", "cites": [1798], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section briefly references one cited paper on groundedness in explanations and makes a general argument about the assumption of human-groundedness in adversarial example research. While it hints at a gap in testing this assumption, it does not deeply synthesize or compare multiple sources. The critical aspect is present in identifying a potential oversight, but the abstraction is limited to a general observation without broader theoretical framing."}}
{"id": "98b9406d-d346-473f-ae52-3c6ea0862154", "title": "Future work", "level": "paragraph", "subsections": [], "parent_id": "f32eb2c8-4d3b-4834-bcee-3e109f98e77c", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Adversarial Examples"], ["subsection", "Discussion"], ["paragraph", "Groundedness"], ["paragraph", "Future work"]], "content": "The difficulty with \\type{sec:adversarial-examples}{adversarial example} explanations lies in the search procedure. For example, \\method{sec:adversarial-examples:hotflip}{HotFlip}  uses a greedy sequential search algorithm and would therefore not be able to identify combinatorial effects like a double-negative. While \\method{sec:adversarial-examples:sea}{SEA}  depends on an expensive paraphrase generation model.\nOne typical limitation of \\type{sec:adversarial-examples}{adversarial example} methods is that they provide no control of the search direction. Hypothetically, while changing ``unpredictable'' to ``unforeseeable'' could provide the largest source of error due to a robustness issue, it might be more interesting to discover that changing ``womans' chess club'' to ``mens' chess club'' also flips the label. Unfortunately, this aspect is usually not considered because the motivation for \\type{sec:adversarial-examples}{adversarial example} generation is often robustness and debiasing.", "cites": [889], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a brief analytical perspective by comparing the search procedures and limitations of two adversarial example methods (HotFlip and SEA). It integrates their characteristics to highlight a common issue with lack of control over search direction. While it identifies a meaningful gap, it does not extensively synthesize multiple sources or generalize to broader frameworks, keeping the abstraction and synthesis at a moderate level."}}
{"id": "1b578ac7-8ced-450a-a988-961d8b744be5", "title": "Groundedness", "level": "paragraph", "subsections": ["f7aa18b4-aac9-4e1a-bcd7-c3a378b84c2a"], "parent_id": "f86329df-3225-4eb2-8142-98ccf2414637", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Similar examples"], ["subsection", "Discussion"], ["paragraph", "Groundedness"]], "content": "\\type{sec:influential-examples}{Influential example} explanations, is one of the few categories with a non-trival but appropiate \\measure{functionally-grounded} metric, namely the label-correction experiment, which is used somewhat consistently across papers. Unfortunately, this experiment has not been used on NLP tasks and in general very little \\measure{functionally-grounded} validation have been done in NLP. \nAdditionally, the label-correction experiment is somewhat limited, as it evaluates the influence of a training observation on itself. This is not how a \\type{sec:influential-examples}{Influential examples} explanation would be used in most applications, for example dataset artifact discovery. We therefore suggest future work also include the experiment from  which uses information removal.", "cites": [8849], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section briefly integrates the concept of the label-correction experiment from the cited paper but does not synthesize it with other relevant works to form a broader narrative. It provides a critical evaluation by pointing out the experiment’s limitations and its lack of application in NLP, suggesting a need for alternative methods. While it identifies a general pattern in validation practices, it does not abstract to a meta-level principle or framework."}}
{"id": "99e52e06-2f5c-4338-9faf-f95a9c4c8b72", "title": "Counterfactuals", "level": "section", "subsections": ["f627f578-8b01-4474-abab-7b714b686cfa"], "parent_id": "baead8c2-91a5-41fd-8dd2-241ca88dd0ef", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Counterfactuals"]], "content": "\\label{sec:counterfactuals}\n\\type{sec:counterfactuals}{Counterfactual explanations} are essentially answering the question ``how would the input need to change for the prediction to be different?''. Furthermore, these \\type{sec:counterfactuals}{counterfactual examples} should be a minimal-edit from the original example and fluent. However, all of these properties can also be said of \\type{sec:adversarial-examples}{adversarial explanations}, and indeed some works confuse these terms. The critical difference is that \\type{sec:adversarial-examples}{adversarial examples} should have the same gold label as the original example, while \\type{sec:counterfactuals}{counterfactual examples} should have a different gold label (often opposite) as the original example . Because \\type{sec:counterfactuals}{Counterfactual explanations} are defined by the output class they are limited to sequence-to-class models.\nAnother common confusion is with \\emph{counterfactual datasets}, also known as \\emph{Contrast Sets}. These datasets are used in robustness research and could consist of \\emph{counterfactual examples}. However, these datasets are generated without using a model , and can therefore not be used to explain the model. \\emph{Contrast Sets} are however important for ensuring a robust model.\nIn social sciences, \\type{sec:counterfactuals}{counterfactual explanations} are considered highly useful for a person's ability to understand causal connections.  explains that ``why'' questions are often answered by comparing \\emph{facts} with \\emph{foils}, where the term \\emph{foils} is the social sciences term for \\type{sec:counterfactuals}{counterfactual examples}.\n\\ifarxiv{\\input{chapters/counterfactuals_polyjuice}}\n\\input{chapters/counterfactuals_mice}", "cites": [1798, 5696], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of counterfactual explanations by connecting the concept to adversarial examples and contrast sets, using cited papers to clarify distinctions and applications. It synthesizes key ideas from both AI and social sciences, but the integration remains relatively surface-level and does not construct a novel framework. The critical perspective is moderate, as it points out common confusions and limitations, but deeper evaluation of methodological strengths and weaknesses is limited."}}
{"id": "1a610238-faaf-4247-bd9f-8c7a7510d3bf", "title": "Natural Language", "level": "section", "subsections": ["c3291617-b678-4113-af6e-655784c37123"], "parent_id": "baead8c2-91a5-41fd-8dd2-241ca88dd0ef", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Natural Language"]], "content": "\\label{sec:natural-language}\nA common concern for many of the explanation methods presented in this survey is that they are difficult to understand for people without specialized knowledge. It is therefore attractive to directly generate an explanation in the form of \\type{sec:natural-language}{natural language}, which can be understood by simply reading the explanation for a given example. Because these utterances explain just a single example, they are a \\category{local explanation}. \nMost research in the area of \\type{sec:natural-language}{natural language} explanation uses the explanations to improve the predictive performance of the model itself. The idea is that by enforcing the model to reason about its behavior, the model can generalize better . These approaches are however in the category of \\intrinsic{intrinsic} methods. While those methods are often quite general, they are not discussed in this survey which focuses on \\posthoc{post-hoc} methods.\nThese \\posthoc{post-hoc} methods are referred to as \\emph{rationalization} methods, in the sense that they attempt to explain after a prediction has been made . Note that the term is a misnomer, as rationalizations in the dictionary sense\\footnote{``the action of attempting to explain or justify behaviour or an attitude with logical reasons, even if these are not appropriate.'' -- Oxford Defintion of \\textit{rationalization}.} can also be false.\n\\input{chapters/natural_language_cage}", "cites": [4325, 7802, 8076, 7801, 4296], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers by identifying a common theme—using natural language explanations for model interpretability—and distinguishing intrinsic vs. post-hoc approaches. It provides some critical discussion by noting that the term 'rationalization' may be misleading. However, it lacks deeper comparative analysis and more abstract generalizations about the broader implications of these methods."}}
{"id": "108c281b-7764-446b-ac36-6bec689a6940", "title": "Groundedness", "level": "paragraph", "subsections": ["7ddadc6e-5cf3-407a-a9b6-fe459e275d35"], "parent_id": "c3291617-b678-4113-af6e-655784c37123", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Natural Language"], ["subsection", "Discussion"], ["paragraph", "Groundedness"]], "content": "This sub-field of natural \\type{sec:natural-language}{natural language} explanations have received criticism in NLP for not evaluating \\measure{functionally-grounded} . This issue is even more problematic because the annotated explanations are provided by humans who have no insights into the model's behavior . The explanation model therefore just learns about humans' thought processes rather than the model's logical process. This issue is somewhat unique to the NLP literature and is better treated in other fields .", "cites": [8078, 8077], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section acknowledges a known criticism in the field (lack of functionally-grounded evaluation) and ties it to the cited works, but the integration is limited and not particularly novel. It provides some critical perspective by pointing out the issue of human-annotated explanations not reflecting the model's logic, but the analysis remains surface-level. The section briefly generalizes the issue to NLP as a whole but does not elevate the discussion to a meta-level or broader framework."}}
{"id": "7ddadc6e-5cf3-407a-a9b6-fe459e275d35", "title": "Future work", "level": "paragraph", "subsections": [], "parent_id": "108c281b-7764-446b-ac36-6bec689a6940", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Natural Language"], ["subsection", "Discussion"], ["paragraph", "Groundedness"], ["paragraph", "Future work"]], "content": "Most work on natural \\type{sec:natural-language}{natural language} explanations uses \\intrinsic{intrinsic} methods, under the motivation that forcing the model to ``reason about itself'' will make it more accurate. Unfortunately, this hypothesis has received criticism because the little \\posthoc{post-hoc} work there exist, show that this is not the case. Additionally, there are theoretical arguments for why this would not be the case .", "cites": [8079], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section briefly references one paper to highlight a trend in natural language explanations, specifically the use of intrinsic methods and their hypothesized benefits. While it identifies a criticism of the underlying hypothesis and points to theoretical arguments, the synthesis is limited and does not connect multiple sources to build a deeper narrative. The critical evaluation is present but remains at a high-level without detailed comparative or methodological critique. The abstraction is minimal, focusing mainly on the specific topic of groundedness in explanations."}}
{"id": "aaacb330-7c12-4e98-9d75-b332816db73a", "title": "Concepts", "level": "section", "subsections": ["87792127-00aa-4e18-9d7d-27b3a7e30190"], "parent_id": "baead8c2-91a5-41fd-8dd2-241ca88dd0ef", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Concepts"]], "content": "\\label{sec:concepts}\nA \\type{sec:concepts}{concept explanation} attempts to explain the model, in terms of an abstraction of the input, called a \\type{sec:concepts}{concept}. A classical example in computer vision, is to explain how the concept of stripes affects the classification of a zebra. Understanding this relationship is important, as a computer vision model could classify a zebra based on a horse-like shape and a savana background. Such relation may yield a high accuracy score but is logically wrong.\nThe term \\type{sec:concepts}{concept} is much more common in computer vision  than in NLP. Instead, the subject is often framed more concretly as bias-detection, in NLP. For example,  uses the concept of occupation-words like \\emph{nurse}, and relates it to the classification of the words \\emph{he} and \\emph{she}.\nRegardless of the field, in both NLP and CV, only a single class or small subset of classes are analyzed. For this reason, \\type{sec:concepts}{concept explanation} belong in its own category of \\category{class explanations}. However, in the future, we will likely see more types of \\category{class explanations}.\n\\input{chapters/concepts_nie}", "cites": [8081, 8080], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from both NLP and computer vision, particularly emphasizing the use of concepts for interpretability. It connects these ideas to the broader category of 'class explanations' and anticipates future developments. While it does offer a critical perspective by noting the current limitation of analyzing only a single class or subset, deeper evaluation of the methods or their implications is limited. The abstraction is reasonable, identifying trends and generalizing the concept framework across fields."}}
{"id": "d674289b-97a9-471a-9376-28750f059fc8", "title": "Future work", "level": "paragraph", "subsections": [], "parent_id": "25b19d23-fd7c-45f5-93f6-a41963fa520f", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Concepts"], ["subsection", "Discussion"], ["paragraph", "Groundedness"], ["paragraph", "Future work"]], "content": "\\type{sec:concepts}{Concept explanation} requires either a new dataset or annotation of an existing dataset. This can be quite expensive and impractical, especially when there is no concrete concept in mind and the user wants a more exploratory explanation. However, there is new research towards discovering concepts automatically .", "cites": [1816], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly acknowledges the need for concept-based explanations and mentions the existence of new research on automatic concept discovery, citing one paper. However, it lacks synthesis of broader ideas from multiple sources, critical evaluation of the cited work’s strengths or weaknesses, and abstraction to general principles. The insight is minimal and primarily serves as an introductory remark rather than a deep analytical discussion."}}
{"id": "2a145c81-3359-4b53-a07c-98a465d7c5b3", "title": "Vocabulary", "level": "section", "subsections": ["99fbbefd-ae8f-49f1-be75-37eba717df96"], "parent_id": "baead8c2-91a5-41fd-8dd2-241ca88dd0ef", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Vocabulary"]], "content": "\\label{sec:vocabulary}\nFor this category, we define the term \\type{sec:vocabulary}{vocabulary explanation} as methods which explain the whole model in relation to each word in the vocabulary and is therefore a \\category{global explanation}. \nIn the sentiment classification context, a useful insight could be if positive and negative words are clustered together respectively. Furthermore, perhaps there are words in those clusters which can not be considered of either positive or negative sentiment. Such a finding could indicate a bias in the dataset.\nBecause \\type{sec:vocabulary}{vocabulary explanations} explain using the model's vocabulary, they can often be applied to both sequence-to-class and sequence-to-sequence models. This is esspecially true for explainations based on the embedding matrix, which so is almost exclusively the case.\nBecause an embedding matrix is often used and because neural NLP models often use pre-trained word embeddings, most research on \\type{sec:vocabulary}{vocabulary explanations} is applied to the pre-trained word embeddings . However, in general, these explanation methods can also be applied to the word embeddings after training.\n\\input{chapters/vocabulary_projection}\n\\input{chapters/vocabulary_rotation}", "cites": [7165], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section introduces the concept of vocabulary explanations but does not synthesize information from the cited paper beyond a minimal reference. It lacks critical evaluation or comparison of methods and provides limited abstraction, focusing mainly on definitions and general applicability rather than deeper insights or overarching patterns."}}
{"id": "ab420a55-f537-4e2b-8a3d-97cd76ed0119", "title": "Ensemble", "level": "section", "subsections": ["4d114ee9-94c8-42dd-867c-058778104f9c"], "parent_id": "baead8c2-91a5-41fd-8dd2-241ca88dd0ef", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Ensemble"]], "content": "\\label{sec:ensemble}\n\\type{sec:ensemble}{Ensemble} explanations attempts to provide a \\category{global explanation} by collecting multiple \\category{local explanations}. This is done such that each \\category{local explaination} represents the different modes of the model.\nThe extreme of this idea would be to provide a \\category{local explaination} for every possible input, thereby providing a \\category{global explanation}. Unfortunately, such an explanation is too much information for a human to understand and would not be \\measure{human-grounded}. As  state, an explanation should be selective. The task of \\type{sec:ensemble}{ensemble} explanations, is therefore to strategically select representative examples and their corresponding \\category{local explainations}.\nThe assumption is that the model operates within different modes. Futhermore, that one example, or a few examples, from each mode can sufficiently represent the models entire behavior. For example, in sentimate classification of movie reviews, a model may have one behavior for comments about the acting, another behavior for comments about the music score, etc.\n\\type{sec:ensemble}{Ensemble} explanations is a very broad category of explanations, as for every type of \\category{local explanation} method there is, an \\type{sec:ensemble}{ensemble} explanation could in principle be constructed. As such, if it can be applied to sequence-to-class or sequence-to-sequence models depends depends on the specific method. However, in practice very few \\type{sec:ensemble}{ensemble} methods have been proposed, and most of them apply only to tabular data .\n\\input{chapters/ensamble_splime}", "cites": [1798, 8082], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of ensemble explanations in post-hoc interpretability by synthesizing the concept of global explanations from multiple local ones. It integrates ideas from the cited papers to explain the rationale and limitations of ensemble methods, particularly in the context of model behavior and human-grounded explanations. While it introduces broader patterns, such as the need for selectivity and the assumption of model modes, it lacks in-depth comparative or evaluative analysis of the specific methods proposed in the cited works."}}
{"id": "9b7cbbe1-ad23-4030-9c64-5f07c038617f", "title": "Future work", "level": "paragraph", "subsections": [], "parent_id": "7f8ff919-46c5-4f7d-93e4-2c309533732a", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Ensemble"], ["subsection", "Discussion"], ["paragraph", "Groundedness"], ["paragraph", "Future work"]], "content": "As mentioned there is not much work using \\type{sec:ensemble}{ensemble} explanations. This is because when non-tabular data is used, it is more challenging to compare the selected explanations to ensure they represent different modes. Even \\method{sec:ensemble:sp-lime}{SP-LIME}  which does apply to NLP tasks, uses a Bag-of-Word representation as a tabular proxy. Additionally, we can imagine that \\type{sec:ensemble}{ensemble} explanations are hard to scale, as datasets increases and models get more complex with more modes.\nThat being said, we would be curious to see more work in this category. For example, an \\type{sec:ensemble}{ensemble} explanation which used a \\type{sec:influential-examples}{influential example} method to show the overall most relevant observations.", "cites": [7507], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a brief analytical outlook on the limitations and potential of ensemble methods in post-hoc interpretability for NLP. It integrates one cited paper (LIME) to contextualize the challenge of applying ensemble methods to non-tabular data but does not deeply synthesize multiple sources or offer a novel framework. The critical analysis is moderate, pointing out scalability issues and data representation constraints. Abstraction is limited, with the section focusing more on specific examples than broader principles."}}
{"id": "abc2fbb4-a5c3-4780-9019-f95f75a5e1c6", "title": "Linguistic Information", "level": "section", "subsections": ["e344757e-e535-463f-8dc0-1c19ef1f1387"], "parent_id": "baead8c2-91a5-41fd-8dd2-241ca88dd0ef", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Linguistic Information"]], "content": "\\label{sec:linguistic-information}\nTo validate that a natural language model does something reasonable, a popular approach is to attempt to align the model with the large body of linguistic theory that has been developed for hundreds of years. Because these methods summarize the model, they are a case of \\category{global explanation}.\nMethods in this category either probe by strategically modifying the input to observe the model's reaction or show alignment between a latent representation and some linguistic representation. The former is called \\method{sec:linguistic-information:behavioral-probes}{behavioral probes} or \\method{sec:linguistic-information:behavioral-probes}{behavioral analysis}, the latter is called \\method{sec:linguistic-information:structural-probes}{structural probes} or \\method{sec:linguistic-information:structural-probes}{structural analysis}. Which type of models these strategies applies to depends on the specific method. However, in general \\method{sec:linguistic-information:behavioral-probes}{behavioral probes} applies primarily to sequence-to-class models and \\method{sec:linguistic-information:structural-probes}{structural probes} applies to both sequence-to-class and sequence-to-sequence models.\nOne especially noteworthy subcategory of \\method{sec:linguistic-information:structural-probes}{Structural Probes} is \\emph{BERTology}, which specifically focuses on explaining the BERT-like models . BERT's popularity and effectiveness have resulted in countless papers in this category , hence the name \\emph{BERTology}. Some of the works use the attention of BERT and are therefore \\intrinsic{intrinsic} explanations, while others simply probe the intermediate representations and are therefore \\posthoc{post-hoc} explanations.\nThere already exist well-written survey papers on \\type{sec:linguistic-information}{Linguistic Information} explanations. In particular,  cover \\method{sec:linguistic-information:behavioral-probes}{behavioral probes} and \\method{sec:linguistic-information:structural-probes}{structural probes},  discuss \\emph{BERTology}, and  cover \\method{sec:linguistic-information:structural-probes}{structural probing} in detail. In this section, we will therefore not go in-depth, but simply provide enough context to understand the field and importantly mention some of the criticisms, that we believe have not been sufficiently highlighted by other surveys.\n\\input{chapters/linguistic_information_behavioral}\n\\input{chapters/linguistic_information_structual}", "cites": [4871, 4485, 8083, 8084, 205, 679, 7, 826], "cite_extract_rate": 0.8, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of linguistic interpretability methods, distinguishing between behavioral and structural probes and linking these to BERTology. It offers some critical perspective by noting that existing surveys may not sufficiently highlight criticisms in the field. However, while it introduces general categories, it lacks deeper abstraction or a novel framework that would elevate the insight level to high."}}
{"id": "e96c9d31-bd15-4f2a-bdeb-4ae712ab8d9e", "title": "Future work", "level": "paragraph", "subsections": [], "parent_id": "a00002fe-5b56-42d7-a64f-b44cd13cc879", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Linguistic Information"], ["subsection", "Discussion"], ["paragraph", "Groundedness"], ["paragraph", "Future work"]], "content": "Considering the \\measure{groundedness} issues in \\type{sec:linguistic-information}{linguistic information} explanations, we advocate for more focus on \\measure{groundedness}.  provide a great solution to how the \\measure{functionally-groundedness} issues can be overcome. However, the field still lacks independent study on \\measure{human-groundedness} and \\measure{functionally-groundedness}.", "cites": [8085], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 3.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section briefly acknowledges the groundedness issues in linguistic information explanations and mentions one cited paper as a solution, but lacks deeper synthesis of multiple sources. It identifies a gap in independent studies on human and functional groundedness, showing some critical awareness, though the critique is minimal. The abstraction level is limited, as the discussion remains close to the specific concept of groundedness without generalizing to broader patterns or principles."}}
{"id": "90d060ce-a162-446e-a348-386da1361d19", "title": "Rules", "level": "section", "subsections": ["7e7edc8e-b4d7-443d-9978-092f91ec4462"], "parent_id": "baead8c2-91a5-41fd-8dd2-241ca88dd0ef", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Rules"]], "content": "\\label{sec:rules}\n\\type{sec:rules}{Rule} explanations attempt to explain the model by a simple set of rules, therefore they are an example of \\category{global explanations}.\nReducing highly complex models like neural networks to a simple set of rules is likely impossible. Therefore, methods that attempt this simplify the objectivity by only explaining one particular aspect of the model.\nDue to the challenges of producing rules, there is little research attempting it. We will present \\method{sec:rules:comp-explain-neuron}{Compositional Explanations of Neurons}  and \\method{sec:rules:sear}{SEAR} .\n\\input{chapters/rules_sear}\n\\ifarxiv{\\input{chapters/rules_compositional}}", "cites": [8081], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a minimal synthesis of the cited work, merely introducing the two methods without connecting them to broader themes or each other. It lacks critical evaluation of the papers' strengths, weaknesses, or assumptions. The abstraction level is low, as it does not generalize beyond the specific methods to highlight underlying principles or trends in rule-based interpretability for NLP."}}
{"id": "cc2662f8-0612-496c-a15f-2f9898451146", "title": "Combining post-hoc with intrinsic methods", "level": "paragraph", "subsections": [], "parent_id": "3bf5abba-da9d-4e0d-9453-54b1d768c2d5", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Future directions and challenges"], ["paragraph", "Measuring Interpretability"], ["paragraph", "Class explanations"], ["paragraph", "Combining post-hoc with intrinsic methods"]], "content": "\\posthoc{Post-hoc} and \\intrinsic{intrinsic} methods are in literature, including this paper, represented as distinct. However, there are important middle grounds. \nAs mentioned in the introduction, most \\intrinsic{intrinsic} methods are not purely intrinsic. They often have an intermediate representation, which can be intrinsically interpretable. However, producing this representation is often done with a black-box model. For this reason, \\posthoc{post-hoc} explanations are needed if the entire model is to be understood.\nBeyond this direction, there are works where the training objective and procedure helps to provide better \\posthoc{post-hoc} explanations. This survey briefly argues that the \\method{sec:input-features:shap}{Kernel SHAP} method exists in this middle ground, as it depends on input-masking being part of the training procedure. In computer vision,  show that adding noise to the input images creates better \\type{sec:input-features}{input feature} explanations. In general, we hope to see more work in this direction.", "cites": [8086], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section attempts to synthesize the relationship between post-hoc and intrinsic methods, highlighting a middle ground where they intersect. It cites one paper to support a specific point about hyperparameter sensitivity but does not deeply integrate multiple works into a novel framework. There is some critical thought regarding the hybrid nature of intrinsic methods and the role of training objectives in improving post-hoc explanations, but the critique remains surface-level. The section abstracts to a degree by identifying a general trend of combining these methods, though it lacks deeper meta-level insights."}}
{"id": "bd49a418-2a9a-4e57-9804-23b5580cb0e5", "title": "Integrated Gradient (IG)", "level": "subsection", "subsections": [], "parent_id": "d86cc4fd-653c-4c14-93ec-672ee2d486f8", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Input features"], ["subsection", "Integrated Gradient (IG)"]], "content": "\\label{sec:input-features:integrated-gradient}\nThe \\method{sec:input-features:gradient}{gradient} approach has been further developed, the most notable development is \\method{sec:input-features:integrated-gradient}{Integrated Gradient} .\n primarily motivate \\method{sec:input-features:integrated-gradient}{Integrated Gradient} via the desirables they call \\emph{sensitivity} and \\emph{completeness}. \\emph{Sensitivity} means, if there exists a combination of $\\mathbf{x}$ and baseline $\\mathbf{b}$ (often an empty sequence), where the logit outputs of $f(\\mathbf{x};\\theta)$ and $f(\\mathbf{b};\\theta)$ are different, then the feature that changed should get a non-zero attribution. This desirable is not satisfied for the gradient method, for example due to the truncation  in $\\operatorname{ReLU}(\\cdot)$. \\emph{Completeness} means, the sum of importance scores assigned to each token should equal the model output relative to the baseline $\\mathbf{b}$.\nTo satify these desirables,  develop equation \\eqref{eq:input-features:integrated-gradient:formulation} which integrates the gradients between an uninformative baseline $\\mathbf{b}$ and the observation $\\mathbf{x}$ .\n\\begin{equation}\n\\begin{aligned}\n    \\mathbf{E}_{\\operatorname{integrated-gradient}}(\\mathbf{x}, c) &= (\\mathbf{x} - \\mathbf{b}) \\odot \\frac{1}{k} \\sum_{i=1}^{k} \\nabla_{\\tilde{\\mathbf{x}}_i} f(\\tilde{\\mathbf{x}}_i;\\theta)_c, \\quad \\tilde{\\mathbf{x}}_i = \\mathbf{b} + \\sfrac{i}{k}(\\mathbf{x} - \\mathbf{b}), \\\\\n    \\text{where }&\\text{$f(\\mathbf{x};\\theta)$ is the model logits.}\n    \\label{eq:input-features:integrated-gradient:formulation}\n\\end{aligned}\n\\end{equation}\nThis approach has been successfully applied to NLP, where the uninformative baseline can be an empty sentence, such as padding tokens .\nAlthough Integrated Gradient has become a popular approach, it has recently received criticism in computer vision (CV) community for not being \\measure{functionally-grounded} . More recent work have applied a similar analysis to NLP, and found that the \\measure{functionally-groundedness} is at the very least task dependent . Additonally,  uses synthetic NLP tasks and arrived at the same task-dependent conclusion. One explanation for the lack of \\measure{functionally-groundedness} is the input mutiplication which is not directly related to the model .", "cites": [5850, 1824, 1617], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.0}, "insight_level": "high", "analysis": "The section provides a clear explanation of Integrated Gradient, drawing on Paper 2 for its theoretical foundations and integrating the concept with its application in NLP. It also critically engages with recent critiques from the CV community (Paper 3) and notes the task dependency in NLP, showing a nuanced evaluation. While not synthesizing a novel framework, it connects key ideas and identifies limitations, contributing to a broader understanding of the method's strengths and weaknesses."}}
{"id": "019d187c-975e-4cf5-9ced-754c4322bcae", "title": "Kernel SHAP", "level": "subsection", "subsections": [], "parent_id": "d86cc4fd-653c-4c14-93ec-672ee2d486f8", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Input features"], ["subsection", "Kernel SHAP"]], "content": "\\label{sec:input-features:shap}\nA limitation of \\method{sec:input-features:lime}{LIME} is that the weights in a linear model are not necessarily \\intrinsic{intrinsically} interpretable. When there exists multicollinearity (input features are linearly correlated with each other) then the model weights can be scaled arbitrarily creating a false sense of importance.\nTo avoid the multicollinearity issue, one approach is to compute Shapley values  which are derived from game theory. The central idea is to fit a linear model for every permutation of features enabled. For example, if there are two features $\\{x_1, x_2\\}$, the Shapley values would aggregate the weights from fitting the datasets with features $\\{\\varnothing\\}, \\{x_1\\}, \\{x_2\\}, \\{x_1, x_2\\}$. If there are $T$ features this would require $\\mathcal{O}(2^T)$ models.\nWhile this method works in theory, it is clearly intractable.  present a framework for producing Shapley values in a more tractable manner. The model-agnostic approach they introduce is called \\method{sec:input-features:shap}{Kernel SHAP}. It combines 3 ideas: it reduces the number of features via a mapping function $h_\\mathbf{x}(\\mathbf{z})$, it uses squared-loss instead of cross-entropy by working on logits, and it weighs each observation by how many features there are enabled.\n\\begin{equation}\n\\begin{aligned}\n    \\mathbf{E}_{\\operatorname{SHAP}}(\\mathbf{x}, c) = &\\argmin_{\\mathbf{w}} \n    \\sum_{\\mathbf{z} \\in \\mathbb{Z}^M} \\pi(\\mathbf{z})\\ (f(h_\\mathbf{x}(\\mathbf{z});\\theta)_c - g(\\mathbf{z}))^2 \\\\\n    &\\text{where } g(\\mathbf{z}) = \\mathbf{w} \\mathbf{z} \\\\\n    &\\phantom{\\text{where }} \\pi(\\mathbf{z}) = \\frac{M - 1}{(M\\, \\operatorname{choose}\\, |\\mathbf{z}|) |\\mathbf{z}| (M - |\\mathbf{z}|)}\n\\end{aligned}\n\\label{eq:input-features:shap}\n\\end{equation}\nIn \\eqref{eq:input-features:shap}, $\\mathbf{z}$ is a $\\{0,1\\}^M$ vector that describes which combined features are enabled. This is then used in $h_\\mathbf{x}(\\mathbf{z})$, which enables those features in $\\mathbf{x}$. Furthermore, $\\mathbb{Z}^M$ represents all permutations of enabled combined features and $|\\mathbf{z}|$ is the number of enabled combined-features. \\Cref{fig:input-features:shap}, demonstrates a fictive example of how input features can be combined and visualize their shapley values.\n\\begin{figure}[h]\n    \\centering\n    \\examplefigure{shap}\n    \\caption{Fictive visualization of \\method{sec:input-features:shap}{Kernel SHAP}. Note how input tokens are combined to a single feature to make \\method{sec:input-features:shap}{SHAP} more tractable to compute, this is the role of $h_\\mathbf{x}(z)$ in \\eqref{eq:input-features:shap}.}\n    \\label{fig:input-features:shap}\n\\end{figure}\n show \\measure{functionally-groundedness} by using that Shapley values uniquely satisfy a set of desirables and that \\method{sec:input-features:shap}{SHAP} values are also Shapley values. Furthermore,  show \\measure{human-groundedness} by asking humans to manually produce importance measures and correlate them with the \\method{sec:input-features:shap}{SHAP} values.\nA criticism of both SHAP and LIME is that they depend on pertubation of the input, this makes it possible to create adverserial models that appear ethical when explained using pertubated inputs but is in reality not ethical when evaluted without pertubation . This means that LIME and SHAP can only provide a \\measure{functionally-grounded} explanation as long as the model is trained without malicious intent.\n\\method{sec:input-features:shap}{SHAP} and Shapley values in general are heavily used in the industry . In NLP literature \\method{sec:input-features:shap}{SHAP} has been used by . This popularity is likely due to their mathematical foundation and the \\texttt{shap} library. In particular, the \\texttt{shap} library also presents Partition SHAP which claims to reduce the number of model evaluations to $M^2$, instead of $2^M$ \\footnote{See documentation \\url{https://shap.readthedocs.io/en/latest/example_notebooks/tabular\\_examples/model_agnostic/Simple\\%20Boston\\%20Demo.html}}. One major disadvantage of \\method{sec:input-features:shap}{SHAP} is it inherently depends on the masked inputs still being valid inputs. For some NLP models, this can be accomplished with a \\texttt{[MASK]} token, while for it is not possible in a \\posthoc{post-hoc} setting. For this reason, \\method{sec:input-features:shap}{SHAP} exists at an intersection between \\posthoc{post-hoc} and \\posthoc{intrinsic} interpretability methods. This intersection is discussed more in \\Cref{sec:future-directions}.", "cites": [4875, 1813, 8087], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of Kernel SHAP by integrating it with the limitations of LIME, its theoretical foundation in Shapley values, and its practical usage in NLP. It connects ideas from the cited papers to build a broader understanding of model-agnostic interpretability and highlights the trade-offs between tractability and validity. However, the synthesis is limited by not explicitly linking how the cited works (e.g., Polyjuice or deployment practices) inform or contrast with SHAP's role in NLP."}}
{"id": "7cd06634-e898-4653-bf19-804ce6010e57", "title": "Anchors", "level": "subsection", "subsections": [], "parent_id": "d86cc4fd-653c-4c14-93ec-672ee2d486f8", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Input features"], ["subsection", "Anchors"]], "content": "\\label{sec:input-features:anchors}\nA further development of the idea, that sparse explanations are easier to understand, is \\method{sec:input-features:anchors}{Anchors}. Instead of giving an importance score, like in the case of the gradient-based methods or \\method{sec:input-features:lime}{LIME}, the \\method{sec:input-features:anchors}{Anchors} simply provides a shortlist of words that were most relevant for making the prediction . The authors show \\measure{human-groundedness} with a similar user setup as in \\method{sec:input-features:lime}{LIME} .\n\\begin{figure}[h]\n    \\centering\n    \\examplefigure{anchor}\n    \\caption{Fictive visalization, showing the \\method{sec:input-features:anchors}{anchors} that are responsible for the prediction.}\n    \\label{fig:input-features:anchors}\n\\end{figure}\nThe list-of-words called ``anchors'' ($A$) is formalized in \\eqref{eq:input-features:anchors}. Note that $c = \\argmax_i p(i|\\mathbf{x};\\theta)$ is a requirement for \\method{sec:input-features:anchors}{anchors}, as using $\\operatorname{prec}(A) = \\mathbb{E}_{\\mathcal{D}(\\tilde{\\mathbf{x}}|A)} \\left[\\mathds{1}_{y = \\tilde{y}}\\right]$ in \\eqref{eq:input-features:anchors} would cause \\method{sec:input-features:anchors}{anchors} to be unaffected by the model.\n\\begin{equation}\n\\begin{aligned}\n    \\mathbf{E}_{\\operatorname{anchors}}(\\mathbf{x}) = &\\argmax_{A \\text{ s.t. } \\operatorname{prec}(A) \\ge \\tau\\, \\wedge\\, A(\\mathbf{x}) = 1} \\operatorname{cov}(A) \\\\\n    &\\text{where } \\operatorname{prec}(A) = \\mathbb{E}_{\\mathcal{D}(\\tilde{\\mathbf{x}}|A)} \\left[\\mathds{1}_{[\\argmax_i p(i|\\mathbf{x};\\theta) = \\argmax_i p(i|\\tilde{\\mathbf{x}};\\theta)]}\\right] \\\\\n    &\\phantom{\\text{where }} \\operatorname{cov}(A) = \\mathbb{E}_{\\mathcal{D}(\\tilde{\\mathbf{x}})} \\left[A(\\tilde{\\mathbf{x}})\\right] \\\\\n    &\\phantom{\\text{where }} A(\\mathbf{x}) = \\begin{cases}\n    1 & \\text{if the anchors $A$ are in $\\mathbf{x}$} \\\\\n    0 & \\text{otherwise}\n    \\end{cases}\n\\end{aligned}\n\\label{eq:input-features:anchors}\n\\end{equation}\nThis formalization says the anchor words should have the highest coverage ($\\operatorname{cov}(A)$), meaning the most sentences in the dataset $D(\\tilde{\\mathbf{x}})$ contains the anchors $A$. Furthermore, only consider anchors $A$ that are sufficiently precise ($\\operatorname{prec}(A) \\ge \\tau$) and in $\\mathbf{x}$. Precision is defined as the ratio of observations $\\tilde{\\mathbf{x}}$ with anchors $A$, denoted $\\mathcal{D}(\\tilde{\\mathbf{x}}|A)$, where the predicted label of $\\tilde{\\mathbf{x}}$ matches the predicted label of $\\mathbf{x}$.\nSolving this optimization problem exactly is infeasible, as the number of anchors is combinatorially large. To approximate it,  model $\\operatorname{prec}(A) \\ge \\tau$ probabilistically  and then use a bottom-up approach, where they add a new word to the $k$-best anchor candidate in each iteration similar to beam-search.", "cites": [7507], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of Anchors, mainly paraphrasing the method's core idea and its mathematical formulation. It briefly contrasts Anchors with gradient-based methods and LIME, but the synthesis is minimal and the critical evaluation is absent. The abstraction level is low, as the section focuses on the specifics of a single method without identifying broader patterns or principles."}}
{"id": "a13dd72c-429b-46f4-9bb7-d0eed131220f", "title": "Polyjuice", "level": "subsection", "subsections": [], "parent_id": "0f9ce63b-dd63-4739-b961-086ebffe8353", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Counterfactuals"], ["subsection", "Polyjuice"]], "content": "\\label{sec:counterfactuals:polyjuice}\n\\method{sec:counterfactuals:polyjuice}{Polyjuice} by  is primarily a \\emph{counterfactual dataset} generator, and the generation is therefore detached from the model. However, by strategically filtering these generated examples such that the model's prediction is changed the most, they condition the \\emph{counterfactual} generation on the model, thereby making a \\posthoc{post-hoc} explanation.\nThe generation is done by fine-tuning a GPT-2 model  on existing \\emph{counterfactual datasets} . For each pair of original and counterfactual example, they produce a training prompt, see \\eqref{fig:counterfactuals:polyjuice:training-prompt} for the exact structure. What the conditoning code is and what is replaced in \\eqref{fig:counterfactuals:polyjuice:training-prompt} is determined by the existing \\emph{counterfactual datasets}.\n\\begin{equation}\n\\begin{aligned}\n    prompt = \\text{``}&\\underbrace{\\text{It is great for kids}}_{\\text{original sentence}}\n    \\ \\texttt{<GENERATE>} \\\\\n    &\\ \\underbrace{\\texttt{[negation]}}_{\\text{conditioning code}}\n    \\ \\underbrace{\\text{It is \\texttt{[BLANK]} great for \\texttt{[BLANK]}}}_{\\text{masked counterfactual}} \\\\\n    &\\ \\texttt{<REPLACE>}\\ \n    \\underbrace{\\text{not \\texttt{[ANSWER]} children \\texttt{[ANSWER]}}}_{\\text{masking answers}}\n    \\ \\texttt{<EOS>}\\text{''}\n\\end{aligned}\n\\label{fig:counterfactuals:polyjuice:training-prompt}\n\\end{equation}\nFor \\emph{counterfactual} generation, they specify the original sentence and optionally the condition code, and then let the model generate the \\emph{counterfactuals}. These \\emph{counterfactuals} are independent of the model. To make them dependent on the model, they filter the \\emph{counterfactuals} and select those examples that change the prediction the most. One important detail is that they adjust the prediction change with an \\type{sec:input-features}{importance measure} (\\method{sec:input-features:shap}{SHAP}), such that the \\type{sec:counterfactuals}{counterfactual examples} that could have been generated by an \\type{sec:input-features}{importance measure} are valued less. An example of this explanation can be seen in \\Cref{fig:counterfactuals:polyjuice}.\n\\begin{figure}[H]\n    \\centering\n    \\examplefigure{polyjuice}\n    \\caption{Hypothetical results of \\method{sec:counterfactuals:polyjuice}{Polyjuice}, showing how some words were either replaced or removed to produce \\type{sec:counterfactuals}{counterfactual examples}.}\n    \\label{fig:counterfactuals:polyjuice}\n\\end{figure}\nTo validate \\method{sec:counterfactuals:polyjuice}{Polyjuice}, for a \\measure{human-grounded} experiment, they show that humans were unable to predict the model's behavior for the \\type{sec:counterfactuals}{counterfactual examples}, thereby concluding that their method highlights potential robustness issues. Whether \\method{sec:counterfactuals:polyjuice}{Polyjuice} is \\measure{functionally-grounded} is somewhat questionable, because the model is not a part of the generation process itself, it is merely used as a filtering step.", "cites": [4276, 5696, 5867, 8087], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a clear description of Polyjuice and its mechanism but does not deeply synthesize or connect it with the broader literature on counterfactuals or interpretability. It includes a minor critique regarding its functional grounding, but this is limited and lacks depth. The content remains largely system-focused with little abstraction to general principles."}}
{"id": "41c7952b-0ef4-4a54-b6f7-6e61bd83ccd5", "title": "Compositional Explanations of Neurons", "level": "subsection", "subsections": [], "parent_id": "1e81338e-e4d0-4fbe-8cfd-95777c0ed844", "prefix_titles": [["title", "Post-hoc Interpretability for Neural NLP: A Survey"], ["section", "Rules"], ["subsection", "Compositional Explanations of Neurons"]], "content": "\\label{sec:rules:comp-explain-neuron}\nIn \\method{sec:rules:comp-explain-neuron}{Compositional Explanations of Neurons} by , the rule generation problem is simplified by only relating the presence of input words to the activation of a single neuron.\nThe rules typically have the form of logical rules, meaning \\texttt{not}, \\texttt{and}, and \\texttt{or}, where the booleans indicate a word is present, although  do not make any hard constraints here. For example, in an NLI task they also have indicators for POS-presence and word-overlap between the hypothesis and premise. If these rules are satisfied it means the neuron activation is above a defined threshold. For example, in a $\\operatorname{ReLU}(\\cdot)$ unit one can threshold if its post-activation is above 0.\n\\begin{figure}[h]\n    \\centering\n    \\examplefigure{comp}\n    \\caption{Hypothetical example showing rules which activates a selected neuron. $\\operatorname{IoU}$ is how often the rule activated the neuron, compared to cases where either the rule is true or the neuron activated (higher is better).}\n    \\label{fig:rules:comp}\n\\end{figure}\nGiven a dataset $\\mathcal{D}$, a neuron activation $z_n(\\mathbf{x})$, a threshold $\\tau$, and a indicator function for the rule $R(\\mathbf{x})$, the the aggrement between the rule and the neuron activation can be measured with the \\emph{Intersection over Union score}:\n\\begin{equation}\n    \\operatorname{IoU}(n, R) = \\frac{\\sum_{x \\in \\mathcal{D}} \\mathds{1}(z_n(\\mathbf{x}) > \\tau \\land R(\\mathbf{x}))}{\\sum_{x \\in \\mathcal{D}} \\mathds{1}(z_n(\\mathbf{x}) > \\tau \\lor R(\\mathbf{x}))}\n\\end{equation}\nFor one particular neuron $n$, the combinatorial rule $R$ is then constructed using beam-search which stops at a pre-defined number of iterations. At each iteration, all feature indicator functions (e.g. word in $\\mathbf{x}$) and their negative, combined with the logical operators \\texttt{and} and \\texttt{or}, are scored using $\\operatorname{IoU}(n, R)$.\nUnfortunately,  do not perform any \\measure{groundedness} validation of this approach. Furthermore, as the method only looks at the relation between the input and the neuron, it is unclear how much the selected neuron affects the output.\n}\n\\end{document}\nThe main LaTeX file is `acm_main.tex`. Supplementary material is included after `\\appendix`.\nThere are no special requirements.", "cites": [8081], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic synthesis of the cited paper by summarizing the approach to compositional explanations of neurons. It includes some analytical elements, such as identifying a limitation (lack of groundedness validation and unclear output influence), but does not compare this method with others or offer a broader framework. The abstraction is limited to rephrasing the paper's core idea without deeper generalization."}}
