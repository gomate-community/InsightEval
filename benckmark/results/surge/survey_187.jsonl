{"id": "bc335212-f5ff-4437-83c6-3c59cb767e11", "title": "Technical Overview", "level": "section", "subsections": [], "parent_id": "ddb87c72-7cac-469b-a909-50c062d068f8", "prefix_titles": [["title", "Active Divergence with Generative Deep Learning - A Survey and Taxonomy"], ["section", "Technical Overview"]], "content": "While not all generative models rely on generative deep learning, we refer here to those that build on artificial neural networks\\footnote{For further reading, a comprehensive overview of generative models is given in .}. Given a data distribution $P$, a generative model will model an approximate distribution $P'$. The parameters for the approximate distribution can be learned by an artificial neural network. This learning task is tackled differently by different architectures and training schemes. E.g. autoencoders  and variational autoencoders (VAE)  learn to approximate the data through reconstruction via an encoding and a decoding network, while generative adversarial networks (GAN)  consists of a generator that is guided by a discriminating network. In most cases, the network learns a mapping from a lower-dimensional latent distribution $X$ to the complex high-dimensional feature space of a domain. The model, thus, generates a sample $p'$ given an input vector $x$ which should resemble samples drawn from the target distribution $P$. In the simplest case of a one layer network the generated sample $p'$ is generated using the function: $p' = \\sigma(Wx+b)$ where $x$ is the input vector from the latent distribution $x \\in X$, $\\sigma$ is a non-linear activation function, $W$ and $b$ are the learned association matrix and bias vector for generating samples in the approximate distribution $p' \\in P'$. The model parameters $W$ and $b$, are typically learned through gradient-based optimisation process. In this process, a loss function will require the model to maximise the likelihood of the data either: (i) explicitly, as in the case of autoencoders, autoregressive  and flow-based generative models ; (ii) approximately, as is the case in VAEs; (iii) or implicitly, as in the case of GANs. Generative models can also be conditioned on labelled data. In the conditional case, the generative model takes two inputs $x$ and $y$, where $y$ represents the class label vector. Another form of conditional generative models are translation models, such as pix2pix , that takes a (high dimensional) data distribution as input $Q$ and learns a mapping to $P'$ which is an approximation of the true target function $f: Q \\rightarrow P$.\nAll deep generative models, and in particular ones that generate high dimensional data domains like images, audio and natural language, will have some level of divergence $D(P||P') \\geq 0$ between the target distribution $P$ and the approximate distribution $P'$, because of the complexity and stochasticity inherent in high dimensional data. The goal of all generative models is to minimise that level of divergence, by maximising the likelihood of generating the given data domain. Active divergence methods however, intentionally seek to create a new distribution $U$ that does not directly approximate a given distribution $P$, or resemble any other known data distribution. This is either done by seeking to find model parameters $W^*$ and $b^*$ (in the single layer case) that generate novel samples $u = \\sigma(W^*x+b^*)$, or by making other kinds of interventions to the chain of computations.", "cites": [2074, 2073, 5680, 896], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a clear technical synthesis of key generative deep learning concepts and integrates ideas from the cited papers on VAEs, GANs, and conditional models. While it offers strong abstraction by framing the discussion around the broader concept of divergence and the idea of 'active divergence,' its critical evaluation is limited to stating that generative models aim to minimize divergence, without deeper critique or evaluation of the cited papers' limitations. Nonetheless, the analytical framing around the notion of active divergence contributes meaningful insight."}}
{"id": "2ce30e78-e621-45db-aba0-79e748cc1d11", "title": "Survey of Active Divergence Methods", "level": "section", "subsections": ["f5bf2625-3b81-4be3-b575-3a129bfe3a92", "dd7d6ead-1683-4cbe-9d71-ca9e62b207f6", "5d3b5f4f-27c6-494a-9c14-16eb127075f3", "40917d3d-0584-443b-9e57-47e7eaaf5e07", "aec51fc1-93a9-47a1-b427-d2e22ca8f3fd", "e3b5b7a0-32a1-4e8e-bb4d-7ba1fd144dfc", "2db92029-2be8-4356-96d3-d8577a343405", "b09b62ec-5a78-43e5-83f1-cea1c4a447f0"], "parent_id": "ddb87c72-7cac-469b-a909-50c062d068f8", "prefix_titles": [["title", "Active Divergence with Generative Deep Learning - A Survey and Taxonomy"], ["section", "Survey of Active Divergence Methods"]], "content": "We present a comprehensive overview and taxonomy of the state of the art in methods for achieving active divergence. In this survey, we will use the term divergence in the statistical sense, as being the distance (or difference) between two distributions. There are other definitions of divergence relevant to research in creativity, such as Guildford's dimensions of divergent thought . While there are some parallels that can be drawn between some of the active divergence methods, and theories of divergent thinking; for the clarity of technical exposition, we will be sticking strictly to the statistical definition of divergence in this overview of active divergence methods. \n\\begin{figure*}[tbp]\n   \\centering\n   \\begin{subfigure}[b]{0.245\\textwidth}\n     \\centering\n     \\includegraphics[width=\\textwidth]{images/strange_fruits.jpg}\n     \\caption{divergent fine-tuning }\n     \\label{fig:fruits}\n   \\end{subfigure}\n   \\hfill\n   \\begin{subfigure}[b]{0.245\\textwidth}\n     \\centering\n     \\includegraphics[width=\\textwidth]{images/derrick_chaining.png}\n     \\caption{chaining models}\n     \\label{fig:chaining}\n   \\end{subfigure}\n   \\begin{subfigure}[b]{0.245\\textwidth}\n     \\centering\n     \\includegraphics[width=\\textwidth]{images/teratome.png}\n     \\caption{network bending}\n     \\label{fig:teratome}\n   \\end{subfigure}\n   \\hfill\n   \\begin{subfigure}[b]{0.245\\textwidth}\n     \\centering\n     \\includegraphics[width=\\textwidth]{images/ukiyo-e-layerswap.png}\n     \\caption{network blending}\n     \\label{fig:layerswap}\n   \\end{subfigure}\n    \\caption{Some visual examples of results produced using various active divergence methods. (a) An image from \\textit{Strange Fruit} by Mal Som~\\protect, that was created by fine-tuning a pre-trained model towards a continously shifting domain. (b)~A frame from the video artwork \\textit{You Are Here} by Derrick Schultz~\\protect, created by chaining multiple models and technniques including: a custom GAN, network bending, image translation, and super-resolution. (c)~An image from the series of artworks \\textit{Teratome}~\\protect, that was created using network bending techniques~\\protect. (d)~An example of network blending~\\protect,  where the image provided has been generated from a model which combines the photorealistic textures from the FFHQ StyleGAN2 model, but the spatial structure from a model trained on an Ukiyo-e dataset~\\protect. All images are reproduced with permission from their respective creators.}\n    \\label{fig:three graphs}\n\\end{figure*}", "cites": [7538, 2075], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section introduces the concept of active divergence and includes a figure with examples from cited papers, but it lacks synthesis of ideas between the works. It does not compare or critically evaluate the methods, nor does it abstract broader principles or frameworks. The content is primarily descriptive, focusing on illustrating results rather than analyzing the underlying techniques."}}
{"id": "f5bf2625-3b81-4be3-b575-3a129bfe3a92", "title": "Novelty search over learned representations", "level": "subsection", "subsections": [], "parent_id": "2ce30e78-e621-45db-aba0-79e748cc1d11", "prefix_titles": [["title", "Active Divergence with Generative Deep Learning - A Survey and Taxonomy"], ["section", "Survey of Active Divergence Methods"], ["subsection", "Novelty search over learned representations"]], "content": "\\label{survey:noveltysearch}\nMethods in this category take existing generative models trained using standard maximum likelihood regimes and then specifically search for the subset of learned representations that do not resemble the training data by systematically sampling from the model\\footnote{An overview of methods for sampling generative models is given in .}. Taking account of the fact that any approximate distribution $P'$ will be somewhat divergent from the true distribution $P$, these methods seek to find the subset $U$ of the approximate distribution which is not contained in the true distribution $U \\subset P' \\wedge U \\not\\subset P$.  present an algorithm for searching for novelty in the latent space of a sparse autoencoder trained on the MNIST dataset . They start by creating a sample of random noise and by using a Markov chain monte carlo (MCMC) method of iteratively re-encoding the sample through the encoder, then refining the sample until it produces a stable representation. They use this approach to map out all the representations the model can generate, then perform k-means clustering on the latent space encoding of these representations. By disregarding clusters that correspond to real digits, they are left with clusters of representations of digits that do not exist in the original data distribution. It has been argued that these `spurious samples' are the inevitable outcome of generative models that learn to generalise from given data distributions  and that there is a trade off between the ability to generalise to every mode in the dataset and the ratio of spurious samples in the resulting distribution.", "cites": [2077, 2078, 2076], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes three papers to explain the concept of novelty search over learned representations, connecting the methodological approach to the broader theme of active divergence. It critically addresses the issue of spurious samples and discusses the trade-off between generalization and novelty, but does not deeply evaluate or contrast the methods' effectiveness. The section abstracts the idea of spurious samples as a general characteristic of generative models, providing a meta-level insight into the implications for creativity research."}}
{"id": "dd7d6ead-1683-4cbe-9d71-ca9e62b207f6", "title": "Novelty generation from an inspiring set", "level": "subsection", "subsections": [], "parent_id": "2ce30e78-e621-45db-aba0-79e748cc1d11", "prefix_titles": [["title", "Active Divergence with Generative Deep Learning - A Survey and Taxonomy"], ["section", "Survey of Active Divergence Methods"], ["subsection", "Novelty generation from an inspiring set"]], "content": "\\label{survey:noveltygeneration}\nThe methods in this section train a model from scratch using a training dataset, but do not attempt to model the data directly, rather using it as reference material to draw inspiration from. We therefore refer to this training set (the given distribution $P$) as the inspiring set .\nAn approach for novel glyph generation utilises a class-conditional generative model trained on the MNIST dataset , but in this case they train the model with `hold-out classes' , additional classes that do not exist in the training data distribution. These hold-out classes can then sampled during inference, which encapsulate the subset $U$ of the approximate distribution $P'$ that is not included in the target distribution $U \\subset P' \\wedge U \\not\\subset P$. These divergent samples can then be generated directly by conditioning the generator with the hold-out class label, without the need for searching the latent space. \nAn approach that directly generates a new distribution $U$ from an inspiring set $P$ is the creative adversarial networks (CAN) algorithm . The algorithm uses the WikiArt dataset , a labelled dataset of paintings classified by `style' (historical art movement). This algorithm draws inspiration from the GAN training procedure , but adapts it such that the discriminator has to classify real and generated samples by style, and the generator is then optimised to maximise the likelihood of the generated results being classified as `artworks' (samples that fit the training distribution of existing artworks) but maximise their deviation from existing styles in order to produce the novel distribution $U$.", "cites": [2079], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates two distinct methods (glyph generation with hold-out classes and Creative Adversarial Networks) under the common theme of using an inspiring set to generate novelty, which shows some synthesis. It abstracts the concept into a general framework of divergence from the target distribution to create a novel subset U. However, it lacks deeper critical analysis of the methods’ limitations or comparative evaluation, and the abstraction remains limited to terminology rather than revealing overarching principles."}}
{"id": "f053c992-4fd5-4458-a277-9cefb335e769", "title": "Multi-generator dynamics", "level": "subsubsection", "subsections": [], "parent_id": "5d3b5f4f-27c6-494a-9c14-16eb127075f3", "prefix_titles": [["title", "Active Divergence with Generative Deep Learning - A Survey and Taxonomy"], ["section", "Survey of Active Divergence Methods"], ["subsection", "Training without data"], ["subsubsection", "Multi-generator dynamics"]], "content": " present an approach to training generative deep learning models without any training data, by using two generator networks, and relying on the dynamics between them for an open-ended optimisation process. This approach took inspiration from the GAN framework, but instead of a generator mimicking real data, two generators attempt to mimic each other while the discriminator attempts to tell them apart. In order to have some level of diversity in the final results, the two generators are simultaneously trying to produce more colours in the generated output than the other generator network, leading to the generation of two novel, yet closely related distributions $U$ and $V$.", "cites": [2080], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the multi-generator dynamics approach from the cited paper, mentioning its inspiration from GANs and the competitive behavior between generators. However, it lacks deeper synthesis with other methods, critical evaluation of the approach's strengths and weaknesses, and generalization to broader principles or frameworks."}}
{"id": "935c14ce-fc34-4ee8-95c5-01e93f8fc69d", "title": "Generation via communication", "level": "subsubsection", "subsections": [], "parent_id": "5d3b5f4f-27c6-494a-9c14-16eb127075f3", "prefix_titles": [["title", "Active Divergence with Generative Deep Learning - A Survey and Taxonomy"], ["section", "Survey of Active Divergence Methods"], ["subsection", "Training without data"], ["subsubsection", "Generation via communication"]], "content": "An alternative approach to generating without data uses a single generator network, and uses the generated distribution $U$ as a channel for communication between two networks, which together learn to generate and classify images that represent numerical and textual information from a range of existing datasets . In subsequent work, by constraining the generator with a strong inductive bias for generating line drawings, this approach can be utilised for novel glyph generation .", "cites": [2081], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of a method for generation via communication, referencing one paper (Neural Glyph) but not integrating it with other approaches or broader themes. It lacks critical evaluation or comparison of the technique, and while it hints at general principles like communication as a medium, it does not abstract or synthesize these into a deeper framework or trend."}}
{"id": "22ad0c9c-45c1-48b2-966c-518d8d25e2d6", "title": "Loss hacking", "level": "subsubsection", "subsections": [], "parent_id": "40917d3d-0584-443b-9e57-47e7eaaf5e07", "prefix_titles": [["title", "Active Divergence with Generative Deep Learning - A Survey and Taxonomy"], ["section", "Survey of Active Divergence Methods"], ["subsection", "Divergent fine-tuning"], ["subsubsection", "Loss hacking"]], "content": "An alternative strategy, is to fine-tune a model without any training data. Instead a loss function is used that directly transforms the approximate distribution $P'$ into a novel distribution $U$ without requiring any other target distribution.  use the frozen weights of the discriminator to directly optimise away from the likelihood of the data, by using the inverse of the adversarial loss function. This process reverses the normal objective of the generator to generate `real' data and instead to generate samples that the discriminator deems to be `fake'. By applying this process to a GAN that can produce photo-realistic images of faces, this fine-tuning procedure crosses the uncanny valley in reverse, taking images indistinguishable from real images, and amplifying the uncanniness of the images before eventually leading to mode collapse. In a similar fashion to Som's practice (see previous sub-section), one instance of the model before mode collapse was hand-selected and a selection of its outputs turned into the series of artworks \\textit{Being Foiled} .", "cites": [2082], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the concept of loss hacking from the cited paper, integrating it with prior ideas like Som's practice to form a narrative about divergent fine-tuning. It provides some analytical depth by illustrating the effect of inverting the GAN objective, particularly the 'reverse uncanny valley' phenomenon. However, the critical evaluation of the method's limitations or broader implications is somewhat limited, and while it generalizes to the theme of active divergence, it does not fully abstract to a meta-level framework."}}
{"id": "63732ffc-87c7-4d2f-a22f-d0df42b7b343", "title": "Infusing external knowledge", "level": "subsubsection", "subsections": [], "parent_id": "40917d3d-0584-443b-9e57-47e7eaaf5e07", "prefix_titles": [["title", "Active Divergence with Generative Deep Learning - A Survey and Taxonomy"], ["section", "Survey of Active Divergence Methods"], ["subsection", "Divergent fine-tuning"], ["subsubsection", "Infusing external knowledge"]], "content": "By harnessing the learned knowledge of externally trained models, it is possible to fine-tune models to infuse that knowledge to transform the original domain data with characteristics defined using the auxiliary model.  utilise a classifier model $C_{classifier}$ trained to differentiate between datasets, in conjunction with the frozen weights of the discriminator $D_{frozen}$ to fine-tune a pre-trained GAN generator model $G$ away from the original distribution and towards a new local minimum defined by the loss function $L$. $L$ is defined as the weighted sum of the two auxiliary models $L = \\alpha C_{classifier}(G(x)) + \\beta D_{frozen}(G(x))$ given the random latent vector $x$, and $\\alpha$ and $\\beta$ being the hyper-parameters defining the weightings for the two components of the loss function. \nThe StyleGAN-NADA framework  takes advantage of the external knowledge of a contrastive language–image pre-training model (CLIP) . CLIP has been trained on billions of text and image pairs from the internet and provides a joint-embedding space of both images and text, allowing for similarity estimation of images and text prompts. In StyleGAN-NADA, pretrained StyleGAN2 models  can be fine-tuned using user-specified text prompts, the CLIP model $C_{clip}$ is then used to encode the text prompts and the generated samples in order to provide a loss function where the cosine similarity $S$ between the clip encodings of the text string $t$ and the generated image embedding $G(x)$ given random latent $x$, can be minimised using the loss $L = S(C_{clip}(t), C_{clip}(G(x))$. This training procedure, guides the generator towards infusing characteristics from an unseen domain defined by the user as text prompts.", "cites": [7539, 1639], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes two distinct methods for infusing external knowledge into GANs—using a cross-dataset classifier and a CLIP-based approach—by highlighting their shared goal of guiding generators toward novel distributions. It provides a critical perspective by emphasizing the limitations of pre-trained GANs in creative divergence and how these methods address them. Some level of abstraction is achieved by framing both techniques under the concept of using external knowledge for domain transformation, though deeper generalization is limited."}}
{"id": "aec51fc1-93a9-47a1-b427-d2e22ca8f3fd", "title": "Chaining models", "level": "subsection", "subsections": [], "parent_id": "2ce30e78-e621-45db-aba0-79e748cc1d11", "prefix_titles": [["title", "Active Divergence with Generative Deep Learning - A Survey and Taxonomy"], ["section", "Survey of Active Divergence Methods"], ["subsection", "Chaining models"]], "content": "\\label{survey:chaining}\nAn approach that is widely used by artists who incorporate generative models into their practice, but not well documented in academic literature, is the practice of chaining multiple custom models trained on datasets curated by the artists. The ensembles used will often utilise standard unconditional generative models, such as GANs, in combination with other conditional generative models such as image-to-image translation networks, such as pix2pix  and CycleGAN , along with other approaches for altering the aesthetic outcomes of results such as style transfer . Artists will often train many models on small custom datasets and test out many combinations of different models, with the aim of finding a configuration that produces unique and expressive results. The artist Helena Sarin will often chain multiple CycleGAN models into one ensemble, and will reuse training data during inference, as the goal of this practice ``is not generalization, my goal is to create appealing art'' . The artist Derrick Schultz draws parallels between the practice of chaining models and Robin Sloan's concept of `flip-flopping' , where creative outcomes can be achieved by ``pushing a work of art or craft from the physical world to the digital world and back, often more than once'' .", "cites": [2083, 7022, 896], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the chaining models approach, referencing several papers and artists' practices. It integrates limited information from the cited works to explain the technique, but does not deeply synthesize or connect broader ideas across the sources. There is minimal critical analysis or abstraction into general principles, focusing instead on examples and anecdotal usage by artists."}}
{"id": "e3b5b7a0-32a1-4e8e-bb4d-7ba1fd144dfc", "title": "Network bending", "level": "subsection", "subsections": [], "parent_id": "2ce30e78-e621-45db-aba0-79e748cc1d11", "prefix_titles": [["title", "Active Divergence with Generative Deep Learning - A Survey and Taxonomy"], ["section", "Survey of Active Divergence Methods"], ["subsection", "Network bending"]], "content": "\\label{survey:bending}\nNetwork bending  is a framework that allows for active divergence using individual pre-trained models without making any changes to the weights or topology of the model. Instead, additional layers that implement standard image filters are inserted into the computational graph of a model and applied during inference to the activation maps of the convolutional features\\footnote{Inserting filters into GANs was also developed independently in the Matlab StyleGAN playground .}. As the computational graph of the model has been altered, the model which previously generated samples from the approximate distribution $P'$, now produces novel samples from the new distribution $U$, without any changes being made to the parameters of the model. In the simplest case of a two layer model an association weight matrix $W_l$ and bias $b_l$ vector for each layer $l$. Which generates sample $p'=\\sigma(W_2(\\sigma(W_1x+b_1))+b_2)$ from input vector $x$ and using a non-linear activation function $\\sigma$. In the network bending framework, a deterministic function $f$ (controlled by the parameter $y$) is inserted into the computational graph of the model and applied to the internal activations of the model $u=\\sigma(W_2(f(\\sigma(W_1x+b_1),y))+b_2)$, allowing the model to produce new samples $u$ from the new distribution $u \\in U$. Beyond the simplest case of a transformation being applied to all features in a layer, the transformation layer can also be applied to a random sub-section of features, or to a pre-selected set of features.  present a clustering algorithm, that in an unsupervised fashion, groups together sets of features within a layer based on the spatial similarity of their activation maps. This clustering algorithm is capable of finding sets of features responsible for the generation of various semantically meaningful components of the generated output across the network (and semantic) hierarchy, which can then be manipulated in tandem allowing for semantic manipulation of the internal representations of the generative model. \nIn addition to applying filters to the activation maps, it is also possible to enlarge samples by increasing the size of the activation maps and interpolating and tiling them . The network bending framework has been extended into the domain of audio synthesis  where it has been applied to neural vocoder models using the differential digital signal processing (DDSP) approach . In order to adapt the framework for the audio domain,  implement a number of filters that operate in the time domain, such as oscillators. Network bending has also been applied in the domain of audio-reactive visual synthesis using generative models , with the deterministic transformations being controlled automatically using features extracted from audio analysis.", "cites": [7540, 2075], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the concept of network bending by integrating the core idea from Paper 2 with extensions into audio synthesis and visual generation, as well as the DDSP approach from Paper 1. It provides a conceptual abstraction by framing network bending as a general framework for altering generative models through deterministic transformations. However, it lacks deeper critical analysis of the methods’ limitations or trade-offs and primarily focuses on describing how the techniques work rather than evaluating them rigorously."}}
{"id": "b0c95209-f9c8-4549-9d87-e9ee0a7855c7", "title": "Blending model predictions", "level": "subsubsection", "subsections": [], "parent_id": "2db92029-2be8-4356-96d3-d8577a343405", "prefix_titles": [["title", "Active Divergence with Generative Deep Learning - A Survey and Taxonomy"], ["section", "Survey of Active Divergence Methods"], ["subsection", "Network blending"], ["subsubsection", "Blending model predictions"]], "content": " present an interactive tool for text generation allowing for the realtime blending of the predicted outputs of an ensemble of long-short term memory network (LSTM) models  trained to perform next character prediction from different text sources. A graphical user interface allows the user to dynamically shift the mixture weights for the weighted sum for the predictions of all of the models in the ensemble, prior to the one hot vector encoding which is used to determine the final predicted character value.", "cites": [2084], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual summary of a specific method for blending model predictions in an ensemble of LSTM models. It integrates the cited paper to describe the technique but lacks deeper connections to other works or broader conceptual synthesis. There is no critical evaluation or abstraction to identify overarching principles or patterns in active divergence methods."}}
{"id": "02c93de7-af23-40ee-ab7c-c1cf7db29097", "title": "Blending model parameters", "level": "subsubsection", "subsections": [], "parent_id": "2db92029-2be8-4356-96d3-d8577a343405", "prefix_titles": [["title", "Active Divergence with Generative Deep Learning - A Survey and Taxonomy"], ["section", "Survey of Active Divergence Methods"], ["subsection", "Network blending"], ["subsubsection", "Blending model parameters"]], "content": "A number of approaches, all demonstrated with StyleGAN2 , take advantage of the large number of pre-trained models that have been shared on the internet . Of these almost all have been transfer-learned from the official model weights trained on the Flickr-Faces High Quality (FFHQ) dataset. It has been shown that the parameters of models transfer-learned $p_{transfer}$ from the same original source $p_{base}$ share commonalities in the way their weights are structured. This makes it possible to meaningfully interpolate between the parameters of the models directly . By using an interpolation weighting $\\alpha$, it is possible to control the interpolation for the creation of a set of parameters $p_{interp} = (1 - \\alpha)p_{base} + \\alpha p_{transfer}$. \nLayers can also be swapped from one model to another , allowing the combination of higher level features of one model with lower level features of another. This layer swapping technique was used to make the popular `toonification' method, which can be used to find the corresponding sample to a real photograph of a person in a Disney-Pixar-esque `toonified' model, simply by sampling from the same latent vector that has been found as the closest match to the person in FFHQ latent space . A generalised approach that combines both weight interpolation and layer-swapping methods for multiple models, uses a cascade of different weightings of interpolation for the various layers of the model .\n presents an evolutionary approach for exploring and finding effective and customisable neural style transfer blends. Upwards of 1000 neural style transfer models trained on 1-10 style images each, can be blended through model interpolation, using an interface that is controlled by the user. MAP-Elites  in combination with a fitness function calculated using the output from a ResNet model  were used in evolutionary searches for optimal neural style transfer blends.", "cites": [97, 2085, 7541, 7538], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple active divergence techniques (e.g., weight interpolation, layer swapping) using the cited papers to explain how they can be combined and applied in creative workflows. It moves beyond mere description by discussing principles such as parameter structure and controllability. However, while it integrates methods well, it lacks deeper critical evaluation of their limitations or comparative analysis with alternative strategies."}}
{"id": "a3e64f89-2b7c-4ab3-81ce-4d1b943c5976", "title": "Stochastic rewriting", "level": "subsubsection", "subsections": [], "parent_id": "b09b62ec-5a78-43e5-83f1-cea1c4a447f0", "prefix_titles": [["title", "Active Divergence with Generative Deep Learning - A Survey and Taxonomy"], ["section", "Survey of Active Divergence Methods"], ["subsection", "Model rewriting"], ["subsubsection", "Stochastic rewriting"]], "content": "To create the series of artworks \\textit{Neural Glitch} the artist Mario Klingemann randomly altered, deleteed or exchanged the trained weights of pre-trained GANs . In a similar fashion, the convolutional layer reconnection technique  randomly swaps convolutional features within layers of pre-trained GANs. This technique is applied in the \\textit{Remixing AIs} audiovisual synthesis framework .", "cites": [7538], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of two stochastic rewriting methods used in creative applications but does not effectively synthesize or integrate information from the cited paper. It lacks critical evaluation of the methods or their limitations and does not abstract beyond specific examples to broader principles or trends in active divergence."}}
{"id": "66fe9891-3851-47fd-9457-4ae3269819ed", "title": "Targeted rewriting", "level": "subsubsection", "subsections": [], "parent_id": "b09b62ec-5a78-43e5-83f1-cea1c4a447f0", "prefix_titles": [["title", "Active Divergence with Generative Deep Learning - A Survey and Taxonomy"], ["section", "Survey of Active Divergence Methods"], ["subsection", "Model rewriting"], ["subsubsection", "Targeted rewriting"]], "content": " present a targeted approach to model rewriting. Here, a sample is taken from the model and manipulated using standard image editing techniques (referred to as a `copy-paste' interface). Once the sample has been altered corresponding to the desired goal (such as removing watermarks from the image, or getting horses to wear hats), a process of constrained optimisation is performed. All of the layers but one are frozen, and the weights of that layer are updated using gradient descent optimisation until the generated sample matches the new target. After this optimisation process is complete, the weights of the model are modified such that the targeted change becomes present in all the samples that the model generates.\nThe CombiNets framework , informed by prior reseach in combinational creativity , can be utilised to create a new model by combining parameters from a number of pre-trained models in a targeted fashion. The parameters of existing models are recombined to take into account a new mode of generation that was not present in the training data (an example given would be a unicorn for a model trained on photographs of non-mythical beings). In this framework, a small number of new samples is provided (not enough to train a model directly) and then heuristic search is used to recombine parameters from existing models to account for this new mode of generation.", "cites": [2086, 2087], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes two distinct targeted rewriting methods—layer-based optimization and CombiNets—into a coherent narrative, linking them under the broader theme of active divergence. It abstracts the general idea of modifying generative models to introduce novel or desired features not present in their training data. However, it provides limited critical analysis, such as comparing the strengths and weaknesses of the methods or discussing their limitations in broader application."}}
{"id": "92769f05-2a4f-4aec-bc7b-77a555aed688", "title": "Human direction vs. creative autonomy", "level": "subsection", "subsections": [], "parent_id": "fe3a8401-2e2b-4b9e-940a-5ae6a684efc1", "prefix_titles": [["title", "Active Divergence with Generative Deep Learning - A Survey and Taxonomy"], ["section", "Further Demarcations"], ["subsection", "Human direction vs. creative autonomy"]], "content": "Very few of the approaches described have been developed with the expressed intention of handing over creative agency to the systems themselves. Most of the methods have been developed by artists or researchers in order to allow people to manipulate, experiment with and explore the unintended uses of these models for creative expression. However, the methods described that are currently designed for, or rely on a high degree of human curation and intervention, could easily be adapted and used in co-creative or autonomous creative systems in the future .", "cites": [7542], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a basic analytical overview of the role of human direction versus creative autonomy in active divergence techniques. It synthesizes information from the cited paper to highlight the current reliance on human curation, but does not deeply connect or integrate multiple sources. It offers a hint of abstraction by suggesting future adaptation for co-creative or autonomous systems, but lacks critical evaluation of limitations or a more nuanced discussion of broader implications."}}
{"id": "629b40d2-a003-403f-924c-bf370e6a847a", "title": "Knowledge recombination", "level": "subsection", "subsections": [], "parent_id": "2fc2927e-faf2-4f49-9dd7-3abf8f58115c", "prefix_titles": [["title", "Active Divergence with Generative Deep Learning - A Survey and Taxonomy"], ["section", "Applications of Active Divergence"], ["subsection", "Knowledge recombination"]], "content": "Reusing and recombining knowledge in efficient ways is an important use-case of active divergence methods. While impressive generalisation can be ascertained from extremely large models trained on corpuses extracted from large portions of the internet , this is out of the capabilities for all but a handful of large tech companies. Instead of relying on ever expanding computational resources, active divergence methods allow for the recombination of styles, aesthetic characteristics and higher level concepts in a much more efficient fashion. Methods like chaining models, network blending and model rewriting offer alternatives routes to achieving flexible knowledge recombination and generalisation to unseen domains without the need for extremely large models or data sources.", "cites": [7339], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a descriptive overview of active divergence methods in the context of knowledge recombination, mentioning techniques like chaining models, network blending, and model rewriting. However, it lacks substantial synthesis across the cited papers, does not critically analyze the methods or their limitations, and only offers minimal abstraction by briefly pointing to efficiency and generalization as broader themes."}}
{"id": "98017b68-705d-452c-962d-6720349784c2", "title": "Unseen domain adaptation", "level": "subsection", "subsections": [], "parent_id": "2fc2927e-faf2-4f49-9dd7-3abf8f58115c", "prefix_titles": [["title", "Active Divergence with Generative Deep Learning - A Survey and Taxonomy"], ["section", "Applications of Active Divergence"], ["subsection", "Unseen domain adaptation"]], "content": "Active divergence methods allow for the possibility of adapting to and exploring unseen domains, for which there is little to no data available. The network blending approach presented by  can be used for the translation of faces while maintaining recognisable identity into a completely synthesised data domain, something which would not be possible with standard techniques for image translation .\nThe model rewriting and network bending approaches offer the possibility of reusing and manipulating existing knowledge in a controlled fashion to create new data from a small number of given examples, or theoretically without any prior examples if external knowledge sources are integrated, as discussed further below. This approach could also be utilised by agents looking to explore hypothetical situations, by reorganising learned knowledge from world models  to explore hypothetical situations or relations.", "cites": [7538, 2088, 7022], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates the cited papers by linking active divergence techniques to the concept of unseen domain adaptation, suggesting their use for creative tasks beyond traditional image translation. It moves beyond mere description by highlighting the limitations of standard methods and the potential of model rewriting and network bending. However, it does not deeply critique the works or offer a novel framework, and its abstraction remains at a moderate level by identifying general possibilities rather than overarching principles."}}
{"id": "1bcf6437-e657-41ea-8d22-4d593863b785", "title": "Multi-agent systems", "level": "subsection", "subsections": [], "parent_id": "8aefc253-e493-4118-be1f-693be9ecbfb2", "prefix_titles": [["title", "Active Divergence with Generative Deep Learning - A Survey and Taxonomy"], ["section", "Future Research Directions"], ["subsection", "Multi-agent systems"]], "content": "It has been argued the the GAN framework is the simplest example of a multi-agent system , and frameworks such as neural cellular automata  offer new possibilities for multi-agent approaches in generative deep learning. The active divergence methods for training without data described in this paper all rely on the dynamics of multiple agents to produce interesting results, but this could be taken much further. It has been argued that art is fundamentally social  and exploring more complex social dynamics between agents  could be a fruitful avenue for exploration in the development of these approaches. There is a large body of work in emergent languages from co-operative multi-agent systems  that could be drawn from in furthering the work in generative multi-agent systems.", "cites": [7083], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section briefly synthesizes a few key ideas around multi-agent systems and generative models, drawing from the cited paper on emergent language. However, it lacks deeper integration or a novel framework. The critical perspective is limited, as it only hints at limitations and potential without detailed evaluation. It offers some abstraction by connecting multi-agent cooperation to broader themes in computational creativity and social dynamics, but this remains at a surface level."}}
