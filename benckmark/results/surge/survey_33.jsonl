{"id": "8c609ab9-3acf-4c8d-b01b-166e324545d7", "title": "Introduction", "level": "section", "subsections": ["cc6aa21e-587a-4a81-a995-223c279c1b29", "fb25d6ba-d2de-4c33-9cb4-b6d70b6dfd2b"], "parent_id": "e196ef7e-046c-453b-afc3-ad6e4e23314a", "prefix_titles": [["title", "Survey on reinforcement learning for language processing"], ["section", "Introduction"]], "content": "\\label{intro}\nMachine learning algorithms have been very successful to solve problems in the natural language processing (NLP) domain for many years, especially supervised and unsupervised methods. However, this is not the case with reinforcement learning (RL), which is somewhat surprising since in other domains, reinforcement learning methods have experienced an increased level of success with some impressive results, for instance in board games such as AlphaGo Zero . Yet, deep reinforcement learning for natural language processing is still in its infancy when compared to supervised learning . Thus, the main goal of this article is to provide a review of applications of reinforcement learning to NLP. Moreover, we present an analysis of the underlying structure of the problems that make them viable to be treated entirely or partially as RL problems, intended as an aid to newcomers to the field. We also analyze some existing research gaps and provide a list of promising research directions in which natural language systems might benefit from reinforcement learning algorithms.", "cites": [166], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 2.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The introduction provides a general overview of the use of reinforcement learning in NLP and mentions a single cited paper (Deep Learning) briefly. It lacks synthesis of multiple sources, critical evaluation of the cited work, and abstraction to broader principles. While it identifies a research gap and hints at potential future directions, these insights are not deeply grounded in the cited literature."}}
{"id": "cc6aa21e-587a-4a81-a995-223c279c1b29", "title": "Reinforcement learning", "level": "subsection", "subsections": [], "parent_id": "8c609ab9-3acf-4c8d-b01b-166e324545d7", "prefix_titles": [["title", "Survey on reinforcement learning for language processing"], ["section", "Introduction"], ["subsection", "Reinforcement learning"]], "content": "\\label{sec:1}\nReinforcement learning is a term commonly used to refer to a family of algorithms designed to solve problems in which a sequence of decisions is needed. Reinforcement learning has also been defined more as a kind of learning problem than as a group of algorithms used to solve such problems . It is important to mention that reinforcement learning is a very different kind of learning than the ones studied in supervised and unsupervised methods. This kind of learning requires the learning system, also known as agent, to discover by itself through the interaction with its environment, which sequence of actions is the best to accomplish its goal. \nThere are three major groups of reinforcement methods, namely, dynamic programming, Monte Carlo methods, and temporal difference methods. Dynamic programming methods estimate state or state-action values by making estimates from other estimates. This iteratively  intertwines policy evaluation and policy improvement updates taking advantage of a model of the environment which is used to calculate rewards. Policy evaluation consists of updating the current version of the value function based on the current policy. Policy improvement consists of greedifying the policy function based on the current value function. Depending on the algorithm and its implementation it might require exhaustive sweeping of the entire state space or not. Monte Carlo methods learn from complete sample returns, instead of immediate rewards. Unlike dynamic programming, Monte Carlo methods only consider one transition path at a time, the path generated with a sample. In other words, they do not bootstrap from successor statesâ€™ values. Therefore, these kinds of methods are more useful when we do not have a model of the environment, the so-called dynamics of the environment. Temporal difference methods do not need a model of the environment since they can learn from experience, which can be generated from interactions with the environment. These methods possess the best of dynamic programming and the best of Monte Carlo. From dynamic programming they inherit the bootstrapping, from Monte Carlo methods they inherit the sampling. As a result of this combination of characteristics, temporal difference methods have been the most widely used. All these methods pose the decision-making problem as a Markov decision process (MDP). An MDP is a mathematical method used to solve decision-making in sequence and considers as the minimum existing elements a set of states $S$, a set of actions $A$, a transition function $T$, and a reward function $R$. Given an MDP $(S, A, T, R)$, we need to find an optimal policy function $\\pi$, which represents the solution of our sequence decision problem. The aim of a reinforcement learning system, or so-called agent, is to maximize some cumulative reward  $r \\in R$ through a sequence of actions. Each pair of state $s$ and action $a$ creates a transition tuple $(s,a,r,s')$, with $s'$ being the resulting state. Depending on the algorithm being used and on the particular settings of our problem, the policy $\\pi$ will be estimated differently.\nA policy $\\pi$ defines the behavior of the agent at any given moment. In other words, a policy is a mapping from the set of states $S$ perceived from the environment to a set of actions $A$ that should be executed in those states. In some cases, the policy can be stored as a lookup table, and in other cases it is stored as a function approximator, such as a neural network. The latter is imperative when we have a large number of states. The policy is the most important mathematical function learned by the reinforcement learning agent, in the sense that it is all the agent needs to control its behavior once the learning process has concluded. In general, a policy can be stochastic and we formally define it as $\\pi: S \\rightarrow A$.\nThe goal in a reinforcement learning problem is specified by the reward function $R$. This function maps each state or pair of state-action perceived in the environment to one real number $r \\in \\Re$ called reward. This reward indicates how good or bad a given state is. As we mentioned before, the goal of an agent is to maximize the total amount of rewards that it gets in the long run, during its interaction with the environment. The reward function cannot be modified by the agent, however, it can serve as a basis for modifying the policy function. For example, if the action selected by the current policy is followed by a low reward, then the policy can be updated in such a way that in the future it indicates a different action when the agent encounters the same situation. In general, the reward function can also be a stochastic function and it is formally defined as $R: S \\rightarrow \\Re$, or $R: S \\times A \\rightarrow \\Re$.\nA value function indicates which actions are good in the long run. The value of a state is basically an estimation of the total amount of rewards that the agent can expect to accumulate in the future, if it starts its path from that state using its current policy. We should not confuse the value function with the reward function. The rewards are given directly by the environment while the values of the states are estimated by the agent, from its interaction with the environment. Many reinforcement learning methods estimate the policy function from the value function. When the value function is a mapping from states to real numbers, it is denoted by the letter $V$. When the mapping is from pairs of state-action to real numbers, it is denoted by $Q$. Formally, we can define the value function as $V: S \\rightarrow \\Re$ or $Q: S \\times A\\rightarrow \\Re$.\nIn the case of model-based RL, the agent also has access to a model of the transition function $T$ of the environment, which may be learnt from experience.\nFor example, given a state and an action, the model could predict the next resulting state and reward. Such world models are used for planning, this is, a way to make decisions about the next actions to be performed, without the need to experience possible situations. In the case of model-free RL, when a model of the environment is missing, we have to solve the reinforcement learning problem without planning and that means that a significant amount of experimentation with the environment will be needed.\nOne of the most popular reinforcement learning algorithms is the Q-learning algorithm . As its name suggests, it works by estimating a state-action value function $Q$. The algorithm does not rely on a model of the transition function $T$, and therefore it has to interact with the environment iteratively. It follows one policy function for exploring the environment and a second greedy policy for updating its estimations of the values of pairs of states and actions that it happens to visit during the learning process. This kind of learning is called off-policy learning. The algorithm uses the following rule for updating the $Q$ values:\n\\[Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\max_{a'} Q(s',a') -  Q(s,a)].\\]\nIn this learning rule, $\\alpha$ is a parameter defined experimentally and it is known as the learning rate. It takes values in the interval $(0, 1)$. Moreover, $r$ is the reward signal, $\\gamma$ is known as the discount parameter and it also takes values in the interval $(0,1)$, and finally $s'$ and $a'$ denote the next state and the next action to be visited and executed during the next interaction with the environment. \nSARSA is an on-policy learning algorithm, meaning that instead of using two policies, one for behavior and one for learning, this algorithm uses only one policy. The same policy that is used to explore the environment is the same policy used in the update rule . The update rule of the SARSA is the following:\n\\[Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma Q(s',a') - Q(s,a)].\\]\nA very important result in recent years was the development of the deep Q-network , in which a convolutional neural network is trained with a variant of Q-learning. This algorithm, originally designed to learn to play several Atari 2600 games at a superhuman level, is now being applied to other learning tasks. Another algorithm, AlphaGo Zero ,  learned to play Go and actually defeated the human world champion in 2016. This algorithm uses a deep neural network, a search algorithm and reinforcement learning rules. The successor model MuZero  learns a representation of state, a dynamics and a reward prediction function to maximize future rewards via tree search-based planning, achieving more successful game play without prior knowledge of the game rules. \nDeep reinforcement learning is an extension of the classical reinforcement learning methods to leverage the representational power of deep models. More specifically, deep neural networks allow reinforcement learning algorithms to approximate and store highly complex value functions, state-action functions, or policy functions. For instance, a $Q(s,a)$ function can be represented as a convolutional neural network or a recurrent one. Similarly to what happened in other domains such as computer vision, deep models are also playing a decisive role in the advancement of reinforcement learning research, especially in MDPs with very large state and action spaces. In fact, reinforcement learning and deep neural networks have stayed recently at the center of attention of many researchers who have studied and applied them to solve different problems, including problems in natural language processing, as we will discuss below.", "cites": [883], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of reinforcement learning concepts, such as MDPs, policy, reward, and value functions, and introduces Q-learning and SARSA with their respective update rules. However, it lacks synthesis of the cited papers, offering no meaningful connections between them or deeper integration of their contributions. There is minimal critical analysis or abstraction, with the content remaining primarily at the level of definition and explanation rather than identifying broader patterns or evaluating the strengths and weaknesses of the approaches."}}
{"id": "fb25d6ba-d2de-4c33-9cb4-b6d70b6dfd2b", "title": "Natural language processing and RL", "level": "subsection", "subsections": [], "parent_id": "8c609ab9-3acf-4c8d-b01b-166e324545d7", "prefix_titles": [["title", "Survey on reinforcement learning for language processing"], ["section", "Introduction"], ["subsection", "Natural language processing and RL"]], "content": "\\label{sec:2}\nIn natural language processing, one of the main goals is the development of computer programs capable of communicating with humans through the use of natural language. In some applications, such as machine translation, these programs are used to help humans who speak different languages to understand each other by translating from one natural language to another. Through the years, NLP research has gone from being heavily influenced by theories of linguistics, such as those proposed by Noam Chomsky~, to the corpus linguistics approach of machine learning algorithms and more recently the use of deep neural networks as neural language models such as BERT  and GPT-3 . \nAccording to Russell and Norvig , to the contrary of formal languages, it is more fruitful to define natural language models as probability distributions over sentences rather than using definitive sets specified by grammars. The main challenges when dealing with natural languages are that they are ambiguous, large and constantly changing. That is why initial approaches to model natural languages using grammars were not as successful as modern machine learning approaches. In the former approaches, the grammars needed to be adapted and their size increased to fulfil the demands for better performance.\nOne important probabilistic approach to modelling natural languages involves the use of $n$-grams. A sequence of written symbols of length $n$ is called an $n$-gram. A model of the probability distribution of strings containing $n$ symbols is therefore called an $n$-gram model. This model is defined as a Markov chain of order $n-1$ in which the probability of some symbol $s_i$ depends only on the immediately preceding $n-1$ symbols. Formally, we say $p(s_i | s_{i-1}, s_{i-2}, \\ldots, s_{2}, s_{1}) =  p(s_i | s_{i-1}, \\ldots, s_{i-n+1})$. Probabilistic natural language models based on $n$-grams can be useful for text classification tasks .\n \\begin{figure}[ht]\n \\centering\n \\includegraphics[width=11cm]{RL_NLP_generic_figure}\n \\caption{Schematic view of a reinforcement learning agent designed for language processing. The language model agent acts by appending, replacing or deleting strings of words. States are strings of words. The language processing environment will provide the agent with the states and rewards after each of the interactions. The reward function is determined by the specific natural language processing task. One simple possibility for a reward function would reinforce every optimal action with a +1.}\n \\label{rl_for_nlp}\n \\end{figure}\nImportant advances in the design of algorithms for training deep neural networks, such as the recurrent long short-term memory (LSTM) network , have allowed researchers to move from probabilistic language models to language models based on neural networks. The LSTM neural model has been successfully applied to machine translation. The performance of current translator programs could not be accomplished using the approach based on language grammars alone. These new neural models are highly complex mathematical functions with thousands of parameters which are estimated iteratively from a massive number of training examples gathered from the Internet.\nSome problems in natural language processing can be defined as Markov decision processes and therefore they can be solved using reinforcement learning algorithms. In Fig.\\ \\ref{rl_for_nlp}, we provide a schematic example of how a reinforcement learning agent would be designed to solve a language processing task in which states, actions and rewards operate mainly over strings. A set of basic operations may include appending, replacing and deleting words.\nIn this article, we review five main categories of such problems, namely,  syntactic parsing, language understanding, text generation systems, machine translation, and conversational systems. Of these, conversational systems are the most studied ones, which involve finding an optimal dialog policy that should be followed by an automated system during a conversation with a human user. The other four categories are not widely known applications of reinforcement learning methods and therefore it is interesting to discuss their main benefits and drawbacks. In some of them, it is even not easy to identify the elements of a well-defined Markov decision process. This might explain why they have not received more attention yet. Identifying these different natural language processing problems is important to discover new research lines at the intersection of natural language processing and reinforcement learning.\nIn the next sections, we describe with more detail these five categories of natural language processing problems and their proposed solutions by means of reinforcement learning. We also discuss the main achievements and core challenges on each of these categories.", "cites": [7, 679], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of NLP and its evolution, briefly mentions cited papers (BERT and GPT-3) in a general context, but does not synthesize their specific contributions or connect them meaningfully to RL. It lacks critical evaluation of the cited works and only offers minimal abstraction by noting that NLP problems can be modeled as Markov decision processes. The narrative is primarily factual with limited insight."}}
{"id": "02101af9-6553-447b-ad16-10886334bb21", "title": "Syntactic parsing", "level": "section", "subsections": [], "parent_id": "e196ef7e-046c-453b-afc3-ad6e4e23314a", "prefix_titles": [["title", "Survey on reinforcement learning for language processing"], ["section", "Syntactic parsing"]], "content": "\\label{parsing}\nSyntactic parsing consists of analyzing a string made of symbols belonging to some alphabet, either in natural languages or in programming languages. Such analysis is often performed according to a set of rules called grammar. There could be many ways to perform parsing, depending on the final goal of the system . One of such goals could be the construction of a compiler for a new programming language when we are working with formal computer languages. Another one could be an application of language understanding for human-computer interaction.\nA grammar can generate many parsing trees and each of these trees specifies the valid structure for sentences of the corresponding language. Since parsing can be represented as a sequential search problem with a parse tree as the final goal state, reinforcement learning methods are tools very well suited for the underlying sequential decision problem. In general, a parse is obtained as a path when an optimal policy is used, in a given MDP.\nConsider for example the simple context-free grammar $G_1$ and the language $L(G_1)$ generated by it. $G_1$ is a 4-tuple $(V, \\Sigma, R, S)$ where\n\\begin{itemize}\n \\item $V = \\{ A, B\\}$ is a finite set of variables,\n \\item $\\Sigma = \\{0,1,\\#\\}$ is a finite set, disjoint of $V$, containing terminal symbols,\n\\item $R$ is the finite set of 4 production rules given in Fig.\\ \\ref{grammar01}, and\n\\item $S \\in V$ is the initial variable.\n\\end{itemize}\n\\begin{figure}[h!]\n \\centering\n\\begin{align}\n S &\\rightarrow 0A1 \\\\\n A &\\rightarrow 0A1 \\; | \\;  B\\\\\n B &\\rightarrow \\#\n\\end{align}\n\\caption{Grammar $G_1$ with 4 production rules.}\n\\label{grammar01}\n\\end{figure}\nThe language $L(G_1)$ generated by grammar $G_1$ is an infinite set of strings. Each of these strings is created by starting with the initial variable $S$ and iteratively selecting and applying one of the production rules in $G_1$, also called substitution rules. For example, the string $0\\#1$ is a valid string belonging to $L(G_1)$ and it can be generated by applying the following sequence of production rules $S \\rightarrow 0A1$, $A \\rightarrow B$ and $B \\rightarrow \\#$. Looking at this application of rules as a path of string substitutions, we have $S \\Rightarrow 0A1 \\Rightarrow 0B1 \\Rightarrow 0\\#1$. A path of substitutions, known also as derivation, can be represented pictorially as a parse tree. For example, the parse tree for the derivation of the string $00\\#11$ is illustrated in Fig.\\ \\ref{ptree01}.\n\\begin{figure}[ht]\n \\centering\n \\includegraphics[width=4cm]{ptree01}\n \\caption{Parse tree of string $00\\#11$ generated from grammar $G_1$.}\n \\label{ptree01}\n \\end{figure}\n From the previous grammar example $G_1$ we can notice the similarity between the elements defined in a context-free grammar $ G = \\{V, \\Sigma, P, S\\}$ and the elements defined in a Markov decision process $M = \\{S,A,T,R\\}$. Let us now analyze this similarity, element by element, from the point of view of an MDP. \n \\begin{itemize}\n\\item The starting state $s$ of an MDP $M$ can be defined as the initial variable of a grammar, denoted by letter $S$ in grammar $G$.\n\\item The set of states $S$ in the MDP $M$ can be defined as the set of strings generated by the grammar, in other words, the language generated by grammar $G$, this is $S = L(G)$.\n\\item The set of actions $A$ can be defined as the set of production rules given by grammar $G$, this is $A=R$; the MDP transition function $T$ would be immediately defined once we have defined the set of production rules itself.\n\\item The reward function $R$ is the only element that cannot be taken straightforward from the elements of the grammar and it should be crafted by the designer of the system.\n \\end{itemize}\nIn the specific application of dependency parsing , it has been shown that a parser can be implemented to use a policy learned by reinforcement learning, in order to select the optimal transition in each parsing stage . Given a sentence with $n$ words $x = w_1 w_2 \\ldots w_n$, we can construct its dependency tree by selecting a sequence of transitions. A stack data structure is used to store partially processed words and also a queue data structure is used to record the remaining input words together with the partially labeled dependency structure constructed by the previous transitions. The construction of the dependency tree is started with an empty stack and the input words being fed into the queue. The algorithm performs 4 different types of transitions until the queue is empty. These 4 transitions are: \\textit{reduce}, which takes one word from the stack; \\textit{shift}, which pushes the next input word into the stack; \\textit{left-arc}, which adds a labeled dependency arc from the next input word to the top of the stack and then takes the word from the top of the stack; and finally \\textit{right-arc}, which adds a dependency arc from the top of the stack to the next input word and pushes that same word into the stack. During the construction of the parsing tree each one of the transitions is selected using a reward signal. In this particular implementation the optimal policy for selecting the transitions is estimated using the SARSA reinforcement learning algorithm. \nAn interesting modification found in the implementation of this algorithm is the replacement of the $Q$ function by an approximation computed through the calculation of the negative free energies of a restricted Boltzmann machine. The results of this approach for dependency parsing using reinforcement learning are comparable with state-of-the-art methods. More recently, it has been shown that reinforcement learning can also be used to reduce error propagation in greedy dependency parsing . In another approach, Neu et al.\\  used a number of inverse reinforcement learning algorithms to solve the parsing problem with probabilistic context-free grammars. In inverse reinforcement learning, given a set of trajectories in the environment, the goal is to find a reward function such that if it is used for estimating the optimal policy, the resulting policy can generate trajectories very similar to the original ones . \nAnother dual learning approach for solving the semantic parsing problem is presented by Cao et al.\\ . This dual learning algorithm follows the same strategy used by Zhu et al.\\ , consisting of an adversarial training scheme that can use both labeled and unlabeled data. The primary task (semantic parsing) learns the transformation from a query to logical form (Q2LF). The secondary task (natural language generation) learns the transformation from a logical form to a query (LF2Q). The agent from the primary task can teach the agent from the secondary task and vice versa in a reinforcement learning fashion. A validity reward by checking the output of the primary model at the surface and at semantic levels is used. This reward function requires prior knowledge of the logical forms of the domain of interest, and it is used to check for completeness and well-formed semantic representations. The experimental results showed that semantic parsing based on dual learning improves performance across datasets.\nIn a probabilistic context-free grammar, each production rule has a probability assigned to it, which results in the generation of expert trajectories. Speeding up the learning of parse trees using reinforcement learning has also been studied, specifically the use of apprenticeship reinforcement learning as a variation of inverse RL has been shown to be an effective method for learning a fast and accurate parser, requiring only a simple set of features . \nBy abstracting the core problem in syntactic parsing, we  can clearly see that it can be posed as an optimization problem in which the input is a language grammar $G$ and one input string $w_1$ to be parsed, and the output is a parse tree that allows the correct parsing of $w_1$.  This problem gives rise to the following MDP $(S,A,T,R)$  :\n\\begin{itemize}\n\\item The set of states $S$ is defined as the set of all possible partial or complete parse trees that can be generated with the given grammar $G$ and the string $w_1$.\n\\item The set of actions $A$ is formed with all the grammar rules contained in $G$, this is, the application of each derivation rule of the grammar is considered to be an action. \n\\item The transition function $T$ can be completely determined and it is deterministic, because given a selected grammar rule and the current partially parsed string, we can know with certainty the next resulting intermediate parse tree of that string. \n\\item Finally, the reward function $R$ can be defined as a function of the number of arcs that are correctly labeled in the resulting parse tree. \n\\end{itemize}\nBased on this MDP we can formulate a reinforcement learning system as illustrated in Fig.\\ \\ref{parsing_scheme}.\n\\begin{figure}[ht]\n \\centering\n \\includegraphics[width=11cm]{parsing}\n\\caption{Schematic view of a reinforcement learning agent designed for syntactic parsing. The language processing environment will provide the agent with the states and rewards after each of the interactions. The reward function can be defined in various ways, for example, a positive reward of 10 may be provided each time an appropriate grammar rule is applied.}\n \\label{parsing_scheme}\n \\end{figure}", "cites": [7469, 7470], "cite_extract_rate": 0.25, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides an insightful analytical overview of syntactic parsing through the lens of reinforcement learning, synthesizing multiple approaches (SARSA, inverse RL, dual learning) and integrating them into a coherent narrative. It also abstracts the parsing process into an MDP and discusses how RL can address parsing challenges, while offering some critical analysis of reward function design and data requirements."}}
{"id": "b386bd41-7469-48c4-8cd4-705282409545", "title": "Language understanding", "level": "section", "subsections": [], "parent_id": "e196ef7e-046c-453b-afc3-ad6e4e23314a", "prefix_titles": [["title", "Survey on reinforcement learning for language processing"], ["section", "Language understanding"]], "content": "Language understanding can also be posed as a Markov decision process and therefore we can apply sophisticated reinforcement learning algorithms designed in recent years. Furthermore, we can implement them together with deep neural networks to cope with the massive amount of data that text understanding applications typically require. \n\\begin{figure}[h!]\n\\begin{align*}\n \\langle\\mathtt{SENTENCE}\\rangle &\\rightarrow \\langle\\mathtt{NOUN\\_PHRASE}\\rangle \\langle\\mathtt{VERB\\_PHRASE}\\rangle\\\\\n\\langle\\mathtt{NOUN\\_PHRASE}\\rangle &\\rightarrow \\langle\\mathtt{CMPLX\\_NOUN}\\rangle \\; | \\;  \\langle\\mathtt{CMPLX\\_NOUN}\\rangle \\langle\\mathtt{PREP\\_PHRASE}\\rangle \\\\\n \\langle\\mathtt{VERB\\_PHRASE}\\rangle & \\rightarrow \\langle\\mathtt{CMPLX\\_VERB}\\rangle \\; | \\;  \\langle\\mathtt{CMPLX\\_VERB}\\rangle \\langle\\mathtt{PREP\\_PHRASE}\\rangle \\\\\n \\langle\\mathtt{PREP\\_PHRASE}\\rangle & \\rightarrow \\langle\\mathtt{PREP}\\rangle \\langle\\mathtt{CMPLX\\_NOUN}\\rangle \\\\\n \\langle\\mathtt{CMPLX\\_NOUN}\\rangle & \\rightarrow \\langle\\mathtt{ARTICLE}\\rangle \\langle\\mathtt{NOUN}\\rangle \\\\\n \\langle\\mathtt{CMPLX\\_VERB}\\rangle & \\rightarrow \\langle\\mathtt{VERB}\\rangle \\; | \\; \\langle\\mathtt{VERB}\\rangle \\langle\\mathtt{NOUN\\_PHRASE}\\rangle \\\\\n \\langle\\mathtt{ARTICLE}\\rangle & \\rightarrow \\mathtt{a} \\; | \\;  \\mathtt{the} \\\\\n \\langle\\mathtt{NOUN}\\rangle & \\rightarrow \\mathtt{customer} \\; | \\; \\mathtt{discount} \\; | \\; \\mathtt{refund} \\\\\n \\langle\\mathtt{VERB}\\rangle & \\rightarrow \\mathtt{wants} \\; | \\; \\mathtt{requests} \\; | \\; \\mathtt{cancelled} \\\\\n \\langle\\mathtt{PREP}\\rangle & \\rightarrow \\mathtt{with}\n \\end{align*}\n\\caption{Grammar defining valid sentences in English, Grammar adapted from .}\n\\label{grammar02}\n\\end{figure}\nConsider a problem of natural language understanding. In such a problem we could have a grammar like the one given in Fig.\\ \\ref{grammar02} that allows a program to automatically determine the elements of a sentence written in English. Using this grammar, sentences such as ``The customer with a discount wants a refund'' and ``The customer with a discount cancelled the refund'' can be analyzed by an automated system to determine the intention of the customer, which in this case is whether she wants a refund or she wants to cancel a refund she had previously requested. Therefore, a grammar can be used to detect users' intentions while reinforcement learning can be used to select the optimal sequence of substitutions during the parsing process of the input sentences. Once the parser program has been used to determine the grammatical role of each word in the input text string, the result can be stored in a vector-type structure such as [\\emph{who}=user02, \\emph{intention}=``wants'', \\emph{content}=``discount'']. This vector-type representation of variables \\emph{who}, \\emph{intention} and \\emph{content}, can be used for another program to determine the most appropriate action to be performed next. For example, informing about a discount to a customer. Figure \\ref{understanding_scheme} outlines the procedure.\n\\begin{figure}[ht]\n \\centering\n \\includegraphics[width=11cm]{understanding}\n\\caption{Schematic view of a reinforcement learning agent designed for text understanding, as an example application. The language model agent acts by assigning values to a vector or list of variables, as a function of the utterance of a user. The current user $user02$ is computed from the user's utterance. The next state is the string generated from the vector of variables as understood by the agent, for example, using a text generator agent (see Fig.\\ \\ref{text_generator_agent}). The language processing environment will provide the agent with the states and rewards after each of the interactions. The environment and the reward function are determined by the language understanding  task being solved, i.e., an infobot.}\n \\label{understanding_scheme}\n \\end{figure}\nAmbiguities are an important problem in language understanding. For example, the sentence ``the child observes the cat in the tree'' may have two interpretations, whether the child is in the tree or the cat is in the tree. This kind of ambiguity in the language is hard to solve even by humans. Sometimes it can be solved by using context or common sense. From the point of view of reinforcement learning, there is no obvious way to solve it either. One approach to this problem would be to leverage the powerful text embedding vectors generated by sophisticated language models such as GPT together with a function that rewards making corrections as learning interactions go on, taking advantage of the context. GPT-based models are very good at keeping contextual information. A reward function could provide a larger reward when the interpretation of the intent is more highly evaluated by a context metric provided by the language model.\nLanguage understanding programs approached by reinforcement learning have to deal with systems that automatically interpret text or voice in the context of a complex control application, and use the knowledge extracted to improve control performance. Usually, the text analysis and the learning of the control strategy are carried out both at the same time. For example, Vogel and Jurafsky  implement a system capable to learn to execute navigational instructions expressed in a natural language. The learning process is carried out using an apprenticeship approach, through pairs of paths in a map and their corresponding descriptions in English. The challenge here is to discover which commands match English instructions for navigation. The correspondence is learned applying reinforcement learning and using the deviation between the given desired path and the route being followed for the reward signal. This work demonstrates that the semantic meaning of spatial terms can be grounded into geometric properties of the paths. In a similar approach to language grounding  the system learns to interpret text in the context of a complex control application. Using this approach, text analysis and control strategies are learned jointly using a neural network and a Monte Carlo search algorithm. The approach is tested on a video game, using its official manual as a text guide.\nDeep reinforcement learning has also been used to automatically play text games , showing that it is possible to extract meaning rather than simply memorizing strings of texts. This is also the case of the work presented by , where an LSTM and a Deep Q-Network are employed to solve the sequence-to-sequence problem. This approach is tested with the problem of rephrasing a natural language sentence. The encoding is performed using the LSTM and the decoding is learned by the DQN. The LSTM initially suggests a list of words which are taken by the DQN to learn an improved rephrasing of the input sentences.\nZhu et al.\\  presented a semi-supervised approach to tackle the dual task of intent detection and slot filling in natural language understanding (NLU). The suggested architecture consists of a dual pseudo-labeling method and a dual learning algorithm. They apply the dual learning method by jointly training the NLU and semantic-to-sentence generation (SSG) models, using one agent for each model. As the feedback rewards are non-differentiable, a reinforcement learning algorithm based on policy gradient is applied for optimization. The two agents collaborate in two closed loops. The NLU2SSG loop starts from a sentence, first generating a possible semantic form by the NLU agent and then reconstructing the original sentence by SSG. The SSG2NLU loop goes in reverse order. Both the NLU and SSG models are pre-trained on labeled data. The corresponding validity rewards for the NLU and SSG evaluate whether the semantic forms are valid. The approach was evaluated on two public datasets, i.e., ATIS and SNIPS, achieving state-of-the-art performance. The proposed framework is agnostic of the backbone model of the NLU task.\nText understanding is one of the most recent natural language problems approached using reinforcement learning, specifically by deep reinforcement learning. This approach consists of mapping text descriptions into vector representations. The main goal is to capture the semantics of the texts. Therefore, learning good representations is key. In this context, it has been argued that LSTMs are better than Bag-Of-Words (BOW) when combined with reinforcement learning algorithms. The reason is that LSTMs are more robust to small variations of word usage, and they can learn some underlying semantics of the sentences .\nAs we have seen above, the main applications of reinforcement learning in the context of language understanding have been focused on the learning of navigational directions. RL or inverse RL recommend themselves over supervised learning due to the good match between sequential decision making and parsing. However, it is not difficult to think of other similar applications that could take advantage of this approach. For example, if we can manage to design a system capable to understand text to some degree of accuracy, such a system could be used to implement intelligent tutors, smart enough to understand the questions posed by the user and select the most appropriate learning resource, whether it is some text, audio, video, hyperlink, etc.\nInterestingly, the successful results recently obtained with the combination of deep neural networks and reinforcement learning algorithms open another dimension of research that appears to be promising in the context of parsing and text understanding. As we have mentioned before, creating natural language models is difficult because natural languages are large and constantly changing. We think that deep reinforcement learning (DRL) could become the next best approach to natural language parsing and understanding. Our reasoning is based primarily on two facts. First, DRL can store optimally thousands of parameters of the grammars as a neural model, and we have already evidence that these neural models can be very effective with other natural language problems such as machine translation. Second, reinforcement learning methods would allow the agent to keep adapting to changes in a natural language, since the very nature of these algorithms is to learn through interaction and this feature allows the reinforcement learning agents to constantly adapt to changes in their environment.", "cites": [7469], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.3, "critical": 2.7, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates multiple concepts, including grammar-based parsing, reinforcement learning for language understanding, and semi-supervised dual learning approaches. It provides a general explanation of how RL can be applied to resolve ambiguities and extract intentions. While it includes some analysis (e.g., on the use of reward functions with language models), it lacks deeper comparisons or critiques of the cited works and primarily builds a conceptual framework rather than offering novel synthesis."}}
{"id": "b1785986-2cf3-46dc-8689-601ed44bd509", "title": "Text generation systems", "level": "section", "subsections": [], "parent_id": "e196ef7e-046c-453b-afc3-ad6e4e23314a", "prefix_titles": [["title", "Survey on reinforcement learning for language processing"], ["section", "Text generation systems"]], "content": "\\label{generation}\nText generation systems are built to automatically generate valid sentences in natural language. One of the components of such systems is a language model. Once the language model is provided or learned, the optimization problem consists of generating valid sequences of substrings that will subsequently complete a whole sentence with some meaning in the domain of the application.\nGiven a vector representation of a set of variables in a computational system and their corresponding values, a reinforcement learning algorithm can be used to generate a sentence in English, or any other natural language, that can serve to communicate specific and meaningful information to a human user. However, using the information stored in a set of program variables and constructing sentences in a natural language representing such information is not an easy task. This problem has been studied in the context of generating navigational instructions for humans, where the first step is to decide about the content that the system wants to communicate to the human, and the second step is to build the correct instructions adding word by word. An interesting point in this approach is that the reward function is implemented as a hidden Markov model  or as a Bayesian network . The reinforcement learning process is carried out with a hierarchical algorithm using semi-MDP's.\n \\begin{figure}[ht]\n  \\centering\n \\includegraphics[width=11cm]{text_generation}\n  \\caption{Schematic view of a reinforcement learning agent designed for language generation, as an example application. The language model agent acts by selecting words from a relevant set of words, which is a function of the current state. The current state is a -- possibly incomplete -- sentence in English. The next state is the sentence resulting from appending the word selected by the agent. The language processing environment will provide the agent with the states and rewards after each of the interactions. Actions might take the form of strings of characters such as n-grams, words, sentences, paragraphs or even full documents. The environment and the reward function are determined by the language processing task being solved, i.e., text generation.}\n  \\label{text_generator_agent}\n \\end{figure}\nText generation has also been  approached using inverse reinforcement learning (IRL)  and generative adversarial networks (GANs) . Shi et al.\\  proposed a new method combining GANs and IRL to generate text. The main result of this work is the alleviation of two problems related to generative adversarial models, namely reward sparsity and mode collapse. The authors of this work also introduced new evaluation measures based on BiLingual Evaluation Understudy (BLEU) score, designed to evaluate the quality of the generated texts in terms of matching human-generated expert translations. They showed that the use of IRL can produce more dense reward signals and it can also generate more diversified texts. With this approach, the reward and the policy functions are learned alternately, following an adversarial model strategy. According to the authors, this model can generate texts with higher quality than previous proposed methods based also on GANs, such as SeqGAN , RankGAN , MaliGAN  and LeakGAN . The adversarial text generation model uses  a discriminator and a generator. The discriminator judges whether a text is real or not, meanwhile the generator learns to generate texts by maximizing a reward feedback provided by the discriminator through the use of reinforcement learning. The generation of entire text sequences that these adversarial models can accomplish helps to avoid the exposure bias problem, a known problem experienced by text generation methods based on RNNs. The exposure bias problem  lets small discrepancies between the training and inference phases accumulate quickly along the generated sequence.\nIn a text generation task the corresponding MDP might be defined as follows:\n\\begin{itemize}\n\\item Each state in $S$ is formed with a feature vector describing the current state of the system being controlled, containing enough information to generate the output string. We can visualize this feature vector as a set of variables that describe the current status of the system.\n\\item Actions in $A$ will consist of adding or deleting words.\n\\item With respect to the transition function $T$, every next state can be determined by the resulting string, after we have added or deleted a word.\n\\item In this task, the reward function could be learned from a corpus of labeled data or more manually, from human feedback.\n\\end{itemize}\nAn advantage of RL methods over supervised learning for text generation becomes apparent when there is a diversity of valid text output, i.e., multiple different generations would be of equal quality. In this case, it is problematic for supervised learning to define a differentiable error for backpropagation. However, evaluation measures like BLEU or the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) can be used well to define a reward function for RL . Future research work can focus on adaptive natural language generation during human-computer interaction, assuming a continuously changing learning environment. In natural language generation the main goal is to build a precise model of the language, and the current existing approaches are far from being generic.\nAnother more complicated possibility is the study of language evolution under a reinforcement learning perspective. In general, language evolution is concerned with how a group of agents can create their own communication system . The communication system emerges from the interaction of a set of agents inhabiting a common environment. A process like this can be modeled as a reinforcement learning multi-agent system .\nLi et al.\\  used reinforcement learning and inverse reinforcement learning for paraphrase generation. One of the components of this approach is a generator. The generator is initially trained using deep learning and then it is fine-tuned using RL. The reward of the generator is given by a second component of the architecture, the evaluator. The evaluator is a deep model trained using inverse RL to evaluate whether two given phrases are similar to each other.", "cites": [1106, 7471, 8403], "cite_extract_rate": 0.21428571428571427, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information effectively by integrating concepts from GAN-based approaches (e.g., SeqGAN, RankGAN) with inverse reinforcement learning for text generation. It provides a coherent narrative of how these methods address challenges like reward sparsity and exposure bias. While it includes some critical analysis (e.g., pointing out limitations of current approaches and identifying problems like mode collapse), the critique is not deeply nuanced. It offers moderate abstraction by generalizing the MDP formulation for text generation and highlighting broader research directions."}}
{"id": "58f55bb8-afc9-4e7f-a458-a291f326fa39", "title": "Machine translation", "level": "section", "subsections": [], "parent_id": "e196ef7e-046c-453b-afc3-ad6e4e23314a", "prefix_titles": [["title", "Survey on reinforcement learning for language processing"], ["section", "Machine translation"]], "content": "\\label{translation}\nMachine translation (MT) consists in automatically translating sentences from one natural language to another one, using a computing device . An MT system is a program that receives text (or speech) in some language as input and automatically generates text (or speech), with the same meaning, but in a different language (see Fig.\\ \\ref{fig:translation}). Early MT systems translate scientific and technical documents, while current developments involve online translation systems, teaching systems, among others. MT systems have been successfully applied to an increasing number of practical problems . Since 1949, when the task of machine translation was proposed to be solved using computers~, several approaches have been studied over the years.\n \\begin{figure}[ht]\n \\centering\n \\includegraphics[width=11cm]{translation}\n \\caption{Schematic view of a reinforcement learning agent designed for language translation. It gets as input a text in some language A, and responds with another text string in a different language B. Input and output text strings have the same meaning. The language model agent acts by selecting the most relevant string of words. The language processing environment will provide the agent with the states and rewards after each of the interactions. The environment and the reward function are determined by the machine translation task being solved, i.e., translation from English to Spanish.}\n\\label{fig:translation}\n \\end{figure}\nStatistical machine translation (SMT) is by far the most studied approach to machine translation. In this paradigm, translations are generated using statistical models whose parameters are estimated through the analysis of many samples of existing human translations, known as bilingual text corpora . SMT algorithms are characterized by their use of machine learning methods, where neural networks have been used with some success .\nIn the last decade neural networks have won the battle against statistical methods in the field of translation. Neural Machine Translation (NMT)  uses large neural networks to predict the likelihood of a sequence of words. NMT methods have been broadly applied to advance up-to-date phrase-based SMT systems, where a unit of translation may be a sequence of words (instead of a single word), called a phrase~. NMT systems became a major area of development since the emergence of deep neural networks in 2012~. Current state-of-the-art machine learning translation systems rely heavily on recurrent neural networks (RNN), such as the Long Short-Term Memory (LSTM) network ~.\nIn the sequence-to-sequence approach~ depicted in Fig.~\\ref{seq2seq}, which was used for translation~, two recurrent neural networks are needed, an encoder and a decoder. The encoder RNN updates its weights as it receives a sequence of input words in order to extract the meaning of the sentence. Then, the decoder RNN updates its corresponding weights to generate the correct sequence of output words, in this case, the translated sentence. \nIn the RNN approach the encoder makes reference to a program that would internally encode or represent the meaning of the source text, meanwhile the decoder will decode that internal representation and output a translated sentence with the correct meaning.\nThere are two problems that arise in the training and testing of seq2seq models. These problems are known as 1) exposure bias, i.e., the discrepancy between ground-truth dependent prediction during training and model-output dependent prediction during testing, and 2) inconsistency between the training and test objectives, i.e., measurement. Both problems have been recently studied and various solutions based on RL have been proposed .\n\\begin{figure}[ht]\n \\centering\n \\includegraphics[width=11cm]{seq2seq}\n \\caption{Sequence-to-sequence RNN architecture for machine translation, adapted from~.}\n \\label{seq2seq}\n \\end{figure}\nSimilarly to what can be accomplished in conversational systems, in machine translation, we see that reinforcement learning algorithms can be used to predict the next word or phrase to be uttered by a person, specially during a simultaneous translation task, where the content is translated in real-time as it is produced~. This prediction is useful to increase the quality and speed up the translation.\nIn the case of the training, when it is done interactively, there is evidence that reinforcement learning can be used to improve the real-time translation performance after several interactions with humans . Gu et al.\\  propose an NMT model for real-time translation, where a task-specific neural network learns to decide which actions to take (i.e., to wait for another source word or to emit a target word) using a fixed pre-trained network and policy gradient techniques. Furthermore, to tackle the need of massive training data in machine translation, He et al.\\ propose a dual learning mechanism, which automatically learns from unlabeled data . This method is based on the fact that using a policy gradient algorithm together with a reward function defined as the likelihood of the language model, it is possible to create a translation model using examples of translation going in both directions, from language one to language two, and from language two to language one. With this approach it is possible to obtain an accuracy similar to the accuracy obtained with other neural models, but using only 10\\% of the total number of training examples.\nSpeech translation systems have improved recently due to simultaneous machine translation, in which translation starts before the full sentence has been observed. In traditional speech translation systems, speech recognition results are first segmented into full sentences, then machine translation is performed sentence-by-sentence. However, as sentences can be long, i.e., in the case of lectures or presentations, this method can cause a significant delay between the speaker's utterance and the translation results, forcing listeners to wait a noticeable time until receiving the translation. Simultaneous machine translation avoids this problem by starting to translate before the sentence boundaries are detected. As a first step in this direction, Grissom II et al.\\  propose an approach that predicts next words and final verbs given a partial source language sentence by modeling simultaneous machine translation as a Markov decision process and using reinforcement learning. The policy introduced in this method works by keeping a partial translation, querying an underlying machine translation system and deciding to commit these intermediate translations occasionally. The policy is learned through the iterative imitation learning algorithm SEARN . By letting the policy predict in advance the final verb of a source sentence, this method has the potential to notably decrease the delay in translation from languages in which, according to their grammar rules, the verb is usually placed in the end of the phrases, such as German. However, the successful use of RL is still very challenging, especially in real-world systems using deep neural networks and huge datasets .\nReinforcement learning techniques have also had a positive impact in statistical machine translation, which uses predictive algorithms to teach a computer how to translate text based on creating the most probable output learned from different bilingual text corpora. As the goal in reinforcement learning is to maximize the expected reward for choosing an action at a given state in an MDP model, algorithms based on bandit feedback for SMT can be visualized as MDP's with one state, where selecting an action represents the prediction of an output , . Bandit feedback inherits the name from the problem of maximizing the amount of rewards obtained after a sequence of plays with a one-armed bandit machine, without apriori knowledge of the reward distribution function of the bandit machine. Sokolov et al.\\  propose a structured prediction in SMT based on bandit feedback, called \\emph{bandit expected loss minimization}. This approach uses stochastic optimization for learning from partial feedback in the form of an expected 1--BLEU loss criterion , , as opposed to learning from a gold standard reference translation. This is a non-convex optimization problem, which they analyzed in the stochastic gradient method of pseudogradient adaptation  that allowed to show convergence of the algorithm. Nevertheless, the algorithm of Sokolov et al.\\  presents slow convergence. In other words, such a system needs many rounds of user feedback in order to learn in a real-world SMT. Moreover, it requires absolute feedback of translation quality. Therefore, Sokolov et al.\\  propose improvements with a strong convexification of the learning objective, formalized as bandit cross-entropy minimization to overcome the convergence speed problem. They also propose a learning algorithm based on pairwise preference rankings, which simplifies the feedback information.\nThe same approach used for machine translation can be used in a rephrasing system . This system receives a sentence as an input, creates an internal representation of the information contained in such a sentence and then generates a second sentence with the same meaning of the first one.  The algorithms used to solve such a challenging problem are the long short-term memory (LSTM) and a deep Q network (DQN). The former is used to learn the representation of the input sentence and the latter is used to generate the output sentence. The experiments presented in this work indicate that the proposed method performs very well at decoding sentences. Furthermore, the algorithm significantly outperformed the baseline when it was used to decode sentences never seen before, in terms of BLEU scores. The generation of the output string is not explicitly computed from a vector of variables, instead, this vector representation is implicitly learned and stored in the weights of the LSTM and the deep Q network. Similarly, this system does not need an explicit model of the language to do the rephrasing, because that model is also learned and stored in its neural networks. Therefore, the inner workings of this system are the same as a machine learning translator. It receives a string of words as input and generates another string of words with the same meaning.  \nThe rephrasing problem aforementioned consists in generating one string $B$ based on some input string $A$, in such a way that both strings have the same meaning. Considering this task we can define an MDP $(S,A,P,R)$ as proposed in :\n\\begin{itemize}\n\\item The set of states $S$ is defined as the set of all possible input strings $w_i$. \n\\item The set of actions $A$ consists of adding and deleting words taken from some vocabulary. \n\\item The transition function $P$ can be completely determined and it is deterministic. The next state is the string that results from adding or deleting a word. \n\\item Finally, the reward function $R$ can be defined as a function that measures how similar the strings $A$ and $B$ are, in semantical terms. \n\\end{itemize}  \nIn general, machine translation can be defined as an optimization problem. In the particular case of simultaneous translation, we can define an MDP $(S,A,P,R)$ and solve it using reinforcement learning as we explain next. Given an utterance in a language $A$, we need to find the optimal utterance $B$ that maximizes a measure of semantic similarity with respect to $A$. In this kind of translation problem, when the sentences need to be translated as fast as possible, reinforcement learning can be used for learning when a part of a sentence should be trusted and used to translate future parts of the same sentence. In this way the person waiting for the translated sentence does not need to wait until the translator gets the last word of the original sentence to start the translation process. Therefore, the translation process can be accelerated by predicting the next noun or verb. The corresponding MDP is the following :\n\\begin{itemize}\n\\item Each state in $S$ contains the string of words already seen by the translator and the next predicted word.\n\\item The actions in $A$ are mainly of three types: to commit to a partial translation, to predict the next word, or to wait for more words.\n\\item The transition function $P$, indicating the transitions from one state to another is fully determined by the current state and the action performed. We can compute the resulting string after applying an action.\n\\item The reward function $R$ can be defined based on the BLEU score~, which basically measures how similar one translation is compared to a reference string, which is assumed to be available for training.\n\\end{itemize}\nThere is a number of improvements that could be researched in simultaneous machine translation using reinforcement learning. One is the implementation of these systems in more realistic scenarios where faster convergence is required. Currently, the experimentation with this approach has involved idealized situations in which the phrase to be translated contains only one verb. This constraint should be dropped if we want to employ them in real-world scenarios.\nExperiments with other languages are also needed, especially for those languages that do not fall into the set of most spoken languages in the world. This will require the estimation of different optimal MDP policies, one for each language. However, if the correct recurrent neural model can be defined, using reinforcement learning might help in autonomously learning machine translation. In the same way that AlphaGo managed to play multiple games against itself and improved in the process, it might be the case that future translator algorithms can learn multiple natural languages by talking to themselves.", "cites": [2401, 7472, 7475, 7473, 207, 168, 7474], "cite_extract_rate": 0.25, "origin_cites_number": 28, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.2, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a coherent narrative by integrating key concepts from the cited papers, particularly those related to sequence-to-sequence models and the application of reinforcement learning (RL) in addressing specific translation challenges. It critically discusses limitations of RL in real-world systems and introduces relevant solutions such as policy gradient techniques and dual learning. However, it could offer more systematic comparison and deeper abstraction by identifying broader patterns or theoretical implications across the RL-based translation approaches."}}
{"id": "82fc9da5-d61d-40f4-b703-c39b1839ba5a", "title": "Conversational systems", "level": "section", "subsections": [], "parent_id": "e196ef7e-046c-453b-afc3-ad6e4e23314a", "prefix_titles": [["title", "Survey on reinforcement learning for language processing"], ["section", "Conversational systems"]], "content": "\\label{conversational}\nConversational systems are designed to interact with various users using natural language,  most commonly in verbal or written form. They are well structured and engineered to serve for instance as automated web assistance or for natural human-robot interaction. The architecture and functionality of such systems are heavily dependent on the application. \nThere are two classes of conversational systems. First, open domain systems, usually known as chatbots. They are built in a Turing-test fashion. This is, they can hold a conversation basically about any topic, or at least they are trained with that goal in mind. Second, closed domain systems which are developed more as expert systems, in the sense that they should serve a conversational purpose very well defined and bounded. They should be able to provide information or assistance about a specific topic. In this article we are more interested in this latter system, since serving a well-defined task, can more easily benefit from reinforcement learning, due to reduced state and action spaces.\nIn this section, we will see that reinforcement learning algorithms can be used to generate suitable responses during a conversation with a human user. If the system can be programmed to predict with some accuracy how a conversation might occur, then it can optimize the whole process in such a way that the system can provide more information in less interactions if we are talking about a system designed to inform humans, or it can make a more interesting conversation if it is designed as a chatbot for entertainment. There are a number of factors that affect the effectiveness of a conversational system, including context identification, dynamic context adaptation, user intention , and domain knowledge .\nConversational systems consist of three basic components whose sophistication will vary from system to system. These components are:\n\\begin{enumerate}\n\\item processing of the input message (perception), \n\\item the internal state representation (semantic decoder), and \n\\item the actions (dialogue manager).\n\\end{enumerate}\n\\begin{figure}[ht]\n \\centering\n \\includegraphics[width=11cm]{conversational}\n \\caption{Information flow of a conversational system. This system receives as input a text string containing a question or simply a comment, and it responds with another text string containing the response. This input and response interaction typically iterates several times. Going from ``Input text string x'' to ``Output response string x'' requires the application in sequence of a text understanding agent (see Fig.\\ \\ref{parsing_scheme}) and a text generator agent (see Fig.\\ \\ref{text_generator_agent}).}\n \\end{figure}\nThe input is a message from the user, for instance, speech, gestures, text, etc. The user's input message is converted to its semantic representation by the semantic encoder. The semantic representation of the message is further processed to determine an internal state of the system from which the next action is determined by the dialogue manager. Finally, the actions might include the generation of natural speech, text or other system actions. \nConversational systems are often heuristically-driven and thus the flow of conversation as well as the capabilities are specifically tailored to a single application. Application-specific rule-based systems can achieve reasonably good performance due to the incorporation of expert domain knowledge. However, this often requires a huge number of rules, which becomes quickly intractable .\nDue to the limitations of rule-based systems there are ongoing efforts to use data-driven or statistical conversational systems based on reinforcement learning since the early 2000s . In theory, these data-driven conversational systems are capable of adapting based on interactions with real users. Additionally, they require less development effort but at a cost of significant learning time. Although very promising they still need to overcome several limitations before they are adopted for real-world applications. These limitations stem from both the problem itself and from reinforcement learning algorithms. \nReinforcement learning could potentially be applied to all three components of a conversational system mentioned above, starting with perception of the input message, internal system representations as well as the decision of the system's output. However, we argue that reinforcement learning is more readily available for improving the dialogue manager which deals directly with the user interaction. More difficult but also possible using deep RL would be the learning of suitable internal representations based on the success of the interactions.\nIn a recent survey on neural approaches to conversational AI , it is recognized that in the last few years, reinforcement learning together with deep learning models have helped to significantly improve the quality of conversational agents in multiple tasks and domains. Key aspects of this combination of learning models are that conversational systems are allowed to adapt to different environments, tasks, domains and even user behaviors.\nA large body of research exists for reinforcement learning-based conversational systems. For instance, POMDP-based conversational systems  emerged as a strategy to cope with uncertainty originating from the perceptual and semantic decoder components. However, they also suffer from very large state representations that often become intractable (\\textit{curse of dimensionality}) which typically necessitates some sort of state space compression . We attribute this limitation to the widespread use of discrete state space representations typical in dialogue management and early days of reinforcement learning algorithms. We believe that such limitation could be overcome with continuous state space representations and the use of function approximation techniques such as DQN , VIN , A3C , TRPO  and many others. Although there have been attempts to use function approximation techniques within dialogue management systems , these have not been scaled up. Li et al.\\  simulated a dialogues between two virtual agents, and sequences that display three useful conversational properties are rewarded. These properties are: informativity, coherence, and ease of answering. This RL model uses policy gradient methods.\nThe main implications of using a continuous representation of the states is that we are required to estimate less parameters than when we use a discrete state representation. This is the case when we are dealing with large state spaces. As a result of handling less parameters the learning of policies can be significantly accelerated. Moreover, the quality of the learned policies is usually better than the policies learned with discretized state spaces. When we are implementing deep reinforcement learning models the number of weights in our neural network used to store the value functions can be large. However, the number of parameters of a deep model is less than the number of discrete states for which we would need to estimate a value.\nLemon et al.\\  showed that natural language generation problems can be solved using reinforcement learning by jointly optimizing the generation of natural language and the management of dialogues. Another approach  based on RL to improve the long-turn coherence and consistency of a conversation is proposed in . With this approach it is possible to obtain smooth transitions between task and non-task interactions. Papaioannou and Lemon  present a chatbot system for task-specific applications. This system for multimodal human-robot interaction can generate longer conversations than a rule-based algorithm. This implies that the learned policy is highly successful in creating an engaging experience for chat and task interactions. A conversational agent can be effectively trained using a simulator . After a preliminary training, the agent is deployed in the real scenario in order to generate interactions with humans. During these interactions with the real world the agent keeps learning. In a similar approach, Li et al.\\  used a movie booking system to test a neural conversational system trained to interact with users by providing information obtained from a structured database. Interestingly, if the action spaces of the agents are treated as latent variables, it is possible to induce those action spaces from the available data in an unsupervised learning manner. This approach can be used to train dialogue agents using reinforcement learning .\nSome researchers have tried to develop question answering (QA) systems with multi-step reasoning capabilities, based on reinforcement learning. Though QA systems cannot be considered full conversational systems, both share some common challenges. DeepPath , MINERVA  and M-Walk  are recent examples of systems that perform multi-step reasoning on a knowledge base through the use of reinforcement learning.\nMore recently, Yang et al.\\  presented a dialogue system that learns a policy that maximizes a joint reward function. The first reward term encourages topic coherence by computing the similarity between the topic representation of the generated response and that of the conversation history. The second term encourages semantic coherence between the generated response and previous utterance by computing mutual information. The last term is based on a language model to estimate the grammatical correctness and fluency of the generated response. Lu et al.\\  used Hindsight Experience Replay (HER) to address the problem of sparse rewards in dialogues. HER allows for learning from failures and is thus effective for learning when successful dialogues are rare, particularly early in learning. Liu et al.\\  showed that the goal is to model understanding between interlocutors rather than to simply focus on mimicking human-like responses. To achieve this goal, a transmitter-receiver-based framework is proposed. The transmitter generates utterances and the receiver measures the similarity between the built impression and the perceived persona. Mutual persona perception is then used as a reward to learn to generate personalized dialogues. Chen et al.\\  proposed a structured actor-critic model to implement structured deep reinforcement learning. It can learn in parallel from data taken from different conversational tasks,  achieving stable and sample-efficient learning. The method is tested on 18 tasks of PyDial . Plato et al.\\  presented a complete attempt at concurrently training conversational agents. Such agents communicate only via self-generated language, outperforming supervised and deep learning baselines. Each agent has a role and a set of objectives, and they interact using only the language they have generated.\nOne major problem regarding the building of conversational systems lies in the amount of training data needed  which could originate from simulations (as in most of the research), offline learning (limited number of interaction data sets) and learning from interactions with real users. In fact, training and evaluating such systems require large amounts of data. Similarly, measuring the performance of conversational systems is itself a challenge and different ways of measuring it have been proposed. One way is based on the use of some predefined metrics that can be used as the reward function of the system, for example, some measurement of the success rate of the system, which can be calculated when the system solves the user's problem. Another way of giving reward to the system is by counting the number of turns, which gives preference to more succinct dialogues. A more sophisticated way would be to automatically assess the sentiment of the evolving conversation, generating larger rewards for positive sentiment . Other metrics that are being explored are the coherence, diversity and personal style of a more human-like conversational system .\nAnother way of measuring the performance is through the use of human simulators. However,  programming human simulators is not a trivial task. Moreover, once we have found a functional dialogue policy, there is no way to evaluate it without relying on heuristic methods. Some simulators are completely built from available data. The way they work is basically by selecting at the start of each training episode a randomly generated goal and a set of constraints. The performance of the system is measured by comparing the sequence of contexts and utterances generated after each step during the training. User simulation is not obvious and is still an ongoing research field. \nIn general, conversational systems can be classified into two different types:\n1) task-oriented systems, and 2) non-task-oriented systems.\nBoth types of systems can be defined as a general optimization problem that can be solved using reinforcement learning algorithms. An MDP $(S,A,T,R)$ with the main elements required to solve such an optimization problem is the following:\n\\begin{itemize}\n\\item The set of states $S$ is defined as the history of all utterances, such as comments, questions and answers happening during the dialogue. \n\\item The set of actions $A$ consists of all the possible sentences that the system can answer to the user in the next time step.\n\\item The transition function $T$. The next state is the updated history of utterances after adding the last sentence generated by the system or the user. The transition function is non-deterministic in the case of non-predictable user responses.\n\\item Finally, the reward function $R$ can be defined as a function that measures the performance of the system, or how similar the generated dialogue is with respect to a reference dialogue from an existing corpus.\n\\end{itemize} \nThe training of conversational systems could be also done using human users or using a model learned from corpora of a human-computer dialogue. However, the large number of possible dialogue states and strategies makes it difficult to be explored without employing a simulator. Therefore, the development of reliable user simulators is imperative for building conversational systems, and this comes with its own set of challenges.\nSimulators are in particular useful for getting effective feedback from the environment during learning. For instance, Schatzmann et al.\\  implemented a user simulator using a stack structure to represent the states. The dialogue history in this approach consists of sequences of push and pop operations. Experiments show the effectiveness of this method to optimize a policy and it was shown to outperform a hand-crafted baseline strategy, in a real-world dialogue system. However, using a simulator always has serious limitations, whether it is manually coded, learned from available data, or a mixture of these approaches. A simulator is by definition not the real environment and therefore a reinforcement learning policy trained on it will need some or many adjustments to make it work properly in the real environment. In general, the development of realistic simulators for reinforcement learning and the related methodologies to fine-tune the policies afterwards to make them generalize well in the real world is still an open question. Moreover, the reward function is key to providing effective feedback. It is well known that the design of reward functions is a challenging task that requires expert knowledge on the task to be learned and on the specific algorithm being used. Very often, it is only after many iterations in the design process and a significant amount of experimentation that reward functions are optimally configured. Su et al.\\ studied reward estimation . This approach is based on the one hand on the use of a recurrent neural network pre-trained off-line to serve as a predictor of success and on the other hand, a dialogue policy and a reward function are trained together. The reward function is modeled with a Gaussian process using active learning.\nChen et al.\\ propose an interactive reinforcement learning framework to address the cold start problem . The framework, referred to as a companion teacher, consists of three parties: 1) one learning agent, 2) a human user, and 3) a human `companion' teacher. The agent (dialogue manager) consists of a dialogue state tracker and a policy model. The human teacher can guide learning at every turn (time step). The teacher can guide learning by both reward or policy-shaping. The authors assume that the dialogue states and policy model are visible to the human teacher. In follow-up work , a rule-based system is used for reward- and policy-shaping, but the same strategy could be used to incorporate human feedback. The learning agent is implemented using a Deep Q-Network (DQN) and two separate experience memories for the agent and teacher. Uncertainty estimation is used to control when to ask for feedback and learn from the experience memories. Simulation experiments showed that the proposed approach could significantly improve learning speed and accuracy.", "cites": [7481, 7482, 7483, 8519, 7478, 7477, 7476, 7479, 1390, 1348, 7480, 1391, 1169], "cite_extract_rate": 0.30952380952380953, "origin_cites_number": 42, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple cited papers to provide a structured analysis of how reinforcement learning can be applied to conversational systems, particularly in dialogue management and policy learning. It offers some critical evaluation of limitations (e.g., discrete state space issues) and highlights broader patterns such as the benefits of continuous representations and deep RL. However, while it provides useful insights, it does not offer a fully novel or meta-level framework that would elevate it to a high insight level."}}
{"id": "594cf080-f5f3-4eab-8d95-8bfb0f84a1fe", "title": "Other language processing tasks", "level": "section", "subsections": [], "parent_id": "e196ef7e-046c-453b-afc3-ad6e4e23314a", "prefix_titles": [["title", "Survey on reinforcement learning for language processing"], ["section", "Other language processing tasks"]], "content": "Reinforcement learning has also been used for the improvement of information extraction through the acquisition and incorporation of external information . In this work, a deep Q-network is trained to select actions based on contextual information, leading the information retrieval system to improve its performance by increasing the accuracy of the retrieved documents. This approach can help to reduce the ambiguity in text interpretation. The selection of actions involves querying and extracting new sources of information repetitively. Actions have two components, a reconciliation decision and a query choice. The reward is designed to maximize the extraction accuracy of the values, and at the same time the number of queries is minimized. The experimental work with two domains shows an improvement over traditional information extractors of 5\\% on average.\nNews feed recommendation can be seen as a combinatorial optimization problem and therefore it can be modeled as a Markov decision process. He et al.\\  studied the prediction of popular Reddit threads using a bi-directional LSTM architecture and reinforcement learning. Another approach to the same problem involves the incorporation of global context available in the form of discussions from an external source of knowledge . An interesting idea explored in this approach is the use of two Q-functions. The first is used to generate a first ranking of the actions and the second one is utilized to rerank top action candidates. By doing this, good actions can be selected, i.e., These actions could otherwise be missed due to the very skewed action space that the algorithm can deal with.\nQuite often we see that dialogue systems provide semantically correct responses which are not necessarily consistent with contextual facts. Mesgar et al.\\  used reinforcement learning to fine-tune the responses, optimizing for consistency and semantics.\nGao et al.\\  approached another language processing task using reinforcement learning, namely document summarization. The proposed paradigm uses learning-to-rank as a way to learn a reward function that is later used to generate near-optimal summaries.", "cites": [3613], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of several applications of reinforcement learning in language processing tasks such as information extraction, news feed recommendation, dialogue systems, and document summarization. While it connects a few ideas (e.g., the use of multiple Q-functions for reranking), the synthesis remains limited and does not build a broader narrative. There is little critical evaluation or abstraction to highlight overarching principles or trends."}}
{"id": "7e9f2704-fdbb-438e-a1cd-2680ad7b7fe9", "title": "Promising research directions", "level": "section", "subsections": [], "parent_id": "e196ef7e-046c-453b-afc3-ad6e4e23314a", "prefix_titles": [["title", "Survey on reinforcement learning for language processing"], ["section", "Promising research directions"]], "content": "\\label{directions}\nBased on our analysis of the problems and approaches here reported, we now take a step further and describe 9 research directions that we believe will benefit from a reinforcement learning approach in the coming years.\n\\begin{enumerate}\n\\item {\\bf Recognition of the user's input.}\nWe noticed that a common element missing or at least underrepresented in natural language processing research is the recognition of the user's input. Commonly, this is treated as being inherently uncertain and most research accepts this and tries to cope with it without attempting to solve the source of the problems. This along with all other machine perception problems are very challenging tasks and far from being solved. We argue that trying to address uncertainty of the user input at the initial stages would be more fruitful than simply regarding it as given. Thus, we argue that a future research direction would be to develop a reinforcement learning approach for generating internal semantic representations of the user's message from which other fields within and beyond natural language processing could benefit. \n\\item {\\bf Internal representation learning.}\nLearning an internal representation of language is a more general research direction. By using deep neural networks and reinforcement learning methods, it is possible to learn to code and decode sequences of text . Although such an architecture was implemented and tested only with a text rephrasing task, we believe that the underlying problem of learning an internal representation of language is inherently related to some of the most important NLP problems, such as text understanding, machine translation, language generation, dialogue system management, parsing, etc. By solving the internal representation problem of language, we may partially solve the aforementioned problems to some extent. Therefore, research on deep learning and reinforcement learning methods in a joint approach is currently of great importance to advance the state of the art in NLP systems.\n\\item {\\bf Exploitation of domain knowledge.}\nAnother interesting research path is the one aiming at discovering ways to enhance RL through the exploitation of domain knowledge available in the form of natural language, as surveyed by Luketina et al.\\ . Some current trends involve methods studying knowledge transfer from descriptive task-dependent language corpora . Pre-trained information retrieval systems can be integrated with RL agents  to improve the quality of the queries. Moreover, relevant information can be extracted from sources of unstructured data such as game manuals .\n\\item {\\bf Exploitation of embodiment.}\nA trend in supervised language learning research considers the importance of embodiment for the emergence of language .\nMultimodal inputs, such as an agent knowing its actuators while performing an action, help in classifying and verbally describing an action and allows better generalisation to novel action-object combinations .\nEmbodied language learning has recently been brought to reinforcement learning scenarios, specifically question answering where an agent needs to navigate in a scene to answer the questions , or where it needs to perform actions on objects to answer questions .\nLike dialogue grounded in vision , such interactive scenarios extend language learning into multiple modalities. Such applied scenarios also allow to introduce tasks, corresponding rewards, and hence seamless integration of language learning with reinforcement learning.\nDeep reinforcement learning neural architectures are a promising research path for the processing of multiple modalities in embodied language learning in a dynamic world.\n\\item {\\bf Language evolution.}\nFrom a more linguistic point of view, the study of language evolution using a reinforcement learning perspective is also a fertile field for research. This process can be modelled by a multi-agent system, where a collection of agents is capable to create their own communication protocol by means of interaction with a common environment and by applying reinforcement learning rules . This kind of research can benefit from the recent advances in multi-agent systems and rising computational power. Moreover, research on cognitive robotics using neural models together with reinforcement learning methods  has reached a point where the addition of language evolution capabilities seems to be more promising than ever before.\n\\item {\\bf Word embeddings.}\nMore important, from our point of view, are the advances in neural language models, especially those for word embedding. The recent trend of continuous language representations might have a huge potential if it is used together with reinforcement learning. Word2vec  supplies a continuous vector  representation of words. In a continuous bag-of-words architecture,  Word2vec trains a simple neural network to predict a word from its surrounding words, achieving on its hidden layer a low-dimensional  continuous representation of words in some semantically meaningful  topology. Other word embeddings are GloVe , which  yields a similar performance more efficiently by using a co-occurrence  matrix of words in their context, and FastText ,  which includes subword information to enrich word vectors and to deal  with out-of-vocabulary words.\nA more powerful class of embeddings are contextualized word  embeddings, which use the context, i.e., previous and following words,  to embed a word. Two recent models are ELMo , which uses bidirectional LSTM, and BERT , which uses a deep  feedforward Transformer network architecture with self-attention. Both  are character-based and hence, like FastText, use morphological cues  and deal with out-of-vocabulary words. By taking into account the context, they handle different meanings of a word (e.g., \n``He touches a  rock'' vs ``He likes rock''). However, simple word embeddings become  meaning embeddings, blurring the distinction between word- and  sentence embeddings.\nFor the representations of utterances, Word2vec has been extended to  Doc2vec , and other simple schemes are based on a  weighted combination of contained word vectors  . However, since these simple  bag-of-words approaches lose word-order information, the original sentence cannot be reconstructed. Sentence generation is also difficult for supervised  sentence embeddings such as InferSent  or Google's  Universal Sentence Encoder .\nAn unsupervised approach to sentence vectors are Skip-Thought Vectors  , which are trained to reconstruct the surrounding  sentences of an encoded one. A simpler model would be an  encoder-decoder autoencoder architecture, where the decoder  reconstructs the same utterance that the encoder gets as input, based  on a constant-length internal representation. Hence, this is a  constant size continuous vector representation of an utterance, from  which the utterance, which itself could consist of continuous word  vectors, could also be reproduced.\nTo train utterance vectors on dialogues, large dialogue corpora exist,  which can be classified into human-machine or human-human;  spontaneously spoken, scripted spoken, or written .  Examples are datasets of annotated telephone dialogues, movie  dialogues, movie recommendation dialogues, negotiation dialogues,  human-robot interaction, and also question answering contains elements  of dialogues.\nSuch continuous language representations could seamlessly play  together with continuous RL algorithms like CACLA ,  Deterministic Policy Gradient (DPG)  or deep DPG  (DDPG) . These algorithms handle continuous state  input and continuous action output. Actions of a dialogue agent would  be the agent's utterances, which would result in a new state after the  response of its communication partner. Continuous utterance  representations would allow optimization of an action by gradient ascent to maximize certain rewards which express desired future state properties. For example, it could be desired to maximize the positive  sentiment of an upcoming utterance which can be estimated by a  differentiable neural network .\nOther possible desired state properties could be to maximize a human's  excitement in order to motivate him to make a decision; to maximize  the duration of the conversation, or lead it to an early end with a  pleased human; to acquire certain information from, or to pass on  information to the human. However, not all goals can be easily  expressed as points in a continuous utterance space that represents a  dialogue. To this end, future research on language needs to be  extended towards representing more of its semantics, which entails  understanding the entire situation.\n\\item {\\bf Intelligent conversational systems.}\nWhen conversing with chatbots, it is common to end up in the situation where the bot starts responding with â€œI donâ€™t know what you are talking aboutâ€ repeatedly, no matter what it is asked. This problem is identified as the generic response problem. The cause for this problem might be that such kind of answers occur very often in the training set. Also they are highly compatible with various questions . Another issue is when a dataset has similar responses to different contexts . One way to improve the efficiency in reinforcement learning is through the combination of model-based and model-free learning . We propose that this approach might be useful to solve the generic response problem.\nFurthermore, all the experience gained from working with algorithms designed for text-based games and applications on learning of navigational directions can be extended and adapted to be useful in the implementation of intelligent tutors, smart enough to understand the questions posed by the user and select the most appropriate learning resource, whether it is some text, audio, video, hyperlink, etc. Those intelligent tutors can improve over time.\n\\item {\\bf Assessment of conversational systems.}\nFinally, in conversational  systems, a critical point that needs further investigation is the definition of robust evaluation schemes that can be automated and used to assess the quality of automatic dialogue systems. Currently, the performance of such systems is measured through ad hoc procedures that depend on the specific application and most importantly, they require the intervention of a human, which makes these systems very difficult to be scaled.\n\\item {\\bf Document-editing RL Assistants.}\nKudashkina et al.\\  proposed the domain of voice document editing as a particularly well-suited one for the development of reinforcement learning intelligent assistants that can engage in a conversation. They argue that in voice document editing, the domain is clearly defined, delimited and the agent has full access to it. These conditions are advantageous for an agent that learns the domain of discourse through model-based reinforcement learning. Important future research questions the authors mention are, first, what level of ambition should the agent's learning have? And second, how should the training of the assistant be performed, online or offline?\n\\end{enumerate}", "cites": [7, 7490, 7484, 7165, 7472, 7488, 8418, 7492, 5865, 7491, 8369, 7487, 7485, 7489, 8385, 1348, 7265, 7486, 7264], "cite_extract_rate": 0.5135135135135135, "origin_cites_number": 37, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section synthesizes multiple papers to form a coherent narrative on the intersection of reinforcement learning and language processing, particularly in dialogue systems. It critically identifies limitations in current approaches (e.g., poor handling of context or user input uncertainty) and suggests how RL could improve them. The abstraction is strong, as it frames research directions in terms of general principles like embodiment, internal representation learning, and language evolution."}}
