{"id": "08c12d9d-0b7e-428d-9155-4374e138d8fa", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "b3633da4-8a97-4b38-9323-a6b254dd1354", "prefix_titles": [["title", "Survey on Causal-based Machine Learning Fairness Notions"], ["section", "Introduction"]], "content": "\\label{intro}\nMachine learning algorithms are increasingly used to inform automated decisions with critical impact on people's lives including job hiring, loan granting, predicting recidivism during parole, etc. Correcting bias in the decision prediction requires first to measure it.  The most commonly used fairness notions are observational and rely on mere correlation between variables. For example, statistical parity~ requires that the proportion of positive outcome (e.g. granting loans) is the same for all sub-populations (e.g. male and female groups).  Equal opportunity~ requires that the true positive rate (TPR) is the same for all sub-populations. The main problem of correlation-based fairness notions is that they fail to detect discrimination in presence of statistical anomalies such as Simpson's paradox~. A famous example of the Simpson's paradox is the gender bias in 1973 Berkeley admission~. In that year, 44\\% of male applicants were admitted against only 34\\% of female applicants. While this looks like a bias against female candidates, when the same data has been analyzed by department, acceptance rates were approximately the same. In other words, the statistical conclusions drawn from the sub-populations differ from that from the whole population. Considering the problem of fairness from the legal and philosophical point of view reveals another limitation of statistical fairness notions. In the disparate treatment liability framework~, discrimination claims require plaintiffs to demonstrate a causal connection between the challenged decision (e.g., hiring, firing, admission) and the sensitive feature (e.g., gender, race). It is then necessary to investigate the causal relationship between the sensitive attribute and the decision rather than the associated relationship. Because of these two limitations, it is now widely accepted that causality is necessary to appropriately address the problem of fairness~. \nVarious causal-based fairness notions have been recently proposed to tackle the problem of fairness through causal inference lenses. These include total effect~, counterfactual fairness~, counterfactual effects~, interventional fairness~, etc. These notions differ from statistical fairness approaches in that they are not totally based on data but consider additional knowledge about the structure of the world, in the form of a causal model. This additional knowledge helps to understand how data is generated in the first place and how changes in variables propagate in a system. Most of these fairness notions are defined in terms of non-observable quantities such as interventions (to simulate random experiments) and counterfactuals (which consider other hypothetical worlds, in addition to the actual world). Such quantities cannot be always uniquely computed from observational data which hinders significantly the applicability of causal-based notions in practical scenarios. Each one of the two main causal frameworks in the literature, namely, structural causal model (SCM) with causal graphs~ and potential outcome~, use a different approach to compute/estimate the causal quantities using observational data. The SCM framework relies mainly on the identifiability criterion~ to generate an expression for the causal quantity based only on observable probabilities. If the identifiability criterion is not satisfied, the causal quantity can not be computed using the available observable data. In such case, as an alternative, if the complete structure of the causal model is available, it is possible to estimate the distribution of the latent variables $U$ and consequently generate an estimation of the counterfactual outcomes~. In the potential outcome framework, causal quantities are approximated using several estimation techniques (e.g., matching, re-weighting, etc.)~. \nNineteen causal-based fairness notions are examined in this paper. Given a real-world scenario, selecting which fairness notion to use is a challenging and error-prone task as using the wrong fairness notion may indicate unfairness in an otherwise fair scenario, or the opposite (failing to detect unfairness in an unfair scenario). This survey paper provides guidelines to help selecting a suitable fairness notion given a specific real-world scenario. The guidelines are summarized in a decision diagram that can be easily navigated using the characteristics of the real-world scenario at hand.  On the other hand, according to Pearl's SCM framework, computing causal quantities (interventions and counterfactuals) depends on their identifiability. Hence, even if a fairness notion is appropriate in some setup, it might not be applicable because of identifiability issues. Placing the various causal-based fairness notions in Pearl's causation ladder with the three corresponding rungs (observation, intervention, and counterfactual)~ allows to rank these notions and indicates how difficult to deploy each one of them in practice. \nThis survey paper is a comprehensive report on assessing machine learning fairness with causality lenses. It starts by illustrating the need for causality through a hypothetical example of teacher firing (Section~\\ref{sec:example}). Then, it provides essential background on causal inference in sufficient detail for our analysis (Section~\\ref{sec:notation}). Section~\\ref{sec:notions} examines a comprehensive list of causal-based fairness notions. Unlike other surveys in the literature, the subtleties of the fairness notions are illustrated using a very simple numerical job hiring example. A survey on the three approaches to compute causal quantities from observable data, namely, identifiability, estimation based on full causal model, and potential outcome estimation, is provided in Section~\\ref{sec:computation}. The main contributions of the survey which are the suitability and applicability of causal-based fairness notions are described in Section~\\ref{sec:applicability}. Finally, Section~\\ref{sec:conclusion} concludes.", "cites": [8705, 3899, 4428, 4441, 3905], "cite_extract_rate": 0.35714285714285715, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The introduction synthesizes key concepts from multiple papers, integrating ideas on the limitations of observational fairness, the necessity of causal reasoning, and the computational challenges of causal-based fairness. It critically discusses the shortcomings of statistical fairness notions, particularly in legal contexts, and highlights the trade-offs between different causal frameworks. The section abstracts these ideas into a broader narrative about the importance of causality in fairness and introduces a framework for evaluating the applicability of fairness notions."}}
{"id": "4f32323d-433d-41a7-bc7b-a7aef512b2b1", "title": "The need for causality: an example", "level": "section", "subsections": [], "parent_id": "b3633da4-8a97-4b38-9323-a6b254dd1354", "prefix_titles": [["title", "Survey on Causal-based Machine Learning Fairness Notions"], ["section", "The need for causality: an example"]], "content": "\\label{sec:example}\nConsider the hypothetical example\\footnote{Inspired by the prior convictions example in~.} of an automated system for deciding whether to fire a teacher at the end of the academic year. Deployed teacher evaluation systems have been suspected of bias in the past. For example, IMPACT is a teacher evaluation system used in the city of Washington DC and have been found to be unfair against teachers from minority groups~. Assume that the system takes as input two features, namely, the location of the school where the teacher is working ($C$) and the initial\\footnote{At the beginning of the academic year.} average level of the students in her class ($A$). The outcome is whether to fire the teacher ($Y$). Assume also that all 3 variables are binary with the following values: if the school is located in a high-income neighborhood, $C=1$, otherwise (the school is located in a low-income neighborhood), $C=0$. If the initial average score for the students assigned to the teacher is high, $A=1$, otherwise (initial level is low), $A=0$. Firing a teacher corresponds to $Y=1$, while retaining her corresponds to $Y=0$. The level of students in a given class can be influenced by several variables, but in this example, assume that it is only influenced by the location of the school; students in high-income neighborhoods are more advantaged and typically perform better in school. \nAssume now that the automated decision system is suspected to be biased by the initial level of students assigned to the teacher. That is, it is claimed that the system will more likely fire teachers who have been assigned classes with low level students at the beginning of the academic year which is clearly unfair. The sensitive attribute in this case is the initial level of students assigned to the teacher ($A$). For concreteness, consider the prediction system that yields the following conditional probabilities:\n\\[\\begin{array}{lcl}\n\t\t\\pr(Y=1\\;|\\;A=1, C=0) = 0.02  & \\quad & \\pr(A=1\\;|\\;C=0) = 0.2 \\\\\n\t\t\\pr(Y=1\\;|\\;A=1, C=1) = 0.0675 & \\quad & \\pr(A=1\\;|\\;C=1) = 0.8 \\\\\n\t\t\\pr(Y=1\\;|\\;A=0, C=0) = 0.01 & \\quad & \\pr(A=0\\;|\\;C=0) = 0.8  \\\\\n\t\t\\pr(Y=1\\;|\\;A=0, C=1) = 0.25 & \\quad & \\pr(A=0\\;|\\;C=1) = 0.2  \\\\\n\\end{array}\\]\nand that the dataset is collected from a population where schools are located with equal proportions in high-income and low-income neighborhood, that is, $\\pr(C=1) = \\pr(C=0) = 0.5$. Assume also that the proportion of classes with a low initial average level of students is the same as the one with high average initial level of students, that is, $\\pr(A=1) = \\pr(A=0) = 0.5$. To keep the scenario simple, assume that the level of students $A$ does not depend on any other feature except $C$ and that the firing decision $Y$ depends only on $A$ and $C$.\nA simple approach to check the fairness of the firing decision $Y$ with respect to the sensitive attribute $A$ is to contrast the conditional probabilities: $\\pr(Y=1\\;|\\;A=0)$ and $\\pr(Y=1\\;|\\;A=1)$ which quantify, respectively, the likelihood of firing a teacher given that she is assigned students with an initial low level versus and the likelihood of firing a teacher given that she is assigned students with an initial high level class. Such probabilities can be computed as follows: \n\\begin{align}\n\t\\pr(Y=1\\;|\\;A=a) & = \\sum_{c\\in\\{0,1\\}} \\pr(Y=1\\;|\\;A=a,C=c,) \\nonumber \\\\\n\t& \\qquad \\qquad \\times\\pr(A=a\\;|\\;C=c) \\label{eq:naiveExample1} \n\\end{align}\nHence, \n\\begin{align}\n\\pr(Y=1\\;|\\;A=1) &= 0.02 \\times 0.2 + 0.0675 \\times 0.8 = 0.058 \\nonumber \\\\\n\\pr(Y=1\\;|\\;A=0) &= 0.01 \\times 0.8 + 0.25 \\times 0.2 = 0.058 \\nonumber\n\\end{align}\nAs the values are equal, the rates of firing between teachers who were assigned low level students and high level students appear to be equal and hence no discrimination is detected\\footnote{This corresponds to statistical parity.}. This conclusion is flawed because it doesn't consider the mechanism by which the observed data is generated. In particular, the location of the school in which the teacher is working influences both the initial level of students assigned to her as well as the decision to fire or retain her. The $\\pr(A|C)$ distribution indicates that $80\\%$ of classes in low income neighborhoods have students with low initial levels ($\\pr(A=0\\;|\\;C=0) = 0.8$) while $80\\%$ of classes in high income neighborhoods have students with high initial levels ($\\pr(A=1\\;|\\;C=1) = 0.8$). The automated decision system is biased in this case because $\\pr(Y=1\\;|\\;A=0,C=1)$, the probability of firing a teacher in high income neighborhoods which is assigned a class with low initial level, is exceptionally high ($0.25$). Using simple conditional probabilities (Eq.~\\ref{eq:naiveExample1}) on this collected dataset fails to appropriately account for that bias because very few teachers in high income neighborhoods are assigned low level classes in this particular dataset ($\\pr(A=0\\;|\\;C=1) = 0.2$). \nIn general, any statistical fairness notion which relies solely on correlation between variables, will fail to detect such bias. \nTo avoid such misleading conclusions, the causal relationships between variables should be considered. Figure~\\ref{fig:firing_example} illustrates the causal relations between the three variables of the above example where the location of the school $C$ is a confounder. Based on such causal graph, a firing decision system is fair if it is as likely to fire teachers in the following two hypothetical cases: (1) when \\em{all teachers in the population} are assigned students of low level on average, and (2) when \\em{all teachers in the population} are assigned students of high level on average. This is achieved using intervention ($do()$ operator)\\footnote{Intervention and the $do()$ operator will be explained further in Section~\\ref{identInterv}.} and allows to break the problematic dependence between $A$ and $C$. The probabilities of firing a teacher in these two hypothetical cases are expressed as $\\pr(Y_{A=0}=1) = \\pr(Y=1\\;|\\;do(A=0))$ and $\\pr(Y_{A=1}=1) = \\pr(Y=1\\;|\\;do(A=1))$ respectively. In this simple graph, and assuming no other variable is used in the prediction, these probabilities can be computed as follows:\n\\begin{align}\n\t\\pr(Y_{A=a}=1) & = \\sum_{c\\in\\{0,1\\}} \\pr(Y=1\\;|\\;A=a,C=c) \\times \\pr(C=c) \\nonumber \n\\end{align}\nHence, \n\\begin{align}\n\\pr(Y_{A=1}=1) &= 0.02 \\times 0.5 + 0.0675 \\times 0.5 = 0.0437 \\nonumber \\\\\n\\pr(Y_{A=0}=1) &= 0.01 \\times 0.5 + 0.25 \\times 0.5 = 0.13 \\nonumber\n\\end{align}\n\\begin{figure}[!h]\n\t\\centering\n\t{\\includegraphics[scale=0.3]{./Graphics/firing_example.eps}}\n\t\\caption{Causal graph of the firing example.}\n\t\\label{fig:firing_example}\n\\end{figure}\nThe values confirm the existence of a bias against teachers which are assigned classes with initial low levels.", "cites": [8706], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 4.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a clear analytical example illustrating how traditional statistical fairness notions can be misleading due to confounding variables, using the cited paper to contextualize the discussion. It integrates the concept of fairness in statistical inference with causal analysis. The critical analysis is strong as it explicitly highlights the limitations of correlation-based approaches and motivates the need for causal interventions, offering a broader understanding of fairness evaluation."}}
{"id": "73cd5821-594b-4dd6-8fde-fcd5ad7097cf", "title": "SCM and graphical models vs potential outcome", "level": "subsection", "subsections": [], "parent_id": "ec16d90a-68c2-4abb-9bb7-64cfbca5e6b1", "prefix_titles": [["title", "Survey on Causal-based Machine Learning Fairness Notions"], ["section", "Preliminaries and Notation"], ["subsection", "SCM and graphical models vs potential outcome"]], "content": "Although both causal frameworks are considered equivalent~, interesting differences exist between them. Depending on the task at hand, one framework might be more appropriate to use than the other. For example, reasoning about causal effects at the individual (unit) level is more straightforward with the potential outcome framework~ (Section 3.4). On the other hand, considering the different paths of causal effects (direct, indirect, and spurious) is much easier achieved using SCMs and causal graphs. More generally, potential outcome framework is more suitable for causal inference problems where the goal is to narrowly estimate the causal (treatment) effect of a cause variable $A$ on an outcome variable $Y$. There are two justifications for this point. First, developing estimators of causal effects and counterfactuals can be more straightforward using the potential outcome framework~. Second, the potential outcome framework provides the possibility of decomposing the sources of inconsistency and bias into: unaccounted-for baseline differences between individuals and treatment effect bias~ (Section 3.4).\nSCMs and causal graphs, however, are more suitable in causal discovery problems where the goal is to learn the causal relations among a set of variables~. Potential outcome framework is not well equipped for such problems because the causal effect of variables other than the treatment (sensitive attribute) are not defined.", "cites": [4430], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section compares the strengths and weaknesses of the potential outcome framework and SCMs in the context of fairness, integrating insights from the cited survey on causal inference. It highlights applicability in different types of problems (inference vs. discovery) and explains why one framework may be more appropriate than the other for specific tasks. However, the critique remains surface-level and the synthesis is limited to a single source, preventing a deeper, more nuanced analysis."}}
{"id": "023451fb-47da-4f07-a008-9e8611e0a373", "title": "Causality-based Fairness notions", "level": "section", "subsections": ["8fc69f92-cfb2-4c9f-bf30-2f54a18fd0e1", "b6922584-d008-4a9c-9f53-9d7ab685862d", "a2114640-6836-4226-b821-4c5d6eb1b09e", "3e6d63fe-4e6a-4a96-aae2-46e2c8be065f", "ce67418b-700b-4094-989e-fc1452431822", "ec9f7d7e-d124-4e75-bb73-33dd4578ad53", "510e91a2-6c41-41b5-b745-cf90a05f78c5", "b90e1479-2617-4ce0-811f-8137245636e9", "930031e7-e6db-4e29-a0dd-58380d5a212e", "f2f4b09a-ad5a-4568-9dd1-8df80615ee52"], "parent_id": "b3633da4-8a97-4b38-9323-a6b254dd1354", "prefix_titles": [["title", "Survey on Causal-based Machine Learning Fairness Notions"], ["section", "Causality-based Fairness notions"]], "content": "\\label{sec:notions}\nWithout loss of generality, assume that the sensitive attribute  $A$ and the outcome $Y$ are binary variables where $A=a_0$ denotes the privileged group (e.g. male), typically considered as the reference in characterizing discrimination, and $A=a_1$ the disadvantaged group (e.g. female).\nWhenever needed, the simple job hiring example will be used where $A$ is the sensitive attribute corresponding to the gender ($A=0$ for male and $A=1$ for female), $C$ is a covariate corresponding to the job type ($C=0$ for flexible schedule job and $C=1$ for non-flexible job schedule), and $Y$ is the outcome corresponding to the hiring decision ($Y=0$ for not-hired and $Y=1$ for hired). \nTable~\\ref{tab:hiringExample1} is an example dataset corresponding to this scenario.\n\\begin{table}[!h] \n\t\\centering\n\t\\caption{A job hiring example with 24 applications. $A$ is the gender (sensitive attribute) where $A=1$: female, $A=0$: male. $C$ is the job type where $C=0$: flexible time job, $C=1$: non-flexible time job. $Y$ is the hiring decision (outcome) where $Y=0$: not-hired, $Y=0$: hired.}\n\\label{tab:hiringExample1} \n\\begin{tabular}{ccccccccc}\n\\multicolumn{4}{c}{Female applicants} & & \\multicolumn{4}{c}{Male applicants} \\\\\n\\multicolumn{4}{c}{(Treatment group)} & & \\multicolumn{4}{c}{(Control Group)} \\\\\n$i$ & $A$ & $C$ & $Y$ & & $i$ & $A$ & $C$ & $Y$ \\\\ \\cline{1-4} \\cline{6-9}\n  1: & 1 & 0 & 1 & & 13: & 0 & 0 & 1\\\\\n  2: & 1 & 0 & 1 & &  14: & 0 & 0 & 0\\\\\n  3: & 1 & 0 & 0 & & 15: & 0 & 0 & 0\\\\\n  4: & 1 & 0 & 0 & & 16: & 0 & 0 & 0\\\\\n  5: & 1 & 0 & 0 & & 17: & 0 & 1 & 1\\\\\n  6: & 1 & 0 & 0 & &  18: & 0 & 1 & 1\\\\\n  7: & 1 & 0 & 0 & & 19: & 0 & 1 & 1\\\\\n  8: & 1 & 0 & 0 & & 20: & 0 & 1 & 1\\\\\n  9: & 1 & 1 & 1 & & 21: & 0 & 1 & 0\\\\\n  10: & 1 & 1 & 1 & & 22: & 0 & 1 & 0\\\\\n  11: & 1 & 1 & 1 & & 23: & 0 & 1 & 0\\\\\n  12: & 1 & 1 & 0 & &  24: & 0 & 1 & 0\\\\\n \\cline{1-4} \\cline{6-9}\n\\end{tabular}  \n\\end{table}\nThe most common non-causal fairness notion is total variation (TV), known as statistical parity, demographic parity, or risk difference. The total variation of $A=a_1$ on the outcome $Y=y$ with reference $A=a_0$ is defined using conditional probabilities as follows:\n\\begin{equation}\n\\label{eq:TV}\nTV_{a_1,a_0} (y) = \\pr(y \\mid a_1) - \\pr(y \\mid a_0)\n\\end{equation}\nIntuitively, $TV_{a_1,a_0} (y)$ measures the difference between the conditional distributions of $Y$ when we (passively) observe $A$ changing from $a_0$ to $a_1$. In the example of Table~\\ref{tab:hiringExample1}: $$TV = \\pr(Y=1 \\mid A=0) - \\pr(Y=1 \\mid A=1) = \\frac{5}{12} - \\frac{5}{12} = 0.$$ So according to $TV$, the predicted hiring decision is fair. The main limitation of $TV$ is its purely statistical nature which makes it unable to reflect the causal relationship between $A$ and $Y$, that is, it is insensitive to the mechanism by which data is generated and collected.  \nTotal effect ($TE$)~\\footnote{Known also as average causal effect ($ACE$).} is the causal version of $TV$ and is defined in terms of experimental probabilities as follows: \n\\begin{equation}\n\\label{eq:TE}\nTE_{a_1,a_0} (y) = \\pr(y_{a_1}) - \\pr(y_{a_0})\n\\end{equation}\n$TE$ measures the effect of the change of $A$ from $a_1$ to $a_0$ on $Y=y$ along all the causal paths from $A$ to $Y$. \nIntuitively, while $TV$ reflects the difference in proportions of $Y=y$ in the current cohort, $TE$ reflects the difference in proportions of $Y=y$ in the entire population. For the binary outcome case, $TE$ is equivalent to the average treatment effect ($ATE$)~ in the potential outcome framework which is defined as follows:\n\\begin{align}\n\\label{eq:ATE}\nATE_{a_1,a_0} & =  \\ep[Y^{a_1} - Y^{a_0}]  \\\\ \n& =  \\frac{1}{n} \\sum_{i=1}^{n}(Y_i^{a_1} - Y_i^{a_0})\n\\end{align}\nwhere $n$ is the number of observed samples. $ATE$ corresponds exactly to $FACE$ in~.\nComputing exactly $ATE$ requires the knowledge of both potential outcomes: the observed and the counterfactual. As the later is almost impossible to observe, exact computation of ATE is typically not possible. However, for the sake of illustration, we assume the counterfactual outcome is available, and show how ATE is computed. Later sections will show how $ATE$ and counterfactual outcomes can be estimated from observable data. Table~\\ref{tab:hiringExample2}, shows the same job hiring dataset, but with counterfactual outcomes.\n\\begin{table}[!h] \n\t\\centering\n\t\\caption{The job hiring example with counterfactual outcomes. $A^{cf}$ denotes the gender of the candidate in the counterfactual world. $Y^{cf}$ denotes the counterfactual potential outcome.}\n\\label{tab:hiringExample2} \n\\begin{tabular}{ccccaacccccaa}\n\\multicolumn{6}{c}{Female applicants} & & \\multicolumn{6}{c}{Male applicants} \\\\\n\\multicolumn{6}{c}{(Treatment group)} & & \\multicolumn{6}{c}{(Control Group)} \\\\\n$i$ & $A$ & $C$ & $Y$ & $A^{cf}$ & $Y^{cf}$ & & $i$ & $A$ & $C$ & $Y$ & $A^{cf}$ & $Y^{cf}$ \\\\ \\cline{1-6} \\cline{8-13}\n  1: & 1 & 0 & 1 & 0 & 1 & & 13: & 0 & 0 & 1 & 1 & 1\\\\\n  2: & 1 & 0 & 1 & 0 & 0 & &  14: & 0 & 0 & 0 & 1 & 1\\\\\n  3: & 1 & 0 & 0 & 0 & 1 & & 15: & 0 & 0 & 0 & 1 & 0\\\\\n  4: & 1 & 0 & 0 & 0 & 0 & & 16: & 0 & 0 & 0 & 1 & 0\\\\\n  5: & 1 & 0 & 0 & 0 & 0 & & 17: & 0 & 1 & 1 & 1 & 1\\\\\n  6: & 1 & 0 & 0 & 0 & 0 & &  18: & 0 & 1 & 1 & 1 & 1\\\\\n  7: & 1 & 0 & 0 & 0 & 0 & & 19: & 0 & 1 & 1 & 1 & 1\\\\\n  8: & 1 & 0 & 0 & 0 & 0 & & 20: & 0 & 1 & 1 & 1 & 1\\\\\n  9: & 1 & 1 & 1 & 0 & 1 & & 21: & 0 & 1 & 0 & 1 & 1\\\\\n  10: & 1 & 1 & 1 & 0 & 1 & & 22: & 0 & 1 & 0 & 1 & 0\\\\\n  11: & 1 & 1 & 1 & 0 & 0 & & 23: & 0 & 1 & 0 & 1 & 0\\\\\n  12: & 1 & 1 & 0 & 0 & 0 & &  24: & 0 & 1 & 0 & 1 & 0\\\\\n \\cline{1-6} \\cline{8-13}\n\\end{tabular}  \n\\end{table}\nATE is computed by considering the average potential outcome if the gender is female $A=1$, that is, $\\frac{1}{n} \\sum_{i=1}^{n}(Y_i^{1})$ and the same if the gender is male $A=0$, $\\frac{1}{n} \\sum_{i=1}^{n}(Y_i^{0})$. The former ($\\sum_{i=1}^{n}(Y_i^{1})$) corresponds to the average of the observed outcomes ($Y$) of samples $1$ to $12$ and counterfactual outcomes ($Y^{cf}$) of samples $13$ to $24$, which gives $\\frac{12}{24} = \\frac{1}{2}$. Similarly, the average potential outcome if gender is male corresponds to the counterfactual outcomes of samples $1$ to $12$ and the observed outcomes of samples $13$ to $24$ which gives $\\frac{9}{24} = \\frac{3}{8}$. Hence, $ATE = \\frac{1}{2} - \\frac{3}{8} = \\frac{1}{8}$ which indicates a positive bias for female. \nComputing the causal effect based only on the observed treatment group samples (e.g. female applicants only) corresponds to a variant of $TE$ called effect of treatment on the treated ($ETT$)~ and is defined as:\n\\begin{equation}\n\\label{eq:ETT}\nETT_{a_1,a_0} (y) = \\pr(y_{a_1} \\mid a_1) - \\pr(y_{a_0} \\mid a_1)\n\\end{equation}\nIn the binary outcome case, $ETT$ corresponds to the average treatment effect on the treated $ATT$~ in the potential outcome framework defined as:\n\\begin{align}\n\\label{eq:ATT}\nATT_{a_1,a_0} & = \\ep[Y^{a_1} | A=a_1] - \\ep[Y^{a_0} | A=a_1] \\\\\n& = \\frac{1}{n_1} \\sum_{i:A=a_1}(Y_i^{a_1} - Y_i^{a_0})\n\\end{align}\nwhere $n_1$ is the number of samples in the treatment group. $ATT$ is also called $FACT$ in~. \nIn the example of Table~\\ref{tab:hiringExample2}, $ATT$ corresponds to the difference between the average observable outcome ($Y$) and the average counterfactual outcome ($Y^{cf})$ in samples $1$ to $12$, that is, $ATT = \\frac{5}{12} - \\frac{4}{12} = \\frac{1}{12}$, which confirms the positive bias for female. \nAverage treatment effect on the control group ($ATC$)~ is the same as $ATT$ but focusing instead on the control group:\n\\begin{align}\n\\label{eq:ATC}\nATC_{a_1,a_0} & = \\ep[Y^{a_1} | A=a_0] - \\ep[Y^{a_0} | A=a_0] \\\\\n& = \\frac{1}{n_2} \\sum_{i:A=a_0}(Y_i^{a_1} - Y_i^{a_0})\n\\end{align}\nwhere $n_2$ is the number of samples in the control group.\nUsing the example of Table~\\ref{tab:hiringExample2}, $ATC = \\frac{7}{12} - \\frac{5}{12} = \\frac{1}{6}$. Conditional average treatment effect ($CATE$)~ is defined in a similar way, but conditioning on some other covariate instead of the sensitive attribute $A$:\n\\begin{align}\n\\label{eq:CATE}\nCATE_{a_1,a_0}(X=x) & = \\ep[Y^{a_1} | X=x] - \\ep[Y^{a_0} | X=x] \\\\\n& = \\frac{1}{n_x} \\sum_{i:X=x}(Y_i^{a_1} - Y_i^{a_0})\n\\end{align}\nwhere $n_x$ is the number of samples in the subgroup $X=x$. Using the covariate $C=0$ (flexible schedule jobs) in the hiring example of Tabel~\\ref{tab:hiringExample2}, $CATE(C=0) = \\frac{4}{12} - \\frac{3}{12} = \\frac{1}{12}$, which is again confirming hiring decisions in favor of female.\nUnlike the SCM framework, in the potential framework, it is possible to define individual treatment effect $ITE$~ which is defined, for every unit $i$ as:\n\\begin{equation}\n\\label{eq:ITE}\nITE_{a_1,a_0}(i) = Y_i^{a_1} - Y_i^{a_0}\n\\end{equation}\nFor instance, in Table~\\ref{tab:hiringExample2}, $ITE(i=3) = 0 - 1 = -1$ which indicates a discrimination against the female applicant $i=3$. $ATC$, $CATE$, and $ITE$ are defined and typically used in the potential outcome framework but have no equivalents in the SCM framework. However, although $ATC$ and $CATE$ can be easily represented in the SCM formalism, $ITE$ cannot be easily formalized in the SCM framework.\nThe job hiring example of Tables~\\ref{tab:hiringExample1} and~\\ref{tab:hiringExample2} is interesting because it illustrates a statistical anomaly where some statistical notions such as $TV$ fail to appropriately account for the bias between sub-populations (e.g. female vs male). Notice first that, according to the collected data, both female and male candidates are hired at the same rate $\\frac{5}{12}$. Notice also that if the hiring rates are adjusted according to the job type, female candidates are hired at an equal or higher rate for both types of jobs: for flexible schedule jobs  ($C=0$), the hiring rates are the same $\\frac{1}{4}$ and for non-flexible jobs ($C=1$), the hiring rates are $\\frac{3}{4}$ for female and $\\frac{4}{8} = \\frac{1}{2}$ for male. The explanation for such counter-intuitive result is that most of female candidates ($8$ out of $12$) are applying for flexible schedule jobs (for family reasons) in which hiring is more difficult. On the other hand, few male candidates ($4$ out of $12$) are applying for flexible schedule jobs, and instead massively applying for the more accessible non-flexible jobs ($8$ out of $12$ applicants). To appropriately assess discrimination in this case, there is a need to adjust on the job type variable $C$, that is, assessing discrimination for each job type separately. This simple job hiring scenario is similar to the Berkeley sex discrimination in college admission~ where data showed a bias for male applicants overall, but when results were analyzed separately for each department, data showed a slight bias in favor of female candidates. The Berkeley scenario is typically used as an example of Simpson's paradox~. In both scenarios, \nby considering the outcome of the observable samples in the counterfactual setup, the above causal-based fairness notions could appropriately assess gender ($A$) discrimination on the outcome ($Y$). \nThe job hiring example illustrating the statistical anomaly can be easily modified to reflect a Simpson's paradox~. Table~\\ref{tab:hiringExample3} shows the same example but with 30 observed samples. In such cohort, $TV = -\\frac{1}{6}$ indicates a discrimination against female applicants. However, all causal notions ($TE$ = $ATE$ = $\\frac{1}{3}$, $ATT = \\frac{1}{3}$, and $ATC = \\frac{1}{3}$, $CATE(C=0) = \\frac{1}{10}$, and $CATE(C=1) = \\frac{2}{10}$) are indicating a bias in favor of female.\n\\begin{table}[!h] \n\t\\centering\n\t\\caption{The job hiring example with a Simpson's paradox.}\n\\label{tab:hiringExample3} \n\\begin{tabular}{ccccaacccccaa}\n\\multicolumn{6}{c}{Female applicants} & & \\multicolumn{6}{c}{Male applicants} \\\\\n\\multicolumn{6}{c}{(Treatment group)} & & \\multicolumn{6}{c}{(Control Group)} \\\\\n$i$ & $A$ & $C$ & $Y$ & $A^{cf}$ & $Y^{cf}$ & & $i$ & $A$ & $C$ & $Y$ & $A^{cf}$ & $Y^{cf}$ \\\\ \\cline{1-6} \\cline{8-13}\n  1: & 1 & 0 & 1 & 0 & 1 & & 16: & 0 & 0 & 1 & 1 & 1\\\\\n  2: & 1 & 0 & 1 & 0 & 1 & & 17: & 0 & 0 & 0 & 1 & 1\\\\\n  3: & 1 & 0 & 1 & 0 & 0 & & 18: & 0 & 0 & 0 & 1 & 0\\\\\n  4: & 1 & 0 & 0 & 0 & 0 & & 19: & 0 & 0 & 0 & 1 & 0\\\\\n  5: & 1 & 0 & 0 & 0 & 0 & & 20: & 0 & 0 & 0 & 1 & 0\\\\\n  6: & 1 & 0 & 0 & 0 & 0 & & 21: & 0 & 1 & 1 & 1 & 1\\\\\n  7: & 1 & 0 & 0 & 0 & 0 & & 22: & 0 & 1 & 1 & 1 & 1\\\\\n  8: & 1 & 0 & 0 & 0 & 0 & & 23: & 0 & 1 & 1 & 1 & 1\\\\\n  9: & 1 & 0 & 0 & 0 & 0 & & 24: & 0 & 1 & 1 & 1 & 1\\\\\n  10: & 1 & 0 & 0 & 0 & 0 & & 25: & 0 & 1 & 1 & 1 & 1\\\\\n  11: & 1 & 1 & 1 & 0 & 1 & & 26: & 0 & 1 & 1 & 1 & 1\\\\\n  12: & 1 & 1 & 1 & 0 & 1 & & 27: & 0 & 1 & 1 & 1 & 1\\\\\n  13: & 1 & 1 & 1 & 0 & 1 & & 28: & 0 & 1 & 0 & 1 & 1\\\\\n  14: & 1 & 1 & 1 & 0 & 0 & & 29: & 0 & 1 & 0 & 1 & 0\\\\\n  15: & 1 & 1 & 0 & 0 & 0 & &  30: & 0 & 1 & 0 & 1 & 0\\\\\n \\cline{1-6} \\cline{8-13}\n\\end{tabular}  \n\\end{table}\nAll the above causal-based fairness notions fall into the framework of disparate impact~ which aims at ensuring the equality of outcomes among all groups (protected/treatment and unprotected/control). An alternative framework is the disparate treatment~ which seeks equality of treatment achievable through prohibiting the use of the sensitive attribute in the decision process. The main idea is to split the causal effect between the sensitive attribute $A$ and the outcome $Y$ into several causal pathways, each of which is either fair, unfair, or spurious.\nCommon fairness notions from the disparate treatment framework include direct effect, indirect effect, and path-specific effect~. An effect can be deemed fair, unfair, or spurious by an expert of the scenario at hand. Unfair effect is called discrimination. Direct discrimination is assessed using causal effect along direct edge from $A$ to $Y$. Indirect discrimination is measured using the  causal effect along causal paths that pass through proxy attributes\\footnote{A proxy is an attribute that cannot be objectively justified if used in the decision making process. It is Known also as redlining attribute.}. A fair or explainable discrimination is measured using causal pathways passing through explaining variables. Spurious effect corresponds to a pathway starting with an incident edge into the sensitive attribute $A$.\n\\begin{figure}[!h]\n\t\\centering\n\t{\\includegraphics[scale=0.3]{./Graphics/hiring_Example_Toy.eps}}\n\t\\caption{Job hiring scenario where $A$ is gender, $Y$: hiring decision, $R$: hobby of a candidate ($R=1$ for mechanical hobby, $R=0$ for non-mechanical hobby), and $E$: education level of the candidate ($E=1$ for college degree, $E=0$ for no college degree).}\n\t\\label{fig:hiringExampleGraph1}\n\\end{figure}\nFigure~\\ref{fig:hiringExampleGraph1} presents a causal graph of the job hiring scenario involving an explaining variable $E$ (e.g. education and academic degrees), and a proxy/redlining variable $R$ (e.g. the hobby of the candidate). Hiring discrimination due to education level is legitimate and considered fair, whereas a discrimination due to the hobby of the candidate is unfair as it is a proxy for the gender (the type of hobby indicates generally the gender of the candidate). Direct effect can be computed by simply ``blocking'' all indirect causal paths. An indirect causal path is a directed path from $A$ to $Y$ going through one or several mediator variables. For example, in Figure~\\ref{fig:hiringExampleGraph1}, there are two indirect causal paths $A\\;\\longrightarrow\\;R\\;\\longrightarrow\\;Y$ and $A\\;\\longrightarrow\\;E\\;\\longrightarrow\\;Y$. To compute the direct causal effect ($A\\;\\longrightarrow\\;Y$), both indirect causal paths need to be blocked by adjusting on variables $R$ and $E$. As there are no confounders, the direct effect can be simply computed as:\n\\begin{align}\nDE_{a_1,a_0} (y) & = \\pr(y\\;|\\;a_1,R,E) - \\pr(y\\;|\\;a_0,R,E) \\nonumber\\\\\n                & = \\sum_{r}\\sum_{e}\\left(\\pr(y\\;|\\;a_1,r,e) - \\pr(y\\;|\\;a_0,r,e)\\right) \\nonumber\n\\end{align}\nIn presence of confounders (between $A$ and $Y$, between $R$ and $Y$, etc.), natural direct effect ($NDE$)~ is a more general notion that measures the direct causal effect and is defined as: \n\\begin{equation}\n\\label{eq:NDE}\nNDE_{a_1,a_0} (y) = \\pr(y_{{a_1},\\mathbf{Z}_{a_0}}) - \\pr(y_{a_0})\n\\end {equation}\nWhere $\\mathbf{Z}$ is the set of mediator variables and $\\pr(y_{{a_1},\\mathbf{Z}_{a_0}})$ is the probability of $Y=y$ had $A$ been $a_1$ and had $\\mathbf{Z}$ been the value it would naturally take if $A=a_0$. That is, $A$ is set to $a_1$ in the single direct path $A \\rightarrow Y$ and is set to $a_0$ in all other indirect paths ($A\\rightarrow R \\rightarrow Y$ and $A \\rightarrow E \\rightarrow Y$). \nTo see how $NDE$ is computed, consider the sample dataset in Table~\\ref{tab:hiringExample4} corresponding to the causal graph in Figure~\\ref{fig:hiringExampleGraph1}. Similarly to the previous examples, we assume the counterfactual values are available (grayed columns).\n\\begin{table}[!h] \n\t\\centering\n\t\\caption{A job hiring scenario corresponding to the causal graph in Figure~\\ref{fig:hiringExampleGraph1}.}\n\\label{tab:hiringExample4} \n\\begin{tabular}{p{0.1cm}p{0.1cm}p{0.1cm}p{0.1cm}p{0.1cm}aaaeaaee}\n$i$ & $A$ & $E$ & $R$ & $Y$ & $Y^{cf}$ & $E_{0}$ & $R_{0}$ & $Y_{1,E_{0},R_{0}}$ & $E_{1}$ & $R_{1}$ & $Y_{0,E_{1},R_{1}}$ & $Y_{1,E_{0},R_{1}}$ \\\\ \\hline \n  1: & 1 & 1 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 1\\\\\n  2: & 1 & 1 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 1\\\\\n  3: & 1 & 1 & 1 & 0 & 1 & 0 & 1 & 1 & 1 & 1 & 1 & 0\\\\\n  4: & 1 & 0 & 0 & 1 & 1 & 1 & 0 & 1 & 0 & 0 & 1 & 1\\\\\n  5: & 1 & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 1 & 1\\\\\n  6: & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  7: & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 1\\\\\n  8: & 0 & 1 & 0 & 1 & 1 & 1 & 0 & 1 & 1 & 0 & 1 & 1\\\\\n  9: & 0 & 1 & 1 & 1 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 0\\\\\n  10: & 0 & 0 & 1 & 1 & 1 & 0 & 1 & 1 & 1 & 0 & 1 & 1\\\\\n  11: & 0 & 0 & 1 & 0 & 1 & 0 & 1 & 1 & 0 & 0 & 1 & 0\\\\\n  12: & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1\\\\\n \\hline\n\\end{tabular}  \n\\end{table}\nThe cohort consists of $6$ female candidates and $6$ male candidates. $Y^{cf}$ is the counterfactual potential outcome (the gender is different from the observed sample). $E_0$ is the education level had the gender was male. $R_0$ is the hobby of the candidate had the gender was male. $Y_{1,E_{0},R_{0}}$ is the hiring decision had (1) the gender was female and (2) the education and hobby were set to the values if the candidate was male. According to Equation~\\ref{eq:NDE}, $NDE_{1,0}(y=1) = \\pr(y_{1,E_{0},R_{0}}) - \\pr(y_0) = \\frac{8}{12} - \\frac{9}{12} = -\\frac{1}{12}$ which indicates a direct discrimination against female candidates.\nNotice the following:\n\\begin{itemize}\n    \\item For rows $7$ to $12$, the values of columns $E_0$, $R_0$, and $Y_{1,E_{0},R_{0}}$ are equal to the values in columns $E$, $R$, and $Y^{cf}$, respectively.\n    \\item $\\pr(y_0)$ is computed based on the values in rows $1$ to $6$ of column $Y^{cf}$ and rows $7$ to $12$ of column $Y$.\n\\end{itemize}\nNatural indirect effect ($NIE$)~ measures the indirect effect of $A$ on $Y$ and is defined as:\n\\begin{equation}\n\\label{eq:NIE}\nNIE_{a_1,a_0} (y) = \\pr(y_{{a_0},\\mathbf{Z}_{a_1}}) - \\pr(y_{a_0})\n\\end {equation}\nIn the example of Table~\\ref{tab:hiringExample4}, \n$NIE_{1,0}(y=1) = \\pr(y_{0,E_{1},R_{1}}) - \\pr(y_0) = \\frac{9}{12} - \\frac{9}{12} = 0$\nThe problem with $NIE$ is that it does not distinguish between the fair (explainable) and unfair (indirect discrimination) effects. Path-specific effect~ is a more nuanced measure that characterizes the causal effect in terms of specific paths.\nGiven a path set $\\pi$, the $\\pi$-specific effect is defined as: \n\\begin{equation}\n\\label{eq:PSE}\nPSE^{\\pi}_{a_1,a_0}(y) = \\pr(y_{a_1 |_\\pi, a_0 |_{\\overline{\\pi}}}) - \\pr(y_{a_0})\n\\end{equation}\nwhere $\\pr(y_{a_1 |_\\pi, a_0 |_{\\overline{\\pi}}})$ is the probability of $Y=y$ in the counterfactual situation where the effect of $A$ on $Y$ with the intervention ($a_1$) is transmitted along $\\pi$, while the effect of $A$ on $Y$ without the intervention ($a_0$) is transmitted along paths not in $\\pi$ (denoted by: $\\overline{\\pi}$). Using the job hiring example of Figure~\\ref{fig:hiringExampleGraph1}, Eq.~\\ref{eq:PSE} can be used to assess only unfair discrimination which is transmitted through the direct path $A\\rightarrow Y$ and the indirect path $A\\rightarrow R \\rightarrow Y$. The third path $A \\rightarrow E \\rightarrow Y$ transmits explainable (fair) discrimination, and hence, should not be considered. Given $\\pi = \\{A\\rightarrow Y, \\;\\;A\\rightarrow R \\rightarrow Y\\}$, $PSE^{\\pi}_{1,0} = \\pr(Y_{1,E_{0},R_{1}}) - \\pr(y_0) = \\frac{8}{12} - \\frac{9}{12} = - \\frac{1}{12}$ which indicates a discrimination against female candidates.", "cites": [4442, 4444, 8788, 4443], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates ideas from multiple papers by discussing causal-based fairness notions such as TE, ATE, ETT, and CATE, and contrasts them with non-causal measures like TV. It provides a clear analytical example using a job hiring dataset to demonstrate how these concepts differ in practice. However, the synthesis remains somewhat focused on individual concepts without a broader unifying framework, and the critical analysis is limited to pointing out general limitations rather than evaluating trade-offs or deeper conceptual issues."}}
{"id": "8fc69f92-cfb2-4c9f-bf30-2f54a18fd0e1", "title": "No unresolved discrimination", "level": "subsection", "subsections": [], "parent_id": "023451fb-47da-4f07-a008-9e8611e0a373", "prefix_titles": [["title", "Survey on Causal-based Machine Learning Fairness Notions"], ["section", "Causality-based Fairness notions"], ["subsection", "No unresolved discrimination"]], "content": "\\label{sec:nounresolved}\nNo unresolved discrimination~ is \na fairness notion that falls into the disparate treatment framework and focuses on  the indirect causal effects from $A$ to $Y$. \nNo unresolved discrimination is satisfied when no directed path from $A$ to $Y$ is allowed, except via a resolving (explaining) variable $E$. A resolving variable is any variable in a causal graph that is influenced by the sensitive attribute in a manner that is accepted as nondiscriminatory. Figure~\\ref{fig:unresolved} presents two alternative causal graphs for the job hiring example. The graph at the left exhibits unresolved discrimination along the heavy paths: $A \\rightarrow R \\rightarrow Y$ and $A \\rightarrow Y$. By contrast, the graph at the right does not exhibit any unresolved discrimination as the effect of $A$ on $Y$ is justified by the resolved variable $E$: $ A \\rightarrow E \\rightarrow Y$. \n\\begin{figure}[!h]\n\t\\centering\n\t\\subfigure{\n\t{\\includegraphics[scale=0.2]{./Graphics/Unresolved.eps}}\n\t\\label{fig:unresolved1}}\n\t\\qquad \\qquad\n\t\\subfigure{\n\t{\\includegraphics[scale=0.2]{./Graphics/Unresolved2.eps}}\n\t\\label{fig:unresolved2}}\n\t\\caption{$Y$ exhibits unresolved discrimination in the left graph (along the heavy paths), but not the right one.}\n\t\\label{fig:unresolved}\n\\end{figure}\nThe use of no unresolved discrimination in real scenarios is limited by the assumption of valid causal graph availability.  provide a formal proof that even with prior knowledge of resolving variables, it is not always possible to tell, based on observational data only, if a predictor $Y$ satisfies no unresolved discrimination.", "cites": [3916], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear explanation of the 'no unresolved discrimination' concept and its relation to the disparate treatment framework, drawing on the cited paper to frame the importance of causal reasoning in fairness. It identifies a key limitation—reliance on valid causal graphs—which reflects a critical perspective, and begins to abstract the idea by discussing resolving variables. However, it does not go beyond a single source or develop a deeper synthesis or critique of the notion."}}
{"id": "b6922584-d008-4a9c-9f53-9d7ab685862d", "title": "No proxy discrimination", "level": "subsection", "subsections": [], "parent_id": "023451fb-47da-4f07-a008-9e8611e0a373", "prefix_titles": [["title", "Survey on Causal-based Machine Learning Fairness Notions"], ["section", "Causality-based Fairness notions"], ["subsection", "No proxy discrimination"]], "content": "\\label{sec:noproxy}\nSimilarly to no unresolved discrimination, no proxy discrimination~ focuses on indirect discrimination. A causal graph exhibits potential proxy discrimination if there exists a path from the protected attribute $A$ to the outcome $Y$ that is blocked by a proxy/redlining variable $R$. It is called proxy because it is used to decide about the outcome $Y$ while it is a descendent of $A$ which is significantly correlated with it in such a way that using the proxy in the decision has almost the same impact as using $A$ directly. An outcome variable $Y$ exhibits no proxy discrimination if the equality: \n\\begin{equation}\n\\label{eq:proxy}\n\\pr(Y\\mid do(R=r)) = \\pr(Y\\mid do(R=r')) \\quad \\forall \\; r, r'\\in dom(R)\n\\end{equation}\nholds for any potential proxy $R$.\nFigure~\\ref{fig:proxy} shows two similar causal graphs for the same job hiring example. The causal graph at the left presents a potential proxy discrimination via the path: $A \\rightarrow R \\rightarrow Y$. However, the graph at the right is free of proxy discrimination as the edge between $A$ and its proxy $R$ has been removed due to the intervention on $R$ ($R=r$).\n\\begin{figure}[!h]\n\t\t\\vspace{-2mm}\n    \\centering\n    \\subfigure {\n{\\includegraphics [scale=0.25]{./Graphics/proxy1.eps} }\n\\label{fig:proxy1}}\n    \\qquad \\qquad\n    \\subfigure {\n    {\\includegraphics[scale=0.25]{./Graphics/proxy2.eps} }\n    \\label{gig:proxy2}}\n    \\caption{The graph at the left exhibits a potential proxy discrimination (along the heavy edge between $A$ and $R$) but not in the right one.}\n    \\label{fig:proxy}\n    \t\t\\vspace{-2mm}\n\\end{figure}\nSimilarly to no unresolved discrimination, no proxy discrimination requires a valid causal graph. Hence, both fairness notions depend on the correct output of the causal discovery task.", "cites": [3916], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the concept of proxy discrimination from the cited paper and integrates it with related notions like 'no unresolved discrimination,' providing a coherent narrative. It offers some abstraction by framing the notion in general terms using a causal graph and equation, but its critical analysis is limited, focusing more on definition and implication than on evaluating the strengths or weaknesses of the approach."}}
{"id": "a2114640-6836-4226-b821-4c5d6eb1b09e", "title": "Counterfactual fairness", "level": "subsection", "subsections": [], "parent_id": "023451fb-47da-4f07-a008-9e8611e0a373", "prefix_titles": [["title", "Survey on Causal-based Machine Learning Fairness Notions"], ["section", "Causality-based Fairness notions"], ["subsection", "Counterfactual fairness"]], "content": "\\label{sec:counterfactual}\n\t\t\\vspace{-1mm}\nCounterfactual fairness~ is a very strong fairness notion that requires equality between the observed outcome and the counterfactual outcome for every individual. That is, an outcome $Y$ is counterfactually fair if under any assignment of values $\\mathbf{X}=\\mathbf{x}$ and any individual in $U$,\n\\begin{equation}\n\\label{eq:counterfactual}\n\\pr(y_{a_1}(U) \\mid \\mathbf{X} =  \\mathbf{x}, A =  a_0) = \\pr(y_{a_0}(U) \\mid \\mathbf{X} = \\mathbf{x}, A =  a_0) \n\\end{equation}\nwhere $\\mathbf{X} = \\mathbf{V}\\backslash\\{A,Y\\}$ is the set of all remaining variables. As the latent variable $U$ appears in  Equation~\\ref{eq:counterfactual}, counterfactual fairness is an individual fairness notion. It is satisfied if the probability distribution of the outcome $Y$ is the same in the actual and counterfactual worlds, for every possible individual. In practice, counterfactual fairness coincides typically with $ITE$ (Eq.~\\ref{eq:ITE}).\n could test counterfactual fairness by making a very strong assumption. That is, they assumed the full structure of the causal model is available including the latent variables $U$. They could then estimate the distribution of $\\pr(U)$ using Markov chain Monte Carlo methods and the observed data. Thanks to the estimated distribution of $\\pr(U)$, they could compute counterfactuals using Pearl's three-step process: abduction, action, and prediction~. Hence, for every individual in the population, another sample with counterfactual sensitive value is generated. Counterfactual fairness is finally assessed by comparing the density functions of the actual and counterfactual samples. The process of testing counterfactual fairness is detailed in Section~\\ref{subsec:ctf}.", "cites": [3905], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the key idea of counterfactual fairness from the cited paper, integrating the formal definition and its practical implications. It provides a clear analytical explanation of how counterfactual fairness can be tested, referencing Pearl's causal framework. However, it lacks deeper critical evaluation of the approach and does not extensively compare or contrast it with other fairness notions, limiting its critical and abstract insight."}}
{"id": "ce67418b-700b-4094-989e-fc1452431822", "title": "Counterfactual Error Rates", "level": "subsection", "subsections": [], "parent_id": "023451fb-47da-4f07-a008-9e8611e0a373", "prefix_titles": [["title", "Survey on Causal-based Machine Learning Fairness Notions"], ["section", "Causality-based Fairness notions"], ["subsection", "Counterfactual Error Rates"]], "content": "\\label{sec:cer}\nEqualized odds~ is an important statistical fairness notion which requires equality of error rates ($TPR$ and $FPR$) across sub-populations, that is, \n\\begin{equation}\n\tER_{a_1,a_0}(\\hat{y}|y) = \\pr(\\hat y \\mid a_1,y) - \\pr(\\hat y \\mid a_0,y) = 0 \\label{eq:eqodds}\n\\end{equation}\nwhere $\\hat{y}$ denotes the prediction while $y$ denotes the true outcome.\nThe problem of this statistical notion is the difficulty to identify the causes behind the discrimination if any.  decompose equalized odds (Eq.~\\ref{eq:eqodds}) using three counterfactual measures corresponding to the direct, indirect and spurious effects of $A$ on $\\hat{Y}$. The three measures are counterfactual direct error rate, counterfactual indirect error rate, and counterfactual spurious error rate. Let $\\hat{y} = f(\\hat{\\mathbf{pa}})$ be a classifier where $\\hat{\\mathbf{PA}}$ is the set of input features (parent variables of $\\hat{Y}$) for the classifier. The counterfactual error rates for a sub-population $a,y$ (with prediction $\\hat{y}\\neq y$) are defined as:\n\\begin{align}\n\\label{eq:cder}\n&ER^d_{a_1,a_0}(\\hat  y \\mid a,y) = \\pr(\\hat y_{a_1,y,(\\hat{\\textbf{PA}}\\setminus A)_{{a_0},y}}\\mid a,y) - \\pr(\\hat y_{{a_0},y}\\mid a,y))\\\\\n\\label{eq:cier}\n&ER^i_{a_1,a_0}(\\hat  y \\mid a,y) = \\pr(\\hat y_{a_0,y,(\\hat{\\textbf{PA}}\\setminus A)_{a_1,y}}\\mid a,y) - \\pr(\\hat y_{a_0,y}\\mid a,y))\\\\\n\\label{eq:cser}\n&ER^s_{a_1,a_0}(\\hat  y \\mid y) = \\pr(\\hat y_{{a_0},y} \\mid a_1,y) - \\pr(\\hat y_{{a_0},y}\\mid a_0,y)\n\\end{align}\nFor example, the counterfactual direct error rate (Eq.~\\ref{eq:cder}) measures the error rate (disparity between the true and the predicted outcome) in terms of the direct effects of the sensitive attribute $A$ on the prediction $\\hat{Y}$. In the job hiring example, considering the female sub-population that \\textit{should} be hired ($A=1$ and $Y=1$), it reads: for a female candidate that should be hired, how would the prediction $\\hat Y$ change had the candidate been a female ($A$ been $1$), while keeping all the other features $\\hat{\\textbf{PA}}\\setminus A$ at the level that they would attain had ``she was male'', compared to the prediction $\\hat Y$ she would receive had ``she was male'' and should have been hired?\n\\begin{table}[!h] \n\t\\centering\n\t\\caption{A job hiring scenario for counterfactual direct error rate $ER^{d}$ computation. $E_{0,1}$ is a short version of $E_{A=0,Y=1}$. $R_{0,1}$ means $R_{A=0,Y=1}$. $\\hat{Y}_{1,1,E_{0,1},R_{0,1}}$ means $\\hat{Y}_{A=1,Y=1,E_{0,1},R_{0,1}}$. $Y_{0,1}$ means $Y_{A=0,Y=1}$.}\n\\label{tab:hiringExample5} \n\\begin{tabular}{ccccccaaaa}\n$i$ & $A$ & $E$ & $R$ & $\\hat{Y}$ & $Y$ & $E_{0,1}$ & $R_{0,1}$ & $\\hat{Y}_{1,1,E_{0,1},R_{0,1}}$ & $\\hat{Y}_{0,1}$ \\\\ \\hline \n  1: & 1 & 1 & 0 & 1 & 1 & 1 & 0 & 1 & 1 \\\\\n  2: & 1 & 1 & 0 & 1 & 1 & 1 & 0 & 1 & 1 \\\\\n  3: & 1 & 1 & 1 & 0 & 1 & 0 & 1 & 1 & 1 \\\\\n  4: & 1 & 0 & 0 & 1 & 1 & 1 & 0 & 1 & 0 \\\\\n  5: & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n  6: & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n  7: & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n  8: & 0 & 1 & 0 & 1 & 1 & 1 & 0 & 1 & 1 \\\\\n  9: & 0 & 1 & 1 & 1 & 0 & 0 & 1 & 0 & 1 \\\\\n  10: & 0 & 0 & 1 & 1 & 1 & 0 & 1 & 0 & 0  \\\\\n  11: & 0 & 0 & 1 & 0 & 1 & 0 & 1 & 1 & 0 \\\\\n  12: & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 \\\\\n \\hline\n\\end{tabular}  \n\\end{table}\nTable~\\ref{tab:hiringExample5} shows the values (observed and counterfactual) needed to compute counterfactual direct error rate $ER^{d}$ for the female candidates that should be hired ($A=1$ and $Y=1$). \n\\begin{align}\nER^{d}(\\hat{Y}=1|A=1,Y=1)  & =  \\pr(\\hat{Y}_{A=1,Y=1,E_{0,1},R_{0,1}}|A=1,Y=1) \\nonumber \\\\ & - \\pr(\\hat{Y}_{A=0,Y=1}|A=1,Y=1) \\nonumber\n\\end{align}\nwhere $E_{0,1}$ is a short version of $E_{A=0,Y=1}$ which refers to the education level of the candidate had ``she'' been male and hired. $R_{0,1}$ means $R_{A=0,Y=1}$ and indicates the hobby of the candidate had ``she'' been male and hired.  $\\hat{Y}_{A=1,Y=1,E_{0,1},R_{0,1}}$ reads the hiring decision had the candidate was female, hired, with education $E_{0,1}$, and hobby $R_{0,1}$. $Y_{A=0,Y=1}$ reads the hiring decision had the candidate was male and hired. Using the values in Table~\\ref{tab:hiringExample5} (rows $1$ to $5$ in the last two columns), $ER^{d}(\\hat{Y}=1|A=1,Y=1) = \\frac{4}{5} - \\frac{3}{5} = \\frac{1}{5}$ which indicates a higher direct error rate for the female group.  \nInterestingly, the statistical equalized odd error rate (Eq.~\\ref{eq:eqodds}) can be decomposed in terms of the three above causal-based error rates:\n\\begin{equation}\n\\label{eq:expFormula_ER}\nER_{a_1,a_0} (\\hat y \\mid y) = ER^d_{a_1,a_0} (\\hat y \\mid a_0,y) - ER^i_{a_0,a_1} (\\hat y \\mid a_0,y) - ER^s_{a_0,a_1} (\\hat y \\mid y)\n\\end{equation}", "cites": [3899], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical explanation of counterfactual error rates by introducing and clarifying their definitions, and it uses an example from the cited paper to illustrate the concepts. It synthesizes the causal decomposition of equalized odds, but only one paper is cited, limiting cross-source integration. While it critiques the limitations of statistical fairness notions, it does not extensively compare or evaluate different approaches in the literature. The abstraction is moderate, as it identifies the idea of decomposing fairness disparities into causal components but does not generalize to broader principles or frameworks."}}
{"id": "510e91a2-6c41-41b5-b745-cf90a05f78c5", "title": "Non-Discrimination Criterion", "level": "subsection", "subsections": [], "parent_id": "023451fb-47da-4f07-a008-9e8611e0a373", "prefix_titles": [["title", "Survey on Causal-based Machine Learning Fairness Notions"], ["section", "Causality-based Fairness notions"], ["subsection", "Non-Discrimination Criterion"]], "content": "\\label{sec:ndc}\nNon-discrimination criterion~ is a group fairness notion that aims to discover and to quantify direct discrimination through the direct causal effect of $A$ on $Y$. Recall that, given a causal graph $G$, a direct effect of $A$ on $Y$ is the causal effect through the edge $A \\rightarrow Y$. The idea is to consider a modified graph $G'$ where the edge in question ($A \\rightarrow Y$) is discarded. A \\textit{block set} $\\mathbf{Q}$ is a set of variables which blocks all causal effects from $A$ to $Y$ in the modified graph $G'$. Hence, $A$ and $Y$ are independent conditioning on $Q$ in $G'$, that is, $(A \\perp Y | \\mathbf{Q})_{G'}$. Hence, conditioning on the same variables $\\mathbf{Q}$, any dependence between $A$ and $Y$ in $G$ is due to the direct effect of $A$ on $Y$ which indicates a direct discrimination. This discrimination can be assessed using simply the risk difference~:\n\\begin{equation}\n\\label{eq:ndc}\n\\mid\\;\\Delta P\\!\\!\\mid_{\\mathbf{q}}\\; \\mid = \\mid \\pr(y\\mid a_1, \\mathbf{q}) - \\pr(y\\mid a_0, \\mathbf{q}) \\mid\n\\end{equation}\nwhere $\\mathbf{q}$ is a value assignment for the block set $\\mathbf{Q}$ and the absolute value to consider both positive and negative discriminations. No direct discrimination can be concluded if the risk difference is less than a threshold $\\tau$ for all combinations of values of all block sets, that is, Eq.~\\ref{eq:ndc} holds for each value assignment $q$ of each block set $\\mathbf{Q}$.\n$NDE$ (Equation~\\ref{eq:NDE}) and counterfactual direct effect $DE$ (Section~\\ref{sec:ctf}) focus also on assessing the direct discrimination, but they both rely on nested counterfactual quantities which are not observable from data. Non-discrimination criterion circumvents this difficulty by using block sets and considering all combinations of values of these block sets. Similarly to individual direct discrimination, it can be considered as an estimation technique to avoid dealing with counterfactual quantities. This approach, however, does not work in semi-markovian models as $A$ and $Y$ will never be independent in $G'$ ($(A \\perp Y | \\mathbf{Q})_{G'}$) because of hidden counfounders.", "cites": [3920], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a clear analytical explanation of the Non-Discrimination Criterion, connecting it to the broader framework of causal fairness. It integrates the concept of block sets and contrasts this approach with others like NDE and DE, highlighting its practical advantages and limitations (e.g., in semi-Markovian models). While it offers a coherent narrative and identifies some key issues, it does not provide a deep comparative or meta-level analysis that would elevate the insight quality to a high level."}}
{"id": "b90e1479-2617-4ce0-811f-8137245636e9", "title": "Equality of Effort", "level": "subsection", "subsections": [], "parent_id": "023451fb-47da-4f07-a008-9e8611e0a373", "prefix_titles": [["title", "Survey on Causal-based Machine Learning Fairness Notions"], ["section", "Causality-based Fairness notions"], ["subsection", "Equality of Effort"]], "content": "\\label{sec:ftee}\nEquality of effort~ fairness notion identifies discrimination by assessing how much effort is needed by the disadvantaged individual/group to reach a certain level of outcome. \nA treatment variable $T$ is selected and used to address the question: ``to what extent the treatment variable $T$ should change to make the individual (or a group of individuals) achieve a certain outcome level?''. Hence, this notion focuses on whether the effort to reach a certain outcome level is the same for the protected and unprotected groups. Considering the simple job hiring example, the education level $E$ is a good choice for the treatment variable.\nTwo equality of effort notions are defined based on the potential outcome framework, individual $\\gamma$-Equal effort and system $\\gamma$-Equal effort. Let $Y^{(t)}_i$ be the potential outcome for individual $i$ had $T$ been $t$ and $\\ep[Y_i^{(t)}]$ be the expected outcome for individual $i$. Situation testing~ is used to estimate the counterfactual potential outcome in a similar way as individual direct discrimination (Section~\\ref{sec:idd}). Let $\\mathbf{S^+}$ and $\\mathbf{S^-}$ be the two sets of similar individuals with $A=a_0$ and $A=a_1$, respectively, and  $\\mathbf{E} [Y_{\\mathbf{S}^+}^{(t)}]$ be the expected outcome under treatment $t$ for the subgroup of individuals $\\mathbf{S^+}$. The minimal effort needed to achieve $\\gamma$-level of outcome variable within the subgroup $\\mathbf{S^+}$ is defined as:\n\\begin{equation}\n\t\\quad \\Psi _{\\mathbf{S}^+} (\\gamma) =\\argmin{t \\in T} \\{ \\ep[Y_{\\mathbf{S}^+}^{(t)}] \\geq \\gamma \\}\n\\end{equation}\nIndividual $\\gamma$-Equal effort is satisfied for individual $i$ if:\n\\begin{equation}\n\\label{eq:effil}\n\\Psi _{\\mathbf{S^+}} (\\gamma) = \\Psi _{\\mathbf{S^-}} (\\gamma)\n\\end{equation}\nSystem $\\gamma$-Equal effort is satisfied for a sub-population (e.g. $A=a_1$) if:\n \\begin{equation}\n\\label{eq:effil2}\n\\Psi _{\\mathbf{D^+}} (\\gamma) = \\Psi _{\\mathbf{D^-}} (\\gamma)\n\\end{equation}\nwhere $\\mathbf{D}^+$ and $\\mathbf{D}^-$ are the subsets of the entire dataset with sensitive attributes $a_0$ and $a_1$, respectively. Both criteria can be used to measure the effort discrepancy between protected and unprotected groups by considering the difference $\\Psi_{X^{+}}(\\gamma) - \\Psi_{X^{-}}(\\gamma)$. Unlike most of causal-based fairness notions who intervene ($do$ operator) on the sensitive attribute $A$ ($y_a$, $Y^{a}_i$, etc.), equality of effort intervenes instead on a treatment variable $T$ ($Y^{(t)}_i$). The main limitation of equality of effort notion is that, typically, a single treatment variable does not appropriately reflect the discrepancy between protected and unprotected groups.", "cites": [4445], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the concept of Equality of Effort from the cited paper, providing definitions for both individual and system-level fairness. It offers some critical analysis by noting that a single treatment variable may not fully capture group discrepancies, but it does not deeply compare or contrast this notion with others. The section abstracts the concept to a general framework, yet the level of generalization remains moderate without strong meta-level insights."}}
{"id": "f2f4b09a-ad5a-4568-9dd1-8df80615ee52", "title": "Individual equalized counterfactual odds", "level": "subsection", "subsections": [], "parent_id": "023451fb-47da-4f07-a008-9e8611e0a373", "prefix_titles": [["title", "Survey on Causal-based Machine Learning Fairness Notions"], ["section", "Causality-based Fairness notions"], ["subsection", "Individual equalized counterfactual odds"]], "content": "\\label{sec:IndECOD}\nIndividual equalized counterfactual odds~ is a stronger version of counterfactual fairness (Section~\\ref{sec:counterfactual}) requiring, in addition, that the factual-counterfactual pair share the same value of the outcome $Y$. The aim is to have a counterfactual version of equalized odds~. This is achieved by conditioning both sides of Eq.~\\ref{eq:counterfactual} on the same outcome $Y=y$. \nA predictor satisfies individual equalized counterfactual odds if:\n\\begin{equation}\n    \\label{eq:IndECOD}\n\\pr(\\hat{y}_{a_1} \\mid \\mathbf{X} =  \\mathbf{x}, y_{a_1} , A =  a_0) = \\pr(\\hat{y}_{a_0} \\mid \\mathbf{X} = \\mathbf{x}, y_{a_0} , A =  a_0) \n\\end{equation}\nThe only difference with Eq.~\\ref{eq:counterfactual} is the additional conditioning $Y = y_{a_1}$ in the LHS and $Y = y_{a_0}$ in the RHS. The only other causal-based fairness notions considering the outcome $Y$ are counterfactual error rates (Section~\\ref{sec:ctf}). However, unlike counterfactual error rates, individual equalized counterfactual odds requires intervention on $Y$. This is the only fairness criterion that requires intervention on the prediction $\\hat{Y}$ and on the actual outcome $Y$.", "cites": [3899, 4446], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear definition of individual equalized counterfactual odds and links it to related concepts like counterfactual fairness and equalized odds. It synthesizes two papers to frame the notion and identifies its distinguishing feature—intervention on both prediction and actual outcome. However, it lacks deeper critical evaluation or broader abstraction, and primarily focuses on analytical positioning within the causal fairness framework."}}
{"id": "b1998d61-6f78-4a86-a6cb-3b833ddeeb50", "title": "Identifiability", "level": "subsection", "subsections": ["08d935b2-0fbd-4e8f-818e-860a2f9875ff", "d3f43314-4b12-4720-8aec-777cea54a9fc", "0936fe42-d5bb-414b-af21-4dc066b4ba3f", "16812128-248f-40fe-9ab3-25e9f13a3633"], "parent_id": "910f06e9-1228-4aea-bb76-b79d05cfc6c4", "prefix_titles": [["title", "Survey on Causal-based Machine Learning Fairness Notions"], ["section", "Computing causal quantities from observable data"], ["subsection", "Identifiability"]], "content": "\\label{sec:ident}\nThe identifiability of causal quantities has been extensively studied in the literature: causal effect (intervention) identifiability~, counterfactual identifiability~, direct/indirect effects~ and path-specific effect identifiability~.\nThis section summarizes the main identifiability conditions as they relate to the specific problem of discrimination discovery.", "cites": [4444, 3898, 8790, 4447, 8789, 4441, 8791], "cite_extract_rate": 0.4375, "origin_cites_number": 16, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section lists various types of causal identifiability but does so in a largely descriptive manner without integrating the cited works into a coherent narrative. It lacks critical evaluation of methods or limitations and does not generalize to broader causal inference patterns or principles in the context of fairness."}}
{"id": "08d935b2-0fbd-4e8f-818e-860a2f9875ff", "title": "Identifiability of causal effect (intervention)", "level": "subsubsection", "subsections": [], "parent_id": "b1998d61-6f78-4a86-a6cb-3b833ddeeb50", "prefix_titles": [["title", "Survey on Causal-based Machine Learning Fairness Notions"], ["section", "Computing causal quantities from observable data"], ["subsection", "Identifiability"], ["subsubsection", "Identifiability of causal effect (intervention)"]], "content": "\\label{identInterv}\nThe causal effect of a cause variable $X$ on an effect variable $Y$ is computed using $\\pr(Y_x) = \\pr (Y|do(X=x))$, the distribution of $Y$ after the intervention $X=x$. In discrimination setup, the cause is typically the sensitive attribute $A$. \nA basic case where identifiability can be avoided altogether is when it is possible to perform experiments by intervening on the sensitive attribute $A$. When this is possible, randomized controlled trial (RCT)~ can be used to estimate the causal effect. RCT consists in randomly assigning subjects (e.g., individuals) to treatments (e.g., gender), then comparing the outcome $Y$ of all treatment groups. \nHowever, in the context of machine learning fairness, RCT is often not an option as experiments can be too costly to implement, physically impossible, or ethically not acceptable (e.g., changing the gender of a job applicant).\nIn Markovian models (no unobserved confounding), the causal effect is always identifiable (Corollary 3.2.6 in~). The simplest case is when there is no confounding between $A$ and $Y$ (Figure~\\ref{subfig:fig33_a}). In that case, the causal effect matches the conditional probability regardless of any mediator:\n\\begin{equation}\n\t\\pr(y_a) = \\pr(y|do(a)) = \\pr(y|a) \\label{eq:int1}\n\\end{equation}\n\\begin{figure}[!h]\n    \\subfigure [Simple Markovian model with a collider $(W)$ and a mediator $(Z)$.]  {\n    {\\includegraphics [scale=0.2]{./Graphics/fig33_a.eps} }\n    \\label{subfig:fig33_a}}\n    \\quad \n    \\subfigure [Simple Markovian model with a confounder $(C)$.] {\n    {\\includegraphics[scale=0.2]{./Graphics/fig33_b.eps} }\n    \\label{subfig:fig33_b}}\n     \\quad \n    \\subfigure [Simple Markovian model with a confounder $(C)$ and a mediator $(Z)$.]  {\n    {\\includegraphics [scale=0.2]{./Graphics/fig33_c.eps} }\n    \\label{subfig:fig33_c}}\n    \\caption{Simple causal graphs}\n    \\label{fig:fig33}\n\\end{figure}\nIn presence of an observable confounder (Figure~\\ref{subfig:fig33_b}), $\\pr(y_a)$ is identifiable by adjusting on the confounder:\n\\begin{equation}\n\t\\pr(y_a) = \\sum_{C}\\pr(y|a,c)\\;\\pr(c)  \\label{eq:int2}\n\\end{equation}\nwhere the summation is on values $c$ in the domain (sample space) of $C$ denoted as $dom(C)$. Eq.~\\ref{eq:int2} is called the back-door formula\\footnote{Called also adjustment formula or stratification.}. \nThe backdoor adjusting formula is different from the joint probability \\[ \\pr(y,a,c) = \\pr(y|a,c)\\;\\pr(a|c)\\;\\pr(c)\\] and the conditional probability \\[ \\pr(y|a) = \\sum_{C}\\pr(y|a,c)\\;\\pr(c|a)\\].\nFor semi-Markovian models, identifiability of $\\pr(y_a)$ is not guaranteed. In case it is identifiable, Pearl~ proposes a $do$-calculus composed of three rules allowing to express interventional probabilities in terms of observational ones:\n\\begin{enumerate}\n\t\\item $\\pr(y_a|\\mathbf{z},w) = \\pr(y_a|\\mathbf{z})$ provided that the set of variables $\\mathbf{Z}$ blocks all backdoor paths from $W$ to $Y$ after all arrows leading to $A$ have been deleted.\n\t\\item $\\pr(y_a|\\mathbf{z}) = \\pr(y|a,\\mathbf{z})$ provided that the set of variables $\\mathbf{Z}$ blocks all backdoor paths from $A$ to $Y$. \n    \\item $\\pr(y_a) = \\pr(y)$ provided that there are no causal paths between $A$ and $Y$.\n\\end{enumerate}\n$do$-calculus has been proven to be sound and complete in the identification of interventional distributions~. \nFor example, $\\pr(y_a)$ is identifiable in Figure~\\ref{subfig:ident_d}. By applying the chain rule following the topological order: $W_2<A<W_1<W_3<Y$, we get:\n\\begin{align}\n     \\label{eq:do1}\n     \\pr(y_a)  & = \\sum_{w_1 w_2 w_3} \\pr(y|do(a),w_1,w_2,w_3)\\;\\pr(w_1|do(a),w_2)\\;\\pr(w_2) \\nonumber\\\\\n             & \\qquad \\qquad \\times \\pr(w_3|w_2,w_1,do(a)) \\\\\n     \\label{eq:do2}\n             & = \\sum_{w_1 w_2} \\pr(y|do(a),w_1,w_2)\\;\\pr(w_1|do(a),w_2)\\;\\pr(w_2)\\\\\n     \\label{eq:do3}\n             & = \\sum_{w_1 w_2} \\pr(y|do(a),w_2)\\;\\pr(w_1|a,w_2)\\;\\pr(w_2)\\\\\n     \\label{eq:do4}\n             & = \\sum_{w_1 w_2} \\sum_{a'} \\pr(y|a',w_2,do(w_1))\\;\\pr(a'|do(w_1),w_2) \\nonumber\\\\\n             & \\qquad \\qquad \\times \\pr(w_1|a,w_2)\\;\\pr(w_2)\\\\\n     \\label{eq:do5}\n             &= \\sum_{w_1'}\\; \\sum_{w_2'}\\; \\sum_{a'}\\; \\pr(y|w_1',w_2',a')\\;\\pr(a'|w_2') \\; \\pr(w_1'|w_2',a)\\;\\pr(w_2')\n\\end{align}\nNote that $w_3$ is omitted from (\\ref{eq:do2}) since it is considered latent~. Applying Rule 2 followed by Rule 3 to the first term in~(\\ref{eq:do2}) yields to $\\pr(y|do(a),w_2)$ (\\ref{eq:do3}). Likewise, applying Rule 2 to the second term in ~(\\ref{eq:do2}) leads to $\\pr P(w_1|a,w_2)$. Thus, the original problem reduces to identifying the term $\\pr(y|do(a),w_2)$ in~(\\ref{eq:do3}). Here we cannot apply Rule 2 to exchange $do(a)$ with $a$ because $G_{\\underline{A}}$ (graph obtained by removing all emanating arrows from $A$) contains a backdoor path from $A$ to $Y$. Thus, to block that path, we need to condition and to sum over all values of $A$ as shown in Eq. (\\ref{eq:do4}) ($\\sum_{a'}\\;\\pr(y|a',w_2,do(w_1))\\\\ \\pr(a'|do(w_1),w_2$)). Now, applying Rule 2 to $\\pr(y|a',w_2,do(w_1))$ and Rule 3 to $\\pr(a'|do(w_1),w_2)$ and adding the other terms results in the final expression in (\\ref{eq:do5}). The problem of $do$-calculus is the difficulty to determine the correct order of application of the rules. Using the wrong order may hinder the identifiability or produce a very complex expression~. As an alternative to using the do-calculus, several contributions in the identifiability literature focused on defining graphical patterns and mapping them to simple and concise intervention-free expressions~.\n\\begin{figure}[!h]\n    \\subfigure []  {\n    {\\includegraphics [scale=0.22]{./Graphics/ident_a.eps} }\n    \\label{subfig:ident_a}}\n    \\qquad \n    \\subfigure [] {\n    {\\includegraphics[scale=0.25]{./Graphics/ident_b.eps} }\n    \\label{subfig:ident_b}}\n     \\qquad \n    \\subfigure []  {\n    {\\includegraphics [scale=0.25]{./Graphics/ident_c.eps} }\n    \\label{subfig:ident_c}}\n    \\subfigure []  {\n    {\\includegraphics [scale=0.25]{./Graphics/ident_d.eps} }\n    \\label{subfig:ident_d}}\n    \\qquad \\quad\n    \\subfigure []  {\n    {\\includegraphics [scale=0.25]{./Graphics/ident_e.eps} }\n    \\label{subfig:ident_e}}\n    \\vspace{-2mm}\n    \\caption{Figure~\\ref{subfig:ident_a} presents the``bow\" graph, Figure~\\ref{subfig:ident_b} illustrates the structure of a c-tree, Figure~\\ref{subfig:ident_c} shows a semi-Markovian model where $\\pr P(y_a)$ is observable, Figure~\\ref{subfig:ident_d} presents a semi-Markovian model where $\\pr P(y_a)$ is identifiable and Figure~\\ref{subfig:ident_e} illustrates a simple example of the front-door criterion.}\n    \\label{fig:ident}\n\\end{figure}\nAll graphical criteria can be generalized to the case where the sensitive attribute is not connected to any of its children through a confounding path. In such case, c-component factorization can be used. A c-component is a set of vertices in the graph such that every pair of vertices are connected by a confounding edge. The idea of c-component factorization is to decompose the identification problem into smaller sub-problems, that is, a disjoint set of c-components in order to calculate $\\pr(y_a)$. For example, in Figure~\\ref{subfig:ident_c}, there are three c-components: $\\{\\{W1,W2\\},\\{A\\}, \\{Z1,Z2,Y\\}\\}$. Hence, as long as there is no confounding path connecting $A$ to any of its direct children, $\\pr(y_a)$ is identifiable. C-component factorization is used in the ID algorithm~ which is proven to be complete for causal effect identification. \nIn case there is an unobservable confounding between the sensitive attribute $A$ and the outcome $Y$, all the above criteria will fail. However, $\\pr(y_a)$ can still be identifiable using the front-door criterion. This criterion is satisfied in Figure~\\ref{subfig:ident_e} and consists in having a mediator variable $Z$ such that:\n\\begin{itemize}\n\\item there are no backdoor paths from $A$ to $Z$\n\\item all backdoor paths from $Z$ to $Y$ are blocked by $A$.\n\\end{itemize}\nA backdoor path from $A$ to $Z$ is any path starting at $A$ with a backward edge $\\leftarrow$ into $A$ (e.g., $A \\leftarrow \\ldots Z$). If such criterion is satisfied, $\\pr(y_a)$ can be computed as follows:\n\\begin{align}\n\t\\pr(y_a)  & = \\sum_{Z} \\pr(y|do(z))\\;\\pr(z|do(a)) \\nonumber \\\\\n\t\t& = \\sum_{Z} \\pr(y|z,a)\\;\\pr(a)\\;\\pr(z|a) \\label{eq:int6}\n\\end{align}\nShpitser and Pearl proved that all the unidentifiable cases of the causal effect $\\pr(y_a)$ boil down to a general graphical structure called the hedge criterion. Based on this criterion, they designed a complete identifiability algorithm called $ID$ which outputs the expression of $\\pr(y_a)$ if it is identifiable, or the reason of the unidentifiability, otherwise.\nThe simplest graph in which the causal effect between $A$ and $Y$ is not identifiable is the``bow\" graph (Figure~\\ref{subfig:ident_a}). This simple unidentifiability criterion can be generalized to a more complex graphs called c-tree. A c-tree is a graph that is at the same time a tree\\footnote{Notice that, in this paper, the direction of the arrows between nodes is reversed compared to the usual tree structure.} and a c-component. Figure~\\ref{subfig:ident_b} shows an example of a c-tree. If the causal graph is a c-tree rooted in the outcome variable $Y$, $\\pr(y_a)$ is unidentifiable~.", "cites": [4449, 4448, 4441], "cite_extract_rate": 0.3, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes causal identifiability concepts from multiple sources, including Pearl’s do-calculus and graphical identifiability approaches, and integrates them into a coherent explanation of causal effect identification in fairness contexts. It provides critical commentary on the limitations of do-calculus (e.g., difficulty in rule application order) and introduces c-component factorization as a structured alternative. The abstraction level is strong, as it generalizes identifiability conditions and discusses broader principles like the front-door criterion."}}
{"id": "d3f43314-4b12-4720-8aec-777cea54a9fc", "title": "Identifiability of counterfactuals", "level": "subsubsection", "subsections": [], "parent_id": "b1998d61-6f78-4a86-a6cb-3b833ddeeb50", "prefix_titles": [["title", "Survey on Causal-based Machine Learning Fairness Notions"], ["section", "Computing causal quantities from observable data"], ["subsection", "Identifiability"], ["subsubsection", "Identifiability of counterfactuals"]], "content": "\\label{sec:ident_ctf}\nMost of causal-based fairness notions in the disparate treatment framework ($NDE$~(Eq.~\\ref{eq:NDE}), path-specific effect~(Eq.~\\ref{eq:PSE}), counterfactual effects~(Section~\\ref{sec:ctf}), etc.) are defined in terms of counterfactual quantities. Hence, the applicability of those notions depends heavily on the identifiability of the counterfactuals composing them. \nIn Markovian, as well as semi-Markovian models, if all parameters of the causal model are known (including $\\pr(\\mathbf{u})$), any counterfactual is identifiable and can be computed using the three steps abduction, action, and prediction (Theorem 7.1.7 in~).   \nLet $P_{*}= \\{P_{\\mathbf{x}}|\\mathbf{X} \\subseteq \\mathbf{V},\\;\\mathbf{x}\\;\\text{\\em a value assignment of }\\; \\mathbf{X}\\}$ be the set of all interventional distributions in a given causal model. While the identifiability of interventional proabilities $\\pr(y_a)$ is characterized based on observational probabilities $\\pr(\\mathbf{v})$, in this section, the identifiability of counterfactuals is characterized in terms of interventional probabilities $P_{*}$. Then, combining these results with the criteria of the previous section, a counterfactual can, in turn, be identified using observational probabilities $\\pr(\\mathbf{v})$. \nGiven a causal graph $G$ of a Markovian model and a counterfactual expression $\\gamma = v_{x}|e$ with $e$ some arbitrary set of evidence, identifying and computing $\\pr(\\gamma)$ requires to construct a counterfactual graph which combines parallel worlds. Every world is represented by a model $M_{x}$ corresponding to each subscript in the counterfactual expression. For example, given the causal graph in Figure~\\ref{fig:firing_example} and the counterfactual expression $y_{a_1}|a_0$, the resulting counterfactual graph is shown in Figure~\\ref{subfig:ident_ctf_e}. \nThe counterfactual graph should be ``reduced'' by merging together vertices that share the same causal mechanism (\\textbf{make-cg} algorithm in~ automates this procedure). The resulting counterfactual graph can be considered as a typical causal graph for a larger causal model. Consequently, all the graphical criteria listed in Section~\\ref{identInterv} for the identifiability of causal effects apply on the counterfactual graph to identify counterfactual quantities, in particular, the c-component factorization of the counterfactual graph~. $ID^*$ and $IDC^*$ algorithms~ automate the identifiability and computation of counterfactuals based on all the above criteria. Note that $ID^*$ and $IDC^*$ output expressions in terms of interventional probabilities $P_*$. Then, $ID$ algorithm is used to express those interventional probabilities in terms of observational probabilities. \n\\begin{figure}[!h]\n    \\subfigure [] {\n    {\\includegraphics[scale=0.25]{./Graphics/ident_ctf_b.eps} }\n    \\label{subfig:ident_ctf_b}}\n     \\qquad \n    \\subfigure []  {\n    {\\includegraphics [scale=0.25]{./Graphics/ident_ctf_c.eps} }\n    \\label{subfig:ident_ctf_c}}\n        \\\\\n    \\subfigure []  {\n    {\\includegraphics [scale=0.25]{./Graphics/ident_ctf_d.eps} }\n    \\label{subfig:ident_ctf_d}}\n        \\qquad \n    \\subfigure []  {\n    {\\includegraphics [scale=0.22]{./Graphics/ident_ctf_e.eps} }\n    \\label{subfig:ident_ctf_e}}\n    \\caption{Causal graphs.}\n    \\label{fig:ident_ctf}\n    \\vspace{-3mm}\n\\end{figure}\nThe simplest unidentifiable counterfactual quantity is $\\pr(y_{a'},y'_a)$ which is called the probability of necessity and sufficiency. The corresponding counterfactual graph is the W-graph that has the same structure as to Figure~\\ref{subfig:ident_ctf_b}. This simple criterion can be generalized to the zig-zag graph (Figure~\\ref{subfig:ident_ctf_c}) where the counterfactual $\\pr(y_a,w_1,w_2,z')$ is not identifiable.\nPearl~ proves two results about the identifiability of counterfactuals. First, for linear causal models (i.e., the functions $\\mathbf{F}$ are linear), any counterfactual is experimentally (using $P_{*}$) identifiable whenever the model parameters are identified. Second, in linear causal models, if some of the model parameters are unknown, any counterfactual of the form  $\\ep(Y_a|e)$ where $e$ is some arbitrary set of evidence, is identifiable provided that  $\\ep(y_a)$ is identifiable. Finally, there is no single necessary and sufficient criterion for the identifiability of counterfactuals in semi-Markovian models~. \nTo illustrate the computation of a counterfactual probability, consider the teacher firing example of Figure~\\ref{fig:firing_example} and the counterfactual probability $\\pr(y_{a_1}|a_0)$ which reads the probability of firing a teacher who is assigned a class with a high initial level of students ($a_0$) had she been assigned a class with a low initial level of students ($a_1$). Applying \\textbf{make-cg} algorithm based on this counterfactual quantity produces the counterfactual graph in Figure~\\ref{subfig:ident_ctf_e} which combines two worlds: the actual world where the teacher has normally $A=a_0$ and the counterfactual world where \\textit{the same} teacher is assigned $A=a_1$. Both variables $C$ are reduced to a single variable and $Y$ and $Y_{a_1}$ are connected by an unobservable confounder. The counterfactual graph is composed of three c-components $\\{C\\},\\{A\\},\\{Y,Y_{a_1}\\}$. \nApplying algorithm $IDC^*$~ results in:\n\\begin{align}\n\t\\pr(y_{a_1}|a_0) & = \\frac{\\sum_{y,c}\\;Q(c)\\;Q(a_0)\\;Q(y,y_{a_1})}{\\pr(a_0)}\n\\end{align}\nwhere $Q(\\mathbf{v}) = \\pr(\\mathbf{v}|pa(\\mathbf{V}))$ in the counterfactual graph.\nHence,\n\\begin{align}\n\t\\pr(y_{a_1}|a_0) & = \\frac{\\sum_{y,c}\\;\\pr(c)\\;\\pr(a_0|c)\\;\\pr(y,y_{a_1}|c)}{\\pr(a_0)} \\nonumber\\\\\n\t& = \\frac{\\sum_{c}\\;\\pr(c)\\;\\pr(a_0|c)\\;\\pr(y_{a_1}|c)}{\\pr(a_0)} \\label{eq:cs2} \\\\\n\t& = \\frac{\\sum_{c}\\;\\pr(c)\\;\\pr(a_0|c)\\;\\pr(y|a_1,c)}{\\pr(a_0)}\\label{eq:cs3} \\\\\n\t\t\t& = \\frac{0.5\\times0.8\\times0.25 + 0.5\\times0.2\\times0.01}{0.5} \\nonumber \\\\\n\t\t\t& = 0.202 \\nonumber\n\\end{align}\n$y$ in Eq.~\\ref{eq:cs2} is cancelled by summation while $\\pr(y_{a_1}|c)$ in the same equation is transformed into $\\pr(y|a_1,c)$ in Eq.~\\ref{eq:cs3} using Rule 2 of the $do$-calculus.", "cites": [4447], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from causal theory, particularly on identifiability of counterfactuals, and integrates it into the discussion of fairness notions. It explains key algorithms (e.g., make-cg, IDC*) and applies them to an illustrative example, showing a moderate level of abstraction. However, it lacks deeper critical analysis of the cited work, primarily describing and applying its results without evaluating limitations or contrasting with other approaches."}}
{"id": "0936fe42-d5bb-414b-af21-4dc066b4ba3f", "title": "Identifiability of direct and indirect effects", "level": "subsubsection", "subsections": [], "parent_id": "b1998d61-6f78-4a86-a6cb-3b833ddeeb50", "prefix_titles": [["title", "Survey on Causal-based Machine Learning Fairness Notions"], ["section", "Computing causal quantities from observable data"], ["subsection", "Identifiability"], ["subsubsection", "Identifiability of direct and indirect effects"]], "content": "In Markovian models, the average natural direct effect $NDE$ and the average natural indirect effect $NIE$ are always identifiable (from observational data) and can be computed as follows~:\n\\begin{equation}\n\tNDE_{a_1,a_0}(Y) = \\sum_{\\mathbf{s}} \\sum_{\\mathbf{z}} \\bigg(\\ep[Y|a_1,\\mathbf{z}]-\\ep[Y|a_0,\\mathbf{z}] \\bigg)\\pr(\\mathbf{z}|a_0,\\mathbf{s})\\pr(\\mathbf{s}) \\label{eq:nde}\n\\end{equation}\n\\begin{equation}\nNIE_{a_1,a_0}(Y) = \\sum_{\\mathbf{s}} \\sum_{\\mathbf{z}} \\ep[Y|a_0,\\mathbf{z}]\\;\\bigg (\\pr(\\mathbf{z}|a_1,\\mathbf{s})-\\pr(\\mathbf{z}|a_0,\\mathbf{s})\\bigg )\\;\\pr(\\mathbf{s}) \\label{eq:nie}\n\\end{equation}\nwhere $\\mathbf{Z}$ is a set of mediator variables and $\\mathbf{S}$ is any set of variables satisfying the back-door criterion between the sensitive variable $A$ and the mediator variables $\\mathbf{Z}$, that is, \\textit{(i)} no variable in $\\mathbf{S}$ is a descendant of $A$ and \\textit{(ii)} $\\mathbf{S}$ blocks all back-door paths between $A$ and $\\mathbf{Z}$.\nA simpler formulation can be used in case there is no confounding between $A$ and $\\mathbf{Z}$, where the need for $\\mathbf{S}$ is dropped altogether:\n\\begin{equation}\n\tNDE_{a_1,a_0}(Y) = \\sum_{\\mathbf{z}} \\bigg(\\ep[Y|a_1,\\mathbf{z}]-\\ep[Y|a_0,\\mathbf{z}] \\bigg)\\;\\pr(\\mathbf{z}|a_0) \\label{eq:nde2}\n\\end{equation}\n\\begin{equation}\nNIE_{a_1,a_0}(Y) = \\sum_{\\mathbf{z}} \\ep[Y|a_0,\\mathbf{z}]\\;\\bigg (\\pr(\\mathbf{z}|a_1)-\\pr(\\mathbf{z}|a_0)\\bigg ) \\label{eq:nie2}\n\\end{equation}\nIn semi-Markovian models, $NDE$ and $NIE$ are not generally identifiable, even if we have the luxury to perform any experiment using $RCT$, because of the nested counterfactuals $\\pr(Y_{a_1},\\mathbf{Z}_{a_0})$ and  $\\pr(Y_{a_0},\\mathbf{Z}_{a_1})$ in Eq.~\\ref{eq:NDE} and Eq.~\\ref{eq:NIE}, respectively.  Nevertheless, these quantities are identifiable \\textit{from experimental data} provided that there is a set of variables $\\mathbf{W}$ which are parents of the outcome variable $Y$ but non-descendants of $A$ and $Z$ such that $Y_{a_0,z} \\independent Z_{a_0} | \\mathbf{W}$ (reads: $Y_{a_0,z}$ and $Z_{a_0}$ are independent conditional on $\\mathbf{W}$). This condition can be easily checked from the causal graph as follows: $\\mathbf{W}$ d-separates $Y$ and $\\mathbf{Z}$ in the graph formed by deleting all arrows emanating from $A$ and $\\mathbf{Z}$, denoted simply as $(Y \\independent \\mathbf{Z} | \\mathbf{W})_{G_{\\underline{AZ}}}$.\nIf such graphical condition is satisfied, $NDE$ and $NIE$ can be computed from experimental quantities as follows:\n\\begin{equation}\nNDE_{a_1,a_0}(Y) = \\sum_{\\mathbf{z,w}}\\bigg (\\ep[Y_{a_1,\\mathbf{z}}|\\mathbf{w}] - \\ep[Y_{a_0,\\mathbf{z}}|\\mathbf{w}]\\bigg )\\;\\pr(\\mathbf{Z}_{a_0}=\\mathbf{z}|\\mathbf{w})\\;\\pr(\\mathbf{w}) \\label{eq:nde3} \n\t\\end{equation}\n\\begin{equation}\nNIE_{a_1,a_0}(Y) = \\sum_{\\mathbf{z,w}} \\ep[Y_{a_0,\\mathbf{z}}|\\mathbf{w}]\\bigg (\\pr(\\mathbf{Z}_{a_1}=\\mathbf{z}|\\mathbf{w})-\\pr(\\mathbf{Z}_{a_0}=\\mathbf{z}|\\mathbf{w}) \\bigg )\\pr(\\mathbf{w}) \\label{eq:nie3} \n\t\\end{equation}", "cites": [4444], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the key ideas from the cited paper on direct and indirect effects, integrating them with the context of causal fairness. It provides a clear analytical framework for when these effects are identifiable under Markovian and semi-Markovian models. While it does not deeply critique the cited work, it offers abstraction by generalizing identifiability conditions and linking them to graphical criteria, enhancing conceptual understanding."}}
{"id": "16812128-248f-40fe-9ab3-25e9f13a3633", "title": "Identifiability of path-specific effects", "level": "subsubsection", "subsections": [], "parent_id": "b1998d61-6f78-4a86-a6cb-3b833ddeeb50", "prefix_titles": [["title", "Survey on Causal-based Machine Learning Fairness Notions"], ["section", "Computing causal quantities from observable data"], ["subsection", "Identifiability"], ["subsubsection", "Identifiability of path-specific effects"]], "content": "\\label{identPS}\nThe identifiability of $PSE_{\\pi}(a_1,a_0)$ in Markovian models depends on whether\\\\ $\\pr(y|do(a_1|_{\\pi},a_0|_{\\bar{\\pi}}))$ is identifiable. Avin et al.~ gave a single necessary and sufficient criterion for the identifiability of $\\pr(y|do(a_1|_{\\pi},a_0|_{\\bar{\\pi}}))$ in Markovian models called recanting witness criterion. This criterion holds when there is a vertex $W$ along the causal path $\\pi$ that is connected to $Y$ through another causal path not in $\\pi$. For instance, Figure~\\ref{subfig:ident_ctf_d} satisfies the recanting witness criterion when $\\pi = A\\rightarrow W \\rightarrow Z \\rightarrow Y$ with $W$ as witness. The corresponding graph structure is called ``kite'' graph. When this criterion is satisfied,  $\\pr(y|do(a_1|_{\\pi},a_0|_{\\bar{\\pi}}))$ is not identifiable, and consequently,  $PSE_{\\pi}(a_1,a_0)$ is not identifiable. \nShpitser~ generalizes this criterion to semi-Markovian models known as recanting district criterion.", "cites": [8791], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a focused explanation of the recanting witness and recanting district criteria for identifying path-specific effects, integrating concepts from Avin et al. and Shpitser. It creates a coherent narrative by connecting these criteria to the broader issue of identifiability in causal fairness. However, it lacks deeper critical evaluation or comparison of the approaches, and while it generalizes the kite graph structure, it does not offer high-level meta-insights into the field of causal fairness."}}
{"id": "e2d37003-6a99-410e-9cf0-92501596073e", "title": "Estimation based on full knowledge of the causal model parameters", "level": "subsection", "subsections": [], "parent_id": "910f06e9-1228-4aea-bb76-b79d05cfc6c4", "prefix_titles": [["title", "Survey on Causal-based Machine Learning Fairness Notions"], ["section", "Computing causal quantities from observable data"], ["subsection", "Estimation based on full knowledge of the causal model parameters"]], "content": "\\label{subsec:ctf}\nThe main reason behind the unidentifiability of causal quantities (causal effect, counterfactuals, etc.) is the presence of unobservable variables, namely, hidden latent variables.\nSome causal-based fairness notions, such as counterfactual fairness~, can be assessed in presence of such unobservable latent variables. The only requirement, however, is the knowledge of the causal model structure (skeleton). Based on the causal model, the latent/background variables are estimated using observable data. Then, the predictor is trained using both observable (non-descendants of the sensitive attributes) as well as the estimated latent variables. Such predictor tends to be more fair than typical predictors (trained using only observable variables) since it takes into consideration hidden bias captured by latent variables. Given the full causal model, counterfactual fairness can be assessed by generating, for every observable data sample, a counterfactual data sample by simply changing the sensitive attribute value (e.g., turn male into female) then using the three-steps process (abduction, action, prediction) to compute the outcome. The predictor is considered fair if the predicted outcomes distributions of both groups (protected and unprotected) are similar. \nIn this survey, and for clarity of presentation, the counterfactually fair learning and estimation approach described by Kusner et al. is illustrated in Algorithm~\\ref{alg:one}. \n\\RestyleAlgo{ruled}\n\\SetKwComment{Comment}{/* }{ */}\n\\begin{algorithm}\n\\caption{\\small Counterfactual learning and assessment}\\label{alg:one}\n\\SetKwInOut{Input}{input}\\SetKwInOut{Output}{output}\n\\Input{\\small Labelled dataset $\\mathcal{D}\\equiv\\{(A^{(i)},X^{(i)},Y^{(i)})\\}$ $i=1\\ldots n$, \\\\Causal model $\\mathcal M$ with causal graph $\\mathcal G$} \n\\Output{\\small Predictor $\\hat{Y}$ w/ counterfactual fairness constraints, \\\\Estimation of counterfactual bias}\nFit the causal model $\\mathcal M$ based on labelled dataset $\\mathcal D$\\label{alg1-line1}\\;\nEstimate the posterior distribution of the latent variable(s) $U$: $\\pr_{\\mathcal M}(U|X,A)$\\label{alg1-line2}\\;\nFor every sample $(x^{(i)},a^{(i)})$ of $\\mathcal D$, generate a counterfactual sample for every possible combination of sensitive attribute values $a^{\\prime(i)}$\\label{alg1-line3}\\;\nFor every counterfactual sample generated in Step~\\ref{alg1-line3}, use the three inference steps: \\textbf{Abduction}, \\textbf{Action}, and \\textbf{Prediction} to compute the values of the remaining variables $x^{\\prime(i)}$ as well as the label $y^{(i')}$ \\label{alg1-line4}\\;\nTrain a predictor $\\hat{Y}$ using only variables non-descendants of $A$ and the estimated latent variables $U$, that is, $\\hat{Y} \\equiv h_{\\theta}(U,X_{\\nsucc A})$ where $\\theta$ represents the predictor parameters and $X_{\\nsucc A} \\subseteq X$ are non-descendants of $A$\\label{alg1-line5}\\;\nUse the trained predictor $\\hat{Y}$ to compute $\\hat{y}^{(i)}$ for every observed as well as generated counterfactual sample in the test set\\label{alg1-line6}\\;\nUsing predicted outcomes $\\hat{y}^{(i)}$ of observed and counterfactual samples, estimate the counterfactual bias\\label{alg1-line7}\\;\n\\end{algorithm}\nThe approach takes as input the labelled dataset $\\mathcal D$ and assumes the structure of the causal model $\\mathcal M$ is available including the relationships between all variables (observed and latent) along with the causal graph. For example, Kusner et al. assumed a single latent variable $U$ and a combination of Normal and Poisson distributions for their illustrated example. If the parameter values are not known, they can be estimated using the observed data (Step~\\ref{alg1-line1}). \nThe approach has two objectives $(1)$ learning a predictor $\\hat{Y}$ while taking into consideration counterfactual fairness constraints and $(2)$ assessing whether the prediction is counterfactually fair. Given the causal model $\\mathcal M$, Step~\\ref{alg1-line1} fits the model parameters to the observed data $\\mathcal D$. This allows to estimate the posterior distribution of the latent variable(s) $U$ (Step~\\ref{alg1-line2}). Step~\\ref{alg1-line3} is straightforward and consists in generating a counterfactual sample for every observed sample by assigning a different value to the sensitive attribute. For example, if the observed sample is $(X=x_1\\;,\\;A=male)$, the counterfactual sample would be $(X=x_1\\;,\\;A=female)$. Step~\\ref{alg1-line4} is the hard part of generating the counterfactual sample, which is, computing the ``would-be'' values of the remaining variables of these ``made-up'' samples. It tries to answer the core question of causal approaches: what would the output be, had the sensitive attribute value was different? This is achieved by the three steps process (Theorem~7.1.7 in~) to compute counterfactuals: \n\\begin{enumerate}\n    \\item \\textbf{Abduction}: compute the posterior distribution of $U$ given the evidence, that is, the observed data $(X=x^{(i)},A=a^{(i)})$\n    \\item \\textbf{Action}: Substitute the observed sensitive attribute equation with the counterfactual value (e.g., $A=female$) in the causal model $\\mathcal M$\n    \\item \\textbf{Prediction}: Compute the remaining variables values (including $Y$) using the new equations.\n\\end{enumerate}\nEnforcing counterfactual fairness while training the predictor (Step~\\ref{alg1-line5}) is achieved through the use of two principles. First, by using only variables non-descendants of the sensitive attribute $A$ which is direct implication of counterfactual fairness definition (Lemma~1 in~). Second, unlike typical predictors, by considering latent variables $U$ in the training. This allows to involve the background sources of the social bias encoded in the latent variables. The predictor is then used to generate predictions for observed and counterfactual samples in the test set\\footnote{$80\\%$ of the data is used for training while $20\\%$ of the data is used for testing.}(Step~\\ref{alg1-line6}). Finally, counterfactual bias can be estimated by comparing the output of observed samples and counterfactual samples (Step~\\ref{alg1-line7}). In Kusner et al.~ and in this survey, density distributions are used to estimate the counterfactual bias.   \nAlgorithm~\\ref{alg:one} illustrates the steps for the (counterfactually) fair learning of a predictor and the estimation of counterfactual bias. However, typical real-world scenarios come with an already trained predictor and hence need only the second goal, that is, counterfactual fairness assessment of that predictor. Algorithm~\\ref{alg:two} is a modified version of Algorithm~\\ref{alg:one} which, given an already trained predictor, tries to tell how counterfactually-fair the predictor is.\n\\begin{algorithm}\n\\caption{\\small Counterfactual fairness assessment}\\label{alg:two}\n\\SetKwInOut{Input}{input}\\SetKwInOut{Output}{output}\n\\Input{\\small Labelled dataset $\\mathcal{D}\\equiv\\{(A^{(i)},X^{(i)},Y^{(i)})\\}$ $i=1\\ldots n$, \\\\\nPredictor $\\hat{Y}$,\\\\Causal model $\\mathcal M$ with causal graph $\\mathcal G$} \n\\Output{\\small Estimation of counterfactual bias in $\\hat{Y}$}\nFit the causal model $\\mathcal M$ based on labelled dataset $\\mathcal D$\\label{alg2-line1}\\;\nEstimate the posterior distribution of the latent variable(s) $U$: $\\pr_{\\mathcal M}(U|X,A)$\\label{alg2-line2}\\;\nFor every sample $(x^{(i)},a^{(i)})$ of $\\mathcal D$, generate a counterfactual sample for every possible combination of sensitive attribute values $a^{\\prime(i)}$\\label{alg2-line3}\\;\nFor every counterfactual sample generated in Step~\\ref{alg2-line3}, use the three inference steps: \\textbf{Abduction}, \\textbf{Action}, and \\textbf{Prediction} to compute the values of the remaining variables $x^{\\prime(i)}$ as well as the label $y^{(i')}$ labels \\label{alg2-line4}\\;\nUse predictor $\\hat{Y}$ to compute $\\hat{y}^{(i')}$ for every generated counterfactual sample in the test set\\label{alg2-line5}\\;\nUsing observed and counterfactual samples, estimate the counterfactual bias\\label{alg2-line6}\\;\n\\end{algorithm}\nIt is important to mention that assessing counterfactual bias of a given predictor requires to have access to that predictor. This is needed to generate predictions for the counterfactual samples (Step~\\ref{alg2-line5}). \n\\begin{table*}[th]\n  \\centering\n  \\begin{tabular}{lll}\n  \\hline\n    \\multicolumn{3}{c}{Law school}\\\\\n    \\hline\n    \\begin{minipage}{.2\\textwidth}\n      \\includegraphics[width=\\textwidth,height=30mm]{./Graphics/BB.eps}\n    \\end{minipage}& \\begin{minipage}{.2\\textwidth}\n      \\includegraphics[width=\\textwidth, height=30mm]{./Graphics/AA.eps}\n    \\end{minipage}& \\begin{minipage}{.2\\textwidth}\n      \\includegraphics[width=\\textwidth, height=30mm]{./Graphics/MM.eps}\n    \\end{minipage}\\\\\n    \\hline\n    \\multicolumn{3}{c}{Communities and crime}\\\\\n    \\hline\n    \\begin{minipage}{.2\\textwidth}\n      \\includegraphics[width=\\textwidth, height=30mm]{./Graphics/BBC.eps}\n    \\end{minipage} & \\begin{minipage}{.2\\textwidth}\n      \\includegraphics[width=\\textwidth,height=30mm]{./Graphics/AAC.eps}\n    \\end{minipage}& \\begin{minipage}{.2\\textwidth}\n      \\includegraphics[width=\\textwidth, height=30mm]{./Graphics/HHC.eps}\n    \\end{minipage}\\\\\n    \\hline\n  \\end{tabular}\n  \\caption{Counterfactual learning and assessment of the law school and communities and crime datasets.}\n  \\label{tab:exp}\n\\end{table*}\n\\begin{figure}[!h]\n    \\subfigure [Law school.]  {\n    {\\includegraphics [scale=0.25]{./Graphics/Law.eps} }\n    \\label{subfig:CGLaw}}\n    \\subfigure [Communities and crime.] {\n    {\\includegraphics[scale=0.25]{./Graphics/crimes.eps} }\n    \\label{subfig:CGComm}}\n    \\caption{Causal graphs of the law school and the communities and crime datasets.}\n    \\label{fig:CG}\n\\end{figure}\nSimilarly to Kusner et al.~, Stan programming language~ is used for counterfactual learning and assessment. However, in addition to the \\textit{law school}~ used in Kusner et al., a second dataset is used in the empirical analysis, namely, \\textit{communities and crime}\\\\ . The \\textit{law school} dataset~ includes information on $21,790$ law students such as their LSAT, their GPA collected prior to law school, and their first year average grade (FYA). Given this data, a school wishes to predict whether an applicant will have a high FYA. Race is considered as a sensitive attribute. \\textit{Communities and crime} dataset~ contains information relevant to per capita violent crime rates in different communities in the United States (e.g., percentage of people under the poverty level, median family income, percentage of population who are divorced, etc,.). The goal is to predict crime rate. Race is considered as sensitive attribute.\nFor both datasets, the experiment consists in training a baseline model using logistic regression\\footnote{The predictor uses all the variables in the dataset, including the sensitive variables to make the predictions.}, then applying  Algorithm~\\ref{alg:two} to assess counterfactual fairness to both the original and counterfactual sampled data and plot how the distribution of predicted FYA changes for that baseline model.\nSince sensitive attributes are not binary, the experiment is repeated for every counterfactual change (e.g., white vs black, white vs asian, etc.). If both distributions are superposed, it means the prediction is counterfactually fair. Far apart distributions indicate counterfactual bias.\nTable~\\ref{tab:exp} shows the density plots of $\\hat{Y}$ for both datasets. We assume that the true model of the world is given by the causal graphs presented in Figure~\\ref{fig:CG}. The causal graphs use a single latent/background variable $U$. For the \\textit{law school}, $U$ can represent the knowledge of the student, while for the \\textit{communities and crime}, it refers to the criminality of the individual which can capture other aspects of the individual that might have been used by the police. The red plots refer to the filtered original samples while the green plots refer to the corresponding counterfactual samples. For example, in plot (a), the red plot represents the predicted first year average ($\\hat{FYA}$) for observed black students whereas the red plot represents $\\hat{FYA}$ for the corresponding counterfactual samples. Density plots for the law school corroborate the findings of Kusner et al.~, that is, racial differences exhibit counterfactual unfairness in favor of whites. The unfairness is peaked when predicting the FYA for white versus black students. The race-based discrimination is present in the \\textit{communities and crime} result in a similar way: the counterfactual bias is peaked for white versus black communities. However, racial comparison can be considered counterfactually fair when comparing hispanic/white and asian/white communities.", "cites": [3905], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the core method of counterfactual fairness from Kusner et al., integrating it into a structured algorithmic framework that clarifies the role of causal models and latent variables. It provides critical analysis by highlighting the challenges in Step 4 and the necessity of latent variables to capture hidden bias. However, the analysis remains focused on a single method and does not broadly compare or contextualize it within the wider landscape of causal fairness approaches, limiting its abstraction and comparative depth."}}
{"id": "e222c326-3dbb-4b8c-944b-194bcf470e97", "title": "Potential outcome estimation techniques", "level": "subsection", "subsections": ["01d9e85b-8d61-4d70-bbba-6c86c6a39d4c", "62338203-770c-43a0-b91d-0624424f2f2c", "dfb61b38-6908-4ec8-9976-5dafacb85739"], "parent_id": "910f06e9-1228-4aea-bb76-b79d05cfc6c4", "prefix_titles": [["title", "Survey on Causal-based Machine Learning Fairness Notions"], ["section", "Computing causal quantities from observable data"], ["subsection", "Potential outcome estimation techniques"]], "content": "\\label{subsec:potEstimation}\nCausal inference in the potential outcome framework focuses on estimating the causal effect of a treatment variable $A$ (e.g., the sensitive attribute) on an effect variable $Y$ (e.g., the decision outcome). As mentioned in Section~\\ref{subsec:po}, there are three assumptions that are typically made for causal effect estimation, SUTVA, ignorability, and positivity. Inline with the potential outcome framework literature, this survey focuses on causal inference approaches that rely on the three assumptions~, namely, re-weighting~, matching~, and stratification~.", "cites": [4428, 4430], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a basic description of potential outcome estimation techniques and lists the three key assumptions from the potential outcome framework literature. It cites two survey papers on causal inference but does not meaningfully synthesize or integrate their content into a broader discussion. There is no critical evaluation or comparison of methods, and while it touches on general principles of causal inference, the abstraction remains limited to standard definitions without deeper insight."}}
{"id": "62338203-770c-43a0-b91d-0624424f2f2c", "title": "Matching", "level": "subsubsection", "subsections": [], "parent_id": "e222c326-3dbb-4b8c-944b-194bcf470e97", "prefix_titles": [["title", "Survey on Causal-based Machine Learning Fairness Notions"], ["section", "Computing causal quantities from observable data"], ["subsection", "Potential outcome estimation techniques"], ["subsubsection", "Matching"]], "content": "\\label{subsec:matching}\nMatching techniques~ focus on estimating the counterfactual outcome of units. The idea is to estimate the counterfactual outcomes $Y^1_{i}|A=0$ and $Y^0_{i}|A=1$ based on the matched neighbours of unit $i$ in the opposite group. For example, given an observed female candidate $f_k$, estimating the counterfactual outcome (hiring decision) had she been a male is based on the units in the male group that are the most comparable to $f_k$. Hence, the first and main issue is to define a similarity metric between two given units (e.g. $x_i$ and $x_j$). The most common approach is to rely on the propensity scores of units:\n\\begin{equation}\n    D(i,j) = |e(x_i) - e(x_j)| \\label{eq:matchingDist1}\n\\end{equation}\nand its logit version:\n\\begin{equation}\n    D(i,j) = |logit(e(x_i)) - logit(e(x_j))| \\label{eq:matchingDist1}\n\\end{equation}\nwhich is preferred as it has been proven to reduce the bias~.\nThe second issue is the matching algorithm, that is, how many neighbours to consider and how these neighbours are weighted to obtain the estimation? Matching algorithms include~:\n\\begin{itemize}\n    \\item Exact matching: uses only identical matches. Typically infeasible since many units will remain unmatched.\n    \\item Nearest neighbour matching (NNM): constructs the counterfactual using the closest neighbours according to a similarity metric. It can run with or without replacement, that is, by returning a matched unit to the pool or not. The no replacement variant makes the estimation dependent on the order in which the units are matched.\n    \\item Caliper matching: a version of NNM that restricts matching to a chosen maximum distance. Its main problem is that some units may not receive matches because no neighbours fall within their caliper. A hybrid variant consists in using caliper, and in case of no neighbours, select a matching neighbour outside the caliper.\n    \\item Radius matching: the same as caliper, but matching is done with replacement.\n    \\item Kernel matching: an extension to all the above algorithms where to match a unit $i$, all units in the opposite group are used. Each unit is weighted according to the distance to the unit $i$. \n\\end{itemize}\n\\begin{table*}[]\n\\centering\n\\caption{Characteristics of the real-world datasets.}\n\\label{tab:datasetsInfo}\n\\setlength{\\tabcolsep}{6pt}\n\\begin{tabular}{l|l|l|l|l|l}\n\\hline\n\\multirow{2}{*}{Dataset} & Sample & Sensitive  & \\multirow{2}{*}{Covariate(s)}     &  \\multirow{2}{*}{Mediator(s)} & \\multirow{2}{*}{Outcome} \\\\\n            & size   & feature(s) &    &     &    \\\\\n\\hline\n\\hline\nCommunities & \\multirow{2}{*}{$1994$} & \\multirow{2}{*}{race}   & age & poverty rate &  \\multirow{2}{*}{crime rate} \\\\\n  and crimes &  &  & unemp. rate & divorce rate &  \\\\\n\\hline\n\\multirow{2}{*}{Compas} & \\multirow{2}{*}{$5915$} & \\multirow{2}{*}{race}   & age& \\multirow{2}{*}{priors}  & \\multirow{2}{*}{recidivism}\\\\\n &    &   & gender &  &  \\\\\n\\hline\n\\multirow{2}{*}{German credit} & \\multirow{2}{*}{$1000$}   & gender & \\multirow{2}{*}{age}& \\multirow{2}{*}{emp. length} & \\multirow{2}{*}{default}\\\\\n&  & marital status &  &  &   \\\\\n\\hline\nBerkeley & $4526$ & gender   & department& department & admission\\\\\n\\hline\n\\end{tabular}\n\\end{table*}", "cites": [8792], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a clear and factual description of matching techniques used in potential outcome estimation, including their variants and characteristics. However, it lacks synthesis of broader themes from the cited papers, offers minimal critical evaluation of the methods, and does not abstract these techniques into a higher-level understanding or framework. It primarily serves as a method overview rather than an insightful analysis."}}
{"id": "dfb61b38-6908-4ec8-9976-5dafacb85739", "title": "Stratification", "level": "subsubsection", "subsections": [], "parent_id": "e222c326-3dbb-4b8c-944b-194bcf470e97", "prefix_titles": [["title", "Survey on Causal-based Machine Learning Fairness Notions"], ["section", "Computing causal quantities from observable data"], ["subsection", "Potential outcome estimation techniques"], ["subsubsection", "Stratification"]], "content": "\\label{subsec:strat}\nStratification~ uses the same principle underlying identifiability approach (Section~\\ref{sec:ident}), that is, adjusting on confounders. The aim is to split the entire observed data into consistent groups such that the units in the same group can be considered as sampled from data under RCT. The two ingredients of stratification are the splitting of groups and then the combination of the created groups. The stratification estimator of $ATE$ can be defined generically as:\n\\begin{equation}\n    \\hat{ATE}^{strat} = \\sum_{k=1}^{K}m(k)[\\overline{Y}_1(k) - \\overline{Y}_0(k)] \\label{eq:atestrat}\n\\end{equation}\nwhere $K$ is the number of stratification groups, $m(k)$ is the portion of units in group $k$ to the total number of units $N$, $\\overline{Y}_1(k)$ and $\\overline{Y}_(k)$ are the $CATE$ (Eq.~\\ref{eq:CATE}) for groups $A=1$ and $A=0$, respectively. $\\hat{ATE}^{strat}$ expression has the same structure as the back-door formula (Eq.~\\ref{eq:int2}). \nIf all variables needed for the stratification are observed and the available data is infinitely large, $ATE^{strat}$ can lead to a consistent and unbiased estimator of $ATE$. However, in typical datasets, stratification may result in strata with few or no units. Consequently, some $CATE$ estimates cannot be calculated with the available data. Propensity score can be used to address this data sparseness problem. The main idea is the following: ``strata with identical propensity scores can be combined into more coarse strata''~. In other words, propensity score can be considered as a single stratifying variable that will usually result in larger strata. The same idea is used in the SCM framework to address the sparseness of data when computing identifiable expressions. \nOther estimation methods in the potential outcome framework include tree-based methods~, representation learning methods~, and meta-learning methods~.", "cites": [9129, 8793], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the concept of stratification from causal inference literature, linking it to the back-door formula and propensity score methods. It provides some abstraction by framing stratification as a general technique for addressing data sparsity. However, the critical analysis is limited—there is no in-depth evaluation of the pros and cons of stratification relative to other methods or a nuanced critique of the cited papers."}}
{"id": "838e946a-55ef-428d-9c58-a12a7b0de30d", "title": "Estimating causal effects on benchmark datasets", "level": "subsection", "subsections": [], "parent_id": "910f06e9-1228-4aea-bb76-b79d05cfc6c4", "prefix_titles": [["title", "Survey on Causal-based Machine Learning Fairness Notions"], ["section", "Computing causal quantities from observable data"], ["subsection", "Estimating causal effects on benchmark datasets"]], "content": "\\label{subsec:empirical}\nWe conduct experiments on four real-world datasets which are commonly used in discrimination-discovery literature, namely: \\textit{communities and crime}~, \\textit{Compas}~, \\textit{German credit}~ and \\textit{Berkely admission}~.\n\\textit{Communities and crime} dataset~ contains information relevant to per capita violent crime rates in $1994$ different communities in the United States and the goal is to predict that crime rate. Race is kept as sensitive attribute. However, in these experiments, we transformed the label into a binary feature by thresholding\\footnote{The median value of the violent crime rate in the dataset is used as threshold.} where $1$ corresponds to high violent crime rate and $0$ corresponds to low violent crime rate. \\textit{Compas} dataset~ includes information of $5915$ individuals from Broward County, Florida, initially compiled by ProPublica~ and the goal is to predict the two-year violent recidivism. That is, whether a convicted individual would commit a violent crime in the following two years ($1$) or not ($0$). We consider race as sensitive feature. \\textit{German credit} dataset comprises data of $1000$ individuals applying for loans. The goal is to predict whether an individual will default on the loan $(1)$ or not $(0)$. The gender and the personal status of an individual are considered sensitive features. Finally, \\textit{Berkeley admission}~ dataset consists of $4526$ applicants to UC-Berkeley's graduate programs in Fall $1973$. The goal is to predict whether a student is admitted $(1)$ or not $(0)$. Gender is treated as sensitive feature. \\textit{Berkeley admission} is commonly used to illustrate the Simpson's Paradox~. \nTable~\\ref{tab:datasetsInfo} summarizes the main characteristics of the four benchmark datasets. In the potential outcome framework, estimation techniques rely on the selection of a set of covariates and mediators. Column four lists the covariate(s)  while column five lists the mediator variables for each dataset. A covariate can be a coufounder or any other variable that might distort the causal effect estimation. A mediator is a covariate which is at the same time influenced by the sensitive feature and influences the outcome\\footnote{Graphically, a mediator is a variable in the causal path between the sensitive feature and the outcome i.e. an explaining or redlining (proxy) variable.}. \nIn addition to total variation $TV$ (statistical parity) serving as baseline measure and also to illustrate the need for causality, we use estimation techniques for seven causal-based fairness notions defined in the potential outcome framework, namely, $ATE\\_{IPW}$, $ATE\\_{match}$, $ATE\\_{DR}$, $ATE\\_{strat}$, $TE\\_{imp}, DE\\_{imp}$, and $IE\\_{imp}$. The four first notions are estimations of $ATE$ using four different approaches. $ATE\\_{IPW}$  uses inverse propensity weighting as explained in Section~\\ref{subsec:re-weighting}. The variables used to estimate the propensity score are the covariates listed in column $4$ of Table~\\ref{tab:datasetsInfo}. $ATE\\_match$ is matching estimation of $ATE$ (Section~\\ref{subsec:matching}). Recall that this approach matches each unit (individual) with one or more ``similar'' units in the other group. Propensity score matching is used.\n$ATE\\_DR$ is a double robust estimation of $ATE$ which requires at least propensity score estimation or outcome regression to be correct as explained in Section~\\ref{subsec:re-weighting}. $ATE\\_strat$ is a startification-based estimation of $ATE$ (Section~\\ref{subsec:strat}). For all these $ATE$ estimations, we used the \\textit{causallib} package implementations~. The three remaining measures, namely, $TE\\_imp$, $DE\\_imp$ and $IE\\_imp$ are imputation-based estimations of total, direct, and indirect effects. The main idea behind imputation approach is to provide $K+1$ outcome models that characterize $\\ep[Y | \\textbf{X},A]$, $\\ep[Y | \\textbf{X},A,M_1],\\; \\ldots\\; \\ep[Y | \\textbf{X},A,M_k]$, where $\\textbf{X}$ is the set of the covariates (column $4$ in Table~\\ref{tab:datasetsInfo}) and $M_1,\\ldots, M_k$ are the mediators (column $5$ in Table~\\ref{tab:datasetsInfo}). The imputation approach involves modeling the conditional means of the outcome variable $Y$, given the sensitive feature $A$, pretreatment confounders $X$, and varying sets of mediators $M$. The \\textit{paths} package implementation~ is used to compute these estimations.\nFigure~\\ref{fig:EstimAll} shows the causal effects (of the sensitive attribute $A$ on the outcome $Y$ values according to the seven estimators on the four benchmark datasets. The values are between $-1$ (full discrimination against one group) and $1$ (full discrimination against the second group). A value $0$ indicates fairness of the outcome with respect to both groups.  \nFor the three first datasets (\\textit{Communities and crimes, Compas,} and \\textit{German credit}), $TV$ overestimates the discrimination as it results in higher values compared to all causal-based notions (except $ATE\\_match$ in \\textit{German credit} where the results are reverted. In other words, the discrimination is now against male applicants: $-0.167$). For \\textit{Berekley}, $TV$ concludes a significant discrimination against women ($-0.14$) while causal-based notions show either the absence of discrimination ($0$ for $ATE\\_match$ and $DE\\_imp$) or a discrimination in favor of women which confirms the Simpson's paradox. For the three last estimations ($TE\\_{imp}, DE\\_{imp}$, and $IE\\_{imp}$), notice that $TE$ is approximately the total of $DE$ and $IE$ which is expected as the total effect is composed of direct and indirect effects. \n\\begin{figure*}[!h]\n    \\includegraphics[scale=0.45]{./Graphics/fairnessResults.eps} \n    \\caption{Estimation of causal effects on real-world datasets.}\n    \\label{fig:EstimAll}\n\\end{figure*}\nAs mentioned earlier, estimation techniques aim to balance the control and the treatment groups in order to remove (or to mitigate) potential selection bias. For the particular case of re-weighting based estimation of $ATE$ ($ATE\\_{IPW}$), one way to assess whether the balancing worked is to use the absolute standard mean difference (\\textbf{abs-smd})~ between the treatment and the control groups for each considered covariate (Column 4 in Table~\\ref{tab:datasetsInfo}). That is, measuring the difference in means between groups, divided by the (pooled) standard deviation. Figure~\\ref{fig:balanceCovariates} shows the \\textbf{abs-smd} for each covariate in the four datasets prior (unweighted) and posterior\\\\ (weighted) to applying IPW re-weighting. One can observe the same pattern in all benchmark datasets. The unweighted \\textbf{abs-smd} across control and treatment groups is at least one order of magnitude higher than the weighted \\textbf{abs-smd}. For instance, the unweighted \\textbf{abs-smd} across control and treatment groups reached $0.68$ standard-deviations for the \\textit{Unemployment rate} covariate in the \\textit{communities and crime}s dataset. After IPW re-weighting, the \\textbf{abs-smd} is significantly reduced ($0.078$). \n\\begin{figure*}[!h]\n    \\subfigure [Communities and crimes] {\n    {\\includegraphics[scale=0.25]{./Graphics/Communities_balancing.eps} }\n    \\label{subfig:balance1}}\n    \\quad\n    \\subfigure [Compas]  {\n    {\\includegraphics [scale=0.25]{./Graphics/Compas_balancing.eps} }\n    \\label{subfig:balance2}}\n    \\quad\n    \\subfigure [German credit]  {\n    {\\includegraphics [scale=0.25]{./Graphics/German_balancing.eps} }\n    \\label{subfig:balance3}}\n    \\quad\n    \\subfigure [Berkeley]  {\n    {\\includegraphics [scale=0.25]{./Graphics/Berk22.eps} }\n    \\label{subfig:balance4}}\n    \\caption{Absolute standard mean difference of covariate values of different groups (e.g. male vs female) prior and posterior to IPW re-weighting.}\n    \\label{fig:balanceCovariates}\n    \\vspace{-3mm}\n\\end{figure*}", "cites": [4450], "cite_extract_rate": 0.125, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes the datasets and methods used for estimating causal fairness notions, connecting them to broader concepts like Simpson’s paradox and causal pathways. It critically evaluates the baseline measure (TV) against causal-based measures, highlighting overestimation and paradoxical results. The abstraction is evident in discussing general principles of causal effect estimation and the role of covariate balancing."}}
{"id": "47430a94-f407-4f04-a25b-a78599f6b250", "title": "Suitability", "level": "subsection", "subsections": [], "parent_id": "24550722-845e-4870-aa8b-784d0cbe0b05", "prefix_titles": [["title", "Survey on Causal-based Machine Learning Fairness Notions"], ["section", "Suitability and applicability"], ["subsection", "Suitability"]], "content": "In this section, we try to systemize the selection process by considering the subtleties of each causal-based fairness notion and defining 6 criteria which correspond to characteristics of the real-world scenario at hand. For each criterion, we check whether it holds in the scenario at hand or not. Then, use these answers to recommend the most suitable causal-based fairness notion. The criteria are list and briefly described as follows.\n\\begin{itemize}\n    \\item \\textbf{Presence of confounding}: A variable which is a common cause of two or more other variables.\n    \\item \\textbf{Presence of explaining variable}: A variable that is correlated with the sensitive attribute such that any discrimination that is explained using that variable is considered legitimate and is acceptable.\n    \\item \\textbf{Likelihood of intersectionality}: A specific type of bias due to the combination of sensitive attributes. An individual might not be discriminated based on race only or based on gender only, but she might be discriminated because of a combination of both.\n    \\item \\textbf{Likelihood of masking}: A form of intentional discrimination that allows decision makers with prejudicial views to discriminate against individuals or groups while masking their intentions. \n    \\item \\textbf{Latent variables are known}: Latent (background) variables are not observable. However, in some scenarios, they are identified and their relationships with observable variables are known.\n    \\item \\textbf{Ground truth or reliable outcome}: the label in the training data can or cannot be reliable. In several scenarios, the outcome is inferred by humans (job hiring, college admission, etc.) and hence can encode bias. The most reliable outcome is when the ground truth is available\\footnote{An example of a scenario where ground truth is available is when predicting whether an individual has a disease. The ground truth value is observed by submitting the individual to a blood test (Assuming the blood test is flawless) for example. An example of a scenario where ground truth is not available is predicting whether a job applicant is hired. The outcome in the training data is inferred by a human decision maker which is often a subjective decision, no matter how hard she is trying to be objective.}.\n\\end{itemize}\nThe diagram in Figure~\\ref{fig:decision_diagram} can be used as a guideline to select an appropriate causal-based fairness notion given a real-world scenario.\nConfounding variables result in backdoor paths between the sensitive attribute ($A$) and the outcome ($Y$). For example, path\\\\ $A \\longleftarrow C \\longrightarrow Y$ in Figure~\\ref{fig:fig33} is a backdoor path. Backdoor paths are not causal paths, but they contribute to the association between the $A$ and $Y$. Therefore, they are the reason why it is said that ``correlation is different than causation''. In the absence of confounding, the total causal effect ($TE$ and $ATE$) coincides with the difference in conditional probabilities $TV = \\pr(y|a_1) - \\pr(y|a_0)$ which correspond to statistical parity. On the other hand, if there are no explaining variables in the model representation of the world, both direct and indirect causal paths are discriminatory\\footnote{Indirect causal paths all go through proxy variables.}. Consequently, assessing unfairness/bias due to the sensitive attribute does not require considering separately the different causal paths (direct, indirect, and path-specific). In such case (absence of confounding and explaining variables), causal inference is not needed to appropriately assess fairness. \nAny unintentional type of bias can also be \"orchestrated\" intentionally by decision makers with prejudicial views. To appropriately assess the bias in presence of such masking attempts, it is recommended to avoid group-based notions as they can more easily be gamed by prejudicial decision makers. Intersectionality is similar to masking as both lead to a discrimination which is difficult to detect at the group-level and hence require more fine-grained measures. Therefore individual causal-based fairness notions are recommended in presence of one of those criteria. For individual notions, in presence of explaining variables, it is recommended to use individual direct discrimination (Section~\\ref{sec:idd}) as it is the only individual notion listed in Section~\\ref{sec:notions} that distinguishes direct from indirect discrimination. Counterfactual fairness (Section~\\ref{sec:counterfactual}) and individual equalized counterfactual odds (Section~\\ref{sec:IndECOD}) are recommended to be used in case the latent variables are known. If the ground-truth is not available or the outcome $Y$ is not reliable, individual equalized counterfactual odds is not recommended. \nFor the group causal-based fairness notions, if there are no explaining variables, there is no need to consider the different causal paths and hence $TE$, $ATE$, or interventional fairness (Section~\\ref{sec:justif}) can be safely used. In presence of explaining variables, the remaining causal-based fairness notions are appropriate to use with two exceptions. First, non-discrimination criterion is misleading if the causal model is semi-Markovian because the variables $A$ can remain dependent even after conditioning on all observable variables because of the hidden counfounders. Second, as counterfactual error rates (Section~\\ref{sec:cer}) are expressed in terms of the true outcome $Y$, they are not recommended in case the ground-truth is not available and the true outcome is not reliable. \nFinally, note that $ETT$, $ATT$, and $ATC$ are not generally used in fairness scenarios because, typically, the bias can be observed in both directions: when considering a disadvantaged group/individual as advantaged or the opposite. $ETT$ is relevant when studying the effect of a treatment medicine on patients. For example, if a patient agrees to take the medicine and it turns out to be painful, she may be wondering about the chances of recovery if she did not take the treatment or if she took it with a lower dose. The opposite direction (the effect of treating an individual in the control group) is not relevant in this case.\nIn his book, \\textit{The Book of Why}~, Pearl describes a causation ladder with three rungs: statistical observations (seeing), intervention (doing), and counterfactual (imagining). In this section, all causal-based fairness notions defined in Pearl's SCM framework (all notions in Section~\\ref{sec:notions} except $ATE$, $ATT$, $ATC$, $ITE$, and equality of effort) are placed in the causation ladder which will help us address the problem of their applicability in real-scenarios. The causation ladder is structured in such a way that a quantity at a certain rung can be identified in terms of quantities at the rung just below it. As a consequence, the higher the rung, the more challenging the problem of identifiability, and hence the less applicable a fairness notion defined at that rung. \nThe diagram in Figure~\\ref{fig:diagram} shows the causation ladder and indicates at which rung every causal-based fairness notion stands. $TV$ which is the only non-causal fairness notion covered in this paper is at rung 1. It is always applicable provided that a set of observations (dataset) is available. No unresolved and non-discrimination criteria are placed midway between rungs 1 and 2 as they are applicable provided that the causal graph is available along the dataset. Non-discrimination criterion, however, requires the Markov property to be applicable because causal dependence through unobservable paths cannot be blocked. It also has an exponential complexity since it considers all combination of values of the parent variables of the outcome $Y$. A relaxation is described by the authors~ but the notion remains computationally intractable.", "cites": [3920], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section provides a structured analytical framework to evaluate the suitability of causal-based fairness notions by integrating the concept of a 'causation ladder' from Pearl. It connects ideas from the cited work on non-discrimination criteria with broader causal inference principles, offering a coherent synthesis. The section also identifies limitations, such as computational intractability and identifiability issues, and explains the implications of these for practical use, demonstrating critical evaluation and abstraction toward general principles."}}
