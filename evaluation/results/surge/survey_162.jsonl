{"id": "eeb81693-4e01-4681-8cc1-44ce54c30d3d", "title": "Cholesky-Based Approach", "level": "subsubsection", "subsections": [], "parent_id": "cb5897c7-b3df-40a5-a1a0-b39feddd9022", "prefix_titles": [["title", "A Survey of Numerical Methods Utilizing Mixed Precision Arithmetic"], ["section", "Dense Linear Algebra"], ["subsection", "Iterative Refinement for Least Squares Problems"], ["subsubsection", "Cholesky-Based Approach"]], "content": "The normal equations method solves\n$$\n   A^TA x = A^Tb\n$$\nusing the Cholesky factorization of $A^TA$ (see Section \\ref{sec.lp-chol}). \nIn general, this method is deprecated by numerical analysts because it \nhas a backward error bound of order $\\kappa_2(A)u$ \\cite[sect.~20.4]{higham2002accuracy} \nand the Cholesky factorization \ncan break down for $\\kappa_2(A) > u^{-1/2}$,\nbut it is used by statisticians with some justification\n.\nHere, we assume that $A$ is (sufficiently) well conditioned.\nWe propose the GMRES-IR-based least squares solver given in\nAlgorithm \\ref{alg.ls-GMRES-IR}.\n\\begin{algorithm}\n\\caption{(Cholesky-based GMRES-IR for the least squares problem) \nLet a full rank $A\\in\\mathbb{R}^{m \\times n}$, where $m \\ge n$, \nand $b\\in\\mathbb{R}^m$ be given in precision $u$. This algorithm solves \nthe least squares problem $\\min_x\\Vert b-Ax\\Vert_2$ using \nCholesky-based GMRES-IR\\@. \nThe scalar $\\theta\\in(0,1]$ and the positive integer $c$ are parameters.}\n\\label{alg.ls-GMRES-IR}\n\\begin{algorithmic}[1]\n\\State Compute $B = AS$, where $S = \\text{diag}(1/\\Vert a_j\\Vert_2)$,\nwith $a_j$ the $j$th column of $A$.\\label{line.ls1}\n\\State $\\mu = \\theta x_{\\text{max}}$\n\\State $B^{(h)} = fl_h(\\mu^{1/2} B)$ \n\\State Compute $C = B^{(h)T} B^{(h)}$ in precision $u_h$. \\label{line.fp16-normal-mat} \n\\label{line.ls2}\n\\State Compute the Cholesky factorization $C + c u_h \\text{diag}(c_{ii}) = R^TR$ \nin precision $u_h$. \\label{line.cLS}\n\\label{line.ls3}\n\\If {Cholesky factorization failed}\n\\State $c \\gets 2c$, goto line~\\ref{line.ls3}\n\\EndIf\n\\State Form $b^{(h)} = fl_h(S A^T b)$.\n\\State Solve $R^TR y_0 = b^{(h)}$ in precision $u_h$ and form $x_0 = \\mu S y_0$\nat precision $u$.\n\\For{$i=0 \\colon i_{\\max}-1$} \\label{line.ir-loop}\n\\State Compute $r_i = A^T(b-Ax_i)$ at precision $u_r$ and round $r_i$\n       to precision $u$.\n\\State{\\label{line.normal-ir}\n  Solve $MA^TA d_i = M r_i$ by \\ac{gmres} \nat precision $u$, where $M = \\mu S R^{-1} R^{-T} S$ and \nmatrix--vector products with $A^TA$ \nare computed at precision $u_r$, and store $d_i$ at precision $u$.}\n\\State $x_{i+1}=x_i+d_i$ at precision $u$.\n\\If {converged}\n\\State return $x_{i+1}$, \\textbf{quit}\n\\EndIf\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm} \nWe make some comments on the algorithm.\nLine~\\ref{line.ls1} produces a matrix $B$ with columns of unit 2-norm.\nThe computation $C = {B}^{(h)T} B^{(h)}$ on line~\\ref{line.ls2}\nproduces a symmetric positive definite matrix with constant diagonal elements $\\mu = \\theta x_{\\text{max}}$,\nso overflow cannot occur for $\\theta < 1$.\nThe shift on line~\\ref{line.ls3} is analogous to that in\nAlgorithm~\\ref{alg.Chol-half},\nbut here the matrix $C$ is already well scaled and \nin precision $u_h$ so there is no need\nto scale $C$ to have unit diagonal.\nThere are two reasons why explicitly forming \n$C = {B}^{(h)T} B^{(h)}$ in Algorithm~\\ref{alg.ls-GMRES-IR} is reasonable\nfrom the numerical stability point of view.\nFirst, $C$ is used to form a preconditioner, so  the usual problems with forming a cross product matrix (loss of significance and condition squaring)\nare less of a concern.\nSecond, if we are working in fp16 on an NVIDIA V100 we can exploit the\ntensor cores when forming $C$ to accumulate block fused multiply-add\noperations in single precision; this leads to a more accurate $C$, as shown\nby the error analysis of Blanchard et al.\\ .\nFor the computed $\\hat{R}$ we have \n$$\n    \\hat{R}^T\\hat{R} \\approx B^{(h)T} B^{(h)} \\approx \\mu S A^TA S,\n$$\nor \n$$\n (A^TA)^{-1} \\approx \\mu S\\hat{R}^{-1}\\hat{R}^{-T} S,\n$$\nso we are preconditioning with an approximation to the inverse of $A^TA$. \nWe apply the preconditioned operator $MA^TA$ to vectors at precision $u_r$.\nComputing $y = A^TA x$ costs $4mn$ flops \nand $SR^{-1}R^{-T}y$ costs another $2n^2+n$ flops, \nmaking $4mn + 2n^2 + n$ flops in total.\nFor $m \\gg n$ and large $n$, computing $y = A^TA x$ costs a factor $n/4$\nfewer flops than the $mn^2$ flops needed to form $A^TA$, \nwhile for $m\\approx n$ the difference is a factor $n/6$.\nFor large $n$, \neven allowing for the fact that the flops we are comparing are at different precisions,\nas long as \\ac{gmres} converges quickly the cost of \nthe refinement stage should be negligible compared with the cost of \nforming $A^TA$ and computing the Cholesky factorization.\nRelated to this work is the Cholesky--QR algorithm for computing a QR factorization $A = QR$.\nIt forms the cross-product matrix $A^TA$, computes the Cholesky factorization $A^TA = R^TR$,\nthen obtains the orthogonal factor $Q$ as $Q = AR^{-1}$,\nand this process can be iterated for better numerical stability;\nsee, for example, \n, \n, \n, .\nOur work differs in that we do not compute $Q$, we carry out the Cholesky factorization \nin lower precision than the working precision,\nand we solve a least squares problem using preconditioned iterative refinement.", "cites": [8326], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the Cholesky-based approach for solving least squares problems using mixed precision arithmetic. It synthesizes relevant concepts from numerical linear algebra and connects them to the proposed algorithm, including the role of preconditioning and error accumulation via tensor cores. The section also critically discusses limitations of the normal equations method and justifies why forming the cross-product matrix is acceptable in this context, though the critique remains limited to a few key points rather than a comprehensive evaluation across multiple works."}}
{"id": "3f7b37a5-f1c5-410f-8097-a10c330139a7", "title": "Data compression", "level": "subsection", "subsections": [], "parent_id": "b2e2514e-854a-4410-b501-3e354347a8a3", "prefix_titles": [["title", "A Survey of Numerical Methods Utilizing Mixed Precision Arithmetic"], ["section", "Data and communication compression for multiprecision algorithms"], ["subsection", "Data compression"]], "content": "\\label{sec:compress}\nData compression for reducing communications is another component\nneeded for the development of mixed-precision algorithms. \nThe simplest form that we consider is the casting, as discussed\nin Section~\\ref{subs:casting}. This is an example of a lossy compression.\nCasting from FP64 to FP32 for example, leads directly to a loss of \nabout 8 decimal digits of accuracy, but reduces the data size by\na factor of two. Casting has been used to accelerate the FP64 solvers\nin MAGMA up to $4\\times$ using the mixed-precision iterative \nrefinement \ntechniques~\nand we use it as benchmark to evaluate the potential of using other\ncompression algorithms.\nWe evaluated for example the possibility to use ZFP \ncompression. ZFP provides lossy compression \nalgorithms, where the compression mode can be specified by the \nuser as either fixed rate, fixed accuracy, or fixed precision\n~. Analysis for the round-off error introduced by \nZFP in compressing floating-point data is presented\nin~.\nThe values in\nthis experiment are taken random numbers and the compression \nspecified is $4\\times$. Note that compared to casting, the\ncompression rate is as casting to FP16, but the accuracy is\ncomparable to casting to FP32. These results make it feasible\nto use tools like ZFP to accelerate memory-bound codes, \ne.g., like FFT (see Section~\\ref{sec:fft}), up to $4\\times$ \nwhile loosing about 8 decimal digits of accuracy.\n\\iffalse\n\\begin{figure}[tb]\n\\centering\n\\includegraphics[width=0.7\\linewidth]{graphics/ZFPcompress4}\n\\caption{Round-off error due to lossy $4\\times$ compression \n         of FP64 data using ZFP on Nvidia V100 GPU.}\n\\label{fig:zfpfour}\n\\end{figure}", "cites": [8327], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic comparison between casting and ZFP compression, integrating limited information from the cited paper on ZFP error analysis. It identifies a potential trade-off between compression rate and accuracy, but lacks deeper synthesis of broader themes and does not present a novel framework. The critical analysis is modest, focusing on performance and accuracy implications rather than methodological limitations."}}
{"id": "e9d2978c-0fc5-400d-9ae6-2f37c3ca5d7a", "title": "Using different precision formats in Multigrid methods", "level": "subsection", "subsections": [], "parent_id": "cdcc9a52-7e5b-4d89-88d5-3b4f969f45c8", "prefix_titles": [["title", "A Survey of Numerical Methods Utilizing Mixed Precision Arithmetic"], ["section", "Multiprecision efforts decoupling the arithmetic format from the memory format"], ["subsection", "Using different precision formats in Multigrid methods"]], "content": "Multigrid methods are highly effective iterative methods. There are basically two types of multigrid methods: geometric multigrid methods (GMG) and algebraic multigrid methods (AMG). GMG requires actual grids on each level to generate its components, whereas AMG can be considered more like a ‘black box’ method, in that it can be given a matrix and right hand side and will generate the components for each level automatically using sensible heuristics. These methods are an interesting target for multiprecision treatment due to their different components which affect the overall algorithm in different ways.\nGMG and AMG components combine smoothers, coarser grid, restriction and prolongation operators on each level. In addition, it is of interest to investigate changes in precision on different levels. Finally, GMG and AMG can be used as preconditioners to other solvers, i.e. there is potential to use lower precision across the whole preconditioner.\nHistorically, most work focused on the use of a lower precision GMG or AMG method as a preconditioner to a double precision solver. Additionally, there are attempts to apply ZFP~ within MG or establish an error analysis framework for AMG.\nLjungkvist and Kronbichler successfully use mixed precision to solve the Laplace problem for different orders with a matrix-free geometric multigrid approach. Their solver infrastructure allows for using mixed-precision arithmetic that performs the multigrid V-cycle in single precision with an outer correction in double precision, increasing throughput by up to 83 percent.\nSimilarly, Glimberg et al  use a single precision multigrid to precondition a double precision defect correction scheme and solve the Laplace problem within a nonlinear water wave application on a GPU architecture. They achieve a speedup of up to 1.6 of the mixed precision version over the double precision version, a speedup of 1.9 for a purely single precision version. \nYamagishi and Matsumura  also apply a single precision multigrid to a double precision conjugate gradient solver to the Poisson/Helmholtz problem within their non-hydrostatic ocean model. They report a speedup up to 2 for a single precision Matvec over a double precision one and improved overall times using this approach, however they compare the full application run only to their CPU version.\nThere are various publications that pursue the same strategy of using a single precision AMG preconditioner to a double precision solver. \nEmans and van de Meer perform a careful analysis of the individual kernels of preconditioned Krylov solvers on multi-core CPUs, including sparse matrix-vector multiplications (SpMV) which make up a large portion of AMG. They also consider the effect of communication, where lower precision leads to smaller size messages, but latencies are still an issue, particularly on the coarsest levels of AMG.  They find that the use of mixed precision for the preconditioner barely affects convergence and therefore speedups for the kernels, which were between 1.1 and 1.5, can potentially carry over to the whole solver and lead to improvements of runtimes within CFD applications. \nSumiyoshi et al  investigate AMG performance on a heterogeneous computer architecture with both CPUs and GPUs for isotropic and anisotropic Poisson problems. They consider smoothed aggregation AMG as a stand-alone solver. They carefully analyze different portions of the algorithm on five different architectures, including one multi-core CPU cluster. They report speedups between 1.2 and 1.6 on the GPU-CPU architectures for the mixed-precision implementation over the double precision version. These speedups are related to SpMV performance (between 1.6 and 1.8) on these architectures. However, the mixed-precision version was slightly slower on the CPU-only architecture, which achieved barely any improvement for the SpMV operations.\nRichter et al  examine the performance of a single precision AMG solver (ML and PETSc) applied to a double precision PCG solver. They apply the method for an electrostatic simulation of the high voltage isolator on a GPU/CPU computer architecture. Their mixed precision version takes about 84 percent of the time of the double precision version.\nAn approach described in a presentation by Kate Clark  takes the use of mixed precision even further to involve half precision. Clark and collaborators achieved good results using a double precision defect correction approach with a single precision Krylov solver and a half precision AMG preconditioner.\nAnother interesting related study by Fox and Kolasinski  examines the use of ZFP, a lossy compression algorithm, within multigrid. Due to the local structure of ZFP, ZFP can easily be integrated into numerical simulations without changing the underlying algorithms. However, since ZFP is a lossy algorithm, it will introduce some error, thus, it is important to understand if the error caused by ZFP overwhelms other traditional sources of error, such as discretization error. The study shows that for MG on a Poisson problem, applying ZFP to the approximation vector, can significantly decrease memory use and thus is expected to decrease runtimes, while the generated errors stay below discretization error. Since a hardware version of ZFP is not available yet, no actual runs were possible, however the results show good potential to use GMG and/or AMG as a preconditioner.\nCurrently, Tamstorf et al  appear to be the only ones who have investigated\nthe theory of multi-precision multigrid methods. Their original intent was to\nimprove the appearance of the movement of cloth within Disney movies, which\nrequires higher than FP64 accuracy. However, their theory\napplies equally to decreased precision. They have created\na theoretical framework with rigorous proofs for a mixed-precision version\nof multigrid for solving the algebraic equations that arise from\ndiscretizing linear elliptic partial differential equations (PDEs).  The arising matrices \nbeing sparse and symmetric positive definite\nenable the use of the so-called energy or $A$ norm to establish convergence\nand error estimates. Bounds on the convergence\nbehavior of multigrid are developed and analyzed as a function of the matrix condition number. Both\ntheoretical and numerical results confirm that convergence\nto the level of discretization accuracy can be achieved with mixed-precision\nversions of V-cycles and full multigrid.   This framework is inspired\nby the results of Carson and Higham  but\nultimately provides tighter bounds for many PDEs. \nTamstorf et al  further extend their theoretical framework to include the quantization error.\nThey use the bounds to guide the choice of precision level in their progressive-precision multigrid scheme\nby balancing quantization, algebraic and descretization errors. They show that while iterative refinement is susceptible to quantization errors during the\nresidual and update computation, the V-cycle used to compute the correction in each iteration is\nmuch more resilient, and continues to work if the system matrices in the hierarchy become indefinite\ndue to quantization.", "cites": [191, 192], "cite_extract_rate": 0.15384615384615385, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.7, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple studies to build a coherent narrative about using mixed precision in multigrid methods, highlighting common strategies such as using single or half precision for preconditioners. It critically evaluates the impact of precision changes on performance and convergence, noting limitations (e.g., no speedup on CPU-only systems). The section also abstracts beyond individual works by identifying broader patterns, such as the resilience of the V-cycle to quantization errors and the theoretical underpinnings enabling discretization-error-accurate solutions."}}
{"id": "585f6d96-f31c-4f0d-a62c-848963875a31", "title": "Low precision and multiprecision technology for Machine Learning", "level": "section", "subsections": [], "parent_id": "be98fc59-cc09-4da8-915d-8f26975446b1", "prefix_titles": [["title", "A Survey of Numerical Methods Utilizing Mixed Precision Arithmetic"], ["section", "Low precision and multiprecision technology for Machine Learning"]], "content": "Modern \\ac{hpc} hardware continues to experience an ongoing shift\ntowards supporting a variety reduced-precision formats for representing\nfloating-point numbers in order to offer a much increased performance\nrate. However, portability is often of little concern as the hardware\ntends to serve only a specific set of workloads that are of special\ninterest to the particular vendor.  The examples include Intel's Cascade\nLake Vector Neural Network Instructions (VNNI) and the recently\nannounced Xe platform for graphics cards, AMD's Radeon Instinct cards\n(MI5, MI8, MI25, MI55, MI60) and NVIDIA's compute cards from the Pascal,\nVolta, and Turing series. Finally, ARM included 16-bit floating point\n(FP16) in its NEON vector unit specification VFP 8.2-A.  These\naccelerators follow two types of specifications for 16-bit\nfloating-point format: IEEE-compliant FLOAT16 and extended-range\nBFLOAT16.\nAt the same time, a new breed of accelerators take the use of reduced\nprecision to a new level as they target new machine learning workloads.\nThis new hardware includes  Cloud AI 100 by Qualcomm, Dot-Product Engine\nby HPE, Eyeriss by MIT~, TPU by\nGoogle~, MAERI by Georgia Institute of Technology~ Deep Learning Boost\nby Intel, CS-1 by Cerebras, and Zion by Facebook.\nIn general, the machine learning community has been more aggressive in evaluating multiple precision to the extent that even a 1-bit Stochastic Gradient Descent has been considered~. The typical use case in machine learning is to use the training with 32-bit arithmetic and use different precision for the inference task. The quantization for the inference is supported in popular frameworks like TensorFlow~ and pyTorch~. Quantization is the approach to store the tensors and compute on them using bitwidths lower than floating point bitwidths. Even in machine learning frameworks, the support for quantizations is limited to just the key functionality needed for a convolutional neural networks or recurrent neural networks with some limited hardware support. For example, pyTorch and TensorFlow supports 8-bit quantization for activation and weights. This allows using 8-bits for inference where the additional 2-4x performance is necessary. On the training front, it has been shown that 16-bit training is sufficient for certain tasks~. The recent Gordon Bell winner demonstrated that lower-precison training can be used for scientific machine learning tasks as well~.\nThe analogous effort to the work in deep learning to the examples of our interest in scientific computing involves training the network in\nlower precision and performing inference in a higher\none~.\nThe compute imbalance between training and inference is even higher than\nthat of factorization and the subsequent iterative refinement. Another\ndifference is that in the context of neural network training, lowering\nthe precision may be incorporated into the model as a regularizer.", "cites": [7198, 194, 7199, 7005, 193], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of hardware and software trends in low-precision computing for machine learning. It integrates some references, such as mentioning 16-bit training from Paper 1 and TPU inference from Paper 4, but lacks a deeper synthesis or comparative framework. There is little critical analysis of the cited works or abstraction to broader principles in numerical methods."}}
