{"id": "2ee018e6-3e07-48f0-9c7b-f9cb9ca3e3ab", "title": "Neural Models", "level": "subsubsection", "subsections": [], "parent_id": "7e358581-87c3-419a-a5aa-c05934b0b72d", "prefix_titles": [["title", "A Survey of Syntactic-Semantic Parsing Based on Constituent and Dependency Structures"], ["section", "Constituent Parsing"], ["subsection", "Chart-Based Parsing"], ["subsubsection", "Neural Models"]], "content": "\\newcite{socher2010learning} is the first work to define scores over phrases by recursive neural networks.\nThe CFG-based constituent trees can be naturally modeled in this way.\nNeural CRF parsing is accordingly proposed by \\newcite{durrett-klein-2015-neural},\nwhich can be regarded as a neural enhancing of \\newcite{hall-etal-2014-less}.\nThe work simply uses feed-forward neural networks to encode atomic features instead of human composition.\nNotice that it is different from \\newcite{socher2010learning} as no recursive composition is used here.\n\\newcite{stern-etal-2017-minimal} propose state-of-the-art chart-based neural models.\nOn the one hand, they use deep bidirectional long-short term memory (LSTM) neural networks to enhance sentence representations,\ndesigning sophisticated strategies for span representation.\nOn the other hand, they also adopt top-down incremental parsing for decoding,\nwhich dilutes the differences between chart-based and transition-based approaches.\nTheir results are very strong on par with the state-of-the-art transition-based methods at the same time.\nThe work is further followed by \\newcite{gaddy-etal-2018-whats} with extensive analysis and \\newcite{kitaev-klein-2018-constituency} with a self-attentive encoder.\nIn particular, \\newcite{kitaev-klein-2018-constituency} exploit contextualized word representation including ELMo \nand BERT ,\nleading to almost the best parsing performance in the literature.", "cites": [7, 8385], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several key neural models for chart-based constituent parsing, connecting them through their use of neural networks and their relation to prior work. It provides some critical distinctions, such as the difference between recursive composition (Socher et al.) and non-recursive approaches (Durrett and Klein). While it identifies broader trends like the shift from human-defined features to neural representations and the influence of contextualized word embeddings, the abstraction remains somewhat limited to parsing-specific observations."}}
{"id": "a4b498b6-823b-4aad-bc1d-f3f3724e3a56", "title": "Statistical Models", "level": "subsubsection", "subsections": [], "parent_id": "70d9f357-92d8-4745-af7e-2d0fedf8a7e9", "prefix_titles": [["title", "A Survey of Syntactic-Semantic Parsing Based on Constituent and Dependency Structures"], ["section", "Constituent Parsing"], ["subsection", "Transition-Based Parsing"], ["subsubsection", "Statistical Models"]], "content": "The transition-based models demonstrate highly promising for constituent parsing .\nThe key idea is to define a transition system with transition states and actions,\nwhere states denote partial parsing outputs,\nand actions specify next-step state-transition operations.\nTransition actions indicate the incremental tree construction process.\nFor constituent parsing, typical actions include the \\emph{shift} to building terminal nodes, the \\emph{unary} to building unary nodes,\nand the \\emph{binary} to building binary nodes.\nThe details can be referred to as \\newcite{sagae-lavie-2005-classifier}.\nThe model is also commonly referred to as the shift-reduce model, where \\emph{unary} and \\emph{binary} are actions of reduction.\nBy converting constituent parsing into predicting a sequence of transition actions,\ndiscriminant classifiers such as max-entropy and support vector machine (SVM) can be applied for the prediction,\nwith rich manually-crafted features.\nThe initial shift-reduce model classifies the sequence of actions for a single constituent tree separately,\ngreedily searching for the best output constituent tree,\nwhich may suffer the error propagation problem since the early step errors can affect later predictions.\nTo this end, globally modeling with beam search is proposed to alleviate the problem,\nwhich decodes the total sequence of actions for a full constituent tree as a whole .\nThe discriminative perceptron-style online learning greatly promotes this line of work ,\nwhich enables legal parameter optimizations towards inexact search.\nFor feature engineering, the contextual lexicalized words, POS tags, distances and their compositions are all extensively investigated,\nwhich can be referred to  for details.", "cites": [9026], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of transition-based statistical models for constituent parsing, introducing key concepts such as shift, unary, and binary actions. It mentions some technical approaches like max-entropy and SVM classifiers but lacks comparative or evaluative depth. The reference to Sagae and Lavie (2005) is minimal and not elaborated, limiting the synthesis and abstraction beyond surface-level explanation."}}
{"id": "1b3d47a3-c033-4c89-9cf0-e261bd71d186", "title": "Neural Models", "level": "subsubsection", "subsections": [], "parent_id": "70d9f357-92d8-4745-af7e-2d0fedf8a7e9", "prefix_titles": [["title", "A Survey of Syntactic-Semantic Parsing Based on Constituent and Dependency Structures"], ["section", "Constituent Parsing"], ["subsection", "Transition-Based Parsing"], ["subsubsection", "Neural Models"]], "content": "\\newcite{watanabe-sumita-2015-transition} and \\newcite{wang-etal-2015-feature} could be the direct extensions of \\newcite{zhu-etal-2013-fast} by using neural networks.\nThe composition of atomic features is achieved by feed-forward neural networks.\n\\newcite{cross-huang-2016-incremental} find that the greedy style decoding can also achieve highly competitive performance when a deep LSTM encoder is exploited.\nThen, several studies suggest dynamic oracles to optimize greedy constituent parsers\n.\nThe main idea is to let models make optimum decisions when facing erroneous transition states .\nA proportion of training instances with erroneous transition states and their oracle actions are added into the original training corpus.\nThere have been several studies exploiting different transition strategies.\n\\newcite{dyer-etal-2016-recurrent} suggest the recurrent neural work grammar (RNNG),\nwhich is a top-down transition-based system.\n\\newcite{liu-zhang-2017-order} design an in-order transition system to make a compromise between top-down and bottom-up transitions.\n\\newcite{coavoux-etal-2019-unlexicalized} present a novel system with an additional GAP action for discontinuous constituency parsing,\nand they also find that unlexicalized models give better performance.\n\\newcite{fernandez2019faster} optimize the transition actions to facilitate the construction of non-binarized constituent nodes,\navoiding the preprocessing of binarization for constituent trees.\n\\newcite{kitaev2019tetra} suggest the tetra-tagging system, which combines sequence tagging and transition action classification.\nThe system achieves state-of-the-art performance on the benchmark PTB dataset with BERT representations.", "cites": [6489], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of various neural transition-based constituent parsing models but lacks in-depth synthesis or critical evaluation. It lists different approaches and their features without clearly connecting them to broader trends or analyzing their strengths and weaknesses. The narrative remains largely factual and does not offer meta-level insights or a novel framework."}}
{"id": "c2138b06-43bc-43b9-9d22-96f5318c5451", "title": "Other Frameworks", "level": "subsection", "subsections": [], "parent_id": "45541c9b-5413-4a2e-8549-37536168b30f", "prefix_titles": [["title", "A Survey of Syntactic-Semantic Parsing Based on Constituent and Dependency Structures"], ["section", "Constituent Parsing"], ["subsection", "Other Frameworks"]], "content": "Neural networks such as deep LSTM and multi-head self-attention\nare capable of encoding global features implicitly into their final representations,\nwhich weakens the role of decoding as a source of feature induction.\nBased on the observation,\nseveral studies attempt to use simple frameworks,\naiming for a wide community for parsing.\nOne representative attempt is to exploit neural sequence-to-sequence models for structural constituent parsing .\nThe key idea is to first linearize a phrase-structural constituent tree into a sequence of symbols by certain traversing strategies,\nand then directly feed the pair of input words and output symbols into a standard sequence-to-sequence model.\nThese models require large-scale corpora for training,\nwhich could be obtained by auto-parsed high-confidence constituent trees from other state-of-the-art parsers.\nNeural sequence labeling models have also been investigated for constituent parsing .\n\\newcite{gomez-rodriguez-vilares-2018-constituent} propose the first work of this line,\nwhich exploits the lowest common ancestor between adjacent words as clues to encode the word roles.\n\\newcite{vilares2020parsing} extend the work by language modeling and enhance parsing with pretraining.\nFurther, more direct schemes have been proposed with local modeling for constituent parsing.\n\\newcite{shen-etal-2018-straight} directly predict the distance of constituent phrases\nand then decode greedily in a top-down way for a full constituent tree.\nSimilarly, \\newcite{teng-zhang-2018-two} propose two models based on local span prediction,\nachieving highly competitive performance on par with transition-based models.\nRecently, \\newcite{zhou-zhao-2019-head} present to exploit the HPSG-based grammar for constituent parsing,\nand further power the model with XLNet word representations ,\nachieving the top performances for both CTB and PTB datasets.\n\\newcite{mrini2019rethinking} revise the multi-head self-attention mechanism in \\newcite{zhou-zhao-2019-head},\nleading to a similar performance with a smaller number of layers.\n\\setlength{\\tabcolsep}{1.5pt}\n\\begin{table}[H]\n\\begin{threeparttable}\n\\footnotesize\n\\caption{A comparison of representative dependency parsing models, where UAS are reported, PTB and CTB5.1 (CTB in the Table for short) are two benchmark datasets for the English and Chinese parsing, respectively.  } \\label{table:dep:performance}\n\\begin{tabular}{l|c|cc}\n\\hline\nModel &  Main Features &  PTB &   CTB \\\\ \\hline \\hline\n\\multicolumn{4}{c}{ \\texttt{Graph-based, Statistical Models}}  \\\\ \\hline\n\\sworkcite{mcdonald-etal-2005-online} &  1-order & 90.9  & 83.0 \\\\\n\\sworkcite{McDonald2006}  &   2-order & 91.5  & 85.2 \\\\\n\\sworkcite{koo-etal-2008-simple} &   word clusters  & 93.2 &  N/A \\\\\n\\sworkcite{chen-etal-2009-improving} &  auto subtrees & 93.2 &  86.7 \\\\\n\\sworkcite{bohnet-2010-top} & feature hashing & 92.9 & N/A  \\\\\n\\sworkcite{koo-collins-2010-efficient}  &   3-order & 93.0  & 86.0 \\\\\n\\sworkcite{ma-zhao-2012-fourth}  &  4-order & \\bf 93.4  & \\bf 87.4 \\\\\n\\hline\n\\hline\n\\multicolumn{4}{c}{ \\texttt{Transition-based, Statistical Models}}  \\\\ \\hline\n\\sworkcite{nivre-cl08} (a) &  arc-standard & 89.7  & 82.7 \\\\\n\\sworkcite{nivre-cl08} (b)  &  arc-eager & 89.9  & 80.3 \\\\\n\\sworkcite{zhang-clark-2008-tale}  &   global learning, beam & 91.4  & 84.3 \\\\\n\\sworkcite{zhang-nivre-2011-transition}  &   rich non-local features & \\bf 92.9  & \\bf 86.0 \\\\\n\\sworkcite{goldberg-nivre-2012-dynamic}  & dynamic oracle & 91.0  & 84.7 \\\\ \\hline\n\\hline\n\\multicolumn{4}{c}{ \\texttt{Graph-based, Neural Models}}  \\\\ \\hline\n\\sworkcite{pei-etal-2015-effective} & feed-forward & 93.3  & N/A \\\\\n\\sworkcite{zhang-etal-2016-probabilistic} & CNN & 93.4  & 87.7 \\\\\n\\sworkcite{wang-chang-2016-graph} &  2-layer LSTM  & 94.1 &  87.6 \\\\\n\\sworkcite{kiperwasser-goldberg-2016-simple}  & 2-layer LSTM & 93.1  & 86.6 \\\\\n\\sworkcite{dozat2016deep}  & 3-layer LSTM, biaffine  & 95.7  & 88.9 \\\\\n\\sworkcite{li2019self} (a) & self-attentive & 95.9  & 92.2 \\\\\n\\sworkcite{li2019self} (b)  &  +ELMO & 96.6  & 90.3 \\\\\n\\sworkcite{li2019self} (c)  &  +BERT & \\bf 96.7  & \\bf 92.2 \\\\\n\\sworkcite{ji-etal-2019-graph}  &  GNN & 96.0  & N/A \\\\ \\hline\n\\hline\n\\multicolumn{4}{c}{ \\texttt{Transition-based, Neural Models}}  \\\\ \\hline\n\\sworkcite{chen-manning-2014-fast}  &  feed-forward & 91.8  & 83.9 \\\\\n\\sworkcite{dyer-etal-2015-transition}  &  stack-LSTM & 93.1  & 87.2 \\\\\n\\sworkcite{zhou-etal-2015-neural}  &   global learning, beam & 93.3  & N/A \\\\\n\\sworkcite{andor-etal-2016-globally}  &   global learning, beam & 94.6  & N/A \\\\\n\\sworkcite{kiperwasser-goldberg-2016-simple} & 2-layer LSTM &  93.9  &  87.6 \\\\\n\\sworkcite{ballesteros-etal-2017-greedy}  & char, stack-LSTM & 93.6  & 87.6 \\\\\n\\sworkcite{ma-etal-2018-stack} &  3-layer LSTM  & \\bf 95.9  & \\bf 90.6   \\\\  \\hline\n\\hline\n\\multicolumn{4}{c}{ \\texttt{Other Methods (report neural models only) }}  \\\\ \\hline\n\\sworkcite{kiperwasser-goldberg-2016-easy}  & easy-first & 93.0  & 87.1 \\\\\n\\sworkcite{li-etal-2018-seq2seq}  & sequence-to-sequence & 92.1  & 86.2 \\\\\n\\sworkcite{strzyz-etal-2019-viable}  & sequence labeling & 93.7  & N/A \\\\\n\\sworkcite{zhou-zhao-2019-head} & HPSG grammar & 97.2  & \\bf 91.2 \\\\\n\\sworkcite{mrini2019rethinking} & HPSG, improved attention  & \\bf 97.3  & \\bf N/A \\\\\n\\hline\n\\end{tabular}\n\\end{threeparttable}\n\\end{table}", "cites": [6490, 11, 2640], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a comparative overview of various constituent parsing frameworks, particularly neural models, and integrates them by discussing common themes such as sequence-to-sequence, sequence labeling, and local span prediction. However, it lacks deeper critical analysis of trade-offs or limitations of these methods. Some level of abstraction is achieved by grouping models into categories like graph-based, transition-based, and other methods, but it does not present a novel or meta-level synthesis of the field."}}
{"id": "8ff2b71e-2cb9-41c4-b4d0-6b1cfb9d55de", "title": "Neural Models", "level": "subsubsection", "subsections": [], "parent_id": "def2b85a-529c-496d-a307-d10b607b90cb", "prefix_titles": [["title", "A Survey of Syntactic-Semantic Parsing Based on Constituent and Dependency Structures"], ["section", "Dependency Parsing"], ["subsection", "Graph-Based Parsing"], ["subsubsection", "Neural Models"]], "content": "\\newcite{pei-etal-2015-effective} present a graph-based neural model\nby embedding all discrete atomic features in the traditional statistical MST models and\nthen composing these embeddings with a similar feed-forward network of \\workcite{chen-manning-2014-fast}.\nConvolution neural network is then applied for neural feature composition in \\newcite{zhang-etal-2016-probabilistic}.\nFollowing, deep bidirectional LSTMs are exploited to substitute the simple neural feed-forward network .\nAs sentence-level global information can be encoded through these neural structures,\nthe performance gap between first- and higher-order decoding is largely reduced.\n\\newcite{dozat2016deep} propose a deep biaffine parser which achieves the impressive performances,\nboosting the UAS and LAS numbers into a new degree.\nThe parser exploits a three-layer bidirectional LSTM as the encoder,\nand a biaffine operation as the decoder to score all possible dependency edges.\nThis work adopts several tricks to reach their final performance,\ne.g., the node-level dropouts, and the same dropout mask at every recurrent timestep.\n\\newcite{li2019self} further enhance the biaffine parser with self-attentive encoder and\ncontextualized word representations such as ELMo and BERT .\n\\newcite{ji-etal-2019-graph} exploit graph neural networks to better the input sentence encoder.", "cites": [8385, 8242, 7], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a straightforward chronological overview of neural models in graph-based dependency parsing, listing each paper and its method without meaningful synthesis or comparison. It lacks critical evaluation of the approaches or their limitations, and does not abstract to broader trends or principles in the field."}}
{"id": "cb812e5a-0c1f-41b0-891c-0c0c8af7c185", "title": "Neural Models", "level": "subsubsection", "subsections": [], "parent_id": "5f0b8234-d541-4cec-b500-c7a1e56fa85d", "prefix_titles": [["title", "A Survey of Syntactic-Semantic Parsing Based on Constituent and Dependency Structures"], ["section", "Dependency Parsing"], ["subsection", "Transition-Based Parsing"], ["subsubsection", "Neural Models"]], "content": "\\workcite{chen-manning-2014-fast} is one millstone work for neural dependency parsing,\nwhich substitutes traditional manually-crafted discrete features with neural features.\nThe work uses simple feed-forward neural networks to compose the embeddings of all atomic features automatically,\nand thus is free of feature engineering.\nFinally, the proposed model obtained much better performance than the corresponding statistical baseline.\nPretrained word embeddings and the neural composition function are the keys to success.\nThere exist several directions to improve the performance of neural transition-based dependency parsing.\nFirst, we can exploit better neural network structures.\nStack-LSTM is presented by \\newcite{dyer-etal-2015-transition} and then followed by several studies ,\nwhich can represent transition states by utilizing partial structural information.\nIn parallel, deep bidirectional LSTM is also investigated .\n\\newcite{ma-etal-2018-stack} exploit a similar encoder as \\newcite{dozat2016deep}, achieving slightly better performances than \\workcite{dozat2016deep}.\nIn fact, with powerful neural encoders, especially pretrained contextualized word representations,\nthe performance difference between graph-based and transition-based is very marginal .\nSeveral researchers suggest global learning with beam-search strategy in \\workcite{zhang-nivre-2011-transition} under the neural setting.\n\\newcite{zhou-etal-2015-neural} make the pioneer attempts for this goal,\nwhich is further perfected with a theoretical guaranty by \\newcite{andor-etal-2016-globally}.\nThese models have achieved state-of-the-art performance before the biaffine parser .\nOne major drawback is that the strategy suffers from the efficiency problem due to the beam search.\nThe dynamic oracle strategy is applied as well making the greedy transition-based neural dependency parsers\n.\nRecently, both global learning and dynamic oracle are difficult to give much-improved capacity\nwhen pretrained contextualized word representations are exploited.", "cites": [8243, 8242, 6492, 6491, 1171, 8244], "cite_extract_rate": 0.75, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of neural models in transition-based dependency parsing, connecting key works like Stack-LSTM and deep bidirectional LSTM. It identifies trends (e.g., the diminishing gap between graph-based and transition-based methods) and discusses the trade-offs of strategies such as beam search and dynamic oracles. While it offers some critical evaluation, especially regarding efficiency, the abstraction level could be deeper to fully generalize overarching principles in the field."}}
{"id": "856e1d07-0da2-4c54-b927-9ca9fdaeacb7", "title": "Model Transferring", "level": "subsubsection", "subsections": [], "parent_id": "1815802f-9ff6-4f30-a090-71db7228a63a", "prefix_titles": [["title", "A Survey of Syntactic-Semantic Parsing Based on Constituent and Dependency Structures"], ["section", "Cross-Lingual Parsing"], ["subsection", "Unsupervised Setting"], ["subsubsection", "Model Transferring"]], "content": "The model transferring approach is straightforward for cross-lingual parsing.\nThe most effort is concerned with language-independent features,\nwhich play consistent functions across languages.\nThis line of work is initially presented by \\newcite{zeman-resnik-2008-cross} which suggests delexiciallized models for cross-lingual dependency parsing,\nand is further developed by \\newcite{mcdonald-etal-2011-multi} for multi-source transferring,\nwhere multiple source languages are used to enhance a target language.\nSeveral researchers resort to various non-lexical features to enhance the delexicalized cross-lingual models\n.\nRecently, \\newcite{tackstrom-etal-2012-cross} exploit cross-lingual word clusters,\nwhich is one king of cross-lingual word representations.\nUnder the neural setting, the exploration of cross-lingual word representations is greatly facilitated.\n\\newcite{guo-etal-2015-cross} propose to use cross-lingual word embeddings for lexicalized cross-lingual dependency parsing.\nThis method is then received much attention and\ncan be further enhanced by various ways such as better neural structures \nand multi-source adaption .\nCross-lingual pretrained contextualized word representations give the state-of-the-art performances of this category.\n\\newcite{schuster-etal-2019-cross} provide a method to learn contextual ELMO representations effectively and\nthen apply the representations on the task, achieving much better performances than cross-lingual word embeddings.\n\\newcite{wang-etal-2019-cross} and \\newcite{wu-dredze-2019-beto} apply cross-lingual mBERT\nto zero-shot cross-lingual dependency parsing.\n\\newcite{lample2019cross} introduce the XLM concurrently to mBERT, which is also a kind of\nstrong multilingual contextualized word representations for cross-lingual parsing .\nAll these recent studies lead to state-of-the-art performances in the literature of this category.", "cites": [6493], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of model transferring in cross-lingual parsing, mentioning key works and their contributions in chronological order. While it attempts to connect different papers by highlighting the shift from delexicalized models to cross-lingual embeddings and contextualized representations, the synthesis is minimal and lacks a deeper analytical framework. There is little critical evaluation or identification of broader trends beyond what is explicitly stated in the papers."}}
{"id": "7e8a332b-5848-44e1-bb7d-f81c702c4a2b", "title": "Annotation Projection", "level": "subsubsection", "subsections": [], "parent_id": "1815802f-9ff6-4f30-a090-71db7228a63a", "prefix_titles": [["title", "A Survey of Syntactic-Semantic Parsing Based on Constituent and Dependency Structures"], ["section", "Cross-Lingual Parsing"], ["subsection", "Unsupervised Setting"], ["subsubsection", "Annotation Projection"]], "content": "The annotation projection approach requires slightly more effort compared with model transferring,\nwhich aims to build a pseudo training corpus through bitext projection.\nWith the pseudo training corpus, the final model can capture rich target-language characteristics.\nThe method relies on a set of parallel sentences between the source and target languages.\nA source parser trained on the source treebank is used to parse the source-side sentences of the parallel corpus,\nand then the automatic source annotations are projected onto the target language sentences according to word alignments,\nresulting in the final pseudo training corpus.\nThere are a range of strategies to achieve the goal.\nFor example, we can use different kinds of parallel corpora,\nsuch as EuroParl and the book Bible,\nand can also exploit various sophisticated methods to improve the projection quality.\nFor constituent parsing,  \\newcite{snyder-etal-2009-unsupervised} exploit the method for unsupervised constituent parsing,\nand find that it can significantly outperform the purely-unsupervised models.\n\\newcite{jiang-etal-2011-relaxed} suggest an EM algorithm to incremental boost the quality of the projected constituent trees with relaxing constraints.\nThe number of studies on constituent parsing is relatively small,\nwhich may be possible due to that the projection of constituent structures is very complex.\nFor dependency parsing, \\newcite{hwa2005bootstrapping} present the first work of this category,\nand then the approach has been extensively studied under different settings, such as\nconfidence-aware learning ,\nneural network enhancing ,\nand multi-source adaption .\nInterestingly, \\newcite{jiang2015joint} propose a joint model for cross-lingual constituent and dependency parsing with annotation projection.\nThe approach achieves great success for cross-lingual dependency parsing.", "cites": [6494], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the annotation projection approach, mentioning a few representative papers and their contributions. It does not deeply synthesize or connect ideas across works, nor does it offer critical evaluation or highlight limitations. The level of abstraction is minimal, focusing on specific applications and methods rather than broader linguistic or computational principles."}}
{"id": "028e35d7-a850-4e17-b54c-010b7d616325", "title": "Joint POS Tagging and Parsing", "level": "subsection", "subsections": [], "parent_id": "6990ee8e-1b57-418d-aa02-ad2b46dc41f6", "prefix_titles": [["title", "A Survey of Syntactic-Semantic Parsing Based on Constituent and Dependency Structures"], ["section", "Joint Models"], ["subsection", "Joint POS Tagging and Parsing"]], "content": "For joint POS tagging and constituent parsing,\nthe chart-based PCFG parsing naturally performs the two tasks concurrently ,\nwhere POS tags can be directly induced from the bottom lexical rules.\nBased on the transition-based framework, joint POS tagging and constituent parsing can be\neasily achieved by the shift operation with one additional parameter to indicate the POS tag of the processing word.\n\\newcite{wang-xue-2014-joint} investigate the joint task and present a number of non-local features.\n\\newcite{li-etal-2011-joint} propose the first joint model of POS tagging and dependency parsing based on graph factoring,\nwhere the basic scoring units are augmented with POS tags.\n\\newcite{li-etal-2012-separately} enhance the model with better learning strategies.\n\\newcite{hatori-etal-2011-incremental} is the first transition-based model for joint POS tagging and dependency parsing.\n\\newcite{bohnet-nivre-2012-transition} extend the transition-based model for non-projective dependency parsing.\nThe two kinds of models achieve comparable performances for both tasks.\nUnder the neural setting, \\newcite{alberti-etal-2015-improved} investigate\nthe model of \\newcite{bohnet-nivre-2012-transition} with neural features.\n\\newcite{zhang-weiss-2016-stack} suggest a joint POS tagging and dependency parsing model by stack propagation.\n\\newcite{yang2017joint} further investigate the neural joint task with LSTMs\nby using graph-based and transition-based frameworks, respectively.\nIn fact, the importance of joint modeling has been largely weakened\nas parsing without POS tags can also obtain strong performance which is close to the same model with POS tags .", "cites": [9027, 1171], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section lists various approaches to joint POS tagging and parsing, mentioning the frameworks and contributions of individual papers. However, it lacks a deeper synthesis of ideas, critical evaluation of methods, or abstraction to broader trends. The narrative remains largely descriptive and does not provide a cohesive analysis or comparative discussion."}}
{"id": "7d10b76d-891b-4635-8a29-8b635d66d356", "title": "Parser Application", "level": "section", "subsections": ["a15ac61c-7a30-49c9-a36a-9c3007fc5f71", "d14b5ead-4320-4835-9289-f382f70c2351"], "parent_id": "3041659b-6b9d-44e0-94d2-651838f3bcad", "prefix_titles": [["title", "A Survey of Syntactic-Semantic Parsing Based on Constituent and Dependency Structures"], ["section", "Parser Application"]], "content": "When a well-trained syntactic/semantic parser is available,\nhow to use it effectively to benefit for downstream applications is one important topic in the parsing community,\nwhich is also related to the verification of the usefulness of syntactic and semantic parsing.\nIn fact, the topic has been extensively studied,\nand the parsing outputs have been demonstrated effective for a number of tasks such as semantic role labeling ,\nrelation extraction , sentiment analysis \nand machine translation .\nThe exploration methods have major changes from the statistical discrete models\nto the neural models.\nHere we briefly summarize the mainstream approaches of parser exploration in terms of the two settings.", "cites": [8412, 6495, 8005, 6496], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a brief overview of how syntactic/semantic parsers are applied to downstream NLP tasks and mentions the transition from statistical discrete to neural models. While it lists several applications and references relevant papers, it lacks in-depth synthesis or comparison of these works and does not critically evaluate their strengths or limitations. It offers some generalization by grouping tasks but remains largely descriptive without deeper analytical or abstract insights."}}
{"id": "d14b5ead-4320-4835-9289-f382f70c2351", "title": "Representation Learning with Neural Networks", "level": "subsection", "subsections": [], "parent_id": "7d10b76d-891b-4635-8a29-8b635d66d356", "prefix_titles": [["title", "A Survey of Syntactic-Semantic Parsing Based on Constituent and Dependency Structures"], ["section", "Parser Application"], ["subsection", "Representation Learning with Neural Networks"]], "content": "One simple method to use parsing features based on neural networks is to embed all the atomic features,\nand then exploit sophisticated neural networks to compose them automatically.\nThe most representative method of this kind is the path-based LSTMs,\nwhich exploit LSTM over sequential-level constituent or dependency paths .\nThe recent tendency of using the end-to-end framework for the majority of NLP tasks\nleads to universal representations based on parser outputs.\nWe build a universal encoder with structural outputs of a parser, and then adapt them to different tasks by decoders, as shown by Figure \\ref{fig-parser-apply}.\nThere are several ways to build the encoder.\nHere we divide the methods into four types: recursive neural network; linearization-based; implicated structural-aware word representations and graph neural networks (GNN).\n\\begin{figure}[H]\n\\includegraphics[scale=0.75]{figures/parser-apply-crop.pdf}\n\\caption{Parser enhanced universal encoder for downstream tasks.}\n\\label{fig-parser-apply}\n\\end{figure}\nThe recursive neural network is one natural method to model tree-structural outputs,\nwhich composes a tree input from bottom-to-up or top-to-down incrementally.\nWe can use various composition operations leading to more sophisticated tree-level neural networks\nsuch as tree convolutions suggested by \\newcite{mou-etal-2015-discriminative} and Tree-LSTM proposed by \\newcite{tai-etal-2015-improved}.\nAll these related studies give improved performances for a range of tasks .\nThe key idea of the linearization-based methods is to convert structural inputs into a sequence of symbols,\nand then adopt standard sequential encoders to model the new sequence directly .\nUsually, the conversion can be referred to as the linearization process of transition-based parsers,\nor we can incrementally traverse a tree or graph in different ways.\nThe method has received fewer concerns which might be due to its extreme simplicity,\nalthough it is effective and meanwhile much efficient .\nThe implicit structural-aware word representations, firstly presented by \\newcite{zhang-etal-2017-end} for relation extraction, are similar to the idea of contextualized word representations,\nwhich exploit the hidden outputs of a well-pretrained parser as inputs for the downstream tasks .\nThis method can also efficiently represent structural information such as syntax and semantics.\nBesides, the method can be easily adapted to the multi-task-learning strategy for\nparser application ,\nwhile parser requires to be jointly trained in multi-task-learning.\nRecently, there are grown interests on the topic of graph neural networks,\nwhich can be naturally applied to encode structural syntactic and semantic graphs.\nIndeed, there have been several studies already\nby using either graph convolutional networks or graph attention networks ,\nand all these works demonstrate the effectiveness of GNN for structure encoding.", "cites": [8337, 6495, 6497, 6498, 9028, 6496, 6499, 265], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the cited works by grouping them into four categories—recursive neural networks, linearization-based methods, implicit structural-aware representations, and GNNs—highlighting their relationships and shared goals. It provides some critical evaluation by mentioning limitations such as error propagation in parsing and the simplicity of linearization methods. The abstraction level is strong as it generalizes the use of syntactic-semantic structures in neural encoders and outlines a universal framework for parser-enhanced representation learning."}}
{"id": "16e12ac4-216b-4e62-b2a3-c5d04b694d13", "title": "Universal Dependencies", "level": "subsection", "subsections": [], "parent_id": "46654902-5367-451c-b03d-c14f5b1c43e5", "prefix_titles": [["title", "A Survey of Syntactic-Semantic Parsing Based on Constituent and Dependency Structures"], ["section", "Corpus and Shared Tasks"], ["subsection", "Universal Dependencies"]], "content": "The present of Universal Dependencies (UD) has received great attention for facilitating multilingual researches,\nwhich aims to develop cross-linguistically consistent treebank annotation for multiple languages.\nUD can capture similarities as well as idiosyncrasies among typologically different languages such as English-alike languages, morphologically-rich languages\nand pro-drop languages.\nThe development of UD is initially based on Stanford typed dependencies  and the universal Google dependency scheme .\nNow it goes through several versions , with significant changes on the guidelines,\nalso supporting language-specific extensions when necessary.\nCurrently the UD treebank version 2.5 includes 157 treebanks over 90 languages.\nBesides multilingual dependency parsing, there is an increasing tendency to exploit them for evaluating monolingual dependency parsing based on the datasets as well .", "cites": [9029], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of Universal Dependencies (UD), mentioning its purpose, development, and current status. It cites one paper but does not synthesize insights across multiple sources or go beyond summarizing factual information. There is minimal critical evaluation or abstraction to broader trends or principles."}}
