{"id": "29469c09-cd5f-4556-9382-b0a3b4de184e", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "4410407c-7d7c-48f1-8174-a9a95319844f", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Introduction"]], "content": "\\setlength{\\abovecaptionskip}{0.cm}\n\tGenerative adversarial networks (GANs)  have been widely used in computer vision, such as image inpainting , style transfer , text-to-image translations , and attribute editing . GANs training is a two-player zero-sum game between a generator and a discriminator, which can be understood from different perspectives: (i) \"Real \\& Fake\" , (ii) \"Fitting distribution\" , and (iii) \"Training dynamics\" . GANs training suffers from several issues, for instance: non-convergence , mode collapses , gradient vanishing , overfitting , discriminator forgetting  and deficiency , and hyperparameters sensitivity . Many solutions to mitigate these issues have been proposed, focusing on designing new architectures , loss functions , optimization methods , regularization , and normalization . Among them, regularization and normalization techniques are compatible with loss functions, model structures, and tasks, which has attracted the attention of scholars. \n\tRegularization and normalization are widely applied in neural networks training to introduce prior knowledge. For supervised tasks, regularization in literature has been proposed to introduce some advantages like overfitting prevention , semi-supervised assumptions , manifold assumptions , feature selection , and low rank representation . On the other hand, normalization  is advantageous for the Stochastic Gradient Descent (SGD) , accelerating convergence and improving accuracy. Unlike the icing on the cake of supervisory tasks, regularization and normalization are utilized inevitably in weak-supervised and unsupervised tasks. GANs' training is a two-player zero-sum game having a solution to Nash equilibrium. The proposal of standard GAN is based upon the non-parametric assumption of the infinite capacity of networks, an unsupervised learning task. Likewise, a good number of research studies targeting GANs training from different perspectives argue that unconstrained training causes unstable training (generator  and discriminator ) and significant bias between real images and fake images (attributes domain  and frequency domain ). Therefore, a large amount of prior should be introduced into GANs training through regularization and normalization\n\t\\begin{figure}\n\t\t\\centering\n\t\t\\includegraphics[scale=.52]{figs/summary_new.pdf}\n\t\t\\caption{The overview of different perspectives of GANs training and the summary of the regularization and normalization for GANs.}\n\t\t\\label{fig:overview}\n\t\\end{figure}\n\tRegularization and normalization are effectively used to stabilize training and improve the performance of GANs in existing literature . Due to diverse nature of the topic, there is a need for systematic literature survey. There exist some literature studies , however, these studies either lack comprehensive coverage of the topic, miss detailed background information and theoretical analysis, or do not correlate different methods. In this paper, based on the different perspectives of GANs training, we propose a new taxonomy, denoted as \\textbf{\"Real \\& Fake\"}, \\textbf{\"Fitting distribution\"}, \\textbf{\"Training dynamics\"}, and \\textbf{\"Other methods\"}, for a better understanding of regularization and normalization during the GANs training as depicted in Figure \\ref{fig:overview}. {\\textbf{\"Real \\& Fake\"} is the low-level (intuitive) perspective of GANs, in which GANs is considered as \"counterfeiters-police\" competition. At this level, D estimates the real probability of both real and fake samples, which is similar to the bi-classification task. Therefore, prior information and additional supervision tasks in classification task are also urgent during the training process of the discriminator. Based on these, some regularization methods, such as {\\textit{Data Augmentation and Preprocessing}}}\\footnote{Data augmentation and preprocess introduce additional data and prior, which is similar to regularization. More importantly, both consistency regularization and self-supervision need different data transformation operations. Hence, this paper also discusses some works on this.}, {\\textit{Consistency Regularization}, and {\\textit{Self-Supervision}} are proposed to mitigate overfitting, improve the representation of discriminator, and avoid discriminator forgetting by introducing additional supervised information and data; \\textbf{\"Fitting distribution\"} is the middle-level perspective of GANs. At this level, generator is considered as a distribution mapping function, and the discriminator is a distribution divergence. Among various distances, Wasserstein distance is a popular form, and Lipschitz continuity is a necessary condition for achieving Wasserstein distance. Based on these, \\textit{Gradient Penalty}, \\textit{Weight Normalization}, \\textit{Weight Regularization}, and  \\textit{Gradient} \\textit{Normalization} are used to fulfill Lipschitz continuity and ensure training stability of discriminator; \\textbf{\"Training Dynamics\"} is the high-level (essential) perspective of GANs. At this level, GANs training is a two-player zero-sum game with a solution to Nash Equilibrium. To achieve theoretical local convergence, \\textit{Jacobian Regularization} needs to be used; Finally, \\textbf{\"Other methods\"} containing \\textit{Layer} \\textit{ Normalization} and \\textit{Inverse Gradient Penalty} are used for conditional generation and easing mode collapse, respectively}\n\tIn summary, we make the following contributions in this survey:\n\t\\begin{itemize}\n\t\t\\item \\textit{Comprehensive analysis of GANs training.} In this study, we analyze the GANs training from three perspectives including \"Real \\& Fake\", \"Fitting distribution\", and \"Training dynamics\". To the best of our knowledge, this survey is the first in this domain with comprehensive analysis.\n\t\t\\item \\textit{New taxonomy.} Based on the analysis of GANs training from different perspectives, we propose a novel taxonomy and contextualize the regularization and normalization of GANs comprehensively.\n\t\t\\item \\textit{ Comparison and analysis.} Following the taxonomy, we also provide quantitative and qualitative analysis and comparison for each type of regularization and normalization techniques, which has helped the researchers and practitioners navigate this space.\n\t\\end{itemize}\n\t{\\textbf{The Scope of This Survey.} This survey aims to systematically analyze the prevalent problems in the GANs training, such as non-convergence, mode collapses, gradient vanishing, and discriminator overfitting. Accordingly, different regularization and normalization technologies have been summarized. Of course, not all regularization and normalization technologies for GANs are covered in this survey. In some case, some regularization and normalization technologies are highly dependent on the task and data in the hand, we recommend looking for domain-specific regularization and normalization techniques from the following reviews: data-efficient generation , medical image generation, image super-resolution , biomedical informatics generation , Spatio- temporal data generation , text generation . Our survey is concerned with general technologies in GANs, which are not dependent on model structures, data, and task. We hope our study can provide general and universal insights of GANs for the community.}\n\tThe rest of this paper is organized as follows: Section 2 introduces the background and different training perspectives of GANs. Section 3, 4, 5, and 6\n\tdescribe regularization and normalization methods in different groups, respectively. Furthermore, we investigate the applications of regularization and normalization techniques that have been frequently employed in SOTA GANs in Section 7 and discuss the current problems and prospects for future work in Section 8.", "cites": [63, 89, 73, 86, 68, 85, 78, 76, 8317, 53, 60, 6998, 66, 57, 70, 88, 80, 6996, 72, 54, 58, 62, 59, 7190, 55, 69, 77, 6997, 82, 74, 56, 81, 87, 6999, 8316, 84, 71, 61, 75, 67, 65, 83, 64, 79], "cite_extract_rate": 0.7213114754098361, "origin_cites_number": 61, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a structured synthesis of regularization and normalization techniques in GANs by organizing them into a novel taxonomy based on training perspectives. It abstracts the problem to highlight broader patterns, such as the necessity of prior knowledge for training stability. While it includes some critical remarks (e.g., the limitations of existing surveys), a deeper evaluation of method limitations or trade-offs could be strengthened."}}
{"id": "5a1f5e65-9650-482a-9b5a-2625a5dd0b96", "title": "Regularization and Normalization", "level": "subsection", "subsections": [], "parent_id": "5fdbf0be-ea34-468a-9943-d62d05b593ca", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Background and Three Perspectives of GANs Training"], ["subsection", "Regularization and Normalization"]], "content": "{Regularization and normalization are common and important techniques to introduce prior knowledge in neural networks.} Regularization is a technique to control the complexity of learning models. Weight decay  is a typical method to minimize the square of weights together with the training loss in the training of neural networks , which can be used to improve generalization. \n\tIn Bayesian learning methods, such as relevance vector machine , probabilistic classification vector machines , and others  , regularization is termed as prior distribution. Specifically, L2 regularization  is equivalent to introducing Gaussian prior to the parameters, and L1 regularization  is equivalent to introducing Laplace prior to the parameters. The theoretical connection between regularization and prior information has been investigated in neural network ensembles research . Regularization does not only control overfitting but also provide other characteristics like semi-supervised assumptions , manifold assumptions , feature selection , low rank representation , and consistency assumptions . {Normalization  is the mapping of data to a specified range, which is advantageous for the Stochastic Gradient Descent (SGD) , accelerating convergence and improving accuracy.}", "cites": [91, 66, 57, 90, 71, 67], "cite_extract_rate": 0.2727272727272727, "origin_cites_number": 22, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of regularization and normalization techniques, citing relevant papers to support general claims. However, it lacks integration of these papers into a cohesive narrative and does not compare or contrast their approaches. There is minimal abstraction or critical evaluation of the methods discussed."}}
{"id": "c2fed4a0-e8f0-4ddc-ab66-70cc7bfe76f8", "title": "GANs", "level": "subsection", "subsections": [], "parent_id": "5fdbf0be-ea34-468a-9943-d62d05b593ca", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Background and Three Perspectives of GANs Training"], ["subsection", "GANs"]], "content": "GANs are two-player zero-sum games, where generator (G) and discriminator ($D$) try to optimize opposing loss functions to find the global Nash equilibrium. In general, GANs can be formulated as follows:\n\t\\begin{equation}\n\t\t\\mathop{\\min}\\limits_{\\phi}\\mathop{\\max}\\limits_{\\theta} f(\\phi,\\theta)\n\t\t=\\mathop{\\min}\\limits_{\\phi}\\mathop{\\max}\\limits_{\\theta}\\mathbb{E}_{x\\sim p_{r}}[g_1(D_\\theta(x))]\n\t\t+\\mathbb{E}_{z\\sim p_z}[g_2(D_\\theta(G_\\phi(z)))],\n\t\t\\label{EQ:eqn1}\n\t\\end{equation}\n\twhere $\\phi$ and $\\theta$ are parameters of the generator $G$ and the discriminator $D$, respectively. $p_r$ and $p_z$ represent the real distribution and the latent distribution, respectively. {$g_1$and $g_2$ are different functions corresponding to different GANs. Specifically, vanilla GAN  can be described as $g_1(t)=g_2(-t)=-\\log(1+e^{-t})$; $f$-GAN  can be written as $g_1(t)=-e^{-t}$, $g_2(t)=1-t$; Morever, Geometric GAN  and WGAN  are described as $g_1(t)=g_2(-t)=-\\mathop{\\max}(0,1-t)$ and $g_1(t)$\\\\$=g_2(-t)=t$, respectively.} \n\t{Different from supervised learning, GANs training is an unsupervised learning, which leads to the urgency of regularization and normalization in the training of GANs.} In the following parts, we elaborate the training of GANs from three perspectives: low level: the perspective of \"Real \\& Fake\", middle level: the perspective of \"Fitting distribution\", and high level: the perspective of \"Training dynamics\". According to different perspectives of GANs, various regularization and normalization have been proposed in the GANs training.\n\t\\begin{comment}", "cites": [64, 92, 54], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a factual description of GANs and their formulations, integrating a few cited papers to illustrate different function choices (e.g., vanilla GAN, f-GAN, Geometric GAN, WGAN). It begins to synthesize these examples under a general GAN framework but lacks deeper analysis or comparison. There is minimal abstraction or critique, focusing mainly on summarizing existing formulations."}}
{"id": "fd688225-4a59-4600-886f-76309f6e8ba9", "title": "Regularization and Normalization", "level": "subsection", "subsections": [], "parent_id": "5fdbf0be-ea34-468a-9943-d62d05b593ca", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Background and Three Perspectives of GANs Training"], ["subsection", "Regularization and Normalization"]], "content": "Regularization is a technique to control the complexity of learning models. Weight decay  is a typical method to minimize the square of weights together with the training loss in the training of neural networks , which can be used to improve generalization. \n\tIn Bayesian learning methods, such as relevance vector machine , probabilistic classification vector machines , and others  , regularization is termed as prior distribution. Specifically, L2 regularization  is equivalent to introducing Gaussian prior to the parameters, and L1 regularization  is equivalent to introducing Laplace prior to the parameters. The theoretical connection between regularization and prior information has been investigated in neural network ensembles research . Regularization does not only control overfitting but also provide other characteristics like semi-supervised assumptions , manifold assumptions , feature selection , low rank representation , and consistency assumptions . {Normalization  is the mapping of data to a specified range, which is advantageous for the Stochastic Gradient Descent (SGD) , accelerating convergence and improving accuracy.\n\t}\n\t\\end{comment}", "cites": [91, 66, 57, 90, 71, 67], "cite_extract_rate": 0.2727272727272727, "origin_cites_number": 22, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of regularization and normalization techniques, briefly mentioning their definitions and purposes. It integrates some ideas (e.g., the Bayesian interpretation of regularization) but does not create a cohesive or novel framework. There is minimal critical analysis or comparison of methods, and the abstraction remains limited to surface-level generalizations without deeper meta-level insights."}}
{"id": "4e9cd8aa-b10a-44ef-a7b7-7d2880558ce2", "title": "Low Level: The Perspective of \"Real \\& Fake\"", "level": "subsection", "subsections": [], "parent_id": "5fdbf0be-ea34-468a-9943-d62d05b593ca", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Background and Three Perspectives of GANs Training"], ["subsection", "Low Level: The Perspective of \"Real \\& Fake\""]], "content": "In low level, GANs is considered as \"counterfeiters-police\" competition, where the generator (G) can be thought of counterfeiters, trying to produce fake currency and use it undetected, while the discriminator (D) is analogous to the police, trying to detect the counterfeit currency. This competition drives both teams to upgrade their methods until the fake currency is indistinguishable from the real ones. Generally, D estimates the real probability of both real and fake samples, which is very similar to the bi-classification task, while G generates fake samples similar to real ones. Hence, the loss function in Eq (\\ref{EQ:eqn1}) is formulated as:\n\t\\begin{equation}\n\t\t\\mathop{\\min}\\limits_{\\phi}\\mathop{\\max}\\limits_{\\theta} f(\\phi,\\theta)\n\t\t=\\mathop{\\min}\\limits_{\\phi}\\mathop{\\max}\\limits_{\\theta}\\mathbb{E}_{x\\sim p_{r}}[\\log(D_\\theta(x))]\n\t\t+\\mathbb{E}_{z\\sim p_z}[\\log(1-D_\\theta(G_\\phi(z)))],\n\t\t\\label{EQ:eqnganlevel1}\n\t\\end{equation}\n\twhere $f(\\phi,\\theta)$ is a binary cross-entropy function, commonly used in binary classification problems. Eq (\\ref{EQ:eqnganlevel1}) is proposed in original GAN  and can be optimized by alternate training. The training of discriminator is:\n\t\\begin{equation}\n\t\t\\mathop{\\max}\\limits_{\\theta} \\mathbb{E}_{x\\sim p_{r}}[\\log(D_\\theta(x))]\n\t\t+\\mathbb{E}_{z\\sim p_z}[\\log(1-D_\\theta(G_\\phi(z)))],\n\t\t\\label{EQ:eqndiscriminator}\n\t\\end{equation}\n\twhich is the same as a bi-classification task between real images and generated images. However, the naive binary cross-entropy function suffers from many problems, such as gradients vanishing. {Gradients vanishing is present when the difference between real and generated images as measured by discriminator is large, which leads generators cannot get optimised directions.} Accordingly, many techniques from classification like loss functions and regularization methods have been used to improve the training of discriminator. For instance, to overcome the gradients vanishing problem, Mao et al.  propose the LSGANs which adopts the least\n\tsquares loss function for the discriminator. The least squares loss function moves the fake samples toward the decision boundary even though they are correctly classified. Based on this property, LSGANs is able to generate samples that are closer to real ones. The loss functions of LSGANs can be defined as follows:\n\t\\begin{equation}\n\t\t\\begin{aligned}\n\t\t\t&\\mathop{\\min}\\limits_{\\theta} \\mathbb{E}_{x \\sim p_{r}}[(D_{\\theta}({x})-b)^{2}] \n\t\t\t+ \\mathbb{E}_{{z} \\sim p_{{z}}}[(D_{\\theta}(G_{\\phi}({z}))-a)^{2}], \\\\\n\t\t\t&\\mathop{\\min}\\limits_{\\phi}  \\mathbb{E}_{z \\sim p_{z}}[(D_{\\theta}(G_{\\phi}({z}))-c)^{2}],\n\t\t\\end{aligned}\n\t\\end{equation}\n\twhere $b$ and $a$ are objectives that $D$ uses for the training of real and fake samples respectively, $c$ denotes the value that $G$ wants $D$ to believe for fake sample. Gradients vanishing problem of the LSGANs only appears with $D_{\\theta}(G_{\\phi}({z}))=c$, which is hard. Furthermore, Lin et al.  use SVM separating hyperplane that maximizes the margin to propose geometric GAN. Authors use the Hinge loss to train the models, which can be formulated as:\n\t\\begin{equation}\n\t\t\\begin{aligned}\n\t\t\t&\\mathop{\\min}\\limits_{\\theta} \\mathbb{E}_{x \\sim p_{r}}[(1-D_{\\theta}({x})]_{+}\n\t\t\t+ \\mathbb{E}_{{z} \\sim p_{{z}}}[1+D_{\\theta}(G_{\\phi}({z}))]_{+}, \\\\\n\t\t\t&\\mathop{\\min}\\limits_{\\phi}  -\\mathbb{E}_{z \\sim p_{z}}D_{\\theta}(G_{\\phi}({z})),\n\t\t\\end{aligned}\n\t\\end{equation}\n\twhere $[x]_{+}=\\mathop{\\max}\\{0,x\\}$.\n\tThe motivation of GANs is to train the generator based on the output of the discriminator. Unlike the direct training objective of the classification task (minimizing cross-entropy loss), the objective of generator is indirect (with the help of the discriminator output). Hence, discriminator should provide a richer representation on the truth or false of samples compared to the classification task. More prior information and additional supervision tasks are urgent during the training process of the discriminator. Based on these, some regularization methods, such as {\\textit{Data Augmentation and Preprocessing}}, {\\textit{Consistency Regularization}}, and {\\textit{Self-Supervision}} are proposed to improve the stability and generalizability  of discriminator.", "cites": [63, 92], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates two specific papers (LSGANs and Geometric GAN) to explain how different loss functions address the gradient vanishing problem in GAN training. It provides a coherent narrative on how these methods alter the discriminator's training objective. While it offers some analysis of the limitations of the binary cross-entropy loss and the motivations for alternative loss functions, it does not deeply compare or critique the proposed methods. The abstraction is moderate, as it highlights the need for richer representations and additional supervision in the discriminator."}}
{"id": "a915370f-85fe-4345-a91b-c8d67718061c", "title": "Middle Level: The Perspective of \"Fitting distribution\"", "level": "subsection", "subsections": [], "parent_id": "5fdbf0be-ea34-468a-9943-d62d05b593ca", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Background and Three Perspectives of GANs Training"], ["subsection", "Middle Level: The Perspective of \"Fitting distribution\""]], "content": "At middle level, generator $G(z)$ is considered as a distribution mapping function that maps latent distribution $p_z(z)$ to generated distribution $P_g(x)$, and the discriminator $D(x)$ is a distribution distance that evaluates the distance between the target distribution $P_r(x)$ and the generated distribution $P_g(x)$ as illustrated in Figure \\ref{FIG}. For the optimal discriminator, the generator $G(z)$ tries to minimize the distance between $P_r(x)$ and $P_g(x)$. For instance, generator of the vanilla GAN\\footnote{Vanilla GAN, also known as standard GAN, is the first GAN model.}  and $f$-GAN\\footnote{\\label{ft:5} $f$-GAN is a collective term for a type of GAN models whose discriminator minimizes $f$ divergence. $f$ divergence is the general form of KL divergence. It can be demonstrated as: $D_f(P||Q)=\\int q(x)f\\big(\\frac{p(x)}{q(x)}\\big)\\rm{d}x$, where $f$ is a mapping function from non-negative real numbers to real numbers ($\\mathbb{R}^*\\rightarrow\\mathbb{R}$) that satisfies: (1) $f(1)=0$. (2) $f$ is a convex function. To be more specific, KL divergence corresponds to $f(u)=u\\log u$ and JS divergence corresponds to $f(u)=-\\frac{u+1}{2}\\log\\frac{1+u}{2}+\\frac{u}{2}\\log u$. More details can be viewed in  }  are considered to minimize Jensen–Shannon (JS) divergence and $f$ divergence\\textsuperscript{\\ref{ft:5}}, respectively. When the conditions of LSGANs loss are set to $b-c=1$ and $b-a=2$, generator of the LSGAN considers the minimization of Pearson ${\\chi}^2$ divergence. Furthermore, generators of the WGAN-div\\footnote{Different from WGAN-div, WGAN  minimize Wasserstein distance, not Wasserstein divergence.}  and GAN-QP  consider the minimization of Wasserstein divergence and Quadratic divergence, respectively.\n\t\\begin{figure}\n\t\t\\centering\n\t\t\\includegraphics[scale=0.55]{figs/GAN.pdf}\n\t\t\\caption{The framework of GANs. $P_z$ is a latent space distribution, $P_r$ and $P_g$  represent the real distribution and the generated distribution, respectively.}\n\t\t\\label{FIG}\n\t\\end{figure}\n\tGenerator is a transportation map from $z$ to $p_g(x)$. In this section, we introduce the optimal transport and the optimal transport with regular term, which leads to the form of Wasserstein GANs with gradient penalty  (WGAN-GP) and Wasserstein GANs with Lipschitz penalty  (WGAN-LP), respectively. Wasserstein distance is a popular and important distance in GANs and it corresponds to the optimal transport of the generator. To solve the dual problem of Wasserstein distance, Lipschitz continuity is introduced, which is the reason why gradient penalty and weight normalization techniques are proposed in the GANs training.\n\t\\begin{comment}\n\tOptimal transport  was proposed in the 18th century to minimize the transportation cost while preserving the measure quantities. Given the space with probability measures $(X,\\mu)$ and $(Y,\\upsilon)$, if there is a map $T:X\\rightarrow Y$ which is measure-preserving, then for any $B\\subset Y$, having:\n\t\\begin{equation}\\label{eqn1}\n\t\t\\int_{T^{-1}(B)}\\mathrm{d}\\mu(x)=\\int_B \\mathrm{d}\\upsilon(y).\n\t\\end{equation}\n\tWriting the measure-preserving map as $T_*(\\mu)=\\upsilon$. For any $x\\in X$ and $y\\in Y$, the transportation distance is defined as $c(x,y)$, the total transportation cost is given by:\n\t\\begin{equation}\\label{eqn1}\n\t\tC(T):=\\int_X c(x,T(x)) \\mathrm{d}\\mu(x).\n\t\\end{equation}\n\tIn the 18th century, Monge et al.  proposed the Optimal Mass Transportation Map that corresponds to the smallest total transportation cost: $C(T)$. The transportation cost corresponding to the optimal transportation map is called the Wasserstein distance between probability measures $\\mu$ and $\\upsilon$:\n\t\\begin{equation}\\label{eqn1}\n\t\tW_c(\\mu,\\upsilon)=\\mathop{\\min}\\limits_{T}\\left\\{\\int_X c(x,T(x)) \\mathrm{d}\\mu(x)\\ |\\ T_*(\\mu)=\\upsilon\\right\\}.\n\t\\end{equation}\n\tIn 1940s, Kantorovich  proved the existence and uniqueness of the solution for Monge problem, and according to the duality of linear programming, the Kantorovich-Rubinstein (KR) duality of Wasserstein distance is given by:\n\t\\begin{equation}\\label{eqn1}\n\t\tW_c(\\mu,\\upsilon)\n\t\t=\\mathop{\\max}\\limits_{\\varphi ,\\psi}\\left\\{\\int_X \\varphi \\mathrm{d}\\mu\\ +\\int_Y \\psi \\mathrm{d}\\upsilon\\ |\\ \\varphi(x)+\\psi(y)\\leq c(x,y)\\right\\}.\n\t\\end{equation}\n\tThis dual problem is constrained, defining the c-transform: $\\psi(y)=\\varphi^c(y):=inf_x\\{c(x,y)-\\varphi(x)\\}$, and the Wasserstein distance becomes:\n\t\\begin{equation}\\label{eqn1}\n\t\tW_c(\\mu,\\upsilon)=\\mathop{\\max}\\limits_{\\varphi}\\left\\{\\int_X \\varphi \\mathrm{d}\\mu\\ +\\int_Y \\varphi^c \\mathrm{d}\\upsilon\\right\\},\n\t\\end{equation}\n\twhere $\\varphi$ is called the Kantorovich potential. It can be shown that if $c(x,y)=|x-y|$ and Kantorovich potential satisfies the 1-Lipschitz continuity, then $\\varphi^c=-\\varphi$. Kantorovich potential can be fitted by a deep neural network, which is recorded as $\\varphi_\\xi$. Wasserstein distance is:\n\t\\begin{equation}\n\t\tW_c(\\mu,\\upsilon)=\\mathop{\\max}\\limits_{||\\varphi_\\xi||_L\\leq 1}\\left\\{\\int_X \\varphi_\\xi \\mathrm{d}\\mu\\ -\\int_Y \\varphi_\\xi \\mathrm{d}\\upsilon\\right\\}.\n\t\t\\label{eqn7}\n\t\\end{equation}\n\tIf $X$ is the generated image space, $Y$ is the real sample space, $Z$ is latent space and $g_\\theta$ is the geneartor, the Wasserstein GANs (WGAN) is formulated as a min-max problem:\n\t\\begin{equation}\\label{eqn1}\n\t\t\\mathop{\\min}\\limits_{\\theta}\\mathop{\\max}\\limits_{||\\varphi_\\xi||_L\\leq 1}\\left\\{\\int_Z \\varphi_\\xi(g_\\theta(z)) \\mathrm{d}z \\ -\\int_Y \\varphi_\\xi(y) \\mathrm{d}y\\right\\}.\n\t\\end{equation}\n\tIn the optimization process, the generator and the Kantorovich potential function (discriminator) are independent of each other, optimized in a step-by-step iteration.\n\tIf $c(x,y)=\\frac{|x-y|^2}{2}$, there is a convex function $u$ that is called Brenier potential . The optimal transportation map is given by the gradient map of Brenier potential: $T(x)=\\nabla u(x)$. There exists a relationship between Kantorovich potential and Brenier potential : \n\t\\begin{equation}\\label{eqn1}\n\t\tu(x)=\\frac{|x|^2}{2}-\\varphi(x).\n\t\\end{equation}\n\tFrom the previous discussion, it is evident that the optimal transportation map (Brenier potential) corresponds to the generator, and Kantorovich potential corresponds to the discriminator. After the discriminator is optimized, the generator is directly drivable without the optimization process .\n\tThe transportation cost of Eq (3) is defined as the form of two distribution distances:\n\t\\begin{equation}\\label{eqn1}\n\t\tOT(P||Q)=\\mathop{inf}\\limits_{\\pi}\\int\\pi(x,y)c(x,y)\\mathrm{d}x\\mathrm{d}y,\n\t\\end{equation}\n\twhere $\\pi(x,y)$ is the joint distribution, satisfying $\\int_y\\pi(x,y)dy=P(x)$ and $\\int_x\\pi(x,y)dx=Q(y)$. The dual form of Eq (10) is derived as follows::\n\t\\begin{equation}\\label{eqn1}\n\t\tOT(P||Q)=\\mathop{\\max}\\limits_{\\varphi ,\\psi}\\{\\int_x \\varphi(x)P(x) \\mathrm{d}x\\ \\\\ +\\int_y\\psi(y)Q(y) \\mathrm{d}y\\ |\\ \\varphi(x)+\\psi(y)\\leq c(x,y)\\}.\n\t\\end{equation} \n\tConsidering the optimal transportation with regular terms, Peyr{\\'e} et al.  added the entropic regularization for optimal transportation that transforms the dual problem into a smooth unconstrained convex problem. The regularized optimal transport is defined as:\n\t\\begin{equation}\\label{eqn1}\n\t\tOT_c(P||Q)=\\mathop{\\min}\\limits_{\\pi}\\int\\pi(x,y)c(x,y)\\mathrm{d}x\\mathrm{d}y+\\epsilon E(\\pi).\n\t\\end{equation}\n\tIf  $E(\\pi)=\\int_x\\int_y\\pi(x,y)\\log(\\frac{\\pi(x,y)}{P(x)Q(y)})\\mathrm{d}x\\mathrm{d}y$, Eq (12) can be written as:\n\t\\begin{equation}\\label{eqn1}\n\t\t\\begin{split}\n\t\t\tOT_c(P||Q)=&\\mathop{\\min}\\limits_{\\pi}\\int\\pi(x,y)c(x,y)\\mathrm{d}x\\mathrm{d}y+\\epsilon \\int_x\\int_y\\pi(x,y)\\log\\left(\\frac{\\pi(x,y)}{P(x)Q(y)}\\right)\\mathrm{d}x\\mathrm{d}y\\\\\n\t\t\ts.t. \\int_y\\pi(x,y)&\\mathrm{d}y=P(x),\\int_x\\pi(x,y)\\mathrm{d}x=Q(y).\n\t\t\\end{split}\n\t\\end{equation}\n\tThe dual form of Eq (13) becomes:\n\t\\begin{equation}\n\t\t\\begin{split}\n\t\t\tOT_c(P||Q)\n\t\t\t&=\\mathop{\\max}\\limits_{\\varphi ,\\psi}\\int_x \\varphi(x)P(x) \\mathrm{d}x\\ +\\int_y\\psi(y)Q(y)\\mathrm{d}y\\\\\n\t\t\t&+\\frac{\\epsilon}{e}\\int_x\\int_y\\exp\\left(\\frac{-\\left(c(x,y)+\\varphi(x)+\\psi(y)\\right)}{\\epsilon}\\right)\\mathrm{d}x\\mathrm{d}y.\n\t\t\\end{split}\n\t\t\\label{EQ:eqn14}\n\t\\end{equation}\n\t\\end{comment}\n\tThe details of optimal transportation and optimal transportation with regular terms for WGAN and Lipschitz continuity can be found on the Section \\ref{sect:A-1} of the Supplementary Online-only Material. It is pertinent to note that \\textit{Gradient Penalty} and \\textit{Gradient} \\textit{Normalization} are two simple and effective ways to implement the Lipschitz continuity. Furthermore,  demonstrates that the spectral norm and the Lipschitz constant have the same meaning. Therefore, the spectral norm can be used to represent the Lipschitz constant. The Lipschitz continuity is achieved by normalizing the spectral norm of the weight, approximately. Hence, \\textit{Weight Normalization} and \\textit{Weight Regularization} can also be used to enable the Lipschitz continuity of the discriminator.\n\t\\begin{comment}", "cites": [88, 54, 64, 93, 94, 95, 78, 8318, 8317], "cite_extract_rate": 0.6428571428571429, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers to present a coherent view of GAN training through the lens of distribution fitting and optimal transport theory. It abstracts key concepts such as Wasserstein distance, Kantorovich and Brenier potentials, and introduces relationships between generator and discriminator roles. While it offers some critical points (e.g., Lipschitz constraints leading to alternatives like WGAN-GP), it could provide deeper evaluation of the cited methods' limitations or trade-offs."}}
{"id": "a397f8d2-2658-471a-8c3b-cbde88271e09", "title": "Lipschitz Continuity and Matrix Norm", "level": "subsection", "subsections": [], "parent_id": "5fdbf0be-ea34-468a-9943-d62d05b593ca", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Background and Three Perspectives of GANs Training"], ["subsection", "Lipschitz Continuity and Matrix Norm"]], "content": "WGAN is a popular and important generative adversarial network. From the optimal transport introduced  in the last subsection, to obtain Eq (\\ref{eqn7}), the discriminator must satisfy the 1-Lipschitz continuity. This section introduces the form of the Lipschitz constant and shows that the spectral norm and the Lipschitz constant have the same meaning.\n\t1-Lipschitz continuity is represented as:\n\t\\begin{equation}\n\t||D(x_1)-D(x_2)||\\leq ||x_1-x_2||.\\footnote{Lipschitz continuity can be defined by any form of norm.}\n\t\\end{equation}\n\tGenerally, considering the K-Lipschitz for a neural network $f(x)$:\n\t\\begin{equation}\n\tf(x)=g_N\\circ\\cdots g_2\\circ g_1(x),\\footnote{$\\circ$ is the symbol for function cascade. Specifically, $h\\circ g(x)=h(g(x))$. This definition of neural network is not general, such as DenseNet  and ResNet , which can not be defined like this. Therefore, we do not strictly derive the relationship between the matrix norm and Lipschitz continuity.}\n\t\\end{equation}\n\twhere $g_i(x)=\\sigma (W_i x+b_i)$. And K-Lipschitz continuity for $f(x)$ is:\n\t\\begin{equation}\n\t||f(x_1)-f(x_2)||\\leq \\mathrm{K}||x_1-x_2||,\n\t\\label{EQ:eqn17}\n\t\\end{equation}\n\twhere K is Lipschitz constant of the function $f$. Due to the consistency of Lipschitz $||h\\circ g||_{Lip}\\leq ||h||_{Lip}\\cdot||g||_{Lip}$, $g_i$ needs to satisfy the C-Lipschitz continuity ($\\mathrm{C}=\\sqrt[N]{\\mathrm{K}}$) so that $f$ satisfies the K-Lipschitz continuity:\n\t\\begin{equation}\n\t||g_i(x_1)-g_i(x_2)||\\leq \\mathrm{C}||x_1-x_2||,\n\t\\end{equation}\n\t\\begin{equation}\n\t||\\sigma(Wx_1+b)-\\sigma(Wx_2+b)||\\leq \\mathrm{C}||x_1-x_2||.\n\t\\label{eq:23}\n\t\\end{equation}\n\tWhen $x_1\\rightarrow x_2$, the Taylor expansion of Eq (\\ref{eq:23}):\n\t\\begin{equation}\n\t||\\frac{\\partial\\sigma}{\\partial x} W(x_1-x_2)||\\leq \\mathrm{C}||x_1-x_2||.\n\t\\end{equation}\n\tNormally, $\\sigma$ is a function with limited derivatives such as Sigmoid, so the $\\mathrm{C'}$-Lipschitz continuity is be written as:\n\t\\begin{equation}\n\t|| W(x_1-x_2)||\\leq \\mathrm{C'}||x_1-x_2||,\n\t\\end{equation}\n\twhere $\\mathrm{C'}$ is a limited constant, which is determined by $\\frac{\\partial\\sigma}{\\partial x}$ and $\\mathrm{C}$.\n\tSimilarly, the spectral norm of matrix is defined by:\n\t\\begin{equation}\n\t||W||_2=\\mathop{\\max}\\limits_{x\\not=0}\\frac{||Wx||}{||x||}.\n\t\\end{equation}\n\tIn this context, the spectral norm $||W||_2$ can be used to represent the Lipschitz constant $\\mathrm{C'}$. The Lipschitz continuity is achieved by normalizing the spectral norm of the weight, approximately. Hence, \\textit{Weight Normalization} and \\textit{Weight Regularization} can also be used to enable the Lipschitz continuity of the discriminator.\n\t\\end{comment}", "cites": [97, 96], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a mathematical analysis of Lipschitz continuity and its relation to matrix norms in the context of GANs, particularly WGAN. It synthesizes the concept with the work of WGAN but only briefly acknowledges limitations in the generalizability of the formulation due to architectures like DenseNet and ResNet. While it abstracts the idea that weight normalization and regularization can approximate Lipschitz continuity, it lacks deeper comparative or critical discussion of these methods."}}
{"id": "0edf29a6-bba8-43a2-a318-3f272fcb91d9", "title": "High Level: The Perspective of \"Training dynamics\"", "level": "subsection", "subsections": [], "parent_id": "5fdbf0be-ea34-468a-9943-d62d05b593ca", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Background and Three Perspectives of GANs Training"], ["subsection", "High Level: The Perspective of \"Training dynamics\""]], "content": "GANs training is a two-player zero-sum game with a solution to Nash Equilibrium. At high level, we analyze the convergence of GANs by understanding the optimization process. Based on these, some regularization techniques are proposed to guide the GANs model to reach the theoretical equilibrium point leading to improvement in the effectiveness of GANs.\n\tReconsidering the Eq (\\ref{EQ:eqn1}) in Section 2, the training of GANs is achieved by solving a two-player zero-sum game via Simultaneous Gradient Descent (SimGD) . The updates of the SimGD are given as:\n\t\\begin{equation} \n\t\t\\label{eq:eqn27}\n\t\t\\phi^{(k+1)}=\\phi^{(k)}-h\\nabla_\\phi f(\\phi^{(k)},\\theta ^{(k)}),\\quad\n\t\t\\theta^{(k+1)}=\\theta^{(k)}+h\\nabla_\\theta f(\\phi^{(k)},\\theta ^{(k)}).\n\t\\end{equation}\n\tAssuming that the objectives of GANs are convex, many research studies discuss their global convergence characteristics . However, due to the high non-convexity of deep networks, even a simple GAN does not satisfy the convexity assumption . A recent study  shows that it is unrealistic to obtain approximate global convergence under the assumption of the optimal discriminator, so the community considers local convergence. It hopes that the trajectory of the dynamic system can enter a local convergence point with continuity iterations, that is, Nash equilibrium:\n\t\\begin{equation} \n\t\t\\bar{\\phi}=\\mathop{\\arg\\max}_{\\phi}-f(\\phi,\\bar{\\theta}),\\quad\n\t\t\\bar{\\theta}=\\mathop{\\arg\\max}_{\\theta}f(\\bar{\\phi},\\theta).\n\t\t\\label{EQ:eq24}\n\t\\end{equation}\n\tIf the point $(\\bar{\\phi},\\bar{\\theta})$ is called the local Nash-equilibrium, Eq  (\\ref{EQ:eq24}) holds in a local neighborhood of  $(\\bar{\\phi},\\bar{\\theta})$.\n\tFor this differentiable two-player zero-sum game, a vector is defined as below:\n\t\\begin{equation}\n\t\tv(\\phi,\\theta)=\n\t\t\\left(\n\t\t\\begin{aligned}\n\t\t\t-\\nabla_\\phi f(\\phi,\\theta)\\\\\n\t\t\t\\nabla_\\theta f(\\phi,\\theta)\n\t\t\\end{aligned}  \n\t\t\\right).\n\t\\end{equation}\n\tThe Jacobian matrix is:\n\t\\begin{equation}\n\t\tv^{'}(\\phi,\\theta)=\n\t\t\\left(\n\t\t\\begin{aligned}\n\t\t\t-&\\nabla^2_{\\phi,\\phi} f(\\phi,\\theta)\\quad-\\nabla^2_{\\phi,\\theta} f(\\phi,\\theta)\\\\\n\t\t\t&\\nabla^2_{\\phi,\\theta} f(\\phi,\\theta)\\quad\\nabla^2_{\\theta,\\theta} f(\\phi,\\theta)\n\t\t\\end{aligned}  \n\t\t\\right).\n\t\\end{equation}\n\tAccording to the propositions on the Section \\ref{sect:A-2} of the Supplementary Online-only Material, under the premise of asymptotic convergence, the local convergence of GAN is equivalent to the absolute value of all eigenvalues of the Jacobian matrix at the fixed point $(v(\\bar{\\phi},\\bar{\\theta})=0)$ being less than 1. To get this condition, \\textit{Jacobian Regularization}  needs to be used.", "cites": [74, 98, 100, 54, 102, 99, 101, 64, 7190], "cite_extract_rate": 0.9, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several papers by framing GAN training as a two-player zero-sum game and using the Jacobian matrix to analyze convergence dynamics. It connects theoretical insights from multiple sources to form a coherent narrative on how regularization can stabilize training. While it provides useful analytical context, it lacks deeper critical evaluation of the methods and broader abstraction beyond the immediate mathematical formulation."}}
{"id": "8469161e-625b-422b-90e0-a6cca9638a04", "title": "Regularization and Normalization of \"Real \\& Fake\"", "level": "section", "subsections": ["6dc08e83-31a2-4629-92a0-ea9cbb6c27a0", "a5e04be5-4733-477b-9081-c52ce2a3be68", "4e2e9490-faa8-4a2f-9dbd-66ae6c225118", "2267f79d-3862-4c72-bdff-9670534c3017"], "parent_id": "4410407c-7d7c-48f1-8174-a9a95319844f", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Real \\& Fake\""]], "content": "From the perspective of the \"Real \\& Fake\", generator is counterfeiter designed to deceive the discriminator, while discriminator is police designed to distinguish between real and fake samples. The motivation of GANs is to train the generator based on the loss of the discriminator. {Compared to supervised classification tasks, discriminator formally needs to perform only bi-classification tasks, which is easy to implement. Therefore, discriminator is very easy to overfit.} Furthermore, unlike the direct training objective of the classification task (Minimizing cross-entropy loss), the objective of GANs training is indirect. Hence, only one-dimensional output of the discriminator does not provide a complete representation on truth or false of samples. Some studies have shown that the present discriminators contain some significant deficiencies in the frequency domain  and attribute domain , which are evidence of the lacking discrimination for discriminators. Excessive shortage of discrimination makes the generator lack incentives from the discriminator to learn useful information of the data. {In addition to discriminator overfitting and lacking discrimination of discriminators, discriminator forgetting is another challenge for GANs.} To alleviate these situations, many regularization methods and additional supervision tasks have been proposed in the literature, which can be divided into three categories: \\textit{Data Augmentation and Preprocessing}, \\textit{Consistency Regularization}, and \\textit{Self-supervision}. {All of them are based on data augmentation and are orthogonal to each other. As shown in Table \\ref{table:SOTA }, The state-of-the-art GANs always adopt two or even all of the above regularization.}", "cites": [6998, 72], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from the cited papers to build a narrative around the limitations of GAN discriminators, particularly in frequency and attribute domains. It abstracts these findings into broader challenges like overfitting, lack of discrimination, and forgetting. While it provides some critical perspective by pointing out these issues, it does not deeply evaluate or compare the specific methods proposed in the papers, limiting its critical depth."}}
{"id": "6dc08e83-31a2-4629-92a0-ea9cbb6c27a0", "title": "Data Augmentation and Preprocessing", "level": "subsection", "subsections": [], "parent_id": "8469161e-625b-422b-90e0-a6cca9638a04", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Real \\& Fake\""], ["subsection", "Data Augmentation and Preprocessing"]], "content": "\\begin{figure}\n\t\t\\centering\n\t\t\\includegraphics[scale=.35]{figs/data_augmentation_framework.pdf}\n\t\t\\caption{Framework of data augmentation and preprocessing for updating D (left) and G (right). (Coming from )}\n\t\t\\label{fig:data_augmentation_framework}\n\t\\end{figure}\n\tData Augmentation plays a significant role in deep learning algorithms. It increases the diversity of the training data naturally, thus reduces the overfitting in many computer vision and graphics applications . Date augmentation adopts different data transformation techniques ({\\color{red}$T$}) to increase the number of training samples. One type of  data transformation is spatial transformation of data, such as $zoomout$, $zoomin$, $translation$, $translationx$, $translationy$, $cutout$ , $cutmix$ ; The other is\n\tvisual transformation, such as $brightness$, $redness$, $greenness$, $blueness$, $mixup$ . {Furthermore, recent study  is also attempting to use frequency transformation  to data augmentation. }\n\tSimilarly, the performance of GANs heavily deteriorates given a limited amount of training data . {For instance,  shows that Frechet Inception Distance (FID) starts to rise at some point during training and outputs of discriminator keep drifting apart during training, when training data is limited. More analysis can be found in the survey  on data-efficient GANs training.} However, recent studies  observe that augmenting only real images (Only applying $T$ to (i) in Figure \\ref{fig:data_augmentation_framework}), only generated images (Only applying $T$ to (ii) in Figure \\ref{fig:data_augmentation_framework}), and only discriminator (Both applying $T$ to (i) and (ii) in Figure \\ref{fig:data_augmentation_framework}) do not help with GANs training. Naturally, one problem needs to be considered: whether the overfitting exists in GANs' training? Some studies  demonstrate that, even with big dataset and state of the art models, the training of GANs suffers from severe overfitting. Furthermore, in case of small training data, overfitting occurs at an early stage in the training. Recently, some studies  on data augmentation for GANs training have been proposed. It is argued that the classical data augmentation approach could mislead the generator to learn the distribution of the augmented data, which could be different from that of the original data. To deal with this problem, these studies augment both real and fake samples and let gradients propagate through the augmented samples to G (Applying $T$ to (i), (ii), and (iii) in Figure \\ref{fig:data_augmentation_framework}). By adding the data augmentation to all processes of GANs training, the performance of GANs has been significantly improved. {However, this \"\\textit{Augment All}\" strategy may lead to the “leaking” of augmentations to the generated samples, which is highly undesirable. The experiments in  demonstrate that as long as the probability of executing the augmentation remains below 0.8, leaks are unlikely to happen in practice.}\n\tData augmentation in GANs has remarkable achievement. However, which augmentation is most beneficial for GANs training is still an open problem. Figure 2 in  shows the FID comparisons of BigGAN on CIFAR-10 dataset. For data augmentation (represented by ‘vanilla\\_rf’), the operations in spatial augmentation such as $translation$, $zoomout$, and $zoomin$, are much more effective than the operations in visual augmentation, such as $brightness$, $colorness$ ($redness$ and $greenness$), and $mixup$. The results indicate that augmentation leads to spatial changes which improves GANs performance compared with cases where visual changes are induced. It is easy to understand that generated images are significantly lacking in detail information compared to the real images, and spatial augmentation improves the ability of the generator to fit detailed textures through spatial changes. {$InstanceNoise$, resulting in images out of the natural data manifold, cannot help with improving GANs performance. Apart form applying only a limited range of augmentations, some studies explore some strong data augmentations in GANs training. For instance, Jeong et al.  adopt contrastive learning to extract more useful information under stronger data augmentation beyond the existing yet limited practices. Combining adaptive strategies and 18 transformations (Both spatial and visual transformations) , even and $InstanceNoise$ only  can bring performance improvement over strong GANs baselines. {Furthermore,  is the first method to tackle the generative learning trilemma with denoising diffusion GANs.} Apart from these, Jiang et al.  also devise an adaptive strategy to control the strength of selecting generated images to augment real data, which can further boosts the performance of GANs.  Data augmentation is popular and significant in GANs training, whose achievements are attributed to improving discrimination, avoiding overfitting, and increasing the overlap  between real and fake distributions.}\n\t{Different from data augmentation that increases the amount of the training data, data preprocessing only adopt prior knowledge and do some uniform data transformation before the network training. Data preprocessing is orthogonal to data augmentation, which can further enhance the performance combining the data augmentation.} Li et al.  indicate that high-frequency components between real and fake images are different, which is not conducive to the GANs training. They propose two preprocessing eliminating high-frequency differences in GANs training: High-Frequency Confusion (HFC) and High-Frequency Filter (HFF). These methods are applied in places (i), (ii), and (iii) in Figure \\ref{fig:data_augmentation_framework} and improve the performance of GANs with a fraction of the cost. \n\tIn summary, both data augmentation and data preprocessing improve the performance of GANs with little cost. Data augmentation uses different transformations to improve discrimination and avoid disciminator overfitting. Furthermore, spatial augmentations achieve better performance than visual augmentations. More specifically, Zhao et al.  demonstrate that hybrid augmentation with $Color + Translation + Cutout$  is especially effective and is widely used in other studies .  Adaptive data augmentation (ADA) is the most popular method in GANs. Besides the data augmentation, data preprocessing is also a remarkable method.\n\t\\begin{figure}\n\t\t\\centering\n\t\t\\includegraphics[scale=.3]{figs/consistency.pdf}\n\t\t\\caption{Overview of consistency regularization, where image consistency regularization updates the G (left) and network consistency regularization updates the D (right). $H$ is the feature mapping function and {\\color{red}$T$} is different data transformation techniques.}\n\t\t\\label{fig:consistency}\n\t\\end{figure}", "cites": [91, 7191, 111, 104, 112, 107, 106, 110, 109, 108, 103, 113, 61, 8319, 105], "cite_extract_rate": 0.7894736842105263, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes insights from multiple papers on data augmentation and preprocessing in GANs, connecting their findings to form a coherent narrative on training stability and performance. It offers a critical perspective by highlighting limitations (e.g., augmentation leaking) and contrasting the effectiveness of spatial vs. visual transformations. The section also generalizes by identifying broader patterns, such as the role of augmentation in improving discrimination and reducing overfitting."}}
{"id": "a5e04be5-4733-477b-9081-c52ce2a3be68", "title": "Consistency Regularization", "level": "subsection", "subsections": ["a0c72661-2f99-4ec9-8181-70dfdeab36dc", "86d17fb0-5862-4222-a247-42d917acf482"], "parent_id": "8469161e-625b-422b-90e0-a6cca9638a04", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Real \\& Fake\""], ["subsection", "Consistency Regularization"]], "content": "In context of semi-supervised and unsupervised learning, consistency regularization has been widely used in . It is motivated by the fact that models should produce consistent predictions given input and their semantics-preserving augmentations, such as image rotating, and adversarial attacks. It is pertinent to note that the supervision of GANs training is weak. To increase the discrimination of discriminator, some consistency regularization techniques have also been used. Due to different goals, we divide these into two parts: \\textit{Image Consistency} and \\textit{Network Consistency} as demonstrated in Figure \\ref{fig:consistency}.", "cites": [115, 114, 7000, 7192], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces consistency regularization and its motivation but does not synthesize or connect the cited papers effectively. It lacks critical evaluation of the approaches and does not abstract broader patterns or principles, instead providing a generic description of the concept."}}
{"id": "a0c72661-2f99-4ec9-8181-70dfdeab36dc", "title": "Image Consistency", "level": "subsubsection", "subsections": [], "parent_id": "a5e04be5-4733-477b-9081-c52ce2a3be68", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Real \\& Fake\""], ["subsection", "Consistency Regularization"], ["subsubsection", "Image Consistency"]], "content": "The purpose of GANs is to generate fake images similar to real ones. In GANs, the discriminator is generally used to distinguish real images and generated images. However, outputs of the discriminator with only one dimension hardly portray the authenticity of the image completely. To improve the representation of the discriminator, some studies extend the outputs of the discriminator, for example, relativistic discriminator , distribution discriminator , and cascading rejection . Apart from this, some studies reduce the training difficulty of discriminators by introducing prior information. Regularizing the distance between the generated and real images with different measurements, namely image consistency, is the focus of this paper. The overview of it is demonstrated in left part of Figure \\ref{fig:consistency}, where consistency regularization is used to update generator (G) and can be formulated as:\n\t\\begin{equation}\n\t\t\\mathcal{L}_C=\\mathbb{E}_{x\\sim p_r, z\\sim p_z} C(H(x),H(G(z))),\n\t\t\\label{EQ:image_consistency}\n\t\\end{equation}\n\twhere $H$ is the feature mapping function and $C$ is the consistency measurement function. Different image consistency regularization have different $H$ and $C$. For instance, Salimans et al.  recommend that the generator is trained using a feature matching procedure. The objective is:\n\t\\begin{equation}\n\t\t\\mathcal{L}_C=||\\mathbb{E}_{x\\sim p_r} f(x)-\\mathbb{E}_{z\\sim p_z}f(G(z))||^2_2,\n\t\\end{equation}\n\twhere $f(x)$ denotes the intermediate layer of the discriminator. Similarly, the intermediate layer of another pre-trained classification model is an alternate option. The empirical results indicate that feature matching is indeed effective in situations where normal GAN becomes unstable. Unlike the above study which only uses mean feature matching to training generators, Mroueh et al.  propose McGAN, which trains both the generator and discriminator using the mean and covariance feature matching. The objective is:\n\t\\begin{equation}\n\t\t\t\\mathcal{L}_C=\\mathcal{L}_\\mu+\\mathcal{L}_\\sigma\n\t\t\t=||\\mu(p_r)-\\mu(G(p_z))||_q+||\\sum\\left(p_r\\right)-\\sum(p(G(z)))||_k,\n\t\\end{equation}\n\twhere $\\mu(p_r)=\\mathbb{E}_{x\\sim p_r} f(x)$ and $\\sum(p_r)=\\mathbb{E}_{x\\sim p_r} f(x)\\cdot f(x)^\\mathrm{T}$ represent the mean and the covariance of the feature layer $f(x)$, respectively. Apart from statistical differences, some studies  focus on the difference in frequency domain between the generated and real image. For instance, Durall et al.  find that the deep generative models based on up-convolution fail to reproduce spectral distributions leading to considerable differences in the spectral distributions between real images and generated images. Thus, the spectral regularization has been proposed as follows:\n\t\\begin{equation}\n\t\t\\mathcal{L}_C=\\frac{1}{M/2-1}\\sum_{i=0}^{M/2-1}AI^{real}_i\\cdot\\log(AI^{fake}_i)\n\t\t+(1-AI^{real}_i)\\cdot\\log(1-AI^{fake}_i),\n\t\\end{equation}\n\twhere $M$ is the image size and $AI$ is the spectral representation from the Fourier transform of the images. Corresponding to Eq (\\ref{EQ:image_consistency}), $H$ and $C$ are implemented with $AI$ and cross-entropy, respectively.\n\tContrary to this, the research study  uses hard example mining to improve the discriminatory of the discriminator based on the difference between real and generated samples under different metrics. Although this paradigm is different from the paradigm of image consistency regularization, both cases are motivated by obtaining generated samples similar to real images under different distance measures, so we integrate them. Chen et al.  consider both downsampling strategies: downsampling with anti-aliasing and downsampling without anti-aliasing, leads to high frequencies missing in the discriminator. High frequencies missing leads to high frequency deviation between real and generated images. To mitigate this issue, authors propose SSD-GAN, which introduces an additional spectral classifier to detect frequency spectrum discrepancy between real and generated images and integrate it into the discriminator of GANs. The overall realness of sample x is represented as:\n\t\\begin{equation}\n\t\tD^{s s}(x)=\\lambda D(x)+(1-\\lambda) C(\\phi(x)),\n\t\\end{equation}\n\twhere the enhanced discriminator $D^{s s}$ consists of two modules, a vanilla discriminator $D$ that measures the spatial realness, and a spectral classifier $C$. $\\lambda$ is a hyperparameter that controls the relative importance of the spatial realness and the spectral realness. The adversarial loss of the framework can be written as:\n\t\\begin{equation}\n\t\t\\mathcal{L}_{D} =\\mathbb{E}_{x \\sim p_{\\text {data }}(x)}\\left[\\log D^{s s}(x)\\right] \n\t\t+\\mathbb{E}_{x \\sim p_{g}(x)}\\left[\\log \\left(1-D^{s s}(x)\\right)\\right].\n\t\\end{equation}\n\t{karnewar et al.  introduce adversarial loss into the intermediate layer of the generator, which provides multiple and richer metrics for the training of generator.}\n\t\\begin{table}\n\t\t\\caption{The summary of the consistency regularization.}\n\t\t\\label{table:consistency}\n\t\t\\tiny\n\t\t\\centering\n\t\t\\begin{tabular}{c| c  }\t\n\t\t\t\\toprule\n\t\t\t\\midrule\n\t\t\tMethod& Consistency regularization term $L_C$\t \\\\\n\t\t\t\\hline\n\t\t\tMean regularization &$||\\mathbb{E}_{x\\sim p_r} f(x)-\\mathbb{E}_{z\\sim p_z}f(G(z))||_q$\\\\\n\t\t\t\\hline\n\t\t\tMean and Convariance regularization &$||\\mathbb{E}_{x\\sim p_r} f(x)-\\mathbb{E}_{z\\sim p_z}f(G(z))||_q+||\\mathbb{E}_{x\\sim p_r} f(x)\\cdot f(x)^\\mathrm{T}-\\mathbb{E}_{z\\sim p_z}f(G(z))\\cdot f(G(z))^\\mathrm{T}||_k$\\\\\n\t\t\t\\hline\n\t\t\tSpectral regularization &$\\mathcal{L}_C=\\frac{1}{M/2-1}\\sum_{i=0}^{M/2-1}AI^{real}_i\\cdot\\log(AI^{fake}_i)+(1-AI^{real}_i)\\cdot\\log(1-AI^{fake}_i)$\\\\\n\t\t\t\\hline\n\t\t\tCR-GAN &$\\mathbb{E}_{x\\sim p_r}|| D(x)-D(T(x))||_2$\\\\\n\t\t\t\\hline\n\t\t\tbCR-GAN &$\\lambda_{real}\\mathbb{E}_{x\\sim p_r}|| D(x)-D(T(x))||_2+\\lambda_{fake}\\mathbb{E}_{z\\sim p_z}||D(G(z))-D(T(G(z)))||_2$\\\\\n\t\t\t\\hline\n\t\t\tzCR-GAN&$\\lambda_{dis}\\mathbb{E}_{z\\sim p_z}||D(G(z))-D(G(T(z)))||_2-\\lambda_{gen}\\mathbb{E}_{z\\sim p_z}||G(z)-G(T(z))||_2$\\\\\n\t\t\t\\bottomrule\n\t\t\\end{tabular}\n\t\\end{table}\n\tThe summary of the image consistency regularization is given in Table \\ref{table:consistency}. In summary, image consistency considers that the real images and the generated images are similar not only in the output of discriminator, but also in statistical information and frequency domain. The analysis of biases between real and generated images using different metrics will be an interesting future research direction.", "cites": [116, 6998, 117, 121, 8320, 119, 118, 120, 91, 69], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple approaches to image consistency in GANs, connecting ideas like feature matching, mean-covariance regularization, and spectral consistency to form a coherent narrative. It includes some critical evaluation, such as explaining how high-frequency missing issues affect training and how different regularization techniques address them. The abstraction level is moderate, identifying the broader goal of similarity under multiple distance measures but not fully generalizing to meta-level principles."}}
{"id": "86d17fb0-5862-4222-a247-42d917acf482", "title": "Network Consistency", "level": "subsubsection", "subsections": [], "parent_id": "a5e04be5-4733-477b-9081-c52ce2a3be68", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Real \\& Fake\""], ["subsection", "Consistency Regularization"], ["subsubsection", "Network Consistency"]], "content": "Network consistency regularization can be regarded as Lipschitz continuity on semantics-preserving transformation. Specifically, we hope discriminator is insensitive to semantics-preserving transformation, which drives the discriminator to pay more attention to the authenticity of the images. For example, in the image domain, the reality of images should not change if we flip the image horizontally or translate the image by a few pixels. To resolve this, Zhang et al.  propose the Consistency Regularization GAN (CR-GAN) that uses the consistency regularization on the discriminator during GANs training:\n\t\\begin{equation}\n\t\t\\mathcal{L}_C=\\mathbb{E}_{x\\sim p_r}|| D(x)-D(T(x))||_2,\n\t\\end{equation}\n\twhere $T(x)$ represents a transformation (shift, flip, cutout, etc.) of images. One key problem with the CR-GAN is that the discriminator might occur the 'mistakenly believe'. 'mistakenly believe' considers that the transformations are actual features of the target dataset, due to only applying these transformations on real images. This phenomenon is not easy to notice for certain types of transformations (e.g. image shifting and flipping). However, some types of transformations, such as cutout transformations, contain visual artifacts not belonging to real images, which effects greatly limits the choice of advanced transformations we could use. To address this issue, Zhao et al.  propose Balanced Consistency Regularization (bCR-GAN) that uses regulation with respect to both real and fake images and balances the training of discriminator between real images and fake images by $\\lambda_{real}$ and $\\lambda_{fake}$:\n\t\\begin{equation}\n\t\t\\mathcal{L}_C=\\lambda_{real}\\mathbb{E}_{x\\sim p_r}|| D(x)-D(T(x))||_2\n\t\t+\\lambda_{fake}\\mathbb{E}_{z\\sim p_z}||D(G(z))-D(T(G(z)))||_2.\n\t\\end{equation}\n\tThe overview of bCR is demonstrated in right part of Figure \\ref{fig:consistency}.\n\tContrary to the methods which focus on consistency regularization with respect to transformations in image space, Zhao et al.  also propose Latent Consistency Regularization (zCR) that considers the consistency regularization on transformations in latent space. Authors expect that output of the discriminator ought not to change much with respect to the small enough perturbation $\\Delta z$ and modify the discriminator loss by enforcing:\n\t\\begin{equation}\n\t\t\\mathcal{L}^D_C=\\lambda_{dis}\\mathbb{E}_{z\\sim p_z}||D(G(z))-D(G(T(z)))||_2,\n\t\\end{equation}\n\twhere $T(z)$ represents the added small perturbation noise. However, if only this loss is added into the GAN loss, mode collapse can easily appear in the training of generators. To avoid this, an inverse gradient penalty (we will describe it in section 6.2) is added to modify the loss function for generator. Hence, we modify the generator loss by enforcing:\n\t\\begin{equation}\n\t\t\\mathcal{L}^D_C=-\\lambda_{gen}\\mathbb{E}_{z\\sim p_z}||G(z)-G(T(z))||_2.\n\t\\end{equation}\n\tNaturally, putting both bCR and zCR together, Improved Consistency Regularization (ICR) is also proposed by Zhao et al. . In addition, there are some applications where cyclic consistency regularization is used for unpaired image-to-image translation . The summary of network consistency regularization is given in Table \\ref{table:consistency}. \n\tIn summary, network consistency considers the networks, especially the discriminator, to be insensitive to semantics-preserving transformation ($T$). The results in  demonstrate that random shift and flip is the best way to perform image transformation on the CIFAR-10 dataset. Furthermore, the FID results with CR, bCR, zCR, and ICR (where transformation is flipping horizontally and shifting by multiple pixels) as presented in  are shown in Table \\ref{table:consistency_results}. The results demonstrate that network consistency regularization can significantly improve the performance of GANs. However, which transformation is best for consistency regularization, is a question. Zhao et al  compare the effect of different data transformation techniques (mentioned in Section 5.1) on bCR. Figure 2 in  shows the FID results of BigGAN adding bCR (represented by 'bcr') on CIFAR-10 dataset. From the results, the best BigGAN FID 8.65 is with transformation technology $translation$ of strength $\\lambda=0.4$, outperforming the corresponding FID 10.54 reported in Zhao et al. . Moreover, spatial transforms, which retain the major content while introducing spatial variances, can substantially improve GANs performance together with bCR. While visual transforms, which retain the spatial variances, can not further improve the performance of GANs compared with data augmentation only. Furthermore, bCR with stronger transformation (larger value of $\\lambda_{aug}$) does not improve the performance of GANs, the optimal value of $\\lambda_{aug}$ is uncertain for different data transformations. \n\t\\begin{table}\n\t\t\\caption{FID scores for class conditional image generation of the network consistency regularization (Data come from ).}\n\t\t\\label{table:consistency_results}\n\t\t\\centering\n\t\t\\begin{tabular}{ccc}\n\t\t\t\\toprule\n\t\t\t\\midrule\n\t\t\tModels & CIFAR-10 & ImageNet \\\\\n\t\t\t\\hline SNGAN & 17.50 & 27.62 \\\\\n\t\t\tBigGAN & 14.73 & 8.73 \\\\\n\t\t\tCR-BigGAN & 11.48 & 6.66 \\\\\n\t\t\tbCR-BigGAN & 10.54 & 6.24 \\\\\n\t\t\tzCR-BigGAN & 10.19 & 5.87 \\\\\n\t\t\tICR-BigGAN & $\\mathbf{9 . 2 1}$ & $\\mathbf{5 . 3 8}$ \\\\\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t\\end{table}", "cites": [91, 8320, 69, 111], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key methods from multiple papers (Zhang et al., Zhao et al.) to explain network consistency regularization in GANs, showing how they build upon and address each other's limitations. It critically evaluates issues like 'mistakenly believe' and the effectiveness of different transformations, while also highlighting the uncertainty in optimal parameters. The abstraction is moderate, as it identifies the general principle of invariance under semantics-preserving transformations but does not fully elevate the discussion to a meta-level framework."}}
{"id": "4e2e9490-faa8-4a2f-9dbd-66ae6c225118", "title": "Self-Supervision", "level": "subsection", "subsections": ["b5c16876-cba3-44e8-b6be-d7b6cabbf7ee", "aff71d0b-4dab-423d-9325-d61d96004987"], "parent_id": "8469161e-625b-422b-90e0-a6cca9638a04", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Real \\& Fake\""], ["subsection", "Self-Supervision"]], "content": "Self-supervised learning aims to learn representations from the data itself without explicit manual supervision. Recently, some self-supervised studies  provide competitive results on ImageNet classification and the representations learned from which transfer well to downstream tasks. Self-supervised learning outperforms its supervised pre-training counterpart in many tasks, such as detection and segmentation, sometimes surpassing it by large margins. This suggests that self-supervised learning obtains more representational features and significantly improve the representation of networks. Based on this, self-supervised learning is introduced into the training of GANs, and we divide them into two categories according to different self-supervision tasks: \\textit{Predictive Self-supervised Learning} and \\textit{Contrastive Self-supervised Learning}.", "cites": [123, 122, 7000], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 2.0, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a basic introduction to self-supervised learning and mentions two categories of self-supervised learning methods, but it does not deeply integrate the cited papers. It lacks critical evaluation or comparison of the methods and does not offer a broader analytical framework. The content remains largely descriptive of the concept and the papers' contributions without drawing significant insights or generalizations."}}
{"id": "b5c16876-cba3-44e8-b6be-d7b6cabbf7ee", "title": "Predictive Self-Supervised Learning (PSS)", "level": "subsubsection", "subsections": [], "parent_id": "4e2e9490-faa8-4a2f-9dbd-66ae6c225118", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Real \\& Fake\""], ["subsection", "Self-Supervision"], ["subsubsection", "Predictive Self-Supervised Learning (PSS)"]], "content": "Predictive self-supervised learning is a popular method to improve the representation of neural networks by introducing additional supervised tasks, such as context  prediction  and rotations prediction . Predictive self-supervised learning is introduced into GANs by Chen et al.  to avoid discriminator forgetting. Discriminator forgetting means that the discriminator does not remember all tasks at the same time during the training process. For example, learning varying levels of detail, structure, and texture, which causes the discriminator to fail to get a comprehensive representation of the current images. \"If the outcome is your focus, then it’s easy to look for shortcuts. And ultimately shortcuts keep you from seeing the truth, it drains your spark for life. What matters most is your will to seek the truth despite the outcome.\"\\footnote{Come from \"JoJo's Bizarre Adventure:Golden Wind\" -Araki Hirohiko.}. The same is true for GANs, which are only driven by the loss of discriminator, which is easy to distinguish between real images and generated images through shortcuts, instead of the texture and structural features we need. Predictive self-supervised learning solves this problem by introducing new generalization tasks, which also helps to prevent overfitting. The overview of the predictive self-supervised learning of GANs are demonstrated on Figure \\ref{fig:predict_self_supervised}. Depending on the data transformation function $T$, we can design different self-supervised tasks. \n\t\\begin{figure}\n\t\t\\centering\n\t\t\\includegraphics[scale=.4]{figs/predict_self_supervised.pdf}\n\t\t\\caption{Overview of the predict self supervised learning of GANs, where $C$ performs the predict classification task and shares the weights with the discriminator except for the last layer, {\\color{red}$T$} is  different data transformation techniques. Furthermore, the self-supervised task of generated images is used to update the generator (left) and the self-supervised task of real images is used to update the classification (right).}\n\t\t\\label{fig:predict_self_supervised}\n\t\\end{figure}\n\tChen et al.  introduced the predictive self-supervision in GANs training. Authors adopt the rotation prediction as the expanding task to prevent the discriminator from forgetting. Besides, plenty of other prediction tasks have also been proposed to improve the discrimination. Huang et al.  exploit the feature exchange to make the discriminator learn the proper feature structure of natural images. Baykal et al.  introduce a reshuffling task to randomly arrange the structural blocks of the images, thus helping the discriminator increase its expressive capacity for spatial structure and realistic appearance. Contrary to the methods for designing tasks at the image or feature level, Patel et al.  propose a self-supervised task with latent transformation detection, which identifies whether the latent transformation applied in the given pair is the same as that of the other pair. All above methods have designed different self-supervised tasks, and their loss functions can be formulated as:\n\t\\begin{equation}\n\t\t\\begin{split}\n\t\t\t\\mathcal{L}_{D,C}=-\\lambda_{r}\\mathbb{E}_{x\\sim p^T_r}\\mathbb{E}_{T_k\\sim {T}}\\log\\big( C_k(x)\\big) \\quad \\text { for } k=1, \\ldots, K,\\\\\n\t\t\t\\mathcal{L}_G=-\\lambda_{g}\\mathbb{E}_{x\\sim p^T_g}\\mathbb{E}_{T_k\\sim {T}}\\log\\big( C_k(x)\\big) \\quad \\text { for } k=1, \\ldots, K,\n\t\t\\end{split}\n\t\\end{equation}\n\twhere ${T}$ represents the different types of image transfer, such as rotation and reshuffling. Furthermore, $T_k$ represents different forms of the transfer ${T}$, such as $0^\\circ,90^\\circ,180^\\circ, 270^\\circ$ for rotation task, $K$ is the number of transformed forms, $C_k$ is the k-th output of the classifier $C$ that shares parameters with discriminator except for two different heads, $P^T_r$ and $P^T_g$ are the transformed distributions of real and generated images, respectively. For rotation conversion task , $K=4$, and the classifier $C$ predicts the rotation angle; For feature exchange task , $K=2$, and the classifier $C$ predicts whether the swap has occurred; For block reshuffling task , the image is divided into $9$ blocks and the number of the permutations is $9!$, which is unnecessarily huge. Thirty different permutations are selected in terms of the Hamming distances between the permutations in . As a result, $K$ is set to 30, and classifier $C$ predicts the Hamming distances of different permutations; For the latent transformation task, $K=2$, and the classifier $C$ predicts whether the transformations parameterized by the same $\\epsilon$ or different. {Besides, some study  introduces the autoencoder task, making discriminator reconstruct the input. }\n\t\\begin{table*}\n\t\t\\renewcommand\\arraystretch{1}\n\t\t\\caption{The summary of the Self-supervision}\n\t\t\\label{table:self-supervised}\n\t\t\\setlength{\\tabcolsep}{0.5mm}\n\t\t\\centering\n\t\t\\begin{tabular}{c| c |c }\t\n\t\t\t\\toprule\n\t\t\t\\midrule\n\t\t\t\\tiny Method&\\tiny Description&\\tiny Types\\\\\n\t\t\t\\hline\n\t\t\t\\tiny Rotation Prediction &\\tiny Predicting the angle of rotation ($0^\\circ,90^\\circ,180^\\circ, 270^\\circ$)&\\tiny PSS\\\\\n\t\t\t\\hline\n\t\t\t\\tiny Feature Exchange Detection &\\tiny Predicting if some exchanges have occurred at the feature level (yes or not)&\\tiny PSS\\\\\n\t\t\t\\hline\n\t\t\t\\tiny Block Reshuffling Prediction &\\tiny Predicting the Hamming distances of different reshuffling in image level (Total 30 categories)&\\tiny PSS\\\\\n\t\t\t\\hline \n\t\t\t\\tiny Latent Transformation Detection &\\tiny Predicting if some exchanges have occurred at the latent space level (yes or not)&\\tiny PSS\\\\\n\t\t\t\\hline\n\t\t\t\\tiny Autoencoder &\\tiny Reconstruct the input of the discriminator&\\tiny PSS\\\\\n\t\t\t\\hline\n\t\t\t\\tiny InfoMax-GAN &\\tiny \\makecell[c]{Positive pairs: Global and local features of an image (both real and fake images)\\\\ Negative pairs: Global and local features of different images (both real fake images)}&\\tiny CSS\\\\\n\t\t\t\\hline \n\t\t\t\\tiny Cntr-GAN &\\tiny \\makecell[c]{Positive pairs: Two different data transformations of the same image (both real and fake images).\\\\ Negative pairs: Otherwise}&\\tiny CSS\\\\\n\t\t\t\\hline\n\t\t\t\\tiny\tContraD &\\tiny \\makecell[c]{Positive pairs: Two different data transformations of the same image (real images only) + Two fake images\\\\ Negative pairs: Otherwise}&\\tiny CSS\\\\\n\t\t\t\\hline\n\t\t\t\\tiny\tInsGen &\\tiny \\makecell[c]{Positive pairs: Two different data transformations (additional latent transformations for fake image)\\\\ of the same image (both real and fake images). Negative pairs: Otherwise.}&\\tiny CSS\\\\\n\t\t\t\\hline\n\t\t\t\\tiny\tFakeCLR &\\tiny \\makecell[c]{Positive pairs: Two different data transformations and additional latent transformations for fake image.\\\\ Negative pairs: Otherwise.}&\\tiny CSS\\\\\n\t\t\t\\bottomrule\n\t\t\\end{tabular}\n\t\\end{table*}\n\tThe above methods design different kinds of self-supervised prediction tasks and participate in the training of the discriminator or generator, independently, having “loophole” that, during generator learning, $G$ could exploit to minimize $\\mathcal{L}_G$ without truly learning the data distribution. To address this issue, Ngoc-TrungTran et al.  introduce true or false judgment along with self-supervised prediction. The number of classification is $K+1$, while the loss function can be expressed as:\n\t\\begin{equation}\n\t\t\\begin{aligned}\n\t\t\t\\mathcal{L}_{D,C}=&-\\lambda_{r}\\Bigg(\\mathbb{E}_{x\\sim p^T_r}\\mathbb{E}_{T_k\\sim {T}}\\log\\big( C_k(x)\\big)\n\t\t\t+\\mathbb{E}_{x\\sim p^T_g}\\mathbb{E}_{T_k\\sim {T}}\\log\\big( C_{K+1}(x)\\big)\\Bigg)\\quad \\text { for } k=1, \\ldots, K,\\\\\n\t\t\t\\mathcal{L}_G=&-\\lambda_{g}\\Bigg(\\mathbb{E}_{x\\sim p^T_g}\\mathbb{E}_{T_k\\sim {T}}\\log\\big( C_k(x)\\big)\n\t\t\t-\\mathbb{E}_{x\\sim p^T_g}\\mathbb{E}_{T_k\\sim {T}}\\log\\big( C_{K+1}(x)\\big)\\Bigg)\\quad \\text { for } k=1, \\ldots, K,\n\t\t\\end{aligned}\n\t\\end{equation}\n\twhere $C_k$ is a classifier that predicts the rotation angles and $C_{K+1}$ is a classifier that predicts the truth of the images. The new self-supervised rotation-based GANs use the multi-class minimax game to avoid the mode collapse, which is better than the original predictive self-supervised paradigm.\n\tIn summary, predictive self-supervised learning improves the discrimination by designing different self-supervised prediction tasks, among them, rotation prediction  is widely used () for its simplicity and practicality. The summary of different methods is illustrated in Table \\ref{table:self-supervised}. {All of these methods are juxtaposed with each other. However, few studies have used multiple self-supervised tasks simultaneously, and the use of multiple self-supervised tasks to improve GANs training is still an open problem.}", "cites": [126, 132, 8321, 128, 124, 129, 130, 131, 7193, 103, 127, 55, 113, 111, 125], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 3.8}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple self-supervised methods from various papers, connecting them under a unified framework (PSS). It provides critical analysis by discussing limitations such as the generator exploiting the self-supervised task without learning the true data distribution. The section also abstracts patterns in task design, showing how transformations and loss formulations vary across approaches."}}
{"id": "aff71d0b-4dab-423d-9325-d61d96004987", "title": "Contrastive Self-Supervised Learning (CSS)", "level": "subsubsection", "subsections": [], "parent_id": "4e2e9490-faa8-4a2f-9dbd-66ae6c225118", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Real \\& Fake\""], ["subsection", "Self-Supervision"], ["subsubsection", "Contrastive Self-Supervised Learning (CSS)"]], "content": "Contrastive self-supervised Learning , as the name implies, learn representations by contrasting positive and negative examples. These techniques have resulted in empirical success in computer vision tasks with unsupervised contrastive pre-training. A handful number of studies demonstrate that self-supervised learning outperforms its supervised pre-training counterpart in many tasks, which indicates contrastive self-supervised learning leads to more expressive features. Considering two views $(v^{(1)}$ and $v^{(2)})$, contrastive self-supervised learning aims to identify whether two views are dependent or not. More specifically, it means to maximize the mutual information of positive pairs. To this end, Oord et al.  propose to minimize InfoNCE loss, which turns out to maximize a lower bound of mutual information. The InfoNCE loss is defined by:\n\t\\begin{equation}\n\t\tL_{\\mathrm{NCE}}\\left({v}_{i}^{(1)} ; {v}^{(2)}, s\\right):=-\\log \\frac{\\exp \\left(s\\left({v}_{i}^{(1)}, {v}_{i}^{(2)}\\right)\\right)}{\\sum_{j=1}^{K} \\exp \\left(s\\left({v}_{i}^{(1)}, {v}_{j}^{(2)}\\right)\\right)},\n\t\\end{equation}\n\twhere $s(,)$ is the score function that measure the similarity, positive pairs ($v_i^{(1)}$ and $v_i^{(2)}$) are different views of the same sample, and negative pairs ($v_i^{(1)}$ and $v_j^{(2)}$, $(i\\neq j)$) are different views of different samples. InfoNCE loss is the cornerstone of contrastive self-supervised learning as depicted in Figure \\ref{fig:contrastive_self_supervised}. \n\t\\begin{figure}\n\t\t\\centering\n\t\t\\includegraphics[scale=.3]{figs/constrative_self_supervised.pdf}\n\t\t\\caption{Overview of the contrastive self supervised learning, where x is real of fake images, $v^{(1)}$ and $v^{(2)}$ are different views of the image x, s is the score function (Usually discriminator in GANs) that measures the similarity, the square on the middle part is the label of InfoNCE (blue, white, and black squares are labels with 1, 0, and undefined respectively.), and the square on the right part is the label of SimCLR. Obviously, SimCLR defines more negative pairs to improve the sample utilization.}\n\t\t\\label{fig:contrastive_self_supervised}\n\t\\end{figure}\n\tMany advanced self-supervised methods are implemented by modifying the views of images $v^{(1)}$, $v^{(2)}$ and score function $s(,)$. Specifically, Deep InfoMAX  maximizes the mutual information between local and global features, that is, image $x$ passes through the encoder $E_{\\psi}=f_{\\psi}\\circ C_{\\psi}$, producing local feature map $C_{\\psi}(x)$ and global feature vector $E_{\\psi}(x)$. To maximize the lower bound of the InfoMax: $\\mathcal{I}\\big(C_{\\psi}(x),E_{\\psi}(x)\\big)$, the theoretical InfoMAX loss has been defined as:\n\t\\begin{equation}\n\t\t\\begin{aligned}\n\t\t\tL_{InfoMAX}(X)=&-\\mathbb{E}_{x\\in X}\\mathbb{E}_{i\\in\\mathcal{A}}\\big[\n\t\t\t\\log \\frac{\\exp \\left(g_{\\theta, \\omega}\\left(C_{\\psi}^{(i)}(x), E_{\\psi}(x)\\right)\\right)}{\\sum_{\\left(x^{\\prime}, i\\right) \\in X \\times \\mathcal{A}} \\exp \\left(g_{\\theta, \\omega}\\left(C_{\\psi}^{(i)}\\left(x^{\\prime}\\right), E_{\\psi}(x)\\right)\\right)}\\big],\\\\\n\t\t\tg_{\\theta, \\omega}&\\left(C_{\\psi}^{(i)}(x), E_{\\psi}(x)\\right)=\\phi_{\\theta}\\left(C_{\\psi}^{(i)}(x)\\right)^{T} \\phi_{\\omega}\\left(E_{\\psi}(x)\\right),\n\t\t\\end{aligned}\n\t\\end{equation}\n\twhere $X=\\{x_1,\\cdots,x_N\\}$ is a set of random images and $\\mathcal{A}=\\{0,1,\\cdots,M^2-1\\}$ represents indices of a $M\\times M$ spatial sized local feature map. Based on this, positive sample pairs are $C^{(i)}_{\\phi}(x)$ and $E_{\\phi}(x)$, and negative sample pairs are $C^{(i)}_{\\phi}(x^{'})$ and $E_{\\phi}(x)$), where $x^{'}$ is a different image from $x$. SimCLR  is another popular contrast learning framework, which applies two independent transformations, namely $t_1$ and $t_2$, to obtain the different views $v^{(1)},v^{(2)}=t_1(x),t_2(x)$. The loss function of SimCLR is defined as: \n\t\\begin{equation}\n\t\t\\begin{aligned}\n\t\t\tL_{\\mathrm{SimCLR}}\\left({v}^{(1)}, {v}^{(2)}\\right)=\\frac{1}{N} \\sum_{i=1}^{N}\\left(L_{\\mathrm{NCE}}\\left({v}_{i}^{(1)} ;\\left[{v}^{(2)} ; {v}_{-i}^{(1)}\\right], s_{\\mathrm{SimCLR}}\\right)\\right),\n\t\t\\end{aligned}\n\t\\end{equation}\n\twhere ${v}_{-i}:={v} \\backslash\\left\\{{v}_{i}\\right\\}$ and $s_{\\mathrm{SimCLR}}$ is defined as:\n\t\\begin{equation}\n\t\ts_{\\text {SimCLR }}\\left({v}^{(1)}, {v}^{(2)} ; f, h\\right)=\\frac{h\\left(f\\left({v}^{(1)}\\right)\\right) \\cdot h\\left(f\\left({v}^{(2)}\\right)\\right)}{\\tau \\cdot\\left\\|h\\left(f\\left({v}^{(1)}\\right)\\right)\\right\\|_{2}\\left\\|h\\left(f\\left({v}^{(2)}\\right)\\right)\\right\\|_{2}}.\n\t\\end{equation}\n\tAs shown in Figure \\ref{fig:contrastive_self_supervised}, SimCLR defines more negative pairs to improve the sample utilization compared to InfoNCE. {However, SimCLR  needs a large batch size to  obtain some sufficiently rich negative samples (In , batch size is set to 4096). To alleviate the attachment of SimCLR to large batch size, MoCo  introduce a negative queue to store and update negative samples.}\n\t\\begin{comment}\n\tSun et al.  used the triplet matching objective as a pretext task for cGANs: pairs of images with the same category will have similar features and vice versa. Specifically, positive pairs contain the same category, such as two images with black hair; negative pairs contain different category, such as a image with black hair and a image with white hair. The loss function can be expressed as:\n\t\\begin{equation}\n\t\\begin{aligned}\n\t\\mathcal{L}_{D}=-\\lambda_{d}&\\mathop{\\mathbb{E}}\\limits_{x_a,x_p\\sim P_{X|y},x_n\\sim P_{X|y'}}\\bigg[\\log\\big(D_{mch}(x_a,x_p)\\big)\\\\\n\t&+\\log\\big(1-D_{mch}(x_a,x_n)\\big)\\bigg],\\\\\n\t\\mathcal{L}_{G}=-\\lambda_{g}&\\mathop{\\mathbb{E}}\\limits_{x_1,x_2\\sim P_{X|y},x_3\\sim P_{X|y'}}\\bigg[\\log\\big(D_{mch}(G(x_1,y),G(x_2,y))\\big)\\\\\n\t&+\\log\\big(1-D_{mch}(G(x_1,y),G(x_3,y'))\\big)\\bigg],\n\t\\end{aligned}\n\t\\end{equation}\n\twhere $y\\neq y'$, $x_a$ and $x_p$ are positive pairs of real images, while $x_a$ and $x_n$ are  negative pairs of real images. \n\t\\end{comment}\n\tThe self-supervised methods mentioned above are also widely applied to the training of GANs. \n\tInspired by Deep InfoMax , Lee et al.  propose InfoMax-GAN maximizing the mutual information between local and global features of real and fake images. The regularization of discriminator is expressed as:\n\t\\begin{equation}\n\t\t\\mathcal{L}_\\mathrm{InfoMax-GAN}=\\lambda_d \\{L_{\\mathrm{InfoMAX}}(x_{r})+L_{\\mathrm{InfoMAX}}(x_{f}) \\}.\n\t\\end{equation}\n\twhere $x_r$ and $x_f$ represent sets of real and fake images, respectively.\n\tInspired by SimCLR , some studies  introduce different data transformation techniques to create positive and negative pairs during GANs training. Zhao et al  propose Cntr-GAN, where SimCLR loss is used to regularize the discriminator on two random augmented copies of both real and fake images. The regularization of discriminator for transformation $T$ is:\n\t\\begin{equation}\n\t\t\\mathcal{L}_{\\mathrm{Cntr-GAN}}=\\lambda_d \\{L_{\\mathrm{SimCLR}}(x_{r}, T(x_{r}))+L_{\\mathrm{SimCLR}}(x_{f}, T(x_{f})) \\}.\n\t\t\\label{Eq:Cntr-GAN}\n\t\\end{equation}\n\tThey also compare the effect of different data transformation techniques (mentioned in Section 5.1) on Cntr-GAN. Figure 6 in  shows the FID results of BigGAN adding SimCLR loss on CIFAR-10 dataset. The results illustrate that spatial transformations still work better than visual transformations and the best FID of 11.87 is achieved by applying adjusted SimCLR transformations with the cropping/resizing strength of 0.3. Although, regularization of auxiliary SimCLR loss improves GAN training, but does not outperform existing methods based on simple data augmentations, $e.g.$, bCR (demonstrated on Figure 2 in ). \n\tTo improve the efficiency of contrastive learning, Jeong et al.  propose Contrastive Discriminator (ContraD), a way of training discriminators of GANs using improved SimCLR. Different from Cntr-GAN with SimCLR loss on both real and generated images, ContraD uses the SimCLR loss on the real images and the supervised contrastive loss on the generated images. Supervised contrastive loss adopts the contrastive between real and generated images, required as a GAN discriminator. More concretely, for two views $v^{(1)},v^{(2)}=t_1(x),t_2(x)$ with ${t}_{1}, {t}_{2} \\sim {T}$, the loss of real images are:\n\t\\begin{equation}\n\t\tL_{\\text {con }}^{+}\\left(D, h_{{r}}\\right)=L_{\\mathrm{SimCLR}}\\left({v}_{{r}}^{(1)}, {v}_{{r}}^{(2)} ; D, h_{{r}}\\right),\n\t\\end{equation}\n\twhere $h_r$ is a projection head for this loss. However, the loss for generated images, an extended version of contrastive loss to support supervised learning by allowing more than one view to be positive. More concretely, they assume all the views from fake samples have the same label against those from real samples. Formally, for each $v_i^{(1)}$, the positive views are represented by $V_i^{(2)}$ that is a subset of $v^{(2)}$. The supervised contrastive loss is defined by:\n\t\\begin{equation}\n\t\tL_{\\text {SupCon }}\\left({v}_{i}^{(1)}, {v}^{(2)}, V_{i+}^{(2)}\\right)=\n\t\t-\\frac{1}{\\left|V_{i+}^{(2)}\\right|} \\sum_{{v}_{i+}^{(2)} \\in V_{i+}^{(2)}} \\log \\frac{\\exp \\left(s_{{SimCLR}}\\left({v}_{i}^{(1)}, {v}_{i+}^{(2)}\\right)\\right)}{\\sum_{j} \\exp \\left(s_{{SimCLR}}\\left({v}_{i}^{(1)}, {v}_{j}^{(2)}\\right)\\right)}.\n\t\\end{equation}\n\tUsing the notation,the ContraD loss for fake samples are:\n\t\\begin{equation}\n\t\tL_{\\text {con }}^{-}\\left(D, h_{f}\\right)=\n\t\t\\frac{1}{N} \\sum_{i=1}^{N} L_{\\text {SupCon }}\\left({v}_{f, i},\\left[{v}_{f,-i} ; {v}_{{r}}^{(1)} ; {v}_{{r}}^{(2)}\\right],\\left[{v}_{f,-i}\\right] ; D, h_{{f}}\\right),\n\t\\end{equation}\n\twhere $v_f= t_3(G(z))$ is a random view of fake samples ($t_3\\sim T$), and $v_{-i}=v\\backslash \\{v_i\\}$ is subset of $v$ that does not contain $v_i$. It is pertinent to note that authors also use an independent projection header $h_f$ in this loss instead of $h_r$ in $L_{\\text {con }}^{+}\\left(D, h_{{r}}\\right)$. {The adopted supervised contrastive learning on the fake images introduce the real/fake information into contrastive learning, which improves the efficiency of contrastive learning, thus improve the discrimination of the discriminator.}\n\tTo sum up, ContraD learns its contrastive representation by minimizing the following regularization loss:\n\t\\begin{equation}\n\t\tL_{\\text {con }}\\left(D, h_{{r}}, h_{{f}}\\right)=L_{\\text {con }}^{+}\\left(D, h_{{r}}\\right)+\\lambda_{\\text {con }} L_{{con}}^{-}\\left(D, h_{{f}}\\right).\n\t\\end{equation}\n\tThe experimental results show that ContraD consistently improves the performance of GANs compared to other methods, such as Cntr-GAN, DiffAug, bCR, and CR. However, ContraD with different data transformations is not discussed further. \n\t{The achievement of above SimCLR-based contrastive learning methods depends on the sufficiently large batch size. However, large-scale GANs training often has only a small batch size for limited computational resources. Therefore, MoCo-based contrastive learning method, namely InsGen , has been introduced into GANs training. InsGen follows the MoCo-v2  to store the various negative samples with an extra queue. Furthermore, it also introduces a latent space augmentation for fake images. Combining with ADA and MoCo-based contrastive learning, InsGen  has achieved state-of-the-art performance on a variety of datasets and training settings. Recently, Li et al.  identify that only latent space augmentation for fake images brings the major performance improvement and contrastive learning in real images causes performance drop on limited data generation (DE-GANs). Based on this, they propose FakeCLR, which only applies contrastive learning on perturbed fake samples and devises three related training techniques. The experimental results manifest the new state of the arts in both few-shot generation and limited-data generation.}\n\tIn summary, contrastive self-supervised learning designs different positive and negative pairs and maximizes the mutual information of positive pairs according to the InfoNCE loss. Different from classification and segmentation tasks, two types of samples (real and fake images) exist for generating adversarial networks, which add more possibilities to the definition of positive and negative pairs. In the future, score-based contrastive lea  rning may be proposed during the training of GANs. The summary of contrastive self-supervised regularization techniques of GANs is given in Table \\ref{table:self-supervised}.", "cites": [134, 133, 123, 130, 103, 125, 122, 7000, 111, 61], "cite_extract_rate": 0.9166666666666666, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes key concepts from multiple papers, especially relating InfoNCE, SimCLR, and their application to GANs. It provides a coherent narrative by connecting different contrastive self-supervised learning methods and their adaptations in GAN training. While it includes some critical analysis (e.g., SimCLR's batch size dependency and the comparison of transformation types), the critique is not deep or nuanced. The section also identifies patterns in how contrastive learning is applied, contributing to some level of abstraction."}}
{"id": "2267f79d-3862-4c72-bdff-9670534c3017", "title": "Summary", "level": "subsection", "subsections": [], "parent_id": "8469161e-625b-422b-90e0-a6cca9638a04", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Real \\& Fake\""], ["subsection", "Summary"]], "content": "{According to the perspective of \\textbf{\"Real $\\&$ Fake\"}, many regularization and normalization technologies inspired from supervised learning have been proposed to GANs training. The key point of them is improving the representation and generalizability of the discriminator. \\textit{Data Augmentation and Preprocessing} is a basic operation containing many types such as spatial augmentation, visual augmentation, frequency augmentation, and noise augmentation. Among them, combining adaptive strategies and all augmentation  has achieved the most remarkable achievement and has been employed as default operations in most GANs training. \\textit{Consistency Regularization} and \\textit{Self-supervision} are designed additional tasks based on data augmentation, which further improve the efficiency of data augmentation and extract more useful information under stronger data augmentation beyond the existing yet limited practices. Currently, combining contrastive self-supervised learning with adaptive data augmentation  has achieved state of the art in GANs training.}", "cites": [130, 108, 125], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key ideas from the cited papers, particularly connecting data augmentation, consistency regularization, and self-supervised learning within the 'Real & Fake' perspective. It abstracts these methods into broader categories and identifies a trend toward adaptive strategies and contrastive learning as state-of-the-art. While it provides some critical insight (e.g., 'existing yet limited practices'), a more detailed comparison of the limitations or trade-offs of each method could enhance its critical depth."}}
{"id": "fdfb947c-602c-4735-9040-96d30810700e", "title": "Regularization and Normalization of \"Fitting distribution\" ", "level": "section", "subsections": ["1d7d66d8-7065-4eb8-b5e7-40c9b5f20e5c", "16b503b7-d01a-45ea-9b9c-0986b73e7409", "3e0461ca-996a-4c2c-8c3e-dbd291acb92b", "ee52e377-012a-4b04-8fa5-1df1ed551949"], "parent_id": "4410407c-7d7c-48f1-8174-a9a95319844f", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Fitting distribution\" "]], "content": "From the perspective of \"Fitting distribution\", generator is considered as a distribution mapping function and the optimal discriminator is considered to be the distribution divergence. Wasserstein distance is a popular and important in GANs, and it corresponds to the optimal transport of the generator. To solve the dual problem of Wasserstein distance, Lipschitz continuity is introduced into the GANs training. The Wasserstein-based GANs (WGAN and WGAN-GP) have achieved remarkable results during the training. However, some studies  suggest that the success of WGAN-GP is not due to the Wasserstein distance and the Lipschitz constraint of discriminator may improve the performance and stability of GANs training regardless of the statistical distance used as a loss function. Therefore, the Lipschitz continuity of discriminator is an essential condition during GANs training. {Weight clipping  is a simple and the first solution to enforce a Lipschitz constraint, which clamps the weights of discriminator to a fixed box after each gradient update.} Furthermore, {gradient penalty}, {weight normalization}, and {weight regularization} are widely applied in GANs training for fulfilling Lipschitz continuity as summarized in subsequent subsections.", "cites": [64, 135, 136, 6997], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes key concepts from multiple papers, connecting Wasserstein GANs to broader training stability and the role of Lipschitz constraints. It critically examines the assumptions behind WGAN-GP, challenging the idea that success is due to Wasserstein distance alone. The abstraction is moderate, identifying the importance of Lipschitz continuity in GANs but not fully generalizing to a meta-level framework."}}
{"id": "1d7d66d8-7065-4eb8-b5e7-40c9b5f20e5c", "title": "Gradient Penalty", "level": "subsection", "subsections": ["258a5900-7326-4a5a-9ab5-4d4abbbbc5a5", "4a0b3b8a-89ed-4329-a9ef-855c13a16be6", "349a6a29-8d68-4bed-a022-d5ef2efaccc2"], "parent_id": "fdfb947c-602c-4735-9040-96d30810700e", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Fitting distribution\" "], ["subsection", "Gradient Penalty"]], "content": "Gradient penalty is a simple and direct way to fulfill Lipschitz continuity. Specifically,  K-Lipschitz continuity of the function $f$ can be accessed by $\\mathop{\\min}\\mathbb{E}_{\\hat{x}\\sim\\pi}(||\\nabla f(\\hat{x})||_2-{\\rm K})^2$. According to the optimal transport theory mentioned on the Section \\ref{sect:A-1} of the Supplementary Online-only Material, gradient penalty can be used for the approximation of $W_c(\\mu,\\upsilon)$ in WGANs, named WGAN-GP . Specifically, WGAN-GP fulfills the 1-Lipschitz continuity of the discriminator by $\\mathop{\\min}\\mathbb{E}_{\\hat{x}\\sim\\pi}(||\\nabla D_{\\theta}(\\hat{x})||_2-{\\rm 1})^2$, which limits the gradient of the discriminator to 1. Although WGAN-GP solves the instability of GANs training to some extent, the assumption of optimal transport is a constrained linear programming problem. Overly strict restriction reduces the exploratory of the discriminator. In contrast, the optimal transport with the regular term mentioned is an unconstrained optimization problem. Like optimal transport corresponds to 1-Lipschitz continuity, the optimal transport with the regular term corresponds to k-Lipschitz continuity (${\\rm k}\\leq1$) of the discriminator, named WGAN-LP , which is implemented by $\\mathop{\\min}\\mathbb{E}_{\\hat{x}\\sim\\pi}\\left[\\left(\\mathop{\\max}\\{0,||\\nabla D_{\\theta}(\\hat{x})||_2-1\\}\\right)^2\\right]$. WGAN-LP achieves better performance by using a weaker regularization term which enforces the Lipschitz constraint of the discriminator. \n\tWGAN-GP and WGAN-LP introduce Wasserstein distance into GANs framework. Due to the gap between limited input samples and the strict Lipschitz constraint on the whole input sample domain, the approximation of the Wasserstein distance is a challenging task. To this end, WGAN-div  introduces a Wasserstein divergence into GANs training. The objective of WGAN-div can be smoothly derived as:\n\t\\begin{equation}\n\t\t\\mathbb{E}_{y\\sim q(y)}[\\varphi(y)]-\\mathbb{E}_{x\\sim p(x)}[\\varphi(x)]+k \\mathbb{E}_{\\hat{x}\\sim\\pi}\\left[ ||\\varphi(\\hat{x})||^p\\right].\n\t\\end{equation}\n\tThe objective of WGAN-div is similar to WGAN-GP and WGAN-LP. It can be considered as achieving 0-Lipschitz continuity of discriminator by adopting $\\mathop{\\min}\\mathbb{E}_{\\hat{x}\\sim\\pi}\\left[||\\nabla D_{\\theta}(\\hat{x})||^p\\right]$. \n\tGenerally, Wasserstein distance and Wasserstein divergence are reliable ways of measuring the difference between fake and real data distribution, which leads to the stable training of WGAN-based algorithms. However, a recent study  shows that the c-transform method  achieves better estimation of Wasserstein divergence but leads to worse performance compared to the gradient penalty method. The results demonstrate that the success of  WGAN-based methodologies cannot truly be attributed to approximate the Wasserstein distance and the gradient penalty methods improve the performance indeed. Furthermore, some studies  also demonstrate that gradient penalty methods of discriminator, such as 1-GP, k-GP (${\\rm k}\\leq 1$), and 0-GP stabilize the training and improve the performance of GANs remarkably regardless of the loss functions. Based on these observations, stabilizing GANs training using gradient penalty is widely applied in the research community for various losses of GANs. In the rest of this section, we discuss gradient penalty methods regardless of the loss function by dividing them into three parts: \\textit{1-GP}:  $\\mathop{\\min}\\mathbb{E}_{\\hat{x}\\sim\\pi}(||\\nabla D_{\\theta}(\\hat{x})||-{\\rm 1})^p$, \\textit{k-GP (${\\rm k}\\leq1$)}: $\\mathop{\\min}\\mathbb{E}_{\\hat{x}\\sim\\pi}\\left[\\left(\\mathop{\\max}\\{0,||\\nabla D_{\\theta}(\\hat{x})||-1\\}\\right)^p\\right]$, and \\textit{0-GP}: $\\mathop{\\min}\\mathbb{E}_{\\hat{x}\\sim\\pi}\\left[||\\nabla D_{\\theta}(\\hat{x})||^p\\right]$, where $\\pi$ is the distribution of different image space (entire image space or part of image space) and $||\\cdot||$ represents the norm of the gradient. Generally, the loss function of the discriminator with GP can be formulated as:\n\t\\begin{equation}\\label{GP}\n\t\t\\mathcal{L}_{D}=f(\\phi,\\theta)+\\lambda\\mathcal{L}_{GP},\n\t\\end{equation}\n\twhere $f(\\phi,\\theta)$ is the uniform loss function defined in Eq (\\ref{EQ:eqn1}) and $\\mathcal{L}_{GP}$ is the gradient penalty regularization.", "cites": [137, 136, 88, 138, 93, 135, 8317, 6997], "cite_extract_rate": 1.0, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 4.0, "abstraction": 3.8}, "insight_level": "high", "analysis": "The section provides a coherent synthesis of gradient penalty methods in WGANs, connecting theoretical formulations with practical implementations. It critically evaluates the effectiveness of these methods, particularly contrasting the assumptions of optimal transport and the actual performance in practice, as supported by multiple cited works. The discussion abstracts beyond individual papers to identify broader patterns, such as the impact of different regularization strengths on stability and performance, offering valuable insights into the role of gradient penalties in GAN training."}}
{"id": "258a5900-7326-4a5a-9ab5-4d4abbbbc5a5", "title": "1-GP", "level": "subsubsection", "subsections": [], "parent_id": "1d7d66d8-7065-4eb8-b5e7-40c9b5f20e5c", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Fitting distribution\" "], ["subsection", "Gradient Penalty"], ["subsubsection", "1-GP"]], "content": "Gulrajani et al.  used 1-GP in WGAN-GP to train GANs. WGAN-GP uses the 2-norm gradient penalty across the entire image domain, which can be formulated as: \n\t\\begin{equation}\\label{wgan-gp}\n\t\t\\mathcal{L}_{GP}=\\mathbb{E}_{\\hat{x}\\sim\\pi}(||\\nabla D_{\\theta}(\\hat{x})||_2-1)^2,\n\t\\end{equation}\n\twhere $\\pi$ is the distribution of entire image space approximated by the interpolation of real distribution ($p_r$) and generated distribution ($p_g$): $\\pi=t\\cdot p_r+(1-t)\\cdot p_g$ for $t\\sim U[0,1]$. Although, WGAN-GP stabilizes the training of GANs to a great extent, the overly strict gradient penalty limits the exploratory of discriminator. To loosen the penalty, many efforts of $\\pi$, $||\\cdot||$, and gradient direction are proposed. \n\tTo relax the image distribution, Kodali et al.  track the training process of GANs and find that the decrease of the Inception Score (IS) is accompanied by a sudden change of the discriminator’s gradient around the real images. Authors propose DRAGAN by restricting the Lipschitz constant around the real images $\\pi=p_r+\\epsilon$, where $\\epsilon\\sim N_d(0,cI)$.\n\tIn order to relax the gradient direction, Zhou et al.  argue that restricting the global Lipschitz constant is unnecessary. Therefore, only maximum gradient is necessary to be penalized:\n\t\\begin{equation}\n\t\t\\mathcal{L}_{GP}=\\left(\\mathop{\\max}\\limits_{\\hat{x}\\sim\\pi}||\\nabla D_{\\theta}(\\hat{x})||_2-1\\right)^2,\n\t\\end{equation}\n\twhere $\\pi=t\\cdot p_r+(1-t)\\cdot p_g$; Furthermore, inspired by  Virtual Adversarial Training (VAT) , D{\\'a}vid et al.  propose a method, called Adversarial Lipschitz Regularization (ALR), which restricts the 1-Lipschitz continuity at $\\pi=p_r\\cup p_g$ in the direction of adversarial perturbation. {Adversarial perturbation direction is the most unstable direction, Restricting the 1-Lipschitz continuity to the adversarial direction means restricting only the largest Lipschitz constant, which is simpler and more efficient than the previous method.} The proposed ALP shows the SOTA performance in terms of Inception Score and Fréchet Inception Distance among non-progressive growing methods trained on CIFAR-10 dataset.\n\tContrary to the methods which penalize the gradient in Euclidean space, Adler et al.  extended the $L_p(p=2)$ space with gradient penalty to Banach space that contains the $L_p$ space and Sobolev space. For the Banach space B, the Banach norm $||.||_{B^*}$ is defined as:  \n\t\\begin{equation}\n\t\t||x^*||_{B^*}=\\sup\\limits_{x\\in B}\\frac{x^*(x)}{||x||_B}.\n\t\\end{equation}\n\tThus, the gradient penalty of Banach wasserstein GAN can be expressed as:\n\t\\begin{equation}\n\t\t\\mathcal{L}_{GP}=\\mathbb{E}_{\\hat{x}\\sim\\pi}(||\\nabla D_{\\theta}(\\hat{x})||_{B^*}-1)^2,\n\t\\end{equation}\n\twhere $\\pi=t\\cdot p_r+(1-t)\\cdot p_g$. {Banach wasserstein GAN expands the Lipschitz continuity into Banach space containing both $L_p$ space and Sobolev space, which has more restriction than wasserstein GAN.}", "cites": [141, 140, 139, 8317, 6997], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes key gradient penalty approaches from multiple papers, including WGAN-GP, DRAGAN, ALR, and Banach Wasserstein GAN, and connects their design choices (e.g., distribution relaxation, gradient direction, and space generalization). It offers some critical insights, such as the overly strict nature of WGAN-GP and the bias introduced by gradient penalty. While it identifies patterns in how gradient constraints are relaxed, the abstraction remains grounded in specific technical implementations rather than reaching a fully meta-level insight."}}
{"id": "4a0b3b8a-89ed-4329-a9ef-855c13a16be6", "title": "k-GP (${\\rm k", "level": "subsubsection", "subsections": [], "parent_id": "1d7d66d8-7065-4eb8-b5e7-40c9b5f20e5c", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Fitting distribution\" "], ["subsection", "Gradient Penalty"], ["subsubsection", "k-GP (${\\rm k"]], "content": "\\leq1$)}\n\t{k-GP (${\\rm k}\\leq1$) was first tested by Gulrajani et al.  and named one sided gradient penalty. It uses the 2-norm gradient penalty across the entire image domain, which is formulated as: }\n\t\\begin{equation}\\label{wgan-lp}\n\t\t\\mathcal{L}_{GP}=\\mathbb{E}_{\\hat{x}\\sim\\pi}\\left[\\left(\\mathop{\\max}\\{0,||\\nabla D_{\\theta}(\\hat{x})||_2-1\\}\\right)^2\\right],\n\t\\end{equation}\n\twhere $\\pi$ is the distribution of entire image space approximated by the interpolation of real distribution ($p_r$) and generated distribution ($p_g$): $\\pi=t\\cdot p_r+(1-t)\\cdot p_g$ for $t\\sim U[0,1]$. Inspired by the optimal transport with the regular term, Petzka et al.  also used k-GP (${\\rm k}\\leq1$) to training GANs named WGAN-LP. Furthermore, Xu et al  show a more general dual form of the Wasserstein distance compared to KR duality (mentioned in section 2.4), named Sobolev duality, which relaxes the Lipschitz constraint but still maintains the favorable gradient\n\tproperty of the Wasserstein distance. Authors also show that the KR duality is a special case of the proposed Sobolev duality. Based on the Sobolev duality, the relaxed gradient penalty of the proposed SWGAN is formulated as:\n\t\\begin{equation}\\label{wgan-sp}\n\t\t\\mathcal{L}_{GP}=\\mathbb{E}_{\\hat{x}\\sim\\pi}\\left[\\left(\\mathop{\\max}\\{0,||\\nabla D_{\\theta}(\\hat{x})||^2-1\\}\\right)^2\\right],\n\t\\end{equation}\n\twhere $\\pi=t\\cdot p_r+(1-t)\\cdot p_g$ for $t\\sim U[0,1]$. It is clear that  above three method have the same form of gradient penalty. Interestingly, different relaxation methods yield the same form of regularization.", "cites": [88, 142, 8317], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from three different papers on gradient penalty techniques, connecting their formulations and noting the shared form of regularization despite different theoretical motivations. It provides some abstraction by identifying a broader pattern in how these methods implement similar penalties through different relaxation approaches. However, critical analysis is limited to stating that implementations are 'still hard to satisfy the restriction,' without deeper evaluation of trade-offs or limitations among the methods."}}
{"id": "349a6a29-8d68-4bed-a022-d5ef2efaccc2", "title": "0-GP", "level": "subsubsection", "subsections": [], "parent_id": "1d7d66d8-7065-4eb8-b5e7-40c9b5f20e5c", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Fitting distribution\" "], ["subsection", "Gradient Penalty"], ["subsubsection", "0-GP"]], "content": "\\begin{table}\n\t\t\\caption{The Gradient penalty of the Discriminator. $\\mu$ and $v$ are real and generated distribution, respectively.}\n\t\t\\footnotesize\n\t\t\\label{table:gradient penalty}\n\t\t\\centering\n\t\t\\begin{tabular}{c | c | c | c }\t\n\t\t\t\\toprule\n\t\t\t\\midrule\n\t\t\t{ Method}& { $\\mathcal{L}_{GP}$}& $\\pi$&Lipschitz continuity\t \\\\\n\t\t\t\\hline\n\t\t\tWGAN-GP &$\\mathbb{E}_{\\hat{x}\\sim\\pi}(||\\nabla D_{\\theta}(\\hat{x})||_2-1)^2$&$t\\cdot p_r+(1-t)\\cdot p_g$&$||D_{\\theta}||_{Lip}\\to1$\\\\\n\t\t\t\\hline\n\t\t\tDRAGAN &$\\mathbb{E}_{\\hat{x}\\sim\\pi}(||\\nabla D_{\\theta}(\\hat{x})||_2-1)^2$&$p_r+\\epsilon$&$||D_{\\theta}||_{Lip}\\to1$\\\\\n\t\t\t\\hline \n\t\t\tMax-GP &$\\left(\\mathop{\\max}\\limits_{\\hat{x}\\sim\\pi}||\\nabla D_{\\theta}(\\hat{x})||_2-1\\right)^2$&$t\\cdot p_r+(1-t)\\cdot p_g$&$||D_{\\theta}||_{Lip}\\to1$\\\\\n\t\t\t\\hline\n\t\t\tALP &$\\mathbb{E}_{\\hat{x}\\sim\\pi}(||\\nabla D_{\\theta}(\\hat{x})||_2-1)^2$&$p_r\\cup p_g$&$||D_{\\theta}||_{ALP-Lip}\\to1$\\\\\n\t\t\t\\hline\n\t\t\tBanach-GP &$\\mathbb{E}_{\\hat{x}\\sim\\pi}(||\\nabla D_{\\theta}(\\hat{x})||_{B^*}-1)^2$&$t\\cdot p_r+(1-t)\\cdot p_g$&$||D_{\\theta}||_{Lip}\\to1$\\\\\n\t\t\t\\hline\n\t\t\tWGAN-LP &$\\mathbb{E}_{\\hat{x}\\sim\\pi}\\left[\\left(\\mathop{\\max}\\{0,||\\nabla D_{\\theta}(\\hat{x})||_2-1\\}\\right)^2\\right]$&$t\\cdot p_r+(1-t)\\cdot p_g$&$||D_{\\theta}||_{Lip}\\leq1$\\\\\n\t\t\t\\hline\n\t\t\tSWGAN &$\\mathbb{E}_{\\hat{x}\\sim\\pi}\\left[\\left(\\mathop{\\max}\\{0,||\\nabla D_{\\theta}(\\hat{x})||^2-1\\}\\right)^2\\right]$&$t\\cdot p_r+(1-t)\\cdot p_g$&$||D_{\\theta}||_{Lip}\\leq1$\\\\\n\t\t\t\\hline\n\t\t\tzc-GP &$\\mathbb{E}_{\\hat{x}\\sim\\pi}||\\nabla D_{\\theta}(\\hat{x})||^2_2$&$p_r\\cup p_g$&$||D_{\\theta}||_{Lip}\\to0$\\\\\n\t\t\t\\hline\n\t\t\tGAN-QP &$\\mathcal{L}_{GP}=\\mathbb{E}_{x_r,x_g\\sim\\pi}\\frac{\\left(D_{\\theta}(x_r)-D_{\\theta}(x_f)\\right)^2}{||x_r-x_f||}$&$\\pi=p_r\\cdot p_g$&$\\frac{\\left(D_{\\theta}(x_r)-D_{\\theta}(x_f)\\right)^2}{||x_r-x_f||}\\to 0$\\\\\n\t\t\t\\hline\n\t\t\tZP-Max &$\\mathop{\\max}\\limits_{\\hat{x}\\sim\\pi}||\\nabla D_{\\theta}(\\hat{x})||^2_2$&$t\\cdot p_r+(1-t)\\cdot p_g$&$||D_{\\theta}||_{Lip}\\to0$\\\\\n\t\t\t\\hline\n\t\t\tZP &$\\mathbb{E}_{\\hat{x}\\sim\\pi}||\\nabla D_{\\theta}(\\hat{x})||^2_2$&$t\\cdot p_r+(1-t)\\cdot p_g$&$||D_{\\theta}||_{Lip}\\to0$\\\\\n\t\t\t\\bottomrule\n\t\t\\end{tabular}\n\t\\end{table} \n\tWu et al.  used 0-GP, and proposed Wasserstein divergence. According to , Wasserstein divergence is solved by minimizing:\n\t\\begin{equation}\n\t\t\\label{0-gp}\n\t\t\\mathcal{L}_{GP}=\\mathbb{E}_{\\hat{x}\\sim\\pi}||\\nabla D_{\\theta}(\\hat{x})||^2,\n\t\\end{equation}\n\twhere $\\pi$ is both the real distribution ($p_r$) and the generated distribution ($p_g$): $\\pi=p_r\\cup p_g$. Furthermore, Mescheder et al.  also demonstrate that the optimization of unregularized GAN is not always locally convergent and some simplified zero centered gradient penalty (zc-GP) techniques, implemented by minimizing Eq (\\ref{0-gp}), can be used to achieve local convergence of GANs. Li et al.  introduce the adversarial training to discriminator training, which is turned out to be an adaptive 0-GP. \n\tBesides, some other 0-GP methods  are derived by different theoretical derivations. For instance, Su et al.  propose a Quadratic Potential (QP) for GANs training with the following formulation:\n\t\\begin{equation}\n\t\t\\mathcal{L}_{GP}=\\mathbb{E}_{x_r,x_g\\sim\\pi}\\frac{\\left(D_{\\theta}(x_r)-D_{\\theta}(x_f)\\right)^2}{||x_r-x_f||},\n\t\\end{equation}\n\twhere $\\pi$ is the joint distribution of the real and generated distributions: $\\pi=p_r\\cdot p_g$; Zhang et al.  combine a Total Variational (TV) regularizing term into the training of GANs, that is $|D_{\\theta}(x_r)-D_{\\theta}(x_f)-\\delta|$. According to , the TV term can be approximated by Eq (\\ref{0-gp}), which is exhilarating; Zhou et al.  propose the Lipschitz GANs, with the maximum of the gradients penalty for guaranteeing the gradient informativeness:\n\t\\begin{equation}\n\t\t\\mathcal{L}_{GP}=\\mathop{\\max}\\limits_{\\hat{x}\\sim\\pi}||\\nabla f(\\hat{x})||^2_2,\n\t\\end{equation}\n\twhere $\\pi=t\\cdot p_r+(1-t)\\cdot p_g$; Thanh-Tung et al.  also propose the 0-GP with gradients penalty at $\\pi=t\\cdot p_r+(1-t)\\cdot p_g$:\n\t\\begin{equation}\n\t\t\\mathcal{L}_{GP}=\\mathbb{E}_{\\hat{x}\\sim\\pi}||\\nabla f(\\hat{x})||^2_2.\n\t\\end{equation}\n\tIn summary, gradient penalty techniques are widely used in the GANs training to achieve Lipschitz continuity of discriminator. As shown in Table \\ref{table:gradient penalty}, many techniques are proposed based on different theories and phenomena. But to the best of our knowledge, there is no fair and comprehensive work comparing the performance of these gradient penalty methods.\n\t\\begin{comment}\n\tThe summary of the gradient penalty of the discriminator is shown in TABLE \\ref{table:gradient penalty}. Based on the summary and the 1-Lipschitz continuity in WGAN-GP, the gradient penalty has been improved in two directions.\n\t\\begin{enumerate}\n\t\\item[*] Limit the Lipschitz constant so that let the $||D||_{Lip}\\leq 1$ or $||D||_{Lip}\\to 0$.\n\t\\item[*] Explore the suitable scope for Lipschitz continuity.\n\t\\end{enumerate}\n\t\\end{comment}\n\tTo compare the performance of various methods intuitively, a comparative experiment on CIFAR-10 and CIFAR-100 datasets is conducted\\footnote{The base framework comes from wgan-gp in \\url{https://github.com/kwotsin/mimicry}}. The results of FID  for various gradient penalty methods with different loss functions are shown in Table \\ref{table: experiments results}. The results validate the conclusion in studies , that the Lipschitz constraint of discriminator may improve the performance and stability of GANs training regardless of the statistical distance used as a loss function. All gradient penalty methods improve the performance of GANs upon all three loss functions. Among them, zc-GP  obtains the best performance and is widely used in SOTA methods as illustrated in Table \\ref{table:SOTA }.\n\t\\begin{comment}\n\t\\begin{table}\n\t\\caption{The experimental results of different Gradient penalty}\n\t\\label{table: experiments results}\n\t\\setlength{\\tabcolsep}{1mm}\n\t\\centering\n\t\\begin{tabular}{c  c  c  }\t\n\t\\toprule\n\t\\midrule\n\t{ Method}&Inception Score& FID \\\\\n\t\\hline\n\tWGAN-GP &7.869&16.62\\\\\n\tWGAN-LP &7.98&15.85\\\\\n\tWGAN-ALP &8.247&14.19\\\\\n\tWGAN-GP-Max &7.956&18.43\\\\\n\tWGAN-ZP-Max &7.908&17.97\\\\\n\tWGAN-ZP-Sample &8.013&15.87\\\\\n\tWGAN-ZP &7.957&16.08\\\\\n\t\\bottomrule\n\t\\end{tabular}\n\t\\end{table} \n\t\\end{comment}\n\t\\begin{table*}\n\t\t\\tiny\n\t\t\\caption{ FID results on the CIFAR-10 and CIFAR-100 datasets for various gradient penalty methods with different GAN losses.}\n\t\t\\centering\n\t\t\\label{table: experiments results}\n\t\t\\begin{tabular}{cccccccccc}\n\t\t\t\\toprule\n\t\t\t\\multirow{2}{*}{Dataset}&\\multirow{2}{*}{Loss} & \\multicolumn{8}{c}{\\makecell*[c]{Gradient Penalty Methods}} \\\\ \n\t\t\t\\cline{3-10} \n\t\t\t&& \\makecell*[c]{None}&GP&DRAGAN &MAX-GP &LP&zc-GP &ZP-MAX &ZP  \\\\ \n\t\t\t\\midrule\n\t\t\t\\multirow{3}{*}{CIFAR-10}&\n\t\t\tGAN& 42.41& 23.45&20.98& 26.65&22.9&\\textbf{19.39}&24.38&23.96 \\\\\n\t\t\t&WGAN&290&30.38& 29.53&37.21&28.31&\\textbf{26.99}&31.28&30.19\\\\\n\t\t\t&Hinge&58.34&21.19&21.77&25.4&20.79&\\textbf{18.75}&23.1&22.58\\\\\n\t\t\t\\cline{1-10} \n\t\t\t\\specialrule{0em}{2pt}{2pt}\n\t\t\t\\multirow{3}{*}{CIFAR-100}&\n\t\t\tGAN&44.5&25.76& 25.37&24.29&23.82&\\textbf{21.81}&26.27&25.38 \\\\\n\t\t\t&WGAN&244&32.28&31.93&38.71&32.19&\\textbf{29.12}&39.75&37.8 \\\\\n\t\t\t&Hinge&59.43&25.13&25.42&28.34&23.67&\\textbf{21.55}&26.19&26.06\\\\\n\t\t\t\\bottomrule\n\t\t\\end{tabular}\n\t\\end{table*}", "cites": [142, 136, 143, 8317, 60, 7002, 88, 7001, 99, 94, 6997, 140, 8322, 141, 92, 64, 93, 135], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 21, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes several gradient penalty methods from multiple papers, organizing them into a coherent table that captures their formulations, sampling strategies, and Lipschitz properties. It provides some analytical insights by highlighting the role of gradient penalties in stabilizing GAN training and improving performance. However, the critical analysis is limited—while it notes that some methods improve FID, it does not deeply evaluate their theoretical weaknesses or compare them in a nuanced way. Abstraction is present in grouping methods under a shared goal of enforcing Lipschitz continuity, but it does not go beyond identifying patterns to form meta-level principles."}}
{"id": "930186fd-4bd2-4fae-a134-426d00072853", "title": "Weight Normalization", "level": "subsubsection", "subsections": [], "parent_id": "16b503b7-d01a-45ea-9b9c-0986b73e7409", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Fitting distribution\" "], ["subsection", "Weight Normalization and Weight Regularization"], ["subsubsection", "Weight Normalization"]], "content": "Spectral norm of the weight and the Lipschitz constant express the same concept. Therefore, weight normalization is another method to achieve Lipschitz continuity. {More important, weight normalization methods are Non-sampling-based, which don’t have the lack of support problem in contrast to gradient penalties.} Spectral normalization of the weight limits the Lipschitz constant to 1. Certainly, upper bound of the spectral norm can be used to normalize the weights, achieving $k (k\\leq1)$ Lipschitz continuity. The following lemmas put forward some upper bounds of the spectral norm.\n\t\\newenvironment{lemma3}{{\\indent\\it \\textbf{Lemma 3.1:}}}{\\hfill \\par}\n\t\\begin{lemma3}\n\t\t\\textit{\n\t\t\tIf $\\lambda_1\\leq\\lambda_2\\leq\\cdots\\leq\\lambda_M$\n\t\t\tare the eigenvalues of the $W^\\top W$, then the spectral norm$||W||_2=\\sqrt{\\lambda_M}$; The Frobenius norm$||W||_F=\\sqrt{\\sum_{i=1}^{M}\\lambda_i}$ \n\t\t}\n\t\\end{lemma3}\n\t\\newenvironment{lemma4}{{\\indent\\it \\textbf{Lemma 3.2:}}}{\\hfill \\par}\n\t\\newenvironment{lemma5}{{\\indent\\it \\textbf{Lemma 3.3:}}}{\\hfill \\par}\n\t\\newenvironment{proof3}{{\\indent\\it Proof 3.1:}}{\\hfill $\\square$\\par}\n\t\\newenvironment{proof4}{{\\indent\\it Proof 3.2:}}{\\hfill $\\square$\\par}\n\t\\newenvironment{proof5}{{\\indent\\it Proof 3.3:}}{\\hfill $\\square$\\par}\n\t\\begin{proof3}\n\t\tSee  and \n\t\\end{proof3}\n\t\\begin{lemma4}\n\t\t\\textit{\n\t\t\tFor a $n\\times m$ matrix, $||W||_1=\\mathop{\\max}\\limits_{j}\\sum_{i=1}^{n}|a_{i,j}|$, $ ||W||_\\infty=\\mathop{\\max}\\limits_{i}\\sum_{j=1}^{m}|a_{i,j}|$, then $||W||_2\\leq\\sqrt{||W||_1||W||_\\infty}$\n\t\t}\n\t\\end{lemma4}\n\t\\begin{proof4}\n\t\tSee \n\t\\end{proof4}\n\t\\begin{lemma5}\n\t\t\\textit{\n\t\t\tFor a $n\\times m$ matrix, \n\t\t\t$||W||_F=\\sqrt{\\left(\\sum_{j=1}^{m}\\sum_{i=1}^{n}|a_{i,j}|^2\\right)}$, then $||W||_2\\leq||W||_F$\n\t\t}\n\t\\end{lemma5}\n\t\\begin{proof5}\n\t\tSee \n\t\\end{proof5}\n\t1-Lipschitz continuity can be expressed by the spectral normalization. Miyato et al.  control the Lipschitz constant through spectral normalization $W_\\sigma=\\frac{W}{||W||_2}$ of each layer for D, leading to a better result than WGAN-GP. {Practically, the power iteration method is used as a fast approximation for the spectral norm ($||W||_2$).} Similarly, according to the optimal transport with regular term, Lipschitz constant of discriminator should be less than or equal to 1. Correspondingly, upper bound of the spectral norm can be utilized to normalize the weight ($||W_\\sigma||_2\\leq1$), achieving $k (k\\leq1)$ Lipschitz continuity. In terms of Lemma 3.2 and Lemma 3.3, $\\sqrt{||W||_1||W||_\\infty}$ and Frobenius norm ($||W||_F$) are simple upper bound of the spectral norm ($||W||_2$) and can be used to normalize the weight. For example, Zhang et al.  use the $\\sqrt{||W||_1||W||_\\infty}$, seeking for an approximation of the spectral norm that is easy to calculate. Miyato et al.  explain that the Frobenius norm is a restriction on all eigenvalues. It is different from the spectral norm, which only constrains the maximum eigenvalue. Authors conjecture that Frobenius normalization affects the network's ability to express, but no experiments are reported to compare it with the spectral normalization. Liu et al.  find that the mode collapse is often accompanied by the collapse of the eigenvalue of the discriminator. Because the spectral normalization only limits the maximum eigenvalue, and the eigenvalue collapse means the remaining eigenvalues suddenly decrease. Therefore, authors adopt the following methods to prevent the collapse of the eigenvalues:\n\t\\begin{equation}\n\t\tW_{\\sigma}=\\frac{W+\\nabla W}{||W||_2}=\\frac{W}{||W||_2}+\\frac{\\nabla W}{||W||_2}.\n\t\\end{equation}\n\tThe results demonstrate that this method effectively prevents mode collapses. Although the experiments are reported in this study, but it misses theoratical proofs. Therefore the relationship between the matrix eigenvalues and GAN performance is not clear.\n\tFew researches focus on weight normalization as demonstrated in Table \\ref{table:norm normalization}. Among these studies, spectral normalization is widely applied in some SOTA methods, as demonstrated in Section 7.\n\t\\begin{table}\n\t\\caption{The summary of the weight normalization and wight regularization.}\n\t\\label{table:norm normalization}\n\t\\centering\n\t\\begin{tabular}{c | c | c  }\t\n\t\t\\toprule\n\t\t\\midrule\n\t\t{ Method}&Implementation &Motivation\t \\\\\n\t\t\\hline\n\t\tSpectral normalization (SN) &$W_\\sigma=W/||W||_2$&$||D||_{Lip}\\to1$\\\\\n\t\t\\hline\n\t\tF normalization &$W_\\sigma=W/||W||_F$&$||D||_{Lip}\\leq1$\\\\\n\t\t\\hline\n\t\tMixed normalization &$W_\\sigma=W/\\sqrt{||W||_1||W||_\\infty}$&$||D||_{Lip}\\leq1$\\\\\n\t\t\\hline\n\t\tSpectral increment normalization &$W_\\sigma=W/||W||_2+\\nabla W/||W||_2$&$||D||_{Lip}\\to1$\\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table}", "cites": [78, 144], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information on weight normalization techniques, particularly spectral normalization, and connects concepts from different papers to explain their role in enforcing Lipschitz continuity. It includes some critical evaluation, such as noting the lack of theoretical proofs in one method and differences between Frobenius and spectral norms. However, the abstraction is limited to mathematical bounds and does not fully generalize to broader principles of GAN training stability."}}
{"id": "9c9a2b69-11c0-4e4b-90b7-101fced8cb32", "title": "Weight Regularization", "level": "subsubsection", "subsections": [], "parent_id": "16b503b7-d01a-45ea-9b9c-0986b73e7409", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Fitting distribution\" "], ["subsection", "Weight Normalization and Weight Regularization"], ["subsubsection", "Weight Regularization"]], "content": "Compared with spectral normalization similar to 1-GP, spectral regularization is similar to the 0-GP. Kurach et al.  use the $\\mathcal{L}_R=||W||_2$ to regularize the loss function. Zhou et al.  also use the $L_P$-norm ($P=1,F,\\infty$) to regularize the discriminator. However, these studies have worse performance than weight normalization and did not catch much attention among researchers.", "cites": [65], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 2.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides a minimal synthesis of the cited papers, merely listing two weight regularization approaches without integrating them into a broader context or framework. It includes a basic comparison by noting that these methods perform worse than weight normalization but lacks deeper analysis or justification for this claim. There is little abstraction or generalization, and the narrative remains very surface-level."}}
{"id": "3e0461ca-996a-4c2c-8c3e-dbd291acb92b", "title": "{Gradient Normalization", "level": "subsection", "subsections": [], "parent_id": "fdfb947c-602c-4735-9040-96d30810700e", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Fitting distribution\" "], ["subsection", "{Gradient Normalization"]], "content": "}\n\tGradient normalization is also a popular method to impose the Lipschitz constraint on the discriminator. As we all know, 1-Lipschitz constraint can be implemented by $\\mathop{\\min}\\mathbb{E}_{{x}\\sim\\pi}(||\\nabla_{x} D_{\\theta}({x})||_2-1)^2$ to let the gradient of the discriminator ($||\\nabla_{{x}} D_{\\theta}({x})||_2$) equal to 1. Therefore,  control the Lipischitz constant through gradient normalization $\\hat{D}_{\\theta}(x)=\\frac{D_{\\theta}(x)}{||\\nabla_{{x}}D_{\\theta}({x})||_2}$ for $D_{\\theta}$. Accordingly, the gradient of $\\hat{D}_{\\theta}$ can be represented as $||\\nabla_{{x}} \\hat {D}_{\\theta}({x})||_2=||\\nabla_{{x}}\\left(\\frac{D_{\\theta}(x)}{||\\nabla_{{x}}D_{\\theta}({x})||_2}\\right)||_2$, equaling to 1. To ensure the boundedness, different studies have different implementation. For instance,  adopts $\\hat{D}_{\\theta}(x)=\\frac{D_{\\theta}(x)}{||\\nabla_{{x}}D_{\\theta}({x})||_2+D_{\\theta}(x)}$ and  adopts $\\hat{D}_{\\theta}(x)=\\frac{D_{\\theta}(x)}{||\\nabla_{{x}}D_{\\theta}({x})||_2+\\epsilon}$.  Extensive experiments  demonstrate that both implementation of gradient normalization attain significant performance gains comparing to gradient penalty, weight normalization, and weight regularization.", "cites": [145, 146], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes information from two papers on gradient normalization methods and provides a basic comparison by mentioning different implementations and noting experimental performance gains. However, it lacks deeper critical analysis of the methods or limitations, and while it introduces the concept of gradient normalization, it does not generalize to broader theoretical patterns or principles in GAN training."}}
{"id": "ee52e377-012a-4b04-8fa5-1df1ed551949", "title": "{Summary", "level": "subsection", "subsections": [], "parent_id": "fdfb947c-602c-4735-9040-96d30810700e", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Fitting distribution\" "], ["subsection", "{Summary"]], "content": "}\n\t{As mentioned above, weight clipping, gradient penalty, weight regularization, weight normalization, and gradient normalization all could enable Lipschitz continuity of the discriminator. However, what are the advantages and disadvantages of investigated techniques? Imposing the Lipschitz constraint on the discriminator can be characterized by three properties . 1) \\textit{model-} or \\textit{module-wise} \\textit{constraint}. Model-wise constraint is defined as methods that constraint objective depends on full model, while module-wise constraint is defined as methods that constraint objective depends on layers. Generally, model-wise constraint is better since module-wise constraint is strict, which limits\n\t\tthe layer capacities and reduces the power of discriminator. 2) \\textit{sampling-based} or \\textit{non-sampling} \\textit{-based} \\textit{constraint}. Sampling-based constraint is defined as requiring sampling data during usage, while non-sampling-based constraint depends on the model, not data sampling. Generally, non-sampling-based constraint performs better since Lipschitz constraint should be fulfilled on the entire data manifold, not only sampling data. 3) \\textit{Hard} or \\textit{soft constraint}. {The accurate constraint of Lipschitz continuity is defined as hard constraint and the converse to be soft constraint. Hard constraint has achieved the exact Lipschitz continuity through limiting the spectral norm, which is expected to perform better. While soft constraint only obtain the Lipschitz continuity approximatively through optimization.} Table \\ref{table:lipschitz continuity} summarizes the properties of different technologies, from which gradient normalization is a model-wise, non-sampling-based, and hard constraint method.}\n\t\\begin{table}\n\t\t\\caption{Summary of different regularization and normalization technologies for imposing Lipschitz continuity.}\n\t\t\\label{table:lipschitz continuity}\n\t\t\\centering\n\t\t\\begin{tabular}{c | c | c | c }\t\n\t\t\t\\toprule\n\t\t\t\\midrule\n\t\t\tMethod& Model-wise&Non-sampling-based&Hard\\\\\n\t\t\t\\hline\n\t\t\tWeight Clipping&&\\checkmark&\\\\\n\t\t\t\\hline\n\t\t\tGradient Penalty&\\checkmark&&\\\\\n\t\t\t\\hline\n\t\t\tWeight Regularization&&\\checkmark&\\\\\n\t\t\t\\hline\n\t\t\tWeight Normalization&&\\checkmark&\\checkmark\\\\\n\t\t\t\\hline\n\t\t\tGradient Normalization&\\checkmark&\\checkmark&\\checkmark\\\\\n\t\t\t\\bottomrule\n\t\t\\end{tabular}\n\t\\end{table}", "cites": [145], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key regularization and normalization methods by organizing them under a novel framework defined by three properties: model-wise vs. module-wise, sampling-based vs. non-sampling-based, and hard vs. soft constraints. It critically evaluates each type of constraint, offering judgments on their effectiveness. The abstraction level is strong, as it moves beyond individual papers to define overarching categories and their implications for GAN training."}}
{"id": "44c32216-8be6-4e5c-aa2f-ca241ea4139f", "title": "Regularization and Normalization of \"Training dynamics\"", "level": "section", "subsections": ["f5ab6a67-88dd-45b6-aeae-1b85b061e943", "8377346c-065d-4bc5-adf5-275fad74664d"], "parent_id": "4410407c-7d7c-48f1-8174-a9a95319844f", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Training dynamics\""]], "content": "Assuming the objectives of GANs are convex-concave, some studies have proposed the global convergence of GANs . However, these theoretical convergence analyses are only applicable to the GANs with the optimal discriminator. Therefore, some studies focus on analyzing the local convergence of GANs. According to Nagarajan et al.  and Mescheder et al. , under some assumptions, GANs dynamics are locally convergent. However, if these assumptions are not satisfied, especially if the data distributions are not continuous, GANs dynamics do not always converge locally unless some regularization techniques are used.\n\tWe review {Jacobian regularization} techniques  in this section, which minimize the Jacobian matrix to achieve local convergence. With the same motivation, Mescheder et al.  propose a simplified gradient penalties method, named zero-centered gradient penalties (zc-GP), that guarantees the local convergence under suitable assumptions. Since it is similar to 0-GP, we cover it in Section 4.", "cites": [98, 54, 99, 7190, 8323], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from multiple papers on the training dynamics of GANs, particularly relating to local convergence and the role of regularization. It makes some connections between different methods (e.g., Jacobian regularization and zc-GP), but the analysis remains focused on specific techniques without a deeper evaluation of their relative strengths or broader theoretical implications. The abstraction level is moderate, identifying a recurring theme (convergence under assumptions) but not elevating it to a higher-level principle."}}
{"id": "f5ab6a67-88dd-45b6-aeae-1b85b061e943", "title": "Jacobian Regularization", "level": "subsection", "subsections": [], "parent_id": "44c32216-8be6-4e5c-aa2f-ca241ea4139f", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Training dynamics\""], ["subsection", "Jacobian Regularization"]], "content": "In \\textit{Proposition 2.2} of Section 2: absolute values of all eigenvalues of the Jacobian matrix ($v^{'}(\\phi,\\theta)$) are expected to be less than 1 at the fixed point, which is equivalent to the real part of the eigenvalue being negative. Additionally, the learning rate must be relatively low . To meet these requirements, Mescheder et al.  used the Consensus Optimization (ConOpt) to make the real part of the eigenvalue negative. Its regularized updates are:\n\t\\begin{equation} \\label{eqn1}\n\t\t\\begin{split}\n\t\t\t\\phi^{(k+1)}=\\phi^{(k)}+h\\nabla_\\phi\\left(- f(\\phi^{(k)},\\theta ^{(k)})-\\gamma L(\\phi^k,\\theta^k)\\right),\\\\\n\t\t\t\\theta^{(k+1)}=\\theta^{(k)}+h\\nabla_\\theta\\left(f(\\phi^{(k)},\\theta ^{(k)})-\\gamma L(\\phi^k,\\theta^k)\\right),\n\t\t\\end{split}\n\t\\end{equation}\n\twhere $L(\\phi^k,\\theta^k)=\\frac{1}{2}||v(\\phi^k,\\theta^k)||^2=\\frac{1}{2}\\left(||\\nabla_\\phi f(\\phi^k,\\theta^k)||^2+||\\nabla_\\theta f(\\phi^k,\\theta^k)||^2\\right)$ is the regularization of the Jacobian matrix.\n\tApart from , Nagaraja et al.  also analyze the relationship between local convergence of GANs and all eigenvalues of the Jacobian\n\tof the gradient vector field. Authors prove the local convergence for absolutely continuous generator and data distributions under certain regularity assumptions. This requires the loss function of the GANs to be strictly concave, which is not the case for some GANs. Based on this, a simple regularization technology that regularized the generator using the gradient of the discriminator is proposed by Nagaraja et al. . The regularized updates for the generator can be expressed as:\n\t\\begin{equation} \\label{eqn1}\n\t\t\\phi^{(k+1)}=\\phi^{(k)}-h\\nabla_\\phi f(\\phi^{(k)},\\theta ^{(k)})-\\frac{1}{2}h\\gamma\\nabla_\\phi||\\nabla_\\theta f(\\phi^{k},\\theta^{k})||^2.\n\t\\end{equation}\n\tHerein, the update of the discriminator is similar to SimGD. Furthermore, Nie et al.  propose a method that only regularizes the discriminator. The regularized update of the discriminator in this case is given by:\n\t\\begin{equation} \\label{eqn1}\n\t\t\\theta^{(k+1)}=\\theta^{(k)}+h\\nabla_\\theta f(\\phi^{(k)},\\theta ^{(k)})-\\frac{1}{2}h\\gamma\\nabla_\\theta||\\nabla_\\phi f(\\phi^{k},\\theta^{k})||^2.\n\t\\end{equation}\n\tThe update of the generator is the same as SimGD. Nie et al.  propose JAcobian REgularization (JARE) that regularizes both the generator and the discriminator. The regularized updates for the generator and the  discriminator are:\n\t\\begin{equation} \\label{eqn1}\n\t\t\\begin{split}\n\t\t\t\\phi^{(k+1)}=\\phi^{(k)}-h\\nabla_\\phi f(\\phi^{(k)},\\theta ^{(k)})-\\frac{1}{2}h\\gamma\\nabla_\\phi||\\nabla_\\theta f(\\phi^{k},\\theta^{k})||^2,\\\\\n\t\t\t\\theta^{(k+1)}=\\theta^{(k)}+h\\nabla_\\theta f(\\phi^{(k)},\\theta ^{(k)})-\\frac{1}{2}h\\gamma\\nabla_\\theta||\\nabla_\\phi f(\\phi^{k},\\theta^{k})||^2.\n\t\t\\end{split}\n\t\\end{equation}\n\tThe key difference between JARE and ConOpt is that JARE does not contain the Hessians $\\nabla^2_{\\phi,\\phi}f(\\phi^{k},\\theta^k)$ and $\\nabla^2_{\\theta,\\theta}f(\\phi^{k},\\theta^k)$ in the regularization term. \n\t{There are several Jacobian regularization methods that have been proposed to deal with the training instabilities of GANs. What is the difference of them? Nie et al.  consider a simple toy example to analyse the convergence of GANs. There may exist two factors of the Jacobian in the GANs dynamics simultaneously that destroy the GANs training: (i) the Phase Factor, i.e., the Jacobian has complex eigenvalues with a large imaginary-to-real ratio; (ii) the Conditioning Factor, i.e., the Jacobian is ill-conditioned. According to the toy example,\n\t\tOnly Regularizing Generator , Only Regularizing Discriminator , and ConOpt  could only alleviate the impact of the Phase Factor but not alleviating the impact of the Conditioning Factor. However, JARE  can address both factors by construction}.\\footnote{Intuitively, a reason for not introducing Hessians in JARE  is to avoid the risk of reversing the gradient flows, which may diverge the GAN training dynamics (see Appendix C in  for a detailed explanation).}\n\tThe above discussions of local convergence during GANs training involve a premise: absolutely continuous data and generator distributions. Indeed, the assumption of absolute continuity is not true for common cases of GANs, where both distributions, specially the data distribution, may lie on lower-dimensional manifolds . More generally, Mescheder et al.  extend the convergence proof by  to the case where the generator and data distribution do not locally have the same support. Based on this, a simplified zero-centered gradient penalties (zc-GP) method is proposed, which guarantees the local convergence under suitable assumptions. Zc-GP is obtained from the training dynamics, which is similar to 0-GP methods mentioned in Section 4.\n\t{Furthermore, there are also other literature studies  analyze the training of GANs through other tools. For instance,  introduces a novel algorithm, competitive gradient descent (CGD), that is a natural extension of gradient descent to the competitive setting. Different from gradient descent ascent (GDA) in Eq (\\ref{eq:eqn27}), CGD does not need to reduce the stepsize to match the increase of the interactions to avoid divergence. Specifically, CGD introduces an equilibrium term that lets each player prefer strategies that are less vulnerable to the actions of the other player.  also elucidates the cause of undesirable convergence of GDA is leader's (discriminator) gradient step takes the system away from the ridge, which has undesirable convergence properties and requires using very small learning rates to converge. To mitigate this, Follow-the-Ridge (FR) term ($\\mathbf{H}_{\\mathbf{\\theta} \\mathbf{\\theta}}^{-1} \\mathbf{H}_{\\mathbf{\\theta} \\mathbf{\\phi}} \\nabla_{\\mathbf{\\phi}} f\\left(\\mathbf{\\phi}^{(k)}, \\mathbf{\\theta}^{(k)}\\right)$) has been added to the updating of the discriminator.  studies the continuous-time dynamics induced by GANs training. In this perspective, instabilities in training GANs arise from the integration error in discretizing the continuous dynamics. It treats GANs training as solving ODEs and shows that higher-order solvers lead to better convergence.}", "cites": [74, 149, 7003, 98, 148, 99, 147, 7190], "cite_extract_rate": 1.0, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.8, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple Jacobian-related regularization methods and links them to training dynamics and convergence properties, creating a cohesive narrative. It critically compares approaches like ConOpt and JARE, highlighting their strengths and limitations in handling Phase and Conditioning Factors. The abstraction level is strong, as it generalizes the issues in GAN training (e.g., complex eigenvalues, ill-conditioning) and identifies broader principles in regularization design."}}
{"id": "8377346c-065d-4bc5-adf5-275fad74664d", "title": "Summary", "level": "subsection", "subsections": [], "parent_id": "44c32216-8be6-4e5c-aa2f-ca241ea4139f", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Training dynamics\""], ["subsection", "Summary"]], "content": "In summary, Jacobian regularization techniques are obtained from the training dynamics of GANs, which are used for achieving local convergence and stabilizing training. The summary of the Jacobian regularization methods is demonstrated in Table \\ref{table:Jacobian regularization}. Jacobian regularization is similar to the Gradient penalty in terms of update form. In general, zc-GP is used in many SOTA methods, as demonstrated in Section 7.\n\t\\begin{table}\n\t\t\\caption{The summary of the Jacobian regularization.}\n\t\t\\tiny\n\t\t\\label{table:Jacobian regularization}\n\t\t\\centering\n\t\t\\begin{tabular}{c|c|c}\t\n\t\t\t\\toprule\n\t\t\t\\midrule\n\t\t\tMethod& regularized updates of generator ($\\phi^{(k+1)}$)&regularized updates of discriminator ($\\theta^{(k+1)}$)\t \\\\\n\t\t\t\\hline\n\t\t\tSimGD &$\\phi^{(k)}-h\\nabla_\\phi f(\\phi^{(k)},\\theta ^{(k)})$&$\\theta^{(k)}+h\\nabla_\\theta f(\\phi^{(k)},\\theta ^{(k)})$\\\\\n\t\t\t\\hline\n\t\t\tConOpt &$\\phi^{(k)}-h\\nabla_\\phi f(\\phi^{(k)},\\theta ^{(k)})-\\frac{1}{2}h\\gamma\\nabla_\\phi||v(\\phi^{(k)},\\theta^{(k)})||^2$&$\\theta^{(k)}+h\\nabla_\\theta f(\\phi^{(k)},\\theta ^{(k)})-\\frac{1}{2}h\\gamma\\nabla_\\theta||v(\\phi^{(k)},\\theta^{(k)})||^2$\\\\\n\t\t\t\\hline\n\t\t\tGenerator &$\\phi^{(k)}-h\\nabla_\\phi f(\\phi^{(k)},\\theta ^{(k)})-\\frac{1}{2}h\\gamma\\nabla_\\phi||\\nabla_\\theta f(\\phi^{(k)},\\theta^{(k)})||^2$&$\\theta^{(k)}+h\\nabla_\\theta f(\\phi^{(k)},\\theta ^{(k)})$\\\\\n\t\t\t\\hline\n\t\t\tDiscriminator &$\\phi^{(k)}-h\\nabla_\\phi f(\\phi^{(k)},\\theta ^{(k)})$&$\\theta^{(k)}+h\\nabla_\\theta f(\\phi^{(k)},\\theta ^{(k)})-\\frac{1}{2}h\\gamma\\nabla_\\theta||\\nabla_\\phi f(\\phi^{(k)},\\theta^{(k)})||^2 $\\\\\n\t\t\t\\hline\n\t\t\tJARE &$\\phi^{(k)}-h\\nabla_\\phi f(\\phi^{(k)},\\theta ^{(k)})-\\frac{1}{2}h\\gamma\\nabla_\\phi||\\nabla_\\theta f(\\phi^{(k)},\\theta^{(k)})||^2$&$\\theta^{(k)}+h\\nabla_\\theta f(\\phi^{(k)},\\theta ^{(k)})-\\frac{1}{2}h\\gamma\\nabla_\\theta||\\nabla_\\phi f(\\phi^{(k)},\\theta^{(k)})||^2 $\\\\\n\t\t\t\\hline\n\t\t\tzc-GP &$\\phi^{(k)}-h\\nabla_\\phi f(\\phi^{(k)},\\theta ^{(k)})$&$\\theta^{(k)}+h\\nabla_\\theta f(\\phi^{(k)},\\theta ^{(k)})-\\frac{1}{2}h\\gamma\\nabla_\\theta||\\nabla D_{\\theta}(x)||^2 $\\\\\n\t\t\t\\bottomrule\n\t\t\\end{tabular}\n\t\\end{table}", "cites": [74, 98, 99, 7190], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily presents a descriptive overview of Jacobian regularization methods, listing techniques and their update forms in a table. It briefly mentions the similarity between Jacobian regularization and gradient penalty but does not critically analyze or synthesize the cited papers' contributions or limitations. There is little abstraction or generalization to broader principles of training dynamics or regularization in GANs."}}
{"id": "253f0a0c-e301-40a7-a3f4-7aa82d059919", "title": "Layer Normalization", "level": "subsection", "subsections": ["e30468f0-4365-4dc0-8ffe-61832a49d364", "f33ea61f-e2e1-4dfc-9f9a-3c4da4ad1e64"], "parent_id": "bc57c604-a3ec-457f-ab6f-07cf5243c526", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Other methods\""], ["subsection", "Layer Normalization"]], "content": "Data in machine learning is expected to be independent and identically distributed ($i.i.d$). However, in terms of deep learning, because of the Internal Covariate Shift (ICS) , inputs of each neuron do not satisfy the $i.i.d$, making the training of the deep neural networks hard and unstable. Layer normalization\\footnote{\\label{ft:10} layer normalization is different from the Layer Normalization (LN), where layer normalization is a general term for a class of methods such as BN, LN. } has been proposed to avoid such problems. The general form of the layer normalization is (the difference between the normalization methods lies in the choice of $h$ and the calculation of $\\mathbb{E}[h]$ and $var[h]$):\n\t\\begin{equation}\n\t\th_N=\\frac{x-\\mathbb{E}[h]}{\\sqrt{var[h]+\\epsilon}}\\cdot\\gamma+\\beta.\n\t\\end{equation}\n\tFor GANs, the layer normalization is divided into two parts: {\\textit{unconditional-based layer normalization}} and {\\textit{conditional-based layer normalization}}. Unconditional-based layer normalizations are used for unconditional generation similar to the other deep neural networks. On the other hand, conditional-based layer normalizations are used for the generator of the conditional generation, where the shift and scale parameters ($\\gamma, \\beta$) depend on the condition information, as given below:\n\t\\begin{equation}\n\t\th_N=\\frac{x-\\mathbb{E}[h]}{\\sqrt{var[h]+\\epsilon}}\\cdot\\gamma(c)+\\beta(c).\n\t\\end{equation}", "cites": [71], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of layer normalization and its application in GANs, referencing the concept of internal covariate shift from the cited paper on batch normalization. However, it lacks synthesis of deeper connections between normalization techniques and GAN training dynamics, and offers little critical analysis or abstraction to broader principles or frameworks."}}
{"id": "e30468f0-4365-4dc0-8ffe-61832a49d364", "title": "Unconditional-based layer Normalization", "level": "subsubsection", "subsections": [], "parent_id": "253f0a0c-e301-40a7-a3f4-7aa82d059919", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Other methods\""], ["subsection", "Layer Normalization"], ["subsubsection", "Unconditional-based layer Normalization"]], "content": "Unconditional-based layer normalization is used for both the generator and discriminator with the same motivation as in other deep neural networks. Ioffe et al.  proposed the first normalization for neural networks, namely, Batch Normalization (BN). Batch normalization adopts the data of the mini-batch to compute the mean and variance, making the data distribution of each mini-batch approximately the same. Miyato et al.  used the BN in GANs. BN normalizes at the mini-batch level, which destroys the difference between pixels during the generation on account of image generation being a pixel-level task. {Therefore, Batch Norm can be less applicable to style transfer and can’t be used with gradient penalty\n\t\tmethods since the gradient would be dependent on multiple inputs.} Contrary to BN which normalizes the same channel with different images, Layer Normalization\\textsuperscript{\\ref {ft:10}} (LN)  normalizes different channels of a single image that also destroys the diversity between channels for the pixel-by-pixel generative model . Instance Normalization (IN)  has also been proposed for style transformation that is adopted for a single channel of a single image. Moreover, Group Normalization (GN)  sits between LN and IN, which first divides the channel into many groups, and normalizes different groups of a single image. Compared to normalization of input of neural networks in BN, LN, IN and GN, Weight Normalization (WN)  normalizes the weight matrix of neural networks. Miyato et al.  also used this normalization in GANs. \n\tIn summary, unconditional-based layer normalization in GANs is similar to other neural networks. The related summaries are shown in Table \\ref{table:layer normalization}. To the best of our knowledge, no study compares the performance of these methods, therefore, we demonstrate the FID results\\footnote{The base framework comes from the SNGAN in \\url{https://github.com/kwotsin/mimicry}} for different normalization methods on CIFAR-10 and CIFAR-100 datasets in Table \\ref{table:normalization}. Among them, LN and GN obtained better performance than the most popular normalization method: Spectral normalization (mentioned in Section 4.2) and other methods significantly affect the stability of GANs training.\n\t\\begin{table*}\n\t\t\\caption{The summary of the layer normalization}\n\t\t\\label{table:layer normalization}\n\t\t\\scriptsize\n\t\t\\centering\n\t\t\\begin{tabular}{c|c | c | c  }\t\n\t\t\t\\toprule\n\t\t\t\\midrule\n\t\t\tMethod&{ Reference}& Classification&Inputs of $\\gamma(c)$ and $\\beta(c)$\t \\\\\n\t\t\t\\hline\n\t\t\tBatch Normalization (BN)&2018 & unconditional-based&-\\\\\n\t\t\t\\hline\n\t\t\tLayer Normalization (LN)&2018 & unconditional-based&-\\\\\n\t\t\t\\hline\n\t\t\tInstance Normalization (IN)&2018 & unconditional-based&-\\\\\n\t\t\t\\hline\n\t\t\tGroup Normalization (GN)&2018 & unconditional-based&-\\\\\n\t\t\t\\hline\n\t\t\tWeight Normalization (WN) &2018 & unconditional-based&-\\\\\n\t\t\t\\hline\n\t\t\tConditional Batch Normalization (CBN)&2018 &conditional-based&class label\\\\\n\t\t\t\\hline\n\t\t\tAdaptive Instance Normalization (AdaIN)&2017 ,2019 &conditional-based&target images\\\\\n\t\t\t\\hline\n\t\t\tSpatially-adaptive (de) Normalization (SPADE)&2019 &conditional-based&sematic segmentation map\\\\\n\t\t\t\\hline\n\t\t\tAttentive Normalization(AN)&2020 &conditional-based&self\\\\\n\t\t\t\\bottomrule\n\t\t\\end{tabular}\n\t\\end{table*}", "cites": [154, 155, 57, 8324, 153, 156, 150, 152, 151, 7194, 157, 78, 71], "cite_extract_rate": 1.0, "origin_cites_number": 13, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic overview of unconditional-based layer normalization methods in GANs and lists several normalization techniques with their classifications and references. While it makes some minimal connections (e.g., contrasting LN and BN), it lacks deeper synthesis of ideas across the cited works. The critical analysis is limited to a few observations, such as the inapplicability of BN for pixel-level generation, but does not delve into trade-offs or evaluate methods in detail. Abstraction is minimal, as it focuses on describing normalization types rather than generalizing broader principles or trends."}}
{"id": "f33ea61f-e2e1-4dfc-9f9a-3c4da4ad1e64", "title": "Conditional-based layer Normalization", "level": "subsubsection", "subsections": [], "parent_id": "253f0a0c-e301-40a7-a3f4-7aa82d059919", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Other methods\""], ["subsection", "Layer Normalization"], ["subsubsection", "Conditional-based layer Normalization"]], "content": "Conditional-based layer normalization is only used for the generator of the conditional generation. It aims to introduce conditional information to each layer of the generator, which helps to improve the quality of the generated images. $\\gamma(c)$ and $\\beta(c)$ in Eq (44) are calculated with different features or class labels as input to the neural network in different methods. Miyato et al.  and Zhang et al.  used the Conditional Batch Normalization (CBN) to encode class labels, thereby improving the quality of conditional generation. Huang et al.  and Karras et al.  used the Adaptive Instance Normalization (AdaIN) with target images to improve the accuracy of style transfer. Park et al.  used the Spatially-Adaptive (de) Normalization (SPADE) with semantic segmentation image to incorporate semantic information into all layers. Wang et al.  used the Attentive Normalization (AN) to model long-range dependent attention, which is similar to self-attention GAN .\n\tIn summary, the main difference between these conditional-based normalizations is the content of conditional inputs (c in Eq (45)). As the information of inputs is gradually enriched, the performance of conditional generation is gradually improved. The related summaries are shown in Table \\ref{table:layer normalization}.", "cites": [154, 8324, 150, 151, 157, 7194], "cite_extract_rate": 1.0, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a factual description of conditional-based layer normalization methods and their applications in GANs, mentioning several techniques and the associated papers. While it highlights the general idea of enriching conditional inputs to improve performance, it lacks in-depth comparison or critical evaluation of the methods. The synthesis is limited to listing how different authors use similar concepts with varying inputs, and abstraction remains at a surface level without identifying deeper patterns or principles."}}
{"id": "ed3fad77-b260-419c-962b-ecc924abe2e8", "title": "Inverse Gradient Penalty", "level": "subsection", "subsections": [], "parent_id": "bc57c604-a3ec-457f-ab6f-07cf5243c526", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Regularization and Normalization of \"Other methods\""], ["subsection", "Inverse Gradient Penalty"]], "content": "Mode collapse is a common phenomenon in GANs' training, that is, changes in the latent space do not cause changes in the generated images. Geometrically, the phenomenon means that all the tangent vectors of the manifold are no longer independent of each other - some tangent vectors either disappear or become linearly correlated with each other. Intuitively, we can solve this problem by maximizing the Lipschitz constant of the generator, which is opposite of the gradient penalty of the discriminator described in the previous section. Based on this, inverse gradient penalty of the generator has been proposed. Concretely, under the little perturbation of the latent space, the generator needs to produce different images. Yang et al.  use it in conditional generation, especially for tasks that are rich in conditional information, such as inpainting and super-resolution. \n\t\\begin{equation}\n\t\t\\mathop{\\max}\\limits_{G}\\mathcal{L}_z(G)=\\mathop{\\max}\\mathbb{E}_{z_1,z_2}\\left[\\mathop{\\min}\\left(\\frac{||G(y,z_1)-G(y,z_2)||}{||z_1-z_2||},\\tau\\right)\\right],\n\t\\end{equation}\n\twhere $y$ is the class label and $\\tau$ is the bound to ensure numerical stability. Unlike the intuition-based study described above, Odena et al.  demonstrate that the decreasing of singular value in the Jacobian matrix of the generator is the main reason for the mode collapse during GANs training. Furthermore, the singular value can be approximated by the gradient, so Jacobian clamping is used to limit singular values to $[\\lambda_{min},\\lambda_{\\max}]$. The loss is expressed as:\n\t\\begin{equation}\n\t\t\\mathop{\\min}\\limits_{G}\\mathcal{L}_z(G)=\\big(\\mathop{\\max}(Q,\\lambda_{\\max})-\\lambda_{\\max}\\big)^2\n\t\t+\\big(\\mathop{\\min}(Q,\\lambda_{\\min})-\\lambda_{\\min}\\big)^2,\n\t\\end{equation}\n\twhere $Q=||G(z)-G(z')||/||z-z'||$. \n\t{In summary, the above two methods  are similar and mitigate the model collapse of generator to some extent. The key point is to improve the sensitivity of the generator to latent space. In addition to the above methods used to implement inverse gradient penalty for generators, some studies  adopt orthogonal regularization to enforce amenability to truncation by conditioning G to be\n\t\tsmooth. Accordingly, the full space of $z$ will map to good output samples. Introducing orthogonality condition  is a direct method:}\n\t\\begin{equation}\n\t\tR(W)=\\beta\\left\\|W^{\\top} W -I\\right\\|_{\\mathrm{F}}^{2},\n\t\\end{equation}\n\t{where W is a weight matrix and $\\beta$ is a hyperparameter. However, this regularization is too limiting . Therefore, a relaxed constraint has been designed by . Brock et al.  apply Off-Diagonal Orthogonal Regularization (Off-Diagonal OR) to the generator directly enforcing the orthogonality condition:}\n\t\\begin{equation}\n\t\tR_o(W)=\\beta\\left\\|W^{\\top} W \\odot(\\mathbf{1}-I)\\right\\|_{\\mathrm{F}}^{2},\n\t\\end{equation}\n\t{where $\\mathbf{1}$ denotes a matrix with all elements set to 1. The Off-Diagonal OR makes G smooth so that the entire space of $z$ will map to good output samples. Orthogonality regularization is different from spectral normalization . Orthogonality regularization destroys the information about the spectrum by setting all the singular values to one, while spectral normalization only makes the maximum singular be one.\n\t}\n\t\\begin{table}\n\t\t\\caption{ FID results for different normalization methods on CIFAR-10 and CIFAR-100 datasets. (  The structure is the same as SNGAN except that the discriminator uses different normalization methods).\\tiny}\n\t\t\\label{table:normalization}\n\t\t\\centering\n\t\t\\begin{tabular}{ccc}\n\t\t\t\\toprule\n\t\t\t\\midrule\n\t\t\tmethods & CIFAR-10 & CIFAR-100 \\\\\n\t\t\t\\hline\n\t\t\tNone & 40.91 & 45.44 \\\\\n\t\t\tBN & 37.63 & 44.45\\\\\n\t\t\tLN & \\textbf{19.21} & 21.15 \\\\\n\t\t\tIN & 34.14 & 43.64\\\\\n\t\t\tGN & 19.31 & $\\textbf{20.80}$ \\\\\n\t\t\tWN & 24.28 & 29.96 \\\\\n\t\t\tSN&19.75&22.89\\\\\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t\\end{table}", "cites": [56, 159, 78, 160, 158], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes several regularization methods for mitigating mode collapse in GANs, particularly inverse gradient penalty and orthogonality regularization, and connects them to Jacobian conditioning and generator sensitivity. It offers some critical insights, such as noting that orthogonality regularization is too limiting and contrasting it with spectral normalization. However, while it identifies some patterns (e.g., the role of Jacobian singular values in mode collapse), the analysis remains somewhat focused on method descriptions rather than broader theoretical abstraction."}}
{"id": "fb4135aa-ff8b-439d-9a28-a790ffddd379", "title": "Applications of Regularization and Normalization in SOTA GANs", "level": "section", "subsections": [], "parent_id": "4410407c-7d7c-48f1-8174-a9a95319844f", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Applications of Regularization and Normalization in SOTA GANs"]], "content": "{In this section, to provide a side view to the selection of regularization and normalization techniques, we investigate the applications of regularization and normalization techniques frequently employed in state-of-the-art and popular GANs. We select six methods (One per year from 2017-2022) categorized into two classes according to different tasks: Unconditional Generation and Conditional Generation. The selected methods and analysis are shown in Table \\ref{table:SOTA }. PGGAN  is a popular GAN model in recent years, which grows the size of both the generator and discriminator progressively. PGGAN empowers high-resolution image generation. Since PGGAN was proposed in 2017, only some simple regularization techniques were applied: WGAN-GP , BN , and LN ; BigGAN  is a popular conditional generative adversarial networks, which uses many regularization and normalization techniques, such as zc-GP, SN , Off-Diagonal OR , and CBN ; AutoGAN  is the first study introducing the Neural architecture search (NAS) to GANs. It defines the search space for the generator architecture and adopts Inception score as the reward to discover the best architecture. The main focus of AutoGAN is architecture, so AutoGAN only comprises SN ; StyleGAN2  is the most popular architecture of GANs, which produces photorealistic images with large varieties and is widely used in image generation, such as Image Completion , Image-to-Image Translation . StyleGAN2-ADA  proposes a novelty adaptive data augmentation methods. Combining StyleGAN2 and adaptive data augmentation, StyleGAN2-ADA  obtains impressive performance in image generation, particularly in data-effficient generation. Furthermore, InsGen  combines StyleGAN2-ADA with contrastive learning, acquiring state of the art on many generation tasks and datasets. Recently, StyleGAN-XL  scales StyleGAN to large diverse datasets and sets a new state-of-the-art on large-scale image synthesis. In summary, many regularization and normalization techniques have been used in state-of-the-art GANs with zc-GP and SN being more attractive to researchers. Data augmentation is a striking method and orthogonal to other ongoing researches on training, architecture, and regularization. Therefore, popular augmentation strategies, such as ADA, have been employed as default operations GANs training. Furthermore, self-supervision has been used to further improve the performance of GANs, which is also orthogonal to other methods.}\n\t\\begin{table}\n\t\t\\caption{The applications of the Regularization and Normalization techniques used in SOTA GANs.}\n\t\t\\label{table:SOTA }\n\t\t\\centering\n\t\t\\footnotesize\n\t\t\\begin{tabular}{c | c | c  |c | c | c|c }\t\n\t\t\t\\hline\n\t\t\t\\hline\n\t\t\t{ Method}& Task&\\makecell[c]{Gradient \\\\Penalty}&\\makecell[c]{Data augmentation\\\\ and preprocessing}&\\makecell{Self\\\\supervision}&\\makecell[c]{Weight\\\\ normalization }&\\makecell[c]{Layer\\\\ normalization}\\\\\n\t\t\t\\hline\n\t\t\t\\makecell{PGGAN\\\\(2017)}&\\makecell{Unconditinal\\\\ Generation}&WGAN-GP&\\makecell{None}&None&\\makecell{None}&BN: G,LN: D\\\\\n\t\t\t\\hline\n\t\t\t\\makecell{BigGAN\\\\ (2018 )}&\\makecell{Conditinal \\\\Generation}&zc-GP&None&None&\\makecell{SN: G, D}&CBN\\\\\n\t\t\t\\hline\n\t\t\t\\makecell{AutoGAN\\\\ (2019 )}&\\makecell{Unconditinal\\\\ Generation}&None&None&None&\\makecell{SN:D}&None\\\\\n\t\t\t\\hline\n\t\t\t\\makecell{StyleGAN2-ADA\\\\(2020)}&\\makecell{Unconditinal \\\\Generation}&zc-GP&\\makecell{Adaptive}&None&None&IN\\\\\n\t\t\t\\hline\n\t\t\t\\makecell{InsGen\\\\(2021)}&\\makecell{Unconditinal \\\\Generation}&zc-GP&\\makecell{Adaptive}&contrastive&None&IN\\\\\n\t\t\t\\hline\n\t\t\t\\makecell{StyleGAN-XL\\\\(2022)}&\\makecell{Conditinal \\\\Generation}&None&\\makecell{Translation\\\\Cutout}&None&SN:D&IN\\\\\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t\\end{table}", "cites": [56, 164, 8324, 108, 99, 130, 161, 162, 163, 62, 157, 78, 8317], "cite_extract_rate": 1.0, "origin_cites_number": 13, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of how regularization and normalization techniques are used in state-of-the-art GANs, with a table summarizing key methods. While it briefly connects some techniques (e.g., zc-GP and SN being attractive to researchers), it lacks deeper synthesis across the cited works. Critical evaluation of methods or identification of broader principles is minimal, and the narrative remains largely at the level of listing applications."}}
{"id": "9cf8af1d-75ca-445d-aa5a-fb3169fa5585", "title": "Summary", "level": "subsection", "subsections": [], "parent_id": "b359d174-4880-4149-b0f4-5fd0e7bd75f4", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Summary and Outlook"], ["subsection", "Summary"]], "content": "Recently, significant achievements of GANs have been made in generation tasks and the network has been widely used in many computer vision tasks, such as image inpainting, style transfer, text-to-image translations, and attribute editing. However, due to the overconfident assumptions, the training faces many challenges, such as non-convergence, mode collapse, gradient vanishing, and overfitting. To mitigate these problems, many solutions focus on designing new architectures, new loss functions, new optimization methods, and regularization and normalization techniques. \n\tIn this paper, we study GANs training from three perspectives and propose a new taxonomy, denoted as \\textbf{\"Training dynamics\"}, \\textbf{\"Fitting distribution\"}, \\textbf{\"Real \\& Fake\"}, and \\textbf{\"Other methods\"}, to survey the different regularization and normalization techniques during GANs training. Our study provides a systematic and comprehensive analysis of the reviewed methods to serve researchers of the community. In addition, we also demonstrate the motivation and objectives of different methods and compare the performance of some popular methods in a fair manner quantitatively, which has implications for future research in selecting their research topics or developing their approaches.\n\t\\begin{comment}\n\tThis paper summarizes the regularization and normalization of generative adversarial networks and explains the problem from three aspects: \n\t\\textbf{Optimal transport and Lipschitz continuity:} First, optimal transport and optimal transport with the regular term are introduced. According to the duality form, WGAN-GP  and WGAN-LP  are proposed to make the discriminator satisfy 1-Lipschitz continuity and k-Lipschitz continuity ($k\\leq1$), respectively. Next, many gradient penalty methods have been analyzed. These methods achieved certain improvements as compared with WGAN-GP in two prespecitves: \\textbf{(1) To limit the Lipschitz constant to a small value}, such as 0 ; \\textbf{(2) To find the right restricted space}. The restricted space of WGAN-GP and WGAN-LP is the interpolation between the real images and fake images. Some works restricted it at the real images or fake images . The relationship between the Lipschitz constant and the spectral norm of the matrix is also derived in this paper. It indicates that the normalization of the spectral norm  can also be used to achieve 1-Lipschitz continuity. Furthermore, the Frobenius norm is proved to be the upper bound of the spectral norm. The results of WGAN-LP can be obtained by normalizing the Frobenius norm. Moreover, the work  analyzed the relationship between the mode collapse of the GAN and the eigenvalue distribution of the discriminator weight matrix, ending up in avoiding the mode collapse by limiting the eigenvalue collapse. However, there is a lack of the specific reason or sound proof.\n\t\\textbf{ Training dynamics:} GANs is a two-player zero-sum game. Due to the nonlinearity of the neural network, it is difficult to find the global convergence solution. In terms of the local convergence, the Jacobian regularization needs to be added. Few research studies are found in this area.\n\t\\textbf{ Representation:} As mentioned earlier, training of GANs is a semi-supervised task. More prior information is needed to improve the presentative ability of the network. Conditional layer normalization is used to better encode condition information and reduce the difficulty of conditional generation. Consistent regularization, data augmentation and self-supervision use unsupervised methods to improve the supervision information in GANs training, and use additional labels and tasks to improve GANs performance. We suggest that this area is promising and more studies can be conducted in this specific direction.\n\t\\end{comment}", "cites": [144, 88, 7001, 99, 8322, 78, 8317, 6997], "cite_extract_rate": 0.8888888888888888, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes key ideas from multiple papers effectively, integrating concepts like Lipschitz continuity, spectral norms, and training dynamics into a coherent taxonomy. It provides critical evaluation by discussing limitations (e.g., lack of sound proof in eigenvalue-based methods) and contrasting approaches (e.g., WGAN-GP vs. Frobenius norm normalization). The abstraction is reasonable, identifying broader themes such as the relationship between weight matrix properties and mode collapse, and suggesting promising research directions in representation and regularization."}}
{"id": "c7f682e9-c24a-41ee-8bfd-ac2766fa59c0", "title": "Outlook", "level": "subsection", "subsections": [], "parent_id": "b359d174-4880-4149-b0f4-5fd0e7bd75f4", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Summary and Outlook"], ["subsection", "Outlook"]], "content": "By reviewing the regularization and normalization of GANs, the following questions and thoughts are proposed based on different perspectives of GANs training:\n\t\\begin{itemize}\n\t\t\\item [1)] \n\t\t\\textit{What is a good distance metric, and which divergence should be used in GANs training?} The priority in the training process of GANs is to find a suitable divergence to measure the distance between the generated distribution and the true distribution. Wasserstein divergence is important for the training of GANs. However, it is uncertain whether the next proposed divergence performs better.\n\t\t\\item [2)]\n\t\t\\textit{What is the main difference between real images and generated images?} During the training of unconstrained and unprioritized GANs, if we can quantitatively represent the difference between real images and generated images from different perspectives, the efficient regularization methods can be designed based on this.\n\t\t\\item [3)]\n\t\t\\textit{How to avoid real images forgetting\\footnote{Real images forgetting is caused by not introducing real images while training the generator, which is different from discriminator forgetting.}?} As acknowledged, real images do not directly participate in the training of the generator, thus the discriminator needs to remember the characteristics of the real images to optimize the generator indirectly. We call this the real images forgetting. We conjecture that real images forgetting may exist, and which may increase the difficulty of GANs training. Some works might serve as basis to prove this hypothesis and propose effective solutions.\n\t\t\\item [4)]\n\t\tRecent studies show that discriminator suffers from overfitting and discriminator forgetting. It is a common problem of neural networks, which is caused by the shortcut of the loss driven method. Some new methods, such as contrastive learning, representation learining, can be proposed to improve the generalization of the discriminator.\n\t\t\\item [5)]\n\t\tRecently, diffusion model  acquires the impressive performance in image generation. One possible reason for success is the phased training strategy in diffusion model . Inspired by this, some strategies to reduce the difficulty of GANs training may be proposed.\n\t\\end{itemize}\n\t\\section*{Acknowledgment}\n\tThe work is partially supported by the National Natural Science Foundation of China under Grand No.U19B2044, No.61836011 and No.91746209. We are very grateful to the help of Jianlin Su, whose blog is \\url{https://spaces.ac.cn/tag/GAN/}.\n\t\\bibliographystyle{ACM-Reference-Format}\n\t\\bibliography{bare_jrnl_transmag}\n\t\\appendix\n\t\\clearpage", "cites": [165], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section presents analytical insights by identifying open research questions and drawing connections between GAN training challenges and recent advancements, such as diffusion models and contrastive learning. While it integrates a limited number of sources (only one is explicitly cited), it abstracts broader issues like the choice of divergence, real image forgetting, and training strategies, suggesting potential future directions. Critical evaluation is moderate, focusing more on conjectures than on detailed limitations or comparisons of existing methods."}}
{"id": "9df0baaf-f220-4a8d-b36a-27ebd583cf6c", "title": "Optimal Transport and Lipschitz Continuity", "level": "subsection", "subsections": [], "parent_id": "f6a7c46a-3c0c-4605-a602-4104833c8854", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Supplementary Online-only Material"], ["subsection", "Optimal Transport and Lipschitz Continuity"]], "content": "\\label{sect:A-1}\n\tOptimal transport  was proposed in the 18th century to minimize the transportation cost while preserving the measure quantities. Given the space with probability measures $(X,\\mu)$ and $(Y,\\upsilon)$, if there is a map $T:X\\rightarrow Y$ which is measure-preserving, then for any $B\\subset Y$, having:\n\t\\begin{equation}\\label{eqn1}\n\t\t\\int_{T^{-1}(B)}\\mathrm{d}\\mu(x)=\\int_B \\mathrm{d}\\upsilon(y).\n\t\\end{equation}\n\tWriting the measure-preserving map as $T_*(\\mu)=\\upsilon$. For any $x\\in X$ and $y\\in Y$, the transportation distance is defined as $c(x,y)$, the total transportation cost is given by:\n\t\\begin{equation}\\label{eqn1}\n\t\tC(T):=\\int_X c(x,T(x)) \\mathrm{d}\\mu(x).\n\t\\end{equation}\n\tIn the 18th century, Monge et al.  proposed the Optimal Mass Transportation Map that corresponds to the smallest total transportation cost: $C(T)$. The transportation cost corresponding to the optimal transportation map is called the Wasserstein distance between probability measures $\\mu$ and $\\upsilon$:\n\t\\begin{equation}\\label{eqn3}\n\t\tW_c(\\mu,\\upsilon)=\\mathop{\\min}\\limits_{T}\\left\\{\\int_X c(x,T(x)) \\mathrm{d}\\mu(x)\\ |\\ T_*(\\mu)=\\upsilon\\right\\}.\n\t\\end{equation}\n\tIn 1940s, Kantorovich  proved the existence and uniqueness of the solution for Monge problem, and according to the duality of linear programming, the Kantorovich-Rubinstein (KR) duality of Wasserstein distance is given by:\n\t\\begin{equation}\\label{eqn1}\n\t\tW_c(\\mu,\\upsilon)\n\t\t=\\mathop{\\max}\\limits_{\\varphi ,\\psi}\\left\\{\\int_X \\varphi \\mathrm{d}\\mu\\ +\\int_Y \\psi \\mathrm{d}\\upsilon\\ |\\ \\varphi(x)+\\psi(y)\\leq c(x,y)\\right\\}.\n\t\\end{equation}\n\tThis dual problem is constrained, defining the c-transform: $\\psi(y)=\\varphi^c(y):=inf_x\\{c(x,y)-\\varphi(x)\\}$, and the Wasserstein distance becomes:\n\t\\begin{equation}\\label{eqn1}\n\t\tW_c(\\mu,\\upsilon)=\\mathop{\\max}\\limits_{\\varphi}\\left\\{\\int_X \\varphi \\mathrm{d}\\mu\\ +\\int_Y \\varphi^c \\mathrm{d}\\upsilon\\right\\},\n\t\\end{equation}\n\twhere $\\varphi$ is called the Kantorovich potential. It can be shown that if $c(x,y)=|x-y|$ and Kantorovich potential satisfies the 1-Lipschitz continuity, then $\\varphi^c=-\\varphi$. Kantorovich potential can be fitted by a deep neural network, which is recorded as $\\varphi_\\xi$. Wasserstein distance is:\n\t\\begin{equation}\n\t\tW_c(\\mu,\\upsilon)=\\mathop{\\max}\\limits_{||\\varphi_\\xi||_L\\leq 1}\\left\\{\\int_X \\varphi_\\xi \\mathrm{d}\\mu\\ -\\int_Y \\varphi_\\xi \\mathrm{d}\\upsilon\\right\\}.\n\t\t\\label{eqn7}\n\t\\end{equation}\n\tIf $X$ is the generated image space, $Y$ is the real sample space, $Z$ is latent space and $g_\\theta$ is the geneartor, the Wasserstein GANs (WGAN) is formulated as a min-max problem:\n\t\\begin{equation}\\label{eqn1}\n\t\t\\mathop{\\min}\\limits_{\\theta}\\mathop{\\max}\\limits_{||\\varphi_\\xi||_L\\leq 1}\\left\\{\\int_Z \\varphi_\\xi(g_\\theta(z)) \\mathrm{d}z \\ -\\int_Y \\varphi_\\xi(y) \\mathrm{d}y\\right\\}.\n\t\\end{equation}\n\tIn the optimization process, the generator and the Kantorovich potential function (discriminator) are independent of each other, optimized in a step-by-step iteration.\n\tIf $c(x,y)=\\frac{|x-y|^2}{2}$, there is a convex function $u$ that is called Brenier potential . The optimal transportation map is given by the gradient map of Brenier potential: $T(x)=\\nabla u(x)$. There exists a relationship between Kantorovich potential and Brenier potential : \n\t\\begin{equation}\\label{eqn1}\n\t\tu(x)=\\frac{|x|^2}{2}-\\varphi(x).\n\t\\end{equation}\n\tFrom the previous discussion, it is evident that the optimal transportation map (Brenier potential) corresponds to the generator, and Kantorovich potential corresponds to the discriminator. After the discriminator is optimized, the generator is directly drivable without the optimization process .\n\tThe transportation cost of Eq (\\ref{eqn3}) is defined as the form of two distribution distances:\n\t\\begin{equation}\\label{eqn10}\n\t\tOT(P||Q)=\\mathop{inf}\\limits_{\\pi}\\int\\pi(x,y)c(x,y)\\mathrm{d}x\\mathrm{d}y,\n\t\\end{equation}\n\twhere $\\pi(x,y)$ is the joint distribution, satisfying $\\int_y\\pi(x,y)dy=P(x)$ and $\\int_x\\pi(x,y)dx=Q(y)$. The dual form of Eq (\\ref{eqn10}) is derived as follows::\n\t\\begin{equation}\\label{eqn1}\n\t\tOT(P||Q)=\\mathop{\\max}\\limits_{\\varphi ,\\psi}\\{\\int_x \\varphi(x)P(x) \\mathrm{d}x\\ \\\\ +\\int_y\\psi(y)Q(y) \\mathrm{d}y\\ |\\ \\varphi(x)+\\psi(y)\\leq c(x,y)\\}.\n\t\\end{equation} \n\tConsidering the optimal transportation with regular terms, Peyr{\\'e} et al.  added the entropic regularization for optimal transportation that transforms the dual problem into a smooth unconstrained convex problem. The regularized optimal transport is defined as:\n\t\\begin{equation}\\label{eqn12}\n\t\tOT_c(P||Q)=\\mathop{\\min}\\limits_{\\pi}\\int\\pi(x,y)c(x,y)\\mathrm{d}x\\mathrm{d}y+\\epsilon E(\\pi).\n\t\\end{equation}\n\tIf  $E(\\pi)=\\int_x\\int_y\\pi(x,y)\\log(\\frac{\\pi(x,y)}{P(x)Q(y)})\\mathrm{d}x\\mathrm{d}y$, Eq (\\ref{eqn12}) can be written as:\n\t\\begin{equation}\\label{eqn13}\n\t\t\\begin{split}\n\t\t\tOT_c(P||Q)=&\\mathop{\\min}\\limits_{\\pi}\\int\\pi(x,y)c(x,y)\\mathrm{d}x\\mathrm{d}y+\\epsilon \\int_x\\int_y\\pi(x,y)\\log\\left(\\frac{\\pi(x,y)}{P(x)Q(y)}\\right)\\mathrm{d}x\\mathrm{d}y\\\\\n\t\t\ts.t. \\int_y\\pi(x,y)&\\mathrm{d}y=P(x),\\int_x\\pi(x,y)\\mathrm{d}x=Q(y).\n\t\t\\end{split}\n\t\\end{equation}\n\tThe dual form of Eq (\\ref{eqn13}) becomes:\n\t\\begin{equation}\n\t\t\\begin{split}\n\t\t\tOT_c(P||Q)\n\t\t\t&=\\mathop{\\max}\\limits_{\\varphi ,\\psi}\\int_x \\varphi(x)P(x) \\mathrm{d}x\\ +\\int_y\\psi(y)Q(y)\\mathrm{d}y\\\\\n\t\t\t&+\\frac{\\epsilon}{e}\\int_x\\int_y\\exp\\left(\\frac{-\\left(c(x,y)+\\varphi(x)+\\psi(y)\\right)}{\\epsilon}\\right)\\mathrm{d}x\\mathrm{d}y.\n\t\t\\end{split}\n\t\t\\label{EQ:eqn14}\n\t\\end{equation}\n\tPetzka et al.  set $c(x,y)=||x-y||_2$ in Eq (\\ref{EQ:eqn14}), and the dual form of optimal transport with the regular term can be expressed as:\n\t\\begin{equation}\n\t\t\\mathop{\\sup}\\limits_{\\varphi,\\psi}\\{\\mathbb{E}_{x\\sim{p(x)}}[\\varphi(x)]-\\mathbb{E}_{y\\sim q(y)}[\\psi(y)]\n\t\t-\\frac{4}{\\epsilon}\\int\\int\\mathop{\\max}\\{0,(\\varphi(x)-\\psi(y)-||x-y||_2)\\}^2\\mathrm{d}p(x)\\mathrm{d}q(y) \\}.\n\t\t\\label{EQ:eqn32}\n\t\\end{equation}\n\tSimilar to dealing with a single function, one can replace $\\varphi = \\psi$ in Eq (\\ref{EQ:eqn32}), which leads to the objective of minimum:\n\t\\begin{equation}\\label{eqn1}\n\t\t\\mathbb{E}_{y\\sim q(y)}[\\varphi(y)]-\\mathbb{E}_{x\\sim p(x)}[\\varphi(x)]\n\t\t+\\frac{4}{\\epsilon}\\int\\int\\mathop{\\max}\\{0,(\\varphi(x)-\\varphi(y)-||x-y||_2)\\}^2\\mathrm{d}p(x)\\mathrm{d}q(y).\n\t\\end{equation}", "cites": [88, 95, 8318], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes key concepts from optimal transport theory and relates them to GAN training, particularly by linking Kantorovich and Brenier potentials to generator and discriminator roles. It provides a coherent analytical framework by integrating mathematical formulations from multiple papers. However, it lacks deeper critical evaluation of the methods or limitations of the cited works, and while it abstracts some general principles, the analysis remains grounded in technical formulations without reaching a meta-level insight."}}
{"id": "375c251d-5d04-454e-83a6-2140b14015f5", "title": "Some Propositions of Training Dynamic in GANs", "level": "subsection", "subsections": [], "parent_id": "f6a7c46a-3c0c-4605-a602-4104833c8854", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Supplementary Online-only Material"], ["subsection", "Some Propositions of Training Dynamic in GANs"]], "content": "\\label{sect:A-2}\n\t\\newenvironment{lemma1}{{\\indent\\it \\textbf{Proposition 2.1:}}}{\\hfill \\par}\n\t\\begin{lemma1}\n\t\t\\textit{For zero-sum games, $v^{'}$is negative semi-definite for any local Nash-equilibrium. Conversely, if $v(\\bar{x})=0$ and $v^{'}$is negative definite, then $\\bar{x}$ is a local Nash-equilibrium.\n\t\t}\n\t\\end{lemma1}\n\t\\newenvironment{proof1}{{\\indent\\it Proof 2.1:}}{\\hfill $\\square$\\par}\n\t\\begin{proof1}\n\t\tRefer to \n\t\\end{proof1}\n\t\\textit{Proposition 2.1}  gives the conditions for the local convergence of GANs, which is converted into the negative semi-definite problem of the Jacobian matrix. Negative semi-definite of the Jacobian matrix corresponds to its eigenvalue less than or equal to 0. If the eigenvalue of the Jacobian matrix at a certain point is a negative real number, the training process can converge; but if the eigenvalue is complex and the real part of the eigenvalue is small and the imaginary part is relatively large, the training process is difficult to converge unless the learning rate is very small. \n\t\\newenvironment{lemma2}{{\\indent\\it \\textbf{Proposition 2.2:}}}{\\hfill \\par}\n\t\\begin{lemma2}\n\t\t\\textit{\n\t\t\tLet $F:\\Omega\\rightarrow\\Omega$ be a continuously differentiable function on an open subset $\\Omega$ of $R^n$ and let $\\bar{x}\\in\\Omega$ be so that: 1. $F(\\bar{x})=\\bar{x}$ and 2. the absolute values of the eigenvalues of the Jacobian $F^{'}(x)$ are all smaller than 1.}\n\t\t\\textit{There is an open neighborhood $U$ of $\\bar{x}$ so that for all $x_0\\in U$, the iterates $F^{(k)}(x_0)$ converge to $\\bar{x}$. The rate of convergence is at least linear. More precisely, the error $||F^{(k)}(x_0)-\\bar{x}||$ is in $\\mathcal{O}(|\\lambda_{max}|^k)$ for $k\\rightarrow\\infty$ where $\\lambda_{max}$ is the eigenvalue of $F^{'}(\\bar{x})$ with the largest absolute value.}\n\t\\end{lemma2}\n\t\\newenvironment{proof2}{{\\indent\\it Proof 2.2:}}{\\hfill $\\square$\\par}\n\t\\begin{proof2}\n\t\tRefer to Section 3 in  and Proposition 4.4.1 in .\n\t\\end{proof2}", "cites": [7190], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section analytically presents propositions related to the training dynamics of GANs, particularly focusing on the Jacobian matrix's eigenvalues and their impact on convergence. It synthesizes the key mathematical insight from the cited paper (The Numerics of GANs) by connecting eigenvalue properties to training stability. While it provides some abstraction by highlighting general conditions for convergence, it lacks deeper critical evaluation of the cited work and does not compare or contrast with alternative viewpoints or methods."}}
{"id": "d07e4dc9-6fdd-4534-85ee-925acd678910", "title": "Spectral Norm and the Lipschitz Constant", "level": "subsection", "subsections": [], "parent_id": "f6a7c46a-3c0c-4605-a602-4104833c8854", "prefix_titles": [["title", "A Systematic Survey of Regularization and Normalization in GANs"], ["section", "Supplementary Online-only Material"], ["subsection", "Spectral Norm and the Lipschitz Constant"]], "content": "\\label{sect:A-3}\n\t1-Lipschitz continuity is represented as:\n\t\\begin{equation}\n\t\t||D(x_1)-D(x_2)||\\leq ||x_1-x_2||.\\footnote{Lipschitz continuity can be defined by any form of norm.}\n\t\\end{equation}\n\tGenerally, considering the K-Lipschitz for a neural network $f(x)$:\n\t\\begin{equation}\n\t\tf(x)=g_N\\circ\\cdots g_2\\circ g_1(x),\\footnote{$\\circ$ is the symbol for function cascade. Specifically, $h\\circ g(x)=h(g(x))$. This definition of neural network is not general, such as DenseNet  and ResNet , which can not be defined like this. Therefore, we do not strictly derive the relationship between the matrix norm and Lipschitz continuity.}\n\t\\end{equation}\n\twhere $g_i(x)=\\sigma (W_i x+b_i)$. And K-Lipschitz continuity for $f(x)$ is:\n\t\\begin{equation}\n\t\t||f(x_1)-f(x_2)||\\leq \\mathrm{K}||x_1-x_2||,\n\t\t\\label{EQ:eqn17}\n\t\\end{equation}\n\twhere K is Lipschitz constant of the function $f$. Due to the consistency of Lipschitz $||h\\circ g||_{Lip}\\leq ||h||_{Lip}\\cdot||g||_{Lip}$, $g_i$ needs to satisfy the C-Lipschitz continuity ($\\mathrm{C}=\\sqrt[N]{\\mathrm{K}}$) so that $f$ satisfies the K-Lipschitz continuity:\n\t\\begin{equation}\n\t\t||g_i(x_1)-g_i(x_2)||\\leq \\mathrm{C}||x_1-x_2||,\n\t\\end{equation}\n\t\\begin{equation}\n\t\t||\\sigma(Wx_1+b)-\\sigma(Wx_2+b)||\\leq \\mathrm{C}||x_1-x_2||.\n\t\t\\label{eq:23}\n\t\\end{equation}\n\tWhen $x_1\\rightarrow x_2$, the Taylor expansion of Eq (\\ref{eq:23}):\n\t\\begin{equation}\n\t\t||\\frac{\\partial\\sigma}{\\partial x} W(x_1-x_2)||\\leq \\mathrm{C}||x_1-x_2||.\n\t\\end{equation}\n\tNormally, $\\sigma$ is a function with limited derivatives such as Sigmoid, so the $\\mathrm{C'}$-Lipschitz continuity is be written as:\n\t\\begin{equation}\n\t\t|| W(x_1-x_2)||\\leq \\mathrm{C'}||x_1-x_2||,\n\t\\end{equation}\n\twhere $\\mathrm{C'}$ is a limited constant, which is determined by $\\frac{\\partial\\sigma}{\\partial x}$ and $\\mathrm{C}$.\n\tSimilarly, the spectral norm of matrix is defined by:\n\t\\begin{equation}\n\t\t||W||_2=\\mathop{\\max}\\limits_{x\\not=0}\\frac{||Wx||}{||x||}.\n\t\\end{equation}\n\tIn this context, the spectral norm $||W||_2$ can be used to represent the Lipschitz constant $\\mathrm{C'}$. The Lipschitz continuity is achieved by normalizing the spectral norm of the weight, approximately.\t\n\\end{document}\n\\endinput", "cites": [97, 96], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical explanation of the mathematical relationship between spectral norm and Lipschitz continuity in the context of GANs. While it references Papers 1 and 2 (DenseNet and ResNet) to point out limitations in the general definition of neural networks, it does not deeply synthesize insights across these works or others. Critical analysis is limited, and abstraction is moderate, as it generalizes the concept of Lipschitz continuity but does not place it within a broader theoretical or comparative context."}}
