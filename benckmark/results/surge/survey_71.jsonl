{"id": "b8456ffc-88d1-4dce-9af7-5cb66e55fd57", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "321e790e-018a-41e3-8473-e7e9883a1392", "prefix_titles": [["title", "Context Dependent Semantic Parsing: A Survey"], ["section", "Introduction"]], "content": "\\blfootnote{\n    \\hspace{-0.65cm}  \n    This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: \\url{http://creativecommons.org/licenses/by/4.0/}.\n}\nSemantic parsing is concerned with mapping natural language (NL) utterances into machine-readable structured \\textit{meaning representations} (\\MRs). These representations are in the formats of formal languages, e.g. Prolog, SQL, and Python. A formal language is typically defined by means of a formal \\textit{grammar}, which consists of a set of rules. Following the convention of the chosen formal language, \\MRs are also referred to as logical forms or programs. An \\MR is often executable in a (programming) environment to yield a result (e.g. results of SQL queries) enabling automated reasoning~.\nMost research work on semantic parsing treats each NL utterance as an \\emph{independent} input, ignoring the text surrounding them~, such as interaction histories in dialogues. \nThe surrounding text varies significantly across different application scenarios. In a piece of free text, we refer to the surrounding text of a current utterance as its \\textit{context}. The context is different with respect to different utterances. In our sequel, we differentiate between context \\textit{independent} semantic parsing (\\CiSP) and context \\textit{dependent} semantic parsing (\\CdSP) by whether a corresponding parser utilizes context information. A knowledge base or a database (on which a \\MR is executed for the purpose of question answering) can be considered as context as well~.\nThis type of context does not change with respect to the utterances. In this survey, we only consider the former kind of context which does vary with different utterances.\n\\input{tab-example-pets.tex}\nThe utilization of context in semantic parsing imposes both challenges and opportunities. \nAs shown in Table \\ref{tab:CoSQL}, one challenge is to resolve references, such as \\textit{those} in ``For each of those, what is the maximum age''. This example shows also another challenge caused by elliptical (incomplete) utterances. The sentence ``What about the average age?'' alone misses information about the database table and the column \\textit{pettype}. The incomplete meaning needs to be complemented by the discourse context. \nCompared with \\CiSP, which usually assumes that the information within the utterance is complete, \\CdSP is expected to tackle challenges posed by involving context in the parsing process  \n. In addition, tackling the above challenges provides us with more opportunities to inspect the linguistic phenomena which could influence semantic parsing. Our survey on \\CdSP fills the gap in the literature, as the recent surveys in the semantic parsing research mainly focus on \\CiSP~. \nThis paper is organised as follows. We start with providing a brief and fundamental understanding of  \\CiSP in \\S2. We then present a comprehensive organization of the recent advances in \\CdSP in \\S3. We discuss current \\CiSP tasks, datasets, and resources in \\S4. Finally, we cover  open research problems in \\S5, and conclude by providing a roadmap for future research in this area.", "cites": [6899, 6900, 6898, 6897, 6896], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The introduction provides a clear definition of context-dependent semantic parsing (CdSP) and contrasts it with context-independent parsing (CiSP), integrating ideas from multiple cited works. It identifies key challenges such as reference resolution and elliptical utterances, drawing from examples in the literature. However, the critical analysis is limited, and while some broader patterns are noted (e.g., the importance of discourse context), the section does not offer a novel or highly abstracted framework for understanding the field."}}
{"id": "a541cf51-c35e-4bd7-b0ec-f7b7038dbad0", "title": "Background", "level": "section", "subsections": ["d1388aa1-f859-436a-97f9-1580b1fa7dad", "0accbdad-eb7e-426a-a0ca-4f0b3266d1fb"], "parent_id": "321e790e-018a-41e3-8473-e7e9883a1392", "prefix_titles": [["title", "Context Dependent Semantic Parsing: A Survey"], ["section", "Background"]], "content": "\\CiSP aims to learn a mapping $\\pi_{\\theta} : \\mathcal{X} \\rightarrow \\mathcal{Y}$, which translates an NL utterance $x \\in \\mathcal{X}$ into an \\MR $y \\in \\mathcal{Y}$. An \\MR $y$ can be executed in a programming environment (e.g. databases, knowledge graphs, etc.) to yield a result $z$, namely denotation. The structure of an \\MR takes a form of either a tree or graph, depending on its underlying formal language. The languages of \\MRs are categorized into three types of formalism\n: logic based (e.g. first order logic), graph based (e.g. AMR~), and programming languages (e.g. Java, Python)~. Some semantic parsers explicitly apply a production grammar to yield \\MRs from utterances. Such a grammar consists of a set of production rules, which define a list of candidate derivations for each NL utterance. Each derivation deterministically produces a grammatically valid \\MR.", "cites": [6900], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of semantic parsing, including the mapping function, meaning representations (MRs), and their execution. It integrates a single cited paper minimally and does not connect ideas across multiple sources. There is no critical evaluation of the cited work or broader analysis of trends, and the abstraction remains limited to surface-level generalizations."}}
{"id": "d1388aa1-f859-436a-97f9-1580b1fa7dad", "title": "Semantic Parsing Models", "level": "subsection", "subsections": ["e551a263-fa5f-4543-97d2-4b3bf2aa80fe"], "parent_id": "a541cf51-c35e-4bd7-b0ec-f7b7038dbad0", "prefix_titles": [["title", "Context Dependent Semantic Parsing: A Survey"], ["section", "Background"], ["subsection", "Semantic Parsing Models"]], "content": "Given an utterance $x \\in \\mathcal{X}$ and its paired \\MR $y \\in \\mathcal{Y}$, a \\CiSP model can form a \\textit{conditional} distribution $p(y | x)$.\nThe model learning can be supervised by either utterance-\\MR pairs or merely utterance-denotation pairs. If only denotations are available, a widely used approach~ is to  marginalize over all possible \\MRs for a denotation $z$, which leads to a \\textit{marginal} distribution $p(z | x) = \\sum_{y} p(z, y| x)$. A parsing algorithm aims to find the optimal \\MR in the combinatorially large search space.\nWe coarsely categorize the existing models into: symbolic approaches, neural approaches, and neural-symbolic approaches based on the category of machine learning methodology and whether any production grammars are explicitly used in models.", "cites": [6900], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of semantic parsing models and introduces the distinction between conditional and marginal distributions. It cites a general survey on semantic parsing but does not synthesize or integrate its content meaningfully. There is minimal critical evaluation of the cited work or broader trends, and abstraction remains at a superficial level without uncovering deeper patterns or principles."}}
{"id": "e551a263-fa5f-4543-97d2-4b3bf2aa80fe", "title": "Symbolic Approaches", "level": "paragraph", "subsections": ["b4beea59-21ae-4c9c-befd-dc1675e2c2f8", "f6a9856b-4e78-4007-961d-d1003c3a47c0"], "parent_id": "d1388aa1-f859-436a-97f9-1580b1fa7dad", "prefix_titles": [["title", "Context Dependent Semantic Parsing: A Survey"], ["section", "Background"], ["subsection", "Semantic Parsing Models"], ["paragraph", "Symbolic Approaches"]], "content": "\\label{sec:pipe}\nA symbolic semantic parser employs production grammars to generate candidate derivations and find the most probable one via a scoring model. The scoring model is a statistical or machine learning model. \n Each derivation is represented by handcrafted features extracted from utterances or partial \\MRs. Let $\\Phi(x, d)$ denote the features of a pair of utterance and derivation, and $G(x)$ be the set of candidate derivations based on $x$. A widely used scoring model is the log linear model~. \n\\begin{equation}\n    p( d | x) = \\frac{\\exp(\\bm{\\theta}\\Phi(x, d))}{\\sum_{d' \\in G(x)} \\exp(\\bm{\\theta}\\Phi(x, d'))}\n\\end{equation}\nwhere $\\bm{\\theta}$ denotes the model parameters. If only utterance-denotation pairs are provided at training time, a model marginalizes over all possible derivations yielding the same denotations by $p(z | x) = \\sum_{d} p(z, d| x)$~. Those corresponding parsers further differentiate between graph-based parsers~ and shift-reduce parsers~ due to the adopted parsing algorithms and the ways to generate derivations. From a machine learning perspective, these approaches are also linked to a structured prediction problem.", "cites": [6900, 6897, 6901], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of symbolic semantic parsing approaches and introduces the log-linear model, but it does not synthesize or connect ideas from the cited papers into a broader narrative. There is minimal critical evaluation or comparison of methods, and abstraction is limited to basic structured prediction terminology without identifying deeper principles or trends."}}
{"id": "b4beea59-21ae-4c9c-befd-dc1675e2c2f8", "title": "Neural Approaches", "level": "paragraph", "subsections": [], "parent_id": "e551a263-fa5f-4543-97d2-4b3bf2aa80fe", "prefix_titles": [["title", "Context Dependent Semantic Parsing: A Survey"], ["section", "Background"], ["subsection", "Semantic Parsing Models"], ["paragraph", "Symbolic Approaches"], ["paragraph", "Neural Approaches"]], "content": "\\label{sec:end}\nNeural approaches apply neural networks to translate NL utterances into \\MRs without using production grammars. These approaches formulate semantic parsing as a machine translation problem by viewing NL as the source language and the formal language of \\MRs as the target language. \nMost work in this category adopts \\Seq  as the backbone architecture, which consists of an encoder and a decoder. The encoder projects NL utterances into hidden representations, whereas the decoder generates linearized \\MRs sequentially.\n Both encoders and decoders employ either recurrent neural networks (RNN)~ or Transformers . Note that, these methods do not apply any production grammars to filter out syntactically invalid \\MRs.\nThe variants of the \\Seq based models also explore structural information of \\MRs. \\textsc{Seq2Tree}  utilizes a tree-structured RNN as the decoder, which constrains generated \\MRs to take syntactically valid tree structures. The \\textsc{Coarse2Fine} model~ adopts a two-stage generation for the task. In the first stage, a \\Seq model is applied to generate \\MR templates, which replace entities in \\MRs by slot variables for a high-level generalization. In the second stage, another \\Seq model is applied to fill the slot variables with the corresponding entities.", "cites": [166, 2401, 6902, 3605, 38], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic overview of neural approaches to semantic parsing, summarizing key architectural components and citing relevant papers. It integrates some ideas (e.g., sequence-to-sequence, Coarse-to-Fine) but lacks deeper synthesis or a unifying framework. There is minimal critical analysis or abstraction beyond the described models."}}
{"id": "f6a9856b-4e78-4007-961d-d1003c3a47c0", "title": "Neural-Symbolic Approaches", "level": "paragraph", "subsections": [], "parent_id": "e551a263-fa5f-4543-97d2-4b3bf2aa80fe", "prefix_titles": [["title", "Context Dependent Semantic Parsing: A Survey"], ["section", "Background"], ["subsection", "Semantic Parsing Models"], ["paragraph", "Symbolic Approaches"], ["paragraph", "Neural-Symbolic Approaches"]], "content": "In order to ensure the generated \\MRs to be syntactically valid without compromising the generalization power of neural networks, neural-symbolic approaches fuse both symbolic and neural approaches by applying production grammars to the generated \\MRs; then the derivations are scored by neural networks. \nThe majority of these methods linearize derivations such that they are able to leverage \\Seq~. At each time step, the decoder of these methods emits either a parse action or a production rule, leading to a grammatically valid \\MR at the end. these works produce derivations by varying grammars. \\textsc{NSM}  uses a subset of Lisp syntax. \\textsc{TranX}~ defines the grammars in Abstract Syntax Description Language, while \\textsc{IRNet}  considers the context-free grammar of a language called SemQL. \nThere are also neural-symbolic approaches adopting neural architectures other than \\Seq. One of such examples is , which adopts a dynamic neural module network (DNMN) to generate \\MRs.", "cites": [6904, 9083, 4279, 6903], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of neural-symbolic approaches, mentioning how they combine symbolic grammars with neural networks. It lists several methods and their grammar-based systems but lacks deeper synthesis, comparison, or critique of their strengths and weaknesses. The content remains at a surface level without abstracting broader patterns or principles."}}
{"id": "0888dee2-d5ac-4bd8-807b-f7439f676fed", "title": "Context Dependent Semantic Parsing", "level": "section", "subsections": ["a7325d38-7cf4-41b3-bf34-266fb7445dcc", "4f2c4e8f-03a9-4d2a-875a-0025d4eaf479", "a42c11dd-cba0-4190-ab4f-e20c9525e938", "640e3315-aedb-4aa7-abfb-05ad7be7ec1b", "6d868391-5ac7-479e-9463-c4e87430bb7a"], "parent_id": "321e790e-018a-41e3-8473-e7e9883a1392", "prefix_titles": [["title", "Context Dependent Semantic Parsing: A Survey"], ["section", "Context Dependent Semantic Parsing"]], "content": "Context dependent semantic parsing involves modelling of context in the parsing process. For each current NL utterance, we define its \\textit{context} as the information beyond this utterance. With this definition, there are two types of context for semantic parsing, \\textit{local} context and \\textit{global} context. The \\textit{local} context for an utterance is the text and multimedia content surrounding it, which is meaningful only for this utterance. In plain texts, the concept of local context is also quite close to discourse, which is defined as a group of collocated, structured, coherent sentences~. In contrast, its \\textit{global} context is the information accessible to more than one utterance, including databases and external text corpora, images or class environment~. The content of local context varies for each NL utterance while the global context is always static. The work in our survey is only concerned with local context. Therefore, we always refer to \"local context\" as \"context\" in the following sections.  \nContext provides additional information to resolve ambiguity and vagueness in current utterances. For semantic parsing, one type of ambiguity is caused by references in current utterances, which need to be resolved to previously mentioned objects and relations. References may include explicit or implicit lexical triggers, such as \\textit{those} in \"For each of those, ...\" in our introductory example (Table \\ref{tab:CoSQL}). Another ambiguity illustrated by the same example is resulted by ellipsis. The previous context provides constraints to restrict the scope of possible \\MRs indicated by current utterances. In addition, context provides information to disambiguate word senses and entities, and link them to knowledge bases to enable complex reasoning. However, semantic parsing literature largely neglects word sense disambiguation, which is regarded as an AI complete problem~. Last but not least, context allows to exploit discourse coherence for semantic parsing. Coherence relations characterize structural relationships between sentences, thus limit the search space of parse candidates for the following utterances of current ones.\nFormally, a context dependent parser takes both an input utterance $x_i$ and its context $C_i$, where $C_i$ could include a broad range of multimedia content. And we consider a group of inter-related utterances with the union set of their context as one \\textit{interaction}, $I = (\\mathbf{x}, \\mathbf{C})$, where $\\mathbf{x} = [x_1, ...,x_i,...,x_T]$ and $\\mathbf{C} = \\cup_{i=1}^{T}C_{i}$. Currently, most \\CdSP work focus on the research problems of context $C_i$ regarding the history utterances, \\MRs, denotations. Such a parser learns a mapping from a current utterance $x_{i}$ to an \\MR $y_i$ by $\\pi_{\\theta}(x_{i}, C_i)$.", "cites": [6905, 6906], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of context in semantic parsing, distinguishing between local and global context and explaining their relevance. It synthesizes concepts from the cited papers, such as the role of context in resolving references and ellipsis, and links them to the broader task of semantic parsing. While it introduces a formal framework, the critical analysis is limited and mostly descriptive in tone, with only a brief mention of limitations in the literature (e.g., neglecting WSD as an AI-complete problem)."}}
{"id": "a7325d38-7cf4-41b3-bf34-266fb7445dcc", "title": "Symbolic Approaches", "level": "subsection", "subsections": ["6aaa20f2-9158-4b75-92a8-7292af7a0e72"], "parent_id": "0888dee2-d5ac-4bd8-807b-f7439f676fed", "prefix_titles": [["title", "Context Dependent Semantic Parsing: A Survey"], ["section", "Context Dependent Semantic Parsing"], ["subsection", "Symbolic Approaches"]], "content": "Existing symbolic approaches formulate \\CdSP as a structured prediction problem by including contextual information into their feature models. Their models capture $ p( d_i | x_i, C_i)$ by including context as a condition. Both \\newcite{zettlemoyer2009learning} and \\newcite{srivastava2017parsing} divide the parsing process into two steps: i) generate initial parses using \\CiSP; ii) complete initial parses using contextual information. In contrast, \\newcite{long2016simpler} parses a sequence of utterances in one step. In all those work, symbolic features are used to represent contexts. \nIn two-step approaches, \\newcite{zettlemoyer2009learning} and \\newcite{srivastava2017parsing} differ in the details of individual steps. In the first step, \\newcite{zettlemoyer2009learning} extends \\MRs with predicates representing references, while \\newcite{srivastava2017parsing} generates a set of context independent parses for each utterance. In the second step, \\newcite{zettlemoyer2009learning} collects possible derivations by applying three heuristic rules to replace references with entities in context and extend initial \\LFs with constraints, then finds the best derivations according to a linear model. In~, their model expands the initial parse set with parses selected from context using heuristic rules, then finds the best parses in the expanded set. Their feature model includes a multinomial random variable indicating the current hidden state of discourse.\nThe shift-reduce parser in~ generates derivations for a whole utterance sequence. This method stores the previously generated derivations in a stack, performs a sequence of \\textit{shift} and \\textit{build} operations to generate \\LFs. In its feature model, a context is represented by a sequence of past \\LFs and a random variable denoting the current world state.\nUtterances and \\MRs histories form a context of a \\CdSP parser. The common practice is to extract handcrafted features from both utterances and \\MRs to represent contexts. Some typical feature patterns are as follows:", "cites": [6907], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of symbolic approaches in context-dependent semantic parsing, briefly comparing two-step versus one-step methods and mentioning how different papers incorporate context. However, the synthesis is limited to surface-level connections between the works, and there is little critical evaluation or abstraction to highlight broader trends or principles. It lacks a deeper analytical perspective to offer a novel or comprehensive understanding of the field."}}
{"id": "4f2c4e8f-03a9-4d2a-875a-0025d4eaf479", "title": "Neural Approaches", "level": "subsection", "subsections": ["28692668-fe8c-48f8-82c5-52ec47621cd5"], "parent_id": "0888dee2-d5ac-4bd8-807b-f7439f676fed", "prefix_titles": [["title", "Context Dependent Semantic Parsing: A Survey"], ["section", "Context Dependent Semantic Parsing"], ["subsection", "Neural Approaches"]], "content": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\textwidth]{figs/SQL_example-cropped.pdf}\n    \\label{fig:semantic_tree}\n    \\caption{Coreference resolution architecture of . Considering the example in Table \\ref{tab:CoSQL}, \\newcite{chen2019context} firstly generates a \\MR template for $Q_2$ as \"SELECT max(petage), \\textit{REF} FROM pets GROUP BY \\textit{REF}\". The \\textit{REF} tokens would then be replaced with the \"pettype\" from the precedent \\MR. }\n    \\label{fig:sql_example}\n\\end{figure}\nExisting neural \\CdSP methods extend the \\Seq architecture to incorporate contextual information in two ways. The first approach is to build context-aware encoders to encode historical utterances or \\MRs into neural representations, which provide decoders contextual information to resolve ambiguity in current utterances. As previously predicted \\MRs provide the constraints and information missed in current utterances, the second approach is to utilize context-aware decoders to reuse or revise those predicted \\MRs for generating current \\MRs.", "cites": [6908], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of neural approaches to context-dependent semantic parsing and mentions two general strategies: context-aware encoders and decoders. It references one paper but does not synthesize ideas across multiple works, nor does it offer critical evaluation or identify broader patterns or principles in the field."}}
{"id": "28692668-fe8c-48f8-82c5-52ec47621cd5", "title": "Context-aware Encoders", "level": "paragraph", "subsections": ["8186c75f-b8c3-4431-b509-a482cbf52d9e"], "parent_id": "4f2c4e8f-03a9-4d2a-875a-0025d4eaf479", "prefix_titles": [["title", "Context Dependent Semantic Parsing: A Survey"], ["section", "Context Dependent Semantic Parsing"], ["subsection", "Neural Approaches"], ["paragraph", "Context-aware Encoders"]], "content": "Encoders of \\CdSP methods differentiate between utterance encoders and \\MR encoders. Utterance encoders construct neural representations for both current and historical utterances, while \\MR encoders build neural representations based on on historical \\MRs.\nUtterance encoders aim to embed rich information hidden in utterances into fixed-length representations, which provide contextual information in addition to current utterances for decoders. They apply first an RNN to map each utterance into a continuous vector of fixed-size. Then there are three ways to encode utterances in context into a fixed-size neural representation.\n\\begin{itemize}\n    \\item \n    For each utterance in a dialogue, a straightforward method is to concatenate its previous $k - 1$ utterances with current utterance in order and encode them with the RNN~.\n    As a result, decoders have access to information in at most $k$ utterances. However, this method fails to access information beyond the $k$ utterances. In addition, it is computationally expensive because if an utterance belongs to multiple contexts, it would be repeatedly encoded for modelling all the contexts.\n    \\item To overcome the above weakness, an alternative method is to treat a sub-sequence of utterances up to time $t$ as a sequence of vectors, and project them into a \\textit{discourse state} vector by using a turn-level RNN~. In another word, those models apply hierarchical RNNs to map each context into a fixed-size vector. In this method, each utterance is encoded only once and reused for modelling different contexts. However, this approach often leads to significant information loss~ due to the challenges imposed by encoding sequences of utterances into single vectors. \n    \\item In order to focus on history utterances most relevant to current decoder states or utterances, soft attention~ is applied to construct context vectors. The query vectors are either the hidden state of an decoder~ or an utterance vector~. To differentiate between positional information, token embeddings of history utterance are concatenated with their position embeddings~, which encode the positions of history utterances relative to the current utterances. This method reflects the observation that similar utterances tend to share relevant information, such as references of the same entities. Both discourse states and attended representations are also widely used by the neural dialogue models~, thus suffer from the same problems caused by composition complexity. As a result, the trained models are found insensitive to utterance order and word order in context~.\n\\end{itemize}\n\\MR encoders construct a neural context representation at time $t$ based on the \\MRs predicted before $t$. As \\MRs are expressed in a formal language, \\MR encoders also apply RNNs to encode each \\MR or segments of \\MRs into embedding vectors. Then \\MR encoders build context representations of historical \\MRs in the same spirit as utterance encoders. In~, they only concatenate the embeddings of $k$ most recent history \\MR tokens as they assume current \\MR is always an extension of previous \\MRs. In~, a bidirectional RNN is applied to construct a vector for each segment, which is extracted from historical \\MRs. Soft attention is also applied in~ for building context vectors, which uses the current hidden state of their decoder as the query vector to attend over the token embeddings of the previous \\MR.", "cites": [6899, 6898, 6909, 1888, 6911, 6910, 6896, 168, 1883], "cite_extract_rate": 0.8181818181818182, "origin_cites_number": 11, "insight_result": {"error": "Failed to parse LLM response", "raw_response": "{\n    \"type\": \"analytical\",\n    \"scores\": {\"synthesis\": 3.5, \"critical\": 3.5, \"abstraction\": 3.0},\n    \"insight_level\": \"medium\",\n    \"analysis\": \"The section provides an analytical overview of context-aware encoders by categorizing them into three main approaches and highlighting their strengths and limitations. It synthesizes ideas from multiple papers to explain how different encoding strategies handle dialogue history and \\MRs. While it identifies key issues such as information loss and sens"}}
{"id": "8186c75f-b8c3-4431-b509-a482cbf52d9e", "title": "Context-aware Decoders", "level": "paragraph", "subsections": [], "parent_id": "28692668-fe8c-48f8-82c5-52ec47621cd5", "prefix_titles": [["title", "Context Dependent Semantic Parsing: A Survey"], ["section", "Context Dependent Semantic Parsing"], ["subsection", "Neural Approaches"], ["paragraph", "Context-aware Encoders"], ["paragraph", "Context-aware Decoders"]], "content": "Decoders in \\CdSP models produce \\MRs based on the neural representations provided by their encoders. Such a decoder yields an \\MR by generating a sequence of \\MR tokens according to model distribution $P(\\mathbf{y}| \\mathbf{x}, \\mathbf{C})$, where $\\mathbf{C}$ denotes context information. There are three major ways to utilize context information.\nOne key problem of \\CdSP is incomplete information in current utterances. The straightforward way is to take neural context representations $\\mathbf{C}$ as additional input of decoders, which are yielded by context-aware encoders. Those context representations contains information from previous utterances, historical \\MRs, or both. The decoders take them as input by concatenating them with the ones from current utterances at each decoding step~. Thus, the quality of decoding depends tightly on the quality of contextual encoding, which is still a challenging problem~.\n\\MRs of current utterances often contain segments from previous \\MRs~. The shared parts are references to previously mentioned entities or constraints implied by context. Reuse of \\MR segments is realized by a designated \\textit{copy} component, which selects a segment to copy when the probability of copying is high. As decoders in \\Seq produce a sequence of decisions for each input, the corresponding model generates a sequence of mixed decisions, including both \\textit{copy} of segments and generation of new \\MR tokens. In a similar manner,  copying of \\MR tokens from previous \\MR is proposed in~.\nCoreference resolution is explicitly addressed in~\\newcite{chen2019context}. As illustrated by the example in Figure \\ref{fig:sql_example}, a special token \\textit{REF} is introduced in the output vocabulary for denoting if an entity in the preceding \\MR is referred in that utterance. If that is the case, the corresponding entity token is copied from the previous \\MR to replace the \\textit{REF} token via a pointer network module~.", "cites": [6899, 6898, 167, 1888, 6908, 6896, 6912], "cite_extract_rate": 1.0, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes ideas from multiple papers by grouping approaches into a coherent framework of context-aware decoding strategies, such as segment reuse, pointer-based copying, and reference tokens. It provides a critical discussion of the challenges, like the dependency on contextual encoding and incomplete information. While it identifies patterns (e.g., reuse of MR segments), it stops short of offering a fully meta-level or novel conceptual framework."}}
{"id": "a42c11dd-cba0-4190-ab4f-e20c9525e938", "title": "Neural-Symbolic Approaches", "level": "subsection", "subsections": [], "parent_id": "0888dee2-d5ac-4bd8-807b-f7439f676fed", "prefix_titles": [["title", "Context Dependent Semantic Parsing: A Survey"], ["section", "Context Dependent Semantic Parsing"], ["subsection", "Neural-Symbolic Approaches"]], "content": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\textwidth]{figs/copy_action.pdf}\n    \\caption{The symbolic memory architecture of . Considering the example in Table \\ref{tab:CoSQL}, \\newcite{guo2018dialog} defines different types of actions, $A_{ac}$ and $A_{ec}$, to copy action sequence $A_{10}, A_{12}, A_{13}$ and the entity \\textit{pettype} from the symbolic memory, respectively.}\n    \\label{fig:copy_action}\n\\end{figure}\nNeural-symbolic approaches introduce grammar into the decoding process or utilize symbolic representations as intermediate representations, while applying neural nets for representation learning. They take advantages from both the good context representation obtained by neural nets and reduced complexity of decoding due to the constraints introduced by grammars. In existing work, those approaches regard the generation of an \\MR as the prediction of a sequence of actions. Neural-symbolic methods normally take the same methods as the neural approaches to encode the contextual information. What differentiate them is the neural-symbolic could handle context by i) designing specific actions, and ii) utilizing symbolic context representations.\nThe context specific actions proposed in~ adopt \\textit{copy} mechanism to reuse the previous \\MRs. \\textsc{CAMP}~ include three actions to copy three different SQL clauses from precedent queries. \n\\newcite{liu2020FarAwayContextModelingSP} allows copying of any actions or subtrees from precedent SQL queries. The \\textit{subsequent} action in~ adds SQL conditions from the previous query into the current semantic parse to address the ellipsis problem. Different from other approaches, \\newcite{iyyer2017search} uses a \\textsc{DynSP}, which is in a similar neural network structure as the DNMN, instead of the \\Seq to generate the action sequences.\nProduction rules are also used to explicitly address the coreference resolution. In~, the authors defined fours actions to instantiate the entities, predicates, types and numbers. Then the pointer network is utilized to find mentions of the four entry semantic categories in the current and history utterances. The entities in utterances are later mapped to entities in knowledge bases by using their entity linking tool.\nInstead of directly copying from previous \\MRs, the parser \\textsc{Dialog2Action}~ incorporates a dialogue memory, which maintains \\textit{symbolic} representations of entities, predicates and action subsequences from an interaction history (Figure \\ref{fig:copy_action}). That parser defines three types of designated actions to copy entities, predicates and action subsequences from the memory respectively. Instead of decisively copying from memory, each type of action probabilistically selects the corresponding segments conditioning on the \\textit{symbolic} representations, which are later integrated into the generated action sequences. \n\\newcite{guo2019coupling} employs the same neural-symbolic models as in  to capture contextual information. Different from other approaches, \\newcite{guo2019coupling} adopts the meta-learning approach to improve the generation ability of \\CdSP models. Inspired by , \\newcite{guo2019coupling} utilize the context from other interactions to guide the learning of \\CdSP over utterances within current interactions via MAML. \\newcite{guo2019coupling} considers an input utterance $x_i$ and its context $C_i$ as an instance. A context-aware retriever would retrieve instances, which are semantically close to the current instances, from other interactions. When learning model parameters, the retrieved instances and the current instances are considered as the support set and test set, respectively, and grouped as tasks as in the common MAML paradigm.", "cites": [6898, 6912, 6913, 7744], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several neural-symbolic approaches effectively, highlighting commonalities such as the use of grammar for decoding and context modeling through copy actions or symbolic memory. It provides a critical perspective by contrasting different strategies (e.g., direct copying vs. probabilistic selection) and pointing out issues like error propagation in sequential methods. While some general patterns are identified (e.g., the role of symbolic representations), the analysis remains focused on methodological descriptions rather than deeper meta-level insights."}}
{"id": "640e3315-aedb-4aa7-abfb-05ad7be7ec1b", "title": "Comparison between Different \\CdSP Approaches", "level": "subsection", "subsections": [], "parent_id": "0888dee2-d5ac-4bd8-807b-f7439f676fed", "prefix_titles": [["title", "Context Dependent Semantic Parsing: A Survey"], ["section", "Context Dependent Semantic Parsing"], ["subsection", "Comparison between Different \\CdSP Approaches"]], "content": "In~, 13 different context modeling methods for both neural and neural-symbolic \\CdSP parsers were evaluated on two benchmark datasets. None of those methods achieve consistent superior results over the others in all experimental settings. Among them, concatenation of $k$ recent utterances for decoders and copy of parse actions from precedent \\MRs are the top performing ones in most settings. ~\\newcite{liu2020FarAwayContextModelingSP} defines 12 fined-grained types summarized with multiple hierarchies according to the contextual linguistic phenomena, and inspects how different linguistic phenomena influence the model behavior. One interesting conclusion is that the methods in their experiments all perform poorly on the instances involving coreference problems that require complex inference. But note that, those methods in that study were not compared with the ones with explicit coreference resolution. Another interesting finding is that all the models perform better on the utterances which only augment the semantics of previous sentences than on the utterances which substitute the partial semantics of the precedent utterances.", "cites": [6898], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a comparative overview of different context-dependent semantic parsing approaches based on one key study, highlighting top-performing methods and findings. It integrates the study's results into a coherent narrative but is limited in synthesis due to reliance on a single paper. Some critical observations are made, such as performance issues with coreference problems and the lack of comparison with coreference-aware models, but deeper critique or broader synthesis is lacking. The abstraction level is moderate, identifying patterns like the difficulty of substitution-based semantics but not offering high-level meta-insights."}}
{"id": "6d868391-5ac7-479e-9463-c4e87430bb7a", "title": "Comparison between \\CdSP and Feedback Semantic Parsing", "level": "subsection", "subsections": [], "parent_id": "0888dee2-d5ac-4bd8-807b-f7439f676fed", "prefix_titles": [["title", "Context Dependent Semantic Parsing: A Survey"], ["section", "Context Dependent Semantic Parsing"], ["subsection", "Comparison between \\CdSP and Feedback Semantic Parsing"]], "content": "Feedback/Interactive Semantic Parsing is another line of research in semantic parsing that utilizes context to refine \\MRs in an iterative manner. Most Feedback Semantic Parsing systems~ start with using an \\CiSP parser to parse a given utterance into an initial \\MR. Then the \\MR is interpreted in natural language and sent to a user. The user provides feedback, based on which the systems revise the initial parse. The process repeats till convergence. Therefore, in Feedback Semantic Parsing, interaction histories are only used to revise the parses. In contrast, \\CdSP focuses on modelling the dependencies between the utterances. \\newcite{elgohary2020speak} empirically compares \\CdSP with Feedback Semantic Parsing. They train a \\CdSP model, EditSQL~, on two \\CdSP datasets, \\Sparc and \\Cosql, and evaluate it on the test set of a feedback semantic parsing dataset, \\SPLASH. The performance is merely 3.4\\% and 3.2\\% in terms of accuracy, indicating that the two tasks are distinct by addressing different aspects of context.", "cites": [6899, 6914, 3063, 6916, 6915], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear comparison between Context Dependent Semantic Parsing (CdSP) and Feedback Semantic Parsing by referencing multiple relevant works and identifying their distinct approaches to handling context. It synthesizes these ideas to some extent, particularly by contrasting the use of interaction history in feedback systems versus utterance dependencies in CdSP. However, the critical depth is limited to a single empirical result (low accuracy on SPLASH) without deeper evaluation of the underlying assumptions or broader implications, and the abstraction remains at a relatively surface level, focusing more on task distinctions than overarching principles."}}
{"id": "1b5a8458-fd5b-4876-ba50-10c54b23220a", "title": "Single-party Scenarios", "level": "paragraph", "subsections": ["164c4a3e-7159-47d0-82af-d206060db598"], "parent_id": "f2b8d82a-6953-4c79-b18d-ac672c464017", "prefix_titles": [["title", "Context Dependent Semantic Parsing: A Survey"], ["section", "Datasets and Resources"], ["subsection", "Scenarios"], ["paragraph", "Single-party Scenarios"]], "content": "In \\Sparc~, \\Sqa~ and \\Atis, the user utterances within each interaction are around a topic described by the provided text. To collect \\Sparc and \\Sqa, crowd-workers are asked to raise questions to obtain the information that answers the questions sampled from other corpora~. But the assumption for \\Sqa is the answers of the current question must be the subset of answers from the last turn. In \\Atis~, crowd-workers raise questions around the detailed scripts describing air travel planning scenarios. \n\\TemporalStructure~ and \\TimeExp~ particularly focused on addressing the temporal-related dependency. In \\TemporalStructure, human users or the simulators raise natural language questions chronologically towards a knowledge base. The facts in the knowledge base are organized in time series. Therefore, the questions in \\TemporalStructure are rich with time expressions. \\TimeExp only annotate temporal mentions (text segments that describe time expressions) instead of complete questions. All the mentions are from the time expression-rich corpora.\n\\Scone~ and \\CSQA~ use semi-automatic approaches to simulate the contextual dependency. Each interaction in \\Scone is merely labeled with an initial denotation and an end denotation. The denotations in \\Scone are regarded as the states that can be manipulated by the programs. Within each interaction, multiple candidate sequences of programs would be automatically generated while only the sequence of programs, which could correctly transit the initial state to the end state, would be kept and described with natural language by the crowd-workers. To create \\CSQA  dataset, the crowd-workers are asked to raise questions that can be answered from single fact tuples (e.g. relation: \\textit{CEO}, subject: \\textit{Google}, object: \\textit{Sundar Pichai}) in the knowledge graph or the complex facts which are the composition of multiple tuples. To create coherent dependency among questions, the questions that share the relations or entities are placed next to each other. And crowd-workers would manually modify the questions such that the sequence of questions would include contextual linguistic properties such as ambiguity, underspecification or coreference. It is worth mentioning that, with such method, \\CSQA includes the largest number of interactions until now, which is over 200k.", "cites": [3063, 8303, 6908, 6907], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual description of several datasets and their collection methods, integrating basic information from multiple cited papers. However, it lacks deeper synthesis, critical evaluation of the approaches, or abstraction to broader trends. The narrative is coherent but primarily descriptive in nature."}}
{"id": "e8038842-2355-4479-9735-294e34339301", "title": "Causal Structure Discovery in Context", "level": "paragraph", "subsections": [], "parent_id": "73fd17e5-1eb7-4596-820a-f4fb6a6042a2", "prefix_titles": [["title", "Context Dependent Semantic Parsing: A Survey"], ["section", "Challenges and Future Directions"], ["paragraph", "Analysis of Linguistic Phenomena Benefiting from Context"], ["paragraph", "Incorporating Far-side Pragmatics"], ["paragraph", "Causal Structure Discovery in Context"]], "content": "A key challenge of context based modelling is composition complexity caused by highly varying context. The empirical results in~ show that the SOTA models can capture well nearby context information but it is still challenging to capture long-range dependencies in context. One possible direction is to find out the underlying causal structure~, which should be sparse and explains well which contextual information leads to current utterances. If we can focus only on the key reasons in context that lead to changes of \\MRs, the influence from noisy information and overfitting of models is expected to decrease significantly. Another potential benefit of understanding causal structures in context is to improve robustness of parsers by ignoring non-robust features~.", "cites": [969, 6898, 3699], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview by identifying the challenge of long-range dependencies in context and proposing causal structure discovery as a potential solution, drawing on the idea of non-robust features from adversarial examples. While it connects ideas from multiple papers, the synthesis is somewhat limited due to missing direct references and deeper integration. It offers a moderate level of abstraction by linking context modeling to robustness and noise reduction but lacks a critical evaluation of the cited works' limitations or trade-offs."}}
{"id": "f4cff5dd-1eff-4533-adfc-1fcd6b740b89", "title": "Low-resource \\CdSP", "level": "paragraph", "subsections": [], "parent_id": "73fd17e5-1eb7-4596-820a-f4fb6a6042a2", "prefix_titles": [["title", "Context Dependent Semantic Parsing: A Survey"], ["section", "Challenges and Future Directions"], ["paragraph", "Analysis of Linguistic Phenomena Benefiting from Context"], ["paragraph", "Incorporating Far-side Pragmatics"], ["paragraph", "Low-resource \\CdSP"]], "content": "Since most \\CdSP datasets are small in terms of the number of utterances and interactions, the direction on addressing the low-resource problem in \\CdSP is quite promising. The meta-learning approaches, such as the MAML \\CdSP in , could be a potential direction to address this issue. The other typical methods to solve low-resource issues, including weakly supervision, data augmentation, semi-supervised learning, self-supervised learning etc., could be further investigated in the scenarios of \\CdSP.\n\\section*{Acknowledgements}\nWe thank Xuanli He and anonymous reviewers for their useful suggestions. \n\\bibliographystyle{coling}\n\\bibliography{coling2020}\n\\end{document}", "cites": [6917], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"error": "Failed to parse LLM response", "raw_response": "{\n    \"type\": \"analytical\",\n    \"scores\": {\"synthesis\": 2.5, \"critical\": 2.0, \"abstraction\": 3.0},\n    \"insight_level\": \"medium\",\n    \"analysis\": \"The section briefly mentions a meta-learning approach from one cited paper and outlines other general methods for low-resource settings. It provides some level of synthesis by connecting these methods to the \\CdSP context, but does not deeply integrate or compare multiple sources. There is minimal critical evaluation of the cited work or alternative a"}}
