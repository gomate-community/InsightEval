{"id": "1d5b201f-602c-4c65-b271-431cd35cb238", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "ac2e27ee-a8f3-4a4c-b292-dda9a70aafc1", "prefix_titles": [["title", "Survey of Attacks and Defenses \\\\on Edge-Deployed Neural Networks"], ["section", "Introduction"]], "content": "Since the rise of deep learning in the last decade, many different libraries and frameworks for running and training deep neural networks  \n(DNN) have been published and open-sourced. In that time, the landscape of software tools for training neural networks has moved from \ndifficult-to-install libraries~, and support for static graphs only~, to industry-ready, easy-to-deploy \nframeworks~, high-development efficiency~, and support for dynamic graphs and \\textit{just-in-time} compilation. \nRecently, as tools have gained maturity, more businesses have started using neural networks in production and exposing services\nbased on neural networks~. Deploying neural networks is non-trivial, and the research frameworks proved insufficient \nto handle high-bandwidth, low-latency inference, leading to the development of production-ready frameworks such as Tensorflow \nServing~ and the standardization of neural network formats with ONNX~. With an increasing number of \nmobile devices and PCs possessing GPUs and custom ASICs, networks have been pushed to smartphones~ and even \nGPU-enabled JavaScript~. With the rise of voice assistants, wearables, and smart cameras, \nthe need for low-power inference has led to the development of many custom DNN acceleration ASICs~. \nThere are many reasons why businesses or users may want to run neural networks on edge devices, \nas an alternative to sending the data to datacenters for processing, including: \n\\noindent \\textbf{Privacy:} users may not want or may not be able to send the data to the cloud for privacy or legal reasons. \nFor example, a hospital may want to process patient data on servers at a different location, but is not willing to risk patient \nprivacy. Even if the patient data is encrypted, if the server is malicious, the patient data may be at risk. \n\\noindent \\textbf{Power}: sending data directly to the cloud may not be the most power-efficient approach to run neural networks.\nFor example, in~, the authors show that in convolutional neural networks (CNN), processing a few of the first \nconvolutional layers before sending the data to the cloud achieves higher power savings compared to processing the whole \nnetwork on the device or sending the input data to the cloud. As more low-power accelerators using approaches such as \nquantization~, stochastic computing~, or \nsparsity~ are released, we expect the ratio between the cost of processing \nnetworks and the cost of transmitting input data to become more significant.\n\\noindent \\textbf{Latency:} many applications have hard latency requirements and must process a network within a certain time limit.\nFurthermore, for certain mission-critical applications with hard availability guarantees, as in the case of \nautonomous drones or self-driving cars, being able to process data on the device is mandatory.\nWhile datacenters are able to provide virtually unlimited computing power, possibly making inference time negligible, the transmit \ntime of inputs over the network often cannot be ignored. Hence, a device must possess the required compute power to process the inputs within the time budget. \n\\noindent \\textbf{Throughput:} several industries dealing with high bandwidth data are faced with the question of whether to store data for offline processing, allowing thorough analysis at the cost of large amounts of storage, or to process the data in-flight\npotentially sacrificing some information, but saving on storage. Take an extreme case: the CERN Large Hadron Collider (LHC) can\ngenerate upwards of hundreds of terabytes of data per second. Storing that data is difficult, so authors of~ propose \nto process the data in-flight using extremely low-latency FPGA designs.", "cites": [7602, 7108, 2672, 2670, 2671, 207], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 18, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of the evolution of DNN frameworks and deployment scenarios, integrating a few cited papers to support these points. It lacks deeper synthesis of ideas across the cited works and does not offer critical evaluation or comparative analysis of the systems. There is minimal abstraction beyond individual examples, focusing primarily on specific technologies and use cases."}}
{"id": "8c285ad1-d555-4c77-aa00-db03890536a0", "title": "Attacker Agenda", "level": "subsection", "subsections": [], "parent_id": "a7995731-2147-49a8-89ee-a4e47605a88d", "prefix_titles": [["title", "Survey of Attacks and Defenses \\\\on Edge-Deployed Neural Networks"], ["section", "Taxonomy of Attacks on and Defenses of Deployed Neural Networks"], ["subsection", "Attacker Agenda"]], "content": "The $y$-axis in Figure~\\ref{fig:taxonomy} represents the attacker's motivation for attacking an edge-deployed neural network.\nWe classify attacker motivations into four categories: \n\\noindent \\textbf{Denial of Service: } attackers may want to prevent a device running a neural network from properly functioning. For example, attackers may want to prevent smart cameras from properly classifying recordings in order not to raise alarms. Denial of Service (DoS) attacks prevent a device from maintaining availability and completing its function. As feedforward neural networks are data-independent and have fixed latencies, DoS attacks targeting DNNs are only applicable to accelerators running data-dependent models, e.g., recurrent neural networks~ or neural networks with early exits like BranchyNets~ or Tree LSTMs~.\n\\noindent \\textbf{User Privacy Violation: } smart devices are increasingly trusted with private user data such as shopping history, voice commands, or medical recordings~. This data is valuable for its advertising, monitoring, or polling value. User privacy violations are cases where the attacker is able to access measured or stored sensor data from the device or user data the device from the network. For example, attacks on voice assistants where the attacker can access previous voice commands constitute a local privacy violation. \n\\noindent \\textbf{Model Privacy Violation: } the attacker may attempt to exfiltrate a neural network model for a number of reasons: (1) models require significant investment to develop, and, as such, may be stolen and sold, or used in ensembles as a black box~, (2) finding adversarial examples is significantly easier if the attacker has access to a model (i.e., the white-box scenario), compared to only having access to model inputs and outputs (i.e., the black-box scenario)~, or (3) the attacker may attempt to learn data from the dataset the model was trained on~. \n\\noindent \\textbf{Integrity Violation: } the attackers may not want to outright prevent the device from functioning, but may want to force the neural network to perform in an unacceptable way. For example, malware may craft adversarial packets in an attempt to fool a network intrusion detection system (IDS) that uses DNNs to identify packets. Local integrity violations are cases where the attacker is able to affect the correctness of a device's neural network inference.\nIn Figure~\\ref{fig:taxonomy}, we list several attacker agendas, sorted by severity. We add two additional categories of general user and integrity violations, which consists of cases that affect not only a single device, but multiple devices, some of which are not under the attacker's physical control. An example of this is data poisoning attacks on federated learning systems~, where attackers controlling one device can insert backdoors into all devices in the network.", "cites": [689, 7318, 2674, 8412, 2673], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from cited papers to categorize attacker motivations, connecting DNN properties (e.g., data independence, early exits) to specific attack types. It critically discusses the implications of these motivations, such as the increased threat of white-box vs black-box attacks and the impact of model theft on downstream security. The abstraction level is strong, as it generalizes across different attack scenarios to identify overarching principles in how attackers exploit edge-deployed neural networks."}}
{"id": "58d424cb-4d29-497e-aee9-393e46254954", "title": "Attacks on Deployed Neural Networks", "level": "section", "subsections": [], "parent_id": "ac2e27ee-a8f3-4a4c-b292-dda9a70aafc1", "prefix_titles": [["title", "Survey of Attacks and Defenses \\\\on Edge-Deployed Neural Networks"], ["section", "Attacks on Deployed Neural Networks"]], "content": "We present a short survey of published attacks on neural network accelerators. We focus primarily on test-time attacks (attacks on already trained models), as we assume that training-time attacks such as data poisoning~ or Neural Trojans~ must happen before the model is deployed. \n\\noindent \\textbf{API attacks:}\nAPI attacks interact with the victim device only through the sensors, the interface, or the network. \nHere we assume that the attack is independent of the hardware platform running the neural network and does not rely on any side-channel information.\nThe majority of the API attacks present in the literature either attempt to (1) exfiltrate the model or the model metaparameters, (2) find adversarial examples, or (3) infer some property of the model's training data.\nIn~, the authors show how machine learning models hosted behind APIs can be exfiltrated. Here the attacker sends crafted inputs and collects outputs from the model until the attacker is able to reconstruct the model behind the API. In the case of simpler ML models such as decision trees, the models can be perfectly reconstructed. However, for more complex models such as neural networks, the attacker cannot simply solve nonlinear equations to arrive at model weights, but must instead train a `student' network on input-output pairs collected from the API~. A similar work~ shows the simplicity of reverse-engineering black-box neural network weights, architecture, optimization method and the training/data split. In~, authors reframe the goal from model theft, to arriving at a `knockoff' model exhibiting the same functionality.  In~, authors ignore model parameters and instead attempt to steal the hyperparameters of a network. Good hyperparameters, while far smaller than models, can be more difficult to arrive at, as they require many experiments and human effort to tune. \nIn order to violate the integrity of a machine learning model, attackers may attempt to find adversarial examples~. While most attacks rely on having access to the white-box model or the output gradient, several works have shown that even black-box networks~ and networks with obfuscated gradients~ are not resistant to determined attackers.\nLastly, attackers may attempt to infer some information about the data the neural network was trained on. Attacks which determine whether a specific input was used in training a model are called \\textit{membership inference} attacks. Though DNN models are typically smaller than the training dataset, they can nonetheless memorize potentially secret information~, as in the example of predictive keyboards memorizing PIN codes or passwords. In~, authors show that even when the model is behind a black-box API, and the adversary has no knowledge of the victim's training dataset, membership inference attacks are still successful.\n\\noindent \\textbf{Software side-channel attacks:}\nAPI and software side-channel (SC) attacks target a similar attack surface, but software side-channel attacks can additionally gain information through side-effects such as timing or cache side-channels. Here, the attacker abuses information about the physical device processing the attackers request to gain an insight into the internal state of the device. \nBoth timing and cache side-channels typically cannot reveal anything about the data being processed on the device - timing SC reveal information about the compute intensity of a certain task, and cache SC reveal information about recently accessed addresses in the caches. As such, they are commonly employed to extract course-grain information such as neural network architecture running on a device. For example, in Cache Telepathy~, attackers use the Flush+Reload~ and Prime+Probe~ cache SC attacks to measure the size of general matrix multiply (GEMM) operations, first counting the number of parameters in the model, and then narrowing down the model architecture. While this attack is restricted to CPUs, GPUs are no less vulnerable to cache-side channels~. A similar work~ is applicable to CPUs, GPUs, and DNN accelerators, and can fingerprint a network after only a single inference operation. It leverages a priori knowledge of major DNN libraries to prime the instruction cache and learn which functions are called during inference.\nTiming attacks are also used to reveal model architecture: in~, the authors assume that the attacker knows the victim's hardware, and is able to buy the same device in order to build timing profiles of different networks. By only knowing the accuracy and the latency of the victim network, the attacker trains many candidate architectures searching for one that has the same signature. This, however, requires the attacker to first steal a part of the training dataset using a membership inference attack~, which negates much of the need for stealing a model architecture.\nSoftware SC attacks may be less successful in the edge domain compared to the cloud, as edge devices typically serve a single user, while SC are typically used for compromising secure multi-user systems~. However, as more networks are pushed to the edge, we can expect multi-network systems with different privileges, goals, and timescales to become increasingly common. An example of this may be predictive keyboards, which perform both inference (text prediction) and NN training on the same device~.\nAnother potential vulnerability may be introduced with the adoption of data-dependent inference latency. For example, DARPA's N-ZERO program~ seeks low-power edge devices that may need to stay dormant for years and have several levels of neural networks, each activating the next one once a certain pattern is sensed. These types of networks are inherently vulnerable to timing attacks, as conventional methods for defending against timing attacks, such as constant time functions negate all the benefits of variable-latency inference. \n\\noindent \\textbf{Physical side-channel attacks:}\nPhysical side-channels typically measure some physical quantity, such as power, electromagnetic radiation, vibration, etc.\nSeveral works have explored using physical side-channels to extract the neural network architecture, weights, or user inputs to an edge device.\nMemory access patterns can trivially reveal model architecture. In~, authors attempt to steal a model and model architecture running on a secure enclave such as Intel SGX~, by observing memory access traces. While traces allow attackers to learn the architecture, the model can only be stolen if the accelerator exploits data-dependent model properties, such as the sparsity of hidden neuron activations~. Power and electromagnetic (EM) side-channel attacks are explored in~, where the authors use EM SCs to learn the activation function, simple power analysis to learn model architecture, and differential power analysis to learn network weights. While simple power analysis does not require invasive measures, differential power analysis may require chip decapsulation, and would need to be classified as an invasive attack. Finally, the authors show how user's private inputs may be extracted using power analysis.\nA similar attack is explored in~, where the authors use a power side-channel to observe the processing of the first layer of a convolutional network and extract user's inputs. The authors explore both active and passive attackers, i.e., attackers that can actively input their own images to the accelerator and attackers that can only observe user inputs.\nAnother line of attacks attempts to induce faults in order to cause misclasifications~ and relies on a microarchitectural or device-level attacks, such as RowHammer~.\n\\noindent \\textbf{Probing attacks:}\nProbing attacks assume that an attacker is able to access the individual components of the device, e.g., the CPU/GPU/ASIC, the RAM memory, non-volatile storage, or busses, but is not able to perform invasive attacks that access the internals of the chips. The attacker has full access to measure signals on any exposed wires or even drive wires themself. This opens up a variety of denial-of-service, integrity, and privacy attacks. Additionally, probing attacks assume that no tamper evidence is left after the attack, unlike invasive attacks. \nA simple attack the attacker can carry out is model theft - here the attacker probes the memory bus and runs an inference operation while recording the model being loaded onto the chip. This can be prevented by storing only the encrypted model in RAM and NVM, and decrypting the model on-the-fly, if power requirements permit~. \nHowever, even if the model is encrypted, just knowing the memory access pattern is enough to reveal the model architecture. Each layer and activation will have a different memory bandwidth, and the attacker can monitor these changes along with memory addresses to learn where layers start and end in memory. While oblivious RAM~ can hide memory addresses, memory access timings are still sufficient to reveal the topology of the model. This forces the defender to either prefetch weights or create fake accesses in order to obfuscate memory access timings~.\nSimilarly, network activations may be larger than the available on-chip memory and may be stored in RAM. These activations also need to be encrypted, because even in cases when the device manufacturer is not concerned about privacy, these activations can be used in order to infer the model weights~. \nThe attacker may also attempt to overwrite parts of RAM or feed their own inputs to the chip in order to subvert any software guards, for example in order to generate more input-output pairs used for API model theft~. Encrypted RAM may defend against this type of attack, but the device is still susceptible to DoS attacks, where fake accesses are inserted on busses. \n\\noindent \\textbf{Invasive attacks:}\nInvasive attacks assume that the attacker has full control over the chip and is able to bypass any tamper-proof packaging.\nThese attacks include freezing the device in order to extract volatile memory, probing the internals of the chip, ionizing parts of the chip in order to induce faults, feeding non-legitimate voltages and clock frequencies to the chip, etc. Mounting these attacks is typically cost-prohibitive and requires substantial expertise and equipment to execute. \nSeveral works have explored invasive attacks on DNN accelerators, and many of the conventional (non-DNN specific) invasive attacks are still applicable to them. In DeepLaser~, the authors decapsulate a chip and are able to induce faults by shining a laser on the chip, causing misclasifications by the neural network. This is done by causing bit-flips in the last layer's activation function, where flipping high-order bits of an output neuron's activation will cause the associated category or value to be dominant. Choosing the minimal amount of bit-flips to achieve a desired output has been studied in two works:  and . Both these works show that, despite the robustness of neural networks to random perturbations, networks are highly susceptible to targeted bit-flips, in a manner similar to non-targeted adversarial attacks~.\nWhile we have not been able to find any examples of this, we expect neural network accelerators to be vulnerable to cold boot attacks~, which may be able to steal unencrypted models stored in volatile memory, or microprobing~, which may be able to bypass model or user data decryption.", "cites": [893, 7603, 8576, 6058, 1193, 912, 2676, 2401, 2677, 2675, 7604, 3481], "cite_extract_rate": 0.4117647058823529, "origin_cites_number": 34, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the cited papers by categorizing attacks into API, software side-channel, physical side-channel, and probing attacks, and connecting ideas across works to form a coherent narrative. It provides some critical evaluation by discussing limitations, such as the dependency on multi-user systems for side-channel effectiveness and the trade-offs in defending variable-latency inference. The abstraction is strong, identifying broader patterns such as the role of device architecture and the implications of edge deployment for security."}}
{"id": "ba5629b9-f615-41c2-857f-763dc33ecc7b", "title": "Defending Edge Devices Running Neural Networks", "level": "section", "subsections": [], "parent_id": "ac2e27ee-a8f3-4a4c-b292-dda9a70aafc1", "prefix_titles": [["title", "Survey of Attacks and Defenses \\\\on Edge-Deployed Neural Networks"], ["section", "Defending Edge Devices Running Neural Networks"]], "content": "We briefly cover proposed defenses for edge devices running neural networks. \n\\noindent \\textbf{API defenses:}\nThe majority of API attacks we have mentioned attempt to steal the model or the model architecture, learn which inputs have been used to train the model, or find adversarial examples for the model running on the device. As finding adversarial examples typically involves first stealing the model~, we focus only on defenses against model exfiltration and membership inference attacks.\nIn a recent work called Prada~, the authors succeed in detecting API model-stealing attacks with a 100\\% detection rate and no false positives. Here, the authors do not attempt to detect if a single query is malicious (as in the case of adversarial attacks), but whether some consecutive set of them is actively trying to steal the model. The authors detect model-stealing queries as they are specifically crafted to extract the maximum amount of information out of the model. However, the authors note that attackers may introduce dummy queries to maintain a benign query distribution, resulting in slower but more covert model-stealing attacks.\nWatermarking is a method for embedding secret information into some system in order to verify the origin of that system at a later date. Watermarking has been proposed as a method of establishing ownership of neural networks~. Here, a watermark is applied to a neural network in such a way that it does not impact the network's accuracy, but can be used to confirm ownership from network outputs. Even if the party responsible for the theft attempts to prune or finetune the network, watermarks can be retained~.\nDefending against membership inference attacks has been explored in several works. In~, the authors claim that overfitting is the reason why models are vulnerable to membership inference attacks and suggest that differential privacy~ used during training can protect against these types of attacks. They propose several defenses, similar to those used in defending against adversarial attacks: (1) reducing the number of predicted classes (in the case of classification problems), (2) reducing the amount of information per class by rounding prediction probabilities, (3) increasing entropy of the prediction values and (4) using stronger regularization during training.\nSimilarly, in~, the authors propose two defenses: dropout~, where authors show that randomly zeroing out neurons during training partially prevents the attackers from inferring membership, and model stacking, where multiple models are used in an ensemble to make a prediction. \n\\noindent \\textbf{Side-channel defenses:}\nDue to the data-independent behavior of non-recurrent DNNs, all of the software side-channel attacks we have listed attempt to steal the network architecture. We have not been able to find any attacks that succeed at violating privacy of the inputs or the model parameters through software side-channels.\nIn DeepRecon~, where attackers prime the instruction cache in order to learn function invocations, the authors propose a defense where the defender simultaneously creates decoy function calls to similar neural network layers. These decoy layers should be small enough not to incur a performance penalty. However, this defense does not stop the attacker from using data cache-based side-channels or timing side-channels.\nCache Telepathy~ suggests less aggressive compiler optimizations, cache partitioning~ or disallowing resource sharing as defenses against cache-based SC. However, these may not be viable solutions without hardware support for secure caches.\nWhile cache-based defenses may help hide some of the accesses, and the defender may go so far as to remove the possibility of an attacker executing code on the same shared resources as the victim, a determined attacker may attempt to probe the memory bus. As neural networks are typically larger than the last-level cache of modern processors, caches will suffer from capacity misses and the network architecture may be exposed to memory probing attacks.\nIn the Trusted Inference Engine (TIE)~, the device can either create fake memory accesses in times of reduced memory bandwidth or prefetch data, given available on-chip storage. As TIE targets networks with data-independent profiles (i.e., not recurrent neural networks), the timing of fake or prefetched accesses can be calculated at compile time.\nSimilar techniques can be applied to counter power and timing side-channels. As long as networks have data-independent behavior, i.e., the accelerator does not attempt to take advantage of zero values~, or the network computation graph is static~, power and timing side-channel attacks should not be able to learn information about the network. \n\\noindent \\textbf{Defenses against invasive and semi-invasive attacks:}\nThere are two common approaches used when an organization needs to deploy software with privacy or integrity requirements. One option is to not trust the edge hardware, and assume that the hardware can be actively malicious, as in the case of untrusted CPUs/GPUs, possible hardware Trojans, broken hardware defenses~, etc. \nThere exist several algorithms that allow processing on private data. Homomorphic encryption~ (HE) for neural networks has been explored in CryptoNets~, where the authors use HE to run neural networks on encrypted data, without decrypting it at any time during the process. One of the issues with using HE is the performance reduction - inference using HE can be 100-1000 times slower than without HE. Several works have, however, been able to accelerate HE for neural networks. In Gazelle~, authors leverage HE for linear layers and Yao's Garbled Circuits~ for offloading calculating nonlinearities to the owner of the private data, as well as an efficient SIMD implementation and a set of homomorphic linear algebra.\nWhile HE is very efficient for linear layers of a network, DNNs typically use nonlinear activations between the layers, requiring many rounds of computationally expensive calculations. An alternative venue for private inference is based on Yao's Garbled Circuits~ (GC). Here, two parties want to compute the output of a function (a neural network in this case), where one party supplies the network, and the other the inputs to the network. The party that supplies the network typically creates a garbled circuit and uses a procedure such as oblivious transfer~ (OT) to acquire the second party's inputs without learning those inputs. A naive implementation of neural networks on GC is very inefficient, and several works have presented domain-specific optimizations to them. In DeepSecure~, authors first prune the network~, and then convert the network to Verilog for which they can apply logic minimization. In~, authors present a modified GC that supports free addition and constant-multiplication on a limited integer range, and a significantly cheaper activation function.  \nAs a third take on efficient DNNs using GC, XONN~ attempts to accelerate XNOR-based networks~ (networks where activations have only values of -1 or 1), as XNOR operations can be processed for free in GC~. While GC requires a linear number of rounds w.r.t. the number of network layers, both~ and~ are able to perform inference in a fixed amount of rounds.\nThe question that arises is whether it makes sense to run any of these algorithms on edge devices. In the case of inference, where both the model and user inputs should be kept private, the defender has the choice of sending encrypted inputs to the cloud or sending the encrypted model to the edge. Since HE is computationally expensive, edge devices may not receive any latency benefits by running the models locally (unless they are not connected to the network at all). \nAnother option for private edge inference is \\textbf{\\textit{hardware root-of-trust}}~. Here, the defender trusts some type of hardware device, which is built with certain security measures, as in the case of secure enclaves~ or secure accelerators~. These devices are typically built to work in adversarial environments, where the threat model assumes that attacker can tamper with the device, but cannot probe chip internals.\nFor example, using secure enclaves, such as Intel SGX~, to perform inference can provide privacy and integrity to the user and neural network deployer, but may be very inefficient. In MLCapsule~, authors develop a machine learning as a service (MLaaS) platform above Trusted Execution Environments (TEE) such as Intel SGX, and formally prove it's security. In~, the authors propose to use Intel SGX as a hardware root-of-trust, but leverage other hardware such as more powerful (but untrusted) CPUs cores and GPUs to perform inference. The authors are able to guarantee both the privacy of the data sent to untrusted devices, as well as the integrity of results received.\nAn alternative venue explores building custom secure neural network accelerators~. Here, the design stores obfuscated or encrypted models in off-chip memory, and performs efficient decryption / deobfuscation on the device. The design leverages secure pseudo-random number generators using physical unclonable functions~ (PUF) as a source of randomness as an alternative to the power-hungry but more secure encryption. The design also provides security against timing attacks by prefetching data or creating fake accesses to RAM memory. \nSince the attacker can still probe peripherals, the device must encrypt data in RAM. However, by timing the memory accesses, the attacker can learn the model architecture. Using oblivious RAM (ORAM) does not help, as ORAM only protects the address values and not access times. Additionally, neural network weights are typically stored in ascending order, so knowing the addresses (but not timings) reveals only the complete model size. To prevent the attacker from timing the RAM, the defender, then, must either have a prefetcher and load weights in advance while maintaining a constant bandwidth, or create fake accesses in times when the bandwidth is unused~. \n\\vspace{-0.1in}", "cites": [2678, 689, 7605, 603, 7607, 7609, 2401, 7606, 7341, 851, 7604, 7610, 7608], "cite_extract_rate": 0.3888888888888889, "origin_cites_number": 36, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple defenses from different papers, organizing them into categories (API, side-channel, and invasive/semi-invasive) and highlighting how they address unique challenges in edge-deployed neural networks. It also provides some critical evaluation, such as the limitations of decoy functions or the performance trade-offs of homomorphic encryption. While there is abstraction in identifying broader patterns (e.g., watermarking as a general ownership mechanism), the section could benefit from deeper comparative insights or a more meta-level discussion of overarching principles in DNN security."}}
