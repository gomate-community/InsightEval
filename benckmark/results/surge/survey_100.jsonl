{"id": "050d9770-ac59-4b1f-8fe3-24003253d68d", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "593ff0cc-d185-411c-9f7c-02ab17399c08", "prefix_titles": [["title", "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios"], ["section", "Introduction"]], "content": "\\blfootnote{* equal contribution}\nMost of today's research in natural language processing (NLP) is concerned with the processing of 10 to 20 high-resource languages with a special focus on English, and thus, ignores thousands of languages with billions of speakers~. \nThe rise of data-hungry deep learning systems increased the performance of NLP for high resource-languages, but the shortage of large-scale data in less-resourced languages makes their processing a challenging problem.\nTherefore, \\newcite{intro/ruder2019problems} named NLP for low-resource scenarios one of the four biggest open problems in NLP nowadays.\nThe umbrella term low-resource covers a spectrum of scenarios with varying resource conditions. It includes work on threatened languages, such as Yongning Na, a Sino-Tibetan language with 40k speakers and only 3k written, unlabeled sentences . Other languages are widely spoken but seldom addressed by NLP research. More than 310 languages exist with at least one million L1-speakers each . Similarly, Wikipedia exists for 300 languages.\\footnote{\\url{https://en.wikipedia.org/wiki/List_of_Wikipedias}} \nSupporting technological developments for low-resource languages can help to increase participation of the speakers' communities in a digital world. \nNote, however, that tackling low-resource settings is even crucial when dealing with popular NLP languages as low-resource settings do not only concern languages but also non-standard domains and tasks, for which -- even in English -- only little training data is available. Thus, the term ``language'' in this paper also includes domain-specific language. \nThis importance of low-resource scenarios and the significant changes in NLP in the last years have led to active research on resource-lean settings and a wide variety of techniques have been proposed. They all share the motivation of overcoming the lack of labeled data by leveraging further sources. However, these works differ greatly on the sources they rely on, e.g., unlabeled data, manual heuristics or cross-lingual alignments. Understanding the requirements of these methods is essential for choosing a technique suited for a specific low-resource setting. Thus, one key goal of this survey is to highlight the underlying assumptions these techniques take regarding the low-resource setup.\n\\def\\sectionautorefname{§}\n\\def\\subsectionautorefname{§}\n\\begin{table*}\n    \\footnotesize\n    \\centering\n    \\begin{tabular}{p{4.2cm}|p{3.3cm}|p{3.75cm}|c|c}\n    \\toprule\n    \\multirow{2}{*}{\\textbf{Method}} & \\multirow{2}{*}{\\textbf{Requirements}}  & \\multirow{2}{*}{\\textbf{Outcome}}  & \\multicolumn{2}{c}{\\textbf{For low-resource}} \\\\\n    & & & \\textbf{languages} & \\textbf{domains}\\\\\n    \\midrule\n    Data Augmentation (\\autoref{sub:data-aug}) & labeled data, heuristics* & additional labeled data & \\ding{51} & \\ding{51} \\\\ \\midrule\n    Distant Supervision (\\autoref{sub:distant}) & unlabeled data, heuristics* & additional labeled data & \\ding{51} & \\ding{51} \\\\ \\midrule\n    Cross-lingual  projections (\\autoref{sub:projections}) & unlabeled data, high-resource labeled data, cross-lingual alignment & additional labeled data & \\ding{51} & \\ding{55} \\\\ \\midrule\n    Embeddings \\& Pre-trained LMs (\\autoref{sub:lm}) & unlabeled data & better language representation & \\ding{51} & \\ding{51} \\\\ \\midrule\n    LM domain adaptation (\\autoref{sub:domain-lm}) & existing LM, \\newline unlabeled domain data & domain-specific language representation & \\ding{55} & \\ding{51} \\\\ \\midrule\n    Multilingual LMs (\\autoref{sub:multi-lm}) & multilingual unlabeled data & multilingual feature representation & \\ding{51} & \\ding{55} \\\\ \\midrule\n    Adversarial Discriminator (\\autoref{sec:ml}) & additional datasets & independent representations & \\ding{51} & \\ding{51} \\\\ \\midrule\n    Meta-Learning (\\autoref{sec:ml}) & multiple auxiliary tasks & better target task performance & \\ding{51} & \\ding{51}\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Overview of low-resource methods surveyed in this paper. * Heuristics are typically gathered manually.}\n    \\label{tab:overview}\n\\end{table*}\nIn this work, we (1) give a broad and structured overview of current efforts on low-resource NLP, (2) analyse the different aspects of low-resource settings, (3) highlight the necessary resources and data assumptions as guidance for practitioners and (4) discuss open issues and promising future directions. Table \\ref{tab:overview} gives an overview of the surveyed techniques along with their requirements a practitioner needs to take into consideration.", "cites": [7165], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of low-resource NLP challenges and methods, using a table to summarize techniques and their requirements. While it references a key paper (Ruder 2019), it does not deeply synthesize ideas across multiple cited works. There is minimal critical evaluation or abstraction into broader principles, and the narrative remains largely introductory and factual."}}
{"id": "7f89d35b-fdb1-45b2-84f8-7e199e15b6ee", "title": "Related Surveys", "level": "section", "subsections": [], "parent_id": "593ff0cc-d185-411c-9f7c-02ab17399c08", "prefix_titles": [["title", "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios"], ["section", "Related Surveys"]], "content": "\\label{sec:surveys-etc}\nRecent surveys cover low-resource machine translation  and unsupervised domain adaptation . \nThus, we do not investigate these topics further in this paper, but focus instead on general methods for low-resource, supervised natural language processing including data augmentation, distant supervision and transfer learning. This is also in contrast to the task-specific survey by \\newcite{survey/magueresse2020low} who review highly influential work for several extraction tasks, but only provide little overview of recent approaches. In Table \\ref{tab:surveys} in the appendix, we list past surveys that discuss a specific method or low-resource language family for those readers who seek a more specialized follow-up.", "cites": [4236], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions existing surveys and one specific paper, but it lacks meaningful synthesis of ideas across the cited works. It does not critically evaluate the strengths or limitations of the surveyed papers, nor does it abstract to broader trends or principles. The content remains largely descriptive and does not offer a deep analytical perspective."}}
{"id": "032c8058-5716-4e46-ab86-4be5446a81ed", "title": "How Low is Low-Resource?", "level": "subsection", "subsections": [], "parent_id": "01449395-0026-492e-b26a-4abacec7957c", "prefix_titles": [["title", "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios"], ["section", "Aspects of ``Low-Resource''"], ["subsection", "How Low is Low-Resource?"]], "content": "On the dimension of task-specific labels, different thresholds are used to define low-resource. \nFor part-of-speech (POS) tagging, \\newcite{distant/garrette2013pos2hours} limit the time of the annotators to 2 hours resulting in up to 1-2k tokens. \\newcite{defining/kann2020POSPoorly} study languages that have less than 10k labeled tokens in the Universal Dependency project  and  \\newcite{survey/loubser2020viability} report that most available datasets for South African languages have 40-60k labeled tokens. \nThe threshold is also task-dependent and more complex tasks might also increase the resource requirements. For text generation, \\newcite{defining/yang2019responseGeneration} frame their work as low-resource with 350k labeled training instances. Similar to the task, the resource requirements can also depend on the language. \\newcite{defining/plank2016MultilingualPOS} find that task performance varies between language families given the same amount of limited training data.\nGiven the lack of a hard threshold for low-resource settings, we see it as a spectrum of resource availability. We, therefore, also argue that more work should evaluate low-resource techniques across different levels of data availability for better comparison between approaches. \nFor instance,  and  show that \nfor very small datasets non-neural methods outperform more modern approaches while the latter obtain better performance in resource-lean scenarios once a few hundred labeled instances are available.", "cites": [4237, 7165], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several papers to explain varying thresholds for defining low-resource settings across tasks and languages, creating a reasonably coherent narrative. It includes some abstraction by framing low-resource as a spectrum and suggests a need for evaluating methods across different data levels. While it does not deeply critique or contrast the methodologies, it does point out gaps and task-dependency, which adds a moderate level of analytical value."}}
{"id": "42c5586a-c1af-4632-8c7f-0d63e89e8355", "title": "Data Augmentation", "level": "subsection", "subsections": [], "parent_id": "50d4dcc0-4105-402a-8978-8fa345f584f7", "prefix_titles": [["title", "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios"], ["section", "Generating Additional Labeled Data"], ["subsection", "Data Augmentation"]], "content": "\\label{sub:data-aug}\nNew instances can be obtained based on existing ones by modifying the features with transformations that do not change the label. In the computer vision community, this is a popular approach where, e.g., rotating an image is invariant to the classification of an image's content. For text, on the token level, this can be done by replacing words with equivalents, such as synonyms , entities of the same type  or words that share the same morphology . Such replacements can also be guided by a language model that takes context into consideration .\nTo go beyond the token level and add more diversity to the augmented sentences, data augmentation can also be performed on sentence parts. Operations that (depending on the task) do not change the label include manipulation of parts of the dependency tree , simplification of sentences by removal of sentence parts  and inversion of the subject-object relation . For whole sentences, paraphrasing through back-translation can be used. This is a popular approach in machine translation where target sentences are back-translated into source sentences . An important aspect here is that errors in the source side/features do not seem to have a large negative effect on the generated target text the model needs to predict. It is therefore also used in other text generation tasks like abstract summarization  and table-to-text generation . Back-translation has also been leveraged for text classification . This setting assumes, however, the availability of a translation system. Instead, a language model can also be used for augmenting text classification datasets . It is trained conditioned on a label, i.e., on the subset of the task-specific data with this label. It then generates additional sentences that fit this label. \\newcite{data-aug/ding2020daga} extend this idea for token level tasks.\nAdversarial methods are often used to find weaknesses in machine learning models . They can, however, also be utilized to augment NLP datasets . Instead of manually crafted transformation rules, these methods learn how to apply small perturbations to the input data that do not change the meaning of the text (according to a specific score). This approach is often applied on the level of vector representations. \nFor instance, \\newcite{data-aug/grundkiewicz2019Grammar} reverse the augmentation setting by applying transformations that flip the (binary) label. In their case, they introduce errors in correct sentences to obtain new training data for a grammar correction task. \n\\textbf{Open Issues:} While data augmentation is ubiquitous in the computer vision community and while most of the above-presented approaches are task-independent, it has not found such widespread use in natural language processing. A reason might be that several of the approaches require an in-depth understanding of the language.  There is not yet a unified framework that allows applying data augmentation across tasks and languages. Recently,  hypothesised that data augmentation provides the same benefits as pre-training in transformer models. However, we argue that data augmentation might be better suited to leverage the insights of linguistic or domain experts in low-resource settings when unlabeled data or hardware resources are limited.", "cites": [4238, 115, 4240, 4239], "cite_extract_rate": 0.5833333333333334, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information across multiple cited papers by integrating diverse data augmentation strategies, such as token-level replacement, sentence-level paraphrasing, and adversarial perturbations. It provides a critical perspective by pointing out that data augmentation has not been as widely adopted in NLP as in computer vision, potentially due to language-specific challenges. The abstraction is moderate, as it generalizes some patterns (e.g., task independence) but does not fully develop a meta-level framework."}}
{"id": "8fd03a5f-b039-4b07-af1e-c88569597ca9", "title": "Distant \\& Weak Supervision", "level": "subsection", "subsections": [], "parent_id": "50d4dcc0-4105-402a-8978-8fa345f584f7", "prefix_titles": [["title", "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios"], ["section", "Generating Additional Labeled Data"], ["subsection", "Distant \\& Weak Supervision"]], "content": "\\label{sub:distant}\nIn contrast to data augmentation, distant or weak supervision uses unlabeled text and keeps it unmodified. The corresponding labels are obtained through a (semi-)automatic process from an external source of information. For named entity recognition (NER), a list of location names might be obtained from a dictionary and matches of tokens in the text with entities in the list are automatically labeled as locations. Distant supervision was introduced by \\newcite{distant/mintz2009distant} for relation extraction (RE) with extensions on multi-instance  and multi-label learning . It is still a popular approach for information extraction tasks like NER and RE where the external information can be obtained from knowledge bases, gazetteers, dictionaries and other forms of structured knowledge sources . The automatic annotation ranges from simple string matching  to complex pipelines including classifiers and manual steps . This distant supervision using information from external knowledge sources can be seen as a subset of the more general approach of labeling rules. These encompass also other ideas like reg-ex rules or simple programming functions . \nWhile distant supervision is popular for information extraction tasks like NER and RE, it is less prevalent in other areas of NLP. Nevertheless, distant supervision has also been successfully employed for other tasks by proposing new ways for automatic annotation. \\newcite{distant/li-etal-2012-wiki} leverage a dictionary of POS tags for classifying unseen text with POS. For aspect classification,  create a simple bag-of-words classifier on a list of seed words and train a deep neural network on its weak supervision. \\newcite{distant/wang2019sentiment} use context by transferring a document-level sentiment label to all its sentence-level instances. \\newcite{distant/mekala2020meta} leverage meta-data for text classification and \\newcite{huber2020discourse} build a discourse-structure dataset using guidance from sentiment annotations. For topic classification, heuristics can be used in combination with inputs from other classifiers like NER  or from entity lists . For some classification tasks, the labels can be rephrased with simple rules into sentences. A pre-trained language model then judges the label sentence that most likely follows the unlabeled input  . An unlabeled review, for instance, might be continued with \"It was great/bad\" for obtaining binary sentiment labels.\n\\textbf{Open Issues:} \nThe popularity of distant supervision for NER and RE might be due to these tasks being particularly suited. There, auxiliary data like entity lists is readily available and distant supervision often achieves reasonable results with simple surface form rules. It is an open question whether a task needs to have specific properties to be suitable for this approach. The existing work on other tasks and the popularity in other fields like image classification  suggests, however, that distant supervision could be leveraged for more NLP tasks in the future.\nDistant supervision methods heavily rely on auxiliary data. In a low-resource setting, it might be difficult to obtain not only labeled data but also such auxiliary data.   find a large gap between the performance on high-resource and low-resource languages for POS tagging pointing to the lack of high-coverage and error-free dictionaries for the weak supervision in low-resource languages. This emphasizes the need for evaluating such methods in a realistic setting and avoiding to just simulate restricted access to labeled data in a high-resource language.\nWhile distant supervision allows obtaining labeled data more quickly than manually annotating every instance of a dataset, it still requires human interaction to create automatic annotation techniques or to provide labeling rules. This time and effort could also be spent on annotating more gold label data, either naively or through an active learning scheme. Unfortunately, distant supervision papers rarely provide information on how long the creation took, making it difficult to compare these approaches. Taking the human expert into the focus connects this research direction with human-computer-interaction and human-in-the-loop setups .", "cites": [4241, 4238, 8565, 7769, 4253, 4242, 4243, 1621, 2466], "cite_extract_rate": 0.56, "origin_cites_number": 25, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the concept of distant and weak supervision across multiple tasks and papers, highlighting commonalities and variations in annotation strategies. It provides critical analysis by discussing limitations such as dependency on auxiliary data and lack of human effort reporting. Furthermore, it abstracts the methods into broader patterns and raises open questions about task suitability and scalability."}}
{"id": "e206a999-bb3c-4e85-8cf0-85d431387307", "title": "Cross-Lingual Annotation Projections", "level": "subsection", "subsections": [], "parent_id": "50d4dcc0-4105-402a-8978-8fa345f584f7", "prefix_titles": [["title", "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios"], ["section", "Generating Additional Labeled Data"], ["subsection", "Cross-Lingual Annotation Projections"]], "content": "\\label{sub:projections}\nFor cross-lingual projections, a task-specific classifier is trained in a high-resource language. Using parallel corpora, the unlabeled low-resource data is then aligned to its equivalent in the high-resource language where labels can be obtained using the aforementioned classifier. These labels (on the high-resource text) can then be projected back to the text in the low-resource language based on the alignment between tokens in the parallel texts . This approach can, therefore, be seen as a form of distant supervision specific for obtaining labeled data for low-resource languages. Cross-lingual projections have been applied in low-resource settings for tasks, such as POS tagging and parsing . \nSources for parallel text can be the OPUS project ,\nBible corpora  or the recent JW300 corpus .\nInstead of using parallel corpora, existing high-resource labeled datasets can also be machine-translated into the low-resource language  . Cross-lingual projections have even been used with English as a target language for detecting linguistic phenomena like modal sense and telicity that are easier to identify in a different language .\n\\textbf{Open issues:} Cross-lingual projections set high requirements on the auxiliary data needing both labels in a high-resource language and means to project them into a low-resource language. Especially the latter can be an issue as machine translation by itself might be problematic for a specific low-resource language. A limitation of the parallel corpora is their domains like political proceedings or religious texts. \\newcite{projection/mayhew-etal-2017-cheap}, \\newcite{projection/fang-cohn-2017-model} and  propose systems with fewer requirements based on word translations, bilingual dictionaries and task-specific seed words, respectively.", "cites": [4244], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 14, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of cross-lingual annotation projections and mentions a few applications and sources of parallel data. However, it lacks substantial synthesis of ideas from multiple papers and does not establish a deeper connection between the cited work and broader trends in low-resource NLP. There is minimal critical analysis or abstraction beyond specific examples."}}
{"id": "90477119-7640-4c8d-8461-34186f884fdc", "title": "Learning with Noisy Labels", "level": "subsection", "subsections": [], "parent_id": "50d4dcc0-4105-402a-8978-8fa345f584f7", "prefix_titles": [["title", "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios"], ["section", "Generating Additional Labeled Data"], ["subsection", "Learning with Noisy Labels"]], "content": "\\label{sec:noisy}\nThe above-presented methods allow obtaining labeled data quicker and cheaper than manual annotations. These labels tend, however, to contain more errors. Even though more training data is available, training directly on this noisily-labeled data can actually hurt the performance. Therefore, many recent approaches for distant supervision use a noise handling method to diminish the negative effects of distant supervision. We categorize these into two ideas: noise filtering and noise modeling.\nNoise filtering methods remove instances from the training data that have a high probability of being incorrectly labeled. This often includes training a classifier to make the filtering decision. The filtering can remove the instances completely from the training data, e.g., through a probability threshold , a binary classifier , or the use of a reinforcement-based agent . Alternatively, a soft filtering might be applied that re-weights instances according to their probability of being correctly labeled  or an attention measure .\nThe noise in the labels can also be modeled. A common model is a confusion matrix estimating the relationship between clean and noisy labels . The classifier is no longer trained directly on the noisily-labeled data. Instead, a noise model is appended which shifts the noisy to the (unseen) clean label distribution. This can be interpreted as the original classifier being trained on a ``cleaned'' version of the noisy labels. In \\newcite{distant/ye2019shift}, the prediction is shifted from the noisy to the clean distribution during testing. In \\newcite{distant/chen-etal-2020-relabel-agents}, a group of reinforcement agents relabels noisy instances. \\newcite{distant/rehbein-ruppenhofer-2017-detecting}, \\newcite{distant/lison-etal-2020-weak-supervision} and \\newcite{ren2020denoising} leverage several sources of distant supervision and learn how to combine them.\nIn NER, the noise in distantly supervised labels tends to be false negatives,\ni.e., mentions of entities that have been missed by the automatic method. Partial annotation learning  takes this into account explicitly. Related approaches learn latent variables , use constrained binary learning  or construct a loss assuming  that only unlabeled positive instances exist .", "cites": [4241, 1695, 4245, 4242], "cite_extract_rate": 0.5, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key ideas from multiple papers, organizing them into a coherent framework around two main approaches: noise filtering and noise modeling. It abstracts from specific implementations to discuss general strategies and their implications, and while it does not deeply critique the papers, it does evaluate their effectiveness and assumptions, such as the focus on false negatives in NER and the use of latent variables."}}
{"id": "898b669f-4c03-4325-bb41-e0b437a71f10", "title": "Non-Expert Support", "level": "subsection", "subsections": [], "parent_id": "50d4dcc0-4105-402a-8978-8fa345f584f7", "prefix_titles": [["title", "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios"], ["section", "Generating Additional Labeled Data"], ["subsection", "Non-Expert Support"]], "content": "\\label{sec:nonexpert}\nAs an alternative to an automatic annotation process, annotations might also be provided by non-experts. Similar to distant supervision, this results in a trade-off between label quality and availability. For instance, \\newcite{distant/garrette2013pos2hours} obtain labeled data from non-native-speakers and without a quality control on the manual annotations. This can be taken even further by employing annotators who do not speak the low-resource language . \n take the opposite direction, integrating speakers of low-resource languages without formal training into the model development process in an approach of participatory research. This is part of recent work on how to strengthen low-resource language communities and grassroot approaches .", "cites": [4241, 4242, 4246], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly mentions non-expert support and cites relevant papers, but it lacks a coherent synthesis of these works. It describes different approaches without critically evaluating their strengths or limitations. Additionally, there is minimal abstraction or generalization beyond the individual papers, resulting in a primarily descriptive and low-insight-level section."}}
{"id": "08729569-e9fe-4099-b79e-d05e287b81c7", "title": "Transfer Learning", "level": "section", "subsections": ["a8c605db-e7c6-44b9-984c-fc150c7a50bc", "df439b98-ef59-4a62-998c-bf63ccc26cd1", "d90e7d5d-8ed8-41ee-a381-537e8d432b20"], "parent_id": "593ff0cc-d185-411c-9f7c-02ab17399c08", "prefix_titles": [["title", "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios"], ["section", "Transfer Learning"]], "content": "\\label{sec:transferlearning}\nWhile distant supervision and data augmentation generate and extend task-specific training data, transfer learning reduces the need for labeled target data by transferring learned representations and models.\nA strong focus in recent works on transfer learning in NLP lies in the use of pre-trained language representations that are trained on unlabeled data like BERT~. Thus, this section starts with an overview of these methods (\\autoref{sub:lm}) and then discusses how they can be utilized in low-resource scenarios, in particular, regarding the usage in domain-specific (\\autoref{sub:domain-lm}) or multilingual low-resource settings (\\autoref{sub:multi-lm}).", "cites": [7165], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section introduces the concept of transfer learning and briefly mentions pre-trained language models like BERT, but it lacks synthesis by not connecting the cited paper (e.g., Efficient Estimation of Word Representations) to broader trends or other methods discussed. There is minimal critical analysis or abstraction, as it primarily serves as a setup for deeper subsections without offering insight into the strengths, weaknesses, or comparative value of the cited work."}}
{"id": "a8c605db-e7c6-44b9-984c-fc150c7a50bc", "title": "Pre-Trained Language Representations", "level": "subsection", "subsections": [], "parent_id": "08729569-e9fe-4099-b79e-d05e287b81c7", "prefix_titles": [["title", "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios"], ["section", "Transfer Learning"], ["subsection", "Pre-Trained Language Representations"]], "content": "\\label{sub:lm}\nFeature vectors are the core input component of many neural network-based models for NLP tasks. They are numerical representations of words or sentences, as neural architectures do not allow the processing of strings and characters as such. \\newcite{pre-train/collobert2011natural} showed that training these models for the task of language-modeling on a large-scale corpus results in high-quality word representations, which can be reused for other downstream tasks as well.\nSubword-based embeddings such as fastText n-gram embeddings~ and byte-pair-encoding embeddings  addressed out-of-vocabulary issues by splitting words into multiple subwords, which in combination represent the original word. \n\\newcite{emb/zhu2019subword} showed that these embeddings leveraging subword information are beneficial for low-resource sequence labeling tasks, such as named entity recognition and typing, and outperform word-level embeddings.  \\newcite{emb/jungmaier2020dirichlet} added smoothing to word2vec models to correct its bias towards rare words and achieved improvements in particular for low-resource settings.\nIn addition, pre-trained embeddings were published for more than 270 languages for both embedding methods.\nThis enabled the processing of texts in many languages, including multiple low-resource languages found in Wikipedia.\nMore recently, a trend emerged of pre-training large embedding models using a language model objective to create context-aware word representations by predicting the next word or sentence. \nThis includes pre-trained transformer models~, such as BERT~ or RoBERTa~. \nThese methods are particularly helpful for low-resource languages for which large amounts of unlabeled data are available, but task-specific labeled data is scarce~.\n\\textbf{Open Issues:} While pre-trained language models achieve significant performance increases compared to standard word embeddings, it is still questionable if these methods are suited for real-world low-resource scenarios. \nFor example, all of these models require large hardware requirements, in particular, considering that the transformer model size keeps increasing to boost performance~.\nTherefore, these large-scale methods might not be suited for low-resource scenarios where hardware is also low-resource.\\\\\n\\newcite{pre-train/van2020optimal} showed that low- to medium-depth transformer sizes perform better than larger models for low-resource languages and \\newcite{bert/schick2020s} managed to train models with three orders of magnitude fewer parameters that perform on-par with large-scale models like GPT-3 on few-shot task by reformulating the training task and using ensembling.\n\\newcite{pre-train/melamud2019} showed that simple bag-of-words approaches are better when there are only a few dozen training instances or less for text classification, while more complex transformer models require more training data.\n found that cross-view training  leverages large amounts of unlabeled data better for task-specific applications in contrast to the general representations learned by BERT.\nMoreover, data quality for low-resource, even for unlabeled data, might not be comparable to data from high-resource languages. \\newcite{defining/alabi-etal-2020-massive} found that word embeddings trained on larger amounts of unlabeled data from low-resource languages are not competitive to embeddings trained on smaller, but curated data sources.", "cites": [8756, 38, 826, 8745, 9, 7165], "cite_extract_rate": 0.8888888888888888, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section provides a structured overview of pre-trained language representations and integrates multiple papers to discuss their relevance to low-resource NLP. It connects different embedding approaches (subword, BERT, RoBERTa) and identifies key challenges such as hardware requirements and data quality. The critical analysis is strong, particularly in highlighting limitations of large models in real-world low-resource settings and presenting alternative, smaller models as viable solutions."}}
{"id": "df439b98-ef59-4a62-998c-bf63ccc26cd1", "title": "Domain-Specific Pre-Training", "level": "subsection", "subsections": [], "parent_id": "08729569-e9fe-4099-b79e-d05e287b81c7", "prefix_titles": [["title", "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios"], ["section", "Transfer Learning"], ["subsection", "Domain-Specific Pre-Training"]], "content": "\\label{sub:domain-lm}\nThe language of a specialized domain can differ tremendously from what is considered the standard language, thus, many text domains are often less-resourced as well. For example, scientific articles can contain formulas and technical terms, which are not observed in news articles. \nHowever, the majority of recent language models are pre-trained on general-domain data, such as texts from the news or web-domain, which can lead to a so-called ``domain-gap'' when applied to a different domain.\nOne solution to overcome this gap is the adaptation to the target domain by finetuning the language model.\n\\newcite{domain/gugurangan2020dontstop} showed that continuing the training of a model with additional domain-adaptive and task-adaptive pre-training with unlabeled data leads to performance gains for both high- and low-resource settings for numerous English domains and tasks.\nThis is also displayed in the number of domain-adapted language models \\cite[(i.a.)]{domain/alsentzer2019publicly,domain/huang2019clinicalbert,domain/adhikari2019docbert,domain/lee2020patent,domain/jain2020nukebert}, \nmost notably BioBERT~ that was pre-trained on biomedical PubMED articles and SciBERT~ for scientific texts. \nFor example,  showed that a general-domain BERT model performs well in the materials science domain, but the domain-adapted SciBERT performs best. \n\\newcite{domain/xu2020dombert} used in- and out-of-domain data to pre-train a domain-specific model and adapt it to low-resource domains.\n\\newcite{domain/aharoni2020unsupervised} found domain-specific clusters in pre-trained language models and showed how these could be exploited for data selection in domain-sensitive training. \nPowerful representations can be achieved by combining high-resource embeddings from the general domain with low-resource embeddings from the target domain . \n\\newcite{emb/kiela-etal-2018-dynamic} showed that embeddings from different domains can be combined using attention-based meta-embeddings, which create a weighted sum of all embeddings. \\newcite{emb/lange2020adversarial} further improved on this by aligning embeddings trained on diverse domains using an adversarial discriminator that distinguishes between the embedding spaces to generate domain-invariant representations.", "cites": [4244, 826, 4247], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes several domain-specific pre-training approaches and connects them to the overarching theme of bridging the domain gap in low-resource NLP. It abstracts some concepts, such as the use of attention-based meta-embeddings and adversarial alignment for domain adaptation. However, the critical analysis is limited, as the section does not deeply evaluate trade-offs, limitations, or compare the effectiveness of the approaches beyond surface-level observations."}}
{"id": "d90e7d5d-8ed8-41ee-a381-537e8d432b20", "title": "Multilingual Language Models", "level": "subsection", "subsections": [], "parent_id": "08729569-e9fe-4099-b79e-d05e287b81c7", "prefix_titles": [["title", "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios"], ["section", "Transfer Learning"], ["subsection", "Multilingual Language Models"]], "content": "\\label{sub:multi-lm}\nAnalogously to low-resource domains, low-resource languages can also benefit from labeled resources available in other high-resource languages.\nThis usually requires the training of multilingual language representations by combining monolingual representations  or training a single model for many languages, such as multilingual BERT~ or XLM-RoBERTa~ .\nThese models are trained using unlabeled, monolingual corpora from different languages and can be used in cross- and multilingual settings, due to many languages seen during pre-training. \nIn cross-lingual zero-shot learning, no task-specific labeled data is available in the low-resource target language. Instead, labeled data from a  high-resource language is leveraged. \nA multilingual model can be trained on the target task in a high-resource language and afterwards, applied to the unseen target languages, such as for named entity recognition , reading comprehension ,  temporal expression extraction , or POS tagging and dependency parsing .\n showed, however, that there is still a large gap between low and high-resource setting.\n and  proposed adding a minimal amount of target-task and -language data (in the range of 10 to 100 labeled sentences) which resulted in a  significant boost in performance for classification in low-resource languages.\nThe transfer between two languages can be improved by creating a common multilingual embedding space of multiple languages. \nThis is useful for standard word embeddings  as well as pre-trained language models. For example, by aligning the languages inside a single multilingual model, i.a., in cross-lingual~ or multilingual settings . \nThis alignment is typically done by computing a mapping between two different embedding spaces, such that the words in both embeddings share similar feature vectors after the mapping . \nThis allows to use different embeddings inside the same model and helps when two languages do not share the same space inside a single model .\nFor example, \\newcite{cross-ling/zhang2019improving} used bilingual representations by creating cross-lingual word embeddings using a small set of parallel sentences between the high-resource language English and three low-resource African languages, Swahili, Tagalog, and Somali, to improve document retrieval performance for the African languages.\n\\textbf{Open Issues:} While these multilingual models are a tremendous step towards enabling NLP in many languages, possible claims that these are universal language models do not hold. \nFor example, mBERT covers 104 and XLM-R 100 languages,\nwhich is a third of all languages in Wikipedia as outlined earlier.  \nFurther, \\newcite{multi-ling/wu-dredze-2020-languages} showed that, in particular, low-resource languages are not well-represented in mBERT.\nFigure~\\ref{fig:language_families} shows which language families with at least 1 million speakers are covered by mBERT and XLM-RoBERTa\\footnote{A language family is covered if at least one associated language is covered. Language families can belong to multiple regions, e.g., Indo-European belongs to Europe and Asia.}. \nIn particular, African and American languages are not well-represented within the transformer models, even though millions of people speak these languages.\nThis can be problematic, as languages from more distant language families are less suited for transfer learning, as \\newcite{lauscher2020zero} showed.\n\\begin{figure}\n    \\centering\n    \\includegraphics[trim=60 330 60\n60,clip,width=0.49\\textwidth]{figure_language_families.pdf}\n    \\caption{Language families with more than 1 million speakers covered by multilingual transformer models. }\n    \\label{fig:language_families}\n\\end{figure}", "cites": [8565, 8328, 4250, 7165, 4246, 4248, 4249, 8745, 4251], "cite_extract_rate": 0.7333333333333333, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes multiple works to explain how multilingual language models and cross-lingual embeddings can address low-resource language tasks. It critically evaluates limitations, such as under-representation of certain language families and the performance gap between low and high-resource settings. The discussion abstracts beyond specific papers by highlighting the importance of embedding alignment and the challenges associated with distant language families in transfer learning."}}
{"id": "ca44ac27-a8bd-46d0-97aa-2a6d9c9defc4", "title": "Ideas From Low-Resource Machine Learning in Non-NLP Communities", "level": "section", "subsections": [], "parent_id": "593ff0cc-d185-411c-9f7c-02ab17399c08", "prefix_titles": [["title", "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios"], ["section", "Ideas From Low-Resource Machine Learning in Non-NLP Communities"]], "content": "\\label{sec:ml}\nTraining on a limited amount of data is not unique to natural language processing. Other areas, like general machine learning and computer vision, can be a useful source for insights and new ideas. We already presented data augmentation and pre-training. Another example is Meta-Learning , which is based on multi-task learning. Given a set of auxiliary high-resource tasks and a low-resource target task, meta-learning trains a model to decide how to use the auxiliary tasks in the most beneficial way for the target task. For NLP, this approach has been evaluated on tasks such as sentiment analysis , user intent classification , natural language understanding , text classification  and dialogue generation . Instead of having a set of tasks, \\newcite{transfer/rahimi-etal-2019-massively} built an ensemble of language-specific NER models which are then weighted depending on the zero- or few-shot target language.\nDifferences in the features between the pre-training and the target domain can be an issue in transfer learning, especially in neural approaches where it can be difficult to control which information the model takes into account. Adversarial discriminators  can prevent the model from learning a feature-representation that is specific to a data source. \\newcite{transfer/gui-etal-2017-part-adversarial}, \\newcite{transfer/liu2017adversarial}, \\newcite{transfer/kasai2019adversarial},  \\newcite{domain/griesshaber2020low} and\n \\newcite{transfer/zhou2019dualAdversarial} learned domain-independent representations using adversarial training. \n \\newcite{transfer/kim-etal-2017-cross-adversarial}, \\newcite{domain/chen2018adversarial} and \\newcite{cross-ling/lange2020adversarial} worked with language-independent representations for cross-lingual transfer. These examples show the beneficial exchange of ideas between NLP and the machine learning community.", "cites": [1690, 8757, 1695], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key concepts like meta-learning and adversarial training from both NLP and non-NLP domains, connecting them to low-resource NLP tasks. It provides a general overview of how these methods work and highlights their application in various NLP scenarios. While it identifies some general challenges (e.g., domain-specific representations), it lacks deeper critical evaluation of the cited works or a broader framework for understanding their implications."}}
{"id": "27505e6f-b9d1-470b-89da-8ce753a42523", "title": "Discussion and Conclusion", "level": "section", "subsections": [], "parent_id": "593ff0cc-d185-411c-9f7c-02ab17399c08", "prefix_titles": [["title", "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios"], ["section", "Discussion and Conclusion"]], "content": "In this survey, we gave a structured overview of recent work in the field of low-resource natural language processing. Beyond the method-specific open issues presented in the previous sections, we see the comparison between approaches as an important point of future work. Guidelines are necessary to support practitioners in choosing the right tool for their task. In this work, we highlighted that it is essential to analyze resource-lean scenarios across the different dimensions of data-availability. This can reveal which techniques are expected to be applicable in a specific low-resource setting. More theoretic and experimental work is necessary to understand how approaches compare to each other and on which factors their effectiveness depends. , for instance, hypothesized that data augmentation and pre-trained language models yield similar kind of benefits. Often, however, new techniques are just compared to similar methods and not across the range of low-resource approaches. While a fair comparison is non-trivial given the different requirements on auxiliary data, we see this endeavour as essential to improve the field of low-resource learning in the future. This could also help to understand where the different approaches complement each other and how they can be combined effectively.\n\\section*{Acknowledgments}\nThe authors would like to thank Annemarie Friedrich for her valuable feedback and the anonymous reviewers for their helpful comments.\nThis work has been partially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project-ID 232722074 – SFB 1102 and the EU Horizon 2020 project ROXANNE under grant number 833635.\n\\bibliography{survey}\n\\bibliographystyle{acl_natbib}\n\\appendix\n\\begin{table*}\n\\footnotesize\n\\centering\n\\begin{tabular}{lll}\n\\toprule\n& Low-resource surveys & \\newcite{survey/cieri2016selection} , \\newcite{survey/magueresse2020low} \\\\\n\\midrule\n\\multirow{9}{*}{\\rotatebox{90}{\\textit{Method-specific}}}\n& Active learning & \\newcite{active/olsson2009literature}, \\newcite{active/settles2009survey}, \\newcite{active/aggarwal2014survey} \\\\\n& Distant supervision & \\newcite{survey/roth2013survey}, \\newcite{survey/smirnova2018relation}, \\newcite{survey/shi2019brief}. \\\\\n& Unsupervised domain adaptation & \\newcite{survey/wilson2018domain}, \\newcite{survey/ramponi2020neural} \\\\\n& Meta-Learning & \\newcite{survey/hospedales2020meta} \\\\\n& Multilingual transfer & \\newcite{survey/steinberger2012survey}, \\newcite{cross-ling/ruder2019survey} \\\\\n& LM pre-training & \\newcite{survey/rogers2020primer}, \\newcite{survey/qiu2020pre} \\\\\n& Machine translation & \\newcite{survey/liu2019survey} \\\\\n& Label noise handling & \\newcite{survey/frenay2013classification}, \\newcite{survey/algan2019image} \\\\\n& Transfer learning & \\newcite{pan2009survey}, \\newcite{weiss2016survey}, \\newcite{tan2018survey} \\\\\n\\midrule\n\\multirow{5}{*}{\\rotatebox{90}{\\textit{Language-}}} \n& African languages & \\newcite{survey/grover2010overview}, \\newcite{survey/de2011introduction}  \\\\\n& Arabic languages & \\newcite{survey/al2018deep}, \\newcite{survey/guellil2019arabic}, \\newcite{survey/younes2020language} \\\\\n& American languages & \\newcite{survey/mager2018challenges} \\\\\n& South-Asian languages & \\newcite{survey/daud2017urdu}, \\newcite{survey/banik2019bengali}, \\newcite{survey/harish2020comprehensive} \\\\\n& East-Asian languages & \\newcite{survey/yude2011brief} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Overview of existing surveys on low-resource topics.}\n\\label{tab:surveys}\n\\end{table*}", "cites": [115], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview by identifying the need for comparative guidelines in low-resource NLP and emphasizing the importance of analyzing methods across data-availability dimensions. It draws a general insight from the cited papers, particularly from 'Unsupervised Data Augmentation for Consistency Training,' to highlight limitations in current comparisons and the potential overlap between data augmentation and pre-training. However, the synthesis and abstraction remain moderate, as the section does not develop a novel framework or deeply analyze broader implications of the cited works."}}
{"id": "ca88a9a2-e919-43be-b4cb-40565c35c380", "title": "Complexity of Tasks", "level": "section", "subsections": [], "parent_id": "593ff0cc-d185-411c-9f7c-02ab17399c08", "prefix_titles": [["title", "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios"], ["section", "Complexity of Tasks"]], "content": "\\label{app:tasks}\nWhile a large number of labeled resources for English are available for many popular NLP tasks, this is not the case for the majority of low-resource languages. To measure (and visualize as done in Figure \\ref{fig:applications} in the main paper) which applications are accessible to speakers of low-resource languages we examined resources for six different languages, ranging from high- to low-resource languages for a fixed set of tasks of varying complexity, ranging from basic tasks, such as tokenization, to higher-level tasks, such as question answering.\nFor this short study, we have chosen the following languages. The number of speakers are the combined L1 and L2 speakers according to \\newcite{intro/Ethnologue2019}.\n\\begin{itemize}\n\\item[(1)] English: The most high-resource language according to the common view and literature in the NLP community.\n\\item[(2)] Yoruba: An African language, which is spoken by about 40 million speakers and contained in the EXTREME benchmark . \nEven with that many speakers, this language is often considered as a low-resource language and it is still discussed whether this language is also endangered .\n\\item[(3)] Hausa: An African language with over 60 million speakers. It is not covered in EXTREME or the universal dependencies project .\n\\item[(4)] Quechua: A language family encompassing about 8 million speakers, mostly in Peru.\n\\item[(5)] Nahuatl and (6) Estonian: Both have between 1 and 2 million speakers, but are spoken in very different regions (North America \\& Europe).\n\\end{itemize}\nAll speaker numbers according to \\newcite{intro/Ethnologue2019} reflecting the total number of users (L1 + L2). The tasks were chosen from a list of popular NLP tasks\\footnote{\\url{https://en.wikipedia.org/wiki/Natural\\_language\\_processing\\#Common\\_NLP\\_Tasks}}.\nWe selected two tasks for the lower-lever groups and three tasks for the higher-level groups, which reflects the application diversity with increasing complexity.\nTable~\\ref{tab:tasks-covered} shows which tasks were addressed for each language.\nWord segmentation, lemmatization, part-of-speech tagging, sentence breaking and (semantic) parsing are covered for Yoruba and Estonian by treebanks from the universal dependencies project . \nCusco Quechua is listed as an upcoming language in the UD project, but no treebank is accessible at this moment.\nThe WikiAnn corpus for named entity recognition  has resources and tools for NER and sentence breaking for all six languages.\nLemmatization resources for Nahuatl were developed by \\newcite{app/martinez2012computer} and \\newcite{app/lozano2013syntactic} developed resources for part-of-speech tagging, tokenization and parsing of Quechuan. \nThe CoNLL conference and SIGMORPHON organized two shared tasks for morphological reinflection which provided lemmatization resources for many languages, including Quechuan .\nBasic resources for simple semantic role labeling and entity linking were developed during the LORELEI program for many low-resource languages , including resources for Yoruba and Hausa (even though the latter \"fell short\" according to the authors).\nEstonian coreference resolution is targeted by \\newcite{app/kubler2016multilingual}, but the available resources are very limited. Estonian sentiment is done by \\newcite{app/pajupuu2016identifying}.\nAll languages are covered by the multilingual fasttext embeddings  and byte-pair-encoding embeddings . Yoruba, Hausa and Estonian are covered by mBERT or XLM-RoBERTa as well.\nText summarization is done for Estonian by \\newcite{app/muurisep2005estsum} and for Hausa by \\newcite{app/bashir2017automatic}.\nThe EXTREME benchmark  covers question answering and natural language inference tasks for Yoruba and Estonian (besides NER, POS tagging and more).\nPublicly available systems for optical character recognition support all six languages .\nAll these tasks are supported for the English language as well, and most often, the English datasets are many times larger and of much higher quality. Some of the previously mentioned datasets were automatically translated, as in the EXTREME benchmark for several languages. As outlined in the main paper, we do not claim that all tasks marked in the Table yield high-performance model, but we instead indicate if any resources or models can be found for a language.\n\\begin{landscape}\n\\setlength{\\tabcolsep}{3pt}\n\\begin{table}\n\\footnotesize\n\\begin{tabular}{llccccc} \\toprule\nGroup & Task\n& Yoruba & Hausa  & Quechuan & Nahuatl & Estonian \\\\ \\midrule\n & Num-Speakers\n & 40 mil. & 60 mil. & 8 mil. & 1.7 mil. & 1.3 mil.\\\\ \\midrule\n\\multirow{2}{*}{Text processing} \n & Word segmentation\n & \\ding{51} & \\ding{51} & \\ding{51} & \\ding{51} & \\ding{51} \\\\\n & Optical character recognition\n &  &  &  &  &  \\\\ \\midrule\n\\multirow{2}{*}{Morphological analysis} \n& Lemmatization / Stemming\n&  &  &  &  &  \\\\\n & Part-of-Speech tagging\n &  &  & \\newcite{app/lozano2013syntactic} & \\ding{55} &  \\\\ \\midrule\n\\multirow{2}{*}{Syntactic analysis} \n & Sentence breaking\n & \\ding{51} & \\ding{51} & \\ding{51} & \\ding{51} & \\ding{51} \\\\\n & Parsing\n &  & \\ding{55} &  & \\ding{55} &  \\\\ \\midrule\n\\multirow{2}{*}{Distributional semantics} \n & Word embeddings\n & FT, BPEmb & FT, BPEmb & FT, BPEmb & FT, BPEmb & FT, BPEmb \\\\\n & Transformer models\n & mBERT & XLM-R & \\ding{55} & \\ding{55} & mBERT, XLM-R \\\\ \\midrule\n\\multirow{2}{*}{Lexical semantics} \n & Named entity recognition\n &  &  &  &  &  \\\\\n & Sentiment analysis\n & \\ding{55} & \\ding{55} & \\ding{55} & \\ding{55} &  \\\\ \\midrule\n\\multirow{3}{*}{Relational semantics} \n & Relationship extraction\n & \\ding{55} & \\ding{55} & \\ding{55} & \\ding{55} & \\ding{55} \\\\\n & Semantic Role Labelling\n &  &  & \\ding{55} & \\ding{55} & \\ding{55} \\\\\n & Semantic Parsing\n &  & \\ding{55} & \\ding{55} & \\ding{55} &  \\\\ \\midrule\n\\multirow{3}{*}{Discourse} \n & Coreference resolution\n & \\ding{55} & \\ding{55} & \\ding{55} & \\ding{55} &  \\\\\n & Discourse analysis\n & \\ding{55} & \\ding{55} & \\ding{55} & \\ding{55} &  \\\\\n & Textual entailment\n &  & \\ding{55} & \\ding{55} & \\ding{55} &  \\\\ \\midrule\n\\multirow{3}{*}{Higher-level NLP} \n & Text summarization\n & \\ding{55} &  & \\ding{55} & \\ding{55} &  \\\\\n & Dialogue management \n & \\ding{55} & \\ding{55} & \\ding{55} & \\ding{55} &  \\ding{55} \\\\\n & Question answering (QA)\n &  & \\ding{55} & \\ding{55} & \\ding{55} &  \\\\ \\midrule\n & SUM\n & 13 & 10 & 8 & 6 & 15\\\\ \\bottomrule\n\\end{tabular}\n\\caption{Overview of tasks covered by six different languages. Note that this list is non-exhaustive and due to space reasons we only give one reference per language and task. }\n\\label{tab:tasks-covered}\n\\setlength{\\tabcolsep}{6pt}\n\\end{table}\n\\end{landscape}\n\\end{document}", "cites": [4253, 7165, 4252, 4237, 8565, 2222], "cite_extract_rate": 0.3888888888888889, "origin_cites_number": 18, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of NLP task coverage across six languages, integrating some information from cited works, but primarily functions as a resource inventory. There is limited synthesis beyond listing which papers address specific tasks, and no significant critical analysis or abstraction to broader trends or principles in low-resource NLP."}}
