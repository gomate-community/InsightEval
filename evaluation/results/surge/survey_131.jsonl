{"id": "40274cbd-dbd6-495f-8626-c1a1e76a7346", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "908d8fa7-e998-4bf3-9426-ce385d023d35", "prefix_titles": [["title", "Reinforcement Learning based Recommender Systems: A Survey"], ["section", "Introduction"]], "content": "We are living in the \\textit{Zettabyte Era}~.  The massive volume of information available on the web leads to the problem of \\textit{information overload}, which makes it difficult for a decision maker to make right decisions.  The realization of this in our everyday lives is when we face a long list of items in an online shopping store; the more items in the list, the tougher it becomes to select among them.  Recommender systems (RSs) are software tools and algorithms that have been developed with the idea of helping users find their items of interest, through predicting their preferences or ratings on items~.  In fact, the idea is to know the users to some extent, i.e., making a \\textit{user profile} based on their feedback on items, and to recommending those items that match their profile.  Today, RSs are an essential part of most giant companies, like Google, Facebook, Amazon, and Netflix, and employed in a wide range of applications, including entertainment~, e-commerce~, news~, e-learning~, and healthcare~. \nNumerous techniques have been proposed to tackle the recommendation problem; traditional techniques include collaborative filtering, content-based filtering, and hybrid methods. Despite some success in providing relevant recommendations, specifically after the introduction of \\textit{matrix factorization}~, these methods have severe problems, such as \\textit{cold start}  (i.e., the system cannot provide useful recommendation when the user or item is new), lack of novelty and diversity, scalability, low quality recommendation, and great computational expense~.  Recently, deep learning~ has also gained popularity in the RS field due to its ability in finding complex and non-linear relationships between users and items and its cutting edge performance in recommendation~.  Nonetheless, deep learning models are usually non-interpretable, data hungry (this is specifically problematic as the amount of data, i.e. rating/user feedback, in the RS field is scarce), and computationally expensive~.\nRL is a semi-supervised machine learning field in which the agent optimizes its behavior through interaction with the environment.  The milestone in the RL field is the combination of deep learning with traditional RL methods, which is known as deep reinforcement learning (DRL)~. This made it possible to apply RL in problems with enormous state and action spaces, including self-driving cars~, robotics~, industry automation~, finance~, healthcare~, and RSs~.  The unique ability of an RL agent in learning from a reward from the environment without any training data makes RL specifically a perfect match for the recommendation problem.  Today, more and more companies are utilizing the power of RL to recommend better items to their customers. For example, in a study by researchers at Google~, it is shown that RL can be employed to recommend better video content to YouTube's users.  In fact, the use of RL in the RS community is not limited to the industry, but it is becoming a trend in academia as well. Fig.~\\ref{fig:year} illustrates this trend.\n\\begin{figure}\n\\centering     \n\\subfigure[]{\\label{fig:year}\\includegraphics[width=55mm]{rlrs}}\n\\subfigure[]{\\label{fig:venue}\\includegraphics[width=55mm]{venues}}\n\\subfigure[]{\\label{fig:ven-type}\\includegraphics[width=35mm]{ven-prop}}\n\\caption{Publications information of 97 surveyed RLRS papers. (a) Distribution of RLRSs publications per year (until September 2021) separated based on RL and DRL methods.\n(b) Venue distribution of published RLRSs. To be clear, we filtered venues with only one publication and termed them as `Others' in the graph.  This includes a long list of venues, including AAMAS, ICML, ICDM, and JMLR. (c) The proportion of different types of publications, i.e., conference proceedings, journal articles, and arXiv preprints, of surveyed RLRSs.}\n\\label{fig:publications}\n\\end{figure}\nThis trend and this topic motivated us to prepare this survey paper, which aims at providing a comprehensive overview of the state-of-the-art in reinforcement learning based recommender systems (RLRSs).  Our main purpose is to depict a high-level picture from the progress in the field since the beginning and show how this trend has been significantly changed with the advent of DRL.  At the same time, we provide detailed information about each method in the form of tables so as the reader can easily observe the similarities and differences between methods. \n\\textbf{Paper Collection Methodology.}  To collect relevant papers, we have used a multi-level search process.  The focus of this survey paper is specifically on RSs that use an RL algorithm.   Accordingly, in order to find relevant papers, we used Google Scholar as the main search engine and searched ``reinforcement learning recommender system'' keyword.  This search resulted in around 33,000 papers.  Out of first 1000 articles found, we collected 500 papers as the result of our first screening level.  Then, to increase the reliability of our paper collection, we also explored related libraries like ACM digital library, IEEE Xplore, SpringerLink, and ScienceDirect with the same keyword and until a point where no more relevant paper was among the results.  We found that all related articles identified in these libraries were available in our initial search using Google Scholar.  With carefully studying collected articles and excluding irrelevant papers, duplicates, theses, survey and review papers, we selected 97 articles to include in our survey paper.   Although we are certain that we did not find all RLRSs through the search process explained, we are confident that we could find the vast majority of relevant publications. \nIt is noteworthy to mention that we did not include RSs based on \\textit{multi-armed bandits}. Bandits are a simplified version of RL.  In particular, in bandits, similar to an RL problem, the agent should learn to maximize a numerical reward through interaction with the environment and solving the \\textit{exploration vs exploitation dilemma}~.   However in bandits, different from \\textit{full} RL, actions are not permitted to affect the state of the environment and the reward~. While bandits have been popular for the recommendation problem~, in light of recent successful applications of DRL and unprecedented interests in full RL by the RS community, we opt to only focus on RSs that use a full RL algorithm.  The curious reader is referred to a recent survey on the application of bandits to RSs~.\n\\textbf{Related Work.} Plenty of research has been done in the field of RSs and a plethora of survey papers have been published, including RSs~, collaborative filtering~, hybrid methods~, multi-media RSs~, explainable recommendation~, and article RSs~, to name a few.  There are also some published survey papers on topics closely related to RLRSs~.  Perhaps two closest survey papers to ours are~.  Ref.~ surveys DRL-based information seeking techniques, such as search, recommendation, and online advertising.  Authors discuss several RSs, which use multi-armed bandits and DRL for policy optimization. However, the work misses many important RLRSs and fails to provide an in-depth analysis of algorithms reviewed.   Zhang et al.~ provide a comprehensive survey on deep learning based RSs.  They consider DRL as a deep learning architectural paradigm and survey a few DRL-based RSs~.  Nonetheless, this classification is not correct and DRL is not a deep learning architecture, but it is an extension to traditional RL algorithms.  Other related surveys target sequence-aware~ and session-based~ recommendation techniques.  Ref.~ surveys sequence-aware RSs. Authors consider RL as a method for sequence learning and review some RL-based RSs~.   In another related survey~, RL is considered as a method for session-based RSs~.  None of previous published survey papers provide a comprehensive overview and in-depth analysis of published RLRSs.  To the best of our knowledge, this is the first survey paper that specifically targets RLRSs. \n\\textbf{Our contribution.}  The goal is to provide the reader with a vista toward the field so that they can quickly understand the topic and major trends and algorithms presented so far. This helps researchers see the big picture, compare algorithms' strengths and weaknesses, and shed some light on ways to advance them in the future.  Our main contributions can be summarized as:\n\\begin{itemize}\n\\item Presenting a framework for RLRSs. We first generally divide RLRSs into RL- and DRL-based methods. Then, we propose a framework with four main components, i.e., state representation, policy optimization, reward formulation, and environment building. This framework can model every RLRS and unify the development process of RLRSs.\n\\item Providing a thorough background on RL. We provide the reader with a fairly complete knowledge on RL/DRL and their various algorithms used by RLRSs. \n\\item Highlighting important trends and emerging topics. Instead of simply summarizing algorithms, our aim is to extract and to illustrate major trends and attempts in each main component of the proposed framework in particular and emerging topics in RLRSs in general. \n\\item  Suggesting some open research directions for the future.  In order to consolidate our survey paper, we finally present some observations about ongoing research in the RLRS field and propose some open research directions to advance the field. \n\\end{itemize}\n The remaining of this paper is organized as follows. In section~\\ref{sec:pre}, to help the reader better understand the topic, we discuss some preliminary concepts and provide a solid background on RL. Section~\\ref{sec:alg} presents RLRSs algorithms in a classified manner.  Emerging topics are highlighted in section~\\ref{sec:ET}.  In section~\\ref{sec:fut-wo}, some open research directions are suggested for the future work, and finally, the paper is concluded in section~\\ref{sec:con}. \n\\vspace{-8pt}", "cites": [5163, 3617, 5171, 5165, 8882, 5164, 5168, 5166, 8691, 5167, 5169, 3724, 8881, 5172, 7336, 8880, 166, 1912, 5170, 5173], "cite_extract_rate": 0.38461538461538464, "origin_cites_number": 52, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple cited papers to present a coherent narrative about the evolution of reinforcement learning in recommender systems, emphasizing the shift from traditional to sequential approaches. It provides a critical perspective by highlighting limitations of existing surveys and the shortcomings of traditional methods like bandits and deep learning. While it identifies broader trends, such as the rise of DRL in the field, it does not fully generalize to meta-level principles or offer a novel framework, which limits its abstraction."}}
{"id": "8a6f09bf-fa36-4d6c-8e8f-26187ae61c27", "title": "From Reinforcement Learning to Deep Reinforcement Learning", "level": "subsection", "subsections": [], "parent_id": "5e197519-a3ce-4341-9b08-2c42799c85e8", "prefix_titles": [["title", "Reinforcement Learning based Recommender Systems: A Survey"], ["section", "Preliminaries"], ["subsection", "From Reinforcement Learning to Deep Reinforcement Learning"]], "content": "\\label{subsec:RL}\nReinforcement learning (RL) is a machine learning field that studies problems and their solutions in which agents, through interaction with their environment, learn  to maximize a numerical reward. According to Sutton and Barto~, three characteristics distinguish an RL problem: (1) the problem is closed-loop, (2) the learner does not have a tutor to teach it what to do, but it should figure out what to do through trial-and-error, and (3) actions influence not only the short-term results, but also the long-term ones. The most common interface to model an RL problem is the \\textit{agent-environment} interface, depicted in Fig.~\\ref{fig:RL-int}.  The learner or decision maker is called \\textit{agent} and the \\textit{environment} is everything outside the agent.   Accordingly, at time step $t$, the agent sees some representations/information about the environment, called \\textit{state}, and based on the current state it takes an \\textit{action}.  On taking this action, it receives a numerical \\textit{reward} from the environment and finds itself in a new state.\n\\begin{figure}[t]\n\\centering     \n\\subfigure[]{\\label{fig:RL-int}\\includegraphics[width=40mm]{RL-interface.pdf}}\n\\subfigure[]{\\label{fig:RL-clas}\\includegraphics[width=110mm]{RLRSs}}\n\\caption{(a) Agent-environment RL interface, (b) RL algorithms used by RLRSs}\n\\end{figure}\nMore formally, the RL problem is typically formulated as a Markov decision process (MDP) in the form of a tuple $(\\pazocal{S}, \\pazocal{A},  \\pazocal{R}, \\pazocal{P}, \\gamma)$, where $\\pazocal{S}$ is the set of all possible states, $\\pazocal{A}$ is the set of available actions in all states, $\\pazocal{R}$ is the reward function, $\\pazocal{P}$ is the transition probability, and $\\gamma$ is the discount factor.\nThe main elements of an RL system are~: \n\\begin{itemize}[noitemsep]\n\\item \\textit{Policy:} policy is usually indicated by $\\pi$ and gives the probability of taking  action $a$ when the agent is in state $s$. Regarding the policy, RL algorithms can be generally divided into \\textit{on-policy} and \\textit{off-policy} methods. In the former, RL methods aim at evaluating or improving the policy they are using to make decisions. In the latter,  they improve or evaluate a policy that is different from the one used to generate the data.  \n\\item \\textit{Reward signal:} upon selecting actions, the environment provides a numerical reward to inform the agent how good or bad are the actions selected. \n\\item \\textit{Value function:} the reward signal is merely able to tell what is good immediately, but the value function defines what is good in the long run. \n\\item \\textit{Model:} model provides the opportunity to make inferences about the behavior of the environment.  For instance, the model can predict next state and next reward in a given state and action~. \n\\end{itemize}\n\\textbf{Algorithms.} Many algorithms have been proposed to solve an RL problem; they can be generally divided into \\textit{tabular} and \\textit{approximate} methods~. In tabular methods, since the size of action and state spaces is small, value functions can be represented as tables and optimal value function and policy can be found. On the other hand, in approximate methods, since the size of state space is enormous, the goal is to find a good approximate solution with the constraint of limited computational resources.  As mentioned earlier, with the foundation of DRL, a substantial change has emerged in the RL field in general. Accordingly, although DRL belongs to the approximate group, we generally divide RL algorithms used by RLRSs into RL-based and DRL-based algorithms as we believe this classification better reflects the recent trend in the RLRS field.  It is noteworthy to mention that the distinguishing factor between DRL and traditional RL algorithms is that DRL algorithms use deep learning for function approximation (a more detailed explanation on this is presented shortly in DRL-based Algorithms section).  In the following, we briefly review those RL algorithms employed by RLRSs.  Fig.~\\ref{fig:RL-clas} illustrates these algorithms.\n\\textit{1) RL-based Algorithms.} As stated before, RL algorithms could be divided into tabular and approximate methods.  Popular tabular methods include \\textit{dynamic programming}, \\textit{Monte Carlo}, and \\textit{temporal difference}.  \\textbf{Dynamic programming} methods assume a perfect model of the environment and use a value function to search for good policies. Two important algorithms from this class are \\textit{policy iteration} and \\textit{value iteration}. \\textbf{Policy iteration} algorithm composes of three steps: initialization, policy evaluation, and policy improvement. First the policy is randomly initialized, i.e., a random action $a \\in A(s)$ is selected for all $s\\in \\pazocal{S}$.  Then, the value of the states are computed and evaluated using\n\\begin{equation}\n\\label{eq:pol-it}\nV(s)\\leftarrow \\sum_{s', r} p \\big (s', r | s, \\pi (s) \\big ) \\big [ r + \\gamma V(s') \\big ],\n\\end{equation}\nwhere $p$ is the transition probability and $s'$ is the next state. Finally, the policy is updated as follows, $\\forall s \\in \\pazocal{S}$:\n\\begin{equation}\n\\label{eq:pol-it1}\n\\pi(s) \\leftarrow \\underset{a}{\\argmax} \\sum_{s', r} p(s', r | s, a) \\big [r + \\gamma V(s') \\big].\n\\end{equation}\nAs pointed out in~, a problem with policy iteration algorithm is that it needs policy evaluation in every iteration, which can be computationally prohibitive. \\textbf{Value iteration} algorithm is a special case of policy iteration algorithm in which policy evaluation is stopped after one \\textit{sweep}. More precisely, $V(s)$ is randomly initialized $\\forall s \\in \\pazocal{S}$. Then, it is updated in each step according to \n\\begin{equation}\n\\label{eq:val-it}\nV(s) \\leftarrow \\underset{a}{\\mathrm{max}} \\sum_{s', r} p(s', r | s, a) \\big [r + \\gamma V(s') \\big ].\n\\end{equation}\n\\textbf{Temporal difference} methods are a combination of dynamic programming and Monte Carlo methods. While they do not need a model from the environment, they can \\textit{bootstrap}, which is the ability to update estimates based on other estimates~.  From this family, Q-learning~ and Sarsa~ are very popular.  \\textbf{Q-learning} is a model-free, off-policy algorithm to learn the value of an action in a given state. The main component of Q-learning is the following \\textit{Bellman} equation:\n\\begin{equation}\n\\label{eq:ql}\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\big [ r_{t+1} + \\gamma \\, \\underset{a}{\\mathrm{max}} \\, Q(s_{t+1}, a) - Q(s_t, a_t) \\big ],\n\\end{equation}\nwhere $\\alpha$ is the learning rate. \\textbf{Sarsa} is an online version of Q-learning with the following update rule:\n\\begin{equation}\n\\label{eq:sarsa}\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\big [ r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\big ].\n\\end{equation}\n\\textbf{Sarsa($\\lambda$)} employs \\textit{eligibility traces}, which unify temporal difference and Monte Carlo methods~. In Sarsa($\\lambda$), the weight vector is updated on each step using\n \\begin{equation}\nw_{t+1} = w_t + \\alpha  \\delta_t  e_t,\n\\end{equation}\nwhere $\\delta_t$ is temporal difference error and defined by\n \\begin{equation}\n\\delta_{t} = r_{t+1} + \\gamma q(s_{t+1}, a_{t+1}, w_t) - q(s_t, a_t, w_t),\n\\end{equation}\nwhere $q$ is the approximate action value. Also, the eligibility trace $e_t$ is defined as\n\\begin{equation}\ne_{t} = \\gamma \\lambda e_{t-1} + \\nabla q(s_t, a_t, w_t), \\quad 0\\leq t \\leq T, e_{-1}=0\n\\end{equation}\nwhere $\\lambda \\in [0,1]$. More details on Sarsa($\\lambda$) can be found in~. \\textbf{R-learning}~ is an extension of Q-learning for continuing tasks where the interaction between agent and environment continues forever without termination.  The main idea in R-learning is the concept of \\textit{average reward}; that means, discounted reward setting is not necessary, even problematic, in continuous function approximation and there is no difference between immediate and delayed rewards~.  That said, Q-learning equation, Eq.~\\eqref{eq:ql}, could be rewritten as~\n\\begin{equation}\nQ(s, a) \\leftarrow Q(s, a) + \\alpha \\big [ r - \\rho + \\underset{a'}{\\mathrm{max}} \\, Q(s', a') - Q(s, a) \\big ],\n\\end{equation}\nwhere $\\rho$ is the average reward and can be achieved by\n \\begin{equation}\n\\rho \\leftarrow \\rho + \\beta \\big [ r - \\rho + \\underset{a'}{\\mathrm{max}} \\, Q(s, a') - \\underset{a}{\\mathrm{max}} Q(s, a) \\big ],\n\\end{equation}\nwhere $\\beta$ is a learning rate. \n There are also methods that combine or unify model-based RL (like Dynamic Programming) with model-free RL (e.g., Monte Carlo or temporal difference)~.  An enhanced version of Monte Carlo tree search \\textbf{(MCTS)} could be categorized in this family. Although MCTS is a search method in base, it is usually enhanced by a method to accumulate value estimates through Monte Carlo simulations~.\nThe basic MCTS is an iterative search tree building algorithm that runs until reaching a predefined computational budget~. It composes of four steps: selection, expansion, simulation, and backpropagation~.  Starting at the root node, a recursive child selection policy is executed to select the leaf node (\\textit{expandable} node).  The tree is then expanded by adding one or more child nodes.  Next, a simulation of a complete episode is performed from newly added leaf nodes to produce an outcome. Finally, the result of simulation is backpropagated in the tree (through selected nodes).  Later in 2016, MCTS was merged with deep learning~.  \n In \\textbf{approximate methods}, a practical approach is to \\textit{generalize} from previous experiences (already seen states) to unseen states. \\textit{Function approximation} is the type of generalization required in RL and many techniques could be used to approximate the function, including \\textit{artificial neural networks}.  \n\\textbf{Fitted Q} algorithm is an approximate method that is inspired by the idea of \\textit{fitted value iteration} proposed by Gordon~.  An interesting fact about fitted Q framework is that it allows to use any regression algorithm for function approximation. For instance, Ernst et al.~ use randomized trees for function approximation and call their method fitted Q iteration (FQI).  Among approximate solutions, \\textbf{policy gradient} methods have been very popular, which learn a \\textit{parameterized} policy and can select actions without the need of a value function.  \\textbf{REINFORCE}~ is a Monte Carlo method that uses episode samples in order to update the policy parameter $\\theta$. It first randomly initializes $\\theta$. Then, it iteratively generates a trajectory following policy $\\pi_\\theta: S_1, A_1, R_1, ..., S_T$. For each step $t=1, 2, ..., T$, it estimates return $G_t$ and updates $\\theta$ using\n\\begin{equation}\n\\label{eq:REINFORCE1}\n\\theta \\leftarrow \\theta + \\alpha \\gamma^t G_t \\nabla_\\theta \\, \\mathrm{log} \\, \\pi (A_t|S_t, \\theta). \n\\end{equation}\nA major problem with REINFORCE is that it has high variance in gradient estimation.  To overcome this problem, a baseline (state-value function) is added to the update rule:\n\\begin{equation}\n\\label{eq:REINFORCE-baseline}\n\\theta \\leftarrow \\theta + \\alpha \\gamma^t \\Big ( G_t - b(S_t) \\Big ) \\nabla_\\theta \\, \\mathrm{log} \\, \\pi (A_t|S_t, \\theta). \n\\end{equation}\nThis method is called REINFORCE-with-baseline (\\textbf{REINFORCE-wb})~.  While REINFORCE-wb reduces the variance, it is still a Monte Carlo method and has a slow convergence.  A better solution is to add bootstrapping to REINFORCE; an idea employed by \\textbf{actor-critic} method~.  In particular, instead of a baseline, a critic is used to criticize the policy generated by the actor.  That said, the update rule for actor-critic is revised as follow\n\\begin{equation}\n\\label{eq:AC}\n\\theta \\leftarrow \\theta + \\alpha \\gamma^t \\big ( G_t - v(S_t, w) \\big ) \\nabla_\\theta \\, \\mathrm{log} \\, \\pi (A_t|S_t, \\theta), \n\\end{equation}\nwhere $v(s, w)$ is a state-value function parameterized with $w$.\n\\textit{2) DRL-based Algorithms.} DRL is an interesting combination of deep learning with RL.  In fact, researchers at DeepMind found that this combination can achieve human-level performance in Atari games~.  Deep Q-network (\\textbf{DQN})~ is a creative combination of convolutional neural networks (CNN)~ with Q-learning.  More precisely, in DQN, a \\textit{Q network} is responsible for action-value approximation, which could be trained to minimize the following loss function:\n\\begin{equation}\n\\label{eq:dqn1}\nL_i(\\theta_i) = \\mathbb{E}_{s, a \\sim \\rho(\\cdot)} \\Big [ \\big ( y_i - Q(s, a; \\theta_i) \\big )^2 \\Big ], \n\\end{equation}\nwhere $y_i = \\mathbb{E}_{s'} [ r+\\gamma \\, \\mathrm{max}_{a'} \\, Q(s', a'; \\theta_{i-1})|s, a]$ is the target for iteration $i$ and $\\rho$ is a probability distribution over transitions $s, a, r, s'$ collected from the environment. Differentiating $L(\\theta)$ in Eq.~\\eqref{eq:dqn1} with respect to $\\theta$ yields the following gradient\n\\begin{equation}\n\\label{eq:dqn2}\n\\nabla_{\\theta_i} L_i(\\theta_i) = \\mathbb{E}_{s, a \\sim \\rho(\\cdot)} \\Big [ \\big (r+\\gamma \\, \\underset{a'}{\\mathrm{max}} \\, Q(s', a'; \\theta_{i-1}) - Q(s, a; \\theta_i) \\big )\\nabla_{\\theta_i}Q(s,a;\\theta_i) \\Big ].\n\\end{equation}\nIt is computationally beneficial to optimize the gradient in Eq.~\\eqref{eq:dqn2} using \\textit{stochastic gradient descent}~.  According to~, DQN modifies the original Q-learning algorithm in three ways: 1) It uses \\textit{experience replay}, first proposed in~ and a method that keeps agents' experiences over various time steps in a replay memory and uses them to update weights in the training phase. 2) In order to reduce the complexity in updating weights, current updated weights are kept fixed and fed into a second (duplicate) network whose outputs are used as Q-learning targets. 3) The error term $(r+\\gamma \\, \\mathrm{max}_{a'} \\, Q(s', a'; \\theta_{i-1}) - Q(s, a; \\theta_i)$ in Eq.~\\eqref{eq:dqn2} is clipped such that it remains in the interval [-1, 1].  All these modifications help improve the stability of DQN.  \nHowever, DQN has some problems; first, following Q-learning algorithm, DQN overestimates action values under certain circumstances, which makes learning inefficient and can lead to sub-optimal policies~.  Double DQN (\\textbf{DDQN}) was proposed to alleviate this problem~.  The difference between DQN and DDQN is that the greedy policy is evaluated using online network, but the target network is used to estimate its value.  Thus, $y_i$ is changed as follows\n\\begin{equation}\n\\label{eq:ddqn}\ny_i =  r + \\gamma Q \\big (s', \\argmax_{a'} \\, Q(s', a'; \\theta_i); \\theta_{i-1} \\big) .\n\\end{equation}\nAn interesting extension on top of DDQN is \\textbf{dueling network}~ whose idea is to have a single Q network with the same \\textit{convolutional layers} as DQN, but to have two streams of \\textit{fully connected} (FC) layers, which provide estimates for value and \\textit{advantage} functions.  This helps better generalize learning between actions.\nSecond, DQN uniformly selects experiences to replay regardless of their significance, which makes the learning process slow and inefficient.  Accordingly, \\textit{prioritized experience replay} was proposed to solve the problem~.   The idea is to replay important experiences more often, so as the network training is improved.  The importance of each transition is measured proportional to temporal difference error, and two variants, \\textit{stochastic prioritization} and \\textit{importance sampling}, are proposed to improve it.  Finally, DQN is not applicable in continuous spaces, so deep deterministic policy gradient (\\textbf{DDPG})~ was proposed, which is a combination of DQN and deterministic policy gradient (DPG)~ in an actor-critic approach.  Actor deterministically maps states to a specific action. Critic  defines the value of the action taken by actor.  In every iteration, critic is updated by\n\\begin{equation}\n\\label{eq:critic}\nL =  \\frac{1}{N}\\sum_i \\big (y_i - Q(s_i, a_i|\\theta^Q) \\big)^2\n\\end{equation}\nand actor is updated by\n\\begin{equation}\n\\label{eq:actor}\n\\nabla_{\\theta^\\mu} J =  \\frac{1}{N} \\sum_i \\nabla_a Q(s, a|\\theta^Q)|\\big(s=s_i, a=\\mu(s_i)\\big) \\nabla_{\\theta^\\mu} \\mu (s|\\theta^\\mu)|s_i,\n\\end{equation}\nwhere $\\theta^\\mu$ and $\\theta^Q$ are the parameters of actor and critic networks, respectively.\nFinally, proximal policy optimization (\\textbf{PPO})~ is another actor-critic algorithm used by RLRSs. In fact, PPO is an improved version of trust region policy optimization (TRPO)~ algorithm, which maximizes a \\textit{surrogate} objective\n\\begin{equation}\n\\mathbb{E}_t\\Big[ \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}A_t \\Big],\n\\end{equation}\nwhere $A_t$ is an estimator of advantage function at $t$.  The core idea in PPO is the introduction of \\textit{clipped surrogate objective}, as\n\\begin{equation}\n\\mathbb{E}_t\\Big[ \\text{min}\\Big(\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}A_t, \\text{clip}(\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}, 1-\\epsilon, 1+\\epsilon)A_t \\Big)\\Big],\n\\end{equation}\nwhere $\\epsilon$ is a hyperparameter.\n\\textbf{RL Challenges.} There are some possible challenges when applying RL to any problem.  A challenge well-known as \\textit{Deadly Triad} states that there is a \\textit{hazard} of instability and divergence when combining three elements in RL: function approximation, bootstrapping, and off-policy training~.   \n Another challenge in RL is \\textit{sample inefficiency}, specifically in model-free RL algorithms~.  Current model-free RL algorithms need a considerable amount of agent-environment interaction in order to learn useful states.  \n Moreover, since DRL is based on deep learning, it consequently inherits the famous feature of neural networks, i.e., being \\textit{black-box}.  It is not obvious how weights and activations are changed, which makes them uninterpretable.  The classical problem of exploration vs exploitation is still a challenge in RL and effective exploration is an open research problem. Finally, the problem of reward formulation in RL is a challenge and designing a good reward function is not very clear or straightforward. \n\\vspace{-8pt}", "cites": [1409, 1391, 620, 3586, 1408, 166, 2219], "cite_extract_rate": 0.28, "origin_cites_number": 25, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a clear and factual overview of RL and DRL concepts, referencing specific papers for algorithmic details. However, it primarily summarizes and defines methods without deeply connecting or synthesizing insights across the cited works. There is limited critical analysis or abstraction to broader patterns in the field."}}
{"id": "c4a63c3a-d945-48c8-9f01-562d0fe74a01", "title": "Why Reinforcement Learning for Recommendation?", "level": "subsection", "subsections": [], "parent_id": "5e197519-a3ce-4341-9b08-2c42799c85e8", "prefix_titles": [["title", "Reinforcement Learning based Recommender Systems: A Survey"], ["section", "Preliminaries"], ["subsection", "Why Reinforcement Learning for Recommendation?"]], "content": "\\label{subsec:why}\nThe nature of user interaction with an RS is sequential~ and the problem of recommending the best items to a user is not only a prediction problem, but a sequential decision problem~.  This suggests that the recommendation problem could be modelled as an MDP and be solved by RL algorithms.  Three unique features of RL make it a perfect match for the recommendation problem.  First, RL is able to handle the \\textit{dynamics} of sequential user-system interaction by adjusting actions according to continuous feedback received from the environment.  Second, RL is able to take into account the long-term user engagement with the system.  Finally, although having user ratings is beneficial, RL, by nature, does not need user ratings and optimizes its policy by sequentially interacting with the environment.  All these reasons suggest that it would be beneficial to use RL to provide better recommendations, as proven by online studies~.\n\\vspace{-8pt}", "cites": [5174, 5171, 5165], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the cited papers well by identifying three key reasons why RL is suitable for recommendation, drawing from the perspectives of dynamics, long-term engagement, and the absence of need for explicit user ratings. It abstracts these ideas into general principles applicable to the recommendation problem. However, it lacks deeper critical analysis of the cited works' limitations or trade-offs, and offers only a high-level justification rather than a nuanced evaluation."}}
{"id": "73df7fe1-622e-4d4c-895d-858bc7c1d8b3", "title": "Proposed RLRS Framework", "level": "subsection", "subsections": [], "parent_id": "5e197519-a3ce-4341-9b08-2c42799c85e8", "prefix_titles": [["title", "Reinforcement Learning based Recommender Systems: A Survey"], ["section", "Preliminaries"], ["subsection", "Proposed RLRS Framework"]], "content": "With carefully studying all RLRSs collected, we found that there are four components common in all of them and believe that a good RLRS should carefully design and address these components.  Accordingly,  to unify the process of RLRS development, we propose a framework for RLRSs with four key components: (1) State Representation, (2) Policy Optimization, (3) Reward Formulation, and (4) Environment Building.  Fig.~\\ref{fig:RLRS-comp} depicts this framework.  In the following, we explain each component.\n\\textbf{State Representation.} In the agent-environment RL interface, the state can be \\textit{any} information available to the agent.  State representation  could be as high-level as symbolic descriptions of objects in a room or as low-level as sensor readings~. What is important is that defined states should have the \\textit{Markov property}. That means, the state signal is not supposed to convey all the information about the environment to the agent, but it should summarize past information such that all relevant information is not missed.  A state signal with this property is called Markov.  In general, selecting state representation is currently more art than science~. \nIn RLRSs, state representation should summarize information about users, items, and the context. We divide state representation in RLRSs into three groups:\n\\textbf{SR1) Treating items as states. } When the item space is small, e.g., it includes several web pages in a website, it is possible to treat each item as a state. However, this approach is certainly not scalable when the item space grows large.  To tackle the scalability problem in larger items spaces, researchers found that states could indicate a set of items previously rated/consumed by the user.  Fig.~\\ref{fig:sr1} depicts this representation.\n\\textbf{SR2) Features from users, items, and context.} A popular way for state representation is to extract some features from users,  items, and context, as shown in Fig.~\\ref{fig:sr2}.  User features include demographic information, such as age, race, and gender.  Item features may include price, category, and popularity. Context features may include time, platform, and location.\n\\textbf{SR3) Encoded Embeddings.} For effective training, deep models in DRL-based RSs need states to be dense, low-dimensional vectors. Fig.~\\ref{fig:sr3} illustrates a general, popular framework for state representation in DRL-based methods.  Typically, first user, items, and context features are translated into dense, low-dimensional, continuous vectors called \\textit{embeddings.}  Then, for better training, this embedding could be encoded using a recurrent neural network (RNN) model, which can help the model learn user's sequential preferences~. Gated recurrent units (GRU) is typically more popular than long short-term memory (LSTM) for the RNN module as it has fewer parameters and can achieve the same or better performance~. To focus on important parts of the input, some researchers also use an attention layer in the encoding module and add some weights to the encoded vectors. Finally, the encoded vectors are concatenated to yield the final state.\n\\begin{figure}[t]\n\\centering     \n\\subfigure[]{\\label{fig:RLRS-comp}\\includegraphics[width=45mm]{RLRS-parts}}\n\\subfigure[]{\\label{fig:sr1}\\includegraphics[width=20mm]{SR1}}\\hspace{3mm}\n\\subfigure[]{\\label{fig:sr2}\\includegraphics[width=35mm]{SR2}}\\hspace{3mm}\n\\subfigure[]{\\label{fig:sr3}\\includegraphics[width=35mm]{SR3}}\n\\caption{(a) The proposed RLRS framework, (b) SR1, (c) SR2, (d) SR3. The dashed Encoding module in SR3 means that some models merely use input embeddings as states. }\n\\end{figure}\n\\textbf{Policy Optimization.} \nWhen states are formulated, it is the policy that determines which action to take (i.e., which items to recommend) in each state.  For policy optimization, various RL algorithms have been utilized by RLRSs. Before the advent of DRL, RL methods used by RLRSs could be generally classified into tabular and approximate methods.  Tabular methods include policy iteration, Q-learning, Sarsa,  Sarsa($\\lambda$), R-learning, and MCTS.  Approximate methods include fitted Q and gradient value iteration.  On the other hand, DRL methods could be generally divided into three groups: value-based (DQN), policy gradient (REINFORCE and REINFORCE-wb), and actor-critic (DDPG and PPO) methods.  A classification of these algorithms is shown in Fig.~\\ref{fig:RL-clas}.\n\\textbf{Reward Formulation.}  As mentioned earlier, the reward signal from environment reflects how good or bad the agent is performing through selecting actions.  Therefore, designing informative reward signal is critical for success/learning of the agent.  In fact, in RL, the reward signal is the only way to tell the agent \\textit{what} to do, not \\textit{how} to do it~.  In general, defining a proper reward function is a hard problem and it is more of a trial-and-error or engineering process.  There is no definite rule to design a good reward function in a specific problem. \nIn RLRSs, we have observed two general trends in designing the reward function: (\\textbf{R1}) the reward function is a simple, sparse numerical reward, or (\\textbf{R2}) the reward is a function of one or several observations from the environment. \n\\textbf{Environment Building.}  In general, evaluating RSs is difficult~.  As a result, building a suitable environment to properly train and evaluate the agent in RLRSs is challenging.  To better distinguish between different environment building methods, we generally divide them into three groups: \\textbf{offline}, \\textbf{simulation}, and \\textbf{online}.  In offline method, the environment is a static dataset containing the ratings of some users on some items.  A common practice in offline methods is to train the agent on the training data (usually 70-80$\\%$ of the data) and then test it on the remaining data. In simulation studies, usually a user model is built and the algorithm is evaluated while interacting with this user model.  This user model could be as simple as a user with some pre-defined behavior, or it could be more complex and be learnt using available data.  In online method, the algorithm is evaluated while interacting with real users and in real-time. This is the best, but most costly method for RLRSs evaluation.\n\\begin{table}\n\\scriptsize\n\\centering\n\\caption{Abbreviation Definition}\n\\pgfplotstabletypeset[\n    every head row/.style={before row=\\toprule,after row=\\midrule},\n    every last row/.style={after row=\\bottomrule},\n    col sep=ampersand,\n    row sep=\\\\,\n    columns={thing,mapsto,thing,mapsto},\n    display columns/0/.style={\n        select equal part entry of={0}{2},\n        string type,\n        column name={Abbreviation},\n        column type={l}, \n    },\n    display columns/1/.style={\n        select equal part entry of={0}{2},\n        string type,\n        column name={Definition},\n        column type={l},\n    },\n    display columns/2/.style={\n    \tselect equal part entry of={1}{2},\n    \tstring type,\n    \tcolumn name={Abbreviation},\n        column type={l}, \n    },\n    display columns/3/.style={\n    \tselect equal part entry of={1}{2},\n    \tstring type,\n        column name={Definition},\n        column type={l}, \n    },\n]{\n thing & mapsto \\\\\n AAR & Average achievable rate\\\\ \n ACN & Average click number per capita\\\\\n ACS & Average click per session\\\\\n ADS & Average depth per session\\\\\n AQ & Average quality\\\\\n AR & Average reward\\\\\n ART & Average return time\\\\\n AT & Average turn\\\\\n AWT & Average watched tag per capita\\\\\n BC & BookCrossing\\\\\n BDV & Book details viewed\\\\  \n BP & Number of books purchased\\\\\n CAC & Combined accuracy and coverage\\\\\n CATIE & Clinical antipsychotic trials of intervention effectiveness\\\\\n CFR & Charging failure rate\\\\\n CMU & Carnegie Mellon university\\\\\n CNN & Convolutional neural networks\\\\\n CR & Click reduction\\\\\n CTR & Click though rate\\\\\n CVR & Conversion rate\\\\\n DDPG & Deep deterministic gradient descent\\\\\n DDQN & Double DQN\\\\\n DHMM & Discriminative hidden Markov model\\\\\n DQN & Deep Q network\\\\\n DRL & Deep reinforcement learning \\\\\n Earn & Average earning\\\\\n EB & Environment building \\\\\n ED & Exponential decay score\\\\\n EMR & Entity matching rate\\\\         \n ES & Energy saving\\\\         \n FC & Fully connected\\\\\n FQI & Fitted Q iteration\\\\\n GANs & Generative adversarial networks\\\\\n GI & Gini index\\\\ \n GRU & Gated recurrent units\\\\\n GVI & Gradient value iteration\\\\\n HR & Hit ratio\\\\\n HRL & Hierarchical reinforcement learning\\\\\n HSP & Historical song playlist\\\\\n ILS & Intra-list similarity\\\\\n $k$-NN & $k$-nearest neighbors\\\\\n LFM & Last.fm\\\\ \n LRR & Low rank rate\\\\\n LSTM & Long short-term memory\\\\\n LTR & Listening time ratio\\\\ \n LTV & Life-time value\\\\\n MARL & Multi-agent reinforcement learning\\\\\n MAP & Mean average precision\\\\ \n MCP & Mean charging price\\\\\n MCTS & Monte Carlo tree search \\\\\n MCWT & Mean charging wait time\\\\\n MDP & Markov decision process\\\\\n MH & Miss-to-hit\\\\\n MIMIC & Multiparameter intelligent monitoring in intensive care\\\\\n MJC & Mean Jaccard coefficient\\\\\n ML & MovieLens\\\\\n MOOCs & Massive open online courses\\\\ \n MR & Miss ratio\\\\ \n MRR & Mean reciprocal rank\\\\\n MT & Movie tweetings\\\\\n NDCG & Normalized discounted cumulative gain\\\\\n NI & Number of user interactions before success\\\\\n NV & Number of violated attributes\\\\\n OU & Ornstein-Uhlenbeck\\\\\n P & Precision\\\\\n PA & Predictive ability\\\\\n PANSS & Positive and negative syndrome scale\\\\\n PI & Popularity rate\\\\\n PO & Policy optimization\\\\\n POI & Point-of-interest\\\\\n POMDP & Partially observable Markov decision process\\\\\n PPO & Proximal policy optimization\\\\\n PP30 & Passenger pickup in 30 minutes\\\\\n PR & Popularity rate\\\\\n QE & Queries executed\\\\\n R & Recall\\\\\n RA & Recommendation acceptance\\\\ \n RC & Recommendation score\\\\           \n RDR & Recommended download rate\\\\\n RF & Reward formulation\\\\ \n RL & Reinforcement learning \\\\\n RLRS & Reinforcement learning based recommender system \\\\\n RMSE & Root-mean-square error\\\\\n RP & Ranking percentile\\\\\n RQ & Recommendation quality\\\\\n RS & Recommender system\\\\ \n SAD & Session ad revenue\\\\ \n SDT & Session dwell time\\\\      \n SL & Session length\\\\  \n SG & Shortcut gains\\\\ \n SR & State representation\\\\ \n ST & Session time\\\\\n SuR & Success rate\\\\\n TSF & Total saving fee\\\\\n TPS & Taste profile subset\\\\\n TWIS & Truncated weighted importance sampling \\\\ \n UR & User rating\\\\\n UV CTR & Click ratio to user view\\\\\n VCT & Vacant cruising time\\\\ \n WI & Wage improvement\\\\   \n WPF & Weighted proportional fairness \\\\\n WQR & Wrong quite rate\\\\ \n WT & Waiting time\\\\\n YM & Yahoo music\\\\\n YC & YooChoose\\\\\n }\n\\end{table}\n\\vspace{-8pt}", "cites": [39, 5169], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a structured analytical overview of the RLRS framework by integrating key components from the cited papers into a unified structure. It synthesizes the concepts of state representation, policy optimization, reward formulation, and environment building effectively. While it briefly critiques the scalability issue of treating items as states and acknowledges the more engineering-driven nature of reward design, deeper comparative analysis or limitations of specific approaches are not fully explored."}}
{"id": "de5078b0-ad20-4444-a43e-028d3fdae6cc", "title": "RL-based RSs", "level": "subsection", "subsections": ["f3d94c9d-55a4-4cd9-ab36-e33e421222b8", "224b1104-16b5-41de-814d-a9ebbc2115c3", "bf95df49-bf75-4c2f-88b2-d460aca3dd1e", "8546a687-96e8-469b-a4ff-4de543cb76ad"], "parent_id": "cf18c921-76fc-427b-918d-fc545685760a", "prefix_titles": [["title", "Reinforcement Learning based Recommender Systems: A Survey"], ["section", "Reinforcement Learning based Recommender Systems Algorithms"], ["subsection", "RL-based RSs"]], "content": "In this section, we present RL-based RSs; i.e., methods that do not use deep learning for policy optimization.  Table~\\ref{tab:rl-based} provides a quick overview on RL-based methods.\n\\begin{table}[t]\n\\caption{RL-based Methods}\n\\centering\n\\scriptsize\n\\begin{adjustwidth}{-.5cm}{}\n\\begin{tabular}{ l c  c  c  c  c  c  c  c}\n\\hline\n\\textbf{RLRS} & \\textbf{Year}& \\textbf{SR} & \\textbf{PO} & \\textbf{RF} & \\textbf{EB} &  \\textbf{Metrics} & \\textbf{Dataset} & \\textbf{Application}* \\\\ \n\\hline\nWebWatcher~ & 1997 & SR1 & Q-learning & R1 & Offline & Accuracy & CMU & Web\\\\\nPreda et al.~ & 2005 & SR1 & Sarsa($\\lambda$) & R1 & Online & RDR & N/A & Web\\\\\nMDP~ & 2005 & SR1 & Policy iteration & R2 & Online & RC, ED & Mitos & E-commerce \\\\\nTaghipour et al.~ & 2007 & SR1 & Q-learning & R2 & Offline & Accuracy, Coverage, SG & DePaul & Web \\\\\nMahmood et al.~ & 2007 & SR2 & Policy iteration & R1 & Simulation & N/A & N/A & Trip  \\\\\nTaghipour et al.~ & 2008 & SR1 & Q-learning & R2 & Offline & HR, PA, CR, RQ & DePaul & Web  \\\\\nMahmood et al.~ & 2009 & SR2 & Policy iteration\t& R1 & Online & 26 variables & N/A & Trip  \\\\\nAPG~ & 2010 & SR1 & Q-learning, Sarsa & R2 & Simulation, Online & MR, MH, LTR, UR & N/A & Music  \\\\\nShortreed et al.~ & 2011 & SR2 & FQI & R2 & Offline & PANSS & CATIE & Health  \\\\\nZhao et al.~ & 2011 & SR2 & Fitted Q & R2 & Simulation & Survival & N/A & Health  \\\\\nRLradio~ & 2012 & SR2 & R-learning & R1 & Online & Listening Time & N/A & Radio channel  \\\\\nMahmood et al.~ & 2014 & SR2 & Q-learning & R1 & Simulation, Online & BP, ST, BDV, QE & Amazon & Trip \\\\\nDJ-MC~ & 2014 & SR2 & MCTS & R2 & Simulation, Online & Reward & Million Song & Music \\\\\nTheocharous et al.~ & 2015 & SR2 & FQI & R1 & Offline & CTR, LTV & N/A & Ad  \\\\\nPOMDP-Rec~ & 2016 & SR2 & Fitted Q & R2 & Offline & RMSE & ML1M, YM &  \\\\\nRLWRec~ & 2017 & SR1 & Q-learning & R2 & Offline & CAC & N/A & Music  \\\\\nZhang et al.~ & 2017 & SR1 & GVI & R2 & Offline & MRR, P, NDCG & ACM & Collaborator  \\\\\nChoi et al.~ & 2018 & SR1 & Q-learning, Sarsa & R2 & Offline & P, R& ML100K, ML1M & \\\\\nIntayoad et al.~ & 2018 & SR1 & Sarsa\t& R1 & Offline & RMSE & N/A & E-learning\\\\\nRPRMS~ &2019& SR3 & Q-learning & R1 & Offline & UR & N/A & Music  \\\\\nCAPR~ &2019& SR2 & MCTS & R2 & Offline & P, R, F1 & Weeplace & POI \\\\\nPHRR~ &2020& SR3 & MCTS\t& R2 & Offline &  HR, F1 & Million Song, TPS, HSP & Music  \\\\\nKokkodis et al.~ &2021& SR1 & Q-learning & R2 & Offline & Market Revenue, WI& N/A & Career path\\\\\n\\hline\n\\multicolumn{9}{l}{\\footnotesize \\textbf{Note:} There is no code shared by RL-based methods.}\\\\\n\\multicolumn{9}{l}{\\footnotesize * The application domain is either explicitly mentioned or implicit in respective publications. An empty Application column means the application}\\\\\n\\multicolumn{9}{l}{domain is not clear.}\n\\end{tabular}\n\\label{tab:rl-based}\n\\end{adjustwidth}\n\\end{table}", "cites": [5163, 5171, 5176, 5175], "cite_extract_rate": 0.17391304347826086, "origin_cites_number": 23, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily functions as a descriptive overview of RL-based recommender systems by listing various methods and their attributes in a table. While it organizes information systematically (e.g., years, algorithms, application domains), it lacks synthesis of ideas across papers and deeper analysis of their contributions or limitations. There is minimal abstraction or generalization beyond the specific systems described."}}
{"id": "f3d94c9d-55a4-4cd9-ab36-e33e421222b8", "title": "State Representation", "level": "subsubsection", "subsections": [], "parent_id": "de5078b0-ad20-4444-a43e-028d3fdae6cc", "prefix_titles": [["title", "Reinforcement Learning based Recommender Systems: A Survey"], ["section", "Reinforcement Learning based Recommender Systems Algorithms"], ["subsection", "RL-based RSs"], ["subsubsection", "State Representation"]], "content": "As illustrated in Table~\\ref{tab:rl-based}, apart from RPRMS and PHRR, all RL-based methods belong to either SR1 or SR2, which share almost the same proportion (see Fig.~\\ref{fig:rl-sr}).  As mentioned earlier, methods in SR1 use all or a set/tuple of items for state representation. For example, WebWatcher~, the first RLRS we identified, treats each item (i.e., web page) as a state in a web recommendation scenario.  Similarly, Refs.~ and~ treat each author and learning object as a state in scientific collaborator recommendation and e-learning scenarios, respectively.  As stated earlier, while this approach is possible in small state spaces, it is certainly not scalable when the item space grows large.  Researchers found that keeping the track of a small set of items already rated/consumed by the user could be informative enough for policy optimization.  Perhaps Refs.~ are the first RLRSs utilizing this idea, but the idea is better formalized for RLRSs by Ref.~.  Specifically, in a web recommendation application, Taghipour and Kardan~ borrow the \\textit{N-gram} model from the \\textit{web usage mining} literature~ and introduce a \\textit{sliding window} to represent states, depicted in Fig.~\\ref{fig:taghi}.  In this figure, circles are states, right arrows are actions, and $V$ and $R$ indicate visited and previously recommended pages, respectively.  While using this model, the authors assume that knowing the last $k$ pages visited by the user provides enough information to predict their future page requests.  \nIt is noteworthy to mention that this set or sliding window in SR1 could indicate any useful information for the purpose of policy optimization, including a set of commercial items~, concepts in a website~, emotion classes of songs~, skills~, and music songs~.  In a different setting, Choi et al.~ formulate the recommendation problem as a \\textit{gridworld game} and each grid cell, with its users and items inside, is considered as a state. \n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=.8\\linewidth]{slide-window.pdf}\n\\caption{The concept of sliding window used in~ for state representation}\n\\label{fig:taghi}\n\\end{figure}\n\\begin{figure}[t]\n\\centering     \n\\subfigure[State Representation]{\\label{fig:rl-sr}\\includegraphics[width=30mm]{RL-SR}}\n\\subfigure[Reward Formulation]{\\label{fig:rl-rew}\\includegraphics[width=30mm]{RL-rew}}\n\\subfigure[Environment Building]{\\label{fig:rl-eval}\\includegraphics[width=30mm]{RL-eval}}\n\\subfigure[Policy Optimization]{\\label{fig:rl-al}\\includegraphics[width=50mm]{RL-al}}\n\\caption{The summary of four components of RLRS framework in RL-based methods}\n\\end{figure} \nOther researchers have proposed to extract some features from user, items, and context and use them for state representation (SR2).  Among the first attempts in SR2 is Mahmood et al. works~ in which a set of variables from user (e.g., the number of times the user has modified his query), agent (e.g., previous action of the agent), and interaction session (e.g., the number of episodes elapsed) are used for state representation.  A similar approach is used in RLradio~ where some variables, containing information about radio channels of user interest and their listening behavior, are defined to represent states.  DJ-MC~ uses an encoding method to represent each song as a vector of song descriptors and each state is the concatenation of $k$ songs vectors in the playlist.  Features from weather conditions and time are used in CAPR~ for state representation in a point of interest (POI) recommender.  SR2 is specifically popular in healthcare applications in which information about patients are typically recorded by several descriptive features~.\n  In a different setting, POMDP-Rec~ formulates states as belief states using \\textit{low-dimensional factor model}~. More precisely, with a partially observed user-item matrix, user's behavior observations (O), items' latent features (V), and users' latent interests (U) can be calculated as \n\\begin{equation}\np(\\text{O}|\\text{U}, \\text{V}, \\sigma^2) = \\prod_{i=1} \\prod_{j=1}\\big(\\pazocal{N} (\\text{O}_{ij}|\\text{U}_i^{\\top} \\text{V}_j, \\sigma^2 )\\big)^{I_{ij}},\n\\end{equation}\n\\begin{equation}\np(\\text{U}|\\sigma^2_\\text{U}) = \\prod_{i=1} \\pazocal{N} (\\text{U}_{i}|0, \\sigma^2_\\text{U} I),\n\\end{equation}\n\\begin{equation}\np(\\text{V}|\\sigma^2_\\text{V}) = \\prod_{j=1} \\pazocal{N} (\\text{V}_{j}|0, \\sigma^2_\\text{V} I),\n\\end{equation}\nwhere $\\pazocal{N}(x|\\mu, \\sigma^2)$ is the probability density function of Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$, $I_{ij}\\in \\{0, 1\\}$, and $I_{ij}=1$ means that user $i$ has rated item $j$. A belief state is then a concatenation of U and V, i.e., $b_{ij}=(\\text{U}_i, \\text{V}_j)$. \nThe only works in RL-based methods that lie in SR3 are RPRMS~ and PHRR~, both are in music recommendation domain. In RPRMS, states are a concatenation of songs lyrics embeddings, generated using Word2Vec~, and audio embeddings, generated using a pre-trained WaveNet model~.  PHRR uses \\textit{weighted matrix factorization}~ and CNN to embed songs, and similar to DJ-MC, each state is the concatenation of several song vectors.", "cites": [5163, 5171, 4722, 1684, 5176, 5175], "cite_extract_rate": 0.2222222222222222, "origin_cites_number": 27, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key approaches to state representation in RL-based recommender systems by grouping methods into SR1, SR2, and SR3 and explaining their rationale and application domains. It provides some critical insight by highlighting scalability issues with item-based representations and the evolution of more efficient feature-based approaches. While it identifies patterns (e.g., the use of N-gram, sliding windows, and latent factors), it does not fully abstract to a meta-level framework or provide deep evaluative comparisons across all cited methods."}}
{"id": "224b1104-16b5-41de-814d-a9ebbc2115c3", "title": "Policy Optimization", "level": "subsubsection", "subsections": [], "parent_id": "de5078b0-ad20-4444-a43e-028d3fdae6cc", "prefix_titles": [["title", "Reinforcement Learning based Recommender Systems: A Survey"], ["section", "Reinforcement Learning based Recommender Systems Algorithms"], ["subsection", "RL-based RSs"], ["subsubsection", "Policy Optimization"]], "content": "According to Fig.~\\ref{fig:rl-al}, temporal difference methods, i.e., Q-learning and Sarsa, have been the most popular RL algorithms among RL-based methods~. The main reason of this popularity is their simplicity; that is, they are online, model-free, need minimal amount of computation, and can be expressed by a single equation (see Eqs.~\\eqref{eq:ql} and~\\eqref{eq:sarsa})~.  Applying Q-learning/Sarsa for policy optimization is quite straightforward and need not any specific modification. Researchers in~ use a simple trick to have a decreasing learning rate $\\alpha=1/1+$visits$(s,a)$ in Eq.~\\eqref{eq:ql}, which helps algorithm convergence.  This trick is also used in~.  A problem with temporal difference methods, like any tabular RL method, is that they lead to the \\textit{curse of dimensionality}~.  To tackle this problem, as discussed earlier, researchers try to manage the state space and keep it small enough. \nAmong the tabular methods, dynamic programming methods are usually impractical due to their great computational expense and the need to perfect knowledge about the environment.  While these algorithms are polynomial in the number of states, performing even one iteration of policy or value iteration methods is often infeasible~.   To make it practical, Ref.~ uses a couple of features in their state space and makes some approximations. For instance, one feature of state space in~ is directionality; the authors argue that a short state cannot follow a long state or the probability of occurring loops in their MDP is not very high.  Moreover, Ref.~ keeps the number of policy iteration run to a limited number.\nMCTS is a decision-time planning algorithm that benefits from online, incremental, sample-based value estimation and policy improvement~ and has been employed by~.  In order to facilitate the agent learning, in case the song space is very large or search time is limited, DJ-MC~ clusters songs according to song types and then applies MCTS to clustered songs.   PHRR~ adopts similar schemes in policy optimization and song clustering. Similar to AlphaGo~, CAPR~ uses UCT (Upper Confidence Bound applied to  Trees)~ to solve the exploration vs exploitation trade-off in the selection step of MCTS (see section~\\ref{subsec:RL}).\nPreda and Popescu~ use Sarsa($\\lambda$) with \\textit{tile coding}~ linear approximation for policy optimization.  To be able to apply Sarsa($\\lambda$), the work transforms epistemic information into arrays of real numbers.  Moling et al.~ define the problem of optimal radio channel recommendation as a continuous task and then employ R-learning~ to solve it.\nOn the other hand, some RL-based RSs have used approximate methods for policy optimization, including fitted  Q~ and gradient value iteration~.  Fitted Q is a flexible framework that can fit any approximation architecture to Q-function~.  Accordingly, any batch-mode supervised regression algorithms can be used to approximate the Q-function, which can scale well to high dimensional spaces~.  However, one problem with this method is that it could have a high computational and memory overhead with the increase in the number of \\textit{four-tuples} $(x_t , u_t , r_t , x_{t +1} )$, where $x_t$ indicates the system state at time $t$, $u_t$ the control action taken, $r_t$ the immediate reward, and $x_{t+1}$ the next state of the system~.  This algorithm has been used by several RLRSs~.  To fit the Q function in these methods, linear regression~, support vector regression~, and neural networks~ are used.  Finally, Zhang et al.~ introduce a gradient descent version of value iteration algorithm for collaborator recommendation in a multi-agent RL setting.", "cites": [5163, 5171, 5176, 5175], "cite_extract_rate": 0.13793103448275862, "origin_cites_number": 29, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of policy optimization methods in RL-based RSs, connecting concepts such as tabular methods, MCTS, and approximate techniques. It synthesizes information by discussing common approaches (e.g., decreasing learning rate, state space reduction) and how different papers address scalability and performance issues. While it identifies limitations like the curse of dimensionality and computational overhead, it does not offer a deeper comparative or evaluative framework, limiting the level of abstraction and critical insight."}}
{"id": "bf95df49-bf75-4c2f-88b2-d460aca3dd1e", "title": "Reward Formulation", "level": "subsubsection", "subsections": [], "parent_id": "de5078b0-ad20-4444-a43e-028d3fdae6cc", "prefix_titles": [["title", "Reinforcement Learning based Recommender Systems: A Survey"], ["section", "Reinforcement Learning based Recommender Systems Algorithms"], ["subsection", "RL-based RSs"], ["subsubsection", "Reward Formulation"]], "content": "Fig.~\\ref{fig:rl-rew} shows that R2, with $60\\%$ proportion, has been more popular than R1, with 40$\\%$, among RL-based methods.  In R1, different numerical values have been used for the immediate reward. For instance, Mahmood et al. use +1 in terminal state and a negative number otherwise~, + 5 for adding a product to travel plan, +1 for showing result page, 0 otherwise~, and +100 for buying a book, -30 for user quit, and 0 otherwise~.  On the other hand in R2, researchers have proposed to use different observations from the environment to formulate the reward, including net profit~, overall survival time~, some clinical scores (i.e., PANSS)~, and Jaccard distance between two states~.", "cites": [5163, 5171], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of reward formulation approaches from two cited papers but lacks synthesis, critical evaluation, or abstraction. It merely lists examples of reward values and formulations without connecting them to broader trends, discussing their implications, or comparing their effectiveness."}}
{"id": "8546a687-96e8-469b-a4ff-4de543cb76ad", "title": "Environment Building", "level": "subsubsection", "subsections": [], "parent_id": "de5078b0-ad20-4444-a43e-028d3fdae6cc", "prefix_titles": [["title", "Reinforcement Learning based Recommender Systems: A Survey"], ["section", "Reinforcement Learning based Recommender Systems Algorithms"], ["subsection", "RL-based RSs"], ["subsubsection", "Environment Building"]], "content": "It is observable from Fig.~\\ref{fig:rl-eval} that the dominant environment building method in RL-based RSs is offline.  This makes sense as training the agent and testing the performance on an available dataset is the easiest and safest option.  Two popular datasets used in RL-based RSs are MovieLens~ and Million Song dataset~.  \nAnother environment building method is simulation, which is a safe method to fine-tune important model parameters before system deployment or an online study.  Simulation could be as simple as assuming constant users with predefined preference patterns~, or it could be more complex and learn user behavior through available data~. For example, in a supervised learning task, Mahmood et al.~ define a user behavior model and use it in a travel RS, called NutKing, to learn transition probabilities.  The aim is to know how the simulated user reacts to a certain action of the system. \nOnline study is the most effective, but costly environment building method for RLRSs.  In an early, valuable attempt~, the performance of the proposed MDP-based RS is evaluated in a two-year online study conducted on an online book store.  The study has had a good daily exposure to users, almost 5000-6000 different users daily, with a reasonable number of items to recommend (over 15,000), compared to other online studies performed by RL-based methods with only five~, 13~, 47~, 469~, and 500~ users. \n\\vspace{-8pt}\n\\begin{table}[htbp]\n\\caption{DRL-based Methods}\n\\centering\n\\scriptsize\n\\begin{tabular}{ l c c  c  c  c  c  c  c  c }\n \\hline\n \\textbf{RLRS} & \\textbf{Year} & \\textbf{SR} & \\textbf{PO} & \\textbf{RF} & \\textbf{EB} & \\textbf{Metrics} &  \\textbf{Dataset} & \\textbf{Application} & \\textbf{Code} \\\\ \n \\hline\n Slate-MDP~ & 2015 & SR1 & DQN & R1 & Simulation & Reward & N/A & & \\\\\n Wolpertinger~ & 2015 & SR1 & DDPG & R1 & Simulation & Reward & N/A & &  \\\\\n Nemati et al.~ & 2016 & SR2 & DQN & R2 & Offline & Reward & MIMIC & Healthcare & \\\\\n CEI~ & 2017& SR3 & REINFORCE-wb & R1 & Simulation & P, F1, BLEU, Reward & ML1M, MT & & \\\\\n Raghu et al.~ &2017& SR2 & Dueling DDQN & R2 & Offline & Mortality Rate & MIMIC & Healthcare & \\cmark \\\\\n DEERS~ & 2018 & SR3 & DQN & R1 & Offline, Simulation & MAP, NDCG & JD & E-commerce &  \\\\ \n Robust DQN~ & 2018 & SR2 & DDQN & R2 & Online & CTR, UV CTR & Taobao & E-commerce &  \\\\\n SRL-RNN~ & 2018 & SR3 & DDPG & R1 & Offline & Mortality Rate, MJS & MIMIC & Healthcare & \\cmark \\\\ \n DRN~& 2018 & SR2 & Dueling DDQN & R2 & Offline, Online & CTR, NDCG, P, ILS & N/A & News & \\\\\n Deep Page~ & 2018 & SR3 & DDPG & R1 & Offline, Simulation & P, R, F1, NDCG, MAP & N/A & E-commerce &  \\\\\n CRM~ & 2018 & SR3 & REINFORCE & R2 & Simulation, Online & Reward, SuR, AT, WQR, LRR & Yelp & E-commerce&\\\\ \n Munemasa et al.~ & 2018 & SR2 & DDPG & R2 & Offline & MRR, R  & Ekiten & Store &  \\\\\n CapDRL~ & 2019 & SR3 & DDPG & R1 & Offline & P, NDCG & ML100K, ML1M & Movie &  \\\\\n FeedRec~ & 2019& SR2 & DQN & R2 & Offline, Simulation  & ACS, ADS, ART & N/A & Feed streaming & \\\\\n DRCGR~ & 2019&SR3 & DQN & R1 & Offline & NDCG, MAP & N/A & E-commerce &  \\\\ \n LIRD~ & 2019&SR3 & DDPG & R1 & Simulation & MAP, NDCG & N/A  & E-commerce &  \\\\\n SlateQ~ & 2019& SR2 & DQN & R2 & Simulation, Online & Reward, AQ & N/A & &  \\\\\n Yu et al.~ &2019& SR3 & Actor-critic & R2 & Simulation & SuR, NI, NV, RP & UT-Zappos50K & E-commerce &  \\\\\n Tsumita~ &2019& SR3 & DQN & R1 & Simulation & AT, SuR & PDS & Restaurant &  \\\\\n Zhang et al.~ &2019& SR3 & REINFORCE & R2 & Offline & HR, NDCG & XuetangX & E-learning & \\cmark \\\\\n DRESS~ &2019& SR3 & PPO & R1 & Offline & CVR, CTR, TWIS  & JD & E-commerce &  \\\\\n Liu et al.~ &2019& SR2 & Dueling DDQN & R2 & Simulation & AAR & N/A & Content &  \\\\\n Yuyan et al.~ &2019& SR3 & DQN & R1 & Offline & RMSE & ML100K, ML1M & Movie &  \\\\\n CROMA~ &2019& SR3 & DDPG & R2 & Offline & P, R, F1, MRR & Twitter & Tweet & \\cmark \\\\\n REINFORCE~ &2019& SR3 & REINFORCE & R1 & Simulation, Online & ViewTime & N/A &  &  \\\\\n PCR~ &2019& SR3 & REINFORCE-wb & R1 & Simulation & SuR, NI, NV & UT-Zappos50K & E-commerce &  \\\\\n UDQN~ &2019& SR2 & DQN & R1 & Offline & Reward & ML100K, ML1M, YM & &  \\\\\n IRecGAN~ &2019& SR3 & REINFORCE & R1 & Simulation & P & CIKM & & \\cmark \\\\\n Den et al.~ &2019& SR2 & Mixed & R2 & Simulation & Reward & N/A  & &  \\\\\n KGRE-Rec~ &2019& SR3 & REINFORCE-wb & R2 & Offline & P, R, HR, NDCG & Amazon & E-commerce & \\cmark \\\\\n TPGR~ &2019& SR3 & REINFORCE & R2 & Simulation & Reward, P, R, F1 & ML10M, Netflix  & & \\cmark \\\\\n Div-FMCTS~ &2019& SR3 & MCTS & R2 & Offline & F1, NDCG & ML & & \\cmark \\\\\n Cascading DQN~ &2019& SR3 & DQN & R2 & Simulation & CTR, Reward & ML, LFM, Yelp, YC, AFN & &  \\\\\n Ekar~ &2019& SR3 & REINFORCE & R1 & Offline & HR, NDCG & ML1M,LFM, DBbook2014 & &  \\\\ \n Pseudo Dyna-Q~ &2020& SR3 & DQN & R2 & Simulation& Click, Diversity, Horizon & Taobao, RetailRocket&E-commerce&\\cmark\\\\\n SADQN~ &2020& SR3 & DQN & R1 & Offline & HR, NDCG & LFM, Ciao, Epinions & &  \\\\\n Zhao et al.~ &2020& SR3 & Duleing DQN & R2 & Offline & SDT, SL, SAD & TikTok & Ad &  \\\\ \n Ji et al.~ &2020& SR2 & REINFORCE & R2 & Simulation & Earn, VCT, WT, PP30 & NY, SF data & Taxi route &  \\\\\n recEnergy~ &2020& SR2 & DQN & R2 & Online & RA, ES & N/A & Energy optimization &  \\\\\n GCQN~ &2020& SR3 & DQN & R1 & Offline  & Reward & ML1M, LFM, Pinterest & & \\\\  \n MaHRL~ &2020& SR3 & DDPG & R2 & Offline, Simulation & MAP, NDCG, Reward & N/A & E-commerce &  \\\\\n DRR~ &2018, 2020& SR3 & DDPG & R2 & Offline, Simulation & P, NDCG, Reward & ML1M, ML100K, YM, Jester& &\\\\\n FairRec~ &2020& SR3 & DDPG & R2 & Offline & CVR, WPF & ML100K, Kiva  & &  \\\\\n EAR~ &2020& SR3 & REINFORCE & R1 & Simulation & SuR, AT & Yelp, LFM & & \\cmark \\\\ \n MASSA~ &2020& SR3 & DDPG & R2 & Offline, Simulation & P, NDCG & Taobao & E-commerce &  \\\\\n DeepChain~ &2020& SR3 & DDPG & R1 & Offline, Simulation & MAP, NDCG, Reward & JD & E-commerce &  \\\\\n KGPolicy~ &2020& SR3 & REINFORCE-wb & R2 & Offline & R, NDCG & Amazon, LFM, Yelp & & \\cmark \\\\\n KGRL~ &2020& SR3 & DDPG & R2 & Offline & P, R, NDCG & 6 datasets & &  \\\\\n CRSAL~ &2020& SR3 & Actor-critic & R2 & Offline & BLEU, ROUGE, EMR, SuR & DSTC2, CR676, MultiWOZ & Restaurant &  \\\\ \n CPR~ &2020& SR3 & DQN & R1 & Simulation & SuR, AT & Yelp, LFM & & \\cmark \\\\ \n Xin et al.~ &2020& SR3 & DDQN, Actor-critic & R1 & Offline & HR, NDCG & YC, RetaiRocket  & &  \\\\\n ADAC~ &2020& SR3 & Actor-critic & R1 & Offline & P, R, NDCG, HR & Amazon  & E-commerce &  \\\\  \n KERL~ &2020& SR3 & REINFORCE & R2 & Offline & HR, NDCG & Amazon, LFM & &  \\\\\n DRprofiling~ &2020& SR3 & REINFORCE & R2 & Offline & P, R, HR & ML, LFM & &  \\\\ \n KGQR~ &2020& SR3 & DQN & R2 & Simulation & Reward, P, R & Data-Crossing, ML20M & &  \\\\\n Singh et al.~&2020 & SR3 & REINFORCE & R2 & Simulation & Reward, Risk & ML1M & &  \\\\  \n EDRR~ &2020& SR3 & DQN, DDPG & R2 & Offline, Simulation & P, NDCG, MAP, Reward& ML1M, Jester  & &  \\\\\n SRR~ &2020& SR3 & DQN, DDPG & R2 & Offline, Simulation & P, NDCG, Reward & ML1M, ML100K, BC, Jester & &  \\\\\n D$^2$RLIR~ &2021& SR3 & DDPG & R2 & Offline & NDCG, P, Diversity & ML1M & &  \\\\\n DRGR~ &2021& SR3 & DDPG & R1 & Simulation & R, NDCG & ML & & \\cmark \\\\ \n FCPO~ &2021& SR3 & DDPG & R1 & Offline  & R, F1, NDCG, GI, PR & ML1M, ML100K & & \\cmark \\\\\n DHCRS~ &2021& SR3 & DQN & R2 & Offline & HR, NDCG & ML1M, 10M, 20M, Netflix & &  \\\\\n DARL~ &2021& SR3 & REINFORCE & R2 & Offline & HR, NDCG & MOOCCourse, MOOCCube & E-learning &  \\\\\n MKRLN~ &2021& SR3 & REINFORCE-wb & R1 & Offline & P, NDCG & Book, MOvie, KKBOX & &  \\\\ \n MASTER~ &2021& SR2 & DDPG & R2 & Offline & MCWT, MCP, TSF, CFR & Beijing, Shanghai Data & Charging spot &  \\\\\n AnchorKG~ &2021& SR3 & Actor-critic & R2 & Offline & P, R, NDCG, HR & MIND, BingNews & News & \\cmark \\\\\n UNICORN~ &2021& SR3 & Dueling DDQN & R1 & Simulation & SuR, AT, NDCG & LFM, Taobao, Yelp & &  \\\\\n HRL-Rec~ &2021& SR3 & DDPG & R2 & Offline, Online & CTR, ACN, AWT & WeChat &  & \\cmark \\\\\n GoalRec~ &2021& SR3 & Dueling DDQN & R2 & Simulation, Online & CTR, Reward & ML25M, Taobao, YC &  &  \\\\\n DEAR~ &2021& SR3 & Dueling DQN & R2 & Offline & Reward & Douyin & Ad &  \\\\\n VPQ~ &2021& SR3 & Actor-critic & R1 & Offline  & HR, NDCG & RetailRocket, YC & &  \\\\ \n SDAC~ &2021& SR2 & Actor-critic & R2 & Offline  & HR, NDCG & YC, Kaggle & &  \\\\  \n URL~&2021& SR3 & REINFORCE & R1 & Offline, Online  & MAP & N/A & &  \\\\\n \\hline\n\\end{tabular}\n\\label{tab:drl-based}\n\\end{table}", "cites": [5171, 5165, 5168, 5181, 3724, 8883, 5182, 5187, 5194, 5170, 3604, 5188, 5189, 5191, 5195, 5175, 5179, 5193, 5192, 5198, 5180, 5190, 5196, 8884, 5197, 5184, 5177, 5169, 5178, 5183, 5185, 5186, 5199], "cite_extract_rate": 0.39285714285714285, "origin_cites_number": 84, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes various RL-based recommender system methods and their environment building approaches, with a table listing the models, datasets, and metrics. It offers minimal synthesis by grouping methods under simulation, offline, and online, but does not deeply integrate or contextualize the cited papers. There is little critical analysis of limitations or comparative evaluation, and abstraction is limited to surface-level observations about method popularity."}}
{"id": "a552440a-0cf0-44d8-b2a3-565d025f9f24", "title": "State Representation", "level": "subsubsection", "subsections": [], "parent_id": "47054969-9b91-489f-afb7-b8069246a898", "prefix_titles": [["title", "Reinforcement Learning based Recommender Systems: A Survey"], ["section", "Reinforcement Learning based Recommender Systems Algorithms"], ["subsection", "DRL-based RSs"], ["subsubsection", "State Representation"]], "content": "As depicted in Fig.~\\ref{fig:drl-sr}, SR3 is the dominant state representation scheme for DRL-based RSs. As stated earlier, this is because deep models are trained more effectively on dense, low-dimensional vectors.  Nonetheless, researchers have taken one step further and tried to make the general framework of SR3 (see Fig.~\\ref{fig:sr3}) more effective.  Typically, in RLRSs, items \\textit{positively} rated by the user are considered as the preferences of the user. However, in DEERS~, authors discuss that the proportion of negative feedback, e.g., skipped items, could be much larger than the positive one, so they propose to have two states: positive and negative states.  Fig.~\\ref{fig:DEERS} illustrates this modification.  In particular, the input is divided into items with positive and negative feedbacks, are passed through embedding and RNN layers, and fed into Q network where they are concatenated.  This technique has also inspired other researchers~.  Instead of RNN layer, DRCGR~ uses a convolution layer (with both horizontal and vertical kernels) to encode the embeddings of positive feedbacks.  On the other hand, a generative adversarial network (GAN) module is trained to generate negative samples.  Deep Page~ also extends the SR3 framework by adding a CNN module between the embedding and RNN layers, in order to learn item spatial display scheme in a page-wise recommendation scenario.  Before passing the item embeddings through the CNN module, a page layer is used to convert item embeddings into a 2D grid/matrix for 2D CNN processing.\nMoreover, authors in~ propose to use a \\textit{position weighting} scheme for state embedding.  Formally, if $W$ is a matrix with historical steps as rows and importance weight of positions as columns, the embedding of a state $s^t$ can be defined as\n\\begin{equation} \ns^t=h(F^{t-m:t-1})=vec \\big[\\sigma(F^{t-m:t-1}W+B) \\big],\n\\end{equation}\nwhere $F$ is the feature vector of the history with $m$ steps, $B$ is a bias matrix, $\\sigma(\\cdot)$ is a nonlinear activation, \nand $vec[\\cdot]$ concatenates the matrix columns. The authors claim that this method for state embedding is more efficient for optimization than LSTM.  Finally, in D$^2$RLIR~, a \\textit{positional encoding} is added to state embeddings so that the model understands the chronological order of items.\n\\begin{figure}\n\\centering     \n\\subfigure[]{\\label{fig:DEERS}\\includegraphics[width=45mm]{DEERS2}}\\hspace{1cm}\n\\subfigure[]{\\label{fig:dqn1}\\includegraphics[width=35mm]{DQN-A1}}\\hspace{1cm}\n\\subfigure[]{\\label{fig:dqn2}\\includegraphics[width=35mm]{DQN-A2}}\n\\caption{(a) The architecture of DEERS~, (b) Q network A1 architecture, (c) Q network A2 architecture}\n\\label{fig:dqn}\n\\end{figure}\nIn DRR~, an individual module called \\textit{state representation module} is proposed for the purpose of state formulation.  Authors propose three structures to model the interactions between user and items.  The first structure, DRR-p, simply concatenates the embeddings of items and their pairwise products, as depicted in Fig.~\\ref{fig:drr-p}.  More formally, if $H=\\{ v_1, v_2, ..., v_n \\}$ is the positive interaction history of the user and\n\\begin{equation}\n P=\\{w_i v_i \\otimes w_j v_j|i,j=1,2, ..., n\\}\n\\end{equation}\nis the weighted pairwise product between items, then state $S$ is defined as the concatenation of $H$ and $P$, i.e., $S=(H, P)$.  In the second structure, DRR-u, the user embedding is also incorporated (shown in Fig.~\\ref{fig:drr-u}). That means, with \n\\begin{equation}\nK=\\{u \\otimes w_i v_i | i = 1, 2, ..., n \\},\n\\end{equation}\n$S = (K, P)$.  In the last structure illustrated in Fig.~\\ref{fig:drr-ave}, DRR-ave, an average pooling layer is introduced to eliminate the items' \\textit{position bias} in the recommended list. In particular, if \n\\begin{equation}\nG=\\{ave(w_i v_i)|i=1, ..., n\\}, \n\\end{equation}\n$S=(u, u \\otimes G, G)$. In~, the authors extend DRR-ave and add an attention network to generate user-dependent weights for each item, as depicted in Fig.~\\ref{fig:drr-att}.  In another work~, the same authors study the effect of updating the state representation module using a supervised learning signal, and through experimental studies, they show that the recommendation performance could be improved.  \n\\begin{figure}\n\\centering     \n\\subfigure[DRR-p]{\\label{fig:drr-p}\\includegraphics[width=36mm]{DRR-p}}\n\\subfigure[DRR-u]{\\label{fig:drr-u}\\includegraphics[width=36mm]{DRR-u}}\n\\subfigure[DRR-ave]{\\label{fig:drr-ave}\\includegraphics[width=36mm]{DRR-ave}}\n\\subfigure[DRR-att]{\\label{fig:drr-att}\\includegraphics[width=36mm]{DRR}}\n\\caption{State representation module in DRR~}\n\\end{figure}\nAround 20$\\%$ of DRL-based RSs belong to SR2.  For instance, DRN~ uses user and context features for the purpose of state representation in news recommendation.   User features extracted in DRN include the features of the news clicked by the user in different time frames, like one hour, six hours, 24 hours, one week, and one year.  These news features include headline provider, ranking, entity name, category, and topic category. Context features used also describe the time context of the news request, including time and weekday.  Similar to RL-based methods, SR2 is the popular state representation method in healthcare applications~.  Nemati et al.~ use a partially-observable MDP (POMDP) formulation in a clinical application.  They formulate states as belief states using discriminative hidden Markov model (DHMM). \nWe found only two works~ lie in SR1.  Perhaps the reason these works use a simple state representation method is that they are representative works not specifically designed for RSs, but developed to target specific challenges in applying DRL to domains like RSs. \n\\vspace{-8pt}", "cites": [5192, 3724, 5193, 5180, 5170, 5169], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.0, "abstraction": 3.8}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple approaches to state representation in DRL-based RSs, drawing connections between SR3, SR2, and SR1. It provides mathematical formulations and describes how different models modify the SR3 framework, offering a coherent analytical progression. While it does not deeply critique the limitations or compare effectiveness in detail, it abstracts beyond individual papers by identifying broader structural trends in how state representations evolve to better model user interactions."}}
{"id": "8a5bdf6f-434d-4c72-98c7-da6d9680bbf0", "title": "Policy Optimization", "level": "subsubsection", "subsections": [], "parent_id": "47054969-9b91-489f-afb7-b8069246a898", "prefix_titles": [["title", "Reinforcement Learning based Recommender Systems: A Survey"], ["section", "Reinforcement Learning based Recommender Systems Algorithms"], ["subsection", "DRL-based RSs"], ["subsubsection", "Policy Optimization"]], "content": "After defining states, the role of policy $\\pi$ is to map states to actions.  Policy optimization algorithms used by DRL-based RSs could be generally divided into value-based, policy gradient, and actor-critic methods. \n\\begin{table}[t]\n\\caption{DQN-based RSs}\n\\centering\n\\scriptsize\n\\begin{tabular}{ l c c  c  c  c}\n \\hline\n \\textbf{RLRS} & \\textbf{Year} & \\textbf{Algorithm} & \\textbf{Architecture} & \\textbf{Experience replay} & \\textbf{Exploration}\\\\ \n \\hline\n Slate-MDP~ & 2015 & DQN & A2 & Uniform & $\\epsilon$-greedy\\\\\n Nemati et al.~ & 2016& DQN & A1 & Uniform* & $\\epsilon$-greedy*\\\\\n Raghu et al.~ & 2017 & Dueling DDQN & A1 & Prioritized & $\\epsilon$-greedy*\\\\\n DEERS~ &2018& DQN &  A2 & Prioritized & $\\epsilon$-greedy*\\\\ \n Robust DQN~ &2018& DDQN & A2 & Stratified & $\\epsilon$-greedy*\\\\\n DRN~&2018& Dueling DDQN & A2 & Uniform & UBGD\\\\\n FeedRec~ &2019&  DQN & A2 & Uniform & Decayed $\\epsilon$-greedy \\\\\n DRCGR~ &2019& DQN & A2 & Uniform & $\\epsilon$-greedy\\\\ \n SlateQ~ &2019& DQN & A2 & Uniform & $\\epsilon$-greedy*\\\\\n Tsumita~ &2019& DQN & A1 & Uniform* & $\\epsilon$-greedy*\\\\\n Liu et al.~ &2019& Dueling DDQN & A1 & Uniform & Decayed $\\epsilon$-greedy\\\\\n Yuyan et al.~ &2019& DQN & A1 & Uniform & $\\epsilon$-greedy\\\\\n UDQN~ &2019& DQN & A1 & Uniform & $\\epsilon$-greedy\\\\\n Cascading DQN~ &2019& DQN & A2 & Uniform & $\\epsilon$-greedy\\\\\n Pseudo Dyna-Q~ &2020& DQN & A2 & Uniform & Decayed $\\epsilon$-greedy\\\\\n Zhao et al.~ &2020& Duleing DQN & A2 & Uniform & $\\epsilon$-greedy*\\\\ \n recEnergy~ &2020& DQN & A1 & Uniform & Boltzmann exploration\\\\ \n SADQN~ &2020& DQN & A2 & Not using & $\\epsilon$-greedy\\\\\n CPR~ &2020& DQN & A1 & Uniform & $\\epsilon$-greedy*\\\\ \n Xin et al.~ &2020&  DDQN & A2 & Uniform & $\\epsilon$-greedy*\\\\  \n KGQR~ &2020& Dueling DDQN & A2 & Uniform & $\\epsilon$-greedy\\\\\n EDRR~ &2020& DQN & A1 & Uniform* & $\\epsilon$-greedy*\\\\\n GCQN~ &2020& DQN & A2 & Uniform* & $\\epsilon$-greedy\\\\\n SRR~ &2020& DQN & A1 & Uniform* & $\\epsilon$-greedy*\\\\ \n DHCRS~ &2021&  DQN & A1 & Uniform & $\\epsilon$-greedy*\\\\\n UNICORN~ &2021& Dueling DDQN & A2 & Prioritized & $\\epsilon$-greedy\\\\\n GoalRec~ &2021& DQN & A2 & Hindsight & Decayed $\\epsilon$-greedy\\\\\n DEAR~ &2021& Dueling DQN & Hybrid & Uniform & $\\epsilon$-greedy*\\\\\n \\hline\n \\multicolumn{6}{l}{\\footnotesize * There is no indication about this element in respective papers and we have assumed uniform/$\\epsilon$-greedy}\\\\\n \\multicolumn{6}{l}{\\footnotesize  because all these algorithms are based on DQN.}\n\\end{tabular}\n\\label{tab:dqn-based}\n\\end{table}\n\\textbf{Value-based methods.} Apart from MCTS used in~, DQN and its extensions, i.e., DDQN, dueling DQN, and dueling DDQN, are the ruling value-based methods.  Basically, there are three main elements in DQN: 1) Q network architecture, 2) experience replay, and 3) exploration.  We survey DQN-based methods according to these elements and Table~\\ref{tab:dqn-based} summarizes DQN-based methods.\n\\textit{1) Q network Architecture.} Fig.~\\ref{fig:dqn} depicts two possible architectures of Q network used by DQN-based RLRSs.  The original architecture (A1), introduced in~, receives the state and emits the Q value of all actions, indicated by $Q_1, ..., Q_n$ in Fig.~\\ref{fig:dqn1}.  While A1 woks fine when the action space is small, its applicability to the RS domain with a large, and even huge (in the order of millions), action space is questionable.  Another possible architecture (A2) is to receive the pair of  state and action, and then to emit the Q value of the pair, i.e., $Q(s, a)$ (depicted in Fig.~\\ref{fig:dqn2}).  Although A2 solves A1's problem, a problem with A2 is that the time complexity of the model could be high.\nDespite the original DQN where CNN is used for Q network to process the image data, Q network in RLRSs is typically composed of several FC layers, as the input, i.e., states or actions, are in the form of 1D vectors. For example, as stated before, DEERS~ uses two types of states as the input into Q network: positive and negative states, depicted in~\\ref{fig:DEERS}. Q network is a five-layer FC network where the first three layers are separate for positive and negative states, and then the last two layers connect both states, emitting the Q value of a given state and action pair.  To take this dual-state architecture into account, the original loss function of DQN in Eq.~\\eqref{eq:dqn1} is modified as\n\\begin{equation}\nL(\\theta_i) = \\mathbb{E}_{s, a \\sim \\rho(\\cdot)} \\Big [ \\big(y_i - Q(s_+, s_-, a; \\theta_i)\\big)^2 \\Big ], \n\\end{equation}\nwhere $y_i = \\mathbb{E}_{s'} [ r+\\gamma \\, \\mathrm{max}_{a'} \\, Q(s'_+, s'_-, a'; \\theta_{i-1})|s_+, s_-, a]$. Consequently, the gradient of the loss function becomes\n\\begin{equation}\n\\nabla_{\\theta_i} L(\\theta_i) = \\mathbb{E}_{s, a \\sim \\rho(\\cdot)} \\Big [ \\big (r+\\gamma \\, \\underset{a'}{\\mathrm{max}} \\, Q(s'_+, s'_-, a'; \\theta_{i-1}) - Q(s_+, s_-, a; \\theta_i) \\big )\\nabla_{\\theta_i}Q(s_+, s_-,a;\\theta_i) \\Big ].\n\\end{equation}\nOther researchers have employed DQN's extensions.  For instance, DRN~ adopts dueling DDQN for policy optimization in news recommendation.  In particular, the authors argue that while the reward of taking an action is impacted by all features, i.e., user, news, context, and user-news features, there is a reward that is impacted by merely user and context features.  Accordingly, the Q function is divided into value function $V(s)$ and advantage function $A(s, a)$.  As depicted in Fig.~\\ref{fig:drn}, while $V(s)$ is fed with state features, the input into $A(s, a)$ is comprised of state and action features. \n\\begin{figure}\n\\centering     \n\\subfigure[]{\\label{fig:drn}\\includegraphics[width=50mm]{DRN}}\n\\subfigure[]{\\label{fig:minmin2019}\\includegraphics[width=50mm]{ReinforceRS}}\n\\subfigure[]{\\label{fig:wolpertinger}\\includegraphics[width=35mm]{wolper}}\n\\caption{(a) Dueling architecture used in DRN~, (b) The neural architecture of policy $\\pi_\\theta$~, (c) Wolpertinger architecture~}\n\\end{figure}\nDEAR~ studies the problem of advertising along with recommendation. It combines the two architectures of DQN Q network, i.e., A1 and A2, and the resulting architecture generates the Q value of a list of candidate ads if inserted in the recommendation list. In other words, the input is similar to A2 architecture, i.e., state and action, and the output is the same as A1, which is a list containing the Q values of all state-action pairs.\n\\textit{2) Experience Replay.} According to Table~\\ref{tab:dqn-based}, the vast majority (22 out of 28) of DQN-based RSs use the original uniform sampling to replay collected experiences.  Also, only three of them use prioritized experience replay~.  Authors in~ propose to use \\textit{stratified sampling} replay instead of uniform sampling to address the variance of sampling in dynamic environments. Stratified sampling is a sampling technique from a population in which the entire population is partitioned into several groups (called strata) and then samples are randomly selected from these strata~.  They propose to use some stable features from customers, like gender, age, and geography, as strata.  \nGoalRec~ uses \\textit{hindsight} experience replay~.  The main idea in hindsight replay is to learn  from an undesirable outcome as much as from a desirable outcome.  Since the goal has no effect on the dynamics of the environment, a failed trajectory is re-labelled as a successful one, as if the state in the trajectory is the actual goal. This considerably improves sample efficiency.\nIn contrast to existing DQN-based methods, SADQN~ does not use experience replay for training.  Instead, in each episode of the training phase,  a user is sampled from the user set and the agent is trained on the available interactions until it is converged. Using experiments, authors claim that the experience replay in fact diminishes the performance of SADQN.\n\\textit{3) Exploration.} Although exploration is an important factor in learning of the agent, many DQN-based methods have seemingly overlooked it, as there is no specific indication about this in respective publications.  Apart from simple exploration techniques like $\\epsilon$-greedy,  DRN~ proposes to use an exploration approach similar to \\textit{dueling bandit gradient descent} algorithm~. In particular, there is a separate network for exploration called \\textit{explore network} and its parameters can be obtained using a disturbance to the parameters of current network with parameters $W$\n\\begin{equation}\n\\Delta W = \\alpha \\cdot \\text{rand}(-1,1) \\cdot W,\n\\end{equation}\nwhere $\\alpha$ is the explore coefficient.  Then, the agent generates a merged list of recommendations using probabilistically interleaving between items found by current network and explore network. \nIn recEnergy~, to balance the exploration vs exploitation trade-off, \\textit{Boltzmann exploration}~ is used. More precisely,  the output Q values of actions from Q network are passed through a softmax equation as\n\\begin{equation}\nP(a)=\\frac{\\textrm{exp}\\frac{Q(a)}{\\tau}}{\\sum_{i=1}^n \\textrm{exp}\\frac{Q(i)}{\\tau}},\n\\end{equation}\nwhere $\\tau$ is a temperature and is decayed over time.  This method guarantees that the model explores more often initially, and then it starts to exploit actions with larger Q values more frequently. \n\\textbf{Policy Gradient methods.}  In contrast with value-based methods,  policy gradient methods learn a parameterized policy without the need to a value function.  REINFORCE is a Monte Carlo, stochastic gradient method that directly updates the policy weights. The major problems of REINFORCE algorithm are high variance and slow learning.  These problems come from the Monte Carlo nature of REINFORCE, as it selects samples randomly and updates are made when the episode is completed.\nIn a valuable work, Ref.~ adapts REINFORCE algorithm to a neural candidate generator with a very large action space.  In particular, in an online RL setting, the estimator of the policy gradient can be expressed as\n\\begin{equation}\n\\label{eq:REINFORCE-rec}\n\\sum_{\\tau \\sim \\pi_\\theta} \\Big[ \\sum_{t=0} ^{|\\tau|}R_t \\Delta_\\theta \\, \\textrm{log} \\, \\pi_\\theta (a_t|s_t)\\Big],\n\\end{equation}\nwhere $\\pi_\\theta$ is the parametrized policy, $\\tau=(s_0, a_0, s_1, ...)$, and $R_t$ is the cumulative reward. Since in the RS setting, unlike classical RL problems, the online or real time interaction between the agent and environment is infeasible and usually only logged feedback is available, applying the policy gradient in Eq.~\\eqref{eq:REINFORCE-rec} is biased and needs correction.  The off-policy-corrected policy gradient estimator is then:\n\\begin{equation}\n\\label{eq:offpolicy-correction}\n\\sum_{\\tau \\sim \\beta} \\frac{\\pi_\\theta (\\tau)}{\\beta(\\tau)} \\Big[ \\sum_{t=0} ^{|\\tau|}R_t \\Delta_\\theta \\textrm{log} \\pi_\\theta (a_t|s_t)\\Big],\n\\end{equation}\nwhere $\\beta$ is the behavior policy and\n\\begin{equation}\n\\frac{\\pi_\\theta (\\tau)}{\\beta(\\tau)}=\\frac{\\rho(s_0)\\prod_{t=0}^{|\\tau|} P(s_{t+1} | s_t, a_t)\\pi (a_t|s_t)}{\\rho(s_0)\\prod_{t=0}^{|\\tau|} P(s_{t+1} | s_t, a_t)\\beta (a_t|s_t)} = \\prod_{t=0}^{|\\tau|}\\frac{\\pi(a_t | s_t)}{\\beta (a_t|s_t)} \n\\end{equation}\n  is the importance weight.  Since this correction generates a huge variance for the estimator due to the chained products, authors use first-order approximation for importance weights, leading to the following biased estimator with a lower variance for the estimator:\n\\begin{equation}\n\\label{eq:chen-final-corrected}\n\\sum_{\\tau \\sim \\beta} \\Big[ \\sum_{t=0} ^{|\\tau|}  \\frac{\\pi_\\theta (a_t|s_t)}{\\beta (a_t|s_t)} R_t \\Delta_\\theta \\textrm{log} \\pi_\\theta (a_t|s_t)\\Big].\n\\end{equation}\nFig.~\\ref{fig:minmin2019} illustrates the neural architecture of the parametrized policy $\\pi_\\theta$ in Eq.~\\eqref{eq:chen-final-corrected}. \nAs discussed in section~\\ref{subsec:RL}, REINFORCE-wb adds a baseline to REINFORCE's update rule in order to decrease the variance (see Eq.~\\eqref{eq:REINFORCE-baseline}).  Several RLRSs have used this approach~.  Specifically, the baseline in these methods is a value network~, a constraint~, and average reward~.  However, it is not clear how other REINFORCE-based RSs~ tackle the variance problem.  \nFollowing SeqGAN~, IRecGAN~ employs GANs to develop a model-based RL recommender.  In particular,\nthe generator is responsible to generate recommendations and to model user behavior, and the discriminator is used to rescale the generated rewards.  Using both generated and offline data, REINFORCE is used to optimize the recommendation policy.  Similar to SeqGAN,  to reduce the variance, IRecGAN uses MCTS with roll-out policy, i.e., sampling $N$ sequences from interaction between the recommender and user model and then averaging the estimations. \n\\textbf{Actor-Critic Methods.} DDPG is the base method used in almost all actor-critic based RLRSs. DDPG uses an actor-critic architecture to combine DPG and DQN. Actor, also called policy network, is responsible to generate actions, and critic, a DQN module, is responsible to evaluate the action taken.  The original DDPG uses either several FC layers or convolutional plus FC layers when the input is pixel. The output layer of actor is a \\texttt{tanh} layer to bound actions.  For exploration, DDPG uses a temporally correlated noise, Ornstein-Uhlenbeck (OU) process~, that is suitable for physical environments with momentum.  Also, similar to DQN, experience replay with uniform sampling is used.\nWolpertinger~ is the first actor-critic method based on DDPG to handle large discrete action spaces, with a recommendation case study.  The idea is to provide a method that has sub-linear complexity w.r.t. action space and generalizable over actions.  As depicted in Fig.~\\ref{fig:wolpertinger}, Wolpertinger consists of two parts: action generation and action refinement.  In the first part, proto-actions are generated by the actor in continuous space and then are mapped to discrete space using $k$-nearest neighbor ($k$-NN) method.  More precisely, the proto-action $\\hat{a}$ is generated by actor as \n\\begin{equation}\n\\hat{a} = f_{\\theta}(s).\n\\end{equation}\nThis proto-action is not likely to be a valid action so $\\hat{a}$ is mapped to an element in $\\pazocal{A}$ as\n\\begin{equation}\ng_k(\\hat{a})=\\argmin_{a \\in \\pazocal{A}}^{k} ~|a-\\hat{a}|_2 .\n\\end{equation}\nIn the second part, outlier actions are filtered using a critic, which  selects the best action that has the maximum Q value. In other words,\n\\begin{equation}\n\\pi_\\theta(s)=\\argmax_{a \\in g_k} ~Q_\\theta(s,a).\n\\end{equation}\nWolpertinger is trained using DDPG. For exploration, for the recommendation task, Wolpertinger uses a guided $\\epsilon$-greedy exploration technique. In particular, the exploration is restricted to a likely good set of actions provided by the environment simulator. \nThe vast majority of actor-critic methods are based on DDPG~.  Table~\\ref{tab:ddpg} summarizes these methods.  As depicted, only DRR uses prioritized experience replay; the remaining algorithms either use uniform sampling or there is no clue about this in respective publications.  Another worthwhile observation from Table~\\ref{tab:ddpg} is that the vast majority of algorithms do not talk about exploration.  Of five algorithms with a described exploration method, three of them, i.e.,  Wolpertinger, DRR, and HRL-Recused, are based on $\\epsilon$-greedy. Similar to DDPG, DRGR uses OU process to encourage better exploration for the actor. However, as stated earlier, OU noise is suitable for physical processes.  Finally, MASSA introduces a novel entropy-regularized method for exploration, a method similar to soft actor-critic~. \n\\begin{table}[t]\n\\caption{DDPG-based RSs}\n\\centering\n\\scriptsize\n\\begin{tabular}{ l c c  c }\n \\hline\n \\textbf{RLRS} & \\textbf{Year} & \\textbf{Experience replay} & \\textbf{Exploration}\\\\ \n \\hline\n Wolpertinger~ & 2015 & Uniform & Guided $\\epsilon$-greedy\\\\\n SRL-RNN~ & 2018 & Uniform & N/A\\\\\n Deep Page~ & 2018& Uniform & N/A\\\\ \n DRR~ &2018, 2020& Prioritized & $\\epsilon$-greedy\\\\\n Munemasa et al.~ &2018& Uniform & N/A\\\\\n CapDRL~ &2019&  Uniform* & N/A\\\\\n LIRD~ & 2019& Uniform & N/A\\\\\n CROMA~ &2019& Uniform & N/A\\\\  \n MaHRL~ &2020& Uniform & N/A\\\\\n FairRec~ &2020& Uniform* & N/A\\\\\n DeepChain~ &2020& Uniform & N/A\\\\\n KGRL~ &2020& Uniform & N/A\\\\\n EDRR~ &2020& Uniform* & N/A\\\\\n SRR~ &2020& Uniform* & N/A\\\\\n MASSA~ &2021& Uniform & Entropy-regularized\\\\\n FCPO~ &2021& Uniform & N/A\\\\\n HRL-Rec~ &2021& Uniform* & $\\epsilon$-greedy\\\\\n MASTER~ &2021& Uniform & N/A\\\\\n D$^2$RLIR~ &2021& Uniform & N/A\\\\\n DRGR~ &2021& Uniform* & OU noise\\\\ \n \\hline\n \\multicolumn{4}{l}{\\footnotesize * There is no indication about this element in respective papers and we have}\\\\\n  \\multicolumn{4}{l}{\\footnotesize assumed uniform because all these algorithms are based on DDPG.}\n\\end{tabular}\n\\label{tab:ddpg}\n\\end{table}\nActor-critic seems as a popular architecture for multi-agent RL (MARL). Centralized learning/training with decentralized execution ~ is a suitable framework for a multi-agent setting and adopted by CROMA, MASSA, DeepChain, and MASTER.  For instance, Fig.~\\ref{fig:maddpg} depicts the architecture of MADDPG~, which utilizes a centralized training and decentralized execution framework.  MASSA builds upon this architecture and adds a signal network to the MADDPG architecture, depicted in Fig.~\\ref{fig:massa}, which is responsible to ease the cooperation between  decentralized actors.\n\\begin{figure}\n\\centering     \n\\subfigure[]{\\label{fig:maddpg}\\includegraphics[width=40mm]{MADDPG}}\n\\subfigure[]{\\label{fig:massa}\\includegraphics[width=40mm]{MASSA}}\n\\subfigure[]{\\label{fig:explain}\\includegraphics[width=55mm]{explainable}}\n\\caption{(a) MADDPG architecture, (b) MASSA architecture, (c) Graph-based reasoning example~}\n\\label{fig:MA}\n\\end{figure}\nThere are a couple of actor-critic based methods that use adversarial training for a better policy learning~.  For instance, CRSAL~ extends soft actor-critic~ with adversarial learning, by adding a discriminator inside the critic to distinguish between dialogues generated by the policy network and real users.  In a path reasoning scenario over knowledge graph, ADAC~ uses adversarial imitation learning~ and defines two path and meta-path discriminators to distinguish expert paths from paths generated by the actor. \nIn contrast to other actor-critic methods, DRESS~ uses PPO and SDAC~ proposes a stochastic discrete actor-critic.  Authors in SDAC propose a general offline framework for RLRSs. They first formulate the recommendation problem as a probabilistic generative model.  Then, a stochastic actor-critic algorithm is proposed to optimize the recommendation policy. \n\\vspace{-8pt}", "cites": [5165, 5168, 5181, 3724, 5182, 5187, 620, 5194, 5170, 3604, 5188, 5189, 5191, 5195, 3450, 5193, 5192, 3592, 5198, 5180, 5190, 5196, 8884, 1389, 5197, 5200, 5177, 5169, 5178, 5183, 5185, 3442, 5186, 5199], "cite_extract_rate": 0.43037974683544306, "origin_cites_number": 79, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a structured overview of DQN-based reinforcement learning approaches in recommender systems and categorizes them by key components such as architecture and experience replay. However, it primarily functions as a descriptive summary of methods rather than offering deep synthesis, critical evaluation, or abstracted insights. Some connections are made (e.g., between different Q-network architectures), but the analysis remains surface-level with limited discussion of broader implications or limitations."}}
{"id": "0938a28f-7fba-4d7f-969d-59245482c016", "title": "Reward Formulation", "level": "subsubsection", "subsections": [], "parent_id": "47054969-9b91-489f-afb7-b8069246a898", "prefix_titles": [["title", "Reinforcement Learning based Recommender Systems: A Survey"], ["section", "Reinforcement Learning based Recommender Systems Algorithms"], ["subsection", "DRL-based RSs"], ["subsubsection", "Reward Formulation"]], "content": "As depicted in Fig.~\\ref{fig:drl-rew}, the majority (60$\\%$) of DRL-based RSs  belong to R2.  A common pattern frequently used by RLRSs in R2 is to formulate the reward as a function, or a simple combination, of several factors or metrics~.  For example, in a news recommendation scenario, reward in DRN~ is a function of user click and user activeness.  The rationale behind factoring in the user activeness is that a good recommendation should motivate the user to use or interact with the system again.  Authors use \\textit{survival models}~ to model user return~ and user activeness.  FeedRec~ formulates the reward as a weighted sum of instant metrics, including user click and purchase, and delayed metrics, like browsing depth and dwell time.  Authors consider user clicks as instant metric, and browsing depth and return time as delayed metrics.  Yu et al.~ design an advantage function composed of visual, attribute, and history matching rewards, in order to tackle the multi-modal recommendation problem.\nRobust DQN~ proposes to use \\textit{approximate regretted reward} to improve reward estimation.  The idea is to use two different rewards, i.e., current and optimal rewards, and then calculate the regret as the final reward.  Since calculating the optimal reward in the real is not possible, they propose to use an alternative, benchmark reward, which is the average reward achieved by applying the model to a subset of users. \nA simple but effective scheme for multi-objective optimization in RLRSs is to formulate the reward as a multi-objective function~.  Singh et al.~, for example, use this idea for a safe RLRS.  The format of reward in their work is as follows\n\\begin{equation}\nR_{mo} = R_t - \\textrm{C}_{\\textrm{risk}},\n\\end{equation}\nwhere $\\textrm{C}_{\\textrm{risk}}$ is a health risk constraint.  This reward function balances between reward maximization and health constraint preservation.  A similar formulation is used to balance the accuracy trade-off with diversity~ and fairness~.\nIn hierarchical RL, two reward functions, i.e., for low-level and high-level agents, should be defined~.  For example, in HRL-Rec~, click times on the recommended channel is considered as the low-level agent's reward, while the reward for the high-level agent is composed of four factors, including click times, dwell time, list-level diversity, and item novelty. \nIn KGRE-Rec~, a delayed reward function is used.  Authors discuss that it is impossible to define a sparse, binary reward when there is no pre-known good/targeted item in their recommendation problem.  Instead, the agent is encouraged to find good paths in the graph, those that lead to an item of user interest with high probability. Thus, the agent is received a reward only in a terminal state.  The same idea can be seen in MASTER~, where a \\textit{lazy reward} is given to the agent when a charging request is successful.  However, the idea of rewarding the agent only in the terminal state is not always practical.  For example, Liu et al. discuss that since there is no well-defined terminal state in AnchorKG~, the reward function should be composed of immediate and terminal rewards. \nAnother reward formulation method used in R2 is to define the reward as a \\textit{distance} between the recommended item and a target item~.  In KGRL~, for instance, the reward is based on the distance between the predicted item and target item in the graph\n\\begin{equation}\nr = \\frac{100}{\\sqrt{d(v_p,v_t)+\\epsilon}}\\cdot W_{pt},\n\\end{equation}\nwhere $d(v_p,v_t)$ is the distance between predicted item $p$ and target item $t$, $\\epsilon$ is a regulizer, and $W_{pt}$ is the sum of weights of the shortest path from $v_p$ to $v_t$.  Distance $d$ is calculated using Dijkstra's algorithm.\nOn the other hand, perhaps the simplest method of reward definition for the designer is to empirically use several real values for different goals in the system, which is usually used in R1.  For example, Zhao et al use a similar pattern of numerical reward in their proposals~ and reward three behaviors of users, namely skip, click, and order, with some numbers, e.g., 0, 1, and 5, respectively.  The same pattern can be observed in other RLRSs belonging to R1~.  \nSimilarly, in a conversational RS scenario, EAR~ defines a sparse reward function with four pre-defined values, i.e., strongly positive reward when the recommendation is successful ($r_{s}$), a positive reward if the user gives positive feedback on asked attribute ($r_{a}$), a strong negative reward for user quit ($r_{q}$), and a slight negative for every conversation turn ($r_p$).  The total reward is the sum of these rewards and in the experiments they use values $r_s=1$, $r_a=0.1$, $r_q=-0.3$, and $r_p=-0.1$.   A similar reward function is used in other conversational RSs~. \nDelayed reward is also used in R1 by some graph-based RLRSs~, where  agent is only rewarded when it reaches the terminal state.  For example in Ekar~, the agent is rewarded with +1 if it reaches an item in the terminal state with which the user has interacted, 0 if it reaches an item but the user has not interacted with, and -1 if the entity reached is not an item in the graph.  \n\\vspace{-8pt}", "cites": [5190, 3604, 8884, 5196, 5197, 5184, 5189, 5191, 5168, 5177, 5169, 5199, 5178, 5181, 5179, 5193, 5194, 5170], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 54, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers to present a coherent narrative on reward formulation in DRL-based RSs, categorizing them into instant, delayed, and multi-objective types. It includes critical perspectives by discussing limitations, such as the impracticality of terminal-only rewards and the need for benchmarking. The analysis abstracts specific approaches into broader patterns like the use of survival models and distance metrics in graph-based systems."}}
{"id": "895ccad7-0690-40de-bd5e-47e34830207a", "title": "Environment Building", "level": "subsubsection", "subsections": [], "parent_id": "47054969-9b91-489f-afb7-b8069246a898", "prefix_titles": [["title", "Reinforcement Learning based Recommender Systems: A Survey"], ["section", "Reinforcement Learning based Recommender Systems Algorithms"], ["subsection", "DRL-based RSs"], ["subsubsection", "Environment Building"]], "content": "As shown in Fig.~\\ref{fig:drl-eval},  more than half of DRL-based RSs use an offline method for environment building.  Almost 40$\\%$ of methods use a simulator, and only in 10$\\%$ online study is used.  Compared to RL-based methods, while a similar proportion use the offline method, the uptake of simulation and online schemes has been doubled and diminished by almost 60$\\%$, respectively.  This graph shows that conducting an online study has become more difficult or costly, and simulation is getting more and more popular among the RLRS community. \nAmong those conducting a simulation study, SlateQ~ introduces an open-source RLRS simulation environment, called RecSim~, which gives the researcher flexibility to evaluate their algorithms in different settings.  Cascading DQN~ uses GANs to simulate a real user and estimate the reward function from logged data.  More precisely, the GAN training is formulated as \n\\begin{equation}\n\\underset{\\theta}{\\textrm{min}} \\, \\underset{\\alpha}{\\textrm{max}} \\Big(\\mathbb{E}_{\\phi_\\alpha}\\big[\\sum_{t=1}^T r_\\theta (s_{true}^t, a^t)\\big]-R(\\phi_\\alpha)/\\eta\\Big) - \\sum_{t=1}^T r_\\theta (s_{true}^t, a_{true}^t),\n\\end{equation}\nwhere $\\eta$ is a regularization term, $true$ means real data, $\\phi$ represents the generator and generates user's next action, and $r$ is the discriminator trying to differentiate between generated actions and real actions. \nIn DEERS~, a user simulator, with the same architecture as DEERS, is trained on user logs.  However, the output layer of the simulator is a softmax layer to predict the user feedback (immediate reward) based on the input (pair of state and recommended item).  Authors claim that the simulator is 90$\\%$ precise in predicting user feedback.  The same approach for simulation study has been used by other RLRSs~.  For instance, a similar idea is used in~, but the simulator (S Network) provides different feedbacks, including user response, dwell time, revisited time, and a binary indicator if the user is leaving or not.  In Pseudo Dyna-Q~, a world model (user simulator) is trained by minimizing an error between online and offline rewards.  \\textit{Truncated importance sampling}~ is used to alleviate the bias in the offline data. \nAnother popular simulation method is to develop a simulator based on collaborative filtering~.  To be specific, LIRD~ builds a memory with $(s, a, r)$ tuples seen in the logs dataset and uses a similarity method, based on cosine similarity, to find the closest state-action pair to the current state and action recommended. DRR~ and DRGR~ use the same intuition but based on probabilistic matrix factorization ~ and matrix factorization, respectively. \nBuilding a simulator for conversational RSs is more challenging than that for typical recommendation scenarios mentioned above, as there are a small number of public datasets available to have both user rating and natural language/user chat for that item-rating pair.  CRM~ tackles this problem by creating simulated users based on Yelp~ data and a dialogue corpus, collected using \\textit{crowd sourcing workers}.  The simulated users have three behaviors: answering the agent question, finding the target item in a list, and leaving the dialogue.  The same scheme has been used in other conversational RLRSs~. \nGenerally speaking, performing a good online study has become more challenging in modern RSs with huge user and item spaces, as the risk of implementing a non-optimal RS is very high. As discussed before, this is the most probable reason of a considerable decrease in the popularity of online study among DRL-based methods compared to RL-based methods. Perhaps two of the best online studies among RLRSs are conducted in~ and~ and performed on YouTube.", "cites": [5196, 5165, 5197, 5189, 5168, 5195, 5169, 5199, 5201, 5187, 5198, 5180, 5170], "cite_extract_rate": 0.4642857142857143, "origin_cites_number": 28, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key ideas from multiple papers on DRL-based RSs, particularly in the context of simulation and environment building. It provides a comparative overview of methods and highlights trends, such as the shift away from online studies and toward simulation. The discussion also abstracts these methods into broader categories (e.g., using GANs, collaborative filtering, or world models for simulation), offering a structured and insightful analysis of the field."}}
{"id": "1f15ced7-f80d-463a-916a-74f1b5cf9989", "title": "Emerging Topics", "level": "section", "subsections": [], "parent_id": "908d8fa7-e998-4bf3-9426-ce385d023d35", "prefix_titles": [["title", "Reinforcement Learning based Recommender Systems: A Survey"], ["section", "Emerging Topics"]], "content": "\\label{sec:ET}\nHaving reviewed RLRSs, we have recognized that there are a couple of trends that are being formed among DRL-based RSs and have the potential to become mature in the course of time.  In this section, we briefly review these emerging topics. \n\\textbf{Multi-agent RL.} Multi-Agent RL (MARL) is a generalization of a single-agent RL and is formulated as a \\textit{Markov/stochastic game}~.  MARL enables RLRSs to target several or complex tasks by dividing them into sub-tasks and each agent can handle one of them.   For example, instead of optimizing a single strategy for all scenarios in an e-commerce application of RSs (like entrance page, item recommendation, and checking out purchases), there could be several RS agents each of which responsible for a specific scenario and the final policy is jointly optimized between them~. From a \\textit{game theory} prospective, MARL methods can be generally divided into three groups: fully cooperative, fully competitive, and a mix of the two~. \nRecently, several RLRSs have employed MARL to tackle the problems of scholarly collaborator recommendation~,  mention recommendation in Twitter~, page-wise recommendation~, whole-chain recommendation~, and charging spot recommendation~.  As stated earlier, actor-critic with centralized training and decentralized execution has been a popular framework for DRL-based RLRSs employing MARL~.  In a cooperative setting~, a challenge is to determine the role of each \\textit{player} in the overall's team success.  CROMA~, with two actors and a centralized critic, tackles this problem by a differentiated advantage scheme using reverse operation.  Specifically, each actor agent can estimate its particular advantage by subtracting the overall Q value of the joint action, computed by the centralized critic, from the Q value of a reverse action.  A similar architecture is used in DeepChain~ to jointly optimize the overall reward of a session.  It is not, however, clear how DeepChain solves the aforementioned problem, i.e., shared reward for two actors, which is critical for their effective training. In MASSA~, a MARL with separate actor and critic agents is used to tackle a multi-module, page-wise recommendation.  A game theory concept called \\textit{correlated equilibrium}~ in the format of a \\textit{signal network} is used to handle the communication between agents.  MASTER~ considers each charging spot for electric vehicles as a distributed agent and uses a centralized critic to coordinate these agents.  A couple of techniques, including \\textit{bidding game} and multiple critics, are employed to address challenges like cooperation between agents, future competition between requests, and multi-objective optimization.  In a different, competitive scenario, authors in~ use MARL to recommend scientific collaborator.  Each author looking for a collaborator is deemed as an agent and  learn an optimal policy using gradient value iteration algorithm.  \n\\textbf{Hierarchical and meta-controller RL.} Hierarchical RL (HRL) was initially sought to address the scalability problem in traditional RL algorithms~.   In HRL, however, it is possible to define multiple layers of policies, each of which can be trained to provide higher levels of temporal and behvioral abstractions, leading to the ability of solving more complex tasks~.  Recommendation is not an exception and several researchers have utilized HRL in the RS domain~.  Generally speaking, all these RLRSs define a HRL with two levels of hierarchies where a high-level agent defines a high-level/abstract goal and a low-level agent tries to satisfy that goal.  CEI~ builds a conversational RS on a deep HRL method~, which uses ideas from a popular and traditional HRL framework, called \\textit{options}~. CEI uses  a meta-controller that selects a goal (chitchat or recommendation) in a given state and a controller makes an action following a goal-specific policy to satisfy the defined goal.  Zhang et al.~ employ HRL for course recommendation in massive open online courses (MOOCs).  The key idea is to develop a profile reviser using HRL, which removes noisy courses from users profiles.  This is decomposed into two high-level and low-level tasks: given a user profile and a target course, should the profile be revised (high-level) and if yes, which courses in the profile should be removed (low-level).  DARL~ improves Zhang et al.'s RS by making the recommendation unit more adaptive. That means, they equip the basic recommendation module in Zhang's work with an attention mechanism to take dynamic users' interest in diverse courses into account. HRL-Rec~ uses HRL in an integrated recommendation scenario.  A low-level agent generates a list of channels, and a high-level agent recommends a list of items with the channel constraint selected by low-level agent.  Moreover, MaHRL~ tackles the sparse conversion metric in e-commerce by using HRL.  More precisely, there is a high-level agent responsible to track long-term sparse conversion interest by setting multiple abstract goals for the low-level agent, while the low-level agent follows these goals and tries to catch short-term click interest.  Finally, DHCRS~ tries to tackle the large action space in RSs through using a two-level HRL, where a high-level DQN selects categories of items and a low-level DQN selects an item in the category to recommend.\nIn an emerging topic, a group of researchers have used RL as a \\textit{meta-controller} module in conversational RSs.  That means, instead of using RL to optimize the recommendation policy, similar to HRL, these methods use RL to select either recommending items or asking questions from users to refine recommendations.  But different from HRL, there is only one level using RL and the recommendation unit uses other techniques, like supervised learning, to generate the recommendations.  This is the common theme in a couple of RLRSs~.  For instance, CRM~ is composed of three main parts: a belief tracker, a recommender, and a policy network (RL module).  The belief tracker unit is responsible to extract facet-value pairs (some constraints) from user utterances and convert them to beliefs using an LSTM network. Factorization machine~ is used in the recommender to generate a set of recommendations. Finally, a neural policy network, optimized by REINFORCE, is used to manage the conversational system, i.e., to decide either to ask for more information from the user or to recommend the items.\n\\textbf{Knowledge graph based RLRSs.} Incorporating knowledge graphs into RSs can boost recommendation accuracy and explainability~.  Utilizing knowledge graphs provide RLRSs with different useful information, which can address sample inefficiency  in DRL. Recently, many researchers started to use this idea and boost recommendation performance and explainability~.  For example, the idea in KGRE-Rec~ is to not only recommend a set of items, but also the paths in the knowledge graph to show the reason why the method has made these recommendations.  An example of this graph reasoning is depicted in Fig.~\\ref{fig:explain}.  For a given user $A$, the algorithm should find items $B$ and $F$ with their reasoning paths in the graph, like \\{User $A$ $\\rightarrow$ Item $A$ $\\rightarrow$ Brand $A$ $\\rightarrow$ Item $B$ \\} and \\{ User $A$ $\\rightarrow$ Feature $B$ $\\rightarrow$ Item $F$ \\}.  Obviously, graph based techniques face the scalability problem as the number of nodes and links can significantly grow, proportional to the number of users and items.  To address this problem, KGRE-Rec proposes a \\textit{user-conditional action pruning} strategy, which uses a scoring function to only keep important edges conditioned on the starting user.  \n\\textbf{Supervised RL.} The key feature that distinguishes RL from supervised learning is whether the training data serves as an evaluation signal, like numerical reward, or as an error signal~.  However, these methods, RL and supervised learning, can be combined to improve policy learning when both signals are available.  Wang et al.~ use this idea to dynamically recommend treatment options to patients.  The idea is while the model should maximize the expected return, it should minimize the difference from doctors' prescriptions.  In particular, in an actor-critic architecture, the actor is responsible to recommend the best prescription by optimizing the following objective function:\n\\begin{equation}\nJ(\\theta) = (1-\\alpha)J_{RL}(\\theta) + \\alpha (-J_{SL}(\\theta)),\n\\end{equation}\nwhere $J_{RL}(\\theta)$ and $J_{SL}(\\theta)$ are the objective function of RL and supervised learning tasks, respectively, and $\\alpha$ is a weight factor.  Similarly, Liu et al.~ leverage supervised learning to guide the RL module in learning better policies.  More precisely, in~, a supervised learning signal helps generate better embeddings for state representation, and in~, a supervised learning model is trained to guide the RL policy to focus on short-term reward and to generate top-aware recommendations.  \n\\textbf{Imitation Learning and Auxiliary Tasks.} In addition to the above emerging topics, there are some topics, although less popular compared to the ones discussed above, we think that they have the potential to become emerging topics in the future.  These topics include adversarial RL/training, safe RL,  self-supervised learning, and imitation learning.\nAdversarial training using GANs is an interesting emerging topic used in~.  As mentioned earlier in Actor-Critic Methods section, CRSAL~ and  ADAC~ use adversarial training integrated with actor-critic architecture for better agent training. Moreover, as discussed in policy gradient methods, IRecGAN~ proposes a model-based RL based on GANs for the purpose of variance reduction and sample efficiency.\nIn safe RL, it is important for the agent to respect some safety constraints, along with maximizing the long term reward~. In Ref.~, an RS based on multi-objective safe RL is proposed to improve the long term well-being of users.  In particular, the agent simultaneously tries to maximize user engagement and the health of worst-case user. \nSelf-supervised learning (SSL) empowers the model to utilize labels available freely with the data. In~, a framework is introduced to augment RLRSs with SSL.  More precisely, authors propose a framework with two heads: RL and SSL. While the RL head is used as a regularizer to tune recommendations, the SSL head provides negative samples to update parameters.\nIn imitation learning, the agent is trained to perform a task from demonstrations~.  Zhang et al.~ combine imagination (model-based RL) and imitation learning to recommend personalized search stories.  They argue that the goal of imitation learning is to imitate the policy of a recommender agent from which the logging data has been collected.   Also, some fictional sessions are imagined by the agent and saved in a separate memory, and are used to fine-tune the agent training. \n\\vspace{-8pt}", "cites": [5196, 8884, 5190, 3604, 5188, 5197, 5189, 5186, 5177, 3588, 5181, 5179, 5203, 5182, 7902, 8881, 5198, 5202, 5194], "cite_extract_rate": 0.3958333333333333, "origin_cites_number": 48, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by grouping diverse approaches into coherent themes (e.g., MARL, HRL, knowledge graphs) and highlighting how each cited paper contributes to these areas. It includes critical observations, such as noting the unclear reward-sharing mechanism in DeepChain. The section also abstracts key ideas like temporal abstraction in HRL and the role of centralized critics in MARL, offering broader conceptual insights."}}
{"id": "e2602b95-f1c2-486e-bd3d-390cc61bc1b8", "title": "Open Research Directions", "level": "section", "subsections": [], "parent_id": "908d8fa7-e998-4bf3-9426-ce385d023d35", "prefix_titles": [["title", "Reinforcement Learning based Recommender Systems: A Survey"], ["section", "Open Research Directions"]], "content": "\\label{sec:fut-wo}\n\\textbf{Slate Recommendation.} RL algorithms have been originally developed to select a single action, e.g., the action with the highest Q value, in each time step from different actions around~.  However, in the RS field, similar to many sequential decision support systems~, it is wise to recommend a slate or list of items and let the user involved in the decision making process choose the best action, as the final goal is typically user satisfaction and recommendation acceptance.   Despite some efforts~, current RL algorithms cannot handle this problem.  There are only two studies~ in the RLRS field that deeply investigate this problem.  Slate-MDP~ tries to solve this problem by searching the policy space for each slot in the slate individually.  SlateQ~ proposes to calculate the combination of the action set and consider each combination as an action.  Slate-MDP cannot guarantee any optimality, and SlateQ is only applicable in two-stage RSs and fails to scale to single stage RSs with large action spaces.  More attention is necessary in this aspect and more studies with solid theoretical foundations should be conducted in the future.\n\\textbf{Explainability.} Explainable recommendation is the ability of an RS to not only provide a recommendation, but also to address why a specific recommendation has been made~.  Explanation about recommendations made could improve user experience, boost their trust in the system, and help them make better decisions~. Explainable methods could be generally divided into two groups: model-intrinsic or model-agnostic~. In the former, explanation is part of the recommendation process, while in the latter, the explanation is provided after the recommendation is made.  An intrinsic explanation method could be the method we reviewed earlier~. On the other hand, as a model-agnostic example~, RL is used to provide explanation for different recommendation methods. In particular, the method uses \\textit{couple agents}; one is responsible to generate explanations and another one predicts if the explanation generated is good enough for the user. One interesting application of explainable recommendation is in debugging the failed RS~.  That is, through explanations provided, we can track the source of problems in our system and to see which parts are not working properly.  Although there have been some efforts in RLRSs to provide explainable recommendations~, there is still a lack in this aspect and more attention is required in the future.  \n\\textbf{Design.} All RLRSs reviewed employ RL/DRL algorithms that have been originally developed in domains other than RSs, like games~.  These methods are typically designed based on physics or Gaussian processes,  not based on complex and dynamic nature of a human.  While sticking to available, cutting-edge RL algorithms and adapting them for RLRSs is wise, sometimes thinking out of the box could make a substantial improvement in the field.  For example, instead of usual MDP-based RL algorithms, Ref.~ uses \\textit{evolution strategies}~ to optimize the recommendation policy, or Ref.~ borrows ideas from a different literature and adapts them to the recommendation problem.  Relevant to this, as surveyed in~, there are many deep learning models developed for RSs.  Because deep learning and DRL are closely related, perhaps wisely combining these models with traditional RL algorithms could outperform existing DRL algorithms.  Last but not least, as illustrated earlier, some RL algorithms like Q-learning have been more popular among RLRSs than other RL algorithms.  Nonetheless, there is no clue or justification behind the use of a specific RL algorithm for an RS application.  Therefore, this would be a great study to possibly find a relationship between the RL algorithm and the RS application.\n\\begin{figure}\n\\centering     \n\\subfigure[Popular metrics distribution]{\\label{fig:metrics}\\includegraphics[width=50mm]{metrics}}\n\\subfigure[Popular datasets distribution]{\\label{fig:dbs}\\includegraphics[width=50mm]{dbs}}\n\\caption{Evaluation metrics and datasets used more often by RLRSs}\n\\label{fig:MA}\n\\end{figure}\n\\textbf{Environment and Evaluation. }\nFig.~\\ref{fig:metrics} depicts the most popular metrics by RLRSs.  As shown, there is no metric specifically developed for RLRSs and almost all metrics are borrowed from Information Retrieval field.  Although RSs and Information Retrieval fields are very close, but they are eventually different fields.  Reward is also among the popular metrics used by RLRSs, which is a common metric in the RL literature.  This analysis shows that there is a lack of metrics specifically designed for RLRSs.  On the other hand, Fig.~\\ref{fig:dbs} illustrates the most popular datasets used to evaluate RLRSs.  MovieLens is the most popular dataset by far. A large number of datasets used for evaluation are not defined or public (see Tables~\\ref{tab:rl-based} and~\\ref{tab:drl-based}).  This limits the application and design of RLRSs to only a few applications, like entertainment.  Therefore, it is important to collect and share more datasets in various domains to better evaluate RLRSs.\nAnother aspect in RLRSs evaluation that needs further improvement in the future is to have a stronger and more unified simulation environment, something that can play the role of a benchmark in RLRSs' evaluation.  As stated earlier, online evaluation is the natural method to evaluate RLRSs; however, it is difficult and costly to conduct a proper online study.  On the other hand, offline environment, i.e., a dataset, is static and biased.  Therefore, this clearly shows the importance of developing a strong, general-purpose simulator for RLRSs, something like OpenAI Gym~ in the RL literature.  Although some environment simulators have recently been developed for RLRSs~, this trend should be continued and fortified. \n\\textbf{Reproducibility. }\nThe effect of reproducibility on the advancement of a field is undeniable.  For example, the field of image synthesis using GANs has seen astonishing results in a short period of time~, and undoubtedly, an effective factor has been the common practice of sharing implementation codes, datasets, and research results.  As illustrated in Tables~\\ref{tab:rl-based} and~\\ref{tab:drl-based}, we cannot see this trend in the RLRS research community and only about 16$\\%$ of researchers have shared their implementation codes.  It would be helpful and can significantly accelerate the field's progress if researchers accurately present the value of important parameters and hyperparameters used in their experiments,  to perform statistical significance testing for results presented, to disclose which random seeds have been used to repeat experiments,\nand to share their implementation codes and datasets (if datasets are not already public).  \n\\vspace{-10pt}", "cites": [5190, 5165, 5206, 5168, 5205, 5207, 62, 1346, 5204, 7621, 8881, 157, 620, 5180, 1382], "cite_extract_rate": 0.45454545454545453, "origin_cites_number": 33, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section provides a thoughtful analysis of open research directions in RL-based recommender systems, synthesizing insights from multiple papers and identifying key challenges and trends. It critically evaluates the limitations of current approaches in slate recommendation, explainability, and evaluation environments. While it generalizes some patterns, such as the lack of domain-specific metrics and datasets, the abstraction remains somewhat grounded in practical concerns rather than reaching a fully meta-level analysis."}}
