{"id": "1446b4fa-7c0c-42d1-a70f-09fde4a71f2c", "title": "CORD", "level": "subsection", "subsections": [], "parent_id": "e695a48d-424b-476b-aca3-58d54c720c98", "prefix_titles": [["title", "A Survey on Edge Computing Systems and Tools"], ["section", "Open Source Edge Computing Projects"], ["subsection", "CORD"]], "content": "\\begin{figure}[!tb]\n\\begin{center}\n\\includegraphics[width=0.5\\textwidth]{CORD.pdf}\n\\caption{Hardware Architecture of CORD.}\\label{fig:CORD}\n\\end{center}\n\\end{figure} \nCORD is an open source project of ONF initiated by AT\\&T and is designed for network operators. Current network infrastructure is built with closed proprietary integrated systems provided by network equipment providers. Due to the closed property, the network capability cannot scale up and down dynamically. And the lack of flexibility results in inefficient utilization of the computing and networking resources. CORD plans to reconstruct the edge network infrastructure to build datacenters with SDN~, NFV~ and Cloud technologies. It attempts to slice the computing, storage and network resources so that these datacenters can act as clouds at the edge, providing agile services for end users.\n\\begin{figure}[!tb]\n\\begin{center}\n\\includegraphics[width=0.5\\textwidth]{CORD_SW.pdf}\n\\caption{Software Architecture of CORD.}\\label{fig:CORD_SW}\n\\end{center}\n\\end{figure}\nCORD is an integrated system built from commodity hardware and open source software. Fig.~\\ref{fig:CORD} shows the hardware architecture of CORD~. It uses commodity servers that are interconnected by a Fabric of White-box switches. White-box switch~ is  a  component of SDN switch, which is responsible to regulate the flow of data according to SDN controller. These commodity servers provide computing, storage resources, and the fabric of switches are used to build the network. This switching fabric is organized to a Spine-Leaf topology , a kind of flat network topology structure which adds a horizontal network structure parallel to the trunk longitudinal network structure, and then adds corresponding switching network on the horizontal structure. Comparing to traditional three-tier network topology, it can provide scalable throughput for greater East-to-West network traffic, that is, traffic coming from network diagram drawings that usually depict local area network (LAN) traffic horizontally. In addition, specialized access hardware is required to connect subscribers. The subscribers can be divided into three categories for different use cases, mobile subscribers, enterprise subscribers and residential subscribers. Each category demands different access hardware due to different access technologies. In terms of software, Fig.~\\ref{fig:CORD_SW} shows the software architecture of CORD~. Based on the servers and the fabric of switches, OpenStack provides with IaaS capability for CORD, it manages the compute, storage and networking resources as well as creating virtual machines and virtual networks. Docker is used to run services in containers for isolation. ONOS(Open Network Operating System) is a network operating system which is used to manage network components like the switching fabric and provide communication services to end-users. XOS provides a control plane to assemble and compose services. Other software projects provide component capabilities, for example, vRouter(Virtual Router) provides with virtual routing functionality.\nThe edge of the operator network is a sweet spot for edge computing because it connects customers with operators and is close to customers' applications as data sources. CORD takes edge computing into consideration and moves to support edge computing as a platform to provide edge cloud services (from the released version $4.1$). CORD can be deployed into three solution, M-CORD (Mobile CORD), R-CORD (Residential CORD) and E-CORD (Enterprise CORD) for different use cases. M-CORD focuses on mobile network, especially 5G network, and it plans to disaggregate and virtualize cellular network functions to enable services be created and scaled dynamically. This agility helps to provide multi-access edge services for mobile applications. For those use cases like driverless cars or drones, users can rent the edge service to run their edge applications. Similarly, R-CORD and E-CORD are designed to be agile service delivery platforms but for different users, residential and enterprise users relatively. \nSo far, the deployment of CORD is still under test among network operators, and more researches are needed to combine CORD with various edge applications.", "cites": [8836], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of CORD and its components, using minimal synthesis with the cited paper which focuses on NFV. There is no clear integration of ideas from multiple sources or deeper analysis of how CORD fits into broader trends in edge computing. The section lacks critical evaluation of CORD's limitations or potential challenges, and while it introduces some concepts (e.g., Spine-Leaf topology), it does not generalize or abstract these to broader principles or patterns in the field."}}
{"id": "5f68758b-531e-45bb-9d03-537df0f852c4", "title": "EdgeX Foundry", "level": "subsection", "subsections": [], "parent_id": "e695a48d-424b-476b-aca3-58d54c720c98", "prefix_titles": [["title", "A Survey on Edge Computing Systems and Tools"], ["section", "Open Source Edge Computing Projects"], ["subsection", "EdgeX Foundry"]], "content": "EdgeX Foundry is a standardized interoperability framework for IoT edge computing, whose sweet spots are edge nodes such as gateways, hubs, and routers~. It can connect with various sensors and devices via different protocols, manage them and collect data from them, and export the data to a local application at the edge or the cloud for further processing. EdgeX is designed to be agnostic to hardware, CPU, operating system, and application environment. It can run natively or run in docker containers.\nFig.~\\ref{fig:EdgeX}~ shows the architecture of EdgeX Foundry. ``South side'' at the bottom of the figure includes all IoT objects, and the edge of the network that communicates directly with those devices, sensors, actuators and other IoT objects to collect the data from them. Relatively, ``north side'' at the top of the figure includes the Cloud (or Enterprise system) where data are collected, stored, aggregated, analyzed, and turned into information, and the part of the network that communicates with the Cloud. EdgeX Foundry connects these two sides regardless of the differences of hardware, software and network. EdgeX tries to unify the manipulation method of the IoT objects from the south side to a common API, so that those objects can be manipulated in the same way by the applications of north side.\nEdgeX uses a Device Profile to describe a south side object. A Device Profile defines the type of the object, the format of data that the object provides, the format of data to be stored in EdgeX and the commands used to manipulate this object. Each Device Profile involves a Device Service, which is a service that converts the format of the data, and translates the commands into instructions that IoT objects know how to execute. EdgeX provides SDK for developers to create Device Services, so that it can support for any combination of device interfaces and protocols by programming.\n\\begin{figure*}[!tb]\n\\begin{center}\n\\includegraphics[width=0.65\\textwidth]{EdgeX2.png}\n\\caption{Architecture of EdgeX Foundry.}\\label{fig:EdgeX}\n\\end{center}\n\\end{figure*}\nEdgeX consists of a collection of microservices, which allows services to scale up and down based on device capability. These microservices can be grouped into four service layers and two underlying augmenting system services, as depicted in Fig.~\\ref{fig:EdgeX}. The four service layers include Device Services Layer, Core Services Layer, Supporting Services Layer and Export Services Layer, respectively; the two underlying augmenting system services are System Management and Security, respectively. Each of the six layers consists of several components and all components use a common Restful API for configuration.\n\\textit{1) Device Services Layer:}\nThis layer consists of Device Services. According to the Device Profiles, Device Service Layer converts the format of the data, sends them to Core Services Layer, and translates the command requests from the Core Services Layer.\n\\textit{2) Core Services Layer:}\nThis layer consists of four components: Core Data, Command, Metadata, and Registry \\& Configuration. Core Data is a persistence repository as well as a management service. It stores and manages the data collected from the south side objects. Command is a service to offer the API for command requests from the north side to Device Services. Metadata is a repository and management service for metadata about IoT objects. For example, the Device Profiles are uploaded and stored in Metadata. Registry \\& Configuration provides centralized management of configuration and operating parameters for other microservices. \n\\textit{3) Supporting Services Layer:}\nThis layer is designed to provide edge analytics and intelligence~. Now the Rules Engine, Alerting and Notification, Scheduling and Logging microservices are implemented. A target range of data can be set to trigger a specific device actuation as a rule and Rules Engine helps realize the rule by monitoring the incoming data. Alerting and Notifications can send notifications or alerts to another system or person by email, REST callback or other methods when an urgent actuation or a service malfunction happens. The scheduling module can set up a timer to regularly clean up the stale data. Logging is used to record the running information of EdgeX.\n\\textit{4) Export Services Layer:}\nThis layer connects EdgeX with North Side and consists of Client Registration and Export Distribution. Client Registration enables clients like a specific cloud or a local application to register as recipients of data from Core Data. Export Distribution distributes the data to the Clients registered in Client Registration. \n\\textit{5) System Management and Security:}\nSystem Management provides management operations including installation, upgrade, starting, stopping and monitoring, as EdgeX is scalable and can be deployed dynamically. Security is designed to protect the data and command of IoT objects connected with EdgeX Foundry.\nEdgeX is designed for the user cases dealing with multitudes of sensors or devices, such as automated factories, machinery systems and lots of other cases in IoT. Now EdgeX Foundry is in the rapid upgrading phase, and more features will be added in future releases. An EdgeX UI is in development as a web-based interface to add and manage the devices.", "cites": [8836], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.2, "abstraction": 1.3}, "insight_level": "low", "analysis": "The section provides a detailed description of EdgeX Foundry's architecture and components but does not effectively synthesize or integrate the cited paper. The only mention of a cited work is a reference to a paper on NFV, which is not elaborated upon or connected to EdgeX's design or functionality. There is minimal critical analysis or abstraction, as the section remains focused on describing the system without evaluating its strengths, limitations, or broader implications."}}
{"id": "e29f4cf4-8bb2-43ab-9414-a3c4becd1890", "title": "Low-power system design and power management", "level": "subsubsection", "subsections": [], "parent_id": "cef0da1f-0af4-4654-af0c-b8d0a6a63c4a", "prefix_titles": [["title", "A Survey on Edge Computing Systems and Tools"], ["section", "Enhancing Energy Efficiency of Edge Computing Systems"], ["subsection", "At the Middle Edge Server Layer"], ["subsubsection", "Low-power system design and power management"]], "content": "In~, the tactical cloudlet is presented and its energy consumption when performing VM synthesis is evaluated particularly, under different cloudlet provisioning mechanisms. The results show that the largest amount of energy is consumed by i) VM synthesis due to the large payload size, and ii) on-demand VM provisioning due to the long application-ready time. Such results lead to the high energy efficiency policy: combining cached VM with cloudlet push for cloudlet provision.\nA service-oriented architecture for fog/edge computing, Fog Data, is proposed and evaluated in~. It is implemented with an embedded computer system and performs data mining and data analytics on the raw data collection from the wearable sensors (in telehealth applications). With Fog Data, orders of magnitude data are reduced for transmission, thus leading to enormous energy saving. Furthermore, Fog Data is with a low power architecture design, and even consumes much less energy than that of a Raspberry Pi.\nIn~, a performance-aware orchestrator for Docker containers, named DockerCap, is developed to meet the power consumption constraints of the edge server (fog node). Following the observe-decide-act loop structure, DockerCap is able to manage container resources at run-time and provide soft-level power capping strategies. The experiments demonstrate that the obtained results with DockerCap is comparable to that from the power capping solution provided by the hardware (Intel RAPL).\nAn energy aware edge computer architecture is designed to be portable and usable in the fieldwork scenarios in~. Based on the architecture, a high-density cluster prototype is built using the compact general-purpose commodity hardware. Power management policies are implemented in the prototype to enable the real-time energy-awareness. Through various experiments, it shows that both the load balance strategies and cluster configurations have big impacts on the system energy consumption and responsiveness.", "cites": [4754], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes several approaches to enhancing energy efficiency at the middle edge server layer, connecting ideas such as VM synthesis, data reduction, and power capping. It provides a clear narrative by highlighting common themes like energy consumption from payload size and provisioning methods. However, it lacks deeper critical evaluation or comparison of limitations, and while some general patterns are noted (e.g., the impact of load balancing and cluster configuration), it does not reach a meta-level abstraction or propose a novel framework."}}
{"id": "92004c3a-1e93-4cfc-9226-2ae148fcfafd", "title": "At the Bottom Device Layer", "level": "subsection", "subsections": ["b5cb6b35-d6f7-494a-b718-94bbbaeaf422", "225a8b24-31ce-4561-a161-bff22095366d"], "parent_id": "a3a2618a-548a-4cd2-95bc-06a60c44ffb0", "prefix_titles": [["title", "A Survey on Edge Computing Systems and Tools"], ["section", "Enhancing Energy Efficiency of Edge Computing Systems"], ["subsection", "At the Bottom Device Layer"]], "content": "As a well-recognized fact, the IoT devices in edge computing usually have strict energy constraints, e.g., limited battery life and energy storage. Thus, it remains a key challenge to power a great number (can up to tens of billions) of IoT devices at the edge, especially for those resource-intensive applications or services~. We review the energy saving strategies adopted at the device layer of the edge computing diagram. Specifically, we go through three major approaches to achieving high energy efficiency in different edge/fog computing systems.", "cites": [3357], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section introduces the topic of energy efficiency at the device layer but lacks synthesis of the cited paper or others. It merely states a general fact about IoT devices and mentions the existence of strategies without elaborating on specific methods or connecting different sources. There is no critical evaluation or abstraction to broader principles or trends."}}
{"id": "98b9a8e8-5e84-4189-aa8d-48851fdccc02", "title": "Systems and Toolkits", "level": "subsection", "subsections": [], "parent_id": "478327fb-c0c5-49d5-ae91-7fa642949abc", "prefix_titles": [["title", "A Survey on Edge Computing Systems and Tools"], ["section", "Deep Learning Optimization at the Edge"], ["subsection", "Systems and Toolkits"]], "content": "Building systems to support deep learning at the edge is currently a hot topic for both industry and academy. There are several challenges when offloading state-of-the-art AI techniques on the edge directly, including computing power limitation, data sharing and collaborating, and mismatch between edge platform and AI algorithms. To address these challenges, OpenEI is proposed as an Open Framework for Edge Intelligence~. OpenEI is a lightweight software platform to equip edges with intelligent processing and data sharing capability. OpenEI consists of three components: a package manager to execute the real-time deep learning task and train the model locally, a model selector to select the most suitable model for different edge hardware, and a library including a RESTFul API for data sharing. The goal of OpenEI is that any edge hardware will has the intelligent capability after deploying it. \nIn the industry, some top-leading tech-giants have published several projects to move the deep learning functions from the cloud to the edge. Except Microsoft published Azure IoT Edge which have been introduced in Sec.~\\ref{subsec:AzureIoT}, Amazon and Google also build their services to support deep learning on the edge.\nTable~\\ref{table: Comparison of Deep learning Systems on Edge} summarizes the features of the systems which will be discussed below.\nAmazon Web Services (AWS) has published IoT Greengrass ML Inference~ after IoT Greengrass. AWS IoT Greengrass ML Inference is a software to support machine learning inferences on local devices. With AWS IoT Greengrass ML Inference, connected IoT devices can run AWS Lambda functions and have the flexibility to execute predictions based on those deep learning models created, trained, and optimized in the cloud. AWS IoT Greengrass consists of three software distributions: AWS IoT Greengrass Core, AWS IoT Device SDK, and AWS IoT Greengrass SDK. Greengrass is flexible for users as it includes a pre-built TensorFlow, Apache MXNet, and Chainer package, and it can also work with Caffe2 and Microsoft Cognitive Toolkit. \nCloud IoT Edge~ extends Google Cloud's data processing and machine learning to edge devices by taking advantages of Google AI products, such TensorFlow Lite and Edge TPU. Cloud IoT Edge can either run on Android or Linux-based operating systems. It is made up of three components: Edge Connect ensures the connection to the cloud and the updates of software and firmware, Edge ML runs ML inference by TensorFlow Lite, and Edge TPU specific designed to run TensorFlow Lite ML models. Cloud IoT Edge can satisfy the real-time requirement for the mission-critical IoT applications, as it can take advantages of Google AI products (such as TensorFlow Lite and Edge TPU) and optimize the performance collaboratively.\n\\begin{table*}[!htp]\n\t\\caption{Comparison of Deep learning Systems on Edge}\n\t\\label{table: Comparison of Deep learning Systems on Edge}\n\t\\begin{tabular}{ l l l l }\n\t\t\\hline\n\t\t\\textbf{Features} & \\textbf{AWS IoT Greengrass} & \\textbf{Azure IoT Edge} & \\textbf{Cloud IoT Edge} \\\\ \\hline\n\t\tDeveloper & Amazon & Microsoft & Google \\\\ \\hline\n\t\tComponents & \\begin{tabular}[c]{@{}l@{}}IoT Greengrass Core, IoT Device SDK,  \\\\ IoT Greengrass SDK\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}IoT Edge modules, IoT Edge runtime,  \\\\ Cloud-based interface\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}Edge Connect, Edge ML, Edge TPU\\end{tabular} \\\\ \\hline\n\t\tOS & Linux, macOS, Windows & Windows, Linux, macOS & Linux, macOS, Windows, Android \\\\ \\hline\n\t\tTarget device & Multiple platforms (GPU-based, Raspberry Pi) & Multiple platforms & TPU\\\\ \\hline\n\t\tCharacteristic & Flexible & Windows friendly & Real-time \\\\ \\hline\n\t\\end{tabular}\n\\end{table*}", "cites": [7842], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic comparison of three industry-led edge computing systems (AWS IoT Greengrass, Azure IoT Edge, and Cloud IoT Edge) and briefly introduces OpenEI. While it does mention common features and target platforms, it lacks deeper synthesis of how these systems address the broader challenges of deep learning at the edge. There is limited critical evaluation of the systems' strengths and weaknesses, and no abstraction to broader principles or frameworks for deep learning optimization at the edge."}}
{"id": "083ff676-0bf0-44e0-859d-63889cdc7ccb", "title": "Deep Learning Packages", "level": "subsection", "subsections": [], "parent_id": "478327fb-c0c5-49d5-ae91-7fa642949abc", "prefix_titles": [["title", "A Survey on Edge Computing Systems and Tools"], ["section", "Deep Learning Optimization at the Edge"], ["subsection", "Deep Learning Packages"]], "content": "Many deep learning packages have been widely used to deliver the deep learning algorithms and deployed on the cloud data centers, including TensorFlow~, Caffe~, PyTorch~, and MXNet~. Due to the limitations of computing resources at the edge, the packages designed for the cloud are not suitable for edge devices. Thus, to support data processing with deep learning models at the edge, several edge-based deep learning frameworks and tools have been released. In this section, we introduce TensorFlow Lite, Caffe2, PyTorch, MXNet, CoreML~, and TensorRT~, whose features are summarized in Tables \\ref{table: Comparison of Deep Learning Frameworks on Edge}.\nTensorFlow Lite~ is TensorFlow's lightweight solution which is designed for mobile and edge devices. TensorFlow is developed by Google in 2016 and becomes one of the most widely used deep learning frameworks in cloud data centers. To enable low-latency inference of on-device deep learning models, TensorFlow Lite leverages many optimization techniques, including optimizing the kernels for mobile apps, pre-fused activations, and quantized kernels that allow smaller and faster (fixed-point math) models. \nFacebook published Caffe2~ as a lightweight, modular, and scalable framework for deep learning in 2017. Caffe2 is a new version of Caffe which is first developed by UC Berkeley AI Research (BAIR) and community contributors. Caffe2 provides an easy and straightforward way to play with the deep learning and leverage community contributions of new models and algorithms. Comparing with the original Caffe framework, Caffe2 merges many new computation patterns, including distributed computation, mobile, reduced precision computation, and more non-vision use cases. Caffe2 supports multiple platforms which enable developers to use the power of GPUs in the cloud or at the edge with cross-platform libraries.\nPyTorch~ is published by Facebook. It is a python package that provides two high-level features: tensor computation with strong GPU acceleration and deep Neural Networks built on a tape-based auto-grad system. Maintained by the same company (Facebook), PyTorch and Caffe2 have their own advantages. PyTorch is geared toward research, experimentation and trying out exotic neural networks, while caffe2 supports more industrial-strength applications with a heavy focus on the mobile. In 2018, Caffe2 and PyTorch projects merged into a new one named PyTorch 1.0, which would combine the user experience of the PyTorch frontend with scaling, deployment and embedding capabilities of the Caffe2 backend.\nMXNet~ is a flexible and efficient library for deep learning. It was initially developed by the University of Washington and Carnegie Mellon University, to support CNN and long short-term memory networks (LSTM). In 2017, Amazon announced MXNet as its choice of deep learning framework. MXNet places a special emphasis on speeding up the development and deployment of large-scale deep neural networks. It is designed to support multiple different platforms (either cloud platforms or the edge ones) and can execute training and inference tasks.  Furthermore, other than the Windows, Linux, and OSX operating systems based devices, it also supports the Ubuntu Arch64 and Raspbian ARM based operating systems.\nCoreML~ is a deep learning framework optimized for on-device performance at memory footprint and power consumption. Published by Apple, users can integrate the trained machine learning model into Apple products, such as Siri, Camera, and QuickType. CoreML supports not only deep learning models, but also some standard models such as tree ensembles, SVMs, and generalized linear models. Built on top of low level technologies, CoreML aims to make full use of the CPU and GPU capability and ensure the performance and efficiency of data processing.\nThe platform of TensorRT~ acts as a deep learning inference to run the models trained by TensorFlow, Caffe, and other frameworks. Developed by NVIDIA company, it is designed to reduce the latency and increase the throughput when executing the inference task on NVIDIA GPU. To achieve computing acceleration, TensorRT leverages several techniques, including weight and activation precision calibration, layer and tensor fusion, kernel auto-tuning, dynamic tensor memory, and multi-stream execution.\nConsidering the different performance of the packages and the diversity of the edge hardware, it is challenging to choose a suitable package to build edge computing systems. To evaluate the deep learning frameworks at the edge and provide a reference to select appropriate combinations of package and edge hardware, pCAMP~ is proposed. It compares the packages' performances (w.r.t. the latency, memory footprint, and energy) resulting from five edge devices and observes that no framework could win over all the others at all aspects.\nIt indicates that there is much room to improve the frameworks at the edge. Currently, developing a lightweight, efficient and high-scalability framework to support diverse deep learning modes at the edge cannot be more important and urgent.\nIn addition to these single-device based frameworks, more researchers focus on distributed deep learning models over the cloud and edge. DDNN~ is a distributed deep neural network architecture across cloud, edge, and edge devices. DDNN maps the sections of a deep neural network onto different computing devices, to minimize communication and resource usage for devices and maximize usefulness of features extracted from the cloud. \nNeurosurgeon~ is a lightweight scheduler which can automatically partition DNN computation between mobile devices and datacenters at the granularity of neural network layers. By effectively leveraging the resources in the cloud and at the edge, Neurosurgeon achieves low computing latency, low energy consumption, and high traffic throughput. \n\\begin{table*}[!htp]\n\t\\centering\n\t\\caption{Comparison of Deep Learning Packages on Edge}\n\t\\label{table: Comparison of Deep Learning Frameworks on Edge}\n\t\\begin{tabular}{ l l l l l l l }\n\t\t\\hline\n\t\t\\textbf{Features} & \\textbf{TensorFlow Lite} & \\textbf{Caffe2} & \\textbf{PyTorch} & \\textbf{MXNet} & \\textbf{CoreML} & \\textbf{TensorRT} \\\\ \\hline\n\t\tDeveloper & Google & Facebook & Facebook & DMLC, Amazon & Apple & NVIDIA \\\\ \\hline\n\t\t\\begin{tabular}[c]{@{}l@{}}Open Source \\\\ License\\end{tabular} & Apache-2.0 & Apache-2.0 & BSD & Apache-2.0 & Not open source & Not open source \\\\ \\hline\n\t\tTask & Inference & Training, Inference & Training, Inference & Training, Inference & Inference & Inference \\\\ \\hline\n\t\tTarget Device & \\begin{tabular}[c]{@{}l@{}}Mobile and \\\\ embedded device\\end{tabular} & Multiple platform & Multiple platform & Multiple platform & Apple devices & NVIDIA GPU \\\\ \\hline\n\t\tCharacteristic & Latency & \\begin{tabular}[c]{@{}l@{}}Lightweight, modular, \\\\ and scalable\\end{tabular} & Research & \\begin{tabular}[c]{@{}l@{}}Large-scale \\\\ deep neural networks\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}Memory footprint and \\\\ power consumption.\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}Latency and\\\\  throughput\\end{tabular} \\\\ \\hline\n\t\\end{tabular}\n\\end{table*}", "cites": [3416, 2672, 2671, 7199, 4755], "cite_extract_rate": 0.45454545454545453, "origin_cites_number": 11, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple cited papers to provide a structured comparison of edge-optimized deep learning frameworks, highlighting their features and target devices. It offers some critical evaluation by discussing limitations, such as the lack of a single framework dominating all performance metrics, and introduces a benchmarking tool (pCAMP) to assess these frameworks. However, the analysis remains largely descriptive and does not deeply generalize or abstract patterns into broader principles."}}
{"id": "bb0c2551-78bb-4b6f-b8fd-cff2ffbb6a59", "title": "FPGA-based Hardware", "level": "subsubsection", "subsections": [], "parent_id": "e71e3ef2-27ab-4b8d-8ebe-b33c2376117c", "prefix_titles": [["title", "A Survey on Edge Computing Systems and Tools"], ["section", "Deep Learning Optimization at the Edge"], ["subsection", "Hardware System"], ["subsubsection", "FPGA-based Hardware"]], "content": "A field-programmable gate array (FPGA) is an integrated circuit and can be configured by the customer or designer after manufacturing. FPGA based accelerators can achieve high performance computing with low energy, high parallelism, high flexibility, and high security~.\n implements a CNN accelerator on a VC707 FPGA board. The accelerator focuses on solving the problem that the computation throughput does not match the memory bandwidth well. By quantitatively analyzing the two factors using various optimization techniques, the authors provide a solution with better performance and lower FPGA resource requirement, and their solution achieves a peak performance of $61.62$ GOPS under a $100MHz$ working frequency. \nFollowing the above work, Qiu et al.~ propose a CNN accelerator designed upon the embedded FPGA, Xilinx Zynq ZC706, for large-scale image classification. It presents an in-depth analysis of state-of-the-art CNN models and shows that Convolutional layers are computational-centric and Fully-Connected layers are memory-centric. The average performances of the CNN accelerator at convolutional layers and the full CNN are $187.8$ GOPS and $137.0$ GOPS under a $150MHz$ working frequency, respectively, which outperform previous approaches significantly.\nAn efficient speech recognition engine (ESE) is designed to speed up the predictions and save energy when applying the deep learning model of LSTM. ESE is implemented in a Xilinx XCKU060 FPGA opearting at $200MHz$. For the sparse LSTM network, it can achieve 282GOPS, corresponding to a $2.52$ TOPS on the dense LSTM network. Besides, energy efficiency improvements of $40x$ and $11.5x$ are achieved, respectively, compared with the CPU and GPU based solution.", "cites": [4756], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a factual description of FPGA-based hardware for deep learning optimization, listing specific implementations and performance metrics from different papers. It offers limited synthesis by briefly connecting these implementations but fails to present a deeper, integrated narrative. There is minimal critical analysis or identification of broader patterns or limitations across the cited works."}}
