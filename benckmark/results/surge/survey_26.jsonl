{"id": "b57b31e0-a2c2-4374-8485-3b551efe589f", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "c94c5602-77e6-49fe-a100-dda7b1e8d141", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Introduction"]], "content": "\\label{Sec:Introduction}}\n\\IEEEPARstart{D}{eep}  neural networks (DNNs) have become the fundamental infrastructure in today's artificial intelligence (AI) systems. Different types of tasks have typically involved different types of networks. For example, multi-layer perceptron (MLP) or the fully connected (FC) network is the classical type of neural network, which is composed of multiple linear layers and nonlinear activations stacked together~. Convolutional neural networks (CNNs) introduce convolutional layers and pooling layers for processing shift-invariant data such as images~. And recurrent neural networks (RNNs) utilize recurrent cells to process sequential data or time series data~. Transformer is a new type of neural network. It mainly utilizes the self-attention mechanism~ to extract intrinsic features~ and shows great potential for extensive use in AI applications.\n\\begin{figure*}[htp] \n\t\\centering\n\t\\includegraphics[width=0.95\\linewidth]{fig/timeline_v5.pdf}\n\t\\vspace{-0.5em}\n\t\\caption{Key milestones in the development of transformer. The vision transformer models are marked in red.}\n\t\\label{fig:timeline}\n\t\\vspace{-1.5em}\n\\end{figure*}\nTransformer was first applied to natural language processing (NLP) tasks where it achieved significant improvements~. For example, Vaswani~\\etal~ first proposed transformer based on attention mechanism for machine translation and English constituency parsing tasks. Devlin~\\etal~ introduced a new language representation model called BERT (short for Bidirectional Encoder Representations from Transformers), which pre-trains a transformer on unlabeled text taking into account the context of each word as it is bidirectional. When BERT was published, it obtained state-of-the-art performance on 11 NLP tasks. Brown~\\etal~ pre-trained a massive transformer-based model called GPT-3 (short for Generative Pre-trained Transformer 3) on 45 TB of compressed plaintext data using 175 billion parameters. It achieved strong performance on different types of downstream natural language tasks without requiring any fine-tuning. These transformer-based models, with their strong representation capacity, have achieved significant breakthroughs in NLP.\nInspired by the major success of transformer architectures in the field of NLP, researchers have recently applied transformer to computer vision (CV) tasks. In vision applications, CNNs are considered the fundamental component~, but nowadays transformer is showing it is a potential alternative to CNN. Chen~\\etal~ trained a sequence transformer to auto-regressively predict pixels, achieving results comparable to CNNs on image classification tasks. Another vision transformer model is ViT, which applies a pure transformer directly to sequences of image patches to classify the full image. Recently proposed by Dosovitskiy~\\etal~, it has achieved state-of-the-art performance on multiple image recognition benchmarks. In addition to image classification, transformer has been utilized to address a variety of other vision problems, including object detection~, semantic segmentation~, image processing~, and video understanding~. Thanks to its exceptional performance, more and more researchers are proposing transformer-based models for improving a wide range of visual tasks.\nDue to the rapid increase in the number of transformer-based vision models, keeping pace with the rate of new progress is becoming increasingly difficult. As such, a survey of the existing works is urgent and would be beneficial for the community. In this paper, we focus on providing a comprehensive overview of the recent advances in vision transformers and discuss the potential directions for further improvement. To facilitate future research on different topics, we categorize the transformer models by their application scenarios, as listed in Table~\\ref{tab:overview}. The main categories include backbone network, high/mid-level vision, low-level vision, and video processing. High-level vision deals with the interpretation and use of what is seen in the image~, whereas mid-level vision deals with how this information is organized into what we experience as objects and surfaces~. Given the gap between high- and mid-level vision is becoming more obscure in DNN-based vision systems~, we treat them as a single category here. A few examples of transformer models that address these high/mid-level vision tasks include DETR~, deformable DETR~ for object detection, and Max-DeepLab~ for segmentation. Low-level image processing mainly deals with extracting descriptions from images (such descriptions are usually represented as images themselves)~. Typical applications of low-level image processing include super-resolution, image denoising, and style transfer. At present, only a few works~ in low-level vision use transformers, creating the need for further investigation. Another category is video processing, which is an important part in both computer vision and image-based tasks. Due to the sequential property of video, transformer is inherently well suited for use on video tasks~, in which it is beginning to perform on par with conventional CNNs and RNNs. Here, we survey the works associated with transformer-based visual models in order to track the progress in this field. Figure~\\ref{fig:timeline} shows the development timeline of vision transformer â€” undoubtedly, there will be many more milestones in the future.\nThe rest of the paper is organized as follows. Section 2 discusses the formulation of the standard transformer and the self-attention mechanism. Section 4 is the main part of the paper, in which we summarize the vision transformer models on backbone, high/mid-level vision, low-level vision, and video tasks. We also briefly describe efficient transformer methods, as they are closely related to our main topic. In the final section, we give our conclusion and discuss several research directions and challenges. Due to the page limit, we describe the methods of transformer in NLP in the supplemental material, as the research experience may be beneficial for vision tasks. In the supplemental material, we also review the self-attention mechanism for CV as the supplementary of vision transformer models. In this survey, we mainly include the representative works (early, pioneering, novel, or inspiring works) since there are many preprinted works on arXiv and we cannot include them all in limited pages.\n\\begin{table*}[htb]\n\t\\centering\n\t\\renewcommand\\arraystretch{1.0}\n\t\\caption{Representative works of vision transformers.}\n\t\\label{tab:overview}\n\t\\vspace{-0.5em}\n\t\\footnotesize\n\t\\setlength{\\tabcolsep}{3.5pt}{\n\t\t\\begin{tabular}{c|c|c|c|c}\n\t\t\t\\Xhline{1.2pt}\n\t\t\tCategory & Sub-category & Method & Highlights & Publication \\\\\n\t\t\t\\hline\n\t\t\t\\multirow{5}{*}{\\shortstack{Backbone}} & \\multirow{3}{*}{Supervised pretraining}  & ViT~   & Image patches, standard transformer & ICLR 2021  \\\\\n\t\t\t& & TNT~   & Transformer in transformer, local attention & NeurIPS 2021 \\\\\n\t\t\t& & Swin~   & Shifted window, window-based self-attention & ICCV 2021 \\\\\n\t\t\t\\cline{2-5}\n\t\t\t& \\multirow{3}{*}{Self-supervised pretraining}  & iGPT~   & Pixel prediction self-supervised learning, GPT model & ICML 2020  \\\\\n\t\t\t& & MoCo v3~   & Contrastive self-supervised learning, ViT & ICCV 2021 \\\\\n\t\t\t& & MAE~~ & Masked image modeling, ViT & CVPR 2022 \\\\\n\t\t\t\\hline\n\t\t\t\\multirow{9}{*}{\\shortstack{High/Mid-level\\\\vision}} & \\multirow{3}{*}{Object detection} & DETR~   & Set-based prediction, bipartite matching, transformer  & ECCV 2020  \\\\\n\t\t\t& & Deformable DETR~   & DETR, deformable attention module & ICLR 2021 \\\\\n\t\t\t& & UP-DETR~   & Unsupervised pre-training, random query patch detection & CVPR 2021 \\\\\n\t\t\t\\cline{2-5}\n\t\t\t& \\multirow{3}{*}{Segmentation} & Max-DeepLab~   &PQ-style bipartite matching, dual-path transformer  & CVPR 2021  \\\\\n\t\t\t& & VisTR~   & Instance sequence matching and segmentation & CVPR 2021 \\\\\n\t\t\t& & SETR~   & Sequence-to-sequence prediction, standard transformer & CVPR 2021 \\\\\n\t\t\t\\cline{2-5}\n\t\t\t& \\multirow{3}{*}{Pose Estimation} & Hand-Transformer~   & Non-autoregressive transformer, 3D point set  & ECCV 2020  \\\\\n\t\t\t& & HOT-Net~   & Structured-reference extractor & MM 2020 \\\\\n\t\t\t& & METRO~   & Progressive dimensionality reduction & CVPR 2021 \\\\\n\t\t\t\\hline\n\t\t\t\\multirow{5}{*}{\\shortstack{Low-level\\\\vision}} & \\multirow{3}{*}{Image generation} & Image Transformer~ & Pixel generation using transformer  & ICML 2018  \\\\\n\t\t\t&  & Taming transformer~ & VQ-GAN, auto-regressive transformer  & CVPR 2021  \\\\\n\t\t\t&  & TransGAN~ & GAN using pure transformer architecture  & NeurIPS 2021  \\\\\n\t\t\t\\cline{2-5}\n\t\t\t& \\multirow{2}{*}{Image enhancement} & IPT~   & Multi-task, ImageNet pre-training, transformer model & CVPR 2021  \\\\\n\t\t\t& & TTSR~  & Texture transformer, RefSR & CVPR 2020 \\\\\t\t\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\shortstack{Video\\\\processing}} & Video inpainting & STTN~  & Spatial-temporal adversarial loss  & ECCV 2020  \\\\\n\t\t\t\\cline{2-5}\n\t\t\t& Video captioning &  Masked Transformer~  & Masking network, event proposal  & CVPR 2018  \\\\\n\t\t\t\\hline\n\t\t\t\\multirow{4}{*}{\\shortstack{Multimodality }} &\\multirow{1}{*}{Classification}  & CLIP~   & NLP supervision for images, zero-shot transfer & arXiv 2021 \\\\\n\t\t\t\\cline{2-5}\n\t\t\t& \\multirow{2}{*}{Image generation}  & DALL-E~   & Zero-shot text-to image generation & ICML 2021  \\\\\n\t\t\t&& Cogview~ & VQ-VAE, Chinese input & NeurIPS 2021 \\\\\n\t\t\t\\cline{2-5}\n\t\t\t& \\multirow{1}{*}{Multi-task}  & GPT-4~   & Large Multi-modal model for NLP \\& CV tasks & arXiv 2023 \\\\\n\t\t\t\\hline\n\t\t\t\\multirow{4}{*}{\\shortstack{Efficient\\\\transformer}} &\\multirow{1}{*}{Decomposition}  & ASH~   & Number of heads, importance estimation & NeurIPS 2019 \\\\\n\t\t\t\\cline{2-5}\n\t\t\t& \\multirow{1}{*}{Distillation}  & TinyBert~   & Various losses for different modules & {\\scriptsize EMNLP Findings 2020}  \\\\\n\t\t\t\\cline{2-5}\n\t\t\t& \\multirow{1}{*}{Quantization}  & FullyQT~   & Fully quantized transformer & {\\scriptsize EMNLP Findings 2020} \\\\\n\t\t\t\\cline{2-5}\n\t\t\t& \\multirow{1}{*}{Architecture design}  & ConvBert~   & Local dependence, dynamic convolution & NeurIPS 2020 \\\\\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t}\n\\vspace{-1em}\n\\end{table*}", "cites": [4765, 7, 8838, 679, 2527, 9115, 7070, 4764, 4485, 168, 4767, 7373, 4766, 97, 732, 7581, 1639, 209, 38, 7361, 7360, 4771, 4770, 4768, 1901, 7339, 810, 4769, 7367, 8839, 2513], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 48, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The introduction effectively synthesizes multiple vision transformer works by categorizing them into application domains and highlighting their key contributions. It abstracts the broader trend of replacing CNNs with transformers in vision tasks and notes the shift from NLP to CV. While it provides some critical perspectives (e.g., the limited adoption in low-level vision and the computational challenges), deeper comparative analysis or evaluation of trade-offs between methods is not yet fully developed in this section."}}
{"id": "5598792b-26cc-42b8-bbae-bb2083cae447", "title": "Formulation of Transformer", "level": "section", "subsections": ["bdc024cb-d616-45fa-93c7-fa44f8eaa353", "b7233156-6aa8-4efc-8d09-9c732c3621ec"], "parent_id": "c94c5602-77e6-49fe-a100-dda7b1e8d141", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Formulation of Transformer"]], "content": "Transformer~ was first used in the field of natural language processing (NLP) on machine translation tasks. As shown in Figure~\\ref{fig:3-3}, it consists of an encoder and a decoder with several transformer blocks of the same architecture. The encoder generates encodings of inputs, while the decoder takes all the encodings and using their incorporated contextual information to generate the output sequence. Each transformer block is composed of a multi-head attention layer, a feed-forward neural network, shortcut connection and layer normalization. In the following, we describe each component of the transformer in detail.\n\\begin{figure}[htp] \n\t\\centering\n\t\\includegraphics[width=0.55\\columnwidth]{fig/3-3.png}\n\t\\vspace{-0.5em}\n\t\\caption{Structure of the original transformer (image from~).}\n\t\\label{fig:3-3}\n\t\\vspace{-1em}\n\\end{figure}", "cites": [38], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of the Transformer architecture, primarily paraphrasing the structure from the cited paper. There is minimal synthesis of ideas beyond the original formulation, no meaningful comparison with other models, and no abstraction to broader principles or trends in visual Transformers."}}
{"id": "bdc024cb-d616-45fa-93c7-fa44f8eaa353", "title": "Self-Attention", "level": "subsection", "subsections": [], "parent_id": "5598792b-26cc-42b8-bbae-bb2083cae447", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Formulation of Transformer"], ["subsection", "Self-Attention"]], "content": "In the self-attention layer, the input vector is first transformed into three different vectors:  the query vector $\\mathbf q$, the key vector $\\mathbf k$ and the value vector $\\mathbf v$ with dimension $d_q=d_k=d_v= d_{model}=512$. Vectors derived from different inputs are then packed together into three different matrices, namely, $\\mathbf Q$, $\\mathbf K$ and $\\mathbf V$. Subsequently, the attention function between different input vectors is calculated as follows (and shown in Figure~\\ref{fig:3-2} left):\n\\begin{itemize}\n\t\\item \\textbf{Step 1: }Compute scores between different input vectors with $\\mathbf S=\\mathbf Q\\cdot \\mathbf K^\\top$;\n\t\\item \\textbf{Step 2: }Normalize the scores for the stability of gradient with $\\mathbf S_n=\\mathbf{S}/{\\sqrt{d_k}}$;\n\t\\item \\textbf{Step 3: }Translate the scores into probabilities with softmax function $\\mathbf P=\\mathrm{softmax}(\\mathbf S_n)$;\n\t\\item \\textbf{Step 4: }Obtain the weighted value matrix with $\\mathbf Z=\\mathbf V\\cdot \\mathbf P$.\n\\end{itemize}\nThe process can be unified into a single function:\n\\begin{equation}\n\\mathrm{Attention}(\\mathbf Q,\\mathbf K,\\mathbf V)=\\mathrm{softmax}(\\frac{\\mathbf Q\\cdot \\mathbf K^\\top}{\\sqrt{d_k}})\\cdot \\mathbf V.\n\\label{eq:3.1}\n\\end{equation}\nThe logic behind Eq.~\\ref{eq:3.1} is simple. Step 1 computes scores between each pair of different vectors, and these scores determine the degree of attention that we give other words when encoding the word at the current position. Step 2 normalizes the scores to enhance gradient stability for improved training, and step 3 translates the scores into probabilities. Finally, each value vector is multiplied by the sum of the probabilities. Vectors with larger probabilities receive additional focus from the following layers.\nThe encoder-decoder attention layer in the decoder module is similar to the self-attention layer in the encoder module with the following exceptions: The key matrix $K$ and value matrix $V$ are derived from the encoder module, and the query matrix $Q$ is derived from the previous layer.\nNote that the preceding process is invariant to the position of each word, meaning that the self-attention layer lacks the ability to capture the positional information of words in a sentence. However, the sequential nature of sentences in a language requires us to incorporate the positional information within our encoding. To address this issue and allow the final input vector of the word to be obtained, a positional encoding with dimension $d_{model}$ is added to the original input embedding. Specifically, the position is encoded with the following equations:\n\\begin{align}\nPE(pos, 2i)=sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}});\\\\\nPE(pos, 2i+1)=cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}),\n\\end{align}\nin which $pos$ denotes the position of the word in a sentence, and $i$ represents the current dimension of the positional encoding. In this way, each element of the positional encoding corresponds to a sinusoid, and it allows the transformer model to learn to attend by relative positions and extrapolate to longer sequence lengths during inference. In apart from the fixed positional encoding in the vanilla transformer, learned positional encoding~ and relative positional encoding~ are also utilized in various models~.\n\\begin{figure}[htp] \n\t\\centering\n\t\\includegraphics[width=0.7\\columnwidth]{fig/3-2.png}\n\t\\vspace{-1em}\n\t\\caption{(Left) Self-attention process. (Right) Multi-head attention. The image is from~.}\n\t\\label{fig:3-2}\n\t\\vspace{-1.5em}\n\\end{figure}\n\\noindent\\textbf{Multi-Head Attention.}\nMulti-head attention is a mechanism that can be used to boost the performance of the vanilla self-attention layer. Note that for a given reference word, we often want to focus on several other words when going through the sentence. A single-head self-attention layer limits our ability to focus on one or more specific positions without influencing the attention on other equally important positions at the same time. This is achieved by giving attention layers different representation subspace. Specifically, different query, key and value matrices are used for different heads, and these matrices can project the input vectors into different representation subspace after training due to random initialization.\nTo elaborate on this in greater detail, given an input vector and the number of heads $h$, the input vector is first transformed into three different groups of vectors: the query group, the key group and the value group. In each group, there are $h$ vectors with dimension $d_{q'}=d_{k'}=d_{v'}=d_{model}/h=64$. The vectors derived from different inputs are then packed together into three different groups of matrices: $\\{\\mathbf Q_i\\}_{i=1}^h$, $\\{\\mathbf K_i\\}_{i=1}^h$ and $\\{\\mathbf V_i\\}_{i=1}^h$. The multi-head attention process is shown as follows:\n\\begin{align}\n\\mathrm{MultiHead}(\\mathbf Q', \\mathbf K', \\mathbf V') &=\\mathrm{Concat}(\\mathrm{head}_1, \\cdots, \\mathrm{head}_h)\\mathbf W^o,\\notag\\\\\n\\text{where } \\mathrm{head}_i &= \\mathrm{Attention}(\\mathbf Q_i,\\mathbf K_i,\\mathbf V_i).\n\\end{align} \nHere, $\\mathbf Q'$ (and similarly $\\mathbf K'$ and $\\mathbf V'$) is the concatenation of $\\{\\mathbf Q_i\\}_{i=1}^h$, and $\\mathbf W^o\\in\\mathbb R^{d_{model}\\times d_{model}}$ is the projection weight.", "cites": [7, 732, 790, 7054, 38], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a clear and factual explanation of the self-attention and multi-head attention mechanisms, drawing from the foundational Transformer paper (doc_id: 38) and related works. It integrates some concepts across papers, such as positional encoding from the original Transformer and alternatives like learned or relative positional encoding mentioned in doc_id: 7054. However, the synthesis is limited to presenting these components in a standard way without deeper connections or novel insights. The analysis is primarily descriptive, with little critical evaluation of the cited approaches or broader conceptual abstraction."}}
{"id": "b7233156-6aa8-4efc-8d09-9c732c3621ec", "title": "Other Key Concepts in Transformer", "level": "subsection", "subsections": [], "parent_id": "5598792b-26cc-42b8-bbae-bb2083cae447", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Formulation of Transformer"], ["subsection", "Other Key Concepts in Transformer"]], "content": "\\noindent\\textbf{Feed-Forward Network.} A feed-forward network (FFN) is applied after the self-attention layers in each encoder and decoder. It consists of two linear transformation layers and a nonlinear activation function within them, and can be denoted as the following function:\n\\begin{equation}\n\\mathrm{FFN}(\\mathbf X)=\\mathbf W_2\\sigma(\\mathbf W_1\\mathbf X),\n\\end{equation}\nwhere $\\mathbf W_1$ and $\\mathbf W_2$ are the two parameter matrices of the two linear transformation layers, and $\\sigma$ represents the nonlinear activation function, such as GELU~. The dimensionality of the hidden layer is $d_h=2048$.\n\\noindent\\textbf{Residual Connection in the Encoder and Decoder.} As shown in Figure~\\ref{fig:3-3}, a residual connection is added to each sub-layer in the encoder and decoder. This strengthens the flow of information in order to achieve higher performance. A layer-normalization~ is followed after the residual connection. The output of these operations can be described as:\n\\begin{equation}\n\\mathrm{LayerNorm}(\\mathbf X+\\mathrm{Attention}(\\mathbf X)).\n\\end{equation}\nHere, $\\mathbf X$ is used as the input of self-attention layer, and the query, key and value matrices $\\mathbf Q, \\mathbf K$ and $\\mathbf V$ are all derived from the same input matrix $\\mathbf X$. A variant pre-layer normalization (Pre-LN) is also widely-used~. Pre-LN inserts the layer normalization inside the residual connection and before multi-head attention or FFN.\nFor the normalization layer, there are several alternatives such as batch normalization~. Batch normalization usually perform worse when applied on transformer as the feature values change acutely~. Some other normalization algorithms~ have been proposed to improve training of transformer.\n\\noindent\\textbf{Final Layer in the Decoder.} The final layer in the decoder is used to turn the stack of vectors back into a word. This is achieved by a linear layer followed by a softmax layer. The linear layer projects the vector into a logits vector with $d_{word}$ dimensions, in which $d_{word}$ is the number of words in the vocabulary. The softmax layer is then used to transform the logits vector into probabilities.\nWhen used for CV tasks, most transformers adopt the original transformer's encoder module. Such transformers can be treated as a new type of feature extractor. Compared with CNNs which focus only on local characteristics, transformer can capture long-distance characteristics, meaning that it can easily derive global information. And in contrast to RNNs, whose hidden state must be computed sequentially, transformer is more efficient because the output of the self-attention layer and the fully connected layers can be computed in parallel and easily accelerated. From this, we can conclude that further study into using transformer in computer vision as well as NLP would yield beneficial results.", "cites": [1512, 57, 1496, 732, 1495, 1467, 1448, 1491, 71], "cite_extract_rate": 1.0, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of key components of the Transformer, such as the feed-forward network, residual connections, and normalization techniques. It integrates information from cited papers to some extent, particularly when discussing alternatives like pre-layer normalization and batch normalization. However, the analysis lacks depth and does not offer strong critical evaluation or abstraction into broader principles or frameworks. The section is more descriptive than analytical."}}
{"id": "c3a7bac1-9144-47a4-bccb-2645b24e83d6", "title": "Backbone for Representation Learning", "level": "subsection", "subsections": ["be89285d-1d15-4d83-8a0f-45370495f624", "7361c1b9-7769-41c5-b01a-a235dc335d84", "f9a01e8d-8fbe-422a-a069-75ca45c75de2", "a7ee423c-e629-4928-92a0-352ecc5968c4"], "parent_id": "dfe7037d-ce2c-483e-a574-d13f6529e3d8", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Vision Transformer"], ["subsection", "Backbone for Representation Learning"]], "content": "Inspired by the success that transformer has achieved in the field of NLP, some researchers have explored whether similar models can learn useful representations for images. Given that images involve more dimensions, noise and redundant modality compared to text, they are believed to be more difficult for generative modeling.\nOther than CNNs, the transformer can be used as backbone networks for image classification. Wu~\\etal~ adopted ResNet as a convenient baseline and used vision transformers to replace the last stage of convolutions. Specifically, they apply convolutional layers to extract low-level features that are then fed into the vision transformer. For the vision transformer, they use a \\emph{tokenizer} to group pixels into a small number of \\emph{visual tokens}, each representing a semantic concept in the image. These \\emph{visual tokens} are used directly for image classification, with the transformers being used to model the relationships between tokens. As shown in Figure~\\ref{backbone-tree}, the works can be divided into purely using transformer for vision and combining CNN and transformer. We summarize the results of these models in Table~\\ref{table-vit} and Figure~\\ref{acc-flops} to demonstrate the development of the backbones. In addition to supervised learning, self-supervised learning is also explored in vision transformer.", "cites": [7844], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of how vision transformers are used as backbones for representation learning, referencing one paper and mentioning the use of a tokenizer to create visual tokens. While it introduces a distinction between purely transformer-based and CNN-transformer hybrid approaches, it lacks deeper synthesis, critical evaluation of strengths and weaknesses, and abstraction to broader principles or trends in the field."}}
{"id": "be89285d-1d15-4d83-8a0f-45370495f624", "title": "Pure Transformer", "level": "subsubsection", "subsections": [], "parent_id": "c3a7bac1-9144-47a4-bccb-2645b24e83d6", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Vision Transformer"], ["subsection", "Backbone for Representation Learning"], ["subsubsection", "Pure Transformer"]], "content": "\\noindent\\textbf{ViT.}\nVision Transformer (ViT)~ is a pure transformer directly applies to the sequences of image patches for image classification task. It follows transformer's original design as much as possible. Figure~\\ref{ViT} shows the framework of ViT. \nTo handle 2D images, the image $X\\in \\mathbb{R}^{h\\times w \\times c}$ is reshaped into a sequence of flattened 2D patches $X_p\\in \\mathbb{R}^{n\\times (p^2 \\cdot c)}$ such that $c$ is the number of channels. $(h,w)$ is the resolution of the original image, while $(p,p)$ is the resolution of each image patch. The effective sequence length for the transformer is therefore $n=hw/p^2$. Because the transformer uses constant widths in all of its layers, a trainable linear projection maps each vectorized path to the model dimension $d$, the output of which is referred to as patch embeddings. \nSimilar to BERT's $[class]$ token, a learnable embedding is applied to the sequence of embedding patches. The state of this embedding serves as the image representation. During both pre-training and fine-tuning stage, the classification heads are attached to the same size. In addition, 1D position embeddings are added to the patch embeddings in order to retain positional information. It is worth noting that ViT utilizes only the standard transformer's encoder (except for the place for the layer normalization), whose output precedes an MLP head. In most cases, ViT is pre-trained on large datasets, and then fine-tuned for downstream tasks with smaller data.\nViT yields modest results when trained on mid-sized datasets such as ImageNet, achieving accuracies of a few percentage points below ResNets of comparable size. Because transformers lack some inductive biases inherent to CNNs--such as translation equivariance and locality--they do not generalize well when trained on insufficient amounts of data. However, the authors found that training the models on large datasets (14 million to 300 million images) surpassed inductive bias. When pre-trained  at sufficient scale, transformers achieve excellent results on tasks with fewer datapoints. For example, when pre-trained on the JFT-300M dataset, ViT approached or even exceeded state of the art performance on multiple image recognition benchmarks. Specifically, it reached an accuracy of 88.36\\% on ImageNet, and 77.16\\% on the VTAB suite of 19 tasks.\nTouvron~\\etal~ proposed a competitive convolution-free transformer, called Data-efficient image transformer (DeiT), by training on only the ImageNet database. DeiT-B, the reference vision transformer, has the same architecture as ViT-B and employs 86 million parameters. With a strong data augmentation, DeiT-B achieves top-1 accuracy of 83.1\\% (single-crop evaluation) on ImageNet with no external data. In addition, the authors observe that using a CNN teacher gives better performance than using a transformer. Specifically, DeiT-B can achieve top-1 accuracy 84.40\\% with the help of a token-based distillation. \n\\begin{figure}[htp]\n\t\\centering\n\t\\includegraphics[width=1.0\\linewidth]{fig/vit.png}\n\t\\vspace{-1em}\n\t\\caption{The framework of ViT (image from~).}\n\t\\label{ViT}\n\t\\vspace{-0.5em}\n\\end{figure}\n\\noindent\\textbf{Variants of ViT.}\nFollowing the paradigm of ViT, a series of variants of ViT have been proposed to improve the performance on vision tasks. The main approaches include enhancing locality, self-attention improvement and architecture design.\nThe original vision transformer is good at capturing long-range dependencies between patches, but disregard the local feature extraction as the 2D patch is projected to a vector with simple linear layer. Recently, the researchers begin to pay attention to improve the modeling capacity for local information~. TNT~ further divides the patch into a number of sub-patches and introduces a novel transformer-in-transformer architecture which utilizes an inner transformer block to model the relationship between sub-patches and an outer transformer block for patch-level information exchange. Twins~ and CAT~ alternately perform local and global attention layer-by-layer. Swin Transformers~ performs local attention within a window and introduces a shifted window partitioning approach for cross-window connections. Shuffle Transformer~ further utilizes the spatial shuffle operation instead of shifted window partitioning to allow cross-window connections. RegionViT~ generates regional tokens and local tokens from an image, and local tokens receive global information via attention with regional tokens. In addition to the local attention, some other works propose to boost local information through local feature aggregation, \\eg, T2T~. These works demonstrate the benefit of the local information exchange and global information exchange in vision transformer.\nAs a key component of transformer, self-attention layer provides the ability for global interaction between image patches. Improving the calculation of self-attention layer has attracted many researchers. DeepViT~ proposes to establish cross-head communication to re-generate the attention maps to increase the diversity at different layers. \nKVT~ introduces the $k$-NN attention to utilize locality of images patches and ignore noisy tokens by only computing attentions with top-$k$ similar tokens. \nRefiner~ explores attention expansion in higher-dimension space and applied convolution to augment local patterns of the attention maps.\nXCiT~ performs self-attention calculation across feature channels rather than tokens, which allows efficient processing of high-resolution images. The computation complexity and attention precision of the self-attention mechanism are two key-points for future optimization.\nThe network architecture is an important factor as demonstrated in the field of CNNs. The original architecture of ViT is a simple stack of the same-shape transformer block.\nNew architecture design for vision transformer has been an interesting topic. The pyramid-like architecture is utilized by many vision transformer models~ including PVT~, HVT~, Swin Transformer~ and PiT~. There are also other types of architectures, such as two-stream architecture~ and U-net architecture~. Neural architecture search (NAS) has also been investigated to search for better transformer architectures, \\eg, Scaling-ViT~, ViTAS~, AutoFormer~ and GLiT~. Currently, both network design and NAS for vision transformer mainly draw on the experience of CNN. In the future, we expect the specific and novel architectures appear in the filed of vision transformer.\nIn addition to the aforementioned approaches, there are some other directions to further improve vision transformer, \\eg, positional encoding~, normalization strategy~, shortcut connection~ and removing attention~.\n\\begin{table}[]\n\t\\centering\n\t\\scriptsize\n\t\\caption{\\small ImageNet result comparison of representative CNN and vision transformer models. Pure transformer means only using a few convolutions in the stem stage. CNN + Transformer means using convolutions in the intermediate layers. Following~, the throughput is measured on NVIDIA V100 GPU and Pytorch, with 224$\\times$224 input size.}\n\t\\label{table-vit}\n\t\\vspace{-1em}\n\t\\begin{tabular}{l|c|c|c|c}\n\t\t\\Xhline{1.2pt}\n\t\t\\multirow{2}{*}{Model} & Params & FLOPs & Throughput & Top-1 \\\\ \n\t\t& (M) & (B) & (image/s) & (\\%) \\\\\n\t\t\\hline\n\t\t\\multicolumn{5}{c}{\\textbf{CNN}} \\\\\n\t\t\\hline\n\t\tResNet-50~ & 25.6 & 4.1 & 1226 & 79.1 \\\\\n\t\tResNet-101~ & 44.7 & 7.9  & 753 & 79.9  \\\\\n\t\tResNet-152~ & 60.2 & 11.5  & 526 & 80.8  \\\\  \n\t\t\\hline\n\t\tEfficientNet-B0~ & 5.3 & 0.39  & 2694 & 77.1 \\\\\n\t\tEfficientNet-B1~ & 7.8 & 0.70  & 1662 & 79.1 \\\\\n\t\tEfficientNet-B2~ & 9.2 & 1.0  & 1255 & 80.1 \\\\\n\t\tEfficientNet-B3~ & 12 & 1.8  & 732 & 81.6 \\\\\n\t\tEfficientNet-B4~ & 19 & 4.2  & 349 & 82.9 \\\\\n\t\t\\hline\n\t\t\\multicolumn{5}{c}{\\textbf{Pure Transformer}} \\\\\n\t\t\\hline\n\t\tDeiT-Ti~  & 5 & 1.3   & 2536 & 72.2 \\\\\n\t\tDeiT-S~   & 22 & 4.6  & 940 & 79.8 \\\\\n\t\tDeiT-B~  & 86 & 17.6   & 292 & 81.8 \\\\  \n\t\t\\hline\n\t\tT2T-ViT-14~  & 21.5 & 5.2  & 764 & 81.5 \\\\\n\t\tT2T-ViT-19~  & 39.2 & 8.9   & 464 & 81.9 \\\\\n\t\tT2T-ViT-24~   & 64.1 & 14.1   & 312 & 82.3 \\\\  \n\t\t\\hline\n\t\tPVT-Small~ & 24.5 & 3.8  & 820 & 79.8 \\\\ \n\t\tPVT-Medium~ & 44.2 & 6.7  & 526 & 81.2 \\\\\n\t\tPVT-Large~ & 61.4 & 9.8  & 367 & 81.7 \\\\  \n\t\t\\hline\n\t\tTNT-S~ & 23.8 & 5.2  & 428 & 81.5  \\\\\n\t\tTNT-B~ & 65.6 & 14.1  & 246 & 82.9  \\\\ \n\t\t\\hline\n\t\tCPVT-S~ & 23 & 4.6  & 930 & 80.5 \\\\\n\t\tCPVT-B~ & 88 & 17.6  & 285 & 82.3 \\\\  \n\t\t\\hline\t\t\n\t\tSwin-T~  & 29 & 4.5  & 755 & 81.3\n \\\\ \n\t\tSwin-S~  & 50 & 8.7  &  437 & 83.0 \\\\\n\t\tSwin-B~  & 88 & 15.4  &  278 & 83.3 \\\\ \n\t\t\\hline\n\t\t\\multicolumn{5}{c}{\\textbf{CNN + Transformer}} \\\\\n\t\t\\hline\n\t\tTwins-SVT-S~ & 24 & 2.9  & 1059 & 81.7 \\\\\n\t\tTwins-SVT-B~ & 56 & 8.6  & 469  & 83.2 \\\\\n\t\tTwins-SVT-L~ & 99.2 & 15.1  & 288 & 83.7 \\\\\n\t\t\\hline \n\t\tShuffle-T~  & 29 & 4.6  & 791 & 82.5 \\\\% https://arxiv.org/pdf/2106.03650.pdf\n\t\tShuffle-S~  & 50 & 8.9  &   450 & 83.5 \\\\ \n\t\tShuffle-B~  &  88 & 15.6   &   279 & 84.0 \\\\\t\n\t\t\\hline\n\t\tCMT-S~  & 25.1 & 4.0  & 563 & 83.5 \\\\ \n\t\tCMT-B~   & 45.7 & 9.3  &  285 & 84.5 \\\\ \n\t\t\\hline \n\t\tVOLO-D1~  & 27 & 6.8  & 481 & 84.2 \\\\\t\n\t\tVOLO-D2~  & 59 & 14.1 & 244 & 85.2 \\\\\n\t\tVOLO-D3~  & 86 & 20.6  & 168 & 85.4 \\\\\n\t\tVOLO-D4~  & 193 & 43.8   & 100 & 85.7 \\\\\n\t\tVOLO-D5~  & 296 & 69.0   & 64 & 86.1 \\\\\n\t\t\\Xhline{1.2pt}\n\t\\end{tabular}\n\\vspace{-1em}\n\\end{table}\n\\footnotetext[1]{}", "cites": [1493, 7846, 4786, 4788, 2579, 7845, 4778, 7847, 7590, 8434, 4772, 4773, 7167, 4784, 4780, 4787, 4783, 4777, 4781, 732, 4790, 97, 8841, 4785, 7660, 4771, 4775, 4782, 7523, 1501, 7367, 8840, 4776, 4774, 4789, 7848, 2577, 4779], "cite_extract_rate": 0.926829268292683, "origin_cites_number": 41, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.3, "critical": 3.7, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes a range of vision transformer models and their variants, connecting ideas around architecture design, attention mechanisms, and performance improvements. It provides critical analysis by highlighting limitations of pure transformers (e.g., inductive bias, attention collapse) and comparing approaches like DeiT with CNN teachers. It abstracts broader design trends such as the importance of locality, the role of attention mechanisms, and the influence of scaling and NAS on transformer design."}}
{"id": "7361c1b9-7769-41c5-b01a-a235dc335d84", "title": "Transformer with Convolution", "level": "subsubsection", "subsections": [], "parent_id": "c3a7bac1-9144-47a4-bccb-2645b24e83d6", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Vision Transformer"], ["subsection", "Backbone for Representation Learning"], ["subsubsection", "Transformer with Convolution"]], "content": "Although vision transformers have been successfully applied to various visual tasks due to their ability to capture long-range dependencies within the input, there are still gaps in performance between transformers and existing CNNs. One main reason can be the lack of ability to extract local information. Except the above mentioned variants of ViT that enhance the locality, combining the transformer with convolution can be a more straightforward way to introduce the locality into the conventional transformer. \nThere are plenty of works trying to augment a conventional transformer block or self-attention layer with convolution. For example, CPVT~ proposed a conditional positional encoding (CPE) scheme, which is conditioned on the local neighborhood of input tokens and adaptable to arbitrary input sizes, to leverage convolutions for fine-level feature encoding. CvT~, CeiT~, LocalViT~ and CMT~ analyzed the potential drawbacks when directly borrowing Transformer architectures from NLP and combined the convolutions with transformers together. Specifically, the feed-forward network (FFN) in each transformer block is combined with a convolutional layer that promotes the correlation among neighboring tokens. LeViT~ revisited principles from extensive literature on CNNs and applied them to transformers, proposing a hybrid neural network for fast inference image classification. BoTNet~ replaced the spatial convolutions with global self-attention in the final three bottleneck blocks of a ResNet, and improved upon the baselines significantly on both instance segmentation and object detection tasks with minimal overhead in latency.\nBesides, some researchers have demonstrated that transformer based models can be more difficult to enjoy a favorable ability of fitting data~, in other words, they are sensitive to the choice of optimizer, hyper-parameter, and the schedule of training. Visformer~ revealed the gap between transformers and CNNs with two different training settings. The first one is the standard setting for CNNs, \\ie, the training schedule is shorter and the data augmentation only contains random cropping and horizental flipping. The other one is the training setting used in~, \\ie, the training schedule is longer and the data augmentation is stronger.  changed the early visual processing of ViT by replacing its embedding stem with a standard convolutional stem, and found that this change allows ViT to converge faster and enables the use of either AdamW or SGD without a significant drop in accuracy. In addition to these two works,  also choose to add convolutional stem on the top of the transformer.\n\\begin{figure}[htp]\n\t\\vspace{-0.5em}\n\t\\centering\n\t\\setlength{\\tabcolsep}{2pt}{\n\t\t\\begin{tabular}{cc}\n\t\t\t\\makecell*[c]{\\includegraphics[width=0.5\\linewidth]{fig/acc-flops.pdf}}  & \\makecell*[c]{\\includegraphics[width=0.5\\linewidth]{fig/acc-latency.pdf}}\n\t\t\t\\\\\n\t\t\t\\small (a) Acc v.s. FLOPs. & \\small (b) Acc v.s. throughput.\n\t\t\\end{tabular}\n\t}\n\t\\vspace{-0.5em}\n\t\\caption{FLOPs and throughput comparison of representative CNN and vision transformer models.}\n\t\\label{acc-flops}\n\t\\vspace{-1.0em}\n\\end{figure}", "cites": [4791, 4794, 1493, 4795, 4792, 732, 7590, 7849, 4773, 4793, 2581], "cite_extract_rate": 1.0, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple papers by highlighting how convolution is integrated into vision transformers to address locality and optimization issues. It provides a coherent narrative on different strategies (e.g., CPE, convolutional stems, hybrid blocks). While it does not offer a fully novel framework, it connects various methods to explain common challenges and solutions. The section includes some critical evaluation of performance trade-offs and training difficulties, but it lacks deeper, nuanced critique of the approaches. It abstracts to a degree by identifying broader trends in using convolutional components to improve transformers, though the abstraction remains grounded in specific techniques."}}
{"id": "f9a01e8d-8fbe-422a-a069-75ca45c75de2", "title": "Self-supervised Representation Learning", "level": "subsubsection", "subsections": [], "parent_id": "c3a7bac1-9144-47a4-bccb-2645b24e83d6", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Vision Transformer"], ["subsection", "Backbone for Representation Learning"], ["subsubsection", "Self-supervised Representation Learning"]], "content": "\\noindent\\textbf{Generative Based Approach.}\nGenerative pre-training methods for images have existed for a long time~. Chen~\\etal~ re-examined this class of methods and combined it with self-supervised methods. After that, several works~ were proposed to extend generative based self-supervised learning for vision transformer.\nWe briefly introduce iGPT~ to demonstrate its mechanism. This approach consists of a pre-training stage followed by a fine-tuning stage. During the pre-training stage, auto-regressive and BERT objectives are explored. To implement pixel prediction, a sequence transformer architecture is adopted instead of language tokens (as used in NLP). Pre-training can be thought of as a favorable initialization or regularizer when used in combination with early stopping. During the fine-tuning stage, they add a small classification head to the model. This helps optimize a classification objective and adapts all weights.\nThe image pixels are transformed into a sequential data by $k$-means clustering. Given an unlabeled dataset ${X}$ consisting of high dimensional data $\\mathbf x=(x_1,\\cdots,x_n)$, they train the model by minimizing the negative log-likelihood\nof the data:\n\\begin{equation}\n\tL_{AR} = \\underset{\\mathbf x\\sim X}{\\mathbb{E}} [-\\log p(\\mathbf x)],\n\\end{equation}\nwhere $p(\\mathbf x)$ is the probability density of the data of images, which can be modeled as:\n\\begin{equation}\n\tp(\\mathbf x) = \\prod_{i=1}^n p(x_{\\pi_i}|x_{\\pi_1},\\cdots,x_{\\pi_{i-1}},\\theta).\n\\end{equation}\nHere, the identity permutation $\\pi_i=i$ is adopted for $1\\leqslant i \\leqslant n$, which is also known as raster order. Chen~\\etal~also considered the BERT objective, which samples a sub-sequence $M\\subset[1,n]$ such that each index $i$ independently has probability 0.15 of appearing in $M$. $M$ is called the BERT mask, and the model is trained by minimizing the negative log-likelihood of the ``masked'' elements $x_M$ conditioned on the ``unmasked'' ones $x_{[1,n] \\backslash M}$:\n\\begin{equation}\n\tL_{BERT}=\\underset{\\mathbf x\\sim X}{\\mathbb{E}} \\underset{M}{\\mathbb{E}} \\sum_{i\\in M}[-\\log p(x_i|x_{[1,n]\\backslash M})].\n\\end{equation}\nDuring the pre-training stage, they pick either $L_{AR}$ or $L_{BERT}$ and minimize the loss over the pre-training dataset. \nGPT-2~ formulation of the transformer decoder block is used. To ensure proper conditioning when training the AR objective, Chen~\\etal~apply the standard upper triangular mask to the $n\\times n$ matrix of attention logits. No attention logit masking is required when the BERT objective is used: Chen~\\etal~zero out the positions after the content embeddings are applied to the input sequence. Following the final transformer layer, they apply a layer norm and learn a projection from the output to logits parameterizing the conditional distributions at each sequence element. When training BERT, they simply ignore the logits at unmasked positions. \nDuring the fine-tuning stage, they average pool the output of the final layer normalization layer across the sequence dimension to extract a $d$-dimensional vector of features per example.\nThey learn a projection from the pooled feature to class logits and use this projection to minimize a cross entropy loss. Practical applications offer empirical evidence that the joint objective of cross entropy loss and pretraining loss ($L_{AR}$ or $L_{BERT}$) works even better. After iGPT, masked image modeling is proposed such as MAE~ and SimMIM~ which achieves competitive performance on downstream tasks.\niGPT and ViT are two pioneering works to apply transformer for visual tasks. The difference of iGPT and ViT-like models mainly lies on 3 aspects: 1) The input of iGPT is a sequence of color palettes by clustering pixels, while ViT uniformly divided the image into a number of local patches; 2) The architecture of iGPT is an encoder-decoder framework, while ViT only has transformer encoder; 3) iGPT utilizes auto-regressive self-supervised loss for training, while ViT is trained by supervised image classification task.\n\\noindent\\textbf{Contrastive Learning Based Approach.}\nCurrently, contrastive learning is the most popular manner of self-supervised learning for computer vision. Contrastive learning has been applied on vision transformer for unsupervised pretraining~.\nChen~\\etal~ investigate the effects of several fundamental components for training self-supervised ViT. The authors observe that instability is a major issue that degrades accuracy, and these results are indeed partial failure and they can be improved when training is made more stable. \nThey introduce a ``MoCo v3'' framework, which is an incremental improvement of MoCo~. Specifically, the authors take two crops for each image under random data augmentation. They are encodes by two encoders, $f_q$ and $f_k$, with output vectors $\\mathbf q$ and $\\mathbf k$. Intuitively, $\\mathbf q$ behaves like a ``query'' and the goal of learning is to retrieve the corresponding ``key''. This is formulated as minimizing a contrastive loss function, which can be written as:\n\\begin{equation}\n\\mathcal{L}_q=-\\text{log} \\frac{\\exp(\\mathbf q \\cdot \\mathbf k^+ /\\tau)}{\\exp(\\mathbf q \\cdot \\mathbf k^+ /\\tau)+\\sum_{\\mathbf k^-} \\exp(\\mathbf q \\cdot \\mathbf k^- /\\tau)}.\n\\end{equation}\nHere $\\mathbf k^+$ is $f_k$'s output on the same image as $\\mathbf q$, known as $\\mathbf q$'s positive sample. The set {$\\mathbf k^-$} consists of $f_k$'s outputs from other images, known as $\\mathbf q$'s negative samples. $\\tau$ is a temperature hyper-parameter for $l_2$-normalized $\\mathbf q$, $\\mathbf k$. MoCo v3 uses the keys that naturally co-exist in the same batch and abandon the memory queue, which they find has diminishing gain if the batch is sufficiently large (\\eg, 4096). With this simplification, the contrastive loss can be implemented in a simple way. The encoder $f_q$ consists of a backbone (\\eg, ViT), a projection head and an extra prediction head; while the encoder $f_k$ has the backbone and projection head, but not the prediction head. $f_k$ is updated by the moving-average of $f_q$, excluding the prediction head.\nMoCo v3 shows that the instability is a major issue of training the self-supervised ViT, thus they describe a simple trick that can improve the stability in various cases of the experiments. They observe that it is not necessary to train the patch projection layer. For the standard ViT patch size, the patch projection matrix is complete or over-complete. And in this case, random projection should be sufficient to preserve the information of the original patches. However, the trick alleviates the issue, but does not solve it. The model can still be unstable if the learning rate is too big and the first layer is unlikely the essential reason for the instability.", "cites": [9149, 507, 2515, 4796, 2527, 7102, 2513, 1254, 2530, 122], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of self-supervised learning for vision transformers, highlighting key differences between generative and contrastive approaches and referencing relevant works. It synthesizes some ideas, such as the role of masked image modeling and the MoCo framework, but does so in a structured rather than novel way. Critical analysis is present, particularly in identifying training instability and over-completeness issues, but is not as deep or comprehensive as it could be. Some abstraction is achieved through the comparison of general training strategies."}}
{"id": "b64eb56a-71c3-403c-bd63-e587ebef744c", "title": "High/Mid-level Vision", "level": "subsection", "subsections": ["34c16f53-f17c-4f01-990d-7900b1f8e40f", "7577f812-d4fd-4834-bb2d-762f1714ca3c", "4451b6d8-c354-4409-9f97-4499fabbc61a", "cc9a3312-5269-40b6-aa0d-579b90407630", "c366fb4c-b924-49d7-94e1-d9300122ba81"], "parent_id": "dfe7037d-ce2c-483e-a574-d13f6529e3d8", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Vision Transformer"], ["subsection", "High/Mid-level Vision"]], "content": "Recently there has been growing interest in using transformer for high/mid-level computer vision tasks, such as object detection~, lane detection~, segmentation~ and pose estimation~. We review these methods in this section.", "cites": [4798, 7360, 8838, 4768, 7850, 4797, 8839, 4799, 4769, 7373], "cite_extract_rate": 0.7692307692307693, "origin_cites_number": 13, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a general overview of the application of vision transformers in high/mid-level vision tasks but fails to synthesize or connect ideas across the cited works. It lacks critical analysis of the methods' strengths, weaknesses, or trade-offs and does not abstract broader patterns or principles from the specific papers. The content is primarily a listing of tasks and associated methods without deeper insight."}}
{"id": "34c16f53-f17c-4f01-990d-7900b1f8e40f", "title": "Generic Object Detection", "level": "subsubsection", "subsections": [], "parent_id": "b64eb56a-71c3-403c-bd63-e587ebef744c", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Vision Transformer"], ["subsection", "High/Mid-level Vision"], ["subsubsection", "Generic Object Detection"]], "content": "Traditional object detectors are mainly built upon CNNs, but transformer-based object detection has gained significant interest recently due to its advantageous capability. \nSome object detection methods have attempted to use transformer's self-attention mechanism and then enhance the specific modules for modern detectors, such as feature fusion module~ and prediction head~. We discuss this in the supplemental material.\nTransformer-based object detection methods are broadly categorized into two groups: transformer-based set prediction methods~ and transformer-based backbone methods~, as shown in Fig.~\\ref{fig:det_diagram}. Transformer-based methods have shown strong performance compared with CNN-based detectors, in terms of both accuracy and running speed. Table~\\ref{table:det_coco} shows the detection results for different transformer-based object detectors mentioned earlier on the COCO 2012 val set.\n\\begin{figure}[h]\n\t\\begin{center}\n\t\t\\includegraphics[width=\\linewidth]{fig/det_diagram}\n\t\\end{center}\n\t\\vspace{-1em}\n\t\\caption{General framework of transformer-based object detection.}\n\t\\label{fig:det_diagram}\n\t\\vspace{-1.0em}\n\\end{figure}\n\\noindent\\textbf{Transformer-based Set Prediction for Detection}. As a pioneer for transformer-based detection method, the detection transformer (DETR) proposed by Carion~\\etal~ redesigns the framework of object detection. DETR, a simple and fully end-to-end object detector, treats the object detection task as an intuitive set prediction problem, eliminating traditional hand-crafted components such as anchor generation and non-maximum suppression (NMS) post-processing. As shown in Fig.~\\ref{fig:detr}, DETR starts with a CNN backbone to extract features from the input image. To supplement the image features with position information, fixed positional encodings are added to the flattened features before the features are fed into the encoder-decoder transformer. The decoder consumes the embeddings from the encoder along with $N$ learned positional encodings (object queries), and produces $N$ output embeddings. Here $N$ is a predefined parameter and typically larger than the number of objects in an image. Simple feed-forward networks (FFNs) are used to compute the final predictions, which include the bounding box coordinates and class labels to indicate the specific class of object (or to indicate that no object exists). Unlike the original transformer, which computes predictions sequentially, DETR decodes $N$ objects in parallel. DETR employs a bipartite matching algorithm to assign the predicted and ground-truth objects. As shown in Eq.~\\ref{eq:hungarian_loss}, the Hungarian loss is exploited to compute the loss function for all matched pairs of objects.\n\\begin{equation}  \\label{eq:hungarian_loss}\n\\scalebox{0.75}{$ \\displaystyle\n\t{\\cal L}_{\\rm Hungarian}{(y, \\hat{y})} = \\sum_{i=1}^N \\left[-\\log  \\hat{p}_{\\hat{\\sigma}(i)}(c_{i}) + {\\mathds{1}_{\\{c_i\\neq\\varnothing\\}}} {\\cal L}_{\\rm box}{(b_{i}, \\hat{b}_{\\hat{\\sigma}}(i))}\\right]\\,,\n\t$}\n\\end{equation}\nwhere $\\hat{\\sigma}$ is the optimal assignment, $c_i$ and $\\hat{p}_{\\hat{\\sigma}(i)}(c_{i})$ are the target class label and predicted label, respectively, and $b_i$ and $\\hat{b}_{\\hat{\\sigma}}(i)$ are the ground truth and predicted bounding box, $y=\\{(c_i, b_i)\\}$ and $\\hat y$ are the ground truth and prediction of objects, respectively.\nDETR shows impressive performance on object detection, delivering comparable accuracy and speed with the popular and well-established Faster R-CNN~ baseline on COCO benchmark.\n\\begin{figure}[h]\n\t\\begin{center}\n\t\t\\includegraphics[width=\\linewidth]{fig/DETR_detail_seagulls}\n\t\\end{center}\n\t\\vspace{-0.5em}\n\t\\caption{The overall architecture of DETR (image from ~).}\n\t\\label{fig:detr}\n\t\\vspace{-1.0em}\n\\end{figure}\nDETR is a new design for the object detection framework based on transformer and empowers the community to develop fully end-to-end detectors. However, the vanilla DETR poses several challenges, specifically,  longer training schedule and poor performance for small objects.\nTo address these challenges, Zhu~\\etal~ proposed Deformable DETR, which has become a popular method that significantly improves the detection performance. The deformable attention module attends to a small set of key positions around a reference point rather than looking at all spatial locations on image feature maps as performed by the original multi-head attention mechanism in transformer. This approach significantly reduces the computational complexity and brings benefits in terms of fast convergence. More importantly, the deformable attention module can be easily applied for fusing multi-scale features. Deformable DETR achieves better performance than DETR with $10\\times$ less training cost and $1.6\\times$ faster inference speed. And by using an iterative bounding box refinement method and two-stage scheme, Deformable DETR can further improve the detection performance.\nThere are also several methods to deal with the slow convergence problem of the original DETR. For example, Sun~\\etal~ investigated why the DETR model has slow convergence and discovered that this is mainly due to the cross-attention module in the transformer decoder. To address this issue, an encoder-only version of DETR is proposed, achieving considerable improvement in terms of detection accuracy and training convergence. In addition, a new bipartite matching scheme is designed for greater training stability and faster convergence and two transformer-based set prediction models, \\ie~TSP-FCOS and TSP-RCNN, are proposed to improve encoder-only DETR with feature pyramids. These new models achieve better performance compared with the original DETR model. Gao~\\etal~ proposed the Spatially Modulated Co-Attention (SMCA) mechanism to accelerate the convergence by constraining co-attention responses to be high near initially estimated bounding box locations. By integrating the proposed SMCA module into DETR, similar mAP could be obtained with about 10$\\times$ less training epochs under comparable inference cost.\nGiven the high computation complexity associated with DETR, Zheng~\\etal~ proposed an Adaptive Clustering Transformer (ACT) to reduce the computation cost of pre-trained DETR. ACT adaptively clusters the query features using a locality sensitivity hashing (LSH) method and broadcasts the attention output to the queries represented by the selected prototypes. ACT is used to replace the self-attention module of the pre-trained DETR model without requiring any re-training. This approach significantly reduces the computational cost while the accuracy slides slightly. The performance drop can be further reduced by utilizing a multi-task knowledge distillation (MTKD) method, which exploits the original transformer to distill the ACT module with a few epochs of fine-tuning. Yao~\\etal~~ pointed out that the random initialization in DETR is the main reason for the requirement of multiple decoder layers and slow convergence. To this end, they proposed the Efficient DETR to incorporate the dense prior into the detection pipeline via an additional region proposal network. The better initialization enables them to use only one decoder layers instead of six layers to achieve competitive performance with a more compact network.\n\\begin{table*}[ht]\n\t\\small\n\t\\caption{\\small Comparison of different transformer-based object detectors on COCO 2017 val set. Running speed (FPS) is evaluated on an NVIDIA Tesla V100 GPU as reported in~. \\textsuperscript\\textdagger Estimated speed according to the reported number in the paper. \\textsuperscript\\textdaggerdbl ViT backbone is pre-trained on ImageNet-21k. $^*$ViT backbone is pre-trained on an private dataset with 1.3 billion images.}\n\t\\label{table:det_coco}\n\t\\vspace{-1.0em}\n\t\\begin{center}\n\t\t\\scriptsize\n\t\t\\resizebox{0.85\\linewidth}{!}{\n\t\t\t\\begin{tabular}{l|c|cccccc|ccl}\n\t\t\t\t\\Xhline{1.2pt}\n\t\t\t\tMethod & Epochs & AP & AP$_\\text{50}$ & AP$_\\text{75}$ & AP$_\\text{S}$ & AP$_\\text{M}$ & AP$_\\text{L}$ & \\#Params (M) & GFLOPs & FPS\\\\\n\t\t\t\t\\hline\n\t\t\t\t\\textit{CNN based} &&&&&&&&&&\\\\\n\t\t\t\tFCOS~ & 36 & 41.0 & 59.8 & 44.1 & 26.2 & 44.6 & 52.2 & - & 177 & 23\\textsuperscript\\textdagger\\\\\n\t\t\t\tFaster R-CNN + FPN~ & 109 & 42.0 & 62.1 & 45.5 & 26.6 & 45.4 & 53.4  & 42 & 180 & 26 \\\\\n\t\t\t\t\\hline\n\t\t\t\t\\textit{CNN Backbone + Transformer Head} &&&&&&&&&&\\\\\n\t\t\t\tDETR~ & 500 & 42.0 & 62.4 & 44.2 & 20.5 & 45.8 & 61.1 & 41 & 86 & 28 \\\\\n\t\t\t\tDETR-DC5~ & 500 &  43.3 & 63.1 & 45.9 & 22.5 & 47.3 & 61.1 & 41 & 187 & 12 \\\\\n\t\t\t\tDeformable DETR~ & 50 & 46.2 & 65.2 & 50.0 & 28.8 & 49.2& 61.7& 40 & 173 & 19 \\\\\n\t\t\t\tTSP-FCOS~ & 36 & 43.1 & 62.3 & 47.0 & 26.6 & 46.8 & 55.9 & - & 189 & 20\\textsuperscript\\textdagger\\\\\n\t\t\t\tTSP-RCNN~ & 96 & {45.0} & {64.5} & {49.6} & {29.7} & {47.7} & 58.0 & - & 188 & 15\\textsuperscript\\textdagger\\\\\n\t\t\t\tACT+MKKD (L=32)~ & - & 43.1 & - & - & 61.4 & 47.1 & 22.2 & - & 169 &  14\\textsuperscript\\textdagger \\\\\n\t\t\t\tSMCA~ & 108 & 45.6 & 65.5 & 49.1 & 25.9 & 49.3 & 62.6 & - & - & - \\\\ \n\t\t\t\tEfficient DETR~ & 36 & 45.1 & 63.1 & 49.1 & 28.3 & 48.4 & 59.0 & 35 & 210 & - \\\\ \n\t\t\t\t{UP-DETR}~  & 150 & {40.5} & 60.8 & 42.6  & 19.0 & {44.4} & {60.0} & 41 & -& -  \\\\\n\t\t\t\t{UP-DETR}~  & 300 & {42.8} & 63.0 & 45.3& 20.8 & {47.1} & {61.7}  & 41 & -& - \\\\\n\t\t\t\t\\hline\n\t\t\t\t\\textit{Transformer Backbone + CNN Head} &&&&&&&&&&\\\\\n\t\t\t\tViT-B/16-FRCNN\\textsuperscript\\textdaggerdbl\n\t\t\t\t~ & 21 & 36.6 & 56.3 & 39.3 & 17.4 & 40.0 & 55.5 &- & -& -  \\\\\n\t\t\t\tViT-B/16-FRCNN$^*$~ & 21 & 37.8 &  57.4 & 40.1 & 17.8 & 41.4 & 57.3 & - & -& -  \\\\\n\t\t\t\tPVT-Small+RetinaNet~ & 12 & 40.4 & 61.3 & 43.0 & 25.0 & 42.9 & 55.7 & 34.2 & 118 & - \\\\ \n\t\t\t\tTwins-SVT-S+RetinaNet~ & 12 & 43.0 & 64.2 & 46.3 & 28.0 & 46.4 & 57.5 & 34.3 & 104 & - \\\\ \n\t\t\t\tSwin-T+RetinaNet~ & 12 & 41.5 & 62.1 & 44.2 & 25.1 & 44.9 & 55.5 & 38.5 & 118 & - \\\\ \n\t\t\t\tSwin-T+ATSS~ & 36 & 47.2 & 66.5 & 51.3 & - & - & - & 36 & 215 & - \\\\ \n\t\t\t\t\\hline\n\t\t\t\t\\textit{Pure Transformer based} &&&&&&&&&&\\\\\n\t\t\t\tPVT-Small+DETR~ & 50 & 34.7 & 55.7 & 35.4 & 12.0 & 36.4 & 56.7 & 40 & - & - \\\\ \n\t\t\t\tTNT-S+DETR~ & 50 & 38.2 & 58.9 & 39.4 & 15.5 & 41.1 & 58.8 & 39 & - & - \\\\ \n\t\t\t\tYOLOS-Ti~ & 300 & 30.0 & - & - & - & - & - & 6.5 & 21 & - \\\\ \n\t\t\t\tYOLOS-S~ & 150 & 37.6 & 57.6 & 39.2 & 15.9 & 40.2 & 57.3 & 28 & 179 & - \\\\ \n\t\t\t\tYOLOS-B~ & 150 & 42.0 & 62.2 & 44.5 & 19.5 & 45.3 & 62.1 & 127 & 537 & - \\\\ \n\t\t\t\t\\Xhline{2\\arrayrulewidth}\n\t\t\t\\end{tabular}\n\t\t}\n\t\\end{center}\n\t\\vspace{-1.5em}\n\\end{table*}\n\\noindent\\textbf{Transformer-based Backbone for Detection}. Unlike DETR which redesigns object detection as a set prediction tasks via transformer, Beal~\\etal~ proposed to utilize transformer as a backbone for common detection frameworks such as Faster R-CNN~. The input image is divided into several patches and fed into a vision transformer, whose output embedding features are reorganized according to spatial information before passing through a detection head for the final results. A massive pre-training transformer backbone could bring benefits to the proposed ViT-FRCNN. There are also quite a few methods to explore versatile vision transformer backbone design~ and transfer these backbones to traditional detection frameworks like RetinaNet~ and Cascade R-CNN~. For example, Swin Transformer~ obtains about 4 box AP gains over ResNet-50 backbone with similar FLOPs for various detection frameworks.\n\\noindent\\textbf{Pre-training for Transformer-based Object Detection}.\nInspired by the pre-training transformer scheme in NLP, several methods have been proposed to explore different pre-training scheme for transformer-based object detection~. Dai~\\etal~ proposed unsupervised pre-training for object detection (UP-DETR). Specifically, a novel unsupervised pretext task named random query patch detection is proposed to pre-train the DETR model. With this unsupervised pre-training scheme, UP-DETR significantly improves the detection accuracy on a relatively small dataset (PASCAL VOC). On the COCO benchmark with sufficient training data, UP-DETR still outperforms DETR, demonstrating the effectiveness of the unsupervised pre-training scheme.\nFang~\\etal~ explored how to transfer the pure ViT structure that is pre-trained on ImageNet to the more challenging object detection task and proposed the YOLOS detector. To cope with the object detection task, the proposed YOLOS first drops the classification tokens in ViT and appends learnable detection tokens. Besides, the bipartite matching loss is utilized to perform set prediction for objects. With this simple pre-training scheme on ImageNet dataset, the proposed YOLOS shows competitive performance for object detection on COCO benchmark.", "cites": [4798, 1513, 3769, 4808, 7373, 4802, 4806, 209, 7360, 7851, 4800, 4803, 1501, 7367, 4804, 4807, 4805, 7850, 4801, 2577, 4779], "cite_extract_rate": 0.9545454545454546, "origin_cites_number": 22, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section provides a coherent synthesis of transformer-based object detection methods, particularly focusing on DETR and its variants. It integrates insights from multiple papers on set prediction, deformable attention, and convergence improvements to form a narrative on how transformer-based detection differs from and improves upon traditional CNN-based approaches. While it offers strong analysis of challenges and solutions, the abstraction is somewhat limited to object detection-specific patterns rather than broader vision transformer principles."}}
{"id": "7577f812-d4fd-4834-bb2d-762f1714ca3c", "title": "Segmentation", "level": "subsubsection", "subsections": [], "parent_id": "b64eb56a-71c3-403c-bd63-e587ebef744c", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Vision Transformer"], ["subsection", "High/Mid-level Vision"], ["subsubsection", "Segmentation"]], "content": "Segmentation is an important topic in computer vision community, which broadly includes panoptic segmentation, instance segmentation and semantic segmentation~\\etc. Vision transformer has also shown impressive potential on the field of segmentation.\n\\noindent\\textbf{Transformer for Panoptic Segmentation.} \nDETR~ can be naturally extended for panoptic segmentation tasks and achieve competitive results by appending a mask head on the decoder. Wang~\\etal~ proposed Max-DeepLab to directly predict panoptic segmentation results with a mask transformer, without involving surrogate sub-tasks such as box detection. Similar to DETR, Max-DeepLab streamlines the panoptic segmentation tasks in an end-to-end fashion and directly predicts a set of non-overlapping masks and corresponding labels. Model training is performed using a panoptic quality (PQ) style loss, but unlike prior methods that stack a transformer on top of a CNN backbone, Max-DeepLab adopts a dual-path framework that facilitates combining the CNN and transformer.\n\\noindent\\textbf{Transformer for Instance Segmentation.}\nVisTR, a transformer-based video instance segmentation model, was proposed by Wang~\\etal~ to produce instance prediction results from a sequence of input images. A strategy for matching instance sequence is proposed to assign the predictions with ground truths. In order to obtain the mask sequence for each instance, VisTR utilizes the instance sequence segmentation module to accumulate the mask features from multiple frames and segment the mask sequence with a 3D CNN.\nHu~\\etal~ proposed an instance segmentation Transformer (ISTR) to predict low-dimensional mask embeddings, and match them with ground truth for the set loss. ISTR conducted detection and segmentation with a recurrent refinement strategy which is different from the existing top-down and bottom-up frameworks.\nYang~\\etal~ investigated how to realize better and more efficient embedding learning to tackle the semi-supervised video object segmentation under challenging multi-object scenarios.\nSome papers such as ~ also discussed using Transformer to deal with segmentation task.\n\\noindent\\textbf{Transformer for Semantic Segmentation.}\nZheng~\\etal~ proposed a transformer-based semantic segmentation network (SETR). SETR utilizes an encoder similar to ViT~ as the encoder to extract features from an input image. A multi-level feature aggregation module is adopted for performing pixel-wise segmentation.\nStrudel~\\etal~ introduced \\textit{Segmenter} which relies on the output embedding corresponding to image patches and obtains class labels with a point-wise linear decoder or a mask transformer decoder.\nXie~\\etal~ proposed a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders, which outputs multiscale features and avoids complex decoders.\n\\noindent\\textbf{Transformer for Medical Image Segmentation.}\nCao~\\etal~ proposed an Unet-like pure Transformer for medical image segmentation, by feeding the tokenized image patches into the Transformer-based U-shaped Encoder-Decoder architecture with skip-connections for local-global semantic feature learning. Valanarasu~\\etal~ explored transformer-based solutions and study the feasibility of using transformer-based network architectures for medical image segmentation tasks and proposed a Gated Axial-Attention model which extends the existing architectures by introducing an additional control mechanism in the self-attention module. Cell-DETR~, based on the DETR panoptic segmentation model, is an attempt to use transformer for cell instance segmentation. It adds skip connections that bridge features between the backbone CNN and the CNN decoder in the segmentation head in order to enhance feature fusion. Cell-DETR achieves state-of-the-art performance for cell instance segmentation from microscopy imagery.", "cites": [8838, 7360, 4771, 732, 4812, 4768, 8839, 8842, 4809, 4810, 4814, 4813, 4811, 2578], "cite_extract_rate": 1.0, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes well by grouping papers into subcategories (panoptic, instance, semantic, and medical image segmentation) and highlighting common themes such as end-to-end frameworks and the use of transformers for global context modeling. It provides some critical insights, such as noting how DETR-based methods streamline pipelines and the limitations of CNNs in learning long-range dependencies. However, while it identifies patterns like dual-path frameworks or the use of lightweight decoders, it could offer more meta-level abstraction to unify the insights into broader principles of transformer-based segmentation."}}
{"id": "4451b6d8-c354-4409-9f97-4499fabbc61a", "title": "Pose Estimation", "level": "subsubsection", "subsections": [], "parent_id": "b64eb56a-71c3-403c-bd63-e587ebef744c", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Vision Transformer"], ["subsection", "High/Mid-level Vision"], ["subsubsection", "Pose Estimation"]], "content": "Human pose and hand pose estimation are foundational topics that have attracted significant interest from the research community. Articulated pose estimation is akin to a structured prediction task, aiming to predict the joint coordinates or mesh vertices from input RGB/D images. Here we discuss some methods~ that explore how to utilize transformer for modeling the global structure information of human poses and hand poses.\n\\noindent\\textbf{Transformer for Hand Pose Estimation.}\nHuang~\\etal~ proposed a transformer based network for 3D hand pose estimation from point sets. The encoder first utilizes a PointNet~ to extract point-wise features from input point clouds and then adopts standard multi-head self-attention module to produce embeddings. In order to expose more global pose-related information to the decoder, a feature extractor such as PointNet++~ is used to extract hand joint-wise features, which are then fed into the decoder as positional encodings.\nSimilarly, Huang~\\etal~ proposed HOT-Net (short for hand-object transformer network) for 3D hand-object pose estimation. Unlike the preceding method which employs transformer to directly predict 3D hand pose from input point clouds, HOT-Net uses a ResNet to generate initial 2D hand-object pose and then feeds it into a transformer to predict the 3D hand-object pose. A spectral graph convolution network is therefore used to extract input embeddings for the encoder.\nHampali~\\etal~ proposed to estimate the 3D poses of two hands given a single color image. Specifically, appearance and spatial encodings of a set of potential 2D locations for the joints of both hands were inputted to a transformer, and the attention mechanisms were used to sort out the correct configuration of the joints and outputted the 3D poses of both hands.\n\\noindent\\textbf{Transformer for Human Pose Estimation.}\nLin~\\etal~ proposed a mesh transformer (METRO) for predicting 3D human pose and mesh from a single RGB image. METRO extracts image features via a CNN and then perform position encoding by concatenating a template human mesh to the image feature. A multi-layer transformer encoder with progressive dimensionality reduction is proposed to gradually reduce the embedding dimensions and finally produce 3D coordinates of human joint and mesh vertices. To encourage the learning of non-local relationships between human joints, METRO randomly mask some input queries during training.\nYang~\\etal~ constructed an explainable model named TransPose based on Transformer architecture and low-level convolutional blocks. The attention layers built in Transformer can capture long-range spatial relationships between keypoints and explain what dependencies the predicted keypoints locations highly rely on.\nLi~\\etal~ proposed a novel approach based on Token representation for human Pose estimation (TokenPose). Each keypoint was explicitly embedded as a token to simultaneously learn constraint relationships and appearance cues from images.\nMao~\\etal~ proposed a human pose estimation framework that solved the task in the regression-based fashion. They formulated the pose estimation task into a sequence prediction problem and solve it by transformers, which bypass the drawbacks of the heatmap-based pose estimator.\nJiang~\\etal~ proposed a novel transformer based network that can learn a distribution over both pose and motion in an unsupervised fashion rather than tracking body parts and trying to temporally smooth them. The method overcame inaccuracies in detection and corrected partial or entire skeleton corruption.\nHao~\\etal~ proposed to personalize a human pose estimator given a set of test images of a person without using any manual annotations. The method adapted the pose estimator during test time to exploit person-specific information, and used a Transformer model to build a transformation between the self-supervised keypoints and the supervised keypoints.", "cites": [4816, 4799, 4817, 7852, 4769, 4815, 7385], "cite_extract_rate": 0.6363636363636364, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a clear description of various transformer-based methods for pose estimation, organizing them into hand and human pose categories. It synthesizes the general approach of using transformers to capture global relationships but does not go beyond listing or paraphrasing the methods with minimal comparative or critical analysis. There is little abstraction or identification of broader patterns or principles in the use of transformers for this task."}}
{"id": "cc9a3312-5269-40b6-aa0d-579b90407630", "title": "Other Tasks", "level": "subsubsection", "subsections": [], "parent_id": "b64eb56a-71c3-403c-bd63-e587ebef744c", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Vision Transformer"], ["subsection", "High/Mid-level Vision"], ["subsubsection", "Other Tasks"]], "content": "There are also quite a lot different high/mid-level vision tasks that have explored the usage of vision transformer for better performance. We briefly review several tasks below.\n\\noindent\\textbf{Pedestrian Detection.}\nBecause the distribution of objects is very dense in occlusion and crowd scenes, \nadditional analysis and adaptation are often required when common detection networks are applied to pedestrian detection tasks.\nLin~\\etal~ revealed that sparse uniform queries and a weak attention field in the decoder result in performance degradation \nwhen directly applying DETR or Deformable DETR to pedestrian detection tasks. \nTo alleviate these drawbacks, the authors proposes Pedestrian End-to-end Detector (PED), \nwhich employs a new decoder called Dense Queries and Rectified Attention field (DQRF) to support dense queries and alleviate the noisy or narrow attention field of the queries.\nThey also proposed V-Match, which achieves additional performance improvements by fully leveraging visible annotations.\n\\noindent\\textbf{Lane Detection.}\nBased on PolyLaneNet~, Liu~\\etal~ proposed a method called LSTR, \nwhich improves performance of curve lane detection by learning the global context with a transformer network. \nSimilar to PolyLaneNet, LSTR regards lane detection as a task of fitting lanes with polynomials and uses neural networks to predict the parameters of polynomials. \nTo capture slender structures for lanes and the global context, LSTR introduces a transformer network into the architecture.\nThis enables processing of low-level features extracted by CNNs. \nIn addition, LSTR uses Hungarian loss to optimize network parameters. \nAs demonstrated in~, LSTR outperforms PolyLaneNet, with 2.82\\% higher accuracy and $3.65\\times$ higher FPS using 5-times fewer parameters.\nThe combination of a transformer network, CNN and Hungarian Loss culminates in a lane detection framework that is precise, fast, and tiny. \nConsidering that the entire lane line generally has an elongated shape and long-range,  \nLiu~\\etal~ utilized a transformer encoder structure for more efficient context feature extraction. This transformer encoder structure improves the detection of the proposal points a lot,\nwhich rely on contextual features and global information, especially in the case where the backbone network is a small model.\n\\noindent\\textbf{Scene Graph.}\nScene graph is a structured representation of a scene that can clearly express the objects, attributes, and relationships between objects in the scene~. To generate scene graph, most of existing methods first extract image-based object representations and then do message propagation between them. Graph R-CNN~ utilizes self-attention to integrate contextual information from neighboring nodes in the graph. Recently, Sharifzadeh~\\etal~ employed transformers over the extracted object embedding. Sharifzadeh~\\etal~ proposed a new pipeline called \\textit{Texema} and employed a pre-trained Text-to-Text Transfer Transformer (T5)~ to create structured graphs from textual input and utilized them to improve the relational reasoning module. The T5 model enables \\textit{Texema} to utilize the knowledge in texts. \n\\noindent\\textbf{Tracking.}\nSome researchers also explored to use transformer encoder-decoder architecture in template-based discriminative trackers, such as TMT~, TrTr~ and TransT~. \nAll these work use a Siamese-like tracking pipeline to do video object tracking and utilize the encoder-decoder network to replace explicit cross-correlation operation for global and rich contextual inter-dependencies.\nSpecifically, the transformer encoder and decoder are assigned to the template branch and the searching branch, respectively. \nIn addition, Sun~\\etal~proposed TransTrack~, which is an online joint-detection-and-tracking pipeline. It utilizes the query-key mechanism to track pre-existing objects and introduces a set of learned object queries into the pipeline to detect new-coming objects.\nThe proposed TransTrack achieves 74.5\\% and 64.5\\% MOTA on MOT17 and MOT20 benchmark.\n\\noindent\\textbf{Re-Identification.}\nHe~\\etal~ proposed TransReID to investigate the application of pure transformers in the field of object re-identification (ReID).\nWhile introducing transformer network into object ReID, TransReID slices with overlap to reserve local neighboring structures around the patches and introduces 2D bilinear interpolation to help handle any given input resolution.\nWith the transformer module and the loss function,  a strong baseline was proposed to achieve comparable performance with CNN-based frameworks.\nMoreover, The jigsaw patch module (JPM) was designed to facilitate perturbation-invariant and robust feature representation of objects and the side information embeddings (SIE) was introduced  to encode side information.\nThe final framework TransReID achieves state-of-the-art performance on both person and vehicle ReID benchmarks.\nBoth Liu~\\etal~ and Zhang~\\etal~ provided solutions for introducing transformer network into video-based person Re-ID.\nAnd similarly, both of the them utilized separated transformer networks to refine spatial and temporal features, and then utilized a cross view transformer to aggregate multi-view features.\n\\noindent\\textbf{Point Cloud Learning.}\nA number of other works exploring transformer architecture for point cloud learning~ have also emerged recently. For example, Guo~\\etal~ proposed a novel framework that replaces the original self-attention module with a more suitable offset-attention module, which includes implicit Laplace operator and normalization refinement. In addition, Zhao~\\etal~ designed a novel transformer architecture called Point Transformer. The proposed self-attention layer is invariant to the permutation of the point set, making it suitable for point set processing tasks. Point Transformer shows strong performance for semantic segmentation task from 3D point clouds.", "cites": [4824, 4822, 4818, 271, 4819, 7853, 4825, 7854, 9, 4823, 4821, 4826, 4820, 4797], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section demonstrates strong synthesis by grouping papers under distinct high/mid-level vision tasks and highlighting how transformers are adapted for each. It includes some critical analysis, such as identifying performance limitations in DETR when applied to pedestrian detection and the overfitting risks in vanilla transformers for Re-ID. While it captures patterns in transformer usage across tasks, the abstraction remains at a moderate level without offering a unified theoretical framework."}}
{"id": "c366fb4c-b924-49d7-94e1-d9300122ba81", "title": "Discussions", "level": "subsubsection", "subsections": [], "parent_id": "b64eb56a-71c3-403c-bd63-e587ebef744c", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Vision Transformer"], ["subsection", "High/Mid-level Vision"], ["subsubsection", "Discussions"]], "content": "As discussed in the preceding sections, transformers have shown strong performance on several high-level tasks, including detection, segmentation and pose estimation.\nThe key issues that need to be resolved before transformer can be adopted for high-level tasks relate to input embedding, position encoding, and prediction loss. Some methods propose improving the self-attention module from different perspectives, for example, deformable attention~, adaptive clustering~ and point transformer~. Nevertheless, exploration into the use of transformers for high-level vision tasks is still in the preliminary stages and so further research may prove beneficial. For example, is it necessary to use feature extraction modules such as CNN and PointNet before transformer for potential better performance? How can vision transformer be fully leveraged using large scale pre-training datasets as BERT and GPT-3 do in the NLP field? And is it possible to pre-train a single transformer model and fine-tune it for different downstream tasks with only a few epochs of fine-tuning? How to design more powerful architecture by incorporating prior knowledge of the specific tasks? Several prior works have performed preliminary discussions for the aforementioned topics and We hope more further research effort is conducted into exploring more powerful transformers for high-level vision.", "cites": [4823, 1513, 7373], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from the cited papers, such as deformable attention, adaptive clustering, and the local-global attention mechanism, to highlight common challenges in high/mid-level vision tasks. It offers some critical evaluation by pointing out that transformer research in this area is still preliminary and raises open questions, but the critique remains general rather than deeply nuanced. The section abstracts to a degree by identifying overarching research directions, though it does not propose a novel meta-framework or principle."}}
{"id": "83e08fc1-983f-4990-92ea-7fda264810f2", "title": "Image Generation", "level": "subsubsection", "subsections": [], "parent_id": "c5c9c14a-5593-4d02-be7e-5f554e8e2cb7", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Vision Transformer"], ["subsection", "Low-level Vision"], ["subsubsection", "Image Generation"]], "content": "An simple yet effective to apply transformer model to the image generation task is to directly change the architectures from CNNs to transformers, as shown in Figure~\\ref{fig:sec6-1} (a). Jiang~\\etal~ proposed TransGAN, which build GAN using the transformer architecture. Since the it is difficult to generate high-resolution images pixel-wise, a memory-friendly generator is utilized by gradually increasing the feature map resolution at different stages. Correspondingly, a multi-scale discriminator is designed to handle the varying size of inputs in different stages. Various training recipes are introduced including grid self-attention, data augmentation, relative position encoding and modified normalization to stabilize the training and improve its performance. Experiments on various benchmark datasets demonstrate the effectiveness and potential of the transformer-based GAN model in image generation tasks. Kwonjoon Lee~\\etal~ proposed ViTGAN, which introduce several technique to both generator and discriminator to stabilize the training procedure and convergence. Euclidean distance is introduced for the self-attention module to enforce the Lipschitzness of transformer discriminator. Self-modulated layernorm and implicit neural representation are proposed to enhance the training for the generator. As a result, ViTGAN is the first work to demonstrate transformer-based GANs can achieve comparable performance to state-of-the-art CNN-based GANs.  \nParmar~\\etal~ proposed Image Transformer, taking the first step toward generalizing the transformer model to formulate image translation and generation tasks in an auto-regressive manner. Image Transformer consists of two parts: an encoder for extracting image representation and a decoder to generate pixels. For each pixel with value $0-255$, a $256 \\times d$ dimensional embedding is learned for encoding each value into a $d$ dimensional vector, which is fed into the encoder as input. The encoder and decoder adopt the same architecture as that in~. \nEach output pixel $q'$ is generated by calculating self-attention between the input pixel $q$ and previously generated pixels $m_1,m_2,...$ with position embedding $p_1,p_2,...$. For image-conditioned generation, such as super-resolution and inpainting, an encoder-decoder architecture is used, where the encoder's input is the low-resolution or corrupted images. For unconditional and class-conditional generation (\\emph{i.e.}, noise to image), only the decoder is used for inputting noise vectors. Because the decoder's input is the previously generated pixels (involving high computation cost when producing high-resolution images), a local self-attention scheme is proposed. This scheme uses only the closest generated pixels as input for the decoder, enabling Image Transformer to achieve performance on par with CNN-based models for image generation and translation tasks, demonstrating the effectiveness of transformer-based models on low-level vision tasks.\nSince it is difficult to directly generate high-resolution images by transformer models, Esser~\\etal~ proposed Taming Transformer. Taming Transformer consists of two parts: a VQGAN and a transformer. VQGAN is a variant of VQVAE~, which uses a discriminator and perceptual loss to improve the visual quality. Through VQGAN, the image can be represented by a series of context-rich discrete vectors and therefore these vectors can be easily predicted by a transformer model through an auto-regression way. The transformer model can learn the long-range interactions for generating high-resolution images. As a result, the proposed Taming Transformer achieves state-of-the-art results on a wide variety of image synthesis tasks.\nBesides image generation, DALL$\\cdot$E~ proposed the transformer model for text-to-image generation, which synthesizes images according to the given captions. The whole framework consists of two stages. In the first stage, a discrete VAE is utilized to learn the visual codebook. In the second stage, the text is decoded by BPE-encode and the corresponding image is decoded by dVAE learned in the first stage. Then an auto-regression transformer is used to learn the prior between the encoded text and image. During the inference procedure, tokens of images are predicted by the transformer and decoded by the learned decoder. The CLIP model~ is introduced to rank generated samples. Experiments on text-to-image generation task demonstrate the powerful ability of the proposed model. Note that our survey mainly focus on pure vision tasks, we do not include the framework of DALL$\\cdot$E in Figure~\\ref{fig:sec6-1}. The image generation has been pushed to a higher level with the introduction of diffusion model~, such as DALLE2~ and Stable Diffusion~.", "cites": [3018, 4765, 4827, 7339, 4828, 2558, 2240, 1639, 38], "cite_extract_rate": 0.8181818181818182, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes information by connecting different transformer-based image generation approaches (TransGAN, ViTGAN, Image Transformer, Taming Transformer, DALLÂ·E) and explaining their design choices and contributions. It provides some critical analysis, such as noting the high computational cost of generating high-resolution images with transformers and how methods like local self-attention and VQGAN address this. While it identifies some patterns (e.g., the use of discrete representations and multi-stage frameworks), it stops short of offering deeper meta-level insights or novel theoretical frameworks."}}
{"id": "4a812f57-04c0-4e6b-9bd1-a5ed11f6cf63", "title": "Image Processing", "level": "subsubsection", "subsections": [], "parent_id": "c5c9c14a-5593-4d02-be7e-5f554e8e2cb7", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Vision Transformer"], ["subsection", "Low-level Vision"], ["subsubsection", "Image Processing"]], "content": "A number of recent works eschew using each pixel as the input for transformer models and instead use patches (set of pixels) as input. For example, Yang~\\etal~ proposed Texture Transformer Network for Image Super-Resolution (TTSR), using the transformer architecture in the reference-based image super-resolution problem. It aims to transfer relevant textures from reference images to low-resolution images. Taking a low-resolution image and reference image as the query $\\mathbf Q$ and key $\\mathbf K$, respectively, the relevance $r_{i,j}$ is calculated between each patch $\\mathbf q_i$ in $\\mathbf Q$ and $\\mathbf k_i$ in $\\mathbf K$ as: \n\\begin{equation}\n\tr_{i,j}=\\left<\\frac{\\mathbf q_i}{\\Vert \\mathbf q_i\\Vert},\\frac{\\mathbf k_i}{\\Vert \\mathbf k_i\\Vert}\\right>.\n\\end{equation} \nA hard-attention module is proposed to select high-resolution features $\\mathbf V$ according to the reference image, so that the low-resolution image can be matched by using the relevance. The hard-attention map is calculated as:\n\\begin{equation}\n\th_i = \\arg \\max_j r_{i,j}\n\\end{equation} \nThe most relevant reference patch is $\\mathbf t_i = \\mathbf v_{h_i}$, where $\\mathbf t_i$ in $\\mathbf T$ is the transferred features. A soft-attention module is then used to transfer $\\mathbf V$ to the low-resolution feature. The transferred features from the high-resolution texture image and the low-resolution feature are used to generate the output features of the low-resolution image. By leveraging the transformer-based architecture, TTSR can successfully transfer texture information from high-resolution reference images to low-resolution images in super-resolution tasks.\n\\begin{figure}[h]\n\t\\centering\n\t\\includegraphics[width=1.0\\linewidth]{./fig/IPT.pdf}\n\t\\caption{Diagram of IPT architecture (image from~).}\n\t\\label{fig:ipt}\n\t\\vspace{-1.0em}\n\\end{figure}\nDifferent from the preceding methods that use transformer models on single tasks, Chen~\\etal~ proposed Image Processing Transformer (IPT), which fully utilizes the advantages of transformers by using large pre-training datasets. It achieves state-of-the-art performance in several image processing tasks, including super-resolution, denoising, and deraining. As shown in Figure~\\ref{fig:ipt}, IPT consists of multiple heads, an encoder, a decoder, and multiple tails. The multi-head, multi-tail structure and task embeddings are introduced for different image processing tasks. The features are divided into patches, which are fed into the encoder-decoder architecture. Following this, the outputs are reshaped to features with the same size. Given the advantages of pre-training transformer models on large datasets, IPT uses the ImageNet dataset for pre-training. Specifically, images from this dataset are degraded by manually adding noise, rain streaks, or downsampling in order to generate corrupted images. The degraded images are used as inputs for IPT, while the original images are used as the optimization goal of the outputs. A self-supervised method is also introduced to enhance the generalization ability of the IPT model. Once the model is trained, it is fine-tuned on each task by using the corresponding head, tail, and task embedding. IPT largely achieves performance improvements on image processing tasks (\\emph{e.g.}, 2 dB in image denoising tasks), demonstrating the huge potential of applying transformer-based models to the field of low-level vision.\nBesides single image generation, Wang~\\etal~ proposed SceneFormer to utilize transformer in 3D indoor scene generation. By treating a scene as a sequence of objects, the transformer decoder can be used to predict series of objects and their location, category, and size. This has enabled SceneFormer to outperform conventional CNN-based methods in user studies.\n\\begin{figure}[h]\n\t\\centering\n\t\\includegraphics[width=0.5\\linewidth]{fig/sec6-2.pdf}\n\t\\vspace{-0.5em}\n\t\\caption{A generic framework for transformer in image processing.}\n\t\\label{fig:sec6-2}\n\t\\vspace{-1.5em}\n\\end{figure}\nIt should be noted that iGPT~ is pre-trained on an inpainting-like task. Since iGPT mainly focus on the fine-tuning performance on image classification tasks, we treat this work more like an attempt on image classification task using transformer than low-level vision tasks.\nIn conclusion, different to classification and detection tasks, the outputs of image generation and processing are images. Figure~\\ref{fig:sec6-2} illustrates using transformers in low-level vision. In image processing tasks, the images are first encoded into a sequence of tokens or patches and the transformer encoder uses the sequence as input, allowing the transformer decoder to successfully produce desired images. In image generation tasks, the GAN-based models directly learn a decoder to generated patches to outputting images through linear projection, while the transformer-based models train a auto-encoder to learn a codebook for images and use an auto-regression transformer model to predict the encoded tokens. A meaningful direction for future research would be designing a suitable architecture for different image processing tasks.", "cites": [7070, 4766, 4829], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers, including TTSR, IPT, and SceneFormer, and connects their approaches under the broader theme of using transformers for image processing tasks. It also provides a basic level of critical analysis by distinguishing between low-level vision and image classification tasks (e.g., iGPT). While it offers a structured overview and a few general insights (e.g., the use of encoder-decoder frameworks), it does not deeply analyze limitations or provide a meta-level abstraction of the field."}}
{"id": "68cdcc7f-6e4a-402d-a913-fbbef5065276", "title": "Video Processing", "level": "subsection", "subsections": ["aa8b784e-e729-4975-b072-e11598fcf9de", "d213c2ac-79ae-458f-ba1e-9647eb50fedc", "2d658147-903a-4a8c-a8cb-ef515f11d288"], "parent_id": "dfe7037d-ce2c-483e-a574-d13f6529e3d8", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Vision Transformer"], ["subsection", "Video Processing"]], "content": "Transformer performs surprisingly well on sequence-based tasks and especially on NLP tasks. In computer vision~(specifically, video tasks), spatial and temporal dimension information is favored, giving rise to the application of transformer in a number of video tasks, such as frame synthesis~, action recognition~, and video retrieval~.", "cites": [4830, 4831], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section briefly mentions the application of transformers in video processing and lists a few tasks without substantial synthesis or comparison of the cited papers. It lacks critical evaluation of the works and does not generalize to broader patterns or principles in the field."}}
{"id": "aa8b784e-e729-4975-b072-e11598fcf9de", "title": "High-level Video Processing", "level": "subsubsection", "subsections": [], "parent_id": "68cdcc7f-6e4a-402d-a913-fbbef5065276", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Vision Transformer"], ["subsection", "Video Processing"], ["subsubsection", "High-level Video Processing"]], "content": "\\noindent\\textbf{Video Action Recognition.} Video human action tasks, as the name suggests, involves identifying and localizing human actions in videos. Context (such as other people and objects) plays a critical role in recognizing human actions. Rohit~\\emph{et al.} proposed the action transformer~ to model the underlying relationship between the human of interest and the surrounding context. Specifically, the I3D~ is used as the backbone to extract high-level feature maps. The features extracted (using RoI pooling) from intermediate feature maps are viewed as the query (Q), while the key (K) and values (V) are calculated from the intermediate features. A self-attention mechanism is applied to the three components, and it outputs the classification and regressions predictions. Lohit~\\emph{et al.}~ proposed an interpretable differentiable module, named temporal transformer network, to reduce the intra-class variance and increase the inter-class variance. In addition, Fayyaz and Gall proposed a temporal transformer~ to perform action recognition tasks under weakly supervised settings. In addition to human action recognition, transformer has been utilized for group activity recognition~. Gavrilyuk~\\emph{et al.} proposed an actor-transformer~ architecture to learn the representation, using the static and dynamic representations generated by the 2D and 3D networks as input. The output of the transformer is the predicted activity.\n\\noindent\\textbf{Video Retrieval.} The key to content-based video retrieval is to find the similarity between videos. Leveraging only the image-level of video-level features to overcome the associated challenges, Shao~\\emph{et al.}~ suggested using the transformer to model the long-range semantic dependency. They also introduced the supervised contrastive learning strategy to perform hard negative mining. The results of using this approach on benchmark datasets demonstrate its performance and speed advantages. In addition, Gabeur~\\emph{et al.}~ presented a multi-modal transformer to learn different cross-modal cues in order to represent videos.\n\\noindent\\textbf{Video Object Detection.} To detect objects in a video, both global and local information is required. Chen~\\emph{et al.} introduced the memory enhanced global-local aggregation~(MEGA)~ to capture more content. The representative features enhance the overall performance and address the \\emph{ineffective} and \\emph{insufficient} problems. Furthermore, Yin~\\emph{et al.}~ proposed a spatiotemporal transformer to aggregate spatial and temporal information. Together with another spatial feature encoding component, these two components perform well on 3D video object detection tasks.\n\\noindent\\textbf{Multi-task Learning.} Untrimmed video usually contains many frames that are irrelevant to the target tasks. It is therefore crucial to mine the relevant information and discard the redundant information. To extract such information, Seong~\\emph{et al.} proposed the video multi-task transformer network~, which handles multi-task learning on untrimmed videos. For the CoVieW dataset, the tasks are scene recognition, action recognition and importance score prediction. Two pre-trained networks on ImageNet and Places365 extract the scene features and object features. The multi-task transformers are stacked to implement feature fusion, leveraging the class conversion matrix~(CCM).", "cites": [2939, 50, 4832, 4830, 783, 1514], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual summary of various vision transformer applications in high-level video processing, such as action recognition, retrieval, and object detection. While it briefly connects methods by mentioning shared mechanisms like self-attention, it does not offer a deeper synthesis of ideas or comparative insights. Critical evaluation of the cited works is minimal, and abstraction to broader principles or trends is largely absent."}}
{"id": "d213c2ac-79ae-458f-ba1e-9647eb50fedc", "title": "Low-level Video Processing", "level": "subsubsection", "subsections": [], "parent_id": "68cdcc7f-6e4a-402d-a913-fbbef5065276", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Vision Transformer"], ["subsection", "Video Processing"], ["subsubsection", "Low-level Video Processing"]], "content": "\\noindent\\textbf{Frame/Video Synthesis.} Frame synthesis tasks involve synthesizing the frames between two consecutive frames or after a frame sequence while video synthesis tasks involve synthesizing a video. Liu~\\emph{et al.} proposed the ConvTransformer~, which is comprised of five components: feature embedding, position encoding, encoder, query decoder, and the synthesis feed-forward network. Compared with LSTM based works, the ConvTransformer achieves superior results with a more parallelizable architecture. Another transformer-based approach was proposed by Schatz~\\emph{et al.}~, which uses a recurrent transformer network to synthetize human actions from novel views.\n\\noindent\\textbf{Video Inpainting.} Video inpainting tasks involve completing any missing regions within a frame. This is challenging, as it requires information along the spatial and temporal dimensions to be merged. Zeng~\\emph{et al.} proposed a spatial-temporal transformer network~, which uses all the input frames as input and fills them in parallel. The spatial-temporal adversarial loss is used to optimize the transformer network.", "cites": [4767, 4831, 50], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of two papers related to video processing tasks but lacks deeper synthesis or critical evaluation. It briefly mentions components and methods without establishing connections between them or identifying broader trends. The abstraction level is minimal, as it focuses on specific models and techniques rather than general principles."}}
{"id": "6d0ff228-46de-4b2d-9105-47d18ed6996d", "title": "Multi-Modal Tasks", "level": "subsection", "subsections": [], "parent_id": "dfe7037d-ce2c-483e-a574-d13f6529e3d8", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Vision Transformer"], ["subsection", "Multi-Modal Tasks"]], "content": "Owing to the success of transformer across text-based NLP tasks, many researches are keen to exploit its potential for processing multi-modal tasks (\\eg, video-text, image-text and audio-text). One example of this is VideoBERT~, which uses a CNN-based module to pre-process videos in order to obtain representation tokens. A transformer encoder is then trained on these tokens to learn the video-text representations for downstream tasks, such as video caption. Some other examples include VisualBERT~ and VL-BERT~, which adopt a single-stream unified transformer to capture visual elements and image-text relationship for downstream tasks such as visual question answering (VQA) and visual commonsense reasoning (VCR). In addition, several studies such as SpeechBERT~ explore the possibility of encoding audio and text pairs with a transformer encoder to process auto-text tasks such as speech question answering (SQA).\n\\begin{figure}[htp]\n\t\\centering\n\t\\includegraphics[width=1.0\\linewidth]{fig/clip.png}\n\t\\caption{The framework of the CLIP (image from~).}\n\t\\label{CLIP}\n\t\\vspace{-0.5em}\n\\end{figure}\nApart from the aforementioned pioneering multi-modal transformers, Contrastive Language-Image Pre-training (CLIP)~ takes natural language as supervision to learn more efficient image representation. CLIP jointly trains a text encoder and an image encoder to predict the corresponding training text-image pairs. The text encoder of CLIP is a standard transformer with masked self-attention used to preserve the initialization ability of the pre-trained language models. For the image encoder, CLIP considers two types of architecture, ResNet and Vision Transformer. CLIP is trained on a new dataset containing 400 million (image, text) pairs collected from the Internet. More specifically, given a batch of $N$ (image, text) pairs, CLIP learns both text and image embeddings jointly to maximize the cosine similarity of those $N$ matched embeddings while minimize $N^{2} - N$ incorrectly matched embeddings. On Zero-Shot transfer, CLIP demonstrates astonishing zero-shot classification performances, achieving $76.2\\%$ top-1 accuracy on ImageNet-1K dataset without using any ImageNet training labels. Concretely, at inference, the text encoder of CLIP first computes the feature embeddings of all ImageNet Labels and the image encoder then computes the embeddings of all images. By calculating the cosine similarity of text and image embeddings, the text-image pair with the highest score should be the image and its corresponding label. Further experiments on 30 various CV benchmarks show the zero-shot transfer ability of CLIP and the feature diversity learned by CLIP.\nWhile CLIP maps images according to the description in text, another work DALL-E~ synthesizes new images of categories described in an input text. Similar to GPT-3, DALL-E is a multi-modal transformer with 12 billion model parameters autoregressively trained on a dataset of 3.3 million text-image pairs. More specifically, to train DALL-E, a two-stage training procedure is used, where in stage 1, a discrete variational autoencoder is used to compress 256$\\times$ 256 RGB images into 32$\\times$32 image tokens and then in stage 2, an autoregressive transformer is trained to model the joint distribution over the image and text tokens. Experimental results show that DALL-E can generate images of various styles from scratch, including photorealistic imagery, cartoons and emoji or extend an existing image while still matching the description in the text. Subsequently, Ding $\\etal$ proposes CogView~, which is a transformer with VQ-VAE tokenizer similar to DALL-E, but supports Chinese text input. They claim CogView outperforms DALL-E and previous GAN-bsed methods and also unlike DALL-E, CogView does not need an additional CLIP model to rerank the samples drawn from transformer, $\\ie$ DALL-E.\nRecently, a Unified Transformer (UniT)~ model is proposed to cope with multi-modal multi-task learning, which can simultaneously handle multiple tasks across different domains, including object detection, natural language understanding and vision-language reasoning. Specifically, UniT has two transformer encoders to handle image and text inputs, respectively, and then the transformer decoder takes the single or concatenated encoder outputs according to the task modality. Finally, a task-specific prediction head is applied to the decoder outputs for different tasks. In the training stage, all tasks are jointly trained by randomly selecting a specific task within an iteration. The experiments show UniT achieves satisfactory performance on every task with a compact set of model parameters.\nIn conclusion, current transformer-based mutil-modal models demonstrates its architectural superiority for unifying data and tasks of various modalities, which demonstrates the potential of transformer to build a general-purpose intelligence agents able to cope with vast amount of applications. Future researches can be conducted in exploring the effective training or the extendability of multi-modal transformers (\\eg, GPT-4~).", "cites": [7361, 1272, 768, 7339, 9115, 7855, 1639], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple multi-modal transformer approaches effectively, presenting them in a coherent narrative that highlights their application domains and mechanisms. It provides some critical insights, such as noting that CogView avoids the need for an additional CLIP model for reranking, and briefly mentions the broader implications of these models. However, deeper comparative analysis and broader meta-level abstraction could be enhanced."}}
{"id": "6da3d6b7-df40-4094-8948-3d535d61bddf", "title": "Pruning and Decomposition", "level": "subsubsection", "subsections": [], "parent_id": "5564eb1e-a1d3-4eda-88ec-dcc4e91fae88", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Vision Transformer"], ["subsection", "Efficient Transformer"], ["subsubsection", "Pruning and Decomposition"]], "content": "In transformer based pre-trained models (\\eg, BERT), multiple  attention operations are performed in parallel to independently model the relationship between  different tokens~. However, specific tasks do not require all heads to be used.  For example, Michel~\\etal~  presented empirical evidence that a large percentage of attention heads can be removed at test time without  impacting performance significantly. The  number of heads required varies across different layers â€” some layers may even require only one head.  Considering the redundancy on attention heads, importance scores are defined to estimate the influence of each head on the final output in , and unimportant heads can be removed for efficient deployment. Dalvi~\\etal~  analyzed the redundancy in pre-trained transformer models from two perspectives: general redundancy and task-specific redundancy. Following the lottery ticket hypothesis~, Prasanna~\\etal~ analyzed the lotteries in BERT and showed that good sub-networks also exist in transformer-based models, reducing both the FFN layers and attention heads in order to achieve high compression rates. For the vision transformer~ which splits an image to multiple patches, Tang~\\etal~ proposed to reduce patch calculation to accelerate the inference, and the redundant patches can be automatically discovered by considering their contributions to the effective output features. Zhu~\\etal~ extended the network slimming approach~ to vision transformers for reducing the dimensions of linear projections in both FFN and attention modules.  \n\\begin{table}[b]\t\n\t\\vspace{-1em}\n\t\\centering\n\t\\renewcommand\\arraystretch{1.0}\n\t\\setlength{\\tabcolsep}{2pt}{\n\t\t\\begin{threeparttable}[b]\n\t\t\t\\caption{List of representative compressed transformer-based models. The data of the Table is from~.}\n\t\t\t\\vspace{-0.5em}\n\t\t\t\\footnotesize\n\t\t\t\\begin{tabular}{c|c|c|c|c}\n\t\t\t\t\\Xhline{1.2pt}\n\t\t\t\tModels & Compress Type & $\\#$Layer &Params & Speed Up \\\\\t\t\n\t\t\t\t\\hline\n\t\t\t\tBERT$_{BASE}~$& Baseline & 12 & 110M & $\\times$1\\\\\n\t\t\t\t\\hline\n\t\t\t\tALBERT~ & Decomposition & 12 & 12M & $\\times$5.6 \\\\\n\t\t\t\t\\hline\n\t\t\t\tBERT-& Architecture & \\multirow{2}*{6} & \\multirow{2}*{66M} & \\multirow{2}*{$\\times$1.94}\\\\\n\t\t\t\tof-Theseus~& design &  &  & \\\\\n\t\t\t\t\\hline\n\t\t\t\tQ-BERT~ & \\multirow{2}*{Quantization} & 12 & \\multirow{2}*{-} & \\multirow{2}*{-} \\\\\n\t\t\t\tQ8BERT~ & &12 & & \\\\\n\t\t\t\t\\hline\n\t\t\t\tTinyBERT~ & \\multirow{5}*{Distillation} & 4 & 14.5M & $\\times$9.4 \\\\\n\t\t\t\tDistilBERT~ & & 6 & 6.6m & $\\times$1.63 \\\\\n\t\t\t\tBERT-PKD~ & & 3$\\sim$6 & 45.7$\\sim$67M & $\\times$3.73$\\sim$1.64 \\\\\n\t\t\t\tMobileBERT~ & &24 &25.3M & $\\times$4.0 \\\\\n\t\t\t\tPD~ & & 6 &67.5M &$\\times$2.0\\\\\n\t\t\t\t\\hline\n\t\t\t\\end{tabular}\n\t\t\t\\label{tbl:compressed_tf]}\n\t\\end{threeparttable}}\n\\end{table}\nIn addition to the width of transformer models, the depth (\\ie, the number of layers) can also be reduced to accelerate the inference process~.\nDiffering from the concept that different attention heads in transformer models can be computed in parallel, different layers have to be calculated sequentially because the input of the next layer depends on the output of previous layers. Fan~\\etal~ proposed a layer-wisely dropping strategy to regularize the training of models, and then the whole layers are removed together at the test phase. \nBeyond the pruning methods that directly discard modules in transformer models, matrix decomposition aims to approximate the large matrices with multiple small matrices based on the low-rank assumption. For example, Wang~\\etal~ decomposed the standard matrix multiplication in transformer models, improving the inference efficiency.", "cites": [4487, 7, 7580, 688, 4515, 1445, 4485, 856, 732, 1150, 4834, 4512, 854, 38, 8843, 2912, 4486, 2488, 4833, 2481], "cite_extract_rate": 0.9545454545454546, "origin_cites_number": 22, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of pruning and decomposition methods for transformer models, mentioning various approaches and citing multiple related papers. However, it lacks deeper synthesis of the ideas, critical evaluation of their strengths and limitations, or abstraction to broader principles. While it introduces some concepts like redundancy and low-rank matrix decomposition, the integration of these ideas is minimal."}}
{"id": "e6ef70fb-bcd6-4521-9f3a-877a3a621cb9", "title": "Knowledge Distillation", "level": "subsubsection", "subsections": [], "parent_id": "5564eb1e-a1d3-4eda-88ec-dcc4e91fae88", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Vision Transformer"], ["subsection", "Efficient Transformer"], ["subsubsection", "Knowledge Distillation"]], "content": "Knowledge distillation aims to train student networks by transferring knowledge from large teacher networks~. Compared  with teacher networks, student networks usually have thinner and shallower architectures, which  are easier to be deployed on resource-limited resources. \nBoth the output and  intermediate features of  neural networks can also be used to transfer effective information from teachers  to students. Focused on  transformer models, Mukherjee~\\etal~ used the pre-trained BERT~ as a teacher to guide the training  of small models, leveraging large amounts of unlabeled data.  Wang~\\etal~  train the student networks to mimic the output of self-attention layers in the pre-trained teacher models. The dot-product between values is introduced as a new form of knowledge for guiding  students. A teacher's assistant~ is also introduced in , reducing the gap between  large pre-trained transformer models and compact student networks, thereby facilitating the mimicking process. Due to the various types of layers in the transformer model (\\ie, self-attention layer, embedding layer, and prediction layers),  Jiao~\\etal~ design different objective functions to transfer knowledge from teachers to students. For example, the outputs of student models' embedding layers  imitate those of teachers via MSE losses.  For the vision transformer, Jia~\\etal~ proposed a fine-grained manifold distillation method, which excavates effective knowledge through the relationship between images and the\ndivided patches.", "cites": [7, 4508, 4511, 681, 4510, 4502], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of knowledge distillation methods for visual transformers, integrating ideas from multiple papers. It synthesizes different strategies such as using intermediate features, self-attention distillation, and teacher assistants to present a cohesive narrative. While it offers some abstraction by highlighting the role of distillation in reducing model size, it lacks deeper critical evaluation of the methods' effectiveness or limitations."}}
{"id": "8979ac7e-1209-41f7-8a85-bab7c69d8672", "title": "Quantization", "level": "subsubsection", "subsections": [], "parent_id": "5564eb1e-a1d3-4eda-88ec-dcc4e91fae88", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Vision Transformer"], ["subsection", "Efficient Transformer"], ["subsubsection", "Quantization"]], "content": "Quantization aims to reduce the number of bits needed to represent network weight or intermediate features~. \nQuantization methods for general neural networks have been discussed at length and achieve  performance on par with the original networks~. Recently, there has been growing interest in how to specially quantize transformer models~. For example, Shridhar~\\etal~ suggested embedding the input into binary high-dimensional vectors, and then using the binary input representation to train the binary neural networks. \nCheong~\\etal~ represented the weights in the transformer models by low-bit~(\\eg, 4-bit) representation. \nZhao~\\etal~ empirically investigated various quantization methods and showed that k-means quantization has a huge development potential. Aimed at machine translation tasks, Prato~\\etal~ proposed a fully quantized transformer, which, as the paper claims, is the first 8-bit model not to suffer any loss in translation quality. Beside, Liu~\\etal~ explored a post-training quantization scheme to reduce the memory storage and computational costs of vision transformers.", "cites": [4838, 4837, 4839, 4835, 4840, 4770, 4836, 4841], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of various quantization approaches for visual and general transformer models but lacks synthesis of ideas across the cited papers. It does not evaluate the methods critically or identify broader patterns or principles. The content remains largely a list of methods with minimal integration or analysis."}}
{"id": "45cdaf41-09ce-4384-a7c9-5f2b841aebdc", "title": "Compact Architecture Design", "level": "subsubsection", "subsections": [], "parent_id": "5564eb1e-a1d3-4eda-88ec-dcc4e91fae88", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Vision Transformer"], ["subsection", "Efficient Transformer"], ["subsubsection", "Compact Architecture Design"]], "content": "Beyond compressing  pre-defined transformer models into smaller ones, some works attempt to design compact models directly~. Jiang~\\etal~ simplified the calculation of self-attention by proposing a new module â€” called span-based dynamic convolution â€” that combine the fully-connected layers and the convolutional layers. Interesting ``hamburger'' layers are proposed in , using matrix decomposition to substitute the original self-attention layers.Compared with standard self-attention operations, matrix decomposition can be calculated more efficiently while clearly reflecting the dependence between different tokens. The design of efficient transformer architectures can also be automated with neural architecture search~(NAS)~, which automatically  searches how to combine different components. For example, Su~\\etal~ searched patch size\nand dimensions of linear projections and head number of attention modules to get an efficient vision transformer. Li~\\etal~ explored a self-supervised search strategy to get a hybrid architecture composing of both convolutional modules and self-attention modules.\n\\begin{figure}[t] \n\t\\centering\n\t\\includegraphics[width=0.9\\columnwidth]{fig/relation}\n\t\\vspace{-0.5em}\n\t\\caption{Different methods for compressing transformers.}\n\t\\label{fig-relation}\n\t\\vspace{-1.5em}\n\\end{figure}\nThe self-attention operation in transformer models calculates the dot product between  representations from different input tokens in a given sequence (patches in image recognition tasks~), whose complexity is $O(N)$, where $N$ is the length of the sequence. Recently, there has been a targeted focus to reduce the complexity  to $O(N)$ in large methods so that transformer models can scale to long sequences~.   For example, Katharopoulos~\\etal~ approximated  self-attention as  a linear dot-product of kernel feature maps and revealed the relationship between tokens via RNNs. Zaheer~\\etal~ considered each token as a vertex in a graph and defined the inner product calculation between two tokens as an edge. Inspired by graph theories~, various sparse graph are combined to approximate the dense graph in transformer models, and can achieve $O(N)$ complexity. \n\\textbf{Discussion.} The preceding methods take different approaches in how they attempt to identify redundancy in transformer models~(see Figure~\\ref{fig-relation}). Pruning and decomposition  methods usually require  pre-defined models with redundancy. Specifically, pruning focuses on reducing the number of components (\\eg, layers, heads) in transformer models while decomposition   represents an original matrix with multiple small matrices. Compact models also can be directly designed either manually (requiring sufficient expertise) or automatically (\\eg, via NAS). The obtained compact models can be further represented with low-bits via quantization methods for  efficient deployment on resource-limited devices.", "cites": [4842, 4739, 732, 8361, 1468, 7581, 1461, 798, 9146, 1499], "cite_extract_rate": 0.7692307692307693, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes diverse approaches to compact vision transformer design, including manual design, matrix decomposition, NAS, and linear attention approximations. It offers some critical discussion by highlighting limitations (e.g., redundancy in pre-defined models) and contrasting methods (e.g., pruning vs. decomposition). While the abstraction is strong in identifying general principles like computational efficiency and model design strategies, it could more explicitly frame overarching trends or implications for future research."}}
{"id": "03ef196d-2ace-402d-8396-b09ffe2bb58c", "title": "Challenges", "level": "subsection", "subsections": [], "parent_id": "b14bed65-de98-4f6c-809e-aee94af9dec1", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Conclusions and Discussions"], ["subsection", "Challenges"]], "content": "Although researchers have proposed many transformer-based models to tackle computer vision tasks, these works are only the first steps in this field and still have much room for improvement. For example, the transformer architecture in ViT~ follows the standard transformer for NLP~, but an improved version specifically designed for CV remains to be explored. Moreover, it is necessary to apply transformer to more tasks other than those mentioned earlier.\nThe generalization and robustness of transformers for computer vision are also challenging. Compared with CNNs, pure transformers lack some inductive biases and rely heavily on massive datasets for large-scale training~. Consequently, the quality of data has a significant influence on the generalization and robustness of transformers. Although ViT shows exceptional performance on downstream image classification tasks such as CIFAR~ and VTAB~, directly applying the ViT backbone on object detection has failed to achieve better results than CNNs~. There is still a long way to go in order to better generalize pre-trained transformers on more generalized visual tasks. Practitioners concern the robustness of transformer (e.g. the vulnerability issue~). Although the robustness has been investigated in~, it is still an open problem waiting to be solved.\nAlthough numerous works have explained the use of transformers in NLP~, it remains a challenging subject to clearly explain why transformer works well on visual tasks. The inductive biases, including translation equivariance and locality, are attributed to CNNâ€™s success, but transformer lacks any inductive bias. The current literature usually analyzes the effect in an intuitive way~. For example, Dosovitskiy~\\etal~ claim that large-scale training can surpass inductive bias. Position embeddings are added into image patches to retain positional information, which is important in computer vision tasks. Inspired by the heavy parameter usage in transformers, over-parameterization~ may be a potential point to the interpretability of vision transformers.\nLast but not least, developing efficient transformer models for CV remains an open problem. Transformer models are usually huge and computationally expensive. For example, the base ViT model~ requires 18 billion FLOPs to process an image. In contrast, the lightweight CNN model GhostNet~ can achieve similar performance with only about 600 million FLOPs. Although several methods have been proposed to compress transformer, they remain highly complex. And these methods, which were originally designed for NLP, may not be suitable for CV. Consequently, efficient transformer models are urgently needed so that vision transformer can be deployed on resource-limited devices.", "cites": [4844, 4849, 4847, 4845, 732, 4846, 6975, 7850, 4851, 4843, 4850, 3699, 4848, 8844, 38], "cite_extract_rate": 0.9375, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.8, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a strong analytical evaluation of the current state of visual transformers by synthesizing insights from multiple papers, connecting ideas such as robustness, efficiency, and interpretability. It critically discusses limitations, such as the lack of inductive bias and high computational cost, while comparing transformers to CNNs. The section also abstracts beyond individual works to highlight broader challenges in generalization, efficiency, and interpretability for visual transformers."}}
{"id": "bdf7ae00-2d0f-4900-ba45-f70cecac8a5c", "title": "Future Prospects", "level": "subsection", "subsections": [], "parent_id": "b14bed65-de98-4f6c-809e-aee94af9dec1", "prefix_titles": [["title", "A Survey on Visual Transformer"], ["section", "Conclusions and Discussions"], ["subsection", "Future Prospects"]], "content": "In order to drive the development of vision transformers, we provide several potential directions for future study.\nOne direction is the effectiveness and the efficiency of transformers in computer vision. The goal is to develop highly effective and efficient vision transformers; specifically, transformers with high performance and low resource cost. The performance determines whether the model can be applied on real-world applications, while the resource cost influences the deployment on devices~~. The effectiveness is usually correlated with the efficiency, so determining how to achieve a better balance between them is a meaningful topic for future study.\nMost of the existing vision transformer models are designed to handle only a single task. Many NLP models such as GPT-3~ have demonstrated how transformer can deal with multiple tasks in one model. IPT~ in the CV field is also able to process multiple low-level vision tasks, such as super-resolution, image denoising, and deraining. Perceiver~ and Perceiver IO~ are the pioneering models that can work on several domains including images, audio, multimodal, point clouds. We believe that more tasks can be involved in only one model. Unifying all visual tasks and even other tasks in one transformer (i.e., a grand unified model) is an exciting topic.\nThere have been various types of neural networks, such as CNN, RNN, and transformer. In the CV field, CNNs used to be the mainstream choice~, but now transformer is becoming popular. CNNs can capture inductive biases such as translation equivariance and locality, whereas ViT uses large-scale training to surpass inductive bias~. From the evidence currently available~, CNNs perform well on small datasets, whereas transformers perform better on large datasets. The question for the future is whether to use CNN or transformer.\nBy training with large datasets, transformers can achieve state-of-the-art performance on both NLP~ and CV benchmarks~. It is possible that neural networks need big data rather than inductive bias. In closing, we leave you with a question: Can transformer obtains satisfactory results with a very simple computational paradigm (\\eg, with only fully connected layers) and massive data training?\n\\section*{Acknowledgement}\nThis research is partially supported by MindSpore (\\url{https://mindspore.cn/}) and CANN (Compute Architecture for Neural Networks).\n\\appendix\n\\subsection*{A1. General Formulation of Self-attention}\n\\label{sec:formulation-sa}\nThe self-attention module~ for machine translation computes the responses at each position in a sequence by estimating attention scores to all positions and gathering the corresponding embeddings based on the scores accordingly. This can be viewed as a form of non-local filtering operations~. We follow the convention~ to formulate the self-attention module. Given an input signal (\\eg, image, sequence, video and feature) $\\mathbf X\\in \\mathbb{R}^{n\\times d}$, where $n=h\\times w$ (indicating the number of pixels in feature) and $d$ is the number of channels, the output signal is generated as:\n\\begin{equation}\n\t\\mathbf y_i = \\frac{1}{C(\\mathbf x_i)}\\sum_{\\forall j}{f(\\mathbf x_i, \\mathbf x_j)g(\\mathbf x_j)},\n\t\\label{eq:self-attention}\n\\end{equation}\nwhere $\\mathbf x_i\\in \\mathbb{R}^{1\\times d}$ and $\\mathbf y_i\\in \\mathbb{R}^{1\\times d}$ indicate the $i^{th}$ position (\\eg, space, time and spacetime) of the input signal $\\mathbf X$ and output signal $\\mathbf Y$, respectively. Subscript $j$ is the index that enumerates all positions, and a pairwise function $f(\\cdot)$ computes a representing relationship (such as affinity) between $i$ and all $j$. The function $g(\\cdot)$ computes a representation of the input signal at position $j$, and the response is normalized by a factor $C(x_i)$.\nNote that there are many choices for the pairwise function $f(\\cdot)$. For example, a simple extension of the Gaussian function could be used to compute the similarity in an embedding space. As such, the function $f(\\cdot)$ can be formulated as:\n\\begin{equation}\n\tf(\\mathbf x_i, \\mathbf x_j) = e^{\\theta(\\mathbf x_i)\\phi(\\mathbf x_j)^T}\n\\end{equation}\nwhere $\\theta(\\cdot)$ and $\\phi(\\cdot)$ can be any embedding layers. If we consider the $\\theta(\\cdot), \\phi(\\cdot), g(\\cdot)$ in the form of linear embedding: $\\theta(\\mathbf X)=\\mathbf X\\mathbf W_{\\theta}$, $\\phi(\\mathbf X)=\\mathbf X\\mathbf W_{\\phi}, g(\\mathbf X)=\\mathbf X\\mathbf W_g$ where $\\mathbf W_{\\theta}\\in \\mathbb{R}^{d\\times d_k}, \\mathbf W_{\\phi}\\in \\mathbb{R}^{d\\times d_k}, \\mathbf W_g\\in \\mathbb{R}^{d\\times d_v}$, and set the normalization factor as $C(\\mathbf x_i)=\\sum_{\\forall j}{f(\\mathbf x_i, \\mathbf x_j)}$, the Eq.~\\ref{eq:self-attention} can be rewritten as:\n\\begin{equation}\n\t\\mathbf y_i = \\frac{e^{\\mathbf x_iw_{\\theta,i}\\mathbf w_{\\phi,j}^T\\mathbf x_j^T}}{\\sum_j{e^{\\mathbf x_i\\mathbf w_{\\theta,i}\\mathbf w_{\\phi,j}^Tx_j^T}}}\\mathbf x_j\\mathbf w_{g,j},\n\\end{equation}\nwhere $\\mathbf w_{\\theta,i}\\in \\mathbb{R}^{d\\times 1}$ is the $i^{th}$ row of the weight matrix $W_{\\theta}$. For a given index $i$, $\\frac{1}{C(\\mathbf x_i)}f(\\mathbf x_i, \\mathbf x_j)$ becomes the softmax output along the dimension $j$. The formulation can be further rewritten as:\n\\begin{equation}\n\t\\mathbf Y = \\mathrm{softmax}(\\mathbf X\\mathbf W_{\\theta}\\mathbf W_{\\phi}^T\\mathbf X)g(\\mathbf X),\n\t\\label{eq:self-attention-softmax}\n\\end{equation}\nwhere $\\mathbf Y\\in \\mathbb{R}^{n\\times c}$ is the output signal of the same size as $\\mathbf X$.\nCompared with the query, key and value representations $\\mathbf Q=\\mathbf X\\mathbf W_q, \\mathbf K=\\mathbf X\\mathbf W_k, \\mathbf V=\\mathbf X\\mathbf W_v$ from the translation module, once $\\mathbf W_q=\\mathbf W_{\\theta}, \\mathbf W_k=\\mathbf W_{\\phi}, \\mathbf W_v=\\mathbf W_g$, Eq.~\\ref{eq:self-attention-softmax} can be formulated as:\n\\begin{equation}\n\t\\mathbf Y = \\mathrm{softmax}(\\mathbf Q\\mathbf K^T)\\mathbf V = \\mathrm{Attention}(\\mathbf Q, \\mathbf K, \\mathbf V),\n\\end{equation}\nThe self-attention module~ proposed for machine translation is, to some extent, the same as the preceding non-local filtering operations proposed for computer vision.\nGenerally, the final output signal of the self-attention module for computer vision will be wrapped as:\n\\begin{equation}\n\t\\mathbf Z = \\mathbf Y\\mathbf W^o + \\mathbf X\n\\end{equation}\nwhere $\\mathbf Y$ is generated through Eq.~\\ref{eq:self-attention-softmax}. If $\\mathbf W^o$ is initialized as zero, this self-attention module can be inserted into any existing model without breaking its initial behavior.\n\\subsection*{A2. Revisiting Transformers for NLP}\\label{Sec:Introduction}\nBefore transformer was developed, RNNs (~\\eg, GRU~ and LSTM~) with added attention~ empowered most of the state-of-the-art language models. However, RNNs require the information flow to be processed sequentially from the previous hidden states to the next one. This rules out the possibility of using acceleration and parallelization during training, and consequently hinders the potential of RNNs to process longer sequences or build larger models. In 2017, Vaswani~\\etal~ proposed transformer, a novel encoder-decoder architecture built solely on multi-head self-attention mechanisms and feed-forward neural networks. Its purpose was to solve seq-to-seq natural language tasks (\\eg, machine translation) easily by acquiring global dependencies. The subsequent success of transformer demonstrates that leveraging attention mechanisms alone can achieve performance comparable with attentive RNNs. Furthermore, the architecture of transformer lends itself to massively parallel computing, which enables training on larger datasets. This has given rise to the surge of large pre-trained models (PTMs) for natural language processing.\nBERT~ and its variants (\\eg, SpanBERT~, RoBERTa~) are a series of PTMs built on the multi-layer transformer encoder architecture. Two tasks are conducted on BookCorpus~ and English Wikipedia datasets at the pre-training stage of BERT: 1) Masked language modeling (MLM), which involves first randomly masking out some tokens in the input and then training the model to predict; 2) Next sentence prediction, which uses paired sentences as input and predicts whether the second sentence is the original one in the document. After pre-training, BERT can be fine-tuned by adding an output layer on a wide range of downstream tasks. More specifically, when performing sequence-level tasks (\\eg, sentiment analysis), BERT uses the representation of the first token for classification; for token-level tasks (\\eg, name entity recognition), all tokens are fed into the softmax layer for classification. At the time of its release, BERT achieved the state-of-the-art performance on 11 NLP tasks, setting a milestone in pre-trained language models. Generative Pre-trained Transformer models (\\eg, GPT~, GPT-2~) are another type of PTMs based on the transformer decoder architecture, which uses masked self-attention mechanisms. The main difference between the GPT series and BERT is the way in which pre-training is performed. Unlike BERT, GPT models are unidirectional language models pre-trained using Left-to-Right (LTR) language modeling. Furthermore, BERT learns the sentence separator ([SEP]) and classifier token ([CLS]) embeddings during pre-training, whereas these embeddings are involved in only the fine-tuning stage of GPT. Due to its unidirectional pre-training strategy, GPT achieves superior performance in many natural language generation tasks.  More recently, a massive transformer-based model called GPT-3, which has an astonishing 175 billion parameters, was developed~. By pre-training on 45 TB of compressed plaintext data, GPT-3 can directly process different types of downstream natural language tasks without fine-tuning. As a result, it achieves strong performance on many NLP datasets, including both natural language understanding and generation. Since the introduction of transformer, many other models have been proposed in addition to the transformer-based PTMs mentioned earlier. We list a few representative models in Table~\\ref{tbl:tf_based} for interested readers, but this is not the focus of our study.\n\\begin{table}[h]\n\t\\centering\n\t\\renewcommand\\arraystretch{1.1}\n\t\\setlength{\\tabcolsep}{3pt}{\n\t\t\\begin{threeparttable}[b]\n\t\t\t\\caption{List of representative language models built on transformer. Transformer is the standard encoder-decoder architecture. Transformer Enc. and Dec. represent the encoder and decoder, respectively. Decoder uses mask self-attention to prevent attending to the future tokens. The data of the Table is from ~.}\n\t\t\t\\footnotesize\n\t\t\t\\begin{tabular}{c|c|c|c}\t\t\t\t\n\t\t\t\t\\Xhline{1.2pt}\n\t\t\t\tModels & Architecture & \\# of Params & Fine-tuning \\\\\t\t\n\t\t\t\t\\hline\n\t\t\t\tGPT~ & Transformer Dec. & 117M & Yes \\\\\n\t\t\t\tGPT-2~& Transformer Dec. & 117M-1542M & No \\\\\n\t\t\t\tGPT-3~& Transformer Dec. & 125M-175B & No \\\\\n\t\t\t\tBERT~ & Transformer Enc. & 110M-340M & Yes \\\\\n\t\t\t\tRoBERTa~ & Transformer Enc. & 355M & Yes \\\\\n\t\t\t\t\\multirow{2}{*}{XLNet~} & Two-Stream & \\multirow{2}{*}{$\\approx$ BERT} & \\multirow{2}{*}{Yes} \\\\\n\t\t\t\t& Transformer Enc. &  &  \\\\\n\t\t\t\tELECTRA~ & Transformer Enc. & 335M & Yes \\\\\n\t\t\t\tUniLM~ & Transformer Enc. & 340M & Yes\\\\\n\t\t\t\tBART~  & Transformer & 110\\% of BERT & Yes\\\\\n\t\t\t\tT5~ & Transfomer & 220M-11B & Yes\\\\\n\t\t\t\tERNIE (THU)~ & Transformer Enc. & 114M & Yes\\\\\n\t\t\t\tKnowBERT~ & Transformer Enc. & 253M-523M & Yes\\\\\n\t\t\t\t\\hline\n\t\t\t\\end{tabular}\n\t\t\t\\label{tbl:tf_based}\n\t\\end{threeparttable}}\n\t\\vspace{-0em}\n\\end{table}\nApart from the PTMs trained on large corpora for general NLP tasks, transformer-based models have also been applied in many other NLP-related domains and to multi-modal tasks. \n\\noindent\\textbf{BioNLP Domain.} Transformer-based models have outperformed many traditional biomedical methods. Some examples of such models include BioBERT~, which uses a transformer architecture for biomedical text mining tasks, and SciBERT~, which is developed by training transformer on 114M scientific articles (covering biomedical and computer science fields) with the aim of executing NLP tasks in the scientific domain more precisely. Another example is ClinicalBERT, proposed by Huang ~\\etal~. It utilizes transformer to develop and evaluate continuous representations of clinical notes. One of the side effects of this is that the attention map of ClinicalBERT can be used to explain predictions, thereby allowing high-quality connections between different medical contents to be discovered. \nThe rapid development of transformer-based models on a variety of NLP-related tasks demonstrates its structural superiority and versatility, opening up the possibility that it will become a universal module applied in many AI fields other than just NLP. The following part of this survey focuses on the applications of transformer in a wide range of computer vision tasks that have emerged over the past two years.\n\\subsection*{A3. Self-attention for Computer Vision}\\label{subsec:selfatt}\nThe preceding sections reviewed methods that use a transformer architecture for vision tasks. We can conclude that self-attention plays a pivotal role in transformer. The self-attention module can also be considered a building block of CNN architectures, which have low scaling properties concerning the large receptive fields. This building block is widely used on top of the networks to capture long-range interactions and enhance high-level semantic features for vision tasks. In this section, we delve deeply into the models based on self-attention designed for challenging tasks in computer vision. Such tasks include semantic segmentation, instance segmentation, object detection, keypoint detection, and depth estimation. Here we briefly summarize the existing applications using self-attention for computer vision.\n\\textbf{Image Classification.}\nTrainable attention for classification consists of two main streams: hard attention~ regarding the use of an image region, and soft attention~ generating non-rigid feature maps. Ba~\\etal~ first proposed the term ``visual attention'' for image classification tasks, and used attention to select relevant regions and locations within the input image. This can also reduce the computational complexity of the proposed model regarding the size of the input image. For medical image classification, AG-CNN~ was proposed to crop a sub-region from a global image by the attention heat map. And instead of using hard attention and recalibrating the crop of feature maps, SENet~ was proposed to reweight the channel-wise responses of the convolutional features using soft self-attention. Jetley~\\etal~ used attention maps generated by corresponding estimators to reweight intermediate features in DNNs. In addition, Han~\\etal~ utilized the attribute-aware attention to enhance the representation of CNNs.\n\\textbf{Semantic Segmentation.}\nPSANet~, OCNet~, DANet~ and CFNet~ are the pioneering works to propose using the self-attention module in semantic segmentation tasks. These works consider and augment the relationship and similarity~ between the contextual pixels. DANet~ simultaneously leverages the self-attention module on spatial and channel dimensions, whereas $A^2$Net~ groups the pixels into a set of regions, and then augments the pixel representations by aggregating the region representations with the generated attention weights. DGCNet~ employs a dual graph CNN to model coordinate space similarity and feature space similarity in a single framework. To improve the efficiency of the self-attention module for semantic segmentation, several works~ have been proposed, aiming to alleviate the huge amount of parameters brought by calculating pixel similarities. For example, CGNL~ applies the Taylor series of the RBF kernel function to approximate the pixel similarities. CCNet~ approximates the original self-attention scheme via two consecutive criss-cross attention modules. In addition, ISSA~ factorizes the dense affinity matrix as the product of two sparse affinity matrices. There are other related works using attention based graph reasoning modules~ to enhance both the local and global representations.\n\\textbf{Object Detection.}\nRamachandran~\\etal~ proposes an attention-based layer and swapped the conventional convolution layers to build a fully attentional detector that outperforms the typical RetinaNet~ on COCO benchmark~. GCNet~ assumes that the global contexts modeled by non-local operations are almost the same for different query positions within an image, and unifies the simplified formulation and SENet~ into a general framework for global context modeling~.\nVo~\\etal~ designs a bidirectional operation to gather and distribute information from a query position to all possible positions. Zhang~\\etal~ suggests that previous methods fail to interact with cross-scale features, and proposes Feature Pyramid Transformer, based on the self-attention module, to fully exploit interactions across both space and scales.\nConventional detection methods usually exploit a single visual representation (\\eg, bounding box and corner point) for predicting the final results. Hu~\\etal~ proposes a relation module based on self-attention to process a set of objects simultaneously through interaction between their appearance features. Cheng~\\etal~ proposes RelationNet++ with the bridging visual representations (BVR) module to combine different heterogeneous representations into a single one similar to that in the self-attention module. Specifically, the master representation is treated as the query input and the auxiliary representations are regarded as the key input. The enhanced feature can therefore bridge the information from auxiliary representations and benefit final detection results.\n\\textbf{Other Vision Tasks.}\nZhang~\\etal~ proposes a resolution-wise attention module to learn enhanced feature maps when training multi-resolution networks to obtain accurate human keypoint locations for pose estimation task. Furthermore, Chang~\\etal~ uses an attention-mechanism based feature fusion block to improve the accuracy of the human keypoint detection model.\nTo explore more generalized contextual information for improving the self-supervised monocular trained depth estimation, Johnston~\\etal~ directly leverages self-attention module. Chen~\\etal~ also proposes an attention-based aggregation network to capture context information that differs in diverse scenes for depth estimation. And Aich~\\etal~ proposes bidirectional attention modules that utilize the forward and backward attention operations for better results of monocular depth estimation.\n\\ifCLASSOPTIONcaptionsoff\n  \\newpage\n\\fi\n\\renewcommand\\refname{References}\n{\\small\n\\bibliographystyle{unsrt2authabbrvpp}\n\\bibliography{ref}\n}\n\\end{document}", "cites": [3124, 4853, 7, 2483, 826, 7840, 679, 7856, 3769, 7070, 1445, 4808, 1746, 4855, 168, 7167, 486, 4861, 4856, 8845, 97, 732, 1777, 2575, 4802, 9, 8492, 7858, 2473, 11, 4862, 50, 38, 4860, 7295, 8846, 4738, 2491, 1557, 4859, 746, 735, 4854, 39, 4857, 4858, 2576, 767, 7857, 8378, 4852, 7859], "cite_extract_rate": 0.6842105263157895, "origin_cites_number": 76, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes insights from multiple papers to discuss the future of vision transformers, particularly in terms of efficiency, multi-task capabilities, and the role of data versus inductive bias. It provides some critical reflections on the limitations of current models and the trade-offs between CNNs and transformers. While it offers a general direction for research, it does not deeply critique specific methodologies or establish a novel theoretical framework."}}
