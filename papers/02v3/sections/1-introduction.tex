\section{Introduction}
% 作者分工：夏再禹

The rapid proliferation of scientific publications has made it increasingly challenging for researchers to identify papers that provide genuine intellectual contributions beyond surface-level reporting~\cite{bornmann2015growth}. While existing tools effectively measure citation impact~\cite{garfield1972citation}, novelty detection~\cite{wang2017novelty,ai-etal-2025-novascore,zhang2026opennoveltyllmpoweredagenticverifiable,Arts_2025}, or writing quality~\cite{ke2019automated}, a critical dimension remains largely unexplored: \textit{insightfulness}---the degree to which a paper advances understanding beyond the knowledge contained in its references.

We define insightfulness as the \textit{argumentative gain} that an author achieves when synthesizing, critiquing, or extending prior work. Consider two scenarios: (1) an author who simply restates findings from reference papers, and (2) an author who identifies gaps, synthesizes disparate findings, or proposes novel interpretations. The latter demonstrates higher insightfulness, yet current evaluation systems---including automated peer review frameworks~\cite{zhu-etal-2025-deepreview,taechoyotin2025remorautomatedpeerreview}, AI-driven scientific writing tools~\cite{lu2024aiscientistfullyautomated,zheng-etal-2025-automation}, and novelty assessment methods~\cite{shahid-etal-2025-literature,Guo2024IdeaBenchBL}---cannot distinguish between these cases.

Inspired by a cognitive hypothesis---that the value of reading a paper can be measured by the extent to which it enhances understanding \textit{beyond} what could be learned from reading all its references alone---we present \textbf{InsightEval}, an automated system for evaluating the insightfulness of scientific papers. InsightEval operationalizes insightfulness through three measurable dimensions:
\begin{itemize}
    \item \textbf{Depth}: Does the author provide deeper analysis or more fundamental understanding of a concept?
    \item \textbf{Breadth}: Does the author synthesize findings across multiple references to reveal broader patterns?
    \item \textbf{Height}: Does the author abstract findings to a higher conceptual level or propose generalizable principles?
\end{itemize}

\begin{figure}[!t]
\centering
% TODO: Insert system architecture diagram
\fbox{\parbox{0.95\columnwidth}{\centering [Figure Placeholder: System Architecture Diagram]\\ Showing the four-stage pipeline: Opinion Extraction $\rightarrow$ Evidence Retrieval $\rightarrow$ Insight Scoring $\rightarrow$ Report Synthesis}}
\caption{An Overview of the InsightEval System Architecture.}
\label{fig:architecture}
\end{figure}

Our system addresses several key requirements of the SIGIR Demonstrations Track. \textit{Target users} include senior reviewers seeking efficient paper quality assessment, junior researchers learning to write insightful introductions, and research institutions conducting systematic evaluation of scholarly output. \textit{The problem} we address is the gap between quantitative bibliometrics and qualitative assessment of intellectual contribution~\cite{Arnaout2025IndepthRI,Vo2024AssessingSI}. Unlike citation analysis tools (e.g., Connected Papers~\cite{connectedpapers}) that focus on network structure, or critical reading assistants (e.g., InsightGUIDE~\cite{Koloveas2025InsightGUIDEAO}) that provide opinion-level guidance, InsightEval examines the \textit{semantic relationship} between a paper's claims and its supporting references and quantifies insightfulness along interpretable dimensions.

In this demo, we introduce InsightEval, a novel system to evaluate the insightfulness of scientific papers in a comprehensive way. The overall architecture of the system consists of two main components:
1) \textbf{the InsightEval Library}: an easy to use evaluation library which implements the four-stage evaluation pipeline, including opinion extraction, evidence retrieval, insight scoring, and report synthesis;
2) \textbf{the InsightEval Studio}: a user-friendly and interactive Web interface which enables users to upload papers, configure evaluation parameters, and inspect detailed insight reports.
Our work makes the following key contributions:
\begin{itemize}
    \item \textbf{The InsightEval Studio for Interactive Paper Evaluation}: The studio enables users to evaluate papers without writing any code. It features an \textit{augmented document reader} with color-coded insight highlighting and an \textit{insight analysis panel} that displays evidence alignment, radar charts, and AI rationale for each opinion sentence.
    \item \textbf{The InsightEval Library for Multi-Dimensional Insight Scoring}: The library provides a comprehensive pipeline covering opinion extraction, semantic retrieval from cited references, and LLM-based scoring along three interpretable dimensions (depth, breadth, height), allowing researchers to build upon and customize the evaluation framework.
    \item \textbf{Empirical Deployment on Human-Written and AI-Generated Papers}: We deploy InsightEval on over 100 human-written and 100 AI-generated scientific papers, demonstrating its applicability across diverse writing sources and revealing distinctive patterns in insightfulness.
    \item \textbf{An Open-Source Implementation}: We open-source the InsightEval framework and provide comprehensive documentation. The InsightEval studio and library are publicly accessible at \url{https://github.com/gomate-community/2026-SIGIR-InsightEval}.
\end{itemize}
