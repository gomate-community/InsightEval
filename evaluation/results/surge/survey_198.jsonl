{"id": "53d8521a-c91c-4ad8-b4a0-76e5a59f261e", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "eebe98c8-dc7c-4bc4-b995-ecf8692f4502", "prefix_titles": [["title", "Software Engineering for AI-Based Systems: A Survey"], ["section", "Introduction"]], "content": "\\label{sec:intro}\nIn the last decade, increased computer processing power, larger datasets, and better algorithms have enabled advances in Artificial Intelligence (AI) . Indeed, AI has evolved towards a new wave, which Deng calls “the rising wave of Deep Learning” (DL) \\footnote{\\url{https://en.wikipedia.org/wiki/History_of_artificial_intelligence##Deep_learning,_big_data_and_artificial_general_intelligence:_2011-present}}. DL has become feasible, leading to Machine Learning (ML) becoming integral to many widely used software services and applications . For instance, AI has brought a number of important applications, such as image- and speech-recognition and autonomous, vehicle navigation, to near-human levels of performance .\nThe new wave of AI has hit the software industry with the proliferation of AI-based systems integrating AI capabilities based on advances in ML and DL . AI-based systems are software systems which include AI components. These systems learn by analyzing their environment and taking actions, aiming at having an intelligent behaviour. As defined by the expert group on AI of the European Commission, “AI-based systems can be purely software-based, acting in the virtual world (e.g. voice assistants, image analysis software, search engines, speech and face recognition systems) or AI can be embedded in hardware devices (e.g. advanced robots, autonomous cars, drones or Internet of Things applications)”\\footnote{\\url{https://ec.europa.eu/digital-single-market/en/news/definition-artificial-intelligence-main-capabilities-and-scientific-disciplines}}.\nBuilding, operating, and maintaining AI-based systems is different from developing and maintaining traditional software systems. In AI-based systems, rules and system behaviour are inferred from training data, rather than written down as program code . AI-based systems require interdisciplinary collaborative teams of data scientists and software engineers . The quality attributes for which we need to design and analyze are different . The evolution of AI-based systems requires focusing on large and changing datasets, robust and evolutionary infrastructure, ethics and equity requirements engineering . Without acknowledging these differences, we may end up creating poor AI-based systems with technical debt .\nIn this context, there is a need to explore Software Engineering (SE) practices to develop, maintain and evolve AI-based systems. This paper aims to characterize SE practices for AI-based systems in the new wave of AI, i.e., \\textbf{Software Engineering for Artificial Intelligence (SE4AI)}. The motivation of this work is to synthesize the current SE knowledge pertinent to AI-based systems for: researchers to quickly understand the state of the art and learn which topics need more research; practitioners to learn about the approaches and challenges that SE entails when applied to AI-based systems; and educators to bridge the gap among SE and AI in their curricula.\nBearing this goal in mind, we have conducted a Systematic Mapping Study (SMS) considering literature from January 2010 to March 2020. The reason to focus on the last decade is that this new wave of AI started in 2010, with industrial applications of DL for large-scale speech  recognition, computer vision and machine translation .\nThe main contributions of this work are the synthesis of:\n\\begin{itemize}\n\\item Bibliometrics of the state of the art in SE4AI (see Section \\ref{sec:results-rq1}).\n\\item Characteristics of AI-based systems, namely scope, application domain, AI technology, and key quality attribute goals (see Section \\ref{sec:results-rq2})).\n\\item SE approaches for AI-based systems following the Knowledge Areas of SWEBOK, a guide to the\nSE Body of Knowledge  (see Section \\ref{sec:results-rq3})).\n\\item Challenges of SE approaches for AI-based systems following SWEBOK Knowledge Areas (see Section \\ref{sec:results-rq4})).\n\\end{itemize}", "cites": [8114, 8115], "cite_extract_rate": 0.2222222222222222, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a general introduction to AI-based systems and outlines the need for SE4AI, while referencing two key papers to support this argument. It synthesizes these works to some extent by highlighting the challenges in engineering AI systems and the need for structured approaches. However, the critical analysis is limited, as it does not deeply evaluate or contrast the cited papers. The abstraction level is moderate, as it identifies broader themes like interdisciplinary collaboration and quality attributes."}}
{"id": "b97ef787-6383-4a91-ada1-79e73c54bb3d", "title": "Related work on SE4AI", "level": "subsection", "subsections": [], "parent_id": "287cc2f2-8b3b-4b22-b4a2-c3dd3614008f", "prefix_titles": [["title", "Software Engineering for AI-Based Systems: A Survey"], ["section", "Background"], ["subsection", "Related work on SE4AI"]], "content": "Several secondary studies in the broad area of SE4AI have been published so far (see Table \\ref{tab:related-work}).\nMasuda et al.  conducted a review to identify techniques for the evaluation and improvement of the software quality of ML applications. They analyzed 101 papers and concluded that the field is still in an early state, especially for quality attributes other than functional correctness and safety.\nWashizaki et al.  conducted a multivocal review to identify architecture and design patterns for ML systems. From 35 resources (both white and grey literature), they extracted 33 unique patterns and mapped them to the different ML phases. They discovered that, for many phases, only very few or even no patterns have been conceptualized so far. Serban and Visser  performed both a case study and a systematic literature review on the topic of software architecture for machine learning. They reviewed 42 studies and performed 10 semi-structured interviews with practitioners from 10 different organisations. In their paper, the authors report 20 challenges and potential solutions.\nOn a similar topic, John et al. , performed a systematic review of both scientific (13 studies) and grey literature (6 studies) on the topic of deployment of ML systems. They report a total of 27 challenges and 52 practices.\nLorenzoni et al.  performed a systematic literature review on the topic of development of machine learning systems. They analysed 33 studies between 2010 and 2020 and classified 10 issues and 13 solutions into seven SE practices.\nA number of reviews have been conducted in the area of software testing. Borg et al.  performed a review of verification and validation techniques for deep neural networks (DNNs) in the automotive industry. From 64 papers, they extracted challenges and verified them with workshops and finally a questionnaire survey with 49 practitioners. They conclude, among other challenges, that a considerable gap exists between safety standards and nature of contemporary ML-based systems.\nAnother study was published by Ben Braiek and Khomh . In their review of testing practices for ML programs, they selected a total of 37 primary studies and extracted challenges, solutions, and gaps. The primary studies were assigned to the categories of detect errors in data (five papers), in ML models (19 papers), and in the training program (13 papers).\nRiccio et al.  extracted testing challenges from 70 primary studies and propose the following categories: realism of test input data (5 papers), adequacy of test criteria (12 papers), identification of behavioural boundaries (2 papers), scenario specification and design (3 papers), oracle (13 papers), faults and debugging (8 papers), regression testing (5 papers), online monitoring and validation (8 papers), cost of testing (10 papers), integration of ML models (2 papers), and data quality assessment (2 papers). \nSimilarly, Zhang et al.  surveyed the literature on ML testing and selected 138 papers. From these, they summarized the tested quality attributes (e.g. correctness or fairness), the tested components (e.g. the data or the learning program), workflow aspects (e.g. test generation or evaluation), and application contexts (e.g. autonomous driving or machine translation).\nIn addition to these specialized reviews, there are also some secondary studies more similar to ours, i.e. that have a general SE focus. Serban et al.  conducted a multivocal review with 21 relevant documents (both white and grey literature) to identify and analyze SE best practices for ML applications. They extracted 29 best practices and used a follow-up questionnaire survey with 313 software professionals to find out the degree of adoption and impact of these practices.\nFurthermore, Wang et al.  took a broader view and conducted a systematic literature review about general synergies between ML/DL and SE, i.e. covering both machine learning for SE (ML4SE) and SE for machine learning (SE4ML) research. However, only 15 of the 906 identified studies covered the SE4ML direction. Based on their results, the authors concluded that “it remains difficult to apply SE practices to develop ML/DL systems”.\nAnother systematic literature review was performed by Kumeno . He focused solely on the extraction of SE challenges for ML applications and mapped them to the different SWEBOK areas. In total, he selected 115 papers (47 papers focusing on SE-related challenges for ML and 68 papers focusing on ML-techniques or ML-applications challenges) from 2000 to 2019.\nMoreover, Lwakatare et al.  conducted a similar review of SE challenges faced by industrial practitioners in the context of large-scale ML systems. They categorized 23 challenges found in the 72 papers selected according to four quality attributes (adaptability, scalability, privacy, safety) and four ML process stages (data acquisition, training, evaluation, deployment). Adaptability and scalability were reported to face a significantly larger number of challenges than privacy and safety. They also identified 8 solutions, e.g. transfer learning and synthetically generated data, solving up to 13 of the challenges, especially adaptability. \nGiray  also conducted a systematic literature review to identify the state of the art and challenges in the area of ML systems engineering. In his sampling method, he exclusively targeted publications from SE venues and selected 141 studies. These studies were then analyzed for their bibliometrics, the used research methods, plus mentioned challenges and proposed solutions.\nLastly, Nascimento et al.  performed an SLR to analyze how SE practices have been applied to develop AI or ML systems, with special emphasis on limitations and open challenges. They also focus on system contexts, challenges, and SE contribution types. While they considered publications between 1990 and 2019, they only selected 55 papers.\nWe summarize in Table \\ref{tab:related-work} the findings of the related work. This summary has a three-fold objective: (i) to provide a synthetic view of the approaches aforementioned; (ii) to make evident our claim that no systematic mapping or general review with a breadth and depth similar to ours has been published so far; (iii) to facilitate the comparison of the results of our study with the related work.\nIn summary, even though several secondary studies have recently been published or submitted (a few of the aforementioned studies are pre-prints), there are still very few works that broadly summarize and classify research in the field of SE for AI-based systems. No systematic mapping or general review with a breadth and depth similar to ours has been published so far. Existing general reviews focus either exclusively on challenges, analyze considerably fewer studies, or only take publications with an industry context into account, i.e. they partially fail to describe the wide spectrum of results in this research area.\n\\begin{small}\n\\begin{longtable}{|p{0.65cm}|p{1.65cm}|p{3.25cm}|p{4.5cm}|p{3.5cm}|}\n    \\caption{Summary of relevant aspects on bibliometrics, AI-based system properties, SE approaches, and challenges found in related work.} \\label{tab:related-work} \\\\\n            \\hline\n            \\textbf{Study} & \\textbf{Bibliometrics} & \\textbf{AI-based systems properties} & \\textbf{SE approaches} & \\textbf{Challenges} \\\\\n            \\hline\n             & 101 studies (2005-2018) & safety, correctness &   practices for the evaluation and improvement of the software quality of ML applications & ML quality assurance\\\\ \n            \\hline\n             & 38 studies (2008-2019) & & 33 unique architecture and design patterns for ML systems & \\\\ \n            \\hline\n             & 64 studies (2002-2007, 2013-2016) & safety, robustness, reliability & verification and validation techniques for safety-critical automotive systems & verification and validation in DNNs\\\\ \n            \\hline\n             & 37 studies (2012-2018) & correctness & testing practices & 18 challenges organized in 6 dimensions: implementation issues, data issues, model issues, written code issues, execution environment issues, mathematical design issues\\\\  \n            \\hline\n             & 70 studies (2004-2019) & fairness, accuracy, safety, consistency & functional testing, test case generation and test oracle, integration testing, system testing & 11 challenges\\\\  \n            \\hline\n             & 138 studies (2007-2019) & correctness, model relevance, robustness, security, data privacy, efficiency, fairness, interpretability &  testing workflow, testing components, testing properties, application scenarios & 4 testing challenges categories: test input generation, test assessment criteria, oracle problem, testing cost reduction \\\\ \n            \\hline\n             & 21 studies (2017-2019) & & 29 best practices for ML systems in six categories: data, training, coding, deployment, team, governance & \\\\ \n            \\hline\n             & 15 studies (out of 906) (2009-2018) & & Model Evaluation, Deployment &\\\\\n            \\hline\n             & 115 studies (2003-2019) & safety, security, ethics and regulation, software structure, testability, maintainability, performance, risk and uncertainty, economic impact & all SWEBOK areas & SE challenges for ML applications\\\\ \n            \\hline\n             & 72 studies (1998-2018) & adaptability, scalability, safety, privacy & Software construction. Software maintenance & 23 SE challenges faced by practitioners in the context of large-scale ML systems. 13 of them had solutions\\\\ \n            \\hline\n             & 141 studies (2007–2019) & & requirements engineering, design, software development and tools, testing and quality, maintenance and configuration management, SE process and management, organizational aspects & 31 challenges and partially solutions that have been raised by SE researchers\\\\ \n            \\hline\n             & 55 studies (1999-2019) & AI ethics, interpretability, scalability, explanation on failure, complexity, efficiency, fairness, imperfection, privacy, safety, safety and stability, robustness, reusability, stability, staleness & \n            AI software quality, data management, project management, infrastructure, testing, model development, requirement engineering, AI engineering, architecture design, model deployment, integration, education, operation support. & SE challenges for ML applications and how SE practices adjusted to deal with them.\\\\ \n            \\hline\n             & 33 studies (2010-2020) & & data processing, documentation and versioning, non-functional requirements, design and implementation, evaluation, deployment and maintenance, software capability maturity model & 10 issues and 13 solutions\\\\ \n            \\hline  & 42 studies (2016-2021) & Performance, scalability, interpretability, hardware resources, interoperability, robustness, generalization, low data quality, scarcity of data, maintainability, privacy, security & Requirements, data, design, testing, operations, organisation & 20 challenges and potential solutions \\\\\n            \\hline  & 19 studies (2017-2020) & & Design, integration, deployment, operation, evolution. & 27 challenges and 52 practices \\\\\n            \\hline\n            This study & 248 studies (2010-2020) & 40 quality attributes (see sections \\ref{sec:results-rq1} and \\ref{sec:results-rq2}) & 11 SWEBOK areas (see Section \\ref{sec:results-rq3}) & 94 challenges (see Section \\ref{sec:results-rq4})\\\\\n            \\hline\n\\end{longtable}\n\\end{small}\nThe main objective of our study is therefore to systematically select and review the broad body of literature and to present a holistic overview of existing studies in SE for AI-based systems. An SMS is a proven and established method to construct such a holistic overview.", "cites": [1636, 6201, 6202, 6200, 6004], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.8, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes information from multiple related works to highlight trends, categorizations, and challenges in SE4AI. It critically evaluates the scope, limitations, and focus of prior studies, identifying gaps such as the lack of a comprehensive systematic mapping study. The abstraction is strong, as it generalizes findings across works to emphasize the breadth and depth of the research landscape."}}
{"id": "9b627684-da75-4ff3-a4ec-4fe1db3d80db", "title": "Research Methodology", "level": "section", "subsections": ["63b0b372-b637-41ff-b287-edcff5f23f96", "edd66983-423e-48f1-8fda-7b9b681b9057", "32504eac-11de-4f21-b25c-c854e7163ff7", "a7d9e9ac-14e0-4156-92e1-987b0e4b36c0", "22ec95d2-9c86-4afc-8203-7973778bedbe"], "parent_id": "eebe98c8-dc7c-4bc4-b995-ecf8692f4502", "prefix_titles": [["title", "Software Engineering for AI-Based Systems: A Survey"], ["section", "Research Methodology"]], "content": "\\label{sec:methodology}\nThis SMS has been developed following the guidelines for performing SMSs from Petersen et al. . Additionally, the guidelines for systematic literature reviews provided by Kitchenham and Charters  have also been used when complementary. This is because the method for searching for the primary studies, and taking a decision about their inclusion or exclusion is very similar between a systematic literature review and an SMS. This SMS has five main steps  described in the following subsections.", "cites": [6203], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section describes the research methodology by citing relevant guidelines but does not synthesize or connect multiple sources in depth. It lacks critical evaluation of the cited works and offers minimal abstraction beyond the specific methods mentioned. As a result, the insight is primarily descriptive rather than analytical or comparative."}}
{"id": "edd66983-423e-48f1-8fda-7b9b681b9057", "title": "Conduct search", "level": "subsection", "subsections": [], "parent_id": "9b627684-da75-4ff3-a4ec-4fe1db3d80db", "prefix_titles": [["title", "Software Engineering for AI-Based Systems: A Survey"], ["section", "Research Methodology"], ["subsection", "Conduct search"]], "content": "Since AI-based systems cover interdisciplinary communities (SE and AI), it is difficult to find an effective search string. Therefore, we decided to execute a hybrid search strategy , applying both a search string in Scopus\\footnote{\\url{https://www.scopus.com}} and a snowballing strategy . This strategy is referred to as \\textbf{“Scopus + BS*FS”} : it first runs a search over Scopus to get a start set of papers and compose a seed set for snowballing. The Scopus database contains peer-reviewed publications from top SE journals and conferences, including IEEE Xplore, ACM Digital library, ScienceDirect (Elsevier), and Springer research papers. We have recently assessed that Scopus’ coverage is optimal when compared to these other databases . Scopus enabled us to search with one engine for relevant studies considering a large library content and using different search engine functionalities such as export of the results and formulation of own search strings. We iteratively created several initial search strings, discussed their containing terms, and compared the results. The final search string used on Scopus was: \n\\begin{itemize}\n    \\item[] TITLE-ABS-KEY(\"software engineering for artificial intelligence\" OR\n    \\item[] \"software engineering for machine learning\" OR\n    \\item[] ((\"software engineering\" OR \"software requirement*\" OR \"requirements engineering\" OR \"software design\" OR \"software architecture\" OR \"software construction\" OR \"software testing\" OR \"software maintenance\" OR \"software configuration management\" OR \"software quality\")\n    \\item[] \\begin{center} AND \\end{center}\n    \\item[] (\"AI-enabled system*\" OR \"AI-based system*\" OR \"AI-infused system*\" OR \"AI software\" OR \"artificial intelligence-enabled system*\" OR \"artificial intelligence-based system*\" OR \"artificial intelligence-infused system*\" OR \"artificial intelligence software\" OR \"ML-enabled system*\" OR \"ML-based system*\" OR \"ML-infused system*\" OR \"ML software\" OR \"machine learning-enabled system*\" OR \"machine learning-based system*\" OR \"machine learning-infused system*\" OR \"machine learning software\"))) \n\\end{itemize}\nWith this search string, we aimed to capture the literature on SE4AI. Since this is a concept recently popular in the SE community, it was directly used as part of the search string. Furthermore, we also included subareas of AI, such as ML. Finally, the search string also considered the combination of SE Knowledge Areas from SWEBOK v3.0  and many similar terms to AI-based systems. We ran a check whether this search string included key papers on the topic of SE4AI from an annotated bibliography of an (external) expert on the topic \\footnote{\\url{https://github.com/ckaestne/seaibib}}, and many were included. Therefore, we considered it as a good starting point to be complemented with other techniques to mitigate selection bias.\\\\\n\\begin{figure}\n  \\centering\n    \\includegraphics[width=\\textwidth]{images/Figure_3_1-snowballing.pdf}\n    \\caption{Search process (selection on title and abstract appear in the snowballing file, new papers are in the data extraction).}\n    \\label{fig:Figure_3_1}\n\\end{figure}\nAn important decision was to only consider primary studies belonging to the new wave of AI from January 2010 to March 2020. The reason is that we wanted to structure the knowledge of SE4AI since the new wave of AI . We applied the search string on Scopus to this interval of time on April 2\\textsuperscript{nd}, 2020, which resulted in 116 studies (see Figure \\ref{fig:Figure_3_1}).\nThen, other papers were obtained from the seed set via backward and forward snowballing. We applied snowballing as indicated in the guidelines by Wohlin . The index used to check the number of references was Google Scholar. While we considered all the citations of each paper during backward snowballing, we established a limit of the first 100 citations returned by Google Scholar during forward snowballing. Each of the authors  checked these references and citations of randomly assigned papers. To avoid missing relevant papers, if there was any hint that a study could be included, it was inserted in our snowballing working document.\nIn total, we performed two snowballing iterations as reported in this study. For each entry in the aforementioned snowballing working document, at least two researchers applied inclusion and exclusion criteria. The next subsection explains this selection process of screening papers.", "cites": [6204], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a reasonable synthesis by integrating the search strategy with the broader SE4AI context, referencing SWEBOK and an external annotated bibliography. It demonstrates some abstraction by framing the search in terms of SE knowledge areas and AI system types. However, the critical analysis is limited—while it acknowledges selection bias and the need for mitigation, it does not deeply evaluate the limitations of the cited works or the hybrid search strategy itself."}}
{"id": "22ec95d2-9c86-4afc-8203-7973778bedbe", "title": "Data extraction and mapping process", "level": "subsection", "subsections": [], "parent_id": "9b627684-da75-4ff3-a4ec-4fe1db3d80db", "prefix_titles": [["title", "Software Engineering for AI-Based Systems: A Survey"], ["section", "Research Methodology"], ["subsection", "Data extraction and mapping process"]], "content": "\\label{sec:methodology-analysis}\nEach of the 248 primary studies was assigned to a single researcher for extraction based on the predefined data extraction form. Extractors were in frequent asynchronous contact to discuss potential inconsistencies. The weekly project meeting was also used for synchronization on this matter. In these meetings, we discussed the most persistent conflicts and shared our way of working as well as emerging findings to ensure cohesion of the team and to minimize subjectivity. Additionally, the data collection form includes the name of the reviewer and space for additional notes. This enabled us also to keep track of this process. \nAfter completing the extraction, we started data analysis and synthesis. In general, we performed both quantitative and qualitative analyses to classify the extracted data. During this process, additional extraction inconsistencies and mistakes were discovered and easily resolved in direct communication with the original extractors. Synthesis per RQ was performed by groups of at least two researchers, who often re-read (parts of) the original papers for this and kept the group in the loop. Final results were presented to the rest of the team and feedback was incorporated. \nRegarding the required mapping and analysis to answer the RQs, we performed the following activities in each RQ: \n\\textbf{For the analysis of RQ1}, we used the assessed rigor and relevance quality  and performed frequency analysis to additionally determine bibliographical data such as the annual publication trend, venue types, authors’ affiliations, geography distribution, and the empirical research type of the primary studies.\n\\textbf{To answer RQ2}, we counted the number of occurrences of various terms related to AI used to characterize the study objects. We then used inductive coding to distill dimensions (Scope, Application Domain, and Technologies of AI) to describe the study objects, coded all primary studies, and reported frequencies. For the key quality attribute goals of AI-based systems, this also included a harmonization of the used terms as well as axial coding to cluster the identified quality properties.\n\\textbf{As RQ3 and RQ4 highly rely on extensive qualitative analysis}, all eight authors supported the analysis. Therefore, we split the primary studies according to the extracted SWEBOK Knowledge Areas and equally distributed the number of studies to groups of two or three researchers. Again, we performed thematic analysis  and derived several codes independently for the different Knowledge Areas (RQ3) and uniformed them to our subcategories for each of the SWEBOK Knowledge Areas. Finally, we mapped the primary studies accordingly, whereby one primary study could belong to several Knowledge Areas. For each Knowledge Area, each primary study could also be mapped to several subcategories, but required at least one.\nAs we created a subcategory called “challenges” for each Knowledge Area, we restricted the analysis for RQ4 regarding challenges in SE4AI explicitly to those primary studies whose classification under RQ3 included a mapping to this subcategory. For the mapping of the challenges, we made use of the different Topics provided for each Knowledge Area by SWEBOK. We mapped a challenge into one or more SWEBOK Topics if the link was evident. Moreover, we added information about root causes, mitigation actions, and impact on the data extraction form as a basis for some additional qualitative analysis. Finally, we uniformed the terminology (near 80\\% of the Topics were rewritten to some extent) and the classification (changes of SWEBOK Topic or even Knowledge Area, assignment of an additional Topic to a challenge, or removal of a Topic). It also resulted in removing some challenges because they are duplicated in another primary study with proper citation (e.g.,  presenting the same four challenges as ) or are too generic to allow proper analysis.\nThe three researchers responsible for RQ4 decided to apply inductive coding and added a new Knowledge Area called \\textit{Software Runtime Behaviour} that they considered not well-covered in SWEBOK. Our new Knowledge Area was finally composed of three Topics: \\textit{Cost \\& Efficiency}, \\textit{Quality}, and \\textit{Robustness \\& Operability}. Furthermore, seeking conceptual consistency inside some SWEBOK Topics, we proposed the following new Topics:\n\\begin{itemize}\n    \\item \\textit{ML/AI methods}, in SE Models and Methods Knowledge Area. This new Topic could be numbered as 9.4.5, i.e., at the same level as existing particular types of methods, e.g., Formal Methods (9.4.2) or Agile Methods (9.4.4).\n    \\item \\textit{ML/AI specific activities}, in Software Life Cycles Topic (8.4.2). The reason is that while these activities are part of ML/AI-based software development, the current formulation of the Topic does not explicitly address the definition of concrete activities in the life cycle.\n    \\item \\textit{Data-related issues} and \\textit{Process-related issues}, as sub-Topics in Software Testing: Key Issues (4.1.2). We considered it convenient to clearly distinguish among these two types of issues related to testing due to their different nature.\n    \\item \\textit{Applicability and adoption of research results in practice}, as sub-Topic of SE Professional Practice. We thought that this important aspect is not covered by any of the three main Topics in the Knowledge Area (Professionalism, Group Dynamics and Psychology, and Communication Skills). Therefore, this proposed Topic could be numbered as 11.4.\n\\end{itemize}\nIn the replication package\\footnote{Replication package available on \\url{https://doi.org/10.6084/m9.figshare.14538324}.}, the reader can see: the results of the search on Scopus for seed papers (and alternative discarded search procedures), the result of the application of inclusion and exclusion criteria for the papers found, the document used for snowballing, the data extraction form, and analysis files for each RQ.", "cites": [1356], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates analytical insight by describing a structured data extraction and mapping process tailored to the RQs, integrating information from the studies and organizing it into novel classifications under SWEBOK Knowledge Areas. It generalizes patterns and introduces new Topics and Knowledge Areas to better capture SE for AI-based systems, showing abstraction. While it includes some critique (e.g., identifying duplicated challenges and overly generic ones), the critical evaluation remains moderate rather than deeply evaluative."}}
{"id": "2efaba44-0250-4e18-a940-255ef67c0783", "title": "What is an AI-based system?", "level": "subsection", "subsections": [], "parent_id": "9f46b8ad-5471-4a23-a9ed-41ea94ba1c3f", "prefix_titles": [["title", "Software Engineering for AI-Based Systems: A Survey"], ["section", "RQ2: What are the characteristics of AI-based systems?"], ["subsection", "What is an AI-based system?"]], "content": "We found a large variety of terms used in the primary studies. In Table~\\ref{tab:Tab_5_1}, there is an overview of all terms used more than twice to discuss the type of system investigated in the corresponding primary study. We observed a mix of very general terms (such as \"machine learning\" or \"AI technologies\"), specific AI technologies (such as \"deep neural networks\" or \"ML libraries\") and AI application domains (such as \"robotics system\" or \"automotive system\").\nWe furthermore noticed that AI seems to be always used in terms of learning components and not including rule-based expert systems. This corresponds to the new wave of AI associated with learning from data.\n\\begin{table}\n\\caption{Terms used in the primary studies to refer to AI-based systems with \"intelligent\" components.}\n\\begin{tabularx}{\\textwidth}{|l|X|}\n\\hline\n\\textbf{Count} & \\textbf{Terms (comma-separated)} \\\\ \\hline\n43 & Machine learning \\\\ \\hline\n40 & ML system \\\\ \\hline\n28 & Deep neural networks \\\\ \\hline\n27 & ML algorithms \\\\ \\hline\n23 & ML models \\\\ \\hline\n21 & Autonomous vehicle \\\\ \\hline\n19 & Autonomous systems, ML components \\\\ \\hline\n18 & AI systems \\\\ \\hline\n17 & AI, Neural network \\\\ \\hline\n16 & Deep learning systems, ML application \\\\ \\hline\n15 & ML techniques \\\\ \\hline\n14 & Autonomous driving system \\\\ \\hline\n11 & Cyber-physical systems with Machine Learning components (CPSML) \\\\ \\hline\n10 & ML software, ML-based system \\\\ \\hline\n9 & AI-based system \\\\ \\hline\n8 & DNN-based software \\\\ \\hline\n7 & Deep learning, Reinforcement Learning (RL) \\\\ \\hline\n6 & AI software, Machine learning classifiers, ML program \\\\ \\hline\n5 & Artificial Neural Networks, Classifier, Intelligent systems \\\\ \\hline\n4 & AI components, AI model, DNN model, ML methods, ML pipeline \\\\ \\hline\n3 & AI applications \\\\ \\hline\n\\end{tabularx}\n\\label{tab:Tab_5_1}\n\\end{table}\nIn the further inductive coding of the terms and study objects, we distilled three dimensions that can be used to classify the contributions of primary studies about AI-based systems. \nThe first dimension is the \\textbf{Scope} of the system under analysis. In particular, the scope refers to the question: \\textit{How is AI implemented inside the system?} AI can either be one component in a system (\"component\"), dominating the entire system (\"system\"), implement one or more particular algorithms (\"algorithm\", such as DNN), or providing a pipeline or infrastructure (\"infrastructure\", e.g., PyTorch).\nThe second dimension is the \\textbf{Application Domain}. Many studies do not concern themselves with what the AI will be used for exactly but study, for example, DNNs in general. There are, however, also several studies focusing on concrete applications of AI. The most frequent of those is the domain of autonomous vehicles that encompasses a part of the autonomous systems and the autonomous/automated driving systems from Table~\\ref{tab:Tab_5_1}. An application domain can also be more generic, such as ML frameworks.\nThe third dimension is the \\textbf{Technologies of AI} under consideration. AI technologies can be, for example, ML in general or DL methods. This is usually stated in some way, and we also think it is important because it can make a huge difference in how generalizable the results of a primary study are. For instance, test approaches for AI components that use random forests might not be useful for AI components built on DNNs.\nWe then used this structure to code all our primary studies (not only those that provided definitions) to get insights into what exactly they investigated. We found that almost half of the primary studies look at SE4AI at the system level (see Figure \\ref{fig:Figure_5_2}). This means that they investigate complete systems such as autonomous cars regarding their AI aspects. A quarter of the primary studies focuses on the AI components directly. An example would be the image recognition component in an autonomous car. The remaining primary studies either investigate or propose methods for specific algorithms (such as DL) or for AI infrastructure (such as TensorFlow) without considering specific applications.\n\\begin{figure}\n  \\centering\n    \\includegraphics[width=0.7\\textwidth]{images/Figure_5_2.png}\n    \\caption{Scope of the research in the primary studies.} \\label{fig:Figure_5_2}\n\\end{figure}\nIn Figure \\ref{fig:Figure_5_3}, we show the number of publications in different domains structured by the AI technology investigated. The only dominant application domain is automotive with its hype on autonomous cars. More than a quarter of the primary studies aim at this domain. Example systems are the pedestrian detection system in autonomous driving at Bosch  or an automated emergency braking system at IEE S.A. (Luxembourg) . Almost half of the primary studies look at AI in a generic way without considering any application domain. The further many domains that we found only make up between 0.8\\% and 3.3\\% of the primary studies. Some domains also overlap, as there are embedded systems in automotive or aviation. Examples for these further domains include WeChat's NMT system for automatic machine translations , the pin recommender system at Pinterest  or the CognIA chatbot for financial advice from IBM .\n\\begin{figure}\n  \\centering\n    \\includegraphics[width=\\textwidth]{images/Figure_5_3.png}\n    \\caption{Domain and AI technology investigated in the primary studies.} \\label{fig:Figure_5_3}\n\\end{figure}", "cites": [6054], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes a diverse set of terms from multiple primary studies to define and classify AI-based systems, integrating them into a coherent structure. It introduces a novel three-dimensional classification framework (Scope, Application Domain, Technologies of AI), which reflects abstraction and insight. However, critical analysis is limited, as the section primarily categorizes and reports findings without deeply evaluating limitations or contrasting approaches."}}
{"id": "62f48d17-4a2d-410b-baa3-543b13abb50e", "title": "Discussion", "level": "subsection", "subsections": [], "parent_id": "9f46b8ad-5471-4a23-a9ed-41ea94ba1c3f", "prefix_titles": [["title", "Software Engineering for AI-Based Systems: A Survey"], ["section", "RQ2: What are the characteristics of AI-based systems?"], ["subsection", "Discussion"]], "content": "\\textbf{Observation 2.1: AI is commonly associated with DL and considered as part of a complex software system.} We propose that any paper that fits into our inclusion criteria discussing the application of SE to AI-based systems should make explicit what exactly they consider on our identified dimensions. This would be a first step in making the scope and limitations of primary studies clearer. \n\\textbf{Observation 2.2: Many various synonymous terms are used to denote a system or component that uses some kind of AI or ML.} Commonly used synonyms used to refer to software systems, which use AI technologies include AI-based system, AI-enabled system, AI-infused system, AI software, ML solution or DL system. We propose to use the following definitions to guide the selection of terms for future studies:\n\\begin{itemize}\n  \\item \\textbf{AI component:} A part of a system. The component uses AI to some extent. Examples range from a component whose behaviour  depends to some extent on some embedded AI code, or an AI library as a special AI component that provides a concrete implementation of AI algorithms.\n  \\item \\textbf{AI-based system:} A system consisting of various software and potentially other components, out of which at least one is an AI component.\n\\end{itemize}\n\\textbf{Observation 2.3: There exists diversity in the terminology to refer to AI-based systems, making unclear what is the object of the research.} We propose that SE4AI papers should use a taxonomy that makes clear what kind of AI-based system is investigated and how general the methods and approaches are supposed to be. We use as an example the paper by Burton et al. . They use the term ``machine learning'' in the title, but then focus on Convolutional Neural Networks. We depicted a corresponding taxonomy in Figure \\ref{fig:Figure_5_6}. On the top level, we keep as close as possible to established discussions of the terms. For that, ML is commonly seen as a part of AI (). Hence, an ML-based system is also an AI-based system. Using the DARPA terminology (\\url{https://www.darpa.mil/news-events/2018-07-20a}), in the first wave of AI, there were mostly rule-based systems. The second wave added statistical learning. We see only papers about AI in at least the second wave sense in our primary studies. Similarly, on the next level, in ML most primary studies investigate neural networks, mostly DL. So, also for Burton et al., we would go further and also refine it to the next level ``Convolutional Neural Network'' as a specific type of DL. Using such a taxonomic classification and explicitly mapping contributions to their respective levels would make the article clearer. We suggest that such a taxonomic classification would be useful for all SE4AI papers. \n\\begin{figure}\n\\begin{tikzpicture}\n[sibling distance=4cm,-,thick]\n\\footnotesize\n\\node {\\textbf{Artificial Intelligence (AI)}}\n  child {node {\\textbf{Machine Learning (ML)}}\n    [sibling distance=4cm]\n    child {node {\\textbf{Neural Networks}}\n      child {node {\\textbf{Deep Learning (DL)}}\n        [sibling distance=4cm]\n        child {node{\\textbf{Convolutional Neural Network (CNN)}}}\n        child {node {\\ldots}}\n        }\n      child {node {\\ldots}}\n    }\n    child {node {Support Vector Machines}}\n    child {node {\\ldots}}\n  }\n  child {node {Rule-Based AI}\n  };\n\\end{tikzpicture}\n    \\caption{Example taxonomic classification for paper . \\label{fig:Figure_5_6}}\n\\end{figure}\n\\textbf{Observation 2.4: Most of the terms used are not defined explicitly.} Most commonly defined terms include \"deep learning system\" (5), \"ML system\" (5), \"AI\" (4), and \"machine learning\" (4). Besides using a taxonomy, we propose explicitly defining the terms used.\n\\textbf{Observation 2.5: Most primary studies focus on software systems.} In the analyzed studies, AI is not just one of its many components of a software system, but typically constitutes its dominating part.\n\\textbf{Observation 2.6: The most studied properties of AI-based systems are dependability and safety.} Overall, we identified a strong focus on qualities related to safety (especially in the context of smart cyber-physical systems like autonomous vehicles) and correctness \\& accuracy. However, there are research gaps for less studied properties, such as usability, portability or particularly important in the context of trustworthiness and the understandability aspect. More importantly, inherent and critical quality characteristics in AI-based systems such as explainability and transparency require the attention of researchers to assure the high maturity level required by industry.\n\\textbf{Observation 2.7: The use of ML and DL has only been extensively used in AI-based systems of the automotive domain, and to a much lesser extent in healthcare and e-commerce.} We believe that the increasing and successful application of ML and DL (e.g., image-, language- classification, and object recognition) in the automotive domain shall inspire researchers and practitioners to explore and apply it in other domains. Based on the primary studies, many domains have received little or no attention (see Figure \\ref{fig:Figure_5_5}).", "cites": [6205], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes terminology and classification approaches from the literature, integrating the conceptual understanding of AI and ML into a coherent framework. It critically highlights issues such as inconsistent terminology, lack of definitions, and domain-specific focus, while proposing ways to improve clarity and generalizability in SE4AI research. The abstraction is strong, as it identifies broader trends and suggests a taxonomy and definitions that could guide future research."}}
{"id": "c55e026a-2a86-45c7-856c-13c387f7677d", "title": "Software requirements (17 studies)", "level": "subsection", "subsections": [], "parent_id": "d73b39bb-961a-4dd6-a9c8-370ab98720b6", "prefix_titles": [["title", "Software Engineering for AI-Based Systems: A Survey"], ["section", "RQ3: Which SE approaches for AI-based systems have been reported in the scientific literature?"], ["subsection", "Software requirements (17 studies)"]], "content": "For \\textit{software requirements}, we identified 17 studies. Based on our thematic analysis, we derived five subcategories, with most papers belonging to several categories. \nNine papers address the Requirements Engineering (RE) \\textit{process} for AI-based systems. Vogelsang and Borg are the only ones to cover the complete process, from elicitation to verification and validation . They summarized important characteristics of the RE process for ML systems, such as detecting data anomalies or algorithmic discrimination. The rest of the primary studies concentrate on one particular RE activity, with \\textit{specification} being the most popular one (six papers). Four of these papers focus on non-functional requirements (NFRs, see below for details). Further examples include methodological aids such as the notion of supplier’s declaration of conformity  or conceptual frameworks to improve the specification of requirements for explainable  or safe  AI-based systems. The two general papers on specification focus on formal aspects related to ambiguity  and the need to consider partial specifications for AI-based systems . Two other papers addressed requirements-driven \\textit{derivation}, in both cases in automotive systems, but with different aims: while Burton et al. derive low-level requirements from safety goals , Tuncali et al. derive test cases from safety and performance requirements . The last paper focuses on the \\textit{elicitation} of safety requirements in automotive systems using a risk-based approach .\nFour papers in the RE process category also propose a particular \\textit{notation} to express requirements. The aforementioned two papers on derivation defined a concrete notation to make the derivation process less ambiguous, namely by using goals in the case of Burton et al.  and temporal logic in Tuncali et al.’s approach . Adedjouma et al. also employ goals to represent safety risks and hazards . The fourth paper expresses requirements as linear arithmetic constraints over real-valued variables to support satisfiability modulo theories (SMT) . \nEight papers did not provide concrete RE approaches, but were more \\textit{conceptual} in nature: the authors tried to provide a foundation for AI software RE research by disseminating current \\textit{practices} and/or \\textit{challenges}, e.g. via the use of interviews  or surveys . Many of these papers analyzed how RE for AI changed in comparison to traditional systems, and especially what issues currently prevent effective practices . In addition to that, Otero and Peter also tried to provide research directions to address some of these challenges .\nFinally, three very diverse papers discuss \\textit{RE foundations} for AI-based systems. As mentioned above, Salay and Czarnecki introduced foundations for partial specifications as appropriate for specifying AI-based systems . Meanwhile, Otero and Peter propose a model of requirements for Big Data analytics software . Lastly, Arnold et al. incorporate traceability into their requirements specification approach .\nAs an additional facet to the above subcategories, 10 of the 17 studies focused specifically on \\textit{NFRs}. While both Horkoff  and Kuwajima et al.  targeted NFRs in general, the other papers were concerned with one or a few specific NFRs, e.g. safety  or model performance . Arnold et al. also included security requirements in their FactSheets approach , whereas Sheh and Monteath were the only ones to study the nature of requirements for explainable AI .\n\\begin{tcolorbox}\n    Key takeaways for software requirements:\n    \\begin{itemize}\n        \\item A lot of focus on NFRs for AI (10 of 17 studies), especially new AI-specific quality attributes\n        \\item Several specification and notation approaches to deal with probabilistic results or ambiguity, e.g. partial specification\n        \\item Very few holistic views on the RE process for AI (only one study), focus on support for RE specification and derivation\n    \\end{itemize}\n\\end{tcolorbox}", "cites": [6208, 6207, 8116, 8117, 6206, 8118], "cite_extract_rate": 0.375, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.8}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by grouping the 17 studies into coherent subcategories (e.g., process, specification, derivation, notation, conceptual) and highlighting overlaps and trends. It also provides some critical analysis by noting the lack of holistic RE processes for AI and the dominance of specification-focused approaches. Abstraction is evident in identifying broader patterns such as the emphasis on NFRs and the emergence of new quality attributes for AI systems."}}
{"id": "ce08e116-d77d-4b44-b7c7-adafffea1230", "title": "Software design (34 studies)", "level": "subsection", "subsections": [], "parent_id": "d73b39bb-961a-4dd6-a9c8-370ab98720b6", "prefix_titles": [["title", "Software Engineering for AI-Based Systems: A Survey"], ["section", "RQ3: Which SE approaches for AI-based systems have been reported in the scientific literature?"], ["subsection", "Software design (34 studies)"]], "content": "In the area of \\textit{software design}, we formed seven categories to group the 34 studies. One of the largest of these -- \\textit{design for X} -- is concerned with design approaches or techniques to improve or assure one specific quality attribute in AI-based systems. It comprises 11 papers, of which the majority is concerned with \\textit{safety} (seven papers). Most of these studies propose design strategies for AI systems in safety-critical domains where unsafe behaviour can have immense negative consequences, like the use of architectural components as a protection layer for safety in autonomous vehicles , design best practices for safety verification methods  or behavior-bounded assurance  for autonomous aerial vehicles, safety design strategies (e.g., inherently safe design, safety reserves, safe fail, and procedural safeguards) for ML components , and specific safety strategies for cyber-physical systems . The remaining four design approaches are for \\textit{reliability} , \\textit{user experience} , and for quality attributes related to ethical AI like \\textit{fairness} .\nAnother large category is \\textit{frameworks or platforms} (11 studies), ranging from a generic level of abstraction for end-to-end ML application development and deployment  to available tool support for managing reliable ML applications , packaging and sharing ML models as reusable microservices , and the development and deployment of ML applications . Several platforms follow model-driven engineering principles , for instance a model representation for production ML models . These types of platforms are relevant for the cyber-physical systems domain, for which we also found dedicated tool support .\nWith six papers, the third-largest category is formed by studies that elicited and described \\textit{design challenges} for AI-based systems, e.g. by conducting empirical studies like surveys  or reporting industry experiences . The scope of the described design challenges varies greatly and covers areas such as intelligent automotive systems , ML model management , or ML fairness . The details of these challenges are explained in Section 7 (RQ4).\nWe also found four papers on \\textit{design patterns} for AI-based systems: an architecture pattern to improve the operational stability of ML systems , patterns to improve the safety of systems with ML components , an architecture pattern to manage N-version ML models in safety critical systems , and finally more abstract solution patterns to address recurrent business analytics problems with ML .\nFinally, we found two studies about technical debt and the design stage, for which we created the category \\textit{design debt} , and resources for teaching software design for AI-based systems .\n\\begin{tcolorbox}\n    Key takeaways for software design:\n    \\begin{itemize}\n        \\item Many design strategies to cope with specific quality attributes, e.g. with safety or reliability\n        \\item Several concrete AI infrastructure proposals, e.g. for sharing models as microservices\n        \\item However, at the system level, there are few proposals for patterns, design standards, or reference architectures\n    \\end{itemize}\n\\end{tcolorbox}", "cites": [6208, 8119, 8123, 6209, 1356, 8115, 8987, 8121, 8120, 1608, 8122, 8116, 8118], "cite_extract_rate": 0.3939393939393939, "origin_cites_number": 33, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes a range of studies into seven coherent categories, connecting ideas on design for safety, reliability, and fairness across multiple sources. It also highlights broader trends, such as the prevalence of design challenges in industry and the lack of system-level design standards. While it identifies some limitations, such as the paucity of design patterns or reference architectures, it does not deeply critique or compare the approaches, keeping the analysis at a moderate level."}}
{"id": "6c1a2d3a-8312-4b23-90a4-82852af61c26", "title": "Software construction (23 studies)", "level": "subsection", "subsections": [], "parent_id": "d73b39bb-961a-4dd6-a9c8-370ab98720b6", "prefix_titles": [["title", "Software Engineering for AI-Based Systems: A Survey"], ["section", "RQ3: Which SE approaches for AI-based systems have been reported in the scientific literature?"], ["subsection", "Software construction (23 studies)"]], "content": "A total of 23 studies were concerned with \\textit{software construction}. Many of these studies either provided specialized \\textit{tools} (six papers) or more holistic \\textit{platforms} (four papers) to support the development of AI systems. Exemplary application contexts for tools were the deployment and serving of general prediction systems~, lowering the barrier for using ML techniques~, or model-based development toolchains~. For platforms, the common goal was to provide a more comprehensive infrastructure to improve and accelerate the AI development workflow~, sometimes in more specialized domains like safety-critical robotics~.\nFurthermore, eight studies reported construction \\textit{practices and guidelines} for AI systems based on diverse experiences, such as implementing ML components to detect and correct transaction errors in SAP~, a systematic comparison of DL frameworks and platforms~, experiences from improving Airbnb search results with DL~, experiences from 150 ML applications at Booking.com~, AI model criteria relevant for end users~, automatic version control in notebooks~, or practices collected via practitioner surveys and/or interviews~. Similarly, seven studies reported current \\textit{challenges} in constructing AI-based systems, most of them focusing on DL. Challenges have been derived via StackOverflow questions~, surveys~, theoretical analyses of the AI development process~, or case studies~.\nLastly, one paper was concerned with the software construction of AI systems in a \\textit{teaching} and education context~: Kästner and Kang describe their \\enquote{SE for AI-enabled systems} course material and infrastructure and share lessons learned from educating Master students in this field.\n\\begin{tcolorbox}\n    Key takeaways for software construction:\n    \\begin{itemize}\n        \\item Many state of practice studies synthesized construction challenges and guidelines to address them.\n        \\item Several tools and platforms have been proposed to improve AI development activities, but their maturity, rationales for their selection, and level of adoption remain vague.\n    \\end{itemize}\n\\end{tcolorbox}", "cites": [8124, 8115, 8121, 8120, 8118, 6210, 8125], "cite_extract_rate": 0.30434782608695654, "origin_cites_number": 23, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple cited papers by grouping them into categories (tools, platforms, practices, challenges, and education), showing a coherent structure. It provides some critical analysis by highlighting the vague maturity and adoption status of tools and platforms. The abstraction level is moderate, as it identifies general trends in practices and challenges but does not formulate overarching principles or frameworks."}}
{"id": "61d2652a-c78b-4ff5-b483-230e90ab94e0", "title": "Software testing (115 studies)", "level": "subsection", "subsections": [], "parent_id": "d73b39bb-961a-4dd6-a9c8-370ab98720b6", "prefix_titles": [["title", "Software Engineering for AI-Based Systems: A Survey"], ["section", "RQ3: Which SE approaches for AI-based systems have been reported in the scientific literature?"], ["subsection", "Software testing (115 studies)"]], "content": "We identified 115 studies focused on \\textit{software testing}. During the analysis, we formed 11 subcategories (\\textit{bugs \\& debugging}, \\textit{challenges}, \\textit{explanations}, \\textit{quality testing}, \\textit{test architecture \\& languages}, \\textit{test case}, \\textit{test case generation}, \\textit{test case selection}, \\textit{testing methods}, \\textit{testing metrics}, and \\textit{generic}), which we partially refined into sub-themes. For each of the 115 studies, we assigned one or more of these subcategories. In three cases, we assigned five different subcategories to a paper. The different subcategories are described below:\nFour papers addressed \\textit{bugs \\& debugging}. Three of these papers conducted empirical studies examining bugs in ML projects~, whereas one paper proposed a specialized debugger for ML models~.\nA total of 12 papers studied the \\textit{challenges} in software testing for AI. Nine of them discussed the challenges, issues, and needs in AI software testing based on the current state of the art, either for generic AI systems~ or focusing on the particular challenges for autonomous vehicles or other safety-critical systems~. Finally, three proposals identified the challenges for generic AI or ML systems through empirical methods like questionnaire surveys with practitioners~.\nTwo papers addressed testing-related \\textit{explanations} for ML systems~. Due to the  difficulty to understand the results of ML systems in some scenarios, these papers provided a method for explaining how the ML system reached a particular result, including failures or how the tester addressed them to correct the ML system.\nIn 35 papers, the focus of the presented testing approach was aimed at improving very specific quality characteristics of the AI-based system under test (subcategory \\textit{quality testing}), including safety (e.g.~), robustness (e.g.~), security (e.g.~), fairness~ or others (e.g.~. Safety was indeed the most addressed quality characteristic with 21 proposals, followed by robustness and security with seven and four papers respectively. \nIn the subcategory \\textit{test architecture \\& languages}, we identified one paper proposing a new testing architecture~, and one paper proposing a specific testing language~.\n17 papers were assigned to the subcategory \\textit{test case}. The type of test cases that these studies addressed were adversarial test cases in 14 occasions (e.g.~) and corner test cases in 3 occasions~. As opposed to the related categories about test case generation and selection, papers in this general category were very focused on conceptualizing different types of test cases.\nRegarding \\textit{test case generation}, 40 papers provided automatic means for the generation of test cases. Most of them augment existing test cases, deriving new tests from an original dataset (e.g.~). Some of these proposals generate these test cases randomly (e.g.~), but others focus on attaining specific objectives when generating test cases, like generating corner case testing inputs (e.g.~), adversarial testing inputs (e.g.~) or increase the coverage of the test suites (e.g.~). Other approaches generate test cases with discriminatory inputs to uncover fairness violations~,  or with specific inputs to uncover disagreements between variants of an AI/ML model~. Approaches like~ generate test suites avoiding too similar test cases to minimize the number of tests to execute. It is worth mentioning that some proposals are based on simulation-based test generation, for instance, to generate tests for autonomous vehicles in simulated environments (e.g.~).\nA total of 12 proposals were categorized with \\textit{test case selection}. Some approaches proposed test case selection techniques as a complementary activity to the test case generation (e.g.~). One approach proposed a technique to select test cases based on a metric of importance~, whereas others proposed techniques to identify corner cases~, adversarial examples~ or likely failure scenarios~. Finally, a few approaches proposed techniques for test input prioritization to select the most important ones and reduce the cost of labeling~ or reduce the performance cost of training and testing huge amounts of data~.\nA total of 36 papers addressed \\textit{testing methods} for AI systems, following different techniques such as combinatorial testing~, concolic testing~, fuzzing (e.g.~, metamorphic testing (e.g.~, or others (e.g.~. From the different methods used, it is interesting to point out that the most popular one is metamorphic testing with 16 studies, followed by fuzzing and mutation testing with six and five studies, respectively. \nMoreover, 20 papers focused on the definition and/or exploration of \\textit{testing metrics} to measure the testing quality. 14 out of 20 focused on test coverage metrics (e.g.~, whereas the rest of metrics were reported only by one study each: diversity~, importance~, suspiciousness~, probability of sufficiency~, and disagreement~.\nFinally, 14 papers were categorized as \\textit{generic}, as they did not address or contribute to a specific testing theme.\n\\begin{tcolorbox}\n    Key takeaways for software testing:\n    \\begin{itemize}\n        \\item The main focus in software testing for AI is test cases (55 unique studies), including the two specialized areas test case generation (40) and test case selection (12).\n        \\item The majority of papers related to testing methods use metamorphic testing (16 out of 36), followed by fuzzing (6) and mutation testing (5).\n        \\item The majority of papers related to testing metrics propose novel coverage criteria (14 out of 20).\n    \\end{itemize}\n\\end{tcolorbox}", "cites": [6208, 6213, 8119, 6014, 6034, 6212, 3500, 8130, 6021, 8971, 6036, 8132, 8133, 7307, 8131, 8128, 8129, 6048, 6042, 6211, 8089, 8126, 8127, 1637, 1609, 8118], "cite_extract_rate": 0.36619718309859156, "origin_cites_number": 71, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.3, "critical": 2.8, "abstraction": 3.7}, "insight_level": "medium", "analysis": "The section effectively synthesizes a large number of studies into 11 structured subcategories, showing thematic organization and partial refinement into sub-themes. It identifies patterns (e.g., dominance of adversarial test cases, popularity of metamorphic testing) and highlights key trends in software testing for AI-based systems. However, while it presents factual analysis and some high-level observations, it lacks deeper critical evaluation of limitations or comparative assessments across methods."}}
{"id": "eafcee39-d36d-4168-84dd-ce04c000a107", "title": "Software maintenance (6 studies)", "level": "subsection", "subsections": [], "parent_id": "d73b39bb-961a-4dd6-a9c8-370ab98720b6", "prefix_titles": [["title", "Software Engineering for AI-Based Systems: A Survey"], ["section", "RQ3: Which SE approaches for AI-based systems have been reported in the scientific literature?"], ["subsection", "Software maintenance (6 studies)"]], "content": "The small \\textit{software maintenance} area only comprises six studies, which we group further into three categories. Two studies empirically analyze the nature and prediction impact of \\textit{bugs} in AI software~. Similarly, two studies are concerned with providing specialized approaches or tool support for the \\textit{debugging} of ML software by focusing on explanatory debugging in interactive ML~, and proposing their Tensorflow debugger based on dataflow graphs~. The remaining two papers elicited and reported maintenance \\textit{challenges}, namely~ as an Amazon experience report in the area of ML model management, and~ via a questionnaire survey with DL practitioners.\n\\begin{tcolorbox}\n    Key takeaways for software maintenance:\n    \\begin{itemize}\n        \\item Hardly any studies on the topic. More research is needed, as there are a few open challenges (see next section)\n        \\item In addition to state of practice analyses, the focus was on bugs in and debugging of AI-based systems.\n    \\end{itemize}\n\\end{tcolorbox}", "cites": [8118], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive summary of six studies grouped into three categories but does not deeply synthesize or integrate their findings. It lacks critical evaluation of the cited works and only briefly notes the low number of studies and the focus areas without generalizing to broader principles or trends."}}
{"id": "49cb2e1d-4afc-4494-b212-54661041ae52", "title": "Software engineering process (31 studies)", "level": "subsection", "subsections": [], "parent_id": "d73b39bb-961a-4dd6-a9c8-370ab98720b6", "prefix_titles": [["title", "Software Engineering for AI-Based Systems: A Survey"], ["section", "RQ3: Which SE approaches for AI-based systems have been reported in the scientific literature?"], ["subsection", "Software engineering process (31 studies)"]], "content": "Many of the studies mapped to the \\textit{SE process} area deal with an \\textit{AI/ML process} (13 studies). Out of those, a few discuss processes in specific application areas, such as recommendation systems~. The remaining ones address processes in general. Although the majority of papers focuses on processes for developing AI-based systems, some papers (e.g.~) address the topic of processes for creating new ML algorithms and tools. Four papers~ present an overview of current practices and challenges faced during AI system development in comparison to traditional software development. For example, Hill et al.~ conclude from their interviews with developers of AI systems that they generally struggle to establish a repeatable AI development process. Other authors address identified challenges by proposing concrete solutions or a general research agenda for AI systems engineering~. Specific approaches include the application of agile development principles to AI model and system development~, the integration of development and runtime methods known from DevOps~ or the adaptation of acknowledged process standards such as ISO 26262~.\nClosely related to the AI/ML process is the \\textit{AI/ML pipeline} category (6 studies), which instead of the overall AI system development process addresses only the part devoted to creating AI models~. \nWe identified three papers that propose \\textit{frameworks} for end-to-end support of AI system development~. The proposed frameworks target different concepts, e.g. software development, ML, algorithms, and data. They also have been designed for different domains and contexts, such as ML-based health systems~, accountability improvement via algorithmic auditing~, and the support of ML solution development within digitalization processes~.\nFurthermore, we identified six studies which provide \\textit{tools} to support the SE process of AI-based systems. An example are Patel’s general-purpose tools to provide AI/ML developers with structure for common processes and pipelines~. More specific use cases are covered by an ML platform to support iterative and rapid development of ML models~, the DEEP platform with a set of cloud-based services for the development and deployment of ML applications~, and a toolbox to support data-driven engineering of neural networks for safety-critical domains~. Lastly, other works envisioned how these platforms should be implemented~.\nThere are also three studies which report \\textit{best practices} to build ML-based systems. Mattos et al. propose five tactics to address challenges during the development of ML-based systems~: minimum viable and explainable model; randomization; disabling imputation in early stages; automation after the prototype validation; and continuous experimentation. Additionally, experiences on large scale real-world ML-based systems from Microsoft~ and IBM~ have led to the proposal of \\textit{maturity models}.\nAs in other SWEBOK areas, several studies talk about \\textit{challenges} of AI development processes~ or a \\textit{roadmap} to address them~. The challenges are reported later in  Section~\\ref{sec:results-rq4}.\n\\begin{tcolorbox}\n    Key takeaways for SE process:\n    \\begin{itemize}\n        \\item Diverse researchers, including R\\&D from large companies, have investigated the process to develop and maintain AI-based systems. Many of them highlight the need to form multidisciplinary teams for effective AI processes, e.g. including software engineers and data scientists.\n        \\item Many analyzed processes have been constructed in an ad-hoc manner based on the early experiences of large companies in AI-based systems.\n        \\item However, six studies have focused on AI pipelines at the model rather than the system level.\n        \\item Process-related support is emerging with tools (6 studies) and frameworks (3 studies).\n    \\end{itemize}\n\\end{tcolorbox}", "cites": [8114, 8134, 6214, 6215, 8135, 8120, 1341], "cite_extract_rate": 0.25925925925925924, "origin_cites_number": 27, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section shows strong synthesis by grouping studies into coherent subcategories (e.g., AI/ML process, pipeline, frameworks, tools, best practices) and connecting them to broader themes like multidisciplinary teams and ad-hoc processes. It provides some abstraction by identifying patterns and trends in how AI-based systems are being engineered. However, the critical analysis is limited, as it primarily describes findings and challenges without evaluating the strengths, weaknesses, or contradictions among the cited works."}}
{"id": "cdda56d0-1909-49e1-8e0d-1dd4771429e9", "title": "Software engineering models and methods (38 studies)", "level": "subsection", "subsections": [], "parent_id": "d73b39bb-961a-4dd6-a9c8-370ab98720b6", "prefix_titles": [["title", "Software Engineering for AI-Based Systems: A Survey"], ["section", "RQ3: Which SE approaches for AI-based systems have been reported in the scientific literature?"], ["subsection", "Software engineering models and methods (38 studies)"]], "content": "From the 38 unique primary studies in this category, the majority formulate concrete proposals on models and methods, while a few also elaborate on challenges, practices, and roadmaps. In terms of topics, the papers lean a little more towards verification \\& validation (V\\&V) methods (24 papers) than models (12 papers, one of them also in the former V\\&V methods category), with three papers reporting generic challenges, practices, and roadmaps. In general, there was a strong dominance of safety as a non-functional aspect and application domains like autonomous vehicles in this SWEBOK area.\nMost of the papers in the \\textit{V\\&V methods} category (18 out of 24) focus on formally verifying safety of cyber-physical systems, such as autonomous vehicles or robotic systems containing AI components. Typically, the AI components are controlled by artificial neural networks (16 papers), in particular deep networks. Two papers~ present an overview of current challenges and practices in V\\&V of autonomous systems, in particular those based on DL. Remaining papers propose alternative V\\&V approaches to ensure correct, robust, and safe AI-based systems. Part of them aim at systems based on specific types of artificial networks, such as multi-layer feed-forward perceptron~ or recurrent neural networks~; remaining papers provide generic approaches independent of specific ML methods being evaluated. One group of V\\&V solutions aim at specific problems of networks being easily fooled by adversarial perturbations, i.e., minimal changes to correctly classified inputs, that cause the network to misclassify them~. These approaches explore space of adversarial counter-examples to identify and ensure safe regions of the input space, within which the network is robust against adversarial perturbations~. Solutions propose direct search for counter-examples or investigation of input space margins and corner cases. Counter-example or guaranteed ranges of inputs on which artificial neural networks correctly are searched using various optimization techniques. Novelty of these approaches lies often in how the optimization problem is formulated and solved. Computation complexity and scalability are typical problems faced in this area. More recent papers attempt to solve these issues, e.g.~.\nIn the \\textit{models} category, we found primary studies targeting \\textit{formal model analysis}, model-driven engineering, model reuse, and practices. Five papers focus on formal model analysis with different goals: analysis of safety and scalability in models for autonomous vehicles~, quantitative analysis for systems based on recurrent neural networks~, improvement and certification of robustness for ML models~, and approaches for formally checking safety~ or security properties of neural networks~. Furthermore, four papers study the use of models as the initial step for derivation of other artefacts (\\textit{model-driven engineering}). Examples are the development of big data ML software~ and the incorporation of safe and robust control policies in ML models~. The other two from the same authors~ target the derivation of testing frameworks for evaluating properties of an autonomous driving system with ML components. Lastly, we identified three primary studies addressing \\textit{model reuse} using different approaches, such as an analysis of current model reuse practices and challenges for building systems based on artificial neural networks~, and tools to retain and reuse implementations of deep neural networks~.\n\\begin{tcolorbox}\n    Key takeaways for SE models and methods:\n    \\begin{itemize}\n        \\item 18 papers cover the verification and validation of cyber-physical systems with AI components.\n        \\item Safety as a non-functional aspect and application domains like autonomous vehicles are very prevalent in this area.\n    \\end{itemize}\n\\end{tcolorbox}", "cites": [3877, 8132, 8136, 1618, 6206, 1614, 8138, 1613, 8137], "cite_extract_rate": 0.5625, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple cited papers to highlight trends in SE models and methods for AI-based systems, particularly in verification and validation. It identifies broader patterns, such as the prevalence of safety concerns and application domains like autonomous vehicles. While it does offer some critical points (e.g., computational complexity and scalability challenges), the analysis is not deeply evaluative or nuanced across all cited works."}}
{"id": "10f0d109-cb31-4ec8-a10e-99b884b88f8e", "title": "Software quality (59 studies)", "level": "subsection", "subsections": [], "parent_id": "d73b39bb-961a-4dd6-a9c8-370ab98720b6", "prefix_titles": [["title", "Software Engineering for AI-Based Systems: A Survey"], ["section", "RQ3: Which SE approaches for AI-based systems have been reported in the scientific literature?"], ["subsection", "Software quality (59 studies)"]], "content": "\\textit{Quality management} is a very broad area of SE, including a number of topics such as specifying quality requirements, measuring / assessing quality, and assuring quality (SWEBOK). So far, quality management during engineering of AI-based systems has been dominated by testing and formal verification methods (see sections on \\textit{testing} and \\textit{models and methods} above). Only 17 publications address the topic of defining and assessing quality of AI-based systems. Among these, two papers~ discuss software technical debt, a derivative of software quality which refers most commonly to increased costs for maintenance and evolution due to earlier quality deficits. In particular, the authors warn software engineers tempted by quick wins of data-driven software systems of forgetting that these wins are not coming for free. To avoid incurring significant technical debt in terms of ongoing maintenance costs with AI systems, the authors explore technical debt-related risks, e.g., related to a software system itself as well as to associated data and data management systems.\nFurthermore, five articles propose \\textit{ML quality certification and assessment} to mitigate quality risks of deployed AI systems~. Several of these investigate challenges of certifying AI systems and look for potential solutions in traditional safety-critical domains such as automotive, avionics, or railway, in particular how certification approaches in these domains evolved to adjust to technological advances. Proposed certification approaches include the assessment of development processes (including workflows and engineering choices) and their impact on the quality of delivered outcomes~. The quality of AI systems is viewed from various perspectives, e.g., prediction performance quality, training mechanism quality, and lifecycle support quality including continuous operations~. Inspired by declarations of conformity -– multi-dimensional fact sheets that capture and quantify various aspects of the product and its development to make it worthy of consumers' trust -– authors propose that AI service providers publish similar documents containing purpose, performance, safety, security, and provenance information for their customers~.\nThe most commonly discussed quality characteristics include safety and related aspects such as robustness or explainability. In addition to specific quality characteristics, meta-characteristics of AI systems, such as provability (extent to which mathematical guarantees can be provided that some functional or non-functional properties are satisfied) or monitorability (extent to which a system provides information that allow to discriminate \\enquote{correct} from \\enquote{incorrect} behavior)~, are discussed in this context as prerequisites for quality assessment and certifications. Several articles investigate quality aspects specifically relevant for AI-based systems, mostly based on important new challenges that software and requirements engineers must address when developing AI systems. Major trends in our sample are ML-specific quality aspects, such as \\textit{ML safety}~, \\textit{ML ethics}~, and \\textit{ML explainability}~. Additionally, three articles from the same team of authors~ discuss how individual AI quality aspects relate to each other in the context of ISO 25000~ as an established SE quality model. They also propose adaptations to the standard and how to quantitatively measure some AI quality aspects.\nDue to the differences between AI-based systems and \\enquote{traditional} software systems, six studies covered the \\textit{update of the ISO 26262 standard} to address this. Contributions range from analyzing the deficiencies of the current version of ISO 26262~, to concrete adaptation proposals~, or a methodology framework for identifying functional deficiencies during system development~.\nWith 11 primary studies, \\textit{ML quality assurance frameworks} constitute another important topic. These frameworks normally focus on specific quality aspects of ML products, such as allowability, achievability, robustness, avoidability and improvability~, safety~, specific safety issues like forward collision mitigation based on the ISO 22839 standard~, security~, algorithmic auditing~, robustness diversity~, data validation~, or the reconciliation of product and service aspects~. Other approaches focus on continuous quality assurance with simulations~ and on run-time monitoring to manage identified risks~. Furthermore, four primary studies explore assurance cases. Ishikawa et al. discuss the use of arguments or assurance cases for ML-based systems~, including a framework for assessing the quality of ML components and systems~. Assurance cases have been also used in arguing the safety of highly automated driving functions, e.g. to solve underspecification with graphical structuring notation~ or to address functional insufficiencies in CNN-based perception functions~.\nFour studies focus on \\textit{ML defects}, i.e. several researchers have studied the specific types of bugs in AI-based systems~. Similarly, five articles discuss \\textit{ML attacks}, mostly with a focus on the use of adversarial examples~, for instance by applying adversarial perturbations under different physical conditions in cyber-physical systems. Tramer et al. also discuss attacks to steal the complete models of AI-based systems~.\nAs with other software systems, ML-based systems require \\textit{monitoring}. We can find monitoring approaches combining ML with runtime monitoring to detect violations of system invariants in the actions' execution policies~, managing identified risks, catching assumption violations, and unknown unknowns as they arise in deployed systems~,  and as a runtime safety~ or ethical~ supervisors.\nThe remaining primary studies focus on either \\textit{challenges} or establishing a research \\textit{roadmap}, which is detailed in Section~\\ref{sec:results-rq4}. We can highlight that in the 11 primary studies discussing roadmaps, they are often related to safety and the standard ISO 26262~. This seems to be a major challenge for which people not only work on detailed research contributions but see the need for a larger research roadmap. These roadmaps usually include suggestions for extensions of the standards and V\\&V methods. There are two primary studies~ that also address security and attacks. One roadmap combines it with the safety roadmap and calls for better integration of ML development into SE methods. The other roadmap concentrates on different types of attacks and countermeasures. Further explicitly mentioned quality attributes for which there is a roadmap are user experience~ and fairness~. One roadmap also discusses quality assurance certification~ and proposes to add FactSheets to ML services to increase trust. Finally, three primary studies~ propose roadmaps for general quality with ML-specific extensions to the ISO 25000 standard series. They include diverse aspects such as processes, V\\&V methods, and formal analysis.\n\\begin{tcolorbox}\n    Key takeaways for software quality:\n    \\begin{itemize}\n        \\item The specific quality aspects of AI-based systems have triggered the need to update standards such as ISO 25000 and ISO 26262.\n        \\item Most of the studies focus on ML quality attributes, frameworks, assurance and certification.\n    \\end{itemize}\n\\end{tcolorbox}", "cites": [6208, 7507, 8119, 6209, 1356, 8987, 7125, 7307, 8122, 8126, 8127, 8116, 1609, 2676, 8125, 8139], "cite_extract_rate": 0.3018867924528302, "origin_cites_number": 53, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers to present a coherent narrative on software quality in AI-based systems, particularly in areas like ML certification, defects, and technical debt. It provides critical analysis by highlighting limitations in current standards and proposing adaptations. The abstraction is strong, as it identifies overarching trends and patterns in quality characteristics and their implications for standards and certification."}}
{"id": "a54bf81b-ea16-4bde-bf02-49895867adc5", "title": "Remaining SWEBOK areas (4 studies)", "level": "subsection", "subsections": [], "parent_id": "d73b39bb-961a-4dd6-a9c8-370ab98720b6", "prefix_titles": [["title", "Software Engineering for AI-Based Systems: A Survey"], ["section", "RQ3: Which SE approaches for AI-based systems have been reported in the scientific literature?"], ["subsection", "Remaining SWEBOK areas (4 studies)"]], "content": "Other SWEBOK areas are present to a lesser extent in our sample. In the \\textit{SE management} area, Wolf et al. showed that AI software projects lead to dynamic and complex settings which necessitates active and engaged sensemaking~: software teams must strive to create coherence between AI environments, AI model ecosystems, and the business contexts that emerge while building AI systems. In the second study in this area, Raji et al. introduced a framework for algorithmic auditing for end-to-end support during the internal AI system development life cycle~. For \\textit{software configuration management}, we identified one paper: Schelter et al. presented experiences with ML model management at Amazon and outlined challenges~. Furthermore, one paper was categorized as \\textit{SE professional practice}. With the FactSheets approach from Arnold et al., AI service providers can publish documentation about their AI system including information on safety or data provenance to create an environment of transparency and trust~. Lastly, we did not identify any primary study in the area of \\textit{SE economics}, nor for \\textit{computing}, \\textit{mathematical}, and \\textit{engineering foundations}.", "cites": [8116], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly synthesizes information from four studies but lacks deeper integration or a novel framework. It mentions the contributions of each paper and notes the low volume of research in these areas, yet it does not critically evaluate the strengths or weaknesses of the approaches or identify broader trends. The abstraction is minimal, with no meta-level insights or generalization beyond the specific papers."}}
{"id": "9fc29915-c58e-48fd-89eb-40827a102fa9", "title": "RQ4: What are the existing challenges associated with SE for AI-based systems?", "level": "section", "subsections": ["476ef222-e37d-410f-a0cf-d0df403277d2", "d8a73b41-0ca7-479f-850b-e5d8faad1357", "e3d9d0c6-5acd-4d75-982b-e67041b0f610", "8feb6af1-ce0d-41ad-bac1-2a44e6be8e5b", "82ca7f2a-a3cb-4b98-a4df-7a3ff9f11073", "6e51010c-aa1c-4dbc-94b5-d7b8caf28faa", "17124742-ab85-45e5-bfe1-a26c72ae9c20", "9a62e316-5da9-4ae7-b764-dc05ff2c6023", "9e028b5d-3692-4b27-9a55-d73d16b671f0", "03b916df-3e85-4ce5-86ed-88914a962287", "d40020e0-3cc8-4e83-ab0e-5aba363a0313"], "parent_id": "eebe98c8-dc7c-4bc4-b995-ecf8692f4502", "prefix_titles": [["title", "Software Engineering for AI-Based Systems: A Survey"], ["section", "RQ4: What are the existing challenges associated with SE for AI-based systems?"]], "content": "\\label{sec:results-rq4}\nAs detailed in Section 3, the 39 papers that we analyzed in RQ4 included 94 challenges. A challenge may be classified into more than one SWEBOK Topic, although most of the challenges (70\\%) were classified under one Topic only. \nTable~\\ref{tab:Tab_7_2} summarizes the number of papers, challenges and assignments to every SWEBOK Knowledge Area. Some Knowledge Areas prevale, although it cannot be said that a Knowledge Area excels significantly from the rest; furthermore, the order is different if we focus on the papers or on the challenges, because as said above several papers identify a number of challenges related to one particular Knowledge Area. \n\\begin{table}\n\\caption{Distribution of challenges into SWEBOK Knowledge Areas.}\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n\\textbf{Category} & \\multicolumn{1}{l|}{\\textbf{\\#papers}} & \\multicolumn{1}{l|}{\\textbf{\\#challenges}} & \\multicolumn{1}{l|}{\\textbf{\\#assignments}} \\\\ \\hline\nSoftware Engineering Models and Methods & 11 & 17 & 17 \\\\ \\hline\nSoftware Quality & 9 & 14 & 15 \\\\ \\hline\nSoftware Construction & 9 & 9 & 10 \\\\ \\hline\nSoftware Testing & 8 & 14 & 19 \\\\ \\hline\nSoftware Design & 7 & 10 & 10 \\\\ \\hline\nSoftware Runtime Behaviour & 6 & 10 & 10 \\\\ \\hline\nSoftware Requirements & 5 & 16 & 18 \\\\ \\hline\nSoftware Engineering Process & 5 & 8 & 12 \\\\ \\hline\nSoftware Engineering Professional Practice & 4 & 11 & 12 \\\\ \\hline\nSoftware Maintenance & 3 & 3 & 3 \\\\ \\hline\nSoftware Engineering Economics & 2 & 3 & 3 \\\\ \\hline\nSoftware Configuration Management & 1 & 1 & 1 \\\\ \\hline\n\\end{tabular}\n\\label{tab:Tab_7_2}\n\\end{table}\nIf we look at the SWEBOK Topics, we see that some of them are quite popular. Table~\\ref{tab:Tab_7_3} shows those referenced by 4 challenges or more. Remarkably, three of the topics that we proposed as an extension of SWEBOK appear in the top 5 positions, which is somehow natural (they naturally emerged because they were popular).\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{rq4-swebok-challenges.pdf}\n    \\caption{Distribution of Challenges into SWEBOK Knowledge Areas.}\n    \\label{fig:Figure_7_2}\n\\end{figure}\n\\begin{table}\n\\caption{Most popular SWEBOK Topics as referenced in the challenges.}\n\\begin{tabular}{llc}\n\\hline\n\\multicolumn{1}{|l|}{\\textbf{SWEBOK Topic}} & \\multicolumn{1}{l|}{\\textbf{SWEBOK Knowledge Areas}} & \\multicolumn{1}{c|}{\\textbf{\\#challenges}} \\\\ \\hline\n\\multicolumn{1}{|l|}{AI/ML methods*} & \\multicolumn{1}{l|}{Software Engineering Models and Methods} & \\multicolumn{1}{c|}{9} \\\\ \\hline\n\\multicolumn{1}{|l|}{Properties and expressions of models} & \\multicolumn{1}{l|}{Software Engineering models and methods} & \\multicolumn{1}{c|}{7} \\\\ \\hline\n\\multicolumn{1}{|l|}{Robustness \\& operability*} & \\multicolumn{1}{l|}{Runtime Behaviour} & \\multicolumn{1}{c|}{7} \\\\ \\hline\n\\multicolumn{1}{|l|}{Interacting with stakeholders} & \\multicolumn{1}{l|}{Software Engineering Professional Practice} & \\multicolumn{1}{c|}{6} \\\\ \\hline\n\\multicolumn{1}{|l|}{Data related issues*} & \\multicolumn{1}{l|}{Software Testing} & \\multicolumn{1}{c|}{5} \\\\ \\hline\n\\multicolumn{1}{|l|}{Software quality assessment} & \\multicolumn{1}{l|}{Software Quality} & \\multicolumn{1}{c|}{5} \\\\ \\hline\n\\multicolumn{1}{|l|}{Software structure and architecture} & \\multicolumn{1}{l|}{Software Design} & \\multicolumn{1}{c|}{5} \\\\ \\hline\n\\multicolumn{1}{|l|}{AI/ML specific activities*} & \\multicolumn{1}{l|}{Software Engineering Process} & \\multicolumn{1}{c|}{4} \\\\ \\hline\n\\multicolumn{1}{|l|}{Quality requirements} & \\multicolumn{1}{l|}{Software Quality} & \\multicolumn{1}{c|}{4} \\\\ \\hline\n\\multicolumn{1}{|l|}{Requirements specification} & \\multicolumn{1}{l|}{Software Requirements} & \\multicolumn{1}{c|}{4} \\\\ \\hline\n\\multicolumn{3}{l}{* Proposed extension of SWEBOK.}\n\\end{tabular}\n\\label{tab:Tab_7_3}\n\\end{table}\nWe describe below the challenges of each Knowledge Area\\footnote{Names of SWEBOK Knowledge Areas and Topics appear in italics}. As for reporting style, we have opted to include the challenges respecting the words of the primary studies’ authors; therefore, our work has consisted mostly in grouping and articulating these challenges into a unifying narrative. This also implies that we are intentionally refraining from adding our own interpretations or enriching in any way the challenges identified in the primary studies. Last, for readability, we do neither quote the challenges nor include all citations in the text; Table~\\ref{tab:Tab_7_4} includes the references for every SWEBOK Knowledge Area.\n\\begin{table}\n\\caption{Primary studies containing challenges per SWEBOK area.}\n\\begin{tabular}{|l|l|}\n\\hline\n\\textbf{SWEBOK Knowledge Area} & \\textbf{List of references} \\\\ \\hline\nSoftware Engineering Models and Methods & \\begin{tabular}[c]{@{}l@{}}\\end{tabular} \\\\ \\hline\nSoftware Requirements & \\begin{tabular}[c]{@{}l@{}}\\end{tabular} \\\\ \\hline\nSoftware Testing & \\begin{tabular}[c]{@{}l@{}}\\end{tabular} \\\\ \\hline\nSoftware Quality & \\begin{tabular}[c]{@{}l@{}}\\end{tabular} \\\\ \\hline\nSoftware Engineering Professional Practice &  \\\\ \\hline\nSoftware Construction & \\begin{tabular}[c]{@{}l@{}}\\end{tabular} \\\\ \\hline\nSoftware Runtime Behaviour & \\begin{tabular}[c]{@{}l@{}}\\end{tabular} \\\\ \\hline\nSoftware Engineering Process & \\begin{tabular}[c]{@{}l@{}}\\end{tabular} \\\\ \\hline\nSoftware Design & \\begin{tabular}[c]{@{}l@{}}\\end{tabular} \\\\ \\hline\nSoftware Maintenance &  \\\\ \\hline\nSoftware Engineering Economics &  \\\\ \\hline\nSoftware Configuration Management &  \\\\ \\hline\n\\end{tabular}\n\\label{tab:Tab_7_4}\n\\end{table}", "cites": [8119, 1356, 6207, 8117, 8122, 8118], "cite_extract_rate": 0.2222222222222222, "origin_cites_number": 27, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "low", "analysis": "The section provides a descriptive summary of the challenges identified in the primary studies, organized by SWEBOK Knowledge Areas and Topics. It mentions the number of papers and challenges but does not deeply connect or synthesize ideas across sources. There is minimal critical analysis or abstraction, as the authors explicitly refrain from adding their own interpretations and focus on categorizing the reported challenges."}}
{"id": "57959d58-8432-48b0-a6ae-81491cad828c", "title": "Threats to validity", "level": "section", "subsections": ["21ac84cd-7981-46da-828d-707e5584d042", "cf597710-d533-47c3-bf20-bd02e48f74b8", "bf80b47b-202c-433e-8045-fecc1dc9fd38"], "parent_id": "eebe98c8-dc7c-4bc4-b995-ecf8692f4502", "prefix_titles": [["title", "Software Engineering for AI-Based Systems: A Survey"], ["section", "Threats to validity"]], "content": "\\label{sec:threats}\nAs any other empirical study, ours faces a series of threats to validity. We report them below according to frequent threats specific to secondary studies  including mitigation actions. These specific threats are divided in three categories: study selection validity, data validity, and research validity. As a global mitigation action, we have considered in our study the ACM SIGSOFT Empirical Standards , and in particular we have ensured: (i) to comply with all the eight essential specific attributes for systematic reviews; (ii) to avoid the three anti-patterns that apply to systematic reviews (i.e., not synthesising findings, not including quality assessment of primary studies, shortage of high-quality primary studies).", "cites": [6216], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from the cited empirical standards paper by connecting it to the structure of threats to validity in secondary studies. It provides a classification into three categories and aligns with the ACM SIGSOFT guidelines, showing some integration. However, it lacks deeper critical analysis of the cited work or broader comparative insights. The abstraction is moderate, as it generalizes threats into categories but does not develop a new meta-framework."}}
