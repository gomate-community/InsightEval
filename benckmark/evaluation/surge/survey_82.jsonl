{"id": "c6721c4f-f568-49fa-a376-8b26124c3f6f", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "43aba542-7f6b-493f-9beb-9ae4164ca84b", "prefix_titles": [["title", "Low-Light Image and Video Enhancement \\\\Using Deep Learning: A Survey"], ["section", "Introduction"]], "content": "\\label{sec:Introduction}}\n\\IEEEPARstart{I}{mages} are often taken under sub-optimal lighting conditions, under the influence of backlit, uneven light, and dim light, due to inevitable environmental and/or technical constraints such as insufficient illumination and limited exposure time. Such images suffer from the compromised aesthetic quality and unsatisfactory transmission of information for high-level tasks such as object tracking, recognition, and detection. Figure \\ref{fig:example} shows some examples of the degradations induced by sub-optimal lighting conditions. \nLow-light enhancement enjoys a wide range of applications in different areas, including visual surveillance, autonomous driving, and computational photography. In particular, smartphone photography has become ubiquitous and prominent. Limited by the size of the camera aperture, the requirement of real-time processing, and the constraint of memory, taking photographs with a smartphone's camera in a dim environment is especially challenging. There is an exciting research arena of enhancing low-light images and videos in such applications.\n\\begin{figure}[!t]\n\t\\begin{center}\n\t\t\\begin{tabular}{c@{ }c@{ }c@{ }}\n\t\t\t\\includegraphics[width=0.3\\linewidth,height=0.2\\linewidth]{figures/Fig1_backlit.png}&\n\t\t\t\\includegraphics[width=0.3\\linewidth,height=0.2\\linewidth]{figures/Fig1_nonuniform.png}&\n\t\t\t\\includegraphics[width=0.3\\linewidth,height=0.2\\linewidth]{figures/Fig1_weak.png}\\\\\n\t\t\t(a) back lit & (b) uneven light &(c) dim light\\\\\n\t\t\t\\includegraphics[width=0.3\\linewidth,height=0.2\\linewidth]{figures/Fig1_dark.png}&\n\t\t\t\\includegraphics[width=0.3\\linewidth,height=0.2\\linewidth]{figures/0027.png}&\n\t\t\t\\includegraphics[width=0.3\\linewidth,height=0.2\\linewidth]{figures/Fig1_noise.png}\\\\\n\t\t\t(d) extremely low &\t(e) colored light & (f) boosted noise\\\\\n\t\t\\end{tabular}\n\t\\end{center}\n\t\\vspace{-3pt}\n\t\\caption{Examples of images taken under sub-optimal lighting conditions. These images suffer from the buried scene content, reduced contrast, boosted noise, and inaccurate color. }\n\t\\label{fig:example}\n\t\\vspace{-5pt}\n\\end{figure}\n\\begin{figure*}[thb]\n\t\\centering  \\centerline{\\includegraphics[width=1\\linewidth]{figures/chronology.png}}\n\t\\vspace{-2pt}\n\t\\caption{A concise milestone of deep learning-based low-light image and video enhancement methods.  \\textbf{Supervised learning-based methods}: LLNet , Chen et al. , MBLLEN , Retinex-Net , LightenNet , SCIE , DeepUPE , Chen et al. , Jiang and Zheng , Wang et al. , KinD , Ren et al. , Xu et al. , Fan et al. , Lv et al. , EEMEFN~, SIDGAN. , LPNet , DLN , TBEFN ,  DSLR ,  Zhang et al. , PRIEN , and Retinex-Net . \\textbf{Reinforcement  learning-based method}: DeepExposure . \\textbf{Unsupervised  learning-based method}: EnlightenGAN . \\textbf{Zero-shot learning-based methods}: ExCNet , Zero-DCE , RRDNet , Zero-DCE++ , RetinexDIP , and RUAS . \\textbf{Semi-supervised learning-based method}: DRBN  and DRBN .}\n\t\\label{fig:milestones}\n\t\\vspace{-4pt}\n\\end{figure*}\nTraditional methods for low-light enhancement include Histogram Equalization-based methods  and Retinex model-based methods . The latter received relatively more attention. A typical Retinex model-based approach decomposes a low-light image into a reflection component and an illumination component by priors or regularizations. The estimated reflection component is treated as the enhanced result.\nSuch methods have some limitations:\n\\textbf{1)} the ideal assumption that treats the reflection component as the enhanced result does not always hold, especially given various illumination properties, which could lead to unrealistic enhancement such as loss of details and distorted colors, \\textbf{2)} the noise is usually ignored in the Retinex model, thus it is remained or amplified in the enhanced results, \\textbf{ 3)} finding an effective prior or regularization is challenging. Inaccurate prior or regularization may result in artifacts and color deviations in the enhanced results, and \\textbf{4)} the runtime is relatively long because of their complicated optimization process.\nRecent years have witnessed the compelling success of deep learning-based LLIE since the first seminal work . Deep learning-based solutions enjoy better accuracy, robustness, and speed over conventional methods, thus attracting increasing attention. A concise milestone of deep learning-based LLIE methods is shown in Figure \\ref{fig:milestones}. As shown, \nsince 2017, the number of deep learning-based solutions has grown year by year. Learning strategies used in these solutions cover Supervised Learning (SL), Reinforcement Learning (RL), Unsupervised Learning (UL), Zero-Shot Learning (ZSL), and Semi-Supervised Learning (SSL). Note that we only report some representative methods in Figure \\ref{fig:milestones}. In fact, there are more than 100 papers on deep learning-based methods from 2017 to 2021. Moreover, although some general photo enhancement methods   can improve the brightness of images to some extent, we omit them in this survey as they are not designed to handle diverse low-light conditions. We concentrate on deep learning-based solutions that are specially developed for low-light image and video enhancement.\n\tDespite deep learning has dominated the research of LLIE, an in-depth and comprehensive survey on deep learning-based solutions is lacking. There are two reviews of LLIE . Wang et al.  mainly reviews conventional LLIE methods while our work systematically and comprehensively reviews recent advances of deep learning-based LLIE. In comparison to Liu et al.  that reviews existing LLIE algorithms, measures the machine vision performance of different methods, provides a low-light image dataset serving both low-level and high-level vision enhancement, and develops an enhanced face detector, our survey reviews the low-light image and video enhancement from different aspects and has the following unique characteristics.\n\t\\textbf{1)} Our work mainly focuses on recent advances of deep learning-based low-light image and video enhancement, where we provide in-depth analysis and discussion in various aspects, covering learning strategies, network structures, loss functions, training datasets, test datasets, evaluation metrics, model sizes, inference speed, enhancement performance, etc. Thus, this survey centers on deep learning and its applications in low-light image and video enhancement.\n\t\\textbf{2)} We propose a dataset that contains images and videos captured by different mobile phones' cameras under diverse illumination conditions to evaluate the generalization of existing methods. This new and challenging dataset is a supplement of existing low-light image and video enhancement datasets as such a dataset is lacking in this research area. Besides, we are the first, to the best of our knowledge,  to compare the performance of deep learning-based low-light image enhancement methods on this kind of data. \n\t\\textbf{3)} We provide an online platform that covers many popular deep learning-based low-light image enhancement methods, where the results can be produced by a user-friendly web interface. With our platform, one without any GPUs can assess the results of different methods for any input images online, which speeds up the development of this research field and helps to create new research.\n\tWe hope that our survey could provide novel insights and inspiration to facilitate the understanding of deep learning-based LLIE, foster research on the raised open issues, and speed up the development of this research field.", "cites": [1875], "cite_extract_rate": 1.0, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of deep learning-based low-light image enhancement by categorizing methods according to learning strategies and highlighting the limitations of traditional approaches. However, while it mentions representative papers and introduces a new dataset and platform, it does not deeply synthesize insights across these works or provide nuanced critical evaluation of their strengths and weaknesses."}}
{"id": "44126fdb-481e-4904-b2e2-6e15d767dcdd", "title": "Learning Strategies", "level": "subsection", "subsections": [], "parent_id": "8eda68ea-596d-47b0-8c59-3c11fbc66273", "prefix_titles": [["title", "Low-Light Image and Video Enhancement \\\\Using Deep Learning: A Survey"], ["section", "Deep Learning-Based LLIE"], ["subsection", "Learning Strategies"]], "content": "According to different learning strategies, we categorize existing LLIE methods into supervised learning, reinforcement learning, unsupervised learning, zero-shot learning, and semi-supervised learning. \nA statistic analysis from different perspectives is presented in Figure \\ref{fig:statistic}. In what follows, we review some representative methods of each strategy. \n\\noindent\n\\textbf{Supervised Learning.} \nFor supervised learning-based LLIE methods,  they are further divided into end-to-end, deep Retinex-based, and realistic data-driven methods.\nThe first deep learning-based LLIE method LLNet~ employs a variant of stacked-sparse\ndenoising autoencoder  to brighten and denoise low-light images simultaneously.  \nThis pioneering work inspires the usage of end-to-end networks in LLIE. \nLv et al.  propose an end-to-end multi-branch enhancement network (MBLLEN). The MBLLEN  improves the performance of LLIE via extracting effective feature representations by a feature extraction module, an enhancement module, and a fusion module. \nThe same authors   propose other three subnetworks including an Illumination-Net, a Fusion-Net, and a Restoration-Net to further improve the performance. \nRen et al.  design a more complex end-to-end network that comprises an encoder-decoder network for image content enhancement and a recurrent neural network for image edge enhancement.  \nSimilar to Ren et al. , Zhu et al.  propose a method called EEMEFN. The EEMEFN consists of two stages: multi-exposure fusion and edge enhancement. \nA multi-exposure fusion network, TBEFN~, is proposed for LLIE. The TBEFN estimates a transfer function in two branches, of which two enhancement results can be obtained. At last, a simple average scheme is employed to fuse these two images and further refine the result via a refinement unit. \nIn addition, pyramid network (LPNet) , residual network , and Laplacian pyramid  (DSLR)\nare introduced into LLIE. These methods learn to effectively and efficiently integrate feature representations via commonly used end-to-end network structures for LLIE.\nBased on the observation that noise exhibits different levels of contrast in different frequency layers, Xu et al.  proposed a frequency-based decomposition-and-enhancement network. This network recovers image contents with noise suppression in the low-frequency layer while inferring the details in the high-frequency layer. \nRecently, a progressive-recursive low-light image enhancement network  is proposed, which uses a recursive unit to gradually enhance the input image.\nTo solve temporal instability when handling low-light videos, Zhang et al.  propose to learn and infer motion field from a single image then enforce temporal consistency.\nIn comparison to directly learning an enhanced result in an end-to-end network,  deep Retinex-based methods enjoy better enhancement performance in most cases owing to the physically explicable Retinex theory . Deep Retinex-based methods usually separately enhance the illuminance component and the reflectance components via specialized subnetworks.\nA Retinex-Net  is proposed, which includes a Decom-Net that splits the input image into light-independent reflectance and structure-aware smooth illumination and an Enhance-Net that adjusts the illumination map for low-light enhancement. Recently, the Retinex-Net  is extended by adding new constraints and advanced network designs for better enhancement performance .\nTo reduce the computational burden, Li et al.  propose a lightweight LightenNet for weakly illuminated image enhancement, which only consists of four layers. \nThe LightenNet takes a weakly illuminated image as the input and then estimates its illumination map. Based on the Retinex theory , the enhanced image is obtained by dividing the input image by the illumination map.\nTo accurately estimate the illumination map, Wang et al.  extract the global and local features to learn an image-to-illumination mapping by their proposed DeepUPE network.\nZhang et al.  separately develop three subnetworks for layer decomposition, reflectance restoration, and illumination adjustment, called KinD. \nFurthermore, the authors alleviate the visual defects left in the results of KinD  by a multi-scale illumination attention module. The improved KinD is called KinD++ .\nTo solve the issue that the noise is omitted in the deep Retinex-based methods,\nWang et al.  propose a progressive Retinex network, where an IM-Net estimates the illumination and a NM-Net estimates the noise level. These two subnetworks work in a progressive mechanism until obtaining stable results. \nFan et al.  integrate semantic segmentation and Retinex model for further improving the enhancement performance in real cases. The core idea is to use semantic prior to guide the enhancement of both the illumination component and the reflectance component. \n\tAlthough some methods can achieve decent performance, they show poor generalization capability in real low-light cases due to the usage of synthetic training data. To solve this issue, some works attempt to generate more realistic training data or capture real data.\n\tCai et al.   build a multi-exposure image dataset, where the low-contrast images of different exposure levels have their corresponding high-quality reference images. \n\tEach high-quality reference image is obtained by subjectively selecting the best output from 13 results enhanced by different methods. Moreover, a frequency decomposition network is trained on the built dataset and separately enhances the high-frequency layer and the low-frequency layer via a two-stage structure. \n\tChen et al.  collect a real low-light image dataset (SID) and \n\ttrain the U-Net  to learn a mapping from low-light raw data to the corresponding long-exposure high-quality reference image.\n\tFurther,  Chen et al.   extend the SID dataset to low-light videos (DRV). The DRV contains static videos with the corresponding long-exposure ground truths. To ensure the generalization capability of processing the videos of dynamic scenes, a siamese network is proposed. \n\tTo enhance the moving objects in the dark, Jiang and Zheng~ design a co-axis optical system to capture temporally synchronized and spatially aligned low-light and well-lighted video pairs (SMOID). \n\tUnlike the DRV video dataset ,  the SMOID video dataset contains dynamic scenes. To learn the mapping from raw low-light video to well-lighted video, a 3D U-Net-based network is proposed.\n\tConsidering the limitations of previous low-light video datasets such as DRV dataset  only containing statistic videos and SMOID dataset  only having 179 video pairs, \n\tTriantafyllidou et al.~ propose a low-light video synthesis pipeline, dubbed SIDGAN. The SIDGAN can produce dynamic video data (raw-to-RGB) by a semi-supervised dual CycleGAN with intermediate domain mapping. \n\tTo train this pipeline, the real-world videos are collected from Vimeo-90K dataset . The low-light raw video data and the corresponding long-exposure images are sampled from DRV dataset . \n\tWith the synthesized training data, this work adopts the same U-Net network as Chen et al.  for low-light video enhancement. \n\t\\noindent\n\t\\textbf{Reinforcement Learning.}\n\tWithout paired training data, Yu et al.   learn to expose photos with reinforcement adversarial learning, named DeepExposure. Specifically, an input image is first segmented into sub-images according to exposures. For each sub-image, local exposure is learned by the policy network sequentially based on reinforcement learning. The reward evaluation function is approximated by adversarial learning. At last, each local exposure is employed to retouch the input, thus obtaining multiple retouched images under different exposures. The final result is achieved by fusing these images. \n\t\\noindent\n\t\\textbf{Unsupervised Learning.}\n\tTraining a deep model on paired data may result in overfitting and limited generalization capability. To solve this issue, an unsupervised learning method named EnligthenGAN  is proposed.\n\tThe EnlightenGAN adopts an attention-guided U-Net  as the generator and uses the global-local discriminators to ensure the enhanced results look like realistic normal-light images. \n\tIn addition to global and local adversarial losses, the global and local self feature preserving losses are proposed to preserve the image content before and after the enhancement. This is a key point for the stable training of such a one-path Generative Adversarial Network (GAN) structure. \n\t\\noindent\n\t\\textbf{Zero-Shot Learning.} \n\tThe supervised learning, reinforcement learning, and unsupervised learning methods either have limited generalization capability or suffer from unstable training. To remedy these issues, zero-shot learning is proposed to learn the enhancement solely from the testing images. \n\tNote that the concept of zero-shot learning in the low-level vision tasks is used to emphasize that the method does not require paired or unpaired training data, which is different from its definition in high-level visual tasks. \n\tZhang et al.  propose a zero-shot learning method, called ExCNet, for back-lit image restoration. \n\tA network is first used to estimate the S-curve that best fits the input image. Once the S-curve is estimated, the input image is separated into a base layer and a detail layer using the guided filter . Then the base layer is adjusted by the estimated S-curve. Finally, the Weber contrast  is used to fuse the detailed layer and the adjusted base layer. To train the ExCNet, the authors formulate the loss function as a block-based energy minimization problem. \n\tZhu et al.  propose a three-branch CNN, called RRDNet, for underexposed images restoration. The RRDNet decomposes an input image into illumination, reflectance, and noise via iteratively minimizing specially designed loss functions. \n\tTo drive the zero-shot learning, a combination of Retinex reconstruction loss, texture enhancement loss, and illumination-guided noise estimation loss is proposed. \n\tZhao et al.  perform Retinex decomposition via neural networks and then enhance the low-light image based on the Retinex model, called RetinexDIP. Inspired by Deep Image Prior (DIP) , RetinexDIP generates the reflectance component and illumination component of an input image by randomly sampled white noise, in which the component characteristics-related losses such as illumination smoothness are used for training.\n\tLiu et al.   propose a Retinex-inspired unrolling method for LLIE, in which the cooperative architecture search is used to discover lightweight prior architectures of basic blocks and non-reference losses are used to train the network.\n\tDifferent from the image reconstruction-based methods , \n\ta deep curve estimation network, Zero-DCE , is proposed. Zero-DCE formulates the light enhancement as a task of image-specific curve estimation, which takes a low-light image as input and produces high-order curves as its output. These curves are used for pixel-wise adjustment on the dynamic range of the input to obtain an enhanced image. Further, an accelerated and lightweight version is proposed, called Zero-DCE++ . \n\tSuch curve-based methods do not require any paired or unpaired data during training. They achieve zero-reference learning via a set of non-reference loss functions. Besides, unlike the image reconstruction-based methods that need high computational resources, the image-to-curve mapping only requires lightweight networks, thus achieving a fast inference speed. \n\t\\begin{figure*}[!t]\n\t\t\\begin{center}\n\t\t\t\\begin{tabular}{c@{ }c@{ }c@{ }c@{ }}\n\t\t\t\t\\includegraphics[width=.23\\textwidth]{figures/Picture1.png}&\n\t\t\t\t\\includegraphics[width=.23\\textwidth]{figures/Picture2.png}&\n\t\t\t\t\\includegraphics[width=.23\\textwidth]{figures/Picture3.png}&\n\t\t\t\t\\includegraphics[width=.23\\textwidth]{figures/Picture4.png}\\\\\n\t\t\t\t(a) learning strategy & (b) network structure& (c) Retinex model & (d)  data format \\\\\n\t\t\t\t\\includegraphics[width=.23\\textwidth]{figures/Picture5.png}&\n\t\t\t\t\\includegraphics[width=.23\\textwidth]{figures/Picture6.png}&\n\t\t\t\t\\includegraphics[width=.23\\textwidth]{figures/Picture7.png}&\n\t\t\t\t\\includegraphics[width=.23\\textwidth]{figures/Picture8.png}\\\\\n\t\t\t\t(e) loss function & (f) training dataset & (g) testing dataset & (h) evaluation metric \\\\\n\t\t\t\\end{tabular}\n\t\t\\end{center}\n\t\t\\vspace{-2pt}\n\t\t\\caption{A statictic analysis of deep learning-based LLIE methods, including learning strategy, network characteristic, Retinex model, data format, loss function, training dataset, testing dataset, and evaluation metric. Best viewed by zooming in.}\n\t\t\\label{fig:statistic}\n\t\t\\vspace{-4pt}\n\t\\end{figure*}\n\t\\noindent\n\t\\textbf{Semi-Supervised Learning.}\n\tTo combine the strengths of supervised learning and unsupervised learning, semi-supervised learning has been proposed in recent years.\n\tYang et al.  propose a semi-supervised deep recursive band network (DRBN). The DRBN first recovers a linear band representation of an enhanced image under supervised learning, and then obtains an improved one by recomposing the given bands via a learnable linear transformation based on unsupervised adversarial learning.  \n\tThe DRBN is extended by introducing  Long Short Term Memory (LSTM) networks and an image quality assessment network pre-trained on an aesthetic visual analysis dataset, which achieves better enhancement performance .\n\tObserving Figure \\ref{fig:statistic}(a), we can find that supervised learning is the mainstream among deep learning-based LLIE methods, of which the percentage reaches 73\\%. This is because supervised learning is relatively easy when paired training data such as LOL , SID  and diverse low-/normal-light image synthesis approaches are used.  \n\tHowever, supervised learning-based methods suffer from some challenges: \n\t\\textbf{1)} collecting a large-scale paired dataset that covers diverse real-world low-light conditions is difficult, \n\t\\textbf{2)}  synthetic low-light images do not accurately represent real-world illuminance conditions such as spatially varying lighting and different levels of noise, and \n\t\\textbf{3)} training a deep model on paired data may result in limited generalization to real-world images of diverse illumination properties. \n\tTherefore, some methods adopt unsupervised learning, reinforcement learning, semi-supervised learning, and zero-shot learning to bypass the challenges in supervised learning. Although these methods achieve competing performance, they still suffer from some limitations: \n\t\\textbf{1)} for unsupervised learning/semi-supervised learning methods, how to implement stable training, avoid color deviations, and build the relations of cross-domain information challenges current methods,\n\t\\textbf{2)} for reinforcement learning methods, designing an effective reward mechanism and implementing efficient and stable training are intricate, and \n\t\\textbf{3)} for zero-shot learning methods, the design of non-reference losses is non-trivial when the color preservation, artifact removal, and gradient back-propagation should be taken into account. \n\t\\begin{table*}\n\t\t\\rowcolors{1}{gray!20}{white}\n\t\t\\centering\n\t\t\\caption{\n\t\t\t{Summary of essential characteristics of representative deep learning-based methods. ``Retinex'' indicates whether the models are Retinex-based or not. ``simulated'' means the testing data are simulated by the same approach as the synthetic training data. ``self-selected'' stands for the real-world images selected by the authors. ``\\#P''represents the number of trainable parameters. ``-'' means this item is not available or not indicated in the paper.}\n\t\t}\n\t\t\\vspace{-6pt}\n\t\t\\label{table:methods}\n\t\t\\begin{threeparttable}\n\t\t\t\\resizebox{1\\textwidth}{!}{\n\t\t\t\t\\setlength\\tabcolsep{2pt}\n\t\t\t\t\\renewcommand\\arraystretch{0.98}\n\t\t\t\t\\begin{tabular}{c|r||c|c|c|c|c|c|c|c|c}\n\t\t\t\t\t\\hline\n\t\t\t\t\t&\\textbf{Method}~~~~~~~~~&\\textbf{Learning} &\\textbf{Network Structure} &\\textbf{Loss Function}\n\t\t\t\t\t&\\textbf{Training Data} & \\textbf{Testing Data} &\\textbf{Evaluation Metric} & \\textbf{Format} &\\textbf{Platform} &\\textbf{Retinex}\\\\\n\t\t\t\t\t\\hline\n\t\t\t\t\t\\hline\n\t\t\t\t\t\\multirow{1}{*}{\\rotatebox{90}{\\textbf{2017}}}\n\t\t\t\t\t&LLNet~ &SL &SSDA &SRR loss\n\t\t\t\t\t&\\tabincell{c}{simulated by \\\\Gamma Correction \\& \\\\Gaussian Noise} &\\tabincell{c}{simulated \\\\ self-selected} &\\tabincell{c}{PSNR SSIM} &RGB &Theano &\\\\\n\t\t\t\t\t\\hline\n\t\t\t\t\t\\hline\n\t\t\t\t\t\\multirow{1}{*}{\\rotatebox{90}{\\textbf{2018}}}\n\t\t\t\t\t&LightenNet~ &SL &four layers &$L_{2}$ loss\n\t\t\t\t\t&\\tabincell{c}{simulated by \\\\random illumination values} &\\tabincell{c}{simulated\\\\self-selected} &\\tabincell{c}{PSNR MAE\\\\SSIM \\\\User Study} &RGB &\\tabincell{c}{Caffe\\\\MATLAB}&\\checkmark\\\\\n\t\t\t\t\t&Retinex-Net~ &SL &\n\t\t\t\t\tmulti-scale network &\\tabincell{c}{$L_{1}$ loss smoothness loss \\\\invariable reflectance loss}\n\t\t\t\t\t&\\tabincell{c}{LOL\\\\simulated by \\\\adjusting histogram} &self-selected &- &RGB &TensorFlow&\\checkmark\\\\\n\t\t\t\t\t&MBLLEN~ &SL &multi-branch fusion &\\tabincell{c}{SSIM loss region loss\\\\perceptual loss}\n\t\t\t\t\t&\\tabincell{c}{simulated by \\\\Gamma Correction \\& \\\\Poisson Noise} &\\tabincell{c}{simulated\\\\ self-selected} &\\tabincell{c}{PSNR SSIM\\\\AB VIF\\\\LOE TOMI} &RGB &\\tabincell{c}{TensorFlow} & \\\\\n\t\t\t\t\t&SCIE~ &SL  &frequency decomposition &\\tabincell{c}{$L_{2}$ loss $L_{1}$ loss SSIM loss}\n\t\t\t\t\t&SCIE &SCIE &\\tabincell{c}{PSNR FSIM\\\\Runtime FLOPs} &RGB&\\tabincell{c}{Caffe\\\\MATLAB}&\\\\\n\t\t\t\t\t&Chen et al.~ &SL  &U-Net &$L_{1}$ loss\n\t\t\t\t\t&SID &SID &\\tabincell{c}{PSNR SSIM} &raw&TensorFlow&\\\\\n\t\t\t\t\t&Deepexposure~ &RL  &\\tabincell{c}{policy network\\\\GAN}  &\\tabincell{c}{deterministic policy gradient\\\\adversarial loss}\n\t\t\t\t\t&MIT-Adobe FiveK&MIT-Adobe FiveK &\\tabincell{c}{PSNR SSIM} &raw&TensorFlow &\\\\\n\t\t\t\t\t\\hline\n\t\t\t\t\t\\hline\n\t\t\t\t\t\\multirow{1}{*}{\\rotatebox{90}{\\textbf{2019}}}\n\t\t\t\t\t&Chen et al.~ &SL &siamese network &\\tabincell{c}{$L_{1}$ loss\\\\ self-consistency loss}\n\t\t\t\t\t&DRV &DRV &\\tabincell{c}{PSNR\\\\ SSIM\\\\ MAE} &raw &TensorFlow&\\\\\n\t\t\t\t\t&Jiang and Zheng~  &SL &3D U-Net &$L_{1}$ loss\n\t\t\t\t\t&SMOID &SMOID &\\tabincell{c}{PSNR SSIM MSE} &raw &TensorFlow &\\\\\n\t\t\t\t\t&DeepUPE~  &SL &illumination map &\\tabincell{c}{$L_{1}$ loss color loss\\\\smoothness loss}\n\t\t\t\t\t&retouched image pairs&MIT-Adobe FiveK&\\tabincell{c}{PSNR SSIM\\\\User Study} &RGB &TensorFlow &\\checkmark\\\\\n\t\t\t\t\t&KinD~ &SL &\\tabincell{c}{three subnetworks\\\\U-Net} &\\tabincell{c}{reflectance similarity loss\\\\illumination smoothness loss\\\\mutual consistency loss\\\\$L_{1}$ loss $L_{2}$ loss SSIM loss\\\\texture similarity loss\\\\illumination adjustment loss}\n\t\t\t\t\t&LOL &\\tabincell{c}{LOL LIME\\\\NPE MEF} &\\tabincell{c}{PSNR SSIM\\\\LOE NIQE} &RGB &TensorFlow&\\checkmark\\\\\n\t\t\t\t\t&Wang et al.~ &SL &\\tabincell{c}{two subnetworks\\\\pointwise Conv} &$L_{1}$ loss \n\t\t\t\t\t&\\tabincell{c}{simulated by \\\\camera imaging model} &\\tabincell{c}{IP100 FNF38\\\\MPI LOL NPE} &\\tabincell{c}{PSNR SSIM\\\\NIQE} &RGB &Caffe &\\checkmark \\\\\n\t\t\t\t\t&Ren et al.~ &SL &\\tabincell{c}{U-Net like network\\\\RNN dilated Conv} &\\tabincell{c}{$L_{2}$ loss perceptual loss\\\\adversarial loss} \n\t\t\t\t\t&\\tabincell{c}{MIT-Adobe FiveK\\\\with Gamma correction \\\\   \\& Gaussion noise} &\\tabincell{c}{simulated \\\\self-selected DPED} &\\tabincell{c}{PSNR SSIM\\\\Runtime} &RGB &Caffe&\\\\\n\t\t\t\t\t&EnlightenGAN~ &UL &U-Net like network &\\tabincell{c}{adversarial loss\\\\self feature preserving loss}\n\t\t\t\t\t&unpaired real images &\\tabincell{c}{NPE LIME\\\\MEF DICM\\\\VV BBD-100K\\\\ExDARK} &\\tabincell{c}{User Study NIQE\\\\Classification} &RGB &PyTorch&\\\\\n\t\t\t\t\t&ExCNet.~  &ZSL &\n\t\t\t\t\t\\tabincell{c}{fully connected layers} &energy minimization loss &real images\n\t\t\t\t\t&$IE_{ps}D$  &\\tabincell{c}{User Study\\\\CDIQA LOD} &RGB &PyTorch&\\\\\n\t\t\t\t\t\\hline\n\t\t\t\t\t\\hline\n\t\t\t\t\t\\multirow{1}{*}{\\rotatebox{90}{\\textbf{2020}}}\n\t\t\t\t\t&Zero-DCE~ &ZSL &U-Net like network &\\tabincell{c}{spatial consistency loss\\\\exposure control loss\\\\color constancy loss\\\\illumination smoothness loss}\n\t\t\t\t\t&SICE &\\tabincell{c}{SICE NPE\\\\LIME MEF\\\\DICM VV\\\\ DARK FACE} &\\tabincell{c}{User Study PI\\\\PNSR SSIM\\\\MAE Runtime\\\\Face detection} &RGB &PyTorch &\\\\\n\t\t\t\t\t&DRBN~  &SSL &recursive network &\\tabincell{c}{SSIM loss perceptual loss\\\\adversarial loss}\n\t\t\t\t\t&\\tabincell{c}{LOL\\\\images selected by MOS} &LOL &\\tabincell{c}{PSNR SSIM\\\\SSIM-GC} &RGB &PyTorch &\\\\\n\t\t\t\t\t&Lv et al.~&SL &U-Net like network &\\tabincell{c}{Huber loss\\\\SSIM loss\\\\perceptual loss\\\\illumination smoothness loss}\n\t\t\t\t\t&\\tabincell{c}{simulated by a\\\\ retouching module} &\\tabincell{c}{LOL SICE \\\\DeepUPE} &\\tabincell{c}{User Study PSNR\\\\SSIM VIF\\\\LOE NIQE\\\\ \\#P Runtime\\\\Face detection} &RGB&\\tabincell{c}{TensorFlow}&\\checkmark\\\\\n\t\t\t\t\t&Fan et al.~ &SL &\\tabincell{c}{four subnetworks\\\\U-Net like network\\\\feature modulation} &\\tabincell{c}{mutual smoothness loss\\\\reconstruction loss\\\\illumination smoothness loss\\\\cross entropy loss\\\\ consistency loss\\\\SSIM loss\\\\gradient loss\\\\ratio learning loss}\n\t\t\t\t\t&\\tabincell{c}{simulated by \\\\illumination adjustment, \\\\slight color distortion, \\\\and noise simulation} &\\tabincell{c}{simulated\\\\self-selected} &\\tabincell{c}{PSNR SSIM\\\\NIQE} &RGB & - &\\checkmark\\\\\n\t\t\t\t\t&Xu et al.~&SL &\\tabincell{c}{frequency decomposition\\\\U-Net like network} &\\tabincell{c}{$L_{2}$ loss\\\\perceptual loss}\n\t\t\t\t\t&SID in RGB&\\tabincell{c}{SID in RGB \\\\self-selected} &\\tabincell{c}{PSNR SSIM} &RGB &PyTorch&\\\\\n\t\t\t\t\t&EEMEFN~&SL &\\tabincell{c}{U-Net like network\\\\edge detection network} & \\tabincell{c}{$L_{1}$ loss\\\\weighted cross-entropy loss}\n\t\t\t\t\t&SID &SID &\\tabincell{c}{PSNR SSIM} &raw& \\tabincell{c}{TensorFlow\\\\PaddlePaddle}&\\\\\n\t\t\t\t\t&DLN~&SL &\\tabincell{c}{residual learning\\\\interactive factor\\\\back projection network} &\\tabincell{c}{SSIM loss\\\\total variation loss}\n\t\t\t\t\t&\\tabincell{c}{simulated by \\\\illumination adjustment, \\\\slight color distortion,\\\\and noise simulation} &\\tabincell{c}{simulated \\\\LOL} &\\tabincell{c}{User Study PSNR\\\\SSIM NIQE} &RGB &PyTorch&\\\\\n\t\t\t\t\t&LPNet~&SL &\\tabincell{c}{pyramid network} &\\tabincell{c}{$L_{1}$ loss\\\\perceptual loss\\\\luminance loss}\n\t\t\t\t\t&\\tabincell{c}{LOL SID in RGB\\\\MIT-Adobe FiveK} &\\tabincell{c}{LOL SID in RGB\\\\MIT-Adobe FiveK\\\\ MEF NPE DICM VV} &\\tabincell{c}{PSNR SSIM\\\\NIQE \\#P\\\\FLOPs Runtime} &RGB &PyTorch&\\\\\n\t\t\t\t\t&SIDGAN~ &SL &U-Net &CycleGAN loss\n\t\t\t\t\t&SIDGAN &SIDGAN &\\tabincell{c}{PSNR SSIM\\\\TPSNR TSSIM ATWE}  &raw &\\tabincell{c}{TensorFlow}&\\\\\n\t\t\t\t\t&RRDNet &ZSL&three subnetworks &\\tabincell{c}{retinex reconstruction loss\\\\texture enhancement loss\\\\noise estimation loss}\n\t\t\t\t\t&-&\\tabincell{c}{NPE LIME\\\\MEF DICM} &\\tabincell{c}{NIQE CPCQI} &RGB&PyTorch &\\checkmark\\\\\n\t\t\t\t\t&TBEFN~&SL&\\tabincell{c}{three stages\\\\U-Net like network} &\\tabincell{c}{SSIM loss\\\\perceptual loss\\\\smoothness loss}\n\t\t\t\t\t&\\tabincell{c}{SCIE \\\\LOL} &\\tabincell{c}{SCIE LOL\\\\DICM MEF\\\\NPE VV} &\\tabincell{c}{PSNR SSIM\\\\NIQE Runtime\\\\ \\#P FLOPs} &RGB &TensorFlow&\\checkmark\\\\\n\t\t\t\t\t&DSLR~&SL&\\tabincell{c}{Laplacian pyramid\\\\U-Net like network} &\\tabincell{c}{$L_{2}$ loss\\\\Laplacian loss\\\\color loss}\n\t\t\t\t\t&MIT-Adobe FiveK &\\tabincell{c}{MIT-Adobe FiveK\\\\self-selected} &\\tabincell{c}{PSNR SSIM\\\\NIQMC NIQE\\\\BTMQI CaHDC} &RGB& PyTorch&\\\\\n\t\t\t\t\t\\hline\n\t\t\t\t\t\\hline\n\t\t\t\t\t\\multirow{1}{*}{\\rotatebox{90}{\\textbf{2021}}}\n\t\t\t\t\t&RUAS~  &ZSL &neural architecture search &\\tabincell{c}{cooperative loss\\\\similar loss\\\\total variation loss}\n\t\t\t\t\t&\\tabincell{c}{LOL\\\\MIT-Adobe FiveK} &\\tabincell{c}{LOL\\\\MIT-Adobe FiveK} &\\tabincell{c}{PSNR SSIM\\\\ Runtime \\#P FLOPs} &RGB &PyTorch &\\checkmark\\\\\n\t\t\t\t\t&Zhang et al.~&SL &U-Net &\\tabincell{c}{$L_{1}$ loss\\\\ consistency loss}\n\t\t\t\t\t&\\tabincell{c}{simulated by illumination  \\\\adjustmentand noise simulation} &\\tabincell{c}{simulated \\\\self-selected} &\\tabincell{c}{User Study PSNR\\\\SSIM AB\\\\MABD WE} &RGB&\\tabincell{c}{PyTorch}&\\\\\n\t\t\t\t\t&Zero-DCE++~ &ZSL &U-Net like network &\\tabincell{c}{spatial consistency loss\\\\exposure control loss\\\\color constancy loss\\\\illumination smoothness loss}\n\t\t\t\t\t&SICE &\\tabincell{c}{SICE NPE\\\\LIME MEF\\\\DICM VV\\\\ DARK FACE} &\\tabincell{c}{User Study PI\\\\PNSR SSIM \\#P\\\\MAE Runtime\\\\Face detection FLOPs} &RGB &PyTorch &\\\\\n\t\t\t\t\t&DRBN~  &SSL &recursive network &\\tabincell{c}{perceptual loss\\\\detail loss quality loss}\n\t\t\t\t\t&LOL &LOL &\\tabincell{c}{PSNR SSIM\\\\SSIM-GC} &RGB &PyTorch &\\\\\n\t\t\t\t\t&Retinex-Net~ &SL &\n\t\t\t\t\tthree subnetworks &\\tabincell{c}{$L_{1}$ loss $L_{2}$ loss\\\\SSIM loss\\\\total variation loss}\n\t\t\t\t\t&\\tabincell{c}{LOL\\\\simulated by adjusting histogram} &\\tabincell{c}{LOL simulated\\\\NPE DICM VV} &\\tabincell{c}{PNSR SSIM\\\\UQI OSS User Study} &RGB &PyTorch&\\checkmark\\\\\n\t\t\t\t\t&RetinexDIP &ZSL & encoder-decoder networks &\\tabincell{c}{reconstruction loss\\\\illumination-consistency loss\\\\reflectnce loss \\\\illumination smoothness loss}&- &\\tabincell{c}{DICM, ExDark\\\\Fusion LIME\\\\ NASA NPE VV} &\\tabincell{c}{NIQE \\\\NIQMC CPCQI}  &RGB &PyTorch&\\checkmark\\\\\n\t\t\t\t\t&PRIEN~ &SL &recursive network &SSIM loss&\\tabincell{c}{MEF LOL\\\\simulated by adjusting histogram} &\\tabincell{c}{LOL LIME\\\\NPE MEF VV} &\\tabincell{c}{PNSR SSIM\\\\LOE TMQI} &RGB &PyTorch &\\\\\n\t\t\t\t\t\\hline\n\t\t\t\t\\end{tabular}\n\t\t\t}\n\t\t\\end{threeparttable}\n\t\\end{table*}", "cites": [1875], "cite_extract_rate": 1.0, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section organizes LLIE methods by learning strategies and briefly describes representative papers, showing basic synthesis. However, it primarily functions as a summary of individual approaches without deep comparative analysis or meta-level abstraction. Some limitations, such as poor generalization due to synthetic data, are mentioned, but the critique remains shallow."}}
{"id": "b98d1366-433d-4a92-8dac-6386ad39398e", "title": "Training Datasets", "level": "subsection", "subsections": [], "parent_id": "3ad6854d-28fe-41e8-80bd-198011ce8b19", "prefix_titles": [["title", "Low-Light Image and Video Enhancement \\\\Using Deep Learning: A Survey"], ["section", "Technical Review and Discussion"], ["subsection", "Training Datasets"]], "content": "Figure \\ref{fig:statistic}(f) reports the usage of a variety of paired training datasets for training low-light enhancement networks. These datasets include real-world captured datasets and synthetic datasets. We list them in Table \\ref{table:training}. \n\\noindent\n\\textbf{Simulated by Gamma Correction.}\nOwing to its nonlinearity and simplicity, \nGamma correction is used to adjust the luminance or tristimulus values in video or still image systems. It is defined by a power-law expression:\n\\begin{equation}\n\t\\label{equ_12}\n\tV_{\\text{out}}=AV_{\\text{in}}^{\\gamma},\n\\end{equation}\nwhere the input $V_{\\text{in}}$ and output $V_{\\text{out}}$ are typically in the range of [0,1]. The constant $A$ is set to 1 in the common case. The power $\\gamma$ controls the luminance of the output. Intuitively, the input is brightened when  $\\gamma<$1 while the input is darkened when $\\gamma>$1. The input can be the three RGB channels of an image or the luminance-related channels such as $L$ channel in the CIELab color space and $Y$ channel in the YCbCr color space. After adjusting the luminance-related channel using Gamma correction, the corresponding channels in the color space are adjusted by equal proportion to avoid producing artifacts and color deviations. \nTo simulate images taken in real-world low-light scenes,  Gaussian noise, Poisson noise, or realistic noise is added to the Gamma corrected images. The low-light image synthesized using Gamma correction can be expressed as:\n\\begin{equation}\n\t\\label{equ_13}\n\tI_{\\text{low}}=n(g(I_{\\text{in}};\\gamma)),\n\\end{equation}\nwhere $n$ represents the noise model, $g(I_{\\text{in}};\\gamma)$ represents the Gamma correction function with Gamma value $\\gamma$, $I_{\\text{in}}$ is a normal-light and high-quality image or luminance-related channel. Although this function produces low-light images of different lighting levels by changing the Gamma value $\\gamma$, \nit tends to introduce artifacts and color deviations into the synthetic low-light images due to the nonlinear adjustment. \n\\begin{table}\n\t\\centering\n\t\\setlength\\tabcolsep{5pt}\n\t\\caption{\n\t\t{Summary of paired training datasets. `Syn' represents Synthetic.}\n\t}\n\t\\vspace{-6pt}\n\t\\label{table:training}\n\t\\begin{tabular}{r|c|c|c|c}\n\t\t\\hline\n\t\t\\textbf{Name}~~~~~~~~~~~~~~~~~~~~~&\\textbf{Number}& \n\t\t\\textbf{Format} & \\textbf{Real/Syn}&\\textbf{Video} \\\\\n\t\t\\hline\n\t\tGamma Correction &+$\\infty$  &RGB \n\t\t&Syn & \\\\\n\t\tRandom Illumination &+$\\infty$  &RGB \n\t\t&Syn & \\\\\n\t\t\\hline\n\t\tLOL  &500  &RGB \n\t\t&Real  &\\\\\n\t\tSCIE  &4,413  &RGB \n\t\t&Real &\\\\\n\t\tVE-LOL-L   &2,500  &RGB \n\t\t&Real+Syn  &\\\\\n\t\tMIT-Adobe FiveK   &5,000  &raw \n\t\t&Real  &\\\\\n\t\tSID   &5,094  &raw \n\t\t&Real  &\\\\\n\t\tDRV   &202  &raw \n\t\t&Real  &\\checkmark\\\\\n\t\tSMOID   &179  &raw \n\t\t&Real  &\\checkmark\\\\\n\t\t\\hline\n\t\\end{tabular}\n\t\\vspace{-4pt}\n\\end{table}\n\\noindent\n\\textbf{Simulated by Random Illumination.}\nAccording to the Retinex model, an image can be decomposed into a reflectance component and an illumination component. Assuming image content is independent of illumination component and local region in the illumination component have the same intensity, a low-light image can be obtained by \n\\begin{equation}\n\t\\label{equ_14}\n\tI_{\\text{low}}=I_{\\text{in}}L,\n\\end{equation}\nwhere $L$ is a random illumination value in the range of [0,1]. Noises can be added to the synthetic image. Such a linear function avoids artifacts, but the strong assumption requires the synthesis to operate only on image patches where local regions have the same brightness. A deep model trained on such image patches may lead to sub-optimal performance due to the negligence of context information.\n\\noindent\n\\textbf{LOL.}\nLOL  is the first paired low-/normal-light image dataset taken in real scenes. The low-light images are collected by changing the exposure time and ISO. LOL contains 500 pairs of low-/normal-light images of size 400$\\times$600 saved in RGB format. \n\\noindent\n\\textbf{SCIE.}\nSCIE is a multi-exposure image dataset of low-contrast and good-contrast image pairs. It includes multi-exposure sequences of 589 indoor and outdoor scenes. Each sequence has 3 to 18 low-contrast images of different exposure levels, thus containing 4,413 multi-exposure images in total. The 589 high-quality reference images are obtained by selecting from the results of 13 representative enhancement algorithms. That is many multi-exposure images have the same high-contrast reference image.\nThe image resolutions are between 3,000$\\times$2,000 and \n6,000$\\times$4,000. The images in SCIE are saved in RGB format.\n\\noindent\n\\textbf{MIT-Adobe FiveK.}\nMIT-Adobe FiveK  was collected for global tone adjustment but has been used in LLIE. This is because the input images have low light and low contrast. MIT-Adobe FiveK contains 5,000 images, each of which is retouched by 5 trained photographers towards visually pleasing renditions, akin to a postcard.  The images are all in raw format. To train the networks that can handle images of RGB format, one needs to use Adobe Lightroom to pre-process the images and save them as RGB format following a dedicated pipeline\\footnote{\\url{https://github.com/nothinglo/Deep-Photo-Enhancer/issues/38\\#issuecomment-449786636}}.\nThe images are commonly resized to have a long edge of 500 pixels.\n\\noindent\n\\textbf{SID.}\nSID  contains 5,094 raw short-exposure images, each with a corresponding long-exposure reference image. The number of distinct long-exposure reference images is 424. In other words, multiple short-exposure images correspond to the same long-exposure reference image. \nThe images were taken using two cameras: Sony $\\alpha$7S II and Fujifilm X-T2 in both indoor and outdoor scenes. Thus, the images have different sensor patterns (Sony camera' Bayer sensor and Fuji camera's APS-C X-Trans sensor). \nThe resolution is 4,240$\\times$2,832 for Sony and 6,000$\\times$4,000 for Fuji. Usually, the long-exposure images are processed by libraw (a raw image processing library) and saved in the RGB color space, and randomly cropped 512$\\times$512 patches for training.\n\\noindent\n\\textbf{VE-LOL.}  VE-LOL  consists of two subsets: paired VE-LOL-L that is used for training and evaluating LLIE methods and unpaired VE-LOL-H that is used for evaluating the effect of LLIE methods on face detection. Specifically, VE-LOL-L includes 2,500 paired images. Among them, 1,000 pairs are synthetic, while 1,500 pairs are real. VE-LOL-H includes 10,940 unpaired images, where human faces are manually annotated with bounding boxes.  \n\\noindent\n\\textbf{DRV.}\nDRV  contains 202 static raw videos, each of which has a corresponding long-exposure ground truth. Each video was taken at approximately 16 to 18 frames per second in a continuous shooting mode and is with up to 110 frames. The images were taken by a Sony RX100 VI camera in both indoor and outdoor scenes, thus all in  raw format of Bayer pattern. The resolution is 3,672$\\times$5,496. \n\\noindent\n\\textbf{SMOID.}\nSMOID  contains 179 pairs of videos taken by a co-axis optical system, each of which has 200 frames. Thus, SMOID includes 35,800 extremely low light raw data of Bayer pattern and their corresponding well-lightened RGB counterparts. SMOID consists of moving vehicles and pedestrians under different illumination conditions.\nSome issues challenge the aforementioned paired training datasets: \n\\textbf{1)} deep models trained on synthetic data may introduce artifacts and color deviations when processing real-world images and videos due to the gap between synthetic data and real data, \n\\textbf{2)} the scale and diversity of real training data are unsatisfactory, thus some methods incorporate synthetic data to augment the training data. This may lead to sub-optimal enhancement, and \n\\textbf{3)} the input images and corresponding ground truths may exist misalignment due to the effects of motion, hardware, and environment. This would affect the performance of deep networks trained using pixel-wise loss functions.", "cites": [1875], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes information about training datasets by categorizing them into simulated and real-world data, and it connects the technical principles of Gamma correction and random illumination. It also critically evaluates the limitations of synthetic data, such as artifacts and misalignment, and highlights the challenges of real-world datasets in terms of scale and diversity. While it provides a coherent framework, its abstraction is moderate, focusing primarily on dataset-level characteristics rather than broader theoretical implications."}}
{"id": "e339615c-7f2d-4a6a-95fe-00d1129c19c1", "title": "Testing Datasets", "level": "subsection", "subsections": [], "parent_id": "3ad6854d-28fe-41e8-80bd-198011ce8b19", "prefix_titles": [["title", "Low-Light Image and Video Enhancement \\\\Using Deep Learning: A Survey"], ["section", "Technical Review and Discussion"], ["subsection", "Testing Datasets"]], "content": "In addition to the testing subsets in the paired datasets , there are several testing data collected from related works or commonly used for experimental comparisons. \nBesides, some datasets such as face detection in the dark  and detection and recognition in low-light images  are employed  to test the effects of LLIE on high-level visual tasks. We summarize the commonly used testing datasets in Table \\ref{table:testing} and introduce the representative testing datasets as follows. \n\\begin{table}\n\t\\centering\n\t\\caption{\n\t\t{Summary of testing datasets.}\n\t}\n\t\\vspace{-6pt}\n\t\\label{table:testing}\n\t\\begin{tabular}{r|c|c|c|c}\n\t\t\\hline\n\t\t\\textbf{Name}~~~~~~~~~&\\textbf{Number}& \n\t\t\\textbf{Format} & \\textbf{Application}&\\textbf{Video} \\\\\n\t\t\\hline\n\t\tLIME  &10 &RGB & & \\\\\n\t\tNPE  &84  &RGB & & \\\\\n\t\tMEF  &17  &RGB  &  &\\\\\n\t\tDICM   &64  &RGB & &\\\\\n\t\tVV\\tablefootnote{\\url{https://sites.google.com/site/vonikakis/datasets}} &24  &RGB &  &\\\\\n\t\t\\hline\n\t\tBBD-100K    &10,000  &RGB &\\checkmark  &\\checkmark\\\\\n\t\tExDARK  &7,363  &RGB  &\\checkmark  &\\\\\n\t\tDARK FACE   &6,000  &RGB  &\\checkmark  &\\\\\n\t\tVE-LOL-H   &10,940  &RGB &\\checkmark  &\\\\\n\t\t\\hline\n\t\\end{tabular}\n\t\\vspace{-4pt}\n\\end{table}\n\\noindent\n\\textbf{BBD-100K.}\nBBD-100K  is the largest driving video dataset with 10,000 videos taken over 1,100-hour driving experience across many different times in the day, weather conditions, and driving scenarios, and 10 tasks annotations. The videos taken at nighttime in BBD-100K are used to validate the effects of LLIE on high-level visual tasks and the enhancement performance in real scenarios.\n\\noindent\n\\textbf{ExDARK.}\nExDARK~ dataset is built for object detection and recognition in low-light images. ExDARK dataset contains 7,363 low-light images from extremely low-light environments to twilight with 12 object classes annotated with image class labels and local object bounding boxes.\n\\noindent\n\\textbf{DARK FACE.}\nDARK FACE  dataset contains 6,000 low-light images captured during the nighttime, each of which is labeled with bounding boxes of the human face.\nFrom Figure \\ref{table:testing}(g) and Table \\ref{table:methods}, we can observe that one prefers using the self-collected testing data in the experiments. The main reasons lie into three-fold: \n\\textbf{1)} besides the test partition of paired datasets, there is no acknowledged benchmark for evaluations, \\textbf{2)}\nthe commonly used test sets suffer from some shortcomings such as small scale (some test sets contain 10 images only), repeated content and illumination properties, and unknown experimental settings, and\\textbf{ 3)} some of the commonly used testing data are not originally collected for evaluating LLIE. In general, current testing datasets may lead to bias and unfair comparisons.", "cites": [1875], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from cited papers and discusses the use of various testing datasets for LLIE, highlighting their applications and limitations. It provides a critical perspective by identifying issues such as lack of benchmarks, small scale, and biased selection criteria. While it offers some general insights into the challenges of evaluating LLIE methods, it does not fully abstract broader principles or present a novel framework, keeping the analysis at a moderate level."}}
{"id": "62b1377f-b6e1-4398-be9f-a1fa2952056b", "title": "Benchmarking and Empirical Analysis", "level": "section", "subsections": ["d8e5b8c1-ee06-4aad-800d-232c4dd98922", "7ee6e01b-208a-4fef-a9a1-3cc6a973f6c4", "67f70829-d734-4fe0-baa6-b7f64f6f5b82", "e142566f-157a-4609-a670-8df6dbfe9874", "8ea7b52c-e5f5-46c9-8ff5-69763784bc99"], "parent_id": "43aba542-7f6b-493f-9beb-9ae4164ca84b", "prefix_titles": [["title", "Low-Light Image and Video Enhancement \\\\Using Deep Learning: A Survey"], ["section", "Benchmarking and Empirical Analysis"]], "content": "\\label{sec:evaluation}\nThis section provides empirical analysis and highlights some key challenges in deep learning-based LLIE. To facilitate the analysis, we propose a  low-light image and video dataset to examine the performance of different solutions. We also develop the first online platform, where the results of LLIE models can be produced via a user-friendly web interface. In this section, we conduct extensive evaluations on several benchmarks and our proposed dataset. \nIn the experiments, we compare 13 representative RGB format-based methods, including eight supervised learning-based methods (LLNet , LightenNet ,  Retinex-Net , MBLLEN ,  KinD ,  KinD++ , TBEFN , DSLR ), one unsupervised learning-based method (EnlightenGAN ), one semi-supervised learning-based method (DRBN ), and three zero-shot learning-based methods (ExCNet , Zero-DCE , RRDNet ). Besides, we also compare two raw format-based methods, including SID  and EEMEFN . Note that RGB format-based methods dominate LLIE. Moreover, most raw format-based methods do not release their code. Thus, we choose two representative methods to provide empirical analysis and insights. For all compared methods, we use the publicly available code to produce their results for fair comparisons. \n\\begin{table}\n\t\\centering\n\t\\caption{{Summary of LLIV-Phone dataset.  LLIV-Phone dataset contains 120 videos (45,148 images) taken by 18 different mobile phones' cameras. ``\\#Video'' and  ``\\#Image'' represent the number of videos and images, respectively.}}\n\t\\vspace{-6pt}\n\t\\label{table:LLIVPhone}\n\t\\begin{tabular}{r|c|c|c}\n\t\t\\hline\n\t\t\\textbf{Phone's Brand}&\\textbf{\\#Video}& \n\t\t\\textbf{\\#Image} &\\textbf{Resolution}\\\\\n\t\t\\hline\n\t\tiPhone 6s &4 &1,029 &1920$\\times$1080\\\\\n\t\tiPhone 7 &13 &6,081 &1920$\\times$1080\\\\\n\t\tiPhone7 Plus &2 &900 &1920$\\times$1080\\\\\n\t\tiPhone8 Plus &1 &489 &1280$\\times$720\\\\\n\t\tiPhone 11 &7 &2,200 &1920$\\times$1080\\\\\n\t\tiPhone 11 Pro &17 &7,739 &1920$\\times$1080\\\\\n\t\tiPhone XS &11 &2,470 &1920$\\times$1080\\\\\n\t\tiPhone XR &16 &4,997 &1920$\\times$1080\\\\\n\t\tiPhone SE &1 &455 &1920$\\times$1080\\\\\n\t\tXiaomi Mi 9 &2 &1,145 &1920$\\times$1080\\\\\n\t\tXiaomi Mi Mix 3 &6 &2,972 &1920$\\times$1080\\\\\n\t\tPixel 3&4 &1,311 &1920$\\times$1080\\\\\n\t\tPixel 4&3 &1,923 &1920$\\times$1080\\\\\n\t\tOppo R17 &6 &2,126 &1920$\\times$1080\\\\\n\t\tVivo Nex &12 &4,097 &1280$\\times$720\\\\\n\t\tLG M322&2 &761 &1920$\\times$1080\\\\\n\t\tOnePlus 5T &1 &293 &1920$\\times$1080\\\\\n\t\tHuawei Mate 20 Pro & 12&4,160 &1920$\\times$1080\\\\\n\t\t\\hline\n\t\\end{tabular}\n\t\\vspace{-4pt}\n\\end{table}", "cites": [1875], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the empirical setup and dataset used for benchmarking deep learning-based LLIE methods. While it lists and categorizes the methods, it lacks in-depth synthesis of their contributions, critical evaluation of their strengths and weaknesses, and broader abstraction to identify underlying trends or principles in the field."}}
{"id": "d8e5b8c1-ee06-4aad-800d-232c4dd98922", "title": "A New Low-Light Image and Video Dataset", "level": "subsection", "subsections": [], "parent_id": "62b1377f-b6e1-4398-be9f-a1fa2952056b", "prefix_titles": [["title", "Low-Light Image and Video Enhancement \\\\Using Deep Learning: A Survey"], ["section", "Benchmarking and Empirical Analysis"], ["subsection", "A New Low-Light Image and Video Dataset"]], "content": "We propose a  Low-Light Image and Video dataset, called LLIV-Phone, to comprehensively and thoroughly validate the performance of LLIE methods. LLIV-Phone is the largest and most challenging real-world testing dataset of its kind. In particular, the dataset contains 120 videos (45,148 images) taken by 18 different mobile phones' cameras including iPhone 6s, iPhone 7, iPhone7 Plus, iPhone8 Plus, iPhone 11, iPhone 11 Pro, iPhone XS, iPhone XR, iPhone SE, Xiaomi Mi 9, Xiaomi Mi Mix 3, Pixel 3, Pixel 4,  Oppo R17, Vivo Nex, LG M322, OnePlus 5T, Huawei Mate 20 Pro under diverse illumination conditions (e.g., weak lighting, underexposure, moonlight, twilight, dark, extremely dark, back-lit, non-uniform light, and colored light.) in both indoor and outdoor scenes.  A summary of the LLIV-Phone dataset is provided in Table \\ref{table:LLIVPhone}. \nWe present several samples of LLIV-Phone dataset in Figure \\ref{fig:dataset_samples}. \nThe LLIV-Phone dataset is available at the project page. \nThis challenging dataset is collected in real scenes and contains diverse low-light images and videos. Consequently, it is suitable for evaluating the generalization capability of different low-light image and video enhancement models. Notably, the dataset can be used as the training dataset for unsupervised learning and the reference dataset for synthesis methods to generate realistic low-light data. \n\\begin{figure}[!t]\n\t\\centering  \\centerline{\\includegraphics[width=1\\linewidth]{figures/dataset_samples.png}}\n\t\\vspace{-2pt}\n\t\\caption{Several images sampled from the proposed LLIV-Phone dataset. The  images and videos are taken by different devices under diverse lighting conditions and scenes.}\n\t\\label{fig:dataset_samples}\n\t\\vspace{-4pt}\n\\end{figure}\n\\begin{figure*} [!t]\n\t\\begin{center}\n\t\t\\begin{tabular}{c@{ }c@{ }c@{ }c@{ }c@{ }c@{ }c}\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/LOL_input.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/LOL_LLNet.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/LOL_LightenNet.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/LOL_RetinexNet.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/LOL_MBLLEN.png}\\\\\n\t\t\t(a) input  & (b) LLNet  & (c) LightenNet  &  (d)  Retinex-Net   & (e) \tMBLLEN   \\\\\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/LOL_KinD.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/LOL_KinD++.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/LOL_TBEFN.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/LOL_DSLR.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/LOL_EnlightenGAN.png}\\\\\n\t\t\t(f) KinD   & (g) KinD++  & (h) TBEFN  &  (i)  \tDSLR   & (j) EnlightenGAN   \\\\\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/LOL_DRBN.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/LOL_ExCNet.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/LOL_ZeroDCE.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/LOL_RRDNet.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/LOL_GT.png}\\\\\n\t\t\t(k) DRBN   & (l)\tExCNet  & (m) Zero-DCE  &  (n) \tRRDNet   & (o) GT  \\\\\n\t\t\\end{tabular}\n\t\\end{center}\n\t\\vspace{-2pt}\n\t\\caption{Visual results of different methods on a low-light image sampled from LOL-test dataset.}\n\t\\label{fig:LOL}\n\t\\vspace{-4pt}\n\\end{figure*}\n\\begin{figure*}[!t]\n\t\\begin{center}\n\t\t\\begin{tabular}{c@{ }c@{ }c@{ }c@{ }c@{ }c@{ }c}\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/5K_input.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/5K_LLNet.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/5K_LightenNet.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/5K_RetinexNet.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/5K_MBLLEN.png}\\\\\n\t\t\t(a) input  & (b) LLNet  & (c) LightenNet  &  (d)  Retinex-Net   & (e) \tMBLLEN   \\\\\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/5K_KinD.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/5K_KinD++.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/5K_TBEFN.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/5K_DSLR.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/5K_EnlightenGAN.png}\\\\\n\t\t\t(f) KinD   & (g) KinD++  & (h) TBEFN  &  (i)  \tDSLR   & (j) EnlightenGAN   \\\\\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/5K_DRBN.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/5K_ExCNet.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/5K_ZeroDCE.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/5K_RRDNet.png}&\n\t\t\t\\includegraphics[width=0.18\\linewidth]{figures/5K_GT.png}\\\\\n\t\t\t(k) DRBN   & (l)\tExCNet  & (m) Zero-DCE  &  (n) \tRRDNet   & (o) GT  \\\\\n\t\t\\end{tabular}\n\t\\end{center}\n\t\\vspace{-2pt}\n\t\\caption{Visual results of different methods on a low-light image sampled from MIT-Adobe FiveK-test dataset.}\n\t\\label{fig:5K}\n\t\\vspace{-4pt}\n\\end{figure*}", "cites": [1875], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section primarily describes the LLIV-Phone dataset, including its size, device diversity, and illumination conditions. It lacks synthesis of the cited paper (e.g., Zero-DCE), critical evaluation of the methods, or abstraction to broader patterns in low-light enhancement research. The inclusion of visual results does not add analytical depth."}}
{"id": "7ee6e01b-208a-4fef-a9a1-3cc6a973f6c4", "title": "Online Evaluation Platform", "level": "subsection", "subsections": [], "parent_id": "62b1377f-b6e1-4398-be9f-a1fa2952056b", "prefix_titles": [["title", "Low-Light Image and Video Enhancement \\\\Using Deep Learning: A Survey"], ["section", "Benchmarking and Empirical Analysis"], ["subsection", "Online Evaluation Platform"]], "content": "Different deep models may be implemented in different platforms such as Caffe, Theano, TensorFlow, and PyTorch. As a result, different algorithms demand different configurations, GPU versions, and hardware specifications. Such requirements are prohibitive to many researchers, especially for beginners who are new to this area and may not even have GPU resources. To resolve these problems, we develop an LLIE online platform, called LLIE-Platform, which is available at \\url{http://mc.nankai.edu.cn/ll/}. \nTo the date of this submission, the LLIE-Platform covers 14 popular deep learning-based LLIE methods including LLNet , LightenNet , Retinex-Net , EnlightenGAN , MBLLEN , KinD , KinD++ , TBEFN , DSLR , DRBN , ExCNet , Zero-DCE , Zero-DCE++ , and  RRDNet ,  where the results of any input can be produced through a user-friendly web interface.  We will regularly offer new methods on this platform.\nWe wish that this LLIE-Platform could serve the\ngrowing research community by providing users a flexible interface\nto run existing deep learning-based LLIE methods and develop their own new LLIE methods. \n\\newcommand{\\addImg}[1]{\\includegraphics[width=0.14\\linewidth,height=0.2\\linewidth]{figures/#1}}\n\\begin{figure*}[!t]\n\t\\centering\n\t\\setlength\\tabcolsep{0.5pt}\n\t\\begin{tabular}{ccccccccc}\n\t\t\\addImg{Phone_input1.png} &\n\t\t\\addImg{Phone_LLNet1.png}&\n\t\t\\addImg{Phone_LightenNet1.png}&\n\t\t\\addImg{Phone_RetinexNet1.png}&\n\t\t\\addImg{Phone_MBLLEN1.png}&\n\t\t\\addImg{Phone_KinD1.png}&\n\t\t\\addImg{Phone_KinD++1.png}&\\\\\n\t\t\t(a)   & (b)  & (c)  &  (d)   & (e) & (f)   & (g)   \\\\\n\t\t\\addImg{Phone_TBEFN1.png}&\n\t\t\\addImg{Phone_DSLR1.png}&\n\t\t\\addImg{Phone_EnlightenGAN1.png}&\n\t\t\\addImg{Phone_DRBN1.png}&\n\t\t\\addImg{Phone_ExCNet1.png}&\n\t\t\\addImg{Phone_ZeroDCE1.png}&\n\t\t\\addImg{Phone_RRDNet1.png}\\\\\n\t\t\t(h) &  (i)  \t  & (j)  & \t(k)  & (l) & (m)  &  (n) \t \\\\\n\t\\end{tabular}\n\t\\vspace{-12pt}\n\t\\caption{Visual results of different methods on a low-light image sampled from  LLIV-Phone-imgT dataset. (a) input. (b) LLNet . (c) LightenNet . (d)  Retinex-Net . (e) \tMBLLEN . (f) KinD . (g) KinD++ . (h) TBEFN . (i)  \tDSLR . (j) EnlightenGAN .\t(k) DRBN . (l)\tExCNet . (m) Zero-DCE . (n) \tRRDNet .}\n\t\\label{fig:Phone1}\n\t\\vspace{-4pt}\n\\end{figure*}\n\\begin{figure*}[!t]\n\t\\setlength\\tabcolsep{0.5pt}\n\t\\centering\n\t\\begin{tabular}{ccccccccc}\n\t\t\\addImg{Phone_input2.png}&\n\t\t\\addImg{Phone_LLNet2.png}&\n\t\t\\addImg{Phone_LightenNet2.png}&\n\t\t\\addImg{Phone_RetinexNet2.png}&\n\t\t\\addImg{Phone_MBLLEN2.png}&\n\t\t\\addImg{Phone_KinD2.png}&\n\t\t\\addImg{Phone_KinD++2.png}&\\\\\n\t\t\t(a)   & (b) & (c) &  (d)   & (e) \t & (f)  & (g)   \\\\\n\t\t\\addImg{Phone_TBEFN2.png}&\n\t\t\\addImg{Phone_DSLR2.png}&\n\t\t\\addImg{Phone_EnlightenGAN2.png}&\n\t\t\\addImg{Phone_DRBN2.png}&\n\t\t\\addImg{Phone_ExCNet2.png}&\n\t\t\\addImg{Phone_ZeroDCE2.png}&\n\t\t\\addImg{Phone_RRDNet2.png}\\\\\n\t\t\t(h)  &  (i)  \t  & (j) & \t(k)  & (l)\t & (m)  &  (n) \t \\\\\n\t\\end{tabular}\n\t\\vspace{-12pt}\n\t\\caption{Visual results of different methods on a low-light image sampled from  LLIV-Phone-imgT dataset. (a) input. (b) LLNet . (c) LightenNet . (d)  Retinex-Net . (e) \tMBLLEN . (f) KinD . (g) KinD++ . (h) TBEFN . (i)  \tDSLR . (j) EnlightenGAN .\t(k) DRBN . (l)\tExCNet . (m) Zero-DCE . (n) \tRRDNet .}\n\t\\label{fig:Phone2}\n\t\\vspace{-4pt}\n\\end{figure*}\n\\begin{figure*}[!t]\n\t\\begin{center}\n\t\t\\begin{tabular}{c@{ }c@{ }c@{ }c@{ }c@{ }}\n\t\t\t\\rotatebox{90}{~~~~~~~~~Bayer~~~~~~~~~~~~APS-C X-Trans}~&\n\t\t\t\\includegraphics[width=0.21\\linewidth]{figures/raw_input.png}&~~\n\t\t\t\\includegraphics[width=0.21\\linewidth]{figures/raw_SID.png}&~~\n\t\t\t\\includegraphics[width=0.21\\linewidth]{figures/raw_EEMEFN.png}&~~\n\t\t\t\\includegraphics[width=0.21\\linewidth]{figures/raw_GT.png}\\\\\n\t\t\t&~\t(a) inputs  &~~ (b) SID  &~~ (c) EEMEFN &~~  (d) GT   \\\\\n\t\t\\end{tabular}\n\t\\end{center}\n\t\\vspace{-2pt}\n\t\\caption{Visual results of different methods on two raw low-light images sampled from  SID-test-Bayer and SID-test-X-Trans test datasets. The inputs are amplified for visualization.}\n\t\\label{fig:raw}\n\t\\vspace{-4pt}\n\\end{figure*}", "cites": [1875], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section primarily describes the development and purpose of an online evaluation platform for LLIE methods, listing 14 implemented approaches. It integrates the cited paper (Zero-DCE) as one of the covered methods but does not synthesize or connect broader ideas from the cited or related works. There is minimal critical evaluation or abstraction beyond the specific tools and systems."}}
{"id": "67f70829-d734-4fe0-baa6-b7f64f6f5b82", "title": "Benchmarking Results", "level": "subsection", "subsections": [], "parent_id": "62b1377f-b6e1-4398-be9f-a1fa2952056b", "prefix_titles": [["title", "Low-Light Image and Video Enhancement \\\\Using Deep Learning: A Survey"], ["section", "Benchmarking and Empirical Analysis"], ["subsection", "Benchmarking Results"]], "content": "To qualitatively and quantitatively evaluate different methods, in addition to the proposed LLIV-Phone dataset, we also adopt the commonly used LOL  and MIT-Adobe FiveK  datasets for RGB format-based methods, and SID  dataset for raw format-based methods. More visual results can be found in the supplementary material. The comparative results on the real low-light videos taken by different mobile phones' cameras can be found at YouTube \\url{https://www.youtube.com/watch?v=Elo9TkrG5Oo&t=6s}.\nWe select five images on average from each video of the LLIV-Phone dataset, forming an image testing dataset with a total of 600 images (denoted as LLIV-Phone-imgT). Furthermore, we randomly select one video from the videos of each phone's brand of LLIV-Phone dataset, forming a video testing dataset with a total of 18 videos (denoted as LLIV-Phone-vidT). We half the resolutions of the frames in both LLIV-Phone-imgT and  LLIV-Phone-vidT because some deep learning-based methods cannot process the full resolution of test images and videos. \nFor the LOL dataset, we adopt the original test set including 15 low-light images captured in real scenes for testing, denoted as LOL-test. For the MIT-Adobe FiveK dataset, we follow the protocol in Chen et al.  to decode the images into PNG format and resize them to have a long edge of 512 pixels using Lightroom. We adopt the same testing dataset as Chen et al.  , MIT-Adobe FiveK-test, including 500 images with the retouching results by expert C as the corresponding ground truths. For the SID dataset, we use the default test set  used in EEMEFN  for fair comparisons, denoted as SID-test (SID-test-Bayer and SID-test-X-Trans), which is a partial test set of SID . The SID-test-Bayer includes 93 images of the Bayer pattern while the SID-test-X-Trans includes 94 images of the APS-C X-Trans pattern. \n\\begin{table*}[t]\n\t\\caption{Quantitative comparisons on LOL-test and MIT-Adobe FiveK-test test datasets in terms of MSE ($\\times10^3$), PSNR (in dB), SSIM , and LPIPS . The best result is in {\\color{red}{red}} whereas the second and third best results are in {\\color{blue}{blue}} and {\\color{purple}{purple}} under each case, respectively.}\n\t\\vspace{-6pt}\n\t\\begin{center}\n\t\t\\begin{tabular}{c|c|c|c|c|c||c|c|c|c}\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Learning}} &\\multirow{2}{*}{\\textbf{Method}} & \\multicolumn{4}{c||}{\\textbf{LOL-test}} & \\multicolumn{4}{c}{\\textbf{MIT-Adobe FiveK-test}} \\\\\n\t\t\t\\cline{3-10}\n\t\t\t\\cline{3-10}\n\t\t\t& &\t\\textbf{MSE}$\\downarrow$ & \\textbf{PSNR} $\\uparrow$ & \\textbf{SSIM}$\\uparrow$ & \\textbf{LPIPS}$\\downarrow$ & \\textbf{MSE}$\\downarrow$ & \\textbf{PSNR} $\\uparrow$ & \\textbf{SSIM}$\\uparrow$ & \\textbf{LPIPS}$\\downarrow$ \\\\\n\t\t\t\\hline\n\t\t\t&input  & 12.613 & 7.773 &0.181 & 0.560& {\\color{blue}{1.670}}& {\\color{blue}{17.824}} & {\\color{purple}{0.779}} &{\\color{blue}{0.148}} \\\\\n\t\t\t\\hline\n\t\t\t&LLNet  & {\\color{red}{1.290}} &  {\\color{red}{17.959}} &0.713 &0.360&4.465 & 12.177 &0.645 &0.292\\\\\n\t\t\t&LightenNet  & 7.614 & 10.301 & 0.402 & 0.394 & 4.127 & 13.579 & 0.744 &{\\color{purple}{0.166}}\\\\\n\t\t\t&Retinex-Net  & 1.651 & 16.774& 0.462 & 0.474 & 4.406 & 12.310 & 0.671 &0.239\\\\\n\t\t\tSL&MBLLEN   &1.444&{\\color{blue}{17.902}}&0.715& 0.247&  {\\color{red}{1.296}} &  {\\color{red}{19.781}} & {\\color{red}{0.825}}&{\\color{red}{0.108}} \\\\\n\t\t\t&KinD  & {\\color{purple}{1.431}} & 17.648& {\\color{blue}{0.779}}& {\\color{red}{0.175}} & 2.675 & 14.535 & 0.741 &0.177 \\\\\n\t\t\t&KinD++  & {\\color{blue}{1.298}}&{\\color{purple}{17.752}}& {\\color{purple}{0.760}}& {\\color{blue}{0.198}} & 7.582 & 9.732 &0.568 & 0.336 \\\\\n\t\t\t&TBEFN  &1.764&17.351&  {\\color{red}{0.786}}& {\\color{purple}{0.210}} & 3.865 & 12.769 &0.704 &0.178 \\\\\n\t\t\t&DSLR  &3.536&15.050& 0.597&0.337 &{\\color{purple}{1.925}} & {\\color{purple}{16.632}} &{\\color{blue}{0.782}} & 0.167 \\\\\n\t\t\t\\hline\n\t\t\tUL\t& EnlightenGAN  &1.998&17.483& 0.677&0.322 &3.628&13.260&0.745& 0.170 \\\\\n\t\t\t\\hline\n\t\t\tSSL\t&DRBN  &2.359&15.125& 0.472&0.316 &3.314& 13.355&0.378 & 0.281 \\\\\n\t\t\t\\hline\n\t\t\t&\tExCNet  &2.292&15.783& 0.515&0.373 &2.927& 13.978&0.710 &0.187 \\\\\t\t\t\n\t\t\tZSL\t&Zero-DCE  &3.282&14.861& 0.589&0.335 &3.476&13.199&0.709 &0.203\\\\\t\t\t\t\n\t\t\t&\tRRDNet  &6.313&11.392&0.468&0.361 &7.057&10.135&0.620 &0.303\\\\\t\t\t\t\t\t\t\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t\\end{center}\n\t\\label{Table:Quan_1}\n\t\\vspace{-4pt}\n\\end{table*}\n\\begin{table*}[t]\n\t\\caption{Quantitative comparisons on SID-test test dataset in terms of MSE ($\\times10^3$), PSNR (in dB), SSIM , and LPIPS . The best result is in {\\color{red}{red}} under each case. To compute the quantitative scores of input raw data, we use the corresponding camera ISP pipelines provided by Chen et al.  to transfer raw data to RGB format.}\n\t\\vspace{-6pt}\n\t\\begin{center}\n\t\t\\begin{tabular}{c|c|c|c|c|c||c|c|c|c}\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Learning}} &\\multirow{2}{*}{\\textbf{Method}} & \\multicolumn{4}{c||}{\\textbf{SID-test--Bayer}} & \\multicolumn{4}{c}{\\textbf{SID-test--X-Trans}} \\\\\n\t\t\t\\cline{3-10}\n\t\t\t\\cline{3-10}\n\t\t\t& &\t\\textbf{MSE}$\\downarrow$ & \\textbf{PSNR} $\\uparrow$ & \\textbf{SSIM}$\\uparrow$ & \\textbf{LPIPS}$\\downarrow$ & \\textbf{MSE}$\\downarrow$ & \\textbf{PSNR} $\\uparrow$ & \\textbf{SSIM}$\\uparrow$ & \\textbf{LPIPS}$\\downarrow$ \\\\\n\t\t\t\\hline\n\t\t\t&input  &5.378 &11.840  &0.063 &0.711 &4.803 &11.880  &0.075 &0.796 \\\\\n\t\t\t\\hline\n\t\t\tSL&SID  &0.140 &28.614   &0.757 &0.465 &0.235  &26.663 &0.680&0.586\\\\\n\t\t\t&EEMEFN  &{\\color{red}{0.126}} &{\\color{red}{29.212}}   &{\\color{red}{0.768}} &{\\color{red}{0.448}}&{\\color{red}{0.191}} &{\\color{red}{27.423}}  &{\\color{red}{0.695}} &{\\color{red}{0.546}}\\\\\t\t\t\t\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t\\end{center}\n\t\\label{Table:Quan_11}\n\t\\vspace{-4pt}\n\\end{table*}\n\\noindent\n\\textbf{Qualitative Comparison.}\nWe first present the results of different methods on the images sampled from LOL-test and MIT-Adobe FiveK-test datasets in Figures \\ref{fig:LOL} and \\ref{fig:5K}. \nAs shown in Figure \\ref{fig:LOL}, all methods improve the brightness and contrast of the input image. However, none of them successfully recovers the accurate color of the input image when the results are compared with the ground truth. In particular, LLNet  produces blurring result.  LightenNet  and RRDNet  produce under-exposed results while MBLLEN  and ExCNet  over-expose the image. KinD , KinD++  , TBEFN , DSLR , EnlightenGAN , and DRBN  introduce obvious artifacts.\nIn Figure \\ref{fig:5K}, LLNet , KinD++ , TBEFN , and RRDNet  produce over-exposed results. Retinex-Net , KinD++ , and RRDNet  yield artifacts and blurring in the results. \nWe found that the ground truths of MIT-Adobe FiveK dataset still contain some dark regions. This is because the dataset is originally designed for global image retouching, where restoring low light regions is not the main priority in this task. \nWe also observed that the input images in LOL dataset and MIT-Adobe FiveK dataset are relatively clean from noise, which is different from real low-light scenes.  \nAlthough some methods  take the MIT-Adobe FiveK dataset as the training or testing dataset, we argue that this dataset is not appropriate for the task of LLIE due to its mismatched/unsatisfactory ground truth for LLIE.\nTo examine the generalization capability of different methods, we conduct comparisons on the images sampled from our  LLIV-Phone-imgT dataset. The visual results of different methods are shown in Figures \\ref{fig:Phone1} and \\ref{fig:Phone2}.\nAs presented in Figure \\ref{fig:Phone1}, all methods cannot effectively improve the brightness and remove the noise of the input low-light image. Moreover, Retinex-Net ,  MBLLEN , and DRBN  produce obvious artifacts.\nIn Figure \\ref{fig:Phone2}, all methods enhance the brightness of this input image. However, only \nMBLLEN  and RRDNet  obtain visually pleasing enhancement without color deviation, artifacts, and over-/under-exposure.  Notably, for regions with a light source, none of the methods can brighten the image without amplifying the noise around these regions. \nTaking light sources into account for LLIE would be an interesting direction to explore.\nThe results suggest the difficulty of enhancing the images of the LLIV-Phone-imgT dataset. \nReal low-light images fail most existing LLIE methods due to the limited generalization capability of these methods. The potential reasons are the use of synthetic training data, small-scaled training data, or unrealistic assumptions such as the local illumination consistency and treating the reflectance component as the final result in the Retinex model in these methods.\nWe further present the visual comparisons of raw format-based methods in Figure \\ref{fig:raw}. As shown, the input raw data have obvious noises. Both SID  and EEMEFN  can effectively remove the effects of noises. In comparison to the simple U-Net structure used in SID , the more complex structure of EEMEFN  obtains better brightness recovery. However, their results are far from the corresponding GT, especially for the input of APS-C X-Trans pattern.\n\\noindent\n\\textbf{Quantitative Comparison.}\nFor test sets with ground truth i.e., LOL-test, MIT-Adobe FiveK-test, and SID-test, we adopt MSE, PSNR,  SSIM , and LPIPS  metrics to quantitatively compare different methods. LPIPS  is a deep learning-based image quality assessment metric that measures the perceptual similarity between a result and its corresponding ground truth by deep visual representations. For LPIPS, we employ the AlexNet-based model to compute the perceptual similarity. A lower LPIPS\nvalue suggests a result that is closer to the corresponding ground truth in terms of perceptual similarity. \nIn Table \\ref{Table:Quan_1} and Table \\ref{Table:Quan_11}, we show the quantitative results of RGB format-based methods and raw format-based methods, respectively.\nAs presented in Table \\ref{Table:Quan_1}, the quantitative scores of supervised learning-based methods are better than those of unsupervised learning-based, semi-supervised learning-based, and zero-shot learning-based methods on LOL-test and MIT-Adobe FiveK-test datasets. Among them, LLNet  obtains the best MSE and PSNR values on the LOL-test dataset; however, its performance drops on the MIT-Adobe FiveK-test dataset. \nThis may be caused by the bias of LLNet  towards the LOL dataset since it was trained using the LOL training dataset. \nFor the LOL-test dataset, TBEFN  obtains the highest SSIM value while KinD  achieves the lowest LPIPS value. There is no winner across these four evaluation metrics on the LOL-test dataset despite the fact that some methods were trained on the LOL training dataset. \nFor the MIT-Adobe FiveK-test dataset, MBLLEN  outperforms all compared methods under the four evaluation metrics in spite of being trained on synthetic training data. Nevertheless, MBLLEN  still cannot obtain the best performance on both two test datasets. \nAs presented in Table \\ref{Table:Quan_11}, both SID  and EEMEFN  improve the quality of input raw data. Compared with the quantitative scores of SID , EEMEFN  achieves consistently better performance across different raw data patterns and evaluation metrics.\nFor LLIV-Phone-imgT test set, we use the non-reference IQA metrics, i.e., NIQE ,  perceptual index (PI) , LOE , and SPAQ  to quantitatively compare different methods. In terms of LOE, the smaller the LOE value is, the better the lightness order is preserved. For NIQE, the smaller the NIQE value is,  the better the visual quality is. A lower PI value indicates better perceptual quality. SPAQ is devised for the perceptual quality assessment of smartphone photography. A larger SPAQ value suggests better perceptual quality of smartphone photography. The quantitative results are provided in Table \\ref{Table:Quan_2}.\n\\begin{table}[ht]\n\t\\caption{Quantitative comparisons on LLIV-Phone-imgT dataset in terms of NIQE , LOE , PI  , and SPAQ . The best result is in {\\color{red}{red}} whereas the second and third best results are in {\\color{blue}{blue}} and {\\color{purple}{purple}} under each case, respectively.}\n\t\\begin{center}\n\t\t\\setlength{\\tabcolsep}{1.7mm}{\n\t\t\t\\begin{tabular}{c|c|c|c|c|c}\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{2}{*}{\\textbf{Learning}}&\\multirow{2}{*}{\\textbf{Method}} & \\multicolumn{4}{c}{\\textbf{LoLi-Phone-imgT}}\\\\\n\t\t\t\t\\cline{3-6}\n\t\t\t\t\\cline{3-6}\n\t\t\t\t&\t& \\textbf{NIQE}$\\downarrow$ & \\textbf{LOE} $\\downarrow$ & \\textbf{PI}$\\downarrow$ & \\textbf{SPAQ}$\\uparrow$ \\\\\n\t\t\t\t\\hline\n\t\t\t\t&\tinput  &6.99 &{\\color{red}{0.00}} &5.86 &44.45 \\\\\n\t\t\t\t\\hline\n\t\t\t\t&\tLLNet  &5.86 &{\\color{blue}{5.86}} &5.66 &40.56 \\\\\n\t\t\t\t&\tLightenNet &5.34 &952.33 &4.58 &45.74 \\\\\n\t\t\t\t&\t\n\t\t\t\tRetinex-Net  &   5.01 &790.21 &{\\color{red}{3.48}} &{\\color{red}{50.95}} \\\\\n\t\t\t\tSL&MBLLEN   &5.08 &220.63 & 4.27 &42.50 \\\\\n\t\t\t\t&\tKinD &4.97 &405.88 &4.37 &44.79 \\\\\n\t\t\t\t&\tKinD++  &{\\color{red}{4.73}}\n\t\t\t\t&681.97 &{\\color{blue}{3.99}} &{\\color{blue}{46.89}} \\\\\n\t\t\t\t&\tTBEFN  &4.81 &552.91 & 4.30 &44.14 \\\\\n\t\t\t\t&\tDSLR &{\\color{blue}{4.77}} &447.98 &4.31 & 41.08\\\\\n\t\t\t\t\\hline\n\t\t\t\tUL &EnlightenGAN  &{\\color{purple}{4.79}} &821.87 &{\\color{purple}{4.19}} &45.48 \\\\\n\t\t\t\t\\hline\n\t\t\t\tSSL&\tDRBN &5.80 &885.75 &5.54 &42.74 \\\\\t\n\t\t\t\t\\hline\n\t\t\t\t&\tExCNet  &5.55 &723.56 &4.38 &46.74 \\\\\t\t\t\n\t\t\t\tZSL &\tZero-DCE  &5.82 &307.09 & 4.76 &{\\color{purple}{46.85}} \\\\ \t\t\t\n\t\t\t\t&\tRRDNet &5.97 &{\\color{purple}{142.89}} &4.84 &45.31 \\\\ \t\t\t\t\t\t\t\n\t\t\t\t\\hline\n\t\t\\end{tabular}}\n\t\\end{center}\n\t\\label{Table:Quan_2}\n\t\\vspace{-4pt}\n\\end{table}\n\\begin{table}[t]\n\t\\caption{Quantitative comparisons on LLIV-Phone-vidT dataset in terms of average luminance variance (ALV)  score. The best result is in {\\color{red}{red}} whereas the second and third best results are in {\\color{blue}{blue}} and {\\color{purple}{purple}}.}\n\t\\begin{center}\n\t\t\\begin{tabular}{c|c|c}\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Learning}}&\\multirow{2}{*}{\\textbf{Method}} & \\multicolumn{1}{c}{\\textbf{LoLi-Phone-vidT}}\\\\\n\t\t\t\\cline{3-3}\n\t\t\t\\cline{3-3}\n\t\t\t&\t& \\textbf{ALV}$\\downarrow$\\\\\n\t\t\t\\hline\n\t\t\t&\tinput  &185.60   \\\\\n\t\t\t\\hline\n\t\t\t&\tLLNet  &{\\color{blue}{85.72}}  \\\\\n\t\t\t&\tLightenNet &643.93\\\\\n\t\t\t&\t\n\t\t\tRetinex-Net  &94.05   \\\\\n\t\t\tSL&MBLLEN   &113.18  \\\\\n\t\t\t&\tKinD &98.05  \\\\\n\t\t\t&\tKinD++  &115.21\\\\\n\t\t\t&\tTBEFN  &{\\color{red}{58.69}} \\\\\n\t\t\t&\tDSLR &175.35  \\\\\n\t\t\t\\hline\n\t\t\tUL &EnlightenGAN  &{\\color{purple}{90.69}}  \\\\\n\t\t\t\\hline\n\t\t\tSSL&\tDRBN &115.04  \\\\\t\n\t\t\t\\hline\n\t\t\t&\tExCNet  &1375.29   \\\\\t\t\t\n\t\t\tZSL &\tZero-DCE  &117.22  \\\\ \t\t\t\n\t\t\t&\tRRDNet &147.11  \\\\ \t\t\t\t\t\t\t\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t\\end{center}\n\t\\label{Table:Quan_33}\n\t\\vspace{-4pt}\n\\end{table}\nObserving Table \\ref{Table:Quan_2}, we can find that the performance of Retinex-Net , KinD++ , and EnlightenGAN  is relatively better than the other methods. Retinex-Net  achieves the best PI and SPAQ scores. The scores suggest the good perceptual quality of the results enhanced by Retinex-Net . However, from Figure~\\ref{fig:Phone1}(d) and Figure~\\ref{fig:Phone2}(d), the results of Retinex-Net  evidently suffer from artifacts and color deviations.\nMoreover, KinD++  attains the lowest NIQE score while the original input achieves the lowest LOE score. For the de-facto standard LOE metric, we question if the lightness order can effectively reflect the enhancement performance. Overall, the non-reference IQA metrics experience biases on the evaluations of the quality of enhanced low-light images in some cases. \nTo prepare videos in the LLIV-vidT testing set,  we first discard videos without obvious objects in consecutive frames. A total of 10 videos are chosen. For each video, we select one object that appears in all frames. We then use a tracker  to track the object in consecutive frames of the input video and ensure the same object appears in the bounding boxes. We discard the frames with inaccurate object tracking. The coordinates of the bounding box in each frame are collected. We employ these coordinates to crop the corresponding regions in the results enhanced by different methods and compute the average luminance variance (ALV) scores of the object in the consecutive frames as: $ALV=\\frac{1}{N}\\sum\\limits_{i=1}^{N}(L_{i}-L_{\\text{avg}})^2$, where $N$ is the number of frames of a video, $L_{i}$ represents the average luminance value of the region of bounding box in the $i$th frame, and $L_{\\text{avg}}$ denotes the average luminance value of all bounding box regions in the video. A lower ALV value suggests better temporal coherence of the enhanced video. \nThe ALV values of different methods averaged over the 10 videos of the LLIV-vidT testing set are shown in Table \\ref{Table:Quan_33}. \nThe ALV values of different methods on each video can be found in the supplementary material. Besides, we follow Jiang and Zheng  to plot their luminance curves in the supplementary material.\nAs shown in Table \\ref{Table:Quan_33}, \tTBEFN  obtains the best temporal coherence in terms of ALV value whereas LLNet  and EnlightenGAN  rank the second and third best, respectively. In contrast, the ALV value of ExCNet , as the worst performer, reaches 1375.29. This is because the performance of the zero-reference learning-based ExCNet  is unstable for the enhancement of consecutive frames.  ExCNet  can effectively improve the brightness of some frames while it does not work well on other frames.\n\\begin{table*}[t]\n\t\\caption{Quantitative comparisons of computational complexity in terms of runtime (in second), number of trainable parameters (\\#Parameters) (in M), and FLOPs (in G).  The best result is in {\\color{red}{red}} whereas the second and third best results are in {\\color{blue}{blue}} and {\\color{purple}{purple}} under each case, respectively.  `-' indicates the result is not available.}\n\t\\vspace{-6pt}\n\t\\begin{center}\n\t\t\\begin{tabular}{c|c|c|c|c|c}\n\t\t\t\\hline\n\t\t\t\\textbf{Learning}&\t\\textbf{Method}  \n\t\t\t& \\textbf{RunTime}$\\downarrow$ & \\textbf{\\#Parameters} $\\downarrow$ & \\textbf{FLOPs}$\\downarrow$ & \\textbf{Platform}\\\\\n\t\t\t\\hline\n\t\t\t&\tLLNet  &36.270 &17.908  &4124.177 & Theano\\\\\n\t\t\t&\tLightenNet  & -&{\\color{red}{0.030}}  &{\\color{red}{30.540}} &MATLAB\\\\\n\t\t\t&\tRetinex-Net  &0.120 &0.555 &587.470 &TensorFlow\\\\\n\t\t\tSL&\tMBLLEN   &13.995&{\\color{purple}{0.450}}&301.120 &TensorFlow\\\\\n\t\t\t&\tKinD  &0.148 &8.160&574.954 &TensorFlow\\\\\n\t\t\t&\tKinD++  &1.068&8.275&12238.026 &TensorFlow\\\\\n\t\t\t&\tTBEFN  &{\\color{purple}{0.050}}&0.486&108.532  &TensorFlow\\\\\n\t\t\t&\tDSLR  &0.074&14.931&{\\color{purple}{96.683}}  &PyTorch\\\\\n\t\t\t\\hline\n\t\t\tUL&\tEnlightenGAN  &{\\color{blue}{0.008}}&8.637& 273.240 &PyTorch\\\\\n\t\t\t\\hline\n\t\t\tSSL\t&\tDRBN  &0.878&0.577&196.359 &PyTorch\\\\\t\n\t\t\t\\hline\n\t\t\t&\tExCNet  &23.280&8.274&- &PyTorch\\\\\t\t\t\n\t\t\tZSL\t&\tZero-DCE  &{\\color{red}{0.003}}&{\\color{blue}{0.079}}&{\\color{blue}{84.990}} &PyTorch \\\\\t\t\t\t\n\t\t\t&\tRRDNet  &167.260 &0.128&- &PyTorch\\\\\t\t\t\t\t\t\t\t\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t\\end{center}\n\t\\label{Table:Quan_4}\n\t\\vspace{-4pt}\n\\end{table*}", "cites": [1875], "cite_extract_rate": 1.0, "origin_cites_number": 6, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section presents a structured comparison of various deep learning methods for low-light image enhancement across multiple datasets, which shows some integration of cited papers. It includes critical observations such as color deviation, over/under-exposure, and artifacts. However, it largely focuses on empirical results rather than synthesizing broader conceptual themes or offering deep abstraction beyond the specific methods evaluated."}}
{"id": "e142566f-157a-4609-a670-8df6dbfe9874", "title": "Computational Complexity", "level": "subsection", "subsections": [], "parent_id": "62b1377f-b6e1-4398-be9f-a1fa2952056b", "prefix_titles": [["title", "Low-Light Image and Video Enhancement \\\\Using Deep Learning: A Survey"], ["section", "Benchmarking and Empirical Analysis"], ["subsection", "Computational Complexity"]], "content": "In Table \\ref{Table:Quan_4}, we compare the computational complexity of RGB format-based methods, including runtime, trainable parameters, and FLOPs averaged over 32 images of size 1200$\\times$900$\\times$3 using an NVIDIA 1080Ti GPU. We omit LightenNet  for fair comparisons because only the CPU version of its code is publicly available.\nBesides, we do not report the FLOPs of ExCNet  and RRDNet  as the number depends on the input images (different inputs require different numbers of iterations).\nAs presented in Table \\ref{Table:Quan_4}, Zero-DCE   has the shortest runtime because it only estimates several curve parameters via a lightweight network. As a result, its number of trainable parameters and FLOPs are much fewer. Moreover, the number of trainable parameters and FLOPs of LightenNet   are the least among the compared methods. This is because LightenNet   estimates the illumination map of input image via a tiny network of four convolutional layers. In contrast, the FLOPs of LLNet  and KinD++  are extremely large, reaching 4124.177G and 12238.026G, respectively.  The runtime of SSL-based ExCNet  and RRDNet  is long due to the time-consuming optimization process.\n\\begin{figure}[!t]\n\t\\centering  \\centerline{\\includegraphics[width=1\\linewidth]{figures/face.png}}\n\t\\vspace{-2pt}\n\t\\caption{The P-R curves  of face detection in the dark.}\n\t\\label{fig:face}\n\t\\vspace{-4pt}\n\\end{figure}\n\\begin{table}[!t]\n\t\\caption{Quantitative comparisons of AP under different IoU thresholds of face detection in the dark. The best result is in {\\color{red}{red}} whereas the second and third best results are in {\\color{blue}{blue}} and {\\color{purple}{purple}} under each case, respectively.}\n\t\\begin{center}\n\t\t\\begin{tabular}{c|c|c|c|c}\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{\\textbf{Learning}}&\\multirow{2}{*}{\\textbf{Method}}&\\multicolumn{3}{c}{\\textbf{IoU thresholds}} \\\\\n\t\t\t\\cline{3-5}\n\t\t\t\\cline{3-5}\n\t\t\t&\t & \\textbf{0.5} & \\textbf{0.6}  & \\textbf{0.7}\\\\\n\t\t\t\\hline\n\t\t\t&input  &0.195  &0.061   &0.007\\\\\n\t\t\t\\hline\n\t\t\t&\tLLNet  &0.208  &0.063&0.006\\\\\n\t\t\t&\tLightenNet  &0.249  &0.085&{\\color{purple}{0.010}}\\\\\n\t\t\t&\tRetinex-Net  &{\\color{blue}{0.261}}  &{\\color{red}{0.101}}&{\\color{red}{0.013}}\\\\\n\t\t\tSL&\tMBLLEN   &0.249  &{\\color{purple}{0.092}}&{\\color{purple}{0.010}}\\\\\n\t\t\t&\tKinD  &0.235  &0.081&{\\color{purple}{0.010}}\\\\\n\t\t\t&\tKinD++  &0.251  &0.090&{\\color{blue}{0.011}}\\\\\n\t\t\t&\tTBEFN  &{\\color{red}{0.268}}  &{\\color{blue}{0.099}}&{\\color{blue}{0.011}}\\\\\n\t\t\t&\tDSLR  &0.223  &0.067&0.007\\\\\n\t\t\t\\hline\n\t\t\tUL&\tEnlightenGAN  &0.246  &0.088&{\\color{blue}{0.011}}\\\\\n\t\t\t\\hline\n\t\t\tSSL\t&\tDRBN  &0.199  &0.061&0.007\\\\\t\n\t\t\t\\hline\n\t\t\t&\tExCNet  &0.256  &{\\color{purple}{0.092}}&{\\color{purple}{0.010}}\\\\\t\t\t\n\t\t\tZSL\t&\tZero-DCE   &{\\color{purple}{0.259}}  &{\\color{purple}{0.092}}&{\\color{blue}{0.011}}\\\\\t\t\t\t\n\t\t\t&\tRRDNet  &0.248  &0.083&{\\color{purple}{0.010}}\\\\\t\t\t\t\t\t\t\t\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t\\end{center}\n\t\\label{Table:Quan_8}\n\t\\vspace{-4pt}\n\\end{table}\n\\subsection {Application-Based Evaluation}\nWe investigate the performance of low-light image enhancement methods on face detection in the dark. \nFollowing the setting presented in Guo et al. , we use the DARK FACE dataset  that is composed of images with faces taken in the dark. Since the bounding boxes of the test set are not publicly available, we perform the evaluation on 500 images randomly sampled from the training and validation sets.\nThe  Dual Shot Face Detector (DSFD)  trained on WIDER FACE dataset  is used as the face detector. We feed the results of different LLIE methods to the DSFD  and depict the precision-recall (P-R) curves under 0.5 IoU threshold in Figure \\ref{fig:face}. We compare the average precision (AP) under different IoU thresholds using the evaluation tool\\footnote{\\url{https://github.com/Ir1d/DARKFACE_eval_tools}} provided in DARK FACE dataset  in Table \\ref{Table:Quan_8}. \nAs shown in Figure \\ref{fig:face}, all the deep learning-based solutions improve the performance of face detection in the dark, suggesting the effectiveness of deep learning-based LLIE solutions for face detection in the dark. As shown in Table \\ref{Table:Quan_8}, the AP scores of best performers under different IoU thresholds range from 0.268 to 0.013 and the AP scores of input under different IoU thresholds are very low. The results suggest that there is still room for improvement. It is noteworthy that Retinex-Net , Zero-DCE , and TBEFN  achieve relatively robust performance on face detection in the dark. We show the visual results of different methods in Figure \\ref{fig:face_visual}. Although Retinex-Net  performs better than other methods on the AP score, its visual result contains obvious artifacts and unnatural textures. In general, Zero-DCE  obtains a good balance between the AP score and the perceptual quality for face detection in the dark. \tNote that the results of face detection in the dark are related to not only the enhanced results but also the face detector including the detector model and the training data of the detector. Here, we only take the pre-trained DSFD  as an example to validate the low-light image enhancement performance of different methods to some extent.", "cites": [1875], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a comparative analysis of computational complexity and application-based performance of LLIE methods, integrating results from multiple sources and highlighting key differences in runtime, parameters, and FLOPs. It includes some critical evaluation by pointing out limitations (e.g., artifacts in Retinex-Net) and noting the balance achieved by Zero-DCE. However, it lacks deeper abstraction or synthesis into a novel framework, and the insights remain largely centered on specific quantitative results."}}
{"id": "8ea7b52c-e5f5-46c9-8ff5-69763784bc99", "title": "Discussion", "level": "subsection", "subsections": [], "parent_id": "62b1377f-b6e1-4398-be9f-a1fa2952056b", "prefix_titles": [["title", "Low-Light Image and Video Enhancement \\\\Using Deep Learning: A Survey"], ["section", "Benchmarking and Empirical Analysis"], ["subsection", "Discussion"]], "content": "From the experimental results, we obtain several interesting observations and insights:\n\\begin{figure*}[!t]\n\t\\begin{center}\n\t\t\\begin{tabular}{c@{ }c@{ }c@{ }c@{ }}\n\t\t\t\\includegraphics[width=0.24\\linewidth]{figures/face_input.png}&\n\t\t\t\\includegraphics[width=0.24\\linewidth]{figures/face_LightenNet.png}&\n\t\t\t\\includegraphics[width=0.24\\linewidth]{figures/face_RetinexNet.png}&\n\t\t\t\\includegraphics[width=0.24\\linewidth]{figures/face_MBLLEN.png}\\\\\n\t\t\t(a) input  & (b) LightenNet  &  (c) Retinex-Net  & (d) \tMBLLEN  \\\\\n\t\t\t\\includegraphics[width=0.24\\linewidth]{figures/face_KinD++.png}&\n\t\t\t\\includegraphics[width=0.24\\linewidth]{figures/face_TBEFN.png}&\n\t\t\t\\includegraphics[width=0.24\\linewidth]{figures/face_DSLR.png}&\n\t\t\t\\includegraphics[width=0.24\\linewidth]{figures/face_EnlightenGAN.png}\\\\\n\t\t\t(e)  KinD++   &\t(f) TBEFN  &  (g) \tDSLR  \t  & (h) EnlightenGAN  \\\\\n\t\t\t\\includegraphics[width=0.24\\linewidth]{figures/face_DRBN.png}&\n\t\t\t\\includegraphics[width=0.24\\linewidth]{figures/face_ExCNet.png}&\n\t\t\t\\includegraphics[width=0.24\\linewidth]{figures/face_ZeroDCE.png}&\n\t\t\t\\includegraphics[width=0.24\\linewidth]{figures/face_RRDNet.png}\\\\\n\t\t\t(i) DRBN  & (j)\tExCNet  & (k) Zero-DCE  &  (l) RRDNet \t \\\\\n\t\t\\end{tabular}\n\t\\end{center}\n\t\\vspace{-2pt}\n\t\\caption{Visual results of different methods on a low-light image sampled from  DARK FACE dataset. Better see with zoom in for the bounding boxes of faces.}\n\t\\label{fig:face_visual}\n\t\\vspace{-4pt}\n\\end{figure*}\n1) The performance of different methods significantly varies  based on the test datasets and evaluation metrics. In terms of the full-reference IQA metrics on commonly used test datasets, MBLLEN , KinD++ , and DSLR  are generally better  than other compared methods. For real-world low-light images taken by mobile phones, supervised learning-based Retinex-Net  and KinD++   obtain better scores measured in the non-reference IQA metrics. For real-world low-light videos taken by mobile  phones, TBEFN  preserves the temporal coherence better. When coming to the computational efficiency, LightenNet   and Zero-DCE  are outstanding. From the aspect of face detection in the dark, TBEFN , Retinex-Net , and  Zero-DCE  rank the first three. No method always wins. Overall, Retinex-Net , , Zero-DCE , and DSLR  are better choice in most cases. For raw data, EEMEFN  obtains relatively better qualitative and quantitative performance than SID . However, from the visual results,  EEMEFN  and  cannot recover the color well when compared with the corresponding ground truth.\n2) LLIV-Phone dataset fails most methods. The generalization capability of existing methods needs further improvements. It is worth noting that it is inadequate to use only the average luminance variance to evaluate the performance of different methods for low-light video enhancement. More effective and comprehensive assessment metrics would guide the development of low-light video enhancement towards the right track.\n3) Regarding learning strategies, supervised learning achieves better performance in most cases, but requires high computational resources and paired training data. In comparison, zero-shot learning is more appealing in practical applications because it does not require paired or unpaired training data. Consequently, zero-shot learning-based methods enjoy better generalization capability. However,  their quantitative performance is inferior to other methods. \n4) There is a gap between visual results and quantitative IQA scores. In other words, a good visual appearance does not always yield a good IQA score. The relationships between human perception and IQA scores are worth more investigation. Pursuing better visual perception or quantitative scores depends on specific applications. For instance, to show the results to observers, more attention should be paid to visual perception. In contrast, accuracy is more important when LLIE methods are applied to face detection in the dark. Thus, more comprehensive comparisons should be performed when comparing different methods.\n5) Deep learning-based LLIE methods are beneficial to face detection in the dark. Such results further support the significance of enhancing low-light images and videos. However, in comparison to the high accuracy of face detection in normal-light images, the accuracy of face detection in the dark is extremely low, despite using LLIE  methods.\n6) In comparison to RGB format-based LLIE methods, raw format-based LLIE methods usually recover details better, obtain more vivid color, and reduce noises and artifacts more effectively. This is because raw data contain more information such as wider color gamut and higher dynamic range. However, raw format-based LLIE methods are limited to specific sensors and formats such as the Bayer pattern of the Sony camera and the APS-C X-Trans pattern of the Fuji camera. In contrast,  RGB format-based LLIE methods are more convenient and versatile since RGB images are commonly found as the final imagery form produced by mobile devices. However, RGB format-based LLIE methods cannot cope well with cases that exhibit low light and excessive noise.", "cites": [1875], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section shows strong critical analysis by identifying limitations in generalization, the mismatch between visual quality and IQA scores, and the trade-offs between learning strategies and formats. It synthesizes results from multiple methods and connects them to broader implications for low-light enhancement. The abstraction is evident in its discussion of overarching patterns, such as the preference for raw data and the need for better assessment metrics."}}
{"id": "c43fed76-06de-4d00-89a5-701cdf5d52e3", "title": "Open Issues", "level": "section", "subsections": [], "parent_id": "43aba542-7f6b-493f-9beb-9ae4164ca84b", "prefix_titles": [["title", "Low-Light Image and Video Enhancement \\\\Using Deep Learning: A Survey"], ["section", "Open Issues"]], "content": "\\label{sec:Issue}\nIn this section, we summarize the open issues in low-light image and video enhancement as follows.\n\\noindent\n\\textbf{Generalization Capability.} Although existing methods can produce some visually pleasing results, they have limited generalization capability. For example, a method trained on MIT-Adobe FiveK dataset  cannot effectively enhance the low-light images of LOL dataset . Albeit synthetic data are used to augment the diversity of training data, the models trained on the combination of real and synthetic data cannot solve this issue well. Improving the generalization capability of LLIE methods is an unsolved open issue.\n\\noindent\n\\textbf{Removing Unknown Noises.} Observing the results of existing methods on the low-light images captured by different types of phones' cameras, we can find that these methods cannot remove the noises well and even amplify the noises, especially when the types of noises are unknown. Despite some methods add Gaussian and/or Poisson noises in their training data, the noise types are different from real noises, thus the performance of these methods is unsatisfactory in real scenarios. Removing unknown noises is still unsolved.\n\\noindent\n\\textbf{Removing Unknown Artifacts.} One may enhance a low-light image downloaded from the Internet. The image may have gone through a serial of degradations such as JPEG compression or editing. Thus, the image may contain unknown artifacts. Suppressing unknown artifacts still challenges existing low-light image and video enhancement methods.\n\\noindent\n\\textbf{Correcting Uneven Illumination.} Images taken in real scenes usually exhibit uneven illumination. For example, an image captured at night has both dark regions and normal-light or over-exposed regions such as the regions of light sources. Existing methods tend to brighten both the dark regions and the light source regions, affecting the visual quality of the enhanced result. It is expected to enhance dark regions but suppress over-exposed regions. However, this open issue is not solved well in existing LLIE methods.\n\\noindent\n\\textbf{Distinguishing Semantic Regions.} Existing methods tend to enhance a low-light image without considering the semantic information of its different regions. For example, the black hair of a man in a low-light image is enhanced to be off-white as the black hair is treated as the low-light regions. An ideal enhancement method is expected to only enhance the low-light regions induced by external environments. How to distinguish semantic regions is an open issue. \n\\noindent\n\\textbf{Using Neighbouring Frames.} \n\tDespite some methods that have been proposed to enhance low-light videos, they commonly process a video frame-by-frame. How to make full use of the neighboring frames to improve the enhancement performance and speed up the processing speed is an unsolved open issue. For example, the well-lit regions of neighboring frames are used to enhance the current frame. For another example, the estimated parameters for processing neighboring frames can be reused to enhance the current frame for reducing the time of parameter estimation.", "cites": [1875], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section provides a clear and insightful analysis of key open issues in low-light image and video enhancement, using observations from different datasets and methods. While the synthesis is based on a single cited paper, the section connects the findings to broader challenges in the field. The critical evaluation is strong, as it identifies specific limitations and real-world implications of existing approaches without merely summarizing them. The abstraction level is moderate, as the issues are framed in general terms, though more explicit patterns across multiple works would strengthen it."}}
