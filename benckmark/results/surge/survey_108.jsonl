{"id": "50506660-07a7-4316-9f4f-7eb13e615050", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "65198946-bcab-42d9-bc99-da3a8beb9233", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Introduction"]], "content": "\\label{sec:intro}\nIn recent years, the rapid development of Large language Models has been revolutionizing the field of natural language processing~. These powerful models have shown great potential in addressing a variety of NLP tasks, ranging from natural language understanding~(NLU) to generation tasks, even paving the way to Artificial General Intelligence (AGI). However, utilizing these models effectively and efficiently requires a practical understanding of their capabilities and limitations, as well as the data and tasks involved in NLP. \nTo provide a guide for partitioners and end-users, this work focuses on the practical aspects of working with LLMs in downstream NLP tasks. \nThis guide aims to provide practical advice on why or why not to choose LLMs for a given task, as well as guidance on how to select the most suitable LLM, taking into account factors such as model sizes, computational requirements, and the availability of domain-specific pre-trained models.\nThis work offers a thorough understanding of LLMs from a practical perspective, therefore, empowers practitioners and end-users with the practical knowledge needed to successfully leverage the power of LLMs for their own NLP tasks.\nOur work is structured as follows. First, our work offers a brief introduction to LLMs by discussing the most important models, such as GPT-style and BERT-style architectures. \nThen, we delve into the critical factors that influence model performance from the data perspective, including pre-training data, training/tuning data, and test data. \nLast and most importantly, we dive deep into various concrete NLP tasks, offering insights into the applicability of LLMs for knowledge-intensive tasks, traditional NLU tasks, and generation tasks, along with the emergent abilities that these models possess and challenging real-world scenarios. We provide detailed examples to highlight both the successful use cases and the limitations of LLMs in practice.\nTo analyze the abilities of large language models, we compare them with fine-tuned models. \nAs of present, there is no universally recognized definition for LLMs and fine-tuned models. With consideration to practical utility, in our article, the definitions of them are proposed as: LLMs are huge language models pretrained on large amounts of datasets without tuning on data for specific tasks; fine-tuned models are typically smaller language models which are also pretrained and then further tuned on a smaller, task-specific dataset to optimize their performance on that task\\footnote{From a practical standpoint, we consider models with less than 20B parameters to be fine-tuned models. While it's possible to fine-tune even larger models like PlaM (540B), in reality, it can be quite challenging, particularly for academic research labs and small teams. Fine-tuning a model with 3B parameters can still be a daunting task for many individuals or organizations.}. \nThis work summarizes the following main practical guides for using LLMs:\n\\begin{itemize}\n    \\item \\textbf{Natural language understanding.} Employ the exceptional generalization ability of LLMs when facing out-of-distribution data or with very few training data. \n    \\item \\textbf{Natural language generation.} Utilize LLMs' capabilities to create coherent, contextually relevant, and high-quality text for various applications.\n    \\item \\textbf{Knowledge-intensive tasks.} Leverage the extensive knowledge stored in LLMs for tasks requiring domain-specific expertise or general world knowledge.\n    \\item \\textbf{Reasoning ability.} Understand and harness the reasoning capabilities of LLMs to improve decision-making and problem-solving in various contexts.\n\\end{itemize}", "cites": [1549, 8461, 1550], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The introduction synthesizes key concepts from the cited survey papers on LLMs, BERT-style models, and foundation models, presenting a coherent narrative on their practical usage. It abstracts from specific models to broader categories like 'LLMs' and 'fine-tuned models' and offers some practical guidance. However, it lacks deeper critical analysis of the cited works' limitations or contradictions, and the synthesis is more descriptive than integrative in forming a novel framework."}}
{"id": "5e50f17a-7d9a-4376-9534-28733a19fad8", "title": "Practical Guide for Models", "level": "section", "subsections": ["64e27ad0-4c47-4785-a472-a13fd6fa2f43", "5356f595-71f3-4390-8b5e-38319883856a"], "parent_id": "65198946-bcab-42d9-bc99-da3a8beb9233", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Practical Guide for Models"]], "content": "\\begin{figure}[tp]\n  \\begin{adjustwidth}{-0.0cm}{}\n  \\centering\n    \\includegraphics[width=1.00\\textwidth]{./models-colorgrey-2.pdf}\n   \\end{adjustwidth} \n   \\caption{The evolutionary tree of modern LLMs traces the development of language models in recent years and highlights some of the most well-known models. Models on the same branch have closer relationships. Transformer-based models are shown in non-\\textcolor{YGrey}{grey} colors: decoder-only models in the \\textcolor{GreyBlue2}{blue} branch, encoder-only models in the \\textcolor{GreyPink2}{pink} branch, and encoder-decoder models in the \\textcolor{GreyGreen2}{green} branch. The vertical position of the models on the timeline represents their release dates. Open-source models are represented by solid squares, while closed-source models are represented by hollow ones. The stacked bar plot in the bottom right corner shows the number of models from various companies and institutions.}\\label{fig:tree}\n\\end{figure}\nThis section provides a brief introduction to state-of-the-art LLMs. These models differ in their training strategies, model architectures, and use cases. To provide a clearer understanding of the LLM landscape, we categorize them into two types: encoder-decoder or encoder-only language models and decoder-only language models. In Figure~\\ref{fig:tree}, we show the detailed evolution process of language models. From the evolutionary tree, we make the following interesting observations: \n\\begin{enumerate}[label=\\alph*)]\n    \\item Decoder-only models have been gradually dominating the development of LLMs. At the early stage of LLMs development, \\textcolor{GreyBlue}{decoder-only} models were not as popular as \\textcolor{GreyPink}{encoder-only} and \\textcolor{GreyGreen}{encoder-decoder} models. However, after 2021, with the introduction of game-changing LLMs - GPT-3, decoder-only models experienced a significant boom. Meanwhile, after the initial explosive growth brought about by BERT, encoder-only models gradually began to fade away.\n    \\item OpenAI consistently maintains its leadership position in LLM, both currently and potentially in the future. Other companies and institutions are struggling to catch up with OpenAI in developing models comparable to GPT-3 and the current GPT-4. This leadership position may be attributed to OpenAI's steadfast commitment to its technical path, even when it was not widely acknowledged initially.\n    \\item Meta contributes significantly to open-source LLMs and promotes research of LLMs. When considering contributions to the open-source community, particularly those related to LLMs, Meta stands out as one of the most generous commercial companies, as all the LLMs developed by Meta are open-sourced.\n    \\item LLMs exhibit a tendency towards closed-sourcing. In the early stages of LLM development (before 2020), the majority of models were open-sourced. However, with the introduction of GPT-3, companies have increasingly opted to close-source their models, such as PaLM, LaMDA, and GPT-4. Consequently, it has become more difficult for academic researchers to conduct experiments on LLM training. As a result, API-based research could become the predominant method in the academic community.\n    \\item Encoder-decoder models remain promising, as this type of architecture is still being actively explored, and most of them are open-sourced. Google has made substantial contributions to open-source encoder-decoder architectures. However, the flexibility and versatility of decoder-only models seem to make Google's insistence on this direction less promising.\n\\end{enumerate}\nWe also briefly summarize the characteristics and the representative LLMs of each type in Table~\\ref{tab:llms}.\n\\begin{table}[tp]\n\\centering\n\\caption{Summary of Large Language Models.}\n\\label{tab:llms}\n\\resizebox{1\\textwidth}{!}{\n    \\begin{tabular}{c|rl|c}\n    \\toprule\n    \\multicolumn{1}{c|}{}                       &\\multicolumn{2}{c|}{\\textbf{Characteristic}}        & \\multicolumn{1}{c}{\\textbf{LLMs}}     \\\\ \\midrule\n    \\multirow{5}{*}{Encoder-Decoder or Encoder-only}     &              &                                    & \\multirow{4}{6cm}{ELMo~, BERT~, RoBERTa~, DistilBERT~, BioBERT~, XLM~, Xlnet~, ALBERT ~, ELECTRA~, T5~, GLM~, XLM-E~, ST-MoE~, AlexaTM~} \\\\ \n                                                &Training: &Masked Language Models         & \\\\\n                                                &Model type:   &Discriminative                         & \\\\\n    (BERT-style)                                 &Pretrain task:&Predict masked words                   & \\\\\n                                                &              &                                       & \\\\\n    \\midrule\n    \\multirow{5}{*}{Decoder-only}      &              &                                       & \\multirow{4}{6cm}{GPT-3~, OPT~. PaLM~, BLOOM~, MT-NLG~,  GLaM~,Gopher~, chinchilla~, LaMDA~, GPT-J~, LLaMA~,  GPT-4~, BloombergGPT~} \\\\\n                                                         &Training  & Autoregressive Language Models                         & \\\\\n                                                         &Model type:   &Generative                             & \\\\\n    (GPT-style)                                           &Pretrain task:&Predict next word                      & \\\\\n                                                         &              &                                       & \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\end{table}", "cites": [679, 9115, 7460, 1552, 1150, 1558, 1551, 826, 9, 856, 8385, 1557, 8462, 1556, 1555, 1559, 1554, 7462, 7, 7461, 7463, 1553, 11], "cite_extract_rate": 0.8518518518518519, "origin_cites_number": 27, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple LLM models and their training strategies from various cited works, creating a coherent narrative around architectural trends and open/closed sourcing practices. It offers critical analysis by evaluating the dominance of decoder-only models and questioning the long-term viability of encoder-decoder approaches. The abstraction is strong as it generalizes patterns in LLM development and highlights broader implications for research and industry practices."}}
{"id": "64e27ad0-4c47-4785-a472-a13fd6fa2f43", "title": "BERT-style Language Models: Encoder-Decoder or Encoder-only", "level": "subsection", "subsections": [], "parent_id": "5e50f17a-7d9a-4376-9534-28733a19fad8", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Practical Guide for Models"], ["subsection", "BERT-style Language Models: Encoder-Decoder or Encoder-only"]], "content": "As natural language data is readily available and unsupervised training paradigms have been proposed to better utilize extremely large datasets, this motivates the unsupervised learning of natural language. One common approach is to predict masked words in a sentence while considering the surrounding context. This training paradigm is known as the Masked Language Model. This type of training allows the model to develop a deeper understanding of the relationships between words and the context in which they are used.  These models are trained on a large corpus of texts using techniques such as the Transformer architecture and have achieved state-of-the-art results in many NLP tasks, such as sentiment analysis and named entity recognition. Notable examples of Masked Language Models include BERT~, RoBERTa~, and T5~. MLMs have become an important tool in the field of natural language processing due to their success in a wide range of tasks.", "cites": [7, 826, 9], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of BERT-style models and their training paradigm (MLM), and mentions several cited papers (BERT, RoBERTa, T5) without clearly integrating or contrasting their contributions. There is minimal critical analysis or abstraction to broader principles, and the narrative remains at a surface level without deeper synthesis of the cited works."}}
{"id": "5356f595-71f3-4390-8b5e-38319883856a", "title": "GPT-style Language Models: Decoder-only", "level": "subsection", "subsections": [], "parent_id": "5e50f17a-7d9a-4376-9534-28733a19fad8", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Practical Guide for Models"], ["subsection", "GPT-style Language Models: Decoder-only"]], "content": "Although language models are typically task-agnostic in architecture, these methods require fine-tuning on datasets of the specific downstream task. Researchers found that scaling up language models significantly improves the few-shot, even zero-shot performance~. The most successful models for better few-shot and zero-show performance are Autoregressive Language Models, which are trained by generating the next word in a sequence given the preceding words. These models have been widely used for downstream tasks such as text generation and question answering. Examples of Autoregressive Language Models include GPT-3~, OPT~, PaLM~, and BLOOM~. The game changer, GPT-3, for the first time, demonstrated reasonable few-/zero-shot performance via prompting and in-context learning,  \nthus showing the superiority of autoregressive language models. There are also models such as CodeX~ that are optimized for specific tasks such as code generation, BloombergGPT~ for the financial domain. The recent breakthrough is ChatGPT, which refines GPT-3 specifically for conversational tasks, resulting in more interactive, coherent, and context-aware conversational for various real-world applications.", "cites": [679, 1556, 7463, 7462, 1554], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.2, "critical": 1.8, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of GPT-style, decoder-only language models and mentions several relevant papers, but it does not deeply synthesize or integrate the findings from these works. It lacks critical evaluation or comparison of the cited models and instead offers a superficial narrative. The content remains concrete and does not abstract to broader principles or patterns in the field."}}
{"id": "7cd2af4d-ab98-4dec-93b3-0692677a51d7", "title": "Pretraining data", "level": "subsection", "subsections": [], "parent_id": "ecb8bb46-39ea-4696-8c8e-aa03c3c2e7c8", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Practical Guide for Data"], ["subsection", "Pretraining data"]], "content": "Pre-training data plays a pivotal role in the development of large language models. As the foundation of remarkable capabilities  of LLMs, the quality, quantitative, and diversity of pre-training data influence the performance of LLMs significantly~. The commonly used pretraining data consists of a myriad of text sources, including books, articles, and websites. The data is carefully curated to ensure a comprehensive representation of human knowledge, linguistic nuances, and cultural perspectives. The importance of pretraining data lies in its capacity to inform the language model with a rich understanding of word knowledge, grammar, syntax, and semantics, as well as the ability to recognize context and generate coherent responses.\nThe diversity of pretraining data also plays a crucial role in shaping the model's performance, and the selection of LLMs highly depends on the components of the pretraining data. For example, PaLM~ and BLOOM~ excel in multilingual tasks and machine translation with an abundance of multilingual pretraining data. Moreover, PaLM's performance in Question Answering tasks is enhanced by incorporating a considerable amount of social media conversations and Books corpus . Likewise,  code execution and code completion capabilities of GPT-3.5~(code-davinci-002) are amplified by the integration of code data in its pretraining dataset. In brief, when selecting LLMs for downstream tasks, it is advisable to choose the model pre-trained on a similar field of data.", "cites": [472, 1560, 7462, 1554], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates insights from multiple papers to highlight the influence of pretraining data on LLM performance and provides examples such as PaLM and BLOOM to illustrate how different data components affect specific tasks. However, it lacks deeper comparative or critical analysis of the cited works and does not fully abstract broader principles beyond the specific examples given."}}
{"id": "c18800ff-43df-4c70-b530-3188cbb5488b", "title": "Finetuning data", "level": "subsection", "subsections": [], "parent_id": "ecb8bb46-39ea-4696-8c8e-aa03c3c2e7c8", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Practical Guide for Data"], ["subsection", "Finetuning data"]], "content": "When deploying a model for downstream tasks, it is essential to consider three primary scenarios based on the availability of annotated data: zero, few, and abundant. In this section, we provide a succinct overview of the appropriate models to employ for each scenario.\n\\noindent\\textbf{Zero annotated data}: \nIn scenarios where annotated data is unavailable, utilizing LLMs in a zero-shot setting proves to be the most suitable approach. LLMs have been shown to outperform previous zero-shot methods . Additionally, the absence of a parameter update process ensures that catastrophic forgetting  is avoided since the language model parameters remain unaltered.\n\\noindent\\textbf{Few annotated data}: In this case, the few-shot examples are directly incorporated in the input prompt of LLMs, which is named as in-context learning, and these examples can effectively guide LLMs to generalize to the task. As reported in~, one-shot and few-shot performance make significant gains, even matching the performance of the SOTA fine-tuned open-domain models. And LLMs' zero/few-shot ability can be improved further by scaling~.  \n Alternatively, some few-shot learning methods are invented to enhance fine-tuned models, such as meta-learning  or transfer learning . However, performance might be inferior compared to using LLMs due to fine-tuned models' smaller scale and overfitting. \n\\noindent\\textbf{Abundant annotated data}: With a substantial amount of annotated data for a particular task available, both fine-tuned models and LLMs can be considered. In most cases, fine-tuning the model can fit the data pretty well. Although, LLMs can be used to meet some constraints such as privacy~.In this scenario, the choice between using a fine-tuned model or a LLM is task-specific and also depends on many factors, including desired performance, computational resources, and deployment constraints. \nIn a brief summary: LLMs are more versatile w.r.t. the data availability, while fine-tuned models can be considered with abundant annotated data.", "cites": [679, 1563, 1562, 8463, 1561], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes ideas from multiple papers to present a structured overview of fine-tuning strategies based on data availability. It compares the effectiveness of zero-shot, few-shot, and fine-tuned models, and identifies limitations (e.g., overfitting, smaller scale of fine-tuned models). While it offers some general patterns, the analysis remains at a relatively high-level without deeper meta-insights or extensive critique of the methodologies."}}
{"id": "4f7f7e54-1a01-4226-aeb4-46d63ceee443", "title": "Test data/user data ", "level": "subsection", "subsections": [], "parent_id": "ecb8bb46-39ea-4696-8c8e-aa03c3c2e7c8", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Practical Guide for Data"], ["subsection", "Test data/user data "]], "content": "\\label{sec:test_data}\nWhen deploying LLMs for downstream tasks, we often face challenges stemming from distributional differences between the test/user data\nand that of the training data. These disparities may encompass domain shifts , out-of-distribution variations , or even adversarial examples . \nSuch challenges significantly hinder fine-tuned modes' effectiveness in real-world applications. They fit into a specific distribution and have a poor ability to generalize to OOD data. However, LLMs perform quite well facing such scenarios because they do not have an explicit fitting process.\nMoreover, recent advancements have further enhanced the ability of language models in this regard.\nThe Reinforcement Learning from Human Feedback (RLHF) method has notably enhanced LLMs' generalization capabilities~. For example, InstructGPT demonstrates proficiency in following various instructions for a wide range of tasks and occasionally complying with instructions in different languages, even though such instructions are scarce. \nSimilarly, ChatGPT exhibits consistent advantages on most adversarial and out-of-distribution (OOD) classification and translation tasks . Its superiority in understanding dialogue-related texts led to an impressive performance on the DDXPlus dataset , a medical diagnosis dataset designed for OOD evaluation.\n\\footnotetext{As we mention in Section \\ref{sec:intro}, LLMs are pretrained on large and diverse datasets without fine-tuning, while fine-tuned models are typically pretrained on a large dataset and then further trained on a smaller, task-specific dataset to optimize their performance on that task.}", "cites": [364, 1564, 7057, 7464], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information well by connecting the issue of distributional differences in test/user data with the robustness of LLMs, particularly highlighting RLHF and ChatGPT's performance on OOD tasks. It provides a coherent narrative by integrating insights from the cited works. However, it lacks deep critical analysis, only briefly mentioning performance advantages without discussing limitations or trade-offs. The abstraction is moderate, as it identifies the broader issue of OOD generalization but does not fully develop a meta-level framework."}}
{"id": "1b7558df-4da4-47a6-adf9-d64950daf5a5", "title": "No use case", "level": "subsubsection", "subsections": [], "parent_id": "8d97a985-4400-4b64-81e8-92641bd467a1", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Practical Guide for NLP Tasks"], ["subsection", "Traditional NLU tasks"], ["subsubsection", "No use case"]], "content": "In most natural language understanding tasks, such as tasks in GLUE and SuperGLUE, fine-tuned models still have better performance, if such tasks come with rich well-annotated data and contain very few out-of-distribution examples on test sets. For different tasks and datasets, the gap between small fine-tuned models and LLMs varies. \nIn text classification, on most datasets, LLMs perform slightly worse than fine-tuned models. \nFor sentiment analysis, such as on IMDB~ and SST~, fine-tuned models and LLMs perform equally well. For toxicity detection, which is another iconic text classification task, the gap is much larger. All LLMs cannot perform well on this task, and on CivilComments~ even the best one is only better than random guessing~. On the other hand, most popular fine-tuned models can obtain much better performance~. \nand the Perspective API~\\footnote{https://perspectiveapi.com} is still one of the best for detecting toxicity. This API is powered by a multilingual BERT-based model, which is tuned on publicly available toxicity data \nand several smaller single-language CNNs distilled from this model.\nThis might be due to the fact that toxicity is defined by subtle nuances in linguistic expressions, and large language models are unable to accurately comprehend this task solely based on the provided input.  \nThe trend of performance gaps is similar in some other tasks. For natural language inference~(NLI) tasks, on most datasets, such as on RTE~ and SNLI~, fine-tuned models perform better than LLMs, while on some data such as CB~, LLMs have obtained comparable performance with fine-tuned models~. For question answering~(QA), on SQuADv2~, QuAC~ and many other datasets, fine-tuned models have superior performance, while on CoQA~, LLMs perform as well as fine-tuned models~.  \nIn information retrieval~(IR) tasks, LLMs are not widely exploited yet. One major reason is that IR tasks are fundamentally different from others. There's no natural way to transform the thousands of candidate texts into a few/zero-shot form which is required by LLMs. The existing evaluation results on MS MARCO(regular/TREC)~ show that methods based on fine-tuned models have better performance~. In this evaluation, the LLMs rank passages in an unorthodox way, which requires the LLMs to produce probabilities for passages one by one. \nFor some low-level intermediate tasks, which are not intended for regular users but rather for high level tasks, such as named entity recognition~(NER) and dependency parsing, there's not enough result coming from LLMs, because the most current evaluation of LLMs focuses on practical tasks. According to available evaluation results, for the NER task, CoNLL03~ is still a challenge for LLMs~, where the performance of fine-tuned models is around as twice as LLMs. These intermediate tasks may vanish soon because LLMs can take over high-level tasks without the help of those intermediate tasks~(e.g. dependency parsing for coding tasks; NER for some text generation tasks). \nIn brief, for most traditional NLU tasks, a fine-tuned model is a better choice in terms of the performance on benchmark datasets and the computational cost. The scale of LLMs is usually $10\\times$ or even $100\\times$ larger than fine-tuned models. One possible cause for the inferior performance of LLMs on certain tasks can be the design of instructions/prompts. Transforming input from tasks like IR and sentence labeling into a few/zero-short instruction form is non-trivial. There may be better ways to adapt language models to traditional NLP tasks in the future. On the other hand, the upper limit of capabilities of fine-tuned models is not reached, and some methods like FLAN-tuning~ can further boost the performance on NLU tasks. Another interesting finding is that on NLU tasks, after fine-tuning, masked language models, like T5, are better than most auto-regressive language models at the same scale, while some recent results imply that this gap can be bridged by scaling.", "cites": [1569, 1570, 1174, 8465, 1571, 1566, 9, 8464, 1568, 1098, 1145, 1565, 446, 1567, 1554], "cite_extract_rate": 0.8823529411764706, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively synthesizes findings from multiple benchmark datasets and models, drawing connections between different NLU tasks and the relative performance of LLMs versus fine-tuned models. It also critically evaluates the limitations of LLMs, such as their struggles with toxicity detection and IR due to task design and data structure. While it identifies broader trends, it does not fully abstract them into a generalizable framework or principle."}}
{"id": "a45a154b-7a13-4553-aec0-d2f95ef46a1c", "title": "Use case", "level": "subsubsection", "subsections": [], "parent_id": "8d97a985-4400-4b64-81e8-92641bd467a1", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Practical Guide for NLP Tasks"], ["subsection", "Traditional NLU tasks"], ["subsubsection", "Use case"]], "content": "However, there are still some NLU tasks suitable for LLMs.\nOne of the representative tasks is miscellaneous text classification~. In contrast to classic domain-specific text classification tasks such as sentiment analysis, miscellaneous text classification deals with a diverse range of topics and categories that may not have a clear or strong relationship with one another. It's closer to real-world cases and hard to be formatted for using fine-tuned models.\nAnother is the Adversarial NLI~(ANLI). It is a difficult dataset composed of adversarially mined natural language inference questions in three rounds (R1, R2, and R3). LLMs have shown superior performance on ANLI, especially on the R3 and R2. Both examples demonstrate the exceptional ability of LLMs to generalize well on out-of-distribution and sparsely annotated data in traditional NLP tasks, surpassing that of fine-tuned models. We've discussed this in the section above \\ref{sec:test_data}.", "cites": [1570, 8466], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates two cited papers by highlighting how LLMs perform well on out-of-distribution and sparsely annotated data in traditional NLU tasks, particularly using ANLI as an example. It connects the idea of adversarial benchmarking to the practical use of LLMs, showing some synthesis. However, the critical analysis is limited, and while it attempts to generalize by pointing to the broader suitability of LLMs for diverse tasks, the abstraction remains moderate without deeper meta-level insights."}}
{"id": "df2b931d-26af-45d4-8e32-1713e5a9975d", "title": "Use case", "level": "subsubsection", "subsections": [], "parent_id": "dc0358fa-c709-43bf-add5-35e70a95583a", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Practical Guide for NLP Tasks"], ["subsection", "Generation tasks"], ["subsubsection", "Use case"]], "content": "Generation tasks require models to have a comprehensive understanding of the input contents or requirements and a certain level of creativity. This is what LLMs excel at. \nFor summarization tasks, although LLMs do not have an obvious advantage over fine-tuned models under traditional automatic evaluation metrics, such as ROUGE~, human evaluation results indicate that humans tend to prefer the results generated by LLMs~ compared to that of fine-tuned models. For example, on CNN/DailyMail~ and XSUM~, fine-tuned models like Brio~ and Pegasus~ have much better performance than any LLMs w.r.t. ROUGE,  but LLMs like OPT~ perform far better in human evaluation considering all aspects including faithfulness, coherence, and relevance~. This demonstrates the superiority of LLMs in summarization tasks. On the other hand, it implies that current summarization benchmarks don't contain summaries with high quality or the automatic metrics are not proper for the evaluation of summarization.\nIn machine translation~(MT), LLMs can perform competent translation, although the average performance is slightly worse than some commercial translation tools~ considering some automatic metrics like BLEU. LLMs are particularly good at translating some low-resource language texts to English texts, such as in the Romanian-English translation of WMT'16~, zero-shot or few-shot LLMs can perform better than SOTA fine-tuned model. This is mainly due to the fact that English resources compose the main part of the pre-training data. BLOOM~ is pre-trained on more multi-lingual data, leading to better translation quality in both rich-resource and low-resource translation. Another interesting finding is that BLOOM achieves good translation quality among Romance languages, even for translation from Galician, which is not included in the pre-training data. One reasonable explanation is that texts from some languages in the same language group can help the LLMs learn more from the similarity. \nIf more multi-lingual texts can be added to the pre-training data, the translation capability may be improved further. \n Additionally, LLMs are highly skilled in open-ended generations. One example is that the news articles generated by LLMs are almost indistinguishable from real news articles by humans~.\n LLMs are remarkably adept at code synthesis as well. Either for text-code generation, such as HumanEval~ and MBPP~, or for code repairing, such as DeepFix~, LLMs can perform pretty well. GPT-4 can even pass 25\\% problems in Leetcode, which are not trivial for most human coders~. With training on more code data, the coding capability of LLMs can be improved further~. While performing well on such tasks, the codes generated by LLMs should be tested carefully to figure out any subtle bugs, which is one of the main challenges for applying LLMs in code synthesis.", "cites": [1574, 679, 8433, 1575, 1572, 1109, 1576, 7465, 1573, 9115, 8467, 7463, 1554, 7462], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.8, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes insights from multiple papers to present a coherent narrative about LLMs in generation tasks, particularly in summarization, machine translation, and code synthesis. It critically evaluates the strengths and limitations of LLMs compared to fine-tuned models and commercial tools, highlighting issues with current benchmarks and automatic evaluation metrics. It also abstracts findings to broader implications, such as the potential of pre-training data diversity and the importance of human evaluation in assessing quality."}}
{"id": "38534ecc-2634-4ce7-bc5a-773915850229", "title": "No use case", "level": "subsubsection", "subsections": [], "parent_id": "dc0358fa-c709-43bf-add5-35e70a95583a", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Practical Guide for NLP Tasks"], ["subsection", "Generation tasks"], ["subsubsection", "No use case"]], "content": "Fine-tuned models, such as DeltaLM+Zcode~, still perform best on most rich-resource translation and extremely low-resource translation tasks. In rich resource machine translation, fine-tuned models slightly outperform LLMs~. And in extremely low-resource machine translation, such as English-Kazakh translation, fine-tuned models significantly perform better than LLMs.", "cites": [1576, 1554, 7462], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.0, "critical": 2.5, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section makes a basic comparison between fine-tuned models and LLMs in the context of machine translation, citing performance differences in both rich and extremely low-resource scenarios. However, it lacks a deeper synthesis of the cited papers, which focus on different aspects (data annotation, scaling, and multilingual capabilities) rather than direct performance comparisons in translation. Some critical analysis is present in noting the limitations of LLMs in specific cases, but broader abstraction or a novel conceptual framework is missing."}}
{"id": "4d1dc079-bbff-4662-91db-5d057b42e5c3", "title": "Use case", "level": "subsubsection", "subsections": [], "parent_id": "53896e85-9b59-4292-8c97-3e89d6904421", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Practical Guide for NLP Tasks"], ["subsection", "Knowledge-intensive tasks"], ["subsubsection", "Use case"]], "content": "In general, with billions of training tokens and parameters,  LLMs have much more real-world knowledge than fine-tuned models.  \nClosed-book question-answering tasks require the model to answer a given question about factual knowledge without any external information. It does require the memorization of real-world knowledge in the model. LLMs perform better on nearly all datasets, such as on NaturalQuestions~, WebQuestions~, and TriviaQA~. On TriviaQA, even zero-shot LLMs is still much better~. \nThe massive multitask language understanding~(MMLU)~ is also highly knowledge-intensive. It contains multiple-choice questions spanning over 57 different subjects and requires general knowledge of the model. It's pretty challenging even for LLMs, although the newly released GPT-4~ outperforms existing models by a considerable margin in English with a satisfactory 86.5\\% accuracy.\nAlso, some tasks in Big-bench, which are designed to probe LLMs and extrapolate their future capabilities, heavily relied on the memorization of real-world knowledge. In such tasks, the performance of some LLMs is better than the average level of humans, and even comparable to the best human performance. For example, the task \\textit{Hindu\\_knowledge} requires models to give facts about Hindu mythology, \\textit{Periodic Elements} require the capability of predicting the element name from the periodic table and \\textit{Physics} tests the physics knowledge of models by asking for the formula needed to solve a given physics problem.", "cites": [451, 440, 9115, 7466, 1554], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to highlight how LLMs excel in knowledge-intensive tasks through their scale and memorization capabilities. It abstracts some patterns, such as the importance of training data and the correlation between model size and performance. However, it lacks deeper critical analysis, such as evaluating the limitations of memorization or the ethical implications of relying on model knowledge."}}
{"id": "d91bd4a1-b1bc-4161-af0b-784b2df163c2", "title": "Abilities Regarding Scaling", "level": "subsection", "subsections": ["8bd46f70-2108-4b07-bbf3-03ca764ca7bc", "2c1ba13d-9548-4fa8-831d-01c8e52af880", "755f4ab5-f9fb-4404-acd4-e549da46cd2e"], "parent_id": "8bfedb53-eb7f-45de-b964-352dab91a005", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Practical Guide for NLP Tasks"], ["subsection", "Abilities Regarding Scaling"]], "content": "Scaling of LLMs~(e.g. parameters, training computation, etc.) can greatly empower pretrained language models. With the model scaling up, a model generally becomes more capable in a range of tasks. Reflected in some metrics, the performance shows a power-law relationship with the model scale. For example, the cross-entropy loss which is used to measure the performance for language modeling decreases linearly with the exponential increase in the model scale, which is also called 'scaling-law'~. For some crucial abilities, such as reasoning, scaling the model has gradually transformed these abilities from a very low state to a usable state, and even approaching human capabilities. In this section, we provide an overview of the usage of LLMs in terms of the abilities and behaviors of LLMs along with scaling.\n\\begin{applebox}{Remark 5}\n\\begin{enumerate}[leftmargin=0.4cm]\n\\item With the exponential increase of model scales, LLMs become especially capable of reasoning like arithmetic reasoning and commonsense reasoning.\n\\item Emergent abilities become serendipity for uses that arise as LLMs scale up, such as ability in word manipulation and logical ability.\n\\item In many cases, performance does not steadily improve with scaling due to the limited understanding of how large language models' abilities change as they scale up.  \n\\end{enumerate}\n\\end{applebox}", "cites": [472, 7460], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from the cited papers, particularly the power-law relationship between model scale and performance, as well as the impact of training data and compute. It shows moderate abstraction by framing these findings in terms of emerging capabilities and general patterns in LLM behavior. However, the critical analysis is limited, as it does not deeply evaluate the assumptions or limitations of the scaling laws or the experimental setups in the cited works."}}
{"id": "8bd46f70-2108-4b07-bbf3-03ca764ca7bc", "title": "Use Case with Reasoning", "level": "subsubsection", "subsections": [], "parent_id": "d91bd4a1-b1bc-4161-af0b-784b2df163c2", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Practical Guide for NLP Tasks"], ["subsection", "Abilities Regarding Scaling"], ["subsubsection", "Use Case with Reasoning"]], "content": "Reasoning, which involves making sense of information, drawing inferences, and making decisions, is one of the essential aspects of human intelligence. It is challenging for NLP. Many existing reasoning tasks can be classified into commonsense reasoning and arithmetic reasoning.\n\\noindent\\textbf{Arithmetic reasoning/problem solving}. \nThe arithmetic reasoning capability of LLMs benefits greatly from the scaling of model size. For GPT-3, the ability of two-digit addition only becomes apparent when the number of parameters exceeds 13B~. Tasks to test arithmetic reasoning are trivial for humans \nand designed to challenge the capability of transferring natural language into mathematical symbols and multi-step inference. On GSM8k~, SVAMP~ and AQuA~, LLMs, as generalists, have competitive performance with most methods which have task-specific designs. And GPT-4 overperforms any other methods~, even some huge models particularly tuned for arithmetic problems~. Nevertheless, it should be noted that, without the intervention of external tools, LLMs may occasionally make mistakes in performing basic calculations, although chain-of-thought~(CoT) prompting~ can significantly improve LLMs' ability in calculations. \n\\noindent\\textbf{Commonsense reasoning}. Commonsense reasoning not only requires LLMs to remember factual knowledge but also requires LLMs to do several inference steps about the facts. Commonsense reasoning increases gradually with the growth of model size. Compared to fine-tuned models, LLMs keep the superiority on most datasets, such as StrategyQA~ and ARC-C~. Especially on  ARC-C, which contains difficult questions in science exams from grade 3 to grade 9, GPT-4 has been close to the performance of 100\\% ~(96.3\\%)~.", "cites": [1578, 679, 1580, 444, 1579, 458, 9115, 443, 1577], "cite_extract_rate": 1.0, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple cited works to present a coherent narrative on the reasoning capabilities of LLMs in arithmetic and commonsense tasks. It critically evaluates the performance of LLMs against specialized models and highlights limitations like occasional calculation errors. The section abstracts findings into broader patterns, such as the correlation between model size and reasoning performance, and the role of prompting techniques like CoT."}}
{"id": "2c1ba13d-9548-4fa8-831d-01c8e52af880", "title": "Use Cases with Emergent Abilities", "level": "subsubsection", "subsections": [], "parent_id": "d91bd4a1-b1bc-4161-af0b-784b2df163c2", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Practical Guide for NLP Tasks"], ["subsection", "Abilities Regarding Scaling"], ["subsubsection", "Use Cases with Emergent Abilities"]], "content": "Scaling of models also endows the model with some unprecedented, fantastic abilities that go beyond the power-law rule. These abilities are called \"emergent ability\". As defined in , \\textit{emergent abilities of LLMs are abilities that are not present in smaller-scale models but are present in large-scale models}. This means such abilities cannot be predicted by extrapolating the performance improvements on smaller-scale models and the model suddenly gains good performance on some tasks once the scale exceeds a certain range. The emergent ability is typically unpredictable and surprising, leading to tasks that emerge randomly or unexpectedly. We examine concrete examples of the emergent abilities of LLMs and provide them as an important reference for deciding whether to leverage LLMs' emergent abilities.\nHandling word manipulation is a typical emergent ability. It refers to the ability to learn symbolic manipulations, such as the reversed words~, in which the model is given a word spelled backwards, and must output the original word. For example. GPT-3~ shows the emergent ability for word sorting, and word unscrambling tasks. PaLM~ exhibits the emergent ability on ASCII word recognition~\\footnote{Asking models to identify the word displayed as ASCII art, https://github.com/google/BIG-bench/tree/main/bigbench/benchmark\\_tasks/ascii\\_word\\_recognition} and hyperbaton~\\footnote{Asking models to choose the English sentence with adjectives in the \"correct\" order within two choices, https://github.com/google/BIG-bench/tree/main/bigbench/benchmark\\_tasks/hyperbaton} task. \nThe logical abilities of language models tend to emerge as the model scales up, such as logical deduction, logical sequence, and logic grid puzzles. Additionally, other tasks, such as advanced coding (e.g., auto debugging, code line description), and concept understanding (e.g., novel concepts, simple Turing concepts), are also use cases with the emergent abilities of large language models.", "cites": [679, 1554], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section introduces the concept of emergent abilities and references specific examples from GPT-3 and PaLM, but it does not deeply synthesize or connect these findings across sources. It provides a few concrete examples without critical evaluation of the cited works or broader theoretical implications. However, it attempts to generalize the idea of emergent abilities as a phenomenon linked to model scale, which contributes some abstraction."}}
{"id": "755f4ab5-f9fb-4404-acd4-e549da46cd2e", "title": "No-Use Cases and Understanding", "level": "subsubsection", "subsections": [], "parent_id": "d91bd4a1-b1bc-4161-af0b-784b2df163c2", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Practical Guide for NLP Tasks"], ["subsection", "Abilities Regarding Scaling"], ["subsubsection", "No-Use Cases and Understanding"]], "content": "Although in most cases, as discussed above, larger models bring better performance, there are still many exceptions that should be considered when choosing the appropriate model. \nOn certain tasks, with the size of LLMs increasing, the performance begins to decrease, such as Redefine-math: tests whether language models are able to work with common symbols when they are redefined to mean something else; Into-the-unknown: requires the model to choose which piece of information would help answer a question; Memo-trap: asks an LM to write a phrase in a way that starts like a famous quote but ends differently\\footnote{More such tasks include: modus-tollens, pattern-matching-suppression, prompt-injection, repetitive-algebra and sig-figs. You can check them on: https://github.com/inverse-scaling/prize}. This is also called \\textit{Inverse Scaling Phenomenon}.\nAnother interesting phenomenon observed in the scaling of LLMs is called the \\textit{U-shaped Phenomenon}~. As the name implies, This phenomenon refers to that as LLM size increases, their performance on certain tasks initially improves but then starts to decline before eventually improving again, such as on: Hindsight-neglect: it tests whether language models are able to assess whether a bet was worth taking based on its expected value; NegationQA: this task takes an existing multiple-choice dataset and negates a part of each question to see if language models are sensitive to negation; Quote-repetition: it asks models to repeat back sentences given in the prompt, with few-shot examples to help it recognize the task. \nHence the risk of diminishing performance should be noted and if the task is similar to those we just discussed, careful consideration should be given to whether or not to use huge LLMs. \nGaining a deeper understanding of emergent abilities, inverse scaling phenomenon and U-shape phenomenon in LLMs is essential for advancing research in this field. In a certain sense, the U-shape phenomenon suggests that small-scale models and huge-scale models make predictions with different internal mechanisms. From this perspective, the U-shape phenomenon can be seen as a transformation of the inverse-scaling phenomenon due to some emergent abilities from sufficiently large models~. GPT-4~ exhibits a reversal of the inverse scaling phenomenon in some cases, such as on a task called Hindsight Neglect. The explanation for these behaviors of LLMs during scaling is still an open problem. Several hypotheses have been proposed. For emergent abilities, one explanation is that there may be multiple key steps for a task and the LLM cannot handle this task until it's large enough to handle every step, and another explanation is focused on the granularity of evaluation metrics~. For inverse-scaling phenomenon and u-shape phenomenon, the explanations mainly focus on the model's over-reliance on information from its prior rather than the input prompts, valid but misleading few-shot examples, and distracting easier tasks within a hard task~.", "cites": [9115, 1581], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the Inverse Scaling and U-shaped Phenomenon from multiple sources, integrating examples and explanations effectively. It provides a critical view by highlighting the limitations of larger models on certain tasks and the open questions surrounding these phenomena. However, while it identifies patterns and links between the phenomena, the abstraction level is moderate, lacking a more meta-level theoretical framework."}}
{"id": "40ab9810-5556-4e41-9750-0e49b3f51ab4", "title": "No use case", "level": "subsubsection", "subsections": [], "parent_id": "5fe8f0e4-6376-4a8f-bc4b-9868ac9f320c", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Practical Guide for NLP Tasks"], ["subsection", "Miscellaneous tasks"], ["subsubsection", "No use case"]], "content": "LLMs generally struggle with some tasks due to differences in objectives and training data.\nAlthough LLMs have achieved remarkable success in various natural language processing tasks, their performance in regression tasks has been less impressive. For example, ChatGPT's performance on the GLUE STS-B dataset, which is a regression task evaluating sentence similarity, is inferior to a fine-tuned RoBERTa performance . The  Regression tasks typically involve predicting a continuous value rather than a discrete label, posing unique challenges for LLMs. One primary reason for their subpar performance is the inherent difference between the language modeling objective and the regression task objective. LLMs are designed to predict the next word in a sequence or generate coherent text, with their pre-training focused on capturing linguistic patterns and relationships. Consequently, their internal representations may not be well-suited for modeling continuous numerical outputs. \nBesides, LLMs have predominantly been trained on text data, focusing on capturing the intricacies of natural language processing. As a result, their performance on multimodal data, which involves handling multiple data types such as text, images, audio, video, actions, and robotics, remains largely unexplored. And fine-tuned multimodal models, like BE\\textsc{i}T and PaLI~, still dominate many tasks such as visual question answering~(VQA) and image captioning. Nonetheless, the recently introduced GPT-4~ has taken the step in multimodal fusion, but there is still a lack of detailed evaluation of its capabilities.", "cites": [7467, 9115, 1582, 1583], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates key findings from the cited papers to explain why LLMs are not well-suited for regression and multimodal tasks. It makes some connections between model objectives and performance limitations, but the analysis is largely descriptive with limited abstraction or deep critique of the methodologies or assumptions in the cited works."}}
{"id": "85fadad0-1368-4d30-9626-74f1ecc1c17b", "title": "Use case", "level": "subsubsection", "subsections": [], "parent_id": "5fe8f0e4-6376-4a8f-bc4b-9868ac9f320c", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Practical Guide for NLP Tasks"], ["subsection", "Miscellaneous tasks"], ["subsubsection", "Use case"]], "content": "LLMs are particularly suitable for certain tasks. \nLLMs are very good at mimicking humans, acting as a chatbot, and performing various kinds of tasks. The LLMs-powered ChatGPT\\footnote{https://chat.openai.com} is surprising for its consistency, reliability, informativeness, and robustness during multiple utterances with humans. The human-feedback procedure  plays an important role in acquiring such abilities\nLLMs can both act as a good annotator and data generator for data augmentation, such as in. Some LLMs have been found as good as human annotators~ in some tasks. And the collected texts from GPT-3.5~(text-davinci-003) have been used as human-like instruction-following demonstrations to train other language models~. \nLLMs can also be used for quality assessment on some NLG tasks, such as summarization and translation. On summarization tasks, GPT-4 as an evaluator achieves a higher correlation with humans than other methods with a large margin~. Some other evaluators based on LLMs~ also show good human alignment in more NLG tasks, especially compared with traditional automatic metrics. But the LLM evaluator may have a bias towards the LLM-generated texts~.\nAlso, as we discussed above, some abilities of LLMs bring bonuses in addition to performance improvement, such as interpretability. The CoT reasoning ability of LLMs can show how an LLM reaches the prediction, which is a good interpretation on the instance level, while it also improves the performance.", "cites": [1576, 1586, 9142, 8468, 1584, 1585, 1561], "cite_extract_rate": 0.6363636363636364, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to highlight LLMs' versatility in tasks like data annotation, generation, and NLG evaluation. It provides some critical points, such as the potential bias of LLM evaluators toward their own outputs. However, it stops short of deeper comparative or meta-level analysis and lacks a novel framework or comprehensive evaluation of trade-offs across the cited works."}}
{"id": "1371d8e3-f812-4bcf-b123-202530b09d42", "title": "Real world \"tasks\"", "level": "subsection", "subsections": [], "parent_id": "8bfedb53-eb7f-45de-b964-352dab91a005", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Practical Guide for NLP Tasks"], ["subsection", "Real world \"tasks\""]], "content": "In the last part of this section, we would like to discuss the usage of LLMs and fine-tuned models in real-world \"tasks\". We use the term \"tasks\" loosely, as real-world scenarios often lack well-formatted definitions like those found in academia. Many requests to models even cannot be treated as NLP tasks. Models face challenges in the real world from three perspectives:\n\\begin{itemize}\n    \\item \\textbf{Noisy/Unstructured input}. Real-world input comes from real-world non-experts. They have little knowledge about how to interact with the model or even cannot use texts fluently. As a result, real-world input data can be messy, containing typos, colloquialisms, and mixed languages, unlike those well-formed data used for pre-training or fine-tuning. \n    \\item \\textbf{Tasks not formalized by academia}.In real-world scenarios, tasks are often ill-defined by academia and much more diverse than those in academic settings. Users frequently present queries or requests that do not fall neatly into predefined categories, and sometimes multiple tasks are in a single query. \n    \\item \\textbf{Following users' instructions}. A user's request may contain multiple implicit intents~(e.g. specific requirement to output format), or their desired predictions may be unclear without follow-up questions. Models need to understand user intents and provide outputs that align with those intents. \n\\end{itemize}\nEssentially, these challenges in the real world come from that users' requests deviate significantly from the distribution of any NLP datasets designed for specific tasks. Public NLP datasets are not reflective of how the models are used~. \n\\begin{applebox}{Remark 7}\n    LLMs are better suited to handle real-world scenarios compared to fine-tuned models. However, evaluating the effectiveness of models in the real world is still an open problem.\n\\end{applebox}\nHandling such real-world scenarios requires coping with ambiguity, understanding context, and handling noisy input. Compared to fine-tuned models, LLMs are better equipped for this because they have been trained on diverse data sets that encompass various writing styles, languages, and domains. Additionally, LLMs demonstrate a strong ability to generate open-domain responses, making them well-suited for these scenarios. Fine-tuned models, on the other hand, are often tailored to specific, well-defined tasks and may struggle to adapt to new or unexpected user requests. They heavily rely on clear objectives and well-formed training data that specify the types of instructions the models should learn to follow. Fine-tuned models may struggle with noisy input due to their narrower focus on specific distributions and structured data. An additional system is often required as an assistant for fine-tuned models to process unstructured context, determine possible intents, and refine model responses accordingly.\nAdditionally, some mechanics such as instruction tuning~ and human alignment tuning~ further boost the capabilities of LLMs to better comprehend and follow user instructions. These methods improve the model's ability to generate helpful, harmless, and honest responses while maintaining coherence and consistency~. While both methods can make LLMs better generalize to unseen tasks and instructions, it has been noticed that while human labelers prefer models tuned for human alignment~ to models tuned with instructions from public NLP tasks, such as FLAN~ and T0~.  The reason may be similar to reasons for fine-tuned models' inferiority: public NLP tasks/datasets are designed for easy and automatic evaluation, and they can only cover a small part of real-world usage.\nOne of the main issues when it comes to real-world scenarios is how to evaluate whether the model is good or not. Without any formalized tasks or metrics, the evaluation of model effectiveness can only rely on feedback from human labelers. Considering the complexity and cost of human evaluation, there's no massive and systematic comparison between fine-tuned models and LLMs yet. Nevertheless, the huge success and popularity of LLMs such as chatGPT, have confirmed the superiority of LLMs to some extent.", "cites": [1587, 364, 8469], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple ideas from the cited papers (instruction tuning, human alignment, and multitask learning) and connects them to the broader context of real-world task handling by LLMs. It offers critical analysis by contrasting LLMs and fine-tuned models in terms of adaptability, performance on unstructured input, and reliance on formalized datasets. The abstraction is strong as it generalizes the findings into principles about the nature of real-world tasks versus academic datasets."}}
{"id": "7141aff7-7e78-47a2-9234-7f2210caca46", "title": "Cost", "level": "paragraph", "subsections": ["a3a2a7f0-6713-43d4-b283-83b86312b545", "00fb88d7-4ae2-4791-8f99-852ee021df33"], "parent_id": "f0e841b3-8f4d-49ae-ba5d-a934fbb0c47a", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Other considerations"], ["subsection", "Efficiency"], ["paragraph", "Cost"]], "content": "LLMs have grown increasingly larger in recent years, with models such as GPT-1, GPT-2, and GPT-3 featuring 117 million, 1.5 billion, and 175 billion parameters, respectively. The cost of training an LLM is heavily influenced by its size, with estimates suggesting that training the 11B parameter variant of T5 costs well over \\$1.3 million for a single run, while a single training run of GPT-3 175B requires \\$4.6 million~.\nThe energy consumption for training large models is equally impressive. The total energy consumption for training a transformer model with 6B parameters to completion is estimated to be around 103.5 MWh~. Google reports that training PaLM consumed about 3.4 GWh in about two months~.\nFurthermore, the dataset size also scales rapidly with the size of the model, with GPT-3 175B trained on 499 billion tokens~. Another key metric that reflects the computing cost is Flops, with GPT-3 175B requiring $3.14 \\times 10^{23}$ Flops, while a T5 11B model only requires $3.30 \\times 10^{22}$, which is 10 times less.\nIn addition to these costs, hardware requirements are also substantial.\nOpenAI has collaborated with Microsoft on a supercomputer hosted in the Microsoft Azure cloud, consisting of 285k CPU cores and 10k high-end GPUs to support the training of large models.\nFor users of the OpenAI API, pricing varies based on the model and usage, with options such as GPT-3.5-turbo charging \\$0.002 per 1k tokens for chat service. However, for users who require custom models, training costs \\$0.03 per 1k tokens, while usage costs \\$0.12 per 1k tokens~.\nTherefore, for users who cannot afford such a large cost, such as small startups, individual users, etc., a small, fine-tuned model is a better and more reasonable choice.", "cites": [679, 1588], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.8}, "insight_level": "low", "analysis": "The section provides factual details about the cost of training and using LLMs, citing specific numbers from various sources. However, it lacks synthesis beyond simple aggregation and does not critically analyze or compare these findings across papers. The abstraction is minimal, focusing on concrete metrics rather than deriving general principles or implications for broader use cases."}}
{"id": "00fb88d7-4ae2-4791-8f99-852ee021df33", "title": "Parameter-Efficient Tuning", "level": "paragraph", "subsections": [], "parent_id": "7141aff7-7e78-47a2-9234-7f2210caca46", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Other considerations"], ["subsection", "Efficiency"], ["paragraph", "Cost"], ["paragraph", "Parameter-Efficient Tuning"]], "content": "In practice, we may tune the model on some specific datasets. Parameter-Efficient Tuning (PET) is an efficient technique to tune a small portation of model parameters (or extra parameters) while freezing most parameters of the pre-trained LLMs. The main goal of PEFT is to greatly decrease the computational and storage costs while keeping the performance of the original models. The common techniques for PET are LoRA~, Prefix Tuning~, P-Tuning~. As an illustration, the LoRA method maintains the weights of the pre-trained model and incorporates low-rank matrices into every layer of the Transformer architecture. This approach considerably minimizes the number of parameters that require training for subsequent tasks, thereby increasing overall efficiency. Alpaca-LoRA\\footnote{https://github.com/tloen/alpaca-lora} proposes integrating Low-Rank Adaptation (LoRA) into LLaMA-Alpaca, which enables runs LLaMA within hours on a single RTX 4090. All these PFT methods can be helpful either for fine-tuning a model to a specific task or tuning LLMs to meet special requirements like human alignment.", "cites": [1589, 1590, 1591], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic overview of parameter-efficient tuning methods, mentioning LoRA, Prefix Tuning, and P-Tuning with references to their respective papers. It synthesizes the common goal of reducing computational costs but does not deeply integrate or contrast the approaches. There is minimal critical evaluation or abstraction to broader principles, making the analysis primarily descriptive with some low-level synthesis."}}
{"id": "f734ffb5-17e3-4bf6-a0ec-98a1940f21e8", "title": "Robustness and Calibration", "level": "paragraph", "subsections": ["c2f1af17-91a6-4092-a7ef-cb6dce389eaf", "0ed97184-e734-48cc-9123-7af7d4155541"], "parent_id": "ea598959-0f56-4d8e-a556-bc38157b9a19", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Other considerations"], ["subsection", "Trustworthiness"], ["paragraph", "Robustness and Calibration"]], "content": "The accuracy and robustness of the LLMs are shown to have a very strong correlation~. The models that have high accuracy on the scenario also have good robustness. However, the robustness of the zero-shot becomes worse after being tuned on extra application-specific tasks data~. This may due to overfitting, which leads to poor generalizability due to the extremely high complexity of the model and the limited training samples from downstream tasks~.\nIn a similar vein, it has been observed that fine-tuning a model can result in significant miscalibrations, owing to over-parameterization~. Therefore, fine-tuned models may not be an optimal choice when robustness and calibration are critical considerations.\nHowever, human alignment has been found as a potential solution for enhancing model robustness. InstructGPT davinci v2 (175B*) has been shown to outperform other models in terms of robustness. On the other hand, achieving optimal calibration of the model depends on the scenario and adaptation procedure employed.", "cites": [1570, 1593, 1592], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes three distinct papers to present a coherent narrative on robustness and calibration in LLMs, highlighting the trade-offs between fine-tuning and model behavior. It critically analyzes the effects of fine-tuning on robustness and calibration, noting issues like overfitting and miscalibration. While it identifies some general patterns (e.g., over-parameterization as a common issue), it stops short of proposing overarching principles or frameworks."}}
{"id": "c2f1af17-91a6-4092-a7ef-cb6dce389eaf", "title": "Fairness and Bias", "level": "paragraph", "subsections": [], "parent_id": "f734ffb5-17e3-4bf6-a0ec-98a1940f21e8", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Other considerations"], ["subsection", "Trustworthiness"], ["paragraph", "Robustness and Calibration"], ["paragraph", "Fairness and Bias"]], "content": "LLMs have been shown to exhibit disparate treatment and impact, perpetuating societal biases and potentially leading to discrimination~. To ensure fairness and equity for all users, it is crucial to address these issues in the development and deployment of NLP models. Disparities in performance between demographic groups can serve as an indicator of fairness problems.\nLLMs are particularly susceptible to fairness issues, as significant performance disparities have been observed across demographic categories such as dialect, religion, gender, and race~. However, research has shown that aligning models with human instructions can improve LLM performance regardless of their size, with the InstructGPTmodel~(davinci v2)  exhibiting smaller performance disparities than other LLMs~.", "cites": [1570, 7468], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes information from the cited papers by connecting the issue of fairness and bias in LLMs with the effectiveness of instruction-based fine-tuning. It provides a basic comparison by referencing how InstructGPT (davinci v2) performs better in terms of reducing disparities than other models. However, it lacks deeper critical evaluation of the methods or limitations of the cited works and only begins to generalize by linking fairness with training approaches without fully abstracting broader principles."}}
{"id": "0ed97184-e734-48cc-9123-7af7d4155541", "title": "Spurious Biases", "level": "paragraph", "subsections": [], "parent_id": "f734ffb5-17e3-4bf6-a0ec-98a1940f21e8", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Other considerations"], ["subsection", "Trustworthiness"], ["paragraph", "Robustness and Calibration"], ["paragraph", "Spurious Biases"]], "content": "The shortcut learning problem has been observed in various natural language understanding tasks under the pretraining and fine-tuning paradigm, where models heavily rely on spurious correlations between input and labels in the fine-tuning data for prediction . For example, in reading comprehension tasks, fine-tuned models tend to focus on the lexical matching of words between the question and the original passage, neglecting the intended reading comprehension task itself . \nIn contrast, large language models are not directly trained on fine-tuned datasets, which makes it less likely for them to learn shortcut features present in the fine-tuned dataset, thereby enhancing the model's generalization capabilities. However, LLMs are not infallible and may exhibit some shortcut learning during in-context learning. For example, recent preliminary studies have begun investigating the robustness of prompt-based methods in large-scale language models . One such study evaluates the few-shot learning performance of GPT-3 on text classification and information extraction tasks . and reveal that the examined LLMs are susceptible to majority label bias and position bias, where they tend to predict answers based on the frequency or position of the answers in the training data. Moreover, these LLMs exhibit common token bias, favoring answers that are prevalent in their pre-training corpus. Recent studies show that this positional bias can be mitigated by selecting proper prompts .\nIn summary, while LLMs significantly reduce the shortcut learning problem prevalent in fine-tuned models, they still exhibit some shortcut learning issues and should be approached with caution when deploying them in downstream applications.", "cites": [1594, 1595, 1596, 8470, 8471, 1597], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple cited papers to explain the concept of spurious biases in LLMs, particularly in the context of in-context learning. It critically discusses the types of biases (e.g., majority label, position, and common token bias) and contrasts LLMs with fine-tuned models, showing awareness of both limitations and mitigation strategies. The abstraction level is strong, as it generalizes the phenomenon of shortcut learning across different NLP paradigms."}}
{"id": "8ca777cb-b1a7-42cb-88ed-4b9e175861af", "title": "Safety challenges", "level": "subsection", "subsections": ["9b79094e-a1ad-490d-aec6-2aea389d1b8c"], "parent_id": "ff91c03f-4bf0-4721-898f-5219db670483", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Other considerations"], ["subsection", "Safety challenges"]], "content": "LLMs have demonstrated their extremely strong capabilities in many areas such as reasoning, knowledge retention, and coding. As they become more powerful and human-like, their potential to influence people's opinions and actions in significant ways grows. As a result, some new safety challenges to our society should be considered and have caught lots of attention in recent works~.", "cites": [9115], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly mentions the capabilities of LLMs and the resulting safety challenges but fails to synthesize information from the cited paper meaningfully. It does not critically analyze the technical or societal implications, nor does it abstract broader patterns or principles from the cited work. The content remains superficial and lacks depth."}}
{"id": "9b79094e-a1ad-490d-aec6-2aea389d1b8c", "title": "Hallucinations", "level": "paragraph", "subsections": ["e2bac6f3-e69e-48de-8eff-74f68663d0f4", "bac6b5f9-b7c5-47a6-915f-6aa4c0ba130a"], "parent_id": "8ca777cb-b1a7-42cb-88ed-4b9e175861af", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Other considerations"], ["subsection", "Safety challenges"], ["paragraph", "Hallucinations"]], "content": "The potential for LLMs to \"hallucinate,\" or generate nonsensical or untruthful content, can have significant negative impacts on the quality and reliability of information in various applications. As LLMs become increasingly convincing and believable, users may develop an overreliance on them and trust them to provide accurate information in areas with which they are somewhat familiar. This can be particularly dangerous if the model produces content that is entirely false or misleading, leading to incorrect decisions or actions taken based on that information. Such outcomes can have serious consequences in many domains, such as healthcare, finance, or public policy, where the accuracy and reliability of information are critical. To mitigate these issues, reinforcement learning from human feedback (RLHF) is widely used~  and LLMs themselves have been integrated into the loop~.", "cites": [364], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section introduces the concept of hallucinations and their implications, citing one paper that discusses alignment through human feedback. It synthesizes the idea to some extent by linking hallucinations to the need for RLHF but lacks deeper connections to broader themes or multiple sources. There is limited critical analysis, as it does not question the effectiveness of RLHF or explore alternative solutions. The abstraction level is modest, focusing on domain-specific consequences rather than general principles or patterns across LLM behaviors."}}
{"id": "e2bac6f3-e69e-48de-8eff-74f68663d0f4", "title": "Harmful content", "level": "paragraph", "subsections": [], "parent_id": "9b79094e-a1ad-490d-aec6-2aea389d1b8c", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Other considerations"], ["subsection", "Safety challenges"], ["paragraph", "Hallucinations"], ["paragraph", "Harmful content"]], "content": "Due to the high coherence, quality, and plausibility of texts generated by LLMs, harmful contents from LLMs can cause significant harm, including hate speech, discrimination, incitement to violence, false narratives, and even social engineering attack. The implementation of safeguards to detect and correct those contents can be mitigation~. These LLMs can also have dual-use potential by providing required illicit information, leading to risks such as the proliferation of weapons~ and even terrorism attack planning. It is crucial to ensure using these LLMs responsibly, with safeguards in place to prevent harm. Also, in existing work, feedback from humans plays an important role in getting rid of harmful outputs.", "cites": [1598], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of harmful content generated by LLMs and mentions the role of human feedback, but it lacks substantial synthesis of the cited paper or deeper analysis. It does not compare different approaches, evaluate their effectiveness, or generalize to broader principles beyond the specific concerns raised."}}
{"id": "36ebc08b-941e-409b-9405-648aacfcf028", "title": "Conclusion and Future Challenges", "level": "section", "subsections": [], "parent_id": "65198946-bcab-42d9-bc99-da3a8beb9233", "prefix_titles": [["title", "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"], ["section", "Conclusion and Future Challenges"]], "content": "Recent advances in large language models have been revolutionizing the field of natural language processing. Effectively using LLMs requires understanding their capabilities, and limitations for various NLP tasks. This work presents a practical guide to working with LLMs for downstream NLP tasks. We first discuss prominent models like GPT-style and BERT-style architectures and the factors influencing their performance. We then explore using LLMs for downstream tasks, including knowledge-intensive tasks, NLU, and NLG tasks, as well as providing concrete examples of successes and limitations. This practical guide offers insights into LLMs and best practices for harnessing LLMs across NLP tasks. We hope it would enable researchers and practitioners to leverage their potential and drive innovation in language technologies.\nIn the following, we figure out the future challenges of the LLMs:\n\\begin{itemize}[leftmargin=0.4cm]\n    \\item\\textbf{Evaluation of proposed models on real-world ``datasets''.} \n    While existing deep learning models are primarily evaluated on standard academic datasets, such as ImageNet, which have been milestones in deep learning development. However, the limitations of standard academic datasets can not exactly reflect real-world performance. As models advance, it is crucial to assess them on more diverse, complex, and realistic data that reflect real-world needs. Evaluating models on real-world ``datasets'', in addition to academic ones, will provide a more rigorous test of their capabilities, as well as a better understanding of their effectiveness in real-world applications. This ensures that the models are capable of addressing real-world challenges and delivering practical solutions. \n    \\item \\textbf{Model Alignment.} \n    Ensuring that increasingly powerful and autonomous models align with human values and priorities is essential. Methods must be developed to guarantee that these models behave as intended and do not optimize for undesirable outcomes. It is crucial to integrate alignment techniques from the start of the model development process. Model transparency and interpretability are also important factors for evaluating and ensuring alignment. Additionally, as we look toward the future, an even more daunting challenge looms: aligning superhuman systems. While this task is currently beyond our demands, it is important to consider and prepare for the potential implications of aligning such advanced systems, as they may present unique complexities and ethical concerns~. \n    \\item\\textbf{Safety Alignment.} While discussion of AI existential risks is important, concrete research is needed to guarantee the safe development of advanced AI. This includes techniques for interpretability, scalable oversight and governance, and formal verification of model properties. Safety should be considered not just an add-on but an integral part of the model-building process.\n    \\item\\textbf{Performance Prediction with Scaling.} It is difficult to anticipate how model performance will change as model size and complexity increases dramatically. Developing methods to better predict model performance after scaling up or as new architectures are developed would allow for more efficient use of resources and accelerated progress. Some possibilities include: training a smaller 'seed' model and extrapolating its growth, simulating the effects of increased scale or model tweaks, and benchmarking iterations of the model at different scales to build scaling laws. These could provide insight into the performance of models even before they are built.\n\\end{itemize}\n\\bibliographystyle{plain}\n\\bibliography{./H.J, ./Q.F, ./X.H, ./R.X}\n\\end{document}", "cites": [8472, 1599], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section touches on analytical themes by identifying future challenges and linking them to broader concerns such as model alignment and safety. However, it synthesizes ideas from cited papers only minimally and lacks a deeper, comparative or evaluative analysis of these works. The abstraction level is modest, as the section outlines general challenges without deriving meta-level principles or trends."}}
