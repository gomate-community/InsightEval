{"id": "4e7495bb-e63c-494e-a5e5-3266c6508430", "title": "Introduction", "level": "section", "subsections": ["e67e16ee-5c30-4e37-a53b-e46d5c2962af"], "parent_id": "03ca5aea-6a90-479f-934f-d9ab95896c40", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Introduction"]], "content": "\\label{sec:intro}\n\\emph{Machine Reading Comprehension (MRC)} has the long-standing goal of developing machines that can reason with natural language. A typical reading comprehension task consists in answering questions about the background knowledge expressed in a textual corpus. Recent years have seen an explosion of models and architectures due to the release of large-scale benchmarks, ranging from open-domain  to commonsense  and scientific  reading comprehension tasks. Research in MRC is gradually evolving in the direction of abstractive inference capabilities, going beyond what is explicitly stated in the text .\nAs the need to evaluate abstractive reasoning becomes predominant, a crucial requirement emerging in recent years is \\emph{explainability} , intended as the ability of a model to expose the underlying mechanisms adopted to arrive at the final answers. Explainability has the potential to tackle some of the current issues in the field:", "cites": [456, 444, 1798, 439, 460, 4274, 8761, 4275], "cite_extract_rate": 1.0, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The introduction provides a basic synthesis of MRC benchmarks and their focus on different reasoning aspects, such as commonsense and multi-hop reasoning, by mentioning several datasets. However, it does not deeply connect the cited papers to form a novel narrative or highlight their relationships in a structured way. There is minimal critical analysis or identification of broader principles; the section primarily sets up the importance of explainability without evaluating the cited works in depth."}}
{"id": "e67e16ee-5c30-4e37-a53b-e46d5c2962af", "title": "Evaluation:", "level": "paragraph", "subsections": ["97b2bd5c-5cbd-44cb-8a30-f0b5c5f07c99"], "parent_id": "4e7495bb-e63c-494e-a5e5-3266c6508430", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Introduction"], ["paragraph", "Evaluation:"]], "content": "Traditionally, MRC models have been evaluated on end-to-end prediction tasks. In other words, the capability of achieving high accuracy on specific datasets has been considered a proxy for evaluating a desired set of reasoning skills. However, recent work have demonstrated that this is not necessarily true for models based on deep learning, which are particularly capable of exploiting biases in the data . Research in explainability can provide novel evaluation frameworks to investigate and analyse the internal reasoning mechanisms ;", "cites": [4277, 2396, 4276, 4278], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates the idea that traditional MRC evaluation is insufficient and connects it to the concept of explainability as a means to better assess internal reasoning. It cites multiple related papers to support this argument, showing a basic synthesis. While it mentions the limitations of deep learning models and the potential of explanation-based evaluation, it does not deeply compare or critique the cited works. It generalizes the issue to MRC evaluation but lacks higher-level abstraction or a novel framework."}}
{"id": "97b2bd5c-5cbd-44cb-8a30-f0b5c5f07c99", "title": "Generalisation:", "level": "paragraph", "subsections": ["f4173d5c-3a3b-441f-8811-b06630ceb118"], "parent_id": "e67e16ee-5c30-4e37-a53b-e46d5c2962af", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Introduction"], ["paragraph", "Evaluation:"], ["paragraph", "Generalisation:"]], "content": "Despite remarkable performance achieved in specific MRC tasks, machines based on deep learning still suffer from overfitting and lack of generalisation. By focusing on explicit reasoning methods, research in explainability can lead to the development of novel models able to perform compositional generalisation  and discover abstract inference patterns in the data , favouring few-shot learning and cross-domain transportability ;", "cites": [7802, 7801, 4279, 4275], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a limited synthesis of the cited papers, connecting them through the theme of reasoning and generalisation but not deeply integrating their contributions. It offers minimal critical analysis and does not evaluate or contrast the approaches in detail. However, it does attempt to generalize by suggesting that explicit reasoning methods in explainable MRC could foster few-shot learning and cross-domain transportability, indicating a moderate level of abstraction."}}
{"id": "f85ddc30-cc46-47dc-98bd-bb29ed2b0a20", "title": "Explainability in Machine Reading Comprehension", "level": "section", "subsections": ["329b2d11-acc7-4022-989c-f8e798e6b4af"], "parent_id": "03ca5aea-6a90-479f-934f-d9ab95896c40", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Explainability in Machine Reading Comprehension"]], "content": "\\label{sec:explanation_as_nli}\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.95\\textwidth]{abstraction_2.pdf}\n\\caption{Dimensions of explainability in Machine Reading Comprehension.}\n\\label{fig:approach}\n\\end{figure}\nIn the field of Explainable AI, there is no consensus, in general, on the nature of explanation . As AI embraces a variety of tasks, the resulting definition of explainability is often fragmented and dependent on the specific scenario. Here, we frame the scope of the survey by investigating the dimensions of explainability in MRC.", "cites": [1798], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section begins with a general observation about the lack of consensus on explainability in AI and connects it to the context of MRC by referencing a cited paper. It introduces the survey's framing of explainability dimensions but does not deeply synthesize multiple sources or provide detailed comparisons or critiques. The inclusion of a figure suggests an abstract framework, but the narrative remains relatively high-level without in-depth analysis."}}
{"id": "329b2d11-acc7-4022-989c-f8e798e6b4af", "title": "The scope of explainability.", "level": "paragraph", "subsections": ["0e6e5551-33c9-4797-b4df-659c6055693e"], "parent_id": "f85ddc30-cc46-47dc-98bd-bb29ed2b0a20", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Explainability in Machine Reading Comprehension"], ["paragraph", "The scope of explainability."]], "content": "We refer to \\emph{explainability} as a specialisation of the higher level concept of \\emph{interpretability}. In general, interpretability aims at developing tools to understand and investigate the behaviour of an AI system. This definition also includes tools that are external to a black-box model, as in the case of post-hoc interpretability  . On the other hand, the goal of explainability is the design of \\emph{inherently interpretable} models, capable of performing transparent inference through the generation of an \\emph{explanation} for the final prediction .\nIn general, an explanation can be seen as an answer to a \\emph{how} question formulated as follows: \\emph{``How did the model arrive at the conclusion $c$ starting from the problem formulation $p$?''}. In the context of MRC, the answer to this question can be addressed by exposing the internal reasoning mechanisms linking $p$ to $c$. This goal can be achieved in two different ways: \n\\begin{enumerate}\n    \\item \\textbf{Knowledge-based explanation:} exposing part of the relevant background knowledge connecting $p$ and $c$ in terms of supporting facts and/or inference rules;  \n    \\item \\textbf{Operational explanation:} composing a set of atomic operations through the generation of a symbolic program, whose execution leads to the final answer $c$.\n\\end{enumerate}\nGiven the scope of explainability in MRC, this survey reviews recent developments in \\emph{knowledge-based} and \\emph{operational explanation} (Sec. 4), emphasising the problem of \\emph{explanatory relevance} for the former -- i.e. the identification of relevant information for the construction of explanations, and of \\emph{question decomposition} for the latter -- i.e. casting a problem expressed in natural language into an executable program.", "cites": [7303, 1798], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key definitions of interpretability and explainability from the cited papers and distinguishes them effectively, forming a coherent conceptual framework. It abstracts these ideas into two distinct explanation types—knowledge-based and operational—offering a structured understanding relevant to MRC. However, the critical analysis is limited to pointing out general challenges without deeper evaluation or comparison of the cited works' approaches."}}
{"id": "0e6e5551-33c9-4797-b4df-659c6055693e", "title": "Explanation and abstraction.", "level": "paragraph", "subsections": [], "parent_id": "329b2d11-acc7-4022-989c-f8e798e6b4af", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Explainability in Machine Reading Comprehension"], ["paragraph", "The scope of explainability."], ["paragraph", "Explanation and abstraction."]], "content": "\\begin{table}[t]\n\\small\n\\centering\n\\ra{0.9}\n\\begin{tabular}{@{}lp{6.5cm}p{6.5cm}@{}}\n\\toprule\n\\multirow{2}{*}{} &\n\\multirow{2}{*}{\\textbf{Extractive MRC}} &\n\\multirow{2}{*}{\\textbf{Abstractive MRC}}\\\\\\\\\n\\midrule\n\\textbf{Question} & When was Erik Watt's father born? & What is an example of a force producing heat?\\\\\n\\textbf{Answer} & May 5, 1939 & Two sticks getting warm when rubbed together\\\\\n\\midrule\n\\textbf{Explanation} & (1) He (Erik Watt) is the son of WWE Hall of Famer Bill Watts; (2) William F. Watts Jr. (born May 5, 1939) is an American former professional wrestler, promoter, and WWE Hall of Fame Inductee (2009). & (1) A stick is a kind of object; (2) To rub together means to move against; (3) Friction is a kind of force; (4) Friction occurs when two object's surfaces move against each other; (5) Friction causes the temperature of an object to increase.\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Explanations for extractive~\\protect and abstractive~\\protect MRC.}\n\\label{tab:extractive_abstractive}\n\\end{table}\nDepending on the nature of the MRC problem, a complete explanation can include pieces of evidence at different levels of abstraction (Fig. \\ref{fig:approach}.3). Traditionally, the field has been divided into \\emph{extractive} and \\emph{abstractive} tasks (e.g. table \\ref{tab:extractive_abstractive}). In extractive MRC, the reasoning required to solve the task is derivable from the original problem formulation. In other words, the correct decomposition of the problem provides the necessary inference steps for the answer, and the role of the explanation is to fill an information gap at the \\emph{extensional level} -- i.e. identifying the correct arguments for a set of predicates, via paraphrasing and coreference resolution. As a result, explanations for extractive MRC are often expressed in the form of supporting passages retrieved from the contextual paragraphs . On the other hand, abstractive MRC tasks usually require going beyond the surface form of the problem with the inclusion of high level knowledge about abstract concepts. In this case, the explanation typically leverages the use of supporting definitions, including taxonomic relations and essential properties, to perform abstraction from the original context in search of high level rules and inference patterns . As the nature of the task impacts explainability, we consider the distinction between extractive and abstractive MRC throughout the survey, categorising the reviewed benchmarks and approaches according to the underlying reasoning capabilities involved in the explanations.", "cites": [460], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a clear analytical framework by distinguishing between extractive and abstractive MRC tasks and relating them to the level of abstraction required in explanations. It integrates the cited paper (e.g., HotpotQA) to support this distinction but does not deeply connect multiple sources. While it offers a structured view of the problem space, it lacks in-depth critical evaluation of the cited works and primarily outlines the general trends without identifying significant limitations or contradictions."}}
{"id": "15e67409-9476-47aa-8a47-bfcb4ac8569f", "title": "Explanation-supporting Benchmarks", "level": "section", "subsections": ["6d576fba-6b66-4a43-adda-53c2ca4d62f2"], "parent_id": "03ca5aea-6a90-479f-934f-d9ab95896c40", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Explanation-supporting Benchmarks"]], "content": "In this section we review the benchmarks that have been designed for the development and evaluation of explainable reading comprehension models. Specifically, we classify a benchmark as \\emph{explanation-supporting} if it exhibits the following properties:\n\\begin{enumerate}\n    \\item \\textbf{Labelled data for training on explanations:} The benchmark includes gold explanations that can be adopted as an additional training signal for the development of explainable MRC models.\n    \\item \\textbf{Design for quantitative explanation evaluation:} The benchmark supports the use of quantitative metrics for evaluating the explainability of MRC  systems, or it is explicitly constructed to test explanation-related inference.\n\\end{enumerate}\nWe exclude from the review all the datasets that do not comply with at least one of these requirements. For a complete overview of the existing benchmarks in MRC, the reader is referred to the following surveys:   .\nThe resulting classification of the datasets with their highlighted properties is reported in Table \\ref{tab:explainable_datasets}. The benchmarks are categorised according to a set of dimensions that depend on the nature of the task -- i.e. domain, format, MRC type, multi-hop inference, and the characteristics of the explanations -- i.e. explanation type, explanation level, format of the background knowledge, and explanation representation.\n\\begin{table}[t]\n\\small\n\\centering\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{>{\\raggedright}p{3.5cm}p{12.5cm}}\n    \\toprule\n     \\textbf{Domain}    & The knowledge domain of the MRC task -- i.e. open domain (OD) , science (SCI),  or commonsense (CS).\\\\\n     \\textbf{Format} &   The task format -- i.e. span retrieval (Span), free-form (Free), multiple-choice (MC), textual entailment (TE).\\\\\n     \\textbf{MRC Type} & The reasoning capabilities involved -- i.e. Extractive (Extr.), Abstractive (Abstr.).\\\\\n     \\textbf{Multi-hop (MH) } & Whether the task requires the explicit composition of multiple facts to infer the answer.\\\\\n     \\midrule\n     \\textbf{Explanation Type (ET)} & The type of explanation -- i.e. knolwedge-based (KB) or operational (OP).\\\\\n      \\textbf{Explanation Level (EL)} & The abstraction level of the explanations -- i.e. Extensional (E) or Intensional (I).\\\\\n     \\textbf{Background Knowledge (BKG)} & The format of the provided background knowledge, if present, from which to extract or construct the explanations -- i.e. single paragraph (SP), multiple paragraph (MP), sentence corpus (C), table-store (TS), suit of atomic operations (AO).\\\\\n     \\textbf{Explanation Representation (ER) } & The explanation representation -- i.e. single passage (S), multiple passages (M), facts composition (FC), explanation graph (EG), generated sentence (GS), symbolic program (PR).\\\\\n    \\bottomrule\\\\\n    \\end{tabular}}\n\\small\n\\centering\n\\begin{tabular}{p{5cm}|cccc|cccc|c}\n\\toprule\n\\textbf{Dataset} &\n\\textbf{Domain} &\n\\textbf{Format} &\n\\textbf{Type} &\n\\textbf{MH} &\n\\textbf{ET} &\n\\textbf{EL} &\n\\textbf{BKG} &\n\\textbf{ER}&\n\\textbf{Year}\\\\\n\\midrule\n\\textbf{WikiQA}~ & OD & Span & Extr. & N & KB & E & SP & S & 2015\\\\\n\\textbf{HotpotQA}~ & OD & Span & Extr. & Y & KB & E & MP & M & 2018\\\\\n\\textbf{MultiRC}~ & OD & MC & Abstr. & Y & KB & E & SP & M & 2018 \\\\\n\\textbf{OpenBookQA}~ & SCI & MC & Abstr. & Y & KB & I &C & FC & 2018\\\\\n\\textbf{Worldtree}~ & SCI & MC & Abstr. & Y & KB  & I & TS & EG & 2018\\\\\n\\textbf{e-SNLI}~ & CS & TE & Abstr. & N & KB  & I &  - & GS & 2018\\\\\n\\textbf{Cos-E}~ & CS & MC & Abstr. & N & KB  & I & - & GS & 2019\\\\\n\\textbf{WIQA}~ & SCI & MC & Abstr. & Y & KB  & I & SP & EG & 2019\\\\\n\\textbf{CosmosQA}~ & CS & MC & Abstr. & N & KB  & I & SP & S & 2019\\\\\n\\textbf{CoQA}  & OD & Free & Extr. & N & KB  & E & SP & S & 2019 \\\\\n\\textbf{Sen-Making}~ & CS & MC & Abstr.& N & KB  & I & - & S & 2019\\\\\n\\textbf{ArtDataset}~ & CS & MC & Abstr. &  N & KB  & I & C & S,GS & 2019\\\\\n\\textbf{QASC}~ & SCI & MC & Abstr. & Y & KB  & I & C & FC & 2020\\\\\n\\textbf{Worldtree V2}~ & SCI & MC & Abstr. & Y & KB  & I & TS & EG & 2020\\\\\n\\textbf{R$^4$C}~ & OD & Span & Extr. & Y & KB  & E & MP & EG & 2020\\\\\n\\textbf{Break}~ & OD & Free, Span & Abstr. & Y & OP  & I & AO & PR & 2020\\\\\n\\textbf{R$^3$}~ & OD & Free & Abstr. & Y & OP  & I & AO & PR & 2020\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Classification of \\emph{explanation-supporting} benchmarks in MRC.}\n\\label{tab:explainable_datasets}\n\\end{table}", "cites": [1147, 8626, 4281, 7802, 4280, 8763, 4283, 4284, 1098, 6988, 4282, 460, 7801, 8761, 8762, 4275, 4278], "cite_extract_rate": 0.8095238095238095, "origin_cites_number": 21, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple papers by classifying explanation-supporting benchmarks based on a structured set of dimensions. It integrates insights on explanation types, levels, and formats across diverse datasets. While it offers a clear analytical framework, it primarily presents the information without deep critique of the datasets or their limitations."}}
{"id": "6d576fba-6b66-4a43-adda-53c2ca4d62f2", "title": "Towards abstractive and explainable MRC.", "level": "paragraph", "subsections": ["0e2bef1b-e40c-4370-9b45-e71c65df80fd"], "parent_id": "15e67409-9476-47aa-8a47-bfcb4ac8569f", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Explanation-supporting Benchmarks"], ["paragraph", "Towards abstractive and explainable MRC."]], "content": "In line with the general research trend in MRC, the development of explanation-supporting benchmarks is evolving towards the evaluation of complex reasoning, testing the models on their ability to go beyond the surface form of the text. \nEarly datasets on open-domain QA have framed explanation as a \\emph{sentence selection} problem , where the evidence necessary to infer the final answer is entirely encoded in a single supporting sentence. Subsequent work has started the transition towards more complex tasks that require the integration of multiple supporting facts.  HotpotQA  is one of the first \\emph{multi-hop} datasets introducing a leaderboard based on a quantitative evaluation of the explanations produced by the systems\\footnote{\\url{https://hotpotqa.github.io/}}. The nature of HotpotQA is still closer to extractive MRC, where the supporting facts can be derived via paraphrasing from the explicit decomposition of the questions . \nMultiRC  combines multi-hop inference with various forms of abstract reasoning such as commonsense, causal relations, spatio-temporal and mathematical operations. The gold explanations in these benchmarks are still expressed in terms of supporting passages, leaving it implicit a consistent part of the abstract inference rules adopted to derive the answer . Following HotpotQA and MultiRC, several benchmarks on open-domain tasks have gradually refined the supporting facts annotation, whose benefits have been demonstrated in terms of interpretability, bias, and performance . Moreover, recent work have focused on complementing knowledge-based explanation with operational interpretability, introducing explicit annotation for the decomposition of multi-hop and discrete reasoning questions  into a sequence of atomic operations .  In parallel with open-domain QA, scientific reasoning has been identified as a suitable candidate for the evaluation of explanations at a higher level of abstraction . Explanations in the scientific domain naturally mention facts about underlying regularities which are hidden in the original problem formulation and that refer to knowledge about abstract conceptual categories . The benchmarks in this domain provide gold explanations for multiple-choice science questions  or related scientific tasks such as what-if questions on procedural text  and explanation via sentences composition . Similarly to the scientific domain, a set of abstractive MRC benchmarks have been proposed for the evaluation of commonsense explanations . Cos-E  and e-SNLI  augment existing datasets for textual entailment  and commonsense QA  with crowd-sourced explanations, framing explainability as a natural language generation problem. Other commonsense tasks have been explicitly designed to test explanation-related inference, such as causal and abductive reasoning . Bhagavatula et al. \\shortcite{bhagavatula2019abductive} propose the tasks of Abductive Natural Language Inference ($\\alpha$NLI) and Abductive Natural Language Generation ($\\alpha$NLG), where MRC models are required to select or generate the hypothesis that best explains a set of observations.", "cites": [4285, 4287, 1147, 1174, 4286, 4281, 7802, 4277, 456, 1098, 6988, 4282, 1142, 460, 7801, 8761, 8762, 4275, 4278], "cite_extract_rate": 0.7916666666666666, "origin_cites_number": 24, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers to present a narrative on the evolution of explanation-supporting benchmarks, from sentence selection to abstractive reasoning. It also highlights broader trends such as the shift toward multi-hop and abstract reasoning, and connects scientific and commonsense domains. While it identifies limitations (e.g., implicit inference rules), it does not deeply critique the methodologies or compare them in detail."}}
{"id": "0e2bef1b-e40c-4370-9b45-e71c65df80fd", "title": "Multi-hop reasoning and explanation.", "level": "paragraph", "subsections": [], "parent_id": "6d576fba-6b66-4a43-adda-53c2ca4d62f2", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Explanation-supporting Benchmarks"], ["paragraph", "Towards abstractive and explainable MRC."], ["paragraph", "Multi-hop reasoning and explanation."]], "content": "The ability to construct explanations in MRC is typically associated with multi-hop reasoning. However, the nature and the structure of the inference can differ greatly according to the specific task. In extractive MRC , multi-hop reasoning often consists in the identification of bridge entities, or in the extraction and comparison of information encoded in different passages. On the other hand,  observe that complete explanations for science questions require an average of 6 facts classified in three main explanatory roles: \\emph{grounding facts} and \\emph{lexical glues} have the function of connecting the specific concepts in the question with abstract conceptual categories, while \\emph{central facts} refer to high-level explanatory knowledge. Similarly, OpenbookQA  provides annotations for the core explanatory sentences, which can only be inferred by performing multi-hop reasoning through the integration of external commonsense knowledge. In general, the number of hops needed to construct the explanations is correlated with \\emph{semantic drift} -- i.e. the tendency of composing spurious inference chains that lead to wrong conclusions .  Recent explanation-supporting benchmarks attempt to limit this phenomenon by providing additional signals to learn abstract composition schemes, via the explicit annotation of valid inference chains  or the identification of common explanatory patterns .", "cites": [1143, 460, 1147, 4275], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple datasets (HotpotQA, OpenBookQA, QASC) to illustrate how multi-hop reasoning is essential for constructing explanations in MRC. It abstracts key concepts such as grounding facts, lexical glues, and central facts, and connects these with the challenge of semantic drift. While it does not deeply critique specific methodologies, it identifies limitations and trends, such as the need for explicit inference chain annotations, which shows a moderate level of critical analysis."}}
{"id": "70bbe41f-8ace-48f8-96e7-be40ad390f2b", "title": "Explainable MRC Architectures", "level": "section", "subsections": ["74afb577-8534-43f2-86de-e72f1ead360a", "ffa18066-38ff-4407-9045-6f4b5d08d7a6"], "parent_id": "03ca5aea-6a90-479f-934f-d9ab95896c40", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Explainable MRC Architectures"]], "content": "This section describes the major architectural trends for Explainable MRC (X-MRC). The approaches are broadly classified according to the nature of the MRC task they are applied to -- i.e. extractive or abstractive. In order to elicit architectural trends, we further categorise the approaches as described in Table~\\ref{tab:approach_category}.\n\\begin{table}[t]\n    \\centering\n    \\small\n    \\resizebox{\\textwidth}{!}{\n    \\begin{tabular}{>{\\raggedright}p{2.5cm}p{13cm}}\n    \\toprule\n     \\textbf{Explanation Type} & (1) Knowledge-based explanation; (2) Operational-based explanation \\\\\n     \\midrule\n     \\textbf{Learning method} & (1) Unsupervised (US): Does not require any annotated data; (2) Strongly Supervised (SS): Requires gold explanations for training or inference; (3) Distantly Supervised (DS): Treats explanation as a latent variable training only on problem-solution pairs.\\\\\n     \\midrule\n     \\textbf{Generated Output} & Denotes whether the explanation is generated or composed from facts retrieved from the background knowledge.\\\\\n     \\midrule\n     \\textbf{Multi-Hop} & Denotes whether the approach is designed for multi-hop reasoning \\\\\n    \\bottomrule\n    \\end{tabular}}\n    \\caption{Categories adopted for the classification of Explainable MRC approaches.}\n    \\label{tab:approach_category}\n\\end{table}\nFigure~\\ref{fig:approaches} illustrates the resulting classification when \nconsidering the underlying architectural components.\nIf an approach employs distinct modules for explanation generation and answer prediction, the latter is marked as $\\bigtriangleup$. For these instances, we only consider the categorization for the explanation extraction module.\nAdmittedly, the boundaries of these categories can be quite fuzzy. For instance, pre-trained embeddings such as ELMo~ are composed of recurrent neural networks, and transformers are composed of attention networks. In cases like these, we only consider the larger component that subsumes the smaller one. If approaches employ both architectures, but as different functional modules, we plot them separately. \n\\begin{figure}[t]\n\\centering\n\\subfloat[Explainable Abstractive MRC Approaches]{{\\includegraphics[width=0.46\\textwidth]{images/abstractive.png} }}\n\\qquad\n\\subfloat[Explainable Extractive MRC Approaches]{{\\includegraphics[width=0.46\\textwidth]{images/extractive.png} }}\n\\caption{\\small Explainable Machine Reading Comprehension (MRC) approaches. \\textbf{Operational Explanations}: (\\textbf{O}), \\textbf{Knowledge-based Explanations}: (\\textit{K}), \\textbf{Operational and Knowledge-based Explanations}: (\\textbf{\\textit{K,O}})\n\\textbf{Learning}: Unsupervised (\\tikzcircle{3pt}), Distantly Supervised (\\tikzcircle{3pt,OliveGreen}), Strongly Supervised (\\tikzcircle{3pt,Blue}).\n\\textbf{Generated Output}: (\\tikzcircle[black,fill=white]{4pt}). \\textbf{Multi Hop}: ($\\fbox{$\\phantom{5}$}$). \\textbf{Answer Selection Module:} ($\\bigtriangleup$). \\textbf{Architectures}: \\textsc{Weighting Schemes} (\\underline{WS}):  Document and query weighting schemes consist of information retrieval systems that use any form of vector space scoring system, \\textsc{Heuristics} (\\underline{HS}): Hand-coded heuristics and scoring functions, \\textsc{Linear Programming} (\\underline{LP}), \\textsc{Convolutional Neural Network} (\\underline{CNN}), \\textsc{Recurrent Neural Networks} (\\underline{RNN}), \\textsc{Pre-Trained Embeddings} (\\underline{Emb}), \\textsc{Attention Network} (\\underline{Att}), \\textsc{Transformers} (\\underline{TR}), \\textsc{Graph Neural Networks} (\\underline{GN}), \\textsc{Neuro-Symbolic} (\\underline{NS}) and \\textsc{Others}.\n}\n\\label{fig:approaches}\n\\end{figure}\nIn general, we observe an overall shift towards supervised methods over the years for both abstractive and extractive MRC. \nWe posit that the advent of explanation-supporting datasets has facilitated the adoption of complex supervised neural architectures.\nMoreover, as shown in the classification, the majority of the approaches are designed for knowledge-based explanation. We attribute this phenomenon to the absence of large-scale datasets for operational interpretability until 2020. However, we note a recent uptake of distantly supervised approaches. We believe that further progress can be made with the introduction of novel datasets supporting symbolic question decomposition such as Break  and R$^3$  (See Sec.~\\ref{tab:explainable_datasets}).", "cites": [8385, 4281, 8762], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from cited papers by classifying explainable MRC approaches along multiple dimensions, such as explanation type, learning method, and architecture. It integrates these ideas into a coherent framework with visual aids. It offers some critical analysis by discussing the shift toward supervised methods and the limitations of dataset availability. The abstraction is strong as it identifies broader trends, such as the move toward explanation-supporting datasets and the dominance of knowledge-based explanations."}}
{"id": "3b5c1596-41e0-45df-bfca-c010f29c69b6", "title": "Linear Programming (LP):", "level": "paragraph", "subsections": [], "parent_id": "29ced539-a3f8-4ada-a1f8-42aa91cc793b", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Explainable MRC Architectures"], ["subsection", "Modeling Explanatory Relevance for Knowledge-based Explanations"], ["subsubsection", "Explicit Models"], ["paragraph", "Linear Programming (LP):"]], "content": "Linear programming has been used for modeling semantic and structural constrains in an unsupervised fashion.\nEarly LP systems, such as TableILP~, formulate the construction of explanations as an optimal sub-graph selection problem over a set of semi-structured tables. Subsequent approaches~ \nhave proposed methods to reason over textual coprora via semantic abstraction, leveraging semi-structured representations automatically extracted through Semantic Role Labeling, OpenIE, and Named Entity Recognition. \nApproaches based on LP have been effectively applied for multiple-choice science questions, when no gold explanation is available for strong supervision.", "cites": [4288, 4290, 4289], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of linear programming approaches in the context of MRC explainability, mentioning the use of semantic abstraction and structured reasoning. However, it lacks deeper synthesis of the cited papers, does not critically evaluate their strengths or weaknesses, and offers little abstraction beyond the specific methods discussed."}}
{"id": "b13d9377-4770-49f0-b4b8-346de4f1f003", "title": "Weighting schemes with heuristics:", "level": "paragraph", "subsections": [], "parent_id": "29ced539-a3f8-4ada-a1f8-42aa91cc793b", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Explainable MRC Architectures"], ["subsection", "Modeling Explanatory Relevance for Knowledge-based Explanations"], ["subsubsection", "Explicit Models"], ["paragraph", "Weighting schemes with heuristics:"]], "content": "The integration of heuristics and weighing schemes have been demonstrated to be effective for the implementation of lightweight methods that are inherently scalable to large corpora and knowledge bases. \nIn the open-domain, approaches based on lemma overlaps and weighted triplet scoring function have been proposed~, along with path-based heuristics implemented with the auxiliary use of external knowledge bases ~. \nSimilarly, path-based heuristics have been adopted for commonsense tasks, where Lv et al.~\\shortcite{lv2019graph} propose a path extraction technique based on question coverage. For scientific and multi-hop MRC, Yadav et al.~\\shortcite{yadav2019quick} propose ROCC, an unsupervised method to retrieve multi-hop explanations that maximise relevance and coverage while minimising overlaps between intermediate hops. Valentino et al.~\\shortcite{valentino2020unification,valentino2020explainable} present an explanation reconstruction framework for multiple-choice science questions based on the notion of unification in science. The unification-based framework models explanatory relevance using two scoring functions: a relevance score representing lexical similarity, and a unification score denoting the explanatory power of fact, depending on its frequency in explanations for similar cases.", "cites": [3154], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of several methods for modeling explanatory relevance in MRC using heuristics and weighting schemes. While it mentions a few distinct approaches and their goals, it lacks deeper synthesis, comparison, or critical evaluation of their strengths and limitations. There is little abstraction or generalization of broader patterns or principles."}}
{"id": "b53d1662-a252-4737-af63-ae5fc07353c8", "title": "Pre-trained embeddings with heuristics:", "level": "paragraph", "subsections": [], "parent_id": "29ced539-a3f8-4ada-a1f8-42aa91cc793b", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Explainable MRC Architectures"], ["subsection", "Modeling Explanatory Relevance for Knowledge-based Explanations"], ["subsubsection", "Explicit Models"], ["paragraph", "Pre-trained embeddings with heuristics:"]], "content": "Pre-trained embeddings have the advantage of capturing semantic similarity, going beyond the lexical overlaps limitation imposed by the use of weighting schemes. This property has been shown to be useful for multi-hop and abstractive tasks, where approaches based on pre-trained word embeddings, such as GloVe , have been adopted to perform semantic alignment between question, answer and justification sentences . \nSilva et al.~\\shortcite{silva2018recognizing,silva2019exploring} employ word embeddings and semantic similarity scores to perform selective reasoning on commonsense knowledge graphs and construct explanations for textual entailment. Similarly, knowledge graph embeddings, such as TransE~, have been adopted for extracting reasoning paths for commonsense QA~.", "cites": [4291, 1684, 1123], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of how pre-trained embeddings and semantic similarity are used in explainable MRC systems. It integrates two of the cited papers by drawing a parallel between their use of embeddings for reasoning and explanation, but lacks deeper critical evaluation or abstraction into broader principles. The narrative remains surface-level without identifying limitations or trends."}}
{"id": "5b1db0ca-6ed8-4309-8f6c-767c8273191f", "title": "Neural models for sentence selection:", "level": "paragraph", "subsections": [], "parent_id": "7d111ebf-fbbc-455e-b2a2-e4338e190d91", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Explainable MRC Architectures"], ["subsection", "Modeling Explanatory Relevance for Knowledge-based Explanations"], ["subsubsection", "Latent Models"], ["paragraph", "Neural models for sentence selection:"]], "content": "This category refers to a set of neural approaches proposed for the \\emph{answer sentence selection} problem. \nThese approaches typically adopt deep \nlearning architectures, such as RNN, CNN and Attention networks via strong or distant supervision.  Strongly supervised approaches~ are trained on gold supporting sentences. In contrast, distantly supervised techniques indirectly learn to extract the supporting sentence by training on the final answer. Attention mechanisms have been frequently used for distant supervision~ to highlight the attended explanation sentence in the contextual passage. Other distantly supervised approaches model the sentence selection problem through the use of latent variables~.", "cites": [5, 4293, 1139, 4294, 4292], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of neural models for sentence selection in MRC, mentioning common architectures and supervision types. It integrates minimal information from the cited works, primarily listing their contributions without deeper synthesis or broader conceptual framing. There is little to no critical analysis or evaluation of the approaches, limitations, or trade-offs."}}
{"id": "0cf2d886-f420-4b93-9ba5-619748a9a28d", "title": "Transformers for multi-hop reasoning:", "level": "paragraph", "subsections": [], "parent_id": "7d111ebf-fbbc-455e-b2a2-e4338e190d91", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Explainable MRC Architectures"], ["subsection", "Modeling Explanatory Relevance for Knowledge-based Explanations"], ["subsubsection", "Latent Models"], ["paragraph", "Transformers for multi-hop reasoning:"]], "content": "Transformers-based architectures  have been successfully applied to learn explanatory relevance in both extractive and abstractive MRC tasks. Banerjee~\\shortcite{banerjee2019asu} and Chia et al.~\\shortcite{chia2019red} adopt a BERT model  to learn to rank explanatory facts in the scientific domain. Shao et al.~\\shortcite{shao2020graph} employ transformers with self-attention on multi-hop QA datasets , demonstrating that the attention layers implicitly capture high-level relations in the text. The Quartet model  has been adopted for reasoning on procedural text and \nproducing structured explanations based on qualitative effects and interactions between concepts.\nIn the distant supervison setting, Niu et al.~\\shortcite{niu2020self} address the problem of lack of gold explanations by training a self-supervised evidence extractor with auto-generated labels in an iterative process. Banerjee and Bara \\shortcite{banerjee2020knowledge} propose a semantic ranking model based on BERT for QASC  and OpenBookQA . Transformers have shown improved performance on downstream answer prediction tasks when applied in combination with explanations constructed through explicit models~.", "cites": [4291, 4295, 4275, 460, 7, 1147, 38], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes individual papers and their applications of transformers for multi-hop reasoning and explainability in MRC. While it groups related works (e.g., BERT-based models), it lacks deeper synthesis of ideas, critical evaluation of methods, and generalization to broader principles or trends. The narrative remains factual and does not offer a nuanced analysis of the strengths or weaknesses of the approaches."}}
{"id": "718e9d67-658a-4372-9fe2-94b2dca60ce1", "title": "Attention networks for multi-hop reasoning:", "level": "paragraph", "subsections": [], "parent_id": "7d111ebf-fbbc-455e-b2a2-e4338e190d91", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Explainable MRC Architectures"], ["subsection", "Modeling Explanatory Relevance for Knowledge-based Explanations"], ["subsubsection", "Latent Models"], ["paragraph", "Attention networks for multi-hop reasoning:"]], "content": "Similar to transformer-based approaches, attention networks have also been employed to extract relevant explanatory facts. However, attention networks are \nusually applied in combination with other neural modules. For HotpotQA, Yang et al.~\\shortcite{yang2018hotpotqa}  propose a model trained in a multi-task setting on both gold explanations and answers, composed of recurrent neural networks and attention layers. \nNishida et al.~\\shortcite{nishida2019answering} introduce a similarly structured model with a query-focused extractor designed to elicit explanations. The distantly supervised MUPPET model~ captures the relevance between question and supporting facts through bi-directional attention on sentence vectors encoded using pre-trained embedding, CNN, and RNN. In the scientific domain, Trivedi et al.~\\shortcite{trivedi2019repurposing} repurpose existing textual entailment datasets to learn the supporting facts relevance for multi-hop QA. Khot et al.~\\shortcite{khot2019s} propose a knowledge gap guided framework to construct explanations for OpenBookQA.", "cites": [9092], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section describes several attention-based models for multi-hop reasoning but lacks deeper synthesis or comparison of their underlying principles. It presents the methods in a list-like manner without evaluating their strengths or weaknesses. There is minimal abstraction or identification of broader trends beyond the specific models mentioned."}}
{"id": "4041fb3c-8c87-469b-b54f-471794259cbf", "title": "Language generation models:", "level": "paragraph", "subsections": [], "parent_id": "7d111ebf-fbbc-455e-b2a2-e4338e190d91", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Explainable MRC Architectures"], ["subsection", "Modeling Explanatory Relevance for Knowledge-based Explanations"], ["subsubsection", "Latent Models"], ["paragraph", "Language generation models:"]], "content": "Recent developments in language modeling along with the creation of explanation-supporting benchmarks, such as e-SNLI  and Cos-E , have opened up the possibility to automatically generate semantically plausible and coherent explanation sentences. \nLanguage models, such as GPT-2~, have been adopted for producing commonsense explanations, whose application has demonstrated benefits in terms of accuracy and zero-shot generalisation . Kumar and Talukdar~\\shortcite{kumar2020nile} present a similar approach for natural language inference, generating explanations for entailment, neutral and contradiction labels. e-SNLI~ present a baseline based on a Bi-LSTM encoder-decoder with attention. Lukasiewicz et al.~\\shortcite{lukasiewiczmake} enhance this baseline by proposing an adversarial framework to generate more consistent and plausible explanations.", "cites": [4296, 7802, 7801], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of how language generation models are used for producing explanations in MRC, referencing key datasets and models. It makes minimal connections between the cited works and does not engage in significant critical evaluation or abstraction beyond the specific examples mentioned."}}
{"id": "5c461c34-c0f3-4ab2-8c2e-1055d2e6b266", "title": "Graph Networks:", "level": "paragraph", "subsections": [], "parent_id": "5df46897-94e6-4356-bd8b-da416a4fec3b", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Explainable MRC Architectures"], ["subsection", "Modeling Explanatory Relevance for Knowledge-based Explanations"], ["subsubsection", "Hybrid Models"], ["paragraph", "Graph Networks:"]], "content": "The relational inductive bias encoded in Graph Networks  provides a viable support for reasoning and learning over structured representations. This characteristic has been identified as particularly suitable for supporting facts selection in multi-hop MRC tasks. A set of graph-based architectures have been proposed for multi-hop reasoning in HotpotQA . Ye et al.~\\shortcite{ye2019multi} build a graph using sentence vectors as nodes \nand edges connecting sentences that share the same named entities. Similarly, Tu et al.~\\shortcite{tu2019select} construct a graph connecting sentences that are part of the same document, share noun-phrases, and have named entities or noun phrases in common with the question. \nThayaparan et al. \\shortcite{thayaparan2019identifying} propose a graph structure including both documents and sentences as nodes. The graph connects documents that mention the same named entities. To improve scalability, the Dynamically Fused Graph Network (DFGN)~ adopts a dynamic construction of the graph, starting from the entities in the question and gradually selecting the supporting facts. Similarly, Ding et al.~\\shortcite{ding2019cognitive} implement a dynamic graph exploration inspired by the dual-process theory~. \nthe Hierarchical Graph Network  leverages a hierarchical graph representation of the background knowledge (i.e. question, paragraphs, sentences, and entities). In parallel with extractive MRC tasks, Graph Networks are applied for answer selection on commonsense reasoning, where a subset of approaches have started exploring the use of explanation graphs extracted from external knowledge bases through path-based heuristics .", "cites": [208, 4298, 4297, 1075, 460, 1123], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of various graph-based approaches in MRC, connecting them to the broader concept of relational inductive bias. It lists different graph constructions and their purposes, such as multi-hop reasoning and commonsense QA, but lacks in-depth comparative analysis or critical evaluation of their strengths and weaknesses. Some abstraction is attempted, particularly in mentioning hierarchical graph representations and external knowledge integration, but it does not rise to the level of identifying overarching principles."}}
{"id": "65879cf5-b58d-411d-9954-b66029fee02e", "title": "Explicit inference chains for multi-hop reasoning:", "level": "paragraph", "subsections": [], "parent_id": "5df46897-94e6-4356-bd8b-da416a4fec3b", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Explainable MRC Architectures"], ["subsection", "Modeling Explanatory Relevance for Knowledge-based Explanations"], ["subsubsection", "Hybrid Models"], ["paragraph", "Explicit inference chains for multi-hop reasoning:"]], "content": "A subset of approaches has introduced end-to-end frameworks explicitly designed to emulate the step-by-step reasoning process involved in multi-hop MRC~.  The baseline approach proposed for Abductive Natural Language Inference~  builds chains composed of hypotheses and observations, and encode them using transformers to identify the most plausible explanatory hypothesis.  Similarly, Das et al.~\\shortcite{das2019chains} embed the reasoning chains retrieved via TF-IDF and lexical overlaps using a BERT model to identify plausible explanatory patterns for multiple-choice science questions. In the open domain, Asai et al.~\\shortcite{asai2019learning} build a graph structure using entities and hyperlinks and adopt recurrent neural networks to retrieve relevant documents sequentially. Nie et al.~\\shortcite{nie2019revealing} introduce a step-by-step reasoning process that first retrieves the relevant paragraph, then the supporting sentence, and finally, the answer.  Dhingra et al.~\\shortcite{dhingra2020differentiable} propose an end-to-end differentiable model that uses Maximum Inner Product Search (MIPS)~ to query a virtual knowledge-base and extract a set of reasoning chains. Feng et al~\\shortcite{feng2020learning} propose a cooperative game approach to select the most relevant explanatory chains from a large set of candidates. In contrast to neural-based methods, Weber et al.~\\shortcite{weber2019nlprolog} propose a neuro-symbolic approach for multi-hop reasoning that extends the unification algorithm in Prolog with pre-trained sentence embeddings.", "cites": [4300, 4299, 9102, 4301, 8626], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 1.8, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a list of papers and their methods related to explicit inference chains for multi-hop reasoning in MRC but does not effectively synthesize or connect the ideas across the cited works. It lacks critical evaluation of the strengths or limitations of the approaches and does not generalize to broader patterns or principles, remaining focused on specific system descriptions."}}
{"id": "2462188e-cd48-4e02-8a37-bc236260eb93", "title": "Neuro-Symbolic models:", "level": "paragraph", "subsections": ["619931af-64a9-47a4-a532-e696b9675768"], "parent_id": "ffa18066-38ff-4407-9045-6f4b5d08d7a6", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Explainable MRC Architectures"], ["subsection", "Operational Explanation"], ["paragraph", "Neuro-Symbolic models:"]], "content": "Neuro-symbolic approaches combine neural models with symbolic programs.\nLiu and Gardner~\\shortcite{liu2020multi} propose a multi-step inference model with three primary operations: Select, Chain, and Predict. The Select operation retrieves the relevant knowledge; the Chain operation composes the background knowledge together; the Predict operation select the final answer. Jiang and Bansel.~\\shortcite{jiang2019self} propose the adoption of Neural Module Networks~ for multi-hop QA by designing four atomic neural modules (Find, Relocate, Compare, NoOp) that allow for both operational explanation and supporting facts selection. Similarly, Gupta et al.~\\shortcite{gupta2019neural} adopt Neural Module Networks to perform discrete reasoning on DROP . In contrast, Chen et al.~\\shortcite{chen2019neural} propose an architecture based on LSTM, attention modules, and transformers to generate compositional programs. While most of the neuro-symbolic approaches are distantly supervised, the recent introduction of question decomposition datasets~ allows for a direct supervision of symbolic program generation .", "cites": [8762, 1142, 4302, 4303], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple neuro-symbolic MRC approaches, drawing connections between their architectural designs and explanation mechanisms. It abstracts some common themes, such as the use of modular reasoning and compositional programs, but the critical analysis is limited to a brief mention of distant supervision and the potential for more direct supervision, without deeper evaluation of trade-offs or limitations. Overall, it provides a coherent analytical overview but lacks substantial critique."}}
{"id": "cdcc17cb-f2d2-4dee-a997-023bad38e00d", "title": "Evaluation", "level": "section", "subsections": ["c888df75-7c18-4652-a987-b7917036a374"], "parent_id": "03ca5aea-6a90-479f-934f-d9ab95896c40", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Evaluation"]], "content": "\\label{sec:evaluation}\nThe development of explanation-supporting benchmarks has allowed for a quantitative evaluation of the explainability in MRC. In open-domain settings, Exact Matching (EM) and F1 score are often employed for evaluating the supporting facts , while explanations for multiple-choice science questions have been evaluated using ranking-based metrics such as Mean Average Precision (MAP) . In contexts where the explanations are produced by language models, natural language generation metrics have been adopted, such as BLEU score and perplexity . Human evaluation still plays an important role, especially for distantly supervised approaches applied on benchmarks that do not provide labelled explanations.", "cites": [2257, 460, 7801], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic overview of evaluation methods used in explainable MRC but lacks depth in connecting or synthesizing the cited works into a broader narrative. It briefly mentions different evaluation types (e.g., EM, F1, BLEU) and their applications, yet fails to critically analyze their strengths or weaknesses. The section also mentions the role of human evaluation but does not elaborate on its implications or compare it with automated metrics."}}
{"id": "c888df75-7c18-4652-a987-b7917036a374", "title": "Silver Explanations", "level": "subsection", "subsections": ["25af0825-5a1a-4017-88bc-3254bd7e47e7"], "parent_id": "cdcc17cb-f2d2-4dee-a997-023bad38e00d", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Evaluation"], ["subsection", "Silver Explanations"]], "content": "Since annotating explanations are expensive and not readily available, approaches automatically curate \\textit{silver} explanations for training. For single-hop Extractive MRC, both  and  use oracle sentence (the sentence containing the answer span) as the descriptive explanation. Similarly, for multi-hop Extractive approaches~ extract explanations by building a path connecting question to the oracle sentence, by linking multiple sentences using inter-sentence knowledge representation. Since there might be multiple paths connecting question and answer, to determine the best path, ~ uses the shortest path with the highest lexical overlap and ~ employs Integer Linear Programming (ILP) with hand-coded heuristics.", "cites": [4292, 4304, 4299, 4293], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of how silver explanations are generated for single-hop and multi-hop MRC tasks, referencing different papers. It makes minimal effort to synthesize the ideas into a broader narrative and lacks critical evaluation or discussion of limitations. The abstraction is limited to identifying that multiple methods exist for path selection, without elevating the discussion to overarching principles or frameworks."}}
{"id": "25af0825-5a1a-4017-88bc-3254bd7e47e7", "title": "Evaluating multi-hop reasoning", "level": "paragraph", "subsections": [], "parent_id": "c888df75-7c18-4652-a987-b7917036a374", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Evaluation"], ["subsection", "Silver Explanations"], ["paragraph", "Evaluating multi-hop reasoning"]], "content": "Evaluating explainability through multi-hop reasoning presents still several challenges . Recent works have demonstrated that some of the questions in multi-hop QA datasets do not require multi-hop reasoning or can be answered by exploiting statistical shortcuts in the data . In parallel, other works have shown that a consistent part of the expected reasoning capabilities for a proper evaluation of reading comprehension are missing in several benchmarks . A set of possible solutions have been proposed to overcome some of the reported issues, including the creation of evaluation frameworks for the gold standards , the development of novel metrics for multi-hop reasoning , and the adoption of adversarial training techniques . A related research problem concerns the faithfulness of the explanations. Subramanian et al. \\shortcite{subramanian2020obtaining} observe that some of the modules in compositional neural networks , particularly suited for operational interpretability, do not perform their intended behaviour. To improve faithfulness the authors suggest novel architectural design choices and propose the use of auxiliary supervision.", "cites": [4307, 4306, 4303, 4285, 8764, 4305], "cite_extract_rate": 0.75, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section analytically addresses challenges in evaluating multi-hop reasoning, integrating insights from multiple papers to highlight limitations in datasets and models. It critically discusses how some questions may not require multi-hop reasoning and evaluates solutions like evaluation frameworks and novel metrics. While not entirely abstract, it identifies broader issues such as benchmark design and explanation faithfulness."}}
{"id": "42f2c83a-4225-46e7-9b7f-f595d1fda0c2", "title": "Conclusion and Open Research Questions", "level": "section", "subsections": [], "parent_id": "03ca5aea-6a90-479f-934f-d9ab95896c40", "prefix_titles": [["title", "A Survey on Explainability in Machine Reading Comprehension"], ["section", "Conclusion and Open Research Questions"]], "content": "This survey has proposed a systematic categorisation of benchmarks and approaches for explainability in MRC. Lastly, we outline a set of open research questions for future work:  \n\\begin{enumerate}\n    \\item \\textbf{Contrastive  Explanations}: while contrastive and conterfactual explanations are becoming central in Explainable AI , this type of explanations is still under-explored for MRC. We believe that contrastive explanations can lead to the development of novel reasoning paradigms, especially in the context of multiple-choice science and commonsense QA\n    \\item \\textbf{Benchmark Design:} to advance research in explainability it is fundamental to develop reliable methods for explanation evaluation, overcoming the issues presented in Section \\ref{sec:evaluation}, and identify techniques for scaling up the annotation of gold explanations \n    \\item \\textbf{Knowledge Representation:} the combination of explicit and latent representations have been useful for explainability in multi-hop, extractive MRC. An open research question is understanding whether a similar paradigm can be beneficial for abstractive tasks to limit the phenomenon of semantic drift observed in many hops reasoning\n    \\item \\textbf{Supervised Program Generation:} large-scale benchmarks for operational explanations have been released just recently . We believe that these corpora open up the possibility to explore strongly supervised methods to improve accuracy and faithfulness in compositional neural networks and symbolic program generation.\n\\end{enumerate}\n\\bibliographystyle{acl}\n\\bibliography{coling2020,approach}\n\\begin{comment}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\end{comment}\n\\end{document}", "cites": [4325, 4291, 4328, 4326, 5, 4329, 4295, 4327, 7803, 4324, 4296, 4323, 3154, 1798, 4318, 4320, 4297, 4278, 4321, 1070, 4315, 4317, 4319, 4294, 4314, 4286, 4301, 1123, 4289, 8626, 4281, 7802, 4300, 4304, 4310, 1139, 4292, 4298, 4293, 4316, 8765, 1075, 4308, 4312, 4288, 4309, 4299, 9092, 460, 4290, 4313, 4322, 7801, 8762, 4311], "cite_extract_rate": 0.7746478873239436, "origin_cites_number": 71, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview by identifying key open research questions and connecting them to broader themes in explainable AI. It synthesizes relevant ideas from the cited works, such as multi-hop reasoning, knowledge representation, and explanation evaluation, but does not present a novel framework. Critical analysis is present in the form of limitations (e.g., annotation costs, semantic drift), though it is not deeply evaluative or nuanced."}}
