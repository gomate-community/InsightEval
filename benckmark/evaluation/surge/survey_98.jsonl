{"id": "06a8a978-bb13-405e-8614-2ae9a01d5c56", "title": "Different ML Techniques", "level": "section", "subsections": ["f49480df-db26-480b-8149-671064eda13f", "c3e4e81a-5169-40c5-91a3-4a2389c943b1", "218b240c-27bf-4571-953a-c14101e51384"], "parent_id": "5631fef3-6389-407d-9101-745f45240141", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "Different ML Techniques"]], "content": "\\label{sec:ml techniques}\nThere are three general frameworks in ML: supervised learning, unsupervised learning and reinforcement learning.\nThese frameworks mainly differentiate on what data are sampled and how these sample data are used to build learning models.\nTable \\ref{table:ml_techniques} summarizes the commonly used ML techniques for computer architecture and system designs.\nSometimes, multiple learning models may work well for one given problem, and the appropriate selection can be made based on available hardware resource and data, implementation overheads, performance targets, etc.\n\\begin{table}[tbp]\n\\vspace{-12pt}\n\\caption{Machine learning techniques.}\n\\vspace{-12pt}\n\\label{table:ml_techniques}\n\\centering\n    \\tiny\n    \\renewcommand{\\arraystretch}{1}\n    \\setlength{\\tabcolsep}{6pt}\n\\begin{tabular}{c|c|c|c}\n\\toprule\n\\textbf{Realm of ML} & \\textbf{Category} & \\makecell[c]{\\textbf{Classical ML}} & \\textbf{Deep Learning Counterpart } \\\\ \\midrule\n\\multirow{10}{*}{\\makecell*[c]{\\textbf{Supervised} \\\\ \\textbf{Learning}}} & \\multirow{1}{*}{Classification} &\n  Logistic regression  & \n  \\multirow{8}{*}{\\makecell{CNN, RNN,\\\\ GNN , etc.}} \\\\ \\cline{2-3}\n & \\multirow{6}{*}{Working for both} & Support vector machines/regression  & \\\\ \\cline{3-3}\n && K-nearest neighbors  & \\\\ \\cline{3-3}\n && Decision tree, e.g., CART , MARS  & \\\\ \\cline{3-3}\n && ANN  & \\\\ \\cline{3-3}\n && Bayesian analysis  & \\\\ \\cline{3-3}\n && \\makecell{Ensemble learning , e.g., gradient boosting, random forest} & \\\\ \\cline{2-3}\n & \\multirow{2}{*}{Regression} & \\makecell{Linear regression with variants , e.g., lasso (L1 regularization),\\\\ ridge (L2 regularization), elastic-net (hybrid L1/L2 regularization)} & \\\\ \\cline{3-3}\n && Non-linear regression  & \\\\\n \\hline\n\\multirow{2}{*}{\\makecell[c]{\\textbf{Unsupervised} \\\\ \\textbf{Learning}}} \n& Clustering & K-means clustering  &  \\multirow{2}{*}{\\makecell[c]{Autoencoder, \\\\ GAN, etc.}} \\\\ \\cline{2-3}\n & Dimension reduction & Principal component analysis (PCA)  &\\\\ \\hline\n\\multirow{3}{*}{\\makecell*[c]{\\textbf{Reinforcement} \\\\ \\textbf{Learning}}} &\n Value-based & Q-learning  & DQN  \\\\ \n \\cline{2-4} \n & \\multirow{2}{*}{Policy-based} & Actor-critic \n & A3C , DDPG  \\\\ \\cline{3-4}\n & & Policy gradient, e.g., REINFORCE \n & PPO  \\\\\n\\bottomrule\n\\end{tabular}\n\\vspace{-15pt}\n\\end{table}\n\\vspace{-5pt}", "cites": [1390, 553, 2219, 166], "cite_extract_rate": 0.21052631578947367, "origin_cites_number": 19, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a basic overview of different ML techniques and maps them to classical and deep learning methods. While it cites relevant papers for specific techniques (e.g., PPO, A3C, and GNNs), the synthesis is minimal, merely listing algorithms without connecting them to broader themes or trends. There is no critical analysis or evaluation of the cited works, and abstraction is limited to general ML categories without deeper conceptual insights."}}
{"id": "f49480df-db26-480b-8149-671064eda13f", "title": "Supervised Learning", "level": "subsection", "subsections": [], "parent_id": "06a8a978-bb13-405e-8614-2ae9a01d5c56", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "Different ML Techniques"], ["subsection", "Supervised Learning"]], "content": "Supervised learning is the process of learning a set of rules able to map an input to an output based on labeled datasets. These learned rules can be generalized to make predictions for unseen inputs.\nWe briefly introduce several prevalent techniques in supervised learning, as shown in Figure \\ref{fig:sl}.\n\\begin{itemize}[leftmargin=*]\n    \\item Regression is a process for estimating the relationships between a dependent variable and one or more independent variables. The most common form is linear regression , and some other forms include different types of non-linear regression . Regression techniques are primarily used for two purposes, prediction/forecasting, and inference of causal relationships.\n    \\item Support vector machines (SVMs)  try to find the best hyperplanes to separate data classes by maximizing margins. One variant is support vector regression (SVR), which is able to conduct regression tasks. Predictions or classifications of new inputs can be decided by their relative positions to these hyperplanes.\n    \\item Decision tree is one representative of logical learning methods, which uses tree structures to build regression or classification models.\n    The final result is a tree with decision nodes and leaf nodes. Each decision node represents a feature and branches of this node represent possible values of the corresponding feature. Starting from the root node, input instances are classified by sequentially passing through nodes and branches, until they reach leaf nodes that represent either classification results or numerical values.\n    \\item Artificial neural networks (ANNs)  are capable to approximate a broad family of functions: a single-layer perceptron is usually used for linear regression; complex DNNs  consisting of multiple layers are able to approximate non-linear functions, such as the multi-layer perceptron (MLP); variants of DNNs that achieve excellent performance in specific fields benefit from the exploitation of certain computation operations, e.g., convolutional neural networks (CNNs) with convolution operations leveraging spatial features, and recurrent neural networks (RNNs) with recurrent connections enabling learning from sequences and histories.\n    \\item Ensemble learning  employs multiple models that are strategically designed to solve a particular problem, and the primary goal is to achieve better predictive performance than those could be obtained from any of the constituent models alone. Several common types of ensembles include random forest and gradient boosting.\n\\end{itemize}\n\\begin{figure}[tbp]\n    \\centering\n    \\vspace{-15pt}\n\t\\includegraphics[width=0.97\\linewidth]{fig1.png}\n\t\\vspace{-10pt}\n    \\caption{Examples of supervised learning: (a) regression, (b) SVM, (c) decision tree, (d) MLP, and (e) ensemble learning.}\n    \\vspace{-20pt}\n    \\label{fig:sl}\n\\end{figure}\nDifferent learning models have different preference of input features: \nSVMs and ANNs generally perform much better with multi-dimension and continuous features, while logic-based systems tend to perform better when dealing with discrete/categorical features.\nIn system design, supervised learning is commonly used for performance modeling, configuration predictions, or predicting higher-level features/behaviors from lower-level features.\nOne thing worth noting is that supervised learning techniques need well labeled training data prior to the training phase, which usually require tremendous human expertise and engineering.\n\\vspace{-5pt}", "cites": [166], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of supervised learning techniques without effectively synthesizing insights from the cited papers. It lists common ML methods with general definitions and applications in system design, but does not engage in meaningful integration of ideas or critical evaluation of the cited work. There is minimal abstraction beyond the specific examples given."}}
{"id": "15e5a658-0b25-4890-8335-c1ec062ad3a2", "title": "ML for Fast System Modeling", "level": "section", "subsections": ["cb797a5f-f6ce-4412-9791-ccdeb4f88886", "9ac2d4c9-6bb8-44b3-aa23-38c568e05513", "419e1fc4-5eb3-40c4-b251-cc170597bc2e"], "parent_id": "5631fef3-6389-407d-9101-745f45240141", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "ML for Fast System Modeling"]], "content": "\\label{sec:system modeling}\nThis section reviews studies that employ ML techniques for fast and accurate system modeling, which involves predictions of performance metrics or some other criteria of interest.\nAlthough cycle-accurate simulators, which are commonly used for system performance prediction, can provide accurate estimations, they usually run multiple orders of magnitude slower than native executions.\nBy contrast, ML-based techniques can balance simulation costs and prediction accuracy, showing great potentials in exploring huge configuration spaces and learning non-linear impacts of configurations.\nMost of existing work applies supervised learning for either pure system modeling or efficient design space exploration (DSE) enabled by fast predictions.\nTable \\ref{table:model_sys} and Table \\ref{table:model_eda} summarize the studies for predictive modeling in computer architecture/system and design automation respectively, in terms of task domains, prediction targets, adopted ML techniques, and corresponding inputs.\n\\begin{table}[tbp]\n\\vspace{-10pt}\n\\caption{Summary of applying ML techniques for fast modeling in computer architecture and systems.}\n\\label{table:model_sys}\n\\centering\n    \\tiny\n    \\renewcommand{\\arraystretch}{0.9}\n    \\setlength{\\tabcolsep}{1pt}\n\\vspace{-10pt}\n\\begin{tabular}{c|c|c|c}\n\\toprule\n\\textbf{Domain}  & \\textbf{Prediction of}  & \\textbf{Technique} & \\textbf{Input} \\\\ \\midrule\n\\multirow{4}{*}{\\makecell[c]{\\textbf{Memory} \\\\ \\textbf{System} \\\\ $( \\S$ \\textbf{\\ref{sec:model_memory}}$)$}} \n& Throughput/cache miss & ANN   & Cache configurations  \\\\ \\cline{2-4} \n& Throughput/lifetime/energy & \\makecell[c]{Gradient boosting/quadratic regression with lasso }  & NVM configurations  \\\\ \\cline{2-4} \n& Throughput   & CNN    & Memory controller placements   \\\\ \\cline{2-4} \n& Disk block correlation  & DNN  & Data blocks in the context window \\\\ \\hline\n\\multirow{4}{*}{\\makecell[c]{\\textbf{NoC} \\\\ $( \\S$ \\textbf{\\ref{sec:model_noc}}$)$}}           \n& Latency  & SVR  & Queue information \\\\ \\cline{2-4} \n& Hotspots & ANN  & Buffer utilization \\\\ \\cline{2-4} \n& Buffer/link utilization  & \\makecell[c]{Regression with ridge regularization , decision tree } & NoC configurations  \\\\ \\cline{2-4} \n& Probability of errors  & Decision tree                      & \\makecell[c]{Link utilization and transistor wearing-out} \\\\ \\hline\n\\multirow{6}{*}{\\makecell[c]{\\textbf{GPU} \\\\ $( \\S$ \\textbf{\\ref{sec:model_gpu}}$)$}} & \\makecell[c]{Speedup or execution time \\\\ by cross-platform inputs} & \\makecell[c]{Nearest neighbor/SVM , \\\\ ensemble of regression-based learners , \\\\ random forest } & \\makecell[c]{Static analysis and/or \\\\ dynamic profiling of \\\\ source CPU code} \\\\ \\cline{2-4} \n & Execution time & \\makecell[c]{Stepwise regression , \\\\ ensemble of linear regression and random forest } & \\multirow{3}{*}{\\makecell[c]{GPU configurations \\\\ and performance counters}} \\\\ \\cline{2-3}\n & Throughput/power & Ensemble of NNs  &  \\\\ \\cline{2-3}\n & Scaling behavior of GPGPU & ANN and K-means clustering  &  \\\\ \\cline{2-4} \n & Kernel affinity and execution time & Logistic regression and linear regression  & Kernel characteristics \\\\ \\cline{2-4}\n & Traffic patterns in GPGPU & CNN  & Grayscale heat maps \\\\ \\hline\n\\multirow{2}{*}{\\makecell[c]{\\textbf{Single-Core} \\\\ \\textbf{CPU} $( \\S$ \\textbf{\\ref{sec:model_single}}$)$}} \n& Throughput & \\makecell[c]{Linear regression , non-linear regression   } & \\multirow{2}{*}{\\makecell[c]{Micro-architectural parameters \\\\ and performance counters}} \\\\ \\cline{2-3}\n & Program execution time & Linear   regression &  \\\\ \\hline\n\\multirow{8}{*}{\\makecell[c]{\\textbf{General}\\\\ \\textbf{Modeling} \\\\ $( \\S$ \\textbf{\\ref{sec:model_general}}$)$}} & \\multirow{6}{*}{Throughput/latency/power} & ANN  & \\multirow{7}{*}{\\makecell[c]{Micro-architectural parameters \\\\ and performance counters}} \\\\ \\cline{3-3}\n &  & \\makecell[c]{Non-linear regression  } &  \\\\ \\cline{3-3}\n &  & Linear regression  &  \\\\ \\cline{3-3}\n &  & Hierarchical Bayesian model  &  \\\\ \\cline{3-3}\n &  & LSTM    &  \\\\ \\cline{3-3}\n &  & Generative model  &  \\\\ \\cline{2-3}\n & \\makecell[c]{Slowdown caused by \\\\ application interference} & \\makecell[c]{Linear regression with elastic-net \\\\ regularization } &  \\\\ \\cline{2-4} \n & \\makecell[c]{Speedup of multi-thread applications} & Gausian process regression  & Profiling of single-thread execution \\\\ \\hline\n\\multirow{9}{*}{\\makecell[c]{\\textbf{Data} \\\\ \\textbf{Center} \\\\ $( \\S$ \\textbf{\\ref{sec:model_dc}}$)$}} & Job completion time & SVR  & \\makecell[c]{Application characteristics \\\\ and cluster configurations} \\\\ \\cline{2-4} \n & Resource demand & \\makecell[c]{Statistical learning , linear regression/MLP } & \\multirow{3}{*}{Workload characterization} \\\\ \\cline{2-3}\n & Incoming workload & ARMA ,   ARIMA  &  \\\\ \\cline{2-3}\n & Workload pattern & Hidden Markov model  &  \\\\ \\cline{2-4} \n & Power   usage effectiveness & MLP  & Data center configurations \\\\ \\cline{2-4} \n & Disk Failure & \\makecell[c]{Bayesian methods , clustering , \\\\ SVM/MLP , random forest } & \\multirow{3}{*}{\\makecell[c]{SMART (Self-Monitoring, Analysis \\\\ and Reporting Technology) attributes \\\\ of data centers}} \\\\ \\cline{2-3}\n & Health assessment of drives & \\makecell[c]{CART , gradient boosted regressions tree , RNN } &  \\\\ \\cline{2-3}\n & \\multirow{2}{*}{Partial drive failure} & \\makecell[c]{CART/random forest/SVM/ANN/logistic regression } &  \\\\ \\cline{3-4} \n &  & Gradient boosted regression trees  & \\makecell[c]{SMART attributes and system-level signals} \\\\ \\bottomrule\n\\end{tabular}\n\\vspace{-15pt}\n\\end{table}", "cites": [4378, 5390, 5389], "cite_extract_rate": 0.04918032786885246, "origin_cites_number": 61, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.2, "critical": 1.8, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of how ML techniques have been applied to fast system modeling in different architectural domains. It organizes the cited works into tables, summarizing their domains, prediction targets, techniques, and inputs, but does not offer deep synthesis, critical evaluation, or abstraction. The narrative remains largely surface-level, with limited discussion of comparative strengths, weaknesses, or overarching trends."}}
{"id": "1e898be7-db20-4db0-b960-280b242ecf62", "title": "Memory System", "level": "subsubsection", "subsections": [], "parent_id": "cb797a5f-f6ce-4412-9791-ccdeb4f88886", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "ML for Fast System Modeling"], ["subsection", "Sub-system Modeling and Performance Prediction"], ["subsubsection", "Memory System"]], "content": "\\label{sec:model_memory}\nIn memory systems, ML-based performance models are exploited to help explore trade-offs among different objectives.\nTo explore non-volatile memory (NVM) based cache hierarchies, Dong \\textit{et al.}  develop an ANN model to predict higher-level features (e.g. miss of cache read/write, and instruction-per-cycle (IPC)) from lower-level features (e.g. cache associativity, capacity and latency). \nTo adaptively select architectural techniques in NVMs for different applications, Memory Cocktail Therapy  estimates lifetime, IPC, and energy consumption through lightweight online predictors by gradient boosting and quadratic regression with lasso. \nTo optimize memory controller placements in throughput processors, Lin \\textit{et al.}  build a CNN model that takes memory controller placements as inputs to predict throughput, which accelerates the optimization process by two orders of magnitude.\nSome studies concentrate on learning efficient representations of memory access patterns.\nBlock2Vec  tries to mine data block correlations by training a DNN to learn the best vector representation of each block and capturing block similarities via vector distances, which enables further optimization for caching and prefetching.\nShi \\textit{et al.}  use a graph neural network (GNN) to learn fused representations of static code and its dynamic execution. This unified representation is capable to model both data flows (e.g., prefetching) and control flows (e.g., branch prediction).", "cites": [5391], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a factual overview of ML applications in memory system modeling and performance prediction, mentioning several papers with their techniques and outcomes. While it groups the works under common themes like performance modeling and representation learning, it lacks deeper critical analysis or comparative discussion of their strengths and weaknesses. Some generalization is attempted, but the meta-level insights remain limited, preventing it from being fully analytical."}}
{"id": "a2990a76-751b-4214-b5a9-77c66ab0dd6a", "title": "Graphics Processing Unit (GPU)", "level": "subsubsection", "subsections": [], "parent_id": "9ac2d4c9-6bb8-44b3-aa23-38c568e05513", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "ML for Fast System Modeling"], ["subsection", "System Modeling and Performance Prediction"], ["subsubsection", "Graphics Processing Unit (GPU)"]], "content": "\\label{sec:model_gpu}\nThere are two types of predictions for GPU modeling: cross-platform predictions and GPU-specific predictions.\nCross-platform predictions are used to decide in advance whether to offload an application from a CPU to a GPU, since not every application benefits from GPU execution and the porting process requires considerably additional efforts;\nGPU-specific predictions are used to estimate metrics of interest and to assist GPU design space exploration, helpful to handle design space irregularities and complicated interactions among configurations.\nCross-platform predictions can be formulated as a binary classification problem that identifies whether the potential GPU speedup of an application would be greater than a given threshold.\nThis task can be solved by the nearest neighbor and SVMs using dynamic instruction profiles  , or a random forest that composes of one thousand decision trees using static analysis of source CPU code (i.e., memory coalescing, branch divergence, kernel size available parallelism and instruction intensities) .\nWith both dynamic and static program properties from single-thread CPU code, an ensemble of one hundred regression-based learners can predict the GPU execution time .\nIn terms of GPU-specific predictions that take GPU configurations and performance counters as input features, the execution time can be predicted by stepwise linear regression, which recognizes the most important input features among many GPU parameters and thus achieves high accuracy even with sparse samples ; the power/throughput can be modeled by an ensemble of NN predictors .\nProvided with profiling results from earlier-generation GPUs, an ensemble of linear and non-linear regression models is capable to predict cross-generation GPU execution time for later/future-generation GPUs, which achieves more than 10,000 times speedup compared to cycle-accurate GPU simulators .\nFocusing on processing-in-memory (PIM) assisted GPU architectures, Pattnaik \\textit{et al.}  classify GPU cores into two types: powerful GPU cores but far away from memory, and auxiliary/simple GPU cores but close to memory. They develop a logistic regression model that takes kernel characteristics as input features to predict architecture affinity of kernels, aiming to accurately identify which kernels would benefit from PIM and offload them accordingly to auxiliary GPU cores. They also build a linear regression model to predict the execution time of each kernel, so that a concurrent kernel management mechanism can be developed based on these two models and kernel dependency information. \nFocusing on general-purpose GPUs (GPGPUs), Wu \\textit{et al.}  model kernel scaling behaviors with respect to the number of compute units, engine frequency, and memory frequency. During training, kernels with similar performance scaling behaviors are grouped by K-means clustering, and when encountering a new kernel, it is mapped to the cluster that best describes its scaling performance by an ANN-based classifier.\nLi \\textit{et al.}  reassess prevailing assumptions of GPGPU traffic patterns, and combine a CNN with a t-distributed stochastic neighbor embedding to classify different traffic patterns.", "cites": [5390], "cite_extract_rate": 0.1111111111111111, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "low", "analysis": "The section provides a factual summary of ML techniques applied to GPU modeling, categorizing them into cross-platform and GPU-specific predictions. While it groups similar approaches and mentions some general techniques (e.g., random forest, SVMs), it lacks deeper integration or a novel framework. There is minimal critical evaluation of the cited works, and abstraction is limited to surface-level patterns without identifying overarching principles or trends."}}
{"id": "3890381b-cc70-4717-9bbf-fa3c92dca1c8", "title": "General Modeling and Performance Prediction", "level": "subsubsection", "subsections": [], "parent_id": "9ac2d4c9-6bb8-44b3-aa23-38c568e05513", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "ML for Fast System Modeling"], ["subsection", "System Modeling and Performance Prediction"], ["subsubsection", "General Modeling and Performance Prediction"]], "content": "\\label{sec:model_general}\nRegression techniques are the mainstream to predict performance metrics from micro-architectural parameters or other features, which attributes to their capability to make high-accuracy estimations with reasonable training costs.\nFor conventional regression-based models, ANNs and non-linear regression with different designs are the common practice to predict throughput/latency  and power/energy .\nConsequently, there are comparisons among different techniques.\nLee \\textit{et al.}  compare piecewise polynomial regression with ANNs, with emphasis that piecewise polynomial regression offers better explainability while ANNs show better generalization ability.\nOzisikyilmaz \\textit{et al.}  contrast several linear regression models and different ANNs, indicating that the pruned ANNs achieve best accuracy while requiring longer training time.\nAgarwal \\textit{et al.}  estimate the parallel execution speedup of multi-threaded applications on a target hardware platform, and mention that Gaussian process regression performs the best among several explored methods in this case.\nMore recent work tends to take advantage of data-driven approaches.\nIthemal  leverages a hierarchical multi-scale RNN with long short term memory (LSTM) to predict throughput of basic blocks (i.e., sequences of instructions with no branches or jumps), and evaluations demonstrate that Ithemal is more accurate and as fast as analytical throughput estimators.\nBy employing a variant of Ithemal as a differentiable surrogate to approximate CPU simulators, DiffTune  is able to apply gradient-based optimization techniques to learn the parameters of x86 basic block CPU simulators such that simulators' error is minimized. The learned parameters finally are plugged back into the original simulator.\nDing \\textit{et al.}  provide some insights in learning-based modeling methods: the improvement of prediction accuracy may receive diminishing returns; the consideration of domain knowledge will be helpful for system optimizations, even if the overall accuracy may not be improved.\nThus, they propose a generative model to handle data scarcity by generating more training data, and apply a multi-phase sampling to improve prediction accuracy of optimal configuration points.\nML-based predictive performance modeling enables efficient resource management and rapid design space exploration to improve throughput.\nEquipped with ANNs for IPC predictions, strategies for resource allocation  and task scheduling  can always select decisions that would bring the best predicted IPC.\nESP  constructs a regression model with elastic-net regularization to predict application interference (i.e., slowdown), which is integrated with schedulers to increase throughput.\nMetaTune  is a meta-learning based cost model for convolution operations, and when combined with search algorithms, it enables efficient auto-tuning of parameters during compilation.\nIn consideration of rapid design space exploration of the uncore (i.e., memory hierarchies and NoCs), Sangaiah \\textit{et al.}  uses a regression-based model with restricted cubic splines to estimate CPI, reducing the exploration time by up to four orders of magnitude.\nML-based predictive performance modeling benefits adaptations between performance and power budgets.\nLeveraging off-line multivariate linear regression to predict IPC and/or power of different architecture configurations, Curtis-Maury \\textit{et al.}  maximize performance of OpenMP applications by dynamic concurrency throttling and dynamic voltage and frequency scaling (DVFS); Bailey \\textit{et al.}  apply hardware frequency-limiting techniques to select optimal hardware configurations under given power constraints.\nTo effectively apply DVFS towards various optimization goals, the designed strategy can adopt predictions for power consumption by a constrained-posynomial model  or job execution time by a linear regression model .\nTo conduct smart power management in a more general manner, LEO  employs hierarchical Bayesian models to predict performance and power, and when integrated for runtime energy optimization, it is capable to figure out the performance-power Pareto frontier and select the configuration satisfying performance constraints with minimized energy.\nCALOREE~ further breaks up the power management task into two abstractions: a learner for performance modeling and an adaptive controller leveraging predictions from the learner. These abstractions enable both the learner to use multiple ML techniques and the controller to maintain control-theoretic formal guarantees. Since no user-specified parameter except the goal is required, CALOREE is applicable even for non-experts.", "cites": [4378, 5392, 5389], "cite_extract_rate": 0.13636363636363635, "origin_cites_number": 22, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers by organizing them under the umbrella of ML-based performance prediction, drawing connections between classical regression and modern data-driven methods. It offers critical comparisons between techniques, such as accuracy versus training time, and highlights limitations and trade-offs. The abstraction is strong, as it identifies broader patterns like the diminishing returns of prediction accuracy and the role of domain knowledge, while framing the work in the context of design space exploration and system optimization."}}
{"id": "df0787ef-e9e6-48be-837b-abd7e6a37852", "title": "Analog Circuit Analysis", "level": "subsubsection", "subsections": [], "parent_id": "419e1fc4-5eb3-40c4-b251-cc170597bc2e", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "ML for Fast System Modeling"], ["subsection", "Performance Modeling in Chip Design and Design Automation"], ["subsubsection", "Analog Circuit Analysis"]], "content": "\\label{sec:model_analog}\nAnalog circuit design is usually a manual process that requires many trial-and-error iterations between pre-layout and post-layout phases.\nIn recent years, the discrepancy between schematic (i.e., pre-layout) performance estimations and post-layout simulation results is further enlarged.\nOn the one hand, the analytical performance estimations from schematics are no longer accurate with device scaling; on the other hand, even though post-layout simulations can provide high-accuracy estimations, they are extremely time-consuming and have become the major bottleneck of design iteration time.\nTo shrink the gap in performance modeling of integrated circuits (ICs), ML techniques are widely applied for fast circuit evaluation.\nWe discuss the studies based on whether their input features are extracted from pre-layout or post-layout information.\n\\textcircled{\\small{1}}\nGiven design schematics, parasitics in layouts can be predicted from pre-layout stage, which helps bridge the gap of performance difference between pre-layout and post-layout simulations.\nParaGraph  builds a GNN model to predict layout-dependent parasitics and physical device parameters.\nMLParest  shows that non-graph based methods (e.g., random forest) also work well for estimating interconnect parasitics, whereas the lack of placement information may cause large variations in predictions.\n\\textcircled{\\small{2}}\nGiven circuit schematics as well as device information as inputs, it is possible to directly model post-layout performance from pre-layout designs.\nAlawieh \\textit{et al.}  propose a hierarchical method that combines the Bayesian co-learning framework and semi-supervised learning to predict power consumption.\nThe entire circuit schematic is partitioned into multiple blocks to build block-level performance models, upon which circuit-level performance models are built. By combining these two low-dimensional models with a large amount of unlabeled data, pseudo samples can be labeled with almost no cost. Finally, a high-dimensional performance model mapping low-level features to circuit-level metrics is trained with pseudo samples and a small amount of labeled samples, which demonstrates the feasibility of performance modeling with inadequate labeled samples.\nSeveral variants of Bayesian-based methods also perform well for estimating post-layout performance, e.g., combining Bayesian regression with SVM to predict circuit performance  and using Bayesian DNNs to compare circuit designs .\n\\textcircled{\\small{3}}\nSince post-layout simulations with SPICE-like simulators is time-consuming, ML techniques are applied to quickly assess layout design performance .\nTo make better use of structural information inside layouts, intermediate layout placement results are represented as 3D images to feed a 3D CNN model , or encoded as graphs to train a customized GNN model , with the goal to predict whether a design specification is satisfied.\n\\begin{table}[tbp]\n\\vspace{-10pt}\n\\caption{Summary of applying ML techniques for performance modeling and prediction in design automation.}\n\\vspace{-10pt}\n\\label{table:model_eda}\n\\centering\n    \\tiny\n    \\renewcommand{\\arraystretch}{1}\n    \\setlength{\\tabcolsep}{0.5pt}\n\\begin{tabular}{c|c|c|c}\n\\toprule\n\\textbf{Domain} & \\textbf{Prediction of} & \\textbf{Technique} & \\textbf{Input} \\\\ \\midrule\n\\multirow{5}{*}{\\textbf{\\begin{tabular}[c]{@{}c@{}}Analog \\\\ Circuit \\\\ $( \\S$ \\ref{sec:model_analog}$)$ \\end{tabular}}} & Parasitics & GNN ,  random forest  & Circuit schematics \\\\ \\cline{2-4} \n & Power/area/bandwidth & \\begin{tabular}[c]{@{}c@{}}Bayesian co-learning and semi-supervised learning , \\\\ Bayesian regression and SVM \\end{tabular} & \\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Circuit schematics \\\\ and device information\\end{tabular}} \\\\ \\cline{2-3}\n & \\begin{tabular}[c]{@{}c@{}}Probability of superiority  between designs\\end{tabular} & Bayesian DNN  &  \\\\ \\cline{2-4} \n & \\begin{tabular}[c]{@{}c@{}}Gain/unity gain frequency/bandwidth/phase margin\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}SVM/ANN/random forest , 3D CNN , GNN \\end{tabular} & \\multirow{2}{*}{Circuit placement} \\\\ \\cline{2-3}\n & Electromagnetic properties & GNN  &  \\\\ \\hline\n\\multirow{9}{*}{\\makecell[c]{\\textbf{HLS} \\\\ $( \\S$ \\textbf{\\ref{sec:model_hls}}$)$}} & \\begin{tabular}[c]{@{}c@{}}Area/latency/throughput/logic utilization\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Random forest  , transfer learning \\end{tabular} & Directives in HLS scripts \\\\ \\cline{2-4} \n & Resource utilization & ANN  & \\multirow{3}{*}{IR graphs from HLS front-ends} \\\\ \\cline{2-3}\n & Resource mapping and clustering & GraphSAGE  &  \\\\ \\cline{2-3}\n & Routing  congestion & \\begin{tabular}[c]{@{}c@{}}Linear regression/ANN/gradient boosted regression tree \\end{tabular} &  \\\\ \\cline{2-4} \n & Power & \\begin{tabular}[c]{@{}c@{}}Linear regression/SVM/tree-based models/DNNs \\end{tabular} & IR graphs and HLS reports \\\\ \\cline{2-4} \n & \\begin{tabular}[c]{@{}c@{}}Throughput and throughput-to-area ratio\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Ensemble learning by stacked regression \\end{tabular} & \\multirow{2}{*}{HLS reports} \\\\ \\cline{2-3}\n & Resource utilization and timing & \\begin{tabular}[c]{@{}c@{}}Linear regression/ANN/gradient tree boosting \\end{tabular} &  \\\\ \\cline{2-4} \n & Cross-platform latency and power & Random forest  & CPU program counters \\\\ \\cline{2-4} \n & Speedup over an ARM processor & ANN  & \\begin{tabular}[c]{@{}c@{}}Application characteristics, \\\\ HLS reports, FPGA configurations\\end{tabular} \\\\ \\hline\n\\multirow{8}{*}{\\textbf{\\begin{tabular}[c]{@{}c@{}}Logic and \\\\ Physical \\\\ Synthesis \\\\ $( \\S$ \\ref{sec:model_logic}$)$\\end{tabular}}} & Area/delay & CNN , LSTM  & Synthesis flows \\\\ \\cline{2-4} \n & \\multirow{2}{*}{DRVs} & \\begin{tabular}[c]{@{}c@{}}Linear regression/ANN/decision tree , MARS \\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Placement and GR information\\end{tabular} \\\\ \\cline{3-4} \n &  & MARS/SVM  , MLP  & Placement \\\\ \\cline{2-4} \n & \\multirow{2}{*}{DRC hotspots} & FCN  & \\begin{tabular}[c]{@{}c@{}}Placement and GR information\\end{tabular} \\\\ \\cline{3-4} \n &  & Variant of FCN  & \\multirow{3}{*}{Placement} \\\\ \\cline{2-3} \n & GR congestion map & FCN  & \\\\ \\cline{2-3} \n & \\multirow{2}{*}{Routing congestion in FPGAs} & Linear regression    &  \\\\ \\cline{3-4} \n &  & Conditional GAN  & Post-placement images \\\\ \\bottomrule\n\\end{tabular}\n\\vspace{-15pt}\n\\end{table}", "cites": [5393, 5395, 5396, 8916, 5394], "cite_extract_rate": 0.15625, "origin_cites_number": 32, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes information from multiple papers to highlight the use of ML in bridging pre-layout and post-layout performance gaps in analog circuit design. It connects different ML techniques (e.g., GNNs, random forest, Bayesian methods) with their respective inputs and goals, forming a coherent narrative. While it provides some critical points, such as the limitations of non-graph-based methods due to missing placement information, the analysis could be deeper. It abstracts some patterns (e.g., use of structural information in layout prediction) but stays largely focused on the domain-specific applications."}}
{"id": "3e9b9f12-6464-4e9c-b39f-515e81853fdb", "title": "High-Level Synthesis (HLS)", "level": "subsubsection", "subsections": [], "parent_id": "419e1fc4-5eb3-40c4-b251-cc170597bc2e", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "ML for Fast System Modeling"], ["subsection", "Performance Modeling in Chip Design and Design Automation"], ["subsubsection", "High-Level Synthesis (HLS)"]], "content": "\\label{sec:model_hls}\nHLS is an automated transformation from behavioral languages (e.g., C/C++/SystemC) to register-transfer level (RTL) designs, which significantly expedites the development of hardware designs involving with field-programmable gate arrays (FPGAs) or application-specific integrated circuits (ASICs).\nSince HLS tools usually take considerable time to synthesize each design, it prevents designers from exploring design space sufficiently, which motivates the application of ML models for fast and accurate performance estimation.\nIn performance estimation of HLS designs, the input features to ML models are extracted from three major sources: HLS directives, IRs from HLS front-ends, and HLS reports.\n\\textcircled{\\small{1}} Taking the directives in an HLS script as input features, random forest is capable to forecast different design metrics, such as area and effective latency , and throughput and logic utilization .\nIn order to reuse knowledge from previous experiences, a transfer learning approach  can transfer knowledge across different applications or synthesis options.\n\\textcircled{\\small{2}} Taking advantages of IR graphs generated by HLS front-ends, Koeplinger \\textit{et al.}  count resource requirements of each node in graphs by using pre-characterized area models, which are then used as inputs to ANNs to predict the LUT routing usage, register duplication, and unavailable LUTs.\nThe exploitation of GNNs makes it possible to automatically predict the mapping from arithmetic operations in IR graphs to different resources on FPGAs . \nTo forecast post-implementation routing congestion, Zhao \\textit{et al.}  build a dataset that connects the routing congestion metrics after RTL implementation with operations in IRs, with the goal to train ML models locating highly congested regions in source code.\n\\textcircled{\\small{3}} Taking the information that can be directly extracted from HLS reports, Dai \\textit{et al.}  try several ML models (linear regression, ANN, and gradient tree boosting) to predict post-implementation resource utilization and timing. \nPyramid  applies the ensemble learning by stacked regression to accurately estimate the throughput or the throughput-to-area ratio.\nHL-Pow  employs features from both IR graphs and HLS reports to predict the power by a variety of ML models. \nThe surge of heterogeneous platforms with FPGA/AISC and CPU provides more possibility of hardware/software co-design, motivating cross-platform performance predictions.\nHLSPredict  uses random forest to predict FPGA cycle counts and power consumption based on program counter measurements obtained from CPU execution. \nWhile HLSPredict targets the same FPGA platform in training and testing, XPPE  considers different FPGA platforms, and uses ANNs to predict the speedup of an application on a target FPGA over an ARM processor.", "cites": [5393, 5394], "cite_extract_rate": 0.18181818181818182, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of ML applications in HLS performance modeling, listing the sources of input features and the techniques used by different papers. While it mentions some methods (e.g., random forest, ANNs, GNNs) and goals (e.g., predicting power, routing congestion), it does not critically evaluate the strengths or limitations of these approaches. There is minimal abstraction or synthesis into a broader framework or trend."}}
{"id": "8ba558a9-ad3c-4f4c-a999-80fc17f11ebf", "title": "Logic and Physical Synthesis", "level": "subsubsection", "subsections": [], "parent_id": "419e1fc4-5eb3-40c4-b251-cc170597bc2e", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "ML for Fast System Modeling"], ["subsection", "Performance Modeling in Chip Design and Design Automation"], ["subsubsection", "Logic and Physical Synthesis"]], "content": "\\label{sec:model_logic}\nIn digital design, logic synthesis converts RTL designs into optimized gate-level representations;\nphysical synthesis then transforms these design netlists into physical layouts.\nSince these two stages may take hours or days to generate final bitstreams/layouts, many problems benefit from the power of ML models for fast performance estimation.\nIn logic synthesis, CNN models  or LSTM-based models  can be leveraged to forecast the delay and area after applying different synthesis flows on specific designs, where the inputs are synthesis flows represented in either matrices or time series.\nIn physical synthesis, routing is a sophisticated problem subject to stringent constraints, and EDA tools typically utilize a two-step method: global routing (GR) and detailed routing (DR). GR tool allocates routing resource coarsely, and provides routing plans to guide DR tools to complete the entire routing.\nIn general, routing congestion can be figured out during or after GR; the routability of a design is confirmed after DR and design rule checking (DRC).\nEndeavors have been made to predict routability from early layout stages, so as to avoid excessive iterations back and forth between placement and routing.\nIn ASICs, some investigations predict routability by estimating the number of design rule violations (DRVs).\nTaking GR results as inputs, Li \\textit{et al.}  explore several ML models (linear regression, ANN, and decision tree) to predict the number of DRVs, final hold slack, power, and area. \nQi \\textit{et al.}  rely on placement data and congestion maps from GR as input features, and use a nonparametric regression technique, multivariate adaptive regression splines (MARS) , to predict the utilization of routing resource and the number of DRVs.\nBy merely leveraging placement information, it is possible to predict routability by MARS and SVM , or to detect DR short violations by an MLP .\nWhen representing placement information as images, fully convolutional networks (FCNs) are capable to predict locations of DRC hotspots by considering GR information as inputs , or to forecast GR congestion maps by formulating the prediction task as a pixel-wise binary classification using placement data .\nJ-Net  is a customized FCN model, and takes both high-resolution pin patterns and low-resolution layout information from the placement stage as features to output a 2D array that indicates if the tile corresponding to each entry is a DRC hotspot.\nIn FPGAs, routing congestion maps can be directly estimated by linear regression  using feature vectors coming from pin counts and wirelength per area of SLICEs.\nBy constructing the routing congestion prediction as an image translation problem, a conditional GAN  is able to take post-placement images as inputs to predict congestion heat maps.", "cites": [5395, 8916], "cite_extract_rate": 0.15384615384615385, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple approaches for predicting performance in logic and physical synthesis using ML, connecting them under a common theme of accelerating design processes. It abstracts by grouping techniques like CNNs, LSTMs, MARS, SVM, and GANs into their respective roles and input representations. However, it lacks deeper critical analysis of the limitations or trade-offs between these methods, and instead primarily summarizes and categorizes them."}}
{"id": "87e1e057-9e86-4a5c-afa3-431c75b16a64", "title": "ML as Design Methodology", "level": "section", "subsections": ["4ce0d49a-419f-4a57-9bf8-bac10689ff82", "93695066-a3b6-43e6-a561-f289514d99b8", "02b1496f-b229-4d07-b91a-238e79b26753", "8b48b63d-94af-4ec0-a218-72746bce7952", "de7c5699-73a2-48d8-9e75-ff1b60c4f5f5", "68c1d9ca-e725-4975-ad78-356d7848d1d1", "e25c32d8-64dc-4364-8501-8063014c67c6"], "parent_id": "5631fef3-6389-407d-9101-745f45240141", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "ML as Design Methodology"]], "content": "\\label{sec:methodology}\nThis section introduces studies that directly employ ML techniques as the design methodology for computer architecture/systems.\nComputer architecture and systems have been becoming increasingly complicated, making it expensive and inefficient for human efforts to design or optimize them.\nIn response, visionaries have argued that computer architecture and systems should be imbued with the capability to design and configure themselves, adjust their behaviors according to workloads' needs or user-specified constraints, diagnose failures, repair themselves from the detected failures, etc.\nWith strong learning and generalization capabilities, ML-based techniques are naturally suitable to resolve these considerations, which can adjust their policies during system designs according to long-term planning and dynamic workload behaviors.\nAs many problems in architecture/system design can be formulated as combinatorial optimization or sequential decision-making problems, RL is broadly explored and exploited.\nTable \\ref{table:dse_sys} and Table \\ref{table:dse_eda} recapitulate the studies that apply ML techniques as the design methodology for computer architecture/system and design automation respectively, in terms of target tasks and adopted ML techniques.\n\\begin{table}[tbp]\n\\vspace{-10pt}\n\\caption{Summary of applying ML techniques as the design methodology for computer architecture/ systems.}\n\\vspace{-10pt}\n\\label{table:dse_sys}\n\\centering\n    \\tiny\n    \\renewcommand{\\arraystretch}{0.9}\n    \\setlength{\\tabcolsep}{3pt}\n\\begin{tabular}{c|c|c}\n\\toprule\n\\textbf{Domain} & \\textbf{Task} & \\textbf{Technique} \\\\ \\midrule\n\\multirow{4}{*}{\\textbf{\\begin{tabular}[c]{@{}c@{}}Memory \\\\ System\\\\ Design \\\\ $( \\S$ \\ref{sec:dse_mem}$)$ \\end{tabular}}} & Cache replacement policy & \\begin{tabular}[c]{@{}c@{}}Perceptron learning   , Markov decision process , LSTM and SVM \\end{tabular} \\\\ \\cline{2-3} \n & Cache prefetching policy & \\begin{tabular}[c]{@{}c@{}}Perceptron learning   , contextual bandit , LSTM \\end{tabular} \\\\ \\cline{2-3} \n & Memory controller policy & Q-learning    \\\\ \\cline{2-3} \n & Garbage collection & Q-learning  \\\\ \\hline\n\\textbf{\\begin{tabular}[c]{@{}c@{}}Branch\\\\ Prediction \\\\ $( \\S$ \\ref{sec:dse_bp}$)$\\end{tabular}} & Branch direction & \\begin{tabular}[c]{@{}c@{}}MLP , piecewise linear regression , \\\\ perceptron   , CNN \\end{tabular} \\\\ \\hline\n\\multirow{8}{*}{\\makecell[c]{\\textbf{NoC} \\\\ $( \\S$ \\textbf{\\ref{NoC}}$)$}} & Link management & ANN    \\\\ \\cline{2-3} \n & DVFS for routers & Q-learning    \\\\ \\cline{2-3} \n & Routing & Q-learning    \\\\ \\cline{2-3} \n & Arbitration policy & DQN    \\\\ \\cline{2-3} \n & Adjusting injection rates & Q-learning ,   ANN  \\\\ \\cline{2-3} \n & Selection of fault-tolerant modes & Q-learning    \\\\ \\cline{2-3} \n & Link placement in 3D NoCs & STAGE algorithm    \\\\ \\cline{2-3} \n & Loop placement in routerless NoCs & Advantage actor-critic with MCTS    \\\\ \\hline\n\\multirow{6}{*}{\\textbf{\\begin{tabular}[c]{@{}c@{}}Power \\\\ Management \\\\ $( \\S$ \\ref{power}$)$ \\end{tabular}}} & DVFS and thread packing & Multinomial logistic regression    \\\\ \\cline{2-3} \n & DVFS and power gating & MLP  \\\\ \\cline{2-3} \n & \\begin{tabular}[c]{@{}c@{}}DVFS, socket allocation, and use of HyperThreads\\end{tabular} & Extra trees/gradient boosting/KNN/MLP/SVM  \\\\ \\cline{2-3} \n & \\begin{tabular}[c]{@{}c@{}}DVFS for CPU cores/uncore/through-silicon interposers\\end{tabular} & Propositional rule   , ANN , Q-learning    \\\\ \\cline{2-3} \n & \\begin{tabular}[c]{@{}c@{}}DVFS for CPU-GPU heterogeneous platforms\\end{tabular} & Weighted majority algorithm    \\\\ \\cline{2-3} \n & DVFS for multi-/many-core systems & \\begin{tabular}[c]{@{}c@{}}Q-learning , semi-supervised RL , hierarchical Q-learning   \\end{tabular} \\\\ \\hline\n\\multirow{6}{*}{\\textbf{\\begin{tabular}[c]{@{}c@{}}Resource \\\\ Management \\\\ \\& Task \\\\ Allocation \\\\ $( \\S$ \\ref{resource management}$)$\\end{tabular}}} & Tuning architecture configurations & \\begin{tabular}[c]{@{}c@{}}Maximum likelihood   , statistical machine learning   \\end{tabular} \\\\ \\cline{2-3} \n & Dynamic cache partitioning & Enforced subpopulations   , Q-learning  \\\\ \\cline{2-3} \n & Task allocation in many-core systems & Q-learning   , DDPG  \\\\ \\cline{2-3} \n & Workflow management & SVM and random forest    \\\\ \\cline{2-3} \n & Hardware resource assignment & REINFORCE   , Bayesian optimization  \\\\ \\cline{2-3} \n & Device placement & \\begin{tabular}[c]{@{}c@{}}REINFORCE   , policy gradient , PPO \\end{tabular} \\\\ \\hline\n\\multirow{2}{*}{\\makecell[c]{\\textbf{Scheduling}\\\\ $( \\S$ \\textbf{\\ref{scheduling}}$)$}} & Scheduling jobs in single-core processors & Q-learning    \\\\ \\cline{2-3} \n & Scheduling jobs in multi-processor systems & Value-based RL    \\\\ \\hline\n\\multirow{9}{*}{\\textbf{\\begin{tabular}[c]{@{}c@{}}Data Center\\\\ Management \\\\ $( \\S$ \\ref{data center}$)$\\end{tabular}}} & Assignment of servers to applications & Value-based RL    \\\\ \\cline{2-3} \n & Content allocation in CDNs & Fuzzy RL    \\\\ \\cline{2-3} \n & \\begin{tabular}[c]{@{}c@{}}Placement of virtual machines onto physical machines\\end{tabular} & PPO  \\\\ \\cline{2-3} \n & Traffic optimization & Policy gradient and DDPG    \\\\ \\cline{2-3} \n & Scheduling jobs with complex dependency & REINFORCE  \\\\ \\cline{2-3} \n & Straggler diagnosis & Statistical ML    \\\\ \\cline{2-3} \n & Data-center-level caching policy & \\begin{tabular}[c]{@{}c@{}}Decision tree , LSTM , gradient boosting , DDPG \\end{tabular} \\\\ \\cline{2-3} \n & Bitrate selection for video chunks & A3C  \\\\ \\cline{2-3} \n & \\begin{tabular}[c]{@{}c@{}}Scheduling video workloads in hybrid CPU-GPU clusters\\end{tabular} & DQN  \\\\ \\hline\n \\multirow{3}{*}{\\textbf{\\begin{tabular}[c]{@{}c@{}}Code\\\\ Generation \\\\ $( \\S$ \\ref{code_generation}$)$\\end{tabular}}} & Code completion & N-gram model and RNN    \\\\ \\cline{2-3} \n & Code   generation & LSTM   \\\\ \\cline{2-3} \n & Program translation & Tree-to-tree encoder-decoder , seq2seq , transformer  \\\\ \\hline\n\\multirow{6}{*}{\\makecell[c]{\\textbf{Compiler}\\\\ $( \\S$ \\textbf{\\ref{sec:compiler}}$)$}} & Instruction scheduling & Temporal difference   ,  projective reparameterization  \\\\ \\cline{2-3} \n & Improving compiler heuristics & NEAT , LSTM  \\\\ \\cline{2-3} \n & Ordering of optimizations & NEAT    \\\\ \\cline{2-3} \n & Automatic vectorization & Imitation learning  \\\\ \\cline{2-3} \n & \\begin{tabular}[c]{@{}c@{}}Program transformation for approximate computing\\end{tabular} & MLP  \\\\ \\cline{2-3} \n & Compilation   for DNN workloads & PPO ,   policy gradient  \\\\ \\bottomrule\n\\end{tabular}\n\\end{table}", "cites": [5397, 5409, 3579, 5400, 5399, 5406, 5398, 5407, 5403, 3603, 5408, 5401, 5405, 4096, 5402, 5404], "cite_extract_rate": 0.15384615384615385, "origin_cites_number": 104, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section organizes cited works into a taxonomy based on domain and task, and lists the ML techniques used. While it provides a structured overview, it lacks deeper synthesis of cross-domain insights or novel frameworks. There is minimal critical evaluation or discussion of limitations, and the abstraction remains at a surface level with no clear meta-principles or trends identified."}}
{"id": "c842f575-5671-48ac-aa2a-715cd98df8a6", "title": "Cache", "level": "subsubsection", "subsections": [], "parent_id": "4ce0d49a-419f-4a57-9bf8-bac10689ff82", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "ML as Design Methodology"], ["subsection", "Memory System Design"], ["subsubsection", "Cache"]], "content": "\\label{cache}\nThe conspicuous disparity in latency and bandwidth between CPUs and memory systems motivates investigations for efficient cache management.\nThere are two major types of studies on cache optimization: improving cache replacement policies, and designing intelligent prefetching policies.\n\\textcircled{\\small{1}} \nTo develop cache replacement policies, perceptron learning is employed to predict whether to bypass or reuse a referenced block in the last-level cache (LLC) .\nInstead of using perceptrons, Beckmann \\textit{et al.}  model the cache replacement problem as a Markov decision process and replace lines according to the difference between their expected hits and the average hits.\nShi \\textit{et al.}~ train an attention-based LSTM model offline to extract insights from history program counters, which are then used to build an online SVM-based hardware predictor to serve as the cache replacement policy.\n\\textcircled{\\small{2}} \nTo devise intelligent prefetchers, Wang \\textit{et al.}  propose a prefetching mechanism that uses conventionally table-based prefetchers to provide prefetching suggestions and a perceptron trained by spatio-temporal locality to reject unnecessary prefetching decisions, ameliorating the cache pollution problem. \nSimilarly, Bhatia \\textit{et al.}  integrate a perceptron-based prefetching filter with conventional prefetchers, increasing the coverage of prefetches without hurting accuracy.\nInstead of the commonly used spatio-temporal locality, a context-based memory prefetcher  leverages the semantic locality that characterizes access correlations inherent to program semantics and data structures, which is approximated by a contextual bandit model in RL.\nInterpreting semantics in memory access patterns is analogous to sequence analysis in natural language processing (NLP), and thus several studies use LSTM-based models and treat the prefetching as either a regression problem  or a classification problem .\nEven with better performance, especially for long access sequences and noise traces, LSTM-based prefetchers suffer from long warm-up and prediction latency, and considerable storage overheads.\nThe discussion of how hyperparameters impact LSTM-based prefetchers' performance  highlights that the lookback size (i.e. memory access history window) and the LSTM model size strongly affect prefetchers' learning ability under different noise levels or workload patterns.\nTo accommodate the large memory space, Shi \\textit{et al.}  introduce a hierarchical sequence model to decouple predictions of pages and offsets by using two separate attention-based LSTM layers, whereas the corresponding hardware implementation is impractical for actual processors.", "cites": [5401, 5398], "cite_extract_rate": 0.18181818181818182, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple approaches to cache optimization using ML by categorizing them into replacement and prefetching policies, and connects different techniques (e.g., perceptron, MDP, LSTM) under these categories. It provides a critical evaluation of LSTM-based prefetchers by discussing their limitations such as latency and storage overheads. The section also abstracts by drawing parallels between memory access analysis and NLP, though deeper generalization or a meta-level framework is not fully developed."}}
{"id": "dc411e9f-ce19-4cdd-9e1c-1930d2c68aa6", "title": "Others", "level": "subsubsection", "subsections": [], "parent_id": "4ce0d49a-419f-4a57-9bf8-bac10689ff82", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "ML as Design Methodology"], ["subsection", "Memory System Design"], ["subsubsection", "Others"]], "content": "A variety of work targets different parts of the memory system.\nMargaritov \\textit{et al.}  accelerate virtual address translation through learned index structures . The results are encouraging in terms of the accuracy, which reaches almost 100\\% for all tested virtual addresses; yet this method has unacceptably long inference latency, leaving practical hardware implementation as the future work.\nWang \\textit{et al.}  reduce data movement energy in interconnects by exploiting asymmetric transmission costs of different bits, where data blocks to be transmitted are dynamically grouped by K-majority clustering to derive energy-efficient expressions for transmission.\nIn terms of garbage collection in NAND flash, Kang \\textit{et al.}  propose an RL-based method to reduce the long-tail latency. The key idea is to exploit the inter-request interval (idle time) to dynamically decide the number of pages to be copied or whether to perform an erase operation, where decisions are made by table-based Q-learning. Their following work  considers more fine-grained states, and introduces a Q-table cache to manage key states among enormous amount of states.", "cites": [3600], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides brief descriptions of different memory-related ML applications and makes minimal connections between them. It includes some critical analysis, such as pointing out the unacceptably long inference latency in one approach and noting future work. However, the synthesis and abstraction are limited, as it does not offer a broader framework or identify overarching patterns across the cited works."}}
{"id": "93695066-a3b6-43e6-a561-f289514d99b8", "title": "Branch Prediction", "level": "subsection", "subsections": [], "parent_id": "87e1e057-9e86-4a5c-afa3-431c75b16a64", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "ML as Design Methodology"], ["subsection", "Branch Prediction"]], "content": "\\label{sec:dse_bp}\nBranch predictor is one of the mainstays of modern processors, significantly improving the instruction-level parallelism. As pipelines gradually deepen, the penalty of mis-prediction increases. Traditional branch predictors often consider limited history length, which may hurt the prediction accuracy.\nIn contrast, the perceptron/MLP-based predictors can handle long histories with reasonable hardware budgets, outperforming prior state-of-the-art non-ML-based predictors.\nStarting with a static branch predictor trained with static features from program corpus and control flow graphs, an MLP is used to predict the direction of a branch at compile time .\nLater, a dynamic branch predictor uses a perceptron-based method . It hashes the branch address to select the proper perceptron and computes the dot product accordingly to decide whether to take this branch, which shows great performance on linearly separable branches. \nIts latency and accuracy can be further improved by applying ahead pipelining and selecting perceptrons based on path history .\nTo attain high accuracy in non-linearly separable branches, the perceptron-based prediction is generalized as piecewise linear branch prediction .\nIn addition to the path history, multiple types of features from different organizations of branch histories can be leveraged to enhance the overall performance .\nWhen considering practical hardware implementation of branch predictors, SNAP  leverages current-steering digital-to-analog converters to transfer digital weights into analog currents and replaces the costly digital dot-product computation to the current summation.\nIts optimized version  equips several new techniques, such as the use of global and per-branch history, trainable scaling coefficients, dynamic training thresholds, etc.\nRather than making binary decisions of whether to take a certain branch, it is possible to directly predict the target address of an indirect branch at the bit level via perceptron-based predictors .\nWhile high accuracy is achieved by current perceptron/MLP-based predictors, Tarsa \\textit{et al.}  notice that a small amount of static branch instructions are systematically mispredicted, referred to as hard-to-predict branches (H2Ps). Consequently, they propose a CNN helper predictor for pattern matching of history branches, ultimately improving accuracy for H2Ps in conditional branches.", "cites": [5405], "cite_extract_rate": 0.1111111111111111, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple ML-based branch prediction techniques, integrating them into a coherent narrative that highlights progression from static to dynamic predictors and improvements via hardware design. It critically evaluates limitations of perceptron-based methods, such as misprediction of hard-to-predict branches, and presents a solution from Tarsa et al. The abstraction level is strong, identifying general principles like the use of global history and the role of CNNs in addressing specific prediction challenges."}}
{"id": "5afafbcf-6c39-4a33-8c76-314585dc4284", "title": "General Design", "level": "subsubsection", "subsections": [], "parent_id": "02b1496f-b229-4d07-b91a-238e79b26753", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "ML as Design Methodology"], ["subsection", "NoC Design"], ["subsubsection", "General Design"]], "content": "With the growing number of cores per chip/system, the increasing heterogeneity of cores, and various performance targets, it is complicated to simultaneously optimize copious design knobs in NoCs.\nOne attempt to automated NoC design is the MLNoC , which utilizes supervised learning to quickly find near-optimal NoC designs under multiple optimization goals. MLNoC is trained by data from thousands of real-world and synthetic SoC (system-on-chip) designs, and evaluated with real-world SoC designs. Despite disclosure of limited details and absence of comprehensive comparison with other design methods, it shows superior performance to manually optimized NoC designs, delivering encouraging results.\nApart from conventional 2D mesh NoCs, a series of investigations focuses on 3D NoC designs, where the STAGE algorithm is applied to optimize vertical and planar placement of communication links in small-world network based 3D NoCs .\nThe STAGE algorithm repeatedly alternates between two stages, the base search that tries to find the local optima based on the learned evaluation function, and the meta-search that uses SVR to learn evaluation functions.\nLater, the STAGE algorithm is extended for multi-objective optimization in heterogeneous 3D NoC systems , which jointly considers GPU throughput, average latency between CPUs and LLCs, temperature, and energy.\nIn terms of routerless NoCs that any two nodes are connected via at least one ring/loop, a deep RL framework that exploits Monte-Carlo tree search for efficient design space exploration is developed to optimize loop placements , and the design constraints can be strictly enforced by carefully devising the reward function.", "cites": [5400, 5406], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates information from multiple papers to present different ML approaches for NoC design, including supervised learning and deep reinforcement learning. While it connects these methods to broader design challenges, it does so with limited depth and without a novel unifying framework. It provides some critical evaluation, such as noting the lack of comprehensive comparisons in MLNoC, but overall focuses more on summarizing the approaches and their applications rather than deep analysis or identifying overarching principles."}}
{"id": "35decbd9-a2aa-43b2-b9e5-0585c9140768", "title": "Resource Management and Task Allocation", "level": "subsubsection", "subsections": [], "parent_id": "8b48b63d-94af-4ec0-a218-72746bce7952", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "ML as Design Methodology"], ["subsection", "Resource Allocation or Management"], ["subsubsection", "Resource Management and Task Allocation"]], "content": "\\label{resource management}\nModern architectures and systems have been becoming so sophisticated and diverse that it is non-trivial to either optimize performance or fully utilize system resource.\nThis rapidly evolving landscape is further complicated by various workloads with specific requirements or targets.\nIn order to keep the pace, one cure is to develop more efficient and automated methods for resource management and task allocation, where ML-based techniques are excelled to explore large design spaces and simultaneously optimize multiple objectives, and preserve better scalablility and portability after carefully designed.\nFor a single-core processor, a regularized maximum likelihood approach  predicts the best hardware micro-architectural configuration for each phase of a program, based on runtime hardware counters. \nFor multi-core processors, a statistical machine learning (SML) based method  can quickly find configurations that simultaneously optimize running time and energy efficiency. Since this method is agnostic to application and micro-architecture domain knowledge, it is a portable alternative to human expert optimization.\nSML can also be applied as a holistic method to design self-evolving systems that optimize performance hierarchically across circuit, platform, and application levels .\nIn addition to tuning architectural configurations, dynamic on-chip resource management is crucial for multi-core processors, where one example is dynamic cache partitioning.\nIn response to changing workload demands, an RNN evolved by the enforced subpopulations algorithm  is introduced to partition L2 cache dynamically.\nWhen integrating dynamic partitioning of LLC with DVFS on cores and uncore, a co-optimization method using table-based Q-learning achieves much lower energy-delay products than any of the techniques applied individually .\nTo guarantee efficient and reliable execution in many-core systems, task allocation should consider several aspects, such as heat and communication issues.\nTargeting the heat interaction of processor cores and NoC routers, Lu \\textit{et al.}  apply Q-learning to assign tasks to cores based on current temperatures of cores and routers, such that the maximum temperature in the future is minimized.\nTargeting the non-uniform and hierarchical on/off-chip communication capability in multi-chip many-core systems, core placement optimization  leverages deep deterministic policy gradient (DDPG)  to map computation onto physical cores, able to work in a manner agnostic to domain-specific information.\nSome studies pay attention to workflow management and general hardware resource assignment.\nSmartFlux  focuses on the workflow of data-intensive and continuous processing. It intelligently guides asynchronous triggering of processing steps with the help of predictions made by multiple ML models (e.g., SVM, random forest), which indicate whether to execute certain steps and to decide corresponding configurations upon each wave of data.\nGiven target DNN models, deployment scenarios, platform constraints, and optimization objectives (latency/energy), ConfuciuX~ applies a hybrid two-step scheme for optimal hardware resource assignments (i.e., assigning the number of processing elements and the buffer sizes to each DNN layer), where REINFORCE  performs a global coarse-grained search followed by a genetic algorithm for fine-grained tuning.\nApollo  is a general architecture exploration framework for sample-efficient accelerator designs, which leverages ML-based black-box optimization techniques (e.g., Bayesian optimization) to optimize accelerator configurations to satisfy use-specified design constraints.\nIn heterogeneous systems with CPUs and GPUs, device placement refers to the process of mapping nodes in computational graphs of neural networks onto proper hardware devices.\nInitially, computational operations are grouped manually, and assigned to devices by REINFORCE that employs a sequence-to-sequence RNN model as the parameterized policy .\nLater, a hierarchical end-to-end model makes this manual grouping process automatic .\nThe training speed is further improved by introduction of proximal policy optimization (PPO) .\nDespite great advance brought by the above approaches, they are not transferable and a new policy should be trained from scratch specifically for each new computational graph.\nBy encode structure of computational graphs with static graph embeddings  or learnable graph embeddings , the trained placement policy exhibits great generalizability to unseen neural networks.", "cites": [5404, 5403, 5407, 5409], "cite_extract_rate": 0.23529411764705882, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple works, integrating them into a coherent narrative about ML-based resource management and task allocation across different architectural scales. It also highlights limitations, such as the lack of transferability in some approaches, and suggests improvements like using graph embeddings for generalizability. The section abstracts beyond individual papers by identifying overarching patterns, such as the role of reinforcement learning in optimizing task placement and the importance of generalizability in heterogeneous systems."}}
{"id": "de7c5699-73a2-48d8-9e75-ff1b60c4f5f5", "title": "Data Center Management", "level": "subsection", "subsections": [], "parent_id": "87e1e057-9e86-4a5c-afa3-431c75b16a64", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "ML as Design Methodology"], ["subsection", "Data Center Management"]], "content": "\\label{data center}\nWith the rapid scale expansion of data centers, issues that may be trivial in a single machine become increasingly challenging, let alone the inherently complicated problems.\nEarly work aims at a relatively simple scenario of resource allocation, i.e., to dynamically assign different numbers of servers to multiple applications. \nThis problem can be modeled as an RL problem with service-level utility functions as rewards:\nthe arbiter will select a joint action that would bring the maximum total return after consulting local value functions estimated via either table-based methods  or function approximation . \nIn order to better model interactions among multiple agents, a multi-agent coordination algorithm with fuzzy RL  can be used to solve the dynamic content allocation in content delivery networks (CDNs), in which each requested content is modeled as an agent, trying to move toward the area with a high demand while coordinating with other agents/contents.\nA recent innovation  pays attention to the placement of virtual machines onto physical machines, so as to minimize the peak-to-average ratio of resource usage across physical machines, where PPO and hindsight imitation learning are evaluated.\nTo improve data center performance and quality of experience (QoE) for users, ML-based techniques have been explored in a few directions.\n\\textcircled{\\small{1}} It is important to efficiently schedule jobs and effectively diagnose stragglers within jobs.\nAiming at traffic optimization (e.g., flow scheduling, load balancing) in data centers, Chen~\\textit{et al.}  develop a two-level RL system: peripheral systems, which are trained by DDPG, reside on end-hosts and locally make instant traffic optimization decisions for short flows; the central system, which is trained by policy gradient, aggregates global traffic information, guides behaviors of peripheral systems, and makes traffic optimization decisions for long flows.\nDecima  exploits GNNs to represent cluster information and dependency among job stages, so that the RL-based scheduler can automatically learn workload-specific scheduling policies to schedule data processing jobs with complex dependency. \nHound  combines statistical ML with meta-learning to diagnose causes of stragglers at data-center-scale jobs.\n\\textcircled{\\small{2}} It is essential to deploy an intelligent data-center-level cache.\nDeepCache  employs an LSTM encoder-decoder model to predict future content popularity, which can be combined with existing cache policies to make smarter decisions.\nSong \\textit{et al.}  apply gradient boosting machines to mimic a relaxed Belady algorithm that evicts an object whose next request is beyond a reuse distance threshold but not necessarily the farthest in the future.\nPhoebe  is an online cache replacement framework leveraging DDPG to predict priorities of objects and to conduct eviction accordingly.\nConsidering non-history based features, Wang \\textit{et al.}~ build a decision tree to predict whether the requested file will be accessed only once in the future. These one-time-access files will be directly sent to users without getting into cache, to avoid cache pollution. \n\\textcircled{\\small{3}} From the workload perspective, video workloads on CDNs or clusters are prevalent but their optimization is quite challenging: first, network conditions fluctuate overtime and a variety of QoE goals should be balanced simultaneously; second, only coarse decisions are available and current decisions will have long-term effects on following decisions.\nThis scenario naturally matches the foundation of RL-based techniques.\nTo optimize users' QoE of streaming videos, adaptive bitrate algorithms have been recognized as the primary tool used by content providers, which are executed on client-side video players and dynamically choose a bitrate for each video chunk based on underlying network conditions.\nPensieve  applies asynchronous advantage actor-critic  to select proper bitrate for future video chunks based on resulting performance from past decisions.\nWhen considering large-scale video workloads in hybrid CPU-GPU clusters, performance degradation often comes from uncertainty and variability of workloads, and unbalanced use of heterogeneous resources. \nTo accommodate this, Zhang \\textit{et al.}  use two deep Q-networks to build a two-level task scheduler, where the cluster-level scheduler selects proper execution nodes for mutually independent video tasks and the node-level scheduler assigns interrelated video subtasks to appropriate computing units. This scheme enables the scheduling model to adjust policies according to runtime status of cluster environments, characteristics of video tasks, and dependency among video tasks.", "cites": [3579, 3603, 1390], "cite_extract_rate": 0.2, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers to present a coherent narrative on how ML, particularly RL, is used in data center management for tasks like scheduling, caching, and adaptive resource allocation. It abstracts some patterns, such as the use of hierarchical RL for different time scales and workload characteristics. While it provides a clear analytical structure, it lacks deeper critical evaluation of limitations or trade-offs among the approaches."}}
{"id": "21901bea-3ca4-4650-b39a-4ecbde92fab7", "title": "Code Generation", "level": "subsubsection", "subsections": [], "parent_id": "68c1d9ca-e725-4975-ad78-356d7848d1d1", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "ML as Design Methodology"], ["subsection", "Code Generation and Compiler"], ["subsubsection", "Code Generation"]], "content": "\\label{code_generation}\nDue to the similarities in syntax and semantics between programming languages and natural languages, the problem of code generation or translation is often modeled as an NLP problem or a neural machine translation (NMT) problem. \nHere, we would like to bring up a brief discussion. \nFor more reference, a comprehensive survey  detailedly contrasts programming languages against natural languages, and discusses how these similarities and differences drive the design and application of different ML models in code.\nTargeting code completion, several statistical language models (N-gram model, RNN, and a combination of these two)  are explored to select sentences that have the highest probability and satisfy constraints to fill up partial programs with holes.\nAs for code generation, CLgen  trains LSTM models by a corpus of hand-written code to learn semantics and structures of OpenCL programs, and generates human-like programs via iteratively sampling from the learned model.\nTargeting program translation, NMT-based techniques are widely applied to migrate code from one language to another.\nFor example, a tree-to-tree model with the encoder-decoder structure effectively translates programs from Java to C\\# ;\nthe sequence-to-sequence (seq2seq) model can translate from CUDA to OpenCL .\nRather than translating between high-level programming languages, Coda  translates binary executables to the corresponding high-level code, which employs a tree-to-tree encoder-decoder structure for code sketch generation and an ensembled RNN-based error predictor for iterative error correction on the generated code.\nNotably, these supervised NMT-based techniques may confront several issues: difficulty to generalize to programs longer than training ones, limited size of vocabulary sets, and scarcity of aligned input-output data.\nFully counting on unsupervised machine translation, TransCoder  adopts a transformer architecture and uses monolingual source code to translate among C++, Java, and Python.", "cites": [5397, 5408, 4096, 5410], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several key papers on code generation and translation by framing them under the broader ML-based design methodology, connecting NLP and NMT approaches to specific tasks. It identifies limitations of supervised methods and highlights unsupervised alternatives, showing some critical perspective. While it offers general patterns in ML approaches for code generation, the abstraction remains task-specific and does not reach a meta-level synthesis."}}
{"id": "9d772652-3e90-4f57-98a3-ad3bb92cfe28", "title": "Compiler", "level": "subsubsection", "subsections": [], "parent_id": "68c1d9ca-e725-4975-ad78-356d7848d1d1", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "ML as Design Methodology"], ["subsection", "Code Generation and Compiler"], ["subsubsection", "Compiler"]], "content": "\\label{sec:compiler}\nThe complexity of compilers grows with the complexity of computer architectures and workloads. \nML-based techniques can optimize compilers from many perspectives, such as instruction scheduling, compiler heuristics, the order to apply optimizations, hot path identification, auto-vectorization, and compilation for specific applications.\n\\textcircled{\\small{1}} For instruction scheduling, the preference function of one scheduling over another can be computed by the temporal difference algorithm in RL .\nRegarding scheduling under highly-constrained code optimization, the projective reparameterization  enables automatic instruction scheduling under constraints of data-dependent partial orders over instructions.\n\\textcircled{\\small{2}} For improving compiler heuristics, Neuro-Evolution of Augmenting Topologies (NEAT)  improves instruction placement heuristics by tuning placement cost functions.\nTo avoid manual feature engineering, LSTM-based model  automatically learns compiler heuristics from raw code, which constructs proper embeddings of programs and simultaneously learn the optimization process.\n\\textcircled{\\small{3}} For choosing the appropriate order to apply different optimizations, NEAT  can automatically generate beneficial optimization orderings for each method in a program.\n\\textcircled{\\small{4}} For path profiling, CrystalBall  uses an LSTM model to statically identify hot paths, the sequences of instructions that are frequently executed. As CrystalBall only relies on IRs, it avoids manual feature crafting and is independent of language or platform. \n\\textcircled{\\small{5}} For automatic vectorization, Mendis~\\textit{et al.}  leverage imitation learning to mimic optimal solutions provided by superword-level-parallelism based vectorization .\n\\textcircled{\\small{6}} For compilation of specific applications, there are studies improving compilation for approximate computing or DNN applications.\nConsidering compilation for approximate computing, Esmaeilzadeh \\textit{et al.}  propose a program transformation method, which trains MLPs to mimic regions of approximable code and eventually replaces the original code with trained MLPs. The following work  extends this algorithmic transformation to GPUs.\nConsidering compilation for DNNs, RELEASE  utilizes PPO to search optimal compilation configurations for DNNs.\nEGRL  optimizes memory placement of DNN tensors during compilation, which combines GNNs, RL, and evolutionary search to figure out optimal mapping onto different on-board memory components (i.e., SRAM, LLC, and DRAM).", "cites": [5402, 5399, 5411], "cite_extract_rate": 0.25, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple ML-based compiler optimization approaches, connecting them to common design challenges like instruction scheduling, heuristics, and path profiling. It provides some abstraction by categorizing these techniques under broader compiler tasks, but lacks a deeper critical evaluation or comparison of the methods' effectiveness or limitations. The narrative is coherent and analytical, but not sufficiently nuanced to reach high insight."}}
{"id": "4b384d57-c0f5-44ec-ae75-2d05825729b5", "title": "Analog Design", "level": "subsubsection", "subsections": [], "parent_id": "e25c32d8-64dc-4364-8501-8063014c67c6", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "ML as Design Methodology"], ["subsection", "Chip Design and Design Automation"], ["subsubsection", "Analog Design"]], "content": "\\label{sec:dse_analog}\nCompared with the highly automated digital design counterpart, analog design usually demands many manual efforts and domain expertise.\nFirst, analog circuits have large design spaces to search proper topology and device sizes. Second, there is an absence of a general framework to optimize or evaluate analog designs, and design specifications often vary case by case.\nRecently, ML techniques have been introduced to expedite analog design automation.\nWe discuss these studies following the top-down flow of analog design: in the circuit level, a proper circuit topology is selected to satisfy system specifications; then in the device level, device sizes are optimized subject to various objectives. These two steps compose of pre-layout designs.\nAfter circuit schematics are carefully designed, analog layouts in the physical level are generated.\n\\textcircled{\\small{1}} \nIn the circuit level, there is an attempt towards automatic circuit generation currently targeting two-port linear analog circuits .\nThe design specifications are encoded by a hypernetwork  to generate weights for an RNN model, which is trained to select circuit components and their configurations.\n\\textcircled{\\small{2}} \nIn the device level, the combination of RL and GNNs enables automatic transistor sizing , which is able to generalize across different circuit topologies or different technology nodes.\nAutoCkt  introduces transfer learning techniques into deep RL for automatic sizing, achieving 40$\\times$ speedup over a traditional genetic algorithm.\nRosa \\textit{et al.}  provide comprehensive discussions of how to address automatic sizing and layout of analog ICs via deep learning and ANNs.\n\\textcircled{\\small{3}}\nIn the physical level, GeniusRoute  automates analog routing through the guidance learned by a generative neural network.\nThe analog placements and routing are represented as images to pass through a variational autoencoder (VAE)  to learn routing likelihoods of each region. GeniusRoute achieves competitive performance to manual layouts and is capable to generalize to circuits of different functionality.\nLiu \\textit{et al.}  apply multi-objective Bayesian optimization to optimize combinations of net weighting parameters, which could significantly change floor plans and placement solutions, so as to improve analog layouts of building block circuits.\n\\begin{table}[tbp]\n\\vspace{-12pt}\n\\caption{Summary of applying ML techniques as the design methodology for design automation.}\n\\vspace{-10pt}\n\\label{table:dse_eda}\n\\centering\n    \\tiny\n    \\renewcommand{\\arraystretch}{0.9}\n    \\setlength{\\tabcolsep}{12pt}\n\\begin{tabular}{c|c|c|c}\n\\toprule\n\\multicolumn{2}{c|}{\\textbf{Domain}} & \\textbf{Task} & \\textbf{Technique} \\\\ \\midrule\n\\multirow{4}{*}{\\textbf{\\begin{tabular}[c]{@{}c@{}}Analog \\\\ Design \\\\ $( \\S$ \\ref{sec:dse_analog}$)$ \\end{tabular}}} & \\textbf{Circuit Level} & Generating circuit topology & RNN and hypernetwork    \\\\ \\cline{2-4} \n & \\textbf{Device   Level} & Device sizing & Actor critic , ANN  \\\\ \\cline{2-4} \n & \\multirow{2}{*}{\\textbf{Physical   Level}} & Routing & VAE  \\\\ \\cline{3-4} \n &  & Optimizing   layout configurations & Multi-objective Bayesian   optimization  \\\\ \\cline{2-4} \n \\hline\n\\multirow{12}{*}{\\textbf{\\begin{tabular}[c]{@{}c@{}}Digital \\\\ Design \\\\ $( \\S$ \\ref{sec:dse_digital}$)$\\end{tabular}}} & \\multirow{3}{*}{\\textbf{HLS}} & Optimizing loop unrolling pragma & Random forest    \\\\ \\cline{3-4} \n &  & Optimizing  placement of multiple pragmas & Bayesian optimization    \\\\ \\cline{3-4} \n &  & Optimizing   resource pragma & Actor-critic and GNN    \\\\ \\cline{2-4} \n & \\multirow{3}{*}{\\textbf{\\begin{tabular}[c]{@{}c@{}}Logic \\\\ Synthesis\\end{tabular}}} & Selecting proper optimizers & MLP  \\\\ \\cline{3-4} \n &  & Logic   optimization & Policy gradient   , actor-critic  \\\\ \\cline{3-4} \n &  & Determining the maximum error of each node & Q-learning    \\\\ \\cline{2-4} \n & \\multirow{5}{*}{\\textbf{\\begin{tabular}[c]{@{}c@{}}Physical \\\\ Synthesis\\end{tabular}}} & Optimizing flip-flop placement in clock networks & K-means clustering    \\\\ \\cline{3-4} \n &  & Optimizing   clock tree synthesis & Conditional GAN  \\\\ \\cline{3-4} \n &  & Optimizing   memory cell placement & PPO  \\\\ \\cline{3-4} \n &  & Optimizing   standard cell placement & Cast as an NN training problem    \\\\ \\cline{3-4} \n &  & Fix design rule violations & PPO  \\\\ \\bottomrule\n\\end{tabular}\n\\vspace{-12pt}\n\\end{table}", "cites": [5414, 727, 5412, 5413, 5415, 166], "cite_extract_rate": 0.3, "origin_cites_number": 20, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a structured description of how ML is applied to analog design across different levels, but it lacks deeper synthesis or analysis of the relationships between the cited works. It summarizes techniques and results without evaluating their effectiveness or limitations. There is minimal abstraction beyond the specific methods, making it more of a descriptive overview than a critical or analytical discussion."}}
{"id": "75c9f4ed-ccff-4fa2-af01-38a78105e680", "title": "Digital Design", "level": "subsubsection", "subsections": [], "parent_id": "e25c32d8-64dc-4364-8501-8063014c67c6", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "ML as Design Methodology"], ["subsection", "Chip Design and Design Automation"], ["subsubsection", "Digital Design"]], "content": "\\label{sec:dse_digital}\nFor the studies applying ML techniques to directly optimize digital designs, we organize them following a top-down flow, i.e., HLS, logic synthesis, and physical synthesis.\nThe design space exploration in HLS designs usually relates to properly assigning directives (pragmas) in high-level source code, since directives significantly impact the quality of HLS designs by controlling parallelism, scheduling, and resource usage.\nThe optimization goal is often to find Pareto solutions between different objectives or to satisfy pre-defined constraints. \nWith IR analysis, the employment of a random forest is able to select suitable loop unrolling factors to optimize a weighted sum of execution latency and resource usage .\nProspector  uses Bayesian optimization to optimize placement of directives (loop unrolling/pipelining, array partitioning, function inlining, and allocation), aiming to find Pareto solutions between execution latency and resource utilization in FPGAs. \nIronMan  targets Pareto solutions between different resources while keeping the latency unchanged.\nIt combines GNNs with RL to conduct a finer-grained design exploration in the operation level, pursuing optimal resource allocation strategies by optimizing assignments of the resource pragma.\nIn logic synthesis, RTL-designs or logic networks are represented by directed acyclic graphs (DAGs). The goal is to optimize logic networks subject to certain constraints.\nLSOracle  employs an MLP to automatically decide which one of the two optimizers should be applied on different parts of circuits.\nThe logic optimization can be formulated as an RL problem solved by the policy gradient  or the advantage actor-critic : the state is the current status of a design; the action is a transformation between two DAGs with equivalent I/O behaviors; the optimization objective is to minimize area or delay of designs.\nQ-ALS  aims at approximate logic synthesis and embeds a Q-learning agent to determine the maximum tolerable error of each node in a DAG, such that the total error rates at primary outputs are bounded by pre-specified constraints.\nIn physical synthesis, placement optimization is a popular topic.\n\\textcircled{\\small{1}} \nTo optimize flip-flop placement in clock networks, Wu \\textit{et al.}  apply a modified K-means clustering to group post-placement flip-flops, and relocate these clusters by reducing the distance between flip-flops and their drivers while minimizing disruption of original placement results. \nTo optimize clock tree synthesis (CTS), Lu \\textit{et al.}  train a regression model that takes pre-CTS placement images and CTS configurations as inputs to predict post-CTS metrics (clock power, clock wirelength, and maximum skew), which is used as the supervisor to guide the training of a conditional GAN, such that the well-trained generator can recommend CTS configurations leading to optimized clock trees. \n\\textcircled{\\small{2}} \nAiming at cell placement, a deep RL approach  is introduced to place macros (memory cells), after which standard cells are placed by a force-directed method. \nThis method is able to generalize to unseen netlists, and outperforms RePlAce  yet several times slower.\nDREAMPlace  casts the analytical standard cell placement optimization into a neural network training problem, achieving over 30$\\times$ speedup without quality degradation compared to RePlAce.\nNVCell  is an automated layout generator for standard cells, which employs RL to fix DRVs after placement and routing.\n\\textcircled{\\small{3}} \nML-based techniques demonstrates their versatility in many design automation tasks, such as post-silicon variation extraction by sparse Bayesian learning, and post-silicon timing tuning to mitigate the effects caused by process variation .", "cites": [5414, 5415, 5413], "cite_extract_rate": 0.21428571428571427, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the cited works by organizing them along the top-down digital design flow and highlighting common themes such as Pareto optimization and the use of ML techniques like GNNs and RL. It also provides some critical evaluation, such as noting the trade-off between performance and runtime in placement optimization. The abstraction is strong, as the section generalizes ML applications across HLS, logic synthesis, and physical synthesis, identifying broader patterns like the versatility of ML in design automation."}}
{"id": "4da21a0b-f62c-4e97-84e3-786911c6b5c8", "title": "Bridging Data Gaps", "level": "subsection", "subsections": [], "parent_id": "a80b2264-c6ef-4ec9-ae9a-838d036e281c", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "Discussion and Potential Directions"], ["subsection", "Bridging Data Gaps"]], "content": "Data are the backbone to ML, however, perfect datasets are sometimes non-available or prohibitively expensive to obtain in computer architecture and system domain.\nHere, we would like to scrutinize two points, the gap between small data and big data, and non-perfect data.\n\\textcircled{\\small{1}} \nIn some EDA problems, such as placement and routing in physical synthesis, the simulation or evaluation is extremely expensive , leading to data scarcity. As ML models usually require enough data to learn underlying statistics and make decisions, this gap between small data and big data often limits the capability of ML-based techniques. There have been different attempts to bridge this gap. From the algorithm side, algorithms that can work with small data await to be developed, where one current technique is Bayesian optimization that is effective in small parameter space ; active learning , which significantly improve sample efficiency, may also be a cure to this problem. From the data side, generative methods can be used to generate synthetic data , mitigating data scarcity.\n\\textcircled{\\small{2}} \nRegarding non-perfect data, even if some EDA tools produce a lot of data, they are not always properly labeled nor presented in the form suitable to ML models. In the absence of perfectly labeled training data, possible alternatives are to use unsupervised learning, self-supervised learning , or to combine supervised with unsupervised techniques . Meanwhile, RL could be a workaround where training data can be generated on-the-fly.\n\\vspace{-5pt}", "cites": [5416], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the concept of data limitations in ML for computer architecture and systems, integrating the idea of using self-supervised learning from the cited paper with other techniques like Bayesian optimization and active learning. While it offers some abstraction by framing the discussion around broader challenges (small data and non-perfect data), the critical analysis is limited to stating alternatives without deep evaluation of their effectiveness or limitations. The narrative is coherent but lacks a novel framework or extensive cross-source connections."}}
{"id": "bf55ec88-5f58-456f-b066-c5e92f556071", "title": "Developing Algorithms", "level": "subsection", "subsections": [], "parent_id": "a80b2264-c6ef-4ec9-ae9a-838d036e281c", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "Discussion and Potential Directions"], ["subsection", "Developing Algorithms"]], "content": "Despite the current achieved accomplishments, we are still expecting novel ML algorithms or schemes to further improve system modeling and optimization, with respect scalability,  domain knowledge interpretability, etc.\n\\textbf{New ML Schemes.}\nClassical analytic-based methods usually adopt a bottom-up or top-down procedure, encouraging ML-based techniques to distill hierarchical structures of systems/architecture.\nOne example is hierarchical RL  that has flexible goal specifications and learns goal-directed behaviors in complex environments with sparse feedback. Such kind of models enables more flexible and effective multi-level design and control.\nAdditionally, many system optimizations involve participation of multiple agents, such as NoC routing, which are naturally suitable to the realm of multi-agent RL . These agents can be fully cooperative, fully competitive, or a mix of the two, enabling versatility of system optimization.\nAnother promising approach is self-supervised learning , beneficial in both improving model robustness and mitigating data scarcity.\nWhile applying a single ML method solely has led to powerful results, hybrid methods, i.e., combining different ML techniques or combining ML techniques with heuristics, unleash more opportunities. For example, RL can be combined with genetic algorithms for hardware resource assignment .\n\\textbf{Scalability.}\nThe system scaling-up poses challenges on scalability issues.\nFrom the algorithm side, multi-level techniques can help reduce the computation complexity, e.g., multi-level Q-learning for DVFS .\nOne implicit workaround is to leverage transfer learning:\nthe pre-training is a one-time cost, which can be amortized in each future use; the fine-tuning provides flexibility between a quick solution from the pre-trained model and a longer yet better one for a particular task. \nSeveral examples  are discussed in Section \\ref{chip_design}.\n\\textbf{Domain Knowledge and Interpretability.}\nMaking better use of domain knowledge unveils possibilities to choose more proper models for different system problems and provide more intuitions or explanations of why and how these models work.\nBy making analogy of semantics between memory access patterns/program languages and natural languages, the prefetching or code generation problems can be modeled as NLP problems, as discussed in Section \\ref{cache} and Section \\ref{code_generation}. \nBy making analogy of graphical representations in many EDA problems, where data are intrinsically presented as graphs (e.g., circuits, logic netlists or IRs), GNNs are expected to be powerful in these fields . \nSeveral examples are provided in Section \\ref{sec:model_eda} and Section \\ref{chip_design}.", "cites": [5202, 5413, 5416, 5407, 3588], "cite_extract_rate": 0.45454545454545453, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple cited papers to present a coherent narrative on how novel ML algorithms and schemes can enhance system design, particularly through hierarchical and multi-agent RL, self-supervised learning, and hybrid methods. It offers abstraction by identifying broader patterns, such as the use of multi-level techniques for scalability and the integration of domain knowledge via analogies to NLP and GNNs. While it does point to potential benefits and limitations of approaches, deeper comparative or evaluative analysis across the cited works is somewhat limited."}}
{"id": "ac2615bc-0a08-4fa7-932a-166c6e497c28", "title": "Improving Implementations and Deployments", "level": "subsection", "subsections": [], "parent_id": "a80b2264-c6ef-4ec9-ae9a-838d036e281c", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "Discussion and Potential Directions"], ["subsection", "Improving Implementations and Deployments"]], "content": "To fully benefit from ML-based methods, we need to consider practical implementations, appropriate selection of deployment scenarios, and post-deployment model maintenance.\n\\textbf{Better Implementations.}\nTo enable practical implementations of ML-based techniques, improvement can be made from either the model side or software/hardware co-design .\nFrom the model level, network pruning and model compression reduce the number of operations and model size ; weight quantization improves computation efficiency by reducing the precision of operations/operands .\nFrom the co-design level, strategies that have been used for DNN acceleration could also be used in applying ML for system.\n\\textbf{Appropriate Scenarios: online vs. offline.}\nWhen deploying ML-based techniques for system designs, it is crucial to deliberate design constraints under different scenarios.\nGenerally, existing work falls into two categories. \n\\textcircled{\\small{1}} \nML-based techniques are deployed online or during runtime, no matter the training phase is performed online or offline. Obviously, the model complexity and runtime overhead are often strictly limited by specific constraints, e.g., power/energy, timing/latency, area, etc. \nTo take one more step, if the online training/learning is further desired, the design constraint will be more stringent. One promising approach is to employ semi-online learning models, which have been applied to solve some classical combinatorial optimization problems, such as bipartite matching  and caching . These models enable smooth interpolation between the best possible online and offline training algorithms.\n\\textcircled{\\small{2}} ML-based techniques are applied offline, which often refers to architectural design space exploration. Such problems leverage ML-based techniques to guide system implementation, and once the designing phase is completed, ML models will not be invoked again. Thus, the offline applications can tolerate relatively higher overheads.\n\\textbf{Model Maintenance.}\nIn the case of offline training and online deployment, ML models employed for computer architecture domain, as in other scenarios, require regular maintenance and updating to meet performance expectations,\nsince workload variations over time and hardware aging often cause data drift or concept drift .\nTo proactively circumvent performance degradation of ML models, some measures could be taken during post-deployment periods.\n\\textcircled{\\small{1}}\nML models can be retrained either at a regular interval or when key performance indicators are below certain thresholds. Retraining models regularly, regardless of their performance, is a more direct way, but it requires a clear understanding of how frequently a model should be updated under its own scenario. The model performance will decline if retraining intervals are too spaced out in the interim. Monitoring key performance indicators relies on a comprehensive panel of measurements that explicitly demonstrate model drift, whereas this may introduce additional hardware/software overhead and incorrect selection of measurements often defeats the intention of this method.\n\\textcircled{\\small{2}} During the retraining of ML models, there is often a trade-off between newly collected data and previous data. Properly assigning importance of input data would improve retraining efficacy .\n\\vspace{-5pt}", "cites": [5418, 7633, 9139, 5417], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes relevant techniques from the cited papers to discuss practical aspects of ML implementation in computer architecture and systems. It abstracts the ideas into categories like 'online vs. offline' deployment and 'model maintenance', but the analysis remains focused on established methods rather than introducing a novel framework. Some critical discussion is present, particularly around trade-offs and limitations of retraining strategies, but it could be more thorough."}}
{"id": "56d8fa9d-3bba-4c5d-9a9d-e6d1744157e3", "title": "Supporting Non-homogeneous Tasks", "level": "subsection", "subsections": [], "parent_id": "a80b2264-c6ef-4ec9-ae9a-838d036e281c", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "Discussion and Potential Directions"], ["subsection", "Supporting Non-homogeneous Tasks"]], "content": "ML-based techniques are supposed to be applicable in both current architectures and emerging systems, leading to long-term advancement in computer architecture and systems.\n\\textbf{Non-homogeneous Components.}\nDesign and development for computer architectures are often based upon earlier-generation architectures of similar purpose, but commonly rely on next-generation hardware components that were not present in earlier generations. Examples include employment of new device nodes with technology scaling, and replacement of conventional constituents in memory systems with NVM- or PIM-based components.\nIn addition to the heterogeneity of components from different generations, one architecture or system usually consists of both standard parts from library and specialized/customized hardware components.\nThis provides the motivation that ML-assisted architectures/systems should have the flexibility to transfer among different-generation components, and to support standard and specialized parts simultaneously.\n\\textbf{Non-homogeneous Applications.}\nIn computer architecture and system design, some issues are universal, while others may arise with the advent of new architecture/systems and new workloads.\n\\textcircled{\\small{1}} For evergreen design areas, several examples include caching in hardware/software/data centers (Section \\ref{cache} and Section \\ref{data center}), resource management and task allocation in single/multi-/many-core CPUs and heterogeneous systems (Section \\ref{resource}), NoC design under various scenarios (Section \\ref{NoC}), etc.\n\\textcircled{\\small{2}} For problems aroused from new systems/workloads, transfer learning and meta-learning  could be helpful in either exploring new heuristics or directly deriving design methodology. \nFor example, combining meta-learning with RL  allows training a \"meta\" agent that is designed to adapt to a specific workload with only a few observations.\n\\vspace{-5pt}", "cites": [1695, 7739, 7612], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes ideas from multiple meta-learning papers to discuss how ML can address non-homogeneous components and applications in architecture/system design, suggesting a coherent analytical direction. While it does offer some abstraction by identifying the need for adaptability in ML-assisted systems, it lacks deeper critical evaluation of the cited works, such as limitations or trade-offs in applying meta-learning techniques. The analysis is moderate and contributes to understanding the broader applicability of ML methods."}}
{"id": "5afb9503-6a23-454b-90e9-dd19be81420a", "title": "Facilitating General Tool Design and Hardware Agile Development", "level": "subsection", "subsections": [], "parent_id": "a80b2264-c6ef-4ec9-ae9a-838d036e281c", "prefix_titles": [["title", "A Survey of Machine Learning for Computer Architecture and Systems"], ["section", "Discussion and Potential Directions"], ["subsection", "Facilitating General Tool Design and Hardware Agile Development"]], "content": "Even though ML-based modeling significantly reduces the evaluation cost during design iteration, making great strides towards the landing of hardware agile development, there is still a long way to go in the ML-based design methodology prospective.\nOne ultimate goal might be the fully automated design, which should entangle two core capabilities: holistic optimization in system-wise, and easy migration across different systems, to enable rapid and agile hardware design.\n\\textbf{Holistic Optimization.}\nFueled by recent advancements, ML techniques have been increasingly explored and exploited in computer system design and optimization . \nThe target problems that await further endeavors could be multi-objective optimizations under highly constrained situations, or optimizing several components in a system simultaneously.\nWe envisage an ML-based system-wise and holistic framework with a panoramic vision: it should be able to leverage information from different levels of systems in synergy, so that it could thoroughly characterize system behaviors as well as their intrinsically hierarchical abstractions; it should also be able to make decisions in different granularity, so that it could control and improve systems precisely and comprehensively.\n\\textbf{Portable, Rapid, and Agile.}\nStriving for portable, rapid, and agile hardware design, there are two potential directions.\n\\textcircled{\\small{1}} The well-designed interfaces between systems/architectures and ML-based techniques would facilitate the portability across different platforms, since ML models can perform well without explicit descriptions of the target domain.\n\\textcircled{\\small{2}} The proliferation of ML-based techniques have more or less transformed the workflow of design automation, directly driving rapid and agile hardware design.\nWe expect GNNs make better use of naturally graphical data in EDA field; we expect deep RL be a powerful and general-purpose tool for many EDA optimization problems, especially when the exact heuristic or objective is obscure; we expect these ML-based design automation tools enhance designers' productivity and thrive in the community.\n\\vspace{-5pt}", "cites": [4334], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "medium", "analysis": "The section provides a forward-looking analytical perspective on the potential of ML in enabling agile hardware design and general tool development. It synthesizes the concept of ML as a design methodology from the cited paper but does not deeply integrate multiple sources. It offers some critical observations about the current limitations and future goals, such as the need for holistic optimization and portability. The abstraction level is strong as it outlines a vision for a system-wise ML framework and identifies overarching patterns in the application of GNNs and deep RL."}}
