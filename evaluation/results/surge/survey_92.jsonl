{"id": "a96dd0ad-2f86-4f16-b5a9-882602bf6a06", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "4e5af7af-552b-4c7f-939b-54b6b4dadeb9", "prefix_titles": [["title", "A Survey of Knowledge Tracing: Models, Variants, and Applications"], ["section", "Introduction"]], "content": "\\IEEEPARstart{W}{ith} the proliferation of the Internet and mobile communication technology, online education has become increasingly popular and is now developing at an unprecedented scale .\n{This innovative style of learning provides a degree of flexibility that conventional education cannot match, which enables teaching and learning to occur at any time and any place.}\nMeanwhile, online learning systems (e.g., Coursera, ASSISTment)  have proven to be even more effective than traditional learning styles, since they can offer more intelligent educational services, such as recommending individualized learning resources to students . \nTo provide these intelligent services, online learning systems continuously record a massive amount of available data about student-system interactions {(e.g., responding to exercises)}, which can be further mined to assess their knowledge levels, learning preferences, and other attributes. Specifically, Knowledge Tracing (KT)  is one of the most fundamental and critical tasks for analyzing students' learning behavior data, which aims to explore the recorded student-system interactions to monitor their evolving knowledge states .\n\\begin{figure*}[t]\n\t\\vspace{-0.5cm}\n\t\\centerline{\\includegraphics[width=\\textwidth]{Figures/learning_case.pdf}}\n\t\\vspace{-0.2cm}\n\t\\caption{A simple schematic diagram of knowledge tracing. Different knowledge concepts are represented in different colors, while exercises are also depicted in the  color relevant to the knowledge concepts. During the learning process, different kinds of side information are also recorded. The evolving process of the knowledge state is assessed by KT models and illustrated by the radar maps.}\n\t\\label{kt}\n\t\\vspace{-0.5cm}\n\\end{figure*}\nFig. \\ref{kt} presents a simple schematic diagram of knowledge tracing. During the learning process, online learning systems continuously record students' learning behavioral data, including exercises and their related knowledge concepts (e.g., \\emph{equality, inequality, plane vector}, \\emph{probability}, represented in various colors), and students' answers (i.e., correct or incorrect responses).  A substantial amount of supplementary information is also simultaneously recorded, including response time, opportunity count, and tutor intervention, which provides a more comprehensive reflection of students' learning process.  \nBased on the collected learning data, researchers are striving to maintain an estimate of students' evolving knowledge states. {For illustration, we give a case in  Fig. \\ref{kt}, where the student's prior knowledge is quantified as 0.2, 0.4, 0.4, and 0.5 across four distinct knowledge concepts. The radar map serves as a visual representation of the student's knowledge mastery, which progressively expands as the student continues to acquire new knowledge in learning.} After a period of learning, the student's knowledge states reach 0.9, 0.8, 0.8, and 0.7 respectively, suggesting good knowledge growth. In the aforementioned learning process, KT models aim to monitor changes in students' knowledge states. Once we understand students' knowledge states, the learning system can customize more suitable learning schemes for different students, thereby enabling the teaching of students in accordance with their proficiency. It also allows students to better comprehend their learning process and gradually focus on improving their skills with poorly mastered concepts  .\nKnowledge tracing has been studied for decades, with the first studies tracing back to the late 1970s. These initial works primarily focused on confirming the effectiveness of mastery learning . To the best of our knowledge, ~ were the first to introduce the concept of knowledge tracing, employing Bayesian networks to model the student learning process, which they referred to as Bayesian Knowledge Tracing. Since then, the significance of KT has been recognized by a broader spectrum of researchers, and increasing attention has been directed towards KT-related research. Many logistic models have been applied to KT, including Learning Factor Analysis  and Performance Factor Analysis . In recent years, deep learning has greatly enhanced research into the KT task, largely due to its capacity to extract and represent features and discover intricate structure. For instance, Deep Knowledge Tracing introduced recurrent neural networks (RNNs)  into the KT task and was found to significantly outperform previous methods . Following this, various methods have been introduced that employ various types of neural networks to the KT task, considering various characteristics of the learning sequence . Moreover, due to the requirements of practical applications, many variants of KT models have been continuously developed, and KT has already been broadly applied in numerous educational scenarios. \nWhile novel KT models continue to emerge, there remains a lack of comprehensive surveys exploring this young research field, particularly {regarding its} numerous variants and applications. To this end, {the current survey aims to systematically review the development of KT}. As depicted in Figure \\ref{tax}, we initially categorize existing KT models from a technical perspective, which is consistent with the majority of existing surveys . This categorization splits them into three categories: (1) Bayesian models, (2) logistic models, and (3) deep learning models. In each category, we further organize specific KT methods according to their various techniques. \nSubsequently, we introduce extensive variants of these fundamental KT models, which consider more stringent assumptions about more complete learning process in different learning phases. In addition, we present several typical applications of KT in real learning scenarios. Due to the complexity of different KT models, we have open sourced two algorithm libraries to better aid researchers and practitioners in implementing KT models and facilitate community development in this domain. These libraries, EduData\\footnote{https://github.com/bigdata-ustc/EduData} and EduKTM\\footnote{https://github.com/bigdata-ustc/EduKTM}, include most existing KT-related datasets, extensible, and unified implementations of existing KT models, and relevant resources. Finally, we discuss potential future research directions. In summary, this paper presents an extensive survey of KT that can serve as a comprehensive guide for both researchers and practitioners. \nThe remainder of this survey is structured as follows. Section \\ref{sec:overview} presents an overview of the KT task and we discuss the differences between this and previous surveys. Section \\ref{sec:models} provides a review of the three categories of fundamental KT models. \nSection \\ref{sec:variants} describes the variants of fundamental KT models. \nSection \\ref{sec:applications} introduces the extensive applications of KT in different scenarios. Section \\ref{sec:dataset} gives the summary of existing datasets for evaluating KT models and details of the algorithm libraries we have released. Section \\ref{sec:future} discusses some potential future research directions. Finally, section \\ref{sec:conclution} summarizes the paper.\n\\begin{figure*}[t]\n\t\\vspace{-0.2cm}\n\t\\centerline{\\includegraphics[width=\\textwidth]{Figures/taxonomy.pdf}}\n\t\\vspace{-0.2cm}\n\t\\caption{{An overview of knowledge tracing models.} }\n\t\\label{tax}\n\t\\vspace{-0.2cm}\n\\end{figure*}", "cites": [5371, 5372, 5370, 5374, 5373], "cite_extract_rate": 0.21739130434782608, "origin_cites_number": 23, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a general overview of knowledge tracing and mentions several cited papers, but it does not deeply synthesize their contributions or compare them in a meaningful way. While it introduces a taxonomy of KT models and hints at trends (e.g., deep learning's impact), it lacks critical evaluation of the models' strengths or weaknesses and does not offer meta-level insights or a novel framework."}}
{"id": "0c71baa7-2bd1-4dbb-a83e-a14394274744", "title": "Problem Definition", "level": "subsection", "subsections": [], "parent_id": "6987e516-0520-40ba-bdfa-9ecf587cfc88", "prefix_titles": [["title", "A Survey of Knowledge Tracing: Models, Variants, and Applications"], ["section", "Overview"], ["subsection", "Problem Definition"]], "content": "In an online learning system, supposing there exists a set of students $\\mathbb{S}$ and a set of exercises $\\mathbb{E}$. {Each exercise is related to specific Knowledge Concepts (KCs). Generally, the name given to the knowledge related to exercises differs across online learning platforms. For instance, it is named \\emph{skill} in ASSISTments . To promote better understanding, we refer to these uniformly as knowledge concepts throughout this paper, and denote the set of all KCs as $\\mathbb{KC}$. Moreover,  $M$ and $K$ are respectively used to represent the total number of exercises and KCs. Students are asked to answer different exercises in order to achieve mastery of the related knowledge.}\nTherefore, the learning sequence of a student can be formulated as $\\bm{X}  = \\{([e_1, k_{e_1}], a_1, r_1), ([e_2, k_{e_2}], a_2, r_2), ..., ([e_t, k_{e_t}], a_t, r_t), ..., \\\\([e_N, k_{e_N}], a_N, r_N)\\}$, where the tuple $([e_t, k_{e_t}], a_t, r_t)$ represents the learning interaction at the $t-$th time step, $e_t$ represents the exercise,  $k_{e_t}$ represents the exercise's related KCs,  $a_t$ represents the correctness label (i.e., with 1 for correct and 0 for incorrect answers),  $r_t$ stands for the side information recorded in this learning interaction, and $N$ is the length of the learning sequence. The research problem of knowledge tracing can thus be defined as follows:\n\\textbf{Given sequences of learning interactions in online learning systems, knowledge tracing aims to monitor students' evolving knowledge states during the learning process and predict their performance on future exercises. The measured knowledge states can be further applied to individualize students' learning schemes in order to maximize their learning efficiency.}\nSome recent works directly regarded the KT task as student performance prediction, without considering students' knowledge states . We agree that predicting student performance is of great significance, as it is now {the best way} to evaluate the quality of the knowledge state traced by KT models.  However, we have to point out that KT focuses more on students' knowledge states, especially their interpretability and rationality, which is related to the students' acceptance of the conclusions given based on the KT model .", "cites": [5376, 5375, 38], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear problem definition and integrates ideas from the cited papers by highlighting the role of attention mechanisms in KT models and their limitations. It moves beyond mere description by emphasizing the importance of interpretability and rationality in knowledge state modeling. However, the synthesis and abstraction are somewhat constrained, as the section does not fully develop a novel framework or deep meta-level insights from the cited works."}}
{"id": "1f61ba5e-8077-4921-924c-62e9bddd3afc", "title": "Differences between this and previous surveys", "level": "subsection", "subsections": [], "parent_id": "6987e516-0520-40ba-bdfa-9ecf587cfc88", "prefix_titles": [["title", "A Survey of Knowledge Tracing: Models, Variants, and Applications"], ["section", "Overview"], ["subsection", "Differences between this and previous surveys"]], "content": "Given the increasing importance of KT, several recent surveys have also examined this area. These include works by , . Here, we will briefly discuss the key distinctions between these studies to highlight the necessity and significance of this survey. \nExisting surveys have either focused on specific categories of KT models or comprehensively reviewed all available KT models.  {For example,  provided an overview of KT in terms of Bayesian models and logistic models,  compared and discussed deep learning based KT models, while  presented to pay more attention to hybird models in KT.}  conducted a bibliometric analysis to examine the evolution of KT research from 1992 to 2021.  also presented a comprehensive survey for the KT literature, including a broad range of methods starting from the early attempts to the recent state-of-the-art techniques utilizing deep learning.  summarized KT methods in the context of student performance modeling problems. \nHowever, current surveys are somewhat limited in their scope, they only provide a detailed introduction to various KT methods and comparisons between them. Given the complexity of online learning systems and the significant importance of KT research in practical applications, this survey places a greater emphasis on the variants and applications of KT models, rather than solely introducing and comparing different KT methods. Moreover, considering that datasets are collected from different systems with various setting, subjects, learning stages, and scales,  \nwe do not report and compare the performance of KT models on the student performance prediction task across various datasets in this survey.   have also empirically verified that no single KT model was always the best, a specific better model must consider multiple student features and the learning context.\nInstead, we have open sourced two algorithm libraries which include the majority of  existing KT-related datasets and unified implementations of existing KT models. Consequently, researchers and practitioners can freely select appropriate KT models based on their specific requirements in various application scenarios. \n\\begin{table*}[t]\n\t\\vspace{-0.6cm}\n\t\\centering\n\t\\renewcommand\\arraystretch{1.0}\n\t\\caption{A summary of different types of fundamental knowledge tracing models.}\n\t\\vspace{-0.3cm}\n\t\\resizebox{\\textwidth}{!}{\n\t\t\\begin{tabular}{|c|c|c|c|c|}\n\t\t\t\\hline\n\t\t\t\\multicolumn{1}{|c|}{Category} & Typical approach & Technique & KC relationship & Knowledge state \\bigstrut\\\\\n\t\t\t\\hline\n\t\t\t\\multicolumn{1}{|c|}{\\multirow{2}[5]{*}{Bayesian models}} & Bayesian knowledge tracing  & Bayesian networks & independent & \\multirow{2}[5]{*}{\\shortstack{unobservable node \\\\ in HMM}} \\bigstrut\\\\\n\t\t\t\\cline{2-4}          & dynamic Bayesian knowledge tracing  & dynamic Bayesian networks & pre-defined & \\multicolumn{1}{c|}{} \\bigstrut\\\\\n\t\t\t\\hline\n\t\t\t\\multicolumn{1}{|c|}{\\multirow{3}[8]{*}{Logistic models}} & learning factor analysis  & \\multirow{2}[4]{*}{logistic regression} & \\multirow{3}[8]{*}{independent} & \\multirow{3}[6]{*}{\\shortstack{the output of \\\\ logistic regression function}} \\bigstrut\\\\\n\t\t\t\\cline{2-2}          & performance factor analysis  & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} \\bigstrut\\\\\n\t\t\t\\cline{2-3}          & knowledge tracing machines & factorization machines & \\multicolumn{1}{c|}{} & \\multicolumn{1}{c|}{} \\bigstrut\\\\\n\t\t\t\\hline\n\t\t\t\\multicolumn{1}{|c|}{\\multirow{4}[6]{*}{\\shortstack{Deep learning \\\\ models}}} & deep knowledge tracing  & RNN/LSTM & discover automatically & the hidden state \\bigstrut\\\\\n\t\t\t\\cline{2-5}          & memory-aware knowledge tracing  & memory networks & correlation weights & \\emph{value} matrix \\bigstrut\\\\\n\t\t\t\\cline{2-5}          & attentive knowledge tracing  & self-attention mechanism & attention weights &  attentive historical knowledge state \\\\\n\t\t\t\\cline{2-5}          & graph-based knowledge tracing   & graph neural networks & edges in graph & aggregate in the graph \\bigstrut\\\\\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t}\n\t\\vspace{-0.6cm}\n\t\\label{tab1}\n\\end{table*}", "cites": [5372, 5377, 5374, 5370, 8911, 5371, 1206], "cite_extract_rate": 0.4117647058823529, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple cited papers to categorize and compare the focus of previous KT surveys, helping to establish a narrative about the evolution of the field. It critically highlights the limitations of prior surveys in not emphasizing variants and applications and refraining from comparing model performance across datasets. The abstraction level is moderate as it begins to frame KT models in broader categories and discusses general trends in model development and practical relevance."}}
{"id": "d181c1c4-a973-41d5-b53b-df201574371b", "title": "Performance Factor Analysis", "level": "subsubsection", "subsections": [], "parent_id": "737cf102-cd65-4a28-91e5-d7967b691341", "prefix_titles": [["title", "A Survey of Knowledge Tracing: Models, Variants, and Applications"], ["section", "Fundamental Knowledge Tracing Models"], ["subsection", "Logistic Models"], ["subsubsection", "Performance Factor Analysis"]], "content": "The PFA model  can be seen as an extension of the LFA model that is especially sensitive to the student performance. In contrast to the LFA model, PFA  considers the following different factors:\n\\begin{itemize}\n\t\\item{Previous failures}: parameter $f$ is the prior failures for the KC of the student;\n\t\\item{Previous successes}: parameter $s$ represents the prior successes for the KC of the student;\n\t\\item{Easiness of KCs}: parameter $\\beta$ means the easiness of different KCs, which is the same as in the LFA model.\n\\end{itemize}\nThe standard PFA model takes the following form:\n\\vspace{-0.15cm}\n\\begin{equation}\n\tp(\\theta) = \\sigma(\\sum_{j \\in KCs}(\\beta_j + \\mu_js_{ij} + \\nu_jf_{ij})),\n\t\\label{pfa}\n\t\\vspace{-0.15cm}\n\\end{equation}\nwhere $\\mu$ and $\\nu$ are the coefficients for $s$ and $f$, which denote the learning rates\nfor successes and failures.\n\\begin{figure}[t]\n\t\\vspace{-0.2cm}\n\t\\centerline{\\includegraphics[width= 0.9\\columnwidth]{Figures/KTM.pdf}}\n\t\\vspace{-0.2cm}\n\t\\caption{Example of activation of a knowledge tracing machine . $V$ refers to the matrix of embeddings, $w$ refers to the vector of biases, $x$ is the encoding vector of the learning interaction.}\n\t\\label{fktm}\n\t\\vspace{-0.6cm}\n\\end{figure}", "cites": [5377], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the PFA model, its factors, and the associated equation. It integrates the PFA model with the LFA model by pointing out their differences but lacks deeper synthesis or connection to other models or broader themes. There is minimal critical analysis or abstraction beyond the specific model, focusing largely on definition and structure."}}
{"id": "609eb20d-a953-453f-b163-f5e296a7c090", "title": "Knowledge Tracing Machines", "level": "subsubsection", "subsections": [], "parent_id": "737cf102-cd65-4a28-91e5-d7967b691341", "prefix_titles": [["title", "A Survey of Knowledge Tracing: Models, Variants, and Applications"], ["section", "Fundamental Knowledge Tracing Models"], ["subsection", "Logistic Models"], ["subsubsection", "Knowledge Tracing Machines"]], "content": "The KTM model, developed by Vie et al. , employs factorization machines (FMs)  to generalize logistic models to higher dimensions. FMs were initially introduced as a general predictor capable of working with any real-valued feature vector, enabling the model to represent all interactions between variables using factorized parameters . FMs provide a means of encoding side information about exercises or students into the model. Figure \\ref{fktm} illustrates the example of KTM, which models the knowledge mastery of the student based on a sparse set of weights for all features involved in the event. Let $L$ be the number of features; here, the features can be related to students, exercises, KCs, or any other side information. The learning interaction is encoded by a sparse vector $\\bm{l}$ of length $L$. When feature $i$ is involved in the interaction, $l_i > 0$. The probability $p(\\theta)$ of the correct answer is determined by the following equations: \n\\vspace{-0.3cm}\n\\begin{equation}\n\tp(\\theta) = \\sigma(\\mu + \\sum_{i = 1}^{L}w_il_i + \\sum_{1 \\leq i < j \\leq L}l_il_j\\langle\\bm{v_i}, \\bm{v_j}\\rangle ),\n\t\\vspace{-0.2cm}\n\\end{equation}\nwhere $\\mu$ is the global bias, the feature $i$ is modeled by the bias $w_i \\in \\bm{R}$ and the embedding $\\bm{v}_i \\in \\bm{R}^d$ ($d$ is the dimension). Note that only features with $l_i > 0$ will have impacts on the predictions.", "cites": [5377], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the Knowledge Tracing Machine (KTM) model and its use of factorization machines, primarily paraphrasing the original paper's contributions. It integrates minimal context from other sources and does not compare KTM with other logistic models or critically analyze its strengths or limitations. The content is largely factual and lacks deeper abstraction or synthesis of broader concepts in knowledge tracing."}}
{"id": "0b0df85b-c041-47ba-8171-c36927847515", "title": "Deep Learning Models", "level": "subsection", "subsections": ["6bda0552-a0c8-4448-bccb-783dc9dbe658", "a1cbba76-ad36-4969-8878-16538f12140f", "b60bbb0c-acf3-4bdd-ace6-fb43306d7dda", "9d414c8f-554a-43a7-afe7-209c3c4d813e"], "parent_id": "bd30f2af-cabd-4e04-ab90-5536a2f60dde", "prefix_titles": [["title", "A Survey of Knowledge Tracing: Models, Variants, and Applications"], ["section", "Fundamental Knowledge Tracing Models"], ["subsection", "Deep Learning Models"]], "content": "\\label{deep}\nThe cognitive process can be influenced by various factors at both the macro and micro levels. It is difficult for Bayesian models or logistic models to adequately capture a cognitive process of high complexity .\nDeep learning, with its potent ability to achieve non-linearity and feature extraction, is well-suited for modeling complex learning processes, particularly when a significant amount of learning interaction data is available . {In recent years, numerous research works have been proposed on deep learning KT models, we will introduce deep learning models from four sub-categories: (1) deep knowledge tracing, (2) memory-aware knowledge tracing, (3) attentive knowledge tracing, and (4) graph-based knowledge tracing.}", "cites": [5378, 5371], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section introduces deep learning models in the context of KT but primarily describes the general suitability of deep learning for the task and lists four subcategories without detailed analysis or comparison of the cited papers. There is minimal synthesis of the cited works, and the critical evaluation is limited to a brief mention of model limitations. Some abstraction is attempted through categorization, but the section lacks deeper meta-level insights or a novel framework."}}
{"id": "6bda0552-a0c8-4448-bccb-783dc9dbe658", "title": "Deep Knowledge Tracing", "level": "subsubsection", "subsections": [], "parent_id": "0b0df85b-c041-47ba-8171-c36927847515", "prefix_titles": [["title", "A Survey of Knowledge Tracing: Models, Variants, and Applications"], ["section", "Fundamental Knowledge Tracing Models"], ["subsection", "Deep Learning Models"], ["subsubsection", "Deep Knowledge Tracing"]], "content": "Deep Knowledge Tracing (DKT) is the pioneering approach that introduces deep learning {to complete the KT task. DKT employs Recurrent Neural Networks (RNNs)  to process the input sequence of learning interactions over time,} maintaining a hidden state that implicitly contains information about the history of all past elements of the sequence. This hidden state evolves based on both the previous knowledge state and the present input learning interaction . DKT provides a high-dimensional and continuous representation of the knowledge state, enabling it to more effectively model the complex learning process. Typically, RNNs' variant, the Long Short-Term Memory (LSTM) networks , are more frequently used in the implementation of DKT, which is further strengthened by considering forgetting. \nFig. \\ref{fdkt} illustrates the process of deep knowledge tracing. In DKT, exercises are represented by their contained KCs. For datasets with different numbers of KCs, DKT applies two different methods to convert students' learning interactions  $\\bm{X} = \\{(e_1, a_1), (e_2, a_2), ..., (e_t, a_t), ..., (e_N, a_N)\\}$ into a sequence of fixed-length input vectors. More specifically, for datasets with a small number $K$ of unique KCs, $\\bm{x}_t \\in \\{0,1\\}^{2K}$ is set as a one-hot embedding, where $ \\bm{x}_t^k = 1 $ if the answer $a_t$ of the exercise with KC $k$ was correct or $\\bm{x}_t^{k + K} = 1$ if the answer was incorrect. For datasets with a large number of unique KCs, one-hot embeddings are considered too sparse. Therefore, DKT assigns each input vector $\\bm{x}_t$ to a corresponding random vector, and then uses the embedded learning sequence as the input of RNNs. A linear mapping and activation function are then applied to the output hidden states to obtain the knowledge state of students:\n\\vspace{-0.1cm}\n\\begin{equation}\n\t\\begin{aligned}\\label{dkt}\n\t\t&\\bm{h}_t = tanh(\\bm{W}_{hs}\\bm{x}_t + \\bm{W}_{hh}\\bm{h}_{t-1} + \\bm{b}_h), \\\\\n\t\t&\\bm{y}_t = \\sigma (\\bm{W}_{yh}\\bm{h}_{t} + \\bm{b}_y),\n\t\\end{aligned}\n\t\\vspace{-0.1cm}\n\\end{equation}\nwhere $tanh$ is the activation function, $\\bm{W}_{hs}$ is the input weights, $\\bm{W}_{hh}$ is the recurrent weights, $\\bm{W}_{yh}$ is the readout weights, and $\\bm{b}_h$ and $\\bm{b}_y$ are the bias terms.\n\\begin{figure}[t]\n\t\\vspace{-0.3cm}\n\t\\centerline{\\includegraphics[width=0.9\\columnwidth]{Figures/DKT_model.pdf}}\n\t\\vspace{-0.3cm}\n\t\\caption{The architecture of DKT .}\n\t\\label{fdkt}\n\t\\vspace{-0.6cm}\n\\end{figure}\nDespite demonstrating superior performance compared to Bayesian and logistic models, DKT has several inherent shortcomings. For instance, the lack of interpretability is a significant drawback. It is challenging to understand how the hidden states represent students' knowledge states, and the model cannot explicitly determine a student's knowledge mastery from the hidden state . Additionally,  identified two unreasonable phenomena in DKT that contravene common sense. These are: (1) the inability to reconstruct observed input, and (2) inconsistent predicted knowledge states across time-steps. However, despite these shortcomings, DKT remains a promising KT model .", "cites": [5378, 5379, 5371], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively synthesizes the key ideas from the cited papers on DKT, particularly highlighting its use of RNNs and LSTM for knowledge state modeling. It also critically addresses DKT’s limitations, such as lack of interpretability and identified unreasonable phenomena, providing a balanced view. However, while it generalizes some principles, it does not offer a meta-level abstraction or connect DKT to broader educational AI trends, limiting its abstraction score."}}
{"id": "a1cbba76-ad36-4969-8878-16538f12140f", "title": "Memory-aware Knowledge Tracing", "level": "subsubsection", "subsections": [], "parent_id": "0b0df85b-c041-47ba-8171-c36927847515", "prefix_titles": [["title", "A Survey of Knowledge Tracing: Models, Variants, and Applications"], ["section", "Fundamental Knowledge Tracing Models"], ["subsection", "Deep Learning Models"], ["subsubsection", "Memory-aware Knowledge Tracing"]], "content": "To enhance the interpretability of DKT, memory-aware knowledge tracing introduces an external memory module, as proposed by . This module is designed to {store the and update the corresponding knowledge mastery of the student.} The most representative example is Dynamic Key-Value Memory Networks (DKVMN) for knowledge tracing, as proposed by . DKVMN highlights students' specific knowledge states on various knowledge categories. It initializes a static matrix, referred to as a $key$ matrix to store latent KCs and a dynamic matrix, called a $value$ matrix to store and update the mastery of corresponding KCs through read and write operations over time.\nAs shown in Fig. \\ref{fdkvmn}, an embedding matrix is first defined to obtain the embedding vector $k_t$ of the exercises. A correlation weight $\\bm{w}_t$ is then obtained by taking the inner product between the exercise embedding $k_t$ and the $key$ vectors $M^k$, followed by the softmax activation:\n\\vspace{-0.2cm}\n\\begin{equation}\n\t\\bm{w}_t = Softmax(k_tM^k),\n\t\\vspace{-0.2cm}\n\\end{equation}\nwhere the correlation weight $\\bm{w}_t$ represents the correlation between the exercises and all latent KCs.\nIn the read operation, DKVMN predicts student performance based on the student's knowledge mastery. Specifically, DKVMN reads students’ mastery of the exercise $\\bm{r}_t$ with reference to the weighted sum of all memory vectors in the $value$ matrix using the correlation weight. The read content and the input exercise embeddings are then concatenated together and passed to a fully connected layer to yield a summary vector $\\bm{f}_t$, which contains both the student's knowledge mastery and the prior difficulty of the exercise. Furthermore, the student's performance can be predicted by applying another fully connected layer with a sigmoid activation function to the summary vector:\n\\vspace{-0.2cm}\n\\begin{equation}\n\t\\begin{aligned}\n\t\t&\\bm{r}_t = \\sum_{i=1}^{N}w_t(i)M_t^v(i), \\\\\n\t\t&\\bm{f}_t = tanh(\\bm{W}_f[\\bm{r}_t, k_t] + \\bm{b}_f),\\\\\n\t\t&p_t = \\sigma(\\bm{W}_p\\bm{f}_t + \\bm{b}_p),\n\t\\end{aligned}\n\t\\vspace{-0.1cm}\n\\end{equation}\nwhere $\\bm{W}_f$ and $\\bm{W}_p$ are the weights, $\\bm{b}_f$ and $\\bm{b}_p$ are bias terms.\nIn the write operation, after an exercise has been answered, DKVMN updates students' knowledge mastery (i.e., the $value$ matrix) based on their performance. Specifically, the learning interaction $(e_t, a_t)$ is first embedded with an embedding matrix $\\bm{B}$ to obtain the student's knowledge growth $\\bm{v}_t$. Then DKVMN calculates an erase vector $\\bm{erase}_t$ from $\\bm{v}_t$ and decides to erase the previous memory with reference to both the erase vector and the correlation weight $\\bm{w}_t$.\nFollowing erasure, the new memory vectors are updated by the new knowledge state and the add vector $\\bm{add}_t$, which forms an $erase$-followed-by-$add$ mechanism that allows forgetting and strengthening knowledge mastery in the learning process:\n\\vspace{-0.1cm}\n\\begin{equation}\n\t\\begin{aligned}\n\t\t&\\bm{erase}_t = \\sigma(\\bm{W}_e\\bm{v}_t + \\bm{b}_e), \\\\\n\t\t&\\widetilde{M}_t^v(i) = M_{t-1}^v(i)[1 - w_t(i)\\bm{erase}_t],\\\\\n\t\t&\\bm{add}_t = tanh(\\bm{W}_d\\bm{v}_t + \\bm{b}_d),\\\\\n\t\t&M_{t}^v(i) = \\widetilde{M}_t^v(i) + w_t(i)\\bm{add}_t,\\\\\n\t\\end{aligned}\n\t\\vspace{-0.1cm}\n\\end{equation}\nwhere $\\bm{W}_e$ and $\\bm{W}_d$ are the weights,  $\\bm{b}_e$ and $\\bm{b}_d$ are bias terms.\n point out that DKVMN failed to capture long-term dependencies in the learning process. Therefore, they propose a Sequential Key-Value Memory Network (SKVMN) to combine the strengths of DKT's recurrent modelling capacity and DKVMN's memory capacity. In SKVMN, a modified LSTM called \\emph{Hop-LSTM} is used to hop across LSTM cells according to the relevance of the latent KCs, which directly captures the long-term dependencies. During the writing process, SKVMN allows for the calculation of the knowledge growth of a new exercise, taking into consideration the current knowledge state, thereby yielding more reasonable results. \n\\begin{figure}[t]\n\t\\centerline{\\includegraphics[width=0.9\\columnwidth]{Figures/DKVMN.pdf}}\n\t\\vspace{-0.3cm}\n\t\\caption{The architecture of DKVMN .}\n\t\\label{fdkvmn}\n\t\\vspace{-0.6cm}\n\\end{figure}", "cites": [5380, 1206], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively synthesizes the key components of DKVMN and SKVMN, integrating their mechanisms into a coherent explanation of memory-aware knowledge tracing. It also provides a critical evaluation by identifying a limitation in DKVMN (failure to capture long-term dependencies) and explaining how SKVMN addresses this. While it abstracts some principles of external memory usage in KT, the analysis remains focused on specific model structures without reaching a broader theoretical framework."}}
{"id": "b60bbb0c-acf3-4bdd-ace6-fb43306d7dda", "title": "Attentive Knowledge Tracing", "level": "subsubsection", "subsections": [], "parent_id": "0b0df85b-c041-47ba-8171-c36927847515", "prefix_titles": [["title", "A Survey of Knowledge Tracing: Models, Variants, and Applications"], ["section", "Fundamental Knowledge Tracing Models"], ["subsection", "Deep Learning Models"], ["subsubsection", "Attentive Knowledge Tracing"]], "content": "\\label{attentive}\nIn the development of deep learning, the Transformer is initially proposed for neural machine translation , which abandons recurrence and solely relies on the self-attention mechanism to capture global dependencies within a sequence.  The Transformer has been demonstrated to excel in feature extraction and dependency capture, while maintaining high computational efficiency. Some representative pre-training models based on the Transformer, such as BERT  and GPT , have obtained state-of-the-art results on various natural language processing tasks.  propose a self-attentive model for knowledge tracing (SAKT), which directly apply the Transformer to capture long-term dependencies between students' learning interactions. Furthermore,  introduce an adaptive sparse self-attention network to generate missing features and simultaneously produce fine-grained predictions of student performance.  employ a multi-head ProbSparse self-attention mechanism to mitigate the time complexity and effectively capture the long-term dependencies in students' learning interactions.  \nHowever, the complexity of the KT task often limits the performance of the aforementioned simple Transformer applications.  introduce a novel approach named Separated Self-Attentive Neural Knowledge Tracing (SAINT) to enhance self-attentive computation for KT adaptability. Specifically, SAINT employs an encoder-decoder structure, with the exercise and answer embeddings being separately encoded and decoded by self-attention layers. The separation of the input allows SAINT to stack self-attention layers multiple times, thus capturing complex relations in student interactions. Subsequently,  introduce the SAINT+ model, which integrates two temporal features into SAINT: namely, the time taken to answer each exercise and the interval time between consecutive learning interactions. Both SAINT and SAINT+ have outperformed the SAKT model on the student performance prediction task. \nAdditionally,  observe that SAKT does not surpass DKT and DKVMN in their experiments. Unlike SAINT and SAINT+, they present a context-aware attentive knowledge tracing (AKT) model. This model integrates the self-attention mechanism with psychometric models, creating a more effective system. AKT is composed of four modules: Rasch model-based embeddings, exercise encoder, knowledge encoder, and knowledge retriever. Specifically, the embedding module employs the classic Rasch model in psychometrics  to construct embeddings for exercises and KCs:\n\\vspace{-0.2cm}\n\\begin{equation}\n\t\\begin{aligned}\n\t\t\\bm{x}_t = \\bm{c}_{c_t} + \\mu_{e_t} \\cdot \\bm{d}_{c_t}, \\\\\n\t\\end{aligned}\n\t\\vspace{-0.2cm}\n\\end{equation}\nwhere $\\bm{c}_{c_t} \\in \\mathbb{R}^{\\bm{D}}$ is the embedding of the KC of this exercise, $\\bm{d}_{c_t} \\in \\mathbb{R}^{\\bm{D}}$ is a vector that summarizes the variation in exercises with the related KC, and  $\\mu_{e_t} \\in \\mathbb{R}^{\\bm{D}}$ is a scalar difficulty parameter that controls the extent to which this exercise deviates from the related KC.\nThe exercise-answer tuple $(e_t, a_t)$ is similarly extended using the scalar difficulty parameter for each pair:\n\\vspace{-0.2cm}\n\\begin{equation}\n\t\\begin{aligned}\n\t\t\\bm{y}_t = \\bm{q}_{(c_t, a_t)} + \\mu_{e_t} \\cdot \\bm{f}_{(c_t, a_t)}, \\\\\n\t\\end{aligned}\n\t\\vspace{-0.2cm}\n\\end{equation}\nwhere $\\bm{q}_{(c_t, a_t)} \\in \\mathbb{R}^{\\bm{D}}$ is the KC-answer embedding, $\\bm{f}_{(c_t, a_t)} \\in \\mathbb{R}^{\\bm{D}}$ is the variation vector. Through the above embedding, exercises labeled as the same KCs are determined to be closely related while retaining important individual characteristics.\nThen, in the exercise encoder, the input is the exercise embeddings $ \\{\\bm{e}_1, . . . , \\bm{e}_t \\} $ and the output is a sequence of context-aware exercise embeddings $\\{\\widetilde{\\bm{e}}_1, . . ., \\widetilde{\\bm{e}}_t\\}$. AKT designs a monotonic attention mechanism to accomplish the above process, where the context-aware embedding of each exercise depends on both itself and the previous exercises, i.e., $\\widetilde{\\bm{e}}_t = f_{enc_1}(\\bm{e}_1, . . . , \\bm{e}_t)$. Similarly, the knowledge encoder takes exercise-answer embeddings $ \\{\\bm{y}_1, . . . , \\bm{y}_t \\} $  as input and\noutputs a sequence of context-aware embeddings of the knowledge acquisitions $\\{\\widetilde{\\bm{y}}_1, . . ., \\widetilde{\\bm{y}}_t\\}$ using the same monotonic attention mechanism; these are also determined by students' answers to both the current exercise and prior exercises, i.e., $\\widetilde{\\bm{y}}_t = f_{enc_1}(\\bm{y}_1, . . . , \\bm{y}_t)$.\nFinally, the knowledge retriever takes the context-aware exercise embedding $\\widetilde{\\bm{e}}_{1:t}$ and exercise-answer pair embeddings $\\widetilde{\\bm{y}}_{1:t}$ as input and outputs a retrieved knowledge state $\\bm{h}_t$ for the current exercise. Since the student’s current knowledge state depends on answering the related exercise, it is also context-aware in AKT.\nThe novel monotonic attention mechanism proposed in AKT is based on the assumption that the learning process is temporal and students' knowledge will decay over time. Therefore, the scaled inner-product attention mechanism utilized in the original Transformer is not suitable for the KT task. AKT uses exponential decay and a context-aware relative distance measure to compute the attention weights. Finally, AKT achieves outstanding performance in predicting students' future answers, as well as demonstrating interpretability due to the combination of the psychometric model.\nIt is important to note that  have recently proposed that attentive knowledge tracing models significantly benefit from students' continuous, repeated interactions on the same exercises throughout the learning process. In their experiments, the removal of these repeated interactions in the dataset led to a decline in AKT's performance, bringing it close to that of DKVMN. \n{Moreover, according to the findings of ,} existing attentive attentive KT models primarily trace patterns of a learner's learning activities, rather than their evolving knowledge states. Consequently, they develop the DTransformer model to facilitate stable knowledge state estimation and tracing, rather than solely focusing on next performance prediction. \n\\begin{figure}[t]\n\t\\centerline{\\includegraphics[width=0.9\\columnwidth]{Figures/gkt.pdf}}\n\t\\vspace{-0.3cm}\n\t\\caption{The architecture of graph-based knowledge tracing .}\n\t\\label{fgkt}\n\t\\vspace{-0.3cm}\n\\end{figure}", "cites": [679, 5376, 5374, 5375, 5370, 38], "cite_extract_rate": 0.5, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple papers to present a coherent narrative on attentive KT models, particularly highlighting the evolution from SAKT to SAINT, SAINT+, and AKT. It includes some critical evaluation, such as noting the limitations of simple Transformer applications and SAKT's underperformance compared to earlier models. The abstraction is moderate, as it identifies the general role of attention mechanisms and temporal modeling in KT, but does not rise to a meta-level framework."}}
{"id": "9d414c8f-554a-43a7-afe7-209c3c4d813e", "title": "Graph-based Knowledge Tracing", "level": "subsubsection", "subsections": [], "parent_id": "0b0df85b-c041-47ba-8171-c36927847515", "prefix_titles": [["title", "A Survey of Knowledge Tracing: Models, Variants, and Applications"], ["section", "Fundamental Knowledge Tracing Models"], ["subsection", "Deep Learning Models"], ["subsubsection", "Graph-based Knowledge Tracing"]], "content": "\\label{graph}\nGraph neural networks (GNNs), which are designed to handle complex graph-related data, have developed rapidly in recent years . The graph represents a kind of data structure that models a set of objects (nodes) and their relationships (edges). From a data structure perspective, there is a naturally existing graph structure within the KCs. Therefore, incorporating the graph structure of the KCs as additional information should be beneficial to the KT task.\n presented graph-based knowledge tracing (GKT), which conceptualizes the potential graph structure of the KCs as a graph $G = (V, E)$, where nodes $V = \\{v_1, v_2, ..., v_N\\}$ represent the set of KCs and the edges $E \\subseteq V \\times V $ represent relationships of these KCs; moreover, $ \\bm{h}^t = \\{\\bm{h}^t_{i \\in V}\\} $ represents the student's temporal knowledge state after answering the exercise at time $t$.\nThe architecture for graph-based knowledge tracing is presented in Figure \\ref{fgkt}, which is composed of three parts: (1) $aggregate$, (2) $update$ and (3) $predict$.\nIn the $aggregate$ module, GKT aggregates the  temporal knowledge state and the embedding for the answered KC $i$ and its neighboring KC $j$:\n\\vspace{-0.1cm}\n\\begin{equation}\n\t\\bm{h}_k^{'t}=\\left\\{\n\t\\begin{aligned}\n\t\t&[\\bm{h}_k^{t}, a^t\\bm{E}_s]  &(k = i), \\\\\n\t\t&[\\bm{h}_k^{t}, \\bm{E}_e(k)]  &(k \\neq i),\n\t\\end{aligned}\n\t\\right.\n\t\\vspace{-0.1cm}\n\\end{equation}\nwhere $a^t$ represents the exercises answered correctly or incorrectly at time step $t$, $\\bm{E}_s$ is the embedding matrix for the learning interactions, $\\bm{E}_e$ is the embedding matrix for the KC, and $k$ represents the $k$-th row of $\\bm{E}_e$.\nIn the $update$ module, GKT updates the temporal knowledge state based on the aggregated features and the knowledge graph structure, as follows:\n\\vspace{-0.1cm}\n\\begin{equation}\n\t\\begin{aligned}\n\t\t&\\bm{m}_k^{t+1}=\n\t\t\\begin{cases}\n\t\t\t&f_{self}(\\bm{h}_k^{'t})  (k = i), \\\\\n\t\t\t&f_{neighbor}(\\bm{h}_i^{'t}, \\bm{h}_k^{'t})  (k \\neq i),\\\\\n\t\t\\end{cases}\\\\\n\t\t&\\widetilde{\\bm{m}}_k^{t+1} = G_{ea}(\\bm{m}_k^{t+1}), \\\\\n\t\t&\\bm{h}_{k}^{t+1} = G_{gru}(\\widetilde{\\bm{m}}_k^{t+1}, \\bm{h}_{k}^{t}), \\\\\n\t\\end{aligned}\n\t\\vspace{-0.1cm}\n\\end{equation}\nwhere $f_{self}$ is the multilayer perceptron, $G_{ea}$ is the same $erase$-followed-by-$add$ mechanism used in DKVMN, and $G_{gru}$ is the gated recurrent unit (GRU) gate . Moreover, $f_{neighbor}$ defines the information propagation to neighboring nodes based on the knowledge graph structure.\nIn the $predict$ module, GKT predicts the student's performance at the next time step according to the updated temporal knowledge state:\n\\vspace{-0.1cm}\n\\begin{equation}\n\ty_k^t = \\sigma(\\bm{W}_k\\bm{h}_k^{t+1} + \\bm{b}_k),\n\t\\vspace{-0.1cm}\n\\end{equation}\nwhere $\\bm{W}_k$ is the weight parameter and $\\bm{b}_k$ is the bias term.\nIn addition to modeling the graph structure in KCs by graph neural networks,  propose to model the educational relation and topology in the concept map, which will be intended to act as mathematical constraints for the construction of the KT model.\nRecently, in the attempt to further explore knowledge structure,  propose structure-based knowledge tracing (SKT), which aims to capture the multiple relations in knowledge structure to model the influence propagation among concepts. SKT is mainly motivated by an education theory, \\emph{transfer of knowledge} , which claims that students' knowledge states on some relevant KCs will also be changed when they are practicing on a specific KC due to the potential knowledge structure among KCs. Therefore, a student's knowledge state is determined by not only the temporal effect from the exercise sequence, but also the spatial effect from the knowledge structure. To concurrently model the latent spatial effects, SKT presents the synchronization and partial propagation methods to characterize the undirected and directed relations between KCs, respectively. In this way, SKT  measures influence propagation in the knowledge structure with both temporal and spatial relations. To get rid of dependence on knowledge structure,  propose the Automatical Graph-based Knowledge Tracing (AGKT), which utilizes the automatical\ngraph to measure students’ knowledge states automatically without annotation manual annotations.", "cites": [243, 553], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes graph-based knowledge tracing models and their components, drawing from relevant work but not effectively synthesizing or integrating ideas from the cited papers. There is minimal critical analysis of the works, and no clear abstraction or generalization beyond individual systems. The narrative is mostly a summary of methods rather than an insightful discussion."}}
{"id": "2737aa4f-b2f6-4977-83f3-36d33949ab77", "title": "Summarization", "level": "subsection", "subsections": [], "parent_id": "bd30f2af-cabd-4e04-ab90-5536a2f60dde", "prefix_titles": [["title", "A Survey of Knowledge Tracing: Models, Variants, and Applications"], ["section", "Fundamental Knowledge Tracing Models"], ["subsection", "Summarization"]], "content": "{It is crucial to emphasize that, despite deep learning models exhibiting superior performance compared to Bayesian models and logistic models, there remains significant room for improvement in their interpretability and explainability.  \nDue to the end-to-end learning strategy, deep learning models are notoriously difficult to interpret. The modeling process itself is also challenging to explain. Specifically, deep learning models are predominantly data-driven and benefit a lot from large-scale student learning data. It is challenging to understand how they calculate a student's knowledge state with no theoretical guidance . \nAll we have are the results generated by these models. Therefore, any errors made by these models will lead students to doubt their reliability. The lack of explainability and interpretability has thus limited their further applicability. }\n{To make the complex KT models interpretable, especially those deep learning models, researchers have attempted various methods.\n presented a post-hoc approach to reveal the interpretability of DKT. Specifically, they employed the layer-wise relevance propagation (LRP) technique  to interpret DKT by measuring the relevance between DKT's output and input. Preliminary experimental results suggest that this post-hoc approach could be a promising method for explaining DKT. Besides, explainable AI (xAI) is proposed to make the black-box deep learning models more transparent, thereby promoting its applications .  proposed to use the xAI technique to interpret the complex KT models based on deep learning. The interpreting results of DKT have been demonstrated to aid in enhancing the trust of students and teachers. Their findings suggested that it is promising to utilize xAI techniques to interpret the deep learning KT models, thereby assisting users in accepting and applying the suggestions provided by these models. \n}", "cites": [5381], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical discussion by highlighting the trade-off between performance and interpretability in deep learning-based KT models and introduces methods like LRP and xAI for improving interpretability. It connects the issue of model opacity across multiple works and emphasizes its impact on trust and practical adoption. However, the synthesis could be deeper with more explicit connections across multiple cited papers, and the critical analysis is somewhat limited, focusing more on general limitations than evaluating specific strengths or weaknesses of the cited approaches."}}
{"id": "242f5268-6395-4e4d-b14a-e118bc61d29b", "title": "Modeling Individualization before Learning", "level": "subsection", "subsections": [], "parent_id": "a705e159-25ef-44ad-8166-1cef83a7c265", "prefix_titles": [["title", "A Survey of Knowledge Tracing: Models, Variants, and Applications"], ["section", "Variants of Knowledge Tracing Models"], ["subsection", "Modeling Individualization before Learning"]], "content": "Everything and everyone possess unique characteristics. For instance,  explored several personalized factors of various tourists to recommend personalized travel packages. Similarly, the concept of individualization in the KT task implies that different students often exhibit different learning characteristics (such as varying learning rates or prior knowledge). Considering the student-specific variability in learning could potentially enhance the KT process, as suggested by . In the subsequent sections, we will introduce various variant KT models that {take into account individualization before learning.}\nThe initial BKT paper has delved into the concept of individualization. Specifically, it uses all students' learning interactions on a specific KC to learn the individual parameter. Similarly, for a specific student, all her learning interactions are utilized to fit her individual learning parameters . Consequently, BKT is able to ascertain different learning and performance parameters for various students and KCs. However, this approach only offers a marginal improvement compared to original BKT.  \nSubsequently,  propose two simple variants of BKT that respectively individualize students' initial probability of mastery and the probability of transition from the unlearned state to the learned state. {Specifically, a student node is added to individualize the initial probability of mastery for each student. The student node assigns each student with a personalized initial probability of mastery. A conditional probability table is designed to determine the value of the student node. Similarly, if changing the connection of the student node to the subsequent knowledge nodes, the transition probability parameter can also be individualized. In this case,  the student node gives individualized transition parameters to each student.}\nMoreover, rather than individualizing only one kind of parameter in BKT, some other variants of BKT opt to individualize all four BKT parameters simultaneously .  suggest that when applied in an intelligent tutoring system, the individualized BKT model can yield good improvements to student learning efficacy, reducing by about half the amount of questions required for 20\\% of students to achieve mastery.\nAnother means of modeling individualization is clustering, which considers a wider range of students in different groups . \nBy clustering the students into various groups, we can train different KT models and make predictions on the test data. The number of clusters is then varied according to the student groups and the predicting process is repeated iteratively. Finally, we can obtain a set of different predictions. Furthermore, there are two common methods used to combine these predictions : (1) uniform averaging, which simply averages the predictions; (2) weighted averaging, which combines the models by means of a weighted average.\nTo realize clustering, K-means is a basic clustering algorithm that randomly initializes a set of cluster centroids, which are identified using Euclidean distance. Another popular clustering algorithm is spectral clustering, which represents the data as an undirected graph and analyzes the spectrum of the graph Laplacian obtained from the pairwise similarities of data points. Recently, some novel clustering algorithms have been proposed, including discrete nonnegative spectral clustering  and clustering uncertain data .\n propose a model named deep knowledge tracing with dynamic student classification (DKT-DSC), which introduces individualization to DKT by exploiting the idea of clustering. \nAccording to students' previous performance, DKT-DSC assigns students with similar learning ability to the same group . The knowledge states of students in different groups are then traced by different DKT models. Moreover, considering the dynamic property of the learning ability, each student's learning sequence is segmented into multiple time intervals. At the start of each time interval, DKT-DSC will reassess students' learning ability and reassign their groups. \nIn DKT-DSC, the K-means clustering algorithm is utilized to split students with similar ability levels into the same group at each time interval. After learning the centroids of all K clusters, each student is assigned to the nearest cluster. Through dynamic student clustering, DKT-DSC offers an effective approach to realizing individualization in DKT.\n claim that it is significant to consider both individual exercise representation and individual prior knowledge. They propose a fine-grained knowledge tracing model, named FGKT. FGKT obtains the individual exercise representation through the acquisition of knowledge cells (KCs) and exercise distinctions. Subsequently, it assesses the individual prior knowledge by evaluating the relevance between current and historical learning interactions. Finally, the above individual representations will be utilized as the input of LSTM in FGKT to evaluate students' evolving knowledge states.  also notice that the individualization of exercises is significant for measuring students' knowledge states. They propose to consider multiple exercise factors, including the difficulty\nand the discrimination, to enhance the performance of DKT.\n propose a convolutional knowledge tracing model (CKT) to implicitly measure student individualization. Specifically, CKT considers two factors that influence students' individualization: individualized learning rates and individualized prior knowledge.\nIndividualized learning rates represent students' differing capacities to absorb knowledge. The sequence of student learning interactions can reflect different learning rates in the sense that students with high learning rates can rapidly master knowledge, while others need to spend more time trying and failing. Therefore, it is reasonable to assess the differences in learning rate by simultaneously processing several continuous learning interactions within a sliding window of convolutional neural networks . Besides, individualized prior knowledge refers to students' prior knowledge, which can be assessed via their historical learning interactions.", "cites": [5382, 166], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 14, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of KT models that incorporate individualization before learning, integrating key concepts from the cited papers. While it briefly explains the technical approaches (e.g., student node, clustering, DKT-DSC, FGKT), it lacks deeper synthesis of how these methods interrelate or contribute to a broader framework. There is limited critical evaluation of the strengths or limitations of each approach, and while it touches on general patterns (e.g., clustering for individualization), it does not offer high-level abstractions or meta-insights."}}
{"id": "2cb054ef-697a-4d85-a2b7-9c0e6282362f", "title": "Considering Forgetting after Learning", "level": "subsection", "subsections": [], "parent_id": "a705e159-25ef-44ad-8166-1cef83a7c265", "prefix_titles": [["title", "A Survey of Knowledge Tracing: Models, Variants, and Applications"], ["section", "Variants of Knowledge Tracing Models"], ["subsection", "Considering Forgetting after Learning"]], "content": "In real-world scenarios, while learning, forgetting is inevitable . The \\emph{Ebbinghaus forgetting curve theory} indicates that students' knowledge proficiency will decline due to forgetting . Recently,  proposed the concept of 'Knowledge Proficiency Tracing' (KPT), a model that can dynamically capture the changes in students' proficiency levels on knowledge concepts over time. This model effectively tracks these changes in an interpretable manner. Therefore, the assumption that students' knowledge states will remain constant over time is untenable. However, fundamental KT models, such as the BKT, often overlook forgetting. In the following, we will introduce some variants of fundamental KT models that have attempted to consider forgetting after learning for more precise knowledge states. \n discover that BKT consistently overestimates the accuracy of students' answers when a day or more had elapsed since {their previous responses.} The underlying reason is that BKT assumes that student performance will remain the same regardless of how much time has passed. To consider how student performance declines with time, they propose a BKT-Forget model, which hypothesizes that students may forget information they have learned as days go by. In the BKT-Forget model, a time node is added to specify which parameters should be affected by a new day and the new day node is fixed with a prior probability of 0.2. {It also introduced parameters to represent the forgetting rate on a new day and denote the forgetting rate on the same day.} However, although BKT-forget does consider the decline in student performance, it can only model forgetting that occurs over the time scale of days. \nTo model the continuous decay of knowledge as time progresses,  incorporate forgetting into BKT based on the assumption that learned knowledge decays exponentially over time . An exponential decay function is thus utilized to update the knowledge mastery level. They further assumed that the chance of forgetting will increase if a student does not practice the knowledge concepts within 30 days.\nMoreover,  introduce an approach that counts the number of intervening trials and treats each as an independent opportunity for forgetting to occur.\nRecall the PFA model in Eq.(\\ref{pfa}), in which the probability of students' mastery is estimated using a logistic function: $ p(\\theta) = \\sigma(\\beta + \\mu s + \\nu f)$. The original PFA model ignores the order of answers, in addition to the time between learning interactions. It is therefore difficult to directly incorporate time information into the original PFA model.\n propose PFAE (PFA Elo/Extended), a variant of the PFA model that combines PFA with some aspects of the Elo rating system . The Elo rating system is originally devised for chess rating (estimating players' skills based on match results). In PFAE, $\\theta$ is updated after each learning interaction:\n\\vspace{-0.1cm}\n\\begin{equation}\n\t\\theta :=\\left\\{\n\t\\begin{aligned}\n\t\t& \\theta + \\mu \\cdot (1 - p(\\theta))& \\mbox{if the answer was correct}, \\\\\n\t\t& \\theta + \\nu \\cdot p(\\theta)&  \\mbox{if the answer was wrong}.\n\t\\end{aligned}\n\t\\right.\n\t\\vspace{-0.1cm}\n\\end{equation}\nAs the forgetting behavior of students is closely related to time, in order to consider forgetting,  add a time effect function $f$ to $\\theta$, i.e., using $p(\\theta+f(t))$ instead of $p(\\theta)$, where $t$ is the time (in seconds) from the last learning interaction, and $f$ is the time effect function.\nTo represent the complex forgetting behavior, the DKT-forget model  introduces forgetting into DKT, which considers three types of side information related to forgetting: (1) the repeated time gap that represents the interval time between the present interaction and the previous interaction with the same KC, (2) the sequence time gap that represents the interval time between the present interaction and the previous interaction, and (3) past trial counts that represent the number of times a student has attempted on the exercise with the same KC. All these three features are discretized at $log_2$ scale.\nThose side information is concatenated as additional information and represented as a multi-hot vector $\\bm{c}_t$, which is integrated with the embedding vector $\\bm{v}_t$ of the learning interaction, as follows:\n\\vspace{-0.1cm}\n\\begin{equation}\n\t\\bm{v}_t^c = \\theta^{in}(\\bm{v}_t, \\bm{c}_t),\n\t\\vspace{-0.1cm}\n\\end{equation}\nwhere $\\theta^{in}$ is the input integration function. The integrated input $\\bm{v}_t^c$  and the previous knowledge state $\\bm{h}_{t-1}$ are passed through the RNNs to update $\\bm{h}_t$ in the same way as in Eq.(\\ref{dkt}). The additional information at the next time step $\\bm{c}_{t+1}$ is also integrated with the updated $\\bm{h}_t$:\n\\vspace{-0.1cm}\n\\begin{equation}\n\t\\bm{h}_t^c = \\theta^{out}(\\bm{h}_t, \\bm{c}_{t+1}),\n\t\\vspace{-0.1cm}\n\\end{equation}\nwhere $\\theta^{out}$ is the output integration function.\n propose a novel HawkesKT model, which introduces the Hawkes process to adaptively model temporal cross-effects. The Hawkes process performs well at modeling sequential events localized in time, as it controls corresponding temporal trends by the intensity function. The intensity function in HawkesKT is designed to characterize the accumulative effects of previous learning interactions, along with their evolutions over time. In HawkesKT, the temporal cross-effects and the ways in which they evolve between historical learning interactions combine to form a dynamic learning process.", "cites": [5378], "cite_extract_rate": 0.08333333333333333, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple approaches to incorporating forgetting into KT models, showing how different papers build on or adapt existing models (e.g., BKT-Forget, PFAE, DKT-forget, HawkesKT). It provides a coherent progression from discrete-time to continuous-time forgetting modeling. While it offers some critical points, such as BKT-Forget's limitation to daily time scales and PFA's neglect of time and order, it could have more deeply compared the effectiveness or trade-offs of these approaches. The abstraction level is moderate, identifying the common goal of modeling forgetting dynamics but not offering a higher-level conceptual framework."}}
{"id": "60b4234d-a28f-4026-ab3b-0291b62ceca6", "title": "Utilizing Side Information across Learning", "level": "subsection", "subsections": [], "parent_id": "a705e159-25ef-44ad-8166-1cef83a7c265", "prefix_titles": [["title", "A Survey of Knowledge Tracing: Models, Variants, and Applications"], ["section", "Variants of Knowledge Tracing Models"], ["subsection", "Utilizing Side Information across Learning"]], "content": "\\label{sec:side information}\nMost KT models primarily {rely on exercises and student responses} to evaluate students' knowledge states. These models have yielded impressive results and have been effectively implemented in online learning systems. Despite this, there are various other types of side information collected across the learning process that could be utilized to enhance {these models.} In this section, we will introduce several variants that aim to leverage this diverse side information across learning. \nIn terms of a student's first response time, a short initial response time could indicate either high proficiency or 'gaming' behavior, while a long initial response time could indicate either careful thinking or lack of concentration. Since the connection between initial response time and knowledge state could be influenced by {complex factors},  propose to discretize the continuous first response time into four categories (i.e., extremely short, short, long, extremely long) {to eliminate unnecessary information and simplify the latent complex possibilities. They then build a one-by-four parameter table, in which each column represents the category of the initial response time of the previous exercise, while the relevant values represent the probability of correct answers}. \nRegarding tutor intervention,  propose the Bayesian evaluation and assessment model, which simultaneously assesses students' knowledge states and evaluates the lasting impact of tutor intervention. More specifically, it adds one observable binary intervention node to BKT: \\emph{True} means that the tutor intervention occurs in corresponding interactions while \\emph{False} indicates the opposite. The connection between the intervention node and knowledge node indicates the potential impact of the tutor intervention on students' knowledge states. The intervention node is linked to all four BKT parameters. As a result, there are a total of eight parameters to learn in order to incorporate tutor intervention. One possible way to reduce the number of parameters is choosing to link only the intervention node to the learning rate parameter .\nSimilarly,  develop the intervention-Bayesian knowledge tracing (Intervention-BKT) model, {which incorporates two types of interventions into BKT and distinguishes their different effects: \\emph{elicit and tell}}. The relations between the intervention and performance nodes represent the impact of teaching interventions on student performance, while the relations between the intervention and knowledge nodes represent the impact of teaching interventions on students' knowledge states. Therefore, at each learning interaction, while the present knowledge state is conditional on both the previous knowledge state and the current intervention, the student's performance depends on both the present knowledge state and the current intervention.\nRather than considering only one kind of side information,  propose a feature-aware student knowledge tracing (FAST) model, which allows for the utilization of all kinds of side information. Traditional BKT uses conditional probability tables for the guessing, slipping, transition and learning probabilities, meaning that the number of features involved in inference grows exponentially. Therefore, as the number of features increases, the time and space complexity of the model also grow exponentially. To deal with this large number of features, FAST uses logistic regression parameters rather than conditional probability tables. The number of features and complexity increase linearly rather than exponentially.  \nFor parameter learning, FAST uses the Expectation Maximization with Features algorithm  and focuses on only emission features. The E step uses the current parameter estimates $\\lambda$ to infer the probability of the student having mastered the KC at each learning interaction.\nThe parameters  $\\lambda$ are now a function of the weight $\\beta$ and the feature vector $\\bm{f}(t)$.  $\\bm{f}$ is the feature extraction function, and  $\\bm{f}(t)$ is the feature vector constructed from the observations at the relevant time step. The emission probability is represented with a logistic function:\n\\vspace{-0.1cm}\n\\begin{equation}\n\t\\begin{aligned}\n\t\t\\lambda(\\beta)^{y^{'},k^{'}} & = \\frac{1}{1 + exp(-\\beta^T \\cdot \\bm{f}(t))},\n\t\\end{aligned}\n\t\\vspace{-0.1cm}\n\\end{equation}\nwhere $\\beta$ is learned by training a weighted regularized logistic regression using a gradient-based search algorithm.\n propose an extension to DKT that explored the inclusion of additional features. Specifically, it incorporates an auto-encoder network layer to convert the higher-dimensional input data into smaller representative feature vectors, thereby reducing both the resource and time requirement for training. Students' response time, opportunity count, and first action are selected as incorporated side information and all input features are converted into a fixed-length input vector. First, all input features are converted into categorical data and represented as a sparse vector by means of one-hot encodings. These encoded features are concatenated together to construct the higher-dimensional input vector:\n\\vspace{-0.2cm}\n\\begin{equation}\n\t\\begin{aligned}\n\t\tC(e_t,a_t)  &= e_t + (max(e) +1)a_t, \\\\\n\t\tv_t &=  O(C(e_t,a_t)) \\oplus O(C(t_t,a_t)) \\oplus O(t_t), \\\\\n\t\tv_t^{'} &=  tanh(W_vv_t + b_v),\n\t\\end{aligned}\n\t\\vspace{-0.2cm}\n\\end{equation}\nwhere $C$ is the cross feature, $O$ is the one-hot encoder format, $v_t$ represents the resulting input vector of each learning interaction, $e_t$ is the exercise, $a_t$ refers to the answer, $t_t$ is the response time, $W_v$ is the weight parameter and $b_v$ is the bias term. Subsequently, an auto-encoder is introduced to reduce the dimensionality without incurring the loss of too much important information. Finally, the feature vectors extracted by auto-encoder will be the new input of DKT.\n presented another extension to DKT, namely the Exercise-aware Knowledge Tracing (EKT), which utilized the potential value of exercises' text contents. Generally, the text content is of great significance for students to understand the exercises. For example,  used text materials to automatically predict their difficulties,  utilized the text content to find similar exercises.  further proposed a pre-training model called QuesNet for learning the unified representations of heterogeneous exercises. \nTherefore, instead of using one-hot encoding of exercises, EKT automatically learns the semantic representation of each exercise from its text contents. EKT first uses $Word2vec$  to pre-train the embedding vector for each word in the exercise. It then constructs a bidirectional LSTM, which captures the word sequence from both forward and backward directions to learn the semantic word representation. The element-wise max-pooling operation is utilized to merge words’ contextual representations into a global embedding. Finally, EKT can update the student's knowledge state with the aid of the semantic representation of each exercise. \nTo achieve more feasible integration of side information,  present a deep knowledge tracing method with decision trees (DKT-DT), which takes advantage of Classification And Regression Trees (CART) to preprocess the heterogeneous input features . Specifically, CART is utilized to automatically partition the feature space and outputs whether or not a student can answer an exercise correctly. {The predicted response and the true response are encoded into a four-bit binary code; for example, the code is $1010$ if the predicted response and the true response are both correct. This binary code is then concatenated with the original one-hot encoding of the exercise as the new input of DKT to train the corresponding model.}\n suggests that the student's language proficiency can serve as supplementary information to improve existing KT models. The student’s language proficiency is extracted by Elo rating score and time window features. Then, the language proficiency information is demonstrated to be effective in promoting several KT models, including DKT, DKVMN, and SAKT. Additionally, the problem of cold start in the KT task is alleviated with the assistance of language proficiency information. \n explore to add side information to the original KT model by auxiliary learning tasks. They specifically introduced two tasks: (1) predicting the KCs of the question, and (2) predicting the individualized prior knowledge. By training with these tasks, KT can enhance its understanding of the intrinsic relationships between questions and KCs, while explicitly capturing student-level variability.  \n{When solving programming problems, we can record students' full code submissions, which can be employed to analyze their programming ability.  collected student programming data and analyzed their personalized programming preferences. The study commenced with a statistical analysis of student errors, followed by an examination of students' programming structures derived from their code submissions, and finally used BKT to measure students' programming abilities.\n transformed students' code submissions into embedded vectors, and applied them in DKT to model students' fine-graded programming knowledge states.\n noticed that a single programming problem generally involves in multiple KCs, thereby they proposed to learn useful information about the programming problem's multiple requirements from students' code submissions. }", "cites": [1684, 5383, 5384], "cite_extract_rate": 0.15789473684210525, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple approaches for integrating side information into KT models, including first response time, tutor interventions, and semantic exercise content. It moves beyond mere description by highlighting how each approach modifies or enhances existing KT models. However, it lacks deeper comparative analysis or critical evaluation of trade-offs and limitations, and while it identifies patterns (e.g., dimensionality challenges), it does not fully abstract them into broader principles."}}
{"id": "260a9040-27f0-4099-a17f-ea39955bc0eb", "title": "Learning Resources Recommendation", "level": "subsection", "subsections": [], "parent_id": "5fc15152-8ae0-4a7f-83a2-d64f287123fe", "prefix_titles": [["title", "A Survey of Knowledge Tracing: Models, Variants, and Applications"], ["section", "Applications"], ["subsection", "Learning Resources Recommendation"]], "content": "Traditionally, learning resources for each student are selected in one of two ways. The first one requires teachers to manually select suitable resources that match students' knowledge levels. However, this approach requires substantial time and effort, and different teachers may have different preferences. The second one allows students themselves to freely choose resources to learn. However, this may result in students choosing too easy or too difficult materials that will not benefit their learning , leading to low learning efficiency. In recent years, the prevalence of intelligent tutoring systems and the development of KT methods have made it possible to automatically recommend appropriate exercises to each student based on artificially designed algorithms.\nExercises are the most common learning resources in learning. Given the inferred knowledge states, one common strategy is selecting the next exercise that will best advance students' knowledge acquisition.  propose two extensions of the original BKT model, which respectively considered  exercises' difficulties and students' multiple-attempt behaviors. These two extensions are integrated into a BKT-sequence algorithm to recommend exercises to students based on their knowledge states. Specifically, BKT-sequence first determines the predicted range of scores for each exercise. It then computes an expected score for each exercise that the student should get to achieve mastery, which is dependent on their current knowledge state (for instance, a lower knowledge state will result in higher expected scores). Finally, the algorithm returns the exercise with a predicted score that is closest to that of the expected score. Therefore, as the knowledge state of a particular KC grows, more difficult exercises will be recommended, as harder exercises are associated with a lower predictive score. Experimental results have shown that students using the BKT-sequence algorithm were able to solve more difficult exercises, obtained higher performance and spent more time in the system than students who used the traditional approach. Moreover, students also expressed that the BKT-sequence algorithm was more efficient.  expanded the DKVMN model  to include the exercise's type and difficulty. This model is then used to assess students' knowledge state and subsequently recommends personalized exercises for each student in Ssmall Private Online Courses (SPOCs). They conducted a randomized controlled trial to show the proposed personalized exercise recommendation could enhance students' learning efficiency.\nIn addition to exercises, there are also some other types of multi-modal learning resources, such as videos and figures.  utilizes an adaptation of BKT to improve student performance prediction by incorporating video observation. Experimental verification demonstrates the impact of both using and eschewing video data, as well as the learning rate associated with a particular video. In this way, they further developed a method to help people evaluate the quality of video resources. Concretely, they proposed the Template 1 Video model to incorporate video observations into BKT, which adds video activity as additional independent observation nodes to the BKT model. This model accordingly considers the probability that a given video resource will impart knowledge to a student. Moreover, the transition probability in BKT is conditional only on the presence of either a video or an exercise. Thus, the quality of the video can be determined by its promotion of learning, and this model can be leveraged as a tool to aid in evaluating and recommending video resources.\nWhen recommending learning resources, the primary aim of existing solutions is to choose a simple strategy for assigning non-mastered exercises to students. \nWhile reasonable, it is also too broad to advance learning effectively.  accordingly propose three more beneficial and specific objectives: \\emph{review and explore}, \\emph{smoothness of difficulty level} and \\emph{student engagement}. In more detail, \\emph{review and explore} considers both enhancing students' non-mastered concepts with timely reviews and reserving certain opportunities to explore new knowledge; \\emph{smoothness of difficulty level} indicates that the difficulty levels of several continuous exercises should vary within a small range as students gradually learn new knowledge; finally, \\emph{student engagement} considers that to promote students' enthusiasm during learning, the recommended exercises should be in line with their preferences. In order to support online intelligent education with the above three domain-specific objectives,\nthey developed a more reasonable multi-objective deep reinforcement learning (DRE) framework. DRE presented three corresponding novel reward functions to capture and quantify the effects of the above three objectives. This DRE framework is a unified platform designed to optimize multiple learning objectives, where more reasonable objectives also can be incorporated if necessary.\nExperimental results show that DRE can effectively learn from the students' learning records to optimize multiple objectives and adaptively recommend suitable exercises.", "cites": [1206], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple KT-related approaches for learning resource recommendation, including BKT extensions, DKVMN adaptations, and a DRE framework, connecting them into a coherent narrative about the evolution of recommendation strategies. It critically evaluates the limitations of traditional methods and introduces domain-specific objectives like review and explore, difficulty smoothness, and engagement. The section abstracts these ideas into broader principles, such as the role of multi-modal data and multi-objective optimization in intelligent education systems."}}
{"id": "f6f4178c-41e5-4941-b347-f40654197f09", "title": "Adaptive Learning", "level": "subsection", "subsections": [], "parent_id": "5fc15152-8ae0-4a7f-83a2-d64f287123fe", "prefix_titles": [["title", "A Survey of Knowledge Tracing: Models, Variants, and Applications"], ["section", "Applications"], ["subsection", "Adaptive Learning"]], "content": "{Adaptive learning, unlike learning resource recommendations, goes beyond the mere provision of resources. It not only concentrates on the selection of appropriate learning materials but also designs effective learning strategies and dynamic learning pathways. These are structured based on both the learning rules and students' evolving knowledge states.\nSpecifically, adaptive learning broadly refers to \\emph{\"a learning process in which the content taught, or the way such content is presented, changes or 'adapts' based on individual student responses, and which dynamically adjusts the level or types of instruction based on individual student abilities or preferences\"} . }\nThe first few attempt made to apply KT to adaptive learning was the ACT Programming Tutor (APT) , where students were asked to write short programs and BKT was utilized to estimate their evolving knowledge state. This tutor can present an individualized sequence of exercises to each student based on their estimated knowledge states until the student has \"mastered\" each rule. \nIn recent years, Massive Open Online Courses (MOOCs) have become an emerging modality of learning, particularly in higher education.  adapt BKT on the edX platform. The research object was a 14-week online course that included weekly video lectures and corresponding lecture problems. BKT was applied to enhance students' learning on this course. In order to better adapt BKT to the learning platform, the original BKT was modified in several respects. First, due to the lack of labeled KCs, the problems would be directly seen as the KCs, while the questions would be seen as the exercises belonging to the KC.\nSecond, in order to capture the varying degrees of students' knowledge acquisition at each attempt, the modified model assigned different guess and slip parameters to different attempt counts. Third, to deal with the problem of multiple pathways in the system, which reflected that the impacts on learning may come from various resources, they framed the influence of resources on learning as a credit/blame inference problem.\nGenerally, students' cognitive structures include both students' knowledge level and the knowledge structure of learning items (e.g., \\emph{one-digit addition} is the prerequisite knowledge of \\emph{two-digit addition}). Therefore, adaptive learning should maintain consistency with both students' knowledge level and the latent knowledge structure. Nevertheless, existing methods for adaptive learning often focus separately on either the knowledge levels of students (i.e., with the help of specific KT models) or the knowledge structure of learning items. To fully exploit the cognitive structure for adaptive learning,  propose a Cognitive Structure Enhanced framework for adaptive Learning (CSEAL). CSEAL conceptualized adaptive learning as a Markov Decision Process. It first utilized DKT to trace the evolving knowledge states of students at each learning step. Subsequently, the authors designed a navigation algorithm based on the knowledge structure to ensure that the learning paths in adaptive learning were logical and reasonable, which also reduced the search space in the decision process. Finally, CSEAL utilized the actor-critic algorithm to dynamically determine what should be learned next. In this way, CSEAL can sequentially identify the most suitable learning resources for different students.", "cites": [5373], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes the concept of adaptive learning and integrates the application of KT models (e.g., BKT, DKT) into a coherent narrative. It highlights how KT supports adaptive learning by modeling student knowledge states and aligning with cognitive structures. While it identifies a gap in current methods focusing separately on knowledge levels and structures, the critical analysis remains moderate and could delve deeper into limitations or trade-offs."}}
{"id": "c5ab0210-1d4b-4488-91f4-cb2e837345b7", "title": "Broader Applications", "level": "subsection", "subsections": [], "parent_id": "5fc15152-8ae0-4a7f-83a2-d64f287123fe", "prefix_titles": [["title", "A Survey of Knowledge Tracing: Models, Variants, and Applications"], ["section", "Applications"], ["subsection", "Broader Applications"]], "content": "{The above two types of applications are most commonly used for KT in student learning. In addition, the KT methods can be expanded to be utilized in any systems that necessitate continuous evaluation of user capabilities or states. We will introduce some broader applications of KT in this section.}\nIn gaming systems, the paradigm of tracing students' knowledge state can also work for player modeling. Here, player modeling, which is the study of computational models of players in games, aims to capture human players' characteristics and cognitive features . For instance,  reveal that children engage in cycles of increasingly sophisticated mathematical thinking over the course of playing an online game.  present an approach to trace player knowledge in a parallel programming educational game, which is capable of measuring the current players' real-time state across the different skills required to play an educational game based only on in-game player activities.  conduct a classroom experiment comparing a commercial game for equation solving, i.e., \\emph{DragonBox}, with a research-based intelligent tutoring system, i.e., \\emph{Lynnette}. The results indicated that students who used \\emph{DragonBox} enjoyed the experience more, while students who used \\emph{Lynnette} performed significantly better on the test. Therefore, it is possible to enable students to learn effectively and happily by designing suitable educational games on the online learning platform.\n{In crowdsourcing, unlabeled data or specific tasks are assigned to various crowd annotators. Understanding the dynamic capabilities of these annotators is crucial in ensuring the reliability of their annotations and promoting the annotation efficiency.  developed a framework called KT4Crowd, which utilized KT methods to predict the performance of annotators, which surpassed traditional rating systems. \nBesides,  observed that students' participation in crowdsourcing tasks can enhance their learning. KT methods can also better comprehend  students' knowledge states, aided by the students' annotated items. \n}\n develop an online citizen science project that employs machine learning techniques to improve the training of new volunteers using authentic tasks featuring uncertain outcomes, such as image classification. Specifically, they employ the BKT model to monitor the knowledge states of volunteers, enabling them to more efficiently complete assigned tasks and contribute meaningfully to the project. \n develop an automated exercise collection approach for teachers, employing the KT model and reinforcement learning. Specifically, the exercise collection need to be well-designed to align with students' abilities. This study first leverages the KT model to forecast students' performance on unseen exercise candidates.  Subsequently, the exercise selector is designed based on the KT model's predictions, ensuring that the exercise collection is both approximate and optimized. Similarly,  design a reinforcement learning guided\nmethod for exam paper generation, where the DKT model is utilized to measure examinees' knowledge states. \n\\iffalse\n\\begin{table*}[t]\n\t\\vspace{-0.2cm}\n\t\\renewcommand\\arraystretch{1.0}\n\t\\caption{A Summary of Public Datasets for Knowledge Tracing. }\n\t\\vspace{-0.3cm}\n\t\\centering\n\t\\begin{tabular}{l|l}\n\t\t\\hline\n\t\tDatasets & Link\\\\\n\t\t\\hline\n\t\tASSISTments2009 & https://sites.google.com/site/assistmentsdata/home/assistment-2009-2010-data\\\\\n\t\tASSISTments2012 & https://sites.google.com/site/assistmentsdata/home/2012-13-school-data-with-affect\\\\\n\t\tASSISTments2015 & https://sites.google.com/site/assistmentsdata/home/2015-assistments-skill-builder-data  \\\\\n\t\tASSISTments2017 &  https://sites.google.com/view/assistmentsdatamining/dataset \\\\\n\t\tKDD-Cup 2010 &  https://pslcdatashop.web.cmu.edu/KDDCup/rules\\_data\\_format.jsp \\\\\n\t\tStatics 2011 &  https://pslcdatashop.web.cmu.edu/DatasetInfo?datasetId=507 \\\\\n\t\tslepemapy.cz &   https://www.fi.muni.cz/adaptivelearning/?a=data\\\\\n\t\tsynthetic & https://github.com/chrispiech/DeepKnowledgeTracing/tree/master/data/synthetic\\\\\n\t\tJunyi &  https://pslcdatashop.web.cmu.edu/DatasetInfo?datasetId=1198 \\\\\n\t\tEdNet &  http://ednet-leaderboard.s3-website-ap-northeast-1.amazonaws.com/\\\\\n\t\tpoj &  http://poj.org/ \\\\\n\t\t\\hline\n\t\\end{tabular}\n\t\\label{tab3}\n\t\\vspace{-0.6cm}\n\\end{table*}\n\\fi", "cites": [5385, 8912], "cite_extract_rate": 0.2222222222222222, "origin_cites_number": 9, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of broader KT applications, including gaming, crowdsourcing, and automated exercise/exam generation, referencing a few specific studies. While it makes some connections between KT and these domains, it lacks deeper synthesis of ideas and critical evaluation of the cited works. Generalization is present at a basic level, but no overarching principles or frameworks are clearly articulated."}}
{"id": "34caa8b6-1d19-4064-837a-9dcb1549a45d", "title": "Datasets and Baselines", "level": "section", "subsections": ["8d0ee175-d87a-4c6c-bbdc-2113cd06121b", "66f9b876-576e-4b0f-af2b-91235cb5ec11"], "parent_id": "4e5af7af-552b-4c7f-939b-54b6b4dadeb9", "prefix_titles": [["title", "A Survey of Knowledge Tracing: Models, Variants, and Applications"], ["section", "Datasets and Baselines"]], "content": "\\label{sec:dataset}\nAfter introducing above KT models and variants, to better help researchers and practitioners who want to further conduct related work and promote the application of KT, we have open sourced two algorithm libraries, i.e., EduData that for downloading and preprocessing most existing KT-related datasets, and EduKTM that includes extensible and unified implementations of existing popular KT models. In the following, we will give detailed introduction of these two algorithm libraries.\n\\begin{table*}[t]\n\t\\vspace{-0.6cm}\n\t\\renewcommand\\arraystretch{1.2}\n\t\\caption{{Basic information and statistics of existing datasets available for evaluating KT models. }}\n\t\\vspace{-0.2cm}\n\t\\centering\n\t\\resizebox{\\textwidth}{!}\n\t{\n\t\t\\begin{tabular}{l|c|c|c|c|c|r|r|r|r}\n\t\t\t\\hline\n\t\t\t\\multirow{3}*{Datasets}&\\multirow{3}{*}{Subjects} &\\multirow{3}{*}{Learning Stages} & \\multirow{3}*{\\makecell[c]{Side \\\\ Information}}& \\multicolumn{2}{c|}{Sources}&\\multicolumn{4}{c}{Statistics} \\\\\n\t\t\t\\cline{5-10}  \n\t\t\t& & & & \\makecell[c]{Online \\\\ platforms} &  \\makecell[c]{Educational \\\\ challenges} & \\makecell[c]{\\# of \\\\ Students} & \\makecell[c]{\\# of \\\\ Exercises} & \\makecell[c]{\\# of \\\\ KCs} & \\makecell[c]{\\# of  \\\\ Learning records}\\\\\n\t\t\t\\hline\n\t\t\tASSISTments2009 & mathematics & middle school  & Yes & \\Checkmark &  & 4,163    & 17,751& 123 & 346,860   \\\\\n\t\t\t\\hline\n\t\t\tASSISTments2012 & mathematics & middle school & Yes & \\Checkmark &  & 46,674    & 179,999& 265 &6,123,270 \\\\\n\t\t\t\\hline\n\t\t\tASSISTments2015 & mathematics & middle school & No & \\Checkmark &  & 19,917   & /& 100  & 708,631  \\\\\n\t\t\t\\hline\n\t\t\tASSISTments2017& mathematics &\\makecell[c]{from middle school \\\\ to college}  & Yes &  & \\Checkmark & 1,709  &3,162& 102 & 942,816 \\\\\n\t\t\t\\hline\n\t\t\tJunyi  & mathematics & \\makecell[c]{from primary \\\\ to high school} & Yes & \\Checkmark &  & 247,606   & 722 & 41 & 25,925,922 \\\\\n\t\t\t\\hline\n\t\t\tEedi2020 & mathematics & \\makecell[c]{from primary \\\\ to high school} & Yes &  & \\Checkmark & 118,971    & 27,613 & 388 & 15,867,850  \\\\\n\t\t\t\\hline\n\t\t\tStatics2011 &engineering &university &Yes & \\Checkmark &  & 335    & 1,224 & 80 & 361,092 \\\\\n\t\t\t\\hline\n\t\t\tEdNet-KT1  & english & / & Yes & \\Checkmark &  &  784,309  & 13,169 & 188 &  95,293,926 \\\\\n\t\t\t\\hline\n\t\t\tEdNet-KT2  & english &/ & Yes & \\Checkmark &  &  297,444  & 13,169 & 188 &  56,360,602 \\\\\n\t\t\t\\hline\n\t\t\tEdNet-KT3  & english &/ & Yes & \\Checkmark &  &  297,915  & 13,169 & 293 &  89,270,654 \\\\\n\t\t\t\\hline\n\t\t\tEdNet-KT4  & english &/& Yes& \\Checkmark &  &  297,915  & 13,169 & 293 &  131,441,538 \\\\\n\t\t\t\\hline\n\t\t\tCodeWorkout  & programming & university  & Yes &  & \\Checkmark & 819    & 50 & 50 &  \\textgreater 130,000 \\\\\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t}\n\t\\label{data}\n\t\\vspace{-0.6cm}\n\\end{table*}", "cites": [5386], "cite_extract_rate": 0.14285714285714285, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily serves as a factual overview of datasets and baselines in the field of Knowledge Tracing, with a table listing various datasets and their characteristics. While it references one paper (EdNet 2020), the integration is minimal and does not connect ideas across sources or provide a coherent narrative. There is no critical analysis or discussion of limitations, and no abstraction of broader patterns or principles is attempted."}}
{"id": "db5459a4-5149-42f0-85da-a9bc2ed105e4", "title": "Eedi2020 Dataset", "level": "subsubsection", "subsections": [], "parent_id": "8d0ee175-d87a-4c6c-bbdc-2113cd06121b", "prefix_titles": [["title", "A Survey of Knowledge Tracing: Models, Variants, and Applications"], ["section", "Datasets and Baselines"], ["subsection", "Datasets"], ["subsubsection", "Eedi2020 Dataset"]], "content": "The Eedi2020 dataset  is also released in an achedemic challenge, i.e., NeurIPS 2020 Education Challenge\\footnote{https://eedi.com/projects/neurips-education-challenge}. This dataset contains students’ answers to mathematics questions from Eedi, an online educational platform which millions of students interact with daily around the globe from school year 2018 to 2020. All exercises are multiple-choice problems with 4 possible answer choices, exactly one of which is correct. In Table \\ref{data}, we give the statistics based on the training data in this competition, the total number of learning records in the full dataset exceeds 17 million. It worth noting that Eedi2020 gives students' exact answer choice so that we can also predict students options . Moreover, for the students, Eedi2020 records lots of valuable context information, including the \\textit{Gender}, \\textit{DateOfBirth}, \\textit{PremiumPupil}. For the learning records, Eedi2020 also presents their \\textit{Confidence}, \\textit{GroupId}, \\textit{QuizId}, and \\textit{SchemeOfWorkId}.", "cites": [8913], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of the Eedi2020 dataset, describing its origin, format, and the types of information it contains. While it references a relevant paper (Option Tracing), it does so only to highlight one feature (predicting student options) without synthesizing deeper insights or comparing it with other approaches. The critical and abstract dimensions are underdeveloped, focusing primarily on dataset attributes without broader analysis or evaluation."}}
{"id": "fd615d1a-1d1e-4b41-9cdf-9cd4873ba07b", "title": "EdNet Dataset", "level": "subsubsection", "subsections": [], "parent_id": "8d0ee175-d87a-4c6c-bbdc-2113cd06121b", "prefix_titles": [["title", "A Survey of Knowledge Tracing: Models, Variants, and Applications"], ["section", "Datasets and Baselines"], ["subsection", "Datasets"], ["subsubsection", "EdNet Dataset"]], "content": "The EdNet dataset  is related to the english subject, which is consisted of students' learning records in the multi-platform AI tutoring system Santa in South Korea\\footnote{https://github.com/riiid/ednet}. EdNet collected learning data of students over two years for their preparation of the e TOEIC (Test of English for International Communication) Listening and Reading Test. EdNet is now the largest public dataset in KT field with a total of 131,441,538 learning records from 784,309 students. Besides, it contains various features of students' learning actions, such as the specific learning material they have interacted, how much time they have spent for answering a given exercise. There are four differnet versions of EdNet, respectively named EdNet-KT1, EdNet-KT2, EdNet-KT3, and EdNet-KT4 with different extents. {We note that the students in this dataset may be in different learning states. Therefore we do not give the learning state information in Table \\ref{data}.}\n\\begin{itemize}[leftmargin=*]\n\t\\item{\\textbf{EdNet-KT1.}}\n\tEdNet-KT1 contains students' basic exercise-answering logs. This dataset has 784,309 students, 13,169 exercises, 188 KCs, and a total of  95,293,926 learning records.\n\tExercises in EdNet-KT1 are organized by bundles, i.e., a collection of exercises sharing a common passage, picture or listening material. Therefore, exercises come up in bundles and students have to answer all contained exercises when a bundle is given.\n\t\\item{\\textbf{EdNet-KT2.}}\n\tEdNet-KT2 recorded students' action sequences, which indicated their full learning behaviors. For example, a student who is not confident about the answer may alternately select among several answer choices before submitting. Such learning behaviors can reflect more fine-grained knowledge state of students. EdNet-KT2 contains three kinds of actions: \\textit{enter} when student first receives and views a bundle , \\textit{respond} when the student selects an answer choice to the exercise, and \\textit{submit} when the student submits his final answers to the the given bundle. It is worth noting that EdNet-KT2 is a subset of EdNet-KT1.\n\t\\item{\\textbf{EdNet-KT3.}}\n\tOn the basis of EdNet-KT2, EdNet-KT3 collected more students' learning activities, such as reading explanations or watching lectures. These learning activities have potential impacts on students' knowledge state so that they are valuable to be analyzed.\n\t\\item{\\textbf{EdNet-KT4.}}\n\tIn EdNet-KT4, the very fine details of actions were provided. In particular, the following types of actions are added to EdNet-KT3: erase choice, undo erase choice, play audio, pause audio, play video, pause video, pay, refund, and enroll coupon.\n\\end{itemize}", "cites": [5386], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of the EdNet dataset and its four versions, primarily summarizing its characteristics and the types of data it contains. It integrates the information from the cited paper to some extent, but lacks deeper synthesis or comparison with other datasets. There is minimal critical analysis or abstraction to broader educational data trends or implications for KT research."}}
{"id": "ee9a478b-7e5c-4c3d-a3b7-01ab255d95f7", "title": "Knowledge Tracing with Interpretability", "level": "subsection", "subsections": [], "parent_id": "18821b92-3ccc-4057-9352-24e5c0409dd6", "prefix_titles": [["title", "A Survey of Knowledge Tracing: Models, Variants, and Applications"], ["section", "Future Research Directions"], ["subsection", "Knowledge Tracing with Interpretability"]], "content": "The performance of KT models is now evaluated indirectly through the student performance prediction task. The higher the precision of students' responses on future exercises, the better the KT model's performance. Nonetheless, interpretability plays a significant role in education, as students often express more concern about the 'why' than the 'what' of a learning decision . Enhancing the interpretability of KT models is therefore crucial. Some educational theories, such as the Rasch model used in AKT  and the transfer of knowledge used in SKT , could be considered for this purpose.   noticed the significance of interpretable KT, particularly for deep learning models, as they have numerous parameters that are challenging to explain meaningfully. Thus, they  introduced a straightforward and interpretable KT approach, based on the causal relationships within the latent features extracted from students' behavioral data. \n attempted to introduce causal inference for explanatory analysis on KT, and they achieved more stable and explainable knowledge tracing based on the analysis results. \nIt is imperative to further refine existing KT models or to explore additional methods for interpretable KT researches. This will lead to the production of more accurate and interpretable evaluations of students' knowledge states.", "cites": [5374, 8914], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the cited papers by identifying the shared theme of interpretability in KT models and how deep learning-based approaches lack meaningful explanations. It offers a basic critique of this limitation and points to causal inference and educational theories as potential solutions. While it moves beyond mere description, it does not fully abstract or compare approaches in depth, nor does it propose a novel framework."}}
{"id": "203ad4ee-8eb1-4e57-ac27-fd2974c1a08f", "title": "Knowledge Tracing with Sparse Learning Interactions", "level": "subsection", "subsections": [], "parent_id": "18821b92-3ccc-4057-9352-24e5c0409dd6", "prefix_titles": [["title", "A Survey of Knowledge Tracing: Models, Variants, and Applications"], ["section", "Future Research Directions"], ["subsection", "Knowledge Tracing with Sparse Learning Interactions"]], "content": "The acquisition of high-quality KT models necessarily requires a substantial amount of data to ensure training stability.  However, online learning systems often achieve less learning interactions in practical educational scenarios, thereby leading to the data sparsity problem. To address this issue,  utilized graph convolutional network to include exercise-KC correlations. \n turned to improve the attention mechanism.\nMore recently, researchers have employed contrastive learning to alleviate the data sparsity problem in KT . For example,  presented a contrastive learning framework to enhance KT, which measure the semantical similarity between various learning interactions to effectively learn their representations. They further designed data augmentation methods to enhance the semantics of students' learning interactions. However, while the aforementioned methods alleviate the data sparsity problem in various aspects, there remains a need for further improvement in addressing this issue comprehensively.", "cites": [5388, 5387, 8915], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the cited works by highlighting the common challenge of data sparsity in KT and how each paper addresses it using different techniques (graph convolution, attention, contrastive learning). It offers a basic analytical perspective by noting that while these approaches help, they are not fully comprehensive. However, the analysis remains at a surface level and does not deeply compare or critique the methods in terms of effectiveness or trade-offs."}}
{"id": "c7707957-a296-4aea-b19d-29d5e30c34c6", "title": "Knowledge Tracing with Large Language Models", "level": "subsection", "subsections": [], "parent_id": "18821b92-3ccc-4057-9352-24e5c0409dd6", "prefix_titles": [["title", "A Survey of Knowledge Tracing: Models, Variants, and Applications"], ["section", "Future Research Directions"], ["subsection", "Knowledge Tracing with Large Language Models"]], "content": "{In recent years, the advancement of large language models (LLM), notably ChatGPT, has led to significant impacts and garnered considerable research interest worldwide . The use of LLM in education is promising to revolutionize the current learning pattern . The current study mainly focuses on generating learning materials, improving student-system interaction, and explaining educational contents. However, it remains  unclear how LLM can assist in understanding students' knowledge states. Given the powerful capabilities of LLM, it has the potential to enhance the generalization and interpretability of existing KT methods. Specifically, LLM can analyze the content of students' responses and evaluate their quality, which directly reflects students' knowledge states. Besides, LLM  can play the role of the instructor, answering questions for students and identifying their strengths and weaknesses. Furthermore, LLM itself can serve as a KT model, which can output reasonable results about students' knowledge states given their previous learning interactions.\nSimultaneously, it is imperative to safeguard the privacy and security of student data when utilizing LLM . \n}", "cites": [8461], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of how large language models (LLMs) could be used in knowledge tracing, drawing from the cited survey on LLMs to frame their capabilities. It connects LLM advancements to potential KT improvements, showing some synthesis and abstraction by identifying broader roles such as content analysis and instruction. However, it lacks deeper critical evaluation or comparison of specific models or limitations, relying on general observations."}}
