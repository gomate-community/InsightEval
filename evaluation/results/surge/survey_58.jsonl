{"id": "4eed61c4-9fdd-442f-8c8c-ebc1eee50d18", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "1da83c5d-fa27-4a53-a17c-f942c45c275a", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Introduction"]], "content": "\\label{sec:1}\nRecent advancement in deep learning has led to great improvements on many different domains, such as image classification , object detection , semantic segmentation  and others. Despite their impressive ability for representation learning , it is shown that these powerful models can overfit to even complete random noise . Various works are devoted to explain this phenomenon , yet regularizing deep neural networks (DNNs) while avoiding overfitting stays to be an important challenge. It gets even more crucial when there exists noise in data. Therefore, various methods are proposed in the literature to train deep neural networks effectively in the presence of noise.\nThere are two kinds of noise in the literature: feature noise and label noise . Feature noise corresponds to the corruption in observed data features, while label noise means the change of label from its actual class. Even though both noise types may cause a significant decrease in the performance , label noise is considered to be more harmful  and shown to deteriorate the performance of classification systems in a broad range of problems . This is due to several factors; the label is unique for each data while features are multiple, and the importance of each feature varies while the label always has a significant impact . This work focuses on label noise; therefore, noise and label noise is used synonymously throughout the article.\nThe necessity of an excessive amount of labeled data for supervised learning is a significant drawback since it requires an expensive dataset collection and labeling process. To overcome this issue, cheaper alternatives have emerged. For example, an almost unlimited amount of data can be collected from the web via search engines or social media. Similarly, the labeling process can be crowdsourced with the help of systems like Amazon Mechanical Turk\\footnote{http://www.mturk.com}, Crowdflower\\footnote{http://crowdflower.com}, which decrease the cost of labeling notably. Another widely used approach is to label data with automated systems. However, all these approaches led to a common problem; label noise. Besides these methods, label noise can occur even in the case of expert annotators. Labelers may lack the necessary experience, or data can be too complex to be correctly classified, even for the experts. Moreover, label noise can also be introduced to data for adversarial poisoning purposes . Being a natural outcome of dataset collection and labeling process makes label noise robust algorithms an essential topic for the development of efficient computer vision systems.  \nSupervised learning with label noise is an old phenomenon with three decades of history . An extensive survey about relatively old machine learning techniques under label noise is available . However, no work is proposed to provide a comprehensive survey on classification methods centered around deep learning in the presence of label noise. This work focuses explicitly on filling this absence. Even though deep networks are considered to be relatively robust to label noise , they have an immense capacity to overfit data . Therefore, preventing DNNs to overfit noisy data is very important, especially for fail-safe applications, such as automated medical diagnosis systems. Considering the significant success of deep learning over its alternatives, it is a topic of interest, and many works are presented in the literature. Throughout the paper, these methods are briefly explained and grouped to provide the reader with a clear overview of the literature.\nThis paper is organized as follows. Section \\ref{preliminaries} explains several concepts that are used throughout the paper. Proposed solutions in literature are categorized into two major groups, and these methods are discussed in \\autoref{noisemodelbased} - \\autoref{noisemodelfree}. In \\autoref{experiments} widely used experimental setups are presented, and leaderboard on a benchmarking dataset is provided. Finally, \\autoref{conclusion} concludes the paper.", "cites": [1223, 1749, 514, 3866, 802, 97, 810, 3630, 4165, 209, 4115, 1191], "cite_extract_rate": 0.5454545454545454, "origin_cites_number": 22, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 2.0, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a general background on deep learning applications in computer vision and introduces the problem of label noise. It cites several foundational papers in image classification and segmentation but primarily uses them to support a descriptive overview rather than synthesize or critically analyze their contributions. The narrative is coherent but lacks deeper integration or evaluation of the cited works."}}
{"id": "417c96e1-e709-450a-8184-e9c4bbb11452", "title": "Label Noise Models", "level": "subsection", "subsections": [], "parent_id": "e2235ea5-b76e-4028-994c-f569fa5a00ad", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Preliminaries"], ["subsection", "Label Noise Models"]], "content": "\\label{labelnoisemodels}\nA detailed taxonomy of label noise is provided in . In this work, we follow the same taxonomy with a little abuse of notation. Label noise can be affected by three factors: data features, the true label of data, and the labeler characteristics. According to the dependence of these factors, label noise can be categorized into three subclasses.\n\\textit{Random noise} is totally random and depends on neither instance features nor its true class. With a given probability $p_e$ label is changed from its true class. \\textit{Y-dependent noise} is independent of image features but depends on its class; $p_e = p(e|y)$. That means data from a particular class are more likely to be mislabeled. For example, in a handwritten digit recognition task, \"3\" and \"8\" are much more likely to be confused with each other rather than \"3\" and \"5\". \\textit{XY-dependent noise} depends on both image features and its class; $p_e = p(e|x,y)$. As in the y-dependent case, objects from a particular class may be more likely to be mislabeled. Moreover, the chance of mislabeling may change according to data features. If an instance has similar features to another instance from another class, it is more likely to be mislabeled. Generating xy-dependent synthetic noise is harder than the previous two models; therefore, some works tried to provide a generic framework by either checking the complexity of data  or their position in feature space . All these types of noises are illustrated in \\autoref{fig:tsneplots}\nThe case of multi-labeled data, in which each instance has multiple labels given by different annotators, is not considered here. In that scenario, works show that modeling each labeler's characteristics and using this information during training significantly boosts the performance . However, various characteristics of different labelers can be explained with given noise models. For example, in a crowd-sourced dataset, some labelers can be total spammers who label with a random selection ; therefore, they can be modeled as random noise. On the other hand, labelers with better accuracies than random selection can be modeled by y-dependent or xy-dependent noise. As a result, the labeler's characteristic is not introduced as an extra ingredient in these definitions.\n\\begin{figure*}[h]\n    \\centering\n    \\includegraphics[width=\\textwidth]{noisetypes.png}\n    \\caption{T-SNE plot of data distribution of MNIST dataset in feature space for 25\\% noise ratio. a) clean data b) random noise c) y-dependent noise which is still randomly distributed in feature domain d) xy-dependent noise in locally concentrated form e) xy-dependent noise that is concentrated on decision boundaries }\n    \\label{fig:tsneplots}\n\\end{figure*}", "cites": [4166, 4168, 4167], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the taxonomy of label noise from the cited works and integrates key concepts such as the role of data features, true class, and labeler behavior. It offers an analytical perspective by explaining how different types of noise affect model training and how annotator characteristics can be modeled within existing noise frameworks. While it provides a coherent narrative and abstraction by identifying general principles behind label noise types, it could have offered more in-depth critical evaluation of the limitations of the models or comparative analysis of their effectiveness."}}
{"id": "11fcd0cc-694c-4f82-9488-03daa690efe0", "title": "Sources of Label Noise", "level": "subsection", "subsections": [], "parent_id": "e2235ea5-b76e-4028-994c-f569fa5a00ad", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Preliminaries"], ["subsection", "Sources of Label Noise"]], "content": "\\label{sourcesoflabelnoise}\nAs mentioned, label noise is a natural outcome of dataset collection process and can occur in various domains, such as medical imaging , semantic segmentation , crowd-sourcing , social network tagging , financial analysis  and many more. This work focuses on various solutions to such problems, but it may be helpful to investigate the causes of label noise to understand the phenomenon better.\nFirstly, with the availability of the immense amount of data on the web and social media, it is a great interest of computer vision community to make use of that . Nevertheless, labels of these data are coming from messy user tags or automated systems used by search engines. These processes of obtaining datasets are well known to result in noisy labels.\nSecondly, the dataset can be labeled by multiple experts resulting in a multi-labeled dataset. Each labeler has a varying level of expertise, and their opinions may commonly conflict with each other, which results in noisy label problem . There are several reasons to get data labeled by more than one expert. Opinions of multiple labelers can be used to double-check each other's predictions for challenging datasets, or crowd-sourcing platforms can be used to decrease the cost of labeling for big data. Despite its cheapness, labels obtained from non-experts are commonly noisy with a differentiating rate of error. Some labelers even can be a total spammer who labels with random selection .\nThirdly, data can be too complicated for even the experts in the field, e.g., medical imaging. For example, to collect gold standard validation data for retinal images, annotations are gathered from 6-8 different experts . This complexity can be due to the subjectiveness of the task for human experts or the lack of annotator experience. Considering the fields where the accurate diagnosis is of crucial importance, overcoming this noise is of great interest.\nLastly, label noise can intentionally be injected in purpose of regularizing  or data poisoning .", "cites": [4166, 4170, 4169, 4168, 3866, 4172, 4171, 4116, 1191], "cite_extract_rate": 0.4090909090909091, "origin_cites_number": 22, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a basic description of label noise sources and mentions several relevant papers, but it lacks deep synthesis or a unifying narrative. It integrates some ideas (e.g., crowdsourcing and medical imaging challenges) but does so in a surface-level manner. There is minimal critical evaluation or abstraction to broader principles."}}
{"id": "dc1dc4ae-1244-4078-9b61-fd1d656be4b9", "title": "Methodologies", "level": "subsection", "subsections": [], "parent_id": "e2235ea5-b76e-4028-994c-f569fa5a00ad", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Preliminaries"], ["subsection", "Methodologies"]], "content": "\\label{methodologies}\nThere are many possible ways to group proposed methods in the literature. For example, one possible way to distinguish algorithms is according to their need for a noise-free subset of data or not. Alternatively, they can be divided according to the noise type they are dealing with or label type such as singly-labeled or multi-labeled. However, these are not handy to understand the main approaches behind the proposed algorithms; therefore, different sectioning is proposed as noise model based and noise model free methods.\nNoise model based methods aim to model the noise structure so that this information can be used during training to come through noisy labels. In general, approaches in this category aim to extract noise-free information contained within the dataset by either neglecting or de-emphasizing information coming from noisy samples. Furthermore, some methods attempt to reform the dataset by correcting noisy labels to increase the quality of the dataset for the classifier. The performance of these methods is heavily dependent on the accurate estimate of the underlying noise. The advantage of noise model based methods is the decoupling of classification and label noise estimation, which helps them to work with the classification algorithm at hand. Another good side is in the case of prior knowledge about the noise structure, noise model based methods can easily be head-started with this extra information inserted to the system.\nDifferently, noise model free methods aim to develop inherently noise robust strategies without explicit modeling of the noise structure. These approaches assume that the classifier is not too sensitive to the noise, and performance degradation results from overfitting. Therefore, the main focus is given to overfit avoidance by regularizing the network training procedure.\nBoth of the mentioned approaches are discussed and further categorized in \\autoref{noisemodelbased} and \\autoref{noisemodelfree}. \\autoref{table:methods} presents all the mentioned methods to provide a clear picture as a whole. It should be noted that most of the time there are no sharp boundaries among the algorithms, and they may belong to more than one category. However, for the sake of integrity, they are placed in the subclass of most resemblance.\n\\begin{singlespace}\n\\begin{table*}[]\n    \\begin{tabular}{|l|l|}\n    \\hline\n    \\multirow{22}{*}{\\rotatebox[origin=c]{90}{\\textbf{\\nameref{noisemodelbased}}}} & \n    \\begin{tabular}[c]{@{}l@{}}\\textbf{1. \\nameref{noisychannel}}\\\\ \n        \\textit{a.\\nameref{noisychannelexplicit}}: predictions on noisy data , predictions on clean data \\\\\n        easy data \\\\\n        \\textit{b.\\nameref{noisychanneliterative}}: EM , fully connected layer , anchor point estimate \\\\\n        Drichlet-distribution \\\\\n        \\textit{c.\\nameref{noisychannelcomplex}}: noise type estimation , relevance estimation \n    \\end{tabular}\\\\\\cline{2-2} \n    & \\begin{tabular}[c]{@{}l@{}}\\textbf{2. \\nameref{labelnoisecleansing}}\\\\ \n        \\textit{a.\\nameref{labelcleansingclean}}: train on clean set , ensemble , graph-based \\\\\n        \\textit{b.\\nameref{labelcleansingcleannoisy}}: iteratively correct , correct for fine-tune \\\\\n        \\textit{c.\\nameref{labelcleansingnoisy}}: calculate posterior , posterior with compatibility \\\\ consistency with model , ensemble , prototypes , quality embedding \\\\\n        partial labels \n    \\end{tabular}\\\\\\cline{2-2} \n    & \\begin{tabular}[c]{@{}l@{}}\\textbf{3. \\nameref{datasetpruning}}\\\\ \n        \\textit{a.Data pruning} network prediction based , ensemble of filters , according to noise rate \\\\\n        transfer learning , cyclic state , K-means \\\\\n        \\textit{b.Label pruning} semi-supervised learning , relabeling \n    \\end{tabular}\\\\\\cline{2-2} \n    & \\begin{tabular}[c]{@{}l@{}}\\textbf{4. \\nameref{samplechoosing}}\\\\ \n        a.\\textit{\\nameref{curriculumlearning}}: Screening loss , teacher-student \\\\\n        selecting uncertain samples , curriculum loss , data complexity \\\\\n        consistency with model \\\\    \n        b.\\textit{\\nameref{multipleclassifiers}}: Consistency of networks , co-teaching \n    \\end{tabular}\\\\\\cline{2-2} \n    & \\begin{tabular}[c]{@{}l@{}}\\textbf{5. \\nameref{sampleimportance}}\\\\ \n        Meta task , siamese network , pLOF , abstention \\\\\n        estimate noise rate , similarity loss , transfer learning , $\\theta$-distribution \n    \\end{tabular}\\\\\\cline{2-2} \n    & \\begin{tabular}[c]{@{}l@{}}\\textbf{6. \\nameref{labelerquality}}\\\\ \n        EM , trace regularizer , crowd-layer , image difficulty estimate \\\\\n        consistency with network , omitting probability variable \\\\\n        softmax layer per labeler \n    \\end{tabular}\\\\\\hline\n    \\multirow{14}{*}{\\rotatebox[origin=c]{90}{\\textbf{\\nameref{noisemodelfree}}}}  & \n    \\begin{tabular}[c]{@{}l@{}}\\textbf{1. \\nameref{robustlosses}}\\\\ \n        Non-convex loss functions , 0-1 loss surrogate , MAE , IMEA \\\\\n        Generalized cross-entropy , symmetric loss , unbiased estimator , \\\\\n        modified cross-entropy for omission , information theoric loss \\\\\n        linear-odd losses , classification calibrated losses  , SGD with robust losses \n    \\end{tabular}\\\\\\cline{2-2} \n    & \\begin{tabular}[c]{@{}l@{}}\\textbf{2. \\nameref{metalearning}}\\\\ \n        Choosing best methods , pumpout , noise tolerant parameter initialization , \\\\\n        knowledge distillation , gradient magnitude adjustment , meta soft labels \n    \\end{tabular}\\\\\\cline{2-2} \n    & \\begin{tabular}[c]{@{}l@{}}\\textbf{3. \\nameref{regularizers}}\\\\ \n        Dropout , adversarial training , mixup , label smoothing \\\\\n        pre-training , dropout on final layer , checking dimensionality \\\\\n        , auxiliary image regularizer \n    \\end{tabular}\\\\\\cline{2-2} \n    & \\begin{tabular}[c]{@{}l@{}}\\textbf{4. \\nameref{ensemblemethods}}\\\\ \n        LogitBoost\\&BrownBoost , noise detection based AdaBoost , rBoost \\\\\n        RBoost1\\&RBoost2 , robust multi-class AdaBoost \n    \\end{tabular}\\\\\\cline{2-2} \n    & \\begin{tabular}[c]{@{}l@{}}\\textbf{5. \\nameref{others}}\\\\ \n        Complementary labels , autoencoder reconstruction error \\\\\n        minimum covariance determinant , less noisy data , \\\\\n        data quality , prototype learning , multiple instance learning \n    \\end{tabular}\\\\\\hline \n    \\end{tabular}\n    \\caption{Existing methods to deal with label noise in the literature}\n    \\label{table:methods}\n\\end{table*}\n\\end{singlespace}", "cites": [7779, 4166, 7782, 4137, 3345, 4134, 301, 4186, 4168, 4184, 7772, 7162, 4180, 4145, 4128, 4171, 4178, 4119, 4131, 4187, 8736, 2277, 3340, 7783, 4175, 7191, 4141, 4126, 4156, 4142, 4174, 4181, 8737, 4138, 4135, 892, 8742, 7780, 8630, 4185, 4136, 4129, 795, 3342, 4182, 4177, 4121, 4173, 4130, 4139, 4238, 4183, 4152, 4253, 4133, 7781, 4188, 4140, 7133, 4132, 3454, 7775, 4179, 3453, 4143, 8743, 8741, 4176, 7774], "cite_extract_rate": 0.5948275862068966, "origin_cites_number": 116, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a structured categorization of methods into 'noise model based' and 'noise model free' groups and lists various approaches under each. It offers some basic synthesis by grouping related techniques. However, it lacks critical analysis or deeper evaluation of the methods, and while it identifies general categories, it does not abstract broader principles or trends in the literature."}}
{"id": "ff7b16cb-76ae-4fa0-9e65-b2494ca8b5bd", "title": "Noisy Channel", "level": "subsection", "subsections": ["c66ea713-c060-4335-bdf2-eab57106d284", "2146baee-5b49-44c5-a416-7c31d548ab10", "437e0291-839d-4bf9-bf2d-4d0ec1de2e48"], "parent_id": "6e51ea88-3a48-46e6-897f-ede2d47445fa", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Noise Model Based Methods"], ["subsection", "Noisy Channel"]], "content": "\\label{noisychannel}\nThe general setup for the noisy channel is illustrated in \\autoref{fig:noisychannel}. Methods belonging to this category minimize the following risk\n\\begin{equation}\n    \\hat{R}_{l,\\mathcal{D}}(f)=\\dfrac{1}{N}\\sum_{i=1}^{N}l(Q(f_\\theta(x_i)),\\tilde{y_i})\n\\end{equation}\nwhere $Q(f_\\theta(x_i)) = p(\\tilde{y_i}|f_\\theta(x_i))$ is the mapping from network predictions to given noisy labels. If $Q$ adapts the noise structure $p(\\tilde{y}|y)$, then network will be forced to learn true mapping $p(y|x)$. \n$Q$ can be formulated with a \\textit{noise transition matrix} $T$ so that $Q(f_\\theta(x_i)) = Tf_\\theta(x_i)$ where each element of the matrix represents the transition probability of given true label to noisy label, $T_{ij}=p(\\tilde{y}=j|y=i)$. Since $T$ is composed of probabilities, weights coming from a single node should sum to one $\\sum_{j}T_{ij}=1$. This procedure of correcting predictions to match given label distribution is also called \\textit{loss-correction} . \nA common problem in noisy channel estimation is scalability. As the number of classes increases, the size of the noise transition matrix increases exponentially, making it intractable to calculate. This can be partially avoided by allowing connections only among the most probable nodes , or predefined nodes . These restrictions are determined by human experts, which allows additional noise information to be inserted into the training procedure.  \nThe noisy channel is used only in the training phase. In the evaluation phase, the noisy channel is removed to get noise-free predictions of the base classifier. In these kinds of approaches, performance heavily depends on the accurate estimation of noisy channel parameters; therefore, works mainly focus on estimating $Q$. Various ways of formulating the noisy channel are explained below.\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{noisychannel.png}\n    \\caption{Noise can be modeled as a noisy channel on top of base classifier. Noisy channel adapts the characteristic of the noise so that base classifier is fed with noise-free gradients during traning.}\n    \\label{fig:noisychannel}\n\\end{figure}", "cites": [7773], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the noisy channel framework, integrating the concept of a noise transition matrix as discussed in the cited paper. However, it lacks deeper synthesis of ideas from multiple sources, does not critically evaluate the methods or their limitations, and offers minimal abstraction beyond the specific formulation of the noisy channel. It remains largely descriptive without offering a comparative or analytical perspective."}}
{"id": "c66ea713-c060-4335-bdf2-eab57106d284", "title": "Explicit calculation", "level": "subsubsection", "subsections": [], "parent_id": "ff7b16cb-76ae-4fa0-9e65-b2494ca8b5bd", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Noise Model Based Methods"], ["subsection", "Noisy Channel"], ["subsubsection", "Explicit calculation"]], "content": "\\label{noisychannelexplicit} Noise transition matrix is calculated explicitly, and then the base classifier is trained using this matrix. Assuming dataset is balanced in terms of clean representative samples and noisy samples, so that there exists samples for each class with $p(y=\\tilde{y}_i|x_i)=1$,  constructs $T$ just based on noisy class probability estimates of a pre-trained model, so-called \\textit{confusion matrix}. A similar approach is followed in ; however, the noise transition matrix is calculated from the network's confusion matrix on the clean subset of data. Two datasets are gathered in , namely: easy data and hard data. The classifier is first trained on the easy data to extract similarity relationships among classes. Afterward, the calculated similarity matrix is used as the noise transition matrix. Another method proposed in  calculates the confusion matrix on both noisy data and clean data. Then, the difference between these two confusion matrices gives $T$.", "cites": [4145, 2277, 4136], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of explicit calculation methods for the noise transition matrix by summarizing techniques from the cited papers. It integrates these methods under the common theme of using confusion matrices but lacks deeper critical analysis or comparison of their strengths and weaknesses. The abstraction is limited to identifying the general idea of using similarity or confusion matrices without broader conceptual generalization."}}
{"id": "2146baee-5b49-44c5-a416-7c31d548ab10", "title": "Iterative calculation", "level": "subsubsection", "subsections": [], "parent_id": "ff7b16cb-76ae-4fa0-9e65-b2494ca8b5bd", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Noise Model Based Methods"], ["subsection", "Noisy Channel"], ["subsubsection", "Iterative calculation"]], "content": "\\label{noisychanneliterative} Noise transition matrix is estimated incrementally during the training of the base classifier. In  expectation-maximization (EM)  is used to iteratively train network to match given distribution and estimate noise transition matrix given the model prediction. The same approach is used on medical data with noisy labels in .  adds a linear fully connected layer as a last layer of the base classifier, which is trained to adapt noise behavior. To avoid this additional layer to converge the identity matrix and base classifier overfitting the noise, the weight decay regularizer is applied to this layer.  suggests using class probability estimates on anchor points (data points that belong to a specific class almost surely) to construct the noise transition matrix. In the absence of a noise-free subset of data, anchor points are extracted from data points with high noisy class posterior probabilities. Then, the matrix is updated iteratively to minimize loss during training. Instead of using softmax probabilities,  models noise transition matrix in Bayesian form by projecting it into a Dirichlet-distributed space.", "cites": [4136, 4152], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a factual description of iterative methods for estimating the noise transition matrix across different papers, but lacks a coherent synthesis or comparison of the approaches. It briefly mentions techniques such as EM, additional fully connected layers, and anchor points, yet does not critically analyze their strengths, weaknesses, or implications. There is minimal abstraction or identification of broader patterns in the methods discussed."}}
{"id": "437e0291-839d-4bf9-bf2d-4d0ec1de2e48", "title": "Complex noisy channel", "level": "subsubsection", "subsections": [], "parent_id": "ff7b16cb-76ae-4fa0-9e65-b2494ca8b5bd", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Noise Model Based Methods"], ["subsection", "Noisy Channel"], ["subsubsection", "Complex noisy channel"]], "content": "\\label{noisychannelcomplex} Different then simple confusion matrix, some works formalize the noisy channel as a more complex function. This enables noisy channel parameters to be calculated not just by using network outputs but additional information about the content of data. For example, three types of label noises are defined in , namely: no noise, random noise, structured noise. An additional convolutional neural network (CNN) is used to interpret the noise type of each sample. Finally, the noisy layer aims to match predicted labels to noisy labels with the help of predicted noise type. Another work in  proposes training an extra network as a relevance estimator, which attains the label's relevance to the given instance. Predicted labels are mapped to noisy labels with the consideration of relevance. If relevance is low, in case of noise, the classifier can still make predictions of true class and doesn't get penalized much for it.", "cites": [4174], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of complex noisy channel methods, mentioning two approaches: one that uses an additional CNN to determine noise type and another that introduces a relevance estimator. However, it lacks deeper synthesis of these ideas, critical evaluation of their strengths or weaknesses, and abstraction to broader principles or frameworks. The narrative remains largely factual and limited in analytical depth."}}
{"id": "46311c1d-0eda-477f-b3bb-af8c05c5cec7", "title": "Label Noise Cleaning", "level": "subsection", "subsections": ["3a6e480e-6188-4e77-8875-fd46cf09ba43", "9bb5a874-af0f-4df0-a75a-43e1fe801ec1", "8c410080-caf1-4216-91e3-bb11d24825f2"], "parent_id": "6e51ea88-3a48-46e6-897f-ede2d47445fa", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Noise Model Based Methods"], ["subsection", "Label Noise Cleaning"]], "content": "\\label{labelnoisecleansing}\nAn obvious solution to noisy labels is to identify and correct suspicious labels to their corresponding true classes. Cleaning the whole dataset manually can be costly; therefore, some works propose to pick only suspicious samples to be sent to a human annotator to reduce the cost . However, this is still not a scalable approach. As a result, various algorithms are proposed in the literature. Including the label correction algorithm, the empirical risk takes the following form\n\\begin{equation}\n    \\hat{R}_{l,\\mathcal{D}}(f)=\\dfrac{1}{N}\\sum_{i=1}^{N}l(f_\\theta(x_i),G(\\tilde{y_i},x_i))\n\\end{equation}\nwhere $G(\\tilde{y_i},x_i)=p(y_i|\\tilde{y_i},x_i)$ represents the label cleaning algorithm. Label cleaning algorithms rely on a feature extractor to map data to the feature domain to investigate noisiness. While some works use a pre-trained network as the feature extractor, others use the base classifier as it gets more and more accurate during training. This approach results in an iterative framework: as the classifier gets better, the label cleaning is more accurate, and as the label quality gets better, the classifier gets better. From this point of view, label cleaning can be viewed as a dynamically evolving component of the system instead of preprocessing of the data. Such methods usually tackle the difficulty of distinguishing informative hard samples from those with noisy labels . As a result, they can end up removing too many samples or changing labels in a delusional way. Approaches for label cleaning can be separated according to their need for clean data or not.", "cites": [4116], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section introduces the concept of label noise cleaning and outlines a general framework using a probabilistic correction function, but it only cites one paper and integrates limited information from it. There is some abstraction in framing label cleaning as an iterative and dynamic process, but the synthesis remains basic with little connection to broader trends or multiple sources. Critical analysis is minimal, as the section only briefly mentions potential pitfalls without deeper evaluation of the cited work."}}
{"id": "3a6e480e-6188-4e77-8875-fd46cf09ba43", "title": "Using data with clean labels", "level": "subsubsection", "subsections": [], "parent_id": "46311c1d-0eda-477f-b3bb-af8c05c5cec7", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Noise Model Based Methods"], ["subsection", "Label Noise Cleaning"], ["subsubsection", "Using data with clean labels"]], "content": "\\label{labelcleansingclean} In the existence of a clean subset of data, the aim is to fuse noise-free label structure to noisy labels for correction. If the clean subset is large enough to train a network, one obvious way is to relabel noisy labels by predictions of the network trained on clean data. For relabeling,  uses alpha blending of given noisy labels and predicted labels. An ensemble of networks trained with different subsets of the dataset is used in . If they all agree on the label, it is changed to the predicted label; otherwise, it is set to a random label. Instead of keeping the noisy label, setting it randomly helps break the noise structure and makes noise more uniformly distributed in label space. In  a graph-based approach is used, where a conditional random field extracts relation among noisy labels and clean labels.", "cites": [8743, 4180], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of methods that utilize clean-labeled data to correct noisy labels, mentioning three approaches with minimal integration of their underlying principles. It lacks critical evaluation of the strengths and limitations of the cited works and does not generalize to broader patterns or principles in label noise cleaning. The narrative remains focused on summarizing the methods rather than offering deeper insights."}}
{"id": "9bb5a874-af0f-4df0-a75a-43e1fe801ec1", "title": "Using data with both clean and noisy labels", "level": "subsubsection", "subsections": [], "parent_id": "46311c1d-0eda-477f-b3bb-af8c05c5cec7", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Noise Model Based Methods"], ["subsection", "Label Noise Cleaning"], ["subsubsection", "Using data with both clean and noisy labels"]], "content": "\\label{labelcleansingcleannoisy} Some works rely on a subset of data, for which both clean and noisy labels are provided. Then label noise structure is extracted from these conflicting labels and used to correct noisy data. In , the label cleaning network gets two inputs: extracted features of instances by the base classifier and corresponding noisy labels. Label cleaning network and base classifier are trained jointly so that label cleaning network learns to correct labels on the clean subset of data and provides corrected labels for base classifier on noisy data. Same approach is decoupled in  in teacher-student manner. First, the student is trained on noisy data. Then features are extracted from the clean data via the student model, and the teacher learns the structure of noise depending on these extracted features. Afterward, the teacher predicts soft labels for noisy data, and the student is again trained on these soft labels for fine-tuning.", "cites": [4178], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of two methods for label noise cleaning using both clean and noisy data. It synthesizes the general approach from the cited paper but lacks deeper integration or contrast with other methods. There is no critical evaluation or identification of broader principles, keeping the insight level low."}}
{"id": "8c410080-caf1-4216-91e3-bb11d24825f2", "title": "Using data with just noisy labels", "level": "subsubsection", "subsections": [], "parent_id": "46311c1d-0eda-477f-b3bb-af8c05c5cec7", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Noise Model Based Methods"], ["subsection", "Label Noise Cleaning"], ["subsubsection", "Using data with just noisy labels"]], "content": "\\label{labelcleansingnoisy} Noise-free data is not always available, so the primary approach in this situation is to estimate cleaner posterior label distribution incrementally. However, there is a possible undesired solution to this approach so that all labels are attained to a single class and base network predicting constant class, which would result in delusional top training accuracy. Therefore, additional regularizers are commonly used to make label posterior distribution even. A joint optimization framework for both training base classifier and propagating noisy labels to cleaner labels is presented in . Using expectation-maximization, both classifier parameters and label posterior distribution is estimated to minimize the loss. A similar approach is used in  with additional compatibility loss conditioned on label posterior. Considering noisy labels are in the minority, this term assures posterior label distribution does not diverge too much from the given noisy label distribution so that majority of the clean label contribution is not lost.  deploy a confidence policy where labels are determined by either network output or given noisy labels, depending on the confidence of the model's prediction. Arguing that, in the case of noisy labels, the model first learns correctly labeled data and then overfits to noisy data,  aims to extract the probability of a sample being noisy or not from its loss value. To achieve this, the loss of each instance is fitted by a beta mixture model, which models the label noise in an unsupervised manner.  proposes a two-level approach. In the first stage, with any chosen inference algorithm, the ground truth labels are determined, and data is divided into two subsets as noisy and clean. In the second stage, an ensemble of weak classifiers is trained on clean data to predict true labels of noisy data. Afterward, these two subsets of data are merged to create the final enhanced dataset.  constructs prototypes that can represent deep feature distribution of the corresponding class for each class. Then corrected labels are found by checking similarity among data samples and prototypes.  introduces a new parameter, namely \\textit{quality embedding}, which represents the trustworthiness of data. Depending on two latent variables, true class probability and quality embedding, an additional network tries to extract each instance's true class. In a multi-labeled dataset, where each instance has multiple labels representing its content, some labels may be partially missing resulting in partial labels. In the case of partial labels,  uses one network to find and estimate easy missing labels and another network to be trained on this corrected data.  formulates video anomaly detection as a classification with label noise problem and trains a graph convolutional label noise cleaning network depending on features and temporal consistency of video snippets.", "cites": [4138, 4135, 4186, 4156, 4181, 4184, 4187], "cite_extract_rate": 0.7, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple methods from different papers into a cohesive narrative about training with only noisy labels, highlighting shared strategies like joint optimization, confidence policies, and prototype-based correction. It provides some critical perspective by noting limitations such as the risk of overfitting and the need for regularization. While it identifies some patterns (e.g., use of latent variables and probabilistic modeling), it stops short of offering a comprehensive meta-framework or deeper theoretical critique."}}
{"id": "39942045-6350-433a-98f1-5356b4e8060f", "title": "Removing Data", "level": "subsubsection", "subsections": [], "parent_id": "ca3a3889-4c87-4b50-9671-2b859b435b44", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Noise Model Based Methods"], ["subsection", "Dataset Pruning"], ["subsubsection", "Removing Data"]], "content": "The most straightforward approach is to remove instances misclassified by the base network .  uses an ensemble of filtering methods, where each of them assigns a noisiness level for each sample. Then, these predictions are combined, and data with the highest noisiness level predictions are removed.  extends this work with label correction. If the majority of noise filters predict the same label for the noisy instance, it's label is corrected to the predicted label. Otherwise, it is removed from the dataset. In , with the help of a probabilistic classifier, training data is divided into two subsets: confidently clean and noisy. Noise rates are estimated according to the sizes of these subsets. Finally, relying on the base network's output confidence in data instances, the number of most unconfident samples is removed according to the estimated noise rate. In , transfer learning is used so that network trained on a clean dataset from a similar domain is fine-tuned on the noisy dataset for relabeling. Afterward, the network is again trained on relabeled data to re-sample the dataset to construct a final clean dataset. In , the learning rate is adjusted cyclicly to change network status between underfitting and overfitting. Since, while underfitted, noisy samples cause high loss, samples with large noise during this cyclic process are removed.  first train network on noisy data and extract feature vectors by using this model. Afterward, data is clustered with the K-means algorithm running on extracted features, and outliers are removed.  provides a comparison of performances of various noise-filtering methods for crowd-sourced datasets.", "cites": [795, 7783, 8741], "cite_extract_rate": 0.375, "origin_cites_number": 8, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of different data removal techniques used in noisy label settings but lacks deeper synthesis or comparison of the underlying principles. It briefly mentions each method's approach without critically analyzing their strengths, weaknesses, or implications. There is minimal abstraction to broader themes or patterns in the field."}}
{"id": "1bbb108f-91c4-4fec-897e-71a2bed5fdea", "title": "Removing Labels", "level": "subsubsection", "subsections": [], "parent_id": "ca3a3889-4c87-4b50-9671-2b859b435b44", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Noise Model Based Methods"], ["subsection", "Dataset Pruning"], ["subsubsection", "Removing Labels"]], "content": "The simplest option is to employ straightforward semi-supervised training on labeled and unlabeled data . Alternatively, label removing can be done iteratively in each epoch to update the dataset for better utilization of semi-supervised learning dynamically.  uses consistency among the given label and moving average of model predictions to evaluate if the provided label is noisy or not. Then the model is trained on clean samples on the next iteration. This procedure continues until convergence to the best estimator. The same approach is used in  with a little tweak. Instead of comparing with given labels, the moving average of predictions is compared with predicted labels in the current epoch. To avoid the data selection biased caused by one model,  uses two models to select an unlabeled set for each other. Afterward, each network is trained in a semi-supervised learning manner on the dataset chosen by its peer network. Another approach in this class is to train a network on labeled and unlabeled data and then use it to relabel noisy data . Assuming that correctly labeled data account for the majority,  proposes splitting datasets into labeled and unlabeled subgroups randomly. Then, labels are propagated to unlabeled data using a similarity index among instances. This procedure is repeated to produce multiple labels per instance, and then the final label is set with majority voting.", "cites": [4129, 4253, 3342, 7780], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a clear, descriptive overview of methods for removing noisy labels, drawing from several cited papers. It synthesizes the main ideas, such as semi-supervised learning and label filtering using prediction consistency. However, it lacks deeper critical analysis of the strengths or weaknesses of each method and offers limited abstraction beyond the specific techniques discussed."}}
{"id": "d151439e-cdab-408e-8517-6d43d77a1e9a", "title": "Curriculum Learning", "level": "subsubsection", "subsections": [], "parent_id": "1f132fef-34b6-4647-964b-3113bd5a48e6", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Noise Model Based Methods"], ["subsection", "Sample Choosing"], ["subsubsection", "Curriculum Learning"]], "content": "\\label{curriculumlearning} Curriculum learning (CL) , inspired from human cognition, proposes to start from easy samples and go through harder samples to guide training. This learning strategy is also called \\textit{self-paced learning}  when prior to sample hardness is not known and inferred from the loss of the current model on that sample. In the noisy label framework, clean labeled data can be accepted as an easy task, while noisily labeled data is the harder task. Therefore, the idea of CL can be transferred to label noise setup as starting from confidently clean instances and go through noisier samples as the classifier gets better. Various screening loss functions are proposed in  to sort instances according to their noisiness level. The teacher-student approach is implemented in , where the teacher's task is to choose confidently clean samples for the student. Instead of using a predefined curriculum, the teacher constantly updates its curriculum depending on the student's outputs. Arguing that CL slows down the learning speed, since it focuses on easy samples,  suggests choosing uncertain samples that are mispredicted sometimes and correctly on others during training. These samples are assumed to be probably not noisy since noisy samples should be mispredicted all the time. Arguing that it is hard to optimize 0-1 loss, \\textit{curriculum loss} that chooses samples with low loss values for loss calculation, is proposed as an upper bound for 0-1 loss in . In , data is split into subgroups according to their complexities. Since less complex data groups are expected to have more clean labels, training will start from less complex data and go through more complex instances as the network gets better. Next samples to be trained on can be chosen by checking the consistency of the label with the network prediction. In , if both label and model prediction of the given sample is consistent, it is used in the training set. Otherwise, the model has a right to disagree. Iteratively this provides better training data and a better model. However, there is a risk of the model being too skeptical and choosing labels in a delusional way; therefore, consistency balance should be established.", "cites": [7775, 4139, 4188], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates multiple papers under the theme of curriculum learning in the context of noisy labels, explaining how the approach is adapted and applied. It identifies different strategies, such as teacher-student frameworks and consistency checks, and discusses potential limitations like model skepticism. However, it lacks deeper comparative analysis or a more abstract, overarching framework to unify the insights."}}
{"id": "59741d1f-f243-46b9-bbf7-5974cc60aa60", "title": "Multiple Classifiers", "level": "subsubsection", "subsections": [], "parent_id": "1f132fef-34b6-4647-964b-3113bd5a48e6", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Noise Model Based Methods"], ["subsection", "Sample Choosing"], ["subsubsection", "Multiple Classifiers"]], "content": "\\label{multipleclassifiers} Some works use multiple classifiers to help each other to choose the next batch of data to train on. This is different from the teacher-student approach since none of the networks supervise the other but rather help each other out. Multiple models can provide robustness since networks can correct each other's mistakes due to their differences in learned representations. For this setup to work, the initialization of the classifiers is essential. They are most likely to be initialized with a different subset of the data. If they have the same weight initializations, there is no update since they agree to disagree with labels. In  label is assumed to be noisy if both networks disagree with the given label, and update on model weights happens only when the prediction of two networks conflicts. The paradigm of \\textit{co-teaching} is introduced in , where two networks select the next batch of data for each other. The next batch is chosen as the data batch, which has small loss values according to the pair network. It is claimed that using one network accumulates the noise-related error, whereas two networks filter noise error more successfully. The idea of co-teaching is further improved by iterating over data where two networks disagree to prevent two networks converging each other with the increasing number of epochs . Another work using co-teaching first trains two networks on a selected subset for a given number of epochs and then moves to the full dataset .", "cites": [3340, 4126, 4140, 4137], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers on co-teaching methods, highlighting how they use multiple classifiers to collaboratively filter noisy labels. It provides a coherent narrative by connecting the initialization strategy, disagreement-based filtering, and iterative improvements across the cited works. While it includes some critical analysis (e.g., noting limitations of co-teaching over time), it stops short of deep evaluation, and the abstraction is moderate, identifying general patterns like the role of disagreement and iteration but not offering a comprehensive meta-framework."}}
{"id": "66c1ab62-f503-459d-aa3a-607aae88807a", "title": "Sample Importance Weighting", "level": "subsection", "subsections": [], "parent_id": "6e51ea88-3a48-46e6-897f-ede2d47445fa", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Noise Model Based Methods"], ["subsection", "Sample Importance Weighting"]], "content": "\\label{sampleimportance}\nSimilar to sample choosing, training can be made more effective by assigning weights to instances according to their estimated noisiness level. This has an effect of emphasizing cleaner instances for a better update on model weights. Following empirical risk is minimized by these algorithms.\n\\begin{equation}\n    \\hat{R}_{l,\\mathcal{D}}(f)=\\dfrac{1}{N}\\sum_{i=1}^{N}\\beta(x_i,y_i)l(f_\\theta(x_i),\\tilde{y_i}))\n\\end{equation}\nwhere $\\beta(x_i,y_i)$ determines the instance dependent weight. If $\\beta$ would be binary, then formulation is the same with sample choosing, as explained in \\autoref{samplechoosing}. Differently, here $\\beta$ is not binary and has a different value for each instance. Like in sample choosing algorithms, $\\beta$ is a dynamic function, which means weights for instances keep changing during the training. Therefore, it is commonly a challenge to prevent $\\beta$ changing too rapidly and sharply, such that it disrupts the stabilized training loop. Moreover, these methods commonly suffer from accumulated errors. As a result, they can easily get biased towards a certain subset of data. There are various methods proposed to obtain optimal $\\beta$ to fade away the negative effects of noise.\nThe simplest approach would be, in case of availability of both clean and noisy data, weighting clean data more . However, this utilizes information poorly; moreover, clean data is not always available. Works of  and , uses meta-learning paradigm to determine the weighting factor. In each iteration, gradient descent step on the given mini-batch for weighting factor is performed so that it minimizes the loss on clean validation data. A similar method is adopted in , but instead of implicit calculation of the weighting factor, multi layer perceptron (MLP) is used to estimate the weighting function. \\textit{Open-set noisy labels}, where data samples associated with noisy labels might belong to a true class that is not present in the training data, are considered in . Siamese network is trained to detect noisy labels by learning discriminative features to apart clean and noisy data. Noisy samples are iteratively detected and pulled from clean samples. Then, each iteration weighting factor is recalculated for noisy samples, and the base classifier is trained on the whole dataset.  also iteratively separates noisy samples and clean samples. On top of that, not to miss valuable information from clean hard samples, noisy data are weighted according to their noisiness level, estimated by pLOF .  introduces \\textit{abstention}, which gives option to abstain samples, depending on their loss value, with an abstention penalty. Therefore, the network learns to abstain from confusing samples, and with the abstention penalty, the tendency to abstain can be adjusted. In , weighting factor is conditioned on distribution of training data, $\\beta(X,Y)= P_\\mathcal{D}(X,Y)/P_{\\mathcal{D}_n}(X,\\tilde{Y})$. The same methodology is extended to the multi-class case in . In , the weighting factor is determined by checking instance similarity to its representative class prototype in the feature domain.  formulates the problem as transfer learning where the source domain is noisy data, and the target domain is a clean subset of data. Then weighting in the source domain is arranged in a way to minimize target domain loss.  uses $\\theta$ values of samples in $\\theta$-distribution to calculate their probability of being clean and use this information to weight clean samples more in training. \n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{vss.png}\n    \\caption{Illustration of different types of algorithms. Starting from left; 1) representation of samples from a single class in the 2d-space. Green samples represent the clean samples, and red ones represent noisy samples. 2) label noise cleaning algorithms aim to correct the labels of noisy data 3) dataset pruning methods aim to eliminate noisy data (or just their labels) 4) sample importance weighting algorithms aim to up-weight clean samples and down-weight noisy samples (which is illustrated by size)}\n    \\label{fig:methods}\n\\end{figure}", "cites": [4171, 4238, 4131, 8630, 4173, 4175, 7781, 3453, 4128, 7774, 4136], "cite_extract_rate": 0.8461538461538461, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers effectively, integrating them under the theme of sample importance weighting in the context of noisy labels. It connects methods such as meta-learning, Siamese networks, and transfer learning to show how they contribute to dynamic weighting and noise resilience. The section also identifies limitations like accumulated errors and rapid weight changes, and abstracts the core principle of weighting as a general strategy to mitigate label noise."}}
{"id": "9700ff2a-e064-4ed7-9c11-038312198934", "title": "Labeler Quality Assessment", "level": "subsection", "subsections": [], "parent_id": "6e51ea88-3a48-46e6-897f-ede2d47445fa", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Noise Model Based Methods"], ["subsection", "Labeler Quality Assessment"]], "content": "\\label{labelerquality}\nAs explained in \\autoref{sourcesoflabelnoise}, there can be several reasons for the dataset to be labeled by multiple annotators. Each labeler may have a different level of expertise, and their labels may occasionally contradict each other. This is a typical case in crowd-sourced data  or datasets that require a high level of expertise such as medical imaging . Therefore, modeling and using labeler characteristics can significantly increase performance .\nIn this setup, there are two unknowns; noisy labeler characteristics and ground truth labels. One can estimate both with expectation-maximization . If noise is assumed to be y-dependent, the labeler characteristic can be modeled with a noise transition matrix, just like in \\autoref{noisychannel}.  adds a regularizer to the loss function, which is the sum of traces of annotator confusion matrices, to force sparsity on each labeler's estimated confusion matrix. A similar approach is implemented in , where a crowd-layer is added to the end of the network. In , xy-dependent noise is also considered by taking image complexities into account. Human annotators and computer vision systems are used mutually in , where consistency among predictions of these two components is used to evaluate labelers' reliability.  deals with the noise when the labeler omits a tag in the image. Therefore, instead of the noise transition matrix for labelers, the omitting probability variable is used, which is estimated together with the true class using the expectation-maximization algorithm. Separate softmax layers are trained for each annotator in  and an additional network to predict the true class of data depending on the outputs of labeler specific networks and features of data. This setup enables to model each labeler and their overall noise structure in separate networks.", "cites": [4166, 4168, 4142], "cite_extract_rate": 0.23076923076923078, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers by identifying a common theme of modeling labeler characteristics to improve performance in noisy settings. It abstracts these methods into a general framework involving estimation of labeler noise and true labels, often using EM or confusion matrices. However, the critical analysis is limited, as it primarily describes methods without evaluating their strengths, weaknesses, or comparing them in depth."}}
{"id": "82c6f8a5-6d4e-4080-9fa2-a73cfba30e9e", "title": "Robust Losses", "level": "subsection", "subsections": [], "parent_id": "48d73752-4351-4ff3-8d4c-865c3c729928", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Noise Model Free Methods"], ["subsection", "Robust Losses"]], "content": "\\label{robustlosses}\nA loss function is said to be noise robust if the classifier learned with noisy and noise-free data achieves the same classification accuracy . Algorithms under this section aim to design loss function so that the existence of the noise would not decrease the performance. However, it is shown that noise can badly affect the performance even for the robust loss functions . Moreover, these methods treat both noisy and clean data in the same way, which prevents the utilization of any prior information over data distribution.\nIn , it is shown that certain non-convex loss functions, such as 0-1 loss, has noise tolerance much more than commonly used convex losses. Extending this work  derives sufficient conditions for a loss function to be noise tolerant for uniform noise. Their work shows that, if the given loss function satisfies $\\sum_{k}l(f_\\theta(x),k) = C, \\forall x \\in X$ where $C$ is a constant value, then loss function is tolerant to uniform noise. In this content, they empirically show that none of the standard convex loss functions has noise robustness while 0-1 loss has, up to a certain noise ratio. However, 0-1 loss is non-convex and non-differentiable; therefore, surrogate loss of 0-1 loss is proposed in , which is still noise sensitive. Widely used \\textit{categorical cross entropy (CCE)} loss is compared with \\textit{mean absolute value of error (MAE)} in the work of , where it is shown empirically that mean absolute value of error is more noise tolerant.  shows that the robustness of MEA is due to its weighting scheme. While CCE is sensitive to abnormal samples and produces bigger gradients in magnitude, MAE treats all data points equally, which would result in an underfitting of data. Therefore, \\textit{Improved mean absolute value of error (IMAE)}, which is an improved version of MAE, is proposed in , where gradients are scaled with a hyper-parameter to adjusts weighting variance of MAE.  also argues that MAE provides a much lower learning rate than CCE; therefore, a new loss function is suggested, which combines the robustness of MAE and implicit weighting of CCE. With a tuning parameter, the loss function characteristic can be adjusted in a line from MAE to CCE. Loss functions are commonly not symmetric, meaning that $l(f_\\theta(x_i),y_i) \\neq l(y_i,f_\\theta(x_i))$. Inspired from the idea of symmetric KL-divergence,  proposes symmetric cross entropy loss $l_{SCE}(f_\\theta(x_i),y_i) = l(f_\\theta(x_i),y_i) + l(y_i,f_\\theta(x_i))$ to battle noisy labels. \nGiven that noise prior is known,  provides two surrogate loss functions using the prior information about label noise, namely, an unbiased and a weighted estimator of the loss function.  considers asymmetric omission noise for the binary classification case, where the task is to find road pixels from a satellite map image. Omission noise makes the network less confident about its predictions, so they modified cross-entropy loss to penalize the network less for producing wrong but confident predictions since these labels are more likely to be noisy. Instead of using distance-based loss,  proposes to use information-theoretic loss, in which determinant based mutual information  between given labels and predictions are evaluated for loss calculation. Weakly supervised learning with noisy labels are considered in , and necessary conditions for loss to be noise tolerant are drawn.  shows that classification-calibrated loss functions are asymptotically robust to symmetric label noise. Stochastic gradient descent with robust losses is analyzed in general  and shown to be more robust to label noise than its counterparts.", "cites": [4132, 4130, 3454, 4121, 4182, 4119, 4179, 4189, 8736, 4134], "cite_extract_rate": 0.625, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers to build a coherent narrative on robust loss functions for label noise, linking theoretical conditions (e.g., uniform noise tolerance) with practical loss designs (e.g., IMAE, symmetric cross entropy). It critically evaluates the trade-offs between different loss functions, such as the underfitting of MAE versus the sensitivity of CCE, and highlights limitations of prior approaches. The discussion abstracts from specific methods to highlight broader principles like symmetry in loss functions and the role of weighting schemes in robustness."}}
{"id": "dea1efe8-e2f2-40c6-8b81-931ed7d08532", "title": "Meta Learning", "level": "subsection", "subsections": [], "parent_id": "48d73752-4351-4ff3-8d4c-865c3c729928", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Noise Model Free Methods"], ["subsection", "Meta Learning"]], "content": "\\label{metalearning}\nWith the recent advancements of deep neural networks, the necessity of hand-designed features for computer vision systems are mostly eliminated. Instead, these features are learned autonomously via machine learning techniques. Even though these algorithms can learn complex functions on their own, there remain many hand-designed parameters such as network architecture, loss function, optimizer algorithm, and so on. Meta-learning aims to eliminate these necessities by learning not just the required complex function for the task but also learning the learning itself . Algorithms belonging to this category usually implement an additional learning loop for the meta objective, optimizing the base learning procedure. In general, the biggest drawback of these methods is their computational cost. Since they require nested loops of gradient computations for each training loop, they are several times slower than the conventional training process.\nDesigning a task beyond classical supervised learning in a meta-learning fashion has been used to deal with label noise as well. A meta task is defined as predicting the most suitable method, among the family of methods, for a given noisy dataset in . \\textit{Pumpout}  presents a meta objective as recovering the damage done by noisy samples by erasing their effect on model via \\textit{scaled gradient ascent}. As a meta-learning paradigm, model-agnostic-meta-learning (MAML)  seeks optimal weight initialization that can easily be fine-tuned for the desired objective. A similar mentality is used in  for noisy labels, which aims to find noise-tolerant model parameters that are less prone to noise under teacher-student training framework . Multiple student networks are fed with data corrupted by synthetic noise. A meta objective is defined to maximize consistency with teacher outputs obtained from raw data without synthetic noise. Therefore, student networks are forced to find most noise robust weight initialization such that weight update will still be consistent after training an epoch on synthetically corrupted data. Then, final classifier weights are set as an exponential moving average of student networks. \nAlternatively, in the case of available clean data, a meta objective can be defined to utilize this information. The approach used in  is to train a teacher network in a clean dataset and transfer its knowledge to the student network to guide the training process in the presence of mislabeled data. They used \\textit{distillation} technique proposed in  for controlled transfer of knowledge from teacher to student. A similar methodology of using \\textit{distillation} and label correction in the human pose estimation task is implemented in . In , the target network is trained on excessive noisy data, and the confidence network is trained on a clean subset. Inspiring from , the confidence network's task is to control the magnitude of gradient updates to the target network so that noisy labels are not resulting in updating gradients.  uses clean data to produce soft labels for noisy data, for which the classifier trained on it would give the best performance on the clean data. As a result, it seeks optimal label distribution to provide the most noise robust learning for the base classifier.", "cites": [7779, 7740, 4091, 4143, 681, 3345, 4185, 1695, 7772, 8737], "cite_extract_rate": 0.7692307692307693, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a coherent synthesis of meta-learning approaches for handling noisy labels by connecting ideas across multiple papers, such as the use of synthetic noise in student networks, the role of distillation, and the integration of meta-learning for label correction. It includes some critical discussion, particularly about the computational cost of meta-learning. While it identifies patterns in the use of teacher-student frameworks and gradient control, it stops short of offering a deeper meta-level abstraction of the field."}}
{"id": "28012b01-301a-4708-92c3-cd44f9415683", "title": "Regularizers", "level": "subsection", "subsections": [], "parent_id": "48d73752-4351-4ff3-8d4c-865c3c729928", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Noise Model Free Methods"], ["subsection", "Regularizers"]], "content": "\\label{regularizers}\nRegularizers are well known to prevent DNNs from overfitting noisy labels. From this perspective, these methods treat performance degradation due to noisy data as overfitting to noise. Even though this assumption is mostly valid in random noise, it may not be the case for more complex noises. Some widely used techniques are weight decay, dropout , adversarial training , mixup , label smoothing .  shows that pre-training has a regularization effect in the presence of noisy labels. In  an additional softmax layer is added, and dropout regularization is applied to this layer, arguing that it provides more robust training and prevents memorizing noise due to randomness of dropout .  proposes a complexity measure to understand if the network starts to overfit. It is shown that learning consists of two steps: 1) dimensionality compression, which models low-dimensional subspaces that closely match the underlying data distribution, 2) dimensionality expansion, which steadily increases subspace dimensionality to overfit the data. The key is to stop before the second step. \\textit{Local intrinsic dimensionality}  is used to measure the complexity of the trained model and stop before it starts to overfit.  takes a pre-trained network on a different domain and fine-tunes it for the noisy labeled dataset. Groups of image features are formed, and group sparsity regularization is imposed so that model is forced to choose relative features and up-weights the reliable images.", "cites": [7162, 4141, 301, 4183, 4133, 7133, 892, 7191], "cite_extract_rate": 0.8, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of regularizers used in training DNNs with noisy labels, connecting multiple papers to explain how different regularization techniques (e.g., dropout, label smoothing, mixup) aim to mitigate overfitting. It identifies a broader learning pattern (dimensionality compression and expansion) from one paper to explain the need for early stopping. However, it lacks in-depth critical evaluation of the trade-offs between methods and does not synthesize a fully novel framework or abstraction."}}
{"id": "26d8730c-2d1a-4e36-bca3-aade882eee4e", "title": "Ensemble Methods", "level": "subsection", "subsections": [], "parent_id": "48d73752-4351-4ff3-8d4c-865c3c729928", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Noise Model Free Methods"], ["subsection", "Ensemble Methods"]], "content": "\\label{ensemblemethods}\nIt is well known that bagging is more robust to label noise than boosting . Boosting algorithms like AdaBoost puts too much weight on noisy samples, resulting in overfitting the noise. However, the degree of label noise robustness changes for the chosen boosting algorithm. For example, it is shown that BrownBoost and LogitBoost are more robust than AdaBoost . Therefore, noise-robust alternatives of AdaBoost is proposed in literature, such as noise detection based AdaBoost , rBoost , RBoost1\\&RBoost2  and robust multi-class AdaBoost .", "cites": [7782], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple works on ensemble methods and their robustness to label noise, particularly highlighting different boosting algorithms and their sensitivity to noise. It offers a basic comparison between AdaBoost and more robust alternatives like BrownBoost and LogitBoost. However, the critical analysis is limited to a brief mention of susceptibility in one method without deeper evaluation of strengths or weaknesses, and abstraction remains at a surface level without identifying broader patterns or principles."}}
{"id": "d7ad34e5-7f03-4a38-8328-caca64a82a36", "title": "Others", "level": "subsection", "subsections": [], "parent_id": "48d73752-4351-4ff3-8d4c-865c3c729928", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Noise Model Free Methods"], ["subsection", "Others"]], "content": "\\label{others}\n\\textit{Complementary labels} define classes that observations do not belong to. For example, in the case of ten classes, there is one true class and nine complimentary classes for each instance. Since annotators are less likely to mislabel, some works propose to work in complementary label space .  uses reconstruction error of autoencoder to discriminate noisy data from clean data, arguing that noisy data tend to have bigger reconstruction error. In , the base model is trained with noisy data. An additional generative classifier is trained on top of the feature space generated by the base model. By estimating its parameters with \\textit{minimum covariance determinant}, noise-robust decision boundaries are aimed to be found. In , a special setup is considered where dataset consists of noisy and \\textit{less-noisy} data for binary classification task.  aims to extract the quality of data instances. Assuming that the training dataset is generated from a mixture of the target distribution and other unknown distributions, it estimates the quality of data samples by checking the consistency between generated and target distributions. \n\\textit{Prototype learning} aims to construct prototypes that can represent features of a class in order to learn clean representations. Some works in the literature  propose to create clean representative prototypes for noisy data, so that base classifier can be trained on them instead of noisy labels.\nIn multiple-instance learning, data are grouped in clusters, called bags, and each bag is labeled as positive if there is at least one positive instance in it and negative otherwise. The network is fed with a group of data and produces a single prediction for each bag by learning the inner discriminative representation of data. Since the group of images is used and one prediction is made, the existence of noisy labels along with true labels in a bag has less impact on learning. In , authors propose to effectively choose training samples from each bag by minimizing the total bag level loss. Extra model is trained in  as an attention model, which determines parts of the images to be focused on. The aim is to focus on a few regions on correctly labeled images and not focus on any region for mislabeled images.", "cites": [4177, 8742, 4176], "cite_extract_rate": 0.3, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of various noise-robust methods, including complementary labels, prototype learning, and multiple-instance learning. It integrates ideas to some extent by grouping them thematically but lacks deeper synthesis or comparative analysis. There is minimal critical evaluation or abstraction to broader principles, focusing mostly on summarizing individual approaches."}}
{"id": "a682ea6c-3e81-4982-b6b7-4cfdd43c2a1e", "title": "Experiments", "level": "section", "subsections": [], "parent_id": "1da83c5d-fa27-4a53-a17c-f942c45c275a", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Experiments"]], "content": "\\label{experiments}\nThis section discusses how the proposed algorithms from the literature conduct experiments to test their robustness against label noise. In general, for quick implementation and testing, most of the works start by testing on toy datasets (MNIST, MNIST-Fashion, CIFAR10\\&CIFAR100) with synthetic label noise. However, as explained in \\autoref{labelnoisemodels}, there are various types of artificial noises. Moreover, each work experiment with a different model architecture and hyper-parameter set. Therefore, results on these datasets are not appropriate for a fair comparison of the algorithms. They are instead used as a proof of concept for the proposed algorithm.\nSome works from the literature use two alternative datasets. The first one is the Food101N dataset  containing 310k images of food recipes belonging to 101 different classes. However, its noise ratio is pretty small (around 20\\%), making it inadequate to evaluate noise robust algorithms' performance. The second option is the WebVision dataset  containing 2.4 million images crawled from Flickr website and Google Images search. This is a big dataset, which requires a lot of computational power to run algorithms. Some works conduct tests on this dataset by using data only from the first 50 classes, aiming to make it computationally feasible. But still, WebVision fails to provide a benchmarking dataset for the evaluation of noise robust algorithms.\nTo fill the absence of a benchmarking dataset,  collects a large amount of images from the web with labels interpreted from the surrounding user tags. As a result, it has real-world noisy labels with an estimated noise ratio of around 40\\%. Dataset consists of one million images belonging to 14 different classes. 50K, 14K and 10K additional images with verified clean labels for train, validation and test purposes. We observed a high majority of the methods do not use additional 50K clean data for training. Furthermore, the literature seems to have a consensus on the experimental setup. All methods use the same model architecture of ResNet50  with pre-trained parameters on Imagenet  and stochastic gradient descent optimizer. Considering the identical experimental setup and real-world noisiness of the dataset, the Clothing1M dataset is widely accepted as a benchmarking dataset.\nWe listed (to the best of our knowledge) all of the results presented on this dataset in \\autoref{table:clothing1m}. We collected results only from the works trained on 1M noisy training data without additional 50K clean data for a fair evaluation. We sorted algorithms according to their test accuracy. Nevertheless, it should be noted that each method has its pros and cons, such as computational cost, memory requirements, etc.\n\\begin{table}[h]\n    \\centering\n    \\begin{tabular}{|l|l|l|}\n        \\hline \\multicolumn{1}{|c|}{\\textbf{Method}}    & \\multicolumn{1}{c|}{\\textbf{Category}}    & \\multicolumn{1}{c|}{\\textbf{Accuracy}}   \\\\ \\hline\n                                    & \\nameref{metalearning}                    & 76.02 \\\\ \n                                  & \\nameref{datasetpruning}                  & 74.76 \\\\ \n                                  & \\nameref{sampleimportance}                & 74.69 \\\\ \n                                      & \\nameref{labelnoisecleansing}             & 74.45 \\\\ \n                                    & \\nameref{noisychannel}                    & 74.18 \\\\ \n                              & \\nameref{datasetpruning}                  & 73.77 \\\\ \n                                      & \\nameref{sampleimportance}                & 73.72 \\\\ \n                              & \\nameref{labelnoisecleansing}             & 73.49 \\\\ \n                               & \\nameref{metalearning}                    & 73.47 \\\\ \n                                 & \\nameref{robustlosses}                    & 73.20 \\\\ \n                               & \\nameref{noisychannel}                    & 73.07 \\\\ \n                             & \\nameref{others}                          & 72.50 \\\\ \n                                      & \\nameref{robustlosses}                    & 72.46 \\\\ \n                                  & \\nameref{labelnoisecleansing}             & 72.23 \\\\ \n                                   & \\nameref{labelnoisecleansing}             & 71.74 \\\\ \n                                   & \\nameref{noisychannel}                    & 71.10 \\\\ \n                                & \\nameref{robustlosses}                    & 71.02 \\\\ \n                            & \\nameref{labelnoisecleansing}             & 71.00 \\\\ \\hline\n    \\end{tabular}\n    \\caption{Leaderboard for algorithms tested on the Clothing1M dataset. All results are taken from the corresponding paper. For fair evaluation only the works which did not used additional 50k clean training data are presented.}\n    \\label{table:clothing1m}\n\\end{table}", "cites": [7773, 4134, 4186, 4156, 4184, 4135, 4238, 97, 652, 4143, 7769, 4152, 4253, 8741, 4187, 4185, 7774], "cite_extract_rate": 0.68, "origin_cites_number": 25, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a comparative view of experimental setups and results on noisy label datasets, particularly highlighting the use and limitations of Food101N and WebVision. It identifies the need for a benchmarking dataset and points to the adoption of Clothing1M, but the synthesis remains somewhat superficial and lacks deeper meta-level abstraction or nuanced critique of the methods."}}
{"id": "8f59dd3e-4249-4d9b-aaf1-ecbaa320f1db", "title": "Conclusion", "level": "section", "subsections": [], "parent_id": "1da83c5d-fa27-4a53-a17c-f942c45c275a", "prefix_titles": [["title", "Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey"], ["section", "Conclusion"]], "content": "\\label{conclusion}\nThroughout this paper, it is shown that label noise is an important obstacle to deal with in order to achieve desirable performance from real-world datasets. Despite its importance for supervised learning in practical applications, it is also an important step to collect datasets from the web , design networks that can learn from unlimited web data with no human supervision . Furthermore, beside image classification, there are more fields where dealing with mislabeled instances is important, such as generative networks , semantic segmentation , sound classification  and more. All these factors make dealing with label noise an important step through self-sustained learning systems.\nDifferent approaches to come through noisy label phenomenon are proposed in the literature. All methods have their advantages and disadvantages, so one can choose the most appropriate algorithm for the use case. However, in order to draw a generic line, we make the following suggestions. If the noise structure is domain-specific and there is prior information or assumption about its structure, noise model based methods are more appropriate. Among these models, one can choose the best-suited method according to need. For example, if noise can be represented as a noise transition matrix, noisy channel or labeler quality assessment for multi labeler case can be chosen. If the purpose is to purify the dataset as a preprocessing stage, then dataset pruning or label noise cleansing methods can be employed. Sample choosing or sample importance weighting algorithms are handy if instances can be ranked according to their informativeness on training. Unlike noise model-based algorithms, noise model free methods do not depend on any prior information about the noise structure. Therefore, they are easier to implement if noise is assumed to be random, and performance degradation is due to overfitting since they do not require the hassle of implementing an external algorithm for noise structure estimation. If there is no clean subset of data, robust losses or regularizers are appropriate options since they treat all samples the same. Meta-learning techniques can be used in the presence of a clean subset of data since they can easily be adapted to utilize this subset. \nEven though an extensive amount of research is conducted for machine learning techniques , deep learning in the presence of noisy labels is certainly an understudied problem. Considering its dramatic effect on DNNs , there still are many open research topics in the field. For example, truly understanding the impact of label noise on deep networks can be a fruitful future research topic.  shows that each layer of CNN learns to extract different features from the data. Moreover, learned representations form a hierarchical pattern, where each layer learns more complex features from the previous layer. A fully connected layer uses features from the last layer to interpret the corresponding label on the final layer. Understanding which parts of the network is highly affected by label noise may help analyze the adverse effect of the label noise on neural networks. For example, if initial layers are affected, one can conclude that learned primitive features are corrupted, so the rest of the network cannot be adequately trained. On the other hand, if final convolution layers are affected, it can be said that the network can not form the hierarchical feature pattern throughout the convolutional layers. Alternatively, if the convolutional layers are not affected but the fully connected layer is the cause of the problem, it can be concluded that feature representation learning is not corrupted, but the network cannot correctly interpret meanings from the extracted features. Moreover, it can be interesting to investigate the cause of the problem for different types of label noise models presented in \\autoref{labelnoisemodels}. If for different noise models, different parts of the network are affected, one can analyze the true nature of the label noise in the dataset by checking corruption in the neural network layers.\nAlternatively, the question of how to train in the existence of both attribute and label noise is an understudied problem with significant potential for practical applications .  shows noisy labels degrades the learning, especially for challenging samples. So, instead of overfitting to noisy samples, underfitting to challenging samples may be the reason for the performance degradation, which is an open question to be answered in the future. Another possible research direction may be on the effort of breaking the structure of the noise to make it uniformly distributed in the feature domain . This approach would be handy where labelers have a particular bias.\nA widely used approach for quick testing of proposed algorithms is to create noisy datasets by adding synthetic label noise to benchmarking toy datasets . However, this prevents fair comparison and evaluation of algorithms since each work adds its own noise type. Some large datasets with noisy labels are proposed in literature . These datasets are collected from the web, and labels are attained from noisy user tags. Even though these datasets provide a useful domain for benchmarking proposed solutions, their noise rates are mostly unknown and they are biased in terms of data distribution for classes. Moreover, one can not adjust the noise rate for testing under extreme or moderate conditions. From this perspective, we believe literature lacks a noisy dataset where a major part of it has both noisy and verified labels; thus, the noise rate can be adjusted as desired.\nMinimal attention is given to the learning from a noisy labeled dataset when there is a small amount of data. This can be a fruitful research direction considering its potential in fields where harvesting dataset is costly. For example, in medical imaging, collecting a cleanly annotated large dataset is not feasible most of the time , due to its cost or privacy. Effectively learning from a small amount of noisy data with no ground truth can significantly improve autonomous medical diagnosis systems. Even though some pioneer researches are available , there is still much more to be explored.\nThe ability to effectively learn from noisily labeled data brings up big opportunities for practical applications of machine learning algorithms. The bottleneck of data collection can easily be resolved with the massive amount of data collected from the web. Labels for this data can be assigned with simple algorithms (such as interpreting from the surrounding text ). By effectively dealing with noisy labels, deep learning algorithms can be fed with massive datasets. Moreover, there are research opportunities for alternative usage of semi-supervised learning algorithms along with noisily labeled data. Common usage of semi-supervised learning methods for noisily labeled data is to remove the labels of noisy data and then train with conventional semi-supervised learning methods. Alternatively, algorithms can be developed to use three types of data; cleanly labeled data, noisily labeled data, and unlabeled data. With the help of these algorithms, a massive amount of unlabeled and noisy data can be effectively used under the supervision of a small cleanly annotated data. \nBesides classification, knowledge of learning from noisily labeled data can be used in alternative fields by transforming the task. For example, in a multi-labeled dataset, where each instance belongs to multiple classes, not all classes are equally relevant. One can assume labels with a small resemblance to the data sample as noisy and employ algorithms designed for learning from noisy labels . Similarly,  divides an untrimmed video into smaller parts and aims to find the video snippet most relevant to the video tag. Irrelevant video parts are assumed to be noisily labeled. Even though the original dataset does not have noisy labels, it can be transformed to use algorithms from the field. As a result, learning from noisy labels has many potentials in various areas besides straight image classification.\n\t\\bibliography{surveyrefs} \n\t\\begin{comment}\n\t\\begin{wrapfigure}{l}{20mm} \n\t\t\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio,angle=270]{pp_gorkem.jpg}\n\t\\end{wrapfigure}\\par\n\t  \\textbf{G\\\"orkem Algan} received his B.Sc. degree in Electrical-Electronics Engineering in 2012, from Middle East Technical University (METU), Turkey. He received his M.Sc. from KTH Royal Institute of Technology, Sweden and Eindhoven University of Technology, Netherlands with double degree in 2014. He is currently a Ph.D. candidate at the Electrical-Electronics Engineering, METU. His current research interests include deep learning in the presence of noisy labels.\n\t\\begin{wrapfigure}{l}{25mm} \n\t\t\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{pp_ilkay.jpg}\n\t\\end{wrapfigure}\\par\n\t  \\textbf{Ilkay Ulusoy} was born in Ankara, Turkey, in 1972. She received the B.Sc. degree from the Electrical and Electronics Engineering Department, Middle East Technical University (METU), Ankara, in 1994, the M.Sc. degree from The Ohio State University, Columbus, OH, USA, in 1996, and the Ph.D. degree from METU, in 2003. She did research at the Computer Science Department of the University of York, York, U.K., and Microsoft Research Cambridge, U.K. She has been a faculty member in the Department of Electrical and Electronics Engineering, METU, since 2003. Her main research interests are computer vision, pattern recognition, and probabilistic graphical models.\t\t\n\t\\end{comment}\n\\end{document}", "cites": [4134, 4170, 4169, 499, 4168, 7784, 4180, 7785, 4238, 652, 4172, 4190, 4171, 3630, 4165, 7769], "cite_extract_rate": 0.48484848484848486, "origin_cites_number": 33, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes insights from multiple cited papers to discuss the broader implications and future research directions in dealing with noisy labels. It provides critical evaluations by pointing out limitations in current datasets and approaches, and it abstracts the issue of label noise into a meta-level discussion of network structure, noise types, and learning paradigms."}}
