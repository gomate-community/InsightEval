{"id": "063274cf-8d23-43c8-9ff1-262d15bb5840", "title": "Introduction", "level": "section", "subsections": ["b4b6ebb5-d4e1-4a03-a23d-b7da6b2904bd"], "parent_id": "8c085b08-ebad-4574-a00e-145f8309ec78", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Introduction"]], "content": "Machine learning (ML) is making rapid advances in many different areas. However, standard ML methods that learn models automatically from training data sets are insufficient for applications that could have a great impact on our everyday life, such as autonomous companion robots or self-driving cars. Generally speaking, to enable their use in more complex and impactful environments, ML algorithms have to overcome further obstacles: how to correctly specify the problem, improve sampling efficiency, and adapt to the particular needs of individual users. \nInteractive ML (IML) proposes to incorporate a human-in-the-loop to tackle the aforementioned problems in current ML algorithms . This configuration intends to integrate human knowledge that improves and/or personalizes the behavior of agents. Reinforcement learning (RL) is one ML paradigm that can benefits from an interactive setting . Interactive RL has been successfully applied to a wide range of problems in different research areas . However, interactive RL has not been widely adopted by the human-computer interaction (HCI) community. \nIn the last few years, the two main roles that HCI researchers have played in IML are (1) designing new interaction techniques  and (2) proposing new applications . To enable HCI researchers to play those same roles in interactive RL, they need to have adequate technical literacy. In this paper, we present a survey on interactive RL to empower HCI researchers and designers with the technical background needed to ideate novel applications and paradigms for interactive RL.\nTo help researchers perform role (1) by describing how different types of feedback can leverage an RL model for different purposes. For instance, the user can personalize the output of an RL algorithm based on the user's preferences , facilitate learning in complex tasks , or guide exploration to promising states in the environment . \nFinally, we help researchers perform role (2) by proposing generic design principles that will provide a guide to effectively implement interactive RL applications. We pay special attention to common problems in multiple research fields that constrain the performance of interactive RL approaches; solving these problems could make interactive RL applications effective in real-world environments.", "cites": [1342, 1341, 8440, 1340, 1339], "cite_extract_rate": 0.35714285714285715, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the cited papers to highlight the role of human input in addressing key challenges in interactive RL, such as exploration and reward specification. It abstracts these ideas into two main roles for HCI researchers and introduces a goal of developing design principles. However, it lacks deeper critical analysis of the limitations or trade-offs of these approaches and does not present a novel, integrative framework."}}
{"id": "b4b6ebb5-d4e1-4a03-a23d-b7da6b2904bd", "title": "Scope of this Paper", "level": "subsection", "subsections": [], "parent_id": "063274cf-8d23-43c8-9ff1-262d15bb5840", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Introduction"], ["subsection", "Scope of this Paper"]], "content": "The literature on RL that could potentially be applied to interactive RL is extensive. For that reason, we focus on works that have been successfully applied to RL methods that use a human-in-the-loop. We further narrow the reach of this survey by considering only works from three research areas, robotics, HCI, and Game AI, published between $2010$ and $2019$. Among these papers, we picked those that covered all the main research directions in the three research areas. We chose these three fields because they use interactive RL systems in similar ways, enabling us to propose generic design rules for interactive RL that will be useful for a wider audience. \nSome surveys have covered larger topics, such as IML in general  or IML for health informatics . Other surveys have focused on particular design dimensions, such as types of user feedback  or the evaluation of results . Surveys have also been done on particular subjects within the interactive RL research area, such as safe RL , the agent alignment problem , and inverse RL . In contrast, our survey focuses only on approaches that can be applied to an interactive RL setting. \nThese constraints allow us to construct a detailed analysis for HCI researchers. Additionally, our survey methodology is favorable for constructing design principles for interactive RL that are generic enough to be applied in physical or simulated environments.", "cites": [1340], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a clear analytical focus by narrowing the scope of the survey and justifying the selection of specific research areas and timeframes. It integrates a few cited works to position its focus relative to prior surveys, showing basic synthesis. However, the critical evaluation is limited, as the section only briefly mentions other surveys without in-depth comparison or critique. It abstracts to some extent by aiming to propose generic design rules, but the synthesis and abstraction remain moderate rather than transformative."}}
{"id": "3dc5d7c8-0db5-4752-8045-ace603552142", "title": "Reinforcement Learning (RL)", "level": "section", "subsections": [], "parent_id": "8c085b08-ebad-4574-a00e-145f8309ec78", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Reinforcement Learning (RL)"]], "content": "\\label{sec::introRL}\nThe \\textit{reinforcement learning} (RL) paradigm is based on the idea of an agent that learns by interacting with its environment . The learning process is achieved by an exchange of signals between the agent and its environment; the agent can perform actions that affect the environment, while the environment informs the agent about the effects of its actions. Additionally, it is assumed that the agent has at least one goal to pursue and -- by observing how its interactions affect its dynamic environment -- it learns how to behave to achieve its goal.\nArguably, the most common approach to model RL agents is the \\textit{Markov decision process} (MDP) formalism. An MDP is an optimization model for an agent acting in a stochastic environment ; that is defined by the tuple $\\langle S,A,T,R,\\gamma\\rangle$ , where $S$ is a set of states; $A$ is a set of actions; $T \\colon S \\times A \\times S \\rightarrow [0, 1]$ is the \\textit{transition function} that assigns the probability of reaching state $s'$ when executing action $a$ in state $s$, that is, $T(s' \\given s,a) = P(s' \\given a,s)$; $R \\colon S \\times A \\rightarrow \\R$ is the \\textit{reward function}, with $R(s, a)$ denoting the immediate numeric reward value obtained when the agent performs action $a$ in state $s$; and $\\gamma \\in [0, 1]$ is the discount factor that defines the preference of the agent to seek immediate or more distant rewards.\nThe reward function defines the goals of the agent in the environment; while the transition function captures the effect of the agent's actions for each particular state.", "cites": [8441], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a basic description of reinforcement learning and Markov decision processes, largely paraphrasing foundational concepts. It cites one paper but does not integrate or synthesize multiple sources, nor does it offer critical evaluation or highlight broader patterns and principles in the field."}}
{"id": "337cf78e-1d14-4527-a28d-98681edf2fc2", "title": "Interactive Reinforcement Learning", "level": "section", "subsections": [], "parent_id": "8c085b08-ebad-4574-a00e-145f8309ec78", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Interactive Reinforcement Learning"]], "content": "\\label{sec::IntroIML}\nIn a typical RL setting, the RL designer builds a reward function and chooses/designs an RL method for the agent to learn an optimal policy. For a survey of the literature on RL, we refer the reader to .  \nIn contrast, an interactive RL approach involves a human-in-the-loop that tailors specific elements of the underlying RL algorithm to improve its performance or produce an appropriate policy for a particular task. One of the first approaches that incorporated a human-in-the-loop in an RL algorithm is the Cobot software agent . This interactive method implements a crowd-sourced environment where multiple users work together to create the reward function for a virtual agent. However, a formal introduction to the term interactive RL was later proposed in . \n\\begin{figure}[!htb]\n\\begin{center}\n\\includegraphics[width = 1.0 \\columnwidth ]{./figures/iRL.png}\n\t\t\\caption{The interactive RL architecture.}\n\\label{Fig:iRL}\n\\end{center}\n\\end{figure}\nFigure \\ref{Fig:iRL} presents a generic interactive RL architecture and shows the interaction channels that allow the user to guide the low-level components of an RL algorithm. The foundation of an interactive RL approach is an RL-based agent that learns by interacting with its environment according to a specific RL algorithm. Next, the user observes and evaluates the resulting agent behavior and gives the agent feedback. Human feedback can be delivered through multiple human knowledge integration methods to a specific element (design dimension) of the underlying RL algorithm. \nThe interactive RL paradigm has the advantage of integrating prior knowledge about the task the RL-based agent is trying to solve. This concept has proven to be more effective than automatic RL algorithms . Moreover, human feedback includes prior knowledge about the world that can significantly improve learning rates . Arguably, any RL algorithm will more quickly find a high-earning policy if it includes human knowledge. However, it is still unclear how to design efficient interactive RL methods that generalize well and can be adapted to feedback from different user types. \nIn the rest of this paper, we will detail all the elements of interactive RL that we have introduced so far.", "cites": [1344, 1343, 8442], "cite_extract_rate": 0.375, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.3, "critical": 3.0, "abstraction": 3.3}, "insight_level": "medium", "analysis": "The section synthesizes basic concepts from the cited papers to introduce interactive RL, connecting the role of human feedback and prior knowledge in improving agent performance. It offers some critical reflection by noting the unclear aspects of designing efficient and generalizable interactive RL methods. The section also abstracts to a degree by proposing a generic architecture and emphasizing the integration of human knowledge as a key benefit. However, deeper comparative or meta-level analysis is limited, and the synthesis remains at a conceptual rather than a novel framework level."}}
{"id": "f6a67027-9b3e-46ed-94e2-389f45b8a251", "title": "Interactive RL Testbeds", "level": "section", "subsections": ["636655ca-e179-496a-af75-2d89d2f3f714"], "parent_id": "8c085b08-ebad-4574-a00e-145f8309ec78", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Interactive RL Testbeds"]], "content": "\\label{sec::Testbeds}\n\\begin{figure*}[t!]\n\\centering     \n\\subfigure[ ]{\\label{fig:a}\\includegraphics[height=28mm]{./figures/gridworld.png}}\n\\subfigure[ ]{\\label{fig:b}\\includegraphics[height=28mm]{./figures/mario.png}}\n\\subfigure[ ]{\\label{fig:b}\\includegraphics[height=28mm]{./figures/pacman.png}}\n\\subfigure[ ]{\\label{fig:b}\\includegraphics[height=28mm]{./figures/nao.jpg}}\n\\subfigure[ ]{\\label{fig:b}\\includegraphics[height=28mm]{./figures/turtlebot.png}}\n\\subfigure[ ]{\\label{fig:b}\\includegraphics[height=28mm]{./figures/sophie.png}}\n\\caption{Selected testbeds for interactive RL. \\scriptsize (a) GridWorld. (b) Mario AI benchmark. (c) Pac-Man. (d) Nao Robot. (e) TurtleBot. (f) Sophie's kitchen.}\n\\label{fig::testbeds}\n\\end{figure*}\nIn Figure \\ref{fig::testbeds}, we present examples of the most common testbeds in interactive RL research. The \\textit{GridWorld} platform is a classic environment for testing RL algorithms. The small state-space size of this testbed allows for fast experiments, and its simplicity does not demand specialized knowledge from the users. These characteristics make GridWorld popular among researchers in the human-computer interaction (HCI) field . The \\textit{Mario AI benchmark}  is a clone of Nintendo's platform game named \\textit{Super Mario Bros.} This game is a testbed with a large state-space that offers the conditions needed to test interactive RL algorithms aimed to personalize bot behaviors and create fast evaluation techniques. Similarly, we have the \\textit{Pac-Man} game that can reduce the size of its state-space to make it more manageable. The \\textit{Nao robot} and the \\textit{TurtleBot} are popular platforms in robotics that are perfect for testing natural ways to interact with human users. The \\textit{Sophie's kitchen}  platform is designed as an online tool for interactive RL that explores the impact of demonstrating uncertainty to the users. Finally, the main focus of the OpenAI Gym platform  is designing and comparing RL algorithms.\nGenerally speaking, these are the classifications of testbeds for each research area included in our survey:       \n\\begin{itemize}\n\\item \\textbf{Robotics:} GridWorld environments (physical and virtual), OpenAI Gym, soccer simulation, Nao robot, Nexi robot, TurtleBot, shopping assistant simulator.\n\\item \\textbf{Game AI:} Pac-Man, Mario AI benchmark, Street Fighter game, Atari emulator.\n\\item \\textbf{HCI:} Pac-Man with small state-space, GridWorld environments.\n\\end{itemize}", "cites": [1345, 1346], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of various testbeds used in interactive RL, listing their features and categorizing them by research area. It integrates minimal insights from the cited papers, merely referencing them without connecting deeper ideas or themes across multiple works. There is little critical evaluation or abstraction into general principles or trends."}}
{"id": "636655ca-e179-496a-af75-2d89d2f3f714", "title": "Applications", "level": "subsection", "subsections": [], "parent_id": "f6a67027-9b3e-46ed-94e2-389f45b8a251", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Interactive RL Testbeds"], ["subsection", "Applications"]], "content": "We can find applications for interactive RL, such as teaching robots how to dance , creating adaptable task-oriented dialogue systems , and learning the generation of dialogue . Furthermore, there is the opportunity to adapt current automatic RL applications to an interactive setting, such as procedural content generation for video games , generating music , and modeling social influences in bots .", "cites": [1349, 1348, 1347], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a brief list of applications for interactive RL, citing three papers, but fails to synthesize or connect the ideas between them. It lacks critical evaluation of the approaches or limitations and offers no broader abstractions or principles. The presentation remains largely descriptive without analytical depth."}}
{"id": "6390143e-9659-4a85-8a17-6a6ac5573b66", "title": "Design Dimensions", "level": "subsection", "subsections": ["ed008cc0-2e2b-475a-8f6d-95bb713d64d9", "cb7e9cc0-69eb-44b1-9a5b-ec755b481ca3", "003d5c7d-ccf9-4801-a0cf-cad5268259cd", "76c18fdb-657c-48ed-91da-cb29e60d487b", "b0584def-3292-4f8e-ad22-c68562232690"], "parent_id": "92a4ba6d-ca5d-4493-bbfc-ec0ba36cef23", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Design Guides for interactive RL"], ["subsection", "Design Dimensions"]], "content": "\\label{sec::DesignDimensions}\n\\begin{table*}[!ht]\n\\scriptsize \n\\begin{center}\n\\begin{tabular}{l l l l l l l}\n\\toprule\n\\textbf{Design Dimension} & \\textbf{Testbed} & \\textbf{Interaction}  & \\textbf{Initiative} & \\textbf{HKI} & \\textbf{Feedback} & \\textbf{Algorithms}\\\\\n\\midrule\nReward Function & Robot in maze-like environment   & FE & Passive & RS using HF $+$ ER & Critique & DQL \\\\\n\t\t\t  ~ & Navigation simulation  & GUI & Passive & Advantage Function & Critique & DAC\\\\\n\t\t\t  ~ & Sophie's Kitchen game  & GUI & Passive, Active & RS using HF $+$ EF & Critique & QL , HRL \\\\\n\t\t\t  ~ & Bowling game  & GUI  & Passive & RS $+$ HF & Scalar-valued & DQL \\\\\n\t\t\t  ~ & Shopping assistant, GridWorld  & GUI & Active & Active IRD & Queries & Model-based RL \\\\\n\t\t\t  ~ & Mario, GridWorld, Soccer simulation   & Coding & Passive & PBRS & Heuristic Function & QL, QSL, QL($\\lambda$), QSL($\\lambda$) \\\\\n\t\t\t  ~ & Navigation simulation  & VC & Passive & RS using HF $+$ ER & AcAd & SARSA , SARSA($\\lambda$) \\\\\n\t\t\t  ~ & Atari, robotics simulation  & GUI & Active & RS using HF & Queries & DRL \\\\\n\\midrule\nPolicy & GridWorld, TurtleBot robot  & GUI, GC & Passive & PS & AcAd & AC($\\lambda$)  \\\\\t\n\t ~ & GridWorld  & VC & Passive & PS & Critique, AcAd & BQL  \\\\\n\t ~ & Pac-Man, Frogger  & GUI & Passive & PS & Critique & BQL \\\\\t\n\\midrule\nExploration Process & Pac-Man, Cart-Pole simulation  & GUI & Passive & GEP & AcAd & QL \\\\\n\t\t\t\t  ~ & Simulated cleaning Robot  & VC & Passive & GEP & AcAd & SARSA\\\\\n\t\t\t\t  ~ & Pac-Man  & GUI & Active & GEP & AcAd  & SARSA($\\lambda$)\\\\\n\t\t\t\t  ~ & Pac-Man  & GUI & Active & Myopic Agent & AcAd & QL, QRL\\\\\t\n\t\t\t\t  ~ & Sophie's Kitchen game  & GUI & Active & ACTG & Guidence & QL \\\\ \n\t\t\t\t  ~ & Street Fighter game  & Not apply & Passive & EB using Safe RL & Demonstration & HRL \\\\\n\t\t\t\t  ~ & Nao Robot  & GUI & Passive & ACTG & Guidence & QL \\\\\n\t\t\t  ~ & Nexi robot   & AT $+$ CT & Passive & Myopic Agent & AcAd & SARSA($\\lambda$) \\\\\n\\midrule\nValue Function & Mountain Car simulation  & GUI & Passive & Weighted VF & Demonstration & SARSA($\\lambda$) \\\\\n\t\t\t ~ & Keepaway simulation  & GUI & Passive &  Weighted VF & Demonstration & SARSA \\\\\n\t\t\t ~ & Mario, Cart Pole  & Not apply & Passive & Initialization of VF & Demonstration & QL($\\lambda$) \\\\\t  \n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\caption{Selected works for each design dimension. \\scriptsize HKI=human knowledge integration, RS=reward shaping, PS=policy shaping, AcAd=action advice, VF=value function, GUI=grafical user interface, FE=facial expression, VC=voice command, GC=game controller, AT=artifact, CT=controller, HF=human feedback, ER=environmental reward, GEP=guided exploration process, PBRS=potential based reward shaping, ACTG=actions containing the target goal, QL=q-learning, $\\lambda$= with elegibility traces, DQL= deep q-learning, QRL= R-learning, QSL=QS-learning, AC($\\lambda$)=actor-critic with elegibility traces, DAC= deep actor-critic, DRL= deep reinforcement learning, BQL=bayesian q-learning, HRL=hierarchical reinforcement learning, SARSA=state-action-reward-state-action, IRD= inverse reward design, Mario=Super Mario Bros.}\n\\label{table:DesignDimensions}\n\\end{table*}\nIn this section, we analyze how human feedback is used to tailor the low-level modules of diverse RL algorithms; we call this form of interaction between users and RL algorithms \\textit{design dimensions}. Furthermore, the summary of selected works in Table \\ref{table:DesignDimensions} presents a concise and precise classification that helps to compare different design dimensions.", "cites": [1354, 620, 1342, 1350, 8443, 1353, 1351, 1339, 1352], "cite_extract_rate": 0.3103448275862069, "origin_cites_number": 29, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily presents a descriptive table of selected works and their characteristics without providing a narrative or deep synthesis of the underlying themes. While the table organizes information by design dimensions, it lacks analysis of trends or limitations across the cited works. There is minimal abstraction or critical evaluation, and the content remains focused on factual categorization."}}
{"id": "ed008cc0-2e2b-475a-8f6d-95bb713d64d9", "title": "Reward Function", "level": "subsubsection", "subsections": [], "parent_id": "6390143e-9659-4a85-8a17-6a6ac5573b66", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Design Guides for interactive RL"], ["subsection", "Design Dimensions"], ["subsubsection", "Reward Function"]], "content": "\\label{subsec::RewardShaping}\nAll works classified in this design dimension tailor the reward function of the RL-based algorithm using human feedback. The main objectives of this feedback are to speed up learning, customize the behavior of the agent to fit the user's intentions, or teach the agent new skills or behaviors.\nDesigning a reward function by hand is trivial for simple problems where it is enough to assign rewards as $1$ for winning and $0$ otherwise. For example, in the simplest setup of a GridWorld environment, the agent receives $-1$ for each time step and $1$ if it reaches the goal state. However, for most problems that an agent can face in a real-world environment, the selection of an appropriate reward function is key — the desired behaviors have to be encoded in the reward function.   \nThe reward design problem is therefore challenging because the RL designer has to define the agent's behaviors as goals that are explicitly represented as rewards. This approach can cause difficulties in complex applications where the designer has to foresee every obstacle the agent could possibly encounter. Furthermore, the reward function has to be designed to handle trade-offs between goals. These reward design challenges make the process an iterative task: RL designers alternate between evaluating the reward function and optimizing it until they find it acceptable. This alternating process is called \\textit{reward shaping} . Ng et al. present a formal definition of this concept in . \nReward shaping (RS) is a popular method of guiding RL algorithms by human feedback . In what is arguably the most common version of RS, the user adds extra rewards that enhance the environmental reward function  as $R' = R + F$, where $F \\colon S \\times A \\times S \\rightarrow \\R$ is the \\textit{shaping reward function} . A hand-engineered reward function has been demonstrated to increase learning rates . In contrast, some approaches use human feedback as the only source of reward for the agent . \nUsing RS is also beneficial in sparse reward environments: the user can provide the agent with useful information in states where there are no reward signals from the environment or in  highly stochastic environments . Another advantage of RS is that it gives researchers a tool with which to better specify, in a granular manner, the goals in the current environment. That is, the computational problem is specified via a reward function .\nOn the other hand, we need to consider the \\textit{credit assignment problem} . This difficulty emerges from the fact that, when a human provides feedback, it is applied to actions that happened sometime in the past — there is always a delay between the action’s occurrence and the human response. Furthermore, the human's feedback might refer to one specific state or an entire sequence of states the bot visited. \n\\textit{Reward hacking} is another negative side effect resulting from a deficient reward design . This side effect can cause nonoptimal behaviors that fail to achieve the goals or intentions of the designers. Usually, these kinds of failure behaviors arise from reward functions that do not anticipate all trade-offs between goals. For example, Clark et al. presented an agent that drives a boat in a race . Instead of moving forward to reach the goal, the agent learned that the policy with the highest reward was to hit special targets along the track.", "cites": [1342, 1356, 1350, 8443, 1339, 1355], "cite_extract_rate": 0.23076923076923078, "origin_cites_number": 26, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes the concept of reward shaping from multiple cited papers, connecting the role of human feedback across different RL contexts. It critically addresses challenges like the credit assignment problem and reward hacking, showing awareness of limitations and unintended consequences in reward design. Furthermore, it abstracts these ideas into a broader understanding of the importance of reward function design in interactive RL, especially for real-world applications."}}
{"id": "003d5c7d-ccf9-4801-a0cf-cad5268259cd", "title": "Exploration Process", "level": "subsubsection", "subsections": [], "parent_id": "6390143e-9659-4a85-8a17-6a6ac5573b66", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Design Guides for interactive RL"], ["subsection", "Design Dimensions"], ["subsubsection", "Exploration Process"]], "content": "RL is an ML paradigm based on learning by interaction . To learn a given task, the agent aims to maximize the accumulated reward signal from the environment. This is achieved by performing the actions that have been effective in the past — a learning step called \\textit{exploitation}. However, to discover the best actions to achieve its goal, the agent needs to perform actions that have not been previously tried — a learning step called \\textit{exploration}. Therefore, to learn an effective policy, the agent has to compromise between exploration and exploitation. \nThe sampling inefficiency of basic RL methods hinders their use in more practical applications, as these algorithms could require millions of samples to find an optimal policy . An approach called the \\textit{guided exploration process} can mitigate this problem. \nThe guided exploration process aims to minimize the learning procedure by injecting human knowledge that guides the agent’s exploration process to states with a high reward . This method biases the exploration process such that the agent avoids trying actions that do not correspond to an optimal policy. This design dimension also assumes that the user understands the task well enough to identify at least one near-optimal policy. Despite this assumption, there is empirical evidence indicating that using human feedback (i.e., guidance) to direct the exploration process is the most natural and sample-efficient interactive RL approach . \nFor example, in , users direct the agent's attention by pointing out an object on the screen. Exploration is driven by performing actions that lead the agent to the suggested object. This interaction procedure thus requires access to a model of the environment, so the agent can determine what actions to perform to get to the suggested object. \nAnother popular approach to guiding the exploration process consists of creating myopic agents . This kind of shortsighted agent constructs its policy by choosing at every time-step to perform the action that maximizes the immediate reward, according to the action suggested by the user; the action’s long-term effects are not considered. Although this guiding approach creates agents that tend to overfit, it has been successfully used in different applications .", "cites": [1354, 1353], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from the cited papers on guided exploration and myopic agents, integrating them into a discussion of how human guidance improves sample efficiency in interactive RL. It provides a general abstraction by framing these methods as design dimensions, but the critical analysis is limited to brief mentions of overfitting and assumptions about user knowledge without deeper evaluation of trade-offs or alternative approaches."}}
{"id": "b0584def-3292-4f8e-ad22-c68562232690", "title": "Design Dimension Alternatives", "level": "subsubsection", "subsections": [], "parent_id": "6390143e-9659-4a85-8a17-6a6ac5573b66", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Design Guides for interactive RL"], ["subsection", "Design Dimensions"], ["subsubsection", "Design Dimension Alternatives"]], "content": "So far, we have explained how RL experts can inject human knowledge through the main components of a basic RL algorithm. There are other ways, however, to interact with low-level features of particular RL approaches. Next, we will explain the two main design dimension alternatives for an interactive RL setting.\n\\textit{Function Approximation} (FA) allows us to estimate continuous state spaces rather than tabular approaches such as tabular Q-learning in a GridWorld domain. For any complex domain with a continuous state space (especially for robotics), we need to represent the value function continuously rather than in discrete tables; this is a scalability issue. The idea behind FA is that RL engineers can identify patterns in the state-space; that is, the RL expert is capable of designing a function that can measure the similarity between different states.\nFA presents an alternative design dimension that can work together with any other type of interaction channel, which means the implementation of an FA in an interactive RL algorithm does not obstruct interaction with other design dimensions. For example, Warnell et al. proposed a reward-shaping algorithm that also uses an FA to enable their method to use raw pixels as input from the videogame they used as a testbed . However, their FA is created automatically.\nAs far as we know, the paper in  is the only work that has performed user-experience experiments for an interactive RL that uses hand-engineered FAs to accelerate the base RL algorithm. Rosenfeld et al. asked the participants of the experiment to program an FA for a soccer simulation environment. The FAs proposed by the participants were successful: they accelerated the learning process of the base RL algorithm. The same experiment was performed again, this time using the game Super Mario Bros.  as a testbed. In the second experiment, the RL’s performance worsened when using the FAs. This evidence suggests that designing an effective FA is more challenging than simply using an RS technique in complex environments .\n\\textit{Hierarchical Decomposition} (HRL) is an effective approach to tackle high-dimensional state-action spaces by decomposing them into smaller sub-problems or temporarily extended actions . In an HRL setting, an expert can define a hierarchy of sub-problems that can be reused as skills in different applications. Although HRL has been successfully tested in different research areas , there are no studies from the user-experience perspective in an interactive RL application.", "cites": [8444, 8443, 1339], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the concept of function approximation and hierarchical reinforcement learning by connecting them to human interaction in complex environments, as supported by the cited works. It provides some critical analysis by highlighting the challenges in designing effective function approximations through user experiments and noting the absence of user-experience studies in HRL. However, the abstraction level is moderate, as it primarily discusses observed patterns without offering a broader theoretical or meta-level framework."}}
{"id": "26ae5a92-d1a0-4989-aec6-b712cfa0e1a8", "title": "Binary critique", "level": "subsubsection", "subsections": [], "parent_id": "62c9329a-f306-4f26-ab6a-7cb7723a61a1", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Design Guides for interactive RL"], ["subsection", "Types of Feedback"], ["subsubsection", "Binary critique"]], "content": "\\label{subsec::critique}\nThe use of \\textit{binary critique} to evaluate an RL model’s policy refers to binary feedback (positive or negative) that indicates if the last chosen action by the agent was satisfactory. This signal of human feedback was initially the only source of reward used . This type of feedback was shown to be less than optimal because people provide an unpredictable signal and stop providing critiques once the agent learns the task . \nOne task for the RL designer is to determine whether a type of reward will be effective in a given application. For example, it was shown that using binary critique as policy information is more efficient than using a reward signal . Similarly, Griffith et al.  propose an algorithm that incorporates the binary critique signal as policy information. From a user experience perspective, it has been shown that using critique to shape policy is unfavorable .\nIt is also worth noting that learning from binary critique is popular because it is an easy and versatile method of using non-expert feedback; the user is required to click only ``$+$'' and ``$-$'' buttons. \n\\textit{Heuristic Function} — The \\textit{heuristic function} approach  is another example of the critique feedback category. Instead of receiving binary feedback directly from the user, in this approach, the critique signal comes from a hand-engineered function. This function encodes heuristics that map state-action pairs to positive or negative critiques. The aim of the heuristic function method is to reduce the amount of human feedback. Empirical evidence suggests that this type of feedback can accelerate RL algorithms; however, more research is needed to test its viability in complex environments such as videogames .\n\\textit{Query} — The authors of the \\textit{active inverse reward design} approach  present a query-based procedure for inverse reward design . A \\textit{query} is presented to the user with a set of sub-rewards, and the user then has to choose the best among the set. The sub-rewards are constructed to include as much information as possible about unknown rewards in the environment, and the set of sub-rewards is selected to maximize the understanding of different suboptimal rewards.", "cites": [1351, 8443, 1357], "cite_extract_rate": 0.2727272727272727, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.3, "critical": 3.0, "abstraction": 2.7}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from three papers, linking binary critique to heuristic functions and query-based methods, and discusses their roles in policy shaping. It includes some critical evaluation by noting limitations such as unpredictability of human feedback and the potential ineffectiveness of critique from a user experience perspective. However, the abstraction is limited to general observations rather than identifying deeper patterns or principles across interactive RL feedback mechanisms."}}
{"id": "b72345a5-8e4b-416d-998d-f4f0f61e5fd9", "title": "Scalar-valued critique", "level": "subsubsection", "subsections": [], "parent_id": "62c9329a-f306-4f26-ab6a-7cb7723a61a1", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Design Guides for interactive RL"], ["subsection", "Types of Feedback"], ["subsubsection", "Scalar-valued critique"]], "content": "In a manner similar to binary critique, with the \\textit{scalar-valued} critique type of feedback, users evaluate the performance of a policy. In this case, the magnitude of the scalar-valued feedback determines how good or bad a policy is. This method can be used on its own to learn a reward function of the environment purely through human feedback. However, it has been shown that humans usually provide nonoptimal numerical reward signals with this approach .", "cites": [1340], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of scalar-valued critique feedback in interactive reinforcement learning and mentions its use for reward function learning. It cites one paper but does not integrate or synthesize ideas from multiple sources, nor does it offer deeper critical evaluation or comparative analysis. The generalization is minimal, focusing only on the surface-level description of the feedback type."}}
{"id": "d9c9d0c0-9340-47e1-be51-250809392875", "title": "Action Advice", "level": "subsubsection", "subsections": [], "parent_id": "62c9329a-f306-4f26-ab6a-7cb7723a61a1", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Design Guides for interactive RL"], ["subsection", "Types of Feedback"], ["subsubsection", "Action Advice"]], "content": "\\label{subsec::actionAdvice}\nIn the \\textit{action advice} type of feedback, the human user provides the agent with the action they believe is optimal at a given state, the agent executes the advised action, and the base RL algorithm continues as usual. From the standpoint of user experience, the immediate effect of the user’s advice on the agent’s policy makes this feedback procedure less frustrating . \nThere are other ways to provide action advice to the agent, such as learning from demonstration , inverse RL , apprentice learning , and imitation learning . All these techniques share the characteristic that the base RL algorithm receives as input an expert demonstration of the task the agent is supposed to learn, which is ideal, as people enjoy demonstrating how agents should behave .", "cites": [1354], "cite_extract_rate": 0.09090909090909091, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a basic description of action advice in interactive RL and mentions related techniques such as inverse RL and imitation learning, but it does not synthesize these ideas in depth or connect them to the cited paper in a meaningful way. It lacks critical evaluation of the methods and only offers a surface-level abstraction by noting that expert demonstrations are 'ideal' based on user preference, without broader conceptual framing."}}
{"id": "d96defb0-ff81-4e22-8d56-f5545ba49056", "title": "Reward Shaping", "level": "subsection", "subsections": [], "parent_id": "0e025963-2eb0-4ce6-8b7d-b6b08a68bdda", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Recent Research Results"], ["subsection", "Reward Shaping"]], "content": "The reward shaping (RS) method aims to mold the behavior of a learning agent by modifying its reward function to encourage the behavior the RL designer wants.\nThomaz and Breazeal analyzed the teaching style of non-expert users depending on the reward channel they had at their disposal in . Users were able to use two types of RS: positive numerical reward and negative numerical reward. These types of feedback directly modify the value function of the RL model. However, when the user gives negative feedback, the agent tries to undo the last action it performed; this reveals the learning progress to the user and motivates them to use more negative feedback, which achieves good performance with less feedback . \nAnother finding in  is that some users give anticipatory feedback to the bot; that is, users assume that their feedback is meant to direct the bot in future states. This analysis displays the importance of studying users' teaching strategies in interactive RL. We need to better understand the user’s preferences to teach agents, as well as how agents should provide better feedback about their learning mechanism to foster trust and improve the quality and quantity of users' feedback.  \nAnother RS strategy is to manually create \\textit{heuristic functions}  that incentivize the agent to perform particular actions in certain states of the environment. This way, the agent automatically receives feedback from the hand-engineered heuristic function. The type of feedback is defined by the RL designer, and it can be given using any of the feedback types reviewed in this paper (i.e., critique or scalar-value). The experiments conducted in  demonstrate that using heuristic functions as input for an interactive RL algorithm can be a natural approach to injecting human knowledge in an RL method.\nThe main shortcoming of heuristic functions is that they are difficult to build and require programming skills. Although it has been investigated how non-experts build ML models in real life , there are not many studies on the use of more natural modes of communication to empower non-experts in ML to build effective heuristic functions that generalize well.     \nThe Evaluative Reinforcement (TAMER) algorithm uses traces of demonstrations as input to build a model of the user that is later used to automatically guide the RL algorithm .  \nWarnell et al. later introduced a version of the TAMER algorithm  modified to work with a deep RL model . In addition to the changes needed to make the TAMER algorithm work with a function approximation that uses a deep convolutional neural network, the authors of  propose a different approach for handling user feedback. Instead of using a loss function that considers a window of samples, they minimize a weighted difference of user rewards for each individual state-action pair.\nSimilarly, the authors of  propose an algorithm called DQN-TAMER that combines the TAMER and Deep TAMER algorithms. This novel combination of techniques aims to improve the performance of the learning agent using both environments and human binary feedback to shape the reward function of the model. Furthermore, Arakawa et al. experimented in a maze-like environment with a robot that receives implicit feedback; in this scenario, the RS method was driven by the facial expression of the user. Since human feedback can be imprecise and intermittent, mechanisms were developed to handle these problems.\nDeep versions of interactive RL methods benefit mostly from function approximation, as use of this technique minimizes the feedback needed to get good results. This advantage is due to the generalization of user feedback among all similar states -- human knowledge is injected into multiple similar states instead of only one.", "cites": [1342, 8443, 1339], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers on reward shaping, connecting ideas about user feedback styles, heuristic functions, and deep RL adaptations to form a coherent narrative. It critically assesses limitations such as the difficulty of building heuristic functions and the imprecision of human feedback, while abstracting toward broader design considerations like natural communication modes and feedback mechanisms that foster trust and performance."}}
{"id": "91d7743f-68c9-4b72-82bb-eb903db4e80a", "title": "Policy Shaping", "level": "subsection", "subsections": [], "parent_id": "0e025963-2eb0-4ce6-8b7d-b6b08a68bdda", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Recent Research Results"], ["subsection", "Policy Shaping"]], "content": "The policy shaping (PS) approach consists of directly molding the policy of a learning agent to fit its behavior to what the RL designer envisions.\nA PS approach directly infers the user's policy from critique feedback . Griffith et al. introduced a Bayesian approach that computes the optimal policy from human feedback, taking as input the critiques for each state-action pair . The results of this approach are promising, as it outperforms other methods, such as RS. However, PS experiments were carried out using a simulated oracle instead of human users. Further experiments with human users should be conducted to validate the performance of this interactive RL method from a user-experience perspective.\nKrening and Feigh conducted experiments to determine which type of feedback — critique or action advice — creates a better user experience in an interactive RL setting . Specifically, they compared the critique approach in  to the proposed Newtonian action advice in . Compared to the critique approach, the action advice type of feedback got better overall results: it required less training time, it performed objectively better, and it produced a better user experience with it.\nMacGlashan et al. introduced the Convergent Actor-Critic by Humans (COACH) interactive RL algorithm . There is also an extension to this work named deep COACH  that uses a deep neural network coupled with a replay memory buffer and an autoencoder. Unlike the COACH implementation in , deep COACH  uses raw pixels from the testbed as input. The authors argue that using this high-level representation as input means their implementation is better suited for real scenarios. However, the testbed consists of simplistic toy problems, and a recent effort demonstrated that deep neural networks using raw pixels as input spend most of their learning capacity extracting useful information from the scene and just a tiny part on the actual behaviors .", "cites": [1345, 1358, 1350, 1352], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates multiple papers on policy shaping approaches, highlighting their contributions and differences in feedback types and implementations. It provides critical evaluations of the methods, such as the use of simulated oracles and the limitations of deep neural networks in real scenarios. While it identifies some patterns, particularly in the comparison of feedback mechanisms, it does not fully abstract these into broader HCI design principles or theoretical frameworks."}}
{"id": "4bb10864-3aea-4301-94ad-62825167d2e2", "title": "Guided Exploration Process", "level": "subsection", "subsections": [], "parent_id": "0e025963-2eb0-4ce6-8b7d-b6b08a68bdda", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Recent Research Results"], ["subsection", "Guided Exploration Process"]], "content": "Guided exploration process methods aim to minimize the learning procedure by injecting human knowledge to guide the agent’s exploration to states with a high reward.\nThomaz and Breazeal conducted human-subject experiments in the Sophie's Kitchen platform to evaluate the performance of diverse human knowledge integration methods . Their results suggest that using guidance feedback is more effective than using scalar reward feedback. Based upon the algorithm in , Suay et al. proposed a variation in which the user can guide exploration by pointing out goal states in the environment ; the bot then biases its policy to choose actions that lead to the indicated goal. These experiments were generally successful, but their highlight is their finding that using only exploration guides from the user produces the best results and reduces the amount of user feedback needed .\nThere have been efforts to create adaptive shaping algorithms that learn to choose the best feedback channels based on the user's preferred strategies for a given problem . For instance, Yu et al. defined an adaptive algorithm with four different feedback methods at its disposal that are based on exploration bias and reward shaping techniques . To measure the effectiveness of the feedback methods at every time-step, the adaptive algorithm asks for human feedback; with this information, a similarity measure between the policy of the shaping algorithms and the user's policy is computed. Then, according to the similarity metric, the best method is selected using a softmax function, and the value function for the selected method is updated using q-learning. Once the adaptive algorithm has enough samples, it considers the cumulative rewards to determine which feedback methodology is the best. Overall, one of the exploration bias-based algorithms produced better results and was chosen most often by the adaptive algorithm, as well as demonstrating good performance on its own. The authors of  call this algorithm action biasing, which uses user feedback to guide the exploration process of the agent. The human feedback is incorporated into the RL model using the sum of the agent and the user value functions as value functions. \nIn general, using human feedback as guidance for interactive RL algorithms appears to be the most effective in terms of performance and user experience. However, to make human feedback work, it is necessary to have a model of the environment, so the interactive RL algorithm knows which actions to perform to reach the state proposed by the user. This can be a strong limitation in complex environments where precise models are difficult to create.", "cites": [1353], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.5, "abstraction": 3.7}, "insight_level": "high", "analysis": "The section effectively synthesizes key findings from multiple studies, particularly connecting human feedback mechanisms across different papers. It includes critical analysis by discussing the effectiveness of various methods and identifying limitations, such as the need for an environment model. The abstraction level is strong as it generalizes the role of guided exploration and highlights overarching design principles and research implications."}}
{"id": "53aa1681-aebd-4529-8b69-980efcdcdd53", "title": "Inverse Reward Design", "level": "subsection", "subsections": [], "parent_id": "0e025963-2eb0-4ce6-8b7d-b6b08a68bdda", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Recent Research Results"], ["subsection", "Inverse Reward Design"]], "content": "\\textit{Inverse reward design} (IRD) is the process of inferring a true reward function from a proxy reward function.\nIRD  is used to reduce reward hacking failures. According to the proposed terminology in , the hand-engineered reward function named the \\textit{proxy reward function} is just an approximation of the true reward function, which is one that perfectly models real-world scenarios. The process of inferring a true reward function from a proxy reward function is IRD. \nTo infer the true reward function, the IRD method takes as input a proxy reward function, the model of the test environment, and the behavior of the RL designer who created it. Then, using Bayesian approaches, a distribution function that maps the proxy reward function to the true reward function is inferred. This distribution of the true reward function makes the agent aware of uncertainty when approaching previously unseen states, so it behaves in a risk-averse manner in new scenarios. The results of the experiments in  reveal that reward hacking problems lessen with an IRD approach.\nThe main interaction procedure of IRD starts as regular RS, and the system then queries the user to provide more information about their preference for states with high uncertainty. This kind of procedure can benefit from interaction techniques to better explain uncertainty to humans and from compelling techniques to debug and fix problems in the model.", "cites": [1357, 1351], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the core concept of Inverse Reward Design by integrating the definitions and methodologies from both cited papers, explaining how IRD reduces reward hacking through Bayesian inference and user interaction. It provides some critical analysis by pointing out the limitations of proxy reward functions and the need for interaction techniques to address uncertainty. The abstraction is moderate, highlighting the broader benefit of user interaction in refining reward functions, but it stops short of proposing a meta-level framework or general principles beyond IRD."}}
{"id": "760a53f7-e8c7-4a81-9573-707b630dd73c", "title": "Feedback", "level": "subsection", "subsections": [], "parent_id": "cb6faf4f-1b11-4264-a15e-6b3cf8c932f1", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Design Principles for interactive RL"], ["subsection", "Feedback"]], "content": "\\textit{Delay of human feedback} has a great impact on the performance of interactive RL applications. Delay refers to the time that a human user needs to evaluate and deliver their feedback to the interactive RL algorithm. Many examples propose different strategies to deal with feedback delay . \nThe most common approach consists of a delay parameter that expresses to how many past time-steps the current feedback will be applied; this parameter is usually found in practice . Warnell et al. conducted an experiment to quantify the effect of different reward distribution delays on the effectiveness of their algorithm Deep TAMER . This experiment revealed that a small change in the parameters of the expected user feedback timing distribution can have a great impact on performance; expert knowledge on this timing distribution is key to achieving good results.\nA different approach to deal with feedback delay is proposed in . In this case, the interactive RL algorithm pauses the agent exploration process every time-step to give the user time to give feedback. More elaborated approaches assume that the feedback delay follows a probability distribution . Likewise, it has been found that less avid users need more time to think and decide on their feedback . It is therefore important to adapt the feedback delay depending on users’ level of knowledge. \n\\textit{Fatigue} of users and its effects on the quantity and quality of feedback should be considered. It has been observed that humans tend to reduce the quantity of feedback they give over time . The quality also diminishes, as humans tend to give less positive feedback over time. According to , this degradation of the quantity and quality of human feedback also depends on the behavior exhibited by the agent. The authors of  found that humans tend to give more positive feedback when they notice that the agent is improving its policy over time: feedback is therefore policy-dependent . On the other hand, the experiments of  offer evidence to support that human users gradually diminish their positive feedback when the agent shows that it is adopting the proposed strategies. Fachantidis et al. performed experiments to determine the impact of the quality and distribution of feedback on the performance of their interactive RL algorithm .\n\\textit{Motivating users to give feedback} — elements of gamification have been adopted with good results; gamification strategies have been shown to improve the quantity and quality of human feedback .\nSome studies focus on improving the quality and quantity of human feedback by incorporating an active question procedure in the learning agent ; that is, the agent can ask the user to give feedback in particular states. Amir et al. present an active interactive RL algorithm in which both the agent and the demonstrator (another agent) work together to decide when to make use of feedback in a setting with a limited advice budget. First, the agent determines if it needs help in a particular state and asks the demonstrator for attention. Depending on the situation of the agent, the demonstrator determines if it will help or not. The results of these experiments are promising because they achieve a good level of performance while requiring less attention from the demonstrator. \n\\textit{Maximizing the use of feedback} is necessary because interactive RL in complex environments might require up to millions of interaction cycles to get good results . It has been demonstrated that the inclusion of an approximation function that propagates human feedback to all similar states is effective to tackle the sample inefficiency of interactive RL in complex environments such as videogames .", "cites": [1354, 1342, 1350, 1339, 1352], "cite_extract_rate": 0.2631578947368421, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple papers to build a coherent narrative around the challenges and strategies related to human feedback in interactive RL. It abstracts key issues such as delay, fatigue, motivation, and feedback maximization, offering broader principles like policy dependency and feedback propagation. While it provides some critical observations (e.g., feedback quality depends on agent behavior), deeper comparative or evaluative analysis of the approaches is limited."}}
{"id": "833d8a51-f5b7-465d-90c4-e2e4017e9c8d", "title": "Typification of the End-user", "level": "subsection", "subsections": [], "parent_id": "cb6faf4f-1b11-4264-a15e-6b3cf8c932f1", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Design Principles for interactive RL"], ["subsection", "Typification of the End-user"]], "content": "We found that the most important features to typify an end-user in interactive RL involves:\n\\textit{Knowledge level in the task at hand} — this has an impact on the quality, quantity, and delay of feedback, and could limit the use of some feedback types that require more precise information, such as demonstrations or action advice.\n\\textit{Preferred teaching strategy} — this is essential to select the feedback type appropriate for a given application (see Figure \\ref{Fig:iRL}). \n\\textit{Safety concerns} — this refers to the severity of the effects of the agent’s actions if an error in its behavior occurs. If the severity of the errors is high (e.g. property damage, personal harm, politically incorrect behavior, etc.), end-users’ perception and willingness to interact with the agent will diminish . The quality and quantity of feedback are therefore also affected. \nAn end-user typification using these features would help researchers to select the best combination of feedback type and design dimension for a particular interactive RL and for the expected type of end-user. For instance, although all design dimensions assume that the user knows a policy that is at least good enough to solve the task, some design dimensions can better handle non-optimal policies from human feedback . Empirical evidence suggests that the exploration process is the most economical design dimension for interactive RL applications . Nonetheless, myopic agents — which use the policy design dimension — have demonstrated great success in some tasks . \nThe hierarchical decomposition and function approximation design dimensions require human users with a deep understanding of the problem to make effective use of them. Another possibility is combining different design dimensions in the same interactive RL method. It would also be useful to design an active interactive RL that learns which design dimension best suits the application and type of use; in this way, the interactive RL algorithm can minimize the feedback needed from the user . For example, a combination of function approximation and policy design dimensions would enable the use in a videogame environment of an interactive RL that needs only $15$ minutes of human feedback to get positive results .", "cites": [1340, 1339], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key concepts from the cited papers to typify end-users in interactive RL, integrating ideas about feedback types, teaching strategies, and design dimensions. It provides a general framework for selecting appropriate methods based on user characteristics, showing abstraction and analytical depth. While it offers some comparative insights (e.g., different design dimensions' suitability), it does not deeply critique the limitations of individual approaches."}}
{"id": "02dce2e6-acbb-4ba4-93d0-e17e5bb1ae9b", "title": "Fast Interaction Cycles", "level": "subsection", "subsections": [], "parent_id": "cb6faf4f-1b11-4264-a15e-6b3cf8c932f1", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Design Principles for interactive RL"], ["subsection", "Fast Interaction Cycles"]], "content": "In an iterative procedure, such as an interactive RL algorithm, a fast evaluation of the agent’s behavior can substantially reduce the time needed for each teaching loop. Some of the approaches meant to lessen the evaluation process consist of evaluation, visualization, or explanatory techniques.\n\\textit{Evaluation techniques} such as  queries, in which the user selects the best reward function among a set , the presentation of multiple trajectories by the agent that then summarizes its behavior , or crowd evaluations that distribute the evaluation task among various users .\n\\textit{Visualization techniques}, such as the HIGHLIGHTS algorithm , focus on creating summaries of agents so people can better understand an agent's behavior in crucial situations. These summaries are sets of trajectories that provide an overview of the agent's behavior in key states. It is therefore fundamental to define a metric that measures the importance of states. This is calculated as the difference between the highest and lowest expected returns for a given state. That is, a state in which taking a particular action leads to a significant decrease in the expected reward is considered important.\n\\textit{Explanatory techniques} are a different way to address the evaluation problem by explaining the agent’s decision process to the human user. This approach enhances the user's understanding of how the agent learns, interprets, and determines the best action to take in a given state.  \nOne example of uncertainty explanation is introduced in . The authors use the gaze of the agent in their experiments as an indicator of its level of uncertainty in any given state. \nThe main idea of the gaze mechanism is to make the agent explicitly express at every time-step the options it considers most promising by staring at the object of interest. If there is more than one promising action in a given state, the agent will have to focus its gaze on different objects in the environment, communicating indecision to the user and giving the user time to give feedback to the agent. The authors also found that users tend to give more feedback when the agent displays uncertainty, which optimizes the exploration process of the robot.", "cites": [1357], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of techniques aimed at reducing evaluation time in interactive RL, including evaluation, visualization, and explanatory methods. While it integrates these ideas in a general sense, it lacks in-depth critical analysis or comparison of their effectiveness. There is a slight abstraction in identifying the importance of fast interaction cycles, but the discussion remains largely at the level of summarizing specific methods without a deeper synthesis or critique."}}
{"id": "bb502246-2f41-4798-99a8-a999042c1844", "title": "High-dimensional Environments", "level": "subsection", "subsections": [], "parent_id": "5fbef427-1993-432b-805d-fb66864a33ae", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Open Challenges"], ["subsection", "High-dimensional Environments"]], "content": "Making interactive RL feasible for real-world applications with high-dimensional environments is still a challenge. Broadening the domains and applications in which interactive RL is useful is key to extending the impact of ML in people's life. \nRecent interactive RL applications that use an autoencoder to spread human feedback to similar states perform well in high-dimensional environments . One underexploited approach is using crowd-sourced methods to provide human feedback  or build a reward function.", "cites": [1350], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section briefly integrates one cited paper to discuss approaches for handling high-dimensional environments in interactive RL but lacks deeper synthesis with other works. It identifies a potential gap (crowd-sourced methods) but does not critically evaluate the cited paper's limitations or compare multiple methods. Some level of abstraction is present by highlighting a general trend in using autoencoders, but broader principles or frameworks are not fully articulated."}}
{"id": "9008649f-412e-4da6-8c4b-82526ab30825", "title": "Lack of Evaluation Techniques", "level": "subsection", "subsections": [], "parent_id": "5fbef427-1993-432b-805d-fb66864a33ae", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Open Challenges"], ["subsection", "Lack of Evaluation Techniques"]], "content": "The evaluation of interactive RL applications from the HCI perspective is limited; most studies focus on improving the performance of algorithms rather than the interaction experience, but proper evaluation is essential to create practical applications for different types of end-users. For instance, the studies by Krening et al.  present an adequate evaluation of the user experience in an interactive RL setting. \nIn the same context, the \\textit{problem of generalization from simple testbeds} (e.g., GridWorld) exists, as the results are not consistent when using more complex testbeds. For example, even though RS has been demonstrated to be effective and easy to use by all types of users in a GridWorld environment , using the same human knowledge integration method to train agents for the Infinite Mario platform is challenging and ineffective, even for expert users .", "cites": [1342, 1350, 8443], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical perspective by identifying a key challenge in evaluating interactive RL from an HCI standpoint. It synthesizes ideas from multiple cited works to highlight a trend in the field, particularly the issue of generalization from simple to complex environments. While it introduces a critique of the current evaluation focus, the analysis remains at a moderate level without deeper comparative insights or a novel framework."}}
{"id": "973af5c1-7f2a-4a2b-883a-eac2bc594417", "title": "Lack of Human-like Oracles", "level": "subsection", "subsections": [], "parent_id": "5fbef427-1993-432b-805d-fb66864a33ae", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Open Challenges"], ["subsection", "Lack of Human-like Oracles"]], "content": "The use of simulated oracles in the early stages of implementation of interactive RL applications is useful to save time and get an idea of the results with human users . However, it has been found that results with simulated oracles can differ from those with human users  — simulated oracles do not behave like humans. More studies are needed to determine what features make human feedback different from simulated oracles.", "cites": [1353], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 2.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly discusses the issue of simulated oracles not behaving like humans in interactive RL but provides minimal synthesis of the cited paper. It hints at a critical observation (simulated vs. human feedback) but lacks deeper analysis or comparison. The abstraction is limited, as it only raises the general idea of needing to understand human feedback differences without deriving broader principles or frameworks."}}
{"id": "c55dbab7-fa2a-4c20-a512-0b949f51a99c", "title": "Fast Evaluation of Behaviors", "level": "subsection", "subsections": [], "parent_id": "5fbef427-1993-432b-805d-fb66864a33ae", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Open Challenges"], ["subsection", "Fast Evaluation of Behaviors"]], "content": "Fast evaluation methods through visualization techniques have not been adequately studied. These approaches include applications to summarize behavior  and explain uncertainty . The main goal of these methods is to reduce the time needed to evaluate the behavior of agents, and assist users to give higher quality feedback.", "cites": [1357], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section is primarily descriptive and lacks depth in synthesis, critical analysis, and abstraction. It only mentions one cited paper and does not integrate ideas from multiple sources or provide a coherent narrative. Additionally, it offers no evaluation or critique of the cited work and fails to generalize to broader patterns or principles in fast evaluation of behaviors."}}
{"id": "448f0e0f-71a7-419c-ae7d-b588f8fa8ebe", "title": "Explainable interactive RL", "level": "subsection", "subsections": [], "parent_id": "5fbef427-1993-432b-805d-fb66864a33ae", "prefix_titles": [["title", "A Survey on Interactive Reinforcement Learning"], ["section", "Open Challenges"], ["subsection", "Explainable interactive RL"]], "content": "RL-based agents have a particular way of representing their environment, depending on the sensors at their disposal and how they interpret the incoming signals. It can therefore be complicated for humans to elicit the state representation of agents, which can lead to giving incorrect feedback to agents. Explainable interactive RL methods aim to help people understand how agents see the environment and their reasoning procedure for computing policies . With more transparency in an agent model, people could find better ways to teach agents.\nAnother way to take advantage of a transparent agent is by debugging its model. That is, users would be able to interpret why the agent made a certain decision in a particular state. This feature would help people find errors or unwanted bias in the model , which is essential in applications that could have a substantial impact on people's lives .", "cites": [1359], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section introduces the concept of explainable interactive RL and connects it to the importance of transparency and user understanding. It cites one paper to illustrate how interactive tools can help detect bias in AI models, but does not synthesize ideas across multiple works. The analysis is limited, focusing more on potential benefits than on evaluating or critiquing the cited work. However, it attempts to abstract a broader principle of model transparency for human oversight and debugging."}}
