{"id": "e9b844de-0b2f-4ecf-94b9-874602da0564", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "40bf4670-36c8-4014-b074-8f86f999eff7", "prefix_titles": [["title", "A Survey on Evolutionary Neural Architecture Search"], ["section", "Introduction"]], "content": "\\IEEEPARstart{D}{eep} Neural Networks (DNNs), as the cornerstone of deep learning~, have demonstrated their great success in diverse real-world applications, including image classification~, natural language processing~, speech recognition~, to name a few. The promising performance of DNNs has been widely documented due to their deep architectures~, which can learn meaningful features directly from the raw data almost without any explicit feature engineering. Generally, the performance of DNNs depends on two aspects: their architectures and the associated weights. Only when both achieve the optimal status simultaneously, the performance of the DNNs could be expected promising. The optimal weights are often obtained through a learning process: using a continuous loss function to measure the discrepencies between the real output and the desired one, and then the gradient-based algorithms are often used to minimize the loss. When the termination condition is satisfied, which is commonly a maximal iteration number, the algorithm can often find a good set of weights~. This kind of process has been very popular owing to its effectiveness in practice, and has become the dominant practice for weight optimization~, although they are principally local-search~ algorithms. On the other hand, obtaining the optimal architectures cannot be directly formulated by a continuous function, and there is even no explicit function to measure the process for finding optimal architectures.\nTo this end, there has been a long time that the promising architectures of DNNs are manually designed with rich expertise. This can be evidenced from the state of the arts, such as VGG~, ResNet~ and DenseNet~. These promising Convolutional Neural Network (CNN) models are all manually designed by the researchers with rich knowledge in both neural networks and image processing. However, in practice, most end users are not with such kinds of knowledge. Moreover, DNN architectures are often problem-dependent. If the distribution of the data is changed, the architectures must be redesigned accordingly. Neural Architecture Search (NAS), which aims to automate the architecture designs of deep neural networks, is identified as a promising way to address the challenge aforementioned. \nMathematically, NAS can be modeled by an optimization problem formulated by Equation~(\\ref{equ_defination_NAS}):\n\\begin{equation}\n\\label{equ_defination_NAS}\n\\left\\{\n\\centering\n\\begin{array}{c}\n\\argmin_A = \\mathcal{L}(A, \\mathcal{D}_{train}, \\mathcal{D}_{fitness}) \\\\\ns.t.\\quad  A \\in \\mathcal{A} \\\\\n\\end{array}\n\\right.\n\\end{equation}\nwhere $\\mathcal{A}$ denotes the search space of the potential neural architectures, $\\mathcal{L}(\\cdot)$ measures the performance of the architecture $A$ on the fitness evaluation dataset $\\mathcal{D}_{fitness}$ after being trained on the training dataset $\\mathcal{D}_{train}$. The $\\mathcal{L}(\\cdot)$ is usually non-convex and non-differentiable~. In principle, NAS is a complex optimization problem experiencing several challenges, e.g., complex constraints, discrete representations, bi-level structures, computationally expensive characteristics and multiple conflicting criteria. NAS algorithms refer to the optimization algorithms which are specifically designed to effectively and efficiently solve the problem represented by Equation~(\\ref{equ_defination_NAS}). The initial work of NAS algorithms is commonly viewed as the work in~, which was proposed by Google. The pre-print version of this work was firstly released in the website of arXiv\\footnote{\\url{https://arxiv.org/abs/1611.01578}} in 2016, and then was formally accepted for publication by the International Conference on Learning Representations (ICLR) in 2017. Since then, a large number of researchers have been investing tremendous efforts in developing novel NAS algorithms.\nBased on the optimizer employed, existing NAS algorithms can be broadly classified into three different categories: Reinforcement Learning (RL)~ based NAS algorithms, gradient-based NAS algorithms, and Evolutionary Computation (EC)~ based NAS algorithms (ENAS). Specifically, RL based algorithms often require thousands of Graphics Processing Cards (GPUs) performing several days even on median-scale dataset, such as the CIFAR-10 image classification benchmark dataset~. Gradient-based algorithms are more efficient than RL based algorithms. However, they often find the ill-conditioned architectures due to the improper relation for adapting to gradient-based optimization. Unfortunately, the relation has not been mathematically proven. In addition, the gradient-based algorithms require to construct a supernet in advance, which also highly requires expertise. The ENAS algorithms solve the NAS by exploiting EC techniques. Specifically, EC is a class of population-based computational paradigms, simulating the evolution of species or the behaviors of the population in nature, to solve challenging optimization problems. In particular, Genetic Algorithms (GAs)~, Genetic Programming (GP)~, and Particle Swarm Optimization (PSO)~ are widely used EC methods in practice. Owing to the promising characteristics of EC methods in insensitiveness to the local minima and no requirement to gradient information, EC has been widely applied to solve complex non-convex optimization problems~, even when the mathematical form of the objective function is not available~.\nIn fact, the EC methods had been frequently used more than twenty years ago, searching for not only the optimal neural architectures but also the weights of neural networks simultaneously, which is also termed as neuroevolution~. The major differences between ENAS and neuroevolution lie in two aspects. Firstly, neuroevolution often uses EC to search for both the neural architectures and the optimal weight values, while ENAS for now focuses mainly on searching for the architectures and the optimal weight values are obtained by using the gradient-based algorithms\\footnote{Please note that we still categorize some existing algorithms as the ENAS algorithm, such as API~, EvoCNN~ and EvoDeep~, although they also concern the weights. This is because the optimal weight values of the DNNs searched by them are still obtained by the gradient-based algorithms. They only searched for the best weight initialization values or the best weight initialization method of the DNNs.} immediately after. Secondly, neuroevolution commonly applies to small-scale and median-scale neuron networks, while ENAS generally works on DNNs, such as the deep CNNs~ and deep stacked autoencoders~, which are stacked by the building blocks of deep learning techniques~. Generally, the first work of ENAS is often viewed as the LargeEvo algorithm~ which was proposed by Google who released its early version in March 2017 in arXiv. Afterwards, this paper got accepted into the 34th International Conference on Machine Learning in June 2017. The LargeEvo algorithm employed a GA to search for the best architecture of a CNN, and the experimental results on CIFAR-10 and CIFAR-100~ have demonstrated its effectiveness. Since then, a large number of ENAS algorithms have been proposed. Fig.~\\ref{fig_histogram} shows the number of similar works\\footnote{These ``submissions\" include the ones which have been accepted for publication after the peer-review process, and also the ones which are only available on the arXiv website without the peer-review process.} published from 2017 to 2020 when this survey paper was ready for submission. As can be seen from Fig.~\\ref{fig_histogram}, from 2017 to 2020, the number of submissions grows with multiple scales.\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=0.8\\linewidth]{histogram}\n\t\\caption{The number of submissions refers to the works of evolutionary neural architecture search. The data is collected from Google Scholar with the keywords of ``evolutionary\" OR ``genetic algorithm'' OR ``particle swarm optimization\" OR ``PSO\" OR ``genetic programming\" AND ``architecture search\" OR ``architecture design'' OR ``CNN'' OR ``deep learning'' OR ``deep neural network'' and the literature on Neural Architecture Search collected from the AutoML.org website by the end of 2020. With these initially collected data, we then have carefully checked each manuscript to make its scope accurately within the evolutionary neural architecture search.}\n\t\\label{fig_histogram}\n\t\\vspace{-0.3cm}\n\\end{figure}\nA large number of related submissions have been made available publicly, but there is no comprehensive survey of the literature on ENAS algorithms. Although recent reviews on NAS have been made in~, they mainly focus on reviewing different methods to realize NAS, rather than concentrating on the ENAS algorithms. Specifically, Elsken \\textit{et al.}~ divided NAS into three stages: search space, search strategy, and performance estimation strategy. Similarly, Wistuba \\textit{et al.}~ followed these three stages with an additional review about the multiple objectives in NAS. Darwish \\textit{et al.}~ made a summary of Swarm Intelligence (SI) and Evolutionary Algorithms (EAs) approaches for deep learning, with the focuses on both NAS and other hyperparameter optimization. Stanley \\textit{et al.} ~ went through a review of neuroevolution, aiming at revealing the weight optimization rather than the architecture of neural networks. Besides, most of the references in these surveys are pre-2019 and do not include an update on the papers published during the past two years when most ENAS works were published. This paper presents a survey involving a large number of ENAS papers, with the expectation to inspire some new ideas for enhancing the development of ENAS. To allow readers easily concentrating on the technical part of this survey, we also follow the three stages to introduce ENAS algorithms, which has been widely adopted by existing NAS survey papers~, but with essential modifications made to specifically suit ENAS algorithms.\nThe remainder of this paper is organized as follows. The background of ENAS is discussed in Section~\\ref{sec_background}. Section~\\ref{sec_encoding_space} documents different encoding spaces, initial spaces and search spaces of ENAS algorithms. In Section~\\ref{sec_architectureEncoding}, the encoding strategy and architecture representation are introduced. Section~\\ref{sec_Population_operators} summarizes the process of population updating, including evolutionary operators and selection strategies. Section~\\ref{sec_evaluation} shows multiple ways to speed up the evolution. Section~\\ref{sec_application} presents the applications of ENAS algorithms. Section~\\ref{sec_challenges_and_issues} discusses the challenges and prospects, and finally Section~\\ref{sec_conclusion} presents conclusions.", "cites": [96, 1110, 166, 6822, 514, 8441, 6821, 6823, 8247, 97, 6824, 7, 684, 872, 6820, 6825], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 28, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple papers to provide a coherent narrative on the evolution and application of EC-based NAS. It critically compares RL, gradient-based, and EC-based NAS approaches, highlighting their limitations and advantages. The section also abstracts broader patterns, such as the scalability challenges of neuroevolution and the distinction between architecture and weight optimization."}}
{"id": "fb246415-ce9a-4259-827b-2d75a5822aa3", "title": "Evolutionary Search Strategy", "level": "subsection", "subsections": [], "parent_id": "2857eea5-3541-4f2f-8282-08d5126a14b3", "prefix_titles": [["title", "A Survey on Evolutionary Neural Architecture Search"], ["section", "Background"], ["subsection", "Evolutionary Search Strategy"]], "content": "\\label{sec2_1}\nENAS is distinguished from other NAS methods by its employed optimization approach, i.e., the EC methods, where the optimization approaches can be further subdivided based on the search strategy that the optimization approach adopted. Fig.~\\ref{fig_EC_taxonomy} provides such an illustration from three aspects: EAs, Swarm Intelligence (SI), and others, and more detailed statistics can be observed in Table~\\ref{table_EC}. In practice, the EA-based methods account for the majority of existing ENAS algorithms, where the GA takes a large part of EA-based methods. The other categories of EC methods are also important parts for realizing ENAS algorithms, such as GP, Evolutionary Strategy (ES), PSO, Ant Colony Optimization (ACO)~, Differential Evolution (DE)~, Firefly Algorithm (FA)~. In this paper, the Hill-Climbing Algorithm (HCA) is classified into EC paradigm because it can be regarded as an EA with a simple selection mechanism and without crossover operation~. HCA has been well known as a widely used local search algorithm in memetic algorithms~.\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=1\\linewidth]{taxonomy1}\n\t\\caption{The categories of ENAS from EC methods regarding the search strategies.}\n\t\\label{fig_EC_taxonomy}\n\t\\vspace{-0.3cm}\n\\end{figure}", "cites": [9078, 6826], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the different evolutionary search strategies used in ENAS, listing several algorithms like GA, GP, ES, PSO, etc. It includes a figure and table for categorization but does not synthesize or connect ideas across the cited papers in depth. There is minimal critical evaluation or abstraction beyond the listed methods, and the analysis remains largely descriptive."}}
{"id": "a76618db-7274-4a81-a316-4d0b05a79b1e", "title": "Common Neural Networks in ENAS", "level": "subsection", "subsections": [], "parent_id": "2857eea5-3541-4f2f-8282-08d5126a14b3", "prefix_titles": [["title", "A Survey on Evolutionary Neural Architecture Search"], ["section", "Background"], ["subsection", "Common Neural Networks in ENAS"]], "content": "\\label{sec2_2}\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=0.9\\linewidth]{taxonomy2}\n\t\\caption{The categories of ENAS from neural network perspective.}\n\t\\label{fig_NN_taxonomy}\n\t\\vspace{-0.3cm}\n\\end{figure}\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=1\\linewidth]{NN}\n\t\\caption{Examples of different neural architectures in ENAS. (a) CNN. (b) DBN. (c) AE. (d) RNN. }\n\t\\label{fig_NN}\n\t\\vspace{-0.3cm}\n\\end{figure}\nThe end product of ENAS is all about DNNs, which can be broadly divided into five different categories: CNN, Deep Belief Network (DBN), Stacked Auto-Encoder (SAE), Recurrent Neural Network (RNN) and others. The brief fact can be seen from Fig.~\\ref{fig_NN_taxonomy}, and a detailed illustration will be shown in Table~\\ref{table_EC}. \nMost ENAS methods are proposed for searching for the optimal CNN architectures. This is because many hand-crafted CNNs, such as VGG~, ResNet~ and DenseNet~, have demonstrated their superiority in handling the image classification tasks which is the most successful applications in the deep learning field. Generally, the CNN architecture is composed of the convolutional layers, the pooling layers and the fully-connected layers, and a common example of CNNs is shown in Fig.~\\ref{fig_NN} (a). The optimization of CNN architecture is mainly composed of three aspects: the hyperparameters of each layer~, the depth of the architecture~ and the connections between layers~. In practice, the majority of the ENAS methods consider the above three aspects collectively~.\nDBN~ is made up by stacking multiple Restricted Boltzmann Machines (RBMs), and an example can be seen in Fig.~\\ref{fig_NN} (b). Specifically, RBMs have connections only between layers, while without any inter-layer connection. Meanwhile, RBMs allow the DBN to be trained in an unsupervised manner to obtain a good weight initialization. There are two commonly used types of hyperparameters needed to be optimized in DBN: the number of neurons in each layer~ and the number of layers~.\nAn SAE is constructed by stacking multiple of its building blocks which are AEs. An AE aims to learn meaningful representations from the raw input data by restoring its input data as its objective~. Generally, an AE is typically composed of two symmetrical components: the encoder and the decoder, and an example including both parts is shown in Fig.~\\ref{fig_NN} (c). Generally, some ENAS methods only take the encoder into evolution~ because the decoder part is symmetric with the encoder and can be derived accordingly. Yet, some other ENAS methods optimize the hyperparameters of encoders and decoders separately~.\nThe most significant difference between RNNs and the neural networks introduced above is its recurrent connection. Fig.~\\ref{fig_NN} (d) shows the time-expanded structure of an RNN, where the value of current hidden layer $h^{t}$ is influenced by the value at its previous time slot $h^{t-1}$ and the output value of its previous layer. Because these layers are reused, all the weights (i.e., \\textit{U}, \\textit{W} and \\textit{V} in the figure) are shared. Different from focusing on the number of neurons and the number of layers in the feedforward neural network, some ENAS methods concern about how many times the RNN should be unfold~. In addition, there remain other neural networks like typical DNNs, which are made up of only fully-connected layers, where the connections are formed by all the neurons in the two adjacent layers. Because such kind of DNNs is not the major target investigated by NAS, we will not introduce it in detail.\nWhen the ENAS algorithms are applied to these DNNs, the goal is to find the best architecture-related parameters. Specifically, for CNNs, they are the number of convolutional layers, pooling layers, fully-connected layers, and parameters related to these layers (such as the kernel size of the convolutional layers, the pooling type, and the number of neurons for fully-connected layers, and so on) as well as their connection situation (such as the dense connection and the skip connection). For DBN and SAE, they are the number of their building blocks, i.e., the RBM for DBN and the AE for SAE, and the number of neurons in each layer. For RNN, in addition to the architecture-related parameters mentioned above, the number of the time slot is also an important parameter to be optimized by ENAS algorithms. For the traditional DNNs, the ENAS algorithms often concern about the neuron number of each layer. In addition, some ENAS algorithms also concern the weights, such as the weight initialization method and weigh initial values. Table~\\ref{table_parameters} summarizes the detail of the common parameters optimized by ENAS methods in different types of neural networks, including the CNNs as the most popular type. The details of these ENAS algorithms will be documented in the following sections. \n\\begin{table}[]\n\t\\renewcommand\\arraystretch{0.8}\n\t\\caption{Common Parameters Optimized in Different DNNs.} \n\t\\label{table_parameters}\n\t\\centering\n\t\\begin{tabular}{p{1.2cm}|p{1.5cm}|p{4.5cm}}\n\t\t\\hline\n\t\t& \\multicolumn{2}{c}{Parameters}                                                                                                                                                                             \\\\ \\hline\n\t\t\\multirow{4}{*}{CNN} & global parameters       & number of layers, connections between layers                                                                                                               \\\\ \\cline{2-3} \n\t\t& convolution layer       & filter size (width and height), stride size (width and height), feature map size, convolution type, standard deviation and mean value of the filter elements \\\\ \\cline{2-3} \n\t\t& pooling layer           & filter size (width and height), stride size (width and height), pooling type                                                                              \\\\ \\cline{2-3} \n\t\t& fully-connected layer   & number of neurons, standard deviation and mean value of weights                                                                                             \\\\ \\hline\n\t\tDBN, AE              & \\multicolumn{2}{l}{number of hidden layers, neurons per layer}                                                                                                                                             \\\\ \\hline\n\t\tRNN                  & \\multicolumn{2}{l}{\\shortstack[l]{number of hidden layers, neurons per layer, number of\\\\ time slot}}                                                                                                                        \\\\ \\hline\n\t\\end{tabular}\n\\vspace{-0.3cm}\n\\end{table}", "cites": [6822, 96, 514, 6821, 870, 8247, 97, 6827, 166], "cite_extract_rate": 0.47368421052631576, "origin_cites_number": 19, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of common neural network architectures used in ENAS, with some synthesis of related concepts (e.g., how ENAS adapts to CNNs, DBNs, AEs, and RNNs). However, it lacks deeper critical analysis or evaluation of the cited works, and while it identifies general patterns, it does not abstract them into higher-level principles or insights."}}
{"id": "7fb1f2d3-d962-4d8c-8b81-c32933ee534a", "title": "Encoding Space", "level": "section", "subsections": ["c45304b4-d500-4803-95c8-a1b032fcb338", "6c612948-3e12-4637-8d86-3b3c5b4c17d5", "0e3df321-149a-48ba-be12-da3f243c2af6"], "parent_id": "40bf4670-36c8-4014-b074-8f86f999eff7", "prefix_titles": [["title", "A Survey on Evolutionary Neural Architecture Search"], ["section", "Encoding Space"]], "content": "\\label{sec_encoding_space}\n\\begin{table*}[]\n\t\\renewcommand\\arraystretch{0.8}\n\t\\caption{Different Encoding Space and the Constraints} \n\t\\label{table_constrains}\n\t\\vspace{-0.3cm}\n\t\\begin{center}\n\t\t\\begin{tabular}{p{3.8cm}|p{1.8cm}|p{2cm}|p{2.5cm}|p{5.5cm}}\n\t\t\t\\hline\n\t\t\t& Fixed depth & Rich initialization & Partial fixed structure & Relatively few constraints\\\\\n\t\t\t\\hline\n\t\t\tLayer-based encoding space & ~ & ~ & ~ & ~\\\\\n\t\t\t\\hline\n\t\t\tBlock-based encoding space & ~ & ~ & ~ & ~ \\\\\n\t\t\t\\hline\n\t\t\tCell-based encoding space & & & ~ & ~\\\\\n\t\t\t\\hline\n\t\t\tTopology-based encoding space &  & ~ & & ~ \\\\\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t\\end{center}\n\t\\vspace{-0.5cm}\n\\end{table*}\nThe encoding space contains all the valid individuals encoded in the population. In terms of the basic units, the encoding space can be divided into three categories according to the basic units they adopt. They are the layer-based encoding space, the block-based encoding space and the cell-based encoding space. In addition, some ENAS methods do not take the configuration of the basic unit into consideration, but care the connections between units. Such kind of encoding space is often called the topology-based encoding space.\nIn addition, the constraints on the encoding space are important. This is because they represent the human intervention which restricts the encoding space and lightens the burden of the evolutionary process. A method with a mass of constraints can obtain a promising architecture easily but prevent to design any novel architecture that does not follow the constraints. Furthermore, the different sizes of the search space would greatly affect the efficiency of the evolution. On the other hand, the effectiveness of the ENAS methods cannot be guaranteed if there is no constraint on the search space: one extreme case is that all the individuals in the search space are mediocre. In this case, an excellent individual can be obtained even without selection. Table~\\ref{table_constrains} shows different kinds of encoding spaces and the constraints of existing ENAS algorithms, where the first row shows the constraints and the first column displays the encoding space. In the following, we will discuss them at length.", "cites": [6851, 6838, 6837, 6844, 6841, 8955, 6829, 6832, 9079, 6850, 6836, 6835, 6843, 6821, 6846, 8247, 6847, 9080, 6834, 6845, 6842, 6826, 6830, 6822, 6848, 870, 6853, 6840, 6833, 6852, 6831, 6849, 6824, 6827, 6839, 6828], "cite_extract_rate": 0.3302752293577982, "origin_cites_number": 109, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section introduces the concept of encoding spaces in ENAS and categorizes them into layer-based, block-based, cell-based, and topology-based. It mentions constraints and their impact on search efficiency and novelty, but lacks detailed synthesis of the cited works. The papers are listed and briefly referenced, but there is minimal integration of their ideas into a cohesive narrative or comparison. The analysis remains superficial, with no deep evaluation or abstraction of broader principles."}}
{"id": "c45304b4-d500-4803-95c8-a1b032fcb338", "title": "Encoding Space and Constraints", "level": "subsection", "subsections": [], "parent_id": "7fb1f2d3-d962-4d8c-8b81-c32933ee534a", "prefix_titles": [["title", "A Survey on Evolutionary Neural Architecture Search"], ["section", "Encoding Space"], ["subsection", "Encoding Space and Constraints"]], "content": "The layer-based encoding space denotes that the basic units in the encoding space are the primitive layers, such as convolution layers and fully-connected layers. This would lead to a huge search space, since it tries to encode so much information in the search space. However, it may take more time to search for a promising individual because there are more possibilities to construct a well-performed DNN from the primitive layers. For example, a promising CNN is commonly composed of hundreds of primitive layers, accordingly, the search process will consume more time to search for such a deep architecture by iteratively stacking the primitive layers. In addition, the promising DNN may not be found if only with the primitive layers. For instance, the promising performance of ResNet is widely recognized due to its skip connections which cannot be represented by the primitive layers.\nTo alleviate the above problems, the block-based encoding space is developed, where various layers of different types are combined as the blocks to serve as the basic unit of the encoding space. Traditional blocks are ResBlock~, DenseBlock~, ConvBlock (Conv2d + BatchNormalization + Activation)~ and InceptionBlock~, etc. Specifically, layers in the blocks have a specific topological relationship, such as the residual connection in ResBlock and the dense connections in DenseBlock. These blocks have promising performance and often require fewer parameters to build the architecture. So it is principally easier to find a good architecture in the block-based encoding space compared to the layer-based encoding space. Some ENAS algorithms used these blocks directly, such as~, while other methods proposed different blocks for different purposes. For example, Chen \\textit{et al.}~ proposed eight blocks including ResBlock and InceptionBlock encoded in a 3-bit string, and used Hamming distance to determine the similar blocks. Song \\textit{et al.}~ proposed three residual-dense-mixed blocks to reduce the amount of computation due to the convolution operation of image super-resolution tasks. \nThe cell-based encoding space is similar to the block-based one, and can be regarded as a special case of the block-based space where all the blocks are the same. The ENAS algorithms employing this space build the architectures by stacking repeated motifs. Chu \\textit{et al.}~ divided the cell-based space into two independent parts: the micro part containing the parameters of cells, and the macro part defining the connections between different cells. To be more specific, the cell-based encoding space concentrates more on the micro part. Different from the block-based space, the layers in the cell can be combined more freely, and the macro part is always determined by human expertise~. Some widely used encoding spaces are classified in this category. For example, NAS-Bench-101~ and NAS-Bench-201~ search for different combinations of layers and connections in cells, and then stacked the cells sequentially. In addition, NASNet~ and DARTS~ search for two kinds of cells, namely normal cell and reduction cell, and each stacked cell is connected to the two previous cells. The cell-based space greatly reduces the size of the encoding space. This is because all the basic units in the encoding space are the same, and the number of parameters in terms of constructing the promising DNN is much fewer. However, Frachon \\textit{et al.}~ claimed that there is no theoretical basis for that the cell-based space can help to obtain a good architecture. \nIn contrast, the topology-based space does not consider the parameters or the structure of each unit (layer or block), yet they are only concerned about the connections between units. One classical example is the one-shot method which treats all the architectures as different subgraphs of a supergraph~. Yang \\textit{et al.}~ proposed CARS to search for the architecture by choosing different connections in the supergraph, and then the architecture was built by the chosen subgraph. CARS can be classified into the topology-based category because it aims at deciding whether or not to keep the connections in the supergraph. But from the perspective of building the architecture, CARS can also be classified into the cell-based category. This is because the subgraph cell was stacked several times to build the architecture. Another typical case is pruning. Wu \\textit{et al.}~ employed a shallow VGGNet~ on CIFAR-10, and the aim was to prune unimportant weight connections from the VGGNet.\nAs observed from Table~\\ref{table_constrains}, the constraints on the encoding space mainly focus on three aspects: fixed depth, rich initialization and partial fixed structure. The fixed depth means all the individuals in the population have the same depth. The fixed depth is a strong constraint and largely reduces the size of the encoding space. Please note that the fixed depth is different from the \\textit{fixed-length encoding strategy} which will be introduced in Section~\\ref{sec_architectureEncoding}. In Genetic CNN~, for example, the \\textit{fixed-length encoding strategy} only limits the maximum depth. The node which is isolated (no connection) is simply ignored. By this way, the individuals can obtain different depths. The second constraint is rich initialization (i.e., the \\textit{well-designed space} to be discussed in Section~\\ref{sec_initial_space}), and it is also a limitation in practice that requires a lot of expertise. In this case, the initialized architectures are manually designed, which goes against the original intention of NAS. The partial fixed structure  means the architecture is partially settled. For example, in~, a max-pooling layer is added to the network after every set of four convolution layers.\nIn Table~\\ref{table_constrains}, the relatively few constraints category means that those methods have no restrictions like the three aspects discussed above. However, it does not imply there is no constraint. For example, in the classification task, the fully-connected layer is often used as the tail of the whole DNNs in some methods~. Moreover, the maximum length is predefined in many methods including both \\textit{fixed-length encoding strategy} methods~ and \\textit{variable-length encoding strategy} methods~ resulting in preventing the method from discovering a deeper architecture. Wang \\textit{et al.}~ tried to break the limit of maximum length by using a Gaussian distribution initialization mechanism. Irwin \\textit{et al.}~ broke the limit by using the evolutionary operators, the crossover operator and the mutation operator, to extend the depth to any size.\nGenerally, the encoding space can be served as the basis of the search space and the initial space. In practice, the encoding space is often the same as the search space, while the initial space is often a subspace of the encoding space. Fig.~\\ref{fig_spaces} shows the relationship between the three spaces. The search space is larger than the initial space when some manual constraints are added to the population initialization. When there is no such manual constrains, search space and initial space are equivalent. Furthermore, the initial space determines what kind of individuals may appear in the initial population, and the search space determines what kind of individuals may appear in the evolutionary search process, as illustrated in Fig.~\\ref{fig_flowchart}. In the following subsections, we will discuss the initial space and the search space in Subsections~\\ref{sec_initial_space} and~\\ref{sec_search_space}, respectively.\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=0.8\\linewidth]{spaces}\n\t\\caption{The relationship between encoding space, search space and initial space.}\n\t\\label{fig_spaces}\n\t\\vspace{-0.3cm}\n\\end{figure}", "cites": [96, 6524, 514, 6821, 6848, 870, 305, 871, 544, 6831, 97, 6849, 6842, 872, 6827, 6839, 6854, 6826], "cite_extract_rate": 0.72, "origin_cites_number": 25, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.7, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a coherent synthesis of different encoding spaces in EC-based NAS, connecting concepts from multiple papers to highlight their relationships, trade-offs, and implications. It includes critical analysis by discussing limitations, such as the lack of theoretical basis for cell-based spaces, and evaluates practical constraints like fixed depth versus flexible encoding strategies. The abstraction is strong, as it generalizes encoding approaches into categories (layer-based, block-based, cell-based, topology-based) and identifies overarching principles and design choices."}}
{"id": "6c612948-3e12-4637-8d86-3b3c5b4c17d5", "title": "Initial Space", "level": "subsection", "subsections": [], "parent_id": "7fb1f2d3-d962-4d8c-8b81-c32933ee534a", "prefix_titles": [["title", "A Survey on Evolutionary Neural Architecture Search"], ["section", "Encoding Space"], ["subsection", "Initial Space"]], "content": "\\label{sec_initial_space}\nIn general, there are three types of architecture initialization approaches in the initial space: starting from trivial initial conditions~, randomly initialization in the encoding space~ and staring from a well-designed architecture (also termed as rich initialization)~. These three types of initialization correspond to three different initial spaces: \\textit{trivial space}, \\textit{random space} and \\textit{well-designed space}.\nThe \\textit{trivial space} contains only a few primitive layers. For example, the LargeEvo algorithm~ initialized the population in a \\textit{trivial space} where each individual constitutes just a single-layer model with no convolutions. Xie \\textit{et al.}~ experimentally demonstrated that a \\textit{trivial space} can evolve to a competitive architecture. The reason for using as little experience as possible is to justify the advantage of EC-based methods where some novel architectures can be discovered, and most of the discovery is different from the manually designed DNN architectures. On the contrary, the \\textit{well-designed space} contains the state-of-the-art architectures. In this way, a promising architecture can be obtained at the beginning of the evolution, whereas it can hardly evolve to other novel architectures. Actually, many of ENAS methods adopting this initial space focus on improving the performance upon the well-designed architecture. For example, the architecture pruning aims at compressing DNNs by removing less important connections~. For a \\textit{random space}, all the individuals in the initial population are randomly generated in the limited space, and it has been adopted by many methods, such as~. The aim of this type of initial spaces is also to reduce the intervention of human experience in the initial population.", "cites": [6829, 8247, 6821, 6827], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers by categorizing initialization approaches and linking them to specific methods and motivations. It offers some abstraction by grouping algorithms into 'trivial space', 'random space', and 'well-designed space' and explains their general implications for EC-based NAS. However, the critical analysis is moderate, as it points out trade-offs between spaces but does not deeply evaluate their effectiveness or limitations in a comparative or nuanced manner."}}
{"id": "13bf9942-9456-4154-adf6-fca4ab8f93d7", "title": "Encoding Strategy", "level": "section", "subsections": ["c0d67ff4-2346-4359-81e9-cfc247725ee8", "2960167f-5dd3-47f9-b647-47b27f2be6ee"], "parent_id": "40bf4670-36c8-4014-b074-8f86f999eff7", "prefix_titles": [["title", "A Survey on Evolutionary Neural Architecture Search"], ["section", "Encoding Strategy"]], "content": "\\label{sec_architectureEncoding}\nThis section will discuss how to encode a network architecture into an individual of the EC methods. Each ENAS method needs to determine its encoding strategy before starting the first stage of the ENAS method, i.e., the population initialization. The most intuitive difference in different encoding strategies of ENAS methods is the length of the encoded individuals. \nGenerally, the encoding strategies can be divided into two different categories according to whether the length of an individual changes or not during the evolutionary process. They are the \\textit{fixed-length encoding strategy} and the \\textit{variable-length encoding strategy}. Particularly, the individuals have the same length during the evolutionary process when the \\textit{fixed-length encoding strategy} is used. In contrast, the individuals are with different lengths during the evolutionary process if the \\textit{variable-length encoding strategy} is employed. The advantage of the \\textit{fixed-length encoding strategy} is that it is easy to use standard evolutionary operations which are originally designed for the individuals with equal lengths. In Genetic CNN~, for example, the fixed-length binary string helped the evolutionary operators (especially the crossover) with easy implementation. Another example is~,  where Loni \\textit{et al.} used a fixed-length string of genes to represent the architecture which led to easy implementation of the one-point crossover in the corresponding encoded information. However, a proper maximal length must be predefined for the fixed-length encoding strategy. Because the maximal length relates to the optimal depth of the DNN architecture, which is unknown in advance, the corresponding ENAS algorithms still rely on expertise and experience. \nCompared with the \\textit{fixed-length encoding strategy}, the \\textit{variable-length encoding strategy} dose not require the human expertise regarding the optimal depth in advance, which has the potential to be fully automated. In addition, the advantage of this encoding strategy is that it can define more details of the architecture with freedom. For example, when solving a new task where there is no expertise in knowing the optimal depth of the DNN, we just initialize the individuals with a random depth, and the optimal depth can be found through the variable-length encoding strategy, where the depths of the corresponding DNN can be changed during the evolutionary process. However, the variable-length encoding strategy also brings some drawbacks. Because the traditional evolutionary operators might not be suitable for this kind of encoding strategy, the corresponding evolutionary operators need to be redesigned, where an example can be seen in~. Another disadvantage is that, due to the flexibility of variable length encoding, it could generally result in over-depth architectures, which sometimes further leads to more time-consuming fitness evaluation. Please note that some works claimed their use of the variable-length encoding strategy, where each individual is designed with a maximal length, and then the placeholder is used to indicate the gene's validation of the gene~. For example, the individual is designed to have $1,000$ genes, while some genes having the values of zeros do not participate in the evolutionary process. In this paper, we also categorize these methods into the fixed-length encoding strategy.\nIn addition, most of the DNN architectures can be represented as directed graphs which are made up of different basic units and the connections between the units. Therefore, the encoding for architecture can be divided into two aspects: configurations of basic units and connections, which will be discussed in the following subsections.", "cites": [6821, 6827], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key concepts from multiple EC-based NAS works, particularly highlighting the distinction between fixed-length and variable-length encoding strategies. It critically evaluates the pros and cons of each approach, noting the reliance on human expertise in fixed-length methods and the potential inefficiencies in variable-length ones. The abstraction is strong, as it generalizes encoding strategies into a broader framework and identifies underlying design principles."}}
{"id": "c0d67ff4-2346-4359-81e9-cfc247725ee8", "title": "Encoding for Configurations of Basic Units", "level": "subsection", "subsections": [], "parent_id": "13bf9942-9456-4154-adf6-fca4ab8f93d7", "prefix_titles": [["title", "A Survey on Evolutionary Neural Architecture Search"], ["section", "Encoding Strategy"], ["subsection", "Encoding for Configurations of Basic Units"]], "content": "In practice, different basic units have different configurations, such as layers, blocks and cells, which are demonstrated in Section~\\ref{sec_encoding_space}. For example, in CNNs, there are multiple parameters in the primitive layers which can be seen in Table~\\ref{table_parameters}. As for the DenseBlock implemented by Sun \\textit{et al.}~, only two parameters are needed to build the block. Because the configuration of cell is more flexible than that of blocks in CNNs, which can be regarded as a microcosm of a complete neural network. For example, the cells in~ are made up by a combination of 10 layers selected from 8 different primitive layers. But the cells do not include some of the configurations of primitive layer, such as the feature map size which is an important parameter in the primitive layer~.", "cites": [6821, 870], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of encoding strategies for configurations of basic units in CNNs but lacks in-depth synthesis of the cited papers. It mentions a few examples and parameters but does not connect or contrast the methods from the two papers in a meaningful way. Additionally, there is minimal critical evaluation or abstraction to broader principles, suggesting a primarily descriptive approach without substantial insight."}}
{"id": "6fd9ba7f-54c9-40c3-908e-224aaa013157", "title": "Linear Architecture", "level": "subsubsection", "subsections": [], "parent_id": "2960167f-5dd3-47f9-b647-47b27f2be6ee", "prefix_titles": [["title", "A Survey on Evolutionary Neural Architecture Search"], ["section", "Encoding Strategy"], ["subsection", "Encoding for Connections"], ["subsubsection", "Linear Architecture"]], "content": "The linear architecture can be found in different kinds of architectures including the one generated from the layer-based encoding space and the block-based encoding space. Its widespread use in ENAS stems from its simplicity. No matter how complex the internal of the basic units is, many ENAS methods stack the basic units one by one to build up the skeleton of the architecture which is linear. For example, in AE-CNN~, Sun \\textit{et al.} stacked different kinds of blocks to generate the architecture.\nOne special case is a linear architecture generated from the layer-based encoding space. In this case, there is no need to solely encode the connections, and only the parameters in each basic unit are enough to build an architecture. One classical example can be seen in~ where Sun \\textit{et al.} explored a great number of parameters based on a linear CNN architecture. However, most architectures are not designed to be linear. The skip connections in ResNet~ and the dense connections in DenseNet~ show the ability to build a good architecture.", "cites": [96, 6821, 97], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of the linear architecture in EC-based NAS, mentioning its simplicity and use in stacking basic units. It includes references to specific works but does not meaningfully synthesize them into a broader narrative, nor does it offer critical evaluation or abstract insights into the design choices or implications of using linear architectures."}}
{"id": "a7c80d6f-1308-48d5-9a45-859a5cedcd95", "title": "Non-linear Architecture", "level": "subsubsection", "subsections": [], "parent_id": "2960167f-5dd3-47f9-b647-47b27f2be6ee", "prefix_titles": [["title", "A Survey on Evolutionary Neural Architecture Search"], ["section", "Encoding Strategy"], ["subsection", "Encoding for Connections"], ["subsubsection", "Non-linear Architecture"]], "content": "Firstly, we will introduce two approaches to encoding a non-linear architecture in this subsection. Specifically, the adjacent matrix is the most popular way to represent the connections for non-linear architectures. Genetic CNN~ used a binary string to represent the connection, and the string can be transformed into a triangular matrix. In the binary string, ``1'' denotes that there is a connection between the two nodes while ``0'' denotes no connection in between. Lorenzo \\textit{et al.}~ used a matrix to represent the skip connections, and this work revolved around the adjacent matrix. Back in the 1990s, Kitano \\textit{et al.}~ began to study the use of the adjacent matrix to represent network connection and explained the process from the connectivity matrix, to the bit-string genotype, to the network architecture phenotype. Another way to represent the connections is to  use an ordered pair $G = (V,E)$ with vertices $V$ and edge $E$ associated with a direction, to represent a directed acyclic graph. Irwin \\textit{et al.}~ also used this strategy to encode the connections.\nSecondly, the non-linear architecture is a more common case in both the macro architecture and the micro architecture. AmoebaNet-A~, as an example of non-linear macro architecture, stacked these two kinds of cells for several times to build up an architecture. In addition, each cell receives two inputs from the previous two cells separately, which means that the direct connection and the skip-connection are both used in this macro structure. Also in AmoebaNet, each cell is a non-linear architecture inside, which means the micro architecture is also non-linear. The non-linear architecture provides the architecture with more flexibility, which makes it more likely to build up a promising architecture than that of the linear ones.", "cites": [870, 6827], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of encoding strategies for non-linear architectures in EC-based NAS, mentioning a few key approaches and citing relevant papers. While it connects some ideas (e.g., binary strings and adjacency matrices), the synthesis remains limited and does not present a novel framework. The analysis lacks depth, as it does not critically evaluate the strengths or weaknesses of the cited methods. Some abstraction is attempted by noting the flexibility of non-linear architectures, but broader patterns or principles are not clearly articulated."}}
{"id": "d3650a8a-21c2-4aa1-9b61-b69c455614a3", "title": "Population Updating", "level": "section", "subsections": ["a152dfdd-9b9e-46a8-8504-9097a22a6736", "f362ecc4-6be9-45c6-b9e4-700fb0393594", "670f0571-da2a-4b7f-ab3e-86d8a80bfc64"], "parent_id": "40bf4670-36c8-4014-b074-8f86f999eff7", "prefix_titles": [["title", "A Survey on Evolutionary Neural Architecture Search"], ["section", "Population Updating"]], "content": "\\label{sec_Population_operators}\n\\begin{table*}[ht]\n\t\\renewcommand\\arraystretch{0.8}\n\t\\caption{Categorization of EC and Different Types of Neural Network} \n\t\\label{table_EC}\n\t\\vspace{-0.3cm}\n\t\\begin{center}\n\t\t\\begin{tabular}{|p{1cm}|p{0.5cm}|p{1.8cm}|p{4.5cm}|p{1.4cm}|p{2cm}|p{1.3cm}|p{2cm}|}\n\t\t\t\\hline\n\t\t\t\\multicolumn{3}{|l|}{}                                                                         & CNN & DBN & RNN & AE & Others  \\\\ \\hline\n\t\t\t\\multirow{12}{*}{\\shortstack[l]{Single\\\\objective}}  & \\multirow{3}{*}{EA}    & GAs                             &~  & ~    &   ~        & ~ & ~  \\\\ \\cline{3-8} \n\t\t\t&                        & GP                                    & ~    &     &    ~       & ~  &  ~    \\\\ \\cline{3-8} \n\t\t\t&                        & ES                                  &  ~   &     &  ~         &  ~ &   ~     \\\\ \\cline{2-8} \n\t\t\t& \\multirow{3}{*}{SI}    & ACO                                &  ~   &     &     ~      &       &     \\\\ \\cline{3-8} \n\t\t\t&                        & PSO                 &  ~   & ~    &           & ~   &    ~     \\\\ \\cline{2-8} \n\t\t\t& \\multirow{6}{*}{Other}   & Memetic                           & ~    &     &           &       &      \\\\ \\cline{3-8} \n\t\t\t&                        & DE                                &  ~   &     &     ~      &  ~     &    ~   \\\\ \\cline{3-8} \n\t\t\t&     & HCA              & ~    &     &           &     &       \\\\ \\cline{3-8} \n\t\t\t&                        & CVOA                               &     &     &     ~      &       &     \\\\ \\cline{3-8} \n\t\t\t&                        & Hyper-heuristic                   &     &  ~   &           &          &   \\\\ \\cline{3-8} \n\t\t\t&                        & FA                       &  ~   &     &           &        &   \\\\ \\cline{3-8} \n\t\t\t&                        & AIS        &  ~   &     &           &   &     \\\\ \\hline\n\t\t\t\\multirow{2}{*}{\\shortstack[l]{Multi-\\\\objective}} & \\multicolumn{2}{l|}{EA}                  &  ~   &     ~    & ~  &  &     ~      \\\\ \\cline{2-8} \n\t\t\t& \\multicolumn{2}{l|}{SI}                                &  ~   &  ~   &           &       &     \\\\ \\hline\n\t\t\\end{tabular}\n\t\\end{center}\n\\vspace{-0.5cm}\n\\end{table*}\nThis section discusses the population updating process as shown in Fig.~\\ref{fig_flowchart}. Generally, the population updating varies greatly among existing ENAS algorithms because they may employ different EC methods which are with different updating mechanisms. Table~\\ref{table_EC} shows the ENAS algorithms which are classified according to the EC methods that they employ and different types of DNNs that they target at. Obviously, the EA-based ENAS algorithms dominate the ENAS. To be more specific, the GA-based ENAS is the most popular approach which largely owes to the convenience of architecture representation in GA. As a result, we give a detailed introduction to the EA-based ENAS including the selection strategy at first. Immediately after, we present a summary of the SI-based ENAS methods and others separately. The corresponding multi-objective ENAS algorithms are also introduced at the end of each subsection.", "cites": [6861, 6851, 6838, 6837, 6863, 6844, 6841, 8955, 6829, 6832, 6856, 6857, 9079, 6858, 6836, 6835, 6843, 6860, 6821, 6846, 6855, 8247, 6847, 6864, 6865, 9080, 6845, 6842, 6859, 6826, 6822, 6830, 6848, 870, 6853, 6840, 6833, 6862, 6831, 6849, 6824, 6827, 6839, 6828], "cite_extract_rate": 0.3188405797101449, "origin_cites_number": 138, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the population updating process in EC-based NAS and classifies cited papers by the type of EC method and targeted neural network. It offers minimal synthesis or integration of ideas across papers and lacks critical evaluation of the methods or limitations. While it identifies EA-based approaches as dominant, it does not abstract broader principles or develop an analytical framework."}}
{"id": "a152dfdd-9b9e-46a8-8504-9097a22a6736", "title": "EAs for ENAS", "level": "subsection", "subsections": [], "parent_id": "d3650a8a-21c2-4aa1-9b61-b69c455614a3", "prefix_titles": [["title", "A Survey on Evolutionary Neural Architecture Search"], ["section", "Population Updating"], ["subsection", "EAs for ENAS"]], "content": "\\begin{table}[]\n\t\\renewcommand\\arraystretch{0.8}\n\t\\caption{Selection Strategy} \n\t\\label{table_selection_strategy}\n\t\\vspace{-0.3cm}\n\t\\begin{center}\n\t\t\\begin{tabular}{p{2cm}|p{5cm}}\n\t\t\t\\hline\n\t\t\tElitism & ~ \\\\\n\t\t\t\\hline\n\t\t\tDiscard the worst or the oldest & ~\\\\\n\t\t\t\\hline\n\t\t\tRoulette & ~ \\\\\n\t\t\t\\hline\n\t\t\tTournament selection & ~\\\\\n\t\t\t\\hline\n\t\t\tOthers & ~ \\\\\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t\\end{center}\n\\vspace{-0.5cm}\t\n\\end{table}\nThe dash box in Fig.~\\ref{fig_flowchart} shows the general flow of population updating in EA-based ENAS. In this section, we will introduce the selection strategy and the evolutionary operations which collectively update the population. Specifically, the first stage of population updating is selection. The selection strategies can be classified into several types, where Table~\\ref{table_selection_strategy} shows the main kinds of selection strategies. Note that the selection strategy can be not only used in choosing individuals as parents to generate offspring with the evolutionary operators, but also used in the environmental selection stage which chooses individuals to make up the next population. Zhang \\textit{et al.}~ termed these two selections as mate selection and environmental selection separately.\nExisting selection strategies can be divided into five categories: elitism, discarding the worst, roulette wheel selection, tournament selection and others. The simplest strategy is elitism which retains the individuals with higher fitness. However, this can cause a loss of diversity in the population, which may lead the population falling into local optima. Discarding the worst is similar to elitism, which removes the individuals with poor fitness values from the population. Real \\textit{et al.}~ used the aging evolution which discards the oldest individual in the population. Aging evolution can explore the search space more, instead of zooming in on good models too early, as non-aging evolution would. The same selection strategy was also used in~. Zhu \\textit{et al.}~ combined these two approaches to discarding the worst individual and the oldest individual at the same time. Roulette wheel selection gives every individual a probability according to its fitness among the population to survive (or be discarded), regardless it is the best or not. Tournament selection selects the best one from an equally likely sampling of individuals. Furthermore, Johner \\textit{et al.}~ used a ranking function to choose individuals by rank.  A selection trick termed as niching was used in~ to avoid stacking into local optima. This trick allows offspring worse than parent to survive for several generations until evolving to a better one.\nMost of the methods focus on preserving the well-performed individuals, however, Liu \\textit{et al.}~ emphasized the genes more than the survived individuals, where a gene can represent any components in the architecture. They believed the individuals which consist of the fine-gene set are more likely to have promising performance.\nSome selection methods aim at preserving the diversity of the population. Elsken \\textit{et al.}~ selected individuals in inverse proportion to their density. Javaheripi \\textit{et al.}~ chose the parents based on the distance (difference) during the mate selection. They chose two individuals with the highest distance to promote the exploration search.\nIn terms of evolutionary operations, mutation and crossover are two of the most commonly used operations in EA-based ENAS algorithms. Particularly, the mutation is only performed on a single individual, while the crossover takes two individuals to generate offspring.\nThe mutation operator aims to search the global optimum around the individual. A simple idea is to allow the encoded information to vary from a given range. Sun \\textit{et al.}~ used the polynomial mutation~ on the parameters of layers which are expressed by real numbers.\nTo make mutation not random, Lorenzo \\textit{et al.}~ proposed a novel Gaussian mutation based on a Gaussian regression to guide the mutation, i.e., the Gaussian regression can predict which architecture may be good, and the newly generated individuals are sampled in the regions of the search space, where the fitness values are likely to be high. This makes mutation to have a ``direction\".\nMoreover, Maziarz \\textit{et al.}~ used an RNN to guide the mutation operation. In this work, the mutation operations were not sampled at random among the possible architectural choices, but were sampled from distributions inferred by an RNN. Using an RNN to control the mutation operation can also be seen in other methods such as~.\nSome researches investigated the diversity of the population after mutation. Qiang \\textit{et al.}~ used a variable mutation probability. They used a higher probability in the early stage for better exploration and a lower probability in the later stage for better exploitation. It has been effectively applied to many other methods~. To maintain the diversity of the population after the mutation operation, Tian \\textit{et al.}~ used \\textit{force mutation} and distance calculation, which ensures the individual in the population is not particularly similar to other individuals (especially the best one). Kramer \\textit{et al.}~ used the (1+1)-evolutionary strategy that generates an offspring based on a single parent with bit-flip mutation, and used a mutation rate to control and niching to overcome local optima.\nThe intensive computational cost of ENAS presents a bottleneck which will be discussed in later sections. To reduce the unaffordable computational cost and time, some kinds of the mutation have also been designed. Zhang \\textit{et al.}~ proposed an exchange mutation which exchanges the position of two genes of the individual, i.e., exchanging the order of layers. This will not bring new layers and the weights in neural networks can be completely preserved, which means that the offspring do not have to be trained from scratch. \nChen \\textit{et al.}~ introduced two function-preserving operators for DNNs, and these operators are termed as network morphisms~. The network morphisms aim to change the DNN architecture without the loss of the acquired experience. The network morphisms change the architecture from $F(\\cdot)$ to $G(\\cdot)$, which satisfies the condition formulated by Equation~(\\ref{equ_function-preserving}):\n\\begin{equation}\n\\label{equ_function-preserving}\n\\forall x,\\quad F(x)=G(x)\n\\end{equation}\nwhere $x$ denotes the input of the DNN. The network morphisms can be regarded as a function-preserving mutation operation. With this operation, the mutated individuals will not have worse performance than their parents. To be more specific, Chen \\textit{et al.}~ proposed \\textit{net2widernet} to obtain a wider net and \\textit{net2deepernet} to obtain a deeper net. Elsken \\textit{et al.}~ extended the network morphisms with two popular network operations: skip connections and batch normalization. Zhu \\textit{et al.}~ proposed five well-designed function-preserving mutation operations to guide the evolutionary process by the information which have already learned. To avoid local optimal, Chen \\textit{et al.}~ added noises in some function-preserving mutation, and in the experiment, they found that by adding noises to pure network morphism, instead of compromising the efficiency, it by contrast, improved the final classification accuracy.\nPlease note that all the network morphisms can only increase the capacity of a network because if one would decrease the networks capacity, the function-preserving property could not be guaranteed~. As a result, the architecture generated by network morphisms is only going to get larger and deeper, which is not suitable for a device with limited computing resources, such as a mobile phone. In order for the network architecture to be reduced, Elsken \\textit{et al.}~ proposed the approximate network morphism, which satisfies Equation~(\\ref{equ_approximate_network_morphism}):\n\\begin{equation}\n\\label{equ_approximate_network_morphism}\n\\forall x,\\quad F(x)\\approx G(x)\n\\end{equation}\nFor the crossover operator, the single-point crossover~ is the most popular method in EA-based ENAS~ because of its implementation simplicity. However, single-point crossover can typically apply to two individuals with equal lengths only. Therefore, this cannot be applied to variable-length individuals. To this end, Sun \\textit{et al.}~ proposed an efficient crossover operator for individuals with variable lengths. Sapra \\textit{et al.}~ proposed a disruptive crossover swapping the whole cluster (a sequence of layers) between both individuals at the corresponding positions rather than only focusing on the parameters of layers. Sun \\textit{et al.}~ used the Simulated Binary Crossover (SBX)~ to do a combination of the encoded parameters from two matched layers. Please note that the encoded parameters after SBX are not the same as that of both parents, which are quite different from other crossover operators.\nEAs for multi-objective ENAS is gaining more and more attention from researchers. The single objective ENAS algorithms are always concerned about only one objective, e.g., the classification accuracy, and these algorithms have only one goal: searching for the architecture with the highest accuracy. In general, most of the multi-objective ENAS algorithms aim at dealing with both the performance of the neural network and the number of parameters simultaneously~.\nHowever, these objective functions are often in conflict with each other. For example, getting a higher accuracy often requires a more complicated architecture with the need of more computational resources. On the contrary, a device with limited computational resource, e.g., a mobile phone, cannot afford such sophisticated architectures. \nThe simplest way to tackle the multi-objective optimization problem is by converting it into a single objective optimization problem with weighting factors, i.e., the weighted summation method. The Equation~(\\ref{equ_multi-fitness})\n\\begin{equation}\n\\label{equ_multi-fitness}\nF = \\lambda f_{1} + (1-\\lambda) f_{2}\n\\end{equation}\nis the classical linear form to weight two objective functions $f_{1},f_{2}$ into a single objective function, where the $\\lambda \\in (0, 1)$ denotes the weighting factor. In~, the multi-objective optimization problem was solved by using the available single objective optimization methods by Equation~(\\ref{equ_multi-fitness}) of the weighted summation. Chen \\textit{et al.}~ did not adopt the linear addition as the objective function, whereas using a nonlinear penalty term. However, the weights manually defined may incur bias~.\nSome algorithms have been designed and widely used in multi-objective optimization, such as NSGA-II~, and MOEA/D~, which have been also used in ENAS methods such as~. These methods aim to find a Pareto-font set (or non-dominant set). Only these methods are in the multi-objective category of Table~\\ref{table_EC}. Some researches have made improvements on these multi-objective optimization methods for better use in ENAS.\nBaldeon \\textit{et al.}~ chose the penalty based boundary intersection approach in MOEA/D because training a neural network involves nonconvex optimization and the form of Pareto Font is unknown.\nLEMONADE~ divided the objective function into two categories: $f_{exp}$ and $f_{cheap}$. $f_{exp}$ denotes the expensive-to-evaluate objectives (e.g., the accuracy), while $f_{cheap}$ denotes the cheap-to-evaluate objectives (e.g., the model size). In every iteration, they sampled parent networks with respect to sparsely distribution based on the cheap objectives $f_{cheap}$ to generate offspring. Therefore, they evaluated $f_{cheap}$ more times than $f_{exp}$ to save time. Schoron \\textit{et al.}~ also took the use of the LEMONADE proposed by Elsken \\textit{et al.}~.\nDue to that NSGA-III~ may fall into the small model trap (this algorithm prefers small models), Yang \\textit{et al.}~ have made some improvements to the conventional NSGA-III for favoring larger models.", "cites": [6851, 6843, 6869, 6821, 870, 9079, 6868, 6867, 6837, 8247, 6847, 6863, 6853, 9080, 6844, 6841, 6834, 6845, 6829, 6831, 6849, 6824, 6866, 6842, 6827, 6839, 6832, 8302, 6858, 6826], "cite_extract_rate": 0.4, "origin_cites_number": 75, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple EA-based ENAS approaches by categorizing and analyzing selection strategies and mutation operations, connecting different papers under broader themes. It provides critical insights, such as the limitations of elitism and the trade-offs between exploration and exploitation. The discussion abstracts beyond individual papers to highlight general principles like function preservation in network morphisms and the balance between model complexity and computational efficiency."}}
{"id": "f362ecc4-6be9-45c6-b9e4-700fb0393594", "title": "SI for ENAS", "level": "subsection", "subsections": [], "parent_id": "d3650a8a-21c2-4aa1-9b61-b69c455614a3", "prefix_titles": [["title", "A Survey on Evolutionary Neural Architecture Search"], ["section", "Population Updating"], ["subsection", "SI for ENAS"]], "content": "PSO is inspired by the bird flocking or fish schooling~, and is easy to implement compared with other SI algorithms.\nJunior \\textit{et al.}~ used their implementation of PSO to update the particles based on the layer instead of the parameters of the layer. Gao \\textit{et al.}~ developed a gradient-priority particle swarm optimization algorithm to handle issues including the low convergence efficiency of PSO when there are a large number of hyper-parameters to be optimized. They expected the particle to find the locally optimal solution at first, and then move to the global optimal solution.\nFor ACO, the individuals are generated in a quite different way. Several ants are in an ant colony\\footnote{The population in ACO also termed as colony.}. Each ant moves from node to node following the pheromone instructions to build an architecture. The pheromone is updated every generation. The paths of well-performed architecture will maintain more pheromone to attract the next ant for exploitation and at the same time, the pheromone is also decaying (i.e., pheromone evaporation), which encourages other ants to explore other areas. Byla \\textit{et al.}~ let the ants choose the path from the node to node in a graph whose depth increases gradually. Elsaid \\textit{et al.}~ introduced different ant agent types to act according to specific roles to serve the needs of the colony, which is inspired by the real ants species.\nSI for multi-objective ENAS started only in the last two years and the research of this field is scarce which can be seen from Table~\\ref{table_EC}.\nLi \\textit{et al.}~ used the bias-variance framework on their proposed multi-objective PSO to get a more accurate and stable architecture.\nWu \\textit{et al.}~ used the MOPSO~ for neural networks pruning. The $G_{best}$ is selected according to the crowding distance in the non-dominant solutions set. Wang \\textit{et al.}~ used the OMOPSO~, which selects the leaders using a crowding factor and the $G_{best}$ is selected from the leaders. To better control the balance between convergence and diversity, Jiang \\textit{et. al.}~ proposed a MOPSO/D algorithm based on an adaptive penalty-based boundary intersection.", "cites": [6859, 6835], "cite_extract_rate": 0.18181818181818182, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic overview of how swarm intelligence (SI) is applied in ENAS, integrating some key ideas from the cited papers. It connects PSO and ACO implementations but does so in a largely descriptive manner without deeper analysis or a unifying framework. While it mentions some methodological differences (e.g., pheromone update, multi-objective approaches), it lacks critical evaluation or abstraction of broader trends or principles."}}
{"id": "670f0571-da2a-4b7f-ab3e-86d8a80bfc64", "title": "Other EC Techniques for ENAS", "level": "subsection", "subsections": [], "parent_id": "d3650a8a-21c2-4aa1-9b61-b69c455614a3", "prefix_titles": [["title", "A Survey on Evolutionary Neural Architecture Search"], ["section", "Population Updating"], ["subsection", "Other EC Techniques for ENAS"]], "content": "Different from GAs, the mutation of DE exploits the information from three individuals. \nSome ENAS methods like~ chose DE to guide the offspring generation. However, there is little difference between different DE-based ENAS algorithms.\nWang \\textit{et al.}~ proposed a hybrid PSO-GA method. They used PSO to guide the evolution of the parameters in each block encoded in decimal notation. Meanwhile, using GA to guide the evolution of the shortcut connections is encoded in binary notation. Because PSO performs well on continuous optimization and GA is suitable for optimization with binary values, this hybrid method can search architectures effectively.\nHCA can be interpreted as a very simple evolutionary algorithm. For example, in~ the evolutionary operators only contain mutation and no crossover, and the selection strategy is relatively simple. The memetic algorithm is the hybrids of EAs and local search. Evans \\textit{et al.}~ integrated the local search (as gradient descent) into GP as a fine-tuning operation. The CVOA~ was inspired by the new respiratory virus, COVID-19. The architecture was found by simulating the virus spreads and infecting healthy individuals. Hyper-heuristic contains two levels: high-level strategy and low-level heuristics, and a domain barrier is between these two levels. Hence the high-level strategy is still useful when the application domain is changed. AIS was inspired by theories related to the mammal immune system and do not require the crossover operator compared to the GA~.", "cites": [6831, 6865, 6826], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section analytically discusses different EC techniques used in ENAS, such as DE, PSO-GA hybrid, HCA, memetic algorithms, CVOA, hyper-heuristics, and AIS, by highlighting their unique features and motivations. While it integrates ideas across multiple papers, the synthesis is moderate and not deeply interconnected. The critical analysis is present but limited to stating that DE-based ENAS algorithms show little difference and briefly mentioning strengths of hybrid approaches. The section identifies patterns (e.g., hybridization of EC methods) but does not offer a meta-level abstraction of principles."}}
{"id": "fa11702e-7516-4849-b9c7-1422e93b4202", "title": "Efficient Evaluation", "level": "section", "subsections": [], "parent_id": "40bf4670-36c8-4014-b074-8f86f999eff7", "prefix_titles": [["title", "A Survey on Evolutionary Neural Architecture Search"], ["section", "Efficient Evaluation"]], "content": "\\label{sec_evaluation}\nIn this section, we will discuss the strategies to improve the efficiency of evaluations, with the consideration that the evaluation is often the most time-consuming stage of ENAS algorithms~.\nReal \\textit{et al.}~ used 250 computers to finish the LargeEvo algorithm over 11 days. Such computational resources are not available for everyone interested in NAS. Almost all of the methods evaluate individuals by training them first and evaluating them on the validation/test dataset. Since the architecture is becoming more and more complex, it will take a lot of time for training each architecture to convergence. So it is needed to investigate new methods to shorten the evaluation time and reduce the dependency on large amounts of computational resources.\nTable~\\ref{table_shorten_time} lists five of the most common methods to reduce the time: weight inheritance, early stopping policy, reduced training set, reduced population, and population memory. We would like to introduce the five kinds of methods first, then other kinds of promising methods next, and finally the surrogate-assisted methods at the end of this section.\nBecause the evolutionary operators do not completely disrupt the architecture of an individual, some parts of the newly generated individuals are the same as their parents. The weights of the same parts can be easily inherited. With the weight inheritance, the neural networks no longer need to be trained completely from scratch. This method has been used in~ 20 years ago.\nMoreover, as mentioned in Section~\\ref{sec_Population_operators}, the network morphisms change the network architecture without loss of the acquired experience. This could be regarded as the ultimate weight inheritance because it solved the weight inheritance problem in the changed architecture part. The ultimate weight inheritance lets the new individual completely inherit the knowledge from their parents, which will save a lot of time.\nThe early stopping policy is another method which has been used widely in NAS. The simplest way is to set a fixed and relatively small number of training epochs. This method is used in~, where training the individuals after a small number of epochs is sufficient. Similarly, Assunccao \\textit{et al.}~ let the individuals undergo the training for the same and short time each epoch (although this time is not fixed and will increase with the epoch), to allow the promising architectures have more training time to get a more precise evaluation, So \\textit{et al.}~ set hurdles after a fixed number of epochs. The weak individuals stop training early to save time.\nHowever, the early stopping policy can lead to inaccurate estimation about individuals' performance (especially the large and complicated architecture), which can be seen in Fig.~\\ref{fig_learning_curve}. In Fig.~\\ref{fig_learning_curve}, \\emph{individual2} performs better than \\emph{individual1} before epoch \\emph{t1}, whereas \\emph{individual1} performs better in the end. Yang \\textit{et al.}~ also discussed this phenomenon. So, it is crucial to determine at which point to stop.\nNote that neural networks can converge or hardly improve its performance after several epochs, as seen in the \\emph{t1} for \\emph{individual2} and the \\emph{t2} for \\emph{individual1} in Fig.~\\ref{fig_learning_curve}. Using the performance estimated at this point can evaluate an individual's relatively accurately with less training time. Therefore, some methods such as~ stopped training when observing there is no significant performance improvement.\nSuganuma \\textit{et al.}~ used the early stopping policy based on a reference curve. If the accuracy curve of an individual is under the reference curve for successive epochs, then the training will be terminated and this individual is regarded as a poor one. After every epoch, the reference curve is updated by the accuracy curve of the best offspring. \nThe reduced training set, i.e., using a subset of data that assuming similar properties to a large dataset, can also shorten the time effectively. Liu \\textit{et al.}~ explored promising architectures by training on a subset and used transfer learning on the large original dataset. Because there are so many benchmark datasets in the image classification field, the architecture can be evaluated on a smaller dataset (e.g., CIFAR-10) first and then applied on a large dataset (such as CIFAR-100 and ImageNet~). The smaller dataset can be regarded as the proxy for the large one.\nThe reduced population is a unique method of ENAS since other NAS approaches do not have a population. Assunccao \\textit{et al.}~ reduced the population based on their previous algorithm~ to speed up the evolution. However, simply reducing the population may not explore the search space well in each epoch and may lose the global search ability. Another way is reducing the population dynamically. For instance, Fan \\textit{et al.}~ used the ($\\mu+\\lambda$) evolution strategy and divided the evolution into three stages with the population reduction, which aims to find the balance between the limited computing resources and the efficiency of the evolution. The large population in the first stage is to ensure the global search ability, while the small population in the last stage is to shorten the evolution time. Instead of reducing the population, Liu \\textit{et al.}~ evaluated the downsizing architecture with smaller size at an early stage of evolution. Similarly, Wang \\textit{et al.}~ did not evaluate the whole architecture but starting with a single block, and then the blocks were stacked to build architecture as evolution proceeds.\n\\begin{table}[]\n\t\\renewcommand\\arraystretch{0.8}\n\t\\caption{Different Methods to Shorten the Evaluation Time} \n\t\\label{table_shorten_time}\n\t\\vspace{-0.2cm}\n\t\\begin{center}\n\t\t\\begin{tabular}{p{2.5cm}|p{5cm}}\n\t\t\t\\hline\n\t\t\tWeight inheritance & ~ \\\\\n\t\t\t\\hline\n\t\t\tEarly stopping policy & ~\\\\\n\t\t\t\\hline\n\t\t\tReduced training set & ~ \\\\\n\t\t\t\\hline\n\t\t\tReduced population & ~\\\\\n\t\t\t\\hline\n\t\t\tPopulation memory & ~\\\\\n\t\t\t\\hline\n\t\t\\end{tabular}\n\t\\end{center}\n\\vspace{-0.3cm}\n\\end{table}\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=0.8\\linewidth]{learning_curve}\n\t\\caption{Two learning curve of two different individuals.}\n\t\\label{fig_learning_curve}\n\t\\vspace{-0.2cm}\n\\end{figure}\nThe population memory is another category of the unique methods of ENAS. It works by reusing the corresponding architectural information that has previously appeared in the population. In the population-based methods, especially the GA based methods (e.g., in~), it is natural to maintain well-performed individuals in the population in successive generations. Sometimes, the individuals in the next population directly inherit all the architecture information of their parents without any modification, so it is not necessary to evaluate the individuals again. Fujino \\textit{et al.}~ used memory to record the fitness of individuals, and if the same architecture encoded in an individual appears, the fitness value is retrieved from memory instead of being reevaluated. Similarly, Miahi \\textit{et al.}~ and Sun \\textit{et al.}~ employed a hashing method for saving pairs of architecture and fitness of each individual and reusing them when the same architecture appears again. Johner \\textit{et al.}~ prohibited the appearance of architectures that have appeared before offspring generation. This does reduce the time, however, the best individuals are forbidden to remain in the population which may lead the population evolve towards a bad direction.\nThere are many other well-performed methods to reduce the time in ENAS. Rather than training thousands of different architectures, one-shot model~ trained only one SuperNet to save time. Different architectures, i.e., the SubNets, are sampled from the SuperNet with the shared parameters. Yang \\textit{et al.}~ believed the traditional ENAS methods without using SuperNet which were less efficient for models to be optimized separately. In contrast, the one-shot model optimizes the architecture and the weights alternatively. But the weight sharing mechanism brings difficulty in accurately evaluating the architecture. Chu \\textit{et al.}~ scrutinized the weight-sharing NAS with a fairness perspective and demonstrated the effectiveness. However, there remain some doubts that cannot explain clearly in the one-shot model. The weights in the SuperNet are coupled. It is unclear why inherited weights for a specific architecture are still effective~.\nMaking the use of hardware can reduce the time, too. Jiang \\textit{et al.}~ used a distributed asynchronous system which contained a major computing node with 20 individual workers. Each worker is responsible for training a single block and uploading its result to the major node in every generation. Wang \\textit{et al.}~ designed an infrastructure which has the ability to leverage all of the available GPU cards across multiple machines to concurrently perform the objective evaluations for a batch of individuals.\nColangelo \\textit{et al.}~ designed a reconfigurable hardware framework that fits the ENAS. As they claimed, this was the first work of conducting NAS and hardware co-optimization.\nFurthermore, Lu \\textit{et al.}~ adopted the concept of proxy models, which are small-scale versions of the intended architectures. For example, in a CNN architecture, the number of layers and the number of channels in each layer are reduced. However, the drawback of this method is obvious: the loss of prediction accuracy. Therefore, they performed an experiment to determine the smallest proxy model that can provide a reliable estimate of performance at a larger scale.\nAll the above methods obtain the fitness of individuals by directly evaluating the performance on the validation dataset. An alternative way is to use indirect methods, namely the performance predictors. As summarized in~, the performance predictors can be divided into two categories: performance predictors based on the learning curve and end-to-end performance predictors, both of which are based on the training-predicting learning paradigm. This does not mean the performance predictor does not undergo the training phase at all, while it means learning from the information obtained in the training phase, and uses the knowledge learned to make a reasonable prediction for other architectures.\nRalwal \\textit{et al.}~ took the use of the learning curve based predictor, where the fitness is not calculated in the last epoch but is predicted by the sequence of fitness from first epochs. Specifically, they used a Long Short Term Memory (LSTM)~ as a sequence to sequence model, predicting the final performance by using the learning results of the first several epochs on the validation dataset.\nSun \\textit{et al.}~ adopted a surrogate-assisted method which is called end-to-end performance predictor. The predictor does not need any extra information about the performance of individuals to be evaluated. The performance predictor is essentially a regression model mapping from the architecture to its performance. The regression model needs to be trained with sufficient training data pairs first, and each pair consists of architecture and its corresponding performance. Specifically, they chose random forest~ as the regression model to accelerate the fitness evaluations in ENAS. When the random forest receives a newly generated architecture as input, the adaptive combination of a huge number of regression trees which have been trained in advance in the forest gives the prediction.", "cites": [6822, 6851, 6843, 6821, 6848, 9079, 6846, 8247, 6871, 6841, 6833, 6845, 8955, 6862, 6831, 6842, 6872, 6870, 1461, 6826, 6835], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 63, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple methods to improve evaluation efficiency in ENAS, such as weight inheritance, early stopping, and population memory, while showing some critical analysis by highlighting limitations (e.g., early stopping leading to inaccurate estimates). However, the integration is not fully novel and the critique remains somewhat surface-level, with general patterns identified but no deep meta-level insights."}}
{"id": "06bc3345-4a23-45a7-a0e1-7338ed3b2e6b", "title": "Overview", "level": "subsection", "subsections": [], "parent_id": "390da3df-188a-48a9-991a-966d654bdd4d", "prefix_titles": [["title", "A Survey on Evolutionary Neural Architecture Search"], ["section", "Applications"], ["subsection", "Overview"]], "content": "Table~\\ref{table_application} shows the applications of existing ENAS algorithms, which contains a wide range of real-world applications. \n\\begin{table}[]\n\t\\renewcommand\\arraystretch{0.8}\n\t\\caption{Applications of Existing ENAS algorithms.} \n\t\\label{table_application}\n\t\\begin{tabular}{|p{1cm}|p{3.5cm}|p{3cm}|}\n\t\t\\hline\n\t\tCategory            & Applications                               & References               \\\\ \\hline\n\t\t(1)                   & Image classification                       & ~                         \\\\ \\hline\n\t\t(1)                   & Image to image                             & ~                         \\\\ \\hline\n\t\t(1)                   & Emotion recognition                        & ~                         \\\\ \\hline\n\t\t(1)                   & Speech recognition                         & ~                         \\\\ \\hline\n\t\t(1)                   & Language modeling                          & ~                         \\\\ \\hline\n\t\t(1)                   & Face De-identification                     & ~         \\\\ \\hline\n\t\t(2)                   & Medical image segmentation                 & ~                         \\\\ \\hline\n\t\t(2)                   & Malignant melanoma detection               & ~         \\\\ \\hline\n\t\t(2)                   & Sleep heart study                          & ~         \\\\ \\hline\n\t\t(2)                   & Assessment of human sperm                  & ~         \\\\ \\hline\n\t\t(3)                   & Wind speed prediction                      & ~   \\\\ \\hline\n\t\t(3)                   & Electricity demand time series forecasting & ~  \\\\ \\hline\n\t\t(3)                   & Traffic flow forecasting                   & ~                \\\\ \\hline\n\t\t(3)                   & Electricity price forecasting              & ~        \\\\ \\hline\n\t\t(3)                   & Car park occupancy prediction              & ~   \\\\ \\hline\n\t\t(3)                   & Energy consumption prediction              & ~  \\\\ \\hline\n\t\t(3)                   & Time series data prediction                & ~       \\\\ \\hline\n\t\t(3)                   & Financial prediction                       & ~         \\\\ \\hline\n\t\t(3)                   & Usable life prediction                     & ~  \\\\ \\hline\n\t\t(3)                   & Municipal waste forecasting                & ~    \\\\ \\hline\n\t\t(4)                   & Engine vibration prediction                & ~     \\\\ \\hline\n\t\t(4)                   & UAV                                        & ~       \\\\ \\hline\n\t\t(4)                   & Bearing fault diagnosis                    & ~    \\\\ \\hline\n\t\t(4)                   & Predicting general aviation flight data    & ~       \\\\ \\hline\n\t\t(5)                   & Crack detection of concrete                & ~          \\\\ \\hline\n\t\t(5)                   & Gamma-ray detection                        & ~   \\\\ \\hline\n\t\t(5)                   & Multitask learning                         & ~    \\\\ \\hline\n\t\t(5)                   & Identify Galaxies                          & ~    \\\\ \\hline\n\t\t(5)                   & Video understanding                        & ~ \\\\ \\hline\n\t\t(5)                   & Comics understanding                       & ~ \\\\ \\hline\n\t\\end{tabular}\n\\vspace{-0.5cm}\n\\end{table}\nGenerally, these applications can be grouped into the following five different categories:\n(1) Image and signal processing, including image classification which is the most popular and competitive field, image to image processing (including image restoration, image denoising, super-resolution and image inpainting), emotion recognition, speech recognition, language modelling, and face de-identification.\n(2) Biological and biomedical tasks, including medical image segmentation, malignant melanoma detection, sleep heart study, and assessment of human sperm.\n(3) Predictions and forecasting about all sorts of data, including the prediction of wind speed, car park occupancy, time-series data, financial and usable life, the forecasting of electricity demand time series, traffic flow, electricity price, and municipal waste.\n(4) Engineering, including engine vibration prediction, Unmanned Aerial Vehicle (UAV), bearing fault diagnosis and predicting general aviation flight data.\n(5) Others, including crack detection of concrete, gamma-ray detection, multitask learning, identify galaxies, video understanding, and comics understanding.", "cites": [6851, 6838, 6837, 6863, 6844, 6841, 8955, 6829, 6876, 6832, 6857, 9079, 6875, 6874, 6850, 6870, 6836, 6835, 6869, 6843, 6860, 6821, 6846, 8247, 6847, 6864, 6873, 6865, 9080, 6834, 6845, 6842, 6866, 6830, 6822, 6848, 870, 6853, 6840, 6852, 6833, 6862, 6831, 6849, 6827, 6839, 6828], "cite_extract_rate": 0.3671875, "origin_cites_number": 128, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of the application areas of ENAS by listing tasks and associating them with cited papers, but it lacks integration of insights across these works. There is minimal critical evaluation or comparison of methods; most references are used to support a broad categorization rather than to draw analytical or evaluative conclusions. The abstraction is limited to grouping applications into categories without deeper generalization or theoretical synthesis."}}
{"id": "ce4d03d7-51ce-4271-828b-0f60a203e534", "title": "Comparisons on CIFAR-10, CIFAR-100 and ImageNet", "level": "subsection", "subsections": [], "parent_id": "390da3df-188a-48a9-991a-966d654bdd4d", "prefix_titles": [["title", "A Survey on Evolutionary Neural Architecture Search"], ["section", "Applications"], ["subsection", "Comparisons on CIFAR-10, CIFAR-100 and ImageNet"]], "content": "\\label{sec_CIFAR}\n\\begin{table*}[!htbp]\n\t\\renewcommand\\arraystretch{0.8}\n\t\\caption{The Comparison of the Classification Error Rate on CIFAR-10, CIFAR-100 and ImageNet}\n\t\\label{table_CIFAR}\n\t\\vspace{-0.2cm}\n\t\\begin{tabular}{|p{3.7cm}|p{1.3cm}|p{1.8cm}|p{2.4cm}|p{2.4cm}|p{3cm}|p{0.5cm}|}\n\t\t\\hline\n\t\t\\textbf{ENAS Methods}                                     & \\textbf{GPU Days}               & \\textbf{Parameters(M)}           & \\textbf{CIFAR-10(\\%)}                 & \\textbf{CIFAR-100(\\%)}           &  {\\textbf{ImageNet(Top1/Top5 \\%)}} & \\textbf{Year}                                                                                                                                                                                                      \\\\ \\hline\n\t\tCGP-DCNN~                               & ---                & 1.1                      & 8.1                                 &   ---                       & --- & 2018                                                                                                                                                     \\\\ \\hline\n\t\tEPT~                                 & 2                      & ---                  & 7.5                                 &    ---                           &   ---  & 2020                                                                                                                                                                                                   \\\\ \\hline\n\t\t\\multirow{2}{*}{GeNet~}                    & \\multirow{3}{*}{17}    & ---                  & 7.1                                 &       ---                    & ---  & \\multirow{3}{*}{2017}                                                                                                          \\\\ \\cline{3-6}\n\t\t&                        & ---                  &      ---                               & 29.03                               &  ---          &                                                                                                                                                                                      \\\\ \\cline{3-6}\n\t\t& & 156 & --- & --- & {27.87 / 9.74} &\n\t\t\\\\ \\hline\n\t\tEANN-Net~                                    & ---                & ---                  & 7.05 $\\pm$ 0.02        &    ---                 &     ---       & 2019           \\\\ \\hline\n\t\t\\multirow{2}{*}{DeepMaker~}            & \\multirow{2}{*}{3.125} & 1                        & 6.9                                 &     ---                  &   ---     &\\multirow{2}{*}{2020}           \\\\ \\cline{3-6}\n\t\t&                        & 1.89                     &    ---                                 & 24.87                               &   ---           &       \\\\ \\hline\n\t\t\\multirow{2}{*}{GeneCai (ResNet-50)~ }               & \\multirow{2}{*}{0.024}                  & ---                  & 6.4                                 &    ---                    &  ---  & \\multirow{2}{*}{2020}                 \\\\ \\cline{3-6}\n\t\t& & --- & --- & --- & {25.7 / 7.9} & \\\\ \\hline\n\t\tCGP-CNN (ConvSet)~                    & ---                & 1.52                     & 6.75                                &   ---          &  --- &2017                   \\\\ \\hline\n\t\tCGP-CNN (ResSet)~                     & ---                & 1.68                     & 5.98                                &  ---           &  ---    &2017                                                                                             \\\\ \\hline\n\t\tMOPSO/D-Net~                          & 0.33                   & 8.1                      & 5.88                                &  ---  &    ---      &2019                                                                                                                                                                                                          \\\\ \\hline\n\t\tReseNet-50 (20\\% pruned)~                     & ---                & 6.44  & 5.85                                &      ---                       &  --- &2018                                                                                                                                                           \\\\ \\hline\n\t\tImmuNeCS~                             & 14                     & ---                  & 5.58                                &    ---  &    --- &2019                                                                                                                                                                                                       \\\\ \\hline\n\t\t\\multirow{2}{*}{EIGEN~}                      & 2                      & 2.6                      & 5.4                                 &            ---                     &  ---  &  \\multirow{2}{*}{2018}                                                                                                                                                                                \\\\ \\cline{2-6}\n\t\t& 5                      & 11.8                     &             ---                       & 21.9                                &     --- &                                                                                                                                                                                            \\\\ \\hline\n\t\t\\multirow{2}{*}{LargeEvo~}     & \\multirow{2}{*}{2750}  & 5.4                      & 5.4                                 &          ---                        & ---  &    \\multirow{2}{*}{2017}                                                                                                                                                                                      \\\\ \\cline{3-6}\n\t\t&                        & 40.4                     &      ---                               & 23                                  &        ---           &        \\\\ \\hline\n\t\t\\multirow{2}{*}{CGP-CNN (ConvSet)~} & 31                     & 1.5                      & 5.92 (6.48 $\\pm$ 0.48) &   ---            &  ---  & \\multirow{4}{*}{2019}                                                     \\\\ \\cline{2-6}\n\t\t& ---                & 2.01                     &        ---                             & 26.7 (28.1 $\\pm$ 0.83)   &  --- &                                                                                                                                                                                                  \\\\ \\cline{1-6}\n\t\t\\multirow{2}{*}{CGP-CNN (ResSet)~}  & 30                     & 2.01                     & 5.01 (6.10 $\\pm$ 0.89) &   ---    &             ---                     &               \\\\ \\cline{2-6}\n\t\t& ---                & 4.6                      &   ---                                  & 25.1 (26.8 $\\pm$ 1.21)   &  --- &                                                                                                                                                                                                  \\\\ \\hline\n\t\tMOCNN~                                   & 24                     & ---                  & 4.49                                &  ---      &   ---  & 2019                                                                                                                                                                                                          \\\\ \\hline\n\t\t\\multirow{2}{*}{NASH~}                   & 4                      & 88                       & 4.4                                 &        ---                        &   ---  &   \\multirow{2}{*}{2017}                                                                                                                                                                                 \\\\ \\cline{2-6}\n\t\t& 5                      & 111.5                    &     ---                               & 19.6                                &  ---   &                                                                                                                                                                                             \\\\ \\hline\n\t\tHGAPSO~                                    & 7+                     & ---                  & 4.37                                &       ---                &    ---   & 2018                                                                                                                                                        \\\\ \\hline\n\t\t\\multirow{3}{*}{DPP-Net~,~}                   & \\multirow{3}{*}{2}                      & 11.39                       & 4.36                                 &     ---                     &   ---    &  \\multirow{3}{*}{2018}         \\\\ \\cline{3-6}\n\t\t&                       & 0.45                    &   5.84                                  &      ---                    &    ---   &                                                                                                                                                                                                 \\\\ \\cline{3-6}\n\t\t& & 4.8 & --- & --- & {26.0 / 8.2} & \\\\ \\hline\n\t\t\\multirow{2}{*}{AE-CNN~}                & 27                     & 2                        & 4.3                                 &        ---                    &    ---     &\\multirow{2}{*}{2019}                                                                                                                                                                                 \\\\ \\cline{2-6}\n\t\t& 36                     & 5.4                      &       ---                              & 20.85                               &   ---   &                                                                                                                                                                                            \\\\ \\hline\n\t\tSI-ENAS~  & 1.8 & --- & 4.07 & 18.64 & --- & 2020   \\\\ \\hline\n\t\tEPSOCNN~  & 4-  & 6.77 & 3.69 & --- & --- & 2019  \\\\ \\hline\n\t\t\\multirow{2}{*}{Hierarchical Evolution~ }              & \\multirow{2}{*}{300}                    & ---                  & 3.63 $\\pm$ 0.10        &  ---  & --- &  \\multirow{2}{*}{2017}                                                                                                                                                                                        \\\\ \\cline{3-6}\n\t\t& & --- & --- & --- & {20.3 / 5.2} & \\\\ \\hline\n\t\t\\multirow{2}{*}{EA-FPNN~}                 & 0.5                    & 5.8                      & 3.57                                &      ---                 &  ---    & \\multirow{2}{*}{2018}                                                                                                                                                  \\\\ \\cline{2-6}\n\t\t& 1                      & 7.2                      &       ---                              & 21.74                               &   ---   &                                                                                                                                                                                            \\\\ \\hline\n\t\t\\multirow{2}{*}{AmoebaNet-A~}  & \\multirow{2}{*}{3150}  & 3.2  & 3.34 $\\pm$ 0.06 & --- & --- & \\multirow{2}{*}{2018}  \\\\ \\cline{3-6}\n\t\t&& 86.7 & --- & --- & {17.2 / 3.9} & \\\\ \\hline\n\t\tFirefly-CNN~                          & ---                & 3.21                     & 3.3                                 & 22.3                 &  ---   & 2019                                                                                                                          \\\\ \\hline\n\t\t\\multirow{3}{*}{CNN-GA~}             & 35                     & 2.9                      & 3.22                                &   ---                 &    ---    &   \\multirow{3}{*}{2018}      \\\\ \\cline{2-6}\n\t\t& 40                     & 4.1                      &      ---                               & 20.53                               &      --- &                                                                                                                                                                                           \\\\ \\cline{2-6}\n\t\t& 35 & --- & --- & --- & {25.2 / 7.7} & \\\\ \\hline\n\t\t\\multirow{3}{*}{JASQNet~}                   & \\multirow{3}{*}{3}                      & 3.3                      & 2.9                                 &     ---                       & ---   & \\multirow{3}{*}{2018}           \\\\ \\cline{3-6}\n\t\t&                       & 1.8                      & 2.97                                &               ---                      &   ---    &                                                                                                                                                                                           \\\\ \\cline{3-6} \n\t\t& & 4.9 & --- & --- & {27.2 / ---} & \\\\ \\hline\n\t\t\\multirow{2}{*}{RENASNet~}                             & \\multirow{2}{*}{6}                      & 3.5                      & 2.88 $\\pm$ 0.02        &    ---   & --- & \\multirow{2}{*}{2018}                                                                                                                  \\\\ \\cline{3-6}\n\t\t& & 5.36 & --- & --- & {24.3 / 7.4} & \\\\ \\hline\n\t\tNSGA-Net~                                      & 4                      & 3.3                      & 2.75                                & ---                     &   ---  & 2018                                                                     \\\\ \\hline\n\t\t\\multirow{4}{*}{CARS~}                       & \\multirow{4}{*}{0.4}   & 2.4                      & 3                                   &       ---                 &  ---   & \\multirow{4}{*}{2019}                                                                                                                              \\\\ \\cline{3-6}\n\t\t&                        & 3.6                      & 2.62                                &              ---                       & ---             &                                                                                                                                                                                    \\\\ \\cline{3-6} \n\t\t& & 3.7 & --- & --- & {27.2 / 9.2} & \\\\ \\cline{3-6}\n\t\t& & 5.1 & --- & --- & {24.8 / 7.5} & \\\\ \\hline\n\t\t\\multirow{3}{*}{LEMONADE~}            & \\multirow{3}{*}{80}    & 13.1                     & 2.58                                &        ---                   &  ---  & \\multirow{3}{*}{2018}                                                                                                                            \\\\ \\cline{3-6}\n\t\t&                        & 0.5                      & 4.57                                &                  ---                   &       ---                    &                                                                                                                                                                       \\\\ \\cline{3-6}\n\t\t& & --- & --- & --- & {28.3 / 9.6} & \\\\ \\hline\n\t\t\\multirow{2}{*}{EENA~}                        & \\multirow{2}{*}{0.65}  & 8.47                     & 2.56                                &            ---            &  ---   & \\multirow{2}{*}{2019}                                                                                                       \\\\ \\cline{3-6}\n\t\t&                        & 8.49                     &           ---                          & 17.71                               &   ---     &                                                                                                                                                                                            \\\\ \\hline\n\t\tEEDNAS-NNMM~                            & 0.5                    & 4.7                      & 2.55                                &           ---          &     ---    & 2019                                                                                                                                                   \\\\ \\hline\n\t\t\\multirow{5}{*}{NSGANet~}                    & \\multirow{5}{*}{27}    & 0.2                      & 4.67                                &     ---                  &    ---    & \\multirow{5}{*}{2019}                                             \\\\ \\cline{3-6}\n\t\t&                        & 4                        & 2.02                                &         ---                            &      ---                 &                                                                                                                                                                           \\\\ \\cline{3-6}\n\t\t&                        & 0.2                      &             ---                        & 25.17                               &    ---                 &                 \\\\ \\cline{3-6}\n\t\t&                        & 4.1                      &       ---                              & 14.38                               &     ---    &                                                                                                                                                                                         \\\\ \\cline{3-6}\n\t\t& & 5.0 & --- & --- & {23.8 / 7.0} & \\\\ \\hline\n\t\\end{tabular}\n\\vspace{-0.3cm}\n\\end{table*}\nIn Table~\\ref{table_application}, it is obvious to see that many ENAS methods are applied to the image classification tasks. The benchmark dataset, CIFAR-10 which contains a total of ten classes, and the CIFAR-100 is the advanced dataset including a hundred classes. These two datasets have been widely used in image classification tasks, and the accuracy on these two challenging datasets can represent the ability of the architecture searched. In addition, ImageNet~ is a more challenging benchmark dataset, which contains 1,000 classes and more than one million images. Because CIFAR-10 and CIFAR-100 are relatively small and easy to be over-fitting nowadays~, the results of the ENAS methods on ImageNet are also included. We have collected the well-performed ENAS methods tested on these three datasets and showed the results in Table~\\ref{table_CIFAR}, where the methods are ranked in an ascending order of their best accuracy on CIFAR-10, i.e., the methods are ranked in a descending order of their error rate based on the conventions. The data shown under column ``CIFAR-10\" and ``CIFAR-100\" denotes the error rate of each method on the corresponding datasets. Especially, as for ImageNet, we report both top1 and top5 error rates. Furthermore, the ``GPU Days\", which was initially proposed in~, denotes the total search time of each method, it can be calculated by Equation~(\\ref{equ_GPUDays})\n\\begin{equation}\n\\label{equ_GPUDays}\nGPU\\ Days = The\\ number\\ of\\ GPUs \\times t\n\\end{equation}\nwhere the $t$ denotes the number of days that each method searched for. In Table~\\ref{table_CIFAR} ``Parameters\" denotes the total number of parameters which can represent the capability of architecture and the complexity. In addition, the symbol ``---\" in Table~\\ref{table_CIFAR} implies there is no result publically reported by the corresponding paper. The year reported in this table is its earliest time made public. Furthermore, there are additional notes provided for Table~\\ref{table_CIFAR}. Firstly, if there are several results reported from literature, such as CGP-DCNN~ and CARS~, we choose to report one or two of the most representative architectures. Secondly, we name two algorithms without proper names in Table~\\ref{table_CIFAR} (i.e., Firefly-CNN~ and EEDNAS-NNMM~) based on the EC methods they used and the first letter of the title. Thirdly, we report the classification errors of CGP-CNN~ in the format of ``best (mean $\\pm$ std)\". Finally, the symbols of ``+\" and ``-\" in the column of ``GPU Days\" denote the meaning of ``more than\" and ``less than\", respectively.\nIn principle, there is no totally fair comparison. Due to the following two reasons: (1) The encoding space including the initial space and the search space is different from each other. There are two extreme cases in the initial space: trivial initialization which starts at the simplest architecture and rich initialization which starts from a well-designed architecture (e.g., ResNet-50~). Meanwhile the size of the search space is largely different, e.g., Ref~ only takes the kernel size into the search space. (2) Different tricks exploited in the methods, e.g., the ``cutout\", can make the comparisons unfair, too. The ``cutout\" refers to a regularization method~ used in the training of CNNs, which could improve the final performance appreciably.\nTable~\\ref{table_CIFAR} shows the progress of ENAS for image classification according to the accuracy on CIFAR-10: LargeEvo algorithm~ (5.4\\%, 2017), LEMONADE~ (2.58\\%, 2018), and NSGANet~ (2.02\\%, 2019). Many ENAS methods have a lower error rate than ResNet-110~ with 6.43\\% error rate on CIFAR-10, which is a manually well-designed architecture. Therefore, the architecture found by ENAS can reach the same level or exceed the architecture designed by experts. It shows that the ENAS is reliable and can be used in other application fields.", "cites": [6869, 6851, 6877, 6821, 870, 6837, 8247, 6847, 6863, 6878, 6844, 6834, 6845, 6829, 6831, 97, 6842, 6827, 8319, 6836, 6826], "cite_extract_rate": 0.5384615384615384, "origin_cites_number": 39, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section presents a table comparing ENAS methods on CIFAR-10, CIFAR-100, and ImageNet, but lacks synthesis of ideas across the papers. It primarily serves as a factual summary with minimal integration of insights. Some limited critical evaluation of performance metrics is implied, but limitations or underlying principles of the methods are not discussed. The abstraction is minimal, focusing on concrete results without generalizing broader patterns."}}
{"id": "d6c13ce5-ab8c-4d29-a77d-718dbcbb0cf9", "title": "The Effectiveness", "level": "subsection", "subsections": [], "parent_id": "e3dda7bf-8a69-4f93-8f3f-07b17269af8c", "prefix_titles": [["title", "A Survey on Evolutionary Neural Architecture Search"], ["section", "Challenges and Issues"], ["subsection", "The Effectiveness"]], "content": "The effectiveness of ENAS is questioned by many researchers. Wistuba \\textit{et al.}~ noticed that the random search can get a well-performed architecture and has proven to be an extremely strong baseline. Yu \\textit{et al.}~ showed the state-of-the-art NAS algorithms performed similarly to the random policy on average. Liashchynskyi \\textit{et al.}~ compared with grid search, random search, and GA for NAS, which showed that the architecture obtained by GA and random search have similar performance. There is no need to use complicated algorithms to guide the search process if random search can outperform NAS based on EC approaches.\nHowever, the evolutionary operator in~ only contains a recombination operator, which limited the performance of ENAS algorithms. Although random search can find a well-performed architecture in the experiments, it cannot guarantee that it will find a good architecture every time. Moreover, recent researches~ also showed the evolutionary search was more effective than random search. Furthermore, the experiments in Amoebanet~ and NAS-Bench-201~ also showed that ENAS can search for and find better architectures. Thus, there is an urgent need to design an elaborated experiment to reveal what components in ENAS are responsible for the effectiveness of the algorithm, especially in a large encoding space.\nIn Section~\\ref{sec_Population_operators}, two types of operators have been introduced. We note that some methods like the LargeEvo algorithm~ only use single individual based operator (mutation) to generate offspring. The main reason that they did not involve the crossover operator in their method come from two reasons: the first is for simplicity~, and the second is that simply combining a section of one individual with a section of another individual seems ``ill-suited\" to the neural network paradigm~. Secondly in~, the authors believed that there was no indication that a recombination operation applied to two individuals with high fitness would result in an offspring with similar or better fitness.\nHowever, the supplemental materials in~ demonstrated the effectiveness of the crossover operator in this method. This method can find a good architecture with the help of the crossover operation. On the contrary, when crossover is not used, the architecture found is not promising, unless it runs for a long time. In fact, the mutation operator let an individual explore the neighbouring region, and it is a gradually incremental search process like searching step by step. Crossover (recombination) can generate offspring dramatically different from the parents, which is more likely a stride. So, this operator has the ability to efficiently find a promising architecture. Chu \\textit{et al.}~ preferred that while a crossover mainly contributes to exploitation, a mutation is usually aimed to introduce exploration. These two operators play different roles in the evolutionary process. But there is not a sufficient explanation of how the crossover operator works. Maybe some additional experiments need to be done on the methods without the crossover operator. \nThe EC approach generally performs well when dealing with practical problems (e.g., the vehicle routing problem). This is mainly because EC is designed collectively by considering the domain knowledge in most cases~. Similarly, another key reason to ensure the effectiveness of the ENAS algorithm is that the EC approach can effectively consider (and embed) some domain knowledge of the neural architecture. Since the original intention of ENAS is to design neural architectures automatically without manual experience, the future development of ENAS will need to continue to reduce any artificial experience by including prior knowledge of the encoding space. In this case, how to effectively embed domain knowledge into ENAS is a big challenge.", "cites": [8955, 6831, 8247, 6880, 870, 6854, 6825, 6879, 6844, 6830], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple cited papers to discuss the effectiveness of ENAS in comparison to random and grid search. It critically evaluates the role of evolutionary operators (crossover vs. mutation) and highlights limitations and contradictions in existing approaches. While it identifies some broader patterns, such as the impact of domain knowledge and the role of operators, it stops short of providing deeper meta-level generalizations."}}
{"id": "8f512a41-ca51-4724-a511-f83f8ff395ee", "title": "Scalability", "level": "subsection", "subsections": [], "parent_id": "e3dda7bf-8a69-4f93-8f3f-07b17269af8c", "prefix_titles": [["title", "A Survey on Evolutionary Neural Architecture Search"], ["section", "Challenges and Issues"], ["subsection", "Scalability"]], "content": "The scale of the datasets used in most ENAS methods is relatively large. Taking the image classification task as an example, the MNIST dataset~ is one of the earliest datasets. In total, it contains $70, 000$ $28\\times28$ grayscale images. Later in 2009, CIFAR-10 and CIFAR-100~ including $60, 000$ $32\\times32$ color images are medium-scale datasets. One of the most well-known large-scale datasets is ImageNet~, which provides more than 14 million manually annotated high-resolution images. Unlike CIFAR-10 and CIFAR-100, which are commonly used in ENAS, fewer methods choose to verify their performance on ImageNet~. This can be explained by the data shown in Table~\\ref{table_CIFAR}, where the GPU Days are usually tens or even thousands. It is unaffordable for most researchers when the medium-scale dataset changes to the lager one.\nHowever, Chen \\textit{et al.}~ believed that the results on the larger datasets like ImageNet are more convincing because CIFAR-10 is easy to be over-fitting. A popular way to deal with this is using a proxy on CIFAR-10 and transfers to ImageNet~. Another alternative approach is to use the down-scaled dataset of a large-scale dataset such as ImageNet$64\\times64$~.", "cites": [6829, 6851, 6881, 6866, 870], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to address the scalability challenge in ENAS, discussing the use of proxy datasets and downscaling techniques. It provides a critical perspective by noting the computational cost differences between datasets and the trade-offs in validation. While it identifies a broader pattern around dataset scalability, it does not fully generalize to a meta-level framework or offer a novel synthesis of ideas."}}
{"id": "0511a9dd-9340-4ba1-ba4f-5c1c82725c9e", "title": "Efficient Evaluation Method and Reduce Computational Cost", "level": "subsection", "subsections": [], "parent_id": "e3dda7bf-8a69-4f93-8f3f-07b17269af8c", "prefix_titles": [["title", "A Survey on Evolutionary Neural Architecture Search"], ["section", "Challenges and Issues"], ["subsection", "Efficient Evaluation Method and Reduce Computational Cost"]], "content": "Section~\\ref{sec_evaluation} has introduced the most popular and effective ways to reduce the fitness evaluation time and the computational cost. In a nutshell, it can be described as a question that strikes the balance between the time spent and the accuracy of the evaluation. Because of the unbearable time for fully training the architecture, we must compromise as little as we can on the evaluation accuracy in exchange for significant reduction in the evaluation time without sufficient computing resources.\nAlthough a lot of ENAS methods have adopted various means to shorten the evaluation time. Even though Sun \\textit{et al.}~ specifically proposed a method for acceleration, the research direction of search acceleration is just getting started. The current approaches have many limitations that need to be addressed. For example, although the LargeEvo algorithm~ used the weight inheritance to shorten the evaluation time and reduced the computational cost, it still ran for several days with the use of lots of computational resources which cannot be easily accessed by many researchers. Furthermore, there is no baseline and common assessment criteria for search acceleration methods. It is a major challenge to propose a novel method to evaluate the architecture accurately and quickly.", "cites": [8247], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a coherent discussion of the challenges in reducing computational cost in ENAS, citing one relevant paper (LargeEvo) and highlighting the trade-off between evaluation time and accuracy. It identifies limitations in current methods and the lack of standardized benchmarks, showing some critical perspective. However, the synthesis is limited to a single paper with minimal cross-source integration, and the abstraction remains at the level of general observations rather than forming a higher-level framework."}}
{"id": "97b2a2f7-bddb-45d3-819a-f3dcbad41cba", "title": "Interpretability", "level": "subsection", "subsections": [], "parent_id": "e3dda7bf-8a69-4f93-8f3f-07b17269af8c", "prefix_titles": [["title", "A Survey on Evolutionary Neural Architecture Search"], ["section", "Challenges and Issues"], ["subsection", "Interpretability"]], "content": "CNNs are known as black-box-like solutions, which are hard to interpret~. Although some works have been done to visualize the process of feature extraction~, they are uninterpretable due to a large number of learned features~. The low interpretability of the manual designed architecture becomes a big obstacle to the development of neural networks. To overcome this obstacle, some researches~ used GP to automatically design the neural network. Being well-known for its potential interpretability, GP aims at solving problems by automatically evolving computer programs~.\nAll the above researches~ gave a further analysis to demonstrate the interpretability. Specifically, Evans \\textit{et al.}~ have made a visualization on the JAFFE dataset~ to expound how the evolved convolution filter served as a form of edge detection, and the large presence of white color in the output of the convolution can help the classification. In their subsequent work~, they made a visualization of the automatically evolved model on the Hands dataset~, where the aggregation function extracts the minimum value of a specific area in the hand image to determine whether the hand is open or closed. Furthermore, Bi \\textit{et al.}~ displayed the features described by the evolved functions like convolution, max-pooling and addition, and the generated salient features are discriminative for face classification.\nDespite the interpretability the existing work made, all these GP-based ENAS methods only aim at shallow NNs and the number of the generated features is relatively small. However, all the most successful NNs have a deep architecture. This is due partly to the fact that very few GP based methods can be run on GPUs. It is necessary to use deep-GP to evolve a deeper GP tree and make a further analysis on the deeper architecture in the future.", "cites": [499, 6865], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the interpretability challenges in CNNs and how GP-based NAS can offer more transparent models. It synthesizes key examples from cited works to show how GP improves interpretability through visualizations and function analysis. However, the critical analysis is somewhat limited to pointing out the current limitations (e.g., shallow networks, GPU incompatibility) without deeper evaluation or contrasting these with alternative methods."}}
{"id": "ed6f44db-5133-4077-be4f-e9675dc00180", "title": "Future Applications", "level": "subsection", "subsections": [], "parent_id": "e3dda7bf-8a69-4f93-8f3f-07b17269af8c", "prefix_titles": [["title", "A Survey on Evolutionary Neural Architecture Search"], ["section", "Challenges and Issues"], ["subsection", "Future Applications"]], "content": "Table~\\ref{table_application} shows various applications which have been explored by ENAS. But these are just a small part of all areas of neural network applications. ENAS can be applied wherever neural networks can be applied, and automate the process of architecture designed which should have been done by experts. Moreover, plenty of the image classification successes of ENAS have proven that ENAS has the ability to replace experts in many areas. The automated architecture design is a trend.\nHowever, this process is not completely automated. The encoding space (search space) still needs to be designed by experts for different applications. For example, for the image processing tasks, CNNs are more suitable, so the encoding space contains the layers including convolution layers, pooling layers and fully connected layers. For the time-series data processing, RNNs are more suitable, so the encoding space may contain the cells including $\\Delta$-RNN cell, LSTM~, Gated Recurrent Unit (GRU)~, Minimally-Gated Unit (MGU)~, and Update-Gated RNN (UGRNN)~. The two manually determined encoding spaces already contain a great deal of artificial experience and the components without guaranteed performance are excluded. The problem is: can a method search the corresponding type of neural network for multiple tasks in a large encoding space, including all the popular widely used components? Instead of searching one multitask network~ which learns several tasks at once with the same neural network, the aim is to find appropriate networks for different tasks in one large encoding space.", "cites": [6882, 39, 6876, 9081], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to highlight the application-specific nature of ENAS encoding spaces, particularly in RNN-based and CNN-based designs. It critically points out that automation is not complete due to the reliance on expert-designed encoding spaces. The abstraction is moderate, as it moves beyond individual papers to suggest a broader research direction involving multitask networks and generalization in search spaces."}}
{"id": "14a9080c-0216-4f7c-899b-00f8965feb35", "title": "Fair Comparisons", "level": "subsection", "subsections": [], "parent_id": "e3dda7bf-8a69-4f93-8f3f-07b17269af8c", "prefix_titles": [["title", "A Survey on Evolutionary Neural Architecture Search"], ["section", "Challenges and Issues"], ["subsection", "Fair Comparisons"]], "content": "Section~\\ref{sec_CIFAR} gives a brief introduction of the unfair comparisons. The unfairness mainly comes from two aspects: (1) the tricks including cutout~, ScheduledDropPath~, etc. (2) The different encoding spaces. For aspect (1), some ENAS methods~ have reported the results with and without the tricks. For aspect (2), the well-designed search space is widely used in different ENAS methods. For instance, the NASNet search space~ is also used in~ because it is well-constructed so that even random search can perform well. The comparison under the same condition can tell the effectiveness of different search methods.\nFortunately, the first public benchmark dataset for NAS, the NAS-Bench-101~ has been proposed. The dataset contains 432K unique convolutional architectures based on the cell-based encoding space. Each architecture can query the corresponding metrics, including test accuracy, training time, etc., directly in the dataset without the large-scale computation. NAS-Bench-201~ was proposed recently and is based on another cell-based encoding space, which does not have no limits on edges. Compared with NAS-Bench-101, which was only tested on CIFAR-10, this dataset collects the test accuracy on three different image classification datasets (CIFAR-10, CIFAR-100, ImageNet-16-120~). But the encoding space is relatively small, and only contains 15.6K architectures. Experiments with different ENAS methods on these benchmark datasets can get a fair comparison and it will not take too much time. However, these datasets are only based on the cell-based encoding spaces and cannot contain all the search space of the existing methods, because the other basic units (layers and blocks) are built using more hyper-parameters, which may lead to a larger encoding space.\nIn the future, a common platform making fair comparisons needs to be built. This platform must have several benchmarks encoding space, such as the NASNet search space, NAS-Bench-101 and NAS-Bench-201. All the ENAS methods can be directly tested on the platform. Furthermore, this platform also needs to solve the problem that different kinds of GPUs have different computing power, which may cause an inaccurate GPU Days based on different standards. The GPU Days cannot be compared directly until they have a common baseline of computing power.", "cites": [871, 6524, 6881, 8319, 6854, 870, 9080], "cite_extract_rate": 0.875, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes information from multiple cited works to explain the challenges in making fair comparisons in EC-based NAS. It integrates discussions on tricks, encoding spaces, and benchmark datasets like NAS-Bench-101 and NAS-Bench-201. It also critically assesses the limitations of current benchmarks and identifies the need for a unified platform. While it offers meta-level insights on the need for standardization, its abstraction remains somewhat constrained to the specific context of EC-based NAS."}}
