{"id": "1107ff3b-7f8f-45af-bf5c-5ef3d043677e", "title": "Introduction", "level": "section", "subsections": ["1866f73b-494a-47fb-ae56-18b7dff9b9ca", "45bdb02e-6313-44fd-a72e-51199db5900d"], "parent_id": "01a5d650-4933-4e6e-b664-6a6a8f2896ea", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Introduction"]], "content": "\\IEEEPARstart{R}{ecent} advances in deep learning techniques have rapidly transformed these approaches into the methodology of choice for analyzing medical images, and in particular for histology image classification problems~. Because of the increasing availability of large scale high-resolution whole-slide images (WSI) of tissue specimens, digital pathology and microscopy have become appealing application areas for deep learning algorithms.\nGiven wide variations in pathology and the often time-consuming diagnosis process, clinical experts have begun to benefit from computer-aided detection and diagnosis methods capable of learning  features that optimally represent the data~.\nThis thorough survey serves as an accurate guide to biomedical engineering and clinical research communities interested in discovering the tissue composition-to-functionality relationship using image-to-graph translation and deep learning.\nThere are several review papers available that analyse the benefits of deep learning for providing reliable support for microscopic and digital pathology diagnosis and treatment decisions~, and specifically for cancer diagnosis~.\nCompared to other medical fields such as dermatology, ophthalmology, neurology, cardiology, and radiology, digital pathology and microscopy is one of the most dominant medical applications of deep learning. One driving force behind innovation in computational pathology has been the introduction of grand challenges\n(\\textit{e.g.} NuCLS~, BACH~, MoNuSeg~). Developed techniques that offer decision support to human pathologists have shown bright prospects for detecting, segmenting, and classifying the cell and nucleus; and detecting and classifying diseases such as cancer.\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{images/Fig1a.png}\n\\caption{\nTraditional CNNs excel at modelling local relations in grid representation, where the topology of the neighborhood is constant (Left).\nGCNs can take into account different neighbouring relations (global relation) by going beyond the local pixel neighbourhoods used by convolutions. On a graph, the neighbours of a node are unordered and variable in size (Right).}\n\\label{fig:Fig1a}\n\\vspace{-10pt}\n\\end{figure}\nDeep learning techniques such as convolutional neural networks (CNNs) have demonstrated success in extracting image-level representations, however, they are inefficient when dealing with relation-aware representations. Modern deep learning variations of graph neural networks (GNNs) have made a significant impact in many technological domains for describing relationships.\nGraphs, by definition, capture relationships between entities and can thus be used to encode relational information between variables~. As a result, special emphasis has been placed on the generalisation of GNNs into non-structured and structured scenarios.\nTraditional CNNs analyse local areas based on fixed connectivity (determined by the convolutional kernel), leading to limited performance, and difficulty in interpreting the structures being modeled.\nGraphs, on the other hand, offer more flexibility to analyse unordered data by preserving neighboring relations. This difference is illustrated in Fig.{~\\ref{fig:Fig1a}}.\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=1\\linewidth]{images/Fig1.png}\n\\vspace{-15pt}\n\\caption{\n\\textbf{Top:} \nGraph-based representation of images for relation-aware human-object interaction, image segmentation, and human pose estimation (left-to-right). Images adapted from~.\n\\textbf{Bottom:} \n\\textbf{A.} Cell-graph representation for prostate cancer.\n\\textbf{B.} Tissue-graph representation for colorectal cancer.\n\\textbf{C.} Hierarchical cell-to-tissue graph representation for breast cancer.\nImages adapted from~.}\n\\label{fig:Fig1}\n\\vspace{-10pt}\n\\end{figure}\nThe adaptation of deep learning from images to graphs has received increased attention, leading to a new cross-domain field of graph-based deep learning which seeks to learn informative representations of graphs in an end-to-end manner. This field has exhibited remarkable success for various tasks as discussed by recent surveys on graph deep learning frameworks and their applications~. \nGraph embeddings have appeared in computer vision tasks where graphs can efficiently define relationships between objects, or for the purpose of graph-structured image analysis. \nInteresting results have been obtained for object detection, semantic segmentation, skeleton-based action recognition, image classification and human-object interaction tasks as illustrated in Fig.~\\ref{fig:Fig1} (Top). \nMedical applications have benefited from rapid progress in the field of computer vision and GNNs. \nThe development of GNNs has seen the application of deep learning methods to GNNs, such as graph convolutional networks (GCNs). These models have been proposed as a powerful tool to model functional and anatomical structures, brain electrical activity, and segmentation of the vasculature system and organs~. \nHistological images depict the micro-anatomy of a tissue sample, and pathologists use histological images to make diagnoses based on morphological changes in tissues, the spatial relationship between cells, cell density, and other factors. Graph-based methods, which can capture geometrical and topological properties, are able to model cell-level information and overall tissue micro-architecture. \nPrior to the advent of deep learning, numerous approaches for processing histopathological images as graphs were investigated~. \nThese methods used classical machine learning approaches, which are less accurate for graph classification compared to GCNs. \nThe capabilities of graph-based deep learning, which bridges the gap between deep learning methods and traditional cell graphs for disease diagnosis, are yet to be sufficiently investigated.\nIn this survey, we analyse how graph embeddings are employed in histopathology diagnosis and analysis. \nWhile graphs are not directly expressed within this data, they can efficiently describe relationships between tissue regions and cells. \nThis setting offers a very different task for GNNs in comparison to analysis of unstructured data such as electrophysiological and neuroimaging recordings where the data can be directly mapped to a graph{~}.\nSelected samples of graph representations in digital pathology (cell-graph, patch-graph, tissue-graph and cell-tissue representation) used to capture and learn relevant morphological regions that will be covered in this review are illustrated in Fig.{~\\ref{fig:Fig1}} (Bottom).\nThis survey offers a comprehensive overview of preprocessing, graph models and explainability tools used in computational pathology, highlighting the capability of GNNs to detect and associate key tissue architectures, regions of interest, and their interdependence.\nAlthough some papers have surveyed conventional cell graphs with handcrafted features to characterize the entities~, and others have briefly touched upon the benefits of GCNs in biology and medicine{~}, to the best of our knowledge, no systematic review exists that presents and discusses all relevant works concerning graph-based representations and deep learning models for computational pathology.\n\\vspace{-6pt}", "cites": [8845, 6936, 262, 553, 885, 6938, 6935, 8305, 550, 6937], "cite_extract_rate": 0.4166666666666667, "origin_cites_number": 24, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes concepts from multiple cited papers to highlight the transition from CNNs to GNNs in histopathology, emphasizing the need for relational modeling. It critically compares traditional CNNs with GCNs and notes limitations of classical graph methods and current gaps in systematic reviews. The section abstracts broader principles of graph-based deep learning and its relevance to medical image analysis, offering a conceptual framework for the field."}}
{"id": "1866f73b-494a-47fb-ae56-18b7dff9b9ca", "title": "Why graph-based deep learning for characterizing diseases through histopathology slides?", "level": "subsection", "subsections": [], "parent_id": "1107ff3b-7f8f-45af-bf5c-5ef3d043677e", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Introduction"], ["subsection", "Why graph-based deep learning for characterizing diseases through histopathology slides?"]], "content": "Deep learning has increased the potential of medical image analysis by enabling the discovery of morphological and textural representations in images solely from the data. Although CNNs have shown impressive performance in the field of histopathology analysis, they are unable to capture complex neighborhood information as they analyse local areas determined by the convolutional kernel. To extract interaction information between objects, a CNN needs to reach sufficient depth by stacking multiple convolutional layers, which is inefficient. This leads to limitations in the performance and interpretability of the analysis of anatomical structures and microscopic samples.\nGraph convolutional networks (GCNs) are a deep learning-based method that operate over graphs, and are becoming increasingly useful for medical diagnosis and analysis~. GCNs can better exploit irregular relationships and preserve neighboring relations compared with CNN-based models~. \nBelow we outline the reasons why current research in histopathology has shifted the analytical paradigm from pixel to entity-graph processing:\n\\begin{enumerate}\n    \\item The potential correlations among images are ignored during traditional CNN feature learning, however, a GCN can be introduced to estimate the dependencies between images and enhance the discriminative ability of CNN features~.\n    \\item CNNs have been commonly used for the analysis of whole slide images (WSI) by classifying fixed-sized biopsy image patches using fixed fusion rules such as averaging features or class scores, or weighted averaging with learnable weights to obtain an image-level classification score. \n    Aggregation using a CNN also includes excessive whitespace, putting undue reliance on the orientation and location of the tissue segment.\n    Even though CNN-based models have practical merits through considering important patches for prediction, they dismiss the spatial relationships between patches, or global contextual information.\n    Architectures are required to be capable of dealing with size and shape variation in region-of-interests (ROIs), and must encode the spatial context of individual patches and their collective contribution to the diagnosis, which can be addressed with graph-based representations~.\n    \\item A robust computer-aided detection system should be able to capture multi-scale contextual features in tissues, which can be difficult with traditional CNN-based models. A pathological image can be transformed into a graph representation to capture the cellular morphology and topology (cell-graph)~, and the attributes of the tissue parts and their spatial relationships (tissue-graph)~.\n    \\item Graph representations can enhance the interpretation of the final representation by modeling relations among different regions of interest. Graph-based models offer a new way to verify existing observations in pathology. \n    Attention mechanisms with GCNs, for example, highlight informative nuclei and inter-nuclear interactions, allowing the production of interpretable maps of tissue images displaying the contribution of each nucleus and its surroundings to the final diagnosis~.\n    \\item By incorporating any task-specific prior pathological information, an entity-graph can be customized in various ways. As a result, pathology-specific interpretability and human-machine co-learning are enabled by the graph format~.\n    \\item GCNs are a complimentary method to CNNs for morphological feature extraction, and they can be employed instead of, or in addition to CNNs during multimodal fusion for fine-grained patient stratification~.\n\\end{enumerate}\n\\vspace{-6pt}", "cites": [6940, 6939, 553, 8305], "cite_extract_rate": 0.36363636363636365, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key ideas from multiple papers, connecting CNN limitations with the advantages of GCNs in histopathology analysis. It provides a critical perspective by highlighting inefficiencies and lack of spatial modeling in traditional CNNs. The abstraction is strong, as it generalizes the use of graph-based methods for multi-scale and spatially aware analysis, offering a conceptual shift in the field."}}
{"id": "45bdb02e-6313-44fd-a72e-51199db5900d", "title": "Contribution and organisation", "level": "subsection", "subsections": [], "parent_id": "1107ff3b-7f8f-45af-bf5c-5ef3d043677e", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Introduction"], ["subsection", "Contribution and organisation"]], "content": "Compared to other recent reviews on traditional deep learning in histopathology slides, our manuscript captures the current efforts relating to entity-graphs and recent advancements in GCNs for characterizing diseases and pathology tasks.\nPapers included in the survey are obtained from various journals, conference proceedings and open-access repositories.\nTable~\\ref{table:Applications} outlines the applications that were addressed across all reviewed publications. It is noted that breast cancer analysis constitutes the major application in digital pathology that has been analyzed using graph-based deep learning techniques.\nThis review is divided into three major sections.\nIn Section~\\ref{sec:sec2} we provide a technical overview of the prevailing tools for entity-graph representation and graph architectures used in accelerating digital pathology research.\nIn Section~\\ref{sec:sec3} we introduce the current applications of deep graph representation learning and cluster these proposals based on the graph construction (cell-graph, patch-graph, tissue-graph, hierarchical graph) and feature level fusion methods followed by the task or organ on which they operate.\nFinally, Section~\\ref{sec:sec4} highlights open problems and perspectives regarding the shifting analytical paradigm from pixel to entity-based processing. Specifically, we discuss the topics of graph construction, embedding expert knowledge, complexity of graph models, training paradigms, and graph model interpretability.\n\\begin{table}[t!]\n\\caption{Summary of applications of graph-based deep learning in histopathology covered in this survey.}\n\\vspace{-2pt}\n\\centering\n\\label{table:Applications}\n\\resizebox{0.44\\textwidth}{!}{\n\\begin{tabular}{\nlcl\n}\n\\toprule\n\\textbf{Application} & \\textbf{\\#Applications} & \\textbf{Reference} \\\\\n\\midrule\nBreast cancer & 11\n&  \\\\\nColorectal cancer & 6\n&  \\\\\nProstate cancer & 3  \n&  \\\\\nLung cancer & 3  \n&  \\\\\nCervical cancer & 2 \n&  \\\\\nLymphoma & 1 \n&  \\\\\nSkin cancer & 1 \n&  \\\\\nRenal cancer & 1  \n&  \\\\\n\\midrule\n\\textbf{Total} & \\textbf{28} \n& \\\\\n\\bottomrule\n\\end{tabular}}\n\\vspace{-9pt}\n\\end{table}\n\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=1\\linewidth]{images/Fig2-updated.png}\n\\caption{\nOverview of a standard graph-based workflow in computational pathology.\nThe WSI image is first transformed into one or more graphs.\n\\textbf{1.} The entities can be nuclei, patches or tissue regions. \n\\textbf{2.} Node features comprise handcrafted or deep learning features to characterize the entities.\n\\textbf{3.} The edges encode intrinsic relationships (spatial or semantic) among the entities.\n\\textbf{4.} Graph encoding and classification (node-level or graph-level prediction): the graph representation is processed using GNNs and its variants such as ChebNet, GCN, GraphSAGE, GAT, and GIN, including different graph pooling strategies (global or hierarchical pooling).\n\\textbf{5.} Graph interpretations: a set of GNN model interpretability tools such as graph attentions or post-hoc graph explainers (\\textit{e.g.}~GNNExplainer and GraphGrad-CAM.)}\n\\label{fig:Fig2}\n\\vspace{-8pt}\n\\end{figure*}", "cites": [6943, 6945, 6936, 6942, 6939, 6941, 6944, 6940], "cite_extract_rate": 0.3076923076923077, "origin_cites_number": 26, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic synthesis by grouping papers into sections of the survey based on their application and methodology, but it lacks deeper integration of ideas or a novel framework. There is minimal critical analysis, focusing primarily on summarizing contributions rather than evaluating their strengths or limitations. Some abstraction is attempted by categorizing applications and graph construction types, but it remains largely surface-level without overarching principles or insights."}}
{"id": "71961fe0-d880-47a5-944e-136229bc22c0", "title": "Graph representation learning \\\\ in digital pathology: Background", "level": "section", "subsections": ["e0b23ea9-2f62-4439-87d8-347ef71f4911", "773fcdc6-0414-4c43-9a26-ec3ad24224ca", "10f6e9a7-6d01-437b-8683-eef537fbe22e", "b0482895-606f-4df3-8a91-6704a57c5554"], "parent_id": "01a5d650-4933-4e6e-b664-6a6a8f2896ea", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Graph representation learning \\\\ in digital pathology: Background"]], "content": "\\label{sec:sec2}\nTranslating patient histopathological images into graphs to encode the spatial context of cells and tissues for a given patient has been used to improve prediction accuracy of various pathology tasks. Graph representations followed by GNN-based models and interpretability approaches allows pathologists to directly comprehend and reason for the outcomes. GNNs can also serve a variety of prediction purposes by adapting different designs, such as performing node-level and graph-level predictions.\nA standard entity-graph based pathological workflow requires several phases, such as node and graph topology definition, as well as the choice of GNN architecture.\nIn this section, we provide technical insights of these phases that are required for graph analytics in computational pathology: \n(1) Graph representation (entity, embeddings and edges definition);\n(2) Graph models (graph structures for processing graph-structured);\nand (3) Explainability (a set of interpretation methodologies such as model-based and post-hoc interpretability).\nA traditional framework with aforementioned phases is illustrated in Fig.{~\\ref{fig:Fig2}}.\nA deep analysis of each GNN model can be found in survey papers that deal with graph architectures~.\n\\vspace{-6pt}", "cites": [553, 550], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section briefly outlines the role of graph representations and GNNs in digital pathology, but it does so in a largely descriptive manner without deeply integrating or synthesizing the cited survey papers. There is minimal critical analysis or identification of limitations, and the abstraction is limited to a generic workflow description without uncovering broader principles or trends."}}
{"id": "d68d0b84-4119-48cb-97c3-a58a87699858", "title": "Node definition", "level": "paragraph", "subsections": [], "parent_id": "44b2f3c3-1a14-491e-8d42-de4b79515c0f", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Graph representation learning \\\\ in digital pathology: Background"], ["subsection", "Histopathology graph representation"], ["subsubsection", "Graph construction"], ["paragraph", "Node definition"]], "content": "WSI usually includes significant non-tissue regions. To identify tissue regions the foreground is segmented with Gaussian smoothing and OTSU thresholding{~}.\nOne of the most common graph representation, cell-graphs, requires model training and fine-tuning for cell detection or segmentation. To detect nuclei several methods have been used such as Hover-Net{~}, CIA-Net{~}, \nUNet{~} and cGANs{~}, that are trained on multi-organ nuclei segmentation datasets (MoNuSeg{~}, PanNuke{~}, CoNSep{~}). The entities can also be calculated using agglomerative clustering{~} of detected cells.\nThe nodes in a graph can also be represented by fixed-sized patches (patch-graphs) randomly sampled from the raw WSI or by using a patch selection method where non-tissue regions are removed{~}. \nImportant patches can be sampled from segmented tissues using color thresholds where patches with similar features (tissue cluster) are modeled as a node.\nPre-trained deep learning models on tissue datasets (\\textit{e.g.} NCT-CRC-HE-100{~}) have also been used to detect the tumor region of the specific pathological task.\nMeaningful tissue regions have been also used as nodes to capture the tissue distribution (tissue-graphs). To separate tissue structures, superpixels{~} obtained using unsupervised algorithms such as simple linear iterative clustering (SLIC){~}) become nodes.", "cites": [8460, 6948, 6946, 6949, 6947, 825], "cite_extract_rate": 0.5454545454545454, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of node definition in histopathology graph construction, citing various methods and models used for cell detection and segmentation. However, it lacks synthesis across the cited papers and does not critically evaluate their approaches or limitations. The content remains at a concrete level without abstracting broader principles or frameworks."}}
{"id": "0e528c80-31df-44a1-b9be-77ac1fbb2150", "title": "Node embeddings", "level": "paragraph", "subsections": [], "parent_id": "44b2f3c3-1a14-491e-8d42-de4b79515c0f", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Graph representation learning \\\\ in digital pathology: Background"], ["subsection", "Histopathology graph representation"], ["subsubsection", "Graph construction"], ["paragraph", "Node embeddings"]], "content": "Node features can comprise hand-crafted features including morphological and topological properties (\\textit{e.g.} shape, size, orientation, nuclei intensity, and the chromaticity using the gray-level co-occurrence matrix).\nFor cell-graph representations, some works include learned features extracted from the trained model used to localise the nuclei.\nIn patch-graph methods, deep neural networks are used to automatically learn a feature representation from patches around the centroids of the nuclei and tissue regions. If the entity is larger than the specified patch size, multiple patches inside the entity are processed, and the final feature is computed as the mean of the patch-level deep features.\nSome works have aggregated features from neighboring patches and combined them to obtain a central node representation to increase feature learning performance.\nAuthors have adopted CNNs (MobileNetV2, DenseNet, ResNet-18 or ResNet-50{~}), and encoder-decoder segmentation models (UNet{~}) for the purpose of deep feature extraction. To generate patch-level embeddings, ImageNet-pretrained CNN as well as a CNN pretrained for tissue sub-compartment classification task have been used.", "cites": [97, 825], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a factual overview of node embeddings in histopathology graph representations, describing common approaches and citing relevant methods like CNNs and UNet. While it integrates some concepts (e.g., hand-crafted vs. learned features), it lacks deeper analysis, comparison, or critique of the cited works. Some general patterns are noted, but the discussion remains at a methodological level without elevating to broader theoretical or conceptual insights."}}
{"id": "df50a9a4-5365-476c-b956-826102762942", "title": "Training paradigms", "level": "subsubsection", "subsections": [], "parent_id": "e0b23ea9-2f62-4439-87d8-347ef71f4911", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Graph representation learning \\\\ in digital pathology: Background"], ["subsection", "Histopathology graph representation"], ["subsubsection", "Training paradigms"]], "content": "From the perspective of supervision, we can categorize graph learning tasks into different training settings. Such approaches have also been used to extract effective representations from data.\n\\begin{itemize}\n    \\item The \\textit{Supervised} learning setting provides labeled data for training.\n    \\item \\textit{Weakly or partially supervised learning} refers to models that are trained using examples that are only partially annotated.\n    \\item \\textit{Semi-supervised learning} trains a model using a small set of annotated samples, then generates pseudo-labels for a large set of samples without annotations, and learns a final model by mixing both sets of samples.\n    \\item \\textit{Self-supervised learning} is a form of unsupervised learning in which the data provides supervisory signals when learning a representation via a proxy task. Annotated data is used to fine-tune the representation once it has been learned.\n    Some self-supervised approaches adopted as feature extractors include contrastive predictive coding (CPC){~}, texture auto encoder (Deep Ten){~}, and variational autoencoders (VAE){~}.\n\\end{itemize}\n\\vspace{-6pt}", "cites": [134, 5680, 6950], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of training paradigms in graph-based deep learning, including supervised, weakly/partially supervised, semi-supervised, and self-supervised learning. It mentions specific methods from the cited papers but does not deeply synthesize or connect their contributions within the broader context of histopathology. There is minimal critical evaluation or abstraction to broader principles."}}
{"id": "773fcdc6-0414-4c43-9a26-ec3ad24224ca", "title": "Graph neural networks models", "level": "subsection", "subsections": ["b8b2f114-15a2-44ed-b36d-1322c72aed62", "138dc01f-1d58-421e-9aaf-52a8356fe628", "cf270f9c-dc5c-4359-a525-8a4fba3901c2", "0028c1f2-3513-42c7-a618-91bea4922763", "b99bda05-8cf2-4f84-ab99-7cfa8a62def6", "04d76a71-f83f-4b41-bcbc-4e67b8ff20f9"], "parent_id": "71961fe0-d880-47a5-944e-136229bc22c0", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Graph representation learning \\\\ in digital pathology: Background"], ["subsection", "Graph neural networks models"]], "content": "Following graph building, the entity graph is processed using a graph-based deep learning model that works with graph-structured data to perform analysis.\nGCNs can be broadly categorised as spectral-based~ and spatial-based~.\nSpectral-based GCNs use spectral convolutional neural networks, that build upon the graph Fourier transform and the normalized Laplacian matrix of the graph. Spatial-based GCNs define a graph convolution operation based on spatial relationships that exist among graph nodes. \nGraph convolutional networks, similar to CNNs, learn abstract feature representations for each feature at a node via message passing, in which nodes successively aggregate feature vectors from their neighborhood to compute a new feature vector at the next hidden layer in the network.\nA basic GNN consists of two components:\nThe \\textit{AGGREGATE} operation can aggregate neighboring node representations of the center node, whereas the \\textit{COMBINE} operation combines the neighborhood node representation with the center node representation to generate the updated center node representation.\nThe Aggregate and Combine at each $l-th$ layer of the GNN can be defined as follows:\n\\vspace{-2pt}\n\\begin{equation}\nh_{\\mathcal{N}_v}^{(t)} = \\text{AGGREGATE}^{(l)} \\left( \\big\\{ h_u^{l-1}, \\forall u \\in \\mathcal{N}_v \\big\\} \\right) ,\n\\end{equation}\nwhere $h_{\\mathcal{N}_v}^{(t)}$ is the aggregated node feature of the neighbourhood, $h_u^{l-1}$ is the node feature in neighbourhood $\\mathcal{N}(\\cdot)$ of node $v$.\n\\vspace{-2pt}\n\\begin{equation}\n\\resizebox{0.43\\textwidth}{!}{$h_v^{(t)} = \\text{COMBINE}^{(l)} \\left( h_v^{t-1}, h_{\\mathcal{N}_v}^{(t)} \\right) =  \\sigma (W^t \\cdot [ h_v^{t-1} \\| h_{\\mathcal{N}_v}^t   ]   ) $}, \n\\end{equation}\nwhere $h_v^{(t)}$ is the node representation at the $l-th$ iteration. $h_v^{(0)} = x_{v}$ where $x_{v}$ is the initial feature vector for the node, $\\sigma$ denotes the logistic sigmoid function, and $\\|$ denotes vector concatenation. \nWith the network structure and node content information as inputs, the outputs of GNNs can focus on various graph analytic tasks using one of the processes listed below:\n\\begin{itemize}\n    \\item \\textit{Node-level prediction}: A GNN operating at the node-level computes values for each node in the graph and is thus useful for node classification and regression purposes.\n    In node classification, the task is to predict the node label for every node in a graph. To compute the node-level predictions, the node embedding is input to a Multi-Layer Perceptron (MLP) (See Fig.{~\\ref{fig:Fig2-1}}).\n    \\item \\textit{Graph-level prediction}: Refers to GNNs that predict a single value for an entire graph. This is mostly used to classify entire graphs, or compute similarities between graphs.\n    To compute graph-level predictions, the same node embedding used in node-level prediction is input to a pooling process followed by a separate MLP (See Fig.{~\\ref{fig:Fig2-2}}).  \n\\end{itemize}\nIn the following subsections, we describe in more detail the GNN architectures considered in digital pathology analysis methods. Different GNN variants employ different aggregators to acquire information from each node's neighbors, as well as different techniques to update the nodes' hidden states. In GNNs, the number of parameters is dependent on the number of node and edge features, as their aggregation is learned.", "cites": [242, 8313], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic description of GNNs, including spectral and spatial approaches, and introduces the AGGREGATE and COMBINE operations with equations. It cites two relevant papers but does not deeply connect or synthesize their contributions into a broader narrative. There is minimal critical analysis or identification of limitations, and while it generalizes some GNN concepts, it does not offer meta-level insights or a novel framework."}}
{"id": "b8b2f114-15a2-44ed-b36d-1322c72aed62", "title": "ChebNet", "level": "subsubsection", "subsections": [], "parent_id": "773fcdc6-0414-4c43-9a26-ec3ad24224ca", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Graph representation learning \\\\ in digital pathology: Background"], ["subsection", "Graph neural networks models"], ["subsubsection", "ChebNet"]], "content": "The convolution operation for spectral-based GCNs is defined in the Fourier domain by determining the eigen decomposition of the graph Laplacian~. \nThe normalized graph Laplacian is defined as $L=I_N-D^{-1/2}AD^{-1/2}=U \\Lambda U^T$ ($D$ is the degree matrix and $A$ is the adjacency matrix of the graph), where the columns of $U$ are the matrix of eigenvectors and $\\Lambda$ is a diagonal matrix of its eigenvalues. The operation can be defined as the multiplication of a signal $x \\in \\mathbb{R}^N$ (a scalar for each node) with a filter $g_{\\theta}=\\text{diag}(\\theta)$, parameterized by $\\theta \\in \\mathbb{R}^N$,\n\\begin{equation}\ng_{\\theta} \\star x =  U g_\\theta (\\Lambda) U^Tx.\n\\end{equation}\nDefferrard et al.~ proposed a Chebyshev spectral CNN (ChebNet), which approximates the spectral filters by truncated Chebyshev polynomials, avoiding the calculation of the eigenvectors of the Laplacian matrix, and thus reducing the computational cost.\nA Chebyshev polynomial $T_m(x)$ of order $m$ evaluated at $\\tilde{L}$ is used. Thus the operation is defined as,\n\\vspace{-2pt}\n\\begin{equation}\ng_{\\theta} \\star x \\approx \\sum_{m=0}^{M-1} \\theta_m T_m (\\tilde{L})x ,\n\\label{eq:eq2}\n\\end{equation}\nwhere $\\tilde{L}$ is a diagonal matrix of scaled eigenvalues defined as $\\tilde{L}=\\nicefrac{2L}{\\lambda_{\\text{max}}}-I_N$. $\\lambda_{\\text{max}}$ denotes the largest eigenvalue of $L$. The Chebyshev polynomials are defined as $T_m(x)=2xT_{k-1}(x)-T_{k-2}(x)$ with $T_0(x)=1$ and $T_1(x)=x$.\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=1\\linewidth]{images/Fig2-1.png}\n\\caption{\nRepresentation of graph architectures for node-level classification. Recreated from{~}.\n}\n\\label{fig:Fig2-1}\n\\vspace{-8pt}\n\\end{figure}", "cites": [213, 553, 8313], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive explanation of ChebNet and its spectral-based graph convolution operation, primarily paraphrasing the original paper. It includes relevant equations and definitions but does not synthesize insights from multiple cited works or critically evaluate the method's strengths and limitations. There is minimal abstraction beyond the specific model."}}
{"id": "138dc01f-1d58-421e-9aaf-52a8356fe628", "title": "GCN", "level": "subsubsection", "subsections": [], "parent_id": "773fcdc6-0414-4c43-9a26-ec3ad24224ca", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Graph representation learning \\\\ in digital pathology: Background"], ["subsection", "Graph neural networks models"], ["subsubsection", "GCN"]], "content": "A GCN is a spectral-based GNN with mean pooling aggregation. Kipf and Welling~ presented the GCN using a localized first-order approximation of ChebNet.\nIt limits the layer-wise convolution filter to $K=1$ and uses a further approximation of $\\lambda \\approx 2$, to avoid overfitting and limit the number of parameters. Thus, Equation{~\\ref{eq:eq2}} can be simplified to,\n\\begin{equation}\ng_{\\theta} \\star x \\approx \\theta_0^{'}x + \\theta_1^{'}x (L-I_N)x = \\theta_0^{'}x + \\theta_1^{'} D^{-1/2}AD^{-1/2}x .\n\\end{equation}\nHere, $\\theta_0^{'}, \\theta_1^{'}$ are two unconstrained variables. \nA GCN further assumes that $ \\theta = \\theta_0^{'} = -\\theta_1^{'} $, leading to the\nfollowing definition of a graph convolution:\n\\vspace{-2pt}\n\\begin{equation}\ng_{\\theta} \\star x \\approx \\theta (I_N + D^{-1/2} A D^{-1/2} ) x\n\\end{equation}\nThe definition to a signal $X \\in \\mathbb{R}^{N \\times C}$ with $C$ input channels and $F$ filters for feature maps is generalized as follows,\n\\vspace{-2pt}\n\\begin{equation}\nZ = \\tilde{D}^{-1/2} \\tilde{A}\\tilde{D}^{-1/2} X \\Theta ,\n\\label{eq:eq3}\n\\end{equation}\nwhere $\\Theta \\in \\mathbb{R}^{C \\times F}$ is the matrix formed by the filter bank parameters, and $Z \\in \\mathbb{R}^{N \\times F}$ is the signal matrix obtained by convolution.\nFrom a spatial-based perspective, Equation{~\\ref{eq:eq3}} is reformulated in{~} as a message passing layer which updates the node's representation $x_i^{k}$ as follows:\n\\vspace{-2pt}\n\\begin{equation}\n\\begin{split}\nm_i^{k+1} = \\sum_{j \\in N(i) \\cup i }  \\frac{x_j^k}{\\sqrt{ |N(J)|  |N(i)|  }} , \\\\\nx_i^{k+1} = \\sigma (W^k m_i^{k+1}), \n\\label{eq:eq4}\n\\end{split}\n\\end{equation}\nwhere $m_i^k$ is the output of a message passing iteration, $|N(J)|$ and $ |N(i)|$ denote the node degree of node $j$ and $i$ respectively, $W^k$ denotes a layer-specific trainable weight matrix and $\\sigma$ is a non-linearity function.", "cites": [216], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a technical description of GCNs and their formulation, largely paraphrasing the original work by Kipf and Welling. It includes a brief mention of a related paper on message passing but does not meaningfully integrate or synthesize the ideas. There is minimal critical analysis or abstraction to broader principles in graph learning for histopathology."}}
{"id": "cf270f9c-dc5c-4359-a525-8a4fba3901c2", "title": "GraphSAGE", "level": "subsubsection", "subsections": [], "parent_id": "773fcdc6-0414-4c43-9a26-ec3ad24224ca", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Graph representation learning \\\\ in digital pathology: Background"], ["subsection", "Graph neural networks models"], ["subsubsection", "GraphSAGE"]], "content": "GraphSAGE is a spatial-GCN which uses a node embedding with max-pooling aggregation. Hamilton et al.~ offer an extension of GCNs for inductive unsupervised representation learning with trainable aggregation functions instead of simple convolutions applied to neighborhoods as in a GCN. The authors propose a batch-training algorithm for GCNs to save memory at the cost of sacrificing time efficiency.\nIn~ three aggregating functions are proposed: the element-wise mean, an LSTM, and max-pooling. \nThe mean aggregator is an approximation of the convolutional operation from the transductive GCN framework~. An LSTM is adapted to operate on an unordered set by permuting the neighbors of the node. In the pooling aggregator, each neighbor's hidden state is fed through a fully-connected layer, and then a max-pooling operation is applied to the set of the node’s neighbors.\nThese aggregator functions are denoted as,\n\\begin{equation}\nh_{\\mathcal{N}_v}^t = \\text{max} \\big\\{ \\sigma ( W_{\\text{pool}} h_u^{t-1} + b_{\\text{pool}}), \\forall u \\in \\mathcal{N}_v    \\big\\}  , \n\\end{equation}\nwhere $\\mathcal{N}_v$ is the neighborhood set of node $v$, $W_{\\text{pool}}$ and $b_{\\text{pool}}$ are the parameters to be learned, and $\\text{max}\\{ \\cdot \\}$ is the element-wise maximum.\nHence, following the message passing formulation in Equation{~\\ref{eq:eq4}}, the node representation is updated according to, \n\\vspace{-2pt}\n\\begin{equation}\n\\begin{split}\nm_i^{k+1} = MEAN_{j \\in N(i) \\cup i } ( x_j^k )  , \\\\\nx_i^{k+1} = \\sigma (W^k m_i^{k+1}), \n\\end{split}\n\\end{equation}", "cites": [242], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual description of GraphSAGE, explaining its aggregators and how it differs from transductive GCNs. However, it lacks synthesis with other models, critical evaluation of its strengths and weaknesses, and abstraction to broader principles or trends in graph representation learning for histopathology."}}
{"id": "0028c1f2-3513-42c7-a618-91bea4922763", "title": "GAT", "level": "subsubsection", "subsections": [], "parent_id": "773fcdc6-0414-4c43-9a26-ec3ad24224ca", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Graph representation learning \\\\ in digital pathology: Background"], ["subsection", "Graph neural networks models"], ["subsubsection", "GAT"]], "content": "Inspired by the self-attention mechanism{~}, graph attention networks (GAT){~} incorporate the attention mechanism into the propagation steps by modifying the convolution operation. \nGAT is a spatial-GCN model that incorporates masked self-attention layers into graph convolutions and uses a neural network architecture to learn neighbor-specific weights.\nVeli{\\v{c}}kovi{\\'c} et al.~ constructed a graph attention network by stacking a single graph attention layer, $a$, which is a single-layer feed-forward neural network, parametrized by a weight vector $\\vec{a} \\in \\mathbb{R}^{2F^{i}}$. The layer computes the coefficients in the attention mechanisms of the node pair $(i,j)$ by,\n\\begin{equation}\n\\alpha_{i,j} = \\frac{ \\text{exp} (\\text{LeakyReLu} ( \\vec{a}^T [W\\vec{h}_i \\mathbin\\Vert W\\vec{h}_j] ) ) }\n{ \\sum_{k \\in N_i \\mathbb{N} }  \\text{exp} (\\text{LeakyReLu} ( \\vec{a}^T [W\\vec{h}_i \\mathbin\\Vert  W\\vec{h}_k] ) ) } ,\n\\end{equation}\nwhere $\\mathbin\\Vert$ represents the concatenation operation. The attention layer takes as input a set of node features $h=\\{\\vec{h_1},\\vec{h_2},...,\\vec{h_N}\\}, \\vec{h_i} \\in R^F$, where $N$ is the number of nodes of the input graph and $F$ the number of features for each node, and produces a new set of node features $h^{'}=\\{\\vec{h_1}^{'},\\vec{h_2}^{'},...,\\vec{h_N}^{'}\\}, \\vec{h_i}^{'} \\in R^F$ as its output.\nTo generate higher-level features, as an initial step a shared linear transformation, parametrized by a weight matrix $W \\in R^{F'*F}$, is applied to every node and subsequently a masked attention mechanism is applied to every node, resulting in the following scores,\n\\begin{equation}\ne_{ij} = a ( W \\vec{h_i}, W \\vec{h_j} ),\n\\end{equation}\nthat indicates the importance of node $j^{'}s$ features to node $i$. The final output feature of each node can be obtained by applying a non-linearity, $\\sigma$,\n\\begin{equation}\nh_i^{'} = \\sigma ( \\sum_{j \\in N_i} \\alpha_{ij} Wh_j ).\n\\end{equation}\nThe layer also uses multi-head attention to stabilise the learning process. $K$ different attention heads are applied to compute mutually independent features in parallel, and then their features are concatenated.\nThe attention coefficients are used to update the node representation according to the following message passing formulation,\n\\begin{equation}\n\\begin{split}\nm_i^{k+1} = \\sum_{j \\in N(i)} \\alpha_{i,j}^k W^k x_j^k , \\\\\nx_i^{k+1} = \\sigma (\\alpha_{i,j}^k W^k x_j^k + m_i^{k+1}),\n\\end{split}\n\\end{equation}", "cites": [180, 38], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section primarily describes the GAT model, including its mathematical formulation and key components, drawing from Veličković et al. (Paper 1) and the self-attention mechanism from Paper 2. While it integrates the attention mechanism's influence on graph neural networks, it lacks deeper critical evaluation of the model’s strengths, limitations, or comparative advantages. The content remains focused on technical details without generalizing to broader principles or trends in graph-based deep learning for histopathology."}}
{"id": "b99bda05-8cf2-4f84-ab99-7cfa8a62def6", "title": "GIN", "level": "subsubsection", "subsections": [], "parent_id": "773fcdc6-0414-4c43-9a26-ec3ad24224ca", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Graph representation learning \\\\ in digital pathology: Background"], ["subsection", "Graph neural networks models"], ["subsubsection", "GIN"]], "content": "The graph isomorphism network (GIN)~ is a spatial-GCN that aggregates neighborhood information by summing the representations of neighboring nodes. Isomorphism graph-based models are designed to interpret graphs with different nodes and edges.\nThe representation of node $i$ itself is then updated using a MLP,\n\\begin{equation}\n\\begin{split}\nm_i^{k+1} = \\sum_{j \\in N(i)} x_j^k , \\\\\nx_i^{k+1} = F((1+\\epsilon)  \\cdot x_i^k+m_i^{k+1}), \n\\end{split}\n\\end{equation}\nwhere $F$ is the MLP and $\\epsilon$ is either a learnable parameter or fixed. GIN’s aggregation and readout functions are injective, and thus are designed to achieve maximum discriminative power~.", "cites": [231], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal description of GIN, focusing on its mathematical formulation and a brief mention of its injective properties. It does not effectively synthesize or contextualize the cited paper within the broader graph neural network literature, nor does it offer critical analysis or abstract insights into its implications for digital pathology."}}
{"id": "04d76a71-f83f-4b41-bcbc-4e67b8ff20f9", "title": "Other GNN architectures in histopathology", "level": "subsubsection", "subsections": [], "parent_id": "773fcdc6-0414-4c43-9a26-ec3ad24224ca", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Graph representation learning \\\\ in digital pathology: Background"], ["subsection", "Graph neural networks models"], ["subsubsection", "Other GNN architectures in histopathology"]], "content": "Other GNN architectures considered for entity-graph evaluation in digital pathology that were proposed by the surveyed works include:\n\\begin{itemize}\n    \\item \\textit{Edge graph neural network (EGNN)}~:\n    Edge features are included when leveraging the graph structure in the network. \n    \\item \\textit{Robust spatial filtering (RSF)}~:  \n    These spatial-based models are more flexible when dealing with heterogenous graphs as the graph inputs can be easily incorporated into the aggregation function.\n    \\item \\textit{Adaptive GraphSAGE}~:\n    Graph networks with the ability to more effectively learn the embedding feature between nodes, by using a learnable pattern to adaptively aggregate multi-level embedding features for each node.\n    3\n    \\item \\textit{Jumping Knowledge Network (JK-Net)}\n    Xu et al.~ proposed the Jumping Knowledge (JK) approach to adaptively leverage, for each node, different neighborhood ranges to better represent feature.\n    \\item \\textit{Feature-enhanced spatial-GCN (FENet)}~:\n    This model is proposed to analyse non-isomorphic graphs, distinct from isomorphic graphs which strictly share the same adjacency neighborhood matrix. The feature-enhance mechanism adaptively selects the node representation from different graph convolution layers. The model adopts sum-pooling to capture the full structural information of the entire graph representation.\n    \\item \\textit{Multi-scale graph wavelet neural network (MS-GWNN)} ~:\n    This spectral model leverages the localization property of graph wavelets to perform multi-scale analysis with a variety of scaling parameters in parallel, offering high efficiency and good interpretability for graph convolution.\n\\end{itemize}\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=1\\linewidth]{images/Fig2-2.png}\n\\caption{\nRepresentation of graph models for graph-level classification. Recreated from{~}.\n}\n\\label{fig:Fig2-2}\n\\vspace{-8pt}\n\\end{figure}\n\\vspace{-6pt}", "cites": [6941, 6951, 231, 246, 6940, 6939, 4005, 553], "cite_extract_rate": 0.6153846153846154, "origin_cites_number": 13, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic listing of various GNN architectures used in histopathology, with minimal synthesis of ideas across papers. While it briefly explains each model's mechanism, it lacks deeper integration, comparison, or evaluation of their relative strengths and weaknesses. The analysis remains superficial, without abstracting broader trends or offering a critical perspective on their efficacy or limitations."}}
{"id": "6aa515ff-8dca-4383-b0d9-dc0bce9d84df", "title": "Global pooling", "level": "paragraph", "subsections": ["91034012-b7fc-4dc9-87bf-bb18f75a0e59"], "parent_id": "10f6e9a7-6d01-437b-8683-eef537fbe22e", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Graph representation learning \\\\ in digital pathology: Background"], ["subsection", "Graph pooling"], ["paragraph", "Global pooling"]], "content": "The most fundamental type of signal pooling on a graph is global pooling. It is also referred to as a readout layer in the literature. Similar to CNNs, mean, max, and sum functions are often utilized as basic pooling methods.\nOther approaches, instead of employing these simple aggregators, transform the vertex representation to a permutation invariant graph-level representation or embedding. In particular, Li et al.{~} proposed a \\textit{global attention pooling} system that uses a soft attention mechanism to determine which nodes are relevant to the present graph-level task and returns the pooled feature vector from all nodes.", "cites": [211], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section provides a basic description of global pooling in graph representation learning, mentioning common aggregation methods like mean, max, and sum, and briefly introducing global attention pooling via a single reference. However, it lacks synthesis of broader ideas, critical evaluation of the methods, and deeper abstraction to highlight overarching principles or trends in the field."}}
{"id": "91034012-b7fc-4dc9-87bf-bb18f75a0e59", "title": "Hierarchical pooling", "level": "paragraph", "subsections": [], "parent_id": "6aa515ff-8dca-4383-b0d9-dc0bce9d84df", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Graph representation learning \\\\ in digital pathology: Background"], ["subsection", "Graph pooling"], ["paragraph", "Global pooling"], ["paragraph", "Hierarchical pooling"]], "content": "A graph pooling layer in the GCN pools information from multiple vertices to one vertex, to reduce graph size and expand the receptive field of the graph filters. Many graph classification methods use hierarchical pooling in conjunction with a final global pooling or readout layer to represent the graph as illustrated in Fig.{~\\ref{fig:Fig2-2}}\nBelow we outline the most common hierarchical pooling techniques used in digital pathology. \n\\begin{itemize}\n    \\item \\textit{DiffPool:} Ying et al.{~} introduced the differentiable graph pooling operator (DiffPool) which uses another graph convolution layer to generate the assignment matrix for each node (\\textit{i.e.} DiffPool does not simply cluster the nodes in a graph, but learns a cluster assignment matrix).\n    \\item \\textit{SAGPool} The self-attention graph pooling (SAGPool) introduced by Lee et al.{~} is a hierarchical pooling method that performs local pooling operations over node embeddings in a graph. The pooling module considers both node features and graph topology and learns to pool features via a self-attention mechanism, which can reduce computational complexity.\n\\end{itemize}\n\\vspace{-6pt}", "cites": [224, 254], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides a basic description of two hierarchical pooling methods, DiffPool and SAGPool, and their relevance to digital pathology. While it integrates the cited papers by highlighting their role in reducing graph size and expanding receptive fields, the synthesis remains surface-level and lacks deeper connections or a unifying narrative. There is minimal critical analysis or abstraction beyond the specific methods described."}}
{"id": "d61bb7b2-2341-4db9-92ea-4aabd5b1f2c4", "title": "Attention mechanisms", "level": "subsubsection", "subsections": [], "parent_id": "b0482895-606f-4df3-8a91-6704a57c5554", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Graph representation learning \\\\ in digital pathology: Background"], ["subsection", "Graph interpretations"], ["subsubsection", "Attention mechanisms"]], "content": "Graph-structured data can be both massive and noisy, and not all portions of the graph are equally important. As such, attention mechanisms can direct a network to focus on the most relevant parts of the input, suppressing uninformative features, reducing computational cost and enhancing accuracy. \nA gate-based attention mechanism{~} controls, for example, the expressiveness of each feature.\nAttention has also been used as an explanation technique where the attention weights highlight the nodes and edges in their relative order of importance, and can be used for discovering the underlying dependencies that have been learnt.\nThe activation map and gradient sensitivity of GAT models are used to interpret the salient input features at both the group and individual levels.\nIn a graph model with attention, selected layers of the graph are connected to an attention layer, and all attention layers are jointly trained with the network. A traditional attention mechanism that can be learned by gradient-based methods{~} can be formulated as,\n\\vspace{-4pt}\n\\begin{equation}\n\\begin{split}\nu_t = \\tanh ( W h_t + b), \\\\\n\\alpha_t = \\dfrac{\\exp(u_t^Tu_w)}{\\sum_{j=1}^{n} \\exp(u_t^Tu_w)} , \\\\\ns_t = \\sum_{t} \\alpha_t h_t ,\n\\end{split}\n\\end{equation}\nwhere $h_t$ is the output of a layer; and $W$, $u_w$ and $b$ are trainable weights and bias. The importance of each element in $h_t$ is measured by estimating the similarity between $u_t$ and $h_t$, which is randomly initialized. $\\alpha_t$ is a softmax function. The scores are multiplied by the hidden states to calculate the weighted combination, $s_t$ (the attention-weighted final output).", "cites": [6952], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section introduces attention mechanisms in graph-based deep learning but primarily describes their general functionality without effectively integrating the cited paper. It mentions a traditional attention mechanism with equations but does not synthesize the gated attention concept from Paper 1 into the broader context of graph interpretation in digital pathology. There is no critical analysis or abstraction of broader principles."}}
{"id": "bad0ed20-5151-4dfe-9b0f-600b52e992a3", "title": "Graph explainers", "level": "subsubsection", "subsections": [], "parent_id": "b0482895-606f-4df3-8a91-6704a57c5554", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Graph representation learning \\\\ in digital pathology: Background"], ["subsection", "Graph interpretations"], ["subsubsection", "Graph explainers"]], "content": "Several post-hoc feature attribution graph explainers have been presented in the literature including excitation backpropagation{~}, a node pruning-based explainer (GNNExplainer){~}, gradient-based explainers (GraphGrad-CAM{~} and GraphGrad-CAM++{~}), a layerwise relevance propagation explainer (GraphLRP)~, and deep graph mapper~.\n\\begin{table*}[t]\n\\caption{Summary of applications and graphs models in computational pathology.}\n\\vspace{-5pt}\n\\centering\n\\label{table:pathology}\n\\resizebox{1\\textwidth}{!}{\n\\begin{tabular}{\nl\n>{\\raggedright\\arraybackslash}p{1.7cm}\n>{\\raggedright\\arraybackslash}p{2.2cm}\nc\n>{\\raggedright\\arraybackslash}p{3cm} \n>{\\raggedright\\arraybackslash}p{10cm}}\n\\toprule\n\\textbf{Authors} &\n\\textbf{Topic} &\n\\textbf{Application} & \n\\textbf{Entity-graph} & \n\\textbf{GNN Model + Explainer} &  \n\\textbf{Input; Training (Node detection/embeddings); Training (GNN model/pathology task); Datasets; Additional remarks} \\\\\n\\midrule\nJaume et al. (2021)~ & Classification & Breast cancer & CG &\nGIN + Post-hoc explainers & WSI; Supervised; Supervised; BRACS~ (5 classes); Post-hoc explainers: GNNExplainer, GraphGrad-CAM, GraphGrad-CAM++, GraphLRP. \\\\ \nJaume et al. (2020)~ & Classification & Breast cancer & CG &\nGIN + CGExplainer & WSI; Supervised; Supervised; BRACS~ (5 classes); Customized cell-graph explainer based on GNNExplainer. \\\\ \nSureka et al. (2020)~ & Classification & Breast cancer / Prostate cancer & CG &\nGCN, RSF + Attention/Node occlusion & WSI, TMAs; Supervised; Supervised; Breast cancer: BACH~ (2 classes), Prostate cancer: TM~ (2 classes); Gleason grade. \\\\\nAnand et al. (2020)~ & Classification & Breast cancer & CG &\nGCN, RSF  & WSI; Supervised; Supervised; BACH~ (4 classes). \\newline \\\\\nStuder et al. (2021)~ & Classification & Colorectal cancer & CG &\nGCN, GraphSAGE, GAT, GIN, ENN, JK-Net & WSI; Supervised; Supervised; pT1-Gland Graph~ (2 classes); Graph-level output. Concatenation of global add, mean and max pooling). Dysplasia of intestinal glands. \\\\ \nZhou et al. (2019)~ & Classification & Colorectal cancer & CG &\nAdaptive GraphSAGE, JK-Net, Graph clustering & WSI; Supervised; Supervised; CRC dataset~ (3 classes); Graph-level output. Hierarchical representation of cells based on graph clustering method from DiffPool). \\\\ \nWang et al. (2020)~ & Classification &  Prostate cancer & CG &\nGraphSAGE, SAGPool & TMA; Self-supervised; Weakly-supervised; UZH prostate TMAs~ (2 classes); Graph-level output. Grade classification (low and high-risk). \\\\\n\\midrule\nOzen et al. (2020)~ & ROI Retrieval &  Breast cancer & PG &\nGCN, DiffPool & WSI; Supervised; Self-Supervised; Department of Pathology at Hacettepe University (private) (4 classes); Histopathological image retrieval (slide-level and ROI-level). \\\\\nLu et al. (2020)~ & Classification & Breast cancer (HER2, PR) & TG &\nGIN & WSI; Supervised; Supervised; TCGA-BRCA~ (2 classes); Graph-level. Status of Human epidermal growth factor receptor 2 (HER2) and Progesterone receptor (PR). \\\\\nAyg{\\\"u}ne{\\c{s}} et al. (2020)~ & Classification &  Breast cancer & PG &\nGCN & WSI; Supervised; Weakly-supervised; Department of Pathology at Hacettepe University (private) (4 classes). ROI-level classification. \\\\\nYe et al. (2019)~ & Classification & Breast cancer & PG &\nGCN & WSI; Supervised; Supervised; BACH~ (4 classes); Graph construction based on the ROI segmentation map. \\\\\nZhao et al. (2020)~ & Classification & Colorectal cancer & PG &\nChebNet, SAGPool & WSI; Self-Supervised; Weakly-supervised;  TCGA-COAD~ (2 classes); Multiple instance learning. Graph-level output. \\\\\nRaju et al. (2020)~ & Classification & Colorectal cancer & TG &\nAdaptive GraphSage + Attention & WSI; Self-Supervised; Weakly-supervised; MCO~ (4 classes); Multiple instance learning. Cluster embedding (Siamese architecture); Tumor node metastasis staging. \\\\\nDing et al. (2020)~ & Classification &  Colorectal cancer & PG &\nSpatial-GCN (FENet) & WSI; Supervised; Supervised; TCGA-COAD and TCGA-READ~ (2 classes); Genetic mutational prediction. \\\\\nAdnan et al. (2020)~ & Classification & Lung cancer & PG &\nChebNet, GraphSAGE + Global attention pooling & WSI; Supervised; Supervised; TCGA-LUSC~ (2 classes),  MUSK1~; Adjacency learning layer. Multiple instance learning. \\\\\nZheng et al. (2019)~ & Retrieval & Lung cancer & PG &\nGNN, DiffPool (GNN-Hash) & WSI; Supervised; Similarity (Hamming distance); ACDC-LungHP~; Hashing methods and binary encoding. Histopathological image retrieval.  \\\\\nLi et al. (2018)~ & Classification & Lung cancer & PG &\nChebNet + Attention & WSI; Self-Supervised; Supervised; TCGA-LUSC~ (2 classes), NLST~ (2 classes); Survival prediction. \\\\\nWu et al. (2019)~ & Classification & Skin cancer & PG &\nGCN & WSI; Supervised; Weakly- and Semi-supervised; BCC data collected from 2 different hospitals (private) (4 classes). \\\\\nAnklin et al. (2021)~ & Segmentation / Classification & Prostate cancer & TG &\nGIN (SegGini) + GraphGrad-CAM & TMA, WSI; Supervised; Weakly-supervised; UZH prostate TMAs~ (4 classes), SICAPv2~ (4 classes); Gleason grade, Post-hoc interpretability. \\\\ \n\\midrule\nPati et al. (2021)~  & Classification & Breast cancer & CG, TG, HR &\nGIN-PNA (HACT-Net) + GraphGrad-CAM & WSI; Supervised; Supervised; BRACS~ (7 classes), BACH~ (4 classes); Cell-to-Tissue Hierarchies. \\\\\nPati et al. (2020)~ & Classification & Breast cancer & CG, TG, HR &\nGIN (HACT-Net) & WSI; Supervised; Supervised; BRACS~ (5 classes); Cell-to-Tissue Hierarchies. \\newline\\\\ \nZhang and Li (2020)~ & Classification & Breast cancer & PG, HR &\nMS-GWNN & WSI; Supervised; Supervised; BACH~ (4 classes), BreakHis~ (2 classes); Multi-scale graph feature learning (node-level and graph-level prediction). \\\\\nLevy et al. (2021)~ & Regression & Colorectal cancer / lymphoma & PG, HR &\nGAT, TDA + Graph Mapper & WSI; Supervised; Supervised; Dartmouth Hitchcock Medical Center (private): colon (9 classes), lymph (4 classes); Hierarchical representation. Tumor invasion score and staging. \\\\\n\\midrule\nShi et al. (2020)~  & Classification & Cervical cancer & CCG &\nFusion CNN-GCN & RGB; Supervised; Semi-supervised; SIPaKMed~ (5 classes), Motic~ (7 classes); Population analyis of isolated cell images. \\\\\nShi et al. (2019)~  & Classification & Cervical cancer & CCG &\nFusion CNN-GCN & RGB; Supervised; Supervised; SIPaKMed~ (5 classes), Motic~ (7 classes); Population analyis of isolated cell images. \\\\\nChen et al. (2020)~ & Classification & Renal Cancer & CG &\nGraphSAGE, SAGPool + Attention & Fusion: WSI+Genome; Self-Supervised; Self-Supervised; TCGA-GBMLGG, TCGA-KIRC~; Survival outcome, Integrated gradient method.  \\\\\n\\bottomrule\n\\multicolumn{6}{p{500pt}}\n{ \nGraph representation: Cell-Graph (CG); Patch-Graph (PG); Tissue-Graph (TG); Hierarchical Representation (HR); Cluster-Centroids-Graph (CCG) \n}\n\\end{tabular}}\n\\vspace{-4pt}\n\\end{table*}\n\\vspace{-3pt}", "cites": [6943, 569, 6945, 6936, 8708, 6942, 6939, 6954, 6935, 6953, 6941, 6944, 6940], "cite_extract_rate": 0.2708333333333333, "origin_cites_number": 48, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.8}, "insight_level": "low", "analysis": "The section primarily lists various graph explainers and their applications in digital pathology without synthesizing or connecting the underlying methodologies or findings across papers. There is minimal critical analysis or evaluation of the strengths and limitations of the explainers. The content remains focused on concrete details rather than offering higher-level insights or patterns."}}
{"id": "1a4e6a83-5ba4-4bcc-9fff-4b074e4f5651", "title": "Breast cancer", "level": "subsubsection", "subsections": [], "parent_id": "622ca37f-363b-4e07-84a5-63ab6cf1d0ef", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Applications of graph deep learning \\\\ in digital pathology"], ["subsection", "Cell-graph representation"], ["subsubsection", "Breast cancer"]], "content": "Breast cancer is the most commonly diagnosed cancer and registers the highest number of cancer deaths among women. A majority of breast lesions are diagnosed along a spectrum of cancer classes that ranges from benign to invasive.\nCancer diagnosis and the detection of breast cancer is one of the most common applications of machine learning and computer vision within digital pathology analysis. CNNs have been used for various digital pathology tasks in breast cancer diagnosis such as nucleus segmentation and classification, and tumor detection and staging. However, these patch-wise approaches do not explicitly capture the inter-nuclear relationships and limit access to global information.\nAnand et al.~ proposed the use of GCNs to classify WSIs represented by graphs of their constituent cells. \nMicro-level features (nuclear morphology) were incorporated as vertex features using local image descriptors, while macro-level features (gland formation) were included as edge attributes based on a mapping of Euclidean distances between nearby nuclei.\nThe vertex features are represented by the average RGB intensity, morphological features and learned features extracted from a pre-trained CNN applied to a window around the nuclei centroid. Finally, each tissue image is classified by giving its cell-graph as an input to the GCN which is trained in a supervised manner. The authors adopted a spatial GCN known as robust spatial filtering (RSF)~, which can take heterogeneous graphs as input. This framework is depicted in Fig.~\\ref{fig:Fig3}.\nThe authors demonstrate competitive performance compared to conventional patch-based CNN approaches to classify patients into cancerous or non-cancerous groups using the Breast Cancer Histology Challenge (BACH) dataset~.\nSureka et al.~ modeled histology tissue as a graph of nuclei and employed the RSF with a GCN~ with attention mechanisms and node occlusion to highlight the relative cell contributions in the image, which fits the mental model used by pathologists. \nIn the first approach, the authors occluded nuclei clusters to assess the drop in the probability of the correct class, while also including a method based on~ to learn enhanced vertex and edge features. \nIn a second approach, an attention layer is introduced before the first pooling operation for visualization of important nuclei for the binary classification of breast cancer on the BACH dataset and Gleason grade classification on a prostate cancer~ dataset. \n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=1\\linewidth]{images/Fig4-new.png}\n\\caption{\nFor a ductal carcinoma, examples of explanations given by graph-based (Left) and pixel-based (Right) explainability algorithms. Recreated from~.\n}\n\\label{fig:Fig4}\n\\vspace{-10pt}\n\\end{figure}\nSeveral explainers have been applied in digital pathology, inspired by explainability techniques for CNN model predictions on images. However, pixel-level explanations fail to encode tumor macro-environment information, and result in ill-defined visual heatmaps of important locations as illustrated in Fig.~\\ref{fig:Fig4}. Thus, graph representations are relevant for both diagnostics and interpretation. Generating intuitive explanations for pathologists is critical to quantify the quality of the explanation.\nTo address this, Jaume et al.~ introduced a framework using entity-based graph analysis to provide pathologically-understandable concepts (\\textit{i.e.} to make the graph decisions understandable to pathologists).\nThe authors proposed a set of quantitative metrics based on pathologically measurable cellular properties to characterize explainability techniques in cell-graph representations for breast cancer sub-typing. \nIn~, the authors first transform the histology image into a cell-graph, and a GIN model is used to map the corresponding class level. Then, a post-hoc graph explainer generates an explanation per entity graph. Finally, the proposed metrics are used to assess explanation quality in identifying the nuclei driving the prediction (nuclei importance maps). \nFour graph explainers were considered in this analysis: \nGNNExplainer~, GraphGrad-CAM~,  GraphGrad-CAM++~, and GraphLRP~.\nThe results on the Breast Carcinoma Subtyping (BRACS) dataset~ confirm that GraphGrad-CAM++ produces the best overall agreement with pathologists. The proposed metrics, which include domain-specific user-understandable terminology, could be useful for quantitative evaluation of graph explainability.\nJaume et al.~ focused on the analysis of cells and cellular interactions in breast cancer sub-typing classification, and introduced an instance-level post-hoc graph-pruning explainer to identify decisive cells and interactions from the input graph in the BRACS dataset~.\nTo create the cell-graph, nuclei are detected with segmentation algorithms and hand-crafted features including shape, texture and color attributes are extracted to represent each nucleus. The cell-graph topology uses the KNN algorithm and is based on the assumption that that spatially close cells encode biological relationships and, as a result, should create an edge. \nThe cell-graph is processed by a GIN model, followed by a MLP to predict the cancer stages.\nJaume et al.~ designed a cell-graph explainer (CGExplainer), based on the GNNExplainer, to remove redundant and uninformative graph components, and the resulting sub-graph will be responsible for class-specific patterns that will aid disease comprehension. \nThis module aims to learn a mask at the node-level that activates or deactivates parts of the graph. Fig.~\\ref{fig:Fig5} provides an overview of the explainer module. The proposed explainer was shown to prune a substantial percentage of nodes and edges to extract valuable information while retaining prediction accuracy (\\textit{e.g.} the explanations retain relevant tumor epithelial nuclei for cancer diagnosis).\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{images/Fig5-new.png}\n\\caption{\nCell-graph explainer (CGExplainer): a customized post-hoc graph explainer based on graph pruning optimization. Recreated from~.\n}\n\\label{fig:Fig5}\n\\vspace{-10pt}\n\\end{figure}", "cites": [6944, 6942, 246, 569, 6939, 9084, 6935, 6953], "cite_extract_rate": 0.6153846153846154, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple works on graph-based methods for breast cancer analysis, connecting them into a coherent narrative on cell-graph representation and explainability. It critically evaluates the limitations of CNN-based and pixel-level explainers, while comparing the performance of graph-based explainers such as GraphGrad-CAM++. The discussion abstracts to broader themes like the importance of biological entity-based explanations and their alignment with pathologists' reasoning, demonstrating strong analytical depth."}}
{"id": "f2db0578-688d-405d-8d0e-25ef66270619", "title": "Colorectal cancer", "level": "subsubsection", "subsections": [], "parent_id": "622ca37f-363b-4e07-84a5-63ab6cf1d0ef", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Applications of graph deep learning \\\\ in digital pathology"], ["subsection", "Cell-graph representation"], ["subsubsection", "Colorectal cancer"]], "content": "Colorectal cancer (CRC) grading is a critical task since it plays a key role in determining the appropriate follow-up treatment, and is also indicative of overall patient outcome. The grade of a cancer is determined, for example, by assessing the degree of glandular formation in the tumour. Nevertheless, automatic CNN-based methods for grading CRC typically use image patches which fail to include information on the micro-architecture of the entire tissue sample, and do not capture correspondence between the tissue morphology and glandular structure. \nTo model nuclear features along with their cellular interactions, Zhou et al.~ proposed a cell-graph model for grading CRC, in which each node is represented by a nucleus within the original image, and cellular interactions are captured as graph edges based on node similarity.\nA nuclear instance segmentation model is used to detect the nucleus and to extract accurate node features including nucleus shape and appearance features. Spatial features such as centroid coordinates, nuclei intensity and dissimilarity extracted from the grey level co-occurrence matrix were used as descriptors for predicting the grade of cancer. To reduce the number of nodes and edges based on the relative inter-node distance, an additional sampling strategy was used.\nTo conduct the graph-level classification, the authors in~ proposed the Adaptive GraphSAGE model, which is inspired by GraphSAGE~ and JK-Net~, to obtain multi-level features (\\textit{i.e.} capturing the gland structure at various scales). \nTo achieve multi-scale feature fusion, Adaptive GraphSAGE employs an attention technique which allows the network to adaptively generate an effective node representation. \nA graph clustering operation, which can be considered as an extension of DiffPool{~}, is used to group cells according to their appearance and tissue type, and to extract more abstract features for hierarchical representation. However, since the tissue hierarchy is inaccessible via this approach, the representation does not include high-level tissue features.\nBased on the degree of gland differentiation, the graph model categorises each image as normal, low-grade, or high-grade. In comparison with a traditional CNN, the proposed model achieves better accuracy by incorporating both nuclear and graph-level features.\n\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=1\\linewidth]{images/Fig6-new.png}\n\\vspace{-10pt}\n\\caption{\nThe nuclei that have been detected are segmented, and a graph is constructed using the centroid of each nuclei. For each node, morphological, texture and contrastive predictive coding features are extracted, and GCNs are used as the graph representation. Recreated from~.\n}\n\\label{fig:Fig6}\n\\vspace{-8pt}\n\\end{figure*}\nDysplasia of intestinal glands is especially important in pT1 colorectal cancer, the earliest stage of invasive colorectal cancer. \nStuder et al.~ introduced the pT1 Gland graph (pT1-GG) dataset that consists of cell-graphs of healthy and dysplastic intestinal glands. In this work, the authors established a baseline for gland classification using labelled cell-graphs and the graph edit distance (GED), which is an error-tolerant measurement of similarity between two graphs. This technique is an improved version of the bipartite graph-matching method (BP2)~ combined with a KNN algorithm to perform classification.\nLater, the same authors investigated different graph-based architectures~ to classify healthy gland tissue and dysplastic glandular areas on the pT1-GG dataset.\nThe GNN architectures evaluated for cell-graph classification are GCN~, GraphSAGE~, GAT~, GIN~, EGNN~ and a 1-dimensional GNN~. All models are trained using three graph convolution layers where GraphSAGE and GCN are also trained with jumping knowledge (JK)~ to allow for an adaptive neighborhood range by aggregating representations across different layers. A concatenation of global sum-pooling, global mean-pooling and global max-pooling is used to get the graph-level output, followed by a MLP to classify an input graph.\nThe results demonstrated that graph-based deep learning methods outperformed classical graph-based and CNN-based methods. \nIt should be emphasised, however, that each node is only linked to its two spatially closest neighbors, resulting in very restricted information sharing during message passing.", "cites": [6941, 231, 5733, 6936, 180, 242, 4005, 224], "cite_extract_rate": 0.6153846153846154, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple works on graph-based deep learning for CRC, particularly Zhou et al. and Studer et al., by connecting their methodologies and outcomes in a coherent narrative. It provides some critical analysis, such as the limited information sharing due to sparse node connections and the absence of high-level tissue features. The abstraction is moderate, as it identifies patterns like the use of multi-scale feature fusion and pooling strategies but does not fully generalize to a broader theoretical or conceptual framework."}}
{"id": "ffea6be3-eb6e-4828-a044-7a5435490a4e", "title": "Prostate cancer", "level": "subsubsection", "subsections": [], "parent_id": "622ca37f-363b-4e07-84a5-63ab6cf1d0ef", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Applications of graph deep learning \\\\ in digital pathology"], ["subsection", "Cell-graph representation"], ["subsubsection", "Prostate cancer"]], "content": "The commonly used Gleason score, which is based on the architectural pattern of tumor tissues and the distribution of glands, determines the aggressiveness of prostate cancer. CNNs have been used for histology image classification including Gleason score assignment, but CNNs are unable to capture the dense spatial relationships between cells and require detailed pixel level annotations for training.\nTo analyse the spatial distribution of the glands in prostate TMAs, Wang et al.~ proposed a weakly-supervised approach for grade classification and to stratify low and high-risk cases (Gleason score $<6$ is normal tissue; Gleason score $\\geq 6$ is abnormal tissue or high-risk).\nThe authors segmented the nuclei and construct a cell-graph for each image with nuclei as the nodes, and the distance between neighboring nuclei as the edges, as illustrated in Fig.~\\ref{fig:Fig6}.\nUsing prostate TMAs with only image-level labels rather than pixel-level labels, a GCN is used to identify high-risk patients via a self-supervised technique known as contrastive predictive coding (CPC)~.\nFeatures for each node are generated by extracting morphological (area, roundness) and texture features (dissimilarity, homogeneity) as well as features from CPC-based learning.\nA GraphSAGE convolution and a self-attention graph pooling (SAGPool)~ are applied to the graph representation to learn from the global distribution of cell nuclei, cell morphology and spatial features. The proposed method can calculate attention scores, focus on the more significant node attributes, and aggregate information at different levels.\n\\vspace{-6pt}", "cites": [6936, 134, 254], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the key components of the weakly-supervised method from Wang et al., including cell-graph construction and the use of CPC and GraphSAGE/SAGPool. It connects these ideas to the broader context of prostate cancer classification, showing a moderate level of integration. While it highlights some advantages of the method (e.g., use of image-level labels and attention-based aggregation), it lacks a deeper critical evaluation of the approach or comparison with other methods. The section identifies the relevance of spatial features in Gleason scoring but stops short of abstracting broader principles of graph-based learning in histopathology."}}
{"id": "6487bd68-30b6-4a63-b364-d724bdfbee76", "title": "Breast cancer", "level": "subsubsection", "subsections": [], "parent_id": "c4273fb9-7544-4380-8214-44083e361bfa", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Applications of graph deep learning \\\\ in digital pathology"], ["subsection", "Patch-graphs and Tissue-graphs representations"], ["subsubsection", "Breast cancer"]], "content": "Multi-class classification of arbitrarily sized ROIs is an important problem that serves as a necessary step in the diagnostic process for breast cancer.\nAyg{\\\"u}ne{\\c{s}} et al.~ proposed to incorporate local context through a graph-based ROI representation over a variable number of patches (nodes) and their spatial proximity relationships (edges).\nA CNN is used to extract a feature vector for each node represented by fixed-sized patches of the ROI.\nThen, to propagate information across patches and incorporate local contextual information, two consecutive GCNs are used, which also aggregate the patch representation to classify the whole ROI into a diagnostic class. \nThe classification is conducted in a weakly-supervised manner over the patches and ROI-level annotations, without having access to patch-level labels.\nResults on a private data collected from the Department of Pathology at Hacettepe University outperformed CNN-based models that incorporated majority-voting, learned-fusion and base-penultimate methods.\nSome traditional CNN-based models have proposed to jointly segment a ROI of an image and classify WSIs and that enabled the classifier to better predict the image class~.\nYe et al.~ captured the topological structure of a ROI image through a GCN where a graph is constructed with segmentation masks of image patches that contain high levels of semantic information.\nThe segmentation mask for each image patch is obtained using an encoder-decoder semantic segmentation framework where each pixel is classified as one of the four classes of tissue samples (normal, benign, in situ, and invasive) of the BACH~ dataset.\nThe combined segmentation masks of the image patches yield the total ROI segmentation mask. The area ratio of each lesion is calculated as the value of the unit node in each picture patch. Then, a graph is constructed to capture the spatial dependencies using the features of the image patch segmentation masks. Finally, the ROI image is classified based on the features learned by the GCNs. \nOne limitation of previous works is that they construct graphs using small patches of the WSI. Lu et al.~ overcome this challenge by introducing a pipeline to construct a graph from the entire WSI using the nuclei level information, including geometry and cellular organization in tissue slides (termed the histology landscape). \nAfter building the graph, the authors used a GIN model to predict the positive or negative human epidermal growth factor receptor 2 (HER2), and the progesterone receptor (PR), which are two valuable biomarkers for breast cancer prognosis.\nThe proposed method in~ consists of four steps as illustrated in Fig.~\\ref{fig:Fig8}. \nThis work first used Hover-Net~ to simultaneously segment and classify the individual nuclei and extract their features. Then, agglomerative clustering~ is used to group spatially neighboring nuclei into clusters which results in reduced computational cost for downstream analysis. Using these clusters, a graph is generated by assigning the tissue clusters to nodes and the edges of the graph encode the cellular topology of the WSI. Lastly, the graph generated from the entire WSI is used as an input to a GCN to predict HER2 or PR status at the WSI-level.\nThe performance of this method is evaluated on the hematoxylin and eosin (H\\&E) stained WSI images from the TCGA-BRCA~ dataset, which consist of 608 HER2 negative and 101 HER2 positive, and 452 PR positive and 256 PR negative samples. \nContent-based histopathological image retrieval has also been investigated for decision support in digital pathology. This system scans a pre-existing WSI database for regions that the pathologist is interested in and returns related regions to the pathologists for comparison. These methods can provide valuable information including diagnosis reports from experts for similar regions. Retrieval methods can also be used for classification by considering the most likely diagnosis~. However the amount of manually labelled training data limits their power. \nOzen et al.~ suggested a generic method that combines GNNs with a self-supervised training method that employs a contrastive loss function without requiring labeled data. \nIn this framework, fixed-size patches and their spatial proximity relations are represented by undirected graphs. \nThe simple framework for constrastive learning of visual representation (SimCLR)~ is adopted for learning representations of ROIs.\nUsing the contrastive loss, the GNN encoder and MLP projection head are trained to maximise the agreement between the representations. A GCN followed by a DiffPool operation is selected as the model configuration. \nFor content-based retrieval tasks, this GNN is trained in a self-supervised setting and is used to extract ROI representations where the Euclidean distance between the extracted representations is used to determine how similar two ROIs are. \nQuantitative results demonstrated that contrastive learning can improve the quality of learned representations, and despite not utilizing class labels could outperforming supervised classification methods.\n\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=1\\linewidth]{images/Fig9-new.png}\n\\caption{\nRepresentation of a GCN-based MIL method. Once the bag of patches are extracted, instance-level feature extraction and selection is conducted followed by a bag-level classification. Recreated from~.\n}\n\\label{fig:Fig9}\n\\vspace{-3pt}\n\\end{figure*}\n\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=0.9\\linewidth]{images/Fig10-new.png}\n\\vspace{-2pt}\n\\caption{\nThe proposed FENet architecture. For each WSI, patches are randomly selected. Each patch corresponds to a node in each non-isomorphic subgraph where a CNN is used to extract node attributes. A feature-enhanced mechanism is adopted to consider all topological structural information. An ensemble approach used majority voting to aggregate all subgraphs' prediction outcomes. Recreated from~.\n}\n\\label{fig:Fig10}\n\\vspace{-6pt}\n\\end{figure*}", "cites": [8460, 6949, 6935, 7000, 6955], "cite_extract_rate": 0.38461538461538464, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates multiple graph-based methods for breast cancer analysis, connecting techniques like GCNs, segmentation, and self-supervised learning to address ROI classification and WSI-level prediction. It critically points out limitations, such as the use of small patches, and discusses alternative strategies like using nuclei-level features and contrastive learning. However, while it identifies some patterns (e.g., the value of spatial dependencies), it stops short of offering a deeper, meta-level abstraction of the field."}}
{"id": "b197c302-3a1a-441b-877e-b85e29b84410", "title": "Colorectal cancer", "level": "subsubsection", "subsections": [], "parent_id": "c4273fb9-7544-4380-8214-44083e361bfa", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Applications of graph deep learning \\\\ in digital pathology"], ["subsection", "Patch-graphs and Tissue-graphs representations"], ["subsubsection", "Colorectal cancer"]], "content": "Although CNN-based approaches have practical merits when identifying important patches for predicting CRC, they do not take into account the spatial relationships between patches, which is important for determining the stage of the tumor. The size and the relative location of the tumor in relation to other tissue partitions are used for tumor node metastasis staging estimation. Furthermore, traditional approaches require the presence of expert pathologists to annotate each WSI.\nWeakly-supervised learning is an important and potentially viable solution to dealing with sparse annotations in medical imagery. Multiple instance learning (MIL) is well-suited to histology slide classification, as it is designed to operate on weakly-labeled data~.\nRaju et al.~ considered the spatial relationship between tumor and other tissue partitions with a graph attention multi-instance learning framework to predict colorectal tumor node metastasis staging. Each graph with nodes representing different tissues serves as an instance, and the multiple instances for a WSI form a bag that aids in tumour stage prediction.\nIn~, given a WSI, a texture autoencoder~ is used to encode the texture from random sample patches. Then a cluster embedding network based on a Siamese architecture~ is trained on a binary classification task to group similar texture features into multiple graphs.\nEach WSI is divided into multiple graphs and each graph has features from all cluster labels.\nThe authors used a tissue wise annotated CRC dataset~ to assign cluster labels for similar image patches.\nThe authors consider the multiple graphs as multiple instances in a bag which are used to predict the tumor staging using an attention MIL method~. The authors adopted an Adaptive GraphSage~ approach with learnable attention weights to assign more importance to instances which contain more information towards predicting the tumor stage.\nThe authors demonstrated that graph attention multi-instance learning can perform better than a GCN on the Molecular and Cellular Oncology (MCO)~ dataset.\nColorectal cancer lymph node metastasis (LNM) is a crucial factor in patient management and prognosis, and its identification suggests the need for dissection to avoid further spread.\nZhao et al.~ introduced a GCN-based multiple instance learning method combined with a feature selection strategy to predict LNM in the colon adenocarcinoma (COAD) cohort of the Cancer Genome Atlas (TCGA) project~.\nFollowing the MIL approach, the training dataset is composed of bags where each bag contains a set of instances. The goal of this work is to teach a model to predict the bag label, where only the bag-level label is available.\nThe overall framework has three major components: instance-level feature extraction, instance-level feature selection, and bag-level classification, as illustrated in Fig.~\\ref{fig:Fig9}. \nFirst, non-overlapping patches are extracted from a WSI which is represented as a bag of patches. Since instance labels are unavailable, the authors introduced a combination of a variational autoencoder (VAE)~ and a generative adversarial network (GAN) for fine-tunning the encoder component as an instance-level feature extractor in a self-supervised manner. In this VAE-GAN model, the architecture of the network for the decoder of the VAE and generator of the GAN is the same network. \\\nThen, a feature selection component is incorporated to remove redundant and unhelpful features to alleviate the workload when generating the bag representation. The maximum mean discrepancy is used to evaluate the feature importance. Finally, the authors employed ChebNet~ followed by SAGPool~ to generate the bag representation and perform the bag-level classification.\nThe authors demonstrated that the proposed model outperformed CNN-based and attention-based MIL models.\nColon adenoma and carcinoma may occur as a result of a series of histopathological changes due to key genetic alterations. Thus, the ability to predict genetic mutations is important for the diagnosis of colon cancer.\nDing et al.~ proposed a feature-enhanced graph network (FENet) using a spatial-GCNs, based on GIN, to predict gene mutations across all three key mutational prediction tasks (APC, KRAS, and TP53) that are associated with colon cancer evolution. In this approach, multiple spatial graphs are created using randomly selected image patches from each patient's WSI. \nThe feature-enhanced mechanism aggregates features from neighboring patches and combines them as the central node representation to increase feature learning performance. \nThe authors introduced GlobalAddPooling as a READOUT function to convert the node representation into a graph representation. The prediction outcome for each sub-graph is classified by fully-connected layers.\nFinally, an ensemble strategy combines the prediction results of all sub-graphs to predict mutated and non-mutated classes. \nFig.~\\ref{fig:Fig10} illustrates the proposed FENet networks.\nThe authors demonstrated that the integration of multiple sub-graph outcomes in the proposed model leads to a significant improvement in prediction performance on the Cancer Genome Atlas Colon Adenocarcinoma dataset~, outperforming graph-based baseline models such as ChebNet, GraphSAGE and GAT.\n\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=0.9\\linewidth]{images/Fig12-new.png}\n\\caption{\nRepresentation of the retrieval framework. Patch-graphs are constructed based on spatial relationships and feature distances between patches, and are fed into the developed GNN-Hash model for graph encoding. \nWhen retrieving, the query region is converted into a patch-graph and a binary code for similarity comparison with samples in the database. Recreated from~.\n}\n\\label{fig:Fig12}\n\\vspace{-6pt}\n\\end{figure*}", "cites": [6941, 254, 6950, 6937, 6956, 6957, 5680, 8313], "cite_extract_rate": 0.5, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to present a coherent narrative on graph-based deep learning for colorectal cancer, particularly in tumor staging and mutation prediction. It critically evaluates the performance of different approaches (e.g., graph attention MIL vs. GCN) and highlights advantages such as better handling of spatial relationships and weak supervision. While there is some abstraction in terms of identifying the broader use of MIL and graph pooling, the section remains grounded in specific methods rather than offering meta-level insights."}}
{"id": "e1d897fa-bb2b-4568-a21d-f416df3c0e79", "title": "Lung cancer", "level": "subsubsection", "subsections": [], "parent_id": "c4273fb9-7544-4380-8214-44083e361bfa", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Applications of graph deep learning \\\\ in digital pathology"], ["subsection", "Patch-graphs and Tissue-graphs representations"], ["subsubsection", "Lung cancer"]], "content": "Lung adenocarcinoma and lung squamous cell carcinoma are the most common subtypes of lung cancer, and distinguishing between them requires a visual examination by an experienced pathologist.\nEfficient mining of survival-related structural features on a WSI is a promising way to improve survival analysis. \nLi et al.~ introduced a GCN-based survival prediction model that integrated local patch features with global topological structures (patch-graph) through spectral graph convolution operators (ChebNet) using the TCGA-LUSC~ and NLST~ datasets.\nThe model utilized a survival-specific graph trained under supervision using survival labels.\nA parallel graph attention mechanism is used to learn attention node features to improve model robustness by reducing the randomness of patch sampling (\\textit{i.e.} an adaptive patch selection by learning the importance of individual patches). \nThis attention network is trained jointly with the prediction network. The authors demonstrated that topological features fine-tuned with survival-specific labels outperformed CNN-based models. \nAdnan et al.~ explored the application of GNNs for MIL. The authors sampled important patches from a WSI and model them as a fully-connected graph where the graph is converted to a vector representation for classification. Each instance is treated as a node of the graph in order to learn end-to-end relationships between nodes.\nIn this approach, a DenseNet is used to extract features from all important patches sampled from a segmented tissue using color thresholds~. \nThen, an adjacency learning layer which uses global information about the patches is adopted to define the connections within nodes in an end-to-end manner. The adjacency matrix is calculated by an adjacency learning block using a series of dense layers and cross-correlation.\nThe constructed graph is passed through two types of graph models (ChebNet and GraphSAGE), followed by a graph pooling layer to get a single feature vector to compare the discrimination of sub-types of lung cancer on the TCGA{~} and MUSK1{~} datasets. \nWith the adopted global attention pooling{~} which uses a soft attention mechanism, it is possible to visualise the importance that the network places on each patch when making the prediction.\nThe pooled representation is fed to two fully connected dense layers to achieve the final classification between lung adenocarcinoma and lung squamous cell carcinoma. The proposed model outperformed CNN-based models that use attention-MIL.\n\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=0.9\\linewidth]{images/Fig13-new.png}\n\\caption{\nRepresentation of the proposed SegGini methodology.\na) Tissue graph construction with tissue superpixels as nodes, and edges computed using a region adjacency graph from the spatial connectivity of superpixels. A GNN is used to learn discriminative\nnode embeddings to perform semantic segmentation.\nb) Graph-head: graph classification and feature atribution based on GraphGrad-CAM.\nc) Node-head: node classification.\nRecreated from~.\n}\n\\label{fig:Fig13}\n\\vspace{-6pt}\n\\end{figure*}\nAs discussed previously, content-based image retrieval seeks to find images that have morphological characteristics that are most similar to a query image.\nBinary encoding and hashing techniques have been successfully adopted to speed up the retrieval process in order to satisfy efficiency requirements~. However, WSI are commonly divided into small patches to index WSIs for region-level retrieval. This process does not consider the contextual information from a broad region surrounding the nuclei and the adjacency relationships that exist for different types of biopsy.\nZheng et al.~ proposed a retrieval framework for a large-scale WSI database based on GNNs and hashing, which is illustrated in Fig.~\\ref{fig:Fig12}.\nPatch-graphs are first built in an offline stage based on patch spatial adjacency, and feature similarity extracted with a pre-trained CNN. Then, the patch-graphs are processed by a GNN-Hash model designed to use a graph encoding, and stored in the retrieval database. The GNN-Hash structure was created by stacking GNN modules and a DiffPool module~. The output of the hierarchical GNN-Hash is modified with a binary encoding layer in the final graph embedding layer. Finally, the relevant regions are retrieved and returned to pathologists after the region the pathologist queries is converted to a binary code. The similarities between the query code and those in the database are measured using Hamming distance.\nExperiments to estimate the adjacency relationships between local regions in WSIs and the similarities with query regions were conducted using the lung cancer ACDC-LungHP~ dataset.\nThe results demonstrated that the proposed retrieval model is scalable to different query region sizes and shapes, and returns tissue samples with similar content and structure.", "cites": [6943, 6948, 6945, 224, 211], "cite_extract_rate": 0.45454545454545453, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple graph-based deep learning approaches for lung cancer analysis, connecting the use of patch-graphs, GNNs, and attention mechanisms across the cited papers. It critically compares the proposed models with CNN-based alternatives and highlights the advantages of incorporating topological and global contextual information. While it provides some abstraction by discussing the broader implications of graph-based methods for survival prediction and retrieval, the insights remain somewhat task-specific and do not fully generalize to a meta-level understanding of the field."}}
{"id": "fb8c0443-99ea-4928-9bef-1419db88edfc", "title": "Prostate cancer", "level": "subsubsection", "subsections": [], "parent_id": "c4273fb9-7544-4380-8214-44083e361bfa", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Applications of graph deep learning \\\\ in digital pathology"], ["subsection", "Patch-graphs and Tissue-graphs representations"], ["subsubsection", "Prostate cancer"]], "content": "Pathologists must go above and beyond normal clinical demands and norms when precisely annotating image data. As a result, a semantic segmentation method should be able to learn from inexact, coarse, and image-level annotations without complex task-specific post-processing steps.\nTo this end, Anklin et al.~ proposed a weakly-supervised semantic segmentation method based on graphs (SegGini) that incorporates both local and global inter-tissue-region relations to perform contextualized segmentation using inexact and incomplete labels.\nThe model is evaluated on the UZH (TMAs)~ and SICAPv2 (WSI)~ prostate cancer datasets for Gleason pattern segmentation and Gleason grade classification.\nFig.~\\ref{fig:Fig13} depicts the proposed SegGini methodology.\nA tissue-graph representation for an input histology image is constructed as proposed in~, where the graph nodes depict tissue superpixels. As the rectangular patches can span multiple distinct structures, superpixels are used~. \nTo characterize the nodes, morphological and spatial features are extracted, and the graph topology is computed with a region adjacency graph (RAG)~, using the spatial connectivity of superpixels.\nGiven a tissue graph, a GIN model learns contextualized features from the tissue microenvironment and inter-tissue interactions to perform semantic segmentation, where the proposed SegGini model assigns a class label to each node.\nThe resulting node features are processed by a graph-head (image label), a node-head (node label), or both, based on the type of weak supervision.\nThe graph-head consists of a graph classification and a feature attribution technique. The authors employed GraphGrad-CAM to measure importance scores towards the classification of each class, where the node attribution maps determine the node labels.\nFurther, the authors in~ found that the node-head simplifies image segmentation into classifying nodes where the node labels are extracted by assigning the most prevalent class within each node.\nFor inexact image label and incomplete scribbles, both heads are jointly trained to improve the individual classification tasks. \nThe outcomes of the heads are used to segment Gleason patterns.\nFinally, to identify image-level Gleason grades from the segmentation map, a classification approach~ is used.\nSegGini outperforms prior models such as HistoSegNet~ in terms of per-class and average segmentation, as well as classification metrics. This model also provides comparable segmentation performance for both inexact and complete supervision; and can be applied to a variety of tissues, organs, and histology tasks.\n\\vspace{-6pt}", "cites": [6945], "cite_extract_rate": 0.125, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the methodological components of the SegGini model by integrating the graph construction, feature extraction, and weakly-supervised learning aspects. It provides some critical evaluation by comparing SegGini's performance to prior methods like HistoSegNet and noting its versatility across tissues. While it abstracts the general idea of using graph-based methods for weakly-supervised segmentation, it could further elevate abstraction by framing these insights within a broader context of graph deep learning in histopathology."}}
{"id": "3ee1aba6-cab9-40ae-8893-af6c9c7ea724", "title": "Breast cancer", "level": "subsubsection", "subsections": [], "parent_id": "636f71e7-362b-4881-bc1e-1ab1bd4357a9", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Applications of graph deep learning \\\\ in digital pathology"], ["subsection", "Hierarchical graph representation (macro and micro architectures)"], ["subsubsection", "Breast cancer"]], "content": "Early detection of cancer can significantly reduce the mortality rate of breast cancer, where it is crucial to capture multi-scale contextual features in cancerous tissue. \nCombinations of CNNs have been used to encode multi-scale information in pathology images via multi-scale feature fusion, where scale is often associated with spatial location.\nZhang and Li~ introduced a multi-scale graph wavelet neural network (MS-GWNN) that uses graph wavelets with different scaling parameters in parallel to obtain multilevel tissue structural information in a graph topology. The graph wavelet neural network (GWNN)~ replaces the graph convolution in a spectral GCN with the wavelet transform which has an excellent localization capability.\nFor breast cancer classification, the authors first transformed pathological images into graph structures where nodes are non-overlapping patches. Then, node classification is performed via a GWNN at different scales in parallel (node-level prediction). After that, multi-level node representations are incorporated to perform graph-level classification.\nThe results and the visualization of the learned node embeddings demonstrated the strong capacity of the model to encode different structural information on two public datasets: BACH~ and BreakHis~. However, this approach is limited by the manual selection of the appropriate scaling parameter.\nA hierarchy defined from the cells with learned pooling layers~ does not include high-level tissue features and approaches that concatenate cell-level and tissue-level information~ cannot leverage the hierarchy between the levels of the tissue representation.\nTo address these issues, Pati et al.~ proposed a hierarchical-cell-to-tissue (HACT) representation that utilizes both nuclei and tissue distribution properties for breast cancer subtype classification.\nThe HACT representation consists of a low-level cell-graph (CG) that captures the cellular morphology and topology; a tissue-graph (TG) at a high-level that captures the properties of the tissue sections as well as their spatial distribution; and the hierarchy between the cell-graph and the tissue-graph that captures the cells' relative distribution within the tissue.\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=1\\linewidth]{images/Fig14.png}\n\\vspace{-6pt}\n\\caption{\nRepresentation of a) CG, b) TG, and c) Hierarchical-cell-to-tissue. Image adapted from~.\n}\n\\label{fig:Fig14}\n\\vspace{-12pt}\n\\end{figure}\nFig.~\\ref{fig:Fig14} illustrates samples of the CG, the TG and the hierarchical cell-to-tissue representation. \nTo construct a CG, each node represents a cell and edges encode cellular interactions, where for each nucleus hand-crafted features such as shape, texture and spatial location are extracted. Then, a KNN algorithm is adopted to build the initial topology based on the assumption that a close cell should be connected and a distant cell should remain disconnected. The Euclidean distances between nuclei centroids in the image space are used to quantify cellular distances.\nThe TG is constructed by first identifying tissue regions (\\textit{e.g.}, epithelium, stroma, lumen, necrosis) by detecting non-overlapping homogeneous superpixels of the tissue and iteratively merging neighboring superpixels that have similar colour attributes.\nThe TG topology is generated assuming that adjacent tissue parts should be connected by constructing a region adjacency graph~ with the spatial centroids of the superpixels.\nThe HACT representation, that jointly represents the low-level (CG) and high-level (TG) relationships, is processed with a hierarchical model (HACT-Net) that employs two GIN models~. The learned cell-node embeddings are combined with the corresponding tissue-node embeddings to predict the classes.\nTo demonstrate the hierarchical-learning, the authors introduce the BRACS dataset to classify five breast cancer subtypes: normal, benign, atypical, ductal carcinoma in situ, and invasive. The authors also evaluate the generalizability to unseen data by splitting the data at the WSI-level (two images from the same slide do not belong to different splits) different from previous approaches that split at the image-level~.\nThe enriched multi-level HACT representation for classification outperformed CNN-based models and standalone cell-graph and tissue-graph models, confirming that for better structure-function mapping, the link between low-level and high-level information must be modelled at the local node level rather than at the graph level.\nLater, Pati et al.~ exploited hierarchical modeling for interpretability in digital pathology, aiming to map the tissue structure to tissue functionality. The authors adopt the hierarchical entity-graph representation of a tissue which is processed via a hierarchical GNN to learn the mapping from tissue compositions to respective tissue categories.\nIn this work, Pati et al.~ improved the HACT representation and the HACT-Net model. HACT-Net is modeled using principal neighborhood aggregation (PNA)~ layers, which use a combination of aggregators to replace the sum operation in GIN and adopt degree-scalers to amplify or dampen neighboring aggregated messages based on the degree of a node.\nGraph normalization followed by batch normalization is incorporated after each PNA layer~, which aids the network in learning discriminative topological patterns when the number of nodes within a class varies dramatically.\nTo further assess the quality of the methodology, a comparison with independent pathologists is conducted. Three board-certified pathologists were recruited to annotate the BRACS test set without having access to the respective WSIs. The results indicate that the model outperforms the domain experts in the 7-class classification task.\nThe authors employed the GraphGrad-CAM to highlight the nuclei and tissue region nodes to show what the HACT-Net focuses on while classifying the tumor regions-of-interest.\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=1\\linewidth]{images/Fig16-new.png}\n\\vspace{-10pt}\n\\caption{\nA-C. Patch-level embeddings, graph representation and classification via a GCN. A refinement phase is incorporated through estimation of uncertainty.\nD-E. The Graph Mapper summarizes high-order relationships over a WSI as a graph, where meaningful histology regions are captured.\nF-G. Tumor invasion scores are used in the prediction model to form an interpretable staging score.\nImage adapted from~.\n}\n\\label{fig:Fig16}\n\\vspace{-8pt}\n\\end{figure}\n\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=0.72\\linewidth]{images/Fig17.png}\n\\caption{\nClassification framework for cervical cell images. Features are extracted with a CNN pre-trained on a cervical cell classification task. K-means clustering is performed on these CNN features. A graph of cluster centroid correlations is built based on intrinsic similarities, and is the input to a GCN model. The encoded representations are incorporated into the CNN features for classification. Image reproduced from~.\n}\n\\label{fig:Fig17}\n\\vspace{-6pt}\n\\end{figure*}", "cites": [6941, 6951, 231, 6958, 6936, 6940, 8251, 6935], "cite_extract_rate": 0.5, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers to present a coherent narrative on hierarchical graph representations for breast cancer classification. It critically evaluates limitations of prior methods, such as manual scaling parameter selection and lack of integration between cell and tissue features, and highlights the improvements introduced by the HACT framework. The section also abstracts to discuss the importance of multi-level structure-function mapping in digital pathology, providing broader insights into the role of hierarchy in modeling histological data."}}
{"id": "86a3816d-947a-4ba7-9a5a-273100e318eb", "title": "Colorectal cancer", "level": "subsubsection", "subsections": [], "parent_id": "636f71e7-362b-4881-bc1e-1ab1bd4357a9", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Applications of graph deep learning \\\\ in digital pathology"], ["subsection", "Hierarchical graph representation (macro and micro architectures)"], ["subsubsection", "Colorectal cancer"]], "content": "Tumor staging includes both tissue and nodal stages, with higher numbers indicating a greater depth of invasion and a greater number of lymph nodes implicated in the tumor, respectively.\nLevy et al.~ introduced a framework that used varied levels of structure to learn both local and global patterns from histological images for determining the degree of tumor invasion. Fig.~\\ref{fig:Fig16} illustrates the proposed framework where the authors combined GCNs to explain the mechanisms by which tissue regions interact, and topological feature extraction methods~ to extract essential contextual information.\nPatch-level classification of colon sub-compartments was conducted via a GCN as well as a refinement of patch-level predictions, in which nodes with high uncertainty were deleted, and the remaining class labels were propagated to unlabeled patches.\nA topological data analysis (TDA) tool for graphs known as Graph Mapper~ was adopted as a post-hoc model explanation technique to elucidate the high-level topology of the WSI. \nThe mapper generates a graph in which each node represents a cluster of WSI patches and each edge represents the degree of shared patches between the clusters. This tool can offer higher level information flow descriptors in a GNN model, substantially simplifying analysis. \nWith the regions of interest (collection of patches) extracted with the mapper, the authors compute tumor invasion scores that measure the degree of overlap between the tumor and adjacent tissue region.\nFinally, cancer staging is predicted via derived invasion scores using a private colon and lymph node dataset collected from the Dartmouth Hitchcock Medical Center, where the results demonstrated the potential of topological methods in the analysis of GNN models.\n\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=0.72\\linewidth]{images/Fig18.png}\n\\caption{\nAn integrated framework for multi-modal fusion of histology and genomics features for survival outcome prediction. Image-based features using CNNs, and graph-based features using GCNs. Recreated from~.\n}\n\\label{fig:Fig18}\n\\vspace{-6pt}\n\\end{figure*}\n\\vspace{-6pt}", "cites": [6959, 6954], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates the work of Levy et al. with concepts from topological data analysis, connecting local and global modeling strategies in colorectal cancer staging. While it does not deeply critique the methods or compare them to alternatives, it does provide some analytical value by explaining how tools like Graph Mapper contribute to model interpretation and invasion scoring. The narrative moves beyond mere description by highlighting the use of hierarchical graph representations and their implications for downstream tasks."}}
{"id": "435960b9-fa38-49a7-a7fc-93979f9cda44", "title": "Multi-modal fusion (Renal cancer)", "level": "subsubsection", "subsections": [], "parent_id": "788e261b-9bd3-43b4-96be-8ae8e8bb6470", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Applications of graph deep learning \\\\ in digital pathology"], ["subsection", "Unimodal and multi-modal feature level fusion"], ["subsubsection", "Multi-modal fusion (Renal cancer)"]], "content": "To predict clinical outcomes, oncologists often use both quantitative and qualitative information from genomics and histology{~}. However, current automated histology methods do not take genomic details into account. The following work exploits the complementary knowledge within morphological information and molecular information from genomics to better quantify tumors using graph-based methods.\nRenal cell carcinoma is the most common malignant tumor of the kidney, and it is a diverse category of tumor with varying histology, clinical outcomes, and therapeutic responses. \nRenal cell carcinoma subtypes can be automatically classified through Deep learning frameworks. These algorithms can also identify features that predict survival outcomes from digital histopathological images. Several authors have used GCNs for cancer histology classification, however, its application to survival outcome prediction is less explored. \nChen et al.~ proposed a framework for multi-modal fusion of histology and genomic features for renal cancer survival outcome prediction on the TCGA datasets (glioma and clear cell renal cell carcinoma)~, which contains paired whole slide images, genotype, and transcriptome data.\nTheir model fuses the histology image (patch features), cell-graph and genomic features into a multi-modal tensor that models interactions between the different modalities and outperforms deep learning-based feature fusion for survival outcome prediction.\nThis framework is illustrated in Fig.~\\ref{fig:Fig18}.\nThe authors first extract morphological features from image-based features using CNNs, and graph-based features using GCNs, to learn cell-to-cell interactions in WSI. Cells are represented as nodes in a graph, with cells segregated using a nuclei segmentation method and connections established using KNN. \nCPC is also adopted as a self-supervised method for cell feature extraction.\nThe authors adopted the aggregating functions of the GraphSAGE architecture. The hierarchical self-attention pooling strategy, SAGPool~, is adopted to encode the hierarchical structure of cell graphs.\nThen, to monitor the expressiveness of each modality, a gating-based attention system is used to perform uni-modal function fusion.\nMulti-modal interpretability was considered by adopting an integrated gradient method for visualizing image saliency feature importance.", "cites": [254], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the work of Chen et al. with broader themes in graph-based deep learning for renal cancer survival prediction, integrating technical components such as CNNs, GCNs, GraphSAGE, and attention mechanisms. It offers a critical view by noting the limited exploration of graph methods for survival outcome prediction, but lacks deeper evaluation of methodological trade-offs or limitations. The abstraction is moderate, as it identifies the use of attention and fusion strategies but does not generalize these into broader principles or frameworks beyond the specific study."}}
{"id": "9ba94eb4-186e-4b6e-b41c-1ce1e1f42bab", "title": "Discussion and open challenges", "level": "section", "subsections": ["2951ed7d-2491-48df-adf6-afcd3145da2d", "73fa0ac6-b74f-4ce3-92ad-ee14a635304d", "d861f337-c37b-4f04-9357-81caa768e47a", "e596faee-7b32-43f7-aa51-281221369c0c", "197b05bf-f415-41b3-a1a4-e5f425458a5c"], "parent_id": "01a5d650-4933-4e6e-b664-6a6a8f2896ea", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Discussion and open challenges"]], "content": "\\label{sec:sec4}\nBeyond generating predictions relating to biology and medicine at molecular, genomic and therapeutic levels~, graph representation learning has also been used to support medical diagnosis through the representation of patient records as graphs by using information including brain electrical activity, functional connectivity and anatomical structures~. \nAs demonstrated throughout this review, graph-based deep learning has been successfully used to capture phenotypical and topological distributions in histopathology to better enable precision medicine. Numerous entity-graph based tissue representations and GNN models have been proposed for computer-aided detection and diagnosis of breast, colorectal, prostate, lung, lymphoma, skin, colon, cervical and renal cancers.\nGiven the utility of graphs across biomedical domains, especially to model the histology of cancer tissue, there has been a major push to exploit recent developments in deep learning for graphs in this domain. However, these applications are still in their nascent stages compared to existing research concerning conventional deep learning methods. There are challenges associated with the adoption of GNNs, and there are graph approaches yet to be explored in this domain that potentially allow a more robust and comprehensive investigation of complex biological processes that merit further investigation.\nIn this section, we discuss several future research directions that need to be addressed to unlock the full power of graph deep learning in digital pathology: \n1) Entity graph construction;\n2) Embedding expert knowledge and clinical adoption of graph  analytics;\n3) Complexity of graph models;\n4) Training paradigms; and\n5) Explainability of graph models.\n\\vspace{-6pt}", "cites": [8305], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section identifies key research directions and challenges in graph-based deep learning for histopathology and links them to the broader utility of graphs in biomedical applications, citing a relevant paper. However, it lacks deeper synthesis of multiple sources and more critical evaluation of specific limitations or trade-offs. It generalizes somewhat by highlighting common themes like graph construction and model complexity, but does not offer a novel or meta-level framework."}}
{"id": "e0210c85-0a3c-4709-aa9c-2001c956e3a0", "title": "Entity definition", "level": "paragraph", "subsections": [], "parent_id": "9d65cfa8-0fba-44e8-a9e9-2427a48e8533", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Discussion and open challenges"], ["subsection", "Entity-graph construction"], ["subsubsection", "Pros and cons of current preprocessing steps for entity-graph construction"], ["paragraph", "Entity definition"]], "content": "Cell-graphs have been one of the most popular graph representations, where cells are the entities used to encode cell microenvironments, including morphology of cells and cellular interactions. Such cell-graph representations were proposed in~. However, modeling a WSI as a cell-graph is non-trivial due to the large number of cells and the many possibly isolated cells and weak nuclear boundaries. This representation relies heavily on cell detection or segmentation methods. \nAlthough some works have used representative node sampling{~} or agglomerative clustering{~} to remove redundancy in the graph and reduce computation cost, the majority of cell-graph based proposals assume that cell-cell interactions are the most salient sources of information. Cell-graphs do not exploit tissue macro-architectural structures, or the hierarchical nature of the tissue.\nAnother traditional technique for analysing WSI that include context information of ROIs is patch-graphs. Although patch-graph representations have been adopted in a number of studies~, not all entities are biologically-defined and methods are limited by the patch definition. The resolution and optimal size of each image patch and the level of context offered are trade-off against one another, and are determined by the data. For example, variations in glandular morphology and size make determining an acceptable image patch size problematic. Operating at lower magnification levels may not capture cell-level features, and higher resolutions limits the ability to capture the tissue micro-environment. Thus, an automated technique that defines these patch regions and an appropriate scaling parameter from the input data is vital. \nTo improve the tissue structure-function mapping, graph representations based on tissue regions have been proposed, which can also deal with one of the limitations of cell-graph as important regions may not need to only contain cells~. Tissue-graphs represent well-defined tissue regions and are used to propagate information across neighboring nodes in a progressive manner at a gland or region level. Although superpixel-based approaches are proposed to address patch-graph limitations, a tissue-graph alone cannot capture local cellular information.\nA combination of cell-level and patch-level features was proposed to capture local and global patterns from histological images{~}. However, this fusion approach cannot take advantage of the hierarchy between levels.\nHierarchical graph representations were proposed as an adequate tissue representation as histological structures cannot be fully represented by cellular or tissue interactions alone. It has been shown that cell-graphs and tissue-graphs provide valuable complementary information (cellular and tissue interactions) to learn the intrinsic characteristics of cancerous tissues.\nSuch hierarchical analysis that captures multivariate tissue information at multiple levels has been addressed only by~. \nNevertheless, this approach is still dependent on the construction of a cell-centered graph, which itself is limited by cell detection accuracy and is subjected to the complexity constraints of the model driven by the number of nodes.\nOther works have dealt with cell detection limitations by exploiting graph wavelets with different scaling parameters{~} to obtain multilevel tissue structural information in a tissue-graph. Further, in{~} micro- and macro architectures of histology images were captured with the combination of a topological data analysis tool (cell-level) and GCN (tissue-level).", "cites": [6943, 6945, 6936, 6942, 6939, 6941, 6944, 6940], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 24, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes information across multiple papers, integrating different graph-based approaches (cell-graphs, patch-graphs, tissue-graphs, and hierarchical graphs) and their limitations. It offers critical analysis by identifying shortcomings such as cell detection dependency, resolution trade-offs, and inability to exploit hierarchy. It also abstracts these ideas to highlight broader patterns, such as the need for multi-level representations in histopathology."}}
{"id": "3db899c7-70e7-4074-8047-1cf246aa7c6b", "title": "Feature extraction", "level": "paragraph", "subsections": [], "parent_id": "9d65cfa8-0fba-44e8-a9e9-2427a48e8533", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Discussion and open challenges"], ["subsection", "Entity-graph construction"], ["subsubsection", "Pros and cons of current preprocessing steps for entity-graph construction"], ["paragraph", "Feature extraction"]], "content": "Handcrafted and CNN-based features have been the typical methods to characterize entities. Such deep feature extraction allows use of features from a pre-trained deep architecture. However, the performance of these methods is compromised because the authors usually utilize a pre-trained model (\\textit{e.g.}, trained on ImageNet) due to a lack of patch labels to fine-tune the network, and thus suffer from the domain gap between natural scene images and histopathological images. To address this limitation, a small number of works trained a feature extractor using self-supervised approaches such as CPC, VAE-GAN and auto encoder in~.", "cites": [6936], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of feature extraction methods in entity-graph construction, highlighting the limitations of pre-trained CNNs due to the domain gap. It connects this issue to broader challenges in histopathological image analysis but only marginally integrates ideas from the cited paper. The critique is present but not deeply nuanced, focusing more on general trends than detailed comparisons or novel frameworks."}}
{"id": "d0e34ea0-3261-48a8-8cdb-78b306c4dd02", "title": "Graph topology", "level": "paragraph", "subsections": [], "parent_id": "9d65cfa8-0fba-44e8-a9e9-2427a48e8533", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Discussion and open challenges"], ["subsection", "Entity-graph construction"], ["subsubsection", "Pros and cons of current preprocessing steps for entity-graph construction"], ["paragraph", "Graph topology"]], "content": "On current entity-graphs, each node is only connected to its spatially nearest neighbors, resulting in relatively limited information exchange during the message passing phase. Only one approach to date has computed the connections between nodes by using an adjacency learning layer in an end-to-end manner that considered the global context of all patches~.\nEdge embeddings in cell-graph and tissue-graph topologies are a poorly studied field with few approaches. Learning takes place primarily at the vertices, with edge attributes serving as auxiliary information. The EGNN has only been applied in{~} for colorectal cancer classification, and shows similar performance to the best model based on a 1-dimensional GNN{~}.\nEdge attributes can also directly inform the message passing phase operating over the vertices. In the MEGNet{~} model, vertices are updated by an aggregation of features from adjacent edges.", "cites": [6943, 9085, 5733], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a basic analytical overview of graph topology in entity-graph construction, integrating limited information from cited papers. It mentions the use of adjacency learning layers, edge embeddings in EGNN, and vertex updates in MEGNet, but the connections between these ideas are not deeply synthesized. There is some critical observation, such as the under-study of edge attributes, but overall, the analysis remains at a surface level without offering a novel framework or deep theoretical abstraction."}}
{"id": "b93c4ca3-1e4b-4713-933c-41ab751be71b", "title": "Automated graph generation", "level": "subsubsection", "subsections": [], "parent_id": "2951ed7d-2491-48df-adf6-afcd3145da2d", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Discussion and open challenges"], ["subsection", "Entity-graph construction"], ["subsubsection", "Automated graph generation"]], "content": "Automated graph structure estimation aims to find a suitable graph to represent the data as input to the GNN model. \nBy modeling graph generation as a sequential process, the graph representation (nodes, edges and embeddings) can be inferred directly from data which would be especially useful when representing tissues with a variety of complex micro- and macro environments. \nHowever, the majority of methods surveyed follow a standard sequential workflow which is highly dependent on the individual performance of each preprocessing step, including tissue mask detection, nuclei detection, super-pixel detection, deep feature extraction, and graph building. \nThe use of neural networks to build generative graph models is gaining popularity to capture both their topology and their attributes, which can in turn lead to more robust algorithms and help to provide more accurate results. However, the effectiveness of such algorithms have not been investigated for histopathology images. Therefore, several requirements are still needed to enable the generation process.\nSeveral works that have adopted GCNs for brain electrical activity analysis tasks {~} have demonstrated that learning the graph structure from data improves classification performance in comparison to approaches where a pre-defined graph topology is used.\nIn digital pathology these predefined parameters per histology task are represented by fixed threshold to differentiate non-tissue pixels; patch size and number of patches for nuclei detection, and nuclei and tissue feature extraction; sample ratio of representative nuclei; thresholded KNN and distance that define topology and edges; the number of superpixels and downsampling factor per image; and selection of handcrafted features and CNN layer from which deep features are extracted. Such definitions limit the generalization of entity-graphs to different tissues, organs, and histology tasks.\nSome graph generation approaches that are worthy of exploration within histopathology diagnosis are GraphGAN~, DGMG~, and GCPN~. For instance, DGMG{~} can be used to generate one node at a time from each histopathology patch and then create edges one by one, to connect each node to the existing partial graph using probabilistic dependencies among nodes.\nIn summary, the preceding discussion exemplified the difficulties in estimating a graph structure with the desired properties from data. While there is emerging work in this field, it is ripe for further investigation. In digital pathology, automated graph generation, in which a graph model infers structural content from data, and the integration of domain knowledge, are also underutilised.\n\\vspace{-6pt}", "cites": [222, 264, 8305], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively integrates ideas from the cited papers to highlight the limitations of current practices in entity-graph construction and introduces potential generative approaches. It critically assesses the over-reliance on predefined parameters and the lack of investigation into generative graph methods in histopathology. While not offering a completely novel framework, it generalizes the discussion to broader challenges in automation and domain adaptation."}}
{"id": "73fa0ac6-b74f-4ce3-92ad-ee14a635304d", "title": "Embedding expert knowledge and clinical adoption of graph analytics", "level": "subsection", "subsections": [], "parent_id": "9ba94eb4-186e-4b6e-b41c-1ce1e1f42bab", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Discussion and open challenges"], ["subsection", "Embedding expert knowledge and clinical adoption of graph analytics"]], "content": "Incorporating domain knowledge into the model has emerged as a promising method for improving medical image analysis~. \nThe use of graph-based mappings with label representations (word embeddings) have been investigated to guide information propagation among nodes~. For example, in basal cell carcinoma classification{~}, the embedding knowledge is represented by encoding patches based on prior expert knowledge, which bridges the gap between patch-level and image-level predictions and results in better performance.\nFurther, pathologists' feedback can help to improve the graph representation in terms of how to best mirror the biological relationship between cells and tissues. Thus, graph-based analysis motivates exploring the inclusion of task-specific pathological prior knowledge in the construction of the graph representations~.\nAnother open research question is how to incorporate interdisciplinary knowledge in a principled way, rather than on a case-by-case basis.\nIntegrating electronic health records for personalized medicine can also boost the diagnostic power of digital pathology. The hierarchical information inherent in medical ontologies naturally lends itself to creating a rich network of medical knowledge, and other data types such as symptoms and genomics~. Thus, by integrating patient records into the graph representation learning environment, tailored predictions can be generated for individual patients.\nAmong the AI-techniques, graph-based tissue image analysis demonstrated performance superior or comparable to domain experts in breast cancer analysis{~}.\nThese results combined with studies examining the effect of explanations on clinical end-user decisions{~} show generally positive results in the translation of this technology into diagnostic pathology.\nSuch translation will require to considered integration of standardised technologies into digital pathology workflows, resulting in an integrated approach to diagnosis and offering pathologists new tools that accelerate their workflow, increase diagnostic consistency, and reduce errors.\nWhile, there is considerable promise for graph analytics in digital pathology, there are some challenges ahead.\nThese include, for example, the ability to generalize a diagnosis technique to a large population of patients which contain outliers; and to develop problem-solving skills that demand complex interactions with other medical disciplines.\nThus, more work should be conducted to investigate how a pathologist could refine a graph model decision via a human-in-the-loop system~\nSuch approaches provide an important safety mechanism for detecting and correcting algorithmic errors that may occur.\nA remaining challenge here is to provide frameworks with the above functionalities with reduced complexity to lower the barriers  between the systems and clinicians, to help facilitate system uptake.\nEntity-graph analysis has the ability to transform pathology by providing applications that speed up workflow, improve diagnosis, and improve patient clinical outcomes. However, there is still a gap between research studies and the effort required to deliver reliable graph analytics that incorporate expert knowledge into the system, and can be integrated into existing clinical workflows.\n\\vspace{-6pt}", "cites": [279, 6944, 6960, 6672], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section integrates multiple cited papers to discuss the incorporation of expert knowledge and clinical adoption in graph-based deep learning for pathology. It synthesizes ideas from graph-based attention models, explainability, and multi-label classification, connecting them to the broader theme of translating graph analytics into clinical practice. While the critical analysis is present, it is more moderate in depth, though the abstraction level is strong as it generalizes towards the need for standardized, explainable, and interdisciplinary systems."}}
{"id": "d861f337-c37b-4f04-9357-81caa768e47a", "title": "Complexity of graph models", "level": "subsection", "subsections": [], "parent_id": "9ba94eb4-186e-4b6e-b41c-1ce1e1f42bab", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Discussion and open challenges"], ["subsection", "Complexity of graph models"]], "content": "Graph-based approaches for histology analysis have a high representational power, and can describe topological and geometric properties of multiple types of cancers. When compared to pixel-based approaches, the graph representation can more seamlessly describe a large tissue region. However, classical graph-based models have a high computational complexity. As a result, in the suggested learning approach, the choice of GNN architecture should be handled as a hyper-parameter.\nThe most common GNNs used by methods in this survey include ChebNet~, GCN~, GraphSAGE~, GAT~, GIN~, and variants such as Adaptive GraphSAGE~, RSF~, MS-GWNN~ and FENet~.\nSpatial-GCNs such as GraphSAGE and GIN demonstrated their learning ability using max-, mean-, or sum-pooling aggregators. GIN has been particularly effective in computational pathology with a provably strong expressive power to learn fixed-size discriminative graph embeddings from cellular and tissue architectures in WSIs, which demonstrate translation and rotation invariance. \nHowever, it is noted that these GNN models inherit considerable complexity from their deep learning lineage, which can be burdensome when scaling and deploying GNNs. This is likely one of the reasons that has seen patch-based approaches remain a popular approach for many problems.\nThe training of GNNs remains one of the most difficult tasks due to their high memory consumption and inference latency compared to patch-based deep learning approaches.\nGNNs usually require the whole graph and the intermediate states of all nodes to be saved in memory. However, the adoption of an efficient training approach is uncommon in the applications surveyed.\nVarious graph sampling approaches have been proposed as a way to alleviate the cost of training GNNs. Rather than training over the full graph, each iteration is run over a sampled sub-graph, whether they are sampled node-wise (GraphSage~), layer-wise (FastGCN~, $L^2$-GCN~), or by clustering (Cluster-GCN~).\nSome works have proposed more efficient and simple architectures that deserve attention for their potential to be adopted in computational histopathology.\nThe simple graph convolution (SGC)~ reduces the complexity of GCNs by repeatedly removing the non-linearities between GCN layers and collapsing multiple weight matrices into a single linear transformation. This model was adopted for emotion recognition and increased the performance speed with a comparable classification accuracy in comparison to other networks~.\nThe simple scalable inception GNN (SIGN)~ is explicitly designed as a shallow architecture that combines graph convolutional filters of different sizes that allow efficient pre-computation.\nThe efficient graph convolution (EGC)~ method does not require trading accuracy for runtime memory or latency reductions based on an adaptive filtering approach.\nGNNs can also deliver high performance for feature matching across images~, which can be incorporated for content-based histopathological image retrieval.\nIt is also important to highlight that some works exploit the cell-graph representation without the complexity of GCN processing. \nThe tissue classification problem was proposed in~ as a cellular community detection based on cell detection and classification into distinct cellular components (cell-graphs), and clustering of image patches (patch-level graphs) into biologically meaningful communities (specific tissue phenotype). \nThe concept of constructing a graph and then using geodesic distance for community detection has outperformed deep neural networks and graph-based deep leaning methods such as ChebNet, GCNs and deep graph infomax learning (DGI)~.\nIn the coming years, a key research topic will be how to effectively learn and compute GNNs in order to realise their full potential. Deep learning on graphs is inherently difficult due to the graphs' complex topological structure, which can be made up of many different types of entities and interactions. \nAs such, the appropriate selection of key parameters of a model prior to representation learning is essential to capture the structural information of the histopathology slides.\n\\vspace{-6pt}", "cites": [8306, 231, 242, 8313, 2618, 240, 237, 6961, 246, 180, 6962, 6941, 230, 6940], "cite_extract_rate": 0.7368421052631579, "origin_cites_number": 19, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.8, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple cited works on GNNs and their efficiency, connecting different graph sampling and simplification techniques to a broader narrative about model complexity in histopathology. It critically examines the trade-offs between complexity, accuracy, and scalability in various GNN architectures. Furthermore, it abstracts these methods into general categories (e.g., layer-wise training, inductive learning, shallow architectures) and highlights overarching principles such as the need for efficient training strategies and model simplification to realize the potential of GNNs."}}
{"id": "41f3c05a-0835-499d-a5b8-6ce13c052c52", "title": "Node embeddings", "level": "subsubsection", "subsections": [], "parent_id": "e596faee-7b32-43f7-aa51-281221369c0c", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Discussion and open challenges"], ["subsection", "Training paradigms"], ["subsubsection", "Node embeddings"]], "content": "The node embeddings are the features that are learned to represent the defined node (\\textit{e.g.} cells, nucleus, patches, super-pixels). Some of the embedding features extracted through attribute networks require labeled datasets and need to be trained in a supervised manner as explained in~.\nHowever, one of the main challenges in deep learning is the lack of large corpora  of manually labeled data for training, which often imposes a limitation on problems in the medical domain.\nThus, self-supervised methods are gaining interest to improve the quality of learned node embeddings~ by learning embedding features directly from histopathology images, rather than relying on extracting features using transfer learning, which is discussed in Subsection~\\ref{subsec:sec4a}.", "cites": [6943, 6936, 6942, 6939, 6941, 6944, 6940], "cite_extract_rate": 0.28, "origin_cites_number": 25, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a general overview of node embeddings in graph-based deep learning for histopathology and briefly mentions the importance of self-supervised learning. It references several relevant papers but does not deeply synthesize their contributions or explicitly compare their approaches. While it highlights a challenge (lack of labeled data), it does so without a detailed critical evaluation or abstraction of broader trends."}}
{"id": "bd2e9e68-3745-492a-911f-5ab75e723bbd", "title": "Node/graph classification", "level": "subsubsection", "subsections": [], "parent_id": "e596faee-7b32-43f7-aa51-281221369c0c", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Discussion and open challenges"], ["subsection", "Training paradigms"], ["subsubsection", "Node/graph classification"]], "content": "Training a GCN for node or graph level classification can be performed in supervised, semi-supervised or even in a self-supervised manner. \nIf sufficient labels are available for nodes or graph data, the common practice is a supervised training approach, such as the methods of~. \nThough supervised methods can achieve high performance, they can place limitations on model complexity and can suffer when annotations are inconsistent or imprecise. \nIn the absence of sufficient labeled data, weakly-supervised or semi-supervised frameworks are proposed to better capture the structure of histopathology data and reduce the human annotation workload. \nAlthough the issue of missing labels is not specific to the graph domain, only a few works have adopted such frameworks (pixel or patch level labels).\nIn semi- or weakly-supervised approach, the node embeddings are learnt from few labeled samples per class~.\nFor example, in a weakly supervised learning approach, the contributions of the individual patches to the ROI-level diagnosis are not known during training{~}.\nIn addition to the above, extensive research over past years in deep learning~ showed that a decision classifier based on Multiple Instance Learning (MIL) can boost the performance in classifying cancer by aggregating instance-level predictions. \nMIL only requires labels for the bag of instances rather than individual instances, which makes it well-suited for histology slide classification. One example is CLAM (Clustering-constrained attention multiple instance learning{~}). Even though these approaches have practical merits and can consider the important patches for predicting the staging, they do not consider the spatial relationships between patches.\nCurrent multiple instance learning approaches using deep graphs~ follow this line of research.\nThey can seamlessly scale to arbitrary tissue dimensions by incorporating an arbitrary number of entities and interactions, thus offering an alternative to traditional MIL{~}.\nMIL methods can be incorporated with a GCN to take advantage of the structural information among instances{~}. For example, the SegGini model{~} outperforms several traditional state-of-the-art methods such as CLAM{~} and Context-Aware CNN (CACNN){~} for weakly-supervised classification of prostate cancer.\nSelf supervised methods have also been successfully deployed as a training paradigm for GCNs. For example, Ozen et al.~ adopted a SimCLR framework~ along with contrastive loss to learn a representation of ROIs and perform classification.\nAlthough the aforementioned training paradigms demonstrate remarkable performance, few works~ have considered end-to-end training and the challenge that brings such as dealing with complexity of constructing a graph or labeled data, and thus this requires investigation in future works.  \nTraining paradigms are dependent on the availability of manually labeled data. In medical imaging and specifically histopathology obtaining a large set of labeled data is a tedious process and so weakly- and self-supervised algorithms are receiving increasing interest for learning node embeddings and performing graph classification. It is expected that in future, further research carry out on a large-scale to analyse histopathology data using GCNs in a weakly- or self-supervised manner. \n\\vspace{-8pt}", "cites": [6943, 6965, 6945, 7000, 6936, 6942, 6939, 6963, 6941, 6944, 6940, 6937, 6964], "cite_extract_rate": 0.41935483870967744, "origin_cites_number": 31, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple cited papers to form a coherent narrative on training paradigms for node/graph classification in graph-based deep learning for histopathology. It critically evaluates the limitations of supervised methods and explores the merits and shortcomings of semi-supervised, weakly-supervised, and self-supervised approaches. The discussion abstracts beyond individual methods by identifying broader trends in data efficiency, explainability, and the need for end-to-end training, positioning the section as insightful and forward-looking."}}
{"id": "197b05bf-f415-41b3-a1a4-e5f425458a5c", "title": "Explainability of graph models", "level": "subsection", "subsections": [], "parent_id": "9ba94eb4-186e-4b6e-b41c-1ce1e1f42bab", "prefix_titles": [["title", "A Survey on Graph-Based Deep Learning \\\\ for Computational Histopathology"], ["section", "Discussion and open challenges"], ["subsection", "Explainability of graph models"]], "content": "To effectively translate graph models into clinical practise, clinicians' trust must be established. Explainability, or a model's ability to justify its outcomes and therefore assist clinicians in understanding a model's prediction, has long been seen as crucial to building trust. Understanding model behaviour beyond traditional performance indicators has thus become an important part of machine learning research, particularly in healthcare{~}.\nExplainability in deep models has focused on providing input-dependent explanations and understanding model behavior from different perspectives, including visual explanations and highlighting salient regions. We can examine the sensitivity between the input features and the predictions, for example, by looking at the gradients or weights. We can also highlight important features or regions of an image by incorporating attention mechanisms{~}.\nNevertheless, compared with traditional image domains, explainability and visualization of deep learning for graphs is less explored{~}, yet explanability is critical to highlight informative structural compositions of tissue and inter-nuclear relationships, as is desired for computational histopathology.\nWhile interpretability approaches are generally lacking within most graph network methods, it is worth noting that a few methods exist and incorporate such explanations in digital pathology as illustrated in Table~\\ref{table:pathology}:\ni) In~ a GCN propagated supervisory information over patches to learn patch-aware interpretability in the form of a probability score.\nii) A robust spatial filtering with an attention-based architecture and node occlusion was used to capture the contribution of each nucleus and its neighborhood to the prediction~.\niii) The Graph Mapper, a topological data analysis tool, was adopted to compress histological information to its essential structures, where meaningful histology regions are captured~.\niv) In~, an integrated gradient method was used to visualise image saliency feature importance.\nv) A graph clustering visualization was used in~ to group cells with similar tissue structures.\nvi) A post-hoc graph-pruning explainer, GCExplainer, was designed to identify decisive cells and interactions from the input graph~. \nvii) The gradient-based saliency method, GraphGrad-CAM, was adopted in~ and~ to measure importance scores and regions that contributed towards the classification of each class.\nThe majority of approaches that have incorporated explainers are limited to cell-graph analysis. Considering the pathologically aligned multi-level hierarchical tissue attributes{~}, the interpretability can reveal crucial entities such as nuclei, tissue parts and interactions which can mimic the pathologist's assessment and therefore, increase the level of trust between experts and AI frameworks.\nExisting works however lack the definition of objectives to validate a model in terms of effective explainability, and only a single work has looked at the quality and utility of the proposed explanation methods for the intended audience (\\textit{i.e.} clinicians). In~, the authors evaluated several graph explainers (GNNExplainer, GraphGrad-CAM, GraphGrad-CAM++, GraphLRP) to provide domain-understandable quantitative metrics based on pathologically measurable cellular properties, to make graph decisions understandable to pathologists. The authors found that at the concept-level, GraphGrad-CAM++ has the highest overall agreement with the pathologists, followed by GraphGrad-CAM and GNNExplainer.\nOther methods not investigated in this survey that focus on instance-level interpretation of deep graph models that deserve attention in digital pathology for explainability at the node, edge, or node feature levels are: excitation BP~, PGM-explainer~, GraphMask~, Graphlime~, and Relex~.\nOther methods such as SubgraphX~ provide subgraph-level explanations which may be more intuitive and human-intelligible for digital pathology.\nKnowing the subset of features from which the model outcome is derived is critical. This allows clinicians to compare model decisions with clinical judgement, which is especially useful when there is a discrepancy. It is also worth noting that clinicians expect variation in the importance of inputs to exist both across patients and populations~. \nHowever, the explanations provided by methods discussed in this survey using gradient-based (GraphGrad-CAM) and perturbation-based methods (GNNExplainer) are limited to single instances. To verify and understand a deep model, pathologists need to check explanations for all input graphs, which is time-consuming and impractical. \nModels that interpret each instance independently, as previously stated, are insufficient to provide a global understanding of the trained model~. Thus, methods to provide GNN predictions on a group of instances collectively (\\textit{i.e.} a population) and provide a global understanding of GNN predictions is less explored in the literature.\nInstance-level methods explain GNNs with respect to each input graph, whereas model-level methods explain GNNs without regard for any specific input example. The latter specifically investigates what input graph patterns can lead to a specific GNN behaviour, such as maximising a target prediction. However, no research on interpreting GNNs at the model-level exists in digital pathology.\nXGNN~ provides model-level explanations by training a graph generator to build graph patterns that optimize a specific model prediction. The authors formulated graph creation as a reinforcement learning problem, with the graph generator predicting how to add an edge to a given graph and build a new graph at each step. The generator is then trained using a policy gradient based on feedback from the trained graph models. Several graph rules are also used to ensure that the explanations are both valid and human-readable. \nPGExplainer~ can also provide an explanation for each instance with a global view of the GNN model by incorporating a generative probabilistic model.\nNonetheless, it is unknown whether XGNN and PGExplainer can be used to perform node classification tasks for histopathology analysis, which is an important area for future research.\nGiven the trend of graph-based processing for a variety of applications in computational pathology, graph explainability and quantitative evaluation with a focus on clinician usability are critical. Interpretability is essential because it can aid, for example, in informed decision-making during cancer diagnosis and treatment planning. However, interpretability of GNNs within digital pathology has received insufficient attention to date.\n\\vspace{-6pt}", "cites": [569, 6973, 6945, 6966, 6971, 6942, 6939, 6967, 6941, 6944, 6969, 6972, 6970, 6968], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 21, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.3, "critical": 4.0, "abstraction": 3.7}, "insight_level": "high", "analysis": "The section effectively synthesizes a range of explainability methods for graph neural networks in digital pathology, connecting ideas such as instance-level vs. model-level explanations. It critically analyzes the limitations of current approaches, particularly their focus on individual instances and lack of clinician-focused validation. The section also abstracts beyond specific methods to highlight broader issues like the need for global interpretability and usability for pathologists."}}
