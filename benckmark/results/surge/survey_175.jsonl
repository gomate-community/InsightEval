{"id": "98fb021a-7d26-4ae0-b9eb-be2dcb0bf98b", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "ccad4ffc-3bf5-441a-9a84-e9a6e986845e", "prefix_titles": [["title", "Transformers in Time Series: A Survey"], ["section", "Introduction"]], "content": "The innovation of Transformer in deep learning~ has brought great interests recently due to its excellent performances in natural language processing (NLP)~, computer vision (CV)~, and speech processing~. Over the past few years, numerous Transformers have been proposed to advance the state-of-the-art performances of various tasks significantly. There are quite a few literature reviews from different aspects, such as in NLP applications~, CV applications~, and efficient Transformers~.\nTransformers have shown great modeling ability for long-range dependencies and interactions in sequential data and thus are appealing to time series modeling. \nMany variants of Transformer have been proposed to address special challenges in time series modeling and have been successfully applied to various time series tasks, such as forecasting~, anomaly detection~, and classification~. Specifically, seasonality or periodicity is an important feature of time series~. How to effectively model long-range and short-range temporal dependency and capture seasonality simultaneously remains a challenge~. \nWe note that there exist several surveys related to deep learning for time series, including forecasting~, classification~, anomaly detection~, and data augmentation~, but there is no comprehensive survey for Transformers in time series. As Transformer for time series is an emerging subject in deep learning, a systematic and comprehensive survey on time series Transformers would greatly benefit the time series community. \nIn this paper, we aim to fill the gap by\nsummarizing the main developments of time series Transformers. We first give a brief introduction about vanilla Transformer, and then propose a new taxonomy from perspectives of both network modifications and application domains for time series Transformers. \nFor network modifications, we discuss the improvements made on both low-level (i.e. module) and high-level (i.e. architecture) of Transformers, with the aim to optimize the performance of time series modeling.\nFor applications, we analyze and summarize Transformers for popular time series tasks including forecasting, anomaly detection, and classification. \nFor each time series Transformer, we analyze its insights, strengths, and limitations. \nTo provide practical guidelines on how to effectively use Transformers for time series modeling, we conduct extensive empirical studies that examine multiple aspects of time series modeling, including robustness analysis, model size analysis, and seasonal-trend decomposition analysis.\nWe conclude this work by discussing possible future directions for time series Transformers, including inductive biases for time series Transformers, Transformers and GNN for time series,\npre-trained Transformers for time series, Transformers with architecture level variants, and Transformers with NAS for time series. To the best of our knowledge, this is the first work to comprehensively and systematically review the key developments of Transformers for modeling time series data. We hope this survey will ignite further research interests in time series Transformers.", "cites": [1830, 7069, 35, 7518, 7302, 1831, 1829, 1827, 7519, 38, 1828, 732, 7, 7520, 1832], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 21, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The introduction section synthesizes information from multiple cited works, connecting trends in time series deep learning with Transformer adaptations. It critically identifies the absence of a comprehensive survey on time series Transformers and outlines the paper’s contribution in addressing this gap. The section abstracts broader challenges such as modeling locality, seasonality, and scalability, positioning these as key themes for future research."}}
{"id": "14423eab-abad-4189-a310-0be6ef034cb7", "title": "Vanilla Transformer", "level": "subsection", "subsections": [], "parent_id": "9c1f377f-f6d2-415d-971d-74fbba06df60", "prefix_titles": [["title", "Transformers in Time Series: A Survey"], ["section", "Preliminaries of the Transformer"], ["subsection", "Vanilla Transformer"]], "content": "The vanilla Transformer~ follows most competitive neural sequence models with an encoder-decoder structure. Both encoder and decoder are composed of multiple identical blocks. Each encoder block consists of a multi-head self-attention module and a position-wise feed-forward network while each decoder block inserts cross-attention models between the multi-head self-attention module and the position-wise feed-forward network.", "cites": [38], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of the vanilla Transformer's architecture, primarily paraphrasing information from the cited paper. It lacks synthesis across multiple sources, critical evaluation of the model's suitability for time series, and abstraction to broader principles or patterns."}}
{"id": "1d0cb312-c2f5-4096-8968-f01c782c1c69", "title": "Absolute Positional Encoding", "level": "subsubsection", "subsections": [], "parent_id": "b79a3ccc-5181-4cb5-9988-2605be36f712", "prefix_titles": [["title", "Transformers in Time Series: A Survey"], ["section", "Preliminaries of the Transformer"], ["subsection", "Input Encoding and Positional Encoding"], ["subsubsection", "Absolute Positional Encoding"]], "content": "\\label{subsubsec:vallina positional encoding}\nIn vanilla Transformer, for each position index $t$, encoding vector is given by\n\\begin{equation}\n{PE}(t)_i = \n\\left\\{ \n\\begin{aligned} \n\\sin(\\omega_{i}t) \\quad i\\%2=0\\\\\n\\cos(\\omega_{i}t) \\quad i\\%2=1\n\\end{aligned}\n\\right.\n\\end{equation}\nwhere $\\omega_i$ is the hand-crafted frequency for each dimension.\nAnother way is to learn a set of positional embeddings for each position which is more flexible~.", "cites": [7, 790], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of absolute positional encoding in Transformers, referencing two papers primarily for context. It lacks in-depth synthesis of the cited works, critical evaluation of their approaches, or abstraction to broader patterns or principles. The content is minimal and does not go beyond stating facts about positional encoding methods."}}
{"id": "4c9f2b1f-398c-447a-977b-d33b4a2d2e53", "title": "Relative Positional Encoding", "level": "subsubsection", "subsections": [], "parent_id": "b79a3ccc-5181-4cb5-9988-2605be36f712", "prefix_titles": [["title", "Transformers in Time Series: A Survey"], ["section", "Preliminaries of the Transformer"], ["subsection", "Input Encoding and Positional Encoding"], ["subsubsection", "Relative Positional Encoding"]], "content": "Following the intuition that pairwise positional relationships between input elements is more beneficial than positions of elements, relative positional encoding methods have been proposed. For example, one of such methods is to add a learnable relative positional embedding to keys of attention mechanism~.\nBesides the absolute and relative positional encodings, there are methods using hybrid positional encodings that combine them together~. Generally, the positional encoding is added to the token embedding and fed to Transformer.", "cites": [1454, 7054], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces the concept of relative positional encoding and mentions one method without elaboration. It cites two relevant papers but does not synthesize their contributions or connect their ideas in a meaningful way. There is minimal critical evaluation or abstraction, as it primarily describes the approach without deeper analysis or generalization."}}
{"id": "496a5502-f36f-4add-b0f3-3e717e8fca6f", "title": "Positional Encoding", "level": "subsection", "subsections": [], "parent_id": "c79d4ba1-c51e-45ad-b6f6-40dc07e6123b", "prefix_titles": [["title", "Transformers in Time Series: A Survey"], ["section", "Network Modifications for Time Series"], ["subsection", "Positional Encoding"]], "content": "As the ordering of time series matters, it is of great importance to encode the positions of input time series into Transformers. A common design is to first encode positional information as vectors and then inject them into the model as an additional input together with the input time series. How to obtain these vectors when modeling time series with Transformers can be divided into three main categories.\n\\textbf{Vanilla Positional Encoding.}\\quad A few works  simply introduce vanilla positional encoding (Section \\ref{subsubsec:vallina positional encoding}) used in  , which is then added to the input time series embeddings and fed to Transformer. Although this approach can extract some positional information from time series, they were unable to fully exploit the important features of time series data. \n\\textbf{Learnable Positional Encoding.}\\quad As the vanilla positional encoding is hand-crafted and less expressive and adaptive, several studies found that learning appropriate positional embeddings from time series data can be much more effective. Compared to fixed vanilla positional encoding, learned embeddings are more flexible and can adapt to specific tasks. ~ introduces an embedding layer in Transformer that learns embedding vectors for each position index jointly with other model parameters.  uses an LSTM network to encode positional embeddings, which can better exploit sequential ordering information in time series.\n\\textbf{Timestamp Encoding.}\\quad When modeling time series in real-world scenarios, the timestamp information is commonly accessible, including calendar timestamps (e.g., second, minute, hour, week, month, and year) and special timestamps (e.g., holidays and events). These timestamps are quite informative in real applications but hardly leveraged in vanilla Transformers. To mitigate the issue, Informer~ proposed to encode timestamps as additional positional encoding by using learnable embedding layers. A similar timestamp encoding scheme was used in Autoformer  and FEDformer~.", "cites": [1833, 1829, 1479, 7069, 7519, 38, 35], "cite_extract_rate": 1.0, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a structured analytical overview of positional encoding in time series Transformers, categorizing approaches and linking them to specific papers. It synthesizes the role of positional encoding across multiple works, though the connections are more thematic than deeply integrative. While it critiques vanilla positional encoding and highlights advantages of learnable and timestamp-based approaches, the analysis remains at a moderate level without offering a novel framework or deep meta-level insights."}}
{"id": "e0a5fa69-43f2-4254-9129-8871d6a0874f", "title": "Attention Module", "level": "subsection", "subsections": [], "parent_id": "c79d4ba1-c51e-45ad-b6f6-40dc07e6123b", "prefix_titles": [["title", "Transformers in Time Series: A Survey"], ["section", "Network Modifications for Time Series"], ["subsection", "Attention Module"]], "content": "Central to Transformer is the self-attention module. It can be viewed as a fully connected layer with weights that are dynamically generated based on the pairwise similarity of input patterns. As a result, it shares the same maximum path length as fully connected layers, but with a much less number of parameters, making it suitable for modeling long-term dependencies. \nAs we show in the previous section the self-attention module in the vanilla Transformer has a time and memory complexity of $\\mathcal{O}(N^2)$ ($N$ is the input time series length), which becomes the computational bottleneck when dealing with long sequences. Many efficient Transformers were proposed to reduce the quadratic complexity that can be classified into two main categories: \n(1) explicitly introducing a sparsity bias into the attention mechanism like LogTrans~ and Pyraformer~; (2) exploring the low-rank property of the self-attention matrix to speed up the computation, e.g. Informer~ and FEDformer~. Table~\\ref{tab:compare_complexity} shows both the time and memory complexity of popular Transformers applied to time series modeling, and more details about these models will be discussed in Section~\\ref{secapp}.\n\\input{tables/complexity}", "cites": [7519, 1479, 35], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the key adaptations of the attention module in time series Transformers by categorizing them into two main approaches—sparsity bias and low-rank approximations. It critically identifies the computational bottleneck of the vanilla Transformer and explains how different models address it. The abstraction level is strong, as it generalizes the strategies used across multiple papers to highlight overarching design principles in attention mechanism modifications."}}
{"id": "89ae3c5f-0e16-429b-87dd-dcb340aeb050", "title": "Architecture-based Attention Innovation", "level": "subsection", "subsections": [], "parent_id": "c79d4ba1-c51e-45ad-b6f6-40dc07e6123b", "prefix_titles": [["title", "Transformers in Time Series: A Survey"], ["section", "Network Modifications for Time Series"], ["subsection", "Architecture-based Attention Innovation"]], "content": "To accommodate individual modules in Transformers for modeling time series, a number of works ~ seek to renovate Transformers on the architecture level. Recent works introduce hierarchical architecture into Transformer to take into account  the multi-resolution aspect of time series. Informer~ inserts max-pooling layers with stride 2 between attention blocks, which down-sample series into its half slice. Pyraformer~ designs a $C$-ary tree-based attention mechanism, in which nodes at the finest scale correspond to the original time series, while nodes in the coarser scales represent series at lower resolutions. Pyraformer developed both intra-scale and inter-scale attentions in order to better capture temporal dependencies across different resolutions. Besides the ability to integrate information at different multi-resolutions, a hierarchical architecture also enjoys the benefits of efficient computation, particularly for long-time series.", "cites": [1479], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates two key papers (Informer and Pyraformer) to discuss hierarchical architecture innovations in time series Transformers, connecting their approaches under a common theme of multi-resolution modeling. It provides some level of abstraction by identifying the general benefit of such architectures in capturing temporal dependencies and improving computational efficiency. However, it lacks deeper critical analysis or comparison of these methods' limitations or trade-offs."}}
{"id": "51fd91b4-9a80-458d-b58b-7b75092e64f5", "title": "Module-level variants", "level": "paragraph", "subsections": [], "parent_id": "598865d5-0baa-4d16-acb0-dae431897eb1", "prefix_titles": [["title", "Transformers in Time Series: A Survey"], ["section", "Applications of Time Series Transformers"], ["subsection", "Transformers in Forecasting"], ["subsubsection", "Time Series Forecasting"], ["paragraph", "Module-level variants"]], "content": "In the module-level variants for time series forecasting, their main architectures are similar to the vanilla Transformer with minor changes. Researchers introduce various time series inductive biases to design new modules. The following summarized work consists of three different types: designing new attention modules, exploring the innovative way to normalize time series data, and utilizing the bias for token inputs, as shown in Figure \\ref{fig:Categorization}. \n\\begin{figure}[!t]\n\\centering\n    \\includegraphics[width=0.42\\textwidth]{plot/module_base5.pdf}\\vspace{-4mm}\n    \\caption{Categorization of module-level Transformer variants for time series forecasting.}\\vspace{-4mm}\n    \\label{fig:Categorization}\n\\end{figure}\nThe first type of variant for module-level Transformers is to design new attention modules, which is the category with the largest proportion. \nHere we first describe six typical works: LogTrans~, Informer~, AST~, Pyraformer~, Quatformer~, and FEDformer~, all of which exploit sparsity inductive bias or low-rank approximation to remove noise and achieve a low-order calculation complexity. \nLogTrans~ proposes convolutional self-attention by employing causal convolutions to generate queries and\nkeys in the self-attention layer. It introduces sparse bias, a Logsparse mask, in self-attention model that reduces computational complexity from $\\mathcal{O}(N^2)$ to $\\mathcal{O}(N\\log N)$.\nInstead of using explicit sparse bias, Informer~ selects dominant queries based on queries and key similarities, thus achieving similar improvements as LogTrans in computational complexity. It also designs a generative style decoder to produce long-term forecasting directly and thus avoids accumulative error in using one forward-step prediction for long-term forecasting.\nAST~ uses a generative adversarial encoder-decoder framework to train a sparse Transformer model for time series forecasting. It shows that adversarial training can improve time series forecasting by directly shaping the output distribution of the network to avoid error accumulation through one-step ahead inference. Pyraformer~ designs a hierarchical pyramidal attention module with a binary tree following the path, to capture temporal dependencies of different ranges with linear time and memory complexity. \nFEDformer~ applies attention operation in the frequency domain with Fourier transform and wavelet transform. It achieves a linear complexity by randomly selecting a fixed-size subset of frequency. Note that due to the success of Autoformer and FEDformer, it has attracted more attention in the community to explore self-attention mechanisms in the frequency domain for time series modeling. Quatformer~ proposes learning-to-rotate attention (LRA) based on quaternions that introduce learnable period and phase information to depict intricate periodical patterns. Moreover, it decouples LRA using a global memory to achieve linear complexity. \nThe following three works focus on building an explicit interpretation ability of models, which follows the trend of Explainable Artificial Intelligence (XAI). TFT~ designs a multi-horizon forecasting model with static covariate encoders, gating feature selection, and temporal self-attention decoder. It encodes and selects useful information from various covariates to perform forecasting. It also preserves interpretability by incorporating global, temporal dependency, and events. \nProTran~ and SSDNet~ combine\nTransformer with state space models to provide probabilistic forecasts. \nProTran designs a generative modeling and inference procedure based on variational inference. SSDNet first uses Transformer to learn the temporal pattern and estimate the parameters of SSM, and then applies SSM to perform the seasonal-trend decomposition and maintain the interpretable ability. \nThe second type of variant for module-level Transformers is the way to normalize time series data. To the best of our knowledge, Non-stationary Transformer~ is the only work that mainly focuses on modifying the normalization mechanism as shown in Figure~\\ref{fig:Categorization}. It explores the over-stationarization problem in time series forecasting tasks with a relatively simple plugin series stationary and De-stationary module to modify and boost the performance of various attention blocks. \nThe third type of variant for module-level Transformer is utilizing the bias for token input. \nAutoformer~ adopts a segmentation-based representation mechanism. \nIt devises a simple seasonal-trend decomposition architecture with an auto-correlation mechanism working as an attention module. The auto-correlation block measures the time-delay similarity between inputs signal and aggregates the top-k similar sub-series to produce the output with reduced complexity.\nPatchTST~ utilizes channel-independent where each channel contains a single univariate time series that shares the same embedding within all the series, and subseries-level patch design which segmentation of time series into subseries-level patches that are served as input tokens to Transformer. Such ViT  alike design improves its numerical performance in long-time time-series forecasting tasks a lot. \nCrossformer~ proposes a Transformer-based model utilizing cross-dimension dependency for multivariate time series forecasting. The input is embedded into a 2D vector array through the novel dimension-segment-wise embedding to preserve time and dimension information. Then, a two-stage attention layer is used to efficiently capture the cross-time and cross-dimension dependency.", "cites": [1834, 7521, 732, 1833, 1835, 1479, 7069, 7519, 35], "cite_extract_rate": 0.6428571428571429, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes information from multiple papers by grouping them into three coherent categories based on design principles. It provides critical analysis by pointing out specific limitations (e.g., non-stationarity, over-stationarization, computational complexity) and how each variant addresses them. Furthermore, it abstracts these approaches into broader design patterns like sparsity, interpretability, and normalization, contributing to a meta-level understanding of module-level innovations."}}
{"id": "cbf305fc-410a-4f47-a730-38b2e0b1d70c", "title": "Architecture-level variants", "level": "paragraph", "subsections": [], "parent_id": "598865d5-0baa-4d16-acb0-dae431897eb1", "prefix_titles": [["title", "Transformers in Time Series: A Survey"], ["section", "Applications of Time Series Transformers"], ["subsection", "Transformers in Forecasting"], ["subsubsection", "Time Series Forecasting"], ["paragraph", "Architecture-level variants"]], "content": "Some works start to design a new transformer architecture beyond the scope of the vanilla transformer. \nTriformer~ design a triangular,variable-specific patch attention. It has a triangular tree-type structure as the later input size shrinks exponentially and a set of variable-specific parameters making a multi-layer Triformer maintain a lightweight and linear complexity.\nScaleformer~ proposes a multi-scale framework that can be applied to the baseline transformer-based time series forecasting models (FEDformer, Autoformer, etc.). It can improve the baseline model's performance by iteratively refining the forecasted time series at multiple scales with shared weights.", "cites": [1836, 7519, 7069], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly introduces two architecture-level variants, Triformer and Scaleformer, but does not synthesize or integrate their contributions into a broader framework. It lacks critical evaluation of the approaches and does not identify overarching patterns or principles in time series forecasting using Transformers."}}
{"id": "47d81e3a-5ec4-400d-8fe9-fafaa2b72816", "title": "Remarks", "level": "paragraph", "subsections": [], "parent_id": "598865d5-0baa-4d16-acb0-dae431897eb1", "prefix_titles": [["title", "Transformers in Time Series: A Survey"], ["section", "Applications of Time Series Transformers"], ["subsection", "Transformers in Forecasting"], ["subsubsection", "Time Series Forecasting"], ["paragraph", "Remarks"]], "content": "Note that DLinear~ questions the necessity of using Transformers for long-term time series forecasting, and shows that a simpler MLP-based model can achieve better results compared to some Transformer baselines through empirical studies. \nHowever, we notice that a recent Transformer model PatchTST  achieves a better numerical result compared to DLinear for long-term time series forecasting. \nMoreover, there is a thorough theoretical study  showing that the Transformer models are universal approximators of sequence-to-sequence functions.\nIt is a overclaim to question the potential of any type of method for time series forecasting based solely on experimental results from some variant instantiations of such method, especially for Transformer models which already demonstrate the performances in most machine learning-based tasks. Therefore, we conclude that summarizing the recent Transformer-based models for time series forecasting is necessary and would benefit the whole community.", "cites": [1835, 1837], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes insights from two contrasting papers, integrating their findings into a coherent discussion about the effectiveness of Transformers in forecasting. It critically evaluates the experimental and theoretical claims, pointing out the limitations of questioning a method solely based on some variants' performance. The section abstracts the broader debate into a principled argument about the necessity of reviewing Transformer-based models despite isolated counterexamples."}}
{"id": "08650ccb-c8d6-4311-a095-dd8292ed5373", "title": "Spatio-Temporal Forecasting", "level": "subsubsection", "subsections": [], "parent_id": "bacedec3-e491-4692-9e05-2f3b005ec712", "prefix_titles": [["title", "Transformers in Time Series: A Survey"], ["section", "Applications of Time Series Transformers"], ["subsection", "Transformers in Forecasting"], ["subsubsection", "Spatio-Temporal Forecasting"]], "content": "In spatio-temporal forecasting, both temporal and spatio-temporal dependencies are taken into account in time series Transformers for accurate forecasting. \nTraffic Transformer~ designs an encoder-decoder structure using a self-attention module to capture temporal-temporal dependencies and a graph neural network module to capture spatial dependencies. Spatial-temporal Transformer~ for traffic flow forecasting takes a step further. Besides introducing a temporal Transformer block to capture temporal dependencies, it also designs a spatial Transformer block, together with a graph convolution network, to better capture spatial-spatial dependencies. Spatio-temporal graph Transformer~ designs an attention-based graph convolution mechanism that is able to learn a complicated temporal-spatial attention pattern to improve pedestrian trajectory prediction.\nEarthformer~ proposes a cuboid attention for efficient space-time modeling, which decomposes the data into cuboids and applies cuboid-level self-attention in parallel. It shows that Earthformer achieves superior performance in weather and climate forecasting. \nRecently, AirFormer~ devises a dartboard spatial self-attention module and a causal temporal self-attention module to efficiently capture spatial correlations and temporal dependencies, respectively. Furthermore, it enhances Transformers with latent variables to capture data uncertainty and improve air quality forecasting.", "cites": [1838, 1839, 1840], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual summary of different Transformer-based models for spatio-temporal forecasting but lacks deeper integration or comparison between them. While it mentions structural elements like encoder-decoder frameworks and attention mechanisms, it does not synthesize these into a broader narrative or framework. There is minimal critical analysis or abstraction beyond the specific methods described."}}
{"id": "cb663798-1662-4497-b753-f33b8a1314f7", "title": "Event Forecasting", "level": "subsubsection", "subsections": [], "parent_id": "bacedec3-e491-4692-9e05-2f3b005ec712", "prefix_titles": [["title", "Transformers in Time Series: A Survey"], ["section", "Applications of Time Series Transformers"], ["subsection", "Transformers in Forecasting"], ["subsubsection", "Event Forecasting"]], "content": "Event sequence data with irregular and asynchronous timestamps are naturally observed in many real-life applications, which is in contrast to regular time series data with equal sampling intervals. \nEvent forecasting or prediction aims to predict the times and marks of future events given the history of past events, and it is often modeled by temporal point processes (TPP)~. \nRecently, several neural TPP models incorporate Transformers in order to improve the performance of event prediction.\nSelf-attentive Hawkes process (SAHP)~ and Transformer Hawkes process (THP)~ adopt Transformer encoder architecture to summarize the influence of historical events and compute the intensity function for event prediction. They modify the positional encoding by translating time intervals into sinusoidal functions such that the intervals between events can be utilized. Later, a more flexible named attentive neural datalog through time (A-NDTT)~ is proposed to extend SAHP/THP schemes by embedding all possible events and times with attention as well. Experiments show that it can better capture sophisticated event dependencies than existing methods.", "cites": [1841, 8503, 7522], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers effectively by connecting their approaches (SAHP, THP, A-NDTT) under the broader context of using Transformers in temporal point processes for event forecasting. It provides a critical comparison by noting that A-NDTT is a more flexible extension and performs better in capturing complex dependencies. However, the critique remains somewhat surface-level and does not deeply evaluate trade-offs or limitations across the models, limiting its abstraction to a meta-level of principles."}}
{"id": "d3800316-c535-4909-b1bd-05902491ac11", "title": "Transformers in Anomaly Detection", "level": "subsection", "subsections": [], "parent_id": "0298aab4-fa66-4b1b-ae76-3e973b044225", "prefix_titles": [["title", "Transformers in Time Series: A Survey"], ["section", "Applications of Time Series Transformers"], ["subsection", "Transformers in Anomaly Detection"]], "content": "Transformer based architecture also benefits the time series anomaly detection task with the ability to model temporal dependency, which brings high detection quality~. Besides, \nin multiple studies, including TranAD~, MT-RVAE~, and TransAnomaly~, researchers proposed to combine Transformer with neural generative models, such as VAEs~ and GANs~, for better performance in anomaly detection. We will elaborate on these models in the following part. \nTranAD~ proposes an adversarial training procedure to amplify reconstruction errors as a simple Transformer-based network tends to miss small deviation of anomaly. GAN style adversarial training procedure is designed by two Transformer encoders and two Transformer decoders to gain stability. Ablation study shows that, if Transformer-based encoder-decoder is replaced, F1 score drops nearly 11\\%, indicating the effect of Transformer architecture on time series anomaly detection.\nMT-RVAE~ and TransAnomaly~ combine VAE with Transformer, but they share different purposes. TransAnomaly combines VAE with Transformer to allow more parallelization and reduce training costs by nearly 80\\%. In MT-RVAE, a multiscale Transformer is designed to extract and integrate time-series information at different scales. It overcomes the shortcomings of traditional Transformers where only local information is extracted for sequential analysis.\nGTA~ combines Transformer with graph-based learning architecture for multivariate time series anomaly detection. \nNote that, MT-RVAE is also for multivariate time series but with few dimensions or insufficient close relationships among sequences where the graph neural network model does not work well. To deal with such challenge, MT-RVAE modifies the positional encoding module and introduces feature-learning module. Instead, GTA contains a graph convolution structure to model the influence propagation process. Similar to MT-RVAE, GTA also considers ``global'' information, yet by replacing vanilla multi-head attention with a multi-branch attention mechanism, that is, a combination of global-learned attention, vanilla multi-head attention, and neighborhood convolution.\nAnomalyTrans~ combines Transformer and Gaussian prior-Association to make anomalies more distinguishable. Sharing similar motivation as TranAD, AnomalyTrans achieves the goal in a different way. The insight is that it is harder for anomalies to build strong associations with the whole series while easier with adjacent time points compared with normality. In AnomalyTrans, prior-association and series-association are modeled simultaneously. Besides reconstruction loss, the anomaly model is optimized by the minimax strategy to constrain the prior- and series- associations for more distinguishable association discrepancy.", "cites": [5680, 7520], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple Transformer-based anomaly detection models by highlighting how they combine Transformers with VAEs, GANs, and graph-based methods. It connects these approaches under the theme of improving detection quality and efficiency through architectural adaptations. While it includes some critical evaluation (e.g., noting the instability of simple Transformers and how different models address it), the analysis is somewhat limited in depth and could provide more nuanced comparison or critique of underlying assumptions."}}
{"id": "947f0a33-ae9d-4d00-b2eb-fc884c083043", "title": "Transformers in Classification", "level": "subsection", "subsections": [], "parent_id": "0298aab4-fa66-4b1b-ae76-3e973b044225", "prefix_titles": [["title", "Transformers in Time Series: A Survey"], ["section", "Applications of Time Series Transformers"], ["subsection", "Transformers in Classification"]], "content": "Transformer is proved to be effective in various time series classification tasks due to its prominent capability in capturing long-term dependency. \nGTN~ uses a two-tower Transformer with each tower respectively working on time-step-wise attention and channel-wise attention. To merge the feature of the two towers, a learnable weighted concatenation (also known as `gating') is used. The proposed extension of Transformer achieves state-of-the-art results on 13 multivariate time series classifications.  studied the self-attention based Transformer for raw optical satellite time series classification and obtained the best results compared with recurrent and convolutional neural networks.\nRecently, TARNet~ designs Transformers to learn task-aware data reconstruction that augments classification performance, which utilizes attention score for important timestamps masking and reconstruction and brings superior performance.\nPre-trained Transformers are also investigated in classification tasks.  studies the Transformer for raw optical satellite image time series classification. The authors use self-supervised pre-trained schema because of limited labeled data.  introduced an unsupervised pre-trained framework and the model is pre-trained with proportionally masked data. The pre-trained models are then fine-tuned in downstream tasks such as classification. \n proposes to use large-scale pre-trained speech processing model for downstream time series classification problems and generates 19 competitive results on 30 popular time series classification datasets.", "cites": [1842, 1843, 1829, 1827], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual summary of several time series classification approaches using Transformers, but lacks deeper integration of ideas, comparative evaluation, or meta-level abstraction. It mentions each paper's method and results without analyzing trade-offs, trends, or limitations across works."}}
{"id": "6488b217-a92c-47d3-b9a0-551240eed616", "title": "Experimental Evaluation and Discussion", "level": "section", "subsections": ["acec7993-0d13-4a22-baf4-76ec7001e5a2"], "parent_id": "ccad4ffc-3bf5-441a-9a84-e9a6e986845e", "prefix_titles": [["title", "Transformers in Time Series: A Survey"], ["section", "Experimental Evaluation and Discussion"]], "content": "We conduct preliminary empirical studies on a typical challenging benchmark dataset ETTm2~ to analyze how Transformers work on time series data. Since classic statistical ARIMA/ETS~ models and basic RNN/CNN models perform inferior to Transformers in this dataset as shown in ~, we focus on \npopular time series Transformers with different configurations in the experiments.", "cites": [7069, 1479], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section is largely descriptive, focusing on the setup of the empirical study and referencing two specific papers without integrating their ideas into a broader narrative. It lacks critical evaluation of the cited works or comparison of their approaches, and does not abstract key patterns or principles from the cited papers."}}
{"id": "dc42385e-eb07-4592-bcdd-36f7d95a8f37", "title": "Model Size Analysis", "level": "subsubsection", "subsections": [], "parent_id": "acec7993-0d13-4a22-baf4-76ec7001e5a2", "prefix_titles": [["title", "Transformers in Time Series: A Survey"], ["section", "Experimental Evaluation and Discussion"], ["subsubsection", "Robustness Analysis"], ["subsubsection", "Model Size Analysis"]], "content": "Before being introduced into the field of time series prediction, Transformer has shown dominant performance in NLP and CV communities~. One of the key advantages Transformer holds in these fields is being able to increase prediction power through increasing model size. Usually, the model capacity is controlled by Transformer's layer number, which is commonly set between 12 to 128. \nYet as shown in the experiments of Table \\ref{tab:model_size}, when we compare the prediction result with different Transformer models with various numbers of layers, the Transformer with 3 to 6 layers often achieves better results. It raises a question about how to design a proper Transformer architecture with deeper layers to increase the model's capacity and achieve better forecasting performance.", "cites": [7, 7518, 38], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes basic information from the cited papers about the scalability and capacity of Transformer models in NLP and CV, and contrasts this with the experimental findings in time series modeling. It offers a moderate level of critical analysis by pointing out a discrepancy in model depth effectiveness. However, the abstraction is limited, as it focuses more on observations from specific experiments rather than deriving broader principles or frameworks."}}
{"id": "3561f963-6281-4139-992a-949ec543b038", "title": "Seasonal-Trend Decomposition Analysis", "level": "subsubsection", "subsections": [], "parent_id": "acec7993-0d13-4a22-baf4-76ec7001e5a2", "prefix_titles": [["title", "Transformers in Time Series: A Survey"], ["section", "Experimental Evaluation and Discussion"], ["subsubsection", "Robustness Analysis"], ["subsubsection", "Seasonal-Trend Decomposition Analysis"]], "content": "In recent studies, researchers~ begin to realize that the seasonal-trend decomposition~ is a crucial part of Transformer's performance in time series forecasting. As an experiment shown in Table \\ref{tab:decompose}, we adopt a simple moving average seasonal-trend decomposition architecture proposed in  to test various attention modules. It can be seen that the simple seasonal-trend decomposition model can significantly boost model's performance by 50 \\% to 80\\%. It is a unique block and such performance boosting through decomposition seems a consistent phenomenon in time series forecasting for Transformer's application, which is worth further investigating for more advanced and carefully designed time series decomposition schemes.", "cites": [7521, 7069, 7519], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from three papers that incorporate seasonal-trend decomposition with Transformers for time series forecasting, highlighting the consistent performance improvement across these models. It begins to generalize the concept as a unique and beneficial block for time series modeling, offering some abstraction. However, the critical analysis is limited, as it does not deeply evaluate the strengths or weaknesses of the decomposition methods or compare them in a structured manner."}}
{"id": "fbd212e0-5237-47b1-89e2-a7f28947bb11", "title": "Inductive Biases for Time Series Transformers", "level": "subsection", "subsections": [], "parent_id": "cf1d9ede-935f-420c-bdc5-ac20d59129b4", "prefix_titles": [["title", "Transformers in Time Series: A Survey"], ["section", "Future Research Opportunities"], ["subsection", "Inductive Biases for Time Series Transformers"]], "content": "Vanilla Transformer does not make any assumptions about data patterns and characteristics. Although it is a general and universal network for modeling long-range dependencies, it also comes with a price, i.e., lots of data are needed to train Transformer to improve the generalization and avoid data overfitting.\nOne of the key features of time series data is its seasonal/periodic and trend patterns~. Some recent studies have shown that incorporating series periodicity~ or frequency processing~ into time series Transformer can enhance performance significantly. Moreover, it is interesting that some studies adopt a seemly opposite inductive bias, but both achieve good numerical improvement:  removes the cross-channel dependency by utilizing a channel-independent attention module, while an interesting work  improves its experimental performance by utilizing cross-dimension dependency with a two-stage attention mechanism. Clearly, we have noise and signals in such a cross-channel learning paradigm, but a clever way to utilize such inductive bias to suppress the noise and extract the signal is still desired. \nThus, one future direction is to consider more effective ways to induce inductive biases into Transformers based on the understanding of time series data and characteristics of specific tasks.", "cites": [1835, 7069, 7519], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes ideas from three cited papers by highlighting different approaches to introducing inductive biases into time series Transformers, such as channel-independence and cross-dimension dependency. It provides a critical view by noting the trade-offs between noise and signal in cross-channel learning and identifying the need for more effective bias-induction methods. The abstraction is moderate, as it identifies patterns in inductive bias strategies but does not offer a fully meta-level or novel framework."}}
{"id": "f23b8e99-bac3-4ccf-899d-2bdf7fa8ff69", "title": "Transformers and GNN for Time Series", "level": "subsection", "subsections": [], "parent_id": "cf1d9ede-935f-420c-bdc5-ac20d59129b4", "prefix_titles": [["title", "Transformers in Time Series: A Survey"], ["section", "Future Research Opportunities"], ["subsection", "Transformers and GNN for Time Series"]], "content": "Multivariate and spatio-temporal time series are becoming increasingly common in applications, calling for additional techniques to handle high dimensionality, especially the ability to capture the underlying relationships among dimensions. \nIntroducing graph neural networks (GNNs) is a natural way to model spatial dependency or relationships among dimensions. Recently, several studies have demonstrated that the combination of GNN and Transformers/attentions could bring not only significant performance improvements like in traffic forecasting~ and multi-modal forecasting~, but also better understanding of the spatio-temporal dynamics and latent causality. \nIt is an important future direction to combine Transformers and GNNs for effectively spatial-temporal modeling in time series.", "cites": [1838, 1830], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the use of GNNs and Transformers for handling multivariate and spatio-temporal time series, drawing from two relevant papers. It abstracts the idea that combining these models can enhance understanding of spatial relationships and improve performance. However, it lacks deeper critical analysis of the cited works, such as evaluating their specific limitations or trade-offs, and the synthesis remains at a general level without proposing a novel framework."}}
{"id": "34670a41-21ee-4cd1-8999-aa5d6a670490", "title": "Pre-trained Transformers for Time Series", "level": "subsection", "subsections": [], "parent_id": "cf1d9ede-935f-420c-bdc5-ac20d59129b4", "prefix_titles": [["title", "Transformers in Time Series: A Survey"], ["section", "Future Research Opportunities"], ["subsection", "Pre-trained Transformers for Time Series"]], "content": "Large-scale pre-trained Transformer models have significantly boosted the performance for various tasks in NLP~ and CV~. \nHowever, there are limited works on pre-trained Transformers for time series, and existing studies mainly focus on time series classification~.\nTherefore, how to develop appropriate pre-trained Transformer models for different tasks in time series remains to be examined in the future.", "cites": [7070, 679, 1829, 1827, 7], "cite_extract_rate": 1.0, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section briefly introduces the idea of pre-trained Transformers in time series, referencing papers from NLP, CV, and a few relevant time series works. It makes a basic synthesis by drawing a parallel between pre-training in NLP/CV and time series, and identifies a research gap by noting that most existing work focuses on classification. However, it lacks deeper integration of ideas and more nuanced critique or comparison of the cited papers."}}
{"id": "df2c4b37-6d22-463f-8123-cfdfce8f91ab", "title": "Transformers with Architecture Level Variants", "level": "subsection", "subsections": [], "parent_id": "cf1d9ede-935f-420c-bdc5-ac20d59129b4", "prefix_titles": [["title", "Transformers in Time Series: A Survey"], ["section", "Future Research Opportunities"], ["subsection", "Transformers with Architecture Level Variants"]], "content": "Most developed Transformer models for time series maintain the vanilla Transformer's architecture with modifications mainly in the attention module. We might borrow the idea from Transformer variants in NLP and CV which also have architecture-level model designs to fit different purposes, such as lightweight~, cross-block connectivity~, adaptive computation time~, and recurrence~. Therefore, one future direction is to consider more architecture-level designs for Transformers specifically optimized for time series data and tasks.", "cites": [7370, 7071, 7269, 1457, 1468], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key architectural innovations from NLP and CV papers (e.g., lightweight design, recurrence) and connects them to potential improvements in time series modeling. While it identifies a broader trend toward architecture-level adaptation, it lacks deeper critical evaluation or a novel framework that would elevate it to high insight level."}}
{"id": "2017fd01-e60c-4d0d-a17d-47c014015009", "title": "Transformers with NAS for Time Series", "level": "subsection", "subsections": [], "parent_id": "cf1d9ede-935f-420c-bdc5-ac20d59129b4", "prefix_titles": [["title", "Transformers in Time Series: A Survey"], ["section", "Future Research Opportunities"], ["subsection", "Transformers with NAS for Time Series"]], "content": "Hyper-parameters, such as embedding dimension and the number of heads/layers, can largely affect the performance of Transformers. Manual configuring these hyper-parameters is time-consuming and often results in suboptimal performance. \nAutoML technique like Neural architecture search (NAS)~ has been a popular technique for discovering effective deep neural architectures, and automating Transformer design using NAS in NLP and CV can be found in recent studies~.\nFor industry-scale time series data which can be of both high dimension and long length, automatically discovering both memory- and computational-efficient Transformer architectures is of practical importance, making it an important future direction for time series Transformers.", "cites": [7523, 1461, 1830], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the role of NAS in Transformer design by citing relevant works from vision and general sequence modeling, highlighting its potential for time series. It abstracts the general problem of hyper-parameter tuning and proposes a direction for automation in the time series domain. However, it lacks deeper critical analysis of these papers' limitations or specific challenges in adapting NAS to time series, keeping its insight at a moderate level."}}
