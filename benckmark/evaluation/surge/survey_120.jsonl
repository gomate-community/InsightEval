{"id": "2ca60ad7-b1a8-4a6c-8e91-ae361d2993c4", "title": "Introduction", "level": "section", "subsections": ["54ccf8f4-8f81-43ab-ac06-0326b1642776", "e2cbaac2-fe2b-4ece-bd59-5c80205f2c6b"], "parent_id": "7aedf107-610a-4e90-9d9d-b0a1b2dcb03c", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Introduction"]], "content": "\\IEEEPARstart{D}{riven} by the recent development and prevalence of computing power,  \nInternet of Things (IoT) systems, and big data, a booming era of AI has emerged, covering a wide spectrum of applications including natural language processing , speech recognition , computer vision , and robotics . Owing to these breakthroughs, AI has achieved unprecedented improvements in multiple sectors of academia, industry, and daily services in order to improve the humans' productivity and lifestyle. As an example, multiple intelligent IoT applications have been designed such as self-driving cars, disease mapping services, smart home appliances, manufacturing robots, and surveillance systems. In this context, studies estimate that AI will have higher impact on the global Gross Domestic Product (GDP) by 2030, accounting for \\$ 13 trillion additional gains compared to 2018 . \n\\begin{figure*}[!h]\n\\centering\n\t\\frame{\\includegraphics[scale=0.55]{Figures/introduction3.pdf}}\n\t\\caption{A scenario illustrating examples of pervasive AI techniques.}\n\t\\label{intro}\n\\end{figure*}\nThe popularity of AI is also related to the abundance of storage and computing devices, ranging from server clusters in the cloud to personal phones and computers, further, to  wearables and IoT units. In fact, the unprecedented amount of data generated by the massive number of ubiquitous devices opens up an attractive opportunity to provide intelligent IoT services that can transform all aspects of our modern life and fuel the continuous advancement of AI. Statistics forecast that, by 2025, the number of devices connected to the internet will reach more than 500 billion  owing to the maturity of their sensing capabilities and affordable prices.  Furthermore, reports revealed that these devices will generate enormous data reaching more than 79 ZB by 2025 and will increase the economic gains up to 11 trillion by the same year . \nWith the rapid evolution of AI and the enormous bulks of data generated by pervasive devices,  conventional wisdom resorts to  centralized cloud servers for analytics. In fact, the high performance of AI systems applied to multiple fields comes at the expense of a huge memory requirement and an intensive computational load to perform both training and inference phases, which requires powerful servers.\n\\begin{comment} More specifically, training an intelligent model is computationally expensive because of the large number of parameters, reaching millions for deep networks, that need to be repetitively fine-tuned over hundreds of iterations. Similarly, the inference phase is computationally intensive due to the high dimension of raw data (e.g., high resolution images) and millions of tasks (e.g., multiplications and max-pooling) in deep networks . To this end, the resource consumption has been considered as an important parameter to evaluate the performance of AI models.\n\\end{comment}\nHowever, this approach is no longer sustainable as it introduces several challenges: (1) the appearance of a new breed of services and the advent of delay-sensitive technologies spanning from self-driving cars to Virtual and Augmented Reality (VR/AR), make the cloud-approaches inadequate for AI tasks due to the long transmission delays. More precisely, the aforementioned applications are real-time and cannot allow any additional latency or connectivity loss. For example, autonomous cars sending camera frames to remote servers need to receive prompt inferences to detect potential obstacles and apply brakes . \nSending data to cloud servers may not satisfy the latency requirements of the real-time applications. Particularly, experiments in  demonstrated that executing a computer vision task on a camera frame  offloaded to an Amazon server takes more than 200 ms. (2) In addition to latency, privacy presents a major concern for cloud-based AI approaches. In fact, end-users are typically reluctant to upload their private data to cloud servers (e.g., photos or audios), as they can be highly exposed to cyber risks, malicious attacks, or disclosures. Among the most popular breaches reported in the 21st century, we can cite the Marriott attack revealed in 2018 and affecting 500 million customers and Equifax breach recorded in 2017 and affecting 147 million users . (3) A tremendous number of AI tasks, involving unstructured and bandwidth-intensive data, needs to be transferred across the Wide Area Network (WAN), which poses huge pressure on the network infrastructure having varying quality. (4) In the same context, offloading the data to remote servers encounters also scalability issues, as the access to the cloud can become a bottleneck when the number of data sources increases, particularly if some devices import irrelevant and noisy inputs. (5) Nowadays, Explainable AI (XAI)  has become extremely popular, aiming to enhance the transparency of learning and detect prediction errors. However, consigning AI tasks to the cloud makes the whole process a black-box vis-a-vis the end-user, and prevents model decomposability and debugging. \nPushing AI to the network edge has been introduced as a viable solution to face latency, privacy, and scalability challenges described earlier. As such, the large amount of computational tasks can be handled by edge devices without exchanging the related data with the remote servers, which guarantees agile IoT services owing to the physical proximity of computing devices to the data sources . In the case when the AI tasks can only be executed at the cloud datacenters, the edge devices  can be used to pre-process the data and polish it from noisy inputs in order to reduce the transmission load . Furthermore, the edge network can play the role of a firewall that enhances the privacy by discarding sensitive information prior to data transfer. A variety of edge devices can be candidate for executing different AI tasks with different computation requirements, ranging from edge servers provisioned with GPUs, to smart-phones with strong processors and even small IoT wearable with Raspberry Pi computing. These edge devices have been continuously improving to fit for deep AI models.\nIn spite of this technological advancement, a large range of pervasive devices used in countless fields of our daily life still suffers from limited power and memory, such as smart home IoT appliances, sensors, and gaming gears.\nGiven the limited resources of edge-devices, computing the full AI model in one device may be infeasible, particularly when the task requires high computational load, e.g., Deep Neural Networks (DNN). A promising solution is to opt for pervasive computing, where different data storage and processing capacities existing everywhere (e.g., distributed cloud datacenters, edge servers, and IoT devices.) cooperate to accomplish AI tasks that need large memory and intensive computation. This marriage of pervasive computing and AI has given rise to a new research area, which garnered considerable attention from both academia and industry. The new research area comprises different concepts (e.g., federated learning, active learning, etc.) that suggest to distribute AI tasks into pervasive devices for multiple objectives. In this paper, we propose to gather all existing concepts having different terminologies under the same umbrella that we named  \\textit{“Pervasive AI\"}. Indeed, we define the pervasive AI as \\textit{“The intelligent and efficient distribution of AI tasks and models over/amongst any types of devices with heterogeneous capabilities in order to execute sophisticated global missions”}.  \nThe pervasive AI concepts are firstly introduced to solve the described challenges of centralized approaches (e.g., on-cloud or on-device computation):\n(1) To preserve privacy and reduce the huge overhead of data collection and the complexity of training an astronomical dataset, \\textit{Federated Learning (FL)} is proposed, where raw data are stored in their source entities and the model is trained collaboratively. Particularly, each entity computes a local model using its collected data, then sends the results to a fusion server to aggregate the global model. Such an approach suggests the distribution of data and the assembly of the trained AI models. (2) To cope with the limited resources provided by edge devices and simultaneously avoid latency overheads caused by cloud transmissions, the inference task is distributed among ubiquitous devices located at the proximity of the source. The basic idea is to divide the trained model into segments and subsequently, each segment is assigned to a participant. Each participant shares the output to the next one until generating the final prediction.  In other words, the \\textit{Pervasive Inference} covers the distribution of the established model resulting from the training phase. (3) Some AI techniques are inherently distributed such as Multi-Agent Reinforcement Learning (MARL) or Multi-agent Bandits (MAB), where agents cooperate to build and improve a policy in real-time enabling them to take on-the-fly decisions/actions based on the environment status. In this case, the distribution covers the online creation and update of the Reinforcement Learning (RL) policy. A scenario illustrating some pervasive AI techniques is presented in Fig. \\ref{intro}.\nThe pervasive AI exploits the on-device computation capacities to collaboratively achieve learning tasks. This requires \na careful scheduling to wisely use the available resources without resorting to remote computing. Yet, some intensive AI tasks can only be performed by involving the cloud servers, which results in higher communication costs. Therefore, leveraging the small and ubiquitous resources and managing the enormous communication overheads present a major bottleneck for the pervasive AI.", "cites": [3409, 97, 514, 301, 7008, 8655, 859, 3410], "cite_extract_rate": 0.5, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple cited papers to build a narrative around the limitations of centralized AI in IoT and the emergence of Pervasive AI as a solution. It introduces the concept of Pervasive AI and connects it with challenges such as latency, privacy, and scalability, drawing from relevant research. However, while it presents an analytical perspective on these issues, it lacks deeper comparative or critical analysis of the cited approaches and does not fully abstract beyond individual systems to present a more generalized or meta-level understanding of the field."}}
{"id": "8ccdc4eb-a5b6-4d13-8e48-3be24f209874", "title": "Data center and cloud servers", "level": "subsubsection", "subsections": [], "parent_id": "fa09ade2-b90a-43ea-b14f-f6ce789482a1", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Fundamentals of pervasive computing"], ["subsection", "Ubiquitous participants"], ["subsubsection", "Data center and cloud servers"]], "content": "Cloud computing  is defined as delivering on-demand services from storage, management, advertising, and computation to artificial intelligence and natural language processing, following different pricing models, such as pay-as-you-go  and subscription-based billing.\nHence, instead of owning computing servers, companies, operators, and end-users can exploit the high-performance facilities offered by the cloud service provider. In this way, they can benefit from better computational capacities, while reducing the cost of owning and maintaining a computation infrastructure, and paying only for their requested services. Cloud computing underpins a broad number of services, including data storage, cloud back-up of photos, video streaming services, and online gaming.", "cites": [3411], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a general description of cloud computing and its role in pervasive AI for IoT but does not effectively synthesize or integrate the cited paper. The paper on QoE-aware resource allocation is only marginally connected to the content, with no explicit analysis or comparison of its approach to other works. The section lacks critical evaluation and broader abstraction, focusing instead on high-level, factual explanations."}}
{"id": "b9c46377-8cf8-458d-b66d-5ecaa53b0e15", "title": "Architecture", "level": "subsubsection", "subsections": [], "parent_id": "f5e2a266-16ab-494b-85d0-e254d34de1e8", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Fundamentals of pervasive computing"], ["subsection", "Architecture and intersection with AI"], ["subsubsection", "Architecture"]], "content": "\\begin{figure}[H]\n\\centering\n\t\\includegraphics[scale=0.645]{Figures/UbiLayers.pdf}\n\t\\caption{Pervasive architecture.}\n\t\\label{Architecture}\n\\end{figure}\nFig. \\ref{Architecture} illustrates the hierarchical architecture of a pervasive system , which is composed of three layers:\n\\begin{itemize}\n    \\item Data source layer: the data is collected from different monitored sources generating information of physical world or human activities, multimedia data such as images and audio, and social media information.\n    \\item Data management layer: this layer involves the storage and integration of heterogeneous data incoming from pervasive sources, the cleaning and pre-processing that tailor the context of the system, and the data analytics that convert the raw information into useful and personalized insights using multiple approaches, such as business and artificial intelligence.\n    \\item Application layer: to this end, the insights generated from the previous layer are used to offer multiple intelligent applications, such as health advisor and smart home applications. \n\\end{itemize}\nIn our paper, we focus only on the data management layer, specifically the data analytics using artificial intelligence. The data source layer is thoroughly discussed in , whereas the application layer can be found in .", "cites": [3410], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of a three-layer pervasive computing architecture and mentions the focus of the paper on the data management layer. It cites one paper but does not engage in meaningful synthesis or connect ideas across sources. There is no critical evaluation of the cited work or broader abstraction beyond the specific architecture described."}}
{"id": "3712d67e-e696-4951-a604-d1fa86a3fe72", "title": "Fundamentals of Artificial Intelligence", "level": "section", "subsections": ["5df83101-053a-4cfe-9d04-c09b58e36f6b", "9342660a-236d-4249-92d7-89fa948e8ef1", "8e6febe5-4bb9-43f7-b364-4dc285c93213", "b85221a3-fb27-44cf-b1c0-77f43b2c6992", "6caa0663-4e5f-484e-83a1-18c728b2a299"], "parent_id": "7aedf107-610a-4e90-9d9d-b0a1b2dcb03c", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Fundamentals of Artificial Intelligence"]], "content": "\\label{AI}\nSince approaches and techniques reviewed in this survey rely on artificial intelligence and deep neural networks, we start first by providing a brief background of deep learning. A deeper and detailed review of AI can be found in the reference book in .", "cites": [166], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.0}, "insight_level": "low", "analysis": "The section provides minimal synthesis by only briefly mentioning the concept of deep learning and its relation to AI and IoT, without integrating ideas from multiple sources or creating a coherent narrative. There is no critical analysis of the cited paper or identification of broader patterns or principles; it simply introduces the topic with a reference to a general review."}}
{"id": "1bf807f0-03f6-4024-96db-b31ce1b91bb8", "title": "Deep learning and Deep Neural Networks", "level": "subsubsection", "subsections": ["010c4789-ae88-4520-ae39-2e919e67a236", "af0fb1b2-5a8e-4998-a900-bf8d2e43b2f2", "b14650c1-0115-4222-87d4-83d1e8ac5490", "aa5955bb-48c6-4f39-a8a9-642b9947d3a2"], "parent_id": "5df83101-053a-4cfe-9d04-c09b58e36f6b", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Fundamentals of Artificial Intelligence"], ["subsection", "Background"], ["subsubsection", "Deep learning and Deep Neural Networks"]], "content": "In the following, we briefly present an overview of the most common deep learning networks. \nNeural networks consist of a first input layer, one or multiple hidden layers, and a last output layer, as shown in Fig. \\ref{DNN_types}. When the neural network contains a high number of sequential layers, it can be called Deep Neural Network (DNN). The DNN layers include smaller units, namely neurons.\nMost commonly, the output of one layer is the input of the next layer and the output of the final layer is either a classification or a feature. The correctness of the prediction is assessed by the loss function that calculates the error between the true and predicted values. \nThe DNN networks have various structures. Hence, we introduce the fundamentals of the most known types as follows: \n\\begin{comment}\n\\begin{table*}[]\n\\footnotesize\n\\tabcolsep=0.08cm\n\\caption{Parameters comparison of state-of-the-art DNNs trained on ImageNet , in terms of flops\\protect\\footnotemark.\\\\ \nMACC:  Multiply-ACCumulate operations.\n}\n\\label{Macc}\n\\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|}\n\\hline\n\\textbf{Model} & \\textbf{Comp} & \\textbf{Add} & \\textbf{Div} & \\textbf{MACC} & \\textbf{Activations} & \\textbf{params} & \\begin{tabular}[c]{@{}l@{}}\\textbf{size} \\\\\\textbf{(Mb)}\\end{tabular} & \\textbf{pros} & \\textbf{cons} \\\\ \\hline\nVGG 16 & 196.85 M & 10 K & 10 K & 154.7 G & 288.03 M & 138.36 M & 512.2 & \\begin{tabular}[c]{@{}l@{}}- Spatial exploitation.\\\\ - Simple and homogeneous\\\\ topology.\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}- Computationally expensive\\\\ fully connected layers.\\end{tabular} \\\\ \\hline\nAlexNet & 17.69 M & 4.78 M & 9.55 M & 7.27 G & 20.81 M & 60.97 M & 217 & \\begin{tabular}[c]{@{}l@{}}- Spatial exploitation.\\\\ - Low, medium, and high\\\\ feature extraction.\\\\ - Introduces regularization \\\\ in CNN.\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}- Inactive neurons in the first\\\\ layers.\\\\ - Large filter size that causes\\\\ artifacts aliasing in the output\\\\ feature maps.\\end{tabular} \\\\ \\hline\nGoogleNet & 161.07 M & 8.83 M & 16.64 M & 16.04 G & 102.19 M & 7 M & 40 & \\begin{tabular}[c]{@{}l@{}}- Spatial exploitation.\\\\ - Multi-scale layers.\\\\ - Reduces number of  params by\\\\    using bottleneck and average\\\\    pooling layers.\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}- Potential lose of important\\\\ information because of \\\\ representational bottleneck.\\end{tabular} \\\\ \\hline\n\\begin{tabular}[c]{@{}l@{}}ResNet 50 \\\\  \\\\\\\\ \\end{tabular} & 10.89 M & 16.21 M & 10.59 M & 3.87 G & 46.72 M & 25.56 M & 97.7 & \\multirow{2}{*}{\\begin{tabular}[c]{@{}l@{}}- Depth and Multi-path.\\\\ - Introduce residual learning.\\\\ - Solve the vanishing gradient\\\\ problem.\\end{tabular}} & \\multirow{2}{*}{\\begin{tabular}[c]{@{}l@{}}- Complex architecture.\\\\ - Multiple layers have no \\\\ contribution for the inference.\\\\ - Potential re-learning of \\\\ redundant feature maps.\\end{tabular}} \\\\ \\cline{1-8}\n\\begin{tabular}[c]{@{}l@{}}ResNet 152 \\\\  \\\\ \\\\\\end{tabular}  & 22.33 M & 35.27 M & 22.03 M & 11.3 G & 100.11 M & 60.19 M & 230 &  &  \\\\ \\hline\nSqueezeNet  & 9.67 M & 226 K & 1.51 M & 861.34 M & 12.58 M & 1.25 M & 4.7 & - \\begin{tabular}[c]{@{}l@{}} Squeezes non-important \\\\ features. \\end{tabular}& - Lower accuracy. \\\\ \\hline\n\\end{tabular}\n\\end{table*}\n\\end{comment}", "cites": [514, 97, 7430, 895, 305], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "comparative", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive and comparative overview of various DNN architectures by listing their computational parameters and characteristics, primarily derived from the cited papers. It includes some synthesis by grouping common themes such as 'spatial exploitation,' but lacks deeper integration or novel framing of ideas. Critical analysis is minimal, with only surface-level mentions of drawbacks like 'computationally expensive' or 'lower accuracy,' without deeper evaluation of trade-offs or implications."}}
{"id": "af0fb1b2-5a8e-4998-a900-bf8d2e43b2f2", "title": "Convolutional Neural Networks (CNN)", "level": "paragraph", "subsections": [], "parent_id": "1bf807f0-03f6-4024-96db-b31ce1b91bb8", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Fundamentals of Artificial Intelligence"], ["subsection", "Background"], ["subsubsection", "Deep learning and Deep Neural Networks"], ["paragraph", "Convolutional Neural Networks (CNN)"]], "content": "\\label{CNN}\nProcessing vision-based tasks (e.g., image data), using MLP, potentially requires a deep model with a huge number of perceptrons, as for each data pixel a perceptron is assigned, which makes the network  hard to train and scale. One of the successors of MLP is CNN that is introduced to solve this problem by defining additional pre-processing layers, (i.e., convolutional (conv) and pooling layers), as shown in Fig. \\ref{DNN_types} (b). \nFurthermore, the convolutional layer includes a set of learning parameters, namely filters that have the same number of channels as the data feature maps with smaller dimensions. Each filter channel passes through the length and width of the corresponding input feature map and calculates the inner product to the data. The summation of all the outputs produces one feature map. Finally, the number of output feature maps equals the number of filters, as illustrated in Fig. \\ref{Conv}. The main difference between the Fc and the conv layers is that each neuron in Fc networks is connected to the entire input, which is not the case of CNN that is connected to only a subset of the input. The second basic component of the CNN network is the pooling task, which has an objective to reduce the spatial size of the input feature maps and minimize the computation time. \nA milestone for CNN applied to computer vision problems is the design of AlexNet  and VGG .\n\\begin{figure}[h]\n\\centering\n\t\\includegraphics[scale=0.65]{Figures/Conv_2.pdf}\n\t\\caption{Convolutional task.}\n\t\\label{Conv}\n\\end{figure}", "cites": [514], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of CNNs, including their structure and function. It mentions specific models (AlexNet, VGG) and cites one relevant paper (VGG), but does not synthesize ideas across sources or offer critical evaluation of the cited works. The explanation remains at a surface level without abstracting broader principles or trends in deep learning for vision tasks."}}
{"id": "b14650c1-0115-4222-87d4-83d1e8ac5490", "title": "Deep Residual Networks", "level": "paragraph", "subsections": [], "parent_id": "1bf807f0-03f6-4024-96db-b31ce1b91bb8", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Fundamentals of Artificial Intelligence"], ["subsection", "Background"], ["subsubsection", "Deep learning and Deep Neural Networks"], ["paragraph", "Deep Residual Networks"]], "content": "\\label{DRN}\nFollowing the victory of AlexNet and VGG, the deep residual networks have achieved a new breakthrough in the computer vision challenges during the recent years. Particularly, the residual networks paved the way for the deep learning community to train up to hundreds and even thousands of layers, while achieving high performance.\nResNet  is the-state-of-the-art variant of the residual network. This model uses the so called shortcut/skip connections that skip multiple nodes and feed the intermediate output to a destination layer (see Fig. \\ref{DNN_types} (c)), which serves as a memory to the model. A similar idea is applied in the Long Short Term  Memory (LSTM) networks , where a forget gate is added to control the information that will be fed to the next time step. LSTM belongs to the Recurrent Neural Networks (RNN) family.", "cites": [97], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of deep residual networks and their significance in computer vision. It mentions the key innovation of shortcut connections and loosely connects the idea to LSTMs, but does not deeply integrate or synthesize multiple sources. There is no critical evaluation or broader abstraction beyond the specific technique and its applications."}}
{"id": "aa5955bb-48c6-4f39-a8a9-642b9947d3a2", "title": "Randomly Wired Networks", "level": "paragraph", "subsections": [], "parent_id": "1bf807f0-03f6-4024-96db-b31ce1b91bb8", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Fundamentals of Artificial Intelligence"], ["subsection", "Background"], ["subsubsection", "Deep learning and Deep Neural Networks"], ["paragraph", "Randomly Wired Networks"]], "content": "The aforementioned networks focus more on connecting operations such as convolutional tasks through wise and sequential paths. Unlike previous DNNs, the randomly wired networks  arbitrarily connect the same operations throughout the sequential micro-architectures, as shown in Fig. \\ref{DNN_types} (d). Still, some decisions are required to design a random DNN, such as the number of stages to down-sample feature maps using Maxpooling and the number of nodes to deploy in each stage. The edge of the randomly wired networks over the other models is that the training is faster, the number of weights is reduced and the memory footprint is optimized.\\\\\nFig. \\ref{DNN_types} presents the NN structures introduced in this section and serving to understand the following sections. Other state-of-the-art structures achieved unprecedented performance in multiple deep learning applications , including Recurrent Neural Networks (RNNs) , Auto-Encoders (AEs) , and Generative Adversarial Networks (GANs) ; however, detailed overview of all models falls outside the scope of this paper.", "cites": [3413, 3412, 7217], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a brief description of randomly wired networks, comparing them to traditional DNNs, and mentions performance advantages. However, it lacks deeper synthesis with the cited works, especially with Paper 1, and does not critically assess the limitations or trade-offs of these networks. The abstraction level is minimal, as it does not generalize these ideas into broader trends or principles of AI for IoT."}}
{"id": "769a0a3f-bd0b-40db-9bd3-432dcab1bb75", "title": "Markov Decision Process (MDP)-based Learning", "level": "paragraph", "subsections": [], "parent_id": "12ea514f-8733-4c19-9346-9ed4ba7a8c74", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Fundamentals of Artificial Intelligence"], ["subsection", "Background"], ["subsubsection", "Reinforcement Learning (RL)"], ["paragraph", "Markov Decision Process (MDP)-based Learning"]], "content": "This RL concept is based on learning how to map MDP's states to actions in order to maximize the long-term reward signal. The  RL-agent is not apprised  which  action  to  choose; instead, it  discovers  the actions that  achieve  the  highest  reward by trying  different combinations and receiving immediate gains and penalties, which can be modeled as MDP process. Different from the bandit learning, the RL chosen  action does not impact only the direct reward, but  also  all subsequent situations and related  rewards.  \nDeep Reinforcement Learning (DRL)  combines reinforcement learning and the deep learning, as illustrated in Fig. \\ref{DRL}. The DRL is well-suited, and even indispensable, when the environment is highly dynamic and dimensional and the number of states is large or continuous. \n\\begin{figure}[!h]\n\\centering\n\t\\includegraphics[scale=0.6]{Figures/DRL.pdf}\n\t\\caption{Deep Reinforcement Learning (DRL) design.}\n\t\\label{DRL}\n\\end{figure}\nVariants of DRL include the deep policy gradient RL , the Deep Q-Networks (DQN) , Distributed Proximal Policy Optimization (DPPO) , and Asynchronous Advantage Actor-Critic .", "cites": [620, 1390, 2219], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of MDP-based learning and DRL, mentioning key concepts and algorithms. It integrates minimal context from the cited papers, primarily referencing them to name specific DRL variants without elaborating on their contributions or interconnections. There is no critical evaluation or abstraction to broader principles, limiting its insight quality."}}
{"id": "d648b147-e10b-4bf1-bfd9-553b4585cffa", "title": "Computation and memory footprint", "level": "subsubsection", "subsections": [], "parent_id": "9342660a-236d-4249-92d7-89fa948e8ef1", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Fundamentals of Artificial Intelligence"], ["subsection", "Performance metrics"], ["subsubsection", "Computation and memory footprint"]], "content": "To perform DNN training/inference, significant cycles are executed for memory data transfer to/from computational array, which makes it a highly intensive and challenging task. For example, VGG 16 and AlexNet require respectively 512 MB and 217 MB of memory to store more than 138 M and 60 M of weights in addition to the model complexity or Multiply-ACCumulate operations (MACC) which is equal to 154.7 G and 7.27 G  . Such amounts of memory and computational tasks, typically measured in Megabyte and number of multiplications respectively, are infeasible to be executed in power and resource constrained devices with a real-time response.", "cites": [3414], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides basic factual information about the computational and memory demands of DNN models like VGG 16 and AlexNet, citing one paper that highlights the inefficiencies of complex architectures. However, it does not critically evaluate or compare the cited work with other approaches, nor does it synthesize broader patterns or abstract principles. The narrative is limited to a descriptive account without deeper analytical engagement."}}
{"id": "2594c2d2-e21b-4f8a-b476-5aacfed5fb63", "title": "Privacy", "level": "subsubsection", "subsections": [], "parent_id": "9342660a-236d-4249-92d7-89fa948e8ef1", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Fundamentals of Artificial Intelligence"], ["subsection", "Performance metrics"], ["subsubsection", "Privacy"]], "content": "IoT devices produce and offload a massive amount of data every second, which can result in serious  privacy vulnerabilities and security attacks such as  white-box attacks , data poisoning , and membership attacks . Guaranteeing the robustness and privacy of the DNN system has become a primary concern for the deep learning community. The traditional wise resorts to data encryption, pre-processing, and watermarking. Yet, all these solutions can be neutralized using model stealing attacks. Hence, more sophisticated defenses need to be designed to secure the DNN training and execution, through data distribution. The robustness of a privacy mechanism is judged by its ability to protect the data from attacks while maintaining the accuracy performance.\nTo design an efficient deep learning network or select the adequate one for the targeted application, a large number of hyperparameters need to be considered. Therefore, understanding the trade-off between these parameters (e.g., latency, accuracy, energy, privacy, and memory.) is essential before designing the model.  Recently, automated machine learning frameworks responsible for DNN selection and parameters tuning, have been introduced, such as Talos .", "cites": [3415, 603], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section synthesizes information from two papers by linking the privacy concerns in IoT (data poisoning and membership inference attacks) to the broader challenge of securing DNN systems. It also connects these threats to the need for more sophisticated privacy mechanisms. However, the critical analysis is limited, as it does not deeply evaluate the approaches or limitations in the cited works. Abstraction is moderate, as it generalizes the privacy issue but stops short of identifying overarching principles or a meta-framework."}}
{"id": "8e6febe5-4bb9-43f7-b364-4dc285c93213", "title": "Pervasive frameworks for AI", "level": "subsection", "subsections": [], "parent_id": "3712d67e-e696-4951-a604-d1fa86a3fe72", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Fundamentals of Artificial Intelligence"], ["subsection", "Pervasive frameworks for AI"]], "content": "Several hardware and software libraries are publicly available for pervasive devices, particularly resource-limited ones, to enable DNN training and inference. As a first example, Google TensorFlow  is an open source deep learning framework released in 2015 to execute DNN tasks on heterogeneous distributed systems based on their estimated computational and communication capacities, which was optimized later to be adequate for resource constrained devices (e.g., Raspberry Pi) and GPU execution.\nAnother lightweight deep learning framework developed by Facebook is Caffe2  that provides a straightforward way to experiment heterogeneous deep learning models on low-power devices.\nCore ML  and DeepLearningKit  are two machine learning frameworks commercialized by Apple to support pre-trained models on iPhone/iPad devices. More specifically, Core ML was designed to leverage the CPU/GPU endowed with the end-device for deep learning applications such as natural language and image processing, while DeepLearningKit supports more complex networks such as CNNs and it is coined to utilize the GPU more efficiently for iOS based applications.\nSince pervasive AI is still in its early stages, only few frameworks are dedicated specifically for distributed learning. One of these deep learning frameworks is MXNet , which is used for pervasive training. MXNet \nuses KVStore\\footnote{www.kvstore.io} to synchronize parameters shared among participants during the learning process. To monitor the utilization of pervasive resources, Ganglia  is designed to identify memory, CPU, and network requirements of the training and track the hardware usage for each participant. As for the inference phase, authors in  designed a hardware prototype targeting distributed deep learning for on-device prediction.", "cites": [3416], "cite_extract_rate": 0.16666666666666666, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.3, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a factual overview of AI frameworks suitable for pervasive IoT devices, mentioning their features and target platforms. It integrates information from cited papers to some extent, such as the purpose of MXNet and its use of KVStore, but lacks in-depth connections between the frameworks. There is minimal critical analysis or abstraction to broader principles, with no evaluation of trade-offs, limitations, or comparative insights across the frameworks."}}
{"id": "b6b18d3b-681d-4dfa-b91f-43cba1cb5f32", "title": "Intelligent vehicles, robots, and drones", "level": "subsubsection", "subsections": [], "parent_id": "b85221a3-fb27-44cf-b1c0-77f43b2c6992", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Fundamentals of Artificial Intelligence"], ["subsection", "Pervasive AI for IoT Applications"], ["subsubsection", "Intelligent vehicles, robots, and drones"]], "content": "Recently, DNNs have been widely used to lead a variety of mobile platforms such as drones, robots, and vehicles, in order to achieve critical tasks. \nIn this context, applications such as driving assistance, autonomous driving, and mobility mapping have become more reliable and commonly used in intelligent mobile systems. As an example, in , the captured image from the vehicle front facing camera is used to decide the steering angle and keep the car in the middle of the lane.\nThe ever-improving online learning is broadly exploited for UAVs/robots guidance, including the work in  where drones learn how to navigate and avoid obstacles while searching target objects. Several start-ups are also using DL for their self-driving systems, such as prime-air UAVs of Amazon used to deliver packages , and Uber self-navigating cars .", "cites": [3417, 887], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of how DNNs are used in intelligent vehicles, robots, and drones, citing two specific papers. It integrates these works into a general narrative about AI-driven mobility systems but lacks deeper synthesis or comparative discussion. There is minimal critical analysis or abstraction into broader principles or trends, with most content being factual and example-based."}}
{"id": "ab510be1-c383-4515-9a75-263d3b0597d4", "title": "Smart homes and cities", "level": "subsubsection", "subsections": [], "parent_id": "b85221a3-fb27-44cf-b1c0-77f43b2c6992", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Fundamentals of Artificial Intelligence"], ["subsection", "Pervasive AI for IoT Applications"], ["subsubsection", "Smart homes and cities"]], "content": "The concept of a smart home covers a large range of applications, that contribute to enhance the productivity, convenience, and life quality of the house occupants. Nowadays, many smart appliances are able to connect to the internet and offer intelligent services, such as smart air conditioners, smart televisions, and lighting control systems. Most of these appliances require the deployment of wireless controllers and sensors in walls, floors, and corners to collect data for motion recognition DL services. Speech/voice DL recognition services are also involved for a better home control, where a Well-known example is Amazon Alexa . \nCompared to smart homes, smart city services are more relevant to the deep learning community as the data collected from different ubiquitous participants is huge and highly heterogeneous, which allows high-quality analysis. Examples involve waste classification , energy consumption and smart grid , and parking control .", "cites": [3418], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a descriptive overview of Pervasive AI applications in smart homes and cities, mentioning specific examples and one cited paper on parking measurement. However, it lacks synthesis of broader ideas from the cited work, offers minimal critical analysis, and does not abstract beyond concrete examples to highlight underlying principles or trends in the field."}}
{"id": "d6893849-c38f-4d25-bff3-63dfab11eba3", "title": "Virtual Reality (VR) and Augmented Reality (AR)", "level": "subsubsection", "subsections": [], "parent_id": "b85221a3-fb27-44cf-b1c0-77f43b2c6992", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Fundamentals of Artificial Intelligence"], ["subsection", "Pervasive AI for IoT Applications"], ["subsubsection", "Virtual Reality (VR) and Augmented Reality (AR)"]], "content": "VR is designed to create an artificial environment, where users are placed into a 3D  experience while AR can be defined as a VR that inserts artificial objects into the real environment. Popular examples of  applications using AR/VR include the tactile internet and holographic telepresence , and multi-players VR games. The latency of the virtual reality systems is measured in terms of “motion-to-photons” metric, which is defined as the delay starting from moving the headset to updating the display according to the movement. This motion-to-photons latency should be in the range of tens to hundreds of milliseconds . Offloading the VR/AR computation to the remote cloud servers may incur higher latencies exceeding the required constraints. Hence, on-device computation is indispensable to achieve real-time performance.\n\\begin{comment}", "cites": [8656], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a basic description of VR and AR, mentioning their definitions and applications. It cites one paper but does not synthesize ideas from multiple sources or offer a deeper analysis of the cited work. There is minimal critical evaluation or abstraction to broader principles or trends in the field."}}
{"id": "91bbca78-24e7-4690-962f-3d81d6f33231", "title": "5G/6G intelligent networks", "level": "subsubsection", "subsections": [], "parent_id": "b85221a3-fb27-44cf-b1c0-77f43b2c6992", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Fundamentals of Artificial Intelligence"], ["subsection", "Pervasive AI for IoT Applications"], ["subsubsection", "5G/6G intelligent networks"]], "content": "The potential applications of DNN aiming to enhance networking performance are countless, particularly after the emergence of the sixth generation (6G). Different from previous generations, the 6G paradigm is based on supporting a wider variety of AI services spanning from high-performance servers to resource-limited devices, making “connected things” evolve into “connected intelligence”. Applications of DL in the new generation networks involve adaptive resource allocation to serve users in real-time , device-to-device (D2D) task offloading using online learning and localization services , proactive caching to minimize remote communication and reduce latency , network energy efficiency ,  and privacy and data security . \n\\end{comment}", "cites": [3419], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions various applications of deep learning in 5G/6G networks but does not effectively synthesize the information from the cited paper or connect it to broader themes. It lacks critical analysis and only provides a surface-level description of concepts. Some general terms like 'privacy and data security' are included, but no overarching patterns or principles are identified."}}
{"id": "e43e3646-a694-4588-a6e6-93cc3e3f42c2", "title": "Related surveys and paper novelty", "level": "section", "subsections": ["06f12287-75d8-4b08-8068-87f5281dd1ae"], "parent_id": "7aedf107-610a-4e90-9d9d-b0a1b2dcb03c", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Related surveys and paper novelty"]], "content": "\\label{related_surveys}\n\\begin{table*}[!h]\n\\footnotesize\n\\centering\n\\tabcolsep=0.09cm\n\\caption{Comparison with existing surveys.}\n\\label{tab:Related_works}\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n\\textbf{Refs} &\\textbf{Summary}  & \\multicolumn{2}{c|}{\\textbf{AI/pervasivity}} & \\multicolumn{3}{c|}{\\textbf{Scope}} & \\multicolumn{3}{c|}{\\textbf{AI technique}} & \\multicolumn{2}{c|}{\\textbf{Topic}} \\\\ \\hline\n &  & \\begin{tabular}[c]{@{}c@{}}AI on pervasive\\\\ networks\\end{tabular} &  \\begin{tabular}[c]{@{}c@{}}AI for pervasive\\\\ networks\\end{tabular} & cloud & \\begin{tabular}[c]{@{}c@{}}edge\\\\ servers\\end{tabular} & IoT & DI & FL & MARL & \\begin{tabular}[c]{@{}c@{}}Deployment:\\\\ hardware,\\\\ software\\\\ techniques, \\\\protocols.\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Management: \\\\ communication, \\\\ resource allocation, \\\\ and algorithms\\end{tabular} \\\\ \\hline\n\\ {}\\begin{tabular}[c]{@{}c@{}} \\\\ (2020-2021)\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Deep Learning \\\\ applications for the \\\\ Mobile Edge \\\\ computing networks\\end{tabular} & \\xmark& \\begin{tabular}[c]{@{}c@{}}\\cmark\\\\ 5G,\\\\ wireless \\\\ networks\\end{tabular}& \\cmark& \\cmark & \\cmark& \\cmark& \\cmark& \\xmark& \\cmark& \\xmark\\\\ \\hline\n\\ {}\\begin{tabular}[c]{@{}c@{}}\\\\ (2019)\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Efficient usage of IoT\\\\  hardware and software\\\\  for AI applications\\end{tabular} & \\cmark& \\xmark& \\xmark& \\xmark& \\cmark& \\xmark& \\xmark& \\xmark& \\cmark& \\xmark\\\\ \\hline\n\\ {}\\begin{tabular}[c]{@{}c@{}}\\\\ (2019-2020)\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Enabling AI on \\\\ edge networks\\end{tabular} & \\cmark & \\cmark& \\xmark& \\cmark& \\cmark& \\cmark& \\cmark& \\xmark& \\cmark& \\xmark\\\\ \\hline\n\\ {}\\begin{tabular}[c]{@{}c@{}} \\\\ (2018)\\end{tabular}& \\begin{tabular}[c]{@{}c@{}}Enabling AI on \\\\ edge networks\\end{tabular} & \\cmark& \\xmark& \\xmark& \\cmark& \\cmark& \\cmark& \\xmark& \\xmark& \\cmark& \\xmark\\\\ \\hline\n\\ {}\\begin{tabular}[c]{@{}c@{}}\\\\ (2018-2020)\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Decision making in \\\\multi-agent \\\\ systems and related \\\\applications\\end{tabular} & \\xmark& \\xmark& \\xmark& \\xmark& \\xmark& \\xmark& \\xmark& \\cmark& \\xmark& \\xmark\\\\ \\hline\n\\ {}\\begin{tabular}[c]{@{}c@{}}\\\\ (2020)\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Deep RL \\\\ for IoT systems\\end{tabular} & \\cmark& \\cmark& \\xmark& \\xmark& \\cmark& \\xmark& \\xmark& \\xmark& \\cmark& \\cmark\\\\ \\hline\n\\ {}\\begin{tabular}[c]{@{}c@{}} \\\\ (2020)\\end{tabular}& \\begin{tabular}[c]{@{}c@{}}Deep RL for \\\\wireless networks\\end{tabular} & \\cmark& \\begin{tabular}[c]{@{}c@{}}\\cmark\\\\ wireless \\\\ networks\\end{tabular} & \\xmark& \\cmark& \\cmark& \\xmark& \\xmark& \\cmark& \\cmark& \\xmark\\\\ \\hline\n\\ {}\\begin{tabular}[c]{@{}c@{}} \\\\ (2020)\\end{tabular}& \\begin{tabular}[c]{@{}c@{}}Distributed ML\\end{tabular} & \\cmark& \\begin{tabular}[c]{@{}c@{}}\\xmark\\end{tabular}& \\xmark& \\xmark& \\xmark& \\xmark& \\xmark& \\xmark& \\xmark& \\cmark\\\\ \\hline\n\\ {}\\begin{tabular}[c]{@{}c@{}} \\\\ (2019)\\end{tabular}& \\begin{tabular}[c]{@{}c@{}}Communication for ML \\\\ and \\\\ML for communication\\end{tabular} & \\cmark& \\begin{tabular}[c]{@{}c@{}}\\cmark\\\\  wireless \\\\ networks\\end{tabular}& \\xmark& \\cmark& \\cmark& \\xmark& \\cmark& \\xmark& \\xmark& \\cmark\\\\ \\hline\n\\ {}\\begin{tabular}[c]{@{}c@{}} \\\\ (2020)\\end{tabular}& \\begin{tabular}[c]{@{}c@{}}Communication efficient \\\\ edge AI\\end{tabular} & \\cmark& \\cmark& \\xmark& \\cmark& \\cmark& \\cmark& \\cmark& \\xmark& \\xmark& \\cmark\\\\ \\hline\n\\ {}\\begin{tabular}[c]{@{}c@{}} \\\\ (2019)\\end{tabular}& \\begin{tabular}[c]{@{}c@{}}AI on mobile \\\\ and wireless networks\\end{tabular} & \\cmark& \\begin{tabular}[c]{@{}c@{}}\\cmark\\\\ 5G,\\\\ wireless \\\\ networks\\end{tabular}& \\xmark& \\cmark& \\cmark& \\xmark& \\xmark& \\xmark& \\cmark& \\cmark\\\\ \\hline\n\\ {}\\begin{tabular}[c]{@{}c@{}} \\\\ (2020)\\end{tabular}& \\begin{tabular}[c]{@{}c@{}}Distributed Training \\\\of DNN\\end{tabular} & \\cmark& \\begin{tabular}[c]{@{}c@{}}\\xmark \\end{tabular}& \\xmark& \\xmark& \\xmark& \\xmark& \\xmark& \\xmark& \\xmark& \\cmark\\\\ \\hline\n\\ {}\\begin{tabular}[c]{@{}c@{}} \\\\ (2021)\\end{tabular}& \\begin{tabular}[c]{@{}c@{}}Federated learning \\\\for IoT applications\\end{tabular} & \\xmark& \\begin{tabular}[c]{@{}c@{}}\\cmark \\end{tabular}& \\cmark& \\cmark& \\cmark& \\xmark& \\cmark& \\xmark& \\xmark& \\xmark\\\\ \\hline\n\\ {}\\begin{tabular}[c]{@{}c@{}} \\\\ (2020)\\end{tabular}& \\begin{tabular}[c]{@{}c@{}}Enabling protocols, \\\\ technologies\\\\ for federated learning\\end{tabular} & \\cmark& \\cmark& \\cmark& \\cmark& \\cmark& \\xmark& \\cmark& \\xmark& \\cmark& \\xmark\\\\ \\hline\n\\ {}\\begin{tabular}[c]{@{}c@{}} \\\\ (2020)\\end{tabular}& \\begin{tabular}[c]{@{}c@{}}Architecture, design and\\\\  applications of centralized,\\\\ distributed and federated \\\\learning\\end{tabular} & \\cmark& \\cmark& \\cmark& \\cmark& \\cmark& \\xmark& \\cmark& \\xmark& \\cmark& \\cmark\\\\ \\hline\n\\ {}\\begin{tabular}[c]{@{}c@{}} \\\\ (2020)\\end{tabular}& \\begin{tabular}[c]{@{}c@{}}Enabling protocols, \\\\technologies\\\\ for federated learning\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\cmark\\\\ Vehicular\\\\ IoT\\end{tabular} & \\xmark& \\xmark& \\xmark& \\cmark& \\xmark& \\cmark& \\xmark& \\cmark& \\xmark\\\\ \\hline\n\\rowcolor[HTML]{ECF4FF} \nOur paper & Pervasive AI & \\cmark& \\xmark& \\cmark& \\cmark& \\cmark& \\cmark& \\cmark& \\cmark& \\xmark& \\cmark\\\\ \\hline\n\\end{tabular}\n\\end{table*}\nThe intersection of pervasive computing and AI is still in its early stage, which attracts the researchers to review the existing works and provide innovative insights, as illustrated in Table \\ref{tab:Related_works}.  First, many efforts discussed the applications of artificial intelligence that support edge networks, \nin order to meet the networking requirements. Multiple edge contexts are explored such as healthcare, smart cities, and grid energy. As an example, two recent surveys  provided an in-depth discussion of the usage of AI in wireless and 5G networks to empower caching and offloading, resource scheduling and sharing, and network privacy. These surveys touched upon the pervasive AI, particularly federated learning and distributed inference. However, the distribution was discussed briefly as one of the techniques that further enables AI in the edge. In our survey, the applications of AI for pervasive networks is not the main topic. Instead, the deployment of AI on pervasive devices is the scope of this paper. \nThe surveys in  conducted a comprehensive review on the systems, architectures, frameworks, software, technologies, and algorithms that enable AI computation on edge networks and discussed the advantages of edge computing to support the AI deployment compared to cloud approaches. However, even though they dedicated a short part for distributed AI, these papers did not discuss the resource and communication challenges of pervasive computing nor the partitioning techniques of AI (e.g., splitting strategies of the trained DNN models or the training data.). Moreover, they did not consider the cloud computing as  indispensable part of the distributed system. Therefore, unlike the previous surveys , we present an in-depth review that covers the resources, communication and computation challenges of distributed AI among ubiquitous devices. More specifically, applying the same classical communication and computation techniques adopted in centralized approaches for pervasive AI is not trivial. As an alternative, both pervasive computing systems and distributed AI techniques are tailored to take into consideration the heterogeneous resources of participants, the AI model, and the requirements of the system.\nThese customized strategies for pervasive AI are the main focus of our survey.\nThe multi-agent reinforcement learning has not been reviewed by any of previous papers. Other papers surveyed the single agent and multi-agents RL, such as . In these tutorials, the authors conducted comprehensive studies to show that the single-agent RL is not sufficient anymore to meet the requirements of emerging networks in terms of efficiency, latency, and reliability. In this context, they highlighted the importance of cooperative MARL to develop decentralized and scalable systems. They also surveyed the existing decision making models including  game theory and Markov decision process and they presented an overview of the evolution of cooperative and competitive MARL, in terms of rewards optimization, policy convergence, and performance improvement. Finally, the applications of MARL for networking problems are also reported. However, despite this recent popularity of MARL, the designed algorithms to achieve  efficient communication between agents and minimum computation are not surveyed yet. To the best of our knowledge, we are the first to survey the computation and communication challenges faced to achieve a consensus on the distributed RL policy. In other words, our focus is not the performance of the RL policy. Instead, we survey the computational load, communication schemes and architectures experienced by cooperative agents during learning and execution.\nUnlike aforementioned papers, authors of  focused only on distributed machine learning. In these papers, they covered the training phase and data partitioning. The survey in  discussed the issues of learning from a data characterised by its large volume, different types of samples, uncertainty, incompleteness, and low value density. Solutions to minimize the learning complexity and divide the data are introduced in , where authors reviewed the algorithms and decision rules to fragment large scale data into distributed datasets. The paper in  described the architectures and topologies of nodes participating in the distributed training by presenting existing frameworks and communication patterns that can be employed to exchange states. Authors in  presented a contemporary and comprehensive survey of distributed ML techniques, which includes the applicability of such concept to wireless communication networks, and the computation and communication efficiency. However, this survey along with the previous works focus only on the training phase. Also, the authors do not provide a comprehensive and deep summary of the complexity, computation and communication efficiency witnessed by different decentralized architectures and the amount of data shared by participants, particularly for the multi-agent reinforcement learning. Our survey aims to bridge the gap by providing a comprehensive review of distributed AI, including both training and inference phases. More specifically, we thoroughly study the architectures of federated learning, active learning and reinforcement learning, and the partitioning strategies of DNN trained models. Then, for each approach, we show the impact on the communication and computation complexity and the algorithms scheduling the collaboration between devices. \nFinally, the authors in  provided a deep review of communication challenges of AI-based applications on edge networks. Specifically, the survey in  provided insights about allocating mobile and wireless networks resources for AI learning tasks. However, the distribution of AI techniques was not targeted in this latter paper. The surveys in  are considered the closest ones to our topic as they explored the communication-efficient AI distribution. However, they mainly focused on the training phase, i.e., federated learning, whereas the pervasive inference and MARL were not studied. The inference distribution is briefly discussed in   from a communication angle, without considering other constraints such as the memory and computation nor presenting the partitioning strategies (i.e., splitting of the trained model), which highly impact the distribution process, the parallelization technique, and participants orchestration. Our paper represents a holistic survey that covers all AI tasks that require cooperation between pervasive devices motivated either by the application requirements or by the system design and the AI model.\nOur search of related papers has been conducted through different databases and engines, including  IEEE Xplorer, ScienceDirect, and ArXiv; and the papers have been chosen from a time frame set between 2017 and 2021, in addition to some well-established research works. More specifically, we selected all surveys with high citation rates that cover AI, pervasive computing, federated learning, reinforcement learning, bandit learning, deep learning applications in IoT systems, and AI deployment on edge networks. In the rest of our survey, we review the research conferences and journal papers with solid results that provide comprehensive studies on resource-efficient distributed inference and training.\n\\begin{comment}", "cites": [1315, 9123, 3420, 7719, 659, 3427, 8657, 3428, 3426, 3425, 1313, 3422, 660, 547, 3423, 3424, 3421], "cite_extract_rate": 0.5862068965517241, "origin_cites_number": 29, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section uses a table to compare existing surveys along multiple dimensions, which provides some synthesis of ideas. However, the narrative remains largely focused on contrasting the authors' scope with others without deeply connecting themes or creating a novel framework. The critical evaluation is present but limited to identifying gaps such as insufficient focus on resource challenges and model partitioning, rather than offering a nuanced critique. Some abstraction is attempted, but it is not fully developed into overarching principles."}}
{"id": "c4778902-b31a-4153-8252-7d9c704e337a", "title": "Federated Learning", "level": "subsection", "subsections": ["7cc9c4fa-1160-4383-a4bc-5fa5eb5646f1", "1b661b5c-6fb5-4606-987f-cc5635444cf4", "eed40686-a7ba-4e6f-be93-9abfee6a5cbd", "77508e12-2980-420b-9045-084b9804e210"], "parent_id": "8f96916e-e394-41c3-94f4-9283f9cfdbaa", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive training"], ["subsection", "Federated Learning"]], "content": "\\label{FL}\nDespite the great potential of deep learning in different applications, it still has major challenges that need to be addressed. These challenges are mainly due to the massive amount of data needed for training deep learning models, which imposes severe communication overheads in both network design and end-users. Moreover, the conventional way of transferring the acquired data to a central server for training comes with many privacy concerns that several applications may not tolerate. In this context, the need for intelligent and on-device DL training has emerged. More specifically, instead of moving the data from the users to a centralized data center, pervasive data-sources engage the server to broadcast a pre-trained model to all of them. Then, each participant deploys and personalizes this generic model by training it on its own data locally. In this way, privacy is guaranteed as the data is processed within the host. The on-device training has been widely used in many applications, such as the medical field, assistance services, and smart education. However, this no-round-trip training technique precludes the end-devices to benefit from others' experiences, which limits the performance of the local models. To this end, Federated Learning (FL) has been advanced, where end-users can fine-tune their learning models while preserving privacy and local data processing. Then, these local models (i.e., model updates) are aggregated and synchronized (averaged) at a centralized server, before being sent back to the end-users. This process is repeated several times (i.e., communication rounds) until reaching converge. Accordingly, each participant builds a model from its local data and benefits from other experiences, without violating privacy constraints. FL is proposed by Google researchers in 2016 , and since then, it has witnessed unprecedented growth in both industry and academia. \nWe present in what follows an overview for this emerging pervasive learning technique, i.e., Federated Learning. In particular, we introduce the computation and communication models of the FL techniques. Then, we present a brief summary of the related works in the literature, while highlighting a use case that considers the application of FL within UAV swarms. It is worth mentioning that the FL can be used for both online and offline learning (i.e., the training can be performed on static datasets at once, or continuously training on new data received by different participants). \n\\begin{comment}\n\\begin{figure}[!h]\n\\centering\n\t\\includegraphics[scale=0.6]{Figures/FL_taxonomy2.pdf}\n\t\\caption{Outline of federated learning.} \n\t\\label{FL_outline}\n\\end{figure}\n\\end{comment}", "cites": [582], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic descriptive overview of Federated Learning, including its motivation, mechanism, and application areas. It integrates a single cited paper to explain the concept and challenges of decentralized data training but lacks deeper synthesis across multiple sources. There is little critical analysis or identification of broader trends or principles in the field."}}
{"id": "7cc9c4fa-1160-4383-a4bc-5fa5eb5646f1", "title": "Profiling computation and communication models \\label{sec:Fundamentals", "level": "subsubsection", "subsections": [], "parent_id": "c4778902-b31a-4153-8252-7d9c704e337a", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive training"], ["subsection", "Federated Learning"], ["subsubsection", "Profiling computation and communication models \\label{sec:Fundamentals"]], "content": "}\nGenerally, the FL system is composed of two main entities, which are the data-sources (i.e., owners of data or pervasive participants) and the centralized server (i.e., model owner). Let $N$ denote the number of data-sources. Each one of these devices has its own dataset $D_i$. This private data is used to train the local model $m_i$, and then the local parameters are sent to the centralized server. Next, the local models are collected and aggregated onto a global model $m_G=\\bigcup_{i=1}^{N} m_i$. The FL is different from training in the remote server, where the distributed data are collected and aggregated first, i.e., $D_G=\\bigcup_{i=1}^{N} D_i$, and then one model $m$ is trained centrally. We assume that  data-sources are honest and submit their real data or their true local models to the centralized server. Otherwise,\ncontrol and incentive techniques are used to guarantee the reliability of FL, including .\nTypically, the life cycle of FL is composed of multiple communication rounds that are completed when the centralized model reaches a satisfactory accuracy. Each round includes the following steps:\n\\begin{itemize}\n    \\item \\textit{Initialization of FL:} The centralized server fixes the training task, the data shape, the initial model parameters, and the learning process (e.g., learning rate). This initial model $m_G^0$ is broadcasted to the selected participants.\n    \\item \\textit{Training and updating the local models:} Based on the current global model $m_G^t$, each data-source $i$ utilizes its own data $D_i$ to update the local model $m_i^t$. We note that $t$ presents the current round index. Hence, at each step $t$, the goal of each participant is to  find the optimal parameters minimizing the loss function $L(m_i^t)$ defined as:\n    \\begin{equation}\n    m_i^{t*}= argmin_{m_i^{t}} L(m_i^t).\n    \\end{equation}\n    Subsequently, the updated parameters of the local models are offloaded to the server by all selected participants.\n    \\item \\textit{Global model aggregation:} The received parameters are aggregated into one global model $m_G^{t+1}$, which will be sent back in its turn to the data owners. This process is repeated continuously, until reaching convergence. The server goal is to minimize the global loss function presented as follows: \n        \\begin{equation}\n      L(m^t_G)= \\frac{1}{N} \\sum\\limits_{i=1}^{N}L(m^t_i).\n    \\end{equation}\n    The aggregation of the global model is the most important phase of FL. A classical and straightforward aggregation technique, namely FedAvg, is proposed by Google reference paper . In this technique, the centralized server tries to minimize the global loss function by averaging the aggregation following the equation below:\n    \\begin{equation}\n    m_G^{t+1}=\\sum\\limits_{i=1}^{N}\\frac{|D_i|}{\\sum\\limits_{j=1}^{N}|D_j|}m_i^{t+1},\n    \\end{equation}\n    where $D_i$ is the local dataset. The FL system is iterated continuously until the convergence of the global loss function or  reaching a desirable accuracy.\n\\end{itemize}\n\\begin{comment}\n\\begin{algorithm}\n\\caption{FederatedAveraging (FedAvg) }\n\\label{alg:FL}\n\\begin{algorithmic}[1]\n\\State \\textbf{Input:} $N$: participants, $D_i$: dataset of the device $i$, $B$: local mini-batches, $E$: number of local epochs, $T$: number of rounds, $\\rho$: learning rate, c: fraction of participants.\n\\State \\textbf{Output:} Global model $m_G$.\n\\State \\textbf{Initialization of FL:} initialize $m_G^0$\n\\State \\textbf{Global model aggregation:} \n\\For{t=1...T} \n\\State $NP\\leftarrow max(c.N,1)$\n\\State $P \\leftarrow$ select random $NP$ participants\n\\For {$i \\in P$ \\textbf{in parallel}}\n\\State $m_i^{t+1}\\leftarrow$ \\textbf{Local model update} $(i,m_G^t)$\n\\EndFor\n\\State $m_G^{t+1}=\\sum\\limits_{i=1}^{N}\\frac{|D_i|}{\\sum\\limits_{j=1}^{N}|D_j|}m_i^{t+1}$ \\text{\\footnotesize\\% Averaging aggregation}\n\\EndFor\n\\State \\textbf{Local model update} (i,m):\n\\State $d \\leftarrow$ split $D_i$ into batches of size $B$\n\\For{j=1..E}\n\\For{samples $b \\in d$}\n\\State $m \\leftarrow m-\\rho \\Delta L(m,b)$ \\text{\\footnotesize\\% $\\Delta L$ is the gradient of $L$}\n\\EndFor\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n\\end{comment}\nA major challenge in FL is the large communication and energy overhead related to exchanging the models updates between different end-users, and the centralized server . Such overheads depend on multiple parameters, including the models' updates size, the number of participating users, the number of epochs per user, and the number of communication rounds required to maintain the convergence. Particularly, the energy consumed by an FL participant $i$ characterized by a frequency $f$, a local dataset $D_i$, and a number of local epochs $E$, is given by :\n\\begin{equation}\n\\label{FL_energy}\ne^c_i= E \\times (\\phi\\gamma |D_i| f^2),\n\\end{equation}\nwhere $\\phi$ is the number of CPU cycles required to compute one input instance, and $\\gamma$ is a constant related to the CPU. The latency required to compute the local model can be expressed as: \n\\begin{equation}\n\\label{FL_latency}\nt^c_i= E \\times (\\frac{\\phi|D_i|}{f}).\n\\end{equation}\nFrom the equations (\\ref{FL_energy}) and (\\ref{FL_latency}), we can see that a trade-off exists between the local training latency and the consumed energy. More specifically, for a fixed accuracy determined by the number of local epochs and a fixed frequency, the latency is accelerated depending on the size of the private data. If the data size and the accuracy are fixed, increasing the CPU frequency can help to minimize the local model computation. However, minimizing the latency comes at the expense of energy consumption that increases to the square of the operating frequency. \n\\begin{figure*}[!h]\n\\centering\n\t\\includegraphics[scale=0.4]{Figures/FL_designs1.pdf}\n\t\\caption{The FL architectures considered in the literature: (a) one-layer FL, (b) edge-assisted FL.}\n\t\\label{fig: FL_arch}\n\\end{figure*}\nThe transmission time to share the model updates between the centralized servers and different FL participants mainly depends on the channel quality, the number of devices and the number of global rounds, illustrated as follows:\n\\begin{equation}\n\\label{FL_transmission}\nt^T= T \\times \\sum\\limits_{i=1}^{N}\\frac{K}{\\rho_i},\n\\end{equation}\nwhere $K$ is the models' parameters size shared with the server and $\\rho$ is the data rate of the participant $i$. On the other hand, the total energy consumed during the federated learning process using the local transmit powers $P_i$ is equal to:\n\\begin{equation}\ne^T= T \\times \\sum\\limits_{i=1}^{N}\\frac{KP_i}{\\rho_i},\n\\end{equation}\nFrom the above equations, we can see that the local iterations $E$ and the global communication rounds $T$ are very important to optimize the energy, computation, and communication costs. Particularly, for a relative local accuracy $\\theta_l$, $E$ can be expressed as follows : \n\\begin{equation}\n\\label{FL_E}\nE= \\alpha \\times log(\\frac{1}{\\theta_l}),\n\\end{equation}\nwhere $\\alpha$ is a parameter that depends on the dataset size and local sub-problems. The global upper bound on the number of iterations to reach the targeted accuracy $\\theta_G$ can be presented as :\n\\begin{equation}\n\\label{FL_T}\nE^g= \\frac{\\zeta log(\\frac{1}{\\theta_G})}{1-\\theta_l}.\n\\end{equation}\nWe note that $\\zeta log(\\frac{1}{\\theta_G})$ is used instead of $O(log(\\frac{1}{\\theta_G}))$, where $\\zeta$ is a positive constant. The computation cost depending on the local iterations $E$ and the communication cost depending on the global rounds $T$ are contradictory. It means, minimizing $E$ implies maximizing $T$ to update the local parameters frequently, which results in increasing the convergence latency. \nTo summarize, FL pervasiveness aspects that are being tackled by different studies, to reduce communication and energy overheads, may include: \n\\begin{enumerate}\n\t\\item reducing communication frequency, i.e., number of communication rounds;\n\t\\item reducing the number of local iterations;\n\t\\item selecting minimum number of participating users in the training process;  \n\t\\item optimizing local devices operating frequencies;\n\t\\item minimizing the entropy of the models updates by using lossy compression schemes;  \n\t\\item using efficient encoding schemes in communicating models updates.   \n\\end{enumerate}  \nIn what follows, we categorize different presented FL schemes in the literature, based on the system architecture, namely one-layer FL and edge-assisted FL. The former refers to user-cloud architecture, where different users share their learning models with a cloud or centralized server for aggregation, while the latter refers to user-edge-cloud architecture, where edge nodes are leveraged to reduce communication overheads and accelerate FL convergence (see Figure \\ref{fig: FL_arch}). \n\\begin{comment}\n\\begin{table*}[!h]\n\\centering\n\\caption{Comparison between privacy-aware FL strategies.\\\\\n(H: High, M: Medium, L: Low).}\n\\label{tab:privacy}\n\\begin{tabular}{|c|c|c|c|c|c|}\n\\hline\n\\begin{tabular}[c]{@{}c@{}}\\textbf{Privacy-aware}\\\\ \\textbf{strategy}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\textbf{Privacy}\\\\ \\textbf{level}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\textbf{Accuracy}\\end{tabular} &  \\begin{tabular}[c]{@{}c@{}}\\textbf{Compatibility}\\\\ \\textbf{with IoT devices}\\end{tabular}  & \\begin{tabular}[c]{@{}c@{}}\\textbf{Communication}\\\\ \\textbf{overhead}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\textbf{Computation}\\\\ \\textbf{overhead}\\end{tabular} \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Differential Privacy \\end{tabular} & M & \\xmark & \\cmark &  \\xmark & M \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Secure multi-party \\\\ computation \\end{tabular} & H & \\xmark & \\xmark & \\xmark & M \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Homomorphic Encryption \\\\  \\end{tabular} & H & \\xmark & \\xmark & \\xmark & M \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Hybrid protocols  \\end{tabular} & H & \\xmark & \\xmark & \\xmark  & M \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Blockchain \\end{tabular} & H & \\cmark & \\xmark & \\cmark & H \\\\ \\hline\n\\end{tabular}\n\\end{table*}\n\\end{comment}", "cites": [602, 3430, 3429, 582], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a clear description of Federated Learning (FL) mechanisms, including the system architecture, model update process, and associated energy and latency equations. However, it lacks significant synthesis of the cited papers, presenting information in a largely additive way. There is minimal critical analysis or evaluation of limitations, and the abstraction remains limited to surface-level observations about trade-offs rather than deeper, generalized insights into FL principles."}}
{"id": "e77d3744-620c-4405-b29f-e28af04c0418", "title": "One-layer FL \\label{sec:Single", "level": "paragraph", "subsections": [], "parent_id": "1b661b5c-6fb5-4606-987f-cc5635444cf4", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive training"], ["subsection", "Federated Learning"], ["subsubsection", "Resource management for Federated learning"], ["paragraph", "One-layer FL \\label{sec:Single"]], "content": "}  \nThe efficiency of FL concept has been proved by different experiments on various datasets.  \nIn particular, the proposed model in  presented a one-layer FL, where the available users/devices could exchange their local models with a centralized server that collects the local models and forms a global model.  \nAfterward, several extensions have been proposed to the original FL. The investigated problems/approaches in FL, considering one-layer architecture, can be categorized into:\n\\begin{itemize}\n\t\\item studying the convergence behaviour of the proposed FL schemes from a theoretical perspective, while optimizing the learning process given limited computational and communication resources ; \n\t\\item considering partial user participation for the FL aggregation process in a resource-constrained environment while balancing between the model accuracy and communication cost ; \n\t\\item presenting communication-efficient schemes that aim at reducing the FL communications cost by adopting distinct sparsification and compression techniques  .  \n\\end{itemize} \nThe effect of non-Independent and Identically Distributed (non-IID) data on the performance of FL has been investigated in . This work illustrated, theoretically and empirically, that highly skewed non-IID data (i.e., the local data at different users are not identically distributed) can substantially decrease the accuracy of the obtained trained model by up to $55\\%$. \nTo solve this issue, the authors suggested to share a small subset of data between all participants. By integrating these data from the neighboring participants with the local data at each participant, the local dataset will be less skewed.  However, sharing data among the available participants is not always feasible, given strict privacy constraints and communication cost of sharing such data.   \nThe convergence analysis of FedAvg scheme using non-IID data has been investigated  in  for strongly convex problems.  \nIn , the authors started first by studying the convergence behaviour of gradient-descent based FL scheme on non-IID data from a theoretical point of view.  \nAfter that, the obtained convergence bound is used to develop a control mechanism, for resource-limited systems, by adjusting the frequency of the global model aggregation in real-time while minimizing the learning loss. \nA new FL algorithm, named FEDL, is presented in .  This algorithm used a local surrogate function that enables each user to solve its local problem approximately up to a certain accuracy level. The authors presented the linear convergence rate of FEDL as a function of the local accuracy and hyper-learning rate. Then, a resource allocation problem over wireless networks was formulated, using FEDL, to capture the trade-off between the training time of FEDL and user's energy consumption.     \nIn , the effect of considering the participation of all users in FL algorithm has been studied. Indeed, it is shown that increasing the number of participant users may lead to increasing the learning time since the central server have to wait for \\textit{stragglers},  i.e., participants with bad wireless channels or large computational delay.  \nTo overcome the impact of \\textit{stragglers}, different schemes have been proposed to select the best subset of users that can participate in the FL aggregation .  \nFor instance, the authors in  presented a control algorithm, leveraging reinforcement learning, in order to accelerate the FL convergence by obtaining the subset of users that can participate in each communication round of FL, while accounting for the effect of non-IID data distribution.  \n{\nTo maintain the balance between computational and communication costs, and global model accuracy, the authors in   presented a joint optimization model for data and users selection. } \nIn , the problem of users selection to minimize the FL training time was investigated for Cell-Free massive Multiple-Input Multiple-Output (CFmMIMO) networks.   \nAlternatively, sparsification and compression techniques are used to decrease the entropy of the exchanged models in FL process.  \nIn particular, instead of communicating  dense models' updates, the authors in  presented a framework that aims at accelerating the distributed stochastic gradient descent process by exchanging sparse updates (i.e., forwarding the fraction of entries with the biggest magnitude for each gradient).  \nIn , a sparse ternary compression technique was presented to compress both the upstream and downstream communications of FL, using sparsification, error accumulation, ternarization, and optimal Golomb encoding.", "cites": [7138, 619, 7721, 7720, 582, 616, 3431], "cite_extract_rate": 0.5833333333333334, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers on one-layer FL by grouping them into coherent themes such as convergence analysis, user selection, and communication efficiency. It also highlights limitations (e.g., non-IID data impact, straggler effect) and mentions trade-offs between accuracy, privacy, and resource constraints. While it offers some level of analysis, especially in identifying issues like communication overhead and data skew, it stops short of deep critique or proposing new frameworks."}}
{"id": "1c741ea7-6543-4f53-818d-35f8e7b21dd2", "title": "Edge-assisted FL \\label{sec:Hierarchical", "level": "paragraph", "subsections": [], "parent_id": "1b661b5c-6fb5-4606-987f-cc5635444cf4", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive training"], ["subsection", "Federated Learning"], ["subsubsection", "Resource management for Federated learning"], ["paragraph", "Edge-assisted FL \\label{sec:Hierarchical"]], "content": "}  \nSome studies have considered edge-assisted FL architecture to tackle the problem of non-IID data. For example, the authors in  extended the work in  in order to  analytically prove the convergence of the edge-assisted FedAvg algorithm. Then, this work was further extended in  to mitigate the effect of \\textit{stragglers} by proposing probabilistic users selection scheme. \nThe authors in  presented two strategies to prevent the bias of training caused by non-IID data. The first strategy was applied before training the global model by performing data augmentation to tackle the challenge of non-IID data. The second strategy utilized mediators, i.e., edge nodes, to reschedule the training of the participants based on the distribution distance between the mediators. \nIn , the impact of non-IID data in edge-assisted FL architecture was investigated and compared to the centralized FL architecture. This study defined the main parameters that affect the learning process of edge-assisted FL. \nTable \\ref{tab:Fl} presents the taxonomy of the federated learning techniques described in this section.\n\\begin{table*}[]\n\\centering\n\\footnotesize\n\\tabcolsep=0.09cm\n\\caption{Taxonomy of federated learning techniques.}\n\\label{tab:Fl}\n\\begin{tabular}{|l|l|l|l|l|l|l|l|}\n\\hline\n\\textbf{Refs} & \\textbf{Year} & \\textbf{FL devices} & \\textbf{Architecture} & \\textbf{Trained model} & \\textbf{Aggregation algorithm} & \\textbf{Dataset} & \\textbf{Targeted metrics} \\\\ \\hline\n & 2017 & \\begin{tabular}[c]{@{}l@{}}Mobile\\\\ devices\\end{tabular} & One-layer & \\begin{tabular}[c]{@{}l@{}}- 2NN\\\\ - CNN \\\\ - LSTM\\end{tabular} & FedAvg & \\begin{tabular}[c]{@{}l@{}}- CIFAR-10 \\\\ - MNIST  \\end{tabular} & - Accuracy vs rounds \\\\ \\hline\n & 2018 & \\begin{tabular}[c]{@{}l@{}}Mobile and\\\\ IoT devices\\end{tabular} & One-layer & - CNN & Enhanced FedAvg & \\begin{tabular}[c]{@{}l@{}}- CIFAR-10\\\\ - MNIST\\\\ - KWS  \\end{tabular} & \\begin{tabular}[c]{@{}l@{}}- Accuracy vs rounds\\\\ - Shared data\\\\ - Weight divergence\\end{tabular} \\\\ \\hline\n & 2019 & End-users & One-layer & - Logistic regression & FedAvg & - MNIST & \\begin{tabular}[c]{@{}l@{}}- Global loss vs rounds\\\\ - Rounds vs local epochs\\end{tabular} \\\\ \\hline\n & 2019 & Edge nodes & One-layer & \\begin{tabular}[c]{@{}l@{}}- Squared-SVM\\\\ - Linear regression,\\\\ - K-means\\\\ - CNN\\end{tabular} & FedAvg & \\begin{tabular}[c]{@{}l@{}}- MNIST \\\\ - Energy  \\\\ - User Knowledge \\\\ Modeling   \\\\ - CIFAR-10\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}- Loss vs nodes\\\\ - Accuracy vs nodes\\end{tabular} \\\\ \\hline\n & 2019 & End-users & One-layer & \\xmark & \\begin{tabular}[c]{@{}l@{}}Non-weighted\\\\ averaging\\end{tabular} & \\xmark & \\begin{tabular}[c]{@{}l@{}}- Communication vs \\\\   computation time\\\\ - Learning time vs energy\\end{tabular} \\\\ \\hline\n & 2019 & End-users & One-layer & - CNN & Averaging & \\begin{tabular}[c]{@{}l@{}}- CIFAR-10\\\\ - Fashion-MNIST  \\end{tabular} & \\begin{tabular}[c]{@{}l@{}}- Accuracy vs time\\\\ - Number of participants\\end{tabular} \\\\ \\hline\n & 2020 & \\begin{tabular}[c]{@{}l@{}}Mobile\\\\ devices\\end{tabular} & One-layer & - CNN & \\begin{tabular}[c]{@{}l@{}}FedAvg with users seclection\\\\ Favor\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}- MNIST\\\\ - Fashion-MNIST\\\\ - CIFAR-10\\end{tabular} & -Accuracy vs rounds \\\\ \\hline\n & 2020 & End-users & One-layer & \\begin{tabular}[c]{@{}l@{}}- MLP\\\\ - CNN\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}FedAvg\\end{tabular} & - MNIST & - Accuracy vs rounds \\\\ \\hline\n & 2020 & \\begin{tabular}[c]{@{}l@{}}Mobile\\\\ devices\\end{tabular} & One-layer & \\xmark & \\xmark & \\xmark & \\begin{tabular}[c]{@{}l@{}}- Transmission time\\\\ - Loss\\end{tabular} \\\\ \\hline\n & 2020 & \\begin{tabular}[c]{@{}l@{}}Mobile \\\\ devices\\end{tabular} & One-layer & \\begin{tabular}[c]{@{}l@{}}- VGG11\\\\ - CNN\\\\ - LSTM \\\\ - Logistic regression\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}Weighted averaging with \\\\ Top-k sparsified  communication\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}- CIFAR\\\\ - KWS\\\\ - MNIST\\\\ - Fashion-MNIST\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}- Communication delay\\\\ - Accuracy\\end{tabular} \\\\ \\hline\n & 2020 & \\begin{tabular}[c]{@{}l@{}}IoT\\\\ devices\\end{tabular} & One-layer & \\begin{tabular}[c]{@{}l@{}}- 2NN\\\\ - CNN\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}communication-efficient\\\\  FedAvg (CE-FedAvg)\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}- CIFAR-10\\\\ - MNIST\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}- Uploaded data\\\\ - communication rounds\\\\ - convergence time\\end{tabular} \\\\ \\hline\n & 2020 & UAVs & One-layer & \\xmark & FedAvg & \\xmark & - Rounds vs bandwidth \\\\ \\hline\n & 2020 & UAVs & One-layer & - FCN & FedAvg & CRAWDAD  & \\begin{tabular}[c]{@{}l@{}}- Accuracy vs rounds\\\\ - local learning time\\end{tabular} \\\\ \\hline\n & 2020 & UAVs & One-layer & - CNN & FedAvg & - MNIST & - Utility of participants \\\\ \\hline\n & 2020 & UAVs & One-layer & \\begin{tabular}[c]{@{}l@{}}- LSTM\\\\ - GRU\\\\ - AQNet  \\end{tabular} & FedAvg & \\begin{tabular}[c]{@{}l@{}}- Ground and aerial \\\\Sensing Data collected \\\\ by authors\\end{tabular} & - Energy consumption \\\\ \\hline\n & 2020 & End-users & Edge-assisted & - CNN & Hierarchical FedAvg & \\begin{tabular}[c]{@{}l@{}}- CIFAR-10\\\\ - MNIST\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}- Accuracy vs epochs\\\\ - Training time\\\\ - Energy consumption\\end{tabular} \\\\ \\hline\n & 2020 & End-users & Edge-assisted & \\begin{tabular}[c]{@{}l@{}}- FCN \\\\ - LeNet-5\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}Weighted averaging with\\\\ Effective Data Coverage (EDC)\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}- MNIST\\\\ - Aerofoil \\end{tabular} & \\begin{tabular}[c]{@{}l@{}}-  Accuracy vs rounds\\\\ - Training time\\\\ - Energy consumption\\end{tabular} \\\\ \\hline\n & 2021 & \\begin{tabular}[c]{@{}l@{}}Mobile\\\\ devices\\end{tabular} & Edge-assisted & - CNN & FedAvg & \\begin{tabular}[c]{@{}l@{}}- EMNIST \\\\ - CINIC-10 \\\\ - CIFAR-10\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}- Accuracy vs rounds\\\\ - Accuracy vs  epochs\\\\ - Storage requirement\\end{tabular} \\\\ \\hline\n & 2021 & End-users & Edge-assisted & \\begin{tabular}[c]{@{}l@{}}- FCN\\\\ - CNN\\end{tabular} & FedAvg & \\begin{tabular}[c]{@{}l@{}}- MNIST\\\\ - Fashion-MNIST\\\\ - CIFAR-10\\end{tabular} & \\begin{tabular}[c]{@{}l@{}}- Accuracy vs rounds\\\\ - Accuracy vs edge \\\\   distance distribution\\\\ - Speed\\end{tabular} \\\\ \\hline\n\\end{tabular}\n\\end{table*}", "cites": [7721, 3434, 582, 3433, 619, 652, 643, 3431, 7720, 646, 7139, 7138, 3432], "cite_extract_rate": 0.4482758620689655, "origin_cites_number": 29, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section primarily describes various edge-assisted federated learning approaches by listing methods, models, and metrics from the cited papers without deeply connecting or synthesizing ideas across them. It lacks critical evaluation of the methods or identification of broader patterns, limiting its insight quality."}}
{"id": "eed40686-a7ba-4e6f-be93-9abfee6a5cbd", "title": "Use case: Learning in the sky", "level": "subsubsection", "subsections": [], "parent_id": "c4778902-b31a-4153-8252-7d9c704e337a", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive training"], ["subsection", "Federated Learning"], ["subsubsection", "Use case: Learning in the sky"]], "content": "\\label{sec:Cases}\n\\begin{figure}[h!]\n\t\\centering\n\t\t\\scalebox{1.4}{\\frame{\\includegraphics[width=0.53\\columnwidth]{Figures/FL_2.pdf}}}\n\t\\caption{An example of FL applications in UAV-assisted environment.}\n\t\\label{fig:FL_CS}\n\\end{figure}\nNowadays, deep learning has been widely used in Flying Ad-hoc Network (FANET). Different tasks can be executed using DL techniques at UAV swarms, such as coordinated trajectory planning  and jamming attack defense . However, due to the related massive network communication overheads, forwarding the generated large amount of data from the UAV swarm to a centralized entity, e.g., ground base stations, makes implementing centralized DL challenging. \nAs a promising solution, FL was introduced within a UAV swarm in several studies  to avoid transferring raw data, while forwarding only local trained models' updates to the centralized entity that generates the global model and send it to the end-user and all participants over the intra-swarm network (see Fig. \\ref{fig:FL_CS}). \nIn , the authors present a FL framework for the swarm of wirelessly connected UAVs flying at the same altitude. The considered swarm includes a leader UAV and a set of followers UAVs. It is assumed that each follower collects data while flying and implements FL for executing inference tasks such as trajectory planning and cooperative target recognition. Hence, each follower exploits its gathered data to train its own learning model, then forwarding its model's updates to the leading UAV. All received models are then aggregated at the leading UAV to generate a global FL model, that will be used by the following UAVs in the next iteration. \nInterestingly,  investigates the impact of wireless factors (such as fading, transmission delay, and UAV antenna angle deviations) on the performance of FL within the UAV swarms. The authors present the convergence analysis of FL while highlighting the communication rounds needed to obtain FL convergence. Using this analysis, a joint power allocation and scheduling optimization problem is then formulated and solved for the UAV swarm network in order to minimize the FL convergence time. The proposed problem considers the resource limitations of UAVs in terms of: (1) the strict energy limitations due to the energy consumed by learning, communications, and ﬂying during FL convergence; and (2) delay constraints imposed by the control system that guarantees the stability of the swarm.", "cites": [3434, 3432], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from two papers by connecting the implementation of FL in UAV swarms for trajectory planning and air quality sensing, showing some integration of ideas. It includes a critical discussion of challenges like wireless communication factors and resource limitations, though deeper evaluation or comparison is limited. The section begins to generalize the concept of FL in aerial environments but does not fully abstract overarching principles or frameworks."}}
{"id": "77508e12-2980-420b-9045-084b9804e210", "title": "Lessons learned", "level": "subsubsection", "subsections": [], "parent_id": "c4778902-b31a-4153-8252-7d9c704e337a", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive training"], ["subsection", "Federated Learning"], ["subsubsection", "Lessons learned"]], "content": "Despite the prompt development of diverse DL techniques in different areas, they still impose a major challenge, which is: How can we efficiently leverage the massive amount of data generated from pervasive IoT devices for training DL models if these data cannot be shared/transferred to a centralized server? \n\\begin{itemize}\n\\item FL has emerged as a promising privacy-preserving collaborative learning scheme to tackle this issue by enabling multiple collaborators to jointly train their deep learning models, using their local-acquired data, without the need of revealing their data to a centralized server .  \n\\item  The model aggregation mechanisms are the most discussed in the FL literature, which are applied to address the communication efficiency, system and model performance, reliability issues, statistical heterogeneity, data security, and scalability. More specifically, one-layer FL approaches are the most studied by previous works, even if researchers are recently investigating decentralized strategies.\n\\item A major dilemma in FL is the large communication overhead associated with transferring  the models' updates. Typically, by following the main steps of FL protocol, every node or collaborator has to send a full model update in every communication round. Such update follows the same size of the trained model, which can be in the range of gigabytes for densely-connected DL models . Given that large number of communication rounds may be needed to reach the FL convergence on big datasets, the overall communication cost of FL can become unproductive or even unfeasible. Thus, minimizing the communication overheads associated with the FL process is still an open research area.\n\\item  We also remark that despite the considerable presented studies that have provided significant insights about different FL scenarios and user selection schemes, optimizing the performance and wireless resources usage for edge-assisted FL is still missing. Most of the existing schemes for FL suffer from slow convergence. Also, considering FL schemes in highly dynamic networks, such as vehicular networks, or resource-constraint environments, such as healthcare systems, is still challenging.\n\\end{itemize}\n\\begin{comment}\n\\begin{figure}[!h]\n\\centering\n\t\\includegraphics[scale=0.635]{Figures/OL_1.pdf}\n\t\\caption{Outline of online learning.}\n\t\\label{OL_outline}\n\\end{figure}\n\\end{comment}", "cites": [7138], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of federated learning (FL) in the context of pervasive AI for IoT, highlighting challenges such as communication overhead and convergence issues. It shows moderate synthesis by connecting different aspects of FL from the literature, but integration is limited. Critical analysis is evident in the identification of open research areas and limitations of existing FL schemes, particularly in dynamic or resource-constrained environments. The section abstracts to a degree by discussing broader themes like scalability and system performance, but it does not present a novel or overarching framework."}}
{"id": "1c9276dc-6802-455a-a4cb-16ea213c580b", "title": "Distributed Bandits Formulations", "level": "paragraph", "subsections": [], "parent_id": "10fee397-c412-4448-a2a8-65fc32b0b26c", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive training"], ["subsection", "Multi-agent reinforcement learning"], ["subsubsection", "Multi-agent multi-arm bandit learning"], ["paragraph", "Distributed Bandits Formulations"]], "content": "In many bandit problem instances, it is appealing to employ more agents to learn collaboratively and concurrently to speed up the learning process. In the distributed bandit problem, there exists a set of agents $[M]$ collaborating to solve the \\emph{same} bandit instance (the $K$ arms are the same). These agents communicate according to some networking topologies. In many contexts, the sequential decision-making problem at hand is distributed by nature. For example, we can consider a recommender system deployed over multiple servers in different locations. While every server aims to always recommend the best item, it is intuitive to reuse each other's experiences and cut the time needed to learn individually. Furthermore, since their communication may violate the latency constraints, it is desirable that this collaboration and reuse of experience achieve minimum communication overhead. \nWhile the classical single-agent bandit algorithm has been proposed since the $~2002$, its multi-agent counterpart is much more recent, with new state-of-the-art algorithms being currently proposed. The work in  initiated the interest in the communication-regret trade-off. The authors established a non-trivial bound on the regret, given an explicitly stated bound on the number of exchanged messages. However, they focused on the full-information setting, assuming that the agents observe the rewards of all actions at each round, and not only the one picked, which is the case in bandit settings. Nonetheless, this work initiated the interest in studying the same trade-off under the bandit settings. The authors of  considered the partial feedback (i.e., bandit settings) and presented an optimal trade-off between performance and communication. This work did not consider regret as the performance criterion but rather assumed the less common ``best arm identification\" setup, where the goal is to purely explore in order to eventually identify the best arm with high probability after some number of rounds. The authors in  studied the regret of distributed bandits with a gossiping-based P2P communication specialized to their setup, where at every step, each agent communicates only with two other agents randomly selected.  studied the regret under the assumption that the reward obtained by each agent is observable by all its neighbors.  proposed a collaborative UCB algorithm on a graph-network of agents and studied the effect of the communication graph topology on the regret bound.  improved this line of work as the approach requires less global information about the communication graph by removing the graph dependent factor multiplying the time horizon in the regret bound. \nOther works go beyond merely studying the effect of the network topology on the regret bound and explicitly accounting for the communication resources to use. The authors in  deduced an upper bound on the number of needed communicated bits, proving the ability to achieve the regret bound in  with a finite number of communication bits. However, the interesting question, particularly from the perspective of pervasive computing design, is whether the use of communication resources can also be bounded, i.e., can the order of optimal regret bound be guaranteed with a maximum number of communicated bits / communicated messages.\nThe work in  established the first logarithmic upper bound on the number of communication rounds needed for an optimal regret bound. The authors considered a complete graph network topology, wherein a set of agents are initialized with a disjoint set of arms. As time progresses, a gossiping protocol is used to spread the best performing arm with agents. The authors showed that, with high probability, all agents will be aware of the best arm while progressively communicating at less (halving) periods. The authors generalized this work with a sequel formulation , which relaxes the assumption of a complete graph, and introduces the option for agents to pull information. However, this approach is still using the same gossiping style of communication. According to , this dependence on pair-wise gossiping communication results in a sub-optimal instance-independent regret bound. The authors in  focused on the regret-communication trade-off in the distributed bandit problem. The networking model utilizes a central node that all agents communicate with. Initially, agents work independently to eliminate bad arms. Then, they start communicating with the central node at the end of each epoch, where epochs' duration grows exponentially, leading to a logarithmic bound on the number of needed messages. \n \\begin{table*}[!h]\n\\centering\n\\footnotesize\n\\tabcolsep=0.09cm\n\\caption{ Multi-agent stochastic bandit learning literature.}\n\\label{table:MAB_previous_work}\n\\begin{tabular}{|c|c|c|c|c|c|}\n\\hline\n\\textbf{Refs}  & \\shortstack{\\textbf{Problem} \\\\ \\textbf{Formulation}} &\t\\shortstack{\\textbf{Communication} \\\\ \\textbf{Model}} &    \\shortstack{\\textbf{Communication}\\\\\\textbf{Guarantee}}\t& \\textbf{Regret Guarantee} & \\textbf{Method}\\\\  \\hline\nP2P-$\\epsilon$-Greedy  & DB & Two Neighbors on a graph & $O(T)$& $O(T)$ & Gossiping arms estimates. \\\\ \\hline \ncoop-UCB2  & DB & Neighbors on a graph & $O(T)$& $O(logT)$ & \\shortstack{A running Consensus on \\\\the estimates of arms total rewards.}  \\\\ \\hline\nUCB-Network  & DB & \\shortstack{Multiple \\\\ (Graph and centralized)} & $O(T)$& $O(logT)$ & \\shortstack{Identifying and utilizing \\\\dominating sets in the network.} \\\\ \\hline \nDDUCB  & DB & Neighbors on a graph & $O(T)$& \\shortstack{$O(logT)$ \\\\ (with improved constants)}& \\shortstack{A running Consensus on \\\\the estimates of arms total rewards.} \\\\ \\hline\n  & DB & \\shortstack{Individual neighbors \\\\ on a complete graph} & $O(constant)$& $O(logT)$ & \\shortstack{Gossiping among different \\\\local Poison clocks.} \\\\ \\hline \nGosInE   & DB & \\shortstack{Neighbors on \\\\ a complete graph}& $\\Omega(T)$& $O(logT)+C_G$ & \\shortstack{Gossiping and information \\\\ pulling.}\\\\ \\hline \nDPE2   & DB & Neighbors on a graph & $O(constant)$& $O(logT)$ & \\shortstack{Leader-election to handle \\\\exploration (exploration is centralized).} \\\\ \\hline \nDEMAB   & DB & Centralized coordinator & $O(logT)$& $O(logT)$ &\\shortstack{ Utilizing public randomness \\\\to divide arms among clients. }\\\\ \\hline \nLCC-UCB   & DB & \\shortstack{Multiple \\\\(Graph and centralized)} & $O(logT)$& $O(logT)$ &\\shortstack{ Communicating estimates \\\\after epochs of doubling lengths. }\\\\ \\hline \n  & FB & Neighbors on a graph & $O(logT)$ & $O(logT+C_G)$ & \\shortstack{Selecting the best arm \\\\according to voting.}\\\\ \\hline \nGossipUCB   & FB & Neighbors on a graph &$O(T)$ & \\shortstack{$O(max\\{logT,log_{C_G}N\\})$} & \\shortstack{Maintaining local belief \\\\that is updated through gossiping.} \\\\ \\hline \n  & FB & Centralized coordinator & $O(logT)$ & $O(logT)$ & \\shortstack{Aggregating estimates through \\\\the controller until a fixed point of time.} \\\\ \\hline\n  & FB & Centralized coordinator & $O(logT)$ & $O(logT)$ & \\shortstack{Mixed target learning objective\\\\ based on local and global objectives.}  \\\\ \\hline\n  & FB & Neighbors on a graph& $O(T)$& $O(logT)$& \\shortstack{Agent use estimates of their\\\\ neighbors weighted by a similarity metric.}\\\\ \\hline\n\\end{tabular}\n\\end{table*}\nThe work in  presents a state of the art distributed bandit learning algorithm. The authors proposed algorithms for both fully connected and partially connected graphs (i.e., assuming that every agent can broadcast to everyone and assuming that agents can communicate with a subset of the others). Similar to elimination-based algorithms, the proposed algorithm proceeds with epochs of doubling lengths, only communicating at the end of an epoch, thus guaranteeing a logarithmic need for communication resources. The communicated messages are only the ID of the action played most often. Furthermore, the regret is proved to be optimal even in instance independent problems, for reasonable values of the time horizon (i.e., $log(T)>2^{14}$). During each epoch, agents maintain a set of arms that are recommended by other agents at the end of previous epochs and implement a UCB algorithm among them.", "cites": [3435, 3439, 7723, 7724, 3436, 3437, 9098, 3438, 7722, 7725, 8658], "cite_extract_rate": 0.6875, "origin_cites_number": 16, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.8, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers by organizing them into a coherent narrative around communication models and regret guarantees in distributed bandit learning. It critically evaluates trade-offs such as communication overhead versus regret performance and identifies limitations in pairwise gossiping approaches. Furthermore, it abstracts these findings to highlight overarching principles in the design of communication-efficient multi-agent bandit algorithms."}}
{"id": "02c1cf97-fdba-478e-b148-fc34d1dd2599", "title": "Federated Bandits Formulations", "level": "paragraph", "subsections": [], "parent_id": "10fee397-c412-4448-a2a8-65fc32b0b26c", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive training"], ["subsection", "Multi-agent reinforcement learning"], ["subsubsection", "Multi-agent multi-arm bandit learning"], ["paragraph", "Federated Bandits Formulations"]], "content": "The federated bandit formulation, shown in Fig. \\ref{fed-bandits} (b), is a recently emerging framework akin to the federated learning framework discussed earlier. In this formulation, there exists a set of agents, each one is facing a \\emph{different} instance of the bandit (but the instances are related to each other). This is different from the distributed bandit formulation discussed in the previous sub-section, where a set of agents collaborate to solve the \\emph{same} instance of the multi-arms bandits. Recall that a bandit instance is determined by the mean reward vector $\\boldsymbol{\\mu}$. By ``related\" instances in the federated bandit settings, we mean that each local bandit instance is a \\emph{noisy} and potentially \\emph{biased} observation of the mean vector. In light of this, collaboration is necessary, as even perfect local learning algorithms might not perform adequately due to their biased observations. \nThe setting of federated bandits is first proposed by  (although not under the same term). The authors proposed an algorithm, where agents agree on the best global arm, and they all play it at the beginning of each round. In this way, communication is needed at the beginning of each round. Recently,  studied this federated setting, where the global arm mean vector is the average of the local ones. Although the authors did not propose a bound on the number of messages needed to be exchanged, the communication model considered a partially connected graph, where each agent communicates only with neighbors but with a focus on constrained communication resources. The algorithm contains two main steps: First, each agent shares a set of local information with other neighbors (the number of times an arm was sampled and its sampled mean). Second, a gossip update is performed, where each agent incorporates information received from neighbors in updating its estimate of each arm's mean.  \n presented a more general formulation, where the global mean vector is not necessarily the average of the local ones. Instead, the local means are themselves \\emph{samples} from a distribution whose mean is unknown. The local observation for each agent is, in turn, samples from the local distributions. The communication model is similar to supervised federated learning, where agents communicate periodically with an orchestrator that updates the estimates of arms payoffs and instructs the agents on which arms to keep and which to eliminate. Although the communication is periodic, the total number of communication rounds is bounded (logarithmic with respect to the horizon $T$). This is because the number of agents incorporated in the learning process decays exponentially with time. Such an approach works since the average of clients' local means concentrates exponentially fast around that global mean (a known result from probability concentration analyses). \nA setting that is slightly different from the federated bandits was studied in. The difference is that although agents have similar yet not identical local models, the reward for each agent is actually sampled from its local distribution. Thus, each agent is trying to identify the best arm in its local instance through using information from other ones on arms that are similar. This work is different from other aforementioned approaches where the agents' rewards are sampled from the \\emph{global} distribution that they are collaboratively trying to estimate from biased local observations. \nTable \\ref{table:MAB_previous_work} summarizes the works in MAB problems. It lists the problem formulation: distributed Bandits (DB) or federated Bandits (FB), the communication model (i.e., the network topology), the communication guarantee (i.e., number of messages needed to achieve the performance), the regret guarantee (i.e., the growth of the regret with respect to the time horizon),  and the main characteristics of the method, which describes how the rewards' estimates are communicated among the agents (Recall that the agents aim to collectively learn an accurate estimates of the rewards distributions). $C_G$ denotes a constant related to the communication graph or gossiping matrix and $N$ is the number of agents.", "cites": [3439, 7725], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.3, "critical": 3.7, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the two cited papers, integrating their formulations of federated bandits and highlighting differences such as the nature of the global mean vector and communication models. It provides a critical comparison of the communication and learning strategies used, including the trade-offs between centralized coordination and decentralized gossip-based updates. The abstraction is strong, as it generalizes the problem into a broader framework of decentralized multi-agent learning with noisy and biased observations."}}
{"id": "2cbf2ee9-d88a-4258-8086-3e1b5a6d98bc", "title": "Use case: MAB for recommender systems", "level": "paragraph", "subsections": [], "parent_id": "10fee397-c412-4448-a2a8-65fc32b0b26c", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive training"], ["subsection", "Multi-agent reinforcement learning"], ["subsubsection", "Multi-agent multi-arm bandit learning"], ["paragraph", "Use case: MAB for recommender systems"]], "content": "Online learning systems are fundamental tools in recommender systems, which are, in turn, a cornerstone in the development of current intelligent user applications, from social media application feeds to content caching across networks. Due to the recent growth in data generation, local geo-distributed servers are often used to support applications that utilize recommender systems. Furthermore, privacy concerns sometimes limit the ability of these local servers to share data with other servers. The work in  studies the case of a set of servers that run a recommender system for their prospective clients. The goal of each one is to recommend the most popular content across all servers. However, due to latency constraints, communication at every decision-making step is infeasible. Besides, sharing individual samples of rewards violates privacy, as all servers will learn about a particular user's choice and preference. Due to these reasons, the authors proposed and utilized a federated bandits algorithm (Fed-UCB) which only communicates $logT$ times in a $T$ horizon to minimize recommendation latency. At each round, only the sample \\emph{means} are communicated, preserving a certain level of privacy (additional improvements are also discussed). Finally, the performance of the system is shown to be near-optimal; thus, achieving the goal of recommending the best item across all servers while meeting the privacy and communication constraints.", "cites": [7725], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of a specific use case (MAB for recommender systems) by integrating the cited paper's approach (Fed-UCB) with broader challenges like privacy and communication latency. It synthesizes the problem context and the proposed solution coherently but does not compare Fed-UCB with other approaches or provide deeper critique. The abstraction is moderate, highlighting general issues (privacy, latency), but does not extend to a broader theoretical framework or meta-level insight."}}
{"id": "77bb70d1-31f6-4366-aae9-289d77691899", "title": "Lessons Learned", "level": "paragraph", "subsections": [], "parent_id": "10fee397-c412-4448-a2a8-65fc32b0b26c", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive training"], ["subsection", "Multi-agent reinforcement learning"], ["subsubsection", "Multi-agent multi-arm bandit learning"], ["paragraph", "Lessons Learned"]], "content": "\\begin{itemize}\n    \\item  Distributed bandits formulations are the most popular in the literature compared to the recent federated formulation. Specifically, we note that distributed bandits with gossip-style communication, like the one introduced in , are a prevalent choice despite their sub-optimal communication resource utilization. This is attributed to the balance between complexity and the robustness resultant from the lack of a central controller.\n    \\item Communication-cognizant Multi-agent Bandit formulations: Online-learning systems need to account for the communication resources. Thus, recent works do not only analyze regret but explicitly optimize the communication resources. This is manifested through two main observations. First, the derived regret guarantees are always affected by the networking topology (e.g., parameters representing the connectivity of a communication graph, number of involved agents, or number of exchanged messages). Second, accompanied by every regret guarantee, an upper bound on communication resource usage is also provided (e.g., the maximum number of exchanged messages or exchanged bits).\n    \\item Towards the federation of the bandit framework: When the bandit instances faced by each agent are local biased instances, the federated bandits framework arises. In such a situation, agents need to learn with the help of a logically centralized controller, similar to supervised federated learning, in order to estimate the true global instance and the true best action. However, if agents are not interested in solving a hidden global instance but rather only their own, they may reuse their peers' experience and an instance-similarity metric to help them solve their own instances .\n\\end{itemize}", "cites": [7725, 7723], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section effectively synthesizes key ideas from the cited papers, connecting distributed and federated bandit formulations. It critically analyzes trade-offs such as communication resource utilization and robustness, and abstracts broader trends like the impact of network topology on regret and communication efficiency. These insights suggest a deep engagement with the literature."}}
{"id": "43587b7d-943c-4595-8dee-22efc7f3cf8b", "title": "Multi-agent Markov decision process learning", "level": "subsubsection", "subsections": ["1c4d2079-0573-4572-94af-65d6a420cc4f", "b6524c20-563a-4101-892f-233c30c024df", "9c26dfa8-82e0-4cde-ba5b-bea1e5f72c4a", "cf290967-0002-4f65-a8cd-25c83dfb9eb3", "df6238e4-c9ad-4836-a1d4-c0085a69b492"], "parent_id": "da7d1e06-40e4-4c83-bd80-f6bc0ee710aa", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive training"], ["subsection", "Multi-agent reinforcement learning"], ["subsubsection", "Multi-agent Markov decision process learning"]], "content": "This section presents an overview of Multi-agent MDP from a pervasive computing perspective. We specifically focus on the \\emph{communication-performance} trade-off and classify previous works according to their approach to handle this trade-off. We note that our perspective is different from previous surveys (i.g., , ), which studied the technical merits and demerits of the learning algorithms. Instead, we are interested in the \\emph{systems} aspects of the considered works. That is, what are the communication topology and protocol used between agents and how do these choices affect the performance (rewards obtained by all agents).", "cites": [3440], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section attempts to synthesize information by focusing on the communication-performance trade-off in multi-agent MDP learning, distinguishing itself from prior surveys by emphasizing systems aspects. However, it lacks detailed integration of multiple cited works into a cohesive framework and offers limited critical evaluation or deep comparison of approaches. It identifies a general abstraction (trade-off between communication and performance) but does not fully explore broader patterns or principles across the literature."}}
{"id": "b6524c20-563a-4101-892f-233c30c024df", "title": "Centralized training and Decentralized Execution (CTDE)", "level": "paragraph", "subsections": [], "parent_id": "43587b7d-943c-4595-8dee-22efc7f3cf8b", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive training"], ["subsection", "Multi-agent reinforcement learning"], ["subsubsection", "Multi-agent Markov decision process learning"], ["paragraph", "Centralized training and Decentralized Execution (CTDE)"]], "content": "The Centralized Training and Decentralized Execution (CTDE) approach is first proposed in . This approach leverages the assumption that, in most application scenarios, the initial training is done on centralized simulators, where agents can communicate with each other with no communication cost. This phase is denoted as centralized training. Then, at deployment, agents are assumed not to communicate at all, or conduct limited communication with each other, and they rely on their ``experience\" at the training phase to actually execute their collaborative policies.\n\\textbullet{ Communication only at training:}\nThe advantage of such an approach is that it does not require communication between agents upon deployment and thus incurs no communication cost. However, this comes at the cost of losing adaptability, which is the major motivation behind online learning. Such loss might occur in case of a major shift in the environment model between the training and deployment, where the learned coordinated policies are no longer performant, and new coordination is needed. The main workaround is to monitor the agents' performance and re-initiating the centralized training phase to learn new coordinated policies whenever needed.\nThis approach has been popularized by recent methods such as VDN , QMIX , and QTRAN . These works adopt \\emph{value function factorization} technique, where factorizations of the global value function in terms of individual (i.e., depending only on local observations) value function are learned during centralized training. Then, the global function (i.e., neural network) can be discarded at execution time, and each individual agent utilizes only the local function. When each agent acts greedily according to its local network, the global optimality can still be guaranteed since, at the training phase, these local networks were trained according to gradient signals with respect to the global reward.\nAnother approach to solving POMG is actor-critic. The CTDE version of actor-critic approaches is represented by learning a centralized critic, which evaluates the global action, and decentralized policy network, which takes an action based only on local observation. During training, the actor-critic networks are jointly learned, and hence the global critic ``guides\" the training of the actors. Then, at execution, the global critic may be discarded, and only actors can be used. The works in  present a deep deterministic policy gradient method that follows the described approach, where each agent learns a centralized critic and decentralized actor. Similarly,  follows the same approach, but all agents learn the same critic. Multiple other variations are done on the same DDPG algorithm aiming to either enhance performance  through incorporating an attention-mechanism, or reducing the use of communication resources (limited budget on the number of messages, or designing the message as (a part of) an agent's state). .\n\\begin{table*}\n\\centering\n\\footnotesize\n\\tabcolsep=0.09cm\n\\caption{ Communication-Cognizent Multi-Agent Reinforcement Learning literature.}\n\\label{table:rl_previous_work}\n\\begin{tabular}{|c|c|c|c|c|c|}\n\\hline\n\\textbf{Refs}  & \\textbf{Framework} &\t\\shortstack{\\textbf{Learning} \\\\ \\textbf{algorithm}} &    \\shortstack{\\textbf{Communication}\\\\\\textbf{scheme}}\t& \\shortstack{\\textbf{Communication}\\\\\\textbf{decision}}  \\\\  \\hline\n\\shortstack{VDN , QMIX ,\\\\ QTRAN } & CTDE & Value-based &NA& \\shortstack{Always during training,\\\\ None at execution.} \\\\  \\hline\n\\shortstack{MADDPG , \\\\ COMA} & CTDE & Actor-critic-based &NA& \\shortstack{Always during training, \\\\ None at execution.}  \\\\  \\hline\n IMAC & CTDE with learned  comm. & Policy gradient & \\shortstack{Learned source \\\\ and destination} &\\shortstack{At every step (limited size)}  \\\\  \\hline\n ATOC & CTDE with learned  comm. & Actor-critic based & \\shortstack{Gated communication \\\\with neighbors} &\\shortstack{When network topology \\\\changes.}  \\\\  \\hline\n IC3Net  & CTDE with learned  comm. & Policy gradient & \\shortstack{Gated communication \\\\with neighbors} &\\shortstack{ Communicate when necessary, \\\\possibly many messages per round. } \\\\  \\hline\n ACML &CTDE with learned  comm. & Actor-critic based  &  \\shortstack{Gated communication \\\\with neighbors} & \\shortstack{Communicate when necessary, \\\\respecting a limited bandwidth.}   \\\\  \\hline\n & Fully decentralized & Value-based & Indirect & No message passing.  \\\\  \\hline\n& Fully decentralized & actor-critic-based & With neighbors & At every step.   \\\\  \\hline\n& Fully decentralized & actor-critic-based & With neighbors &  At every step (limited size).  \\\\  \\hline\n& Fully decentralized & Policy gradient & \\shortstack{Broadcast to all through \\\\central controller} & At every step.   \\\\  \\hline\n\\end{tabular}\n\\end{table*}\n\\textbullet{ Learned communication:}\nAn important line of work within the MARL community is the study of learned communication between agents. In these settings, agents are allowed to send arbitrary bits through a communication channel to their peers in order to convey useful information for collaboration. These agents need to learn \\emph{what} to send, and \\emph{how} to interpret the received messages so that they inform each other of action selection. Thus, the agents are effectively learning communication protocols, which is a difficult task .\nWhile the learned communication can be trained centrally and executed in a decentralized fashion, agents can still communicate at the execution phase through a limited bandwidth channel. Hence, we distinguish this setting from the works discussed in the previous subsection. Yet, similar approaches can be followed. For example, discarding the critic in execution (sometime used interchangeably with the term CTDE) but still maintaining the learned communication  and parameter sharing and gradient pushing in , where in execution, these messages are discretized.\nWithin the learned communication line of work, the authors in  aimed to learn to schedule communication between agents in a wireless environment and focused only on collision avoidance mechanism in the wireless environment. In , information theoretic approach was used to compress the content of the message. In addition, source and destination are also learned through a scheduler. On the other hand, a popular line of work targeted the design of the so-called \\emph{gating mechanism} techniques in order to accomplish the efficiency of the learned communication protocols. In this line of work, agents train a gating network, which generates a binary action to specify whether the agent should communicate with others or not at a given time step, limiting the number of communicated bits/messages needed to realize a certain desirable behavior.  investigates the adaptability of these communication protocols and demonstrates the importance of communicating only with selected groups. Specifically, agents cannot distinguish messages that are particularly important to them (i.e., have implications on their actions) from the messages of all other agents. Thus, they introduce an attention scheme within each agent where an attention unit receives encoded local observation and action intention of the agent to decide whether a communication with others in its observable field is needed. The communication group dynamically changes and persists only when necessary.\nThe authors in  looked at \\textit{communication at scale} and proposed an Individualized Controlled Continuous Communication Model (IC3Net), where agents are trained according to their own rewards (hence the approach can work for competitive scenarios also). Then, they demonstrated that their designed gating mechanism allows agents to block their communication, which is useful in competitive scenarios and reduces communication in cooperative scenarios by opting out from sending unnecessary messages. However, the effect of the gate on communication efficiency was not thoroughly studied, and the focus was instead on the emerging behavior. The work in  presents the state-of-the-art on efficient learned communication. The authors introduced Actor-Critic Message Learner (ACML), wherein the gate adaptively prunes less beneficial messages. To quantify the benefit of an action, Gated-ACML adopts a global Q-value difference as well as a specially designed threshold. Then, it applies the gating value to prune the messages, which do not hold values. The authors showed that surprisingly, not only the communication-efficiency significantly increases, but in specific scenarios, even the performance improves as a result of well-tuned communication. The reason behind this is that, since the communication protocol is learned, it is probable to hold redundant information that agents do not decode successfully. The proposed gating mechanism can also be integrated with several other learned communication methods.", "cites": [7726, 3444, 3448, 3450, 3441, 3449, 3442, 3447, 3446, 8659, 3443, 3445], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.8, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section provides a strong synthesis of CTDE methods, connecting key works like VDN, QMIX, QTRAN, and MADDPG, and contrasts them with learned communication approaches. It offers critical evaluation by highlighting trade-offs between adaptability and communication cost, as well as limitations in specific methods like the lack of thorough study on IC3Net's communication efficiency. The section abstracts concepts such as communication protocols, value function factorization, and gating mechanisms, contributing to a broader understanding of CTDE in MARL."}}
{"id": "9c26dfa8-82e0-4cde-ba5b-bea1e5f72c4a", "title": "Fully Decentralized Agents", "level": "paragraph", "subsections": [], "parent_id": "43587b7d-943c-4595-8dee-22efc7f3cf8b", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive training"], ["subsection", "Multi-agent reinforcement learning"], ["subsubsection", "Multi-agent Markov decision process learning"], ["paragraph", "Fully Decentralized Agents"]], "content": "In fully decentralized reinforcement learning, there is no distinction between training and testing environments. Thus, the communication model stays the same throughout the agents' interaction with the environment. Under these settings, we recognize two extreme cases. First, agents do not communicate with each other, and learn to coordinate solely through the obtained rewards. In the case of no communication, the major challenge faced by the agents is the non-stationarity of the environment. A non-stationary environment from the perspective of the agents is when the distribution of the next states varies for the same current state and action pairs. The fully decentralized DRL was recently popularized by . In , the authors proposed a 3-dimensional reply buffer whose axes are the episode index, timestep index, and agent index. It was illustrated that conditioning on data from that buffer helps agents to minimize the effect of the perceived non-stationarity of the environment.\nOn the other extreme, agents can be modeled to be able to communicate at every step. Specifically, the problem of graph networked agents is investigated in . In this paper, agents are connected via a time-varying and possibly sparse communication graph. The policy of each agent takes actions that are based on the local observation and the neighbors' messages to maximize the globally averaged return. The authors fully decentralized actor-critic algorithms and provided convergence guarantees when the value functions are approximated by linear functions. However, a possible disadvantage of this algorithm is that the full parameter vector of the value function is required to be transmitted at each step. This has been addressed in , where also graph-networked agents are assumed, but each agent broadcasts only one (scaled) entry of its estimate of parameters. This significantly reduces communication cost (given that it occurs at every iteration). The paper also does not assume a bidirectional communication matrix and deals with only unidirectional ones, which is a more general formulation that appeals to more applications. The decentralized actor-critic-based algorithm also solves the distributed reinforcement learning problem for strongly connected graphs with linear value function approximation.\n considered the communication efficiency in fully decentralized agents, but with the assumption of a centralized controller. The paper utilizes policy gradient solution methods, where the controller aggregates the gradients of the agents to update the policy parameters. This process is akin to federated learning clients selection. The authors propose a process to determine which clients should communicate to the controller based on the amount of progress in their local optimization. They also propose a methodology to quantify the importance of local gradient (i.e., the local optimization progress) and then only involve agents who are above a certain threshold. Following this approach, the authors showed that the performance (i.e., cumulative reward) is similar to the case where all clients are participating, with considerable communication round savings.\nTable \\ref{table:rl_previous_work} summarises the works discussed above according to their communication model and the approach in handling the communication-performance tradeoff. We first identify the framework (CTDE, CTDE with learned communication, or fully decentralized) as well as the learning framework (value, policy gradient, or actor-critic). Note that these MARL algorithmic frameworks, which are based on a single-agent variant of the problem, involve learning the state space transition operator as it plays a major role in estimating the future expected sum of rewards. \nThen, we list two important configurations. First, the communication scheme, which states \\emph{how} agents communicate with each other. In CTDE, the training is done in simulation. Thus, agents are logically centralized and do not communicate. If no messages are passed between agents and their collaboration is solely learned through rewards, then the communication scheme is \\textit{indirect}. Otherwise, it is either \\textit{gated with neighbors} directly or through a \\textit{central controller}. Lastly, the communication decision states \\emph {when} the communication is made, which can be at every step (with optimized message size or not), or according to other conditions as detailed in the discussion.", "cites": [7726, 3445], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers on fully decentralized multi-agent reinforcement learning, integrating their approaches and identifying key distinctions such as communication schemes and decision timing. It provides a critical perspective by highlighting disadvantages (e.g., full parameter transmission) and how later works address them. While it introduces a framework for categorizing communication models, the abstraction remains within the context of MARL and does not reach a meta-level generalization."}}
{"id": "df6238e4-c9ad-4836-a1d4-c0085a69b492", "title": "Lessons learned", "level": "paragraph", "subsections": [], "parent_id": "43587b7d-943c-4595-8dee-22efc7f3cf8b", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive training"], ["subsection", "Multi-agent reinforcement learning"], ["subsubsection", "Multi-agent Markov decision process learning"], ["paragraph", "Lessons learned"]], "content": "\\begin{itemize}\n    \\item  Most of the MDP MARL works focused on performance gains and benchmarking, with little attention to resource utilization. This is because MARL is applied for games and other areas (e.g., robotics) where the priority is for performance. IoT applications, where resource utilization plays major role, is yet to make full use of state-of-the-art MARL algorithms.\n    \\item \\textit{CTDE-a practical middle ground}: We note that CTDE is the most adopted in pervasive/IoT scenarios. We attribute this to the simplicity in the way agents communicate in this framework. Specifically, CTDE algorithm leverages the fact that training is often done in simulators, where there is no communication cost, and agents may share experience tuples, network parameters, and observations freely, in order to train policies that can be executed later on, based on only local observations. This approach seems to model most of the pervasive computing applications where agents do not need to start training while being decentralized. In this framework, the actor-critic-based algorithms are more popular, where a centralized critic network that uses the observations of all agents guides the training of a decentralized policy network that uses only the local observations. The critic network can be discarded at execution time, enabling decentralized execution. The framework is emerging as a possible alternative to the fully decentralized extremes, where agents communicate at every step or do not communicate at all and try to indirectly and independently learn collaborative policies . \n    \\item \\textit{Scheduling for efficient learned communication}: In learned communication, agents learn to encode and decode useful messages. In this area, gating mechanisms are the main tools towards efficient communication . In gate design, agents learn when to send and refrain from sending a messages by quantifying the benefit (i.e., reward) of actions following this communication. More general \\emph{schedulers} modules investigate the design of communication module that learn to minimize the content of the messages as well (i.e., compressing the communication messages) . Overall, scheduling mechanisms are being increasingly used in MARL settings with learned communication, in order to face the limited bandwidth problems often encountered in practical scenarios.\n\\end{itemize}\n{", "cites": [3444, 3448, 3442, 3443], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple concepts from the cited papers to explain the trends and design choices in MDP-based MARL for pervasive AI in IoT. It critically assesses the trade-offs between communication efficiency and performance, and highlights the CTDE framework and scheduling mechanisms as practical solutions. The abstraction is strong, as it identifies broader principles like the balance between centralized training and decentralized execution."}}
{"id": "925ca5d6-5fa4-4ce1-a4e4-cdb64f74b962", "title": "Overview", "level": "paragraph", "subsections": ["a389510b-9c5d-412a-8879-96984e6bfae6", "a7289a44-2845-45ba-b0a2-8896e5daae85"], "parent_id": "73c038a7-dfdd-4ff9-b0ca-29be710e2068", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive training"], ["subsection", "Active Learning (AL)"], ["paragraph", "Overview"]], "content": "\\label{AL_Overview}\nThe main idea behind AL is that an active learner is allowed to actively select over time  the most informative data to be added to its training dataset in order to enhance its learning goals , . \nHence, in AL framework, the training dataset is not static, which means that the training dataset and learning model are progressively updated in order to continuously promote the learning quality  .   \nSpecifically, the main steps of AL are: (1) acquiring new data from the contiguous nodes; (2) picking out the most informative data to append to the training dataset; (3) retraining the learning model using newly-acquired data. Hence, the communication overheads associated with different AL schemes will depend on:\n\\begin{itemize}\n\t\\item Type and amount of exchanged data between the contiguous nodes. We remark here that contiguous nodes can exchange labels, features, or samples. Hence, based on the type and amount of changed data there will be always a tradeoff between enhancing the performance and decreasing communication overheads. \n\t\\item Number of selected nodes that will be considered in the AL process.  \n\\end{itemize}\nIt is worth mentioning also that FL allows multiple nodes to cooperatively train a global model without sharing their local data, which differs from AL in many ways. In particular, FL seeks for obtaining a synchronization between different cooperative nodes, in addition to the presence of a centralized node (or server) to generate the global model. Thus, AL and FL are addressing orthogonal problems – the former leverages the newly-acquired data from the contiguous nodes to retrain its model, while the latter trains its model in a distributed manner by sharing the model's updates with the contiguous nodes .", "cites": [1050, 3452, 3451], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of active learning by describing its core mechanism and key steps, and connects it to the broader context of pervasive training in IoT. It integrates concepts from the cited papers on cost-effective and progressive learning, but synthesis is limited to general descriptions without deeper unification of ideas. The critical analysis is minimal, primarily contrasting AL with FL in terms of objectives and architectures, but it does not delve into limitations or tradeoffs among the cited works. Abstraction is moderate, as it generalizes communication overhead factors and learning strategies, but stops short of offering high-level conceptual frameworks."}}
{"id": "a389510b-9c5d-412a-8879-96984e6bfae6", "title": "Applications of AL", "level": "paragraph", "subsections": [], "parent_id": "925ca5d6-5fa4-4ce1-a4e4-cdb64f74b962", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive training"], ["subsection", "Active Learning (AL)"], ["paragraph", "Overview"], ["paragraph", "Applications of AL"]], "content": "\\label{AL_Applications}\nTraditionally, AL algorithms depend on the presence of an accurate classifier that generates the ground-truth labels for unlabeled data. However, this  assumption becomes hard to maintain in several real-time applications, such as crowdsourcing applications and  automated vehicles. Specifically, in crowdsourcing, many sources   are typically weak labelers, which may generate noisy data, i.e., data that may be affected by errors due to low resolution and age of information problems.  \nHowever, most of the existing studies on AL investigate the noisy data  (or imperfect labels) effect on the  binary classification problems , while few works consider the general problem of multi-class or multi-labeled data  .  \nOne of the main problems in crowdsourcing is how to collect large amount of labeled data with high quality, given that the labeling can be done by volunteers or non-expert labelers. Hence, the process of acquiring large amount of labeled data turned to be challenging, computationally demanding, resource hungry, and often redundant. Moreover, crowdsourced data with cheap labels comes with its own problems. Despite being labels cheap, it is still expensive to handle the problem of noisy labels. \nThus, when data/labelers are not selected carefully, the acquired data may be very noisy , due to many reasons such as varying degrees of competence, labelers biases, and disingenuous behavior, which significantly affects the performance of supervised learning. \nSuch challenges have encouraged the researcher to design innovative schemes that can enhance the quality of the acquired data from different labelers. \nFor instance,   tackles the problem of demanding deep learning techniques to large datasets by presenting an AL-based solution that leverages multiple freely accessible crowdsourced geographic data to increase datasets' size. However, in order to effectively deal with the noisy labels extracted from these data and avoid performance degradation, the authors have proposed a customized loss function that integrates multiple datasets by assigning different weights to the acquired data based on  the estimated noise.  \n enhances the performance of supervised learning with noisy labels in crowdsourcing systems by introducing a simple quality metric and selecting the $\\epsilon$-optimal labeled data samples. The authors investigate the data subset selection problem based on the Probably Approximately Correct (PAC) learning model. Then, they consider the majority voting label integration method and propose two data selection algorithms that optimally select a subset of $k$ samples with high labelling quality. \nIn , the authors investigate the problem of   imbalanced noisy data, where the acquired labeled data are not uniformly distributed across different classes. \nThe authors therein aim to label training data given received noisy labels from diverse sources. Then, they used their learning model to predict the labels for new unlabeled data,  and update their learning model until some conditions are met (e.g., the performance of the learned model meets a predefined requirement, or it cannot be improved any more). Specifically, for labeled data, they implemented a label integration and data selection scheme that considers data uncertainty and class imbalance level, while  classifying the unlabeled data using the trained model before adding them to the training dataset. Hence, the proposed framework presents two core procedures: label integration and sample selection. In the label integration procedure, a Positive LAbel Threshold (PLAT) algorithm is used to infer the correct label from the received noisy labels of each sample in the training set. After that, three sample selection schemes are proposed to enhance the learning performance. These schemes are respectively based on the uncertainty derived from the received-noisy labels, the uncertainty derived from the learned model, and the combination method.  \nA different application of AL is investigated in , where AL is exploited for  incremental face identification. Conventional incremental face recognition approaches,  such as incremental subspace approaches, have limited performance on complex and large-scale environment. Typically, the performance may drastically drop when the training data of face images is either noisy or insufficient. Moreover, most of existing incremental methods suffer from noisy data or outliers when updating the learning model. Hence, the authors in  present an active self-paced learning framework, which combines: active learning and Self-Paced Learning (SPL). The latter refers to a recently developed learning approach that mimics the learning process of humans by gradually adding to the training set the easy to more complex data, where easy data is the one with high classification confidence. \nIn particular, this study aims to solve the incremental face identification problem by building a classifier that progressively selects and labels the most informative samples in an active self-paced way, then adds them to the training set. \nAL has been also considered in various applications of intelligent transportation systems. For instance, the authors in  investigate the vehicle type recognition problem, in which labeling a sufficient amount of data in surveillance images is very time consuming. To tackle this problem, this work leveraged fully labeled web data to decrease the required labeling time of surveillance images using deep transfer learning. Then, the  unlabeled images with  high uncertainty are selected to be queried in order to be added later to the training set. Indeed, the cross-domain similarity metric is linearly combined with the entropy in the objective function of the query criteria to actively select the best samples. \nUltimately, we highlight that most of the presented studies so far consider in their AL framework specific classifiers (or learning models), which cannot be easily used in other learning models . Accordingly, obtaining an optimal label integration and data selection strategy that can be used with a generic  multi-class classification techniques is still worth further investigation.}\n{", "cites": [3451, 3454, 3453], "cite_extract_rate": 0.3, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.7, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers on Active Learning (AL) in the context of noisy labels and resource constraints, creating a coherent narrative on challenges and strategies. It critically evaluates limitations such as model specificity and computational demands, and identifies broader patterns like the need for generic multi-class AL strategies. The abstraction is strong, as it generalizes across applications (crowdsourcing, face recognition, transportation systems) and highlights core procedures like label integration and sample selection."}}
{"id": "1fc2e5e9-36e2-4079-a976-bf35b5ed230d", "title": "Overview and definitions", "level": "paragraph", "subsections": [], "parent_id": "21a6c43d-7438-4644-821d-fd45df28a551", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive Inference"], ["subsection", "Profiling computation and communication models"], ["subsubsection", "Computation models"], ["paragraph", "Overview and definitions"]], "content": "\\mbox{}\\\\\n\\textbf{\\indent Binary offloading}:  Relatively simple or highly complex tasks that cannot be divided into sub-tasks and have to be computed as a whole, either locally at the source devices or sent to the remote servers because of resource constraints, are called binary offloading tasks. These tasks can be denoted by the three-tuple notation $T(K,\\tau, c)$. This commonly used notation illustrates the size of the data to be classified presented by $K$ and the constraint $\\tau$ (e.g., completion deadline, the maximum energy, or the required accuracy). The computational load to execute the input data of the DNN task is modeled as a variable $c$, typically defined as the number of multiplications per second .\nAlthough binary offloading has been widely studied in the literature, we note that it is out of the scope of this survey covering the pervasivity and distribution of AI tasks.\n\\newline\n\\textbf{ \\indent Partial offloading}: In practice, DNN classification is composed of multiple subtasks (e.g., layers execution, multiplication tasks, and feature maps creation), which allows to implement fine-grained (partial) computations. More specifically, the AI task can be split into two or more segments, where the first one can be computed at the source device and the others are offloaded to pervasive participants (either remote servers or neighboring devices). \n\\begin{figure}[h!]\n\\centering\n\t\\includegraphics[scale=0.6]{Figures/parallelization_2.pdf}\n\t\\caption{Inference parallelization: data and model parallelization}\n\t\\label{parallelization}\n\\end{figure}\n\\textbf{Data parallelization:} The most manageable task of partial offloading is the data parallelization, where duplicated offloaded segments are independent and can be arbitrarily divided into different groups and executed by different participants of the pervasive computing system, e.g., segments from different classification requests (as shown in Fig. \\ref{parallelization} (a)). We highlight that the input data to  parallel segments are independent and can be different or akin.\n\\textbf{Model parallelization:} A more sophisticated partial offloading pattern is the model parallelization, where the execution of one task is split across multiple pervasive devices. Accordingly, the input data is also split and fed to different parallel segments. Then, their outputs are merged again. In this offloading pattern, the dependency between different tasks cannot be ignored as it affects the execution of the inference. Particularly, the computation order of different tasks (e.g., layers) cannot be determined arbitrarily because the outputs of some segments serve as the inputs of others (as shown in Fig. \\ref{parallelization} (b)). In this context, the inter-dependency between different computational parts of the DNN model needs to be defined. It is worth mentioning that many definitions of data and model parallelism are presented in the literature, which are slightly different. In our paper, we opted for the definitions presented in .\n\\textbf{Typical dependencies:} Different DNN networks can be abstracted as task-call graphs.  These graphs are generally presented by Directed Acyclic Graphs (DAGs), which have a finite directed structure with no cycles. Each DNN graph is defined as $G(V,E)$, where the set of vertices $V$ presents different segments of the network, while the set of edges $E$ denotes their relations and dependencies. Typically, three types of dependencies contribute to determining the partition strategies, namely the sequential dependency which occurs in the conventional CNN networks with sequential layers and without any residual block (e.g., VGG ), the parallel dependency which depicts the relation between different tasks in the same layer (e.g., different feature maps transformations), and the general dependency existing in general DNN models (e.g., randomly wired CNN ). Different dependencies are depicted in Fig. \\ref{partitioning}. The required computation workload and memory are specified for each vertex $V$ and the amount of the input and output data can be defined on the edges.\n\\begin{figure}[!h]\n\\centering\n\t\\includegraphics[scale=0.6]{Figures/Partitionning_2.pdf}\n\t\\caption{Typical typologies of DNNs and partitioning strategies.}\n\t\\label{partitioning}\n\\end{figure}\nBased on the presented dependencies, two partition strategies can be introduced, namely per-layer and per-segment partitioning (see Fig. \\ref{partitioning}). Per-layer partitioning defines dividing the model into layers and allocating each set of layers within a pervasive participant (e.g., IoT device or remote servers). On the other hand, per-segment partitioning denotes segmenting the DNN model into smaller tasks such as feature maps transformations, multiplication tasks and even per-neuron segmentation.\n\\textbf{Computation latency:}\nThe primary and most common engine of the pervasive devices to perform local computation is the CPU. The performance of the CPU is assessed by cycle frequency/ clock speed $f$  or the multiplication speed $e$ . In the literature, authors adopt the multiplication speed to control the performance of the devices executing the deep inference. In practice, $e$ is bounded by a maximum value $e_{max}$ reflecting the limitation of the device computation capacity. Based on the model introduced for binary offloading, the computation latency of the inference task $T(K,\\tau,c)$ is calculated as follows :\n\\begin{equation}\n    \\begin{aligned}\n               t^c=\\frac{c}{e}.\n    \\end{aligned}\n    \\label{eq:1}\n\\end{equation}\nImportantly, a higher computational capacity $e_{max}$ is desirable to minimize the computation latency at the cost of energy consumption. As end-devices are energy constrained, the energy consumption of the local computation is considered as a key measure for evaluating the inference efficiency. More specifically, a high amount of energy consumed by AI applications is not desirable by end-devices due to their incurred cost. Similarly, significant energy consumption of edge nodes (e.g., access points or MEC servers.) increases the cost envisaged by the service providers.\n\\begin{table*}[]\n\\footnotesize\n\\tabcolsep=0.09cm\n\\caption{Characteristics of different splitting strategies:\\\\\n{\\footnotesize A: After, B: Before, $N_{Fc}$: number of fully connected layers, $n$: number of input neurons (Fc), $m$: number of output neurons (Fc), $H_1, W_1, D_1$: dimensions of the input data (Conv), $H_2, W_2, D_2$: dimensions of the output data (Conv), $H_f, W_f, D_1$: dimensions of the filter (Conv), $k/D_2$: number of filters, $d_x,d_y$: dimensions of the spatial splitting (Conv), $N$: Number of participants, $k'_i$: Number of segments per participant.}}\n\\label{tab:splitting}\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|}\n\\hline\n\\begin{tabular}[c]{@{}c@{}}\\textbf{Partitioning}\\\\ \\textbf{strategy}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\textbf{$N^{o}$ of smallest} \\\\\\textbf{segments}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\textbf{Activation}\\\\ \\textbf{task}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\textbf{Inputs}\\\\ \\textbf{per segment}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\textbf{Filters weights}\\\\ \\textbf{per device}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\textbf{Outputs}\\\\ \\textbf{per segment}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\textbf{Computation}\\\\ \\textbf{per segment}\\end{tabular} &\\begin{tabular}[c]{@{}c@{}}\\textbf{Transmitted data}\\\\ \\textbf{per layer}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\textbf{Merging}\\\\ \\textbf{strategy}\\end{tabular} \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Per-layer:\\\\ Fully-connected (Fc)\\end{tabular} & $N_{Fc}$ & A & $n$ & \\xmark & $m$& $n \\times m$ &$n+m$ & Seq\\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Per-segment: Output\\\\ splitting for Fc layers\\end{tabular} & $\\sum\\limits^{N_{Fc}}_{i=1} m_i$ & B/A & $n$ &  \\xmark& 1 & $n$  & $n \\times N +m$& Concat  \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Per-segment: Input \\\\ splitting for Fc layers\\end{tabular} & $\\sum\\limits_{i=1}^{N_{Fc}} n_i$& A & 1 & \\xmark & $m$ & $m$  & $N \\times m +n$& Sum \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Per-layer:\\\\ Convolution (Conv)\\end{tabular} & $N_{Conv}$ & A & $H_1 \\times W_1 \\times D_1$ & \\begin{tabular}[c]{@{}c@{}}$k \\times D_1 \\times$ \\\\ $(H_f \\times W_f)$\\end{tabular}  & $H_2 \\times W_2 \\times k$ &  \\begin{tabular}[c]{@{}c@{}} $cp= D_1 \\times $\\\\ $(W_f \\times H_f) \\times$  \\\\ $ k \\times (W_2 \\times H_2)$  \\end{tabular} & \\begin{tabular}[c]{@{}c@{}} $H_1 \\times W_1 \\times D_1 +$\\\\ $H_2 \\times W_2 \\times k$\\end{tabular} & Seq \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Per-segment: channel\\\\ splitting for Conv\\end{tabular} & \\begin{tabular}[c]{@{}c@{}} $ \\sum\\limits_{i=1}^{N_{Conv}}k_i$ \\end{tabular} & B/A & $H_1 \\times W_1 \\times D_1$ & \\begin{tabular}[c]{@{}c@{}}$k'_i \\times D_1 \\times$ \\\\ $(H_f \\times W_f)$\\end{tabular} &  $H_2 \\times W_2$ & $\\frac{cp}{k}$ &\\begin{tabular}[c]{@{}c@{}} $(N \\times H_1 \\times W_1 \\times D_1)$ \\\\$+ (k \\times H_2 \\times W_2)$\\end{tabular}  &Concat \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Per-segment: spatial\\\\ splitting for Conv\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}$\\sum\\limits_{i=1}^{ N_{Conv}}\\frac{H_1^i\\times W_1^i}{d^i_x \\times d^i_y}$ \\end{tabular}& B/A & \\begin{tabular}[c]{@{}c@{}}$\\frac{H_1 \\times W_1 \\times D_1}{d_x \\times d_y} +$ \\\\ $padding$\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}$k \\times D_1 \\times$ \\\\ $(H_f \\times W_f)$\\end{tabular} & $\\frac{H_2 \\times W_2 \\times k}{d_x \\times d_y}$ & $cp /\\frac{H_1 \\times W_1}{d_x\\times d_y}$&\\begin{tabular}[c]{@{}c@{}} $H_1 \\times W_1 \\times D_1 +$\\\\ $H_2 \\times W_2 \\times k+$ \\\\  $N\\times padding$\\end{tabular}& Concat \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Per-segment: filter \\\\ splitting for Conv\\end{tabular} & \\begin{tabular}[c]{@{}c@{}} $\\sum\\limits_{i=1}^{N_{Conv}}D^i_1\\times k_i$\\end{tabular} & A & $H_1 \\times W_1$ & \\begin{tabular}[c]{@{}c@{}}$k'_i \\times$ \\\\ $(H_f \\times W_f)$\\end{tabular} & $H_2 \\times W_2$ &$ \\frac{cp}{D_1 \\times k}$ & \\begin{tabular}[c]{@{}c@{}} $(D_1 \\times H_1 \\times W_1)+$ \\\\$ (N \\times H_2 \\times W_2 \\times k)$\\end{tabular}&  \\begin{tabular}[c]{@{}c@{}}Sum+ \\\\concat \\end{tabular}\\\\ \\hline\n\\end{tabular}\n\\end{table*}\n\\textbf{Computation energy:}\nIf the inference is executed at the data source, the consumed energy is mainly associated to the task computation. In contrast, if the task is delegated to remote servers or to neighboring devices, the power consumption consists of the required energy to transfer the data between participants, the amount of energy consumed for the computation of different segments, and the energy required to await and receive the classification results. Suppose that the inference task/sub-task $T_i$ takes a time $t^c_i$ to be computed locally in the device participating in the pervasive inference and let $P_i$ denote the processing power to execute the task per second. The energy consumed to accomplish an inference task $T_i$ locally at the computing device is equal to :\n\\begin{equation}\\label{eq:2}\n    \\begin{aligned}\n              e^{local}_i= t^c_i \\times P_i.\n    \\end{aligned}\n\\end{equation}\nNext, we profile the DNN partitioning strategies presented in the literature, in terms of computation and memory requirements first and then in terms of communicated  data to offload the output of segments. The key idea of partitioning a DNN network is to evenly or unequally distributing the computational load and the data weights across pervasive devices intending to participate in the inference process, while minimizing the classification latency. A partitioning can be achieved by simply segmenting the model per-layer or set of layers (see Fig. \\ref{parallelization} (a)) or by splitting the layers' tasks (see Fig. \\ref{parallelization} (b)). Then, each part is mapped to a participant.", "cites": [3455, 3357, 3413, 514, 7198], "cite_extract_rate": 0.7142857142857143, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the concepts of binary and partial offloading, drawing on multiple papers to present a structured overview of computation models, including data and model parallelism. It abstracts these ideas into a general framework using DAGs and partitioning strategies. While it provides some critical perspectives, such as the trade-off between computation latency and energy consumption, a deeper evaluation of the cited works’ limitations or assumptions is limited."}}
{"id": "f8893cd8-c14c-44f9-9a91-88831f18771f", "title": "Overview", "level": "paragraph", "subsections": [], "parent_id": "8ef4c5b6-14e4-460d-a25c-bb3d171f49bd", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive Inference"], ["subsection", "Profiling computation and communication models"], ["subsubsection", "Communication models"], ["paragraph", "Overview"]], "content": "\\mbox{}\\\\\n\\textbf{ \\indent Communication latency}: In the literature, the communication channels between different pervasive devices are abstracted as bit-pipes with either constant rates or random rates with a defined distribution. However, this simplified bit-pipe model is insufficient to illustrate the fundamental properties of wireless propagation. More specifically, wireless channels are characterized by different key aspects, including: (1) the multi-path fading caused by the reflections from objects existing in the environment (e.g., walls, trees, and buildings); (2) the interference with other signals occupying the same spectrum due to the broadcast nature of the wireless transmissions, which reduces their Signal-to-Interference-plus-Noise-Ratios (SINRs) and increases the probability of errors; (3) bandwidth shortage,  motivating the research community to exploit new spectrum resources, design new spectrum sharing and aggregation, and propose new solutions (e.g., in-device caching and data compression). Based on these characteristics, the communication/upload latency between two devices, either resource-constrained devices or high-performant servers, can be expressed as follows:\n\\begin{equation}\\label{eq:6}\n    \\begin{aligned}\n  t^u=\\frac{K}{\\rho_{i,j}},\n    \\end{aligned}\n\\end{equation}\nwhere $K$ is the size of the transmitted data and $\\rho_{i,j}$ is the achievable data rate between two participants $i$ and $j$.\n\\begin{comment}\ndefined as follows:\n\\begin{equation}\\label{eq:7}\n    \\begin{aligned}\n \\rho_{i,j}=B_i \\times log_2(1+\\Gamma_{i,j}),\n    \\end{aligned}\n\\end{equation}\n$B_i$ denotes the bandwidth of the device $i$. Furthermore, the average SINR of the link between $i$ and $j$, namely $\\Gamma_{i,j}$, is given by:\n\\begin{equation}\\label{eq:8}\n    \\begin{aligned}\n \\Gamma_{i,j}=\\frac{P_{i,j}h_{i,j}}{\\sum_{q, q\\neq j} I_{q,j} + \\sigma^2},\n    \\end{aligned}\n\\end{equation}\nwhere $P_{i,j}$ and $h_{i,j}$ are the transmit power and the channel gain between $i$ and $j$, $\\sigma^2$ is the Gaussian noise, and $\\sum_{q, q\\neq j} I_{q,j}$ is the total interference power at the receiver $j$ resulting from neighboring devices transmitting over the same channel. \n\\end{comment}\nThe total transmission latency $t^T$ of the entire inference is related to the type of dependency between different layers of the model. This latency is defined in eq. (\\ref{eq:9}), if the dependency is sequential (e.g., layers) and in eq. (\\ref{eq:10}) if the dependency is parallel (e.g., feature maps). In case the dependency is general (e.g., randomly wired networks), we formulate the total latency as the sum of sequential communication and the maximum of parallel transmissions.\n\\begin{equation}\\label{eq:9}\n    \\begin{aligned}\nt^T=\\sum_{s=1}^{S} t^u_s.\n    \\end{aligned}\n\\end{equation}\n\\begin{equation}\\label{eq:10}\n    \\begin{aligned}\nt^T= max(t^u_s, \\quad \\forall s \\in \\{1...S\\}).\n    \\end{aligned}\n\\end{equation}\n\\newline\n\\textbf{ \\indent Communication energy}: The energy consumption to offload the inference sub-tasks to other participants consists of the amounts of energy consumed on outwards data transmissions and when receiving the classification results generated by the last segment of the task $T$. This energy is formulated as follows :\n\\begin{equation}\\label{eq:3}\n    \\begin{aligned}\n              e^{ofd}_i= t^u_i. P_i+\\sum_s\\sum_k\\sum_j \\frac{K_s}{\\rho_{k,j}}.P_s.X_{k,s}X_{j,s+1},\n    \\end{aligned}\n\\end{equation}\nwhere $t^u_i$ is the upload delay to send the original data/task $i$ to the first participant, $K_s$ is the output of the segment $s$ (e.g., layers or feature maps), $\\rho_{k,j}$ denotes the data rate of the communication, and $X_{k,s}$ is a binary variable indicating if the participant $k$ executes the segment $s$. \nUsing only the onboard battery and resources, the source-generating device may not be able to accomplish the inference task within the required delays and the energy constraint. In such a case, partitioning the task among neighboring devices or offloading the whole inference to the remote servers are desirable solutions.", "cites": [3357], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of communication models in pervasive AI, integrating key aspects from the cited literature to explain wireless propagation challenges and formulate latency and energy consumption models. It synthesizes relevant concepts such as multi-path fading, interference, and bandwidth limitations into a coherent discussion. However, it lacks deeper critical analysis of the cited work and does not compare multiple approaches or highlight their strengths and weaknesses explicitly. The abstraction level is moderate, as it generalizes wireless communication properties and presents mathematical models to capture system behavior."}}
{"id": "f8b27998-c7bb-4ec7-a541-7fcabaa1cb5a", "title": "Per-layer distribution - one split point", "level": "paragraph", "subsections": [], "parent_id": "695e849b-fe1c-443f-9b10-15b02a8ec11b", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive Inference"], ["subsection", "Resource management for distributed inference"], ["subsubsection", "Remote collaboration"], ["paragraph", "Per-layer distribution - one split point"]], "content": "The partial offloading leverages the unique structure of the deep model, particularly layers, to allow the collaborative inference between the source device and the remote servers. More specifically, in such an offloading approach, some layers are executed in the data-generating device whereas the rest are computed by the cloud or the edge servers, as shown in Fig. \\ref{distribution} (b). In this way, latency is potentially reduced owing to the high computing cycles of the powerful remote entities. Furthermore, latency to communicate the intermediate data resultant from the DNN partitioning  should lead to an overall classification time benefit. The key idea behind the per-layer partitioning is that after the shallow layers, the size of the intermediate data is relatively small compared to the original raw data thanks to the sequential filters. This can speed up the transmission over the network, which motivates the partition at deep layers.  \nNeurosurgeon  is one of the first works that investigated layer-wise partitioning, where the split point is decided intelligently depending on the network conditions. Particularly, the authors examined deeply the status quo of the cloud and in-device inference and confirmed that the wireless network is the bottleneck of the cloud approach and that the mobile device can outperform the cloud servers only when holding a GPU unit. As a next step, the authors investigated the DNN split performance in terms of computing and output data size of multiple state-of-the-art DNNs over multiple types of devices and wireless networks and concluded that layers have significantly different characteristics. Based on the computation and the latency to transmit the output data of the DNN layers, the optimal partition points that minimize the energy consumption and end-to-end latency are identified. Finally, after collecting these data, Neurosurgeon is trained to predict the power consumption and latency based on the layer type and network configuration and dynamically partition the model between the data source and the cloud server.\nHowever, while the DNN splitting significantly minimizes the inference latency by leveraging the computational resources of the remote server, this strategy is constrained by the characteristics of intermediate layers that can still generate high-sized data, which is the case of VGG 16 illustrated in Fig. \\ref{VGG16}. \n\\begin{figure}[!h]\n\\centering\n\t\\includegraphics[scale=0.5]{Figures/VGG16.pdf}\n\t\\caption{The transmitted data size between different layers of the VGG16 network.}\n\t\\label{VGG16}\n\\end{figure}\n\\newline\nTo tackle the problem of sized intermediate data, the authors of  proposed to combine the early-exit strategy, namely BranchyNet , with their splitting approach. The objective is to execute only few layers and exit the model without resorting to the cloud, if the accuracy is satisfactory. In this way, the model inference is accelerated, while sacrificing the accuracy of the classification. We note that BranchyNet is a model trained to tailor the right size of the network with minimum latency and higher accuracy. Accordingly, both models cooperate to select the optimal exit and split points. \nThe authors extended the work by replacing both trained models with a reinforcement learning strategy , namely Boomerang. This RL approach offers a more flexible and adaptive solution to real-time networks and presents less complex and more optimal split and exit points’ selection. The early-exit strategy is also proposed along with the layer-wise partitioning by the ADDA approach , where authors implemented the first layers on the source device and encouraged the exit point before the split point to use only local computing and eliminate the transmission time.  Similarly, authors in , formulated the problem of merging the exit point selection and the splitting strategy, while aiming to minimize the transmission energy, instead of focusing on latency. \nIn addition to using the early-exit to accelerate the inference, other efforts adopted compression combined with the partitioning to reduce the shared data between collaborating entities. Authors in  introduced a distribution approach with feature space encoding, where the edge device computes up to an intermediate layer, compresses the output features (loss-less or lossy), and offloads the compressed data to the host device to compute the rest of the inference, which enhances the bandwidth utilization. To maintain  high accuracy, the authors proposed to re-train the DNN with the encoded features on the host side.  The works in  also suggested compressing the intermediate data through quantization, aiming at reducing the transmission latency between edge and cloud entities. The authors examined the trade-off between the output data quantization and the model accuracy for different partitioning scenarios. Then, they designed accordingly a model to predict the edge and cloud latencies and the communication overhead. Finally, they formulated an optimization problem to find the optimal split layer constrained by the accuracy requirements. To make the solution adaptive to runtime, an RL-based channel-wise feature compression, namely JALAD, is introduced by the authors in . Pruning is another compression technique proposed in  to be joined with the partitioning strategy. The authors introduced a 2-step pruning framework, where the first step mainly focuses on the reduction of the computation workload and the second one handles the removal of non-important features transmitted between collaborative entities, which results in less computational and offloading latency. This can be done by pruning the input channels, as their height, length, and number impact directly the size of the output data and the computation requirements, which we illustrated in Table \\ref{tab:splitting}.", "cites": [3458, 689, 3456, 3457], "cite_extract_rate": 0.4, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple approaches to per-layer distribution in DNN inference, connecting ideas from Neurosurgeon, BranchyNet, Boomerang, ADDA, JALAD, and a pruning-based method. It provides a critical perspective by pointing out limitations of intermediate data size and highlighting trade-offs between latency, accuracy, and transmission cost. The section abstracts these methods to emphasize broader strategies like early-exit, reinforcement learning, and data compression in the context of distributed AI for IoT."}}
{"id": "c3827174-8142-4574-b07e-3af2b4f28a29", "title": "Per-layer distribution - back and forth, and hierarchical distribution", "level": "paragraph", "subsections": [], "parent_id": "695e849b-fe1c-443f-9b10-15b02a8ec11b", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive Inference"], ["subsection", "Resource management for distributed inference"], ["subsubsection", "Remote collaboration"], ["paragraph", "Per-layer distribution - back and forth, and hierarchical distribution"]], "content": "Solely offloading the deep learning computation to the cloud can violate the latency constraints of the AI application requiring real-time and prompt intervention. Meanwhile, using only the edge nodes or IoT devices can deprive the system from powerful computing resources and potentially increase the processing time. Hence, a judicious selection of multiple cuts and distribution between different resources, i.e., IoT device – edge server – cloud, contribute to establishing a trade-off between minimizing the transmission time and exploiting the powerful servers. Additionally, the layers of the DNN model are not always stacked in a sequential dependency. More specifically, layers can be arranged in a general dependency as shown in Fig. \\ref{partitioning} (c), where some of them can be executed in parallel or do not depend on the output of the previous ones. In this case, adopting an optimized \\textit{back and forth} distribution strategy, where the end-device and the remote servers parallelize the computation of the layers and merge the output, can be beneficial for the inference latency. Authors in  designed a Dynamic Adaptive DNN Surgery (DADS) scheme that optimally distributes complex structured deep models, presented by DAG graphs, under variable network conditions. In case the load of requests is light, the min-cut problem  is applied to minimize the overall delay of processing one frame of the DNN structure. When the load condition is heavy, scheduling the computation of multiple requests (data parallelization) is envisaged using the 3-approximation ratio algorithm  that maximizes the parallelization of the frames from different requests. Complex DNN structures were also the focus in , where the authors used the shortest path problem to formulate the allocation of different frames of the DNN \\textit{back and forth} between the cloud and the end-device. The path, in this case, is defined as latency or energy of the end-to-end inference.\nOn the other hand, \\textit{hierarchical architecture} for sequential structures is very popular as a one way distribution solution to establish a trade-off between transmission latency and computation delay (see Fig. \\ref{distribution} (c)). The papers in  proposed to divide the trained DNN over a hierarchical distribution, comprising “IoT-edge-cloud” resources. Furthermore, they adopted the state-of-the-art work BranchyNet  to early exit the inference if the system has a good accuracy. In this way, fast, private, and localized inference using only shallow layers becomes possible at the end and edge devices, and an offloading to the cloud is only performed when additional processing is required. Hierarchical distribution can also be combined with compressing strategies to reduce the size of the data to be transmitted and accordingly minimize the communication delay and the time of the entire inference, such as using the encoding techniques as done in . Authors in  also opted for hierarchical offloading, while focusing primarily on fault-tolerance of the shared data. Particularly, authors in  considered two fault-tolerance methods, namely reassigning and monitoring, where the first one consists of assigning all layers tasks at least once, and then the unfinished tasks are reassigned to all participants regardless of their current state. This method, is generating a considerable communication and latency overhead related to allocating redundant tasks, particularly to devices with  limited-capacities. Hence, a second strategy is designed to monitor the availability of devices before the re-assignment. Finally, the work in  proposed to add skip blocks  to the DNN model and include at least one block in each partition, to enhance the robustness of the system in case the previous layer connection fails.\n\\begin{comment}\n\\begin{table*}[]\n\\centering\n\\footnotesize\n\\tabcolsep=0.09cm\n\\caption{Performance of different partitioning strategies per inference compared to the remote strategy.}\n\\label{tab:performance}\n\\begin{tabular}{|l|l|l|l|l|l|}\n\\hline\n\\begin{tabular}[c]{@{}l@{}}Partitioning\\\\ strategy\\end{tabular} & Latency & Bandwidth & Energy & computation/memory & throughput \\\\ \\hline\nNeurosurgeon  & 3.1 $\\times$  $\\rightarrow$ 40.7 $\\times$ & \\xmark & 59.5 \\% $\\rightarrow$ 94.7\\% & \\xmark & 1.5 $\\times$  $\\rightarrow$ 6.7 $\\times$ \\\\ \\hline\n\\begin{tabular}[c]{@{}l@{}}Edgent  \\\\ Boomerang \\end{tabular} & 4 $\\times$ & \\xmark & \\xmark & \\xmark & \\xmark \\\\ \\hline\nADDA  & 1.7 $\\times$  $\\rightarrow$ 6.6 $\\times$ & \\xmark & \\xmark & \\xmark & \\xmark \\\\ \\hline\n & \\xmark & \\xmark & 15.3 $\\times$ & \\xmark & 16.5 $\\times$ \\\\ \\hline\nJALAD  & 1.1 $\\times$  $\\rightarrow$ 11.7 $\\times$ & \\xmark & \\xmark & 1.4 $\\times$  $\\rightarrow$ 25.1 $\\times$ & \\xmark \\\\ \\hline\nJointDNN  & 18 $\\times$  $\\rightarrow$ 32 $\\times$ & \\xmark & 18 $\\times$  $\\rightarrow$ 32 $\\times$ & \\xmark & \\xmark \\\\ \\hline\n & 4.81 $\\times$ & 25.6 $\\times$ & \\xmark & 6.01 $\\times$ & \\xmark \\\\ \\hline\n & \\xmark & \\xmark & 0.016 J $\\rightarrow$ 0.0482 J & \\xmark & \\xmark \\\\ \\hline\nDADS  & 8.08 $\\times$ & \\xmark & \\xmark & 14.01 $\\times$ & \\xmark \\\\ \\hline\nAuto tuning  & 1.13 $\\times$ $\\rightarrow$ 1.7 $\\times$ & \\xmark & \\xmark & 85\\% $\\rightarrow$ 99\\% & \\xmark \\\\ \\hline\nDDNN  & \\xmark & 20 $\\times$ & \\xmark & \\xmark & \\xmark \\\\ \\hline\nCOLT-OPE  & 3 $\\times$ & \\xmark & \\xmark & \\xmark & \\xmark \\\\ \\hline\n & 0.35 $\\times$ $\\rightarrow$ 5.28 $\\times$ & \\xmark & \\xmark & \\xmark & \\xmark \\\\ \\hline\nDINA& 2.4 $\\times$ $\\rightarrow$ 4.2 $\\times$& \\xmark & \\xmark & \\xmark & \\xmark \\\\ \\hline\nMoDNN&  2.17 $\\times$ $\\rightarrow$  4.28 $\\times$& \\xmark & \\xmark & \\xmark & \\xmark \\\\ \\hline\n&  34\\%  $\\rightarrow$  84\\% & \\xmark & \\xmark & \\xmark & \\xmark \\\\ \\hline\nAAIoT &  1 $\\times$ $\\rightarrow$  10 $\\times$& \\xmark & \\xmark & \\xmark & \\xmark \\\\ \\hline\nDeepWear &  5.08 $\\times$ $\\rightarrow$  23 $\\times$& \\xmark & 53.5\\% $\\rightarrow$  85.5\\%  & \\xmark & \\xmark \\\\ \\hline\n\\end{tabular}\n\\end{table*}\n\\end{comment}\n\\begin{table*}[]\n\\centering\n\\footnotesize\n\\tabcolsep=0.09cm\n\\begin{threeparttable}\n\\caption{Performance of distribution strategies compared to:\n\\protect \\begin{tikzpicture}\n\\protect\\filldraw[color=black!60, fill=white!5,  thick](-1,0) circle (0.15);\n\\protect\\end{tikzpicture} cloud only;\n\\protect\\begin{tikzpicture}\n\\protect\\filldraw[color=black!60, fill={rgb,255:red,218; green,232; blue,252},  thick](-1,0) circle (0.15);\n\\protect\\end{tikzpicture} on-device only;\\\\\n\\protect\\begin{tikzpicture}\n\\protect\\filldraw[color=black!60, fill={rgb,255:red,213;green,232;blue,212},  thick](-1,0) circle (0.15); \n\\end{tikzpicture} edge-server only.\n}\n\\label{tab:performance}\n\\begin{tabular}{|l|l|l|l|l|l|l|}\n\\hline\n\\begin{tabular}[c]{@{}l@{}}\\textbf{Refs}\\end{tabular} & \\textbf{Latency} & \\textbf{Bandwidth} & \\textbf{Energy} & \\textbf{computation/ memory} & \\textbf{throughput} & \\begin{tabular}[c]{@{}l@{}}\\textbf{Inference}\\\\ \\textbf{rate}\\end{tabular} \\\\ \\hline\nNeurosurgeon  & 3.1 $\\times$  $\\rightarrow$ 40.7 $\\times$ & \\xmark & 59.5 \\% $\\rightarrow$ 94.7\\% & \\xmark & 1.5 $\\times$  $\\rightarrow$ 6.7 $\\times$ & \\xmark \\\\ \\hline\n\\begin{tabular}[c]{@{}l@{}}Edgent  \\\\ Boomerang \\end{tabular} & \\cellcolor[HTML]{DAE8FC}2.3 $\\times$ & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark \\\\ \\hline\n & 1.2 $\\times$  $\\rightarrow$ 2 $\\times$ & \\xmark & \\xmark & \\xmark & \\xmark & \\xmark \\\\ \\cline{2-7} \n\\multirow{-2}{*}{ADDA } & \\cellcolor[HTML]{DAE8FC}1.7 $\\times$ $\\rightarrow$ 3 $\\times$ & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark \\\\ \\hline\n & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}15.3 $\\times$ & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}16.5 $\\times$ & \\cellcolor[HTML]{DAE8FC}\\xmark \\\\ \\cline{2-7} \n\\multirow{-2}{*}{} & \\cellcolor[HTML]{D5E8D4}\\xmark & \\cellcolor[HTML]{D5E8D4}\\xmark & \\cellcolor[HTML]{D5E8D4}2.3 $\\times$ & \\cellcolor[HTML]{D5E8D4}\\xmark & \\cellcolor[HTML]{D5E8D4}2.5 $\\times$ & \\cellcolor[HTML]{D5E8D4}\\xmark \\\\ \\hline\nJALAD  & 1.1 $\\times$  $\\rightarrow$ 11.7 $\\times$ & \\xmark & \\xmark & \\xmark & \\xmark & \\xmark \\\\ \\hline\nJointDNN  & 3 $\\times$ & \\xmark & 7 $\\times$ & \\xmark & \\xmark & \\xmark \\\\ \\hline\n & 8.08 $\\times$ & \\xmark & \\xmark & 14.01 $\\times$ & \\xmark & \\xmark \\\\ \\cline{2-7} \n\\multirow{-2}{*}{DADS } & \\cellcolor[HTML]{D5E8D4}6.45 $\\times$ & \\cellcolor[HTML]{D5E8D4}\\xmark & \\cellcolor[HTML]{D5E8D4}\\xmark & \\cellcolor[HTML]{D5E8D4}8.31 $\\times$ & \\cellcolor[HTML]{D5E8D4}\\xmark & \\cellcolor[HTML]{D5E8D4}\\xmark \\\\ \\hline\nAuto tuning  & 1.13 $\\times$ $\\rightarrow$ 1.7 $\\times$ & \\xmark & \\xmark & 85\\% $\\rightarrow$ 99\\% & \\xmark & \\xmark \\\\ \\hline\nDDNN  & \\xmark & 20 $\\times$ & \\xmark & \\xmark & \\xmark & \\xmark \\\\ \\hline\n & 2 $\\times$ & \\xmark & \\xmark & \\xmark & \\xmark & \\xmark \\\\ \\cline{2-7} \n\\multirow{-2}{*}{COLT-OPE } & \\cellcolor[HTML]{DAE8FC}4 $\\times$ & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark \\\\ \\hline\n & 48.11 \\% & \\xmark & \\xmark & \\xmark & \\xmark & \\xmark \\\\ \\cline{2-7} \n\\multirow{-2}{*}{} & \\cellcolor[HTML]{DAE8FC}39.75 \\% & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}70\\% & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark \\\\ \\hline\nDINA & \\cellcolor[HTML]{D5E8D4}2.6 $\\times$ $\\rightarrow$ 4.2 $\\times$ & \\cellcolor[HTML]{D5E8D4}\\xmark & \\cellcolor[HTML]{D5E8D4}\\xmark & \\cellcolor[HTML]{D5E8D4}\\xmark & \\cellcolor[HTML]{D5E8D4}\\xmark & \\cellcolor[HTML]{D5E8D4}\\xmark \\\\ \\hline\nMoDNN & \\cellcolor[HTML]{DAE8FC}2.17 $\\times$ $\\rightarrow$  4.28 $\\times$ & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark \\\\ \\hline\nAAIoT  & \\cellcolor[HTML]{D5E8D4}1 $\\times$ $\\rightarrow$  10 $\\times$ & \\cellcolor[HTML]{D5E8D4}\\xmark & \\cellcolor[HTML]{D5E8D4}\\xmark & \\cellcolor[HTML]{D5E8D4}\\xmark & \\cellcolor[HTML]{D5E8D4}\\xmark & \\cellcolor[HTML]{D5E8D4}\\xmark \\\\ \\hline\nDeepWear  & \\cellcolor[HTML]{DAE8FC}5.08 $\\times$ $\\rightarrow$  23 $\\times$ & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}53.5\\% $\\rightarrow$  85.5\\% & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark \\\\ \\hline\n & \\cellcolor[HTML]{DAE8FC}2 $\\times$ $\\rightarrow$  6 $\\times$ & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark \\\\ \\hline\n & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}1.7 $\\times$ $\\rightarrow$  4.69 $\\times$ \\\\ \\hline\nDeepThings  & \\cellcolor[HTML]{DAE8FC}0.6 $\\times$ $\\rightarrow$  3 $\\times$ & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}68\\% & \\cellcolor[HTML]{DAE8FC}\\xmark & \\cellcolor[HTML]{DAE8FC}\\xmark \\\\ \\hline\n\\end{tabular}\n\\begin{tablenotes}\n   \\footnotesize\n   \\item - The results in the table present the enhancement of the proposed strategies compared to the baseline approaches.\n   \\item - $\\times$ stands for the number of times the metric is improved, i.e., how many times the latency, bandwidth usage, energy, computation, and memory are reduced, and how many times the throughput and inference rate are increased compared to the baselines.\n\\end{tablenotes}\n\\end{threeparttable}\n\\end{table*}", "cites": [3455, 3456, 3457, 3460, 2672, 3458, 97, 689, 3462, 3459, 3461, 8660], "cite_extract_rate": 0.41379310344827586, "origin_cites_number": 29, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.8}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from multiple papers on per-layer distribution strategies, including back-and-forth and hierarchical approaches, to build a coherent narrative on trade-offs between latency, computation, and communication. It provides some critical discussion of limitations (e.g., overhead from reassigning tasks) and compares the effectiveness of different strategies. While it identifies patterns like fault tolerance and model compression, it does not fully abstract to a novel framework or theory."}}
{"id": "c17ed41c-1986-47f8-a1ad-682f2828329c", "title": "Per-segment distribution", "level": "paragraph", "subsections": [], "parent_id": "695e849b-fe1c-443f-9b10-15b02a8ec11b", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive Inference"], ["subsection", "Resource management for distributed inference"], ["subsubsection", "Remote collaboration"], ["paragraph", "Per-segment distribution"]], "content": "The per-segment partitioning is generally more popular when distributing the inference among IoT devices with limited capacities, as some devices, such as sensors, cannot execute the entire layer of a deep network. Furthermore, per-segment partitioning creates a huge dependency between devices; and consequently, multiple communications with remote servers are required. That is why only few works adopted this strategy for inference collaboration between end devices and edge/fog servers, including . Authors in  proposed a spatial splitting (see Fig. \\ref{splitting} (b)) that minimizes the communication overhead per device. Then, a distribution solution is designed based on the matching theory  and the swap matching problem , to jointly accomplish the DNN inference. The matching theory is a mathematical framework in economics that models interactions between two sets of selfish agents, each one is competing to match agents of the other set. The objective was to reduce the total computation time while increasing the utilization of the resources related to the two sets of IoT and fog devices.", "cites": [3463], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview by introducing the concept of per-segment partitioning and connecting it to the cited paper using matching theory. It integrates the idea of spatial splitting with optimization goals, but the critical evaluation remains limited. The section identifies a broader principle of resource dependency and communication overhead, offering some abstraction beyond individual systems."}}
{"id": "eb0b0f83-fecd-4d90-aa43-aa41c69f497a", "title": "Per-layer distribution", "level": "paragraph", "subsections": [], "parent_id": "ac6550c4-66e5-4a7e-ba42-b511a72e29ea", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive Inference"], ["subsection", "Resource management for distributed inference"], ["subsubsection", "Localized collaboration"], ["paragraph", "Per-layer distribution"]], "content": "The layer-wise partitioning can itself be classified under two categories, the one splitting point strategy where only two participants are involved and multiple splitting points where two or more devices are collaborating. For example, the DeepWear  approach splits the DNN into two sub-models that are separately computed on a wearable and a mobile device. First, the authors conducted in-depth measurements on different devices and for multiple models to demystify the performance of the wearable-side DL and study the potential gain from the partial offloading. The derived conclusions are incorporated into a prediction-based online scheduling algorithm that judiciously determines how, and when to offload, in order to minimize latency and energy consumption of the inference. On the other hand, authors in  proposed a methodology for optimal placement of CNN layers among multiple IoT devices, while being constrained by their computation and memory capacities. This methodology minimizes the latency of decision-making, which is measured as the total of processing times and transmissions between participants. Furthermore, this proposed technique can be applied both to CNNs in which the number of layers is fixed and CNNs with an early-exit. Similarly, authors in  proposed a CNN multi-splitting approach to accelerate the inference process, namely AAIoT. Unlike the above-mentioned efforts, AAIoT deploys the layers of the neural network on multi-layer IoT architecture. More specifically, the lower-layer device presents the data source, and the higher-layer devices have more powerful capacities. Offloading the computation to higher participants implies sacrificing the transmission latency to reduce the computation time. However, delivering the computation to lower participants does not bring any benefit to the system. An optimal solution and an online algorithm that uses dynamic programming are designed to make the best architectural offloading strategy. Other than capacity-constrained IoT devices, the distribution of the inference process over cloudlets in a 5G-enabled MEC system is the focus of the work in , where authors proposed to minimize the energy consumption, while meeting stringent delay requirements of AI applications, using a RL technique.", "cites": [3455, 3459], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes key ideas from multiple papers on per-layer distribution strategies, contrasting different approaches (e.g., DeepWear, CNN layer placement, AAIoT). It provides a coherent narrative by connecting concepts of offloading, latency, and energy trade-offs. While it includes some critical evaluation (e.g., noting that offloading to lower participants brings no benefit), it could offer deeper analysis of limitations or broader implications. The abstraction level is moderate, as it identifies patterns in how layer distribution impacts performance but does not fully generalize to overarching principles."}}
{"id": "a7725cd7-ce64-48a9-a5dc-49ecbf3f3f1c", "title": "Per-segment distribution", "level": "paragraph", "subsections": [], "parent_id": "ac6550c4-66e5-4a7e-ba42-b511a72e29ea", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive Inference"], ["subsection", "Resource management for distributed inference"], ["subsubsection", "Localized collaboration"], ["paragraph", "Per-segment distribution"]], "content": "The per-segment distribution is defined as allocating fine-grained partitioned DNN on lightweight devices such as Raspberry Pis. The partitioning strategy is based on the system configuration and the pervasive network characteristics, including the memory, computation, and communication capabilities of the IoT devices and their number. The segmentation of the DNN models varies from neurons partitioning to channels, spatial, and filters splitting, as discussed in section \\ref{profiling}. For example, the work in  opted for the spatial splitting (see Fig. \\ref{splitting} (b)), where the input and the output feature maps are partitioned into a grid and distributed among lightweight devices. The authors proposed to allocate the cells along the longer edge of the input matrix (rows or columns) to each participant, in order to reduce the padding overhead produced by the spatial splitting. Different segments are distributed to IoT devices according to the load-balancing principles using the MapReduce model. The same rows/columns partitioning is proposed in , namely the data-lookahead strategy. More specifically, each block contains data from other blocks within the same layer such that its connected blocks in subsequent layers can be executed independently without requesting intermediate/padding data from other participants. The spatial splitting is also adopted in , where authors proposed a Fused Tile Partitioning (FTP) method. This method fuses the layers and divides them into a grid. Then, cells connected across layers are assigned to one participant, which largely reduces the communication overhead and the memory footprint. \nThe previous works introduced homogeneous partitioning, where segments are similar. Unlike these strategies, authors in  proposed a heterogeneous partitioning of the input data to be compatible with the IoT system containing devices with different capabilities ranging from small participants that fit only few cells to high capacity participants suitable for layer computation. For the same purpose, authors in  jointly conducted per-layer and per-segment partitioning, where the neurons and links of the network are modeled as a DAG. In this work, grouped convolutional techniques  are used to boost the model parallelization of different nodes of the graph. The papers in  studied different partitioning strategies of the convolutional layers (channel, spatial and filters splitting) and fully connected layers (output and input splitting). Next, they emphasized that an optimal splitting depends greatly on the parameters of the CNN network and that the inference speedup depends on the number of tasks to be parallelized, which is related to the adopted splitting method. Hence, one partitioning approach cannot bring benefits to all types of CNNs. Based on these conclusions, a dynamic heuristic is designed to select the most adequate splitting and model parallelism for different inference scenarios.\nTable \\ref{tab:performance} shows the performance of these techniques in terms of latency, bandwidth, energy, computation, memory, and throughput, whereas Table \\ref{tab:my-table} presents a comparison between different distributed inference techniques introduced in this section.", "cites": [3464], "cite_extract_rate": 0.09090909090909091, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers on distributed DNN inference techniques by connecting different partitioning strategies (spatial, channel, filter, input/output) and their alignment with IoT device capabilities. It critically evaluates the trade-offs between homogeneity and heterogeneity in partitioning, and highlights how splitting methods impact performance metrics like latency and communication overhead. The section abstracts from specific techniques to emphasize broader principles, such as the importance of dynamic heuristics in adapting to different CNN architectures and inference scenarios."}}
{"id": "5c6d2e7f-95b8-4a0a-84eb-fbf75fef871b", "title": "Lessons learned", "level": "subsubsection", "subsections": [], "parent_id": "ca471488-ff37-437f-9c43-59fb13ca9bae", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive Inference"], ["subsection", "Resource management for distributed inference"], ["subsubsection", "Lessons learned"]], "content": "The lessons acquired from the literature review covering the DNN distribution can be summarized as follows:\n\\begin{itemize}\n    \\item  The per-layer strategy with remote collaboration is the most studied approach in the literature, owing to its simple splitting scheme and its assets in using high-performance servers while reducing the transmitted data. However, such strategies may not be efficient in terms of privacy or for networks with unstable transmission links, and hence may not be suitable for all applications.\n    \\item In per-layer strategies, selecting the split points depends on multiple parameters, which are the capacity of the end device that constrains the length of the first segment, the characteristics of the network (e.g., wi-fi, 4G, or LTE) that impact the transmission time, and the DNN topology that determines the intermediate data size.\n    \\item The deep neural networks with a small-reduction capacity of pooling layers or with fully-connected layers of similar sizes undergo small variations in the per-layer data size. In this case, remote collaboration is not beneficial for data transmission. Hence, compression (e.g., quantization, pruning, and encoding) can be a good solution to benefit from remote capacity with the minimum of communication overhead.\n    \\item  Recently, even if it is still not mature yet, multiple efforts have focused on the localized inference through per-segment distribution that allows to involve resource-limited devices and avoid the transmission to remote servers. This kind of works targeted the model parallelization and aimed to maximize the concurrent computation of different segments within the same request. However, fewer works covered data parallelization and real-time adaptability to the dynamics and number of requests. Particularly, the load of inferences highly impacts the distribution of segments to fit them to the capacity of participants.\n    \\item Adopting a mixed partitioning strategy is advantageous for heterogeneous systems composed of high and low-capacity devices and multiple DNNs, which allows to fully utilize the pervasive capacities while minimizing the dependency and data transmission between devices.  \n\\end{itemize}\n\\begin{table*}[]\n\\centering\n\\footnotesize\n\\tabcolsep=0.09cm\n\\caption{Comparison between Distributed Inference techniques.}\n\\label{tab:my-table}\n\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n\\textbf{Refs} & \\textbf{Year} & \\textbf{End-Device} & \\begin{tabular}[c]{@{}c@{}}$N^{o}$ \\textbf{. of}\\\\  \\textbf{end}\\\\ \\textbf{Devices}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\textbf{Localized} \\\\ \\textbf{inference}\\end{tabular} & \\textbf{Context} & \\begin{tabular}[c]{@{}c@{}}\\textbf{Real-time}\\\\\\textbf{processing}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\textbf{Partitioning}\\\\ \\textbf{mechanism}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}$N^{o}$\\textbf{. of}\\\\ \\textbf{partitions}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\textbf{Model or data}\\\\ \\textbf{parallelism}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\textbf{other}\\\\  \\textbf{techniques}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\textbf{Runtime} \\\\ \\textbf{adaptability}\\end{tabular} \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Neuroseurgeon\\\\ \\end{tabular} & 2017 & Tegra  TKI & 1 & \\xmark & \\xmark & \\xmark & Per-layer & 1 & \\xmark & \\xmark & \\xmark \\\\ \\hline\nDDNN  & 2017 & \\xmark & Many & \\xmark & \\xmark & \\cmark & Per-layer & Many & Data & Early exit & \\xmark \\\\ \\hline\nMoDNN  & 2017 & LG Nexus 5 & 4 & \\cmark & \\xmark & \\xmark & Per-segment & Many & Model & \\xmark & \\xmark \\\\ \\hline\nEdgent  & 2018 & RaspBerry Pi 3 & 1 & \\xmark & \\xmark & \\cmark & Per-layer & 1 & \\xmark & Early exit & \\xmark \\\\ \\hline\n & 2018 & \\xmark & 1 & \\xmark & \\xmark & \\xmark & Per-layer & 1 & \\xmark & Compression & \\xmark \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}DeepThings \\\\ \\end{tabular} & 2018 & RaspBerry Pi 3 & Many & \\cmark & \\xmark & \\cmark & Per-segment & Many & Model & \\xmark & \\xmark \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Collaborative\\\\ robots \\end{tabular} & 2018 & RaspBerry Pi & 12 & \\cmark & \\begin{tabular}[c]{@{}c@{}}Robots and\\\\ image \\\\recognition\\end{tabular} & \\cmark & Per-segment & Many & Both & \\xmark & \\cmark \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Musical chair\\\\ \\end{tabular} & 2018 & RaspBerry Pi & Many & \\cmark & \\begin{tabular}[c]{@{}c@{}}object/action \\\\ recognition\\end{tabular} & \\cmark & Per-segment & Many & Both & \\xmark & \\cmark \\\\ \\hline\nHDDNN  & 2018 & \\xmark & Many & \\xmark & \\xmark & \\cmark & Per-layer & Many & Data & Encryption & \\xmark \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Auto tuning\\\\ \\end{tabular} & 2018 & Jetson TX2 & Many & \\xmark & \\xmark & \\xmark & Per-layer & Many & \\xmark & Quantization & \\xmark \\\\ \\hline\nJALAD  & 2018 & \\begin{tabular}[c]{@{}c@{}}GPU \\\\Quadro k620 \\end{tabular}& 1 & \\xmark & \\xmark & \\xmark & Per-layer & 1 & \\xmark & Quantization & \\cmark \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}} KLP \\\\ \\end{tabular} & \\begin{tabular}[c]{@{}c@{}}2018\\\\ 2019\\end{tabular} & STM32F469 & Many & \\cmark & \\xmark & \\xmark & Per-segment & Many & Model & \\xmark & \\xmark \\\\ \\hline\nADDA  & 2019 & RaspBerry Pi 3 & 1 & \\xmark & \\xmark & \\xmark & Per-layer & 1 & \\xmark & Early exit & \\xmark \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Boomerang\\\\ \\end{tabular} & 2019 & RaspBerry Pi 3 & 1 & \\xmark & \\xmark & \\cmark & Per-layer & 1 & \\xmark & Early exit & \\cmark \\\\ \\hline\n & 2019 & Krait CPU & 12 & \\cmark & \\begin{tabular}[c]{@{}c@{}}sensors\\\\  fault tolerance\\end{tabular} & \\cmark & \\xmark & Many & Model & \\xmark & \\cmark \\\\ \\hline\n & 2019 & \\begin{tabular}[c]{@{}c@{}}RaspBerry Pi\\\\ STM32H7\\end{tabular} & Many & \\cmark & \\xmark & \\xmark & Per-layer & Many & Data  & \\xmark & \\xmark \\\\ \\hline\nDADS  & 2019 & \\begin{tabular}[c]{@{}c@{}}RaspBerry Pi 3\\\\ model B\\end{tabular} & 1 & \\xmark & \\xmark & \\cmark & Per-layer & Many & \\xmark & \\xmark & \\cmark \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}COLT-OPE \\\\  \\end{tabular} & 2019 & \\xmark & 1 & \\xmark &\\xmark  & \\xmark & Per-layer & Many & \\xmark & Early exit & \\cmark \\\\ \\hline\nEDDL  & 2019 & Fog nodes & Many & \\cmark & \\xmark & \\xmark & \\begin{tabular}[c]{@{}c@{}}Per-layer\\\\ Per-segment\\end{tabular} & Many & Model & \\begin{tabular}[c]{@{}c@{}}Sparsification\\\\ Early exit\\end{tabular} & \\xmark \\\\ \\hline\n & 2019 & \\begin{tabular}[c]{@{}c@{}}GPU \\\\ GTX1080 \\end{tabular}& 1 & \\xmark & \\xmark & \\cmark & Per-layer & 1 & \\xmark & \\xmark & \\cmark \\\\ \\hline\n & 2019 & \\xmark & 7 & \\cmark & \\xmark & \\xmark & \\begin{tabular}[c]{@{}c@{}}Per-layer\\\\ Per-segment\\end{tabular} & Many & Model & \\xmark & \\xmark \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}deepFogGuard\\\\ \\end{tabular} & 2019 & \\xmark & Many & \\xmark & \\xmark & \\xmark & Per-layer & Many & \\xmark & \\xmark & \\xmark \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}2steps-pruning\\\\  \\end{tabular} & 2019 & \\xmark & 2 & \\xmark & \\xmark & \\xmark & Per-layer & 1 & \\xmark & pruning & \\xmark \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}JointDNN \\\\  \\end{tabular}& 2019 & jetson tx2 & 1 & \\xmark & \\xmark & \\xmark & Per-layer & 1 & \\xmark & \\xmark & \\cmark \\\\ \\hline\nAAIoT  & 2019 & \\begin{tabular}[c]{@{}c@{}}Raspberry Pi,\\\\  Mobile PC, \\\\ Desktop PC, \\\\ Server\\end{tabular} & Many & \\cmark & \\xmark & \\xmark & Per-layer & Many & \\xmark & \\xmark & \\xmark \\\\ \\hline\nMWWP  & 2020 & \\xmark & Many & \\xmark & health care & \\cmark & Per-layer & Many & Data & \\xmark & \\cmark \\\\ \\hline\n & 2020 & Raspberry Pi & Many & \\cmark & \\begin{tabular}[c]{@{}c@{}}multi-view \\\\ object \\\\ detection\\end{tabular} & \\xmark & Per-segment & Many & Model  & Compression & \\xmark \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}CONVENE \\\\ \\end{tabular} & 2020 & \\xmark & 1 & \\xmark & \\begin{tabular}[c]{@{}c@{}}Parallel data \\\\ sharing on \\\\antennas\\end{tabular} & \\xmark & Per-segment & Many & Model  & \\xmark & \\cmark \\\\ \\hline\nDINA  & 2020 & \\xmark & Many & \\xmark & \\xmark & \\xmark & Per-segment & Many & Both & \\xmark & \\cmark \\\\ \\hline\n & 2020 & \\xmark & Many & \\cmark & \\begin{tabular}[c]{@{}c@{}}Intelligent \\\\Connected\\\\  Vehicles\\end{tabular} & \\cmark & \\xmark & Many & \\xmark & \\xmark & \\xmark \\\\ \\hline\n & 2020 & \\xmark & 1 & \\xmark & \\xmark & \\xmark & Per- layer & 1 & \\xmark & Compression & \\xmark \\\\ \\hline\n & 2020 & Huawei & 1 & \\xmark & \\begin{tabular}[c]{@{}c@{}}augmented \\\\reality\\\\  in 5G\\end{tabular} & \\cmark & Per-layer & 2 & Data & Early-exit & \\cmark \\\\ \\hline\n & 2020 & Raspberry Pi 3 & Many & \\cmark & \\begin{tabular}[c]{@{}c@{}}Visual based \\\\ applications\\end{tabular} & \\xmark & Per-segment & Many & Both & \\xmark & \\xmark \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Deep Wear \\\\  \\end{tabular}& 2020 & Android wear & 2 & \\cmark & \\begin{tabular}[c]{@{}c@{}} Wearable\\\\ devices\\end{tabular} & \\cmark & Per-layer & 1 & \\xmark & Compression & \\cmark \\\\ \\hline\n & 2021 & Cloudlet & Many & \\cmark & 5 G & \\cmark & Per-layer & Many & Data & \\xmark & \\cmark \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}DistPrivacy \\\\ \\end{tabular} & 2021 & \\begin{tabular}[c]{@{}c@{}}Raspberry Pi\\\\ STM32H7\\\\ LG Nexus 5\\end{tabular} & Many & \\cmark & Data privacy & \\cmark & Per-segment & Many & Both & \\xmark & \\cmark \\\\ \\hline\n\\end{tabular}\n\\end{table*}", "cites": [3455, 3465, 3456, 3457, 3460, 2672, 3464, 3458, 3462, 3459, 3461], "cite_extract_rate": 0.2894736842105263, "origin_cites_number": 38, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by integrating key observations from multiple cited works into a structured set of lessons, highlighting trends in distributed DNN strategies. It offers critical evaluation by identifying limitations in per-layer approaches and noting the underdevelopment of per-segment methods in terms of real-time adaptability. The abstraction is high as it generalizes insights about the trade-offs between partitioning strategies and system requirements, moving beyond individual papers to broader design principles."}}
{"id": "6f7ddb5a-d1b9-4bec-a004-bea50c65f5dd", "title": "Use case: Distribution on moving robots", "level": "subsection", "subsections": [], "parent_id": "caa8daf7-97f9-471c-b534-e135a402b2d0", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Pervasive Inference"], ["subsection", "Use case: Distribution on moving robots"]], "content": "Currently, robotic systems have been progressively converging to computationally expensive AI networks for tasks like path planning and object detection. However, resource-limited robots, such as low power UAVs, have insufficient on-board power-battery or computational resources to scalably execute the highly accurate neural networks. \n\\begin{comment}\nIn surveillance applications, the aim is to monitor specific objects or identify threats within the target region. Moving devices are the most suitable technology to provide information about the target object from different angles, which makes the identification more accurate. These data-generating devices are only responsible for collecting the data, while servers with higher capacities generate the identification results. The traditional wisdom resorts to cloud or edge servers to compute heavy tasks. However, due to the harsh environments where robots move (e.g., military border zones, forests, and offshore oil reserves), the communication with remote servers is strongly affected by the weather. Also, the processing might be difficult or even impossible because of the interference resulting from the UAV altitude or the underground environment (e.g., high-rise building effect on path loss). Furthermore, as surveillance devices send high-resolution images to cloud/edge servers at each small interval of time and knowing that incidents rarely occur, the  large  data  volume transmitted by source units has  become  problematic, particularly  for such systems  characterized  by an  unstable  bandwidth  availability. Because of this tremendous amount of data obtained during the robots mission, AI should be integrated within the design of the devices. However, moving robots come with distinct features, often understated, such as that communicating with the remote servers while moving incurs unstable latency, more energy consumption, and potential loss of data.\n\\end{comment}\n\\begin{figure}[!h]\n\\centering\n\t\\frame{\\includegraphics[scale=0.53]{Figures/UAVs.pdf}}\n\t\\caption{A fire detection scenario with distributed DNN.}\n\t\\label{UAVs_mobility}\n\\end{figure}\nThe work in  examined the case of per-layer distribution with one split point between one UAV and one MEC server (see Fig. \\ref{UAVs_mobility}). More specifically, the authors proposed a framework for AI-based visual target tracking system, where low-level layers of the DNN classifier are deployed in the UAV device and high-level layers are assigned to the remote servers. The classification can be performed using only the low-level layers, if the image quality is good. Otherwise, the output of these layers should be further processed in the MEC server, for higher accuracy. In this context, the authors formulated a weighted-sum cost minimization problem for binary offloading and partial offloading, while taking into consideration the error rate/accuracy, the data quality, the communication bandwidth, and the computing capacity of the MEC and the UAV. The offloading probability is derived for the binary offloading and the offloading ratio (i.e., the segment of DNN to execute in the MEC) is obtained for the partial offloading scheme. In this model, the mobility of the UAVs (i.e., the distance between the UAV and the server) is involved through the transmission data rate between the device and the MEC. Additionally, the distance between the UAV and the target impacts the quality of the image and consequently impacts the offloading decisions. In the proposed framework, multiple trade-offs are experienced:\n\\begin{itemize}\n    \\item The accuracy is achieved at the expense of delay and transmitted data: if most of the images have bad quality, the system is not able to accomplish low average latency as on-board inference is not sufficient. For this reason, different inferences should be extended wisely using the segment allocated in the MEC, particularly if the environment is challenging such as bad weather or when the target is highly dynamic.\n    \\item A trade-off exists also between the accuracy and latency, and the position of the UAVs: when the device is close to the target, high resolution images can be taken, which allows obtaining good accuracy on-board and avoiding the data offloading. Being close to the targets is not always possible, particularly  in harsh environments or when the surveillance should be hidden.\n    \\item The battery life is increased at the expense of the inference latency: the battery can be saved, if the processing coefficient is decreased, which enlarges the computation time of the classification. \n    \\item  The split point selection: if the intermediate data is smaller than the raw data, the offloading is encouraged to enhance the accuracy.\n\\end{itemize}\nAn online solution of this offloading trade-off using reinforcement learning is presented in .\nThe previous works adopted the per-layer wise with one split point and remote collaboration approach. This strategy is more adequate for flying devices that can enhance their link quality by approaching the MEC stations. However, for ground robots, offloading segments of the inference to remote servers costs the system a large transmission overhead and high energy consumption. Authors in  studied the distribution of the DNN network among ground robots and profiled the energy consumed for such tasks, when moving or being in idle mode. Several conclusions are stated:\n\\begin{itemize}\n    \\item When the robot is idle, the DNN computation and offloading increase the power consumption of the device by 50\\%.\n    \\item If the device is moving, the DNN execution causes high spikes in power consumption, which may limit the device to attain a high performance as this variation incurs a frequent change of the power saving settings in the CPU.\n    \\item Distributing the inference contributes to reducing the energy consumed per device, even-though the total power consumption is higher. This is due to the reduced computation and memory operations per device and the idle time experienced after offloading the tasks.\n\\end{itemize}\nBased on the energy study of moving robots, the authors proposed to distribute the DNN model into smaller segments among multiple low-power robots to achieve an equilibrium  of performance in terms of energy and number of executed tasks . Still, the distribution of the model into small segments (e.g., filter splitting) requires the intervention of a large number of robots that are highly dependent, which is not realistic.", "cites": [8648], "cite_extract_rate": 0.2, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates a single cited paper into a broader discussion of DNN offloading in moving robots, connecting its findings to general challenges such as energy consumption, latency, and image quality. While it provides a structured analysis of the trade-offs and limitations of the proposed methods, the synthesis remains largely confined to one paper, with limited comparison or cross-source integration. It identifies practical issues (e.g., feasibility of filter splitting) but does not offer a high-level abstraction or a novel framework."}}
{"id": "3b093e3f-f514-445b-a29d-c2ae8d01e7d3", "title": "Privacy and security challenges", "level": "subsubsection", "subsections": [], "parent_id": "8c5da2a2-c8ee-423c-8129-9a53ffb36936", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Privacy of pervasive AI systems"], ["subsection", "Privacy for pervasive training"], ["subsubsection", "Privacy and security challenges"]], "content": "In some federated learning settings, participants can randomly join or leave the training process, which raises various vulnerabilities from different sources, including malicious servers, insider adversaries and outsider attackers. More specifically, aggregation servers can be honest but curious to inspect the models without introducing any changes. On the other hand, potential malicious servers , as well as untrusted participants can tamper the model during the learning rounds or dissimulating participation in order to obtain the final aggregated model without actually contributing with any data or only contributing with a small number of samples. This attack is called free-riding . Outsider eavesdroppers can also intercept the communication channels between trusted devices and the server to spoof the model or inject noisy data (data poisoning). Authors in  proved that it is possible to extract sensitive information from a trained model, as it implies the correlation between the training samples. The research work in  showed that confidence information returned by ML classifiers introduce new model inversion attacks that enable the adversary to reconstruct samples of training subjects with high accuracy. Inferring the sensitive information is also possible through querying the prediction model.\nBandit and MARL algorithms are also prone to attacks if one of the agents is compromised. In general, if any of the agents starts communicating false data (i.e., false data injection attacks), the regret-guarantees in bandits, and convergence behavior in MARL no longer hold. In addition to these expected effects, recent works in  have demonstrated that a malicious agent may not only be disruptive but can also actively sway the policy into malicious objectives by driving other agents to reach a policy of its choice.", "cites": [3466, 3469, 3468, 3467], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to present a cohesive overview of privacy and security challenges in pervasive AI systems, particularly in federated learning and MARL contexts. It critically discusses the impact of attacks like free-riding, model inversion, and false data injection, though it could offer more in-depth evaluation of the solutions or limitations of these approaches. The discussion abstracts to a degree by identifying broader patterns such as the fragility of consensus mechanisms and privacy leakage risks, but it remains grounded in specific attack scenarios."}}
{"id": "1ea199c0-0420-410c-b0c2-bd29b8f983bb", "title": "Differential Privacy (DP)", "level": "paragraph", "subsections": [], "parent_id": "15b70f61-0ec4-40e9-b32c-0dabef9489fc", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Privacy of pervasive AI systems"], ["subsection", "Privacy for pervasive training"], ["subsubsection", "Defense techniques and solutions"], ["paragraph", "Differential Privacy (DP)"]], "content": "DP is a data perturbation technique that was first introduced  for ML in . In DP, a statistical noise is injected to the sensitive data to mask it and an algorithm is called differentially private, if its output cannot give any insight or reveal any information about its input. The DP has been widely used to preserve the privacy of the learning, although it is always criticized by its effect on the accuracy of the results due to noise growth. For this, a careful calibration between the privacy level and the model usability is needed. The use of differential privacy for distributed learning systems becomes a very active research area. The authors in  proposed a differentially private stochastic gradient descent technique  that adds random noise (e.g., Gaussian mechanism) to the trained parameters, before sending them to the aggregation server. Then, during the local training, each participant keeps calculating the probability that an attacker succeeds to exploit the shared data, until reaching a predefined threshold at which it stops the process. Moreover, in each round, the aggregation server chooses random participants. In this way, neither the local parameters can be used, nor the global model, as the attacker has no information about the devices participating in the current round. DP has been also used to ensure the agent's privacy in federated bandits , where the authors considered the federated bandit formulation with contextual information (in both, centralized aggregator and p2p communication style). The authors provided regret and privacy guarantees so that the peers, or the central aggregator, do not learn individual agent's samples.\nWhile concealing the agents contribution during the  training, a trade-off between the privacy and\nlearning performance should be established. In this context, authors in  tested the performance of FL applied to real-world healthcare datasets while securing the private data using differential privacy techniques. Results show that a significant performance loss is witnessed, even though the privacy level is increased. This encouraged the research community to propose alternative approaches to ensure privacy in federated learning.", "cites": [3471, 7727, 3470, 7608], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers on differential privacy in distributed learning, particularly federated learning and federated bandits, and connects them under the broader theme of privacy during pervasive training. It includes a critical note on the trade-off between privacy and model performance, referencing a specific study that demonstrates performance loss. While it identifies a general pattern (the need for balancing privacy and accuracy), it does not go beyond the cited examples to present overarching principles or deeper theoretical insights."}}
{"id": "7b2f9a6d-5726-4ea5-9dea-c62d51533af1", "title": "Homomorphic Encryption (HE)", "level": "paragraph", "subsections": [], "parent_id": "15b70f61-0ec4-40e9-b32c-0dabef9489fc", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Privacy of pervasive AI systems"], ["subsection", "Privacy for pervasive training"], ["subsubsection", "Defense techniques and solutions"], ["paragraph", "Homomorphic Encryption (HE)"]], "content": "HE is a form of encryption that consists of performing computational operations on cipher texts while having the same results that can be generated by the original data.\nThe approach in  and  ensures the integrity of DL training process against outsider attackers as well as  honest-but-curious server. The key idea is to encode and compress the parameters of the trained neural networks before sharing them with the server. Then, these aggregated updates are directly computed with decoder on the server. This guarantees their privacy during the communication and after decoding. Although the encryption technique can preclude the server from extracting information of local models, it costs the system more communication rounds and cannot prevent the collusion between the server and a malicious participant. To solve this problem, authors in  proposed to adopt hybrid solution which integrates both lightweight homomorphic encryption and differential privacy. In this work, intentional noises are added to perturb the original parameters in case the curious server accomplices with one of the participants to get encryption parameters.\nEven though the encryption is a robust approach to achieve privacy preservation for many applications, its adoption for deep learning is facing various challenges as it can only be deployed on tasks with certain degrees and complexities. In other words, the fully homomorphic schemes are still not efficient for practical use.", "cites": [3472], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section integrates the concept of Homomorphic Encryption (HE) with the context of pervasive AI training, particularly in federated learning, by referencing a relevant paper. It provides a critical perspective by highlighting limitations such as increased communication rounds and vulnerability to collusion. However, the synthesis is somewhat basic and lacks a deeper framework or meta-level generalization beyond the specific application described in the cited work."}}
{"id": "540f89bb-bbb8-4bf5-af2d-f366725ca68a", "title": "Blockchain-based solutions", "level": "paragraph", "subsections": [], "parent_id": "15b70f61-0ec4-40e9-b32c-0dabef9489fc", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Privacy of pervasive AI systems"], ["subsection", "Privacy for pervasive training"], ["subsubsection", "Defense techniques and solutions"], ["paragraph", "Blockchain-based solutions"]], "content": "Blockchain is a recent distributed ledger system initially designed for cryptocurrency and later increasingly applied to the IoT systems, where a record of transactions is deployed distributively in a peer-to-peer network . Authors in  proposed to use a blockchain-based communication scheme to exchange updates in a distributed ML system, with the aim of leveraging the blockchain's security features in the learning process.\nIn such practice, local models are shared and verified in the trusted blockchain network. Furthermore, this framework can prevent participants from free-riding as their updates are checked and they receive rewards proportional to the number of trained data samples. However, in contrast to vanilla FL, Block FL needs to take into consideration the extra delay incurred by the blockchain network. To address this, the Block FL is formulated by considering communication, computation, and the block generation rate, i.e., the proof of work difficulty. A possible drawback of this approach is its vulnerability against any latency increase.\nAlso, the use of blockchain implies the addition of a significant cost to implement and  maintain miners.", "cites": [3473], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of blockchain-based solutions for pervasive training in AI systems, particularly in the context of federated learning. It integrates the key idea from the cited paper regarding the use of blockchain for secure, decentralized model updates and introduces a minor critique about latency and implementation costs. However, the synthesis remains limited to one paper, and the abstraction does not rise beyond specific technical aspects without identifying broader implications or patterns."}}
{"id": "7aaeb880-f099-4baf-9dca-eec3253adebf", "title": "Secure Multi-Party Computation (SMC)", "level": "paragraph", "subsections": [], "parent_id": "15b70f61-0ec4-40e9-b32c-0dabef9489fc", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Privacy of pervasive AI systems"], ["subsection", "Privacy for pervasive training"], ["subsubsection", "Defense techniques and solutions"], ["paragraph", "Secure Multi-Party Computation (SMC)"]], "content": "SMC is a sub-field of cryptographic protocols that has as a goal to secure the data except the output when multiple participants jointly perform an arbitrarily function over their private input. A study in  has used SMC to build FL systems. The proposed protocols consider secret sharing, which adds new round at the beginning of the process for the keys sharing, double-masking round that protects from potential malicious server, and server-mediated key agreement that minimizes trust.", "cites": [3474], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a brief descriptive overview of Secure Multi-Party Computation (SMC) and mentions a single paper with some key features of the proposed protocol. However, it lacks synthesis of broader themes or connections to other related work, critical evaluation of the methods, and abstraction to highlight overarching principles or patterns in the field."}}
{"id": "a0a5e105-95e7-4a00-81ed-60edad5342b8", "title": "Prevention against data poisoning", "level": "paragraph", "subsections": [], "parent_id": "15b70f61-0ec4-40e9-b32c-0dabef9489fc", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Privacy of pervasive AI systems"], ["subsection", "Privacy for pervasive training"], ["subsubsection", "Defense techniques and solutions"], ["paragraph", "Prevention against data poisoning"]], "content": "Data poisoning is one of the attacks that is very destructive for ML, where an attacker injects poisoned data (e.g., mislabeled samples and wrong parameters) into the dataset, which can mislead the learning process. Authors in  and  propose secure decentralized techniques to protect the learning against data poisoning, as well as other system attacks. A zero-sum game is proposed to formulate the conflicting objectives between  honest participants that utilize Distributed Support Vector Machines (DSVMs) and a malicious attacker that can change data samples and labels. This game characterizes the contention between the honest learner and the attacker. Then, a fully distributed and iterative algorithm is developed based on Alternating Direction Method of Multipliers (ADMoM)  to procure the instantaneous responses of different agents. Blockchain-based solutions can also be used to prevent the FL system from data poisoning attacks.", "cites": [3475, 3476], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes two game-theoretic approaches for securing DSVMs against data poisoning, highlighting their shared use of adversarial modeling. It provides a basic analytical perspective by framing the problem in terms of conflict between honest learners and attackers but does not deeply compare or critique the methods. The mention of blockchain as a complementary solution adds a level of abstraction beyond the cited works."}}
{"id": "bf40eab4-c4ab-4d73-ae5e-83a863f08334", "title": "Other techniques", "level": "paragraph", "subsections": [], "parent_id": "15b70f61-0ec4-40e9-b32c-0dabef9489fc", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Privacy of pervasive AI systems"], ["subsection", "Privacy for pervasive training"], ["subsubsection", "Defense techniques and solutions"], ["paragraph", "Other techniques"]], "content": "Most of the aforementioned techniques protect the private data from outsider attackers while assuming that the server is trustful and participants are honest. However, one malicious insider can cause serious privacy threats. Motivated by this challenge, authors in  proposed a collaborative DL framework to solve the problem of internal attackers. The key idea is to select only a small number of gradients to share with the server and similarly receive only a part of the global parameters instead of uploading and updating the whole set of parameters. In this way, a malicious participant cannot have the whole information and hence cannot infer it. However, this approach suffers from accuracy loss. Furthermore, authors in  presented a new attack based on Generative Adversarial Networks (GANs) that can infer sensitive information from a victim participant even with just a portion of shared parameters. In the same context, a defense approach based on GANs is designed by authors in , in which participants generate artificial data that can replace the real samples. In this way, the trained model is called federated generative model and the private data parameters are not exposed to external malicious devices. Still, this approach can lead to potential learning instability and performance reduction due to the fake data used in the training.", "cites": [3477, 3478], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates two distinct GAN-based approaches to address privacy in pervasive training, linking them to the broader issue of insider threats. It also critically evaluates the trade-offs (e.g., accuracy loss, learning instability) of these methods. While it identifies some patterns (e.g., the use of GANs for both attack and defense), it does not generalize to a higher-level framework or principle."}}
{"id": "cf0c8e7a-5d04-424c-9716-c82e2f5c654d", "title": "Privacy and security challenges", "level": "subsubsection", "subsections": [], "parent_id": "20737907-d0e1-4e53-a293-c60550831e94", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Privacy of pervasive AI systems"], ["subsection", "Privacy for pervasive inference"], ["subsubsection", "Privacy and security challenges"]], "content": "The data captured by end-devices and sent to remote servers (e.g., from cameras or sensors to cloud servers) may contain sensitive information such as camera images, GPS coordinates of critical targets, or vital signs of patients. Exposing these data has become a big security concern for the deep learning community. This issue is even more concerning when the data is collected from a small geographical area (e.g., edge computing) involving a set of limited and cooperating users. In fact, if an attacker reveals some data (even public or slightly sensitive), a DL classifier can be trained to automatically infer the private data of a known community. These attacks, posing severe privacy threats, are called inference attacks that analyze trivial or available data to illegitimately acquire knowledge about more robust information without accessing it, by only capturing their statistical correlations. An example of a popular inference attack is the Cambridge Analytica scandal in 2016, where public data of Facebook users were exploited to predict their private attributes (e.g., political view and location).  Some well-known inference attacks are summarized in Table \\ref{inference_attacks}.\n\\begin{table}[h]\n\\centering\n\\caption{Examples of inference attacks.}\n\\label{inference_attacks}\n\\begin{tabular}{|c|c|c|}\n\\hline\n\\textbf{Inference attacks} & \\textbf{Exposed data} & \\textbf{Sensitive data} \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Side-channel attacks\\\\ \\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Processing time,\\\\ power consumption.\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Cryptographic\\\\ keys\\end{tabular} \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Location inference\\\\ attacks \\end{tabular} & \\begin{tabular}[c]{@{}c@{}}smartphones' sensor\\\\ data.\\end{tabular} & Location \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Feature inference\\\\ attacks \\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Prediction results,\\\\ partial features of the\\\\ DNN model.\\end{tabular} & DNN structure \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Membership inference\\\\ attacks \\end{tabular} & \\begin{tabular}[c]{@{}c@{}}confidence level \\\\ of classes,\\\\ gradients.\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}membership of \\\\ a sample to a\\\\ dataset.\\end{tabular} \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}attribute inference \\\\ attacks \\end{tabular} & \\begin{tabular}[c]{@{}c@{}}social data, likes, \\\\ friends.\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Gender, ages,\\\\ preferences.\\end{tabular} \\\\ \\hline\n\\end{tabular}\n\\end{table}\nEdge computing naturally enhances privacy of the sensitive information by minimizing the data transfer to the cloud through the public internet. However, additional privacy techniques should be adopted to further protect the data from eavesdroppers. In this context, in addition to its ability to allow the pervasive deployment of neural networks, the DNN splitting was also used for privacy purposes. Meaning, by partitioning the model,  partially processed data is sent to the untrusted party instead of transmitting raw data. In fact,\nin contrast to the training data that belongs to a specific dataset and generally follows a statistical distribution, the inference samples are random and harder to be reverted.\nFurthermore, the model parameters are independent from the input data, which makes the inference process reveal less information about the sample . While preserving privacy, the inevitable challenge of DNN partitioning that remains valid, is selecting the splitting point that meets the latency requirements of the system.\n\\begin{table*}[!h]\n\\centering\n\\caption{Comparison between privacy-aware distribution strategies.\\\\\n(H: High, M: Medium, L: Low).}\n\\label{tab:privacy}\n\\begin{tabular}{|c|c|c|c|c|c|c|c|}\n\\hline\n\\begin{tabular}[c]{@{}c@{}}\\textbf{Privacy-aware}\\\\ \\textbf{strategy}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\textbf{Privacy}\\\\ \\textbf{level}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\textbf{Accuracy}\\\\ \\textbf{preserving}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\textbf{DNN}\\\\ \\textbf{re-training}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\textbf{Compatibility}\\\\ \\textbf{with IoT and DNNs}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\textbf{Partitioning}\\\\ \\textbf{strategy}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\textbf{Communication}\\\\ \\textbf{overhead}\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}\\textbf{Computation}\\\\ \\textbf{overhead on}\\\\ \\textbf{source-device}\\end{tabular} \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Deep split \\end{tabular} & H & \\cmark & \\xmark & \\cmark & per-layer & L & H \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Feature extraction \\\\ \\end{tabular} & L & \\xmark & \\xmark & \\cmark & per-layer & L & M \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Noise addition\\\\  \\end{tabular} & M & \\xmark & \\cmark & \\xmark & per-layer & M & H \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Cryptography  \\end{tabular} & H & \\xmark & \\cmark & \\xmark & per-layer & M & H \\\\ \\hline\n\\begin{tabular}[c]{@{}c@{}}Privacy-aware\\\\  partitioning \\end{tabular} & M & \\cmark & \\xmark & \\cmark & \\begin{tabular}[c]{@{}c@{}}Filter\\\\ splitting \\end{tabular} & H & L \\\\ \\hline\n\\end{tabular}\n\\end{table*}", "cites": [3465, 3480, 3483, 3484, 3482, 3481, 3479], "cite_extract_rate": 0.5384615384615384, "origin_cites_number": 13, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear comparison of privacy-aware strategies through a table and describes the types of inference attacks. It synthesizes some information from the cited papers but does not create a novel framework. The critical analysis is limited, as the section primarily outlines the trade-offs of each approach without deeper evaluation or identifying research limitations."}}
{"id": "e43326f0-f867-4d51-8b73-92517f516e6f", "title": "Features extraction", "level": "paragraph", "subsections": [], "parent_id": "1497a56b-96a9-4eaf-9373-f2b742ebc789", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Privacy of pervasive AI systems"], ["subsection", "Privacy for pervasive inference"], ["subsubsection", "Defense techniques and solutions"], ["paragraph", "Features extraction"]], "content": "Authors in  proposed to extract the features sufficient and necessary to conduct the classification from the original image or from one of the layers' outputs using an encoder and transmit these data to the centralized server for inference. This approach prevents the exposure of irrelevant information to the untrusted party that may use it for unwanted inferences.\nThe work in  also proposed feature extraction for data privacy, while achieving a trade-off between on-device computation, the size of transmitted data, and security constraints. In fact, selecting the split layer from where the data will be extracted intrinsically presents a security  compromise. Particularly, as we go deeper in the DNN network, the features become more task specific and the irrelevant data that can involve sensitive information are mitigated . Hence, if the split is performed in a deep layer, the privacy is more robust and the transmission overhead is lower. However, a higher processing load is imposed on the source device.  The latter work , along with the work in , advised to perform deep partition in case the source device has enough computational capacity. If the source device is resource-constrained, the model should be partitioned in the shallow layers, although most of the output features are not related to the main task. Authors in  proposed a solution based on Siamese fine-tuning  and dimensionality reduction to manipulate the intermediate data and send only the primary measures without any irrelevant information. In addition to enhancing privacy, this mechanism contributes to reducing the communication overhead between the end-device and the remote server. \nHowever, to this end, the arms race between attacks and defenses for DNN models has come to a forefront, as the amount of extracted features can be sufficient for adversary approaches to recover the original image. Whereas, less shared features may also result in low classification accuracy. The works in  proposed adversarial attacks to predict the inference input data (or the trained model), using only available features from shared outputs between participants. Authors in  focused particularly on the privacy threats presented by the DNN distribution; and accordingly, designed a white-box attack assuming that the structure of the trained model is known and the intermediate data can be inverted through a regularized Maximum Likelihood Estimation (rMSE). Additionally, a black-box attack is also proposed, where  the malicious participant only has knowledge about his segment and attempts to design an inverse DNN network to map the received features to the targeted input and recover the original data. Authors demonstrated that reversing the original data is possible, when the neural system is distributed into layers.", "cites": [3485, 629, 3484], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple works to explain how feature extraction can enhance privacy in pervasive inference, connecting ideas about layer selection, computational trade-offs, and inverse modeling. It critically addresses the privacy-accuracy trade-off and discusses adversarial attacks that can undermine feature-based defenses. While it offers a structured analysis, the abstraction remains limited to the specific context of DNN partitioning and inversion attacks, without broader generalization to other privacy paradigms in pervasive AI."}}
{"id": "ed39e46e-8f57-429c-b574-55bab1950337", "title": "Noise addition", "level": "paragraph", "subsections": [], "parent_id": "1497a56b-96a9-4eaf-9373-f2b742ebc789", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Privacy of pervasive AI systems"], ["subsection", "Privacy for pervasive inference"], ["subsubsection", "Defense techniques and solutions"], ["paragraph", "Noise addition"]], "content": "Adding noise to the intermediate data is adopted in . In this paper, the authors proposed to perform a simple data transformation in the source-device to extract relevant features and add noise. Next, these features extracted from shallow layers are sent to the cloud to complete the inference. To maintain a high classification accuracy, the neural network is re-trained with a dataset containing noisy samples. However, adding noise to the intermediate data costs the system  additional energy consumption and computational overhead. Therefore, the splitting should be done at a layer where the output size is minimal. Though, the latter work did not describe the partition strategy. The Shredder approach  resolved this dilemma by considering the computation overhead during the noise injection process. The idea is to conduct an offline machine learning training to find the noise distribution that strikes a balance between privacy (i.e., information loss) and accuracy. In this way, the DNN model does not require retraining with the noisy data and the network can be cut at any point to apply directly the noise distribution. The partitioning decision is based on the communication and computation cost. A higher privacy level and lower communication overhead are guaranteed when the split is performed at deep layers; however, the allocation at the end-device becomes less scalable. Adding noise or extracting task-specific data can be included under the umbrella of differential privacy, which at a high level ensures that the model does not reveal any information about the private input data, while still presenting satisfactory classification. The performance of differential privacy is assessed by a privacy budget parameter $\\epsilon$ that denotes the level of distinguishability. Authors in  conducted theoretical analysis  to minimize $\\epsilon$, while considering  accuracy and the communication overhead to offload the intermediate features among fog participants.", "cites": [3479, 3483], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively synthesizes the key concepts from both cited papers, connecting the idea of noise addition with model partitioning and differential privacy. It offers some critical analysis by pointing out limitations such as energy consumption and scalability issues. While it identifies patterns across the approaches, the abstraction remains moderate and focuses more on technical implications rather than broader theoretical frameworks."}}
{"id": "12d8d2c7-f137-4970-9abe-a14165af9428", "title": "Distribution for privacy", "level": "paragraph", "subsections": [], "parent_id": "1497a56b-96a9-4eaf-9373-f2b742ebc789", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Privacy of pervasive AI systems"], ["subsection", "Privacy for pervasive inference"], ["subsubsection", "Defense techniques and solutions"], ["paragraph", "Distribution for privacy"]], "content": "All the previous techniques applied additional tasks to secure the shared data, e.g., feature extraction, adding noise, and encryption, which overloads the pervasive devices with computational overhead. Different from previous works, DistPrivacy  used the partitioning scheme to guarantee privacy of the data. In fact, all the existing privacy-aware approaches adopted the per-layer distribution of the DNN model. This partitioning strategy incurs an intermediate shared information that can be reverted easily using adversarial attacks. The main idea in  is to divide the data resulting from each layer into small segments and distribute it to multiple IoT participants, which contributes to hiding the proprieties of the original image as each participant has only small amount of information. Particularly, the authors adopted the filter splitting strategy, in such a way that each device computes only a part of the feature maps. However, as stated in section \\ref{profiling}, this partitioning strategy results in large data transmission between participants. Therefore, the authors formulated an optimization that establishes a trade-off between privacy and communication overhead.\nTable \\ref{tab:privacy} illustrates different privacy-aware strategies for distributed inference existing in the literature and shows their performance. We can see that choosing the adequate strategy depends on the requirements of the pervasive computing system, as multiple trade-offs need to be established, such as the security level and accuracy, or the computation and communication loads.\n\\begin{comment}\n\\begin{figure}[!h]\n\\centering\n\t\\includegraphics[scale=0.41]{Figures/privacy2.pdf}\n\t\\caption{Privacy-aware distribution strategies.}\n\t\\label{privacy_aware}\n\\end{figure}\n\\end{comment}", "cites": [3465], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the key idea of DistPrivacy, explaining how it differs from prior work by using partitioning for privacy. It provides some critical analysis by pointing out the vulnerability of intermediate shared information to adversarial attacks and highlights the trade-off between privacy and communication overhead. While it offers a general pattern of how privacy strategies affect system performance, the analysis remains somewhat focused on the cited paper and lacks a broader meta-level abstraction."}}
{"id": "b44fd244-9c67-458c-95ac-769538143b6d", "title": "Pervasive AI-as-a-service", "level": "subsubsection", "subsections": [], "parent_id": "7787760b-199f-46be-baca-8a1610f30587", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Future directions and open challenges"], ["subsection", "Deployment of Pervasive AI in emerging systems"], ["subsubsection", "Pervasive AI-as-a-service"]], "content": "While the 5G main goal is to provide high speed mobile services, the 6G pledges to establish next-generation softwarization and improve the network configurability in order to support pervasive AI services deployed on ubiquitous devices. However, the research on 6G is still in its infancy, and only the first steps are taken to conceptualize its design, study its implementation, and plan for use cases. Toward this end, academia and industry communities should pass from theoretical studies of AI distribution to real-world deployment and standardization, aiming at instating the concept of Pervasive AI-as-a-service (PAIaas). PAIaas allows the service operators and AI developers to be more domain-specific and focus on enhancing users’ quality of experience, instead of worrying about tasks distribution. Moreover, it permits to systemize the mass-production and unify the interfaces to access the joint software that gathers all participants and applications. \nSome recent works, including  and , started to design distributed AI services. However, authors did not present an end-to-end architecture enclosing the whole process, neither they have envisaged an automated trusted management of the service provisioning.", "cites": [7140], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 2.5, "abstraction": 2.0}, "insight_level": "medium", "analysis": "The section introduces the concept of Pervasive AI-as-a-service and links it to 6G advancements, but it only weakly synthesizes the cited paper. There is a brief mention of a limitation (lack of end-to-end architecture and automated trusted management), indicating some level of critical analysis, though it is not deeply developed. The abstraction is minimal, focusing on the need for standardized deployment rather than broader conceptual or theoretical generalizations."}}
{"id": "039bcb47-6102-4382-9971-f2c49444e1fe", "title": "Privacy-aware distributed inference", "level": "subsubsection", "subsections": [], "parent_id": "94d6caee-ed7e-43e0-93c5-bd7d0a529643", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Future directions and open challenges"], ["subsection", "Efficient algorithms for pervasive inference"], ["subsubsection", "Privacy-aware distributed inference"]], "content": "Guaranteeing the privacy of the data shared between collaborative devices is one of the main concerns of pervasive computing systems, since untrusted participants may join the inference and observe critical information.  Because of this heterogeneity of ubiquitous devices, the trained models are subject to malicious attacks, such as black-box and white-box risks, by which the original inputs may be in jeopardy. In this case, privacy-aware mechanisms should be enhanced to ensure the security of the distributed inference process. Many efforts have been conducted in this context, such as noise addition and cryptography. Even though these techniques succeeded in hiding features of the data from untrusted devices, most of them suffer from computation overhead and incompatibility with some end-devices or DNNs. More specifically, noisy or encrypted data need to be re-trained to preserve the accuracy of the prediction, and each input has to be obfuscated, which adds a computation overhead. Moreover, encryption may not be applicable for all DNN operations nor possible in some end-devices due to the crypto key management requirements. A notable recent work in  and  proposed to use the distribution for data privacy, without applying any additional task requiring computation overhead. In fact, per-segment splitting leads by design to assigning only some features of the input data to participants. Authors, of this work, applied filter partitioning and conducted empirical experiments to test the efficiency of black-box attacks on different segments’ sizes (i.e., number of feature maps per device). The lower the number of feature maps per device, the higher the privacy. However, filter partitioning incurs high communication load and dependency between devices. This study is still immature. Other partitioning strategies (e.g., channel and spatial.) can be examined to identify the optimal partitioning and distribution that guarantee satisfactory privacy and minimum resource utilization per participant.", "cites": [3486, 3465], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section synthesizes concepts from two related papers, integrating their ideas on privacy-aware distributed inference in IoT systems. It critically evaluates the trade-offs of existing techniques (e.g., noise addition, encryption) and points out their limitations, particularly in terms of computational overhead and incompatibility. The analysis also generalizes by identifying broader patterns in partitioning strategies and their impact on privacy and resource usage, suggesting future research directions."}}
{"id": "0f99d7a1-3b25-45ba-838b-c2d5a1bd9b3e", "title": "Trajectory optimization of moving robots for latency-aware distributed inference", "level": "subsubsection", "subsections": [], "parent_id": "94d6caee-ed7e-43e0-93c5-bd7d0a529643", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Future directions and open challenges"], ["subsection", "Efficient algorithms for pervasive inference"], ["subsubsection", "Trajectory optimization of moving robots for latency-aware distributed inference"]], "content": "The usage of robots (e.g., UAVs) proved its efficiency to improve services in critical and hard-reaching regions. Recently, moving robots have been used for real-time image analysis, such as highway inspection, search and rescue operations, and border surveillance missions. These devices have numerous challenges, including energy consumption and unstable communication with remote servers. Recent works, e.g., , proposed to avoid remote AI inferences and leverage the computation capacity of ground robots to accomplish the predictive tasks. However, only few works covered the distribution of the inference among flying drones, characterized by their faster navigation, higher power consumption, and ability to reach areas with high interferences (e.g., high-rise buildings) compared to ground devices . Moreover, recent efforts did not cover the path planning for different moving robots to complete their assigned missions, while performing latency-aware predictions. More specifically, the time period between capturing the data to the moment when tasks from all the points are collected, should be minimized by optimizing the devices’ trajectories, and planning close paths for participants handling subsequent segments. Furthermore, the trajectories of devices with available resources should cross the paths of the nodes that need to offload the tasks, because of resource constraints.", "cites": [3487], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a general analytical overview of trajectory optimization for moving robots in the context of distributed inference, citing one relevant paper and highlighting key characteristics of flying versus ground robots. It begins to connect these ideas to broader challenges like latency and resource distribution. However, the synthesis remains limited due to the lack of multiple cited sources, and the critical analysis and abstraction are not deeply developed."}}
{"id": "1a610d15-77eb-49a9-a0e6-ea927023f779", "title": "Remote inference of non-sequential DNN models", "level": "subsubsection", "subsections": [], "parent_id": "94d6caee-ed7e-43e0-93c5-bd7d0a529643", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Future directions and open challenges"], ["subsection", "Efficient algorithms for pervasive inference"], ["subsubsection", "Remote inference of non-sequential DNN models"]], "content": "A major part of pervasive inference literature analyzes the remote collaboration, where the source device computes the shallow layers of the model, while the cloud handles the deep layers . In this context, the split point is chosen based on the size of the shared data, the resource capability of the end-device, and the network capacity. This DNN partitioning approach may work well for the standard sequential model, where filters are sequentially reducing the size of the intermediate data. However, state-of-the-art networks do not only include sequential layers with reduced outputs. Indeed, generative models (GAN)  proved their efficiency for image generation, image quality enhancement, text-to-image enhancement, etc. Auto-encoders also showed good performance for image generation, compression, and denoising. These types of networks have large-sized inputs and outputs. Hence, despite the reduced intermediate data, the cloud servers have to return the high-sized results to the source device, which implies high transmission overhead. Another family of efficient neural networks is the RNN (see section \\ref{DRN}) , used mostly for speech recognition and natural language processing. These networks  include loops in their structures and multiple outputs of a single layer, which imposes multiple communications with remote servers in case of partitioning. Other complex DNN structures prevent remote collaboration wisdom, such as the randomly wired networks and Bolzman Machines (BM) having a non-sequential dependency. Keeping up with ever-advancing deep learning designs is a major challenge for per-layer splitting, particularly for remote collaboration. Based on these insights, the scheduling of DNN partitioning should have various patterns depending on the model structure.", "cites": [7217], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from cited papers and external knowledge to explain the challenges of remote inference for non-sequential DNN models. It connects different types of models (e.g., GANs, RNNs, autoencoders) and highlights their structural implications on partitioning and communication overhead. While it provides some critical insight into limitations of per-layer splitting, it lacks deeper comparative analysis or a novel framework that would elevate its insight quality to a high level."}}
{"id": "c1e679bc-ad2d-4c41-9f1f-49880ba0cf43", "title": "Fault-tolerance of distributed inference", "level": "subsubsection", "subsections": [], "parent_id": "94d6caee-ed7e-43e0-93c5-bd7d0a529643", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Future directions and open challenges"], ["subsection", "Efficient algorithms for pervasive inference"], ["subsubsection", "Fault-tolerance of distributed inference"]], "content": "When a deep neural network is split into small segments and distributed among multiple physical devices, the risk of nodes failure is increased, which leads to performance drop and even inference abortion. The typical networking wisdom resorts to re-transmission mechanisms along with scheduling redundant paths. These failure management techniques inevitably consume additional bandwidths. The DNNs are characterized by a unique structure that may enclose skip connections, convolutional neural connections, and recurrent links. These features of state-of-the-art networks implicitly increase the robustness and resiliency of the joint inference. More specifically, skip blocks allow receiving information from an intermediate layer in addition to the data fed from the previous one. These connections, serving as a memory for some DL models (e.g., ResNet), can play the role of fault-tolerant paths. If one of the devices fails or leaves the joint system, information from a prior participant can still be propagated forward to the current device via the skip blocks, which adds some failure resiliency. The skip connections proved an unprecedented ability to enhance the accuracy of deep models, in addition to its potential to strengthen the fault-tolerance of  pervasive computing. However, transmission overheads are experienced, particularly for failure-free systems. Thus, a trade-off between accuracy, resilience, and resource utilization should be envisaged. Another vision to be investigated is to train the system without skip connections and use them only in case of failures. This idea is inspired from the Dropout  technique that is used to reduce the data overfitting problem. It is based on randomly dropping some neurons during the training and activating them during the inference. Studying the impact of cutting off some transmissions during the inference for different splitting strategies while re-thinking the dropout training is interesting to strengthen the fault-tolerance of pervasive computing. Very recent works  started to discuss such insights; however, they are still immature.", "cites": [3488, 3460], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from the cited papers to highlight how DNN architecture, specifically skip connections, contributes to fault tolerance in distributed inference. It introduces a novel perspective by drawing an analogy to the Dropout technique, suggesting a training-inference trade-off for enhancing resilience. While it does not deeply critique the cited works, it offers abstract, forward-looking ideas that generalize beyond specific systems, indicating high insight quality."}}
{"id": "be3bba61-0d64-4575-8c25-1903eba1b84b", "title": "Data-locality-aware algorithms", "level": "subsubsection", "subsections": [], "parent_id": "94d6caee-ed7e-43e0-93c5-bd7d0a529643", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Future directions and open challenges"], ["subsection", "Efficient algorithms for pervasive inference"], ["subsubsection", "Data-locality-aware algorithms"]], "content": "Most of the efforts, studying the pervasive inference, focus on splitting and parallelizing the DNN tasks related to a predictive request. Next, based on the resource requirements and their availability in the joint system (e.g., computation and energy), tasks are distributed and assigned to the participants. However, in terms of memory, only the weight of the input data is considered, whereas the weights to store the DNN structure are never taken into account.\nFor example, VGG-16 model has 138 M parameters and requires 512 Mb to store its filters . What worsens the situation is that some partitions impose copying the filters to all participants (e.g., spatial splitting.). Moreover, if the intelligent application is led by multiple DNN models and different segments are assigned to each device, a huge memory burden is experienced. Therefore, data-locality-aware algorithms should be designed. More specifically, the distribution system has to account for the past tasks assigned to each participant and try to maximize the re-usability of previously-stored weights, with consideration to the capacity of the devices. Minimizing the number of weights assigned to each participant, not only contributes to reduce the memory usage, but also guarantees the privacy of the structure against white-box attacks .", "cites": [514], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section integrates the concept of DNN memory usage by referencing the VGG-16 model from the cited paper to illustrate the scale of the problem. It provides a coherent explanation of how current pervasive inference methods neglect certain memory aspects. While there is some critical analysis of existing practices, the section does not extensively compare or contrast different approaches, nor does it offer a comprehensive framework beyond the immediate issue of data-locality-awareness."}}
{"id": "55b167bc-3d59-475a-b656-22fc4ca1df47", "title": "Blending inter and intra data  parallelism for federated learning", "level": "subsubsection", "subsections": [], "parent_id": "e41078b4-d37b-46ef-9b13-b3186be7dfd7", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Future directions and open challenges"], ["subsection", "Enhanced federated learning algorithms"], ["subsubsection", "Blending inter and intra data  parallelism for federated learning"]], "content": "Deep neural networks require intensive memory and computational loads.  This challenge is compounded, when the model is larger and deeper, as it becomes infeasible to acquire training results from a single resource-limited device. Triggered by this challenge, federated learning is proposed to train deep models over tens and even hundreds of CPUs and GPUs, by taking advantage of \\textit{inter-data parallelism} . At present, federated learning techniques split the data to be trained among pervasive nodes while copying the whole DL model to all of them. Still, small devices cannot participate in such a process due to their limited capacities. Hence, blending the \\textit{inter-data parallelism} where the trained data is distributed and the \\textit{intra-data parallelism} where the intermediate data of the model are partitioned, can be a feasible solution to enable training within non-GPU devices. Certainly, the practicality, gains and bottleneck of such an approach are to be examined and studied, as the backpropagation characterizing the training phase imposes huge dependency and communication between devices.", "cites": [9123], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of federated learning in the context of resource constraints by distinguishing between inter and intra data parallelism. It synthesizes the idea from the cited paper and expands on the limitations of current FL approaches for non-GPU devices. While it introduces a potential solution, the analysis remains somewhat general and lacks deeper evaluation of specific methods or broader meta-level insights."}}
{"id": "eebe87bb-7058-4cf1-b2f1-3f065bd3c846", "title": "Demonstrated applications", "level": "subsubsection", "subsections": [], "parent_id": "56e15db5-6a15-4c2d-a643-b7f908271672", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Future directions and open challenges"], ["subsection", "Communication-efficient multi-agent reinforcement learning"], ["subsubsection", "Demonstrated applications"]], "content": "Since most of the MAB algorithms discussed in this paper are recent  , it remains interesting to see their implications on practical applications, for example, quantifying the effect of bounded communication resources or energy used in wearable devices and congestion between edge nodes. Similarly, quantifying the improvement in regret bounds on actual and  Quality of Experience (QoE) metrics can be promising.", "cites": [3439, 7725, 9098, 3436], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly touches on the practical implications of MAB algorithms in the context of communication efficiency and real-world constraints such as energy and congestion. However, it does not effectively synthesize the cited papers into a cohesive narrative, nor does it critically evaluate or compare their approaches. The discussion remains at a high level without deeper abstraction or identification of broader principles."}}
{"id": "bc4fd7b2-df00-4f6c-908e-107c575662d1", "title": "Heterogeneity of Bandit agents", "level": "subsubsection", "subsections": [], "parent_id": "56e15db5-6a15-4c2d-a643-b7f908271672", "prefix_titles": [["title", "Pervasive AI for IoT applications: A Survey on Resource-efficient Distributed Artificial Intelligence"], ["section", "Future directions and open challenges"], ["subsection", "Communication-efficient multi-agent reinforcement learning"], ["subsubsection", "Heterogeneity of Bandit agents"]], "content": "In MAB settings, agents might not only differ in the instances they are trying to solve, but also in their computational capabilities. Different computational capabilities mean that agents interact with their environments at different rates, collecting an additional amount of samples and hence having different quality estimates. While the effect of this computational heterogeneity is heavily studied in supervised federated learning , it is not yet investigated either in distributed or federated bandits.", "cites": [3489], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 3.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section introduces the concept of computational heterogeneity in MAB settings and acknowledges its under-researched nature in distributed or federated bandits. It draws a connection to a cited paper on federated learning, but the synthesis is limited to a single source. The critical analysis is present in noting that the problem is 'not yet investigated' in bandit settings, indicating a gap. However, the abstraction is modest, focusing on specific characteristics of bandit agents without broader conceptual generalization."}}
