{"id": "19f07446-4c43-4ad2-99fe-8b45ff3603eb", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "e585d106-fa8f-4b66-996f-1964403cf37d", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning Algorithms for \\\\Motion Planning and Control of Autonomous Vehicles"], ["section", "Introduction"]], "content": "Automated and semi-automated vehicles are gaining popularity in assisting our daily transportation. There is a considerable amount of studies in the past decade focusing on autonomous driving applications . Specifically, a large number of research activities based on deep learning have been conducted for advanced driving assistance systems (ADAS) and automated driving applications, aiming to automate as much of the driving task as possible. Supervised learning approaches rely heavily on large amounts of labeled data to be able to generalize and it is basically trained on each task in isolation. However, obtaining a big amount of data for each individual task in autonomous driving is costly and time-consuming. Moreover, it requires massive human labor to label such data and still may not cover all the complex situations in the real-world driving.\nOn the other hand, reinforcement learning (RL) algorithms have been extensively applied to vehicle decision making and control problems . Specifically, RL is able to learn in a trial-and-error way and does not require explicit human labeling or supervision on each data sample. Instead, it needs a well-defined reward function to receive reward signals in its learning process. Additionally, there is a wide variety of deep RL algorithms and high flexibility in the implementation level, such as the state space, the action space, and the reward function, etc. \nIn general, existing work applying deep RL to the motion planning and control of autonomous vehicles can be divided into the hierarchical (pipeline) approach and the end-to-end approach , as demonstrated in Fig. \\ref{fig_architecture}. Specifically, the pipeline approach can be typically categorized into different modules such as perception, decision making, motion planning, low-level control, and so on. In this paper we'll review and examine papers that apply deep RL algorithms to specifically accomplish functionalities of motion planning\nand vehicle control.\nEach of these modules is engineered manually, and the interfaces between the modules are typically implemented by hands. However, this modular distribution is obviously targeted for the convenience of human interpretation, rather than the highest attainable system performance. For example, if there is a pipeline system with parts that can improve with data, and with parts that they don't, then those parts that do not improve with data will eventually become the bottleneck. \nRecently, more efforts have been devoted to the second approach enabled by deep RL, which is the end-to-end approach that can optimize all those modules of abstractions to map sensory input to control commands with the minimal number of processing steps.\nThere are mainly two reasons why we might want to apply end-to-end techniques to autonomous driving: 1) better performance; and 2) smaller system scales . Better performance will result because the internal components can be self-optimized to maximize overall system performance, instead of optimizing human-selected intermediate criteria. Smaller networks are possible because the system learns to solve problems with minimal processing steps.\n\\begin{figure*}[!t]\n\\centering\n\\includegraphics[width=\\linewidth]{figure/Architecture.pdf}\n\\caption{An illustration of the pipeline and the end-to-end approach for motion planning and control of autonomous vehicles, figure adapted from .}\n\\label{fig_architecture}\n\\end{figure*}\nIn this context, this paper seeks to summarize existing work that explains how deep RL algorithms, when combined together with deep neural network representations, can generalize and perform automated vehicle decision making and control. The rest of the paper is organized as follows. In Sections II and III, we introduce how different deep RL algorithms can be leveraged to accomplish behavioral decision making, motion planning and control modules in the pipeline approach. Next, in Section IV, we take a deep dive into end-to-end learning methods for both real-world applications and the sim-to-real approach. Section V summarizes existing challenges and future work directions in applying deep RL to autonomous vehicles. Finally, a conclusion is provided in Section VI.", "cites": [3174, 3175, 3176, 3177, 3178], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides a coherent narrative by synthesizing key themes from the cited surveys, particularly contrasting pipeline and end-to-end approaches in autonomous driving. It identifies some limitations of the pipeline method and highlights the benefits of end-to-end methods, showing a degree of critical thinking. The section also abstracts beyond specific papers to generalize trends and potential system-level implications, though it stops short of offering a novel or meta-level framework."}}
{"id": "7c724cc9-7332-4c0a-8b74-dd654f7889fc", "title": "Deep Reinforcement Learning for Decision Making and Motion Planning", "level": "section", "subsections": [], "parent_id": "e585d106-fa8f-4b66-996f-1964403cf37d", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning Algorithms for \\\\Motion Planning and Control of Autonomous Vehicles"], ["section", "Deep Reinforcement Learning for Decision Making and Motion Planning"]], "content": "In this section, we focus on recent advances in behavioral decision-making and motion planning for autonomous vehicles based on deep RL. As shown in Fig. \\ref{fig_architecture}, the typical pipelines of an autonomous driving system process a stream of observation from vehicle on-board sensors with high level routing plans to the executable control output such as steering angles, accelerations and braking actions. Typically, a hierarchical structure in the design of planning systems for autonomous driving is usually desired, since driving is naturally hierarchical as higher level decisions are made on discrete state transitions and lower level executions are performed in continuous state space. Behavior layer is a decision making system that determines the transition of discrete state of mid-level driving behaviors such as lane changing, car-following, turning left/right, etc. When the behavior decision is made, the motion planning system is responsible for providing a safe, comfortable, and dynamically feasible continuous trajectory to achieve the selected driving behavior from the decision making system.  \nDeep reinforcement learning has shown great success in the area of vehicle behavioral decision makings, especially in the highway scenarios and urban intersections. To reduce the sample complexity, some of the studies choose to adopt the mid-level inputs from the perception system processes and extract the vehicle state and relative states of surrounding vehicles as inputs. Hoel et al  trained a Deep Q-Network (DQN) in a simulation environment to issue driving behavioral commands (e.g. change lanes to the right/left, cruise on the current lane and etc.) and compared the different neural network structures' effects on the agent performance. To migrate the concerns of safety performance of a trained RL agent, it is common to add rule-based safety constraints that can verify unsafe actions before they are executed. In , a prediction model combined with Deep Q-Network is proposed given the outputs from the perception system to label the unsafe behavioral decisions in unprotected turn scenarios. Some studies   alternatively trained a lane change decision making system based on DQN for behavioral level decision-making and uses an underlying rule-based layer to verify the safety of a planned trajectory before it is executed by the vehicle control system. In , a hierarchical RL based architecture is presented to combine lane change decisions and motion planning together. Specifically, a deep Q-network (DQN) is trained to decide when to conduct the maneuver based on safety considerations, while a deep Q-learning framework with quadratic approximator is designed to complete the maneuver in longitudinal direction. On the other hand, some of the studies choose to learn from human demonstrations via imitation learning and introduce perturbation to discourage undesirable behavior . \nMore recently, actor-critic policy-based RL methods are introduced in autonomous vehicle decision making and motion planning. Compared to value-based RL methods such as Deep Q-Networks that approximate the value function using neural networks in an off-policy way, the primary advantage of actor-critic policy-based RL method is that they can directly compute actions from the policy gradient rather than optimizing from the value function, while remaining stable during function approximations. On the other hand, the  merit of the critic is to supply the actor with the knowledge of performance in low variance. Actor-Critic algorithm has been successfully applied in both discrete behavior decision makings and continuous motion planning tasks .", "cites": [3179, 3182, 3181, 3183, 3180, 3184], "cite_extract_rate": 0.75, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the cited papers by grouping them into categories such as hierarchical RL, imitation learning, and actor-critic methods, and connects them to the broader themes of decision-making and motion planning in autonomous vehicles. It provides some critical analysis, particularly in highlighting the use of rule-based safety constraints and the trade-offs between performance and safety. The section also offers limited abstraction by identifying trends like the shift from value-based to policy-based methods and the integration of high- and low-level modules, though deeper meta-level insights are not fully developed."}}
{"id": "1e25712d-586a-481f-9ca1-85074e02f85e", "title": "Deep Reinforcement Learning for Vehicle Control", "level": "section", "subsections": [], "parent_id": "e585d106-fa8f-4b66-996f-1964403cf37d", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning Algorithms for \\\\Motion Planning and Control of Autonomous Vehicles"], ["section", "Deep Reinforcement Learning for Vehicle Control"]], "content": "In the pipeline approach, the perception layer such as CNN is typically used to process and recognize objects in digital images. Afterwards, deep RL algorithms can be applied to understand and learn intelligent actions and controls. Many papers are devoted to accomplishing low-level vehicle control with deep RL methods, such as lane keeping , lateral control , longitudinal control  , or both .\nFor lane-keeping, a DDPG model was implemented in continuous state and action space in  to guide its training and apply it to solve the lane keeping (steering control) problem in self-driving or autonomous driving. It is shown that the proposed method can help speed up RL training remarkably for the lane keeping task as compared to the RL algorithm without exploiting the state-action permissibility-based guidance and other baselines that employ constrained action space exploration strategies. To improve efficiency and reduce failures in autonomous vehicles, two different algorithms, namely the robust adversarial RL and neural fictitious self play are proposed in , and compares performance on lane keeping and lane changing scenarios. The results exhibit improved driving efficiency while effectively reducing collision rates compared to baseline control policies produced by traditional RL methods.\nIn terms of the vehicle lateral control, to address the high computational complexity of model predictive control (MPC) for real-time implementation, a fast integrated planning and control framework is proposed in  that combines a driving policy layer and an execution layer. \nSeveral example driving scenarios demonstrated that the performance of the policy layer can be improved quickly and continuously online. In , DDPG was implemented to formulate the lane change behavior with continuous action in a model-free dynamic driving environment, and the reward function takes the position deviation status and the maneuvering time into account. Eventually, the RL agent is trained to smoothly and stably change to the target lane with a success rate of 100\\% under diverse driving situations in simulation. In , a vision-based lateral control system was broken into a perception module and a control module, which is based on deep RL to make a control decision based on features coming from the perception module. The trained RL controller in visual TORCS outperforms the linear quadratic regulator (LQR) controller and model MPC controller on different tracks.", "cites": [3189, 7128, 3185, 3844, 3186, 3187, 3188], "cite_extract_rate": 0.4666666666666667, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.8}, "insight_level": "medium", "analysis": "The section synthesizes several papers on deep RL for vehicle control, connecting them to common tasks like lane keeping and lateral control. It provides some comparative insights, such as improved success rates and reduced collision rates, but lacks a deeper, systematic comparison or evaluation of limitations. While it abstracts slightly by categorizing approaches into policy and execution layers, the insights remain grounded in specific methods rather than offering a broader theoretical or conceptual framework."}}
{"id": "da368349-6332-4058-8435-c2b4911cc5df", "title": "Joint Optimization in Real-World Applications", "level": "subsection", "subsections": [], "parent_id": "ac3263cd-63c1-4273-9eac-9762f732911c", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning Algorithms for \\\\Motion Planning and Control of Autonomous Vehicles"], ["section", "End-to-End Deep Reinforcement Learning for Autonomous Driving"], ["subsection", "Joint Optimization in Real-World Applications"]], "content": "A very early attempt that successfully steered a car on public roads using the end-to-end approach can be found in Pomerleau's pioneering work in 1989 , in which controlling an autonomous land vehicle was trained using a neural network system. In the early 2000's, LeCun et al. built a small off-road robot that uses an end-to-end learning system to avoid obstacles solely from visual input . The robot named DAVE was trained on images sampled from human driving videos, paired with the corresponding steering command $1/r$. While DAVE demonstrated the potential of end-to-end learning, its performance was not reliable enough to provide a complete alternative to more modular methods of off-road driving, as its mean distance between crashes was around 20 meters in complex environments. \nWith the evolution of hardware computation capabilities and the advancement of modern deep learning algorithms, in 2016, researchers at NVIDIA proposed an end-to-end learning based on CNN that learns the entire processing pipeline needed to directly steer an automobile . Training data was collected from less than a hundred hours of expert driving on a wide variety of roads, paired with time-synchronized steering commands generated by the human driver. The sampled images are fed into a CNN consisting of five convolutional layers with three fully connected layers, which then computes the proposed steering command. The weights of the CNN are optimized by minimizing the mean squared error between the proposed command and the benchmark command for that image. This milestone also empirically validated that CNNs can ``learn the entire task of lane and road following without manual decomposition into road or lane marking detection, semantic abstraction, path planning, and control'' . \nDirect deep supervised learning usually requires a large amount of data to learn a generic driving policy and the ground truth labeling for training. For example,  proposes a novel FCN-LSTM architecture to leverage both previous vehicle states and current visual observations using a long short-term memory temporal encoder with a fully convolutional visual encoder. Meanwhile, this work also released the Berkeley DeepDrive Video dataset (BDDV) for learning driving models and used the human driving behavior as the ground truth label for training. As such, human labeling can be costly and time-consuming. Furthermore, the policy is trained to mimic human behavior in a certain scenarios and it is hard to cover all the real-world scenarios.\nBy contrast, a full end-to-end reinforcement learning approaches learn policy in a trail-and-error way, and would not require such supervision. As presented in Sections II and III, deep RL algorithms have been widely employed as independent motion planning or control modules for autonomous vehicles. If extended, they can be also trained together with convolutional layers to constitute an end-to-end approach via joint optimization. For instance, using a single monocular image as input, the actor-critic algorithm was adopted in  to learn a policy for lane following in a handful of training episodes, which is trained to maximize the reward of distance that an agent can travel before intervention by a safety driver. Similarly, an asynchronous actor critic (A3C) framework established in  was used to learn vehicle control, and a thorough evaluation was conducted on unseen tracks and using legal speed limits. This work also demonstrated good domain adaptation capability when testing the proposed control commands on real videos.\nBesides the aforementioned end-to-end on-road driving tasks that need to generalize to a larger domain and contend with moving objects such as cars and pedestrians, an end-to-end learning system for agile, off-road autonomous driving using only low-cost on-board sensors is presented in . Compared with paved roads, the surface of our dirt tracks are constantly evolving and highly stochastic. As a result, to successfully perform high-speed driving in our task, high-frequency decision and execution of both steering and throttle commands are required. By imitating an optimal controller, a deep neural network control policy was successfully trained to map raw, high-dimensional observations to continuous steering and throttle commands.", "cites": [3192, 3190, 3191, 887], "cite_extract_rate": 0.5714285714285714, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes multiple end-to-end approaches, connecting historical efforts to modern deep RL methods and showing a progression in capabilities and challenges. It provides critical insights by highlighting limitations like data cost, generalization, and reliability. Abstraction is present in identifying trends such as the shift from supervised to reinforcement learning, but the analysis remains grounded in specific applications rather than reaching a higher conceptual level."}}
{"id": "cfa88be3-f941-4436-94bf-ef2b41513ae3", "title": "Simulation", "level": "subsection", "subsections": [], "parent_id": "ac3263cd-63c1-4273-9eac-9762f732911c", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning Algorithms for \\\\Motion Planning and Control of Autonomous Vehicles"], ["section", "End-to-End Deep Reinforcement Learning for Autonomous Driving"], ["subsection", "Simulation"]], "content": "Despite the remarkable advancement of end-to-end learning in real-world applications, there are still many challenges that hinders their on-road deployment in full autonomy. Specifically, many of these challenges arise from the fact that rarely are these vehicles trained or tested at all possible scenarios (including corner cases). However, manually creating these cases and collecting data in the real world can be a process that is expensive and often impractical. \nTherefore, a very promising approach is to leverage simulation to gain experience in these situations. In simulation, the cost of putting an obstacle on the road or simulating a car crash is negligible, but it still offers valuable experiences, whilst in a real-world scenario, such unexpected events can lead to severe financial losses and even threaten the lives of pedestrians or people in neighboring vehicles. \n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=3.3in]{figure/Carla.pdf}\n\\caption{Carla environments: Hard Rainy in Town 1 .}\n\\label{fig_Carla}\n\\end{figure}\nWhile simulation has long been an essential part of testing autonomous driving systems, such as Carcraft of Waymo  and Drive Constellation of NVIDIA , only recently has simulation been applied to building and training end-to-end self-driving vehicles on driving simulation platforms, including TORCS , CARLA , Unity , WRC6 , and Vdrift . \nAs a highly portable multi platform car racing simulation, TORCS  can be used as an ordinary car racing game, but also as a research platform. To our knowledge, the first attempt to control vehicles on TORCS using end-to-end technique is presented in  for an RL competition  using vision from the driverâ€™s perspective, which is later processed with CNN and RNN. Since then, many different deep RL algorithms have been applied to drive a vehicle end-to-end on this platform, such as deep deterministic policy gradient (DDPG) , SafeDAgger , and A3C , etc. Specifically,  is the original paper that proposed the DDPG algorithm to handle complex state and action spaces in continuous domain, which is extremely useful to generate continuous action spaces that can adapt to the complex real-world driving scenarios.\nBoth CARLA  and Unity  can be used to efficiently create more realistic simulation environments that are rich in sensory and physical complexity. Their major difference lies in CARLA being an explicit driving simulation while the basic Unity being a more generic engine, and does not describe a very specific realization. CARLA has been developed to support development, training, and validation of autonomous driving systems, which also provides an interface allowing an RL agent to control a vehicle and interact with a dynamic environment. An illustration of the CARLA environment with Hard Rainy in Town 1 is presented in Fig. \\ref{fig_Carla}. Since a vehicle trained end-to-end to imitate an expert cannot be controlled at test time (i.e., cannot take a specific turn at an upcoming intersection), a condition imitation learning approach was proposed in  to enable the learned driving policy to behave as a chauffeur that handles sensorimotor coordination but also continues to respond to navigational commands. To alleviate the the inefficiency of exploring large continuous action spaces that often prohibits the use of classical RL in challenging real driving tasks,  proposed a controllable imitative RL based on DDPG to explore over a reasonably constrained action space guided by encoded experiences that imitate human demonstrations. Extensive experiments on CARLA demonstrate its superior performance in terms of the percentage of successfully completed episodes and good generalization capability in unseen environments. Similarly, to achieve a better robustness of the agent learning strategies when acting in complex and unstable environments, an advantage actor-critic algorithm was implemented in  with multi-step returns.\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=\\columnwidth]{figure/Imitative_model.pdf}\n\\caption{An autonomous driving model using combined deep imitation learning and model-based reinforcement learning .}\n\\label{fig_imitative_model}\n\\end{figure}\nAnother innovative deep RL algorithm that is worth mentioning is introduced by Rhinehart et al. in  and . Specifically, the proposed imitative model, as depicted in Fig. \\ref{fig_imitative_model}, combines imitation learning with model-based RL to develop probabilistic models to overcome the difficulty of specifying appropriate reward functions that are crucial to evoke desirable behaviors. Relying on LiDAR data inputs, the effectiveness of the proposed imitative model to predict expert-like vehicle trajectories is validated using CARLA, but without including other vehicles and pedestrians.\n\\begin{table*}\n\\centering\n    \\caption{A Summary of Different Deep RL Formulations on the Motion Planning of Autonomous Vehicles.}\n    \\resizebox{\\linewidth}{!}{\n\\begin{tabular}{lcccc}\n\\hline\n\\rowcolor[HTML]{C0C0C0} \n                & \\textbf{Algorithm}       & \\textbf{State} $s_t \\in S$ & \\textbf{Action} $a_t \\in A$  & \\textbf{Reward} $R$ / \\textbf{Loss} $\\ell$\\\\\n\\hline\n\\rowcolor[HTML]{EFEFEF} \n\\textbf{Motion Planning (Pipeline)} &          &          &          &        \\\\\n\\hline\nHoel et al. (2018)   &   DQN  &   Vehicle \\& relative states &  $A=\\{\\mathrm{Lane~change, Acc., Brake}\\}$   &   $R\\{\\mathrm{+:Efficiency, Comfort, Safety}\\}$ \\\\ \nIsele et al. (2018)   &   Classical RL   &   Local states &  $A=\\{\\mathrm{Safe~driving~decision}\\}$   &   $R\\{\\mathrm{+:Safety}\\}$ \\\\ \nBansal et al. (2018)   &   Imitation learning   &   Traffic light/dynamic object states  &  $A=\\{\\mathrm{Driving~trajectory}\\}$    &  9 training losses  \\\\\nWang et al. (2019)   &   DQN   &   Vehicle states  &  $A=\\{\\mathrm{Lane~change~decision}\\}$    &   $R\\{\\mathrm{+:Speed; -: Collision, Invalid~Decision}\\}$  \\\\\nMirchevska et al. (2018)   &   DQN   &   Vehicle \\& relative states  &  $A=\\{\\mathrm{Lane~change~decision}\\}$    &   $R\\{\\mathrm{+:Speed}\\}$  \\\\\nFerdowsi et al. (2018)   &   Adversarial RL   &   Vehicle states  &  $A=\\{\\mathrm{Optimal~safe~action}\\}$    &   $R\\{\\mathrm{+:Safety (distance)}\\}$  \\\\\nYe et al. (2019)   &   DDPG   &   Vehicle \\& relative states  &  $A=\\{\\mathrm{Lane~change~decision}\\}$    &   $R\\{\\mathrm{+:Efficiency, Comfort, Safety}\\}$  \\\\\nShi et al. (2019)   &   DQN   &   Vehicle relative states &  $A=\\{\\mathrm{Lane~change~decision}\\}$   &   Hand-Crafted $R(s_t,a_t)$ \\\\  \nNishi et al. (2019)   &   MPDM \\& pAC   &   Vehicle relative states &  $A=\\{\\mathrm{Lane~merging~trajectory}\\}$   &   Hand-Crafted $R(s_t,a_t)$ \\\\  \nYou et al. (2019)   &   Deep inverse RL   &   Grid-form states &  $A=\\{\\mathrm{Optimal~driving~strategy}\\}$   &   Hand-Crafted $R(s_t,a_t)$ \\\\  \nBouton et al. (2019)   &   Q-learning   &   Vehicle \\& agent states &  $A=\\{\\mathrm{Strategic~maneuvers}\\}$   &   $R\\{\\mathrm{+:Goal, Efficiency; -: Collision}\\}$ \\\\\nYe et al. (2020)   &   PPO   &   Vehicle \\& relative states  &  $A=\\{\\mathrm{Lane~change~decision, Car~ following}\\}$    &   $R\\{\\mathrm{+:Efficiency, Comfort, Safety}\\}$  \\\\\n\\hline\n\\rowcolor[HTML]{EFEFEF} \n\\textbf{End-to-End}      &                 &             &         &        \\\\\n\\hline\nLeCun et al. (2005)      &    Imitation learning   &   Camera image   &  $A=\\{a|a\\in [-\\max Left, +\\max Right]\\}$    &  $\\ell^2~\\mathrm{loss}$    \\\\\nLillicrap et al. (2015)         &   DDPG      &  Simulator image     & $A=\\{\\mathrm{Steering, Acc., Brake}\\}$     &   $R\\{\\mathrm{+1:Direction; -1:Collision}\\}$     \\\\\nBojarski et al. (2016)      &    Imitation learning   &   Camera image   &  $A=\\{1/r|r\\in \\mathrm{\\{turning~radius\\}}\\}$    &  $\\ell^2~\\mathrm{loss}$    \\\\\nVitelli et al. (2016)      &   DQN   &   Images and estimated states   &  $A=\\{\\mathrm{Steering, Acc., Brake}\\}$    &  Hand-Crafted $R(s_t,a_t)$ \\\\\nXu et al. (2016)      &   FCN-LSTM   &   Visual and sensor states   &  \\begin{tabular}[c]{@{}c@{}}Disc. $A=\\{\\text{Straight, Stop, Left, Right}\\}$\\\\ Continuous $A=\\left\\{\\vec{v}|\\vec{v} \\in \\mathbb{R}^{2}\\right\\}$\\end{tabular}    &  Cross entropy loss    \\\\\nZhang et al. (2016)      &   SafeDAgger   &   Camera image   &  $A=\\{\\mathrm{Steering, Acc., Brake}\\}$    &  $R\\{\\mathrm{+1:No~crash}\\}$    \\\\\nPerot et al. (2017)      &   A3C   &   Game states   &  $A=\\{\\mathrm{Steering, Acc., Brake}\\}$    &  $R=v(\\cos \\theta-d)$  \\\\\nCodevilla et al. (2017)      &   Imitation learning   &  Images \\& driver internal states   &  $A=\\{\\mathrm{Steering, Acc.}\\}$    &  $\\ell^2~\\mathrm{loss}$  \\\\\nPan, Cheng et al. (2017)      &   Imitation learning   &   Camera image \\& vehicle speed  &  $A=\\{\\mathrm{Steering, Throttle}\\}$    &  Steering/Throttle loss  \\\\\nPan, You et al. (2017)   &   A3C   &   Virtual world images  &  $A=\\{\\mathrm{Steering, Acc., Brake}\\}$    &  $R=\\{+v(\\cos \\alpha-d),-\\mathrm{Collision}\\}$  \\\\\nWang et al. (2018)   &  DDPG   &   LiDAR sensor \\& camera image  &  $A=\\{\\mathrm{Steering, Acc., Brake}\\}$   &  \\begin{tabular}[c]{@{}c@{}}$R_{t}=V_{x} \\cos (\\theta)-\\alpha V_{x} \\sin (\\theta)$\\\\ $~~~~~~-\\gamma|P o s|-\\beta V_{x} |P o s |$ \\end{tabular}   \\\\\nKendall et al. (2018)   &   DDPG   &   Camera image   &  $A=\\{\\mathrm{Steering, Speed}\\}$    &  Distance travelled without infraction  \\\\\nJaritz et al. (2018)   &   A3C   &   Input image   &  $A=\\{\\mathrm{Steering, Acc., Brake, Hand~brake}\\}$    &  $R\\{\\mathrm{+1:On~track; In~lane}\\}$  \\\\\nKlose et al. (2018)   &   DQN   &   Raw sensor input   &  $A=\\{\\mathrm{Acc., Brake}\\}$    &  $R=\\sum_{i=1}^{3}\\exp \\left(-0.5 \\cdot\\left[{x_{t}^{i}}/{\\theta_{i}}\\right)^{2}\\right]$ \\\\\nLiang et al. (2018)   &   Imitative RL   &   Human driving videos   &  $A=\\{\\mathrm{Steering, Acc., Brake}\\}$    &  $R\\{\\mathrm{+:Speed; -1:Collision, Steering}\\}$ \\\\\nRhinehart et al. (2018)   &   Imitative model   &   LiDAR image  &  $A=\\{\\mathrm{Driving~way~points}\\}$    &   Probabilistic inference objectives  \\\\\nMin et al. (2019)   &  DDPG   &   Raw sensor input  &  $A=\\{\\mathrm{Lane~change, Acc., Brake}\\}$   &  $R\\{\\mathrm{+1:Speed; -1:Collision}\\}$   \\\\\nChhor et al. (2019)   &   DDPG   &   Raw sensor input   &  $A=\\{\\mathrm{Steering, Acc., Brake, Hand~brake}\\}$    & $R=V \\cos \\theta-V \\sin \\theta-V|trackPos|$ \\\\\nJaafra et al. (2019)   &   A2C   &   Camera image   &  $A=\\{\\mathrm{Steering, Throttle, Break}\\}$    & $R\\{\\mathrm{+1:Closing~goals;-1: Not~in~lane}\\}$  \\\\\nRhinehart et al. (2019)   &   Imitative model   &   LiDAR image  &  $A=\\{\\mathrm{Driving~way~points}\\}$    &   Probabilistic inference objectives  \\\\\n\\hline\n\\end{tabular}}\n    \\label{tab:DL_sum1}\n\\end{table*}", "cites": [3190, 2851, 3197, 3183, 3195, 3184, 3191, 7699, 3194, 887, 3182, 3179, 3192, 3193, 8628, 7698, 8629, 3181, 3180, 3196], "cite_extract_rate": 0.5128205128205128, "origin_cites_number": 39, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.5, "abstraction": 3.7}, "insight_level": "high", "analysis": "The section synthesizes multiple papers to highlight the role of simulation in end-to-end deep RL for autonomous driving, connecting ideas about cost-effectiveness, exploration efficiency, and policy adaptability. It includes critical evaluations of limitations in imitation learning and exploration challenges. While it presents a coherent narrative, it primarily builds on existing trends rather than offering a novel theoretical framework."}}
{"id": "851468f3-35f0-44aa-9761-a3431529ebf2", "title": "The Sim-to-Real Approach", "level": "subsection", "subsections": [], "parent_id": "ac3263cd-63c1-4273-9eac-9762f732911c", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning Algorithms for \\\\Motion Planning and Control of Autonomous Vehicles"], ["section", "End-to-End Deep Reinforcement Learning for Autonomous Driving"], ["subsection", "The Sim-to-Real Approach"]], "content": "By leveraging the simulated driving scenarios and experiences obtained via simulation, the sim-to-real approach is a good alternative to train an end-to-end driving policy without using real data, as shown in Fig. \\ref{fig_Sim_to_Real}. This field has received a lot of attention during the recent years, and it has been shown in  that just by randomizing the simulator very carefully, a policy can be trained to fly drones indoors using the simulated data without having to do very careful system identification.\nSpecifically, the simulator can facilitate the training of deep neural networks by generating abundant labeled data in many corner cases to extract task-relevant features and acquire good state representations, and the learned knowledge is expected to promote faster learning and better performance in real world scenarios. Therefore, the sim-to-real transfer is an important area of research that can adapt the learned knowledge from vehicle-traffic simulations to the real-world environment for decision making, planning and control.\nHowever, driving autonomously in the urban environment consists of multiple tasks that involve complex and uncertain driving behaviors and interactions with the surrounding traffic. Besides some typical tasks that are shared with highway driving, such as lane keeping, lane changing, overtaking, and car following, driving in the urban environment also includes taking left-turns, complying with road signs and traffic lights, keeping an eye on lower-speed pedestrians and bicyclists, etc. While each of these specific tasks can be separately modeled in the simulator to train the autonomous driving policy with an outstanding performance, the knowledge transfer from the simulator to the real-world scenario would be more challenging due to the large number and complexity of tasks that further intensifies the difference between the source domain (simulator) and the target domain (real-world). Moreover, there are other technical challenges such as real-world visual signal\nnoises, and the training environment in a car driving simulator is often significantly different from real-world\ndriving in terms of their visual appearance .\nDue to the aforementioned challenges, while many research work applied some variant of deep RL algorithms on the simulation platform, only a handful of them attempted to transfer the knowledge learned from the simulator to real-world applications, i.e., . In fact,  claimed itself to be ``the first time of a deep RL driving'', which is trained using the simulator, ``is shown working on real images'', and foresees simulation based RL can be used as initialization strategy for networks used in real applications. However, it is also reported in  that ``end-to-end models trained solely in CARLA were unable to transfer to the real world'', so some domain randomization and domain adaptation methods are needed for bridging the sim-to-real gap in simulators.\n\\begin{figure}\n\\centering\n\\includegraphics[width=\\columnwidth]{figure/Sim_to_Real.pdf}\n\\caption{Training and deployment of policies from Sim-to-Real transfer .}\n\\label{fig_Sim_to_Real}\n\\end{figure}", "cites": [3198, 3197, 3191], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple cited papers into a coherent narrative about the sim-to-real approach in end-to-end deep RL for autonomous driving. It highlights both the potential benefits and the key challenges, such as domain differences and the need for adaptation techniques. While it offers some critical evaluation (e.g., noting the difficulty of transferring models trained in CARLA to the real world), it does not deeply compare or critique the specific methods of the cited papers, and abstraction is moderate, focusing on general trends rather than meta-level insights."}}
{"id": "711f2ddb-e6b4-482b-859f-3b01e66a7426", "title": "The Sim-to-Real Approach", "level": "subsubsection", "subsections": [], "parent_id": "7a58b808-c3cc-4b3a-ac95-c8ee036e569d", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning Algorithms for \\\\Motion Planning and Control of Autonomous Vehicles"], ["section", "Challenges and Future Work Directions"], ["subsection", "Challenges"], ["subsubsection", "The Sim-to-Real Approach"]], "content": "Generally speaking, training with data from only the human driver is not adequate, and collecting a sufficient amount of data from every possible driving condition can be extremely expensive and dangerous. While the sim-to-real approach can help people get away with no real-world data at all, this approach might not be sufficient to solve the problem, even though it is still an excellent way to get the network and parameters initialized. This is because instead of designing each of these pipeline modules by hand, we'll still have to design our simulator by hand. In the end, the simulator will become the bottleneck due to challenges for developing a sufficiently realistic simulator with diverse environments , including the modeling of any interacting traffic participant with realistic dynamics.", "cites": [3199], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear analytical perspective on the sim-to-real approach in deep RL for autonomous vehicles. It synthesizes information from the cited paper by discussing the limitations of real-world data collection and the role of simulators in training. It also critically points out that simulators themselves can be a bottleneck due to the difficulty of modeling realistic environments. However, the analysis remains somewhat surface-level and does not present a novel framework or deep comparative evaluation across multiple sources."}}
{"id": "145dc663-3d64-4782-8604-3b53af585784", "title": "Future Work Directions", "level": "subsection", "subsections": [], "parent_id": "179e351e-a465-4c9d-8ac3-6cd94c2857be", "prefix_titles": [["title", "A Survey of Deep Reinforcement Learning Algorithms for \\\\Motion Planning and Control of Autonomous Vehicles"], ["section", "Challenges and Future Work Directions"], ["subsection", "Future Work Directions"]], "content": "Despite the exciting progress of applied deep RL algorithms in autonomous driving tasks, these approaches are mostly trained for one task at a time, and each new task requires training a new agent, which is data-inefficient and fails to exploit the learned properties of similar tasks. In contrast, humans have the ability to not only learn complex tasks, but they can also adapt rapidly to new or evolving situations. \nTherefore, some methods have been proposed to increase the generalization of deep RL algorithms in a variety of driving tasks. \nOn the one hand, randomizing the simulator environment has been applied to generalize the trained policies . On the other hand, transferring the knowledge accumulated from past experience through continual learning and meta learning has recently been explored to facilitate the generalization in both reinforcement learning  and imitation learning . The ability to continuously learn and adapt quickly is essential to achieving real-world automated driving, which motivates further studies to introduce transfer learning and meta learning concepts into deep RL in the realm of autonomous driving.", "cites": [2695, 3200, 3201], "cite_extract_rate": 1.0, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the cited papers by connecting domain randomization (Paper 1) and meta/continual learning (Papers 2 and 3) to a common theme of improving generalization in deep RL for autonomous vehicles. It abstracts beyond individual methods to highlight a broader trend toward more adaptive and transferable learning strategies. While it provides some critical evaluation (e.g., data inefficiency of model-free methods), the critique is not as deep or nuanced as it could be."}}
