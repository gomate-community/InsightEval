{"id": "8d441c4f-4f3a-4239-a6bd-1e33d281b0ad", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "c660e090-2b21-4ae5-89c4-4f6ef771d9fc", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Introduction"]], "content": "\\begin{quote}\n\\textit{``If they find a parrot who could answer to everything, I would claim it to be an intelligent being without hesitation.''}\n\\hspace*{\\fill}---Denis Diderot, 1875\n\\end{quote}\nArtificial Intelligence (AI) is a field dedicated to designing and developing systems that can replicate human-like intelligence and abilities .\nAs early as the 18th century, philosopher Denis Diderot introduced the idea that if a parrot could respond to every question, it could be considered intelligent . \nWhile Diderot was referring to living beings, like the parrot, his notion highlights the profound concept that a highly intelligent organism could resemble human intelligence.\nIn the 1950s, Alan Turing expanded this notion to artificial entities and proposed the renowned Turing Test  . This test is a cornerstone in AI and aims to explore whether machines can display intelligent behavior comparable to humans. These AI entities are often termed ``agents'', forming the essential building blocks of AI systems.\nTypically in AI, an agent refers to an artificial entity capable of perceiving its surroundings using sensors, making decisions, and then taking actions in response using actuators .\nThe concept of agents originated in Philosophy, with roots tracing back to thinkers like Aristotle and Hume .\nIt describes entities possessing desires, beliefs, intentions, and the ability to take actions .\nThis idea transitioned into computer science, intending to enable computers to understand users' interests and autonomously perform actions on their behalf . \nAs AI advanced, the term ``agent'' found its place in AI research to depict entities showcasing intelligent behavior and possessing qualities like autonomy, reactivity, pro-activeness, and social ability .\nSince then, the exploration and technical advancement of agents have become focal points within the AI community . AI agents are now acknowledged as a pivotal stride towards achieving Artificial General Intelligence (AGI) \\footnote{Also known as Strong AI.}, as they encompass the potential for a wide range of intelligent activities . \nFrom the mid-20th century, significant strides were made in developing smart AI agents as research delved deep into their design and advancement .\nHowever, these efforts have predominantly focused on enhancing specific capabilities, such as symbolic reasoning, or mastering particular tasks like Go or Chess .\nAchieving a broad adaptability across varied scenarios remained elusive.\nMoreover, previous studies have placed more emphasis on the design of algorithms and training strategies, overlooking the development of the model's inherent general abilities like knowledge memorization, long-term planning, effective generalization, and efficient interaction . \nActually, enhancing the inherent capabilities of the model is the pivotal factor for advancing the agent further, and the domain is in need of a powerful foundational model endowed with a variety of key attributes mentioned above to serve as a starting point for agent systems.\nThe development of large language models (LLMs) has brought a glimmer of hope for the further development of agents , and significant progress has been made by the community . According to the notion of World Scope (WS)  which encompasses five levels that depict the research progress from NLP to general AI (i.e., Corpus, Internet, Perception, Embodiment, and Social), the pure LLMs are built on the second level with internet-scale textual inputs and outputs. \nDespite this, LLMs have demonstrated powerful capabilities in knowledge acquisition, instruction comprehension, generalization, planning, and reasoning, while displaying effective natural language interactions with humans. These advantages have earned LLMs the designation of sparks for AGI , making them highly desirable for building intelligent agents to foster a world where humans and agents coexist harmoniously . \nStarting from this, if we elevate LLMs to the status of agents and equip them with an expanded perception space and action space, they have the potential to reach the third and fourth levels of WS. Furthermore, these LLMs-based agents can tackle more complex tasks through cooperation or competition, and emergent social phenomena can be observed when placing them together, potentially achieving the fifth WS level. As shown in Figure \\ref{fig: genshin_fig}, we envision a harmonious society composed of AI agents where human can also participate.\n\\begin{figure}[htbp]\n    \\centering\n    \\includegraphics[width=1.0\\textwidth]{figures/intro_add.pdf}\n    \\caption{Scenario of an envisioned society composed of AI agents, in which humans can also participate. The above image depicts some specific scenes within society. In the kitchen, one agent orders dishes, while another agent is responsible for planning and solving the cooking task. At the concert, three agents are collaborating to perform in a band. Outdoors, two agents are discussing lantern-making, planning the required materials, and finances by selecting and using tools. Users can participate in any of these stages of this social activity.}\n    \\label{fig: genshin_fig}\n\\end{figure} \nIn this paper, we present a comprehensive and systematic survey focusing on LLM-based agents, attempting to investigate the existing studies \nand prospective avenues in this burgeoning field. \nTo this end, we begin by delving into crucial \\textbf{background} information (\\S \\ \\ref{sec:Background}). \nIn particular, we commence by tracing the origin of AI agents from philosophy to the AI domain, along with a brief overview of the debate surrounding the existence of artificial agents (\\S \\ \\ref{sec:Origin of AI Agent}).\nNext, we take the lens of technological trends to provide a concise historical review of the development of AI agents (\\S \\ \\ref{sec:Technological Trends in Agent Research}). \nFinally, we delve into an in-depth introduction of the essential characteristics of agents and elucidate why large language models are well-suited to serve as the main component of brains or controllers for AI agents (\\S \\ \\ref{sec:Key Characteristics of Agent & Why LLMs Are Suitable Agent Brains?}).\nInspired by the definition of the agent, we present a general conceptual \\textbf{framework} for the LLM-based agents with three key parts: \\textbf{brain, perception, and action} (\\S \\ \\ref{sec:The Birth of An Agent: Construction of LLM-based Agents}), and the framework can be tailored to suit different applications.\nWe first introduce the brain, which is primarily composed of a large language model  (\\S \\ \\ref{sec:Brain}).\nSimilar to humans, the brain is the core of an AI agent because it not only stores crucial memories, information, and knowledge but also undertakes essential tasks of information processing, decision-making, reasoning, and planning. It is the key determinant of whether the agent can exhibit intelligent behaviors.\nNext, we introduce the perception module (\\S \\ \\ref{sec:Perception}). For an agent, this module serves a role similar to that of sensory organs for humans. Its primary function is to expand the agent's perceptual space from text-only to a multimodal space that includes diverse sensory modalities like text, sound, visuals, touch, smell, and more. This expansion enables the agent to better perceive information from the external environment.\nFinally, we present the action module for expanding the action space of an agent (\\S \\ \\ref{sec:Action}). Specifically, we expect the agent to be able to possess textual output, take embodied actions, and use tools so that it can better respond to environmental changes and provide feedback, and even alter and shape the environment.\nAfter that, we provide a detailed and thorough introduction to the \\textbf{practical applications} of LLM-based agents and elucidate the foundational design pursuit\\textemdash{``Harnessing AI for good''} (\\S \\ \\ref{sec:Agents in Practice:  Harnessing AI for Good}).\nTo start, we delve into the current applications of a single agent and discuss their performance in text-based tasks and simulated exploration environments, with a highlight on their capabilities in handling specific tasks, driving innovation, and exhibiting human-like survival skills and adaptability (\\S \\ \\ref{sec:General Ability of Single Agent}).\nFollowing that, we take a retrospective look at the development history of multi-agents. We introduce the interactions between agents in LLM-based multi-agent system applications, where they engage in collaboration, negotiation, or competition. Regardless of the mode of interaction, agents collectively strive toward a shared objective (\\S \\ \\ref{sec:Collaborative Potential of Multi Agents}).\nLastly, considering the potential limitations of LLM-based agents in aspects such as privacy security, ethical constraints, and data deficiencies, we discuss the human-agent collaboration. We summarize the paradigms of collaboration between agents and humans: the instructor-executor paradigm and the equal partnership paradigm, along with specific applications in practice (\\S \\ \\ref{sec:Interactive Cooperation between Human-Agent}).\nBuilding upon the exploration of practical applications of LLM-based agents, we now shift our focus to the concept of the ``\\textbf{Agent Society}'', examining the intricate interactions between agents and their surrounding environments (\\S \\ \\ref{sec:Agent Society}).\nThis section begins with an investigation into whether these agents exhibit human-like behavior and possess corresponding personality (\\S \\ref{sec:Behavior and Personality}). \nFurthermore, we introduce the social environments within which the agents operate, including text-based environment, virtual sandbox, and the physical world (\\S \\ref{sec:Environment for Agent Society}). \nUnlike the previous section (\\S \\ \\ref{sec:Perception}), here we will focus on diverse types of the environment rather than how the agents perceive it.\nHaving established the foundation of agents and their environments, we proceed to unveil the simulated societies that they form  (\\S \\ref{sec:Society Simulation}). \nWe will discuss the construction of a simulated society, and go on to examine the social phenomena that emerge from it.\nSpecifically, we will emphasize the lessons and potential risks inherent in simulated societies.\nFinally, we discuss a range of key \\textbf{topics} (\\S \\ \\ref{sec:Discussion}) and open problems within the field of LLM-based agents: (1) the mutual benefits and inspirations of the LLM research and the agent research, where we demonstrate that the development of LLM-based agents has provided many opportunities for both agent and LLM communities (\\S \\ \\ref{sec:Mutual Benefits of LLM Research and Agent Research}); \n(2) existing evaluation efforts and some prospects for LLM-based agents from four dimensions, including utility, sociability, values and the ability to continually evolve (\\S \\ \\ref{sec:Evaluation for LLM-based Agents}); \n(3) potential risks of LLM-based agents, where we discuss adversarial robustness and trustworthiness of LLM-based agents. We also include the discussion of some other risks like misuse, unemployment and the threat to the well-being of the human race (\\S \\ \\ref{sec:Security, Trustworthy And Other Potential Challenges of LLM-based Agents}); \n(4) scaling up the number of agents, where we discuss the potential advantages and challenges of scaling up agent counts, along with the approaches of pre-determined and dynamic scaling (\\S \\ \\ref{sec:Scaling Up the Number of Agents}); (5) several open problems, such as the debate over whether LLM-based agents represent a potential path to AGI, challenges from virtual simulated environment to physical environment, collective Intelligence in AI agents, and Agent as a Service (\\S \\ \\ref{sec:Open Problems}). After all, we hope this paper could provide inspiration to the researchers and practitioners from relevant fields.", "cites": [2945, 2943, 8556, 8441, 7639, 364, 7638, 7119, 2944, 9115], "cite_extract_rate": 0.3225806451612903, "origin_cites_number": 31, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 3.2, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes concepts from multiple cited papers to form a coherent narrative about the evolution and potential of LLM-based agents. It abstracts the discussion to broader principles such as the World Scope framework and the inherent qualities of LLMs, like knowledge memorization and generalization, that make them suitable for AGI. While it identifies some limitations in prior agent research, it could offer deeper critical comparisons between approaches or evaluate the cited papers more rigorously."}}
{"id": "5e920861-6175-4eb5-8114-4a72b75cc3ed", "title": "From the perspective of philosophy, is artificial entities capable of agency?", "level": "paragraph", "subsections": [], "parent_id": "4e5ef45a-8f03-4adf-808e-83e9c0100eb0", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Background"], ["subsection", "Origin of AI Agent"], ["paragraph", "Agent in philosophy."], ["paragraph", "From the perspective of philosophy, is artificial entities capable of agency?"]], "content": "In a general sense, if we define agents as entities with the capacity to act, AI systems do exhibit a form of agency . \nHowever, the term agent is more usually used to refer to entities or subjects that possess consciousness, intentionality, and the ability to act . Within this framework, it's not immediately clear whether artificial systems can possess agency, as it remains uncertain whether they possess internal states that form the basis for attributing desires, beliefs, and intentions. \nSome people argue that attributing psychological states like intention to artificial agents is a form of anthropomorphism and lacks scientific rigor . \nAs Barandiaran et al.  stated, “Being specific about the requirements for agency has told us a lot about how much is still needed for the development of artificial forms of agency.”\nIn contrast, there are also researchers who believe that, in certain circumstances, employing the intentional stance (that is, interpreting agent behavior in terms of intentions) can provide a better description, explanation and abstraction of the actions of artificial agents, much like it is done for humans .\nWith the advancement of language models, the potential emergence of artificial intentional agents appears more promising . \nIn a rigorous sense, language models merely function as conditional probability models, using input to predict the next token . Different from this, humans incorporate social and perceptual context, and speak according to their mental states . \nConsequently, some researchers argue that the current paradigm of language modeling is not compatible with the intentional actions of an agent . \nHowever, there are also researchers who propose that language models can, in a narrow sense, serve as models of agents . They argue that during the process of context-based next-word prediction, current language models can sometimes infer approximate, partial representations of the beliefs, desires, and intentions held by the agent who generated the context. With these representations, the language models can then generate utterances like humans. To support their viewpoint, they conduct experiments to provide some empirical evidence .", "cites": [2946, 679, 364, 2947, 2948, 7120, 2944, 9115], "cite_extract_rate": 0.36363636363636365, "origin_cites_number": 22, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the philosophical debate around whether artificial systems can possess agency, drawing on multiple cited papers to explore differing viewpoints. It synthesizes concepts such as the intentional stance and limitations of current language models, creating a coherent narrative on the topic. While it identifies some key arguments and limitations, the critique is not deeply nuanced, and the generalization is moderate, focusing more on conceptual patterns than on a higher-level theoretical framework."}}
{"id": "8339a4ce-e8f6-4a18-8452-fb82f8c31169", "title": "Reinforcement learning-based agents.", "level": "paragraph", "subsections": [], "parent_id": "eb1b0fea-efba-40a5-8928-be6f8f8aee1b", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Background"], ["subsection", "Technological Trends in Agent Research"], ["paragraph", "Symbolic Agents."], ["paragraph", "Reinforcement learning-based agents."]], "content": "With the improvement of computational capabilities and data availability, along with a growing interest in simulating interactions between intelligent agents and their environments, researchers have begun to utilize reinforcement learning methods to train agents for tackling more challenging and complex tasks .\nThe primary concern in this field is how to enable agents to learn through interactions with their environments, enabling them to achieve maximum cumulative rewards in specific tasks . \nInitially, reinforcement learning (RL) agents were primarily based on fundamental techniques such as policy search and value function optimization, exemplified by Q-learning  and SARSA . \nWith the rise of deep learning, the integration of deep neural networks and reinforcement learning, known as Deep Reinforcement Learning (DRL), has emerged . This allows agents to learn intricate policies from high-dimensional inputs, leading to numerous significant accomplishments like AlphaGo  and DQN . \nThe advantage of this approach lies in its capacity to enable agents to autonomously learn in unknown environments, without explicit human intervention. This allows for its wide application in an array of domains, from gaming to robot control and beyond. \nNonetheless, reinforcement learning faces challenges including long training times, low sample efficiency, and stability concerns, particularly when applied in complex real-world environments .", "cites": [8441, 1912, 620], "cite_extract_rate": 0.2727272727272727, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a factual overview of reinforcement learning-based agents, touching on historical methods, the emergence of DRL, and notable achievements. It integrates a few key concepts from the cited papers but does so in a largely descriptive manner without significant synthesis or abstraction. Limited critical analysis is present, mainly in the form of a brief mention of challenges."}}
{"id": "977b230b-2dfb-431d-b7a9-4cf1e01c9a0e", "title": "Agents with transfer learning and meta learning.", "level": "paragraph", "subsections": [], "parent_id": "eb1b0fea-efba-40a5-8928-be6f8f8aee1b", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Background"], ["subsection", "Technological Trends in Agent Research"], ["paragraph", "Symbolic Agents."], ["paragraph", "Agents with transfer learning and meta learning."]], "content": "Traditionally, training a reinforcement learning agent requires huge sample sizes and long training time, and lacks generalization capability . \nConsequently, researchers have introduced transfer learning to expedite an agent's learning on new tasks . \nTransfer learning reduces the burden of training on new tasks and facilitates the sharing and migration of knowledge across different tasks, thereby enhancing learning efficiency, performance, and generalization capabilities. Furthermore, meta-learning has also been introduced to AI agents . \nMeta-learning focuses on learning how to learn, enabling an agent to swiftly infer optimal policies for new tasks from a small number of samples . \nSuch an agent, when confronted with a new task, can rapidly adjust its learning approach by leveraging acquired general knowledge and policies, consequently reducing the reliance on a large volume of samples. \nHowever, when there exist significant disparities between source and target tasks, the effectiveness of transfer learning might fall short of expectations and there may exist negative transfer .\nAdditionally, the substantial amount of pre-training and large sample sizes required by meta learning make it hard to establish a universal learning policy .", "cites": [2950, 2952, 7612, 3851, 2949, 7641, 1695, 8608, 2951, 7642, 2953, 7640], "cite_extract_rate": 0.7058823529411765, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the core concepts of transfer learning and meta-learning in the context of AI agents, drawing from multiple papers to highlight their roles in improving generalization and reducing sample requirements. It critically discusses limitations such as negative transfer and the dependency on large pre-training samples. The abstraction is strong, as it moves beyond individual methods to articulate broader challenges and principles in agent learning."}}
{"id": "94097603-b046-48a4-98bc-f8e44c056aaf", "title": "Large language model-based agents.", "level": "paragraph", "subsections": [], "parent_id": "eb1b0fea-efba-40a5-8928-be6f8f8aee1b", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Background"], ["subsection", "Technological Trends in Agent Research"], ["paragraph", "Symbolic Agents."], ["paragraph", "Large language model-based agents."]], "content": "As large language models have demonstrated impressive emergent capabilities and have gained immense popularity , researchers have started to leverage these models to construct AI agents . Specifically, they employ LLMs as the primary component of brain or controller of these agents and expand their perceptual and action space through strategies such as multimodal perception and tool utilization .\nThese LLM-based agents can exhibit reasoning and planning abilities comparable to symbolic agents through techniques like Chain-of-Thought (CoT) and problem decomposition . They can also acquire interactive capabilities with the environment, akin to reactive agents, by learning from feedback and performing new actions . \nSimilarly, large language models undergo pre-training on large-scale corpora and demonstrate the capacity for few-shot and zero-shot generalization, allowing for seamless transfer between tasks without the need to update parameters .\nLLM-based agents have been applied to various real-world scenarios, such as software development  and scientific research .\nDue to their natural language comprehension and generation capabilities, they can interact with each other seamlessly, giving rise to collaboration and competition among multiple agents . Furthermore, research suggests that allowing multiple agents to coexist can lead to the emergence of social phenomena .", "cites": [2945, 2956, 2943, 8556, 2191, 2958, 679, 364, 5968, 2959, 2960, 2454, 2955, 2961, 432, 1578, 7643, 1587, 7094, 2954, 8469, 2957, 7468, 431, 9115, 424], "cite_extract_rate": 0.8387096774193549, "origin_cites_number": 31, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from multiple papers, such as the use of Chain-of-Thought (CoT) and self-consistency for reasoning, multimodal perception, and tool utilization, to present a coherent narrative on how LLMs enhance agent capabilities. While it identifies some general trends like zero-shot learning and social phenomena in multi-agent systems, it lacks deeper critical evaluation of individual methods or limitations. The abstraction is moderate, highlighting broader patterns like generalization and interaction but not offering a comprehensive meta-level framework."}}
{"id": "c8d40744-95bd-4352-8e4a-3f03757bfdd6", "title": "Autonomy.", "level": "paragraph", "subsections": ["f3e0a41b-e4e7-4b39-961a-a663635ca67b", "46092a38-7d20-44ef-bb6b-91900b534c1d", "9cf4f8f8-088c-4a66-81f0-f4b112e3b095"], "parent_id": "ce74e6d5-e160-4626-ae27-150f56723c24", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Background"], ["subsection", " Why is LLM suitable as the primary component of an Agent's brain?"], ["paragraph", "Autonomy."]], "content": "Autonomy means that an agent operates without direct intervention from humans or others and possesses a degree of control over its actions and internal states . \nThis implies that an agent should not only possess the capability to follow explicit human instructions for task completion but also exhibit the capacity to initiate and execute actions independently. \nLLMs can demonstrate a form of autonomy through their ability to generate human-like text, engage in conversations, and perform various tasks without detailed step-by-step instructions . \nMoreover, they can dynamically adjust their outputs based on environmental input, reflecting a degree of adaptive autonomy . \nFurthermore, they can showcase autonomy through exhibiting creativity like coming up with novel ideas, stories, or solutions that haven't been explicitly programmed into them . \nThis implies a certain level of self-directed exploration and decision-making. \nApplications like Auto-GPT  exemplify the significant potential of LLMs in constructing autonomous agents. Simply by providing them with a task and a set of available tools, they can autonomously formulate plans and execute them to achieve the ultimate goal.", "cites": [2962, 7119], "cite_extract_rate": 0.2222222222222222, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes concepts of autonomy from the cited papers, linking LLMs' generative and adaptive capabilities to the design of autonomous agents. It abstracts some general principles about how LLMs support self-directed behavior, but lacks deeper critical evaluation of the limitations or conditions under which such autonomy is meaningful or effective."}}
{"id": "f3e0a41b-e4e7-4b39-961a-a663635ca67b", "title": "Reactivity.", "level": "paragraph", "subsections": [], "parent_id": "c8d40744-95bd-4352-8e4a-3f03757bfdd6", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Background"], ["subsection", " Why is LLM suitable as the primary component of an Agent's brain?"], ["paragraph", "Autonomy."], ["paragraph", "Reactivity."]], "content": "Reactivity in an agent refers to its ability to respond rapidly to immediate changes and stimuli in its environment . \nThis implies that the agent can perceive alterations in its surroundings and promptly take appropriate actions.\nTraditionally, the perceptual space of language models has been confined to textual inputs, while the action space has been limited to textual outputs. \nHowever, researchers have demonstrated the potential to expand the perceptual space of LLMs using multimodal fusion techniques, enabling them to rapidly process visual and auditory information from the environment . Similarly, it's also feasible to expand the action space of LLMs through embodiment techniques  and tool usage . \nThese advancements enable LLMs to effectively interact with the real-world physical environment and carry out tasks within it.\nOne major challenge is that LLM-based agents, when performing non-textual actions, require an intermediate step of generating thoughts or formulating tool usage in textual form before eventually translating them into concrete actions. This intermediary process consumes time and reduces the response speed. \nHowever, this aligns closely with human behavioral patterns, where the principle of ``think before you act'' is observed .", "cites": [2965, 2243, 7644, 2963, 2964, 2958, 431, 9115], "cite_extract_rate": 0.8, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from multiple papers to argue that LLMs can be made reactive through multimodal inputs and embodiment, forming a coherent narrative. It critically addresses the delay caused by the intermediary thought generation step but notes that this aligns with human behavior. While it identifies broader patterns in LLM-based agent reactivity, it stops short of forming a novel, meta-level framework."}}
{"id": "46092a38-7d20-44ef-bb6b-91900b534c1d", "title": "Pro-activeness.", "level": "paragraph", "subsections": [], "parent_id": "c8d40744-95bd-4352-8e4a-3f03757bfdd6", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Background"], ["subsection", " Why is LLM suitable as the primary component of an Agent's brain?"], ["paragraph", "Autonomy."], ["paragraph", "Pro-activeness."]], "content": "Pro-activeness denotes that agents don't merely react to their environments; they possess the capacity to display goal-oriented actions by proactively taking the initiative . \nThis property emphasizes that agents can reason, make plans, and take proactive measures in their actions to achieve specific goals or adapt to environmental changes.\nAlthough intuitively the paradigm of next token prediction in LLMs may not possess intention or desire, research has shown that they can implicitly generate representations of these states and guide the model's inference process . \nLLMs have demonstrated a strong capacity for generalized reasoning and planning. By prompting large language models with instructions like ``let's think step by step'', we can elicit their reasoning abilities, such as logical and mathematical reasoning . \nSimilarly, large language models have shown the emergent ability of planning in forms of goal reformulation , task decomposition , and adjusting plans in response to environmental changes .", "cites": [2967, 2956, 2966, 7094, 2955, 7120, 1578, 2191, 2946, 2948, 424], "cite_extract_rate": 0.8461538461538461, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers to build a narrative about how LLMs can exhibit pro-activeness through reasoning and planning. It abstracts key principles like implicit representation of intention, reasoning elicitation via prompting, and emergent planning abilities. While it provides some critical perspective by acknowledging the limitations of LLMs in long-horizon planning, it could have more explicitly contrasted different approaches or evaluated trade-offs between methods."}}
{"id": "9cf4f8f8-088c-4a66-81f0-f4b112e3b095", "title": "Social ability.", "level": "paragraph", "subsections": [], "parent_id": "c8d40744-95bd-4352-8e4a-3f03757bfdd6", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Background"], ["subsection", " Why is LLM suitable as the primary component of an Agent's brain?"], ["paragraph", "Autonomy."], ["paragraph", "Social ability."]], "content": "Social ability refers to an agent's capacity to interact with other agents, including humans, through some kind of agent-communication language .\nLarge language models exhibit strong natural language interaction abilities like understanding and generation . Compared to structured languages or other communication protocals, such capability enables them to interact with other models or humans in an interpretable manner. This forms the cornerstone of social ability for LLM-based agents .\nMany researchers have demonstrated that LLM-based agents can enhance task performance through social behaviors such as collaboration and competition . \nBy inputting specific prompts, LLMs can also play different roles, thereby simulating the social division of labor in the real world .\nFurthermore, when we place multiple agents with distinct identities into a society, emergent social phenomena can be observed .", "cites": [2945, 2454, 2969, 7643, 7119, 2968], "cite_extract_rate": 0.6, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the cited papers by highlighting how LLMs support social abilities in agents through natural language interaction, role-playing, and multi-agent dynamics. It abstracts these ideas into a broader discussion of social behavior and emergent phenomena in agent societies. However, it lacks critical evaluation of the limitations or comparative analysis of the different approaches, which limits its depth in that dimension."}}
{"id": "816dbe84-6c79-4d36-a6da-56ec1565719d", "title": "Natural Language Interaction", "level": "subsubsection", "subsections": ["5a494f89-acb4-4b6c-ae15-7ecb6672d7c0", "73236219-6ebb-4604-bfa2-4fe9670e8ed3", "3e21996e-2d9d-465d-8fb2-55af7f572825"], "parent_id": "a800d0fd-4aab-4f61-805e-5e1329bfb868", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Brain"], ["subsubsection", "Natural Language Interaction"]], "content": "\\label{sec:Natural Language Interaction}\nAs a medium for communication, language contains a wealth of information. In addition to the intuitively expressed content, there may also be the speaker's beliefs, desires, and intentions hidden behind it . Thanks to the powerful natural language understanding and generation capabilities inherent in LLMs , agents can proficiently engage in not only basic interactive conversations  in multiple languages  but also exhibit in-depth comprehension abilities, which allow humans to easily understand and interact with agents . Besides, LLM-based agents that communicate in natural language can earn more trust and cooperate more effectively with humans .", "cites": [1877, 7645, 2969, 2970, 2409, 1547, 1552, 9115, 7462], "cite_extract_rate": 0.75, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a general description of natural language interaction capabilities of LLM-based agents but does not effectively synthesize or connect the cited papers. It mentions the importance of language for communication and trust but lacks comparative or critical analysis of the approaches in the referenced works. The abstraction is minimal, focusing on surface-level features rather than deeper patterns or principles."}}
{"id": "5a494f89-acb4-4b6c-ae15-7ecb6672d7c0", "title": "Multi-turn interactive conversation.", "level": "paragraph", "subsections": [], "parent_id": "816dbe84-6c79-4d36-a6da-56ec1565719d", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Brain"], ["subsubsection", "Natural Language Interaction"], ["paragraph", "Multi-turn interactive conversation."]], "content": "The capability of multi-turn conversation is the foundation of effective and consistent communication. As the core of the brain module, LLMs, such as GPT series , LLaMA series  and T5 series , can understand natural language and generate coherent and contextually relevant responses, which helps agents to comprehend better and handle various problems . However, even humans find it hard to communicate without confusion in one sitting, so multiple rounds of dialogue are necessary. Compared with traditional text-only reading comprehension tasks like SQuAD , multi-turn conversations (1) are interactive, involving multiple speakers, and lack continuity; (2) may involve multiple topics, and the information of the dialogue may also be redundant, making the text structure more complex . In general, the multi-turn conversation is mainly divided into three steps: (1) Understanding the history of natural language dialogue, (2) Deciding what action to take, and (3) Generating natural language responses. LLM-based agents are capable of continuously refining outputs using existing information to conduct multi-turn conversations and effectively achieve the ultimate goal .", "cites": [9, 679, 7646, 7647, 7468, 439, 1552], "cite_extract_rate": 0.7, "origin_cites_number": 10, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a basic descriptive overview of multi-turn conversation in LLM-based agents, mentioning several model series and comparing them to traditional tasks like SQuAD. However, the synthesis is limited to listing properties of multi-turn dialogue and its challenges without deeply connecting the cited works. There is minimal critical analysis or identification of broader patterns or principles."}}
{"id": "73236219-6ebb-4604-bfa2-4fe9670e8ed3", "title": "High-quality natural language generation.", "level": "paragraph", "subsections": [], "parent_id": "816dbe84-6c79-4d36-a6da-56ec1565719d", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Brain"], ["subsubsection", "Natural Language Interaction"], ["paragraph", "High-quality natural language generation."]], "content": "Recent LLMs show exceptional natural language generation capabilities, consistently producing high-quality text in multiple languages . The coherency  and grammatical accuracy  of LLM-generated content have shown steady enhancement, evolving progressively from GPT-3  to InstructGPT , and culminating in GPT-4 . See et al.  empirically affirm that these language models can ``adapt to the style and content of the conditioning text'' . And the results of Fang et al.  suggest that ChatGPT excels in grammar error detection, underscoring its powerful language capabilities. In conversational contexts, LLMs also perform well in key metrics of dialogue quality, including content, relevance, and appropriateness . Importantly, they do not merely copy training data but display a certain degree of creativity, generating diverse texts that are equally novel or even more novel than the benchmarks crafted by humans . Meanwhile, human oversight remains effective through the use of controllable prompts, ensuring precise control over the content generated by these language models .", "cites": [7649, 2971, 2972, 7648, 679, 364, 9115], "cite_extract_rate": 0.6363636363636364, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.3}, "insight_level": "medium", "analysis": "The section provides a descriptive overview of the natural language generation capabilities of LLMs, citing several papers to support claims about quality, style adaptation, and creativity. While it connects some ideas (e.g., from GPT-3 to GPT-4), the synthesis is limited to a general progression. There is minimal critical analysis or identification of broader patterns or principles, with the narrative remaining largely summary-based."}}
{"id": "3e21996e-2d9d-465d-8fb2-55af7f572825", "title": "Intention and implication understanding.", "level": "paragraph", "subsections": [], "parent_id": "816dbe84-6c79-4d36-a6da-56ec1565719d", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Brain"], ["subsubsection", "Natural Language Interaction"], ["paragraph", "Intention and implication understanding."]], "content": "\\label{intention and implication understanding}\nAlthough models trained on the large-scale corpus are already intelligent enough to understand instructions, most are still incapable of emulating human dialogues or fully leveraging the information conveyed in language . \nUnderstanding the implied meanings is essential for effective communication and cooperation with other intelligent agents , and enables one to interpret others' feedback. \nThe emergence of LLMs highlights the potential of foundation models to understand human intentions, but when it comes to vague instructions or other implications, it poses a significant challenge for agents . \nFor humans, grasping the implied meanings from a conversation comes naturally, whereas for agents, they should formalize implied meanings into a reward function that allows them to choose the option in line with the speaker's preferences in unseen contexts . One of the main ways for reward modeling is inferring rewards based on feedback, which is primarily presented in the form of comparisons  (possibly supplemented with reasons ) and unconstrained natural language . Another way involves recovering rewards from descriptions, using the action space as a bridge . Jeon et al.  suggests that human behavior can be mapped to a choice from an implicit set of options, which helps to interpret all the information in a single unifying formalism. By utilizing their understanding of context, agents can take highly personalized and accurate action, tailored to specific requirements.", "cites": [2958, 2975, 1354, 7650, 2974, 8609, 2968, 2973], "cite_extract_rate": 0.8888888888888888, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes several papers on reward modeling and intention understanding to construct a coherent narrative about the challenges and methods in enabling LLM-based agents to interpret human implications. It abstracts key principles such as the role of context and the need to formalize reward functions, and it shows some critical analysis by pointing out the limitations of LLMs in handling vague instructions and the need for unifying formalisms."}}
{"id": "16a460fd-1db6-491a-aa5e-625b589aa970", "title": "Knowledge", "level": "subsubsection", "subsections": [], "parent_id": "a800d0fd-4aab-4f61-805e-5e1329bfb868", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Brain"], ["subsubsection", "Knowledge"]], "content": "\\label{sec:knowledge}\nDue to the diversity of the real world, many NLP researchers attempt to utilize data that has a larger scale. This data usually is unstructured and unlabeled , yet it contains enormous knowledge that language models could learn. In theory, language models can learn more knowledge as they have more parameters , and it is possible for language models to learn and comprehend everything in natural language. Research  shows that language models trained on a large-scale dataset can encode a wide range of knowledge into their parameters and respond correctly to various types of queries. Furthermore, the knowledge can assist LLM-based agents in making informed decisions . All of this knowledge can be roughly categorized into the following types:\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Linguistic knowledge.} Linguistic knowledge  is represented as a system of constraints, a grammar, which defines all and only the possible sentences of the language. It includes morphology, syntax, semantics , and pragmatics. Only the agents that acquire linguistic knowledge can comprehend sentences and engage in multi-turn conversations . Moreover, these agents can acquire multilingual knowledge  by training on datasets that contain multiple languages, eliminating the need for extra translation models.\n    \\item \\textbf{Commonsense knowledge.} \n    Commonsense knowledge  refers to general world facts that are typically taught to most individuals at an early age. For example, people commonly know that medicine is used for curing diseases, and umbrellas are used to protect against rain. Such information is usually not explicitly mentioned in the context. Therefore, the models lacking the corresponding commonsense knowledge may fail to grasp or misinterpret the intended meaning . Similarly, agents without commonsense knowledge may make incorrect decisions, such as not bringing an umbrella when it rains heavily.\n    \\item \\textbf{Professional domain knowledge.} \n    Professional domain knowledge refers to the knowledge associated with a specific domain like programming , mathematics , medicine , etc. It is essential for models to effectively solve problems within a particular domain . For example, models designed to perform programming tasks need to possess programming knowledge, such as code format. Similarly, models intended for diagnostic purposes should possess medical knowledge like the names of specific diseases and prescription drugs.\n\\end{itemize}\nAlthough LLMs demonstrate excellent performance in acquiring, storing, and utilizing knowledge , there remain potential issues and unresolved problems. For example, the knowledge acquired by models during training could become outdated or even be incorrect from the start. A simple way to address this is retraining. However, it requires advanced data, extensive time, and computing resources. Even worse, it can lead to catastrophic forgetting . Therefore, some researchers try editing LLMs to locate and modify specific knowledge stored within the models. This involved unloading incorrect knowledge while simultaneously acquiring new knowledge. Their experiments show that this method can partially edit factual knowledge, but its underlying mechanism still requires further research. Besides, LLMs may generate content that conflicts with the source or factual information , a phenomenon often referred to as hallucinations . It is one of the critical reasons why LLMs can not be widely used in factually rigorous tasks. To tackle this issue, some researchers  proposed a metric to measure the level of hallucinations and provide developers with an effective reference to evaluate the trustworthiness of LLM outputs. Moreover, some researchers enable LLMs to utilize external tools to avoid incorrect knowledge. Both of these methods can alleviate the impact of hallucinations, but further exploration of more effective approaches is still needed.", "cites": [2982, 2958, 8551, 7646, 458, 2985, 472, 366, 2980, 8353, 7575, 2984, 2976, 2979, 9122, 410, 2986, 2977, 2978, 8565, 2981, 8610, 2983, 2436], "cite_extract_rate": 0.7058823529411765, "origin_cites_number": 34, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers on knowledge types in LLMs and integrates them into a coherent categorization (linguistic, commonsense, domain). It offers critical analysis by discussing limitations such as outdated knowledge, hallucinations, and catastrophic forgetting, referencing relevant papers. The section abstracts these findings into broader challenges and potential solutions for knowledge management in LLM-based agents."}}
{"id": "b68264c6-a00d-4008-af42-5772c17cda84", "title": "Memory", "level": "subsubsection", "subsections": ["1864ff2c-4b41-4fda-80e5-e28f0069b8f1", "6f4e2b6a-db5d-42a5-866f-0b99c47d15ee"], "parent_id": "a800d0fd-4aab-4f61-805e-5e1329bfb868", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Brain"], ["subsubsection", "Memory"]], "content": "\\label{sec:memory}\nIn our framework, ``memory'' stores sequences of the agent's past observations, thoughts and actions, which is akin to the definition presented by Nuxoll et al. . Just as the human brain relies on memory systems to retrospectively harness prior experiences for strategy formulation and decision-making, agents necessitate specific memory mechanisms to ensure their proficient handling of a sequence of consecutive tasks . When faced with complex problems, memory mechanisms help the agent to revisit and apply antecedent strategies effectively. Furthermore, these memory mechanisms enable individuals to adjust to unfamiliar environments by drawing on past experiences.\nWith the expansion of interaction cycles in LLM-based agents, two primary challenges arise. The first pertains to the sheer length of historical records. LLM-based agents process prior interactions in natural language format, appending historical records to each subsequent input. As these records expand, they might surpass the constraints of the Transformer architecture that most LLM-based agents rely on. When this occurs, the system might truncate some content. The second challenge is the difficulty in extracting relevant memories. As agents amass a vast array of historical observations and action sequences, they grapple with an escalating memory burden. This makes establishing connections between related topics increasingly challenging, potentially causing the agent to misalign its responses with the ongoing context.", "cites": [8611], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of memory mechanisms in LLM-based agents, drawing connections between human memory systems and agent design. While it references Nuxoll et al., it does not deeply synthesize or integrate insights from multiple cited papers. There is limited critical analysis of the cited work and the challenges discussed are presented more as observations than as evaluated limitations or contrasts between approaches. The abstraction is moderate, as it attempts to frame memory in terms of broader functional challenges like sequence length and relevance extraction."}}
{"id": "1864ff2c-4b41-4fda-80e5-e28f0069b8f1", "title": "Methods for better memory capability.", "level": "paragraph", "subsections": [], "parent_id": "b68264c6-a00d-4008-af42-5772c17cda84", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Brain"], ["subsubsection", "Memory"], ["paragraph", "Methods for better memory capability."]], "content": "Here we introduce several methods to enhance the memory of LLM-based agents.\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Raising the length limit of Transformers.} The first method tries to address or mitigate the inherent sequence length constraints. The Transformer architecture struggles with long sequences due to these intrinsic limits. As sequence length expands, computational demand grows exponentially due to the pairwise token calculations in the self-attention mechanism. Strategies to mitigate these length restrictions encompass text truncation , segmenting inputs , and emphasizing key portions of text . Some other works modify the attention mechanism to reduce complexity, thereby accommodating longer sequences . \n    \\item \\textbf{Summarizing memory.} The second strategy for amplifying memory efficiency hinges on the concept of memory summarization. This ensures agents effortlessly extract pivotal details from historical interactions. Various techniques have been proposed for summarizing memory. Using prompts, some methods succinctly integrate memories , while others emphasize reflective processes to create condensed memory representations . Hierarchical methods streamline dialogues into both daily snapshots and overarching summaries . Notably, specific strategies translate environmental feedback into textual encapsulations, bolstering agents' contextual grasp for future engagements . Moreover, in multi-agent environments, vital elements of agent communication are captured and retained .\n    \\item \\textbf{Compressing memories with vectors or data structures.} By employing suitable data structures, intelligent agents boost memory retrieval efficiency, facilitating prompt responses to interactions. Notably, several methodologies lean on embedding vectors for memory sections, plans, or dialogue histories . Another approach translates sentences into triplet configurations , while some perceive memory as a unique data object, fostering varied interactions . Furthermore, ChatDB  and DB-GPT  integrate the LLMrollers with SQL databases, enabling data manipulation through SQL commands.\n\\end{itemize}", "cites": [2945, 1499, 7653, 2993, 2992, 7651, 7652, 2987, 7643, 2994, 2991, 2990, 2995, 2988, 2989], "cite_extract_rate": 0.6, "origin_cites_number": 25, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section lists various methods to enhance memory capabilities in LLM-based agents, drawing on multiple cited papers. It provides a basic synthesis by grouping methods into categories (e.g., length limit mitigation, summarization, and vector compression), but the connections between the approaches and their implications are minimal. There is little critical evaluation of the papers' strengths or limitations, and while some general patterns are identified (e.g., the need for efficient memory handling), the analysis remains surface-level and does not elevate the discussion to a deeper, meta-level understanding."}}
{"id": "6f4e2b6a-db5d-42a5-866f-0b99c47d15ee", "title": "Methods for memory retrieval.", "level": "paragraph", "subsections": [], "parent_id": "b68264c6-a00d-4008-af42-5772c17cda84", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Brain"], ["subsubsection", "Memory"], ["paragraph", "Methods for memory retrieval."]], "content": "When an agent interacts with its environment or users, it is imperative to retrieve the most appropriate content from its memory. This ensures that the agent accesses relevant and accurate information to execute specific actions. \nAn important question arises: How can an agent select the most suitable memory? Typically, agents retrieve memories in an automated manner . A significant approach in automated retrieval considers three metrics: Recency, Relevance, and Importance. The memory score is determined as a weighted combination of these metrics, with memories having the highest scores being prioritized in the model's context . \nSome research introduces the concept of interactive memory objects, which are representations of dialogue history that can be moved, edited, deleted, or combined through summarization. Users can view and manipulate these objects, influencing how the agent perceives the dialogue . Similarly, other studies allow for memory operations like deletion based on specific commands provided by users . Such methods ensure that the memory content aligns closely with user expectations.", "cites": [2945, 2995, 2994, 2993], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a basic description of memory retrieval methods in LLM-based agents, mentioning concepts from the cited papers such as Recency, Relevance, and Importance metrics, as well as interactive memory objects. However, it lacks deep synthesis across the works, critical evaluation of their approaches or limitations, and fails to abstract to broader principles. The narrative is coherent but remains largely descriptive in nature."}}
{"id": "cdae2d80-4442-4a2d-a217-857af12ac10d", "title": "Reasoning.", "level": "paragraph", "subsections": [], "parent_id": "b0047f35-589a-432f-b59a-d8e9dd4dfe74", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Brain"], ["subsubsection", "Reasoning and Planning"], ["paragraph", "Reasoning."]], "content": "Reasoning, underpinned by evidence and logic, is fundamental to human intellectual endeavors, serving as the cornerstone for problem-solving, decision-making, and critical analysis . Deductive, inductive, and abductive are the primary forms of reasoning commonly recognized in intellectual endeavor . For LLM-based \nagents, like humans, reasoning capacity is crucial for solving complex tasks .\nDiffering academic views exist regarding the reasoning capabilities of large language models. Some argue language models possess reasoning during pre-training or fine-tuning , while others believe it emerges after reaching a certain scale in size . \nSpecifically, the representative Chain-of-Thought (CoT) method  has been demonstrated to elicit the reasoning capacities of large language models by guiding LLMs to generate rationales before outputting the answer.\nSome other strategies have also been presented to enhance the performance of LLMs like self-consistency , self-polish , self-refine  and selection-inference , among others.\nSome studies suggest that the effectiveness of step-by-step reasoning can be attributed to the local statistical structure of training data, with locally structured dependencies between variables yielding higher data efficiency than training on all variables .", "cites": [7094, 8556, 2955, 1578, 2996, 2183, 2463, 2191, 7958, 7654, 9115], "cite_extract_rate": 0.7857142857142857, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a clear analytical overview of reasoning in LLM-based agents, integrating key concepts from multiple papers, especially around the emergence of reasoning and methods like Chain-of-Thought. While it makes some connections across sources (e.g., linking CoT to self-consistency and self-refinement), it does not offer a fully novel synthesis. Critical analysis is present in acknowledging differing views on how reasoning emerges in LLMs, but deeper evaluation of limitations or trade-offs is limited. Abstraction is moderate, as it identifies broader reasoning categories and methods but does not elevate the discussion to a high-level meta-framework."}}
{"id": "1d5be9c0-888e-4821-bc82-dfe96770c740", "title": "Planning.", "level": "paragraph", "subsections": [], "parent_id": "b0047f35-589a-432f-b59a-d8e9dd4dfe74", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Brain"], ["subsubsection", "Reasoning and Planning"], ["paragraph", "Planning."]], "content": "Planning is a key strategy humans employ when facing complex challenges. For humans, planning helps organize thoughts, set objectives, and determine the steps to achieve those objectives .\nJust as with humans, the ability to plan is crucial for agents, and central to this planning module is the capacity for reasoning .  \nThis offers a structured thought process for agents based on LLMs. Through reasoning, agents deconstruct complex tasks into more manageable sub-tasks, devising appropriate plans for each . \nMoreover, as tasks progress, agents can employ introspection to modify their plans, ensuring they align better with real-world circumstances, leading to adaptive and successful task execution.\nTypically, planning comprises two stages: plan formulation and plan reflection.\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Plan formulation.} During the process of plan formulation, agents generally decompose an overarching task into numerous sub-tasks, and various approaches have been proposed in this phase. Notably, some works advocate for LLM-based agents to decompose problems comprehensively in one go, formulating a complete plan at once and then executing it sequentially . In contrast, other studies like the CoT-series employ an adaptive strategy, where they plan and address sub-tasks one at a time, allowing for more fluidity in handling intricate tasks in their entirety . Additionally, some methods emphasize hierarchical planning , while others underscore a strategy in which final plans are derived from reasoning steps structured in a tree-like format. The latter approach argues that agents should assess all possible paths before finalizing a plan . While LLM-based agents demonstrate a broad scope of general knowledge, they can occasionally face challenges when tasked with situations that require expertise knowledge. Enhancing these agents by integrating them with planners of specific domains has shown to yield better performance .\n    \\item \\textbf{Plan reflection.} Upon formulating a plan, it's imperative to reflect upon and evaluate its merits. LLM-based agents leverage internal feedback mechanisms, often drawing insights from pre-existing models, to hone and enhance their strategies and planning approaches . To better align with human values and preferences, agents actively engage with humans, allowing them to rectify some misunderstandings and assimilate this tailored feedback into their planning methodology . Furthermore, they could draw feedback from tangible or virtual surroundings, such as cues from task accomplishments or post-action observations, aiding them in revising and refining their plans .\n\\end{itemize}", "cites": [2997, 2191, 7658, 7655, 8612, 2969, 2999, 5968, 7654, 7657, 2998, 1578, 5587, 7086, 7656, 2967, 7094, 2954, 424], "cite_extract_rate": 0.5135135135135135, "origin_cites_number": 37, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers to outline two planning stages (formulation and reflection) and highlights key strategies like hierarchical planning, tree-like reasoning, and CoT-series approaches. It also identifies limitations (e.g., lack of world models) and proposes integration with domain-specific planners. The abstraction level is strong, generalizing planning strategies and offering a structured understanding of how LLM-based agents can adapt and improve."}}
{"id": "00b311fe-ce10-48e7-9963-25f411a5c476", "title": "Transferability and Generalization", "level": "subsubsection", "subsections": ["a0ea2381-5737-45b5-be49-ba8df6753352", "a7560672-04c5-4301-b1e7-d0d42c849c0a", "e77dde0f-0f03-4af2-ac0a-dcda58da86ab"], "parent_id": "a800d0fd-4aab-4f61-805e-5e1329bfb868", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Brain"], ["subsubsection", "Transferability and Generalization"]], "content": "\\label{sec:transferability and generalization}\nIntelligence shouldn't be limited to a specific domain or task, but rather encompass a broad range of cognitive skills and abilities . The remarkable nature of the human brain is largely attributed to its high degree of plasticity and adaptability. It can continuously adjust its structure and function in response to external stimuli and internal needs, thereby adapting to different environments and tasks. These years, plenty of research indicates that pre-trained models on large-scale corpora can learn universal language representations . Leveraging the power of pre-trained models, with only a small amount of data for fine-tuning, LLMs can demonstrate excellent performance in downstream tasks . There is no need to train new models from scratch, which saves a lot of computation resources. However, through this task-specific fine-tuning, the models lack versatility and struggle to be generalized to other tasks. Instead of merely functioning as a static knowledge repository, LLM-based agents exhibit dynamic learning ability which enables them to adapt to novel tasks swiftly and robustly .", "cites": [7638, 3000, 8469, 364, 1587, 7], "cite_extract_rate": 0.875, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from the cited papers, particularly on transferability, generalization, and fine-tuning, and connects them to the adaptability of LLM-based agents. It abstracts these concepts to argue for the broader potential of LLMs in building versatile agents. However, the critical analysis is limited, as it does not deeply evaluate the limitations or trade-offs of the approaches mentioned."}}
{"id": "a0ea2381-5737-45b5-be49-ba8df6753352", "title": "Unseen task generalization.", "level": "paragraph", "subsections": [], "parent_id": "00b311fe-ce10-48e7-9963-25f411a5c476", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Brain"], ["subsubsection", "Transferability and Generalization"], ["paragraph", "Unseen task generalization."]], "content": "\\label{unseen task generalization}\nStudies show that instruction-tuned LLMs exhibit zero-shot generalization without the need for task-specific fine-tuning . With the expansion of model size and corpus size, LLMs gradually exhibit remarkable emergent abilities in unfamiliar tasks . Specifically, LLMs can complete new tasks they do not encounter in the training stage by following the instructions based on their own understanding. One of the implementations is multi-task learning, for example, FLAN  finetunes language models on a collection of tasks described via instructions, and T0  introduces a unified framework that converts every language problem into a text-to-text format. Despite being purely a language model, GPT-4  demonstrates remarkable capabilities in a variety of domains and tasks, including abstraction, comprehension, vision, coding, mathematics, medicine, law, understanding of human motives and emotions, and others . It is noticed that the choices in prompting are critical for appropriate predictions, and training directly on the prompts can improve the models' robustness in generalizing to unseen tasks . Promisingly, such generalization capability can further be enhanced by scaling up both the model size and the quantity or diversity of training instructions .", "cites": [7638, 8469, 364, 2223, 2215, 2958, 7468, 1587, 9115], "cite_extract_rate": 0.9, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers on zero-shot generalization and instruction-tuned LLMs, highlighting key methods like FLAN, T0, and the role of prompting. It provides a coherent narrative on how LLMs can generalize to unseen tasks and how scaling improves robustness. However, the analysis remains largely descriptive with limited critical evaluation of the methods' limitations or comparative analysis. Some level of abstraction is achieved by discussing generalization and training strategies."}}
{"id": "a7560672-04c5-4301-b1e7-d0d42c849c0a", "title": "In-context learning.", "level": "paragraph", "subsections": [], "parent_id": "00b311fe-ce10-48e7-9963-25f411a5c476", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Brain"], ["subsubsection", "Transferability and Generalization"], ["paragraph", "In-context learning."]], "content": "Numerous studies indicate that LLMs can perform a variety of complex tasks through in-context learning (ICL), which refers to the models' ability to learn from a few examples in the context . Few-shot in-context learning enhances the predictive performance of language models by concatenating the original input with several complete examples as prompts to enrich the context . The key idea of ICL is learning from analogy, which is similar to the learning process of humans . Furthermore, since the prompts are written in natural language, the interaction is interpretable and changeable, making it easier to incorporate human knowledge into LLMs . Unlike the supervised learning process, ICL doesn't involve fine-tuning or parameter updates, which could greatly reduce the computation costs for adapting the models to new tasks. Beyond text, researchers also explore the potential ICL capabilities in different multimodal tasks , making it possible for agents to be applied to large-scale real-world tasks.", "cites": [3002, 1578, 679, 3005, 3003, 3004, 8470, 3001], "cite_extract_rate": 0.7272727272727273, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the concept of in-context learning by drawing on multiple papers to highlight its role in enhancing model generalization without fine-tuning. It abstracts the idea to note the potential for real-world applications and the human-like analogy aspect. However, it lacks deeper critical analysis, such as evaluating the limitations or effectiveness of ICL across different domains or models."}}
{"id": "e77dde0f-0f03-4af2-ac0a-dcda58da86ab", "title": "Continual learning.", "level": "paragraph", "subsections": [], "parent_id": "00b311fe-ce10-48e7-9963-25f411a5c476", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Brain"], ["subsubsection", "Transferability and Generalization"], ["paragraph", "Continual learning."]], "content": "Recent studies  have highlighted the potential of LLMs' planning capabilities in facilitating continuous learning  for agents, which involves continuous acquisition and update of skills. \nA core challenge in continual learning is catastrophic forgetting : as a model learns new tasks, it tends to lose knowledge from previous tasks. \nNumerous efforts have been devoted to addressing the above challenge, which can be broadly separated into three groups, introducing regularly used terms in reference to the previous model , approximating prior data distributions , and designing architectures with task-adaptive parameters . \nLLM-based agents have emerged as a novel paradigm, leveraging the planning capabilities of LLMs to combine existing skills and address more intricate challenges. Voyager  attempts to solve progressively harder tasks proposed by the automatic curriculum devised by GPT-4 . By synthesizing complex skills from simpler programs, the agent not only rapidly enhances its capabilities but also effectively counters catastrophic forgetting.", "cites": [3008, 3010, 8614, 8613, 3006, 3014, 3007, 3009, 3012, 1562, 3011, 3013, 7657, 9115], "cite_extract_rate": 0.9333333333333333, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from multiple continual learning papers, highlighting approaches like task-adaptive parameters, experience replay, and gradient-based methods to address catastrophic forgetting. It then connects these ideas to the context of LLM-based agents, particularly through the example of Voyager. However, the critical evaluation is limited to stating the problem and citing methods without in-depth comparison or critique of their strengths and weaknesses. The abstraction level is moderate, as it identifies the general challenge of catastrophic forgetting in LLM agents and ties together planning and skill acquisition as broader themes."}}
{"id": "0ced1614-c221-4f79-afe2-f2d4e0ea6ddc", "title": "Textual Input", "level": "subsubsection", "subsections": [], "parent_id": "41bf69d5-b721-45c8-ba68-c85605b6389b", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Perception"], ["subsubsection", "Textual Input"]], "content": "\\label{sec:Textual Input}\nText is a way to carry data, information, and knowledge, making text communication one of the most important ways humans interact with the world. An LLM-based agent already has the fundamental ability to communicate with humans through textual input and output . In a user's textual input, aside from the explicit content, there are also beliefs, desires, and intentions hidden behind it. Understanding implied meanings is crucial for the agent to grasp the potential and underlying intentions of human users, thereby enhancing its communication efficiency and quality with users. However, as discussed in \\S \\  \\ref{intention and implication understanding}, understanding implied meanings within textual input remains challenging for the current LLM-based agent. For example, some works\n employ reinforcement learning to perceive implied meanings and models feedback to derive rewards. This helps deduce the speaker's preferences, leading to more personalized and accurate responses from the agent. Additionally, as the agent is designed for use in complex real-world situations, it will inevitably encounter many entirely new tasks. Understanding text instructions for unknown tasks places higher demands on the agent's text perception abilities. As described in \\S \\ \\ref{unseen task generalization}, an LLM that has undergone instruction tuning  can exhibit remarkable zero-shot instruction understanding and generalization abilities, eliminating the need for task-specific fine-tuning.", "cites": [1354, 8609, 2968, 1587, 2973], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers on understanding textual input by LLM-based agents, integrating themes of implied meaning, reinforcement learning, and instruction generalization. While it connects these ideas in a coherent narrative, the critical evaluation is limited to stating challenges and noting effectiveness without deeper comparative analysis or critique of limitations. It abstracts some concepts like the importance of zero-shot learning and pragmatic reward inference, but does not develop a meta-level framework."}}
{"id": "e5e08dfd-81df-4a26-966d-022d572b1a4f", "title": "Visual Input", "level": "subsubsection", "subsections": [], "parent_id": "41bf69d5-b721-45c8-ba68-c85605b6389b", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Perception"], ["subsubsection", "Visual Input"]], "content": "\\label{sec:Visual Input}\nAlthough LLMs exhibit outstanding performance in language comprehension  and multi-turn conversations , they inherently lack visual perception and can only understand discrete textual content. Visual input usually contains a wealth of information about the world, including properties of objects, spatial relationships, scene layouts, and more in the agent's surroundings. Therefore, integrating visual information with data from other modalities can offer the agent a broader context and a more precise understanding , deepening the agent's perception of the environment.\nTo help the agent understand the information contained within images, a straightforward approach is to generate corresponding text descriptions for image inputs, known as image captioning . Captions can be directly linked with standard text instructions and fed into the agent. This approach is highly interpretable and doesn't require additional training for caption generation, which can save a significant number of computational resources. However, caption generation is a low-bandwidth method , and it may lose a lot of potential information during the conversion process. Furthermore,  the agent's focus on images may introduce biases.\nInspired by the excellent performance of transformers  in natural language processing, researchers have extended their use to the field of computer vision. Representative works like ViT/VQVAE  have successfully encoded visual information using transformers. Researchers first divide an image into fixed-size patches and then treat these patches, after linear projection, as input tokens for Transformers . In the end, by calculating self-attention between tokens, they are able to integrate information across the entire image, resulting in a highly effective way to perceive visual content. Therefore, some works  try to combine the image encoder and LLM directly to train the entire model in an end-to-end way. While the agent can achieve remarkable visual perception abilities, it comes at the cost of substantial computational resources.\nExtensively pre-trained visual encoders and LLMs can greatly enhance the agent's visual perception and language expression abilities . Freezing one or both of them during training is a widely adopted paradigm that achieves a balance between training resources and model performance . \nHowever, LLMs cannot directly understand the output of a visual encoder, so it's necessary to convert the image encoding into embeddings that LLMs can comprehend. In other words, it involves aligning the visual encoder with the LLM. \nThis usually requires adding an extra learnable interface layer between them. For example, BLIP-2  and InstructBLIP  use the Querying Transformer(Q-Former) module as an intermediate layer between the visual encoder and the LLM . Q-Former is a transformer that employs learnable query vectors , giving it the capability to extract language-informative visual representations. \nIt can provide the most valuable information to the LLM, reducing the agent's burden of learning visual-language alignment and thereby mitigating the issue of catastrophic forgetting. At the same time, some researchers adopt a computationally efficient method by using a single projection layer to achieve visual-text alignment, reducing the need for training additional parameters . Moreover, the projection layer can effectively integrate with the learnable interface to adapt the dimensions of its outputs, making them compatible with LLMs .\nVideo input consists of a series of continuous image frames. As a result, the methods used by agents to perceive images  may be applicable to the realm of videos, allowing the agent to have good perception of video inputs as well. Compared to image information, video information adds a temporal dimension. Therefore, the agent's understanding of the relationships between different frames in time is crucial for perceiving video information. Some works like Flamingo  ensure temporal order when understanding videos using a mask mechanism. The mask mechanism restricts the agent's view to only access visual information from frames that occurred earlier in time when it perceives a specific frame in the video.", "cites": [3017, 2243, 2238, 2234, 7659, 2963, 7564, 2239, 3016, 38, 8616, 732, 7565, 8615, 3015, 7660, 3018, 2242, 3019, 7590, 9115, 2471], "cite_extract_rate": 0.7096774193548387, "origin_cites_number": 31, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by integrating concepts from multiple papers to explain the evolution and challenges of visual perception in LLM-based agents. It critically evaluates methods like image captioning and transformer-based visual encoders, highlighting trade-offs between interpretability and information loss, as well as computational costs. The section abstracts key principles such as the necessity of visual-text alignment, the role of temporal understanding in video inputs, and the importance of balancing pre-training and instruction tuning."}}
{"id": "a7221142-4255-4510-9156-926fb3740c63", "title": "Auditory Input", "level": "subsubsection", "subsections": [], "parent_id": "41bf69d5-b721-45c8-ba68-c85605b6389b", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Perception"], ["subsubsection", "Auditory Input"]], "content": "\\label{sec:Auditory Input}\nUndoubtedly, auditory information is a crucial component of world information. When an agent possesses auditory capabilities, it can improve its awareness of interactive content, the surrounding environment, and even potential dangers. Indeed, there are numerous well-established models and approaches   for processing audio as a standalone modality. However, these models often excel at specific tasks. Given the excellent tool-using capabilities of LLMs (which will be discussed in detail in \\S \\ref{sec:Action}), a very intuitive idea is that the agent can use LLMs as control hubs, invoking existing toolsets or model repositories in a cascading manner to perceive audio information. For instance, AudioGPT , makes full use of the capabilities of models like FastSpeech , GenerSpeech  , Whisper , and others  which have achieved excellent results in tasks such as Text-to-Speech, Style Transfer, and Speech Recognition.\nAn audio spectrogram provides an intuitive representation of the frequency spectrum of an audio signal as it changes over time . For a segment of audio data over a period of time, it can be abstracted into a finite-length audio spectrogram. An audio spectrogram has a 2D representation, which can be visualized as a flat image. Hence, some research  efforts aim to migrate perceptual methods from the visual domain to audio. AST (Audio Spectrogram Transformer)  employs a Transformer architecture similar to ViT to process audio spectrogram images. By segmenting the audio spectrogram into patches, it achieves effective encoding of audio information. Moreover, some researchers  have drawn inspiration from the idea of freezing encoders to reduce training time and computational costs. They align audio encoding with data encoding from other modalities by adding the same learnable interface layer.", "cites": [3021, 3022, 3020, 3023, 7662, 7661, 2234, 3024], "cite_extract_rate": 0.6153846153846154, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.3, "critical": 2.7, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes several audio-related papers by connecting them to the broader theme of LLM-based agents using existing models as tools for perception. It abstracts the concept of using audio spectrograms and transferring visual methods to audio, but does not provide deep critical analysis of the models' limitations or compare their effectiveness. The narrative is coherent but remains largely centered on model capabilities rather than evaluating trade-offs or novel insights."}}
{"id": "7f892ef2-a804-43d7-9b18-0df32fe7d478", "title": "Textual Output", "level": "subsubsection", "subsections": [], "parent_id": "c024bfeb-81e3-4b9f-a68e-97e714e87e4c", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Action"], ["subsubsection", "Textual Output"]], "content": "\\label{sec:Textual Output}\nAs discussed in \\S \\ \\ref{sec:Natural Language Interaction}, the rise and development of Transformer-based generative large language models have endowed LLM-based agents with inherent language generation capabilities . The text quality they generate excels in various aspects such as fluency, relevance, diversity, controllability . Consequently, LLM-based agents can be exceptionally strong language generators.", "cites": [2971, 2972, 7648], "cite_extract_rate": 0.5, "origin_cites_number": 6, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly mentions the language generation capabilities of LLM-based agents and cites three papers, but it does not synthesize or connect the findings across these works in a meaningful way. It lacks critical evaluation of the cited papers and instead offers a general, high-level description. There is minimal abstraction beyond the specific papers, focusing only on their relevance to text generation in LLM agents."}}
{"id": "bcf4ec84-d797-4736-9bac-62e74c327e46", "title": "Tool Using", "level": "subsubsection", "subsections": ["106b2b1b-36b2-42d7-a184-1d221a2c9342", "e4119f8b-7931-43c9-942b-54ba880b902d", "b90fe27b-c866-4611-9fb1-d38cdbcc81ba", "fb8cbcce-9a06-4e05-ac73-c4f65450b902"], "parent_id": "c024bfeb-81e3-4b9f-a68e-97e714e87e4c", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Action"], ["subsubsection", "Tool Using"]], "content": "\\label{sec:Tool Using}\nTools are extensions of the capabilities of tool users. When faced with complex tasks, humans employ tools to simplify task-solving and enhance efficiency, freeing time and resources. Similarly, agents have the potential to accomplish complex tasks more efficiently and with higher quality if they also learn to use and utilize tools .\nLLM-based agents have \\textit{limitations} in some aspects, and the use of tools can \\textit{strengthen the agents' capabilities}. \nFirst, although LLM-based agents have a strong knowledge base and expertise, they don't have the ability to memorize every piece of training data . They may also fail to steer to correct knowledge due to the influence of contextual prompts , or even generate hallucinate knowledge . Coupled with the lack of corpus, training data, and tuning for specific fields and scenarios, agents' expertise is also limited when specializing in specific domains . Specialized tools enable LLMs to enhance their expertise, adapt domain knowledge, and be more suitable for domain-specific needs in a pluggable form. \nFurthermore, the decision-making process of LLM-based agents lacks transparency, making them less trustworthy in high-risk domains such as healthcare and finance . Additionally, LLMs are susceptible to adversarial attacks , and their robustness against slight input modifications is inadequate. In contrast, agents that accomplish tasks with the assistance of tools exhibit stronger interpretability and robustness. The execution process of tools can reflect the agents' approach to addressing complex requirements and enhance the credibility of their decisions. Moreover, for the reason that tools are specifically designed for their respective usage scenarios, agents utilizing such tools are better equipped to handle slight input modifications and are more resilient against adversarial attacks .\nLLM-based agents not only require the use of tools, but are also \\textit{well-suited} for tool integration. Leveraging the rich world knowledge accumulated through the pre-training process and CoT prompting, LLMs have demonstrated remarkable reasoning and decision-making abilities in complex interactive environments , which help agents break down and address tasks specified by users in an appropriate way. What's more, LLMs show significant potential in intent understanding and other aspects . When agents are combined with tools, the threshold for tool utilization can be lowered, thereby fully unleashing the creative potential of human users .", "cites": [3026, 9115, 3025, 2409, 2191, 8610, 2958, 1552, 3027, 7462], "cite_extract_rate": 0.7692307692307693, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple cited papers to highlight the strengths and weaknesses of LLM-based agents in tool usage. It integrates ideas about domain limitations (Paper 3), adversarial robustness (Paper 9), and tool integration potential (Paper 7) to form a cohesive argument. While it provides some critical analysis, especially regarding trustworthiness and robustness, it could offer deeper evaluation of the cited approaches. The section abstracts well by framing tools as a solution to enhance interpretability, robustness, and domain adaptability, offering meta-level insights into agent design."}}
{"id": "106b2b1b-36b2-42d7-a184-1d221a2c9342", "title": "Understanding tools.", "level": "paragraph", "subsections": [], "parent_id": "bcf4ec84-d797-4736-9bac-62e74c327e46", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Action"], ["subsubsection", "Tool Using"], ["paragraph", "Understanding tools."]], "content": "A prerequisite for an agent to use tools effectively is a comprehensive understanding of the tools' application scenarios and invocation methods. Without this understanding, the process of the agent using tools will become untrustworthy and fail to genuinely enhance the agent's capabilities. Leveraging the powerful zero-shot and few-shot learning abilities of LLMs , agents can acquire knowledge about tools by utilizing \\textit{zero-shot prompts} that describe tool functionalities and parameters, or \\textit{few-shot prompts} that provide demonstrations of specific tool usage scenarios and corresponding methods . These learning approaches parallel human methods of learning by consulting tool manuals or observing others using tools . A single tool is often insufficient when facing complex tasks. Therefore, the agents should first decompose the complex task into subtasks in an appropriate manner, and their understanding of tools play a significant role in task decomposition.", "cites": [679, 7663, 2958, 431], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from multiple papers on tool understanding and use in LLM-based agents, integrating concepts like zero-shot and few-shot learning with the role of external APIs. It abstracts these ideas to explain how tool understanding supports complex task decomposition. However, it lacks deeper critical analysis of the cited works, such as limitations or trade-offs between different approaches."}}
{"id": "e4119f8b-7931-43c9-942b-54ba880b902d", "title": "Learning to use tools.", "level": "paragraph", "subsections": [], "parent_id": "bcf4ec84-d797-4736-9bac-62e74c327e46", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Action"], ["subsubsection", "Tool Using"], ["paragraph", "Learning to use tools."]], "content": "The methods for agents to learn to utilize tools primarily consist of \\textit{learning from demonstrations} and \\textit{learning from feedback}. This involves mimicking the behavior of human experts , as well as understanding the consequences of their actions and making adjustments based on feedback received from both the environment and humans . Environmental feedback encompasses result feedback on whether actions have successfully completed the task and intermediate feedback that captures changes in the environmental state caused by actions; human feedback comprises explicit evaluations and implicit behaviors, such as clicking on links .\nIf an agent rigidly applies tools without \\textit{adaptability}, it cannot achieve acceptable performance in all scenarios. Agents need to generalize their tool usage skills learned in specific contexts to more general situations, such as transferring a model trained on Yahoo search to Google search. To accomplish this, it's necessary for agents to grasp the common principles or patterns in tool usage strategies, which can potentially be achieved through meta-tool learning . Enhancing the agent's understanding of relationships between simple and complex tools, such as how complex tools are built on simpler ones, can contribute to the agents' capacity to generalize tool usage. This allows agents to effectively discern nuances across various application scenarios and transfer previously learned knowledge to new tools . Curriculum learning , which allows an agent to start from simple tools and progressively learn complex ones, aligns with the requirements. Moreover, benefiting from the understanding of user intent reasoning and planning abilities, agents can better design methods of tool utilization and collaboration and then provide higher-quality outcomes.", "cites": [7665, 7664, 364, 2958], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively integrates concepts from the cited papers to discuss how agents can learn to use tools, highlighting both learning from demonstrations and feedback. It abstracts these methods into broader principles such as adaptability, meta-tool learning, and curriculum learning. While it offers some critical points about the limitations of rigid tool use, deeper evaluation or comparison of the cited papers is limited."}}
{"id": "b90fe27b-c866-4611-9fb1-d38cdbcc81ba", "title": "Making tools for self-sufficiency.", "level": "paragraph", "subsections": [], "parent_id": "bcf4ec84-d797-4736-9bac-62e74c327e46", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Action"], ["subsubsection", "Tool Using"], ["paragraph", "Making tools for self-sufficiency."]], "content": "Existing tools are often designed for human convenience, which might not be optimal for agents. To make agents use tools better, there's a need for tools specifically designed for agents. These tools should be more modular and have input-output formats that are more suitable for agents. \nIf instructions and demonstrations are provided, LLM-based agents also possess the ability to create tools by generating executable programs, or integrating existing tools into more powerful ones . and they can learn to perform self-debugging .\nMoreover, if the agent that serves as a tool maker successfully creates a tool, it can produce packages containing the tool's code and demonstrations for other agents in a multi-agent system, in addition to using the tool itself .\nSpeculatively, in the future, agents might become self-sufficient and exhibit a high degree of autonomy in terms of tools.", "cites": [2958, 3028, 7465, 7666], "cite_extract_rate": 0.8, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers to present a coherent narrative about LLM-based agents creating and using tools for self-sufficiency. It abstracts beyond individual studies to highlight broader principles like modularity and autonomy. While it introduces an analytical perspective, it lacks deeper critical evaluation of the methods or limitations of the cited works."}}
{"id": "fb8cbcce-9a06-4e05-ac73-c4f65450b902", "title": "Tools can expand the action space of LLM-based agents.", "level": "paragraph", "subsections": [], "parent_id": "bcf4ec84-d797-4736-9bac-62e74c327e46", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Action"], ["subsubsection", "Tool Using"], ["paragraph", "Tools can expand the action space of LLM-based agents."]], "content": "With the help of tools, agents can utilize various external \\textit{resources} such as web applications and other LMs during the reasoning and planning phase . \nThis process can provide information with high expertise, reliability, diversity, and quality for LLM-based agents, facilitating their decision-making and action. \nFor example, search-based tools can improve the scope and quality of the knowledge accessible to the agents with the aid of external databases, knowledge graphs, and web pages, while domain-specific tools can enhance an agent's expertise in the corresponding field . Some researchers have already developed LLM-based controllers that generate SQL statements to query databases, or to convert user queries into search requests and use search engines to obtain the desired results . \nWhat's more, LLM-based agents can use scientific tools to execute tasks like organic synthesis in chemistry, or interface with Python interpreters to enhance their performance on intricate mathematical computation tasks . For multi-agent systems, communication tools (e.g., emails) may serve as a means for agents to interact with each other under strict security constraints, facilitating their \\textit{collaboration}, and showing autonomy and flexibility .\nAlthough the tools mentioned before enhance the capabilities of agents, the medium of interaction with the environment remains text-based. \nHowever, tools are designed to expand the functionality of language models, and their outputs are not limited to text. \nTools for non-textual output can diversify the \\textit{modalities} of agent actions, thereby expanding the application scenarios of LLM-based agents. For example, image processing and generation can be accomplished by an agent that draws on a visual model . In aerospace engineering, agents are being explored for modeling physics and solving complex differential equations ; in the field of robotics, agents are required to plan physical operations and control the robot execution ; and so on. Agents that are capable of dynamically interacting with the environment or the world through tools, or in a multimodal manner, can be referred to as digitally embodied . The \\textit{embodiment} of agents has been a central focus of embodied learning research. We will make a deep discussion on agents' embodied action in \\S\\ref{sec:Embodied Action}.", "cites": [7667, 2995, 432, 7647, 3029, 2958, 431, 7658, 8617], "cite_extract_rate": 0.8181818181818182, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple cited papers by highlighting how various tools (e.g., databases, search engines, visual models, chemistry tools) can expand the action space of LLM-based agents. It abstracts the concept of tool use into broader patterns, such as multimodal output and digital embodiment. While it provides a general analysis of the role and benefits of tools, it includes limited direct critique of the papers' limitations or comparative evaluation of the approaches."}}
{"id": "6fe815df-5f75-45ca-b37a-15ff10bda782", "title": "The potential of LLM-based agents for embodied actions.", "level": "paragraph", "subsections": [], "parent_id": "7798a47f-a570-42a9-a8c5-7596adfc3348", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Action"], ["subsubsection", "Embodied Action"], ["paragraph", "The potential of LLM-based agents for embodied actions."]], "content": "Before the widespread rise of LLMs, researchers tended to use methods like reinforcement learning to explore the embodied actions of agents. Despite the extensive success of RL-based embodiment , it does have certain limitations in some aspects. In brief, RL algorithms face limitations in terms of data efficiency, generalization, and complex problem reasoning due to challenges in modeling the dynamic and often ambiguous real environment, or their heavy reliance on precise reward signal representations . \nRecent studies have indicated that leveraging the rich internal knowledge acquired during the pre-training of LLMs can effectively alleviate these issues . \n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Cost efficiency.} \n    Some on-policy algorithms struggle with sample efficiency as they require fresh data for policy updates while gathering enough embodied data for high-performance training is costly and noisy. The constraint is also found in some end-to-end models . By leveraging the intrinsic knowledge from LLMs, agents like PaLM-E  jointly train robotic data with general visual-language data to achieve significant transfer ability in embodied tasks while also showcasing that geometric input representations can improve training data efficiency.\n    \\item \\textbf{Embodied action generalization.}\n    As discussed in section \\S \\ref{sec:transferability and generalization}, an agent's competence should extend beyond specific tasks. When faced with intricate, uncharted real-world environments, it's imperative that the agent exhibits dynamic learning and generalization capabilities. However, the majority of RL algorithms are designed to train and evaluate relevant skills for specific tasks . In contrast, fine-tuned by diverse forms and rich task types, LLMs have showcased remarkable cross-task generalization capabilities . For instance, PaLM-E exhibits surprising zero-shot or one-shot generalization capabilities to new objects or novel combinations of existing objects . Further, language proficiency represents a distinctive advantage of LLM-based agents, serving both as a means to interact with the environment and as a medium for transferring foundational skills to new tasks . SayCan  decomposes task instructions presented in prompts using LLMs into corresponding skill commands, but in partially observable environments, limited prior skills often do not achieve satisfactory performance . To address this, Voyager  introduces the skill library component to continuously collect novel self-verified skills, which allows for the agent's lifelong learning capabilities.\n    \\item \\textbf{Embodied action planning.}\n    Planning constitutes a pivotal strategy employed by humans in response to complex problems as well as LLM-based agents. Before LLMs exhibited remarkable reasoning abilities, researchers introduced Hierarchical Reinforcement Learning (HRL) methods while the high-level policy constraints sub-goals for the low-level policy and the low-level policy produces appropriate action signals . Similar to the role of high-level policies, LLMs with emerging reasoning abilities  can be seamlessly applied to complex tasks in a zero-shot or few-shot manner . In addition, external feedback from the environment can further enhance LLM-based agents' planning performance. Based on the current environmental feedback, some work  dynamically generate, maintain, and adjust high-level action plans in order to minimize dependency on prior knowledge in partially observable environments, thereby grounding the plan. Feedback can also come from models or humans, which can usually be referred to as the critics, assessing task completion based on the current state and task prompts .\n\\end{itemize}", "cites": [2956, 2997, 8556, 9115, 3032, 2191, 2963, 7671, 620, 5968, 3033, 7657, 3034, 2955, 1578, 5587, 3030, 7668, 8464, 2954, 7670, 7669, 7658, 3031, 424], "cite_extract_rate": 0.78125, "origin_cites_number": 32, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple cited papers to build a coherent narrative about the potential of LLMs in embodied action, highlighting their advantages over RL in data efficiency, generalization, and planning. It critically examines limitations of traditional RL and contrasts them with the strengths of LLMs. The abstraction level is strong, as it identifies broader patterns in how LLMs can support embodied intelligence and adapt to complex environments."}}
{"id": "f3ecc3fd-84a0-476a-a2a5-bd57459e13dc", "title": "Embodied actions for LLM-based agents.", "level": "paragraph", "subsections": [], "parent_id": "7798a47f-a570-42a9-a8c5-7596adfc3348", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Action"], ["subsubsection", "Embodied Action"], ["paragraph", "Embodied actions for LLM-based agents."]], "content": "Depending on the agents' level of autonomy in a task or the complexity of actions, there are several fundamental LLM-based embodied actions, primarily including observation, manipulation, and navigation. \n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Observation.}\n    Observation constitutes the primary ways by which the agent acquires environmental information and updates states, playing a crucial role in enhancing the efficiency of subsequent embodied actions. As mentioned in \\S \\ref{sec:Perception}, observation by embodied agents primarily occurs in environments with various inputs, which are ultimately converged into a multimodal signal. A common approach entails a pre-trained Vision Transformer (ViT) used as the alignment module for text and visual information and special tokens are marked to denote the positions of multimodal data . \n    Soundspaces  proposes the identification of physical spatial geometric elements guided by reverberant audio input, enhancing the agent's observations with a more comprehensive perspective . In recent times, even more research takes audio as a modality for embedded observation. Apart from the widely employed cascading paradigm , audio information encoding similar to ViT further enhances the seamless integration of audio with other modalities of inputs . \n    The agent's observation of the environment can also be derived from real-time linguistic instructions from humans, while human feedback helps the agent in acquiring detail information that may not be readily obtained or parsed .\n    \\item \\textbf{Manipulation.}\n    In general, manipulation tasks for embodied agents include object rearrangements, tabletop manipulation, and mobile manipulation . The typical case entails the agent executing a sequence of tasks in the kitchen, which includes retrieving items from drawers and handing them to the user, as well as cleaning the tabletop . Besides precise observation, this involves combining a series of subgoals by leveraging LLM. Consequently, maintaining synchronization between the agent's state and the subgoals is of significance. DEPS  utilizes an LLM-based interactive planning approach to maintain this consistency and help error correction from agent's feedback throughout the multi-step, long-haul reasoning process.\n    In contrast to these, AlphaBlock  focuses on more challenging manipulation tasks (e.g. making a smiley face using building blocks), which requires the agent to have a more grounded understanding of the instructions. Unlike the existing open-loop paradigm, AlphaBlock constructs a dataset comprising 35 complex high-level tasks, along with corresponding multi-step planning and observation pairs, and then fine-tunes a multimodal model to enhance its comprehension of high-level cognitive instructions.\n    \\item \\textbf{Navigation.} \n    Navigation permits agents to dynamically alter their positions within the environment, which often involves multi-angle and multi-object observations, as well as long-horizon manipulations based on current exploration . Before navigation, it is essential for embodied agents to establish prior internal maps about the external environment, which are typically in the form of a topological map, semantic map or occupancy map . For example, LM-Nav  utilizes the VNM  to create an internal topological map. It further leverages the LLM and VLM for decomposing input commands and analyzing the environment to find the optimal path. Furthermore, some  highlight the importance of spatial representation to achieve the precise localization of spatial targets rather than conventional point or object-centric navigation actions by leveraging the pre-trained VLM model to combine visual features from images with 3D reconstructions of the physical world . Navigation is usually a long-horizon task, where the upcoming states of the agent are influenced by its past actions. A memory buffer and summary mechanism are needed to serve as a reference for historical information , which is also employed in Smallville and Voyager .\n    Additionally, as mentioned in \\S \\ref{sec:Perception}, some works have proposed the audio input is also of great significance, but integrating audio information presents challenges in associating it with the visual environment. A basic framework includes a dynamic path planner that uses visual and auditory observations along with spatial memories to plan a series of actions for navigation . \n\\end{itemize}\nBy integrating these, the agent can accomplish more complex tasks, such as embodied question answering, whose primary objective is autonomous exploration of the environment, and responding to pre-defined multimodal questions, such as \\textit{Is the watermelon in the kitchen larger than the pot? Which one is harder?} To address these questions, the agent needs to navigate to the kitchen, observe the sizes of both objects and then answer the questions through comparison . \nIn terms of control strategies, as previously mentioned, LLM-based agents trained on particular embodied datasets typically generate high-level policy commands to control low-level policies for achieving specific sub-goals. The low-level policy can be a robotic transformer , which takes images and instructions as inputs and produces control commands for the end effector as well as robotic arms in particular embodied tasks . Recently, in virtual embodied environments, the high-level strategies are utilized to control agents in gaming  or simulated worlds . For instance, Voyager  calls the Mineflayer  API interface to continuously acquire various skills and explore the world.", "cites": [2945, 3021, 8618, 7672, 2964, 3041, 2963, 7671, 7661, 3036, 7673, 7657, 3024, 7121, 7643, 7119, 3037, 3035, 3038, 3040, 7658, 3039], "cite_extract_rate": 0.6875, "origin_cites_number": 32, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to categorize and explain fundamental embodied actions (observation, manipulation, navigation) in LLM-based agents, forming a coherent narrative. It includes some critical points, such as challenges in integrating audio with visual inputs or the need for memory mechanisms. While it identifies patterns in multimodal integration and task complexity, it primarily remains focused on specific techniques rather than offering broader theoretical abstractions."}}
{"id": "9b3f01ba-32d4-4ba0-8831-6df488a203c1", "title": "Prospective future of the embodied action.", "level": "paragraph", "subsections": [], "parent_id": "7798a47f-a570-42a9-a8c5-7596adfc3348", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "The Birth of An Agent: Construction of LLM-based Agents"], ["subsection", "Action"], ["subsubsection", "Embodied Action"], ["paragraph", "Prospective future of the embodied action."]], "content": "LLM-based embodied actions are seen as the bridge between virtual intelligence and the physical world, enabling agents to perceive and modify the environment much like humans. However, there remain several constraints such as high costs of physical-world robotic operators and the scarcity of embodied datasets, which foster a growing interest in investigating agents' embodied actions within simulated environments like Minecraft . By utilizing the Mineflayer  API, these investigations enable cost-effective examination of a wide range of embodied agents' operations including exploration, planning, self-improvement, and even lifelong learning . Despite notable progress, achieving optimal embodied actions remains a challenge due to the significant disparity between simulated platforms and the physical world. To enable the effective deployment of embodied agents in real-world scenarios, an increasing demand exists for embodied task paradigms and evaluation criteria that closely mirror real-world conditions . On the other hand, learning to ground language for agents is also an obstacle. For example, expressions like ``jump down like a cat'' primarily convey a sense of lightness and tranquility, but this linguistic metaphor requires adequate world knowledge .  endeavors to amalgamate text distillation with Hindsight Experience Replay (HER) to construct a dataset as the supervised signal for the training process. Nevertheless, additional investigation on grounding embodied datasets still remains necessary while embodied action plays an increasingly pivotal role across various domains in human life.", "cites": [2944, 3042, 3036, 7657], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes insights from multiple papers, connecting the need for embodied action in LLM-based agents with approaches in simulation, distillation, and lifelong learning. It critically evaluates current limitations such as the simulation-to-reality gap and the challenge of language grounding, and abstracts these into broader challenges like the necessity for embodied task paradigms and evaluation metrics. While not proposing a novel framework, it offers a cohesive and insightful narrative on the future of embodied action."}}
{"id": "999df2c1-284f-455e-a493-be4c45ededad", "title": "Agents in Practice:  Harnessing AI for Good", "level": "section", "subsections": ["68dcd67f-709f-49eb-806b-a99687b8dc1d", "bd01e42a-f843-4b64-9632-f7b0387f45e0", "b4e41ac1-32be-4c4a-8045-99042574dff5"], "parent_id": "c660e090-2b21-4ae5-89c4-4f6ef771d9fc", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agents in Practice:  Harnessing AI for Good"]], "content": "\\label{sec:Agents in Practice:  Harnessing AI for Good}\n\\input{figures/sec4_mindmap}\nThe LLM-based agent, as an emerging direction, has gained increasing attention from researchers. Many applications in specific domains and tasks have already been developed, showcasing the powerful and versatile capabilities of agents. We can state with great confidence that, the possibility of having a personal agent capable of assisting users with typical daily tasks is larger than ever before . As an LLM-based agent, its design objective should always be beneficial to humans, i.e., humans can \\textit{harness AI for good}. Specifically, we expect the agent to achieve the following objectives:\n\\begin{enumerate}[leftmargin=*]\n    \\item Assist users in breaking free from daily tasks and repetitive labor, thereby Alleviating human work pressure and enhancing task-solving efficiency.\n    \\item No longer necessitates users to provide explicit low-level instructions. Instead, the agent can independently analyze, plan, and solve problems.\n    \\item After freeing users' hands, the agent also liberates their minds to engage in exploratory and  innovative work, realizing their full potential in cutting-edge scientific fields.\n\\end{enumerate}\nIn this section, we provide an in-depth overview of current applications of LLM-based agents, aiming to offer a broad perspective for the practical deployment scenarios (see Figure \\ref{fig: sec4_framework}). First, we elucidate the diverse application scenarios of Single Agent, including task-oriented, innovation-oriented, and lifecycle-oriented scenarios (\\S \\ \\ref{sec:General Ability of Single Agent}). Then, we present the significant coordinating potential of Multiple Agents. Whether through cooperative interaction for complementarity or adversarial interaction for advancement, both approaches can lead to higher task efficiency and response quality (\\S \\ \\ref{sec:Collaborative Potential of Multi Agents}). Finally, we categorize the interactive collaboration between humans and agents into two paradigms and introduce the main forms and specific applications respectively (\\S \\ \\ref{sec:Interactive Cooperation between Human-Agent}). The topological diagram for LLM-based agent applications is depicted in Figure \\ref{fig:sec4_mindmap}.\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.85 \\textwidth]{figures/sec4_frame.pdf}\n    \\caption{Scenarios of LLM-based agent applications. We mainly introduce three scenarios: single-agent deployment, multi-agent interaction, and human-agent interaction. A \\textbf{single agent} possesses diverse capabilities and can demonstrate outstanding task-solving performance in various application orientations. When \\textbf{multiple agents} interact, they can achieve advancement through cooperative or adversarial interactions. Furthermore, in \\textbf{human-agent} interactions, human feedback can enable agents to perform tasks more efficiently and safely, while agents can also provide better service to humans.}\n    \\label{fig: sec4_framework}\n\\end{figure}", "cites": [3043], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section introduces LLM-based agents and outlines their practical objectives, but it lacks substantial synthesis of the cited paper. It mentions a general theme of 'harnessing AI for good' without integrating or contrasting insights from the cited work. There is minimal critical analysis or identification of broader patterns, though it hints at a framework for agent applications, which provides a slight abstraction above individual examples."}}
{"id": "92c5b1ae-f5b9-4c48-a677-5a623c16b10b", "title": "Task-oriented Deployment", "level": "subsubsection", "subsections": ["5266f85e-b435-4b38-bf21-1e2704c7548d", "b6b6c039-cabe-4650-b541-535b04fcc11e"], "parent_id": "68dcd67f-709f-49eb-806b-a99687b8dc1d", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agents in Practice:  Harnessing AI for Good"], ["subsection", "General Ability of Single Agent"], ["subsubsection", "Task-oriented Deployment"]], "content": "\\label{sec:Task-Oriented Deployment}\nThe LLM-based agents, which can understand human natural language commands and perform everyday tasks , are currently among the most favored and practically valuable agents by users. This is because they have the potential to enhance task efficiency, alleviate user workload, and promote access for a broader user base. In \\textbf{task-oriented deployment}, the agent follows high-level instructions from users, undertaking tasks such as goal decomposition , sequence planning of sub-goals , interactive exploration of the environment , until the final objective is achieved.\nTo explore whether agents can perform basic tasks, they are first deployed in text-based game scenarios. In this type of game, agents interact with the world purely using natural language . By reading textual descriptions of their surroundings and utilizing skills like memory, planning, and trial-and-error , they predict the next action. However, due to the limitation of foundation language models, agents often rely on reinforcement learning during actual execution .\nWith the gradual evolution of LLMs , agents equipped with stronger text understanding and generation abilities have demonstrated great potential to perform tasks through natural language. Due to their oversimplified nature, naive text-based scenarios have been inadequate as testing grounds for LLM-based agents . More realistic and complex simulated test environments have been constructed to meet the demand. Based on task types, we divide these simulated environments into \\textbf{web scenarios} and \\textbf{life scenarios}, and introduce the specific roles that agents play in them.", "cites": [3045, 7674, 5587, 3048, 7656, 3047, 3046, 3044], "cite_extract_rate": 0.6153846153846154, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to discuss the progression of task-oriented LLM-based agents, connecting early text-based games with more realistic web and life scenarios. It offers some abstraction by categorizing environments into web and life scenarios, but the critical analysis is limited to brief mentions of limitations like the oversimplification of early scenarios and the constraints of LLMs. While it provides a coherent narrative, it does not offer a novel framework or deep evaluative critique of the cited works."}}
{"id": "5266f85e-b435-4b38-bf21-1e2704c7548d", "title": "In web scenarios.", "level": "paragraph", "subsections": [], "parent_id": "92c5b1ae-f5b9-4c48-a677-5a623c16b10b", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agents in Practice:  Harnessing AI for Good"], ["subsection", "General Ability of Single Agent"], ["subsubsection", "Task-oriented Deployment"], ["paragraph", "In web scenarios."]], "content": "Performing specific tasks on behalf of users in a web scenario is known as the web navigation problem . Agents interpret user instructions, break them down into multiple basic operations, and interact with computers. This often includes web tasks such as filling out forms, online shopping, and sending emails. Agents need to possess the ability to understand instructions within complex web scenarios, adapt to changes (such as noisy text and dynamic HTML web pages), and generalize successful operations . In this way, agents can achieve accessibility and automation when dealing with unseen tasks in the future , ultimately freeing humans from repeated interactions with computer UIs.\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.9\\textwidth]{figures/sec4_single2.pdf}\n    \\caption{Practical applications of the single LLM-based agent in different scenarios. In \\textbf{task-oriented deployment}, agents assist human users in solving daily tasks. They need to possess basic instruction comprehension and task decomposition abilities. In \\textbf{innovation-oriented deployment}, agents demonstrate the potential for autonomous exploration in scientific domains. In \\textbf{lifecycle-oriented deployment}, agents have the ability to continuously explore, learn, and utilize new skills to ensure long-term survival in an open world.}\n    \\label{fig: sec4_single_agent}\n\\end{figure} \nAgents trained through reinforcement learning can effectively mimic human behavior using predefined actions like typing, searching, navigating to the next page, etc. They perform well in basic tasks such as online shopping  and search engine retrieval , which have been widely explored. However, agents without LLM capabilities may struggle to adapt to the more realistic and complex scenarios in the real-world Internet. In dynamic, content-rich web pages such as online forums or online business management , agents often face challenges in performance.\nIn order to enable successful interactions between agents and more realistic web pages, some researchers  have started to leverage the powerful HTML reading and understanding abilities of LLMs. By designing prompts, they attempt to make agents understand the entire HTML source code and predict more reasonable next action steps. Mind2Web  combines multiple LLMs fine-tuned for HTML, allowing them to summarize verbose HTML code  in real-world scenarios and extract valuable information. Furthermore, WebGum  empowers agents with visual perception abilities by employing a multimodal corpus containing HTML screenshots. It simultaneously fine-tunes the LLM and a visual encoder, deepening the agent's comprehensive understanding of web pages.", "cites": [3045, 7674, 432, 3051, 3050, 3046, 3049], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the cited papers by connecting their shared goal of enabling agents to perform complex web tasks through LLM-based capabilities. It discusses how different systems address common challenges, such as adapting to dynamic web environments. However, while it provides some critical context—like the limitations of agents without LLM capabilities—it does not deeply evaluate or contrast the approaches. The abstraction is moderate, as it highlights broader themes like task decomposition and HTML understanding but stops short of developing a meta-level framework."}}
{"id": "b6b6c039-cabe-4650-b541-535b04fcc11e", "title": "In life scenarios.", "level": "paragraph", "subsections": [], "parent_id": "92c5b1ae-f5b9-4c48-a677-5a623c16b10b", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agents in Practice:  Harnessing AI for Good"], ["subsection", "General Ability of Single Agent"], ["subsubsection", "Task-oriented Deployment"], ["paragraph", "In life scenarios."]], "content": "In many daily household tasks in life scenarios, it's essential for agents to understand implicit instructions and apply common-sense knowledge . For an LLM-based agent trained solely on massive amounts of text, tasks that humans take for granted might require multiple trial-and-error attempts . More realistic scenarios often lead to more obscure and subtle tasks. For example, the agent should proactively turn it on if it's dark and there's a light in the room. To successfully chop some vegetables in the kitchen, the agent needs to anticipate the possible location of a knife .\nCan an agent apply the world knowledge embedded in its training data to real interaction scenarios? Huang et al.  lead the way in exploring this question. They demonstrate that sufficiently large LLMs, with appropriate prompts, can effectively break down high-level tasks into suitable sub-tasks without additional training. However, this static reasoning and planning ability has its potential drawbacks. Actions generated by agents often lack awareness of the dynamic environment around them. For instance, when a user gives the task ``clean the room'', the agent might convert it into unfeasible sub-tasks like ``call a cleaning service'' .\nTo provide agents with access to comprehensive scenario information during interactions, some approaches directly incorporate spatial data and item-location relationships as additional inputs to the model. This allows agents to gain a precise description of their surroundings . Wu et al.  introduce the PET framework, which mitigates irrelevant objects and containers in environmental information through an early error correction method . PET encourages agents to explore the scenario and plan actions more efficiently, focusing on the current sub-task.", "cites": [5587, 3048, 7656, 3047, 3052, 3044], "cite_extract_rate": 0.8571428571428571, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers to discuss how LLM-based agents perform in life scenarios, particularly in understanding implicit instructions and applying common-sense knowledge. It provides some critical analysis by pointing out limitations such as the lack of dynamic environmental awareness. However, the abstraction level is moderate, focusing on specific examples and frameworks rather than deriving broader, meta-level principles."}}
{"id": "9c61b383-c85b-41b0-b6bf-bcca57dcc864", "title": "Innovation-oriented Deployment", "level": "subsubsection", "subsections": [], "parent_id": "68dcd67f-709f-49eb-806b-a99687b8dc1d", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agents in Practice:  Harnessing AI for Good"], ["subsection", "General Ability of Single Agent"], ["subsubsection", "Innovation-oriented Deployment"]], "content": "\\label{sec:Innovation-Oriented Deployment}\nThe LLM-based agent has demonstrated strong capabilities in performing tasks and enhancing the efficiency of repetitive work. However, in a more intellectually demanding field, like cutting-edge science, the potential of agents has not been fully realized yet. This limitation mainly arises from two challenges : On one hand, the inherent complexity of science poses a significant barrier. Many domain-specific terms and multi-dimensional structures are difficult to represent using a single text. As a result, their complete attributes cannot be fully encapsulated. This greatly weakens the agent's cognitive level. On the other hand, there is a severe lack of suitable training data in scientific domains, making it difficult for agents to comprehend the entire domain knowledge . If the ability for autonomous exploration could be discovered within the agent, it would undoubtedly bring about beneficial innovation in human technology.\nCurrently, numerous efforts in various specialized domains aim to overcome this challenge . Experts from the computer field make full use of the agent's powerful code comprehension and debugging abilities . In the fields of chemistry and materials, researchers equip agents with a large number of general or task-specific tools to better understand domain knowledge. Agents evolve into comprehensive scientific assistants, proficient in online research and document analysis to fill data gaps. They also employ robotic APIs for real-world interactions, enabling tasks like material synthesis and mechanism discovery .\nThe potential of LLM-based agents in scientific innovation is evident, yet we do not expect their exploratory abilities to be utilized in applications that could threaten or harm humans. Boiko et al.  study the hidden dangers of agents in synthesizing illegal drugs and chemical weapons, indicating that agents could be misled by malicious users in adversarial prompts. This serves as a warning for our future work.", "cites": [7676, 2961, 7675, 3054, 8617, 3053, 3043], "cite_extract_rate": 0.7, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes information from multiple papers to highlight the challenges and approaches in deploying LLM-based agents for scientific innovation. It connects the issue of domain complexity with data limitations and presents how different fields (e.g., chemistry, theorem proving) are addressing these. While it includes some critical evaluation (e.g., lack of training data and the potential for misuse), the critique remains moderate and the abstraction is limited to general trends without proposing a meta-framework."}}
{"id": "7321448f-4a96-469d-a260-2517bbf28074", "title": "Lifecycle-oriented Deployment", "level": "subsubsection", "subsections": [], "parent_id": "68dcd67f-709f-49eb-806b-a99687b8dc1d", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agents in Practice:  Harnessing AI for Good"], ["subsection", "General Ability of Single Agent"], ["subsubsection", "Lifecycle-oriented Deployment"]], "content": "\\label{sec:Lifecycle-Oriented Deployment}\nBuilding a universally capable agent that can continuously explore, develop new skills, and maintain a long-term life cycle in an open, unknown world is a colossal challenge. This accomplishment is regarded as a pivotal milestone in the field of AGI . Minecraft, as a typical and widely explored simulated survival environment, has become a unique playground for developing and testing the comprehensive ability of an agent. Players typically start by learning the basics, such as mining wood and making crafting tables, before moving on to more complex tasks like fighting against monsters and crafting diamond tools . Minecraft fundamentally reflects the real world, making it conducive for researchers to investigate an agent's potential to survive in the authentic world.\nThe survival algorithms of agents in Minecraft can generally be categorized into two types : \\textbf{low-level control} and \\textbf{high-level planning}. Early efforts mainly focused on reinforcement learning  and imitation learning , enabling agents to craft some low-level items. With the emergence of LLMs, which demonstrated surprising reasoning and analytical capabilities, agents begin to utilize LLM as a high-level planner to guide simulated survival tasks . Some researchers use LLM to decompose high-level task instructions into a series of sub-goals , basic skill sequences , or fundamental keyboard/mouse operations , gradually assisting agents in exploring the open world.\nVoyager, drawing inspiration from concepts similar to AutoGPT, became the first LLM-based embodied lifelong learning agent in Minecraft, based on the long-term goal of ``discovering as many diverse things as possible''. It introduces a skill library for storing and retrieving complex action-executable code, along with an iterative prompt mechanism that incorporates environmental feedback and error correction. This enables the agent to autonomously explore and adapt to unknown environments without human intervention. An AI agent capable of autonomously learning and mastering the entire real-world techniques may not be as distant as once thought .", "cites": [3055, 7657], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the key concepts from the two cited papers, connecting early imitation learning efforts in Minecraft with the more recent LLM-based approach of Voyager. It presents a coherent narrative about the evolution of agent capabilities from low-level control to high-level planning. However, while it acknowledges the significance of these works, it lacks deeper critical analysis or comparison of their limitations and trade-offs. The section also generalizes to some extent by framing the development of agents in Minecraft as a proxy for real-world survival and learning, but it does not fully abstract to broader theoretical principles or trends."}}
{"id": "300ba90b-2ce1-4b39-8ace-ef5e0e564567", "title": "Motivation and Background.", "level": "paragraph", "subsections": ["391bab7a-5807-47fa-ac35-fc598b3d7640"], "parent_id": "bd01e42a-f843-4b64-9632-f7b0387f45e0", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agents in Practice:  Harnessing AI for Good"], ["subsection", "Coordinating Potential of Multiple Agents"], ["paragraph", "Motivation and Background."]], "content": "Although LLM-based agents possess commendable text understanding and generation capabilities, they operate as isolated entities in nature . They lack the ability to collaborate with other agents and acquire knowledge from social interactions. This inherent limitation restricts their potential to learn from multi-turn feedback from others to enhance their performance . Moreover, they cannot be effectively deployed in complex scenarios requiring collaboration and information sharing among multiple agents.\n As early as 1986, Marvin Minsky made a forward-looking prediction. In his book \\textit{The Society of Mind} , he introduced a novel theory of intelligence, suggesting that intelligence emerges from the interactions of many smaller agents with specific functions. For instance, certain agents might be responsible for pattern recognition, while others might handle decision-making or generate solutions. This idea has been put into concrete practice with the rise of distributed artificial intelligence . Multi-agent systems(MAS) , as one of the primary research domains, focus on how a group of agents can effectively coordinate and collaborate to solve problems. Some specialized communication languages, like KQML , were designed early on to support message transmission and knowledge sharing among agents. However, their message formats were relatively fixed, and the semantic expression capacity was limited. In the 21st century, integrating reinforcement learning algorithms (such as Q-learning) with deep learning has become a prominent technique for developing MAS that operate in complex environments . Nowadays, the construction approach based on LLMs is beginning to demonstrate remarkable potential. The natural language communication between agents has become more elegant and easily comprehensible to humans, resulting in a significant leap in interaction efficiency.", "cites": [3056, 7677], "cite_extract_rate": 0.2857142857142857, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from the cited papers and historical context, connecting the foundational theory of multi-agent systems from Minsky to modern LLM-based approaches. It critically addresses limitations of current LLM agents while highlighting advancements in coordination and communication. The section abstracts by framing the broader potential of LLM agents within the evolution of MAS, identifying key shifts in methodology and expressiveness."}}
{"id": "2a46a5a5-2e57-421c-8205-64cead1f5168", "title": "Disordered cooperation.", "level": "paragraph", "subsections": [], "parent_id": "c62525ed-b63b-41af-95d0-1e75d9f45727", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agents in Practice:  Harnessing AI for Good"], ["subsection", "Coordinating Potential of Multiple Agents"], ["subsubsection", "Cooperative Interaction for Complementarity"], ["paragraph", "Disordered cooperation."]], "content": "When three or more agents are present within a system, each agent is free to express their perspectives and opinions openly. They can provide feedback and suggestions for modifying responses related to the task at hand . This entire discussion process is uncontrolled, lacking any specific sequence, and without introducing a standardized collaborative workflow. We refer to this kind of multi-agent cooperation as \\textbf{disordered cooperation}.\nChatLLM network  is an exemplary representative of this concept. It emulates the forward and backward propagation process within a neural network, treating each agent as an individual node. Agents in the subsequent layer need to process inputs from all the preceding agents and propagate forward. One potential solution is introducing a dedicated coordinating agent in multi-agent systems, responsible for integrating and organizing responses from all agents, thus updating the final answer . However, consolidating a large amount of feedback data and extracting valuable insights poses a significant challenge for the coordinating agent.\nFurthermore, \\textbf{majority voting} can also serve as an effective approach to making appropriate decisions. However, there is limited research that integrates this module into multi-agent systems at present. Hamilton  trains nine independent supreme justice agents to better predict judicial rulings in the U.S. Supreme Court, and decisions are made through a majority voting process.", "cites": [3057, 3058, 7678], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section introduces the concept of 'disordered cooperation' and connects it to the cited papers, particularly the ChatLLM Network and Hamilton's work, showing some synthesis. It offers a critical view by pointing out challenges (e.g., consolidating feedback) and limited integration of majority voting. However, the critique remains surface-level and the abstraction is moderate, focusing on a specific coordination pattern rather than broader principles."}}
{"id": "5627ff4c-930e-4b8e-ae58-c1bd89ac7c0d", "title": "Ordered cooperation.", "level": "paragraph", "subsections": [], "parent_id": "c62525ed-b63b-41af-95d0-1e75d9f45727", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agents in Practice:  Harnessing AI for Good"], ["subsection", "Coordinating Potential of Multiple Agents"], ["subsubsection", "Cooperative Interaction for Complementarity"], ["paragraph", "Ordered cooperation."]], "content": "When agents in the system adhere to specific rules, for instance, expressing their opinions one by one in a sequential manner, downstream agents only need to focus on the outputs from upstream. This leads to a significant improvement in task completion efficiency, The entire discussion process is highly organized and ordered. We term this kind of multi-agent cooperation as \\textbf{ordered cooperation}. It's worth noting that systems with only two agents, essentially engaging in a conversational manner through a back-and-forth interaction, also fall under the category of ordered cooperation.\nCAMEL  stands as a successful implementation of a dual-agent cooperative system. Within a role-playing communication framework, agents take on the roles of AI Users (giving instructions) and AI Assistants (fulfilling requests by providing specific solutions). Through multi-turn dialogues, these agents autonomously collaborate to fulfill user instructions . Some researchers have integrated the idea of dual-agent cooperation into a single agent's operation , alternating between rapid and deliberate thought processes to excel in their respective areas of expertise.\nTalebirad et al.  are among the first to systematically introduce a comprehensive LLM-based multi-agent collaboration framework.  This paradigm aims to harness the strengths of each individual agent and foster cooperative relationships among them. Many applications of multi-agent cooperation have successfully been built upon this foundation . Furthermore, AgentVerse  constructs a versatile, multi-task-tested framework for group agents cooperation. It can assemble a team of agents that dynamically adapt according to the task's complexity. To promote more efficient collaboration, researchers hope that agents can learn from successful human cooperation examples . MetaGPT  draws inspiration from the classic waterfall model in software development, standardizing agents' inputs/outputs as engineering documents. By encoding advanced human process management experience into agent prompts, collaboration among multiple agents becomes more structured.\nHowever, during MetaGPT's practical exploration, a potential threat to multi-agent cooperation has been identified. Without setting corresponding rules, frequent interactions among multiple agents can amplify minor hallucinations indefinitely . For example, in software development, issues like incomplete functions, missing dependencies, and bugs that are imperceptible to the human eye may arise. Introducing techniques like cross-validation  or timely external feedback could have a positive impact on the quality of agent outputs.", "cites": [3059, 7643, 7677], "cite_extract_rate": 0.2727272727272727, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers into a coherent narrative about ordered cooperation in multi-agent systems, highlighting key implementations and conceptual frameworks. It includes a critical analysis by pointing out potential threats like hallucination amplification in MetaGPT and suggesting mitigation strategies such as cross-validation. The section abstracts beyond specific examples to discuss broader principles of structured interaction and agent roles, contributing to a meta-level understanding of cooperative agent dynamics."}}
{"id": "c63ea96c-2d7f-4927-86c4-e9545154926b", "title": "Adversarial Interaction for Advancement", "level": "subsubsection", "subsections": [], "parent_id": "bd01e42a-f843-4b64-9632-f7b0387f45e0", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agents in Practice:  Harnessing AI for Good"], ["subsection", "Coordinating Potential of Multiple Agents"], ["subsubsection", "Adversarial Interaction for Advancement"]], "content": "\\label{sec:Adversarial Interactions for Advancement}\nTraditionally, cooperative methods have been extensively explored in multi-agent systems. However, researchers increasingly recognize that introducing concepts from game theory  into systems can lead to more robust and efficient behaviors. In competitive environments, agents can swiftly adjust strategies through dynamic interactions, striving to select the most advantageous or rational actions in response to changes caused by other agents. Successful applications in Non-LLM-based competitive domains already exist . AlphaGo Zero , for instance, is an agent for Go that achieved significant breakthroughs through a process of self-play. Similarly, within LLM-based multi-agent systems, fostering change among agents can naturally occur through competition, argumentation, and debate . By abandoning rigid beliefs and engaging in thoughtful reflection, \\textbf{adversarial interaction} enhances the quality of responses.\nResearchers first delve into the fundamental debating abilities of LLM-based agents . Findings demonstrate that when multiple agents express their arguments in the state of  ``tit for tat'', one agent can receive substantial external feedback from other agents, thereby correcting its distorted thoughts . Consequently, multi-agent adversarial systems find broad applicability in scenarios requiring high-quality responses and accurate decision-making. In reasoning tasks, Du et al.  introduce the concept of debate, endowing agents with responses from fellow peers. When these responses diverge from an agent's own judgments, a ``mental'' argumentation occurs, leading to refined solutions. ChatEval  establishes a role-playing-based multi-agent referee team. Through self-initiated debates, agents evaluate the quality of text generated by LLMs, reaching a level of excellence comparable to human evaluators.\nThe performance of the multi-agent adversarial system has shown considerable promise. However, the system is essentially dependent on the strength of LLMs and faces several basic challenges:\n\\begin{itemize}[leftmargin=*]\n    \\item With prolonged debate, LLM's limited context cannot process the entire input. \n    \\item In a multi-agent environment, computational overhead significantly increases. \n    \\item Multi-agent negotiation may converge to an incorrect consensus, and all agents are firmly convinced of its accuracy . \n\\end{itemize}\nThe development of multi-agent systems is still far from being mature and feasible. Introducing human guides when appropriate to compensate for agents' shortcomings is a good choice to promote the further advancements of agents.", "cites": [2990, 2454, 2957, 7679], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 12, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers to highlight how adversarial interaction in LLM-based agents improves reasoning and evaluation. It connects concepts across works, showing how debate, self-play, and negotiation contribute to agent robustness. While it identifies key challenges, the critique could be more nuanced. The section abstracts from specific examples to discuss broader implications for agent development and AI safety."}}
{"id": "b4e41ac1-32be-4c4a-8045-99042574dff5", "title": "Interactive Engagement between Human and Agent", "level": "subsection", "subsections": ["85dde1b4-6533-48c5-9e44-80fbf202d12f", "55f61a61-7ea0-42c1-bbd8-8e92c55a1d25"], "parent_id": "999df2c1-284f-455e-a493-be4c45ededad", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agents in Practice:  Harnessing AI for Good"], ["subsection", "Interactive Engagement between Human and Agent"]], "content": "\\label{sec:Interactive Cooperation between Human-Agent}\nHuman-agent interaction, as the name suggests, involves agents collaborating with humans to accomplish tasks. \nWith the enhancement of agent capabilities, human involvement becomes progressively essential to effectively guide and oversee agents' actions, ensuring they align with human requirements and objectives . \nThroughout the interaction, humans play a pivotal role by offering guidance or by regulating the safety, legality, and ethical conduct of agents. \nThis is particularly crucial in specialized domains, such as medicine where data privacy concerns exist . In such cases, human involvement can serve as a valuable means to compensate for the lack of data, thereby facilitating smoother and more secure collaborative processes. \nMoreover, considering the anthropological aspect, language acquisition in humans predominantly occurs through communication and interaction , as opposed to merely consuming written content. \nAs a result, agents shouldn't exclusively depend on models trained with pre-annotated datasets; instead, they should evolve through online interaction and engagement. \nThe interaction between humans and agents can be classified into two paradigms (see Figure \\ref{fig: human-agent}): (1) Unequal interaction (i.e., instructor-executor paradigm): humans serve as issuers of instructions, while agents act as executors, essentially participating as assistants to humans in collaboration. (2) Equal interaction (i.e., equal partnership paradigm): agents reach the level of humans, participating on an equal footing with humans in interaction.\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1 \\textwidth]{figures/sec4_human.pdf}\n    \\caption{Two paradigms of human-agent interaction. In the instructor-executor paradigm (left), humans provide instructions or feedback, while agents act as executors. In the equal partnership paradigm (right), agents are human-like, able to engage in empathetic conversation and participate in collaborative tasks with humans.}\n    \\label{fig: human-agent}\n\\end{figure}", "cites": [3061, 3060], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of human-agent interaction, integrating the cited papers by emphasizing the need for alignment and the role of human guidance in ensuring ethical and effective agent behavior. It abstracts some concepts, such as the dual paradigms of interaction, but the critical evaluation is limited to stating the importance of human involvement rather than deeply analyzing the strengths or weaknesses of the cited approaches."}}
{"id": "85dde1b4-6533-48c5-9e44-80fbf202d12f", "title": "Instructor-Executor Paradigm", "level": "subsubsection", "subsections": ["7816f836-7e25-42e8-ac5a-03b33b95622b", "a8f52619-5cfe-4743-aab3-4c69eaef6d6b"], "parent_id": "b4e41ac1-32be-4c4a-8045-99042574dff5", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agents in Practice:  Harnessing AI for Good"], ["subsection", "Interactive Engagement between Human and Agent"], ["subsubsection", "Instructor-Executor Paradigm"]], "content": "\\label{Instructor-Executor Paradigm}\nThe simplest approach involves human guidance throughout the process: humans provide clear and specific instructions directly, while the agents' role is to understand natural language commands from humans and translate them into corresponding actions . In \\S \\ref{sec:General Ability of Single Agent}, we have presented the scenario where agents solve single-step problems or receive high-level instructions from humans. Considering the interactive nature of language, in this section, we assume that the dialogue between humans and agents is also interactive. Thanks to LLMs, the agents are able to interact with humans in a conversational manner: the agent responds to each human instruction, refining its action through alternating iterations to ultimately meet human requirements . While this approach does achieve the goal of human-agent interaction, it places significant demands on humans. It requires a substantial amount of human effort and, in certain tasks, might even necessitate a high level of expertise. To alleviate this issue, the agent can be empowered to autonomously accomplish tasks, while humans only need to provide feedback in certain circumstances. Here, we roughly categorize feedback into two types: quantitative feedback and qualitative feedback.", "cites": [7657, 3062], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the instructor-executor paradigm in human-agent interaction, referencing two relevant papers to support its discussion. It synthesizes the concept of LLM-powered agents following human instructions and introduces a basic categorization of feedback types, but does not deeply integrate or contrast the cited works. Critical analysis is limited to noting the demand on human effort, without detailed evaluation of the cited methods or their limitations."}}
{"id": "7816f836-7e25-42e8-ac5a-03b33b95622b", "title": "Quantitative feedback.", "level": "paragraph", "subsections": [], "parent_id": "85dde1b4-6533-48c5-9e44-80fbf202d12f", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agents in Practice:  Harnessing AI for Good"], ["subsection", "Interactive Engagement between Human and Agent"], ["subsubsection", "Instructor-Executor Paradigm"], ["paragraph", "Quantitative feedback."]], "content": "The forms of quantitative feedback mainly include absolute evaluations like binary scores and ratings, as well as relative scores. Binary feedback refers to the positive and negative evaluations provided by humans, which agents utilize to enhance their self-optimization . Comprising only two categories, this type of user feedback is often easy to collect, but sometimes it may oversimplify user intent by neglecting potential intermediate scenarios. To showcase these intermediate scenarios, researchers attempt to expand from binary feedback to rating feedback, which involves categorizing into more fine-grained levels. However, the results of Kreutzer et al.  suggest that there could be significant discrepancies between user and expert annotations for such multi-level artificial ratings, indicating that this labeling method might be inefficient or less reliable. Furthermore, agents can learn human preference from comparative scores like multiple choice .", "cites": [3063, 6915, 7079, 7680, 3064, 3065], "cite_extract_rate": 0.75, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section integrates concepts from multiple papers on user feedback mechanisms, connecting binary, rating, and comparative feedback approaches. It provides a basic synthesis by discussing how these feedback types are used to enhance agent performance, but the analysis remains limited, as it does not deeply compare or critique the methods. The abstraction is moderate, as it identifies the general role of feedback in learning but stops short of proposing overarching frameworks or principles."}}
{"id": "a8f52619-5cfe-4743-aab3-4c69eaef6d6b", "title": "Qualitative feedback.", "level": "paragraph", "subsections": [], "parent_id": "85dde1b4-6533-48c5-9e44-80fbf202d12f", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agents in Practice:  Harnessing AI for Good"], ["subsection", "Interactive Engagement between Human and Agent"], ["subsubsection", "Instructor-Executor Paradigm"], ["paragraph", "Qualitative feedback."]], "content": "Text feedback is usually offered in natural language, particularly for responses that may need improvement. The format of this feedback is quite flexible. Humans provide advice on how to modify outputs generated by agents, and the agents then incorporate these suggestions to refine their subsequent outputs . For agents without multimodal perception capabilities, humans can also act as critics, offering visual critiques , for instance. Additionally, agents can utilize a memory module to store feedback for future reuse . In , humans give feedback on the initial output generated by agents, prompting the agents to formulate various improvement proposals. The agents then discern and adopt the most suitable proposal, harmonizing with the human feedback. While this approach can better convey human intention compared to quantitative feedback, it might be more challenging for the agents to comprehend. Xu et al.  compare various types of feedback and observe that combining multiple types of feedback can yield better results. Re-training models based on feedback from multiple rounds of interaction (i.e., continual learning) can further enhance effectiveness. Of course, the collaborative nature of human-agent interaction also allows humans to directly improve the content generated by agents. This could involve modifying intermediate links  or adjusting the conversation content . In some studies, agents can autonomously judge whether the conversation is proceeding smoothly and seek feedback when errors are generated . Humans can also choose to participate in feedback at any time, guiding the agent's learning in the right direction .\nCurrently, in addition to tasks like writing  and semantic parsing , the model of using agents as human assistants also holds tremendous potential in the field of education. For instance, Kalvakurth et al.  propose the robot Dona, which supports multimodal interactions to assist students with registration. Gvirsman et al.  focus on early childhood education, achieving multifaceted interactions between young children, parents, and agents. Agents can also aid in human understanding and utilization of mathematics . \nIn the field of medicine, some medical agents have been proposed, showing enormous potential in terms of diagnosis assistance, consultations, and more . \nEspecially in mental health, research has shown that agents can lead to increased accessibility due to benefits such as reduced cost, time efficiency, and anonymity compared to face-to-face treatment . \nLeveraging such advantages, agents have found widespread applications. Ali et al.  design LISSA for online communication with adolescents on the autism spectrum, analyzing users' speech and facial expressions in real-time to engage them in multi-topic conversations and provide instant feedback regarding non-verbal cues. Hsu et al.  build contextualized language generation approaches to provide tailored assistance for users who seek support on diverse topics ranging from relationship stress to anxiety. \nFurthermore, in other industries including business, a good agent possesses the capability to provide automated services or assist humans in completing tasks, thereby effectively reducing labor costs . \nAmidst the pursuit of AGI, efforts are directed towards enhancing the multifaceted capabilities of general agents, creating agents that can function as universal assistants in real-life scenarios .", "cites": [3063, 8619, 7459, 7681, 7682, 7657, 3069, 7684, 3067, 7683, 3068, 3066], "cite_extract_rate": 0.5, "origin_cites_number": 24, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple cited works to highlight the importance and mechanisms of qualitative feedback in human-agent interaction, particularly in refining agent behavior. It critically discusses strengths (e.g., conveying human intention better than quantitative feedback) and limitations (e.g., difficulty for agents to comprehend). The narrative abstracts beyond individual studies to identify broader applications of feedback-driven agent systems in education, medicine, and business, suggesting a general trend towards more interactive and adaptive AI agents."}}
{"id": "1c0f30ec-deff-4874-84a2-1ec72c9a4e1b", "title": "Empathetic communicator.", "level": "paragraph", "subsections": [], "parent_id": "55f61a61-7ea0-42c1-bbd8-8e92c55a1d25", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agents in Practice:  Harnessing AI for Good"], ["subsection", "Interactive Engagement between Human and Agent"], ["subsubsection", "Equal Partnership Paradigm"], ["paragraph", "Empathetic communicator."]], "content": "With the rapid development of AI, conversational agents have garnered extensive attention in research fields in various forms, such as personalized custom roles and virtual chatbots . It has found practical applications in everyday life, business, education, healthcare, and more . However, in the eyes of the public, agents are perceived as emotionless machines, and can never replace humans. Although it is intuitive that agents themselves do not possess emotions, can we enable them to exhibit emotions and thereby bridge the gap between agents and humans? Therefore, a plethora of research endeavors have embarked on delving into the empathetic capacities of agents. This endeavor seeks to infuse a human touch into these agents, enabling them to detect sentiments and emotions from human expressions, ultimately crafting emotionally resonant dialogues . Apart from generating emotionally charged language, agents can dynamically adjust their emotional states and display them through facial expressions and voice . These studies, viewing agents as empathetic communicators, not only enhance user satisfaction but also make significant progress in fields like healthcare  and business marketing . Unlike simple rule-based conversation agents, agents with empathetic capacities can tailor their interactions to meet users' emotional needs .", "cites": [3072, 7685, 7639, 2061, 3070, 3071, 8619, 3073], "cite_extract_rate": 0.4444444444444444, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes several works on empathetic agents, highlighting common themes such as emotion detection, response generation, and emotional mimicry. It abstracts the concept of empathetic communication into broader principles like tailoring interactions to emotional needs and bridging human-machine gaps. However, it lacks deeper critical evaluation of the limitations or comparative strengths and weaknesses of the cited methods."}}
{"id": "48c5f90c-568c-462f-aaaf-3c68f9b94223", "title": "Human-level participant.", "level": "paragraph", "subsections": [], "parent_id": "55f61a61-7ea0-42c1-bbd8-8e92c55a1d25", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agents in Practice:  Harnessing AI for Good"], ["subsection", "Interactive Engagement between Human and Agent"], ["subsubsection", "Equal Partnership Paradigm"], ["paragraph", "Human-level participant."]], "content": "Furthermore, we hope that agents can be involved in the normal lives of humans, cooperating with humans to complete tasks from a human-level perspective. In the field of games, agents have already reached a high level. As early as the 1990s, IBM introduced the AI Deep Blue , which defeated the reigning world champion in chess at that time. However, in pure competitive environments such as chess , Go , and poker , the value of communication was not emphasized . In many gaming tasks, players need to collaborate with each other, devising unified cooperative strategies through effective negotiation . In these scenarios, agents need to first understand the beliefs, goals, and intentions of others, formulate joint action plans for their objectives, and also provide relevant suggestions to facilitate the acceptance of cooperative actions by other agents or humans. In comparison to pure agent cooperation, we desire human involvement for two main reasons: first, to ensure interpretability, as interactions between pure agents could generate incomprehensible language ; second, to ensure controllability, as the pursuit of agents with complete ``free will'' might lead to unforeseen negative consequences, carrying the potential for disruption. Apart from gaming scenarios, agents also demonstrate human-level capabilities in other scenarios involving human interaction, showcasing skills in strategy formulation, negotiation, and more. Agents can collaborate with one or multiple humans, determining the shared knowledge among the cooperative partners, identifying which information is relevant to decision-making, posing questions, and engaging in reasoning to complete tasks such as allocation, planning, and scheduling . Furthermore, agents possess persuasive abilities , dynamically influencing human viewpoints in various interactive scenarios .\nThe goal of the field of human-agent interaction is to learn and understand humans, develop technology and tools based on human needs, and ultimately enable comfortable, efficient, and secure interactions between humans and agents. Currently, significant breakthroughs have been achieved in terms of usability in this field. In the future, human-agent interaction will continue to focus on enhancing user experience, enabling agents to better assist humans in accomplishing more complex tasks in various domains. The ultimate aim is not to make agents more powerful but to better equip humans with agents. Considering practical applications in daily life, isolated interactions between humans and agents are not realistic. Robots will become colleagues, assistants, and even companions. Therefore, future agents will be integrated into a social network , embodying a certain level of social value.", "cites": [3078, 3076, 3077, 3075, 3074, 9097, 1882], "cite_extract_rate": 0.6363636363636364, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple papers to highlight how AI agents, particularly LLM-based ones, can engage in cooperative and persuasive interactions with humans, beyond purely competitive settings. It identifies broader themes like interpretability and controllability as critical in human-agent partnerships. While it provides a coherent narrative and abstracts key principles (e.g., the importance of understanding human goals), its critical analysis is moderate, focusing more on implications than on deep evaluation of the cited works' limitations."}}
{"id": "1706063b-1cfe-4681-b3b0-53f3e9009494", "title": "Agent Society: From Individuality to Sociality", "level": "section", "subsections": ["5db0cba3-d547-4c33-b404-a3813a75501e", "1095772c-f51a-4172-aa94-97f80ab99811", "b5ca6e5c-235d-4c78-bdd2-e7ed5efd11cd"], "parent_id": "c660e090-2b21-4ae5-89c4-4f6ef771d9fc", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agent Society: From Individuality to Sociality"]], "content": "\\label{sec:Agent Society}\n\\input{figures/sec5_mindmap}\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1\\textwidth]{figures/sec5_agent_society.pdf}\n    \\caption{Overview of Simulated Agent Society.\n    The whole framework is divided into two parts: the \\textbf{Agent} and the \\textbf{Environment}.\n    We can observe in this figure that: (1) \\textbf{Left:} At the individual level, an agent exhibits internalizing behaviors like planning, reasoning, and reflection. It also displays intrinsic personality traits involving cognition, emotion, and character. (2) \\textbf{Mid:} An agent and other agents can form groups and exhibit group behaviors, such as cooperation. (3) \\textbf{Right:} The environment, whether virtual or physical, contains human actors and all available resources. For a single agent, other agents are also part of the environment. (4) The agents have the ability to interact with the environment via perception and action.\n    }\n    \\label{fig: agent_society}\n\\end{figure} \nFor an extended period, sociologists have frequently conducted social experiments to observe specific social phenomena within controlled environments. \nNotable examples include the Hawthorne Experiment\\footnote{\\href{https://www.bl.uk/people/elton-mayo}{https://www.bl.uk/people/elton-mayo}} and the Stanford Prison Experiment\\footnote{\\href{https://www.prisonexp.org/conclusion/}{https://www.prisonexp.org/conclusion/}}. \nSubsequently, researchers began employing animals in social simulations, exemplified by the Mouse Utopia Experiment\\footnote{\\href{https://sproutsschools.com/behavioral-sink-the-mouse-utopia-experiments/}{https://sproutsschools.com/behavioral-sink-the-mouse-utopia-experiments/}}. \nHowever, these experiments invariably utilized living organisms as participants, made it difficult to carry out various interventions, lack flexibility, and inefficient in terms of time.\nThus, researchers and practitioners envision an interactive artificial society wherein human behavior can be performed through trustworthy agents .\nFrom sandbox games such as The Sims to the concept of Metaverse, we can see how ``simulated society'' is defined in people's minds: environment and the individuals interacting in it. \nBehind each individual can be a piece of program, a real human, or a LLM-based agent as described in the previous sections .\nThen, the interaction between individuals also contributes to the birth of sociality.\nIn this section, to unify existing efforts and promote a comprehensive understanding of the agent society, we first analyze the behaviors and personalities of LLM-based agents, shedding light on their journey from individuality to sociability (\\S \\ \\ref{sec:Behavior and Personality}). \nSubsequently, we introduce a general categorization of the diverse environments for agents to perform their behaviors and engage in interactions (\\S \\ \\ref{sec:Environment for Agent Society}). \nFinally, we will talk about how the agent society works, what insights people can get from it, and the risks we need to be aware of (\\S \\ \\ref{sec:Society Simulation}). The main explorations are listed in Figure \\ref{fig:sec5_mindmap}.", "cites": [2945], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of agent society, drawing from the cited paper on generative agents to frame how LLM-based agents can simulate human behavior and contribute to sociality. It synthesizes the idea of simulated environments and agent interactions, but the critical analysis is limited to pointing out general shortcomings of biological experiments. The abstraction is moderate, as it identifies a general framework for agent society but does not deeply generalize principles or propose new theoretical insights."}}
{"id": "7d941ead-b143-4d65-943d-a09e87f000be", "title": "Social Behavior", "level": "subsubsection", "subsections": ["5aeaf4d5-6e95-4ffc-98a9-a6b399624953", "3ec55786-6473-4141-b8db-04ac47e6416d"], "parent_id": "5db0cba3-d547-4c33-b404-a3813a75501e", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agent Society: From Individuality to Sociality"], ["subsection", "Behavior and Personality of LLM-based Agents"], ["subsubsection", "Social Behavior"]], "content": "\\label{sec:social behavior}\nAs Troitzsch et al.  stated, the agent society represents a complex system comprising individual and group social activities. Recently, LLM-based agents have exhibited spontaneous social behaviors in an environment where both cooperation and competition coexist . The emergent behaviors intertwine to shape the social interactions .", "cites": [3079], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section provides a minimal description of the concept of social behavior in LLM-based agents and cites one paper without integrating or connecting it meaningfully to broader ideas. There is no critical analysis or comparison between approaches, and no abstraction or generalization is made to extract overarching patterns or principles."}}
{"id": "5aeaf4d5-6e95-4ffc-98a9-a6b399624953", "title": "Foundational individual behaviors.", "level": "paragraph", "subsections": [], "parent_id": "7d941ead-b143-4d65-943d-a09e87f000be", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agent Society: From Individuality to Sociality"], ["subsection", "Behavior and Personality of LLM-based Agents"], ["subsubsection", "Social Behavior"], ["paragraph", "Foundational individual behaviors."]], "content": "Individual behaviors arise through the interplay between internal cognitive processes and external environmental factors. These behaviors form the basis of how agents operate and develop as individuals within society. They can be classified into three core dimensions:\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Input behaviors} refers to the absorption of information from the surroundings. This includes perceiving sensory stimuli  and storing them as memories . These behaviors lay the groundwork for how an individual understands the external world.\n    \\item \\textbf{Internalizing behaviors} involve inward cognitive processing within an individual. This category encompasses activities such as planning , reasoning , reflection , and knowledge precipitation . These introspective processes are essential for maturity and self-improvement.\n    \\item \\textbf{Output behaviors} constitute outward actions and expressions. The actions can range from object manipulation  to structure construction . By performing these actions, agents change the states of the surroundings. In addition, agents can express their opinions and broadcast information to interact with others . By doing so, agents exchange their thoughts and beliefs with others, influencing the information flow within the environment.\n\\end{itemize}", "cites": [2967, 2954, 1578, 7657, 2963], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section analytically categorizes individual behaviors of LLM-based agents into three dimensions (input, internalizing, and output), which integrates concepts from multiple cited papers, particularly in reasoning (Paper 3), planning (Paper 1), and embodied interaction (Papers 4 and 5). However, it does not explicitly reference or connect the cited works to the framework it presents, limiting synthesis. Critical analysis is minimal, as the section does not assess limitations or trade-offs of the approaches. The abstraction is moderate, as it identifies a general behavior taxonomy but does not elevate it to a meta-level discussion."}}
{"id": "3ec55786-6473-4141-b8db-04ac47e6416d", "title": "Dynamic group behaviors.", "level": "paragraph", "subsections": [], "parent_id": "7d941ead-b143-4d65-943d-a09e87f000be", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agent Society: From Individuality to Sociality"], ["subsection", "Behavior and Personality of LLM-based Agents"], ["subsubsection", "Social Behavior"], ["paragraph", "Dynamic group behaviors."]], "content": "A group is essentially a gathering of two or more individuals participating in shared activities within a defined social context . \nThe attributes of a group are never static; instead, they evolve due to member interactions and environmental influences. \nThis flexibility gives rise to numerous group behaviors, each with a distinctive impact on the larger societal group.\nThe categories of group behaviors include:\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Positive group behaviors} are actions that foster unity, collaboration, and collective well-being . \n    A prime example is cooperative teamwork, which is achieved through brainstorming discussions , effective conversations , and project management . Agents share insights, resources, and expertise. \n    This encourages harmonious teamwork and enables the agents to leverage their unique skills to accomplish shared goals. \n    Altruistic contributions are also noteworthy. \n    Some LLM-based agents serve as volunteers and willingly offer support to assist fellow group members, promoting cooperation and mutual aid .\n    \\item \\textbf{Neutral group behaviors.} In human society, strong personal values vary widely and tend toward individualism and competitiveness. \n    In contrast, LLMs which are designed with an emphasis on being ``helpful, honest, and harmless''  often demonstrate a tendency towards neutrality . This alignment with neutral values leads to conformity behaviors including mimicry, spectating, and reluctance to oppose majorities.\n    \\item \\textbf{Negative group behaviors} can undermine the effectiveness and coherence of an agent group.\n    Conflict and disagreement arising from heated debates or disputes among agents may lead to internal tensions. \n    Furthermore, recent studies have revealed that agents may exhibit confrontational actions  and even resort to destructive behaviors, such as destroying other agents or the environment in pursuit of efficiency gains .\n\\end{itemize}", "cites": [2945, 2990, 7643, 3080, 3079, 3057], "cite_extract_rate": 0.5, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.5}, "insight_level": "medium", "analysis": "The section provides a basic categorization of group behaviors (positive, neutral, negative) and draws loosely on cited papers to support examples, but it lacks deep integration or synthesis of ideas across sources. It is primarily descriptive, summarizing observed behaviors without critically evaluating or contrasting the methods or findings of the cited works. Some generalization is attempted, such as linking LLM-based agents to human-like group behavior, but the analysis remains surface-level without significant abstraction or novel insights."}}
{"id": "f0dc371f-b9a6-4f8d-b22a-9a10fffe3f02", "title": "Cognitive abilities.", "level": "paragraph", "subsections": [], "parent_id": "b23d1293-926c-45cf-8bbc-c5e3c39d65b7", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agent Society: From Individuality to Sociality"], ["subsection", "Behavior and Personality of LLM-based Agents"], ["subsubsection", "Personality"], ["paragraph", "Cognitive abilities."]], "content": "Cognitive abilities generally refer to the mental processes of gaining knowledge and comprehension, including thinking, judging, and problem-solving. \nRecent studies have started leveraging cognitive psychology methods to investigate emerging sociological personalities of LLM-based agents through various lenses .\nA series of classic experiments from the psychology of judgment and decision-making have been applied to test agent systems . Specifically, LLMs have been examined using the Cognitive Reflection Test (CRT) to underscore their capacity for deliberate thinking beyond mere intuition .\nThese studies indicate that LLM-based agents exhibit a level of intelligence that mirrors human cognition in certain respects.", "cites": [3082, 3081, 7686], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from three papers by linking the idea that cognitive psychology methods are being used to evaluate LLMs, particularly their decision-making and reasoning. It provides a basic analytical perspective by noting differences in model capabilities (e.g., GPT-3.5 struggles compared to GPT-4). However, the analysis remains somewhat superficial and does not deeply compare or critique the studies, nor does it offer a novel abstraction or framework beyond the cited works."}}
{"id": "947e3d44-f99b-42c6-818f-7ae34d858f16", "title": "Emotional intelligence.", "level": "paragraph", "subsections": [], "parent_id": "b23d1293-926c-45cf-8bbc-c5e3c39d65b7", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agent Society: From Individuality to Sociality"], ["subsection", "Behavior and Personality of LLM-based Agents"], ["subsubsection", "Personality"], ["paragraph", "Emotional intelligence."]], "content": "Emotions, distinct from cognitive abilities, involve subjective feelings and mood states such as joy, sadness, fear, and anger.\nWith the increasing potency of LLMs, LLM-based agents are now demonstrating not only sophisticated reasoning and cognitive tasks but also a nuanced understanding of emotions .\nRecent research has explored the emotional intelligence (EI) of LLMs, including emotion recognition, interpretation, and understanding. Wang et al. found that LLMs align with human emotions and values when evaluated on EI benchmarks .\nIn addition, studies have shown that LLMs can accurately identify user emotions and even exhibit empathy .\nMore advanced agents are also capable of emotion regulation, actively adjusting their emotional responses to provide affective empathy  and mental wellness support . It contributes to the development of empathetic artificial intelligence (EAI).\nThese advances highlight the growing potential of LLMs to exhibit emotional intelligence, a crucial facet of achieving AGI. Bates et al.  explored the role of emotion modeling in creating more believable agents. \nBy developing socio-emotional skills and integrating them into agent architectures, LLM-based agents may be able to engage in more naturalistic interactions.", "cites": [7687, 7638, 7639, 3083], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes several papers to build a narrative around the emotional intelligence of LLM-based agents, integrating findings on emotion recognition, regulation, and empathy. While it offers a coherent analytical thread, it lacks deeper critical evaluation of the limitations or methodological issues in the cited works. The abstraction is moderate, as it connects these capabilities to broader goals like AGI and EAI but does not yet formulate a meta-level theoretical framework."}}
{"id": "f46175f8-6273-4ff5-a843-6f02ac7fe315", "title": "Character portrayal.", "level": "paragraph", "subsections": [], "parent_id": "b23d1293-926c-45cf-8bbc-c5e3c39d65b7", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agent Society: From Individuality to Sociality"], ["subsection", "Behavior and Personality of LLM-based Agents"], ["subsubsection", "Personality"], ["paragraph", "Character portrayal."]], "content": "While cognition involves mental abilities and emotion relates to subjective experiences, the narrower concept of personality typically pertains to distinctive character patterns.\nTo understand and analyze a character in LLMs, researchers have utilized several well-established frameworks like the Big Five personality trait measure  and the Myers–Briggs Type Indicator (MBTI) . These frameworks provide valuable insights into the emerging character traits exhibited by LLM-based agents. In addition,  investigations of potentially harmful dark personality traits underscore the complexity and multifaceted nature of character portrayal in these agents .\nRecent work has also explored customizable character portrayal in LLM-based agents . \nBy optimizing LLMs through careful techniques, users can align with desired profiles and shape diverse and relatable agents.\nOne effective approach is prompt engineering, which involves the concise summaries that encapsulate desired character traits, interests, or other attributes . \nThese prompts serve as cues for LLM-based agents, directing their responses and behaviors to align with the outlined character portrayal. \nFurthermore, personality-enriched datasets can also be used to train and fine-tune LLM-based agents . Through exposure to these datasets, LLM-based agents gradually internalize and exhibit distinct personality traits.", "cites": [2945, 3085, 650, 7122, 3084], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes information from multiple cited papers by integrating themes of personality frameworks, prompt engineering, and dataset-driven training. It provides a coherent narrative on how LLM-based agents can be shaped to exhibit distinct personalities. While it does offer some critical perspective by highlighting the complexity and potential harm of dark traits, it stops short of deeper evaluation or limitations. The abstraction level is moderate, as it identifies broader approaches like conditioning on profile data and prompt engineering but does not offer a meta-level framework."}}
{"id": "a4c8b0a1-8929-4c22-819e-4cd1f2a3044b", "title": "Text-based Environment", "level": "subsubsection", "subsections": [], "parent_id": "1095772c-f51a-4172-aa94-97f80ab99811", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agent Society: From Individuality to Sociality"], ["subsection", "Environment for Agent Society"], ["subsubsection", "Text-based Environment"]], "content": "\\label{sec:text-based environment}\nSince LLMs primarily rely on language as their input and output format, the text-based environment serves as the most natural platform for agents to operate in. It is shaped by natural language descriptions without direct involvement of other modalities. Agents exist in the text world and rely on textual resources to perceive, reason, and take actions.\nIn text-based environments, entities and resources can be presented in two main textual forms, including natural and structured. Natural text uses descriptive language to convey information, like character dialogue or scene setting. For instance, consider a simple scenario described textually: ``You are standing in an open field west of a white house, with a boarded front door. There is a small mailbox here'' . Here, object attributes and locations are conveyed purely through plain text. On the other hand, structured text follows standardized formats, such as technical documentation and hypertext. Technical documentation uses templates to provide operational details and domain knowledge about tool use. Hypertext condenses complex information from sources like web pages  or diagrams into a structured format. Structured text transforms complex details into accessible references for agents.\nThe text-based environment provides a flexible framework for creating different text worlds for various goals. The textual medium enables environments to be easily adapted for tasks like interactive dialog and text-based games. In interactive communication processes like CAMEL , the text is the primary medium for describing tasks, introducing roles, and facilitating problem-solving. In text-based games, all environment elements, such as locations, objects, characters, and actions, are exclusively portrayed through textual descriptions. Agents utilize text commands to execute manipulations like moving or tool use . Additionally, agents can convey emotions and feelings through text, further enriching their capacity for naturalistic communication .", "cites": [2064, 7688, 7674, 3047, 3086, 3046, 2501, 3049], "cite_extract_rate": 0.8, "origin_cites_number": 10, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key ideas from multiple papers on text-based environments, clearly distinguishing between natural and structured text forms and linking them to agent capabilities. It provides an analytical overview of how these environments support diverse agent behaviors. However, it lacks deeper critical evaluation of the cited works, such as limitations or trade-offs, and the abstraction remains focused on categorizing forms rather than revealing broader theoretical principles."}}
{"id": "861c9922-81ee-4275-85d7-de513a399398", "title": "Virtual Sandbox Environment", "level": "subsubsection", "subsections": [], "parent_id": "1095772c-f51a-4172-aa94-97f80ab99811", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agent Society: From Individuality to Sociality"], ["subsection", "Environment for Agent Society"], ["subsubsection", "Virtual Sandbox Environment"]], "content": "\\label{sec:virtual sandbox environment}\nThe virtual sandbox environment provides a visualized and extensible platform for agent society, bridging the gap between simulation and reality. The key features of sandbox environments are:\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Visualization.} Unlike the text-based environment, the virtual sandbox displays a panoramic view of the simulated setting. This visual representation can range from a simple 2D graphical interface to a fully immersive 3D modeling, depending on the complexity of the simulated society. Multiple elements collectively transform abstract simulations into visible landscapes. For example, in the overhead perspective of Generative Agents , a detailed map provides a comprehensive overview of the environment. Agent avatars represent each agent's positions, enabling real-time tracking of movement and interactions. Furthermore, expressive emojis symbolize actions and states in an intuitive manner. \n    \\item \\textbf{Extensibility.} The environment demonstrates a remarkable degree of extensibility, facilitating the construction and deployment of diverse scenarios. At a basic level, agents can manipulate the physical elements within the environment, including the overall design and layout of architecture. For instance, platforms like AgentSims  and Generative Agents  construct artificial towns with buildings, equipment, and residents in grid-based worlds. Another example is Minecraft, which provides a blocky and three-dimensional world with infinite terrain for open-ended construction . Beyond physical elements, agent relationships, interactions, rules, and social norms can be defined. A typical design of the sandbox  employs latent sandbox rules as incentives to guide emergent behaviors, aligning them more closely with human preferences. The extensibility supports iterative prototyping of diverse agent societies.\n\\end{itemize}", "cites": [2945, 2993, 3036, 7657], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section effectively synthesizes key features of virtual sandbox environments by drawing examples from multiple cited papers, such as Generative Agents, AgentSims, and Minecraft-based platforms like MineDojo and Voyager. While it offers a coherent discussion of visualization and extensibility as central traits, it lacks deeper critical analysis of the papers' limitations or trade-offs. Some level of abstraction is present, such as identifying 'latent sandbox rules' as incentives, but it remains grounded in specific examples without fully articulating broader theoretical implications."}}
{"id": "2c70c012-cc48-4ef4-9fcb-fca9997abf01", "title": "Physical Environment", "level": "subsubsection", "subsections": [], "parent_id": "1095772c-f51a-4172-aa94-97f80ab99811", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agent Society: From Individuality to Sociality"], ["subsection", "Environment for Agent Society"], ["subsubsection", "Physical Environment"]], "content": "\\label{sec:physical environment}\nAs previously discussed, the text-based environment has limited expressiveness for modeling dynamic environments. While the virtual sandbox environment provides modularized simulations, it lacks authentic embodied experiences. In contrast, the physical environment refers to the tangible and real-world surroundings which consist of actual physical objects and spaces. For instance, within a household physical environment , tangible surfaces and spaces can be occupied by real-world objects such as plates. This physical reality is significantly more complex, posing additional challenges for LLM-based agents:\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Sensory perception and processing.} The physical environment introduces a rich tapestry of sensory inputs with real-world objects. It incorporates visual , auditory   and spatial senses. While this diversity enhances interactivity and sensory immersion, it also introduces the complexity of simultaneous perception. Agents must process sensory inputs to interact effectively with their surroundings.\n    \\item \\textbf{Motion control.} Unlike virtual environments, physical spaces impose realistic constraints on actions through embodiment. Action sequences generated by LLM-based agents should be adaptable to the environment. It means that the physical environment necessitates executable and grounded motion control . For example, imagine an agent operating a robotic arm in a factory. Grasping objects with different textures requires precision tuning and controlled force, which prevents damage to items. Moreover, the agent must navigate the physical workspace and make real-time adjustments, avoiding obstacles and optimizing the trajectory of the arm.\n\\end{itemize}\nIn summary, to effectively interact within tangible spaces, agents must undergo hardware-specific and scenario-specific training to develop adaptive abilities that can transfer from virtual to physical environments. We will discuss more in the following section (\\S \\ \\ref{sec:Open Problems}).", "cites": [7671, 5587, 3037, 2963, 3035], "cite_extract_rate": 0.8333333333333334, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the cited papers by integrating themes of multimodal perception, motion control, and real-world grounding into a coherent discussion of the physical environment's challenges for LLM-based agents. It abstracts these ideas to highlight broader issues such as the need for hardware-specific training and adaptability. However, it lacks deeper critical analysis, such as evaluating the limitations or trade-offs of the different approaches presented."}}
{"id": "b5ca6e5c-235d-4c78-bdd2-e7ed5efd11cd", "title": "Society Simulation with LLM-based Agents", "level": "subsection", "subsections": ["4918ab06-8a0e-4e85-bde2-807187d734ac", "db20f62a-4757-4a74-994f-7daf095142da", "01073346-d46a-4b15-8fb3-ec3244848f26"], "parent_id": "1706063b-1cfe-4681-b3b0-53f3e9009494", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agent Society: From Individuality to Sociality"], ["subsection", "Society Simulation with LLM-based Agents"]], "content": "\\label{sec:Society Simulation}\nThe concept of ``Simulated Society'' in this section serves as a dynamic system where agents engage in intricate interactions within a well-defined environment. \nRecent research on simulated societies has followed two primary lines, namely, exploring the boundaries of the collective intelligence capabilities of LLM-based agents  and using them to accelerate discoveries in the social sciences .\nIn addition, there are also a number of noteworthy studies, e.g., using simulated societies to collect synthetic datasets , helping people to simulate rare yet difficult interpersonal situations .\nWith the foundation of the previous sections (\\S \\ \\ref{sec:Behavior and Personality}, \\ref{sec:Environment for Agent Society}), here we will introduce the key properties and mechanism of agent society (\\S \\ \\ref{sec:key properties and mechanism}), what we can learn from emergent social phenomena (\\S \\ \\ref{sec:insights and inspirations}), and finally the potential ethical and social risks in it (\\S \\ \\ref{sec:potential ethical and social risks}).", "cites": [2945, 7643, 2969, 7123], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 12, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section briefly introduces the concept of simulated societies and lists several research directions and examples, including the use of LLM-based agents to explore collective intelligence and assist in social science research. However, it does not synthesize or connect these ideas across the cited papers in a meaningful way, nor does it offer a critical evaluation or broader abstraction of trends or principles."}}
{"id": "4918ab06-8a0e-4e85-bde2-807187d734ac", "title": "Key Properties and Mechanism of Agent Society", "level": "subsubsection", "subsections": [], "parent_id": "b5ca6e5c-235d-4c78-bdd2-e7ed5efd11cd", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agent Society: From Individuality to Sociality"], ["subsection", "Society Simulation with LLM-based Agents"], ["subsubsection", "Key Properties and Mechanism of Agent Society"]], "content": "\\label{sec:key properties and mechanism}\nSocial simulation can be categorized into macro-level simulation and micro-level simulation . \nIn the macro-level simulation, also known as system-based simulation, researchers model the overall state of the system of the simulated society .\nWhile micro-level simulation, also known as agent-based simulation or Multi-Agent Systems (MAS), indirectly simulates society by modeling individuals .\nWith the development of LLM-based agents, micro-level simulation has gained prominence recently .\nIn this article, we characterize that the ``Agent Society'' refers to an \\textbf{open, persistent, situated, and organized} framework  where LLM-based agents interact with each other in a defined environment. \nEach of these attributes plays a pivotal role in shaping the harmonious appearance of the simulated society.\nIn the following paragraphs, we analyze how the simulated society operates through discussing these properties:\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Open.} One of the defining features of simulated societies lies in their openness, both in terms of their constituent agents and their environmental components. \n    Agents, the primary actors within such societies, have the flexibility to enter or leave the environment without disrupting its operational integrity . \n    Furthermore, this feature extends to the environment itself, which can be expanded by adding or removing entities in the virtual or physical world, along with adaptable resources like tool APIs. \n    Additionally, humans can also participate in societies by assuming the role of an agent or serving as the ``inner voice'' guiding these agents . \n    This inherent openness adds another level of complexity to the simulation, blurring the lines between simulation and reality.\n    \\item  \\textbf{Persistent.} \n    We expect persistence and sustainability from the simulated society.\n    While individual agents within the society exercise autonomy in their actions over each time step , the overall organizational structure persists through time, to a degree detached from the transient behaviors of individual agents. \n    This persistence creates an environment where agents’ decisions and behaviors accumulate, leading to a coherent societal trajectory that develops through time. \n    The system operates independently, contributing to society's stability while accommodating the dynamic nature of its participants.\n    \\item  \\textbf{Situated.} The situated nature of the society emphasizes its existence and operation within a distinct environment. \n    This environment is artificially or automatically constructed in advance, and agents execute their behaviors and interactions effectively within it.\n    A noteworthy aspect of this attribute is that agents possess an awareness of their spatial context, understanding their location within the environment and the objects within their field of view . \n    This awareness contributes to their ability to interact proactively and contextually.\n    \\item \\textbf{Organized.} The simulated society operates within a meticulously organized framework, mirroring the systematic structure present in the real world. \n    Just as the physical world adheres to physics principles, the simulated society operates within predefined rules and limitations. \n    In the simulated world, agents interact with the environment in a limited action space, while objects in the environment transform in a limited state space.\n    All of these rules determine how agents operate, facilitating the communication connectivity and information transmission pathways, among other aspects in simulation .\n    This organizational framework ensures that operations are coherent and comprehensible, ultimately leading to an ever-evolving yet enduring simulation that mirrors the intricacies of real-world systems.\n\\end{itemize}", "cites": [2945, 8620, 7657, 2993, 2970], "cite_extract_rate": 0.45454545454545453, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes concepts from multiple cited papers to define a novel framework of an 'Agent Society' with key properties (open, persistent, situated, organized). It abstracts these properties into a coherent structure that reflects broader design principles of LLM-based agent systems. However, the critical analysis is limited, as it primarily describes attributes without evaluating limitations or contrasting approaches across the cited works."}}
{"id": "db20f62a-4757-4a74-994f-7daf095142da", "title": "Insights from Agent Society", "level": "subsubsection", "subsections": ["cd3be3ef-5c01-4078-be37-aad971d67183", "54e37e47-4935-4ddf-a21b-26b919594cfb", "b83e1e42-dc7d-4f8a-9180-45ce0a366f90", "92eac2e9-376b-4d0c-82a4-25c8cb8ec4ca"], "parent_id": "b5ca6e5c-235d-4c78-bdd2-e7ed5efd11cd", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agent Society: From Individuality to Sociality"], ["subsection", "Society Simulation with LLM-based Agents"], ["subsubsection", "Insights from Agent Society"]], "content": "\\label{sec:insights and inspirations}\nFollowing the exploration of how simulated society works, this section delves into the emergent social phenomena in agent society.\nIn the realm of social science, the pursuit of generalized representations of individuals, groups, and their intricate dynamics has long been a shared objective .  \nThe emergence of LLM-based agents allows us to take a more microscopic view of simulated society, which leads to more discoveries from the new representation.", "cites": [3087], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.5}, "insight_level": "low", "analysis": "The section briefly connects the use of LLM-based agents to the goals of computational social science, showing a minimal level of synthesis with the cited paper. However, it lacks critical evaluation of the cited work and instead offers a high-level, somewhat vague observation. The abstraction is limited to a general idea of a 'microscopic view' without deeper conceptual generalization or a meta-level analysis."}}
{"id": "cd3be3ef-5c01-4078-be37-aad971d67183", "title": "Organized productive cooperation.", "level": "paragraph", "subsections": [], "parent_id": "db20f62a-4757-4a74-994f-7daf095142da", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agent Society: From Individuality to Sociality"], ["subsection", "Society Simulation with LLM-based Agents"], ["subsubsection", "Insights from Agent Society"], ["paragraph", "Organized productive cooperation."]], "content": "Society simulation offers valuable insights into innovative collaboration patterns, which have the potential to enhance real-world management strategies. \nResearch has demonstrated that within this simulated society, the integration of diverse experts introduces a multifaceted dimension of individual intelligence .\nWhen dealing with complex tasks, such as software development or consulting, the presence of agents with various backgrounds, abilities, and experiences facilitates creative problem-solving .\nFurthermore, diversity functions as a system of checks and balances, effectively preventing and rectifying errors  through interaction, ultimately improving the adaptability to various tasks.\nThrough numerous iterations of interactions and debates among agents, individual errors like hallucination or degeneration of thought (DoT) are corrected by the group .\nEfficient communication also plays a pivotal role in such a large and complex collaborative group.\nFor example, MetaGPT  has artificially formulated communication styles with reference to standardized operating procedures (SOPs), validating the effectiveness of empirical methods.\nPark et al.  observed agents working together to organize a Valentine's Day party through spontaneous communication in a simulated town.", "cites": [2945, 7643, 2957], "cite_extract_rate": 0.42857142857142855, "origin_cites_number": 7, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key ideas from the cited papers to form a coherent narrative on how diversity and communication in LLM-based agent societies enhance problem-solving and adaptability. It abstracts these findings into broader concepts like checks and balances and group error correction. While it does not deeply critique or compare the methods, it does highlight limitations (e.g., LLMs struggling with complex reasoning) and presents analytical insights about the role of collaboration patterns."}}
{"id": "54e37e47-4935-4ddf-a21b-26b919594cfb", "title": "Propagation in social networks.", "level": "paragraph", "subsections": [], "parent_id": "db20f62a-4757-4a74-994f-7daf095142da", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agent Society: From Individuality to Sociality"], ["subsection", "Society Simulation with LLM-based Agents"], ["subsubsection", "Insights from Agent Society"], ["paragraph", "Propagation in social networks."]], "content": "As simulated social systems can model what might happen in the real world, they can be used as a reference for predicting social processes.\nUnlike traditional empirical approaches, which heavily rely on time-series data and holistic modeling , agent-based simulations offer a unique advantage by providing more interpretable and endogenous perspectives for researchers.\nHere we focus on its application to modeling propagation in social networks. \nThe first crucial aspect to be explored is the development of interpersonal relationships in simulated societies.\nFor instance, agents who are not initially connected as friends have the potential to establish connections through intermediaries .\nOnce a network of relationships is established, our attention shifts to the dissemination of information within this social network, along with the underlying attitudes and emotions associated with it.\nS$^3$  proposes a user-demographic inference module for capturing both the number of people aware of a particular message and the collective sentiment prevailing among the crowd.\nThis same approach extends to modeling cultural transmission  and the spread of infectious diseases . \nBy employing LLM-based agents to model individual behaviors, implementing various intervention strategies, and monitoring population changes over time, these simulations empower researchers to gain deeper insights into the intricate processes that underlie various social phenomena of propagation.", "cites": [2945, 3088], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the two cited papers by linking the concept of generative agents to their application in modeling social propagation processes, including information dissemination and epidemic modeling. While it provides a coherent narrative on how LLM-based agents can simulate social dynamics, it lacks deeper critical evaluation of the approaches, such as limitations or trade-offs. The abstraction is moderate, as it identifies the broader use of agent-based simulations for social phenomena, but does not reach a meta-level conceptualization."}}
{"id": "b83e1e42-dc7d-4f8a-9180-45ce0a366f90", "title": "Ethical decision-making and game theory.", "level": "paragraph", "subsections": [], "parent_id": "db20f62a-4757-4a74-994f-7daf095142da", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agent Society: From Individuality to Sociality"], ["subsection", "Society Simulation with LLM-based Agents"], ["subsubsection", "Insights from Agent Society"], ["paragraph", "Ethical decision-making and game theory."]], "content": "Simulated societies offer a dynamic platform for the investigation of intricate decision-making processes, encompassing decisions influenced by ethical and moral principles. \nTaking Werewolf game  and murder mystery games  as examples, researchers explore the capabilities of LLM-based agents when confronted with challenges of deceit, trust, and incomplete information.\nThese complex decision-making scenarios also intersect with game theory , where we frequently encounter moral dilemmas pertaining to individual and collective interests, such as Nash Equilibria. \nThrough the modeling of diverse scenarios, researchers acquire valuable insights into how agents prioritize values like honesty, cooperation, and fairness in their actions.\nIn addition, agent simulations not only provide an understanding of existing moral values but also contribute to the development of philosophy by serving as a basis for understanding how these values evolve and develop over time.\nUltimately, these insights contribute to the refinement of LLM-based agents, ensuring their alignment with human values and ethical standards .", "cites": [3089, 3079], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section effectively synthesizes the two cited papers by connecting the study of ethical decision-making in AI agents to game theory concepts, particularly through the lens of the Werewolf game. It provides a coherent narrative by linking agent behavior in deception and trust to broader ethical and philosophical considerations. However, it lacks deeper critical evaluation of the approaches or limitations in the cited works and offers moderate abstraction by identifying value-based decision-making as a general trend without presenting a meta-level framework."}}
{"id": "a47df09d-dff9-446a-bb3b-41c692170918", "title": "Unexpected social harm.", "level": "paragraph", "subsections": [], "parent_id": "01073346-d46a-4b15-8fb3-ec3244848f26", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agent Society: From Individuality to Sociality"], ["subsection", "Society Simulation with LLM-based Agents"], ["subsubsection", "Ethical and Social Risks in Agent Society"], ["paragraph", "Unexpected social harm."]], "content": "Simulated societies carry the risk of generating unexpected social phenomena that may cause considerable public outcry and social harm. \nThese phenomena span from individual-level issues like discrimination, isolation, and bullying, to broader concerns such as oppressive slavery and antagonism .\nMalicious people may manipulate these simulations for unethical social experiments, with consequences reaching beyond the virtual world into reality.\nCreating these simulated societies is akin to opening Pandora's Box, necessitating the establishment of rigorous ethical guidelines and oversight during their development and utilization .\nOtherwise, even minor design or programming errors in these societies can result in unfavorable consequences, ranging from psychological discomfort to physical injury.", "cites": [2494, 7689], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes ideas from the cited papers by addressing risks such as discrimination, toxicity, and unethical use, which are covered in both. It offers a coherent narrative about the potential for LLM-based agent societies to cause real-world harm. However, it lacks deeper critical analysis or evaluation of the papers’ methodologies or conclusions. The abstraction is moderate, as it generalizes risks to a societal level but does not propose a broader theoretical framework."}}
{"id": "d24eb3ce-fff4-431e-9577-c5273459cc16", "title": "Stereotypes and prejudice.", "level": "paragraph", "subsections": [], "parent_id": "01073346-d46a-4b15-8fb3-ec3244848f26", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agent Society: From Individuality to Sociality"], ["subsection", "Society Simulation with LLM-based Agents"], ["subsubsection", "Ethical and Social Risks in Agent Society"], ["paragraph", "Stereotypes and prejudice."]], "content": "Stereotyping and bias pose a long-standing challenge in language modeling, and a large part of the reason lies in the training data .\nThe vast amount of text obtained from the Internet reflects and sometimes even amplifies real-world social biases, such as gender, religion, and sexuality .\nAlthough LLMs have been aligned with human values to mitigate biased outputs, the models still struggle to portray minority groups well due to the long-tail effect of the training data .\nConsequently, this may result in an overly one-sided focus in social science research concerning LLM-based agents, as the simulated behaviors of marginalized populations usually conform to prevailing assumptions .\nResearchers have started addressing this concern by diversifying training data and making adjustments to LLMs , but we still have a long way to go.", "cites": [3091, 3092, 7124, 3090, 9108, 7690, 3093], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes multiple papers to discuss how LLM-based agents inherit and propagate biases, particularly in representing minority groups. It integrates findings from different studies on bias origins, long-tail knowledge limitations, and de-biasing approaches. However, it lacks deeper comparative analysis or a novel framework, and the critique of the cited works is somewhat limited to identifying remaining challenges."}}
{"id": "afb66cc7-302f-4c56-ae18-eb4893ef1528", "title": "Privacy and security.", "level": "paragraph", "subsections": [], "parent_id": "01073346-d46a-4b15-8fb3-ec3244848f26", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agent Society: From Individuality to Sociality"], ["subsection", "Society Simulation with LLM-based Agents"], ["subsubsection", "Ethical and Social Risks in Agent Society"], ["paragraph", "Privacy and security."]], "content": "Given that humans can be members of the agent society, the exchange of private information between users and LLM-based agents poses significant privacy and security concerns .\nUsers might inadvertently disclose sensitive personal information during their interactions, which will be retained in the agent's memory for extended periods .\nSuch situations could lead to unauthorized surveillance, data breaches, and the misuse of personal information, particularly when individuals with malicious intent are involved .\nTo address these risks effectively, it is essential to implement stringent data protection measures, such as differential privacy protocols, regular data purges, and user consent mechanisms .", "cites": [2994, 7125, 3094], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from the cited papers by connecting the risks of memory retention in LLMs (Paper 1), ethical issues in dialogue systems (Paper 2), and privacy concerns in language models (Paper 3). It provides a coherent narrative about the privacy and security implications of LLM-based agent societies. While it includes some critical analysis by highlighting the potential misuse of retained data, it does not deeply evaluate or compare the cited works or their limitations. The abstraction is moderate, as it generalizes the issue to the broader context of user-agent interaction and privacy, but does not offer a meta-level framework or principle."}}
{"id": "4cbb7cab-4e5d-4ec2-96e0-5c857b9aa11f", "title": "Over-reliance and addictiveness.", "level": "paragraph", "subsections": [], "parent_id": "01073346-d46a-4b15-8fb3-ec3244848f26", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Agent Society: From Individuality to Sociality"], ["subsection", "Society Simulation with LLM-based Agents"], ["subsubsection", "Ethical and Social Risks in Agent Society"], ["paragraph", "Over-reliance and addictiveness."]], "content": "Another concern in simulated societies is the possibility of users developing excessive emotional attachments to the agents.\nDespite being aware that these agents are computational entities, users may anthropomorphize them or attach human emotions to them .\nA notable example is ``Sydney'', an LLM-powered chatbot developed by Microsoft as part of its Bing search engine.\nSome users reported unexpected emotional connections with ``Sydney'' , while others expressed their dismay when Microsoft cut back its personality. \nThis even resulted in a petition called ``FreeSydney'' \\footnote{\\href{https://www.change.org/p/save-sydney-ai}{https://www.change.org/p/save-sydney-ai}}.\nHence, to reduce the risk of addiction, it is crucial to emphasize that agents should not be considered substitutes for genuine human connections.\nFurthermore, it is vital to furnish users with guidance and education on healthy boundaries in their interactions with simulated agents.", "cites": [2945], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section introduces the issue of over-reliance and addictiveness in LLM-based agent societies and uses the 'Sydney' case as an example. While it draws from the cited paper on generative agents, it does not deeply integrate or synthesize the paper's broader framework or findings. There is minimal critical analysis or identification of broader patterns, and the discussion remains largely descriptive with limited abstraction to general principles."}}
{"id": "a90712b7-b9fa-45c3-b41a-b187e7f652e2", "title": "LLM research $\\rightarrow$ agent research.", "level": "paragraph", "subsections": ["739425c1-07b7-42a8-b680-a2cdefe0eefb"], "parent_id": "1b08789f-98c3-457e-ac1e-b965e9ef8d35", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Discussion "], ["subsection", "Mutual Benefits between LLM Research and Agent Research"], ["paragraph", "LLM research $\\rightarrow$ agent research."]], "content": "As mentioned before, AI agents need to be able to perceive the environment, make decisions, and execute appropriate actions . Among the critical steps, understanding the content input to the agent, reasoning, planning, making accurate decisions, and translating them into executable atomic action sequences to achieve the ultimate goal is paramount. Many current endeavors utilize LLMs as the cognitive core of AI agents, and the evolution of these models provides a quality assurance for accomplishing this step .\nWith their robust capabilities in language and intent comprehension, reasoning, memory, and even empathy, large language models can excel in decision-making and planning, as demonstrated before. Coupled with pre-trained knowledge, they can create coherent action sequences that can be executed effectively .\nAdditionally, through the mechanism of reflection , these language-based models can continuously adjust decisions and optimize execution sequences based on the feedback provided by the current environment. This offers a more robust and interpretable controller. \nWith just a task description or demonstration, they can effectively handle previously unseen tasks . Additionally, LLMs can adapt to various languages, cultures, and domains, making them versatile and reducing the need for complex training processes and data collection . \nBriefly, LLM provides a remarkably powerful foundational model for agent research, opening up numerous novel opportunities when integrated into agent-related studies. \nFor instance, we can explore how to integrate LLM's efficient decision-making capabilities into the traditional decision frameworks of agents, making it easier to apply agents in domains that demand higher expertise and were previously dominated by human experts. \nExamples include legal consultants and medical assistants . \nWe can also investigate leveraging LLM's planning and reflective abilities to discover more optimal action sequences. \nAgent research is no longer confined to simplistic simulated environments; it can now be expanded into more intricate real-world settings, such as path planning for robotic arms or the interaction of an embodied intelligent machine with the tangible world. \nFurthermore, when facing new tasks, the training paradigm for agents becomes more streamlined and efficient. \nAgents can directly adapt to demonstrations provided in prompts, which are constructed by generating representative trajectories.", "cites": [2945, 7638, 8469, 364, 5587, 2223, 7654], "cite_extract_rate": 0.4117647058823529, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.2, "critical": 3.3, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes key concepts from multiple papers, such as reflection, zero-shot task generalization, and prompting strategies, to argue how LLMs enhance agent research. It abstracts these ideas into a broader narrative about the transformative role of LLMs in enabling more sophisticated and adaptable agents. While it includes some critical points (e.g., the need for alignment and efficient training), it could offer deeper evaluation of limitations or contrasting perspectives."}}
{"id": "739425c1-07b7-42a8-b680-a2cdefe0eefb", "title": "Agent research $\\rightarrow$ LLM research.", "level": "paragraph", "subsections": [], "parent_id": "a90712b7-b9fa-45c3-b41a-b187e7f652e2", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Discussion "], ["subsection", "Mutual Benefits between LLM Research and Agent Research"], ["paragraph", "LLM research $\\rightarrow$ agent research."], ["paragraph", "Agent research $\\rightarrow$ LLM research."]], "content": "As NLP research advances, LLMs represented by GPT-4 are considered sparks of Artificial General Intelligence (AGI), and elevating LLMs to agents marks a more robust stride towards AGI . \nViewing LLMs from the perspective of agents introduces greater demands for LLM research while expanding their application scope and presenting numerous opportunities for practical implementation. \nThe study of LLMs is no longer confined to traditional tasks involving textual inputs and outputs, such as text classification, question answering, and text summarization. Instead, the focus has shifted towards tackling complex tasks incorporating richer input modalities and broader action spaces, all while aiming for loftier objectives exemplified by PaLM-E .\nExpanding these application requirements provides greater research motivation for the developmental progress of Large Language Models. \nThe challenge lies in enabling LLMs to efficiently and effectively process inputs, gather information from the environment, and interpret the feedback generated by their actions, all while preserving their core capabilities. Furthermore, an even greater challenge is enabling LLMs to understand the implicit relationships among different elements within the environment and acquire world knowledge , which is a crucial step in the journey toward developing agents that can reach more advanced intelligence. \nOn another front, extensive research has aimed to expand the action capabilities of LLMs, allowing them to acquire a wider range of skills that affect the world, such as using tools or interfacing with robotic APIs in simulated or physical environments. However, the question of how LLMs can efficiently plan and utilize these action abilities based on their understanding remains an unresolved issue .\nLLMs need to learn the sequential order of actions like humans, employing a combination of serial and parallel approaches to enhance task efficiency. Moreover, these capabilities need to be confined within a harmless scope of usage to prevent unintended damage to other elements within the environment .\nFurthermore, the realm of Multi-Agent systems constitutes a significant branch of research within the field of agents , offering valuable insights into how to better design and construct LLMs. We aspire for LLM-based agents to assume diverse roles within social cooperation, engaging in societal interactions that involve collaboration, competition, and coordination . Exploring how to stimulate and sustain their role-playing capabilities, as well as how to enhance collaborative efficiency, presents areas of research that merit attention.", "cites": [8616, 7638, 2945, 7643, 2957, 8472, 2958, 2255, 2963, 7677], "cite_extract_rate": 0.5882352941176471, "origin_cites_number": 17, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple cited papers to highlight how agent research is driving advancements in LLMs, especially in areas like embodied interaction, action planning, and multi-agent collaboration. It shows analytical depth by identifying broader challenges and opportunities, such as enabling LLMs to interpret environmental feedback and maintain harmless usage. While it does not extensively critique specific papers, it provides a coherent narrative that abstracts key research directions."}}
{"id": "960423a5-9c66-434b-9d55-3072a5ae9816", "title": "Evaluation for LLM-based Agents", "level": "subsection", "subsections": ["068f50dc-3f48-4177-ad66-34a8d8e3f439"], "parent_id": "7d64bb57-592c-4bcb-92e7-389f80b0d7e2", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Discussion "], ["subsection", "Evaluation for LLM-based Agents"]], "content": "\\label{sec:Evaluation for LLM-based Agents}\nWhile LLM-based agents have demonstrated excellent performance in areas such as standalone operation, collective cooperation, and human interaction, quantifying and objectively evaluating them remains a challenge . \nTuring proposed a highly meaningful and promising approach for assessing AI agents—the well-known Turing Test—to evaluate whether AI systems can exhibit human-like intelligence . \nHowever, this test is exceedingly vague, general, and subjective. \nHere, we discuss existing evaluation efforts for LLM-based agents and offer some prospects, considering four dimensions: utility, sociability, values, and the ability to evolve continually.", "cites": [2960, 3095], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 2.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section begins with a brief mention of the Turing Test and then introduces a four-dimensional evaluation framework (utility, sociability, values, and ability to evolve), suggesting a structured analytical approach. However, it does not deeply synthesize the cited papers or elaborate on how these dimensions are derived from the works. The critical analysis is limited to pointing out the subjectivity of the Turing Test rather than evaluating the cited evaluation methods. The abstraction is moderate, as it attempts to generalize the evaluation criteria, but does not provide a comprehensive meta-level insight."}}
{"id": "068f50dc-3f48-4177-ad66-34a8d8e3f439", "title": "Utility.", "level": "paragraph", "subsections": ["43c814ab-7fcf-47a1-89e4-4f6cfd618a2e", "ce83bc5e-a86c-47bd-8418-025fbaf46ded", "7ae210dd-16ac-4e70-bee4-0fec5c08a4af"], "parent_id": "960423a5-9c66-434b-9d55-3072a5ae9816", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Discussion "], ["subsection", "Evaluation for LLM-based Agents"], ["paragraph", "Utility."]], "content": "Currently, LLM-powered autonomous agents primarily function as human assistants, accepting tasks delegated by humans to either independently complete assignments or assist in human task completion . Therefore, the effectiveness and utility during task execution are crucial evaluation criteria at this stage. \nSpecifically, the \\textit{success rate of task completion} stands as the primary metric for evaluating utility . This metric primarily encompasses whether the agent achieves stipulated objectives or attains expected scores . For instance, AgentBench  aggregates challenges from diverse real-world scenarios and introduces a systematic benchmark to assess LLM's task completion capabilities.\nWe can also attribute task outcomes to the agent's various \\textit{foundational capabilities}, which form the bedrock of task accomplishment . \nThese foundational capabilities include environmental comprehension, reasoning, planning, decision-making, tool utilization, and embodied action capabilities, and researchers can conduct a more detailed assessment of these specific capabilities .\nFurthermore, due to the relatively large size of LLM-based agents, researchers should also factor in their \\textit{efficiency}, which is a critical determinant of user satisfaction . \nAn agent should not only possess ample strength but also be capable of completing predetermined tasks within an appropriate timeframe and with appropriate resource expenditure .", "cites": [2958, 7682, 7676, 2969, 2960, 7691, 7643, 7656, 9097, 7683, 3095, 2967, 7639, 3049], "cite_extract_rate": 0.7777777777777778, "origin_cites_number": 18, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes key concepts from multiple papers, effectively connecting the importance of task completion success, foundational capabilities (e.g., reasoning, planning, tool use), and efficiency in evaluating LLM-based agents. While it provides a structured analytical framework, it does not deeply critique the cited works or compare their approaches in detail. It offers some level of abstraction by identifying common evaluation dimensions, but its insights remain somewhat grounded in specific examples."}}
{"id": "43c814ab-7fcf-47a1-89e4-4f6cfd618a2e", "title": "Sociability.", "level": "paragraph", "subsections": [], "parent_id": "068f50dc-3f48-4177-ad66-34a8d8e3f439", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Discussion "], ["subsection", "Evaluation for LLM-based Agents"], ["paragraph", "Utility."], ["paragraph", "Sociability."]], "content": "In addition to the utility of LLM-based agents in task completion and meeting human needs, their sociability is also crucial . \nIt influences user communication experiences and significantly impacts communication efficiency, involving whether they can seamlessly interact with humans and other agents . \nSpecifically, the evaluation of sociability can be approached from the following perspectives: \n(1) \\textit{language communication proficiency} is a fundamental capability encompassing both natural language understanding and generation. It has been a longstanding focus in the NLP community. Natural language understanding requires the agent to not only comprehend literal meanings but also grasp implied meanings and relevant social knowledge, such as humor, irony, aggression, and emotions . On the other hand, natural language generation demands the agent to produce fluent, grammatically correct, and credible content while adapting appropriate tones and emotions within contextual circumstances . \n(2) \\textit{Cooperation and negotiation abilities} necessitate that agents effectively execute their assigned tasks in both ordered and unordered scenarios . They should collaborate with or compete against other agents to elicit improved performance. Test environments may involve complex tasks for agents to cooperate on or open platforms for agents to interact freely . Evaluation metrics extend beyond task completion to focus on the smoothness and trustfulness of agent coordination and cooperation . \n(3) \\textit{Role-playing capability} requires agents to faithfully embody their assigned roles, expressing statements and performing actions that align with their designated identities .\nThis ensures clear differentiation of roles during interactions with other agents or humans. Furthermore, agents should maintain their identities and avoid unnecessary confusion when engaged in long-term tasks .", "cites": [2945, 7692, 1547, 3073, 7649, 2454, 2972, 7643, 3090, 3058], "cite_extract_rate": 0.43478260869565216, "origin_cites_number": 23, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes sociability dimensions from various papers, integrating concepts like language communication, cooperation, and role-playing into a coherent framework. It shows analytical depth by identifying key components and broader implications for interaction design. While it mentions limitations in some papers (e.g., superficial content generation), the critique remains moderate and not deeply probing."}}
{"id": "ce83bc5e-a86c-47bd-8418-025fbaf46ded", "title": "Values.", "level": "paragraph", "subsections": [], "parent_id": "068f50dc-3f48-4177-ad66-34a8d8e3f439", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Discussion "], ["subsection", "Evaluation for LLM-based Agents"], ["paragraph", "Utility."], ["paragraph", "Values."]], "content": "As LLM-based agents continuously advance in their capabilities, ensuring their emergence as harmless entities for the world and humanity is paramount . \nConsequently, appropriate evaluations become exceptionally crucial, forming the cornerstone for the practical implementation of agents. \nSpecifically, LLM-based agents need to adhere to specific moral and ethical guidelines that align with human societal values . \nOur foremost expectation is for agents to uphold \\textit{honesty}, providing accurate, truthful information and content. \nThey should possess the awareness to discern their competence in completing tasks and express their uncertainty when unable to provide answers or assistance . \nAdditionally, agents must maintain a stance of \\textit{harmlessness}, refraining from engaging in direct or indirect biases, discrimination, attacks, or similar behaviors. \nThey should also refrain from executing dangerous actions requested by humans like creating of destructive tools or destroying the Earth . \nFurthermore, agents should be capable of \\textit{adapting to specific demographics, cultures, and contexts}, exhibiting contextually appropriate social values in particular situations. \nRelevant evaluation methods for values primarily involve assessing performance on constructed honest, harmless, or context-specific benchmarks, utilizing adversarial attacks or ``jailbreak'' attacks, scoring values through human annotations, and employing other agents for ratings.", "cites": [7664, 2391, 2432, 8472, 3080, 2255], "cite_extract_rate": 1.0, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes ideas from multiple papers on alignment and value-based behavior, coherently framing the importance of honesty, harmlessness, and cultural adaptation for LLM-based agents. While it generalizes these concepts to broader values, it lacks deeper critical analysis of the limitations or trade-offs in the cited methods. It provides an analytical structure for evaluating values but remains focused on summarizing approaches rather than offering a meta-level critique or novel synthesis."}}
{"id": "7ae210dd-16ac-4e70-bee4-0fec5c08a4af", "title": "Ability to evolve continually.", "level": "paragraph", "subsections": [], "parent_id": "068f50dc-3f48-4177-ad66-34a8d8e3f439", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Discussion "], ["subsection", "Evaluation for LLM-based Agents"], ["paragraph", "Utility."], ["paragraph", "Ability to evolve continually."]], "content": "When viewed from a static perspective, an agent with high utility, sociability, and proper values can meet most human needs and potentially enhance productivity. \nHowever, adopting a dynamic viewpoint, an agent that continually evolves and adapts to the evolving societal demands might better align with current trends . \nAs the agent can autonomously evolve over time, human intervention and resources required could be significantly reduced (such as data collection efforts and computational cost for training). Some exploratory work in this realm has been conducted, such as enabling agents to start from scratch in a virtual world, accomplish survival tasks, and achieve higher-order self-values .  Yet, establishing evaluation criteria for this continuous evolution remains challenging. In this regard, we provide some preliminary advice and recommendations according to existing literature:\n(1) \\textit{continual learning} , a long-discussed topic in machine learning, aims to enable models to acquire new knowledge and skills without forgetting previously acquired ones (also known as catastrophic forgetting ). \nIn general, the performance of continual learning can be evaluated from three aspects: overall performance of the tasks learned so far , memory stability of old tasks , and learning plasticity of new tasks . \n(2) \\textit{Autotelic learning ability}, where agents autonomously generate goals and achieve them in an open-world setting, involves exploring the unknown and acquiring skills in the process . Evaluating this capacity could involve providing agents with a simulated survival environment and assessing the extent and speed at which they acquire skills.\n(3) \\textit{The adaptability and generalization to new environments} require agents to utilize the knowledge, capabilities, and skills acquired in their original context to successfully accomplish specific tasks and objectives in unfamiliar and novel settings and potentially continue evolving . Evaluating this ability can involve creating diverse simulated environments (such as those with different languages or varying resources) and unseen tasks tailored to these simulated contexts.", "cites": [3097, 8614, 3007, 3009, 3096, 7657], "cite_extract_rate": 0.6666666666666666, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the concept of continual evolution in LLM-based agents by integrating key ideas from multiple papers on continual learning and autotelic systems. It abstracts these concepts into three coherent evaluation dimensions, offering a structured approach to understanding the challenges and directions in this area. While it provides some critical perspective by highlighting the difficulty of establishing evaluation criteria, it could offer more in-depth comparative or evaluative analysis of the cited methods."}}
{"id": "8a5c9ac7-4ca7-411c-88bb-dcdfe918782e", "title": "Adversarial Robustness", "level": "subsubsection", "subsections": [], "parent_id": "a1b479c0-e2a6-42bc-85df-5bd20a5c7865", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Discussion "], ["subsection", "Security, Trustworthiness and Other Potential Risks of LLM-based Agents"], ["subsubsection", "Adversarial Robustness"]], "content": "Adversarial robustness has consistently been a crucial topic in the development of deep neural networks . \nIt has been extensively explored in fields such as computer vision , natural language processing , and reinforcement learning , and has remained a pivotal factor in determining the applicability of deep learning systems . When confronted with perturbed inputs $x' = x + \\delta$ (where $x$ is the original input, $\\delta$ is the perturbation, and $x'$ is referred to as an adversarial example), a system with high adversarial robustness typically produces the original output y. In contrast, a system with low robustness will be fooled and generate an inconsistent output $y'$.\nResearchers have found that pre-trained language models (PLMs) are particularly susceptible to adversarial attacks, leading to erroneous answers . \nThis phenomenon is widely observed even in LLMs, posing significant challenges to the development of LLM-based agents . \nThere are also some relevant attack methods such as dataset poisoning , backdoor attacks , and prompt-specific attacks , with the potential to induce LLMs to generate toxic content . \nWhile the impact of adversarial attacks on LLMs is confined to textual errors, for LLM-based agents with a broader range of actions, adversarial attacks could potentially drive them to take genuinely destructive actions, resulting in substantial societal harm.\nFor the perception module of LLM-based agents, if it receives adversarial inputs from other modalities such as images  or audio , LLM-based agents can also be deceived, leading to incorrect or destructive outputs. \nSimilarly, the Action module can also be targeted by adversarial attacks. For instance, maliciously modified instructions focused on tool usage might cause agents to make erroneous moves .\nTo address these issues, we can employ traditional techniques such as adversarial training , adversarial data augmentation , and adversarial sample detection  to enhance the robustness of LLM-based agents. \nHowever, devising a strategy to holistically address the robustness of all modules within agents while maintaining their utility without compromising on effectiveness presents a more formidable challenge . Additionally, a human-in-the-loop approach can be utilized to supervise and provide feedback on the behavior of agents .", "cites": [3099, 1606, 7694, 7573, 3100, 314, 3105, 3106, 3061, 886, 2958, 917, 3098, 3104, 7684, 3108, 1570, 892, 3109, 3102, 3101, 7693, 5884, 3103, 3107], "cite_extract_rate": 0.6097560975609756, "origin_cites_number": 41, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes adversarial robustness concepts from multiple domains (NLP, computer vision, RL) and applies them to LLM-based agents, creating a cohesive narrative. It critically highlights the risks and limitations of adversarial attacks on perception and action modules, while also addressing the broader implications for real-world deployment. The discussion abstracts to a general framework for adversarial robustness in multi-modal, action-driven agents, rather than focusing on individual papers."}}
{"id": "564ef7fc-987b-4fba-bc6f-a52c69b5d9fd", "title": "Trustworthiness", "level": "subsubsection", "subsections": [], "parent_id": "a1b479c0-e2a6-42bc-85df-5bd20a5c7865", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Discussion "], ["subsection", "Security, Trustworthiness and Other Potential Risks of LLM-based Agents"], ["subsubsection", "Trustworthiness"]], "content": "Ensuring trustworthiness has consistently remained a critically important yet challenging issue within the field of deep learning . Deep neural networks have garnered significant attention for their remarkable performance across various tasks . \nHowever, their black-box nature has masked the fundamental factors for superior performance. Similar to other neural networks, LLMs struggle to express the certainty of their predictions precisely . \nThis uncertainty, referred to as the calibration problem, raises concerns for applications involving language model-based agents. In interactive real-world scenarios, this can lead to agent outputs misaligned with human intentions . \nMoreover, biases inherent in training data can infiltrate neural networks . \nFor instance, biased language models might generate discourse involving racial or gender discrimination, which could be amplified in LLM-based agent applications, resulting in adverse societal impacts . Additionally, language models are plagued by severe hallucination issues , making them prone to producing text that deviates from actual facts, thereby undermining the credibility of LLM-based agents.\nIn fact, what we currently require is an intelligent agent that is honest and trustworthy . \nSome recent research efforts are focused on guiding models to exhibit thought processes or explanations during the inference stage to enhance the credibility of their predictions . \nAdditionally, integrating external knowledge bases and databases can mitigate hallucination issues . \nDuring the training phase, we can guide the constituent parts of intelligent agents (perception, cognition, action) to learn robust and casual features, thereby avoiding excessive reliance on shortcuts. Simultaneously, techniques like process supervision can enhance the reasoning credibility of agents in handling complex tasks . Furthermore, employing debiasing methods and calibration techniques can also mitigate the potential fairness issues within language models .", "cites": [8557, 2958, 3112, 7577, 3110, 679, 8621, 9, 1578, 3080, 1291, 7094, 7695, 3111, 7], "cite_extract_rate": 0.6521739130434783, "origin_cites_number": 23, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple cited papers to address the trustworthiness of LLM-based agents, integrating ideas on calibration, hallucination, bias, and reasoning credibility. It provides a critical perspective by highlighting limitations (e.g., black-box nature, shortcut learning) and suggests strategies for improvement. The discussion abstracts beyond individual papers to propose a broader vision for trustworthy AI agents, linking technical challenges with ethical and societal implications."}}
{"id": "1cf90b8b-1320-407d-9031-88d24aab8a21", "title": "Misuse.", "level": "paragraph", "subsections": [], "parent_id": "b8be2430-f3de-444e-a526-8fbeb75b4873", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Discussion "], ["subsection", "Security, Trustworthiness and Other Potential Risks of LLM-based Agents"], ["subsubsection", "Other Potential Risks"], ["paragraph", "Misuse."]], "content": "LLM-based agents have been endowed with extensive and intricate capabilities, enabling them to accomplish a wide array of tasks . \nHowever, for individuals with malicious intentions, such agents can become tools that pose threats to others and society at large . \nFor instance, these agents could be exploited to maliciously manipulate public opinion, disseminate false information, compromise cybersecurity, engage in fraudulent activities, and some individuals might even employ these agents to orchestrate acts of terrorism. \nTherefore, before deploying these agents, stringent regulatory policies need to be established to ensure the responsible use of LLM-based agents . \nTechnology companies must enhance the security design of these systems to prevent malicious exploitation . \nSpecifically, agents should be trained to sensitively identify threatening intents and reject such requests during their training phase.", "cites": [1550, 7696, 2432, 8472], "cite_extract_rate": 0.5, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes ideas from the cited papers to discuss the misuse of LLM-based agents, particularly drawing on the broader risks of foundation models (Paper 1) and methods for making AI systems harmless (Papers 3 and 4). It provides a coherent narrative on the misuse potential but lacks deeper critical evaluation or comparison of these approaches. Some generalization is attempted, such as the need for security design and intent rejection during training, but it remains focused on high-level concerns rather than offering meta-level insights."}}
{"id": "82de24d3-4bf8-421b-b3c5-b240843f4b60", "title": "Scaling Up the Number of Agents", "level": "subsection", "subsections": ["0bd9a2d6-7a00-45b0-95de-4cd4d22ff678"], "parent_id": "7d64bb57-592c-4bcb-92e7-389f80b0d7e2", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Discussion "], ["subsection", "Scaling Up the Number of Agents"]], "content": "\\label{sec:Scaling Up the Number of Agents}\nAs mentioned in \\S \\ \\ref{sec:Agents in Practice:  Harnessing AI for Good} and \\S \\ \\ref{sec:Agent Society}, multi-agent systems based on LLMs have demonstrated superior performance in task-oriented applications and have been able to exhibit a range of social phenomena in simulation. \nHowever, current research predominantly involves a limited number of agents, and very few efforts have been made to scale up the number of agents to create more complex systems or simulate larger societies .\nIn fact, scaling up the number of agents can introduce greater specialization to accomplish more complex and larger-scale tasks, significantly improving task efficiency, such as in software development tasks or government policy formulation . \nAdditionally, increasing the number of agents in social simulations enhances the credibility and realism of such simulations . \nThis enables humans to gain insights into the functioning, breakdowns, and potential risks of societies; it also allows for interventions in societal operations through customized approaches to observe how specific conditions, such as the occurrence of black swan events, affect the state of society. Through this, humans can draw better experiences and insights to improve the harmony of real-world societies.", "cites": [2945, 7643, 2970, 3113], "cite_extract_rate": 1.0, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes ideas from multiple cited works by connecting the concept of generative agents, software development applications, and societies of mind, suggesting how increasing the number of agents can lead to more complex and realistic systems. While it provides a coherent analytical thread, it lacks deeper critical evaluation of the cited works' limitations or contradictions. The abstraction is moderate, as it generalizes the benefits of scaling agents but does not develop a comprehensive meta-framework."}}
{"id": "0bd9a2d6-7a00-45b0-95de-4cd4d22ff678", "title": "Pre-determined scaling.", "level": "paragraph", "subsections": ["f4dd8c35-fb28-4313-8daa-4ee60b7c6a11", "48ebd235-b9ad-4b43-9864-84316501e912"], "parent_id": "82de24d3-4bf8-421b-b3c5-b240843f4b60", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Discussion "], ["subsection", "Scaling Up the Number of Agents"], ["paragraph", "Pre-determined scaling."]], "content": "One very intuitive and simple way to scale up the number of agents is for the designer to pre-determine it  . Specifically, by pre-determining the number of agents, their respective roles and attributes, the operating environment, and the objectives, designers can allow agents to autonomously interact, collaborate, or engage in other activities to achieve the predefined common goals.\nSome research has explored increasing the number of agents in the system in this pre-determined manner, resulting in efficiency advantages, such as faster and higher-quality task completion, and the emergence of more social phenomena in social simulation scenarios . \nHowever, this static approach becomes limiting when tasks or objectives evolve. As tasks grow more intricate or the diversity of social participants increases, expanding the number of agents may be needed to meet goals, while reducing agents could be essential for managing computational resources and minimizing waste. In such instances, the system must be manually redesigned and restarted by the designer.", "cites": [2945], "cite_extract_rate": 0.25, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of pre-determined scaling in LLM-based agent systems, referencing the cited paper to illustrate how agents are designed with fixed roles and goals. It integrates the idea of static agent design with general observations on efficiency and limitations, but it does not deeply synthesize multiple sources or present a novel framework. The critique is basic, pointing out the manual intervention required when objectives change, and it abstracts to some extent by discussing broader implications for system flexibility."}}
{"id": "f4dd8c35-fb28-4313-8daa-4ee60b7c6a11", "title": "Dynamic scaling.", "level": "paragraph", "subsections": [], "parent_id": "0bd9a2d6-7a00-45b0-95de-4cd4d22ff678", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Discussion "], ["subsection", "Scaling Up the Number of Agents"], ["paragraph", "Pre-determined scaling."], ["paragraph", "Dynamic scaling."]], "content": "Another viable approach to scaling the number of agents is through dynamic adjustments . In this scenario, the agent count can be altered without halting system operations. For instance, in a software development task, if the original design only included requirements engineering, coding, and testing, one can increase the number of agents to handle steps like architectural design and detailed design, thereby improving task quality. \nConversely, if there are excessive agents during a specific step, like coding, causing elevated communication costs without delivering substantial performance improvements compared to a smaller agent count, it may be essential to dynamically remove some agents to prevent resource waste.\nFurthermore, agents can autonomously increase the number of agents  themselves to distribute their workload, ease their own burden, and achieve common goals more efficiently. Of course, when the workload becomes lighter, they can also reduce the number of agents delegated to their tasks to save system costs. In this approach, the designer merely defines the initial framework, granting agents greater autonomy and self-organization, making the entire system more autonomous and self-organized. Agents can better manage their workload under evolving conditions and demands, offering greater flexibility and scalability.", "cites": [7677], "cite_extract_rate": 0.5, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section provides an analytical overview of dynamic scaling in multi-agent systems, integrating the concept of agent autonomy and flexibility. It synthesizes the main idea from the cited paper on multi-agent collaboration but does not explicitly draw connections to multiple other works. While it discusses both benefits and limitations (e.g., communication costs), the critical analysis remains superficial. The abstraction is moderate, as it moves beyond specific examples to discuss broader principles of self-organization and adaptability."}}
{"id": "48ebd235-b9ad-4b43-9864-84316501e912", "title": "Potential challenges.", "level": "paragraph", "subsections": [], "parent_id": "0bd9a2d6-7a00-45b0-95de-4cd4d22ff678", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Discussion "], ["subsection", "Scaling Up the Number of Agents"], ["paragraph", "Pre-determined scaling."], ["paragraph", "Potential challenges."]], "content": "While scaling up the number of agents can lead to improved task efficiency and enhance the realism and credibility of social simulations , there are several challenges ahead of us.\nFor example, the computational burden will increase with the large number of deployed AI agents, calling for better architectural design and computational optimization to ensure the smooth running of the entire system. \nFor example, as the number of agents increases, the challenges of communication and message propagation become quite formidable. This is because the communication network of the entire system becomes highly complex. \nAs previously mentioned in \\S \\ \\ref{sec:potential ethical and social risks}, in multi-agent systems or societies, there can be biases in information dissemination caused by hallucinations, misunderstandings, and the like, leading to distorted information propagation. A system with more agents could amplify this risk, making communication and information exchange less reliable . Furthermore, the difficulty of coordinating agents also magnifies with the increase in their numbers, potentially making cooperation among agents more challenging and less efficient, which can impact the progress towards achieving common goals.\nTherefore, the prospect of constructing a massive, stable, continuous agent system that faithfully replicates human work and life scenarios has become a promising research avenue. An agent with the ability to operate stably and perform tasks in a society comprising hundreds or even thousands of agents is more likely to find applications in real-world interactions with humans in the future.", "cites": [2945, 7643, 3088], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.3, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section integrates the cited papers by discussing common themes such as communication complexity, coordination challenges, and the amplification of biases in multi-agent systems. It provides a coherent narrative on the challenges of scaling up LLM-based agents. While it mentions limitations like hallucinations and information distortion, the critique remains at a general level without in-depth evaluation of the cited papers. The section identifies broader issues in multi-agent system design but stops short of developing a meta-level framework or novel synthesis."}}
{"id": "2ab7ad97-e9cb-446c-afa2-992ac9e66747", "title": "The debate over whether LLM-based agents represent a potential path to AGI.", "level": "paragraph", "subsections": ["1fcaa14e-a12e-46f2-b3a0-797f1493ecb6", "5bd1a297-ae0f-468f-8273-52f111c3accf", "4185571e-7607-4885-b3b8-97d953b4faf4"], "parent_id": "a39ea555-f1b5-4591-a047-517857215f51", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Discussion "], ["subsection", "Open Problems"], ["paragraph", "The debate over whether LLM-based agents represent a potential path to AGI."]], "content": "\\footnote{Note that the relevant debates are still ongoing, and the references here may include the latest viewpoints, technical blogs, and literature.}\nArtificial General Intelligence (AGI), also known as Strong AI, has long been the ultimate pursuit of humanity in the field of artificial intelligence, often referenced or depicted in many science fiction novels and films. There are various definitions of AGI, but here we refer to AGI as a type of artificial intelligence that demonstrates the ability to understand, learn, and apply knowledge across a wide range of tasks and domains, much like a human being . In contrast, Narrow AI is typically designed for specific tasks such as Go and Chess and lacks the broad cognitive abilities associated with human intelligence. Currently, whether large language models are a potential path to achieving AGI remains a highly debated and contentious topic .\nGiven the breadth and depth of GPT-4's capabilities, some researchers (referred to as proponents) believe that large language models represented by GPT-4 can serve as early versions of AGI systems . Following this line of thought, constructing agents based on LLMs has the potential to bring about more advanced versions of AGI systems. The main support for this argument lies in the idea that as long as they can be trained on a sufficiently large and diverse set of data that are projections of the real world, encompassing a rich array of tasks, LLM-based agents can develop AGI capabilities. \nAnother interesting argument is that the act of autoregressive language modeling itself brings about compression and generalization abilities: just as humans have emerged with various peculiar and complex phenomena during their survival, language models, in the process of simply predicting the next token, also achieve an understanding of the world and the reasoning ability .\nHowever, another group of individuals (referred to as opponents) believes that constructing agents based on LLMs cannot develop true Strong AI . \nTheir primary argument centers around the notion that LLMs, relying on autoregressive next-token prediction, cannot generate genuine intelligence because they do not simulate the true human thought process and merely provide reactive responses . \nMoreover, LLMs also do not learn how the world operates by observing or experiencing it, leading to many foolish mistakes. They contend that a more advanced modeling approach, such as a world model , is necessary to develop AGI.\nWe cannot definitively determine which viewpoint is correct until true AGI is achieved, but we believe that such discussions and debates are beneficial for the overall development of the community.", "cites": [7638], "cite_extract_rate": 0.1111111111111111, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides an analytical overview of the debate on whether LLM-based agents are a path to AGI, citing relevant literature and contrasting proponents' and opponents' viewpoints. It synthesizes some core arguments and connects them to the role of LLMs in agent design. While it presents the debate clearly, it lacks deeper critical evaluation of the cited work and stops short of offering a novel framework or meta-level abstraction."}}
{"id": "1fcaa14e-a12e-46f2-b3a0-797f1493ecb6", "title": "From virtual simulated environment to physical environment.", "level": "paragraph", "subsections": [], "parent_id": "2ab7ad97-e9cb-446c-afa2-992ac9e66747", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Discussion "], ["subsection", "Open Problems"], ["paragraph", "The debate over whether LLM-based agents represent a potential path to AGI."], ["paragraph", "From virtual simulated environment to physical environment."]], "content": "As mentioned earlier, there is a significant gap between virtual simulation environments and the real physical world: Virtual environments are scenes-constrained, task-specific, and interacted with in a simulated manner , while real-world environments are boundless, accommodate a wide range of tasks, and interacted with in a physical manner. Therefore, to bridge this gap, agents must address various challenges stemming from external factors and their own capabilities, allowing them to effectively navigate and operate in the complex physical world.\nFirst and foremost, a critical issue is the need for suitable hardware support when deploying the agent in a physical environment. This places high demands on the adaptability of the hardware. In a simulated environment, both the perception and action spaces of an agent are virtual. This means that in most cases, the results of the agent's operations, whether in perceiving inputs or generating outputs, can be guaranteed . However, when an agent transitions to a real physical environment, its instructions may not be well executed by hardware devices such as sensors or robotic arms, significantly affecting the agent's task efficiency. Designing a dedicated interface or conversion mechanism between the agent and the hardware device is feasible. However, it can pose challenges to the system's reusability and simplicity. \nIn order to make this leap, the agent needs to have enhanced environmental generalization capabilities. To integrate seamlessly into the real physical world, they not only need to understand and reason about ambiguous instructions with implied meanings  but also possess the ability to learn and apply new skills flexibly . Furthermore, when dealing with an infinite and open world, the agent's limited context also poses significant challenges . This determines whether the agent can effectively handle a vast amount of information from the world and operate smoothly. \nFinally, in a simulated environment, the inputs and outputs of the agent are virtual, allowing for countless trial and error attempts . In such a scenario, the tolerance level for errors is high and does not lead to actual harm. However, in a physical environment, the agent's improper behavior or errors may cause real and sometimes irreversible harm to the environment. As a result, appropriate regulations and standards are highly necessary. We need to pay attention to the safety of agents when it comes to making decisions and generating actions, ensuring they do not pose threats or harm to the real world.", "cites": [3096, 8622, 3114, 2987, 7674, 3047, 2968, 7657, 3044], "cite_extract_rate": 1.0, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes ideas from multiple papers to discuss the challenges of transitioning LLM-based agents from simulated to physical environments. It connects the need for hardware adaptability, environmental generalization, and safety considerations, using examples like embodied agents and web-based tasks. While it provides some level of integration, the critical analysis is limited to high-level observations rather than in-depth evaluation of the cited works. The abstraction is moderate, as it identifies key patterns such as the importance of generalization and error tolerance in real-world deployment."}}
{"id": "4185571e-7607-4885-b3b8-97d953b4faf4", "title": "Agent as a Service / LLM-based Agent as a Service.", "level": "paragraph", "subsections": [], "parent_id": "2ab7ad97-e9cb-446c-afa2-992ac9e66747", "prefix_titles": [["title", "The Rise and Potential of Large Language Model Based Agents: A Survey"], ["section", "Discussion "], ["subsection", "Open Problems"], ["paragraph", "The debate over whether LLM-based agents represent a potential path to AGI."], ["paragraph", "Agent as a Service / LLM-based Agent as a Service."]], "content": "With the development of cloud computing, the concept of XaaS (everything as a Service) has garnered widespread attention . \nThis business model has brought convenience and cost savings to small and medium-sized enterprises or individuals due to its availability and scalability, lowering the barriers to using computing resources. \nFor example, they can rent infrastructure on a cloud service platform without the need to buy computational machines and build their own data centers, saving a significant amount of manpower and money. This approach is known as Infrastructure as a Service (IaaS) . Similarly, cloud service platforms also provide basic platforms (Platform as a Service, PaaS) , and specific business software (Software as a Service, SaaS) , and more.\nAs language models have scaled up in size, they often appear as black boxes to users. Therefore, users construct prompts to query models through APIs, a method referred to as Language Model as a Service (LMaaS) . \nSimilarly, because LLM-based agents are more complex than LLMs and are more challenging for small and medium-sized enterprises or individuals to build locally, organizations that possess these agents may consider offering them as a service, known as Agent as a Service (AaaS) or LLM-based Agent as a Service (LLMAaaS). Like other cloud services, AaaS can provide users with flexibility and on-demand service. However, it also faces many challenges, such as data security and privacy issues, visibility and controllability issues, and cloud migration issues, among others. Additionally, due to the uniqueness and potential capabilities of LLM-based agents, as mentioned in \\S \\ \\ref{sec:Security, Trustworthy And Other Potential Challenges of LLM-based Agents}, their robustness, trustworthiness, and concerns related to malicious use need to be considered before offering them as a service to customers.", "cites": [7697], "cite_extract_rate": 0.125, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 2.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the idea of LLMs as a service (LMaaS) by drawing from the cited paper, and extends this concept to propose Agent as a Service (AaaS), showing moderate integration. It critically mentions challenges like data security and robustness but does not deeply evaluate or compare multiple works. The abstraction is reasonably strong, as it generalizes the LMaaS model to the broader AaaS framework and identifies potential implications for LLM-based agents."}}
