{"id": "428f0e83-f11e-4df6-8b69-0c0beb42e5b5", "title": "Introduction", "level": "section", "subsections": [], "parent_id": "419df863-69f4-4c35-93d0-018d167ed4ec", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Introduction"]], "content": "\\label{chap:introduction}}\n\\IEEEPARstart{B}{uilding} high-quality machine learning (ML) models demands a massive amount of training data. Yet, the communication cost and privacy concerns impinge on the process of collecting large volumes of data from diverse sources. It is not until recently that governments started to regulate the commercial use of data with privacy-preserving legislation (e.g., GDPR~, HIPAA~, and CCPA~). Compliance violations can be costly, with hefty fines up to hundreds of millions of dollars a year~. As such, the desire for multiple entities (e.g., mobile devices or large organizations) to collaboratively train a shared model efficiently and privately gives birth to a new ML paradigm called federated learning (FL)~. FL promises not to expose the clients' raw data, and has been widely adopted in many industries with applications ranging from mobile devices~ to financial management~ and medical care~.\nApart from providing strong privacy guarantees, the key to the success of a federated training system lies in its efficiency. A typical efficiency metric is the \\emph{time-to-accuracy}, which is the wall clock time taken to train a model until it reaches the target accuracy. Despite the rich body of work that explored various optimization strategies, there is still plenty of room for further improvement due to the following distinct challenges posed to FL (\\cref{chap:background}): (1) the \\textit{lack of information for optimization}: the information needed for optimally configuring the system is usually outdated or simply unavailable due to privacy constraints and scaling issues; (2) the \\textit{tradeoff between statistical and system utility}: statistical utility (the number of iterations taken to reach a plausible target accuracy) and system utility (the duration of an iteration), the two determining factor for time-to-accuracy, are usually at odds in FL; (3) \\textit{client heterogeneity}: clients cannot be treated uniformly due to the intrinsic differences in terms of resources, data, and states; and (4) a \\textit{large configuration space}: the operational dimensions for system developers are too many to explore within an acceptable time. Given these challenges, it is worth summarizing existing research efforts to give researchers a holistic view of the lessons learned and to solicit further explorations.\nTo position existing research attempts in optimizing the time-to-accuracy\nperformance in FL, we propose a layered approach that categorizes them by the\ntraining phases in which they take effect: selection, configuration, and\nreporting (\\cref{chap:prior}). For the \\textit{selection phase} where the\nserver chooses clients for participation, there are mainly two lines of\noptimization efforts: (1) prioritizing clients either with\nhigh statistical utility or system utility~\\cite\n{zhang2021client, nishio2019client, wang2020optimizing}, and (2) explicitly considering\nboth utilities and developing a more informed solution in response to client\ndynamics in practice~. \nAs for the \\textit{configuration phase} where the server sends the global\nmodel to the selected clients with auxiliary configuration information, and\nclients perform local training, we sort out four lines of work: (1) the first\ntwo lines advocate mitigating the communication cost by reducing the model\nsize~ and\ndecreasing the synchronization frequency~\\cite\n{hsieh2017gaia, kamp2018efficient, luping2019cmfl, chen2019communication,\nchen2021communication}; (2) the last two lines minimize the computational\noverhead by accelerating the training speed in each round~\\cite\n{anh2019efficient, nguyen2020resource, wang2020towards, li2018federated, diao2020heterofl, ren2020accelerating} and reducing the number of training\nrounds~. \nIn terms of the \\textit{reporting phase}, we focus on the aggregation and\noutline two related optimizations: (1) reducing the aggregation\nlatency by adopting hierarchical methods~\\cite\n{liu2020client, abad2020hierarchical, wu2020accelerating} and developing\nlightweight privacy-preserving methods~\\cite\n{so2021turbo, zhang2020batchcrypt, jiang2021flashe}, and (2)\nimproving the long-term convergence rate with adaptive\noptimizers on the server~. For each of the attempted optimizations, our discussion\nincludes necessary details for readers to understand the motivation,\nmechanisms, and major results. In addition, works such as measurement\nstudies~ and benchmarking tools~\\cite\n{caldas2018leaf, hu2020oarf, lai2021fedscale, fate, he2020fedml,\nbeutel2020flower, plato} are indispensable in system research. We also survey the\nstatus quo as a tutorial on FL practice (\\cref{chap:concerstone}).\nOur work focuses on the system-level efforts made in improving the time-to-accuracy performance for synchronous federated training. We also share some implications derived from the literature and our survey process. It thus differs from existing surveys which mainly focus on ML algorithms and privacy-preserving algorithms. We expect this work to be an initial attempt to bridge the gap of system-oriented surveys in FL literature, as well as soliciting more contributions to related research.", "cites": [585, 589, 584, 581, 587, 580, 583, 586, 7262, 582, 7261, 588], "cite_extract_rate": 0.6, "origin_cites_number": 20, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section demonstrates strong synthesis by organizing cited works into a structured training workflow (selection, configuration, reporting) and connecting ideas across different papers to form a coherent narrative. It abstracts common challenges and optimization strategies, offering a meta-level view of FL system optimization. Critical analysis is present but not as deep, as the section mainly identifies gaps and contrasts approaches without extensive evaluation of their limitations."}}
{"id": "74ace61b-35de-40e1-bc2b-a3118bd28783", "title": "Federated Training", "level": "subsection", "subsections": [], "parent_id": "498f24aa-02f6-4623-986c-cb8907d7dc9c", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Background, Problem and Challenges"], ["subsection", "Federated Training"]], "content": "\\label{sec:background_federated}\nFederated learning (FL)~ has recently emerged\nas a new paradigm of collaborative machine learning (ML) that allows multiple\ndistributed clients (e.g., mobile devices or business organizations) to\ncollaboratively train or evaluate a model with decentralized data. Compared\nto traditional distributed learning in datacenter environments, FL mainly\ndiffers in orchestration, resource constraints, data distribution, and\nparticipation scale~. At its core, FL keeps private\ndata on-premises, while using a central server to maintain a global model and\niteratively refine it by aggregating each client's local updates. This design\nreduces not only the communication cost but also the privacy risk in\ngathering clients' raw data. Owing to its privacy guarantees, FL has found\nwide applications in various domains. On mobile devices, Google runs FL to\nimprove the user experience for Google Keyboard~\\cite\n{yang2018applied, hard2018federated, ramaswamy2019federated,\nchen2019federated} and Assistant~, while Apple deploys FL to\nevaluate and tune speech recognition models~; in\nfintech, both IBM~ and WeBank~ utilize\nFL to detect financial misconducts; in healthcare, NVIDIA applies FL\nto create medical imaging AI~ and predict patients' needs\nfor oxygen~.\nWhile both model training and evaluation play important roles in the development of an FL model, they have different criteria in system design. In this survey, we limit the scope to the \\textit{training} process, which is the most time-consuming and resource-intensive stage throughout the development of an FL model. Due to its predominance in practice, we focus on the support for the \\textit{synchronous} mode, wherein an ML model is trained across a pool of candidate clients in rounds, and in each round, the server needs to wait until a predefined deadline or receiving a sufficient number of clients' updates prior to deriving an aggregated update. In more detail, each round consists of the following three phases (Fig.~\\ref{fig:fl}).\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1.0\\columnwidth]{standard-fl-protocol}\n    \\caption{Standard synchronous federated training protocol~.}\n    \\label{fig:fl}\n\\end{figure}\n\\begin{itemize}\n    \\item \\textit{Client Selection}. At the beginning of each round, the server waits for a sufficient number of clients with eligible status (i.e., currently charging and connected to an unmetered network) to check in. The server then selects a subset of them based on certain strategies (e.g., randomly or selectively) for participation, and notifies the others to reconnect later.\n    \\item \\textit{Configuration}. The server next sends the global model status and configuration profiles (e.g., the number of local epochs or the reporting deadline) to each of the selected clients. Based on the instructed configuration, the clients perform local model training independently with their private data.\n    \\item \\textit{Reporting}. The server then waits for the participating clients to report local updates until reaching the predefined deadline. The current round is aborted if no enough clients report in time. Otherwise, the server aggregates the received local updates, uses the aggregate to update the global model status, and concludes the round.\n\\end{itemize}", "cites": [590, 587, 7261, 582, 583, 588, 591], "cite_extract_rate": 0.6363636363636364, "origin_cites_number": 11, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 1.5, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a clear overview of federated training and its phases (client selection, configuration, and reporting) with references to relevant works. However, it primarily describes these components in a general way without deeply synthesizing or connecting the cited papers' contributions. There is little critical evaluation or abstraction beyond the specific examples provided."}}
{"id": "172d329c-9179-421a-a850-1b10255f24b6", "title": "Statistical Utility and Sytem Utility: A Case Study~\\label{sec:background_problem_utility", "level": "subsubsection", "subsections": [], "parent_id": "e9b69d5b-e1b1-49ab-9b47-79a64bec7fd4", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Background, Problem and Challenges"], ["subsection", "System Optimization: The Problem\\label{sec:background_problem"], ["subsubsection", "Statistical Utility and Sytem Utility: A Case Study~\\label{sec:background_problem_utility"]], "content": "}\nIntuitively, the time-to-accuracy performance of federated training is determined by two factors: the number of rounds taken to reach the accuracy and the average duration of training rounds. As in , we regard the former as \\textit{statistical utility} and the latter as \\textit{system utility} throughout this survey.\nTo illustrate, we conduct an FL case study where 16 clients collaboratively train the LeNet5~ model to classify the images from the MNIST~ dataset. To emulate the cross-device environment, each client is run atop an AWC EC2 \\texttt{c5.xlarge} instance (4 vCPUs and 8 GB memory) and the network bandwidth is throttled to 21 Mbps to dictate the average mobile connection speed as of 2021~. We run FedAvg~ for model aggregation without loss of generality, atop which we enforce clients' privacy by implementing the SecAgg protocol~ (also see~\\cref{sec:prior_configuration_latency}) which makes the server blind to each client's local updates. Essentially, we design three schemes to compare:\n\\begin{itemize}\n    \\item \\textit{Baseline}: all clients' data is independent and identically distributed (IID). They also share the same computational speed.\n    \\item \\textit{NonIID}: the same as \\textit{Baseline} except that clients' data is non-IID as depicted by Fig.~\\ref{fig:toy_noniid}. This is achieved by latent Dirichlet allocation (described in~\\cref{sec:cornerstone_benchmarking_datasets}) with the concentration vectors being all 0.1's.\n    \\item \\textit{Straggler}: the same as \\textit{Baseline} except that clients’ computation speeds follow the Zipf's distribution (with $\\alpha=1.2$, i.e., moderately skewed). As shown in Fig.~\\ref{fig:toy_straggler}, the straggler is 5$\\times$ slower than the fastest who shares the same speed as clients in \\textit{Baseline}.\n\\end{itemize}\nWe first study the impact of statistical utility by comparing \\textit{Baseline} and \\textit{NonIID}. As indicated in Fig.~\\ref{fig:toy_round_time}, they proceed a round at the same speed. However, as the learning curves in Fig.~\\ref{fig:toy_time_to_acc} dictate, \\textit{Baseline} reaches the target accuracy (96.1\\%) with only 8.4 min, while \\textit{NonIID} takes 43.5 min, which is 5$\\times$ slower than \\textit{Baseline} does. Referring to the round-to-accuracy performance as in~Fig.~\\ref{fig:toy_round_to_acc}, one can know that the reason for this contrast lies in the difference in the numbers of rounds taken to convergence: 10 for \\textit{Baseline} while 40 for \\textit{NonIID}. Hence, despite having the same system utility, \\textit{Baseline} achieves much better time-to-accuracy performance for having greater statistical utility due to more evenly distributed data across clients. Of course, data distribution is uncontrollable in FL practice and developers can barely end up with IID cases as in \\textit{Baseline}. This adds up to the challenges of system-level optimizations in FL training, as later discussed in~\\cref{sec:background_challenge_optimality}.\nNext, we provide a sense of how system utility affects the end-to-end performance by comparing \\textit{Baseline} with \\textit{Straggler}. As shown in Fig.~\\ref{fig:toy_round_to_acc}, both cases converge with the same amount of rounds: 10. However, it takes \\textit{Straggler} slightly longer in time (15.8 min, i.e., 1.9$\\times$) to reach the target accuracy, as demonstrated in Fig.~\\ref{fig:toy_time_to_acc}. By observing Fig.~\\ref{fig:toy_round_to_acc} and Fig.~\\ref{fig:toy_round_time}, we know that the source of the difference does not stem from the number of rounds used but the making span of each round. As all clients have to proceed at the same speed as the slowest clients in synchronous training, \\textit{Straggler} features lower system utility than \\textit{Baseline} does due to the presence of slow clients, rendering worse time-to-accuracy. Similarly, the disparity of clients' capabilities is inevitable in FL practice, which also complicates the design of system-level optimizations as mentioned in~\\cref{sec:background_challenge_optimality}.", "cites": [580, 582], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 2.5, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes the key concepts of statistical and system utility from the cited papers, integrating them into a coherent case study to highlight how data distribution and client computation speed impact time-to-accuracy. It provides some abstraction by framing these as broader challenges in FL optimization. However, the critical analysis is limited, as the section primarily describes the case study findings without evaluating or contrasting the methodologies or limitations of the cited works."}}
{"id": "1485b3df-12ae-4a59-a7c0-b7477f1b132f", "title": "What is beyond the Scope~\\label{sec:background_problem_beyond", "level": "subsubsection", "subsections": [], "parent_id": "e9b69d5b-e1b1-49ab-9b47-79a64bec7fd4", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Background, Problem and Challenges"], ["subsection", "System Optimization: The Problem\\label{sec:background_problem"], ["subsubsection", "What is beyond the Scope~\\label{sec:background_problem_beyond"]], "content": "}\nWe emphasize that the solutions discussed in this survey operate at the \\textit{system-level}. Therefore, the following directions of work are excluded from discussion, though they could effectively improve the statistical and/or system efficiency in synchronous federated training:\n\\begin{itemize}\n    \\item Hardware updates, e.g., adapting programmable switches to enjoy the communication efficiency brought by in-network aggregation~.\n    \\item Security mechanisms, e.g., employing robust aggregation methods to protect the statistical utility from being impaired by model poisoning attacks~.\n    \\item Paradigm innovations, e.g., (1) letting clients fit different sets of model parameters by personalization~ or models of heterogeneous architectures through knowledge distillation~ to tackle data heterogeneity (a concept mentioned in~\\cref{sec:background_challenge_optimality}), (2) allowing clients to exchange data representations for realizing prototype learning~ to reduce communication burdens, or (3) permitting clients to send encoded versions of local datasets to the server to reduce the computational complexity~.\n    \\item Optimizations on upstream parts of the FL pipeline, e.g., searching for neural architectures that yield better predictive accuracy~.\n\\end{itemize}", "cites": [593, 595, 596, 600, 598, 599, 594, 592, 7038, 597], "cite_extract_rate": 0.625, "origin_cites_number": 16, "insight_result": {"type": "descriptive", "scores": {"synthesis": 2.0, "critical": 2.0, "abstraction": 2.0}, "insight_level": "low", "analysis": "The section provides a list of topics beyond the system-level scope of the survey, citing relevant papers for each category. However, it does not synthesize or integrate the cited papers into a broader narrative, nor does it offer critical evaluation or comparison of these works. The abstraction is limited, with only surface-level categorization without deeper insight into overarching principles or trends."}}
{"id": "e9339976-85db-45bd-8a4a-f9969c5967b9", "title": "On the Optimality of a Solution\\label{sec:background_challenge_optimality", "level": "subsubsection", "subsections": [], "parent_id": "a9660a24-bfd4-4549-8a91-101fb1f84585", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Background, Problem and Challenges"], ["subsection", "What Makes It Hard: The Challenges\\label{sec:background_challenge"], ["subsubsection", "On the Optimality of a Solution\\label{sec:background_challenge_optimality"]], "content": "}\nFirst, the information needed in decision-making may be \\textit{outdated or even unavailable}. For example, to estimate a client's system utility, it is common to refer to its most recent response latency~. However, due to the dynamics over time, such information may not accurately reflect the client's current status. There also exists a cold-start issue, where we are unaware of a client's system capabilities until its first participation. As for estimating a client's statistical utility, the amount of available information is further limited by privacy concerns. According to the recent FL literature, exploratory attacks such as property inference~, membership inference~, and data reconstruction~ can be made possible with model updates. As such, even exposing model updates can discourage clients from participation, let alone inquiring about their data distributions or even raw data~. Note that the uncertainty in clients' statistical utility and system utility can be accumulated over time.\nEven given a holistic view of the environment, the problem remains hard due to the \\textit{coupled nature} of statistical utility and system utility. Intuitively, improving system utility is equivalent to minimizing the average resource consumption (e.g., time or bandwidth) per task unit. On the other hand, reducing the resources invested in a task unit inevitably downgrades the quality of the outcome (e.g., statistical utility) as long as no resource is redundant. To exemplify, by constantly picking the fastest clients in client selection, the average duration of each round indeed decreases, whereas the number of rounds taken to target accuracy may be increased as well when other clients' data are under-represented in the global model. Another example can be found in model compression. To improve communication efficiency, a client can send only an important subset of model updates by sparsification~, or a low-bit representation of them by quantization~. Although the per-round communication duration can be significantly reduced by adopting a higher compression ratio, the convergence has to take more rounds to occur due to the loss of arithmetic precision.\nThe problem is further complicated by \\textit{client heterogeneity.} Federated training involves tens to potentially millions of clients, each of which intrinsically differs from one another in the following three aspects: \n\\begin{itemize}\n    \\item \\textit{Resource Heterogeneity}: due to the variability in hardware specifications and system-level constraints, clients in federated training typically possess different capabilities in computation (CPU/GPU/NPU, memory, and storage), communication (connectivity and bandwidth) and power (battery level and lifespan)~. These types of heterogeneity complicate the optimization of the overall system utility. For example, merely improving the communication speed does not necessarily lead to shorter end-to-end latency, especially when the straggler is bottlenecked by the computation~.\n    \\item \\textit{Data Heterogeneity}: as the training datasets of clients are typically generated based on their local activities and contexts, they are not IID. More specifically, clients' datasets mainly differ in two aspects\\footnote{See a more complete categorization of non-IID scenarios in \\cref{sec:cornerstone_benchmarking_datasets}.}: (1) sample quantity (i.e., the number of data samples), and (2) label partition (i.e., the distribution of data labels)~. As a result, not all of them are representative of the population distribution. In case we do not include all the clients in the federation, optimization for statistical utility has to additionally account for such heterogeneity.\n    \\item \\textit{State Heterogeneity}: as observed from real-world traces~, the available slots of mobile device clients vary significantly in temporal distribution due to different user behaviors (e.g., screen locking or battery charging). Therefore, in each round, there can be different sets of candidate clients to choose from, as well as different client drop-out outcomes. On top of the non-IID distribution of clients' data, this type of heterogeneity further complicates statistical utility optimization. Nevertheless, in the cross-silo settings, it may be less of a concern due to the stable and dedicated nature of clients' computing power~.\n\\end{itemize}\nLast but not least, it is \\textit{infeasible to search through the entire configuration space} for the global optimum. On the one hand, the space is prohibitively large, as a federated training task typically spans $10^1$--$10^6$ users and $10^2$--$10^4$ rounds~, wherein each phase of a round (\\cref{sec:background_federated}) has multiple configurable hyperparameters and alternative policies (e.g., client selection choices in the selection phase, or the number of local steps in configuration phase). On the other hand, most of the online decisions are made on the critical path of the task, meaning that the time spent on working out a solution also counts towards the end-to-end runtime performance, the very objective of the optimization. As a result, it is desirable to be guided by efficient and effective heuristic algorithms, especially balancing the exploration and exploitation efforts made in the configuration space.", "cites": [3469, 618, 617, 608, 591, 605, 602, 603, 606, 613, 616, 612, 580, 607, 610, 614, 604, 583, 611, 615, 601, 4338, 609], "cite_extract_rate": 0.92, "origin_cites_number": 25, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.5}, "insight_level": "high", "analysis": "The section synthesizes information from multiple papers to explain the trade-offs between system and statistical utility in federated learning, and how client heterogeneity complicates optimization. It offers a critical perspective by discussing limitations like information unavailability and cold-start issues. The section abstracts broader challenges such as configuration space complexity and the inherent tension between efficiency and model quality, providing a conceptual framework for understanding FL optimization."}}
{"id": "b7818ca2-24b7-43f4-a5b2-dddedbcf0e1e", "title": "On the Practicality of a Solution\\label{sec:background_challenge_practicality", "level": "subsubsection", "subsections": [], "parent_id": "a9660a24-bfd4-4549-8a91-101fb1f84585", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Background, Problem and Challenges"], ["subsection", "What Makes It Hard: The Challenges\\label{sec:background_challenge"], ["subsubsection", "On the Practicality of a Solution\\label{sec:background_challenge_practicality"]], "content": "}\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=0.9\\textwidth]{skeleton}\n    \\caption{Taxonomy of the approaches discussed in \\cref{chap:prior}.}\n    \\label{fig:opt}\n\\end{figure*}\nApart from navigating the performance-accuracy-privacy trade-off, the design process of a practical optimization solution should mitigate the accompanying side-effects on other aspects such as the loss of \\textit{robustness} to attacks and failures~. For example, to evaluate the statistical utility of a client, the server may require it to report the loss values generated in local training~. However, a malicious or free-rider client may intentionally respond with arbitrary values in the hope of messing with the orchestration or reaping the benefits of the federation without making solid contributions. As such backdoors are introduced by the optimization solution, the developers should take charge of eliminating the undesirable exploitations of these security loopholes. Other possible concerns that may arise as a result of a system optimization solution include but are not limited to \\textit{fairness} (e.g., whether participant bias is introduced in the solution), \\textit{generality} (e.g., whether the solution applies to diverse tasks), and \\textit{ease of deployment} (e.g., whether the solution can be implemented with moderate engineering efforts). In other words, a mature system optimization solution should not only improve the time-to-accuracy performance by enhancing statistical and system utility but also minimize the adverse impacts on other aspects that federated training also values in practice.", "cites": [580, 591], "cite_extract_rate": 1.0, "origin_cites_number": 2, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from the cited papers by discussing the practical considerations of system optimization in FL, such as robustness and fairness. It provides a critical perspective by highlighting potential security issues and trade-offs that may result from optimization strategies. The abstraction is moderate, as it generalizes concerns like robustness and generality but does not introduce a novel framework or meta-level insights."}}
{"id": "6750018e-2b8c-43be-8977-a671addc47f2", "title": "Optimizing the Selection Phase\\label{sec:prior_selection", "level": "subsection", "subsections": ["2c47120e-34e5-44cd-b940-4eb997711491", "c9c46878-34a7-4fcb-8626-cb64857d9ade"], "parent_id": "a91a3aeb-ed4e-41b8-921a-e2c112807c50", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Recent Optimization Approaches\\label{chap:prior"], ["subsection", "Optimizing the Selection Phase\\label{sec:prior_selection"]], "content": "}\nDue to the (potentially) large population size and the heterogeneity across clients, the effectiveness of the used participant selection algorithm plays a critical role in the time-to-accuracy performance in federated training. However, the state-of-the-practice system still relies on randomly picking participants~, which inevitably leads to waste of resources and suboptimal convergence speed. In response, there is an array of work to guide the selection, which can be roughly categorized by the target utilities that they improve.", "cites": [590], "cite_extract_rate": 1.0, "origin_cites_number": 1, "insight_result": {"type": "descriptive", "scores": {"synthesis": 1.5, "critical": 1.0, "abstraction": 1.5}, "insight_level": "low", "analysis": "The section briefly introduces the importance of the selection phase in federated learning and mentions the current practice of random selection, but it does not synthesize or integrate ideas from the cited paper effectively. It lacks critical evaluation or comparison of approaches and fails to generalize or abstract broader patterns or principles."}}
{"id": "2c47120e-34e5-44cd-b940-4eb997711491", "title": "Partial Optimization Attempts\\label{sec:prior_selection_either", "level": "subsubsection", "subsections": [], "parent_id": "6750018e-2b8c-43be-8977-a671addc47f2", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Recent Optimization Approaches\\label{chap:prior"], ["subsection", "Optimizing the Selection Phase\\label{sec:prior_selection"], ["subsubsection", "Partial Optimization Attempts\\label{sec:prior_selection_either"]], "content": "}\nThis line of work does not consider the interplay between statistical utility and system utility. Instead, they mainly focus on lifting either utility while leaving the other one ignored or controlled to a limited extent.\n\\PHM{Statistics-Oriented.} To approach the convergence rate in centralized settings where the data is IID, CSFedAvg~ advocates that clients with a lower degree of non-IID data should participate more often. To this end, the authors propose weight divergence to capture the non-IID degree of data owned by a client. More precisely, it measures the normalized Euclidean distance between a client's model and the reference model trained by the server with auxiliary IID data. According to the 500-client simulation over CIFAR-10 and Fashion MNIST, CSFedAvg reduces the time-to-accuracy by up to 4.0$\\times$ and 2.7$\\times$, respectively, compared to random selection.\nBesides heuristic methods, some researchers use reinforcement learning (RL) algorithms to learn which clients to select in the presence of data heterogeneity. For example, FAVOR~ seeks to reduce the number of rounds to reach a target accuracy with a deep Q-learning network (DQN)~. To capture each client's statistical characteristics, it takes the low-dimension representations of local models as the RL states. Compared to random selection, FAVOR can reduce the communication rounds by up to 49\\% in three image classification tasks. On the other hand, the training overhead for the RL agent may become an obstacle to FAVOR's real-world applications. Specifically, it is reported to take more than 100 episodes to train an agent suited for a specific learning task, which could be prohibitive as each episode corresponds to an entire FL process, i.e., training a global model from scratch for reaching a target accuracy.\n\\PHM{System-Oriented.} In synchronous training, clients with the lowest system utility bottleneck the speed of a federation round. A straightforward way to bound the time usage is setting a deadline for randomly selected clients' to report updates and ignoring any update submitted after the deadline. To avoid waste of computing resources, FedCS~ takes a step further by proactively selecting a set of clients whose participation is not likely to miss the deadline according to latency estimation results. As there can be multiple eligible sets, FedCS further favors the solution with the largest scale of participation, which reduces part of the loss in statistical utility. Technically, the whole problem is formalized as a complex combinatorial optimization, and the authors resort to a greedy algorithm for efficient approximation. As indicated in their 1000-client simulation, FedCS outperforms FedLim (modified FedAvg with per-round deadlines imposed) by up to 1.2$\\times$ and 1.8$\\times$ in the time-to-accuracy when training over the non-IID CIFAR-10 and Fashion MNIST datasets, respectively.", "cites": [619, 620], "cite_extract_rate": 0.5, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.8, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section effectively synthesizes two distinct approaches—statistics-oriented and system-oriented client selection—by categorizing and contrasting their goals and methodologies. It critically evaluates the trade-offs and limitations, such as the high training overhead of RL in FAVOR and the computational inefficiency of FedCS's greedy approach. The abstraction is strong in highlighting broader challenges (e.g., statistical vs. system utility trade-offs) and patterns in partial optimization strategies, though it stops short of forming a meta-level framework."}}
{"id": "c9c46878-34a7-4fcb-8626-cb64857d9ade", "title": "Co-Optimizing Statistical/System Utility\\label{sec:prior_selection_both", "level": "subsubsection", "subsections": [], "parent_id": "6750018e-2b8c-43be-8977-a671addc47f2", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Recent Optimization Approaches\\label{chap:prior"], ["subsection", "Optimizing the Selection Phase\\label{sec:prior_selection"], ["subsubsection", "Co-Optimizing Statistical/System Utility\\label{sec:prior_selection_both"]], "content": "}\nGiven the coupled nature of clients' system utility and statistical utility, it is more desirable to navigate the sweet point of jointly maximizing both of them.\n\\PHM{Coarse-Grained.} TiFL~ first considers increasing the system utility. To that end, it divides clients into different tiers based on the observed runtime performance, and at each round only selects clients from the same tier for mitigating the waste of resources due to idle waiting for stragglers. To reduce the average iteration span, it also limits the number of times a (slow) tier can be selected. On top of that, the statistical utility is respected by prioritizing tiers with lower testing accuracy whenever there is more than one electable tier. Compared with FedCS, TiFL bears some resemblance in limiting the participation of less capable clients, while being more aware of the statistical utility. As reported in a 50-client cluster with 5 client tiers, TiFL achieves an improvement over random selection by up to 3$\\times$ speedup in overall training time and by 6\\% in accuracy.\n\\PHM{Fine-Grained.}  Compared to TiFL, Oort~ reconciles the demand for enhancing both system utility and statistical utility in finer granularity. Specifically, it associates each client with a continuous score and prioritizes those clients with higher scores. The score is meant to be a principled measurement of both the statistical utility (determined by the training loss) and the system utility (estimated from historical response latency). As some components of the score cannot be known in advance until the corresponding client's first participation, or cannot be guaranteed to be stable due to the client's runtime dynamics, the score estimation process is modeled as a Multi-Armed Bandit (MAB) problem and tackled by the Upper Confidence Bound (UCB) algorithm~. Apart from the scoring backbone, Oort also aims to address other practical issues like staleness and robustness. Oort was evaluated on a 1300-client GPU cluster with realistic datasets and simulation on the client heterogeneity. Compared to random selection, it reduces the training time by up to 14$\\times$ and improves model accuracy by up to 9.8\\%.\nBesides the UCB algorithm, researchers also experiment with more sophisticated RL methods to cherrypick participants. Notably, AutoFL~ learns to select participants and execution targets for each individual based on the Q-Learning algorithm~. To achieve efficient FL execution, it identifies RL states that are critical to energy efficiency, convergence time, and accuracy. It also defines RL rewards that track the energy consumption of clients and model accuracy. Compared to random selection, AutoFL is reported to improve the average FL energy efficiency by up to 4.3$\\times$, while also exhibiting better training accuracy. Similar to FAVOR (mentioned in \\cref{sec:prior_selection_either}), however, training the Q-Learning model to converge needs multiple episodes, each of which corresponds to an entire FL training process. Its practicality could thus be challenged whenever offline training is infeasible or costly.", "cites": [7262, 580, 585], "cite_extract_rate": 0.6, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the key ideas from TiFL, Oort, and AutoFL, highlighting the progression from coarse-grained to fine-grained selection and the integration of statistical and system utility. It critically evaluates the practicality and limitations of each approach, particularly noting the training cost of RL-based methods. The discussion abstracts beyond individual papers by framing participant selection as a broader problem of balancing utility dimensions and modeling it as an MAB problem."}}
{"id": "0b02d7a9-809d-41bd-95d3-97624f811da6", "title": "Model Update Size Reduction\\label{sec:prior_configuration_size", "level": "subsubsection", "subsections": [], "parent_id": "ded2658f-08bb-4199-8696-66ba4578329c", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Recent Optimization Approaches\\label{chap:prior"], ["subsection", "Optimizing the Configuration Phase\\label{sec:prior_configuration"], ["subsubsection", "Model Update Size Reduction\\label{sec:prior_configuration_size"]], "content": "}\nPrior arts of model update size reduction mainly fall into three camps: quantization, sketching, and sparsification. Ahead of the emergence of FL, the exploration of these directions has already been initiated in the context of traditional distributed learning. While their communication merits are mostly reproducible in FL, they also face new challenges due to the privacy regulations and client heterogeneity, which we will also point out hereafter.\n\\PHM{Quantization.} Quantization converts each scalar in a model update to its low-bit representation which takes up less space. While quantization has already gained its fame in traditional distributed learning and we refer the readers to dedicated surveys like~ for more details, here we only introduce the most representative work. As the first quantization work in model training with rigorous convergence proof, QSGD~ performs unbiased quantization with standard random dithering, a technique borrowed from image processing. Since its birth, related works have been emerging with more aggressive quantization bit-widths and more appealing empirical performance. For example, TernGrad~ advocates using only ternary values (0, $\\pm$1) in the uplink direction, while signSGD~ can use only binary signs ($\\pm$) in both uplink and downlink communication. It is worth mentioning that a popular technique in tackling the precision loss brought by quantization is error feedback, whose basic idea is to accumulate the previous quantization errors and compensate for them in the current round. Leveraging this technique, ECQ-SGD~ performs consistently better than QSD in terms of both convergence speed and accuracy, while EF-SGD~ also achieves a narrower generalization gap from centralized training compared to signSGD.\nDespite their generality, there are some practical concerns about applying these general quantization strategies to FL due to privacy constraints and client heterogeneity. For example, determining the clipping threshold for quantization needs to exploit the knowledge about its inputs (i.e., local model updates) for reducing the induced error as in dACIQ~. However, an FL client does neither possess a priori knowledge of others' model updates nor should it request these sensitive values. To work out a globally applicable clipping threshold, we may need to share some less sensitive information (e.g., the maximum and minimum values in local updates) across clients for threshold estimation as in BatchCrypt~. Still, whether such a circumvention guarantees robust estimation and immunity to privacy attacks remains an open question.\n\\PHM{Sketching.} Existing quantization approaches assume the input values follow a certain distribution (e.g., uniform or bell-shaped), which may not always be the case in model updates~. To be more general, some researchers introduce sketching methods where some memory-saving data structures are used to approximate the exact distribution of model update values in a single processing pass over the values. For example, SketchML~ utilizes a quantile sketch method to generate a non-uniform mapping from gradient values to low-bit integers. SketchML achieves empirical success such as decreasing the gradient size by around 7$\\times$ and is the first effort to introduce sketching for compressing model updates in ML training. Similar to quantization, sketch algorithms can also make use of error feedback techniques to efficiently amend the errors induced by the approximation, as demonstrated in SketchedSGD~ and FetchSGD~. There is also sketching practice that compresses auxiliary variables apart from model updates, such as sketching clients' momenta and per-coordinate learning rates as in~.\n\\PHM{Sparsification.} While quantization and sketching operate at the precision level in terms of model size reduction, sparsification operates at the coordinate level. Specifically, sparsification allows each client to transmit only a sparse subset of its model updates, while the rest are accumulated and incorporated into future training. Technically, the sparsified gradient is obtained by first performing element-wise multiplication on the original gradient with a certain 0/1 mask and then discarding zero elements. The mask is typically randomly generated as in~, while another commonly used variant is the top $s$\\% scheme where 1 is given to the coordinates that rank top $s$\\% in absolute magnitude and 0 otherwise~. The top $s$\\% method is reported to reduce network traffic by up to three orders of magnitude, while still preserving model quality~.\nWhile similar cost savings are transferable to plaintext FL, it is unclear whether sparsification can be further compatible with cryptographic techniques that are widely adopted for privacy enforcement in FL. For example, apart from the uplink model updates, it is also desirable to sparsify the downlink global update for fully releasing the potential for communication improvement. However, implementing the downlink sparsification may not be feasible when the server is not aware of the plaintext values of the aggregated update as a result of the applied Secure Multi-Party Computation (SMPC)~ or Homomorphic Encryption (HE)~ techniques.\nIt is noteworthy that as quantization (or sketching) and sparsification are orthogonal to each other, they can be combined to reap the most benefits in terms of model size reduction~.", "cites": [618, 617, 626, 628, 624, 622, 606, 613, 616, 621, 625, 610, 611, 8366, 627, 615, 601, 623, 4338], "cite_extract_rate": 0.7307692307692307, "origin_cites_number": 26, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 4.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes information from multiple papers on model update size reduction in FL, integrating quantization, sketching, and sparsification approaches into a coherent narrative. It provides critical analysis by discussing limitations such as privacy constraints and compatibility with cryptographic methods. The section also abstracts these methods into broader categories and highlights their applicability and challenges in the FL context, including orthogonal combinations and open research questions."}}
{"id": "77437726-b7b4-48cb-ac0d-7210459652b2", "title": "Synchronization Frequency Reduction\\label{sec:prior_configuration_freq", "level": "subsubsection", "subsections": [], "parent_id": "ded2658f-08bb-4199-8696-66ba4578329c", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Recent Optimization Approaches\\label{chap:prior"], ["subsection", "Optimizing the Configuration Phase\\label{sec:prior_configuration"], ["subsubsection", "Synchronization Frequency Reduction\\label{sec:prior_configuration_freq"]], "content": "}\nAt its core, the reduction in the synchronization frequency is achieved by identifying and precluding redundant synchronization efforts. This can be operated at different granularities ranging from clients, layers to individual parameters in a model update.\n\\PHM{Client-Level.} The importance of an entire model update is usually measured by some numerical features. In the most intuitive form, a model update in Gaia~ is considered significant if its magnitude relative to the current value $\\mid \\frac{Update}{Value} \\mid$ exceeds a specific threshold such as 1\\%. While the magnitude may serve as a good indicator for how data center learning performs, it does not work in FL where determining an appropriate threshold is hard due to clients' heterogeneity. As such, some researchers propose to involve the comparison with some reference points for more robust measurement of the importance. For example,~ tracks the Euclidean distance between the local model and a reference model, while CMFL~ focuses on the number of coordinates with the same sign in the local model and the most recent global model.\n\\PHM{Layer-Level.} Apart from considering a model update as a whole, another line of work tries to reduce the synchronization frequency on a layer basis. A representative work done in this direction is TWAFL~ where the model aggregation is conducted layer-wise. As observations made in deep neural network (DNN) fine-tuning~, shallow layers in a DNN learn general features across different datasets while deep layers learn ad hoc ones. TWAFL hence proposes to update shallow layers more frequently than deep ones as they are more responsible for the overall quality of the global model.\n\\PHM{Parameter-Level.} Some industrial practitioners also consider whether to synchronize for each round at the level of individual parameters. Noticing that each parameter usually evolves in a transient-then-stable manner, i.e., it first varies drastically and then settles down around a certain value with slight oscillation, APF~ proposes to stop synchronizing those parameters whose evolution has reached a stationary phase.", "cites": [630, 629], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.0, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes the cited works effectively by categorizing synchronization frequency reduction strategies at different granularities—client, layer, and parameter levels. It abstracts general principles such as the transient-then-stable behavior of parameters and the role of shallow vs. deep layers in DNNs. While it provides some critical context by highlighting limitations (e.g., difficulty in setting thresholds in FL), it could offer more explicit comparison or deeper critique of the methods."}}
{"id": "75004bfc-b89a-4773-8188-0896026459ce", "title": "Training Latency Reduction\\label{sec:prior_configuration_latency", "level": "subsubsection", "subsections": [], "parent_id": "ded2658f-08bb-4199-8696-66ba4578329c", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Recent Optimization Approaches\\label{chap:prior"], ["subsection", "Optimizing the Configuration Phase\\label{sec:prior_configuration"], ["subsubsection", "Training Latency Reduction\\label{sec:prior_configuration_latency"]], "content": "}\nA client's training latency is determined by both its computational workload and resource capabilities. While the latter cannot be altered, the former still leaves room for optimization innovations. We discuss one major line of such efforts.\n\\PHM{Load Balancing.} Given the variations in computing power and data volume, clients may not finish the training process at the same time. To mitigate the resulting straggler effects,  suggest balancing the amount of training data across clients. Specifically, they turn to RL techniques for determining the optimal number of data units used in a communication round for each participant, intending to minimize the time and energy consumption and maximize the volume of involved data. While these solutions can reduce the round latency, the number of rounds needed for convergence does not necessarily remain unchanged because they do not account for data heterogeneity and thus the contribution of slow clients with important data could be throttled. To jointly consider heterogeneous system speed and data distribution during load balancing,  carefully formulates an optimization problem where each client is associated with a weight that diminishes exponentially with the disparity between its number of labels and that of the population. Compared to even data allocation, this work manages to reduce the end-to-end time-to-accuracy.\nLoad balancing can also be achieved by varying the number of optimization steps or the complexity of local models. For example, FedProx~ balances the system load across clients by formulating an inexact learning problem that allows variable steps of local solvers. On the other hand, HeteroFL~ assigns sub-models with different widths of hidden channels to clients so that clients with fewer capabilities can train smaller sub-models. All sub-models share the same model architecture, and thus normal model aggregation is still possible. The authors empirically show that the quality of the global model trained with heterogeneous sub-models is comparable to that trained with full-sized local models.\nApart from the computational load, it is sometimes also beneficial to balance the communication load across clients, especially when the network conditions are complicated as in wireless connections. For example, targeting a mobile edge computing (MEC) scenario where Time Division Multiple Access (TDMA) is implemented,  optimizes both the data batch size and uplink/downlink frame time slots for each client to achieve the maximum learning efficiency. In addition to coping with CPU computing, the authors further extend the optimization problem to the scenario where devices are equipped with GPUs for training.", "cites": [631, 632], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 6, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple approaches to training latency reduction, linking them under the theme of load balancing and explaining how they address system and data heterogeneity. It provides critical analysis by pointing out limitations, such as not considering data importance in some methods. The abstraction is strong, as it generalizes across methods to highlight optimization strategies and their impacts on FL training efficiency."}}
{"id": "c1657bca-2a2d-4dc4-a8d5-1e81d9a7c3f6", "title": "Training Round Reduction\\label{sec:prior_configuration_round", "level": "subsubsection", "subsections": [], "parent_id": "ded2658f-08bb-4199-8696-66ba4578329c", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Recent Optimization Approaches\\label{chap:prior"], ["subsection", "Optimizing the Configuration Phase\\label{sec:prior_configuration"], ["subsubsection", "Training Round Reduction\\label{sec:prior_configuration_round"]], "content": "}\nIn the settings with heterogeneous data, more local computation in a communication round does not necessarily lead to fewer numbers of rounds for reaching satisfying optima~. Thus, adopting heterogeneity-aware techniques such as adaptive optimization and bias reduction can help guarantee the convergence speed in FL practice.\n\\PHM{Optimizer State Synchronization.} It is common practice for first-order moment optimization to apply momentum to dampen oscillations~. However, in the federated settings, if the clients' momenta are separately updated, they may deviate from each other due to non-IID data distributions. Thus, there are researchers proposing optimizer state synchronization frameworks where clients' optimizer states are synchronized by the server periodically. PR-SGD-Momentum~ is aligned with this direction and also gives proof of the linear speedup of convergence w.r.t. the number of workers. FedAC~ also applies momentum at clients with periodic synchronization, while it is proven to obtain the same linear speedup property with asymptotically fewer rounds of synchronization. MFL~ is another similar idea with theoretical guarantees, but it focuses on accelerating deterministic gradient descent (DGD) instead of SGD, unlike the previous two studies.\n\\PHM{Client Bias Reduction.} Due to data heterogeneity, clients' model updates can be biased towards the minima of local objectives, known as ``client drift'' in the literature~, which hinders the convergence of the global model. To reduce the variance across clients, a natural idea is to regularize local objective functions for minimizing the drift. For example, assuming bounded dissimilarity between local functions, FedProx~ limits the Euclidean distance between local models and the global one by regularizing the square of the distance. FedDANE~ uses the same regularization term, while it formulates the other part of the objective function following the Distributed Approximate NEwton (DANE) method~ in classic distributed optimization. Despite having a similar theoretical convergence rate as FedProx, FedDANE underperforms FedProx in practice where the data heterogeneity is high and the participation rate is low, suggesting a discrepancy between theory and practice which needs further investigation. The objective function in FedDyn~ also considers the combination of a linear term and an L2 regularizer, with a different linear term which is formulated to align the empirical loss surfaces of clients. In theory, FedDyn ensures that the consensus point of model convergence across clients will be consistent with the global stationary solution as long as local models converge, regardless of the degree of data heterogeneity.\nApart from regularizing local objectives, variants in clients' local updates can also be reduced by leveraging the idea of control variates borrowed from the convex optimization literature. For example, in SCAFFOLD~, each of the clients and the server maintains a control variate, and at each local step, a client de-biases its local updates with two control variates: one of its own and the other broadcast by the server. SCAFFOLD converges provably faster than FedAvg~ without any assumption made on the client selection or data heterogeneity. MIME~ considers a similar idea but makes a different choice on the specific definition of control variates.\nWhile the use of control variates requires persistent client states, there exists another line of work that works for stateless clients: posterior averaging. Instead of approaching FL as optimization, this line of work formulates the problem as a posterior inference one. Compared to traditional federated optimization, posterior inference can benefit from an increased amount of local computation without risking stagnating at inferior optima. FedPA~ instantiates this idea with an efficient algorithm to conduct federated posterior inference with linear computation and communication costs.", "cites": [634, 638, 635, 640, 636, 639, 641, 633, 582, 632, 637], "cite_extract_rate": 0.7857142857142857, "origin_cites_number": 14, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes multiple papers on training round reduction in FL, grouping them under sub-themes like optimizer state synchronization, client bias reduction, and posterior averaging. It critically compares methods such as FedDANE, FedProx, and SCAFFOLD, highlighting discrepancies between theory and practice. The abstraction is strong as it identifies broader patterns, such as how different approaches handle data heterogeneity and client drift."}}
{"id": "542ef1b6-edee-4a8a-bd3d-68e0c8f5c762", "title": "Aggregation Latency Reduction\\label{sec:prior_reporting_round", "level": "subsubsection", "subsections": [], "parent_id": "d6884228-aa74-48a6-9eb1-ee9a7ae15668", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Recent Optimization Approaches\\label{chap:prior"], ["subsection", "Optimizing the Reporting Phase\\label{sec:prior_reporting"], ["subsubsection", "Aggregation Latency Reduction\\label{sec:prior_reporting_round"]], "content": "}\nCompared to local training in the configuration phase, model aggregation involves less intensive computation. However, its latency can still be salient because (1) large-scale participation can put pressure on the communication, and (2) the deployment of security methods can complicate the computation. We hereafter introduce the respective optimization efforts in the literature.\n\\PHM{Hierarchical Aggregation.} The downsides of the traditional two-layer (server-client) system include (1) instability: the network bandwidth may be slow or unpredictable, especially in public networks and/or under geo-distributed settings; (2) risk of scalability: the cloud server may suffer from network congestion when concurrently receiving too many local updates; (3) heterogeneity: the straggler effects could be exacerbated by imbalanced network bandwidth. \nTo address these issues, some researchers resort to a hierarchical design of model aggregation by introducing an extra level of edge servers, each of which is typically responsible for a small number of clients with proximity. For instance, in HierFAVG~, after a fixed number of local updates on clients, each edge server aggregates its own clients' models. Subsequently, after another fixed interval of edge aggregation, the cloud server aggregates all the edge servers' models. It is proven that HierFAVG still guarantees convergence, and empirical studies with synthetic FL datasets show that it reduces the time-to-accuracy by up to 2.7$\\times$ in a simulated cloud-edge-client environment. A concurrent work HFL~ also considers a similar design, while it does not attach theoretical analysis on its convergence behaviors. HybridFL~ further extends this primary design with two ideas: (1) quota-triggered edge-level aggregation: edge nodes stop waiting for more local updates once receiving a sufficient number of them; and (2) immediate cloud aggregation: cloud-level aggregation is conducted right after the edge-level one is completed. This decouples each pair of interactions (i.e., cloud-edge and edge-client), thereby further mitigating the impact of client drop-out and stragglers.\nDespite changing the aggregation rules, the hierarchical designs mentioned above still focus on establishing the convergence to a single global model, which does not deviate from the learning paradigm discussed throughout this survey. On the other hand, there also exist other formulations of collaborative learning such as clustered FL where clients are assigned to different groups and aggregation takes place within a group . As they aim to build personalized models for each group of clients, they are considered paradigm innovations to standard FL and thus fall out of the scope of this survey (\\cref{sec:background_problem}).\n\\PHM{Lightweight Privacy-Preserving Aggregation.} As mentioned in \\cref{sec:background_challenge_optimality}, uploading model updates in the clear may be vulnerable to exploratory attacks which plague clients' privacy. Therefore, model aggregation is preferably safeguarded by cryptographic techniques, which inevitably induces extra computation and communication overhead. \nOne line of this work leverages secure multiparty computation (SMPC). One of the most impactful works is Google's secure aggregation (SecAgg) protocol, where pseudorandom masks are used for data confidentiality and Shamir's secret sharing scheme~ is used to accommodate client dropout~. It has provable rigorous privacy guarantees under both passive and active adversary models. Given its quadratic communication overhead during secret sharing, SecAgg has inspired many other SMPC protocols with improved performance. For example, SecAgg+~ optimizes SecAgg in both communication and computation by replacing the complete graph with a random sparse one of logarithmic degree. TurboAgg~, on the other hand, attempts to optimize SecAgg through the use of multi-group circular topology, additive secret sharing, and Lagrange coding. Compared to SecAgg+, TurboAgg incurs a slightly higher overhead in the asymptotic sense for both communication and computation, and only focuses on tackling honest-but-curious adversaries. While all of the above schemes rely on Shamir's scheme for sharing clients' secrets, there also exist other secret sharing techniques that offer different trade-offs across performance, privacy, and dropout resilience, e.g., encoding using Maximum Distance Separable matrices as in LightSecAgg~, or Fast Fourier Transform as in FastSecAgg~.\nBesides SA, Homomorphic Encryption (HE) is another commonly used privacy-preserving aggregation technique that comes with prohibitively high message inflation and runtime overhead. In response, BatchCrypt~ implements an end-to-end solution for batching multiple plaintexts into one large plaintext so that HE-related operations can be performed in a data-parallel manner. BatchCrypt is shown to speed up the training by 23$\\times$-93$\\times$ compared to plain Paillier~ (a prevalent variant of HEs), but it still leaves the message inflation suboptimal and is incompatible with top s\\% sparsification approaches~. Instead of optimizing traditional HE schemes by batching, FLASHE~ proposes a lightweight HE scheme that is tailored for cross-silo FL. It induces negligible ($\\leq6\\%$) computational overhead and no network communication overhead compared to plaintext FL for staying symmetric. Its performance is also optimized when interacting with sparsification techniques.", "cites": [643, 646, 644, 621, 645, 647, 7039, 642], "cite_extract_rate": 0.5333333333333333, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.3, "critical": 3.8, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes multiple works on hierarchical aggregation and privacy-preserving techniques, highlighting their design choices, trade-offs, and performance improvements. It critically evaluates different methods (e.g., SMPC vs. HE) and their limitations, such as overhead and compatibility with sparsification. The section abstracts key principles like the impact of topology and secret sharing schemes on scalability and privacy, while distinguishing between paradigm innovations and standard FL approaches."}}
{"id": "cc07a19d-bfd0-4b7a-9057-e912fa55b00d", "title": "Adaptive Aggregation\\label{sec:prior_reporting_end", "level": "subsubsection", "subsections": [], "parent_id": "d6884228-aa74-48a6-9eb1-ee9a7ae15668", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Recent Optimization Approaches\\label{chap:prior"], ["subsection", "Optimizing the Reporting Phase\\label{sec:prior_reporting"], ["subsubsection", "Adaptive Aggregation\\label{sec:prior_reporting_end"]], "content": "}\nIn FedAvg~, the de facto standard aggregation method, local model updates simply get weighted by the corresponding numbers of training samples and then added up. While it guarantees convergence when even dealing with non-convex empirical risk functions in IID data settings~, it is observed to yield unstable convergence behavior or even divergence when it faces models trained with arbitrarily non-IID data. There are thus rising interests on whether the aggregation can be more adaptive w.r.t. the data heterogeneity across clients.\n\\PHM{Server-Side Optimizers.} Other than accelerating convergence with local momentum (\\cref{sec:prior_configuration_round}), there are also exploration efforts on server-side momentum. As there is originally no optimizer at the server in FL, these methods first need to generalize the existing aggregation algorithm. Specifically, at each round, instead of collecting local model weights, the server instead collects their changes and treats these changes as the ``pseudo-gradient'' for the server, which the server can use to update the global model with adaptive optimizers. FedAvgM~ initiates the empirical studies with the simplest form of momentum applied at the server, while SlowMo~ independently proposes a similar scheme and also attaches the theoretical analysis for its convergence behaviors. A recent work~ makes more sophisticated use of momentum such as adopting AdaGrad~, Adam~ and YOGI~ optimizers (which correspond to FedAdaGrad, FedAdam, and FedYOGI, respectively). It is shown that FedYOGI consistently outperforms FedAvgM in terms of validation performance for both sparse- and dense-gradient FL tasks. Server-side momentum methods feature no need for persistent states or computation burdens at the client end, making them well suited to cross-device scenarios.", "cites": [635, 8367, 582, 648, 649], "cite_extract_rate": 0.5555555555555556, "origin_cites_number": 9, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes multiple works on adaptive aggregation methods in FL, connecting them through the idea of server-side momentum and pseudo-gradients. It provides a critical comparison by highlighting the limitations of FedAvg and the relative strengths of methods like FedYOGI. While it identifies broader patterns (e.g., the role of adaptive optimizers in improving convergence), it does not fully abstract to a meta-level framework or theory."}}
{"id": "b59ba9be-d908-40e7-8000-731ab0a60a79", "title": "Measurement-Based Research\\label{sec:cornerstone_measurement", "level": "subsection", "subsections": [], "parent_id": "52489191-afca-4640-b251-831b75c284de", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Measurement and Benchmarking Tools\\label{chap:concerstone"], ["subsection", "Measurement-Based Research\\label{sec:cornerstone_measurement"]], "content": "}\nDue to the complicated interplay between statistical utility and system utility, there are a few measurement studies which dedicate to conducting thorough investigations and providing actionable implications for interested researchers.\n\\begin{itemize}\n    \\item \\textit{FLASH}\\footnote{We name it after the corresponding Github repository handle~.}~: FLASH particularly studies the impacts that heterogeneity has on both the statistical utility and system utility. This work contributes to the community mainly with three aspects: (1) it establishes a novel large-scale dataset that reflects the system and state heterogeneity among 136,000 real-world smartphones; (2) it implements and open-sources an FL platform that provides a heterogeneity-aware environment for experiments; (3) it reports comprehensive findings that stem from the authors' extensive measurements atop the platform.\n    Among the authors' numerous implications, there are two fresh findings at the time when the study is written. First, gradient compression methods (e.g., Gradient Dropping~ and SignSGD~) can hardly shorten the convergence time under heterogeneous cross-device settings. Second, advanced aggregation algorithms that overlook some aspects of heterogeneity will be less effective in realistic settings. Readers can refer to the paper for more details.\n\\end{itemize}", "cites": [601, 583, 616], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section synthesizes information from the cited papers to highlight the impact of heterogeneity on FL training, particularly in the context of gradient compression and aggregation techniques. It offers some critical insight by pointing out that these methods may not be effective in realistic, heterogeneous settings. However, the abstraction is limited to general observations rather than deeper, overarching principles."}}
{"id": "e04c90d3-4295-47a6-81b9-5f869d19382c", "title": "Training Datasets\\label{sec:cornerstone_benchmarking_datasets", "level": "subsubsection", "subsections": [], "parent_id": "271672fe-a947-4489-af72-cdac79330349", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Measurement and Benchmarking Tools\\label{chap:concerstone"], ["subsection", "Benchmarking Suites\\label{sec:cornerstone_benchmarking"], ["subsubsection", "Training Datasets\\label{sec:cornerstone_benchmarking_datasets"]], "content": "}\nThere are two prevalent categories of training datasets. One line of work is \\textit{derived from conventional ML datasets} (e.g., \\texttt{CIFAR}~, \\texttt{MNIST}~, and \\texttt{Fashion-MNIST}~). To synthesize the non-IID nature as in real FL scenarios, the data partitions in these datasets are typically formed by restricting the number of data classes each client has (e.g., partitioning by shard-based methods as in~ or latent Dirichlet allocation (LDA) processes as in~). Although the data generated in such a way are indeed non-IID, they may not perfectly represent the real-world characteristics. For instance, besides the label distribution skew, in reality, non-IID data may also involve feature distribution skew (e.g., same words with different stroke widths), same labels with different features (e.g., images of clothing vary due to regional differences) and same features with different labels (e.g., the same context mapped to different next words due to personal habits)~.\nIn contrast, the other type of datasets is \\textit{collected in real distributed scenarios} and thus naturally captures the FL features. We briefly introduce existing open-source attempts in curating such datasets as follows.\n\\begin{itemize}\n    \\item \\textit{LEAF}~: LEAF is an actively maintained project, which currently consists of 6 datasets spanning multiple applications such as computer vision (CV) (\\texttt{FEMNIST} and \\texttt{CelebA}) and natural language processing (NLP) (\\texttt{Shakespeare}, \\texttt{Reddit}, and \\texttt{Sentiment140}). Each dataset is generally formed by splitting the corresponding public dataset by the original contributors of the data samples. In other words, the non-IID nature comes from the unique behavior style of each contributor.\n    \\item \\textit{FedScale}~: Similar to LEAF, FedScale also collects realistic datasets and partitions them by unique client identification. FedScale currently includes 18 datasets (including \\texttt{iNature}, \\texttt{OpenImage}, \\texttt{Google Landmark} etc.) which span 10 FL tasks. Apart from the comprehensive coverage of tasks, FedScale has further made four contributions to the community: (1) it has the training, validation, and testing set well established; (2) it streamlines different datasets into a unified format; (3) it accounts for various participation scale from hundreds to millions of clients; and (4) it provides handy APIs for users to customize their datasets.\n    \\item \\textit{OARF}~: for a specific FL task, OARF assembles real-world datasets from different sources to realize data heterogeneity. For example, for sentiment analysis, it combines both the \\texttt{IMDB Movie Review} and \\texttt{Amazon Moview Review} datasets. In training, datasets belonging to different sources are distributed to different parties. As such, the data are split in a dataset-wise manner instead of a sample-wise one. OARF currently covers 9 tasks in CV and NLP.\n\\end{itemize}\nBesides the above systematic collection, there are also other separately maintained realistic datasets, such as \\texttt{Stackoverflow}~ and \\texttt{PERSONA-CHAT}~.", "cites": [605, 591, 8367, 633, 652, 653, 582, 650, 637, 648, 651], "cite_extract_rate": 0.7333333333333333, "origin_cites_number": 15, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section synthesizes key ideas about training datasets in FL by integrating insights from multiple papers, distinguishing between synthetic and real-world data approaches. It critically evaluates the limitations of shard-based and LDA partitioning methods, emphasizing their insufficient representation of real-world non-IID characteristics. The section abstracts from specific datasets to highlight broader patterns in how data heterogeneity is modeled in FL, suggesting a meta-level understanding of the problem."}}
{"id": "6b106f87-b95f-4ecf-a35e-e685df38e3b3", "title": "Production Systems and Simulation Platforms\\label{sec:cornerstone_benchmarking_platforms", "level": "subsubsection", "subsections": [], "parent_id": "271672fe-a947-4489-af72-cdac79330349", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Measurement and Benchmarking Tools\\label{chap:concerstone"], ["subsection", "Benchmarking Suites\\label{sec:cornerstone_benchmarking"], ["subsubsection", "Production Systems and Simulation Platforms\\label{sec:cornerstone_benchmarking_platforms"]], "content": "}\nIn addition to setting up data heterogeneity, we also need to incorporate system heterogeneity in realistic benchmarking. The most straightforward way to study FL designs with system utility borne in mind is to deploy in \\textit{production-oriented systems}. Such systems not only embed the ML backbone but also address practical problems like authentication, communication, encryption, and deployment to physical distributed environments. We sketch the open-source representatives.\n\\begin{itemize}\n    \\item \\textit{FATE}~: FATE is an FL framework that can be deployed in distributed environments. In addition to its flexible ML pipeline, FATE also features several aspects that further facilitate the research on various goals of practical FL: (1) it supports privacy-preserving computation by implementing cryptographic algorithms such as the Diffie-Hellman key agreement~ and homomorphic encryption~; (2) it covers different training architectures including horizontal FL, vertical FL, and federated transfer learning; and (3) it allows a certain degree of customization on the FL pipeline such as the aggregation step. Given its heaviness in resource consumption, FATE is preferable in powering cross-silo applications instead of cross-device ones.\n    \\item \\textit{FedML}~: FedML is also a secure and versatile FL framework that supports distributed mode. Compared to FATE, FedML is more flexible in communication engineering due to the ease of customizing message flow and topology definitions. It is also more lightweight and can thus accommodate training on mobile or IoT devices. Moreover, it can be accelerated with GPUs, while FATE is currently not compatible with hardware accelerators.\n    \\item \\textit{Flower}~: Flower is a concurrent work with FedML, and it concentrates on providing a unified approach for FL with mobile devices. Similar to FedML, Flower bears in mind the goals of being (1) lightweight, (2) extensible, (3) scalable, and (4) compatible with diverse mobile platforms (e.g., Android and iOS) and ML-frameworks (e.g., PyTorch~ and Tensorflow~). It also implements secure aggregation methods like SecAgg (c.f. \\cref{sec:prior_reporting_round}) with easily configurable APIs~.\n    \\item \\textit{Plato}~: similar to Flower, Plato also aims to facilitate FL research atop multiple ML backends including PyTorch, Tensorflow, and MindSpore~. Different from its counterparts, Plato is optimized in the development process, e.g., making extensive use of plugin mechanisms to maximize extensibility and maintainability. Apart from lightweight distributed deployment, Plato also offers solutions to integrate secure communication, reverse proxy, and load balancing for best fitting in production environments.\n\\end{itemize}\nAlthough using production systems yields the most realistic insights, it may not be practical for researchers with limited resources and time budgets. To meet the growing demand for conducting agile FL research, several platforms \\textit{that enable system-aware simulation} have been developed. As opposed to system-unaware simulators (e.g., Tensorflow Federated~, PySyft~, LEAF~, OARF~, and FedEval~), these platforms respect the impact of client system heterogeneity by associating each client with her computation and communication speed, as well as availability dynamics, which are either set manually by developers or by replaying realistic traces. In addition, these platforms also excel in producing comprehensive metrics needed in performance analysis. Compared to real deployment, on the other hand, these systems allow researchers to make fast-forward progress without being blocked by real-world bottlenecks in computation and communication.\n\\begin{itemize}\n    \\item \\textit{Flower}~: besides deployment on real mobile devices as just mentioned, Flower also supports simulation in the cloud with configurable system-level parameters such as bandwidth constraints and computational capabilities. With that, researchers can experiment with larger and more compute-intensive FL workloads that cannot be run on today's mobile devices.\n    \\item \\textit{FedScale}~: aside from curating real-world datasets (\\cref{sec:cornerstone_benchmarking_datasets}), FedScale also builds an automated runtime to simulate FL in realistic settings. By design, FedScale integrates \\texttt{AI Benchmark} and \\texttt{MobiPerf Measurements} system traces to simulate clients' heterogeneous training speed and network throughput, respectively. It also incorporates a large-scale user behavior dataset that was formulated in~ to emulate clients' availability dynamics. Compared to Flower, it lacks support for deployment on real distributed devices. Still, it broadly simulates realistic cross-device heterogeneity and can embrace new behavior traces with its APIs.\n\\end{itemize}", "cites": [656, 655, 612, 41, 7199, 583, 653, 654, 651], "cite_extract_rate": 0.5294117647058824, "origin_cites_number": 17, "insight_result": {"type": "comparative", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides a comparative overview of different FL benchmarking platforms, integrating their features and capabilities. It connects ideas across papers by highlighting similarities (e.g., support for mobile devices) and differences (e.g., FedML vs. FATE in resource consumption). However, the analysis remains largely descriptive with limited deeper critique or abstraction of broader system design principles."}}
{"id": "526fca83-6724-45b6-99ae-76110ef046e4", "title": "Related Surveys\\label{sec:conclusion_related", "level": "subsection", "subsections": [], "parent_id": "24e6f906-e64a-4296-a2e5-eb81d7da883e", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Related Work and Concluding Remarks\\label{chap:conclusion"], ["subsection", "Related Surveys\\label{sec:conclusion_related"]], "content": "}\nThe motivation for this survey stems from three observations. First, few in-depth surveys focus on system optimization for FL. As FL features strict compliance to privacy regulations as opposed to traditional distributed learning, many survey efforts are directed to the unique challenges such as enforcing data privacy and model robustness~, while the system optimization issues receive less attention in dedicated surveys.\nAs for common system issues shared by FL and traditional distributed learning, there do exist extensive surveys with detailed discussion. In~, the authors discuss the realm of mobile distributed machine learning, where algorithms are classified into three categories: 1) machine learning optimizers, 2) distributed optimization algorithms, and 3) data aggregation methods. In , the authors provide a detailed survey of communication-efficient distributed training algorithms. They identify four key factors that affect the communication cost in distributed learning and organize the literature accordingly: 1) synchronous scheme, 2) system architecture, 3) compression techniques and 4) parallelism of communication and computation.~ also studies the communication issues in distributed deep learning, however, with a focus on the commonly used lossless methods. It also contains a quantitative analysis of these methods based on real-world experiments. Compared to our survey, the scope of these surveys is not narrowed down to FL and thus does not fully capture all the system optimization challenges that are unique in FL. Notably, FL has in its standard workflow the selection phase which needs particular investigation due to client heterogeneity, while traditional distributed learning does not even have the notion of clients.\nWe notice that there are comprehensive surveys that cover a wide range of FL techniques including system-level optimizations. In~, the authors discuss in-depth the advances and open problems in FL at the time of writing. The presented problems and solutions are presented in four categories: 1) efficiency and effectiveness, 2) privacy preservation, 3) robustness enforcement and 4) fairness establishment. In~, the authors extensively introduce the challenges and research directions of FL at and for mobile edge network. As for FL at mobile edge network, they particularly elaborate techniques on 1) communication efficiency, 2) resource allocation, and 3) privacy and security. In~, the authors provide recommendations on algorithm-level optimization of FL applications, while they also briefly sketch the system-level constraints and practices. They discuss the reduction of communication, computation, and memory costs, as well as propose a basic model to estimate the communication efficiency of cross-device FL deployment. In~, the authors present a fine-grained multi-level classification of the FL literature, spanning six major topics: 1) statistical challenges, 2) communication efficiency, 3) client selection and scheduling, 4) security concerns, 5) privacy concerns, and 6) service pricing. Compared to this survey, the above-mentioned work is neither intended to focus on system-level optimizations nor do they categorize the literature by the phases in the synchronous training process. The survey most closely related to ours is~ which is dedicated to summarizing system-level optimizations used in asynchronous FL. As their studied architecture is fundamentally different from ours, their survey could be a good complement to this survey. It is also noteworthy that their classification mechanism on existing techniques is built upon the type of client heterogeneity, which also differs from our taxonomy.\nIn short, this survey aims to provide a succinct yet complete view of an essential domain in the FL literature: system optimization in synchronous federated training. We also summarize in Tab.~\\ref{tab:survey} the main similarities and differences between our survey and the existing surveys that are closely related.\n\\begin{table*}[h]\n    \\begin{center}\n        \\caption{Comparative summary between our survey and the most related work.}\n    \\label{tab:survey}\n    \\resizebox{1.0\\linewidth}{!}{\n    \\begin{tabular}{cm{9cm}m{9cm}}\n        \\toprule\n        Survey & \\multicolumn{1}{c}{Similarities} & \\multicolumn{1}{c}{Differences} \\\\\n        \\midrule\n        Gu et al.~\n        &\n        Similar to our survey, this survey addresses some common challenges of synchronous federated training such as communication efficiency.\n        & \\hspace{-5mm}\n        \\begin{minipage}[b]{0.52\\textwidth}\n            \\vspace{1mm}\n            \\begin{itemize}\n            \\item As for machine learning optimizers, it does not discuss the server-side ones which are FL-specific.\n            \\item Our work addresses additional challenges that are not mentioned in this survey such as participant selection and aggregation efficiency.\n            \\end{itemize}\n            \\vspace{1mm}\n        \\end{minipage} \\\\\n        Tang et al.~\n        & Similar to our survey, this survey addresses some common challenges of synchronous federated training such as communication efficiency.\n        & \\hspace{-5mm}\n        \\begin{minipage}[b]{0.52\\textwidth}\n            \\vspace{1mm}\n            \\begin{itemize}\n            \\item It considers distributed vanilla SGD, which is different in convergence characteristics from FL, a special case of local SGD.\n            \\item Our work addresses additional challenges that are not mentioned in this survey such as participant selection and client bias reduction.\n            \\end{itemize}\n            \\vspace{1mm}\n        \\end{minipage} \\\\\n        Shi et al.~\n        & Similar to our survey, this survey addresses some common challenges of synchronous federated training such as communication efficiency.\n        & \\hspace{-5mm}\n        \\begin{minipage}[b]{0.52\\textwidth}\n            \\vspace{1mm}\n            \\begin{itemize}\n            \\item It focuses on controlling the system architecture and scheduling algorithms in data center learning, which is not applicable to synchronuous FL.\n            \\item Our work addresses additional challenges that are not mentioned in this survey such as participant selection and data heterogeneity.\n            \\end{itemize}\n            \\vspace{1mm}\n        \\end{minipage} \\\\\n        Kairouz et al.~\n        & Similar to our survey, this survey addresses some common challenges of synchronous federated training such as client bias reduction and communication efficiency.\n        & \\hspace{-5mm}\n        \\begin{minipage}[b]{0.52\\textwidth}\n            \\vspace{1mm}\n            \\begin{itemize}\n            \\item The scope of this survey is quite board, where for example theoretical analysis of convergence and privacy-preserving mechanisms are also extensively discussed.\n            \\item Our work addresses additional challenges that are not mentioned in this survey such as participant selection and load balancing.\n            \\end{itemize}\n            \\vspace{1mm}\n        \\end{minipage} \\\\\n        Lim et al.~\n        & Similar to our survey, this survey addresses some common challenges of FL such as communication efficiency and participant selection. It also mentions the standard workflow of synchronous training.\n        & \\hspace{-5mm}\n        \\begin{minipage}[b]{0.52\\textwidth}\n            \\vspace{1mm}\n            \\begin{itemize}\n            \\item The scope of this survey is quite board, where many applications of FL in mobile edge network, e.g., cyberattack detection, are also elaborated.\n            \\item They do not organize system-level optimizations by the phases in the standard training workflow.\n            \\item Our work addresses additional challenges that are not mentioned in this survey such as client bias reduction and lightweight privacy-preserving aggregation.\n            \\end{itemize}\n            \\vspace{1mm}\n        \\end{minipage} \\\\\n        Wang et al.~\n        & Similar to our survey, this survey addresses some common challenges of FL such as communication efficiency and lightweight privacy-preserving aggregation.\n        & \\hspace{-5mm}\n        \\begin{minipage}[b]{0.52\\textwidth}\n            \\vspace{1mm}\n            \\begin{itemize}\n            \\item It centers on the algorithm-level optimizations as well as their theoretical guanrantees, while the challenges and directives of system-level issues are briefly introduced.\n            \\item Our work addresses additional challenges that are not mentioned in this survey such as participant selection and load balancing.\n            \\end{itemize}\n            \\vspace{1mm}\n        \\end{minipage} \\\\\n        Wahab et al.~\n        & Similar to our survey, this survey addresses some common challenges of FL such as participant selection and communication efficiency.\n        & \\hspace{-5mm}\n        \\begin{minipage}[b]{0.52\\textwidth}\n            \\vspace{1mm}\n            \\begin{itemize}\n            \\item The scope of this survey is quite board, where even training service pricing are also included.\n            \\item Our work contains additional exploratory work in the system community that is not mentioned in this survey including measurement studies and benchmarking suites.\n            \\end{itemize}\n            \\vspace{1mm}\n        \\end{minipage} \\\\\n        Xu et al.~\n        & Similar to our survey, this survey focuses on optimizing FL training process in the precence of client heterogeneity.\n        & \\hspace{-5mm}\n        \\begin{minipage}[b]{0.52\\textwidth}\n            \\vspace{1mm}\n            \\begin{itemize}\n            \\item Its target architecture is asynchornous and thus the intersection of its introduced techniques and ours are fairly small.\n            \\item Our work sorts the literature by the phases in the standard synchronous training workflow, which is distinct from the classification mechanism used in this survey.\n            \\end{itemize}\n            \\vspace{1mm}\n        \\end{minipage} \\\\\n        \\bottomrule\n    \\end{tabular}\n    }\n    \\end{center}\n\\end{table*}", "cites": [661, 636, 657, 658, 8368, 659, 660, 591, 628], "cite_extract_rate": 0.6923076923076923, "origin_cites_number": 13, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section shows strong analytical insight by synthesizing the cited surveys, highlighting their similarities and differences, and positioning the current survey in the broader FL literature. It critically evaluates the scope and focus of other surveys, identifying gaps such as lack of emphasis on FL-specific system optimizations and workflow-based categorization. The abstraction is evident in its recognition of overarching themes like client selection and communication efficiency, but could offer more generalized frameworks for deeper insight."}}
{"id": "e4f64951-7cd3-49d1-bb82-2ae75da77f76", "title": "On the Selection Phase\\label{sec:conclusion_future_selection", "level": "subsubsection", "subsections": [], "parent_id": "1404800d-4173-49f1-b574-96677b669f93", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Related Work and Concluding Remarks\\label{chap:conclusion"], ["subsection", "Future Research Directions\\label{sec:conclusion_future"], ["subsubsection", "On the Selection Phase\\label{sec:conclusion_future_selection"]], "content": "}\nWhile RL agents are commonly used in guiding client selection for their beneficial balance between exploitation and exploration~, their training costs are inherently high. Before an RL agent is ready for use, it has to learn from tens to hundreds of full-sized FL training processes, whose cost may not be justifiable in practice. Inspired by the experience from container management~, we are interested in whether certain techniques, e.g., transfer learning~, can be adopted so that an RL agent can be trained from prior FL tasks, instead of learning from scratch for every single task.\nMoreover, existing algorithms all assume the participation scale (i.e., the number of clients training simultaneously) to be the magnitude of a hundred, because involving more clients in a round is observed to have marginal benefits under primary aggregation methods (e.g., FedAvg~). On the other hand, the number of available clients at each minute can be as many as thousands in the cross-device practice. It is still desirable to explore the selection of a larger crowd for boosting the time-to-accuracy performance.", "cites": [585, 582], "cite_extract_rate": 0.4, "origin_cites_number": 5, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.0, "critical": 3.0, "abstraction": 3.0}, "insight_level": "medium", "analysis": "The section provides analytical insights by identifying limitations in current client selection methods, particularly the high training cost of RL agents and the assumption of limited participation scale. It connects these ideas to broader trends in FL optimization but does not fully synthesize or compare multiple papers. The critique is valid but not deeply nuanced, and the generalization stops at identifying a potential research direction rather than revealing overarching principles."}}
{"id": "3dbd150d-cc33-4585-ba9e-a4c4d2278414", "title": "On the Configuration Phase\\label{sec:conclusion_future_configuration", "level": "subsubsection", "subsections": [], "parent_id": "1404800d-4173-49f1-b574-96677b669f93", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Related Work and Concluding Remarks\\label{chap:conclusion"], ["subsection", "Future Research Directions\\label{sec:conclusion_future"], ["subsubsection", "On the Configuration Phase\\label{sec:conclusion_future_configuration"]], "content": "}\nAligned with the observations in the literature~, most prior arts consistently configure different clients. Although there exist some heterogeneity-aware efforts like load balancing where the number of batches, batch size, and number of local epochs can vary across clients (\\cref{sec:prior_configuration_latency}), we anticipate that the design space for heterogeneity-aware client configurations could be larger, e.g., using different compression ratios or synchronization frequencies.\nTo achieve efficient and secure communication, it is also worth studying how to combine sparsification techniques with privacy-preserving methods. For example, model sparsification is by nature not compatible with secure multi-party computation protocols such as SecAgg~, because the coordinates of sparsified model values often vary across clients, preventing the pairwise masks from being canceled out during model aggregation as required by SecAgg. A recent work named SparseSecAgg~ attempts to tackle this issue, however, it implies the need for sharing some sparsified locations between each pair of clients, which cannot be directly extended to conventional sparsification schemes such as the top $s\\%$ scheme (\\cref{sec:prior_configuration_size}). More general reconciliation between the two techniques is still ripe for future investigation.", "cites": [662], "cite_extract_rate": 0.3333333333333333, "origin_cites_number": 3, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the concept of client configuration in FL with specific focus on sparsification and secure aggregation, integrating the cited paper on SparseSecAgg to highlight compatibility issues. It provides a critical analysis by identifying limitations in current approaches, such as the incompatibility between standard sparsification and SecAgg. While it offers some abstraction by pointing to broader research opportunities, the insights remain grounded in the specific context of FL configuration and privacy techniques."}}
{"id": "0e591dcb-87fb-4bba-a561-d583bde20c52", "title": "On the Reporting Phase\\label{sec:conclusion_future_reporting", "level": "subsubsection", "subsections": [], "parent_id": "1404800d-4173-49f1-b574-96677b669f93", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Related Work and Concluding Remarks\\label{chap:conclusion"], ["subsection", "Future Research Directions\\label{sec:conclusion_future"], ["subsubsection", "On the Reporting Phase\\label{sec:conclusion_future_reporting"]], "content": "}\nAs the system bottleneck is usually assumed to locate in clients instead of the server, most of the existing optimization efforts focus on improving the utility (system and statistical) of clients. It is thus interesting to investigate whether such an assumption holds in practice, especially when the scalability of the server is restricted due to rigid capabilities or limited budgets, or when its aggregation latency is not negligible due to the presence of large-scale models. \nBesides, existing lightweight privacy-preserving aggregation methods are not compatible with robustness enforcement techniques~. Because the true values of local models are concealed from the server, it has no way of inspecting their numerical features~ or validating their performance for anomaly detection~. This raises the tension between privacy and robustness. Leveraging Trusted Execution Environment (TEE)  to perform model inspection securely might be a helpful workaround. However, due to current limits on TEE's hardware capabilities , there could be a foreseeably large performance downgrade of doing so. It thus remains largely unexplored as to how to navigate the sweet point of jointly maximizing accuracy, performance, privacy, and robustness in synchronous federated training.", "cites": [664, 663, 666, 7038, 665, 591], "cite_extract_rate": 0.5454545454545454, "origin_cites_number": 11, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 4.0, "abstraction": 3.5}, "insight_level": "high", "analysis": "The section integrates several papers on Byzantine-robust learning and poisoning attacks to identify a key tension between privacy and robustness in the reporting phase. It critically analyzes the incompatibility of lightweight privacy methods with robustness techniques and highlights the limitations of current Trusted Execution Environments (TEEs). While not forming a novel framework, it offers meta-level insights about the trade-offs and challenges in designing secure and efficient federated learning systems."}}
{"id": "19eb0c2c-4b7e-4dcc-9766-a74abafd9f2f", "title": "Cross-Device FL and Cross-Silo FL\\label{sec:conclusion_dicussion_cross", "level": "subsubsection", "subsections": [], "parent_id": "02bbb984-68d7-4c50-804c-74430a998104", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Related Work and Concluding Remarks\\label{chap:conclusion"], ["subsection", "Discussion\\label{sec:conclusion_dicussion"], ["subsubsection", "Cross-Device FL and Cross-Silo FL\\label{sec:conclusion_dicussion_cross"]], "content": "}\nFL applications are often categorized as either cross-device FL (where the participants are a mass of less capable mobile or IoT devices) or cross-silo FL (where the participants are 2-100 organizational entities)~. While the FL workflow that we base on throughout this survey is primarily proposed for cross-device FL~, it also generalizes to cross-silo settings. Hence, the scope of this survey does not preclude cross-silo FL, and hence many practical methods mentioned here should apply to both settings. For those techniques that are suitable for merely one setting, we have emphasized their limitations and stated the practical reasons behind them.", "cites": [590, 591, 583], "cite_extract_rate": 0.75, "origin_cites_number": 4, "insight_result": {"type": "analytical", "scores": {"synthesis": 3.5, "critical": 3.0, "abstraction": 3.5}, "insight_level": "medium", "analysis": "The section synthesizes the concept of FL into two main categories—cross-device and cross-silo—drawing from the cited papers to establish a coherent distinction. It also abstracts to a broader understanding by noting that the survey’s FL workflow generalizes across both settings. While it mentions limitations of certain techniques, the critical analysis is moderate and does not deeply evaluate or contrast the cited works."}}
{"id": "23cebd68-fb90-4604-b70b-25f0eba0e52b", "title": "Horizontal FL and Vertical FL\\label{sec:conclusion_dicussion_horizontal", "level": "subsubsection", "subsections": [], "parent_id": "02bbb984-68d7-4c50-804c-74430a998104", "prefix_titles": [["title", "Towards Efficient Synchronous\\\\Federated Training: A Survey on\\\\System Optimization Strategies"], ["section", "Related Work and Concluding Remarks\\label{chap:conclusion"], ["subsection", "Discussion\\label{sec:conclusion_dicussion"], ["subsubsection", "Horizontal FL and Vertical FL\\label{sec:conclusion_dicussion_horizontal"]], "content": "}\nBased on the characteristics of data distribution, FL applications can also basically be classified as horizontal FL (where clients' data have the same feature space but different samples) or vertical FL (where clients' data share the same sample space but have different features)~. Horizontal FL is typically initiated by a service provider who wants to improve the quality of an ML application to better fit the end users' data of a specific domain. For example, by combining the statistics of users' input habits, the developers of a mobile keyboard application could achieve a more intelligent prediction of the next words typed by the users, thus enhancing their experience.\nOn the other hand, vertical FL participants are usually a few organizations who hold data in different domains while being likely to achieve win-win cooperation by knowledge sharing. For instance, a regional e-commerce company and a bank may share the same set of clients. By incorporating the knowledge of the clients' revenue and expenditure recorded at the bank as well as the purchasing traces collected by the e-commerce, they can build a more accurate prediction model on the clients' purchasing behaviors, benefiting both of their businesses.\nGiven that there exist diverse training workflows of vertical FL and the community has not yet achieved a consensus on the use of any one~, much discussion in this survey is biased towards horizontal FL that has a widely acknowledged architecture (\\cref{sec:background_federated}). Still, we can provide some insights for optimizing vertical FL training in general:\n\\begin{enumerate}\n    \\item As the data is feature-partitioned, vertical FL needs the participation of all clients in each model update attempt. Thus, optimizations on participant selection (\\cref{sec:prior_selection}) are not applicable to vertical FL.\n    \\item In terms of communication, model-agnostic compression techniques (\\cref{sec:prior_configuration_size}) should still apply to plaintext variants of vertical FL such as~. However, synchronization frequency reduction (\\cref{sec:prior_configuration_freq}) is not possible in vertical FL, as each client does not have a complete model and cannot conduct training independently.\n    \\item As for local training, as all participants need processing data of the same sample space, load balancing techniques that assign a different amount of data samples to each client (\\cref{sec:prior_configuration_latency}) are not feasible in vertical FL. Moreover, as clients inherently have different local models, adaptive optimization and bias reduction methods (\\cref{sec:prior_configuration_round}) from horizontal FL do not generalize well to vertical FL. \n    \\item Unlike horizontal FL, the forms of aggregation in vertical FL differ significantly by the model architecture. Thus, the mentioned optimizations (\\cref{sec:prior_reporting}) can hardly be a generic remedy.\n\\end{enumerate}\nBesides horizontal FL and vertical FL, there recently emerges a novel collaborative learning paradigm called Federated Transfer Learning (FTL)~ which can cope with more relaxed assumptions of client data distribution using transfer learning techniques~. Specifically, it deals with the cases where two clients' data not only differ in sample space but also feature space. Toward this end, FTL learns a common feature representation between the two clients and minimizes the empirical errors in predicting one client's labels by leveraging the labels of other clients. As FTL's training workflow ensembles some variants of vertical FL~, we believe that the above-mentioned insights for vertical FL also hold for FTL.", "cites": [669, 670, 667, 671, 668], "cite_extract_rate": 0.625, "origin_cites_number": 8, "insight_result": {"type": "analytical", "scores": {"synthesis": 4.0, "critical": 3.5, "abstraction": 4.0}, "insight_level": "high", "analysis": "The section effectively synthesizes the cited papers to distinguish between horizontal and vertical FL and extrapolates their implications for system optimization. It provides a critical analysis by highlighting why certain optimization strategies are not applicable to vertical FL and abstracts broader patterns in training workflow differences. While the critique is not deeply nuanced, it offers clear analytical insights into the architectural and operational distinctions of FL paradigms."}}
